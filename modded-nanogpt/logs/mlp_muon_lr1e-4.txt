import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_muon_lr1e-4"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-4, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 03:48:17 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   29C    P0             110W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   28C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   27C    P0             108W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   29C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:69ms step_avg:68.77ms
step:2/2330 train_time:153ms step_avg:76.53ms
step:3/2330 train_time:166ms step_avg:55.49ms
step:4/2330 train_time:181ms step_avg:45.16ms
step:5/2330 train_time:193ms step_avg:38.62ms
step:6/2330 train_time:223ms step_avg:37.24ms
step:7/2330 train_time:257ms step_avg:36.70ms
step:8/2330 train_time:301ms step_avg:37.57ms
step:9/2330 train_time:334ms step_avg:37.14ms
step:10/2330 train_time:378ms step_avg:37.78ms
step:11/2330 train_time:412ms step_avg:37.50ms
step:12/2330 train_time:457ms step_avg:38.06ms
step:13/2330 train_time:491ms step_avg:37.80ms
step:14/2330 train_time:535ms step_avg:38.24ms
step:15/2330 train_time:570ms step_avg:38.00ms
step:16/2330 train_time:613ms step_avg:38.34ms
step:17/2330 train_time:649ms step_avg:38.16ms
step:18/2330 train_time:693ms step_avg:38.48ms
step:19/2330 train_time:727ms step_avg:38.27ms
step:20/2330 train_time:771ms step_avg:38.54ms
step:21/2330 train_time:805ms step_avg:38.35ms
step:22/2330 train_time:850ms step_avg:38.62ms
step:23/2330 train_time:884ms step_avg:38.44ms
step:24/2330 train_time:928ms step_avg:38.66ms
step:25/2330 train_time:963ms step_avg:38.50ms
step:26/2330 train_time:1008ms step_avg:38.76ms
step:27/2330 train_time:1048ms step_avg:38.80ms
step:28/2330 train_time:1098ms step_avg:39.21ms
step:29/2330 train_time:1136ms step_avg:39.16ms
step:30/2330 train_time:1182ms step_avg:39.39ms
step:31/2330 train_time:1217ms step_avg:39.26ms
step:32/2330 train_time:1261ms step_avg:39.41ms
step:33/2330 train_time:1296ms step_avg:39.29ms
step:34/2330 train_time:1341ms step_avg:39.44ms
step:35/2330 train_time:1376ms step_avg:39.31ms
step:36/2330 train_time:1421ms step_avg:39.46ms
step:37/2330 train_time:1456ms step_avg:39.34ms
step:38/2330 train_time:1500ms step_avg:39.48ms
step:39/2330 train_time:1535ms step_avg:39.36ms
step:40/2330 train_time:1580ms step_avg:39.51ms
step:41/2330 train_time:1615ms step_avg:39.40ms
step:42/2330 train_time:1660ms step_avg:39.53ms
step:43/2330 train_time:1695ms step_avg:39.43ms
step:44/2330 train_time:1740ms step_avg:39.55ms
step:45/2330 train_time:1775ms step_avg:39.45ms
step:46/2330 train_time:1820ms step_avg:39.56ms
step:47/2330 train_time:1855ms step_avg:39.47ms
step:48/2330 train_time:1900ms step_avg:39.59ms
step:49/2330 train_time:1935ms step_avg:39.50ms
step:50/2330 train_time:1981ms step_avg:39.62ms
step:51/2330 train_time:2017ms step_avg:39.55ms
step:52/2330 train_time:2064ms step_avg:39.69ms
step:53/2330 train_time:2100ms step_avg:39.63ms
step:54/2330 train_time:2146ms step_avg:39.75ms
step:55/2330 train_time:2183ms step_avg:39.69ms
step:56/2330 train_time:2229ms step_avg:39.81ms
step:57/2330 train_time:2267ms step_avg:39.76ms
step:58/2330 train_time:2311ms step_avg:39.85ms
step:59/2330 train_time:2347ms step_avg:39.79ms
step:60/2330 train_time:2392ms step_avg:39.87ms
step:61/2330 train_time:2427ms step_avg:39.79ms
step:62/2330 train_time:2472ms step_avg:39.88ms
step:63/2330 train_time:2507ms step_avg:39.79ms
step:64/2330 train_time:2552ms step_avg:39.87ms
step:65/2330 train_time:2587ms step_avg:39.80ms
step:66/2330 train_time:2631ms step_avg:39.87ms
step:67/2330 train_time:2666ms step_avg:39.80ms
step:68/2330 train_time:2711ms step_avg:39.87ms
step:69/2330 train_time:2746ms step_avg:39.79ms
step:70/2330 train_time:2791ms step_avg:39.87ms
step:71/2330 train_time:2826ms step_avg:39.80ms
step:72/2330 train_time:2871ms step_avg:39.87ms
step:73/2330 train_time:2906ms step_avg:39.80ms
step:74/2330 train_time:2951ms step_avg:39.88ms
step:75/2330 train_time:2988ms step_avg:39.83ms
step:76/2330 train_time:3033ms step_avg:39.91ms
step:77/2330 train_time:3069ms step_avg:39.86ms
step:78/2330 train_time:3115ms step_avg:39.94ms
step:79/2330 train_time:3151ms step_avg:39.88ms
step:80/2330 train_time:3197ms step_avg:39.96ms
step:81/2330 train_time:3232ms step_avg:39.91ms
step:82/2330 train_time:3278ms step_avg:39.98ms
step:83/2330 train_time:3313ms step_avg:39.92ms
step:84/2330 train_time:3358ms step_avg:39.98ms
step:85/2330 train_time:3393ms step_avg:39.92ms
step:86/2330 train_time:3437ms step_avg:39.97ms
step:87/2330 train_time:3473ms step_avg:39.92ms
step:88/2330 train_time:3517ms step_avg:39.97ms
step:89/2330 train_time:3553ms step_avg:39.92ms
step:90/2330 train_time:3598ms step_avg:39.98ms
step:91/2330 train_time:3634ms step_avg:39.93ms
step:92/2330 train_time:3679ms step_avg:39.99ms
step:93/2330 train_time:3714ms step_avg:39.93ms
step:94/2330 train_time:3758ms step_avg:39.98ms
step:95/2330 train_time:3794ms step_avg:39.94ms
step:96/2330 train_time:3839ms step_avg:39.99ms
step:97/2330 train_time:3875ms step_avg:39.94ms
step:98/2330 train_time:3920ms step_avg:40.00ms
step:99/2330 train_time:3957ms step_avg:39.97ms
step:100/2330 train_time:4003ms step_avg:40.03ms
step:101/2330 train_time:4038ms step_avg:39.98ms
step:102/2330 train_time:4083ms step_avg:40.03ms
step:103/2330 train_time:4119ms step_avg:39.99ms
step:104/2330 train_time:4163ms step_avg:40.03ms
step:105/2330 train_time:4199ms step_avg:39.99ms
step:106/2330 train_time:4245ms step_avg:40.05ms
step:107/2330 train_time:4281ms step_avg:40.01ms
step:108/2330 train_time:4327ms step_avg:40.06ms
step:109/2330 train_time:4363ms step_avg:40.03ms
step:110/2330 train_time:4408ms step_avg:40.07ms
step:111/2330 train_time:4444ms step_avg:40.04ms
step:112/2330 train_time:4491ms step_avg:40.10ms
step:113/2330 train_time:4526ms step_avg:40.05ms
step:114/2330 train_time:4571ms step_avg:40.10ms
step:115/2330 train_time:4607ms step_avg:40.06ms
step:116/2330 train_time:4652ms step_avg:40.11ms
step:117/2330 train_time:4689ms step_avg:40.08ms
step:118/2330 train_time:4734ms step_avg:40.12ms
step:119/2330 train_time:4770ms step_avg:40.08ms
step:120/2330 train_time:4815ms step_avg:40.13ms
step:121/2330 train_time:4851ms step_avg:40.09ms
step:122/2330 train_time:4896ms step_avg:40.13ms
step:123/2330 train_time:4931ms step_avg:40.09ms
step:124/2330 train_time:4976ms step_avg:40.13ms
step:125/2330 train_time:5011ms step_avg:40.09ms
step:126/2330 train_time:5057ms step_avg:40.13ms
step:127/2330 train_time:5093ms step_avg:40.10ms
step:128/2330 train_time:5139ms step_avg:40.15ms
step:129/2330 train_time:5174ms step_avg:40.11ms
step:130/2330 train_time:5219ms step_avg:40.14ms
step:131/2330 train_time:5254ms step_avg:40.11ms
step:132/2330 train_time:5299ms step_avg:40.15ms
step:133/2330 train_time:5334ms step_avg:40.11ms
step:134/2330 train_time:5380ms step_avg:40.15ms
step:135/2330 train_time:5415ms step_avg:40.11ms
step:136/2330 train_time:5461ms step_avg:40.15ms
step:137/2330 train_time:5496ms step_avg:40.12ms
step:138/2330 train_time:5541ms step_avg:40.15ms
step:139/2330 train_time:5576ms step_avg:40.12ms
step:140/2330 train_time:5622ms step_avg:40.15ms
step:141/2330 train_time:5658ms step_avg:40.13ms
step:142/2330 train_time:5703ms step_avg:40.16ms
step:143/2330 train_time:5740ms step_avg:40.14ms
step:144/2330 train_time:5785ms step_avg:40.17ms
step:145/2330 train_time:5820ms step_avg:40.14ms
step:146/2330 train_time:5865ms step_avg:40.17ms
step:147/2330 train_time:5900ms step_avg:40.13ms
step:148/2330 train_time:5945ms step_avg:40.17ms
step:149/2330 train_time:5980ms step_avg:40.14ms
step:150/2330 train_time:6026ms step_avg:40.17ms
step:151/2330 train_time:6061ms step_avg:40.14ms
step:152/2330 train_time:6106ms step_avg:40.17ms
step:153/2330 train_time:6142ms step_avg:40.14ms
step:154/2330 train_time:6187ms step_avg:40.18ms
step:155/2330 train_time:6223ms step_avg:40.15ms
step:156/2330 train_time:6269ms step_avg:40.19ms
step:157/2330 train_time:6305ms step_avg:40.16ms
step:158/2330 train_time:6352ms step_avg:40.20ms
step:159/2330 train_time:6387ms step_avg:40.17ms
step:160/2330 train_time:6432ms step_avg:40.20ms
step:161/2330 train_time:6467ms step_avg:40.17ms
step:162/2330 train_time:6512ms step_avg:40.20ms
step:163/2330 train_time:6547ms step_avg:40.17ms
step:164/2330 train_time:6593ms step_avg:40.20ms
step:165/2330 train_time:6628ms step_avg:40.17ms
step:166/2330 train_time:6673ms step_avg:40.20ms
step:167/2330 train_time:6708ms step_avg:40.17ms
step:168/2330 train_time:6754ms step_avg:40.20ms
step:169/2330 train_time:6790ms step_avg:40.18ms
step:170/2330 train_time:6835ms step_avg:40.20ms
step:171/2330 train_time:6869ms step_avg:40.17ms
step:172/2330 train_time:6914ms step_avg:40.20ms
step:173/2330 train_time:6950ms step_avg:40.17ms
step:174/2330 train_time:6996ms step_avg:40.20ms
step:175/2330 train_time:7031ms step_avg:40.18ms
step:176/2330 train_time:7076ms step_avg:40.21ms
step:177/2330 train_time:7111ms step_avg:40.18ms
step:178/2330 train_time:7157ms step_avg:40.21ms
step:179/2330 train_time:7192ms step_avg:40.18ms
step:180/2330 train_time:7236ms step_avg:40.20ms
step:181/2330 train_time:7272ms step_avg:40.18ms
step:182/2330 train_time:7317ms step_avg:40.20ms
step:183/2330 train_time:7353ms step_avg:40.18ms
step:184/2330 train_time:7398ms step_avg:40.21ms
step:185/2330 train_time:7434ms step_avg:40.18ms
step:186/2330 train_time:7480ms step_avg:40.21ms
step:187/2330 train_time:7516ms step_avg:40.19ms
step:188/2330 train_time:7561ms step_avg:40.22ms
step:189/2330 train_time:7598ms step_avg:40.20ms
step:190/2330 train_time:7642ms step_avg:40.22ms
step:191/2330 train_time:7677ms step_avg:40.19ms
step:192/2330 train_time:7722ms step_avg:40.22ms
step:193/2330 train_time:7758ms step_avg:40.20ms
step:194/2330 train_time:7804ms step_avg:40.22ms
step:195/2330 train_time:7839ms step_avg:40.20ms
step:196/2330 train_time:7884ms step_avg:40.23ms
step:197/2330 train_time:7920ms step_avg:40.20ms
step:198/2330 train_time:7965ms step_avg:40.22ms
step:199/2330 train_time:8000ms step_avg:40.20ms
step:200/2330 train_time:8045ms step_avg:40.23ms
step:201/2330 train_time:8081ms step_avg:40.21ms
step:202/2330 train_time:8127ms step_avg:40.23ms
step:203/2330 train_time:8162ms step_avg:40.21ms
step:204/2330 train_time:8208ms step_avg:40.23ms
step:205/2330 train_time:8244ms step_avg:40.21ms
step:206/2330 train_time:8290ms step_avg:40.24ms
step:207/2330 train_time:8326ms step_avg:40.22ms
step:208/2330 train_time:8371ms step_avg:40.25ms
step:209/2330 train_time:8407ms step_avg:40.23ms
step:210/2330 train_time:8453ms step_avg:40.25ms
step:211/2330 train_time:8488ms step_avg:40.23ms
step:212/2330 train_time:8533ms step_avg:40.25ms
step:213/2330 train_time:8569ms step_avg:40.23ms
step:214/2330 train_time:8614ms step_avg:40.25ms
step:215/2330 train_time:8650ms step_avg:40.23ms
step:216/2330 train_time:8696ms step_avg:40.26ms
step:217/2330 train_time:8732ms step_avg:40.24ms
step:218/2330 train_time:8777ms step_avg:40.26ms
step:219/2330 train_time:8812ms step_avg:40.24ms
step:220/2330 train_time:8857ms step_avg:40.26ms
step:221/2330 train_time:8893ms step_avg:40.24ms
step:222/2330 train_time:8938ms step_avg:40.26ms
step:223/2330 train_time:8973ms step_avg:40.24ms
step:224/2330 train_time:9018ms step_avg:40.26ms
step:225/2330 train_time:9053ms step_avg:40.24ms
step:226/2330 train_time:9098ms step_avg:40.26ms
step:227/2330 train_time:9133ms step_avg:40.24ms
step:228/2330 train_time:9179ms step_avg:40.26ms
step:229/2330 train_time:9215ms step_avg:40.24ms
step:230/2330 train_time:9260ms step_avg:40.26ms
step:231/2330 train_time:9296ms step_avg:40.24ms
step:232/2330 train_time:9341ms step_avg:40.26ms
step:233/2330 train_time:9377ms step_avg:40.24ms
step:234/2330 train_time:9422ms step_avg:40.26ms
step:235/2330 train_time:9458ms step_avg:40.25ms
step:236/2330 train_time:9503ms step_avg:40.27ms
step:237/2330 train_time:9539ms step_avg:40.25ms
step:238/2330 train_time:9584ms step_avg:40.27ms
step:239/2330 train_time:9620ms step_avg:40.25ms
step:240/2330 train_time:9665ms step_avg:40.27ms
step:241/2330 train_time:9701ms step_avg:40.25ms
step:242/2330 train_time:9747ms step_avg:40.28ms
step:243/2330 train_time:9782ms step_avg:40.26ms
step:244/2330 train_time:9827ms step_avg:40.28ms
step:245/2330 train_time:9863ms step_avg:40.26ms
step:246/2330 train_time:9910ms step_avg:40.28ms
step:247/2330 train_time:9947ms step_avg:40.27ms
step:248/2330 train_time:9993ms step_avg:40.29ms
step:249/2330 train_time:10028ms step_avg:40.27ms
step:250/2330 train_time:10073ms step_avg:40.29ms
step:250/2330 val_loss:5.7716 train_time:10160ms step_avg:40.64ms
step:251/2330 train_time:10174ms step_avg:40.53ms
step:252/2330 train_time:10188ms step_avg:40.43ms
step:253/2330 train_time:10199ms step_avg:40.31ms
step:254/2330 train_time:10233ms step_avg:40.29ms
step:255/2330 train_time:10268ms step_avg:40.27ms
step:256/2330 train_time:10312ms step_avg:40.28ms
step:257/2330 train_time:10347ms step_avg:40.26ms
step:258/2330 train_time:10391ms step_avg:40.28ms
step:259/2330 train_time:10426ms step_avg:40.25ms
step:260/2330 train_time:10471ms step_avg:40.27ms
step:261/2330 train_time:10510ms step_avg:40.27ms
step:262/2330 train_time:10559ms step_avg:40.30ms
step:263/2330 train_time:10597ms step_avg:40.29ms
step:264/2330 train_time:10643ms step_avg:40.32ms
step:265/2330 train_time:10679ms step_avg:40.30ms
step:266/2330 train_time:10724ms step_avg:40.32ms
step:267/2330 train_time:10760ms step_avg:40.30ms
step:268/2330 train_time:10805ms step_avg:40.32ms
step:269/2330 train_time:10841ms step_avg:40.30ms
step:270/2330 train_time:10886ms step_avg:40.32ms
step:271/2330 train_time:10921ms step_avg:40.30ms
step:272/2330 train_time:10965ms step_avg:40.31ms
step:273/2330 train_time:11000ms step_avg:40.29ms
step:274/2330 train_time:11046ms step_avg:40.32ms
step:275/2330 train_time:11084ms step_avg:40.31ms
step:276/2330 train_time:11132ms step_avg:40.33ms
step:277/2330 train_time:11168ms step_avg:40.32ms
step:278/2330 train_time:11214ms step_avg:40.34ms
step:279/2330 train_time:11249ms step_avg:40.32ms
step:280/2330 train_time:11293ms step_avg:40.33ms
step:281/2330 train_time:11328ms step_avg:40.31ms
step:282/2330 train_time:11372ms step_avg:40.33ms
step:283/2330 train_time:11407ms step_avg:40.31ms
step:284/2330 train_time:11453ms step_avg:40.33ms
step:285/2330 train_time:11489ms step_avg:40.31ms
step:286/2330 train_time:11535ms step_avg:40.33ms
step:287/2330 train_time:11573ms step_avg:40.32ms
step:288/2330 train_time:11619ms step_avg:40.34ms
step:289/2330 train_time:11656ms step_avg:40.33ms
step:290/2330 train_time:11702ms step_avg:40.35ms
step:291/2330 train_time:11737ms step_avg:40.33ms
step:292/2330 train_time:11781ms step_avg:40.35ms
step:293/2330 train_time:11817ms step_avg:40.33ms
step:294/2330 train_time:11862ms step_avg:40.35ms
step:295/2330 train_time:11897ms step_avg:40.33ms
step:296/2330 train_time:11941ms step_avg:40.34ms
step:297/2330 train_time:11976ms step_avg:40.32ms
step:298/2330 train_time:12022ms step_avg:40.34ms
step:299/2330 train_time:12057ms step_avg:40.33ms
step:300/2330 train_time:12103ms step_avg:40.34ms
step:301/2330 train_time:12138ms step_avg:40.33ms
step:302/2330 train_time:12185ms step_avg:40.35ms
step:303/2330 train_time:12221ms step_avg:40.33ms
step:304/2330 train_time:12266ms step_avg:40.35ms
step:305/2330 train_time:12302ms step_avg:40.33ms
step:306/2330 train_time:12347ms step_avg:40.35ms
step:307/2330 train_time:12383ms step_avg:40.33ms
step:308/2330 train_time:12429ms step_avg:40.35ms
step:309/2330 train_time:12465ms step_avg:40.34ms
step:310/2330 train_time:12510ms step_avg:40.36ms
step:311/2330 train_time:12546ms step_avg:40.34ms
step:312/2330 train_time:12592ms step_avg:40.36ms
step:313/2330 train_time:12628ms step_avg:40.35ms
step:314/2330 train_time:12674ms step_avg:40.36ms
step:315/2330 train_time:12710ms step_avg:40.35ms
step:316/2330 train_time:12755ms step_avg:40.37ms
step:317/2330 train_time:12791ms step_avg:40.35ms
step:318/2330 train_time:12837ms step_avg:40.37ms
step:319/2330 train_time:12872ms step_avg:40.35ms
step:320/2330 train_time:12918ms step_avg:40.37ms
step:321/2330 train_time:12955ms step_avg:40.36ms
step:322/2330 train_time:13001ms step_avg:40.37ms
step:323/2330 train_time:13036ms step_avg:40.36ms
step:324/2330 train_time:13081ms step_avg:40.37ms
step:325/2330 train_time:13117ms step_avg:40.36ms
step:326/2330 train_time:13162ms step_avg:40.37ms
step:327/2330 train_time:13198ms step_avg:40.36ms
step:328/2330 train_time:13243ms step_avg:40.38ms
step:329/2330 train_time:13278ms step_avg:40.36ms
step:330/2330 train_time:13324ms step_avg:40.37ms
step:331/2330 train_time:13361ms step_avg:40.36ms
step:332/2330 train_time:13407ms step_avg:40.38ms
step:333/2330 train_time:13442ms step_avg:40.37ms
step:334/2330 train_time:13487ms step_avg:40.38ms
step:335/2330 train_time:13523ms step_avg:40.37ms
step:336/2330 train_time:13569ms step_avg:40.38ms
step:337/2330 train_time:13604ms step_avg:40.37ms
step:338/2330 train_time:13650ms step_avg:40.38ms
step:339/2330 train_time:13685ms step_avg:40.37ms
step:340/2330 train_time:13730ms step_avg:40.38ms
step:341/2330 train_time:13766ms step_avg:40.37ms
step:342/2330 train_time:13811ms step_avg:40.38ms
step:343/2330 train_time:13847ms step_avg:40.37ms
step:344/2330 train_time:13892ms step_avg:40.39ms
step:345/2330 train_time:13928ms step_avg:40.37ms
step:346/2330 train_time:13973ms step_avg:40.38ms
step:347/2330 train_time:14009ms step_avg:40.37ms
step:348/2330 train_time:14054ms step_avg:40.38ms
step:349/2330 train_time:14090ms step_avg:40.37ms
step:350/2330 train_time:14135ms step_avg:40.38ms
step:351/2330 train_time:14171ms step_avg:40.37ms
step:352/2330 train_time:14216ms step_avg:40.39ms
step:353/2330 train_time:14253ms step_avg:40.38ms
step:354/2330 train_time:14299ms step_avg:40.39ms
step:355/2330 train_time:14335ms step_avg:40.38ms
step:356/2330 train_time:14380ms step_avg:40.39ms
step:357/2330 train_time:14417ms step_avg:40.38ms
step:358/2330 train_time:14462ms step_avg:40.40ms
step:359/2330 train_time:14499ms step_avg:40.39ms
step:360/2330 train_time:14543ms step_avg:40.40ms
step:361/2330 train_time:14578ms step_avg:40.38ms
step:362/2330 train_time:14625ms step_avg:40.40ms
step:363/2330 train_time:14660ms step_avg:40.39ms
step:364/2330 train_time:14706ms step_avg:40.40ms
step:365/2330 train_time:14741ms step_avg:40.39ms
step:366/2330 train_time:14787ms step_avg:40.40ms
step:367/2330 train_time:14822ms step_avg:40.39ms
step:368/2330 train_time:14868ms step_avg:40.40ms
step:369/2330 train_time:14903ms step_avg:40.39ms
step:370/2330 train_time:14949ms step_avg:40.40ms
step:371/2330 train_time:14985ms step_avg:40.39ms
step:372/2330 train_time:15031ms step_avg:40.41ms
step:373/2330 train_time:15067ms step_avg:40.40ms
step:374/2330 train_time:15112ms step_avg:40.41ms
step:375/2330 train_time:15147ms step_avg:40.39ms
step:376/2330 train_time:15193ms step_avg:40.41ms
step:377/2330 train_time:15228ms step_avg:40.39ms
step:378/2330 train_time:15273ms step_avg:40.41ms
step:379/2330 train_time:15309ms step_avg:40.39ms
step:380/2330 train_time:15356ms step_avg:40.41ms
step:381/2330 train_time:15392ms step_avg:40.40ms
step:382/2330 train_time:15438ms step_avg:40.41ms
step:383/2330 train_time:15474ms step_avg:40.40ms
step:384/2330 train_time:15520ms step_avg:40.42ms
step:385/2330 train_time:15556ms step_avg:40.41ms
step:386/2330 train_time:15601ms step_avg:40.42ms
step:387/2330 train_time:15637ms step_avg:40.40ms
step:388/2330 train_time:15682ms step_avg:40.42ms
step:389/2330 train_time:15718ms step_avg:40.41ms
step:390/2330 train_time:15763ms step_avg:40.42ms
step:391/2330 train_time:15798ms step_avg:40.40ms
step:392/2330 train_time:15844ms step_avg:40.42ms
step:393/2330 train_time:15880ms step_avg:40.41ms
step:394/2330 train_time:15926ms step_avg:40.42ms
step:395/2330 train_time:15961ms step_avg:40.41ms
step:396/2330 train_time:16007ms step_avg:40.42ms
step:397/2330 train_time:16042ms step_avg:40.41ms
step:398/2330 train_time:16087ms step_avg:40.42ms
step:399/2330 train_time:16123ms step_avg:40.41ms
step:400/2330 train_time:16168ms step_avg:40.42ms
step:401/2330 train_time:16203ms step_avg:40.41ms
step:402/2330 train_time:16249ms step_avg:40.42ms
step:403/2330 train_time:16285ms step_avg:40.41ms
step:404/2330 train_time:16331ms step_avg:40.42ms
step:405/2330 train_time:16367ms step_avg:40.41ms
step:406/2330 train_time:16412ms step_avg:40.42ms
step:407/2330 train_time:16448ms step_avg:40.41ms
step:408/2330 train_time:16494ms step_avg:40.43ms
step:409/2330 train_time:16529ms step_avg:40.41ms
step:410/2330 train_time:16575ms step_avg:40.43ms
step:411/2330 train_time:16611ms step_avg:40.42ms
step:412/2330 train_time:16657ms step_avg:40.43ms
step:413/2330 train_time:16693ms step_avg:40.42ms
step:414/2330 train_time:16738ms step_avg:40.43ms
step:415/2330 train_time:16775ms step_avg:40.42ms
step:416/2330 train_time:16822ms step_avg:40.44ms
step:417/2330 train_time:16858ms step_avg:40.43ms
step:418/2330 train_time:16903ms step_avg:40.44ms
step:419/2330 train_time:16939ms step_avg:40.43ms
step:420/2330 train_time:16983ms step_avg:40.44ms
step:421/2330 train_time:17019ms step_avg:40.42ms
step:422/2330 train_time:17065ms step_avg:40.44ms
step:423/2330 train_time:17101ms step_avg:40.43ms
step:424/2330 train_time:17147ms step_avg:40.44ms
step:425/2330 train_time:17182ms step_avg:40.43ms
step:426/2330 train_time:17227ms step_avg:40.44ms
step:427/2330 train_time:17262ms step_avg:40.43ms
step:428/2330 train_time:17309ms step_avg:40.44ms
step:429/2330 train_time:17345ms step_avg:40.43ms
step:430/2330 train_time:17391ms step_avg:40.44ms
step:431/2330 train_time:17427ms step_avg:40.43ms
step:432/2330 train_time:17472ms step_avg:40.45ms
step:433/2330 train_time:17508ms step_avg:40.43ms
step:434/2330 train_time:17553ms step_avg:40.44ms
step:435/2330 train_time:17588ms step_avg:40.43ms
step:436/2330 train_time:17633ms step_avg:40.44ms
step:437/2330 train_time:17668ms step_avg:40.43ms
step:438/2330 train_time:17713ms step_avg:40.44ms
step:439/2330 train_time:17750ms step_avg:40.43ms
step:440/2330 train_time:17796ms step_avg:40.44ms
step:441/2330 train_time:17831ms step_avg:40.43ms
step:442/2330 train_time:17877ms step_avg:40.45ms
step:443/2330 train_time:17913ms step_avg:40.44ms
step:444/2330 train_time:17959ms step_avg:40.45ms
step:445/2330 train_time:17995ms step_avg:40.44ms
step:446/2330 train_time:18040ms step_avg:40.45ms
step:447/2330 train_time:18076ms step_avg:40.44ms
step:448/2330 train_time:18122ms step_avg:40.45ms
step:449/2330 train_time:18158ms step_avg:40.44ms
step:450/2330 train_time:18203ms step_avg:40.45ms
step:451/2330 train_time:18239ms step_avg:40.44ms
step:452/2330 train_time:18284ms step_avg:40.45ms
step:453/2330 train_time:18320ms step_avg:40.44ms
step:454/2330 train_time:18366ms step_avg:40.45ms
step:455/2330 train_time:18402ms step_avg:40.44ms
step:456/2330 train_time:18446ms step_avg:40.45ms
step:457/2330 train_time:18483ms step_avg:40.44ms
step:458/2330 train_time:18529ms step_avg:40.46ms
step:459/2330 train_time:18565ms step_avg:40.45ms
step:460/2330 train_time:18611ms step_avg:40.46ms
step:461/2330 train_time:18647ms step_avg:40.45ms
step:462/2330 train_time:18693ms step_avg:40.46ms
step:463/2330 train_time:18728ms step_avg:40.45ms
step:464/2330 train_time:18773ms step_avg:40.46ms
step:465/2330 train_time:18808ms step_avg:40.45ms
step:466/2330 train_time:18853ms step_avg:40.46ms
step:467/2330 train_time:18889ms step_avg:40.45ms
step:468/2330 train_time:18934ms step_avg:40.46ms
step:469/2330 train_time:18970ms step_avg:40.45ms
step:470/2330 train_time:19015ms step_avg:40.46ms
step:471/2330 train_time:19051ms step_avg:40.45ms
step:472/2330 train_time:19097ms step_avg:40.46ms
step:473/2330 train_time:19134ms step_avg:40.45ms
step:474/2330 train_time:19179ms step_avg:40.46ms
step:475/2330 train_time:19216ms step_avg:40.45ms
step:476/2330 train_time:19262ms step_avg:40.47ms
step:477/2330 train_time:19297ms step_avg:40.45ms
step:478/2330 train_time:19342ms step_avg:40.47ms
step:479/2330 train_time:19378ms step_avg:40.46ms
step:480/2330 train_time:19425ms step_avg:40.47ms
step:481/2330 train_time:19460ms step_avg:40.46ms
step:482/2330 train_time:19506ms step_avg:40.47ms
step:483/2330 train_time:19542ms step_avg:40.46ms
step:484/2330 train_time:19587ms step_avg:40.47ms
step:485/2330 train_time:19622ms step_avg:40.46ms
step:486/2330 train_time:19667ms step_avg:40.47ms
step:487/2330 train_time:19703ms step_avg:40.46ms
step:488/2330 train_time:19749ms step_avg:40.47ms
step:489/2330 train_time:19786ms step_avg:40.46ms
step:490/2330 train_time:19831ms step_avg:40.47ms
step:491/2330 train_time:19867ms step_avg:40.46ms
step:492/2330 train_time:19912ms step_avg:40.47ms
step:493/2330 train_time:19947ms step_avg:40.46ms
step:494/2330 train_time:19993ms step_avg:40.47ms
step:495/2330 train_time:20028ms step_avg:40.46ms
step:496/2330 train_time:20073ms step_avg:40.47ms
step:497/2330 train_time:20109ms step_avg:40.46ms
step:498/2330 train_time:20155ms step_avg:40.47ms
step:499/2330 train_time:20191ms step_avg:40.46ms
step:500/2330 train_time:20236ms step_avg:40.47ms
step:500/2330 val_loss:5.6919 train_time:20327ms step_avg:40.65ms
step:501/2330 train_time:20341ms step_avg:40.60ms
step:502/2330 train_time:20354ms step_avg:40.55ms
step:503/2330 train_time:20366ms step_avg:40.49ms
step:504/2330 train_time:20401ms step_avg:40.48ms
step:505/2330 train_time:20435ms step_avg:40.47ms
step:506/2330 train_time:20479ms step_avg:40.47ms
step:507/2330 train_time:20514ms step_avg:40.46ms
step:508/2330 train_time:20558ms step_avg:40.47ms
step:509/2330 train_time:20593ms step_avg:40.46ms
step:510/2330 train_time:20639ms step_avg:40.47ms
step:511/2330 train_time:20680ms step_avg:40.47ms
step:512/2330 train_time:20729ms step_avg:40.49ms
step:513/2330 train_time:20767ms step_avg:40.48ms
step:514/2330 train_time:20814ms step_avg:40.49ms
step:515/2330 train_time:20850ms step_avg:40.49ms
step:516/2330 train_time:20895ms step_avg:40.49ms
step:517/2330 train_time:20931ms step_avg:40.49ms
step:518/2330 train_time:20976ms step_avg:40.49ms
step:519/2330 train_time:21011ms step_avg:40.48ms
step:520/2330 train_time:21057ms step_avg:40.49ms
step:521/2330 train_time:21092ms step_avg:40.48ms
step:522/2330 train_time:21136ms step_avg:40.49ms
step:523/2330 train_time:21171ms step_avg:40.48ms
step:524/2330 train_time:21216ms step_avg:40.49ms
step:525/2330 train_time:21251ms step_avg:40.48ms
step:526/2330 train_time:21297ms step_avg:40.49ms
step:527/2330 train_time:21333ms step_avg:40.48ms
step:528/2330 train_time:21378ms step_avg:40.49ms
step:529/2330 train_time:21413ms step_avg:40.48ms
step:530/2330 train_time:21458ms step_avg:40.49ms
step:531/2330 train_time:21493ms step_avg:40.48ms
step:532/2330 train_time:21538ms step_avg:40.48ms
step:533/2330 train_time:21574ms step_avg:40.48ms
step:534/2330 train_time:21621ms step_avg:40.49ms
step:535/2330 train_time:21659ms step_avg:40.48ms
step:536/2330 train_time:21707ms step_avg:40.50ms
step:537/2330 train_time:21744ms step_avg:40.49ms
step:538/2330 train_time:21790ms step_avg:40.50ms
step:539/2330 train_time:21826ms step_avg:40.49ms
step:540/2330 train_time:21872ms step_avg:40.50ms
step:541/2330 train_time:21908ms step_avg:40.50ms
step:542/2330 train_time:21954ms step_avg:40.51ms
step:543/2330 train_time:21990ms step_avg:40.50ms
step:544/2330 train_time:22035ms step_avg:40.50ms
step:545/2330 train_time:22070ms step_avg:40.49ms
step:546/2330 train_time:22114ms step_avg:40.50ms
step:547/2330 train_time:22150ms step_avg:40.49ms
step:548/2330 train_time:22195ms step_avg:40.50ms
step:549/2330 train_time:22232ms step_avg:40.49ms
step:550/2330 train_time:22276ms step_avg:40.50ms
step:551/2330 train_time:22312ms step_avg:40.49ms
step:552/2330 train_time:22357ms step_avg:40.50ms
step:553/2330 train_time:22392ms step_avg:40.49ms
step:554/2330 train_time:22437ms step_avg:40.50ms
step:555/2330 train_time:22472ms step_avg:40.49ms
step:556/2330 train_time:22517ms step_avg:40.50ms
step:557/2330 train_time:22553ms step_avg:40.49ms
step:558/2330 train_time:22599ms step_avg:40.50ms
step:559/2330 train_time:22636ms step_avg:40.49ms
step:560/2330 train_time:22682ms step_avg:40.50ms
step:561/2330 train_time:22720ms step_avg:40.50ms
step:562/2330 train_time:22767ms step_avg:40.51ms
step:563/2330 train_time:22802ms step_avg:40.50ms
step:564/2330 train_time:22848ms step_avg:40.51ms
step:565/2330 train_time:22884ms step_avg:40.50ms
step:566/2330 train_time:22929ms step_avg:40.51ms
step:567/2330 train_time:22964ms step_avg:40.50ms
step:568/2330 train_time:23010ms step_avg:40.51ms
step:569/2330 train_time:23046ms step_avg:40.50ms
step:570/2330 train_time:23091ms step_avg:40.51ms
step:571/2330 train_time:23127ms step_avg:40.50ms
step:572/2330 train_time:23172ms step_avg:40.51ms
step:573/2330 train_time:23207ms step_avg:40.50ms
step:574/2330 train_time:23253ms step_avg:40.51ms
step:575/2330 train_time:23288ms step_avg:40.50ms
step:576/2330 train_time:23333ms step_avg:40.51ms
step:577/2330 train_time:23369ms step_avg:40.50ms
step:578/2330 train_time:23414ms step_avg:40.51ms
step:579/2330 train_time:23449ms step_avg:40.50ms
step:580/2330 train_time:23495ms step_avg:40.51ms
step:581/2330 train_time:23531ms step_avg:40.50ms
step:582/2330 train_time:23576ms step_avg:40.51ms
step:583/2330 train_time:23612ms step_avg:40.50ms
step:584/2330 train_time:23658ms step_avg:40.51ms
step:585/2330 train_time:23694ms step_avg:40.50ms
step:586/2330 train_time:23739ms step_avg:40.51ms
step:587/2330 train_time:23776ms step_avg:40.50ms
step:588/2330 train_time:23823ms step_avg:40.51ms
step:589/2330 train_time:23859ms step_avg:40.51ms
step:590/2330 train_time:23905ms step_avg:40.52ms
step:591/2330 train_time:23941ms step_avg:40.51ms
step:592/2330 train_time:23986ms step_avg:40.52ms
step:593/2330 train_time:24022ms step_avg:40.51ms
step:594/2330 train_time:24068ms step_avg:40.52ms
step:595/2330 train_time:24103ms step_avg:40.51ms
step:596/2330 train_time:24148ms step_avg:40.52ms
step:597/2330 train_time:24184ms step_avg:40.51ms
step:598/2330 train_time:24229ms step_avg:40.52ms
step:599/2330 train_time:24265ms step_avg:40.51ms
step:600/2330 train_time:24310ms step_avg:40.52ms
step:601/2330 train_time:24345ms step_avg:40.51ms
step:602/2330 train_time:24390ms step_avg:40.52ms
step:603/2330 train_time:24426ms step_avg:40.51ms
step:604/2330 train_time:24472ms step_avg:40.52ms
step:605/2330 train_time:24508ms step_avg:40.51ms
step:606/2330 train_time:24554ms step_avg:40.52ms
step:607/2330 train_time:24590ms step_avg:40.51ms
step:608/2330 train_time:24635ms step_avg:40.52ms
step:609/2330 train_time:24670ms step_avg:40.51ms
step:610/2330 train_time:24716ms step_avg:40.52ms
step:611/2330 train_time:24752ms step_avg:40.51ms
step:612/2330 train_time:24797ms step_avg:40.52ms
step:613/2330 train_time:24834ms step_avg:40.51ms
step:614/2330 train_time:24880ms step_avg:40.52ms
step:615/2330 train_time:24917ms step_avg:40.52ms
step:616/2330 train_time:24963ms step_avg:40.52ms
step:617/2330 train_time:24998ms step_avg:40.52ms
step:618/2330 train_time:25042ms step_avg:40.52ms
step:619/2330 train_time:25079ms step_avg:40.52ms
step:620/2330 train_time:25125ms step_avg:40.52ms
step:621/2330 train_time:25160ms step_avg:40.52ms
step:622/2330 train_time:25204ms step_avg:40.52ms
step:623/2330 train_time:25240ms step_avg:40.51ms
step:624/2330 train_time:25287ms step_avg:40.52ms
step:625/2330 train_time:25323ms step_avg:40.52ms
step:626/2330 train_time:25368ms step_avg:40.52ms
step:627/2330 train_time:25404ms step_avg:40.52ms
step:628/2330 train_time:25449ms step_avg:40.52ms
step:629/2330 train_time:25485ms step_avg:40.52ms
step:630/2330 train_time:25531ms step_avg:40.53ms
step:631/2330 train_time:25566ms step_avg:40.52ms
step:632/2330 train_time:25612ms step_avg:40.53ms
step:633/2330 train_time:25647ms step_avg:40.52ms
step:634/2330 train_time:25693ms step_avg:40.52ms
step:635/2330 train_time:25729ms step_avg:40.52ms
step:636/2330 train_time:25775ms step_avg:40.53ms
step:637/2330 train_time:25811ms step_avg:40.52ms
step:638/2330 train_time:25857ms step_avg:40.53ms
step:639/2330 train_time:25893ms step_avg:40.52ms
step:640/2330 train_time:25938ms step_avg:40.53ms
step:641/2330 train_time:25974ms step_avg:40.52ms
step:642/2330 train_time:26019ms step_avg:40.53ms
step:643/2330 train_time:26055ms step_avg:40.52ms
step:644/2330 train_time:26101ms step_avg:40.53ms
step:645/2330 train_time:26137ms step_avg:40.52ms
step:646/2330 train_time:26183ms step_avg:40.53ms
step:647/2330 train_time:26219ms step_avg:40.52ms
step:648/2330 train_time:26264ms step_avg:40.53ms
step:649/2330 train_time:26300ms step_avg:40.52ms
step:650/2330 train_time:26345ms step_avg:40.53ms
step:651/2330 train_time:26381ms step_avg:40.52ms
step:652/2330 train_time:26427ms step_avg:40.53ms
step:653/2330 train_time:26462ms step_avg:40.52ms
step:654/2330 train_time:26507ms step_avg:40.53ms
step:655/2330 train_time:26543ms step_avg:40.52ms
step:656/2330 train_time:26588ms step_avg:40.53ms
step:657/2330 train_time:26624ms step_avg:40.52ms
step:658/2330 train_time:26670ms step_avg:40.53ms
step:659/2330 train_time:26706ms step_avg:40.52ms
step:660/2330 train_time:26752ms step_avg:40.53ms
step:661/2330 train_time:26787ms step_avg:40.53ms
step:662/2330 train_time:26833ms step_avg:40.53ms
step:663/2330 train_time:26869ms step_avg:40.53ms
step:664/2330 train_time:26915ms step_avg:40.53ms
step:665/2330 train_time:26950ms step_avg:40.53ms
step:666/2330 train_time:26996ms step_avg:40.53ms
step:667/2330 train_time:27032ms step_avg:40.53ms
step:668/2330 train_time:27076ms step_avg:40.53ms
step:669/2330 train_time:27112ms step_avg:40.53ms
step:670/2330 train_time:27158ms step_avg:40.53ms
step:671/2330 train_time:27194ms step_avg:40.53ms
step:672/2330 train_time:27240ms step_avg:40.54ms
step:673/2330 train_time:27275ms step_avg:40.53ms
step:674/2330 train_time:27321ms step_avg:40.54ms
step:675/2330 train_time:27359ms step_avg:40.53ms
step:676/2330 train_time:27404ms step_avg:40.54ms
step:677/2330 train_time:27440ms step_avg:40.53ms
step:678/2330 train_time:27486ms step_avg:40.54ms
step:679/2330 train_time:27521ms step_avg:40.53ms
step:680/2330 train_time:27566ms step_avg:40.54ms
step:681/2330 train_time:27602ms step_avg:40.53ms
step:682/2330 train_time:27647ms step_avg:40.54ms
step:683/2330 train_time:27683ms step_avg:40.53ms
step:684/2330 train_time:27728ms step_avg:40.54ms
step:685/2330 train_time:27764ms step_avg:40.53ms
step:686/2330 train_time:27810ms step_avg:40.54ms
step:687/2330 train_time:27845ms step_avg:40.53ms
step:688/2330 train_time:27890ms step_avg:40.54ms
step:689/2330 train_time:27927ms step_avg:40.53ms
step:690/2330 train_time:27972ms step_avg:40.54ms
step:691/2330 train_time:28008ms step_avg:40.53ms
step:692/2330 train_time:28054ms step_avg:40.54ms
step:693/2330 train_time:28090ms step_avg:40.53ms
step:694/2330 train_time:28136ms step_avg:40.54ms
step:695/2330 train_time:28172ms step_avg:40.54ms
step:696/2330 train_time:28217ms step_avg:40.54ms
step:697/2330 train_time:28252ms step_avg:40.53ms
step:698/2330 train_time:28297ms step_avg:40.54ms
step:699/2330 train_time:28333ms step_avg:40.53ms
step:700/2330 train_time:28378ms step_avg:40.54ms
step:701/2330 train_time:28415ms step_avg:40.53ms
step:702/2330 train_time:28460ms step_avg:40.54ms
step:703/2330 train_time:28497ms step_avg:40.54ms
step:704/2330 train_time:28542ms step_avg:40.54ms
step:705/2330 train_time:28578ms step_avg:40.54ms
step:706/2330 train_time:28623ms step_avg:40.54ms
step:707/2330 train_time:28659ms step_avg:40.54ms
step:708/2330 train_time:28704ms step_avg:40.54ms
step:709/2330 train_time:28741ms step_avg:40.54ms
step:710/2330 train_time:28787ms step_avg:40.54ms
step:711/2330 train_time:28823ms step_avg:40.54ms
step:712/2330 train_time:28869ms step_avg:40.55ms
step:713/2330 train_time:28905ms step_avg:40.54ms
step:714/2330 train_time:28951ms step_avg:40.55ms
step:715/2330 train_time:28986ms step_avg:40.54ms
step:716/2330 train_time:29032ms step_avg:40.55ms
step:717/2330 train_time:29068ms step_avg:40.54ms
step:718/2330 train_time:29114ms step_avg:40.55ms
step:719/2330 train_time:29150ms step_avg:40.54ms
step:720/2330 train_time:29195ms step_avg:40.55ms
step:721/2330 train_time:29231ms step_avg:40.54ms
step:722/2330 train_time:29277ms step_avg:40.55ms
step:723/2330 train_time:29312ms step_avg:40.54ms
step:724/2330 train_time:29357ms step_avg:40.55ms
step:725/2330 train_time:29393ms step_avg:40.54ms
step:726/2330 train_time:29438ms step_avg:40.55ms
step:727/2330 train_time:29474ms step_avg:40.54ms
step:728/2330 train_time:29519ms step_avg:40.55ms
step:729/2330 train_time:29555ms step_avg:40.54ms
step:730/2330 train_time:29600ms step_avg:40.55ms
step:731/2330 train_time:29636ms step_avg:40.54ms
step:732/2330 train_time:29683ms step_avg:40.55ms
step:733/2330 train_time:29720ms step_avg:40.55ms
step:734/2330 train_time:29766ms step_avg:40.55ms
step:735/2330 train_time:29802ms step_avg:40.55ms
step:736/2330 train_time:29846ms step_avg:40.55ms
step:737/2330 train_time:29882ms step_avg:40.55ms
step:738/2330 train_time:29927ms step_avg:40.55ms
step:739/2330 train_time:29964ms step_avg:40.55ms
step:740/2330 train_time:30009ms step_avg:40.55ms
step:741/2330 train_time:30045ms step_avg:40.55ms
step:742/2330 train_time:30091ms step_avg:40.55ms
step:743/2330 train_time:30126ms step_avg:40.55ms
step:744/2330 train_time:30172ms step_avg:40.55ms
step:745/2330 train_time:30207ms step_avg:40.55ms
step:746/2330 train_time:30253ms step_avg:40.55ms
step:747/2330 train_time:30289ms step_avg:40.55ms
step:748/2330 train_time:30335ms step_avg:40.55ms
step:749/2330 train_time:30370ms step_avg:40.55ms
step:750/2330 train_time:30415ms step_avg:40.55ms
step:750/2330 val_loss:5.6663 train_time:30505ms step_avg:40.67ms
step:751/2330 train_time:30520ms step_avg:40.64ms
step:752/2330 train_time:30533ms step_avg:40.60ms
step:753/2330 train_time:30544ms step_avg:40.56ms
step:754/2330 train_time:30580ms step_avg:40.56ms
step:755/2330 train_time:30614ms step_avg:40.55ms
step:756/2330 train_time:30659ms step_avg:40.55ms
step:757/2330 train_time:30693ms step_avg:40.55ms
step:758/2330 train_time:30738ms step_avg:40.55ms
step:759/2330 train_time:30775ms step_avg:40.55ms
step:760/2330 train_time:30824ms step_avg:40.56ms
step:761/2330 train_time:30863ms step_avg:40.56ms
step:762/2330 train_time:30910ms step_avg:40.56ms
step:763/2330 train_time:30948ms step_avg:40.56ms
step:764/2330 train_time:30993ms step_avg:40.57ms
step:765/2330 train_time:31029ms step_avg:40.56ms
step:766/2330 train_time:31073ms step_avg:40.57ms
step:767/2330 train_time:31108ms step_avg:40.56ms
step:768/2330 train_time:31152ms step_avg:40.56ms
step:769/2330 train_time:31188ms step_avg:40.56ms
step:770/2330 train_time:31233ms step_avg:40.56ms
step:771/2330 train_time:31267ms step_avg:40.55ms
step:772/2330 train_time:31312ms step_avg:40.56ms
step:773/2330 train_time:31346ms step_avg:40.55ms
step:774/2330 train_time:31392ms step_avg:40.56ms
step:775/2330 train_time:31429ms step_avg:40.55ms
step:776/2330 train_time:31475ms step_avg:40.56ms
step:777/2330 train_time:31510ms step_avg:40.55ms
step:778/2330 train_time:31556ms step_avg:40.56ms
step:779/2330 train_time:31592ms step_avg:40.55ms
step:780/2330 train_time:31636ms step_avg:40.56ms
step:781/2330 train_time:31672ms step_avg:40.55ms
step:782/2330 train_time:31717ms step_avg:40.56ms
step:783/2330 train_time:31753ms step_avg:40.55ms
step:784/2330 train_time:31800ms step_avg:40.56ms
step:785/2330 train_time:31836ms step_avg:40.56ms
step:786/2330 train_time:31884ms step_avg:40.57ms
step:787/2330 train_time:31922ms step_avg:40.56ms
step:788/2330 train_time:31967ms step_avg:40.57ms
step:789/2330 train_time:32003ms step_avg:40.56ms
step:790/2330 train_time:32049ms step_avg:40.57ms
step:791/2330 train_time:32084ms step_avg:40.56ms
step:792/2330 train_time:32130ms step_avg:40.57ms
step:793/2330 train_time:32166ms step_avg:40.56ms
step:794/2330 train_time:32210ms step_avg:40.57ms
step:795/2330 train_time:32246ms step_avg:40.56ms
step:796/2330 train_time:32290ms step_avg:40.57ms
step:797/2330 train_time:32326ms step_avg:40.56ms
step:798/2330 train_time:32371ms step_avg:40.57ms
step:799/2330 train_time:32407ms step_avg:40.56ms
step:800/2330 train_time:32453ms step_avg:40.57ms
step:801/2330 train_time:32488ms step_avg:40.56ms
step:802/2330 train_time:32535ms step_avg:40.57ms
step:803/2330 train_time:32570ms step_avg:40.56ms
step:804/2330 train_time:32616ms step_avg:40.57ms
step:805/2330 train_time:32652ms step_avg:40.56ms
step:806/2330 train_time:32697ms step_avg:40.57ms
step:807/2330 train_time:32733ms step_avg:40.56ms
step:808/2330 train_time:32780ms step_avg:40.57ms
step:809/2330 train_time:32816ms step_avg:40.56ms
step:810/2330 train_time:32863ms step_avg:40.57ms
step:811/2330 train_time:32898ms step_avg:40.57ms
step:812/2330 train_time:32943ms step_avg:40.57ms
step:813/2330 train_time:32978ms step_avg:40.56ms
step:814/2330 train_time:33024ms step_avg:40.57ms
step:815/2330 train_time:33060ms step_avg:40.56ms
step:816/2330 train_time:33105ms step_avg:40.57ms
step:817/2330 train_time:33141ms step_avg:40.56ms
step:818/2330 train_time:33187ms step_avg:40.57ms
step:819/2330 train_time:33222ms step_avg:40.56ms
step:820/2330 train_time:33267ms step_avg:40.57ms
step:821/2330 train_time:33302ms step_avg:40.56ms
step:822/2330 train_time:33347ms step_avg:40.57ms
step:823/2330 train_time:33383ms step_avg:40.56ms
step:824/2330 train_time:33430ms step_avg:40.57ms
step:825/2330 train_time:33466ms step_avg:40.56ms
step:826/2330 train_time:33511ms step_avg:40.57ms
step:827/2330 train_time:33547ms step_avg:40.56ms
step:828/2330 train_time:33593ms step_avg:40.57ms
step:829/2330 train_time:33629ms step_avg:40.57ms
step:830/2330 train_time:33674ms step_avg:40.57ms
step:831/2330 train_time:33710ms step_avg:40.57ms
step:832/2330 train_time:33757ms step_avg:40.57ms
step:833/2330 train_time:33792ms step_avg:40.57ms
step:834/2330 train_time:33839ms step_avg:40.57ms
step:835/2330 train_time:33875ms step_avg:40.57ms
step:836/2330 train_time:33921ms step_avg:40.58ms
step:837/2330 train_time:33957ms step_avg:40.57ms
step:838/2330 train_time:34002ms step_avg:40.58ms
step:839/2330 train_time:34038ms step_avg:40.57ms
step:840/2330 train_time:34083ms step_avg:40.57ms
step:841/2330 train_time:34118ms step_avg:40.57ms
step:842/2330 train_time:34164ms step_avg:40.58ms
step:843/2330 train_time:34200ms step_avg:40.57ms
step:844/2330 train_time:34245ms step_avg:40.57ms
step:845/2330 train_time:34280ms step_avg:40.57ms
step:846/2330 train_time:34325ms step_avg:40.57ms
step:847/2330 train_time:34361ms step_avg:40.57ms
step:848/2330 train_time:34406ms step_avg:40.57ms
step:849/2330 train_time:34442ms step_avg:40.57ms
step:850/2330 train_time:34487ms step_avg:40.57ms
step:851/2330 train_time:34524ms step_avg:40.57ms
step:852/2330 train_time:34570ms step_avg:40.58ms
step:853/2330 train_time:34607ms step_avg:40.57ms
step:854/2330 train_time:34653ms step_avg:40.58ms
step:855/2330 train_time:34690ms step_avg:40.57ms
step:856/2330 train_time:34736ms step_avg:40.58ms
step:857/2330 train_time:34771ms step_avg:40.57ms
step:858/2330 train_time:34817ms step_avg:40.58ms
step:859/2330 train_time:34853ms step_avg:40.57ms
step:860/2330 train_time:34898ms step_avg:40.58ms
step:861/2330 train_time:34934ms step_avg:40.57ms
step:862/2330 train_time:34979ms step_avg:40.58ms
step:863/2330 train_time:35014ms step_avg:40.57ms
step:864/2330 train_time:35059ms step_avg:40.58ms
step:865/2330 train_time:35096ms step_avg:40.57ms
step:866/2330 train_time:35141ms step_avg:40.58ms
step:867/2330 train_time:35177ms step_avg:40.57ms
step:868/2330 train_time:35222ms step_avg:40.58ms
step:869/2330 train_time:35258ms step_avg:40.57ms
step:870/2330 train_time:35303ms step_avg:40.58ms
step:871/2330 train_time:35339ms step_avg:40.57ms
step:872/2330 train_time:35383ms step_avg:40.58ms
step:873/2330 train_time:35419ms step_avg:40.57ms
step:874/2330 train_time:35464ms step_avg:40.58ms
step:875/2330 train_time:35500ms step_avg:40.57ms
step:876/2330 train_time:35545ms step_avg:40.58ms
step:877/2330 train_time:35581ms step_avg:40.57ms
step:878/2330 train_time:35627ms step_avg:40.58ms
step:879/2330 train_time:35664ms step_avg:40.57ms
step:880/2330 train_time:35710ms step_avg:40.58ms
step:881/2330 train_time:35747ms step_avg:40.58ms
step:882/2330 train_time:35793ms step_avg:40.58ms
step:883/2330 train_time:35830ms step_avg:40.58ms
step:884/2330 train_time:35876ms step_avg:40.58ms
step:885/2330 train_time:35911ms step_avg:40.58ms
step:886/2330 train_time:35956ms step_avg:40.58ms
step:887/2330 train_time:35992ms step_avg:40.58ms
step:888/2330 train_time:36037ms step_avg:40.58ms
step:889/2330 train_time:36073ms step_avg:40.58ms
step:890/2330 train_time:36118ms step_avg:40.58ms
step:891/2330 train_time:36154ms step_avg:40.58ms
step:892/2330 train_time:36199ms step_avg:40.58ms
step:893/2330 train_time:36236ms step_avg:40.58ms
step:894/2330 train_time:36282ms step_avg:40.58ms
step:895/2330 train_time:36318ms step_avg:40.58ms
step:896/2330 train_time:36363ms step_avg:40.58ms
step:897/2330 train_time:36399ms step_avg:40.58ms
step:898/2330 train_time:36444ms step_avg:40.58ms
step:899/2330 train_time:36479ms step_avg:40.58ms
step:900/2330 train_time:36525ms step_avg:40.58ms
step:901/2330 train_time:36561ms step_avg:40.58ms
step:902/2330 train_time:36607ms step_avg:40.58ms
step:903/2330 train_time:36643ms step_avg:40.58ms
step:904/2330 train_time:36689ms step_avg:40.58ms
step:905/2330 train_time:36725ms step_avg:40.58ms
step:906/2330 train_time:36772ms step_avg:40.59ms
step:907/2330 train_time:36807ms step_avg:40.58ms
step:908/2330 train_time:36853ms step_avg:40.59ms
step:909/2330 train_time:36888ms step_avg:40.58ms
step:910/2330 train_time:36933ms step_avg:40.59ms
step:911/2330 train_time:36969ms step_avg:40.58ms
step:912/2330 train_time:37015ms step_avg:40.59ms
step:913/2330 train_time:37051ms step_avg:40.58ms
step:914/2330 train_time:37095ms step_avg:40.59ms
step:915/2330 train_time:37131ms step_avg:40.58ms
step:916/2330 train_time:37177ms step_avg:40.59ms
step:917/2330 train_time:37213ms step_avg:40.58ms
step:918/2330 train_time:37258ms step_avg:40.59ms
step:919/2330 train_time:37294ms step_avg:40.58ms
step:920/2330 train_time:37339ms step_avg:40.59ms
step:921/2330 train_time:37375ms step_avg:40.58ms
step:922/2330 train_time:37422ms step_avg:40.59ms
step:923/2330 train_time:37457ms step_avg:40.58ms
step:924/2330 train_time:37503ms step_avg:40.59ms
step:925/2330 train_time:37539ms step_avg:40.58ms
step:926/2330 train_time:37585ms step_avg:40.59ms
step:927/2330 train_time:37621ms step_avg:40.58ms
step:928/2330 train_time:37667ms step_avg:40.59ms
step:929/2330 train_time:37703ms step_avg:40.58ms
step:930/2330 train_time:37749ms step_avg:40.59ms
step:931/2330 train_time:37784ms step_avg:40.58ms
step:932/2330 train_time:37830ms step_avg:40.59ms
step:933/2330 train_time:37866ms step_avg:40.58ms
step:934/2330 train_time:37911ms step_avg:40.59ms
step:935/2330 train_time:37948ms step_avg:40.59ms
step:936/2330 train_time:37993ms step_avg:40.59ms
step:937/2330 train_time:38028ms step_avg:40.59ms
step:938/2330 train_time:38073ms step_avg:40.59ms
step:939/2330 train_time:38109ms step_avg:40.58ms
step:940/2330 train_time:38154ms step_avg:40.59ms
step:941/2330 train_time:38192ms step_avg:40.59ms
step:942/2330 train_time:38237ms step_avg:40.59ms
step:943/2330 train_time:38273ms step_avg:40.59ms
step:944/2330 train_time:38319ms step_avg:40.59ms
step:945/2330 train_time:38354ms step_avg:40.59ms
step:946/2330 train_time:38400ms step_avg:40.59ms
step:947/2330 train_time:38436ms step_avg:40.59ms
step:948/2330 train_time:38482ms step_avg:40.59ms
step:949/2330 train_time:38517ms step_avg:40.59ms
step:950/2330 train_time:38564ms step_avg:40.59ms
step:951/2330 train_time:38600ms step_avg:40.59ms
step:952/2330 train_time:38645ms step_avg:40.59ms
step:953/2330 train_time:38680ms step_avg:40.59ms
step:954/2330 train_time:38726ms step_avg:40.59ms
step:955/2330 train_time:38761ms step_avg:40.59ms
step:956/2330 train_time:38807ms step_avg:40.59ms
step:957/2330 train_time:38843ms step_avg:40.59ms
step:958/2330 train_time:38890ms step_avg:40.59ms
step:959/2330 train_time:38927ms step_avg:40.59ms
step:960/2330 train_time:38972ms step_avg:40.60ms
step:961/2330 train_time:39008ms step_avg:40.59ms
step:962/2330 train_time:39053ms step_avg:40.60ms
step:963/2330 train_time:39089ms step_avg:40.59ms
step:964/2330 train_time:39134ms step_avg:40.60ms
step:965/2330 train_time:39170ms step_avg:40.59ms
step:966/2330 train_time:39215ms step_avg:40.60ms
step:967/2330 train_time:39252ms step_avg:40.59ms
step:968/2330 train_time:39298ms step_avg:40.60ms
step:969/2330 train_time:39333ms step_avg:40.59ms
step:970/2330 train_time:39380ms step_avg:40.60ms
step:971/2330 train_time:39415ms step_avg:40.59ms
step:972/2330 train_time:39462ms step_avg:40.60ms
step:973/2330 train_time:39497ms step_avg:40.59ms
step:974/2330 train_time:39543ms step_avg:40.60ms
step:975/2330 train_time:39579ms step_avg:40.59ms
step:976/2330 train_time:39624ms step_avg:40.60ms
step:977/2330 train_time:39659ms step_avg:40.59ms
step:978/2330 train_time:39704ms step_avg:40.60ms
step:979/2330 train_time:39740ms step_avg:40.59ms
step:980/2330 train_time:39785ms step_avg:40.60ms
step:981/2330 train_time:39821ms step_avg:40.59ms
step:982/2330 train_time:39866ms step_avg:40.60ms
step:983/2330 train_time:39902ms step_avg:40.59ms
step:984/2330 train_time:39947ms step_avg:40.60ms
step:985/2330 train_time:39983ms step_avg:40.59ms
step:986/2330 train_time:40029ms step_avg:40.60ms
step:987/2330 train_time:40066ms step_avg:40.59ms
step:988/2330 train_time:40111ms step_avg:40.60ms
step:989/2330 train_time:40147ms step_avg:40.59ms
step:990/2330 train_time:40193ms step_avg:40.60ms
step:991/2330 train_time:40229ms step_avg:40.59ms
step:992/2330 train_time:40274ms step_avg:40.60ms
step:993/2330 train_time:40311ms step_avg:40.60ms
step:994/2330 train_time:40356ms step_avg:40.60ms
step:995/2330 train_time:40393ms step_avg:40.60ms
step:996/2330 train_time:40438ms step_avg:40.60ms
step:997/2330 train_time:40474ms step_avg:40.60ms
step:998/2330 train_time:40520ms step_avg:40.60ms
step:999/2330 train_time:40556ms step_avg:40.60ms
step:1000/2330 train_time:40601ms step_avg:40.60ms
step:1000/2330 val_loss:5.5267 train_time:40691ms step_avg:40.69ms
step:1001/2330 train_time:40705ms step_avg:40.66ms
step:1002/2330 train_time:40717ms step_avg:40.64ms
step:1003/2330 train_time:40728ms step_avg:40.61ms
step:1004/2330 train_time:40764ms step_avg:40.60ms
step:1005/2330 train_time:40799ms step_avg:40.60ms
step:1006/2330 train_time:40843ms step_avg:40.60ms
step:1007/2330 train_time:40878ms step_avg:40.59ms
step:1008/2330 train_time:40923ms step_avg:40.60ms
step:1009/2330 train_time:40957ms step_avg:40.59ms
step:1010/2330 train_time:41002ms step_avg:40.60ms
step:1011/2330 train_time:41042ms step_avg:40.60ms
step:1012/2330 train_time:41089ms step_avg:40.60ms
step:1013/2330 train_time:41125ms step_avg:40.60ms
step:1014/2330 train_time:41171ms step_avg:40.60ms
step:1015/2330 train_time:41206ms step_avg:40.60ms
step:1016/2330 train_time:41251ms step_avg:40.60ms
step:1017/2330 train_time:41287ms step_avg:40.60ms
step:1018/2330 train_time:41332ms step_avg:40.60ms
step:1019/2330 train_time:41367ms step_avg:40.60ms
step:1020/2330 train_time:41413ms step_avg:40.60ms
step:1021/2330 train_time:41448ms step_avg:40.60ms
step:1022/2330 train_time:41493ms step_avg:40.60ms
step:1023/2330 train_time:41528ms step_avg:40.59ms
step:1024/2330 train_time:41573ms step_avg:40.60ms
step:1025/2330 train_time:41611ms step_avg:40.60ms
step:1026/2330 train_time:41660ms step_avg:40.60ms
step:1027/2330 train_time:41698ms step_avg:40.60ms
step:1028/2330 train_time:41743ms step_avg:40.61ms
step:1029/2330 train_time:41778ms step_avg:40.60ms
step:1030/2330 train_time:41824ms step_avg:40.61ms
step:1031/2330 train_time:41859ms step_avg:40.60ms
step:1032/2330 train_time:41905ms step_avg:40.61ms
step:1033/2330 train_time:41942ms step_avg:40.60ms
step:1034/2330 train_time:41988ms step_avg:40.61ms
step:1035/2330 train_time:42024ms step_avg:40.60ms
step:1036/2330 train_time:42070ms step_avg:40.61ms
step:1037/2330 train_time:42107ms step_avg:40.60ms
step:1038/2330 train_time:42151ms step_avg:40.61ms
step:1039/2330 train_time:42187ms step_avg:40.60ms
step:1040/2330 train_time:42232ms step_avg:40.61ms
step:1041/2330 train_time:42267ms step_avg:40.60ms
step:1042/2330 train_time:42312ms step_avg:40.61ms
step:1043/2330 train_time:42348ms step_avg:40.60ms
step:1044/2330 train_time:42393ms step_avg:40.61ms
step:1045/2330 train_time:42430ms step_avg:40.60ms
step:1046/2330 train_time:42474ms step_avg:40.61ms
step:1047/2330 train_time:42509ms step_avg:40.60ms
step:1048/2330 train_time:42555ms step_avg:40.61ms
step:1049/2330 train_time:42591ms step_avg:40.60ms
step:1050/2330 train_time:42638ms step_avg:40.61ms
step:1051/2330 train_time:42675ms step_avg:40.60ms
step:1052/2330 train_time:42721ms step_avg:40.61ms
step:1053/2330 train_time:42756ms step_avg:40.60ms
step:1054/2330 train_time:42800ms step_avg:40.61ms
step:1055/2330 train_time:42835ms step_avg:40.60ms
step:1056/2330 train_time:42881ms step_avg:40.61ms
step:1057/2330 train_time:42918ms step_avg:40.60ms
step:1058/2330 train_time:42964ms step_avg:40.61ms
step:1059/2330 train_time:43000ms step_avg:40.60ms
step:1060/2330 train_time:43046ms step_avg:40.61ms
step:1061/2330 train_time:43082ms step_avg:40.60ms
step:1062/2330 train_time:43128ms step_avg:40.61ms
step:1063/2330 train_time:43164ms step_avg:40.61ms
step:1064/2330 train_time:43210ms step_avg:40.61ms
step:1065/2330 train_time:43245ms step_avg:40.61ms
step:1066/2330 train_time:43291ms step_avg:40.61ms
step:1067/2330 train_time:43326ms step_avg:40.61ms
step:1068/2330 train_time:43371ms step_avg:40.61ms
step:1069/2330 train_time:43406ms step_avg:40.60ms
step:1070/2330 train_time:43451ms step_avg:40.61ms
step:1071/2330 train_time:43487ms step_avg:40.60ms
step:1072/2330 train_time:43532ms step_avg:40.61ms
step:1073/2330 train_time:43568ms step_avg:40.60ms
step:1074/2330 train_time:43613ms step_avg:40.61ms
step:1075/2330 train_time:43649ms step_avg:40.60ms
step:1076/2330 train_time:43695ms step_avg:40.61ms
step:1077/2330 train_time:43731ms step_avg:40.60ms
step:1078/2330 train_time:43778ms step_avg:40.61ms
step:1079/2330 train_time:43814ms step_avg:40.61ms
step:1080/2330 train_time:43859ms step_avg:40.61ms
step:1081/2330 train_time:43894ms step_avg:40.61ms
step:1082/2330 train_time:43939ms step_avg:40.61ms
step:1083/2330 train_time:43977ms step_avg:40.61ms
step:1084/2330 train_time:44022ms step_avg:40.61ms
step:1085/2330 train_time:44058ms step_avg:40.61ms
step:1086/2330 train_time:44104ms step_avg:40.61ms
step:1087/2330 train_time:44140ms step_avg:40.61ms
step:1088/2330 train_time:44186ms step_avg:40.61ms
step:1089/2330 train_time:44222ms step_avg:40.61ms
step:1090/2330 train_time:44267ms step_avg:40.61ms
step:1091/2330 train_time:44302ms step_avg:40.61ms
step:1092/2330 train_time:44348ms step_avg:40.61ms
step:1093/2330 train_time:44383ms step_avg:40.61ms
step:1094/2330 train_time:44428ms step_avg:40.61ms
step:1095/2330 train_time:44464ms step_avg:40.61ms
step:1096/2330 train_time:44510ms step_avg:40.61ms
step:1097/2330 train_time:44546ms step_avg:40.61ms
step:1098/2330 train_time:44592ms step_avg:40.61ms
step:1099/2330 train_time:44627ms step_avg:40.61ms
step:1100/2330 train_time:44672ms step_avg:40.61ms
step:1101/2330 train_time:44708ms step_avg:40.61ms
step:1102/2330 train_time:44753ms step_avg:40.61ms
step:1103/2330 train_time:44790ms step_avg:40.61ms
step:1104/2330 train_time:44835ms step_avg:40.61ms
step:1105/2330 train_time:44871ms step_avg:40.61ms
step:1106/2330 train_time:44917ms step_avg:40.61ms
step:1107/2330 train_time:44954ms step_avg:40.61ms
step:1108/2330 train_time:45000ms step_avg:40.61ms
step:1109/2330 train_time:45035ms step_avg:40.61ms
step:1110/2330 train_time:45081ms step_avg:40.61ms
step:1111/2330 train_time:45118ms step_avg:40.61ms
step:1112/2330 train_time:45164ms step_avg:40.62ms
step:1113/2330 train_time:45200ms step_avg:40.61ms
step:1114/2330 train_time:45245ms step_avg:40.61ms
step:1115/2330 train_time:45281ms step_avg:40.61ms
step:1116/2330 train_time:45326ms step_avg:40.61ms
step:1117/2330 train_time:45361ms step_avg:40.61ms
step:1118/2330 train_time:45407ms step_avg:40.61ms
step:1119/2330 train_time:45444ms step_avg:40.61ms
step:1120/2330 train_time:45490ms step_avg:40.62ms
step:1121/2330 train_time:45525ms step_avg:40.61ms
step:1122/2330 train_time:45571ms step_avg:40.62ms
step:1123/2330 train_time:45606ms step_avg:40.61ms
step:1124/2330 train_time:45652ms step_avg:40.62ms
step:1125/2330 train_time:45688ms step_avg:40.61ms
step:1126/2330 train_time:45733ms step_avg:40.62ms
step:1127/2330 train_time:45769ms step_avg:40.61ms
step:1128/2330 train_time:45815ms step_avg:40.62ms
step:1129/2330 train_time:45851ms step_avg:40.61ms
step:1130/2330 train_time:45897ms step_avg:40.62ms
step:1131/2330 train_time:45932ms step_avg:40.61ms
step:1132/2330 train_time:45979ms step_avg:40.62ms
step:1133/2330 train_time:46015ms step_avg:40.61ms
step:1134/2330 train_time:46061ms step_avg:40.62ms
step:1135/2330 train_time:46097ms step_avg:40.61ms
step:1136/2330 train_time:46142ms step_avg:40.62ms
step:1137/2330 train_time:46178ms step_avg:40.61ms
step:1138/2330 train_time:46223ms step_avg:40.62ms
step:1139/2330 train_time:46259ms step_avg:40.61ms
step:1140/2330 train_time:46305ms step_avg:40.62ms
step:1141/2330 train_time:46341ms step_avg:40.61ms
step:1142/2330 train_time:46387ms step_avg:40.62ms
step:1143/2330 train_time:46422ms step_avg:40.61ms
step:1144/2330 train_time:46468ms step_avg:40.62ms
step:1145/2330 train_time:46504ms step_avg:40.62ms
step:1146/2330 train_time:46550ms step_avg:40.62ms
step:1147/2330 train_time:46585ms step_avg:40.61ms
step:1148/2330 train_time:46632ms step_avg:40.62ms
step:1149/2330 train_time:46667ms step_avg:40.62ms
step:1150/2330 train_time:46712ms step_avg:40.62ms
step:1151/2330 train_time:46748ms step_avg:40.62ms
step:1152/2330 train_time:46793ms step_avg:40.62ms
step:1153/2330 train_time:46829ms step_avg:40.62ms
step:1154/2330 train_time:46875ms step_avg:40.62ms
step:1155/2330 train_time:46911ms step_avg:40.62ms
step:1156/2330 train_time:46956ms step_avg:40.62ms
step:1157/2330 train_time:46993ms step_avg:40.62ms
step:1158/2330 train_time:47039ms step_avg:40.62ms
step:1159/2330 train_time:47075ms step_avg:40.62ms
step:1160/2330 train_time:47121ms step_avg:40.62ms
step:1161/2330 train_time:47157ms step_avg:40.62ms
step:1162/2330 train_time:47201ms step_avg:40.62ms
step:1163/2330 train_time:47237ms step_avg:40.62ms
step:1164/2330 train_time:47284ms step_avg:40.62ms
step:1165/2330 train_time:47321ms step_avg:40.62ms
step:1166/2330 train_time:47366ms step_avg:40.62ms
step:1167/2330 train_time:47401ms step_avg:40.62ms
step:1168/2330 train_time:47448ms step_avg:40.62ms
step:1169/2330 train_time:47482ms step_avg:40.62ms
step:1170/2330 train_time:47528ms step_avg:40.62ms
step:1171/2330 train_time:47564ms step_avg:40.62ms
step:1172/2330 train_time:47610ms step_avg:40.62ms
step:1173/2330 train_time:47646ms step_avg:40.62ms
step:1174/2330 train_time:47692ms step_avg:40.62ms
step:1175/2330 train_time:47728ms step_avg:40.62ms
step:1176/2330 train_time:47773ms step_avg:40.62ms
step:1177/2330 train_time:47808ms step_avg:40.62ms
step:1178/2330 train_time:47853ms step_avg:40.62ms
step:1179/2330 train_time:47889ms step_avg:40.62ms
step:1180/2330 train_time:47935ms step_avg:40.62ms
step:1181/2330 train_time:47970ms step_avg:40.62ms
step:1182/2330 train_time:48016ms step_avg:40.62ms
step:1183/2330 train_time:48052ms step_avg:40.62ms
step:1184/2330 train_time:48098ms step_avg:40.62ms
step:1185/2330 train_time:48134ms step_avg:40.62ms
step:1186/2330 train_time:48179ms step_avg:40.62ms
step:1187/2330 train_time:48215ms step_avg:40.62ms
step:1188/2330 train_time:48261ms step_avg:40.62ms
step:1189/2330 train_time:48298ms step_avg:40.62ms
step:1190/2330 train_time:48344ms step_avg:40.63ms
step:1191/2330 train_time:48380ms step_avg:40.62ms
step:1192/2330 train_time:48425ms step_avg:40.63ms
step:1193/2330 train_time:48461ms step_avg:40.62ms
step:1194/2330 train_time:48506ms step_avg:40.62ms
step:1195/2330 train_time:48542ms step_avg:40.62ms
step:1196/2330 train_time:48588ms step_avg:40.63ms
step:1197/2330 train_time:48624ms step_avg:40.62ms
step:1198/2330 train_time:48670ms step_avg:40.63ms
step:1199/2330 train_time:48705ms step_avg:40.62ms
step:1200/2330 train_time:48751ms step_avg:40.63ms
step:1201/2330 train_time:48786ms step_avg:40.62ms
step:1202/2330 train_time:48832ms step_avg:40.63ms
step:1203/2330 train_time:48867ms step_avg:40.62ms
step:1204/2330 train_time:48912ms step_avg:40.62ms
step:1205/2330 train_time:48948ms step_avg:40.62ms
step:1206/2330 train_time:48994ms step_avg:40.62ms
step:1207/2330 train_time:49030ms step_avg:40.62ms
step:1208/2330 train_time:49075ms step_avg:40.62ms
step:1209/2330 train_time:49111ms step_avg:40.62ms
step:1210/2330 train_time:49156ms step_avg:40.62ms
step:1211/2330 train_time:49193ms step_avg:40.62ms
step:1212/2330 train_time:49239ms step_avg:40.63ms
step:1213/2330 train_time:49276ms step_avg:40.62ms
step:1214/2330 train_time:49322ms step_avg:40.63ms
step:1215/2330 train_time:49358ms step_avg:40.62ms
step:1216/2330 train_time:49403ms step_avg:40.63ms
step:1217/2330 train_time:49440ms step_avg:40.62ms
step:1218/2330 train_time:49486ms step_avg:40.63ms
step:1219/2330 train_time:49520ms step_avg:40.62ms
step:1220/2330 train_time:49566ms step_avg:40.63ms
step:1221/2330 train_time:49602ms step_avg:40.62ms
step:1222/2330 train_time:49648ms step_avg:40.63ms
step:1223/2330 train_time:49684ms step_avg:40.62ms
step:1224/2330 train_time:49730ms step_avg:40.63ms
step:1225/2330 train_time:49765ms step_avg:40.62ms
step:1226/2330 train_time:49811ms step_avg:40.63ms
step:1227/2330 train_time:49847ms step_avg:40.63ms
step:1228/2330 train_time:49893ms step_avg:40.63ms
step:1229/2330 train_time:49928ms step_avg:40.62ms
step:1230/2330 train_time:49973ms step_avg:40.63ms
step:1231/2330 train_time:50008ms step_avg:40.62ms
step:1232/2330 train_time:50053ms step_avg:40.63ms
step:1233/2330 train_time:50089ms step_avg:40.62ms
step:1234/2330 train_time:50135ms step_avg:40.63ms
step:1235/2330 train_time:50171ms step_avg:40.62ms
step:1236/2330 train_time:50217ms step_avg:40.63ms
step:1237/2330 train_time:50253ms step_avg:40.63ms
step:1238/2330 train_time:50300ms step_avg:40.63ms
step:1239/2330 train_time:50335ms step_avg:40.63ms
step:1240/2330 train_time:50381ms step_avg:40.63ms
step:1241/2330 train_time:50419ms step_avg:40.63ms
step:1242/2330 train_time:50465ms step_avg:40.63ms
step:1243/2330 train_time:50500ms step_avg:40.63ms
step:1244/2330 train_time:50545ms step_avg:40.63ms
step:1245/2330 train_time:50581ms step_avg:40.63ms
step:1246/2330 train_time:50626ms step_avg:40.63ms
step:1247/2330 train_time:50662ms step_avg:40.63ms
step:1248/2330 train_time:50708ms step_avg:40.63ms
step:1249/2330 train_time:50744ms step_avg:40.63ms
step:1250/2330 train_time:50789ms step_avg:40.63ms
step:1250/2330 val_loss:5.4556 train_time:50878ms step_avg:40.70ms
step:1251/2330 train_time:50890ms step_avg:40.68ms
step:1252/2330 train_time:50902ms step_avg:40.66ms
step:1253/2330 train_time:50913ms step_avg:40.63ms
step:1254/2330 train_time:50951ms step_avg:40.63ms
step:1255/2330 train_time:50985ms step_avg:40.63ms
step:1256/2330 train_time:51030ms step_avg:40.63ms
step:1257/2330 train_time:51065ms step_avg:40.62ms
step:1258/2330 train_time:51109ms step_avg:40.63ms
step:1259/2330 train_time:51144ms step_avg:40.62ms
step:1260/2330 train_time:51193ms step_avg:40.63ms
step:1261/2330 train_time:51234ms step_avg:40.63ms
step:1262/2330 train_time:51283ms step_avg:40.64ms
step:1263/2330 train_time:51319ms step_avg:40.63ms
step:1264/2330 train_time:51366ms step_avg:40.64ms
step:1265/2330 train_time:51402ms step_avg:40.63ms
step:1266/2330 train_time:51447ms step_avg:40.64ms
step:1267/2330 train_time:51482ms step_avg:40.63ms
step:1268/2330 train_time:51527ms step_avg:40.64ms
step:1269/2330 train_time:51563ms step_avg:40.63ms
step:1270/2330 train_time:51832ms step_avg:40.81ms
step:1271/2330 train_time:51844ms step_avg:40.79ms
step:1272/2330 train_time:51856ms step_avg:40.77ms
step:1273/2330 train_time:51886ms step_avg:40.76ms
step:1274/2330 train_time:51929ms step_avg:40.76ms
step:1275/2330 train_time:51963ms step_avg:40.76ms
step:1276/2330 train_time:52008ms step_avg:40.76ms
step:1277/2330 train_time:52043ms step_avg:40.75ms
step:1278/2330 train_time:52218ms step_avg:40.86ms
step:1279/2330 train_time:52252ms step_avg:40.85ms
step:1280/2330 train_time:52295ms step_avg:40.86ms
step:1281/2330 train_time:52331ms step_avg:40.85ms
step:1282/2330 train_time:52375ms step_avg:40.85ms
step:1283/2330 train_time:52410ms step_avg:40.85ms
step:1284/2330 train_time:52454ms step_avg:40.85ms
step:1285/2330 train_time:52489ms step_avg:40.85ms
step:1286/2330 train_time:52533ms step_avg:40.85ms
step:1287/2330 train_time:52568ms step_avg:40.85ms
step:1288/2330 train_time:52612ms step_avg:40.85ms
step:1289/2330 train_time:52646ms step_avg:40.84ms
step:1290/2330 train_time:52690ms step_avg:40.85ms
step:1291/2330 train_time:52725ms step_avg:40.84ms
step:1292/2330 train_time:52769ms step_avg:40.84ms
step:1293/2330 train_time:52804ms step_avg:40.84ms
step:1294/2330 train_time:52848ms step_avg:40.84ms
step:1295/2330 train_time:52884ms step_avg:40.84ms
step:1296/2330 train_time:52928ms step_avg:40.84ms
step:1297/2330 train_time:52964ms step_avg:40.84ms
step:1298/2330 train_time:53008ms step_avg:40.84ms
step:1299/2330 train_time:53043ms step_avg:40.83ms
step:1300/2330 train_time:53092ms step_avg:40.84ms
step:1301/2330 train_time:53135ms step_avg:40.84ms
step:1302/2330 train_time:53184ms step_avg:40.85ms
step:1303/2330 train_time:53221ms step_avg:40.85ms
step:1304/2330 train_time:53267ms step_avg:40.85ms
step:1305/2330 train_time:53303ms step_avg:40.85ms
step:1306/2330 train_time:53349ms step_avg:40.85ms
step:1307/2330 train_time:53384ms step_avg:40.84ms
step:1308/2330 train_time:53429ms step_avg:40.85ms
step:1309/2330 train_time:53465ms step_avg:40.84ms
step:1310/2330 train_time:53510ms step_avg:40.85ms
step:1311/2330 train_time:53545ms step_avg:40.84ms
step:1312/2330 train_time:53590ms step_avg:40.85ms
step:1313/2330 train_time:53625ms step_avg:40.84ms
step:1314/2330 train_time:53670ms step_avg:40.84ms
step:1315/2330 train_time:53705ms step_avg:40.84ms
step:1316/2330 train_time:53749ms step_avg:40.84ms
step:1317/2330 train_time:53784ms step_avg:40.84ms
step:1318/2330 train_time:53829ms step_avg:40.84ms
step:1319/2330 train_time:53864ms step_avg:40.84ms
step:1320/2330 train_time:53908ms step_avg:40.84ms
step:1321/2330 train_time:53944ms step_avg:40.84ms
step:1322/2330 train_time:53989ms step_avg:40.84ms
step:1323/2330 train_time:54026ms step_avg:40.84ms
step:1324/2330 train_time:54073ms step_avg:40.84ms
step:1325/2330 train_time:54112ms step_avg:40.84ms
step:1326/2330 train_time:54160ms step_avg:40.84ms
step:1327/2330 train_time:54197ms step_avg:40.84ms
step:1328/2330 train_time:54244ms step_avg:40.85ms
step:1329/2330 train_time:54280ms step_avg:40.84ms
step:1330/2330 train_time:54326ms step_avg:40.85ms
step:1331/2330 train_time:54363ms step_avg:40.84ms
step:1332/2330 train_time:54408ms step_avg:40.85ms
step:1333/2330 train_time:54443ms step_avg:40.84ms
step:1334/2330 train_time:54488ms step_avg:40.85ms
step:1335/2330 train_time:54524ms step_avg:40.84ms
step:1336/2330 train_time:54569ms step_avg:40.85ms
step:1337/2330 train_time:54605ms step_avg:40.84ms
step:1338/2330 train_time:54650ms step_avg:40.84ms
step:1339/2330 train_time:54685ms step_avg:40.84ms
step:1340/2330 train_time:54730ms step_avg:40.84ms
step:1341/2330 train_time:54765ms step_avg:40.84ms
step:1342/2330 train_time:54809ms step_avg:40.84ms
step:1343/2330 train_time:54844ms step_avg:40.84ms
step:1344/2330 train_time:54889ms step_avg:40.84ms
step:1345/2330 train_time:54925ms step_avg:40.84ms
step:1346/2330 train_time:54970ms step_avg:40.84ms
step:1347/2330 train_time:55007ms step_avg:40.84ms
step:1348/2330 train_time:55053ms step_avg:40.84ms
step:1349/2330 train_time:55091ms step_avg:40.84ms
step:1350/2330 train_time:55139ms step_avg:40.84ms
step:1351/2330 train_time:55176ms step_avg:40.84ms
step:1352/2330 train_time:55223ms step_avg:40.85ms
step:1353/2330 train_time:55259ms step_avg:40.84ms
step:1354/2330 train_time:55305ms step_avg:40.85ms
step:1355/2330 train_time:55342ms step_avg:40.84ms
step:1356/2330 train_time:55388ms step_avg:40.85ms
step:1357/2330 train_time:55424ms step_avg:40.84ms
step:1358/2330 train_time:55469ms step_avg:40.85ms
step:1359/2330 train_time:55504ms step_avg:40.84ms
step:1360/2330 train_time:55549ms step_avg:40.84ms
step:1361/2330 train_time:55584ms step_avg:40.84ms
step:1362/2330 train_time:55629ms step_avg:40.84ms
step:1363/2330 train_time:55664ms step_avg:40.84ms
step:1364/2330 train_time:55709ms step_avg:40.84ms
step:1365/2330 train_time:55744ms step_avg:40.84ms
step:1366/2330 train_time:55789ms step_avg:40.84ms
step:1367/2330 train_time:55824ms step_avg:40.84ms
step:1368/2330 train_time:55869ms step_avg:40.84ms
step:1369/2330 train_time:55904ms step_avg:40.84ms
step:1370/2330 train_time:55949ms step_avg:40.84ms
step:1371/2330 train_time:55985ms step_avg:40.84ms
step:1372/2330 train_time:56031ms step_avg:40.84ms
step:1373/2330 train_time:56068ms step_avg:40.84ms
step:1374/2330 train_time:56115ms step_avg:40.84ms
step:1375/2330 train_time:56153ms step_avg:40.84ms
step:1376/2330 train_time:56199ms step_avg:40.84ms
step:1377/2330 train_time:56236ms step_avg:40.84ms
step:1378/2330 train_time:56282ms step_avg:40.84ms
step:1379/2330 train_time:56318ms step_avg:40.84ms
step:1380/2330 train_time:56364ms step_avg:40.84ms
step:1381/2330 train_time:56401ms step_avg:40.84ms
step:1382/2330 train_time:56447ms step_avg:40.84ms
step:1383/2330 train_time:56483ms step_avg:40.84ms
step:1384/2330 train_time:56528ms step_avg:40.84ms
step:1385/2330 train_time:56564ms step_avg:40.84ms
step:1386/2330 train_time:56609ms step_avg:40.84ms
step:1387/2330 train_time:56644ms step_avg:40.84ms
step:1388/2330 train_time:56689ms step_avg:40.84ms
step:1389/2330 train_time:56725ms step_avg:40.84ms
step:1390/2330 train_time:56769ms step_avg:40.84ms
step:1391/2330 train_time:56804ms step_avg:40.84ms
step:1392/2330 train_time:56849ms step_avg:40.84ms
step:1393/2330 train_time:56885ms step_avg:40.84ms
step:1394/2330 train_time:56930ms step_avg:40.84ms
step:1395/2330 train_time:56966ms step_avg:40.84ms
step:1396/2330 train_time:57013ms step_avg:40.84ms
step:1397/2330 train_time:57049ms step_avg:40.84ms
step:1398/2330 train_time:57096ms step_avg:40.84ms
step:1399/2330 train_time:57132ms step_avg:40.84ms
step:1400/2330 train_time:57178ms step_avg:40.84ms
step:1401/2330 train_time:57214ms step_avg:40.84ms
step:1402/2330 train_time:57259ms step_avg:40.84ms
step:1403/2330 train_time:57295ms step_avg:40.84ms
step:1404/2330 train_time:57342ms step_avg:40.84ms
step:1405/2330 train_time:57377ms step_avg:40.84ms
step:1406/2330 train_time:57424ms step_avg:40.84ms
step:1407/2330 train_time:57459ms step_avg:40.84ms
step:1408/2330 train_time:57506ms step_avg:40.84ms
step:1409/2330 train_time:57542ms step_avg:40.84ms
step:1410/2330 train_time:57588ms step_avg:40.84ms
step:1411/2330 train_time:57624ms step_avg:40.84ms
step:1412/2330 train_time:57669ms step_avg:40.84ms
step:1413/2330 train_time:57705ms step_avg:40.84ms
step:1414/2330 train_time:57750ms step_avg:40.84ms
step:1415/2330 train_time:57785ms step_avg:40.84ms
step:1416/2330 train_time:57829ms step_avg:40.84ms
step:1417/2330 train_time:57865ms step_avg:40.84ms
step:1418/2330 train_time:57910ms step_avg:40.84ms
step:1419/2330 train_time:57945ms step_avg:40.84ms
step:1420/2330 train_time:57991ms step_avg:40.84ms
step:1421/2330 train_time:58028ms step_avg:40.84ms
step:1422/2330 train_time:58075ms step_avg:40.84ms
step:1423/2330 train_time:58112ms step_avg:40.84ms
step:1424/2330 train_time:58157ms step_avg:40.84ms
step:1425/2330 train_time:58193ms step_avg:40.84ms
step:1426/2330 train_time:58239ms step_avg:40.84ms
step:1427/2330 train_time:58275ms step_avg:40.84ms
step:1428/2330 train_time:58321ms step_avg:40.84ms
step:1429/2330 train_time:58357ms step_avg:40.84ms
step:1430/2330 train_time:58403ms step_avg:40.84ms
step:1431/2330 train_time:58438ms step_avg:40.84ms
step:1432/2330 train_time:58484ms step_avg:40.84ms
step:1433/2330 train_time:58519ms step_avg:40.84ms
step:1434/2330 train_time:58565ms step_avg:40.84ms
step:1435/2330 train_time:58600ms step_avg:40.84ms
step:1436/2330 train_time:58646ms step_avg:40.84ms
step:1437/2330 train_time:58682ms step_avg:40.84ms
step:1438/2330 train_time:58727ms step_avg:40.84ms
step:1439/2330 train_time:58763ms step_avg:40.84ms
step:1440/2330 train_time:58809ms step_avg:40.84ms
step:1441/2330 train_time:58844ms step_avg:40.84ms
step:1442/2330 train_time:58889ms step_avg:40.84ms
step:1443/2330 train_time:58924ms step_avg:40.83ms
step:1444/2330 train_time:58970ms step_avg:40.84ms
step:1445/2330 train_time:59007ms step_avg:40.83ms
step:1446/2330 train_time:59052ms step_avg:40.84ms
step:1447/2330 train_time:59089ms step_avg:40.84ms
step:1448/2330 train_time:59135ms step_avg:40.84ms
step:1449/2330 train_time:59171ms step_avg:40.84ms
step:1450/2330 train_time:59217ms step_avg:40.84ms
step:1451/2330 train_time:59253ms step_avg:40.84ms
step:1452/2330 train_time:59300ms step_avg:40.84ms
step:1453/2330 train_time:59336ms step_avg:40.84ms
step:1454/2330 train_time:59381ms step_avg:40.84ms
step:1455/2330 train_time:59418ms step_avg:40.84ms
step:1456/2330 train_time:59464ms step_avg:40.84ms
step:1457/2330 train_time:59500ms step_avg:40.84ms
step:1458/2330 train_time:59546ms step_avg:40.84ms
step:1459/2330 train_time:59582ms step_avg:40.84ms
step:1460/2330 train_time:59627ms step_avg:40.84ms
step:1461/2330 train_time:59663ms step_avg:40.84ms
step:1462/2330 train_time:59708ms step_avg:40.84ms
step:1463/2330 train_time:59744ms step_avg:40.84ms
step:1464/2330 train_time:59789ms step_avg:40.84ms
step:1465/2330 train_time:59825ms step_avg:40.84ms
step:1466/2330 train_time:59870ms step_avg:40.84ms
step:1467/2330 train_time:59906ms step_avg:40.84ms
step:1468/2330 train_time:59950ms step_avg:40.84ms
step:1469/2330 train_time:59986ms step_avg:40.83ms
step:1470/2330 train_time:60031ms step_avg:40.84ms
step:1471/2330 train_time:60068ms step_avg:40.83ms
step:1472/2330 train_time:60115ms step_avg:40.84ms
step:1473/2330 train_time:60151ms step_avg:40.84ms
step:1474/2330 train_time:60196ms step_avg:40.84ms
step:1475/2330 train_time:60232ms step_avg:40.84ms
step:1476/2330 train_time:60278ms step_avg:40.84ms
step:1477/2330 train_time:60315ms step_avg:40.84ms
step:1478/2330 train_time:60361ms step_avg:40.84ms
step:1479/2330 train_time:60396ms step_avg:40.84ms
step:1480/2330 train_time:60442ms step_avg:40.84ms
step:1481/2330 train_time:60478ms step_avg:40.84ms
step:1482/2330 train_time:60523ms step_avg:40.84ms
step:1483/2330 train_time:60559ms step_avg:40.84ms
step:1484/2330 train_time:60605ms step_avg:40.84ms
step:1485/2330 train_time:60641ms step_avg:40.84ms
step:1486/2330 train_time:60687ms step_avg:40.84ms
step:1487/2330 train_time:60722ms step_avg:40.84ms
step:1488/2330 train_time:60767ms step_avg:40.84ms
step:1489/2330 train_time:60803ms step_avg:40.83ms
step:1490/2330 train_time:60848ms step_avg:40.84ms
step:1491/2330 train_time:60884ms step_avg:40.83ms
step:1492/2330 train_time:60929ms step_avg:40.84ms
step:1493/2330 train_time:60965ms step_avg:40.83ms
step:1494/2330 train_time:61011ms step_avg:40.84ms
step:1495/2330 train_time:61047ms step_avg:40.83ms
step:1496/2330 train_time:61093ms step_avg:40.84ms
step:1497/2330 train_time:61129ms step_avg:40.83ms
step:1498/2330 train_time:61176ms step_avg:40.84ms
step:1499/2330 train_time:61211ms step_avg:40.83ms
step:1500/2330 train_time:61257ms step_avg:40.84ms
step:1500/2330 val_loss:5.4193 train_time:61347ms step_avg:40.90ms
step:1501/2330 train_time:61360ms step_avg:40.88ms
step:1502/2330 train_time:61372ms step_avg:40.86ms
step:1503/2330 train_time:61382ms step_avg:40.84ms
step:1504/2330 train_time:61420ms step_avg:40.84ms
step:1505/2330 train_time:61455ms step_avg:40.83ms
step:1506/2330 train_time:61500ms step_avg:40.84ms
step:1507/2330 train_time:61534ms step_avg:40.83ms
step:1508/2330 train_time:61578ms step_avg:40.83ms
step:1509/2330 train_time:61613ms step_avg:40.83ms
step:1510/2330 train_time:61659ms step_avg:40.83ms
step:1511/2330 train_time:61702ms step_avg:40.84ms
step:1512/2330 train_time:61751ms step_avg:40.84ms
step:1513/2330 train_time:61786ms step_avg:40.84ms
step:1514/2330 train_time:61832ms step_avg:40.84ms
step:1515/2330 train_time:61867ms step_avg:40.84ms
step:1516/2330 train_time:61912ms step_avg:40.84ms
step:1517/2330 train_time:61948ms step_avg:40.84ms
step:1518/2330 train_time:61993ms step_avg:40.84ms
step:1519/2330 train_time:62029ms step_avg:40.84ms
step:1520/2330 train_time:62075ms step_avg:40.84ms
step:1521/2330 train_time:62111ms step_avg:40.84ms
step:1522/2330 train_time:62155ms step_avg:40.84ms
step:1523/2330 train_time:62191ms step_avg:40.83ms
step:1524/2330 train_time:62237ms step_avg:40.84ms
step:1525/2330 train_time:62274ms step_avg:40.84ms
step:1526/2330 train_time:62320ms step_avg:40.84ms
step:1527/2330 train_time:62357ms step_avg:40.84ms
step:1528/2330 train_time:62401ms step_avg:40.84ms
step:1529/2330 train_time:62439ms step_avg:40.84ms
step:1530/2330 train_time:62483ms step_avg:40.84ms
step:1531/2330 train_time:62517ms step_avg:40.83ms
step:1532/2330 train_time:62563ms step_avg:40.84ms
step:1533/2330 train_time:62598ms step_avg:40.83ms
step:1534/2330 train_time:62770ms step_avg:40.92ms
step:1535/2330 train_time:62803ms step_avg:40.91ms
step:1536/2330 train_time:62848ms step_avg:40.92ms
step:1537/2330 train_time:62883ms step_avg:40.91ms
step:1538/2330 train_time:62927ms step_avg:40.92ms
step:1539/2330 train_time:62962ms step_avg:40.91ms
step:1540/2330 train_time:63007ms step_avg:40.91ms
step:1541/2330 train_time:63042ms step_avg:40.91ms
step:1542/2330 train_time:63087ms step_avg:40.91ms
step:1543/2330 train_time:63122ms step_avg:40.91ms
step:1544/2330 train_time:63166ms step_avg:40.91ms
step:1545/2330 train_time:63201ms step_avg:40.91ms
step:1546/2330 train_time:63246ms step_avg:40.91ms
step:1547/2330 train_time:63280ms step_avg:40.91ms
step:1548/2330 train_time:63325ms step_avg:40.91ms
step:1549/2330 train_time:63360ms step_avg:40.90ms
step:1550/2330 train_time:63405ms step_avg:40.91ms
step:1551/2330 train_time:63440ms step_avg:40.90ms
step:1552/2330 train_time:63484ms step_avg:40.90ms
step:1553/2330 train_time:63519ms step_avg:40.90ms
step:1554/2330 train_time:63563ms step_avg:40.90ms
step:1555/2330 train_time:63603ms step_avg:40.90ms
step:1556/2330 train_time:63655ms step_avg:40.91ms
step:1557/2330 train_time:63696ms step_avg:40.91ms
step:1558/2330 train_time:63745ms step_avg:40.91ms
step:1559/2330 train_time:63781ms step_avg:40.91ms
step:1560/2330 train_time:63827ms step_avg:40.91ms
step:1561/2330 train_time:63863ms step_avg:40.91ms
step:1562/2330 train_time:63908ms step_avg:40.91ms
step:1563/2330 train_time:63944ms step_avg:40.91ms
step:1564/2330 train_time:63988ms step_avg:40.91ms
step:1565/2330 train_time:64023ms step_avg:40.91ms
step:1566/2330 train_time:64068ms step_avg:40.91ms
step:1567/2330 train_time:64104ms step_avg:40.91ms
step:1568/2330 train_time:64149ms step_avg:40.91ms
step:1569/2330 train_time:64185ms step_avg:40.91ms
step:1570/2330 train_time:64231ms step_avg:40.91ms
step:1571/2330 train_time:64266ms step_avg:40.91ms
step:1572/2330 train_time:64311ms step_avg:40.91ms
step:1573/2330 train_time:64347ms step_avg:40.91ms
step:1574/2330 train_time:64392ms step_avg:40.91ms
step:1575/2330 train_time:64427ms step_avg:40.91ms
step:1576/2330 train_time:64472ms step_avg:40.91ms
step:1577/2330 train_time:64507ms step_avg:40.91ms
step:1578/2330 train_time:64553ms step_avg:40.91ms
step:1579/2330 train_time:64590ms step_avg:40.91ms
step:1580/2330 train_time:64638ms step_avg:40.91ms
step:1581/2330 train_time:64674ms step_avg:40.91ms
step:1582/2330 train_time:64721ms step_avg:40.91ms
step:1583/2330 train_time:64758ms step_avg:40.91ms
step:1584/2330 train_time:64804ms step_avg:40.91ms
step:1585/2330 train_time:64840ms step_avg:40.91ms
step:1586/2330 train_time:64886ms step_avg:40.91ms
step:1587/2330 train_time:64922ms step_avg:40.91ms
step:1588/2330 train_time:64967ms step_avg:40.91ms
step:1589/2330 train_time:65002ms step_avg:40.91ms
step:1590/2330 train_time:65048ms step_avg:40.91ms
step:1591/2330 train_time:65083ms step_avg:40.91ms
step:1592/2330 train_time:65128ms step_avg:40.91ms
step:1593/2330 train_time:65163ms step_avg:40.91ms
step:1594/2330 train_time:65208ms step_avg:40.91ms
step:1595/2330 train_time:65244ms step_avg:40.91ms
step:1596/2330 train_time:65289ms step_avg:40.91ms
step:1597/2330 train_time:65324ms step_avg:40.90ms
step:1598/2330 train_time:65369ms step_avg:40.91ms
step:1599/2330 train_time:65405ms step_avg:40.90ms
step:1600/2330 train_time:65450ms step_avg:40.91ms
step:1601/2330 train_time:65485ms step_avg:40.90ms
step:1602/2330 train_time:65531ms step_avg:40.91ms
step:1603/2330 train_time:65568ms step_avg:40.90ms
step:1604/2330 train_time:65613ms step_avg:40.91ms
step:1605/2330 train_time:65650ms step_avg:40.90ms
step:1606/2330 train_time:65697ms step_avg:40.91ms
step:1607/2330 train_time:65734ms step_avg:40.90ms
step:1608/2330 train_time:65780ms step_avg:40.91ms
step:1609/2330 train_time:65816ms step_avg:40.90ms
step:1610/2330 train_time:65861ms step_avg:40.91ms
step:1611/2330 train_time:65897ms step_avg:40.90ms
step:1612/2330 train_time:65943ms step_avg:40.91ms
step:1613/2330 train_time:65978ms step_avg:40.90ms
step:1614/2330 train_time:66023ms step_avg:40.91ms
step:1615/2330 train_time:66058ms step_avg:40.90ms
step:1616/2330 train_time:66103ms step_avg:40.91ms
step:1617/2330 train_time:66139ms step_avg:40.90ms
step:1618/2330 train_time:66184ms step_avg:40.90ms
step:1619/2330 train_time:66220ms step_avg:40.90ms
step:1620/2330 train_time:66265ms step_avg:40.90ms
step:1621/2330 train_time:66301ms step_avg:40.90ms
step:1622/2330 train_time:66346ms step_avg:40.90ms
step:1623/2330 train_time:66381ms step_avg:40.90ms
step:1624/2330 train_time:66427ms step_avg:40.90ms
step:1625/2330 train_time:66463ms step_avg:40.90ms
step:1626/2330 train_time:66509ms step_avg:40.90ms
step:1627/2330 train_time:66545ms step_avg:40.90ms
step:1628/2330 train_time:66590ms step_avg:40.90ms
step:1629/2330 train_time:66626ms step_avg:40.90ms
step:1630/2330 train_time:66671ms step_avg:40.90ms
step:1631/2330 train_time:66708ms step_avg:40.90ms
step:1632/2330 train_time:66754ms step_avg:40.90ms
step:1633/2330 train_time:66790ms step_avg:40.90ms
step:1634/2330 train_time:66837ms step_avg:40.90ms
step:1635/2330 train_time:66874ms step_avg:40.90ms
step:1636/2330 train_time:66919ms step_avg:40.90ms
step:1637/2330 train_time:66955ms step_avg:40.90ms
step:1638/2330 train_time:67001ms step_avg:40.90ms
step:1639/2330 train_time:67037ms step_avg:40.90ms
step:1640/2330 train_time:67082ms step_avg:40.90ms
step:1641/2330 train_time:67117ms step_avg:40.90ms
step:1642/2330 train_time:67162ms step_avg:40.90ms
step:1643/2330 train_time:67198ms step_avg:40.90ms
step:1644/2330 train_time:67243ms step_avg:40.90ms
step:1645/2330 train_time:67279ms step_avg:40.90ms
step:1646/2330 train_time:67325ms step_avg:40.90ms
step:1647/2330 train_time:67361ms step_avg:40.90ms
step:1648/2330 train_time:67406ms step_avg:40.90ms
step:1649/2330 train_time:67442ms step_avg:40.90ms
step:1650/2330 train_time:67488ms step_avg:40.90ms
step:1651/2330 train_time:67523ms step_avg:40.90ms
step:1652/2330 train_time:67569ms step_avg:40.90ms
step:1653/2330 train_time:67604ms step_avg:40.90ms
step:1654/2330 train_time:67650ms step_avg:40.90ms
step:1655/2330 train_time:67686ms step_avg:40.90ms
step:1656/2330 train_time:67731ms step_avg:40.90ms
step:1657/2330 train_time:67768ms step_avg:40.90ms
step:1658/2330 train_time:67813ms step_avg:40.90ms
step:1659/2330 train_time:67849ms step_avg:40.90ms
step:1660/2330 train_time:67895ms step_avg:40.90ms
step:1661/2330 train_time:67931ms step_avg:40.90ms
step:1662/2330 train_time:67976ms step_avg:40.90ms
step:1663/2330 train_time:68013ms step_avg:40.90ms
step:1664/2330 train_time:68058ms step_avg:40.90ms
step:1665/2330 train_time:68094ms step_avg:40.90ms
step:1666/2330 train_time:68141ms step_avg:40.90ms
step:1667/2330 train_time:68176ms step_avg:40.90ms
step:1668/2330 train_time:68221ms step_avg:40.90ms
step:1669/2330 train_time:68256ms step_avg:40.90ms
step:1670/2330 train_time:68303ms step_avg:40.90ms
step:1671/2330 train_time:68339ms step_avg:40.90ms
step:1672/2330 train_time:68385ms step_avg:40.90ms
step:1673/2330 train_time:68420ms step_avg:40.90ms
step:1674/2330 train_time:68465ms step_avg:40.90ms
step:1675/2330 train_time:68501ms step_avg:40.90ms
step:1676/2330 train_time:68546ms step_avg:40.90ms
step:1677/2330 train_time:68582ms step_avg:40.90ms
step:1678/2330 train_time:68628ms step_avg:40.90ms
step:1679/2330 train_time:68664ms step_avg:40.90ms
step:1680/2330 train_time:68709ms step_avg:40.90ms
step:1681/2330 train_time:68745ms step_avg:40.90ms
step:1682/2330 train_time:68790ms step_avg:40.90ms
step:1683/2330 train_time:68826ms step_avg:40.89ms
step:1684/2330 train_time:68871ms step_avg:40.90ms
step:1685/2330 train_time:68906ms step_avg:40.89ms
step:1686/2330 train_time:68953ms step_avg:40.90ms
step:1687/2330 train_time:68989ms step_avg:40.89ms
step:1688/2330 train_time:69035ms step_avg:40.90ms
step:1689/2330 train_time:69072ms step_avg:40.89ms
step:1690/2330 train_time:69118ms step_avg:40.90ms
step:1691/2330 train_time:69154ms step_avg:40.90ms
step:1692/2330 train_time:69198ms step_avg:40.90ms
step:1693/2330 train_time:69234ms step_avg:40.89ms
step:1694/2330 train_time:69278ms step_avg:40.90ms
step:1695/2330 train_time:69315ms step_avg:40.89ms
step:1696/2330 train_time:69359ms step_avg:40.90ms
step:1697/2330 train_time:69395ms step_avg:40.89ms
step:1698/2330 train_time:69442ms step_avg:40.90ms
step:1699/2330 train_time:69477ms step_avg:40.89ms
step:1700/2330 train_time:69523ms step_avg:40.90ms
step:1701/2330 train_time:69559ms step_avg:40.89ms
step:1702/2330 train_time:69606ms step_avg:40.90ms
step:1703/2330 train_time:69641ms step_avg:40.89ms
step:1704/2330 train_time:69686ms step_avg:40.90ms
step:1705/2330 train_time:69721ms step_avg:40.89ms
step:1706/2330 train_time:69767ms step_avg:40.90ms
step:1707/2330 train_time:69803ms step_avg:40.89ms
step:1708/2330 train_time:69849ms step_avg:40.90ms
step:1709/2330 train_time:69885ms step_avg:40.89ms
step:1710/2330 train_time:69931ms step_avg:40.90ms
step:1711/2330 train_time:69967ms step_avg:40.89ms
step:1712/2330 train_time:70012ms step_avg:40.89ms
step:1713/2330 train_time:70049ms step_avg:40.89ms
step:1714/2330 train_time:70095ms step_avg:40.90ms
step:1715/2330 train_time:70132ms step_avg:40.89ms
step:1716/2330 train_time:70178ms step_avg:40.90ms
step:1717/2330 train_time:70213ms step_avg:40.89ms
step:1718/2330 train_time:70258ms step_avg:40.90ms
step:1719/2330 train_time:70294ms step_avg:40.89ms
step:1720/2330 train_time:70339ms step_avg:40.89ms
step:1721/2330 train_time:70374ms step_avg:40.89ms
step:1722/2330 train_time:70419ms step_avg:40.89ms
step:1723/2330 train_time:70455ms step_avg:40.89ms
step:1724/2330 train_time:70502ms step_avg:40.89ms
step:1725/2330 train_time:70538ms step_avg:40.89ms
step:1726/2330 train_time:70583ms step_avg:40.89ms
step:1727/2330 train_time:70619ms step_avg:40.89ms
step:1728/2330 train_time:70665ms step_avg:40.89ms
step:1729/2330 train_time:70700ms step_avg:40.89ms
step:1730/2330 train_time:70746ms step_avg:40.89ms
step:1731/2330 train_time:70782ms step_avg:40.89ms
step:1732/2330 train_time:70828ms step_avg:40.89ms
step:1733/2330 train_time:70864ms step_avg:40.89ms
step:1734/2330 train_time:70910ms step_avg:40.89ms
step:1735/2330 train_time:70947ms step_avg:40.89ms
step:1736/2330 train_time:70992ms step_avg:40.89ms
step:1737/2330 train_time:71028ms step_avg:40.89ms
step:1738/2330 train_time:71074ms step_avg:40.89ms
step:1739/2330 train_time:71110ms step_avg:40.89ms
step:1740/2330 train_time:71156ms step_avg:40.89ms
step:1741/2330 train_time:71192ms step_avg:40.89ms
step:1742/2330 train_time:71237ms step_avg:40.89ms
step:1743/2330 train_time:71272ms step_avg:40.89ms
step:1744/2330 train_time:71317ms step_avg:40.89ms
step:1745/2330 train_time:71353ms step_avg:40.89ms
step:1746/2330 train_time:71399ms step_avg:40.89ms
step:1747/2330 train_time:71435ms step_avg:40.89ms
step:1748/2330 train_time:71480ms step_avg:40.89ms
step:1749/2330 train_time:71516ms step_avg:40.89ms
step:1750/2330 train_time:71560ms step_avg:40.89ms
step:1750/2330 val_loss:5.2976 train_time:71650ms step_avg:40.94ms
step:1751/2330 train_time:71663ms step_avg:40.93ms
step:1752/2330 train_time:71675ms step_avg:40.91ms
step:1753/2330 train_time:71686ms step_avg:40.89ms
step:1754/2330 train_time:71723ms step_avg:40.89ms
step:1755/2330 train_time:71757ms step_avg:40.89ms
step:1756/2330 train_time:71801ms step_avg:40.89ms
step:1757/2330 train_time:71836ms step_avg:40.89ms
step:1758/2330 train_time:71880ms step_avg:40.89ms
step:1759/2330 train_time:71915ms step_avg:40.88ms
step:1760/2330 train_time:71960ms step_avg:40.89ms
step:1761/2330 train_time:71996ms step_avg:40.88ms
step:1762/2330 train_time:72042ms step_avg:40.89ms
step:1763/2330 train_time:72078ms step_avg:40.88ms
step:1764/2330 train_time:72124ms step_avg:40.89ms
step:1765/2330 train_time:72160ms step_avg:40.88ms
step:1766/2330 train_time:72206ms step_avg:40.89ms
step:1767/2330 train_time:72241ms step_avg:40.88ms
step:1768/2330 train_time:72287ms step_avg:40.89ms
step:1769/2330 train_time:72323ms step_avg:40.88ms
step:1770/2330 train_time:72368ms step_avg:40.89ms
step:1771/2330 train_time:72403ms step_avg:40.88ms
step:1772/2330 train_time:72447ms step_avg:40.88ms
step:1773/2330 train_time:72483ms step_avg:40.88ms
step:1774/2330 train_time:72529ms step_avg:40.88ms
step:1775/2330 train_time:72569ms step_avg:40.88ms
step:1776/2330 train_time:72620ms step_avg:40.89ms
step:1777/2330 train_time:72656ms step_avg:40.89ms
step:1778/2330 train_time:72702ms step_avg:40.89ms
step:1779/2330 train_time:72740ms step_avg:40.89ms
step:1780/2330 train_time:72786ms step_avg:40.89ms
step:1781/2330 train_time:72823ms step_avg:40.89ms
step:1782/2330 train_time:72868ms step_avg:40.89ms
step:1783/2330 train_time:72904ms step_avg:40.89ms
step:1784/2330 train_time:72949ms step_avg:40.89ms
step:1785/2330 train_time:72985ms step_avg:40.89ms
step:1786/2330 train_time:73030ms step_avg:40.89ms
step:1787/2330 train_time:73065ms step_avg:40.89ms
step:1788/2330 train_time:73109ms step_avg:40.89ms
step:1789/2330 train_time:73144ms step_avg:40.89ms
step:1790/2330 train_time:73190ms step_avg:40.89ms
step:1791/2330 train_time:73226ms step_avg:40.89ms
step:1792/2330 train_time:73271ms step_avg:40.89ms
step:1793/2330 train_time:73307ms step_avg:40.89ms
step:1794/2330 train_time:73353ms step_avg:40.89ms
step:1795/2330 train_time:73388ms step_avg:40.88ms
step:1796/2330 train_time:73433ms step_avg:40.89ms
step:1797/2330 train_time:73469ms step_avg:40.88ms
step:1798/2330 train_time:73516ms step_avg:40.89ms
step:1799/2330 train_time:73553ms step_avg:40.89ms
step:1800/2330 train_time:73599ms step_avg:40.89ms
step:1801/2330 train_time:73635ms step_avg:40.89ms
step:1802/2330 train_time:73680ms step_avg:40.89ms
step:1803/2330 train_time:73717ms step_avg:40.89ms
step:1804/2330 train_time:73762ms step_avg:40.89ms
step:1805/2330 train_time:73799ms step_avg:40.89ms
step:1806/2330 train_time:73846ms step_avg:40.89ms
step:1807/2330 train_time:73883ms step_avg:40.89ms
step:1808/2330 train_time:73929ms step_avg:40.89ms
step:1809/2330 train_time:73965ms step_avg:40.89ms
step:1810/2330 train_time:74009ms step_avg:40.89ms
step:1811/2330 train_time:74044ms step_avg:40.89ms
step:1812/2330 train_time:74090ms step_avg:40.89ms
step:1813/2330 train_time:74124ms step_avg:40.88ms
step:1814/2330 train_time:74169ms step_avg:40.89ms
step:1815/2330 train_time:74204ms step_avg:40.88ms
step:1816/2330 train_time:74249ms step_avg:40.89ms
step:1817/2330 train_time:74284ms step_avg:40.88ms
step:1818/2330 train_time:74329ms step_avg:40.89ms
step:1819/2330 train_time:74365ms step_avg:40.88ms
step:1820/2330 train_time:74410ms step_avg:40.88ms
step:1821/2330 train_time:74446ms step_avg:40.88ms
step:1822/2330 train_time:74493ms step_avg:40.89ms
step:1823/2330 train_time:74529ms step_avg:40.88ms
step:1824/2330 train_time:74575ms step_avg:40.89ms
step:1825/2330 train_time:74612ms step_avg:40.88ms
step:1826/2330 train_time:74657ms step_avg:40.89ms
step:1827/2330 train_time:74694ms step_avg:40.88ms
step:1828/2330 train_time:74740ms step_avg:40.89ms
step:1829/2330 train_time:74776ms step_avg:40.88ms
step:1830/2330 train_time:74822ms step_avg:40.89ms
step:1831/2330 train_time:74858ms step_avg:40.88ms
step:1832/2330 train_time:74904ms step_avg:40.89ms
step:1833/2330 train_time:74940ms step_avg:40.88ms
step:1834/2330 train_time:74986ms step_avg:40.89ms
step:1835/2330 train_time:75021ms step_avg:40.88ms
step:1836/2330 train_time:75067ms step_avg:40.89ms
step:1837/2330 train_time:75102ms step_avg:40.88ms
step:1838/2330 train_time:75146ms step_avg:40.88ms
step:1839/2330 train_time:75182ms step_avg:40.88ms
step:1840/2330 train_time:75227ms step_avg:40.88ms
step:1841/2330 train_time:75263ms step_avg:40.88ms
step:1842/2330 train_time:75308ms step_avg:40.88ms
step:1843/2330 train_time:75343ms step_avg:40.88ms
step:1844/2330 train_time:75390ms step_avg:40.88ms
step:1845/2330 train_time:75426ms step_avg:40.88ms
step:1846/2330 train_time:75472ms step_avg:40.88ms
step:1847/2330 train_time:75508ms step_avg:40.88ms
step:1848/2330 train_time:75554ms step_avg:40.88ms
step:1849/2330 train_time:75590ms step_avg:40.88ms
step:1850/2330 train_time:75637ms step_avg:40.88ms
step:1851/2330 train_time:75673ms step_avg:40.88ms
step:1852/2330 train_time:75719ms step_avg:40.89ms
step:1853/2330 train_time:75755ms step_avg:40.88ms
step:1854/2330 train_time:75801ms step_avg:40.88ms
step:1855/2330 train_time:75836ms step_avg:40.88ms
step:1856/2330 train_time:75881ms step_avg:40.88ms
step:1857/2330 train_time:75917ms step_avg:40.88ms
step:1858/2330 train_time:75962ms step_avg:40.88ms
step:1859/2330 train_time:75998ms step_avg:40.88ms
step:1860/2330 train_time:76044ms step_avg:40.88ms
step:1861/2330 train_time:76079ms step_avg:40.88ms
step:1862/2330 train_time:76124ms step_avg:40.88ms
step:1863/2330 train_time:76160ms step_avg:40.88ms
step:1864/2330 train_time:76205ms step_avg:40.88ms
step:1865/2330 train_time:76240ms step_avg:40.88ms
step:1866/2330 train_time:76286ms step_avg:40.88ms
step:1867/2330 train_time:76322ms step_avg:40.88ms
step:1868/2330 train_time:76368ms step_avg:40.88ms
step:1869/2330 train_time:76404ms step_avg:40.88ms
step:1870/2330 train_time:76449ms step_avg:40.88ms
step:1871/2330 train_time:76486ms step_avg:40.88ms
step:1872/2330 train_time:76533ms step_avg:40.88ms
step:1873/2330 train_time:76569ms step_avg:40.88ms
step:1874/2330 train_time:76615ms step_avg:40.88ms
step:1875/2330 train_time:76651ms step_avg:40.88ms
step:1876/2330 train_time:76697ms step_avg:40.88ms
step:1877/2330 train_time:76734ms step_avg:40.88ms
step:1878/2330 train_time:76780ms step_avg:40.88ms
step:1879/2330 train_time:76815ms step_avg:40.88ms
step:1880/2330 train_time:76860ms step_avg:40.88ms
step:1881/2330 train_time:76895ms step_avg:40.88ms
step:1882/2330 train_time:76941ms step_avg:40.88ms
step:1883/2330 train_time:76976ms step_avg:40.88ms
step:1884/2330 train_time:77022ms step_avg:40.88ms
step:1885/2330 train_time:77057ms step_avg:40.88ms
step:1886/2330 train_time:77102ms step_avg:40.88ms
step:1887/2330 train_time:77138ms step_avg:40.88ms
step:1888/2330 train_time:77183ms step_avg:40.88ms
step:1889/2330 train_time:77219ms step_avg:40.88ms
step:1890/2330 train_time:77265ms step_avg:40.88ms
step:1891/2330 train_time:77301ms step_avg:40.88ms
step:1892/2330 train_time:77346ms step_avg:40.88ms
step:1893/2330 train_time:77383ms step_avg:40.88ms
step:1894/2330 train_time:77429ms step_avg:40.88ms
step:1895/2330 train_time:77464ms step_avg:40.88ms
step:1896/2330 train_time:77510ms step_avg:40.88ms
step:1897/2330 train_time:77546ms step_avg:40.88ms
step:1898/2330 train_time:77591ms step_avg:40.88ms
step:1899/2330 train_time:77627ms step_avg:40.88ms
step:1900/2330 train_time:77673ms step_avg:40.88ms
step:1901/2330 train_time:77710ms step_avg:40.88ms
step:1902/2330 train_time:77756ms step_avg:40.88ms
step:1903/2330 train_time:77792ms step_avg:40.88ms
step:1904/2330 train_time:77838ms step_avg:40.88ms
step:1905/2330 train_time:77874ms step_avg:40.88ms
step:1906/2330 train_time:77920ms step_avg:40.88ms
step:1907/2330 train_time:77956ms step_avg:40.88ms
step:1908/2330 train_time:78001ms step_avg:40.88ms
step:1909/2330 train_time:78036ms step_avg:40.88ms
step:1910/2330 train_time:78081ms step_avg:40.88ms
step:1911/2330 train_time:78117ms step_avg:40.88ms
step:1912/2330 train_time:78162ms step_avg:40.88ms
step:1913/2330 train_time:78198ms step_avg:40.88ms
step:1914/2330 train_time:78242ms step_avg:40.88ms
step:1915/2330 train_time:78278ms step_avg:40.88ms
step:1916/2330 train_time:78324ms step_avg:40.88ms
step:1917/2330 train_time:78360ms step_avg:40.88ms
step:1918/2330 train_time:78406ms step_avg:40.88ms
step:1919/2330 train_time:78442ms step_avg:40.88ms
step:1920/2330 train_time:78488ms step_avg:40.88ms
step:1921/2330 train_time:78526ms step_avg:40.88ms
step:1922/2330 train_time:78571ms step_avg:40.88ms
step:1923/2330 train_time:78607ms step_avg:40.88ms
step:1924/2330 train_time:78652ms step_avg:40.88ms
step:1925/2330 train_time:78689ms step_avg:40.88ms
step:1926/2330 train_time:78734ms step_avg:40.88ms
step:1927/2330 train_time:78769ms step_avg:40.88ms
step:1928/2330 train_time:78815ms step_avg:40.88ms
step:1929/2330 train_time:78851ms step_avg:40.88ms
step:1930/2330 train_time:78897ms step_avg:40.88ms
step:1931/2330 train_time:78933ms step_avg:40.88ms
step:1932/2330 train_time:78979ms step_avg:40.88ms
step:1933/2330 train_time:79014ms step_avg:40.88ms
step:1934/2330 train_time:79060ms step_avg:40.88ms
step:1935/2330 train_time:79095ms step_avg:40.88ms
step:1936/2330 train_time:79140ms step_avg:40.88ms
step:1937/2330 train_time:79176ms step_avg:40.88ms
step:1938/2330 train_time:79221ms step_avg:40.88ms
step:1939/2330 train_time:79257ms step_avg:40.88ms
step:1940/2330 train_time:79302ms step_avg:40.88ms
step:1941/2330 train_time:79338ms step_avg:40.87ms
step:1942/2330 train_time:79383ms step_avg:40.88ms
step:1943/2330 train_time:79419ms step_avg:40.87ms
step:1944/2330 train_time:79464ms step_avg:40.88ms
step:1945/2330 train_time:79500ms step_avg:40.87ms
step:1946/2330 train_time:79546ms step_avg:40.88ms
step:1947/2330 train_time:79583ms step_avg:40.87ms
step:1948/2330 train_time:79629ms step_avg:40.88ms
step:1949/2330 train_time:79667ms step_avg:40.88ms
step:1950/2330 train_time:79712ms step_avg:40.88ms
step:1951/2330 train_time:79748ms step_avg:40.88ms
step:1952/2330 train_time:79793ms step_avg:40.88ms
step:1953/2330 train_time:79829ms step_avg:40.88ms
step:1954/2330 train_time:79874ms step_avg:40.88ms
step:1955/2330 train_time:79909ms step_avg:40.87ms
step:1956/2330 train_time:79955ms step_avg:40.88ms
step:1957/2330 train_time:79991ms step_avg:40.87ms
step:1958/2330 train_time:80036ms step_avg:40.88ms
step:1959/2330 train_time:80072ms step_avg:40.87ms
step:1960/2330 train_time:80118ms step_avg:40.88ms
step:1961/2330 train_time:80153ms step_avg:40.87ms
step:1962/2330 train_time:80199ms step_avg:40.88ms
step:1963/2330 train_time:80236ms step_avg:40.87ms
step:1964/2330 train_time:80281ms step_avg:40.88ms
step:1965/2330 train_time:80317ms step_avg:40.87ms
step:1966/2330 train_time:80362ms step_avg:40.88ms
step:1967/2330 train_time:80398ms step_avg:40.87ms
step:1968/2330 train_time:80443ms step_avg:40.88ms
step:1969/2330 train_time:80479ms step_avg:40.87ms
step:1970/2330 train_time:80525ms step_avg:40.88ms
step:1971/2330 train_time:80562ms step_avg:40.87ms
step:1972/2330 train_time:80608ms step_avg:40.88ms
step:1973/2330 train_time:80645ms step_avg:40.87ms
step:1974/2330 train_time:80690ms step_avg:40.88ms
step:1975/2330 train_time:80726ms step_avg:40.87ms
step:1976/2330 train_time:80773ms step_avg:40.88ms
step:1977/2330 train_time:80808ms step_avg:40.87ms
step:1978/2330 train_time:80854ms step_avg:40.88ms
step:1979/2330 train_time:80890ms step_avg:40.87ms
step:1980/2330 train_time:80935ms step_avg:40.88ms
step:1981/2330 train_time:80971ms step_avg:40.87ms
step:1982/2330 train_time:81016ms step_avg:40.88ms
step:1983/2330 train_time:81052ms step_avg:40.87ms
step:1984/2330 train_time:81098ms step_avg:40.88ms
step:1985/2330 train_time:81134ms step_avg:40.87ms
step:1986/2330 train_time:81179ms step_avg:40.88ms
step:1987/2330 train_time:81215ms step_avg:40.87ms
step:1988/2330 train_time:81261ms step_avg:40.88ms
step:1989/2330 train_time:81296ms step_avg:40.87ms
step:1990/2330 train_time:81341ms step_avg:40.87ms
step:1991/2330 train_time:81376ms step_avg:40.87ms
step:1992/2330 train_time:81422ms step_avg:40.87ms
step:1993/2330 train_time:81458ms step_avg:40.87ms
step:1994/2330 train_time:81504ms step_avg:40.87ms
step:1995/2330 train_time:81539ms step_avg:40.87ms
step:1996/2330 train_time:81586ms step_avg:40.87ms
step:1997/2330 train_time:81623ms step_avg:40.87ms
step:1998/2330 train_time:81669ms step_avg:40.88ms
step:1999/2330 train_time:81706ms step_avg:40.87ms
step:2000/2330 train_time:81750ms step_avg:40.88ms
step:2000/2330 val_loss:5.2430 train_time:81840ms step_avg:40.92ms
step:2001/2330 train_time:81854ms step_avg:40.91ms
step:2002/2330 train_time:81867ms step_avg:40.89ms
step:2003/2330 train_time:81878ms step_avg:40.88ms
step:2004/2330 train_time:81913ms step_avg:40.87ms
step:2005/2330 train_time:81947ms step_avg:40.87ms
step:2006/2330 train_time:81991ms step_avg:40.87ms
step:2007/2330 train_time:82026ms step_avg:40.87ms
step:2008/2330 train_time:82071ms step_avg:40.87ms
step:2009/2330 train_time:82106ms step_avg:40.87ms
step:2010/2330 train_time:82153ms step_avg:40.87ms
step:2011/2330 train_time:82193ms step_avg:40.87ms
step:2012/2330 train_time:82243ms step_avg:40.88ms
step:2013/2330 train_time:82281ms step_avg:40.87ms
step:2014/2330 train_time:82326ms step_avg:40.88ms
step:2015/2330 train_time:82363ms step_avg:40.87ms
step:2016/2330 train_time:82408ms step_avg:40.88ms
step:2017/2330 train_time:82443ms step_avg:40.87ms
step:2018/2330 train_time:82488ms step_avg:40.88ms
step:2019/2330 train_time:82523ms step_avg:40.87ms
step:2020/2330 train_time:82567ms step_avg:40.87ms
step:2021/2330 train_time:82603ms step_avg:40.87ms
step:2022/2330 train_time:82649ms step_avg:40.87ms
step:2023/2330 train_time:82685ms step_avg:40.87ms
step:2024/2330 train_time:82729ms step_avg:40.87ms
step:2025/2330 train_time:82766ms step_avg:40.87ms
step:2026/2330 train_time:82812ms step_avg:40.87ms
step:2027/2330 train_time:82847ms step_avg:40.87ms
step:2028/2330 train_time:82892ms step_avg:40.87ms
step:2029/2330 train_time:82927ms step_avg:40.87ms
step:2030/2330 train_time:82971ms step_avg:40.87ms
step:2031/2330 train_time:83007ms step_avg:40.87ms
step:2032/2330 train_time:83052ms step_avg:40.87ms
step:2033/2330 train_time:83089ms step_avg:40.87ms
step:2034/2330 train_time:83136ms step_avg:40.87ms
step:2035/2330 train_time:83175ms step_avg:40.87ms
step:2036/2330 train_time:83222ms step_avg:40.88ms
step:2037/2330 train_time:83259ms step_avg:40.87ms
step:2038/2330 train_time:83305ms step_avg:40.88ms
step:2039/2330 train_time:83342ms step_avg:40.87ms
step:2040/2330 train_time:83387ms step_avg:40.88ms
step:2041/2330 train_time:83424ms step_avg:40.87ms
step:2042/2330 train_time:83469ms step_avg:40.88ms
step:2043/2330 train_time:83504ms step_avg:40.87ms
step:2044/2330 train_time:83549ms step_avg:40.88ms
step:2045/2330 train_time:83585ms step_avg:40.87ms
step:2046/2330 train_time:83629ms step_avg:40.87ms
step:2047/2330 train_time:83665ms step_avg:40.87ms
step:2048/2330 train_time:83710ms step_avg:40.87ms
step:2049/2330 train_time:83745ms step_avg:40.87ms
step:2050/2330 train_time:83791ms step_avg:40.87ms
step:2051/2330 train_time:83826ms step_avg:40.87ms
step:2052/2330 train_time:83871ms step_avg:40.87ms
step:2053/2330 train_time:83906ms step_avg:40.87ms
step:2054/2330 train_time:83951ms step_avg:40.87ms
step:2055/2330 train_time:83987ms step_avg:40.87ms
step:2056/2330 train_time:84033ms step_avg:40.87ms
step:2057/2330 train_time:84070ms step_avg:40.87ms
step:2058/2330 train_time:84116ms step_avg:40.87ms
step:2059/2330 train_time:84153ms step_avg:40.87ms
step:2060/2330 train_time:84200ms step_avg:40.87ms
step:2061/2330 train_time:84237ms step_avg:40.87ms
step:2062/2330 train_time:84284ms step_avg:40.87ms
step:2063/2330 train_time:84319ms step_avg:40.87ms
step:2064/2330 train_time:84365ms step_avg:40.87ms
step:2065/2330 train_time:84402ms step_avg:40.87ms
step:2066/2330 train_time:84447ms step_avg:40.87ms
step:2067/2330 train_time:84483ms step_avg:40.87ms
step:2068/2330 train_time:84528ms step_avg:40.87ms
step:2069/2330 train_time:84564ms step_avg:40.87ms
step:2070/2330 train_time:84608ms step_avg:40.87ms
step:2071/2330 train_time:84643ms step_avg:40.87ms
step:2072/2330 train_time:84689ms step_avg:40.87ms
step:2073/2330 train_time:84724ms step_avg:40.87ms
step:2074/2330 train_time:84770ms step_avg:40.87ms
step:2075/2330 train_time:84805ms step_avg:40.87ms
step:2076/2330 train_time:84850ms step_avg:40.87ms
step:2077/2330 train_time:84886ms step_avg:40.87ms
step:2078/2330 train_time:84930ms step_avg:40.87ms
step:2079/2330 train_time:84966ms step_avg:40.87ms
step:2080/2330 train_time:85011ms step_avg:40.87ms
step:2081/2330 train_time:85047ms step_avg:40.87ms
step:2082/2330 train_time:85094ms step_avg:40.87ms
step:2083/2330 train_time:85131ms step_avg:40.87ms
step:2084/2330 train_time:85178ms step_avg:40.87ms
step:2085/2330 train_time:85214ms step_avg:40.87ms
step:2086/2330 train_time:85259ms step_avg:40.87ms
step:2087/2330 train_time:85296ms step_avg:40.87ms
step:2088/2330 train_time:85342ms step_avg:40.87ms
step:2089/2330 train_time:85378ms step_avg:40.87ms
step:2090/2330 train_time:85424ms step_avg:40.87ms
step:2091/2330 train_time:85460ms step_avg:40.87ms
step:2092/2330 train_time:85505ms step_avg:40.87ms
step:2093/2330 train_time:85541ms step_avg:40.87ms
step:2094/2330 train_time:85586ms step_avg:40.87ms
step:2095/2330 train_time:85621ms step_avg:40.87ms
step:2096/2330 train_time:85666ms step_avg:40.87ms
step:2097/2330 train_time:85701ms step_avg:40.87ms
step:2098/2330 train_time:85747ms step_avg:40.87ms
step:2099/2330 train_time:85783ms step_avg:40.87ms
step:2100/2330 train_time:85828ms step_avg:40.87ms
step:2101/2330 train_time:85864ms step_avg:40.87ms
step:2102/2330 train_time:85909ms step_avg:40.87ms
step:2103/2330 train_time:85945ms step_avg:40.87ms
step:2104/2330 train_time:85990ms step_avg:40.87ms
step:2105/2330 train_time:86026ms step_avg:40.87ms
step:2106/2330 train_time:86071ms step_avg:40.87ms
step:2107/2330 train_time:86108ms step_avg:40.87ms
step:2108/2330 train_time:86153ms step_avg:40.87ms
step:2109/2330 train_time:86189ms step_avg:40.87ms
step:2110/2330 train_time:86237ms step_avg:40.87ms
step:2111/2330 train_time:86274ms step_avg:40.87ms
step:2112/2330 train_time:86319ms step_avg:40.87ms
step:2113/2330 train_time:86355ms step_avg:40.87ms
step:2114/2330 train_time:86400ms step_avg:40.87ms
step:2115/2330 train_time:86438ms step_avg:40.87ms
step:2116/2330 train_time:86483ms step_avg:40.87ms
step:2117/2330 train_time:86519ms step_avg:40.87ms
step:2118/2330 train_time:86565ms step_avg:40.87ms
step:2119/2330 train_time:86600ms step_avg:40.87ms
step:2120/2330 train_time:86645ms step_avg:40.87ms
step:2121/2330 train_time:86681ms step_avg:40.87ms
step:2122/2330 train_time:86725ms step_avg:40.87ms
step:2123/2330 train_time:86761ms step_avg:40.87ms
step:2124/2330 train_time:86807ms step_avg:40.87ms
step:2125/2330 train_time:86842ms step_avg:40.87ms
step:2126/2330 train_time:86888ms step_avg:40.87ms
step:2127/2330 train_time:86924ms step_avg:40.87ms
step:2128/2330 train_time:86970ms step_avg:40.87ms
step:2129/2330 train_time:87005ms step_avg:40.87ms
step:2130/2330 train_time:87050ms step_avg:40.87ms
step:2131/2330 train_time:87086ms step_avg:40.87ms
step:2132/2330 train_time:87131ms step_avg:40.87ms
step:2133/2330 train_time:87168ms step_avg:40.87ms
step:2134/2330 train_time:87214ms step_avg:40.87ms
step:2135/2330 train_time:87250ms step_avg:40.87ms
step:2136/2330 train_time:87295ms step_avg:40.87ms
step:2137/2330 train_time:87332ms step_avg:40.87ms
step:2138/2330 train_time:87379ms step_avg:40.87ms
step:2139/2330 train_time:87416ms step_avg:40.87ms
step:2140/2330 train_time:87462ms step_avg:40.87ms
step:2141/2330 train_time:87497ms step_avg:40.87ms
step:2142/2330 train_time:87542ms step_avg:40.87ms
step:2143/2330 train_time:87578ms step_avg:40.87ms
step:2144/2330 train_time:87623ms step_avg:40.87ms
step:2145/2330 train_time:87659ms step_avg:40.87ms
step:2146/2330 train_time:87705ms step_avg:40.87ms
step:2147/2330 train_time:87741ms step_avg:40.87ms
step:2148/2330 train_time:87786ms step_avg:40.87ms
step:2149/2330 train_time:87822ms step_avg:40.87ms
step:2150/2330 train_time:87868ms step_avg:40.87ms
step:2151/2330 train_time:87904ms step_avg:40.87ms
step:2152/2330 train_time:87950ms step_avg:40.87ms
step:2153/2330 train_time:87985ms step_avg:40.87ms
step:2154/2330 train_time:88031ms step_avg:40.87ms
step:2155/2330 train_time:88067ms step_avg:40.87ms
step:2156/2330 train_time:88112ms step_avg:40.87ms
step:2157/2330 train_time:88149ms step_avg:40.87ms
step:2158/2330 train_time:88194ms step_avg:40.87ms
step:2159/2330 train_time:88230ms step_avg:40.87ms
step:2160/2330 train_time:88275ms step_avg:40.87ms
step:2161/2330 train_time:88312ms step_avg:40.87ms
step:2162/2330 train_time:88357ms step_avg:40.87ms
step:2163/2330 train_time:88393ms step_avg:40.87ms
step:2164/2330 train_time:88440ms step_avg:40.87ms
step:2165/2330 train_time:88476ms step_avg:40.87ms
step:2166/2330 train_time:88522ms step_avg:40.87ms
step:2167/2330 train_time:88557ms step_avg:40.87ms
step:2168/2330 train_time:88603ms step_avg:40.87ms
step:2169/2330 train_time:88639ms step_avg:40.87ms
step:2170/2330 train_time:88685ms step_avg:40.87ms
step:2171/2330 train_time:88720ms step_avg:40.87ms
step:2172/2330 train_time:88766ms step_avg:40.87ms
step:2173/2330 train_time:88802ms step_avg:40.87ms
step:2174/2330 train_time:88847ms step_avg:40.87ms
step:2175/2330 train_time:88883ms step_avg:40.87ms
step:2176/2330 train_time:88929ms step_avg:40.87ms
step:2177/2330 train_time:88965ms step_avg:40.87ms
step:2178/2330 train_time:89010ms step_avg:40.87ms
step:2179/2330 train_time:89046ms step_avg:40.87ms
step:2180/2330 train_time:89091ms step_avg:40.87ms
step:2181/2330 train_time:89128ms step_avg:40.87ms
step:2182/2330 train_time:89173ms step_avg:40.87ms
step:2183/2330 train_time:89209ms step_avg:40.87ms
step:2184/2330 train_time:89254ms step_avg:40.87ms
step:2185/2330 train_time:89291ms step_avg:40.87ms
step:2186/2330 train_time:89338ms step_avg:40.87ms
step:2187/2330 train_time:89374ms step_avg:40.87ms
step:2188/2330 train_time:89419ms step_avg:40.87ms
step:2189/2330 train_time:89454ms step_avg:40.87ms
step:2190/2330 train_time:89499ms step_avg:40.87ms
step:2191/2330 train_time:89535ms step_avg:40.87ms
step:2192/2330 train_time:89581ms step_avg:40.87ms
step:2193/2330 train_time:89618ms step_avg:40.87ms
step:2194/2330 train_time:89663ms step_avg:40.87ms
step:2195/2330 train_time:89699ms step_avg:40.87ms
step:2196/2330 train_time:89745ms step_avg:40.87ms
step:2197/2330 train_time:89780ms step_avg:40.86ms
step:2198/2330 train_time:89825ms step_avg:40.87ms
step:2199/2330 train_time:89861ms step_avg:40.86ms
step:2200/2330 train_time:89907ms step_avg:40.87ms
step:2201/2330 train_time:89943ms step_avg:40.86ms
step:2202/2330 train_time:89990ms step_avg:40.87ms
step:2203/2330 train_time:90025ms step_avg:40.86ms
step:2204/2330 train_time:90071ms step_avg:40.87ms
step:2205/2330 train_time:90106ms step_avg:40.86ms
step:2206/2330 train_time:90151ms step_avg:40.87ms
step:2207/2330 train_time:90187ms step_avg:40.86ms
step:2208/2330 train_time:90232ms step_avg:40.87ms
step:2209/2330 train_time:90269ms step_avg:40.86ms
step:2210/2330 train_time:90314ms step_avg:40.87ms
step:2211/2330 train_time:90350ms step_avg:40.86ms
step:2212/2330 train_time:90396ms step_avg:40.87ms
step:2213/2330 train_time:90431ms step_avg:40.86ms
step:2214/2330 train_time:90477ms step_avg:40.87ms
step:2215/2330 train_time:90512ms step_avg:40.86ms
step:2216/2330 train_time:90558ms step_avg:40.87ms
step:2217/2330 train_time:90594ms step_avg:40.86ms
step:2218/2330 train_time:90641ms step_avg:40.87ms
step:2219/2330 train_time:90677ms step_avg:40.86ms
step:2220/2330 train_time:90722ms step_avg:40.87ms
step:2221/2330 train_time:90758ms step_avg:40.86ms
step:2222/2330 train_time:90804ms step_avg:40.87ms
step:2223/2330 train_time:90840ms step_avg:40.86ms
step:2224/2330 train_time:90885ms step_avg:40.87ms
step:2225/2330 train_time:90920ms step_avg:40.86ms
step:2226/2330 train_time:90967ms step_avg:40.87ms
step:2227/2330 train_time:91003ms step_avg:40.86ms
step:2228/2330 train_time:91049ms step_avg:40.87ms
step:2229/2330 train_time:91084ms step_avg:40.86ms
step:2230/2330 train_time:91129ms step_avg:40.87ms
step:2231/2330 train_time:91165ms step_avg:40.86ms
step:2232/2330 train_time:91210ms step_avg:40.86ms
step:2233/2330 train_time:91246ms step_avg:40.86ms
step:2234/2330 train_time:91292ms step_avg:40.86ms
step:2235/2330 train_time:91327ms step_avg:40.86ms
step:2236/2330 train_time:91373ms step_avg:40.86ms
step:2237/2330 train_time:91409ms step_avg:40.86ms
step:2238/2330 train_time:91455ms step_avg:40.86ms
step:2239/2330 train_time:91491ms step_avg:40.86ms
step:2240/2330 train_time:91537ms step_avg:40.86ms
step:2241/2330 train_time:91573ms step_avg:40.86ms
step:2242/2330 train_time:91619ms step_avg:40.86ms
step:2243/2330 train_time:91654ms step_avg:40.86ms
step:2244/2330 train_time:91699ms step_avg:40.86ms
step:2245/2330 train_time:91736ms step_avg:40.86ms
step:2246/2330 train_time:91781ms step_avg:40.86ms
step:2247/2330 train_time:91817ms step_avg:40.86ms
step:2248/2330 train_time:91862ms step_avg:40.86ms
step:2249/2330 train_time:91898ms step_avg:40.86ms
step:2250/2330 train_time:91944ms step_avg:40.86ms
step:2250/2330 val_loss:5.2000 train_time:92032ms step_avg:40.90ms
step:2251/2330 train_time:92046ms step_avg:40.89ms
step:2252/2330 train_time:92058ms step_avg:40.88ms
step:2253/2330 train_time:92069ms step_avg:40.87ms
step:2254/2330 train_time:92107ms step_avg:40.86ms
step:2255/2330 train_time:92141ms step_avg:40.86ms
step:2256/2330 train_time:92186ms step_avg:40.86ms
step:2257/2330 train_time:92221ms step_avg:40.86ms
step:2258/2330 train_time:92266ms step_avg:40.86ms
step:2259/2330 train_time:92300ms step_avg:40.86ms
step:2260/2330 train_time:92350ms step_avg:40.86ms
step:2261/2330 train_time:92390ms step_avg:40.86ms
step:2262/2330 train_time:92437ms step_avg:40.87ms
step:2263/2330 train_time:92472ms step_avg:40.86ms
step:2264/2330 train_time:92518ms step_avg:40.86ms
step:2265/2330 train_time:92554ms step_avg:40.86ms
step:2266/2330 train_time:92601ms step_avg:40.87ms
step:2267/2330 train_time:92638ms step_avg:40.86ms
step:2268/2330 train_time:92682ms step_avg:40.87ms
step:2269/2330 train_time:92718ms step_avg:40.86ms
step:2270/2330 train_time:92762ms step_avg:40.86ms
step:2271/2330 train_time:92797ms step_avg:40.86ms
step:2272/2330 train_time:92842ms step_avg:40.86ms
step:2273/2330 train_time:92877ms step_avg:40.86ms
step:2274/2330 train_time:92922ms step_avg:40.86ms
step:2275/2330 train_time:92959ms step_avg:40.86ms
step:2276/2330 train_time:93006ms step_avg:40.86ms
step:2277/2330 train_time:93041ms step_avg:40.86ms
step:2278/2330 train_time:93085ms step_avg:40.86ms
step:2279/2330 train_time:93121ms step_avg:40.86ms
step:2280/2330 train_time:93165ms step_avg:40.86ms
step:2281/2330 train_time:93200ms step_avg:40.86ms
step:2282/2330 train_time:93246ms step_avg:40.86ms
step:2283/2330 train_time:93283ms step_avg:40.86ms
step:2284/2330 train_time:93330ms step_avg:40.86ms
step:2285/2330 train_time:93367ms step_avg:40.86ms
step:2286/2330 train_time:93413ms step_avg:40.86ms
step:2287/2330 train_time:93450ms step_avg:40.86ms
step:2288/2330 train_time:93496ms step_avg:40.86ms
step:2289/2330 train_time:93532ms step_avg:40.86ms
step:2290/2330 train_time:93578ms step_avg:40.86ms
step:2291/2330 train_time:93614ms step_avg:40.86ms
step:2292/2330 train_time:93659ms step_avg:40.86ms
step:2293/2330 train_time:93695ms step_avg:40.86ms
step:2294/2330 train_time:93740ms step_avg:40.86ms
step:2295/2330 train_time:93775ms step_avg:40.86ms
step:2296/2330 train_time:93820ms step_avg:40.86ms
step:2297/2330 train_time:93856ms step_avg:40.86ms
step:2298/2330 train_time:93902ms step_avg:40.86ms
step:2299/2330 train_time:93938ms step_avg:40.86ms
step:2300/2330 train_time:93983ms step_avg:40.86ms
step:2301/2330 train_time:94019ms step_avg:40.86ms
step:2302/2330 train_time:94063ms step_avg:40.86ms
step:2303/2330 train_time:94099ms step_avg:40.86ms
step:2304/2330 train_time:94145ms step_avg:40.86ms
step:2305/2330 train_time:94180ms step_avg:40.86ms
step:2306/2330 train_time:94225ms step_avg:40.86ms
step:2307/2330 train_time:94262ms step_avg:40.86ms
step:2308/2330 train_time:94309ms step_avg:40.86ms
step:2309/2330 train_time:94346ms step_avg:40.86ms
step:2310/2330 train_time:94393ms step_avg:40.86ms
step:2311/2330 train_time:94428ms step_avg:40.86ms
step:2312/2330 train_time:94473ms step_avg:40.86ms
step:2313/2330 train_time:94509ms step_avg:40.86ms
step:2314/2330 train_time:94555ms step_avg:40.86ms
step:2315/2330 train_time:94590ms step_avg:40.86ms
step:2316/2330 train_time:94636ms step_avg:40.86ms
step:2317/2330 train_time:94672ms step_avg:40.86ms
step:2318/2330 train_time:94718ms step_avg:40.86ms
step:2319/2330 train_time:94753ms step_avg:40.86ms
step:2320/2330 train_time:94799ms step_avg:40.86ms
step:2321/2330 train_time:94836ms step_avg:40.86ms
step:2322/2330 train_time:94881ms step_avg:40.86ms
step:2323/2330 train_time:94916ms step_avg:40.86ms
step:2324/2330 train_time:94962ms step_avg:40.86ms
step:2325/2330 train_time:94998ms step_avg:40.86ms
step:2326/2330 train_time:95043ms step_avg:40.86ms
step:2327/2330 train_time:95079ms step_avg:40.86ms
step:2328/2330 train_time:95124ms step_avg:40.86ms
step:2329/2330 train_time:95160ms step_avg:40.86ms
step:2330/2330 train_time:95205ms step_avg:40.86ms
step:2330/2330 val_loss:5.1917 train_time:95293ms step_avg:40.90ms
peak memory allocated: 29226 MiB reserved: 38888 MiB
