import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = divide_by_norm(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = divide_by_norm(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_gd_lr7e-2"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=7e-2, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 05:32:03 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   33C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   31C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   31C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             115W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:64ms step_avg:64.00ms
step:2/2330 train_time:153ms step_avg:76.47ms
step:3/2330 train_time:169ms step_avg:56.17ms
step:4/2330 train_time:180ms step_avg:44.98ms
step:5/2330 train_time:190ms step_avg:37.98ms
step:6/2330 train_time:251ms step_avg:41.82ms
step:7/2330 train_time:272ms step_avg:38.86ms
step:8/2330 train_time:326ms step_avg:40.81ms
step:9/2330 train_time:348ms step_avg:38.69ms
step:10/2330 train_time:403ms step_avg:40.32ms
step:11/2330 train_time:425ms step_avg:38.63ms
step:12/2330 train_time:481ms step_avg:40.04ms
step:13/2330 train_time:502ms step_avg:38.65ms
step:14/2330 train_time:558ms step_avg:39.87ms
step:15/2330 train_time:580ms step_avg:38.65ms
step:16/2330 train_time:635ms step_avg:39.71ms
step:17/2330 train_time:657ms step_avg:38.66ms
step:18/2330 train_time:712ms step_avg:39.58ms
step:19/2330 train_time:735ms step_avg:38.69ms
step:20/2330 train_time:791ms step_avg:39.53ms
step:21/2330 train_time:813ms step_avg:38.71ms
step:22/2330 train_time:868ms step_avg:39.47ms
step:23/2330 train_time:891ms step_avg:38.73ms
step:24/2330 train_time:946ms step_avg:39.44ms
step:25/2330 train_time:968ms step_avg:38.73ms
step:26/2330 train_time:1024ms step_avg:39.37ms
step:27/2330 train_time:1046ms step_avg:38.73ms
step:28/2330 train_time:1105ms step_avg:39.46ms
step:29/2330 train_time:1129ms step_avg:38.94ms
step:30/2330 train_time:1190ms step_avg:39.68ms
step:31/2330 train_time:1214ms step_avg:39.15ms
step:32/2330 train_time:1273ms step_avg:39.77ms
step:33/2330 train_time:1297ms step_avg:39.29ms
step:34/2330 train_time:1354ms step_avg:39.81ms
step:35/2330 train_time:1376ms step_avg:39.33ms
step:36/2330 train_time:1432ms step_avg:39.78ms
step:37/2330 train_time:1455ms step_avg:39.32ms
step:38/2330 train_time:1510ms step_avg:39.74ms
step:39/2330 train_time:1533ms step_avg:39.31ms
step:40/2330 train_time:1589ms step_avg:39.73ms
step:41/2330 train_time:1611ms step_avg:39.30ms
step:42/2330 train_time:1666ms step_avg:39.67ms
step:43/2330 train_time:1688ms step_avg:39.26ms
step:44/2330 train_time:1744ms step_avg:39.64ms
step:45/2330 train_time:1766ms step_avg:39.24ms
step:46/2330 train_time:1821ms step_avg:39.59ms
step:47/2330 train_time:1843ms step_avg:39.21ms
step:48/2330 train_time:1898ms step_avg:39.55ms
step:49/2330 train_time:1920ms step_avg:39.18ms
step:50/2330 train_time:1975ms step_avg:39.51ms
step:51/2330 train_time:1998ms step_avg:39.18ms
step:52/2330 train_time:2055ms step_avg:39.51ms
step:53/2330 train_time:2079ms step_avg:39.23ms
step:54/2330 train_time:2137ms step_avg:39.57ms
step:55/2330 train_time:2161ms step_avg:39.30ms
step:56/2330 train_time:2221ms step_avg:39.66ms
step:57/2330 train_time:2244ms step_avg:39.36ms
step:58/2330 train_time:2302ms step_avg:39.68ms
step:59/2330 train_time:2324ms step_avg:39.39ms
step:60/2330 train_time:2381ms step_avg:39.68ms
step:61/2330 train_time:2404ms step_avg:39.41ms
step:62/2330 train_time:2460ms step_avg:39.68ms
step:63/2330 train_time:2482ms step_avg:39.40ms
step:64/2330 train_time:2538ms step_avg:39.65ms
step:65/2330 train_time:2560ms step_avg:39.38ms
step:66/2330 train_time:2616ms step_avg:39.63ms
step:67/2330 train_time:2638ms step_avg:39.38ms
step:68/2330 train_time:2694ms step_avg:39.62ms
step:69/2330 train_time:2717ms step_avg:39.37ms
step:70/2330 train_time:2772ms step_avg:39.60ms
step:71/2330 train_time:2794ms step_avg:39.36ms
step:72/2330 train_time:2850ms step_avg:39.58ms
step:73/2330 train_time:2873ms step_avg:39.35ms
step:74/2330 train_time:2928ms step_avg:39.57ms
step:75/2330 train_time:2951ms step_avg:39.35ms
step:76/2330 train_time:3008ms step_avg:39.58ms
step:77/2330 train_time:3031ms step_avg:39.36ms
step:78/2330 train_time:3088ms step_avg:39.59ms
step:79/2330 train_time:3112ms step_avg:39.39ms
step:80/2330 train_time:3169ms step_avg:39.61ms
step:81/2330 train_time:3193ms step_avg:39.42ms
step:82/2330 train_time:3251ms step_avg:39.64ms
step:83/2330 train_time:3273ms step_avg:39.44ms
step:84/2330 train_time:3331ms step_avg:39.65ms
step:85/2330 train_time:3354ms step_avg:39.46ms
step:86/2330 train_time:3410ms step_avg:39.66ms
step:87/2330 train_time:3434ms step_avg:39.47ms
step:88/2330 train_time:3490ms step_avg:39.66ms
step:89/2330 train_time:3513ms step_avg:39.47ms
step:90/2330 train_time:3569ms step_avg:39.66ms
step:91/2330 train_time:3592ms step_avg:39.47ms
step:92/2330 train_time:3648ms step_avg:39.66ms
step:93/2330 train_time:3670ms step_avg:39.47ms
step:94/2330 train_time:3726ms step_avg:39.64ms
step:95/2330 train_time:3748ms step_avg:39.45ms
step:96/2330 train_time:3804ms step_avg:39.62ms
step:97/2330 train_time:3825ms step_avg:39.44ms
step:98/2330 train_time:3882ms step_avg:39.62ms
step:99/2330 train_time:3905ms step_avg:39.44ms
step:100/2330 train_time:3961ms step_avg:39.61ms
step:101/2330 train_time:3983ms step_avg:39.44ms
step:102/2330 train_time:4039ms step_avg:39.60ms
step:103/2330 train_time:4062ms step_avg:39.44ms
step:104/2330 train_time:4120ms step_avg:39.61ms
step:105/2330 train_time:4143ms step_avg:39.45ms
step:106/2330 train_time:4200ms step_avg:39.63ms
step:107/2330 train_time:4223ms step_avg:39.46ms
step:108/2330 train_time:4280ms step_avg:39.63ms
step:109/2330 train_time:4302ms step_avg:39.47ms
step:110/2330 train_time:4359ms step_avg:39.62ms
step:111/2330 train_time:4381ms step_avg:39.47ms
step:112/2330 train_time:4439ms step_avg:39.63ms
step:113/2330 train_time:4461ms step_avg:39.48ms
step:114/2330 train_time:4517ms step_avg:39.63ms
step:115/2330 train_time:4540ms step_avg:39.48ms
step:116/2330 train_time:4596ms step_avg:39.62ms
step:117/2330 train_time:4619ms step_avg:39.47ms
step:118/2330 train_time:4675ms step_avg:39.62ms
step:119/2330 train_time:4698ms step_avg:39.47ms
step:120/2330 train_time:4753ms step_avg:39.61ms
step:121/2330 train_time:4777ms step_avg:39.48ms
step:122/2330 train_time:4833ms step_avg:39.61ms
step:123/2330 train_time:4856ms step_avg:39.48ms
step:124/2330 train_time:4912ms step_avg:39.61ms
step:125/2330 train_time:4935ms step_avg:39.48ms
step:126/2330 train_time:4991ms step_avg:39.61ms
step:127/2330 train_time:5014ms step_avg:39.48ms
step:128/2330 train_time:5070ms step_avg:39.61ms
step:129/2330 train_time:5094ms step_avg:39.48ms
step:130/2330 train_time:5151ms step_avg:39.62ms
step:131/2330 train_time:5174ms step_avg:39.49ms
step:132/2330 train_time:5230ms step_avg:39.62ms
step:133/2330 train_time:5254ms step_avg:39.50ms
step:134/2330 train_time:5310ms step_avg:39.63ms
step:135/2330 train_time:5333ms step_avg:39.51ms
step:136/2330 train_time:5391ms step_avg:39.64ms
step:137/2330 train_time:5414ms step_avg:39.52ms
step:138/2330 train_time:5470ms step_avg:39.64ms
step:139/2330 train_time:5494ms step_avg:39.52ms
step:140/2330 train_time:5550ms step_avg:39.65ms
step:141/2330 train_time:5573ms step_avg:39.52ms
step:142/2330 train_time:5630ms step_avg:39.65ms
step:143/2330 train_time:5653ms step_avg:39.53ms
step:144/2330 train_time:5709ms step_avg:39.65ms
step:145/2330 train_time:5732ms step_avg:39.53ms
step:146/2330 train_time:5788ms step_avg:39.65ms
step:147/2330 train_time:5811ms step_avg:39.53ms
step:148/2330 train_time:5866ms step_avg:39.64ms
step:149/2330 train_time:5889ms step_avg:39.52ms
step:150/2330 train_time:5945ms step_avg:39.64ms
step:151/2330 train_time:5967ms step_avg:39.52ms
step:152/2330 train_time:6024ms step_avg:39.63ms
step:153/2330 train_time:6046ms step_avg:39.51ms
step:154/2330 train_time:6102ms step_avg:39.63ms
step:155/2330 train_time:6124ms step_avg:39.51ms
step:156/2330 train_time:6181ms step_avg:39.62ms
step:157/2330 train_time:6203ms step_avg:39.51ms
step:158/2330 train_time:6261ms step_avg:39.62ms
step:159/2330 train_time:6283ms step_avg:39.52ms
step:160/2330 train_time:6340ms step_avg:39.62ms
step:161/2330 train_time:6362ms step_avg:39.51ms
step:162/2330 train_time:6419ms step_avg:39.62ms
step:163/2330 train_time:6441ms step_avg:39.52ms
step:164/2330 train_time:6498ms step_avg:39.62ms
step:165/2330 train_time:6520ms step_avg:39.52ms
step:166/2330 train_time:6577ms step_avg:39.62ms
step:167/2330 train_time:6600ms step_avg:39.52ms
step:168/2330 train_time:6656ms step_avg:39.62ms
step:169/2330 train_time:6679ms step_avg:39.52ms
step:170/2330 train_time:6735ms step_avg:39.62ms
step:171/2330 train_time:6759ms step_avg:39.52ms
step:172/2330 train_time:6815ms step_avg:39.62ms
step:173/2330 train_time:6838ms step_avg:39.52ms
step:174/2330 train_time:6894ms step_avg:39.62ms
step:175/2330 train_time:6917ms step_avg:39.53ms
step:176/2330 train_time:6973ms step_avg:39.62ms
step:177/2330 train_time:6996ms step_avg:39.52ms
step:178/2330 train_time:7052ms step_avg:39.62ms
step:179/2330 train_time:7075ms step_avg:39.52ms
step:180/2330 train_time:7132ms step_avg:39.62ms
step:181/2330 train_time:7155ms step_avg:39.53ms
step:182/2330 train_time:7211ms step_avg:39.62ms
step:183/2330 train_time:7234ms step_avg:39.53ms
step:184/2330 train_time:7290ms step_avg:39.62ms
step:185/2330 train_time:7313ms step_avg:39.53ms
step:186/2330 train_time:7370ms step_avg:39.62ms
step:187/2330 train_time:7393ms step_avg:39.54ms
step:188/2330 train_time:7450ms step_avg:39.63ms
step:189/2330 train_time:7474ms step_avg:39.54ms
step:190/2330 train_time:7531ms step_avg:39.64ms
step:191/2330 train_time:7553ms step_avg:39.55ms
step:192/2330 train_time:7610ms step_avg:39.63ms
step:193/2330 train_time:7632ms step_avg:39.55ms
step:194/2330 train_time:7689ms step_avg:39.63ms
step:195/2330 train_time:7711ms step_avg:39.55ms
step:196/2330 train_time:7767ms step_avg:39.63ms
step:197/2330 train_time:7790ms step_avg:39.54ms
step:198/2330 train_time:7847ms step_avg:39.63ms
step:199/2330 train_time:7869ms step_avg:39.54ms
step:200/2330 train_time:7925ms step_avg:39.62ms
step:201/2330 train_time:7947ms step_avg:39.54ms
step:202/2330 train_time:8003ms step_avg:39.62ms
step:203/2330 train_time:8025ms step_avg:39.53ms
step:204/2330 train_time:8083ms step_avg:39.62ms
step:205/2330 train_time:8105ms step_avg:39.54ms
step:206/2330 train_time:8162ms step_avg:39.62ms
step:207/2330 train_time:8184ms step_avg:39.54ms
step:208/2330 train_time:8240ms step_avg:39.62ms
step:209/2330 train_time:8262ms step_avg:39.53ms
step:210/2330 train_time:8319ms step_avg:39.61ms
step:211/2330 train_time:8342ms step_avg:39.53ms
step:212/2330 train_time:8399ms step_avg:39.62ms
step:213/2330 train_time:8421ms step_avg:39.54ms
step:214/2330 train_time:8479ms step_avg:39.62ms
step:215/2330 train_time:8501ms step_avg:39.54ms
step:216/2330 train_time:8558ms step_avg:39.62ms
step:217/2330 train_time:8580ms step_avg:39.54ms
step:218/2330 train_time:8637ms step_avg:39.62ms
step:219/2330 train_time:8659ms step_avg:39.54ms
step:220/2330 train_time:8716ms step_avg:39.62ms
step:221/2330 train_time:8739ms step_avg:39.54ms
step:222/2330 train_time:8795ms step_avg:39.62ms
step:223/2330 train_time:8818ms step_avg:39.54ms
step:224/2330 train_time:8873ms step_avg:39.61ms
step:225/2330 train_time:8897ms step_avg:39.54ms
step:226/2330 train_time:8953ms step_avg:39.62ms
step:227/2330 train_time:8976ms step_avg:39.54ms
step:228/2330 train_time:9033ms step_avg:39.62ms
step:229/2330 train_time:9056ms step_avg:39.55ms
step:230/2330 train_time:9112ms step_avg:39.62ms
step:231/2330 train_time:9135ms step_avg:39.55ms
step:232/2330 train_time:9191ms step_avg:39.62ms
step:233/2330 train_time:9214ms step_avg:39.55ms
step:234/2330 train_time:9271ms step_avg:39.62ms
step:235/2330 train_time:9294ms step_avg:39.55ms
step:236/2330 train_time:9351ms step_avg:39.62ms
step:237/2330 train_time:9374ms step_avg:39.55ms
step:238/2330 train_time:9430ms step_avg:39.62ms
step:239/2330 train_time:9453ms step_avg:39.55ms
step:240/2330 train_time:9510ms step_avg:39.62ms
step:241/2330 train_time:9532ms step_avg:39.55ms
step:242/2330 train_time:9589ms step_avg:39.63ms
step:243/2330 train_time:9613ms step_avg:39.56ms
step:244/2330 train_time:9669ms step_avg:39.63ms
step:245/2330 train_time:9691ms step_avg:39.56ms
step:246/2330 train_time:9749ms step_avg:39.63ms
step:247/2330 train_time:9771ms step_avg:39.56ms
step:248/2330 train_time:9828ms step_avg:39.63ms
step:249/2330 train_time:9850ms step_avg:39.56ms
step:250/2330 train_time:9907ms step_avg:39.63ms
step:250/2330 val_loss:5.5161 train_time:10001ms step_avg:40.00ms
step:251/2330 train_time:10014ms step_avg:39.90ms
step:252/2330 train_time:10026ms step_avg:39.79ms
step:253/2330 train_time:10036ms step_avg:39.67ms
step:254/2330 train_time:10064ms step_avg:39.62ms
step:255/2330 train_time:10085ms step_avg:39.55ms
step:256/2330 train_time:10140ms step_avg:39.61ms
step:257/2330 train_time:10163ms step_avg:39.54ms
step:258/2330 train_time:10218ms step_avg:39.60ms
step:259/2330 train_time:10240ms step_avg:39.54ms
step:260/2330 train_time:10295ms step_avg:39.60ms
step:261/2330 train_time:10322ms step_avg:39.55ms
step:262/2330 train_time:10383ms step_avg:39.63ms
step:263/2330 train_time:10408ms step_avg:39.57ms
step:264/2330 train_time:10466ms step_avg:39.64ms
step:265/2330 train_time:10488ms step_avg:39.58ms
step:266/2330 train_time:10545ms step_avg:39.64ms
step:267/2330 train_time:10568ms step_avg:39.58ms
step:268/2330 train_time:10624ms step_avg:39.64ms
step:269/2330 train_time:10646ms step_avg:39.58ms
step:270/2330 train_time:10701ms step_avg:39.63ms
step:271/2330 train_time:10723ms step_avg:39.57ms
step:272/2330 train_time:10778ms step_avg:39.63ms
step:273/2330 train_time:10801ms step_avg:39.56ms
step:274/2330 train_time:10857ms step_avg:39.62ms
step:275/2330 train_time:10879ms step_avg:39.56ms
step:276/2330 train_time:10936ms step_avg:39.62ms
step:277/2330 train_time:10959ms step_avg:39.56ms
step:278/2330 train_time:11016ms step_avg:39.63ms
step:279/2330 train_time:11039ms step_avg:39.57ms
step:280/2330 train_time:11095ms step_avg:39.62ms
step:281/2330 train_time:11117ms step_avg:39.56ms
step:282/2330 train_time:11172ms step_avg:39.62ms
step:283/2330 train_time:11195ms step_avg:39.56ms
step:284/2330 train_time:11254ms step_avg:39.63ms
step:285/2330 train_time:11276ms step_avg:39.56ms
step:286/2330 train_time:11333ms step_avg:39.63ms
step:287/2330 train_time:11357ms step_avg:39.57ms
step:288/2330 train_time:11415ms step_avg:39.64ms
step:289/2330 train_time:11439ms step_avg:39.58ms
step:290/2330 train_time:11496ms step_avg:39.64ms
step:291/2330 train_time:11519ms step_avg:39.59ms
step:292/2330 train_time:11576ms step_avg:39.64ms
step:293/2330 train_time:11599ms step_avg:39.59ms
step:294/2330 train_time:11654ms step_avg:39.64ms
step:295/2330 train_time:11677ms step_avg:39.58ms
step:296/2330 train_time:11732ms step_avg:39.64ms
step:297/2330 train_time:11755ms step_avg:39.58ms
step:298/2330 train_time:11811ms step_avg:39.63ms
step:299/2330 train_time:11833ms step_avg:39.57ms
step:300/2330 train_time:11889ms step_avg:39.63ms
step:301/2330 train_time:11911ms step_avg:39.57ms
step:302/2330 train_time:11968ms step_avg:39.63ms
step:303/2330 train_time:11989ms step_avg:39.57ms
step:304/2330 train_time:12046ms step_avg:39.62ms
step:305/2330 train_time:12068ms step_avg:39.57ms
step:306/2330 train_time:12125ms step_avg:39.62ms
step:307/2330 train_time:12146ms step_avg:39.56ms
step:308/2330 train_time:12203ms step_avg:39.62ms
step:309/2330 train_time:12225ms step_avg:39.56ms
step:310/2330 train_time:12282ms step_avg:39.62ms
step:311/2330 train_time:12305ms step_avg:39.56ms
step:312/2330 train_time:12361ms step_avg:39.62ms
step:313/2330 train_time:12384ms step_avg:39.57ms
step:314/2330 train_time:12441ms step_avg:39.62ms
step:315/2330 train_time:12465ms step_avg:39.57ms
step:316/2330 train_time:12521ms step_avg:39.62ms
step:317/2330 train_time:12545ms step_avg:39.57ms
step:318/2330 train_time:12600ms step_avg:39.62ms
step:319/2330 train_time:12624ms step_avg:39.57ms
step:320/2330 train_time:12680ms step_avg:39.62ms
step:321/2330 train_time:12703ms step_avg:39.57ms
step:322/2330 train_time:12759ms step_avg:39.63ms
step:323/2330 train_time:12782ms step_avg:39.57ms
step:324/2330 train_time:12838ms step_avg:39.62ms
step:325/2330 train_time:12861ms step_avg:39.57ms
step:326/2330 train_time:12917ms step_avg:39.62ms
step:327/2330 train_time:12941ms step_avg:39.57ms
step:328/2330 train_time:12997ms step_avg:39.63ms
step:329/2330 train_time:13020ms step_avg:39.57ms
step:330/2330 train_time:13076ms step_avg:39.63ms
step:331/2330 train_time:13099ms step_avg:39.57ms
step:332/2330 train_time:13155ms step_avg:39.62ms
step:333/2330 train_time:13178ms step_avg:39.57ms
step:334/2330 train_time:13235ms step_avg:39.63ms
step:335/2330 train_time:13258ms step_avg:39.58ms
step:336/2330 train_time:13315ms step_avg:39.63ms
step:337/2330 train_time:13339ms step_avg:39.58ms
step:338/2330 train_time:13397ms step_avg:39.63ms
step:339/2330 train_time:13420ms step_avg:39.59ms
step:340/2330 train_time:13476ms step_avg:39.64ms
step:341/2330 train_time:13499ms step_avg:39.59ms
step:342/2330 train_time:13556ms step_avg:39.64ms
step:343/2330 train_time:13579ms step_avg:39.59ms
step:344/2330 train_time:13636ms step_avg:39.64ms
step:345/2330 train_time:13658ms step_avg:39.59ms
step:346/2330 train_time:13714ms step_avg:39.64ms
step:347/2330 train_time:13737ms step_avg:39.59ms
step:348/2330 train_time:13793ms step_avg:39.64ms
step:349/2330 train_time:13816ms step_avg:39.59ms
step:350/2330 train_time:13871ms step_avg:39.63ms
step:351/2330 train_time:13894ms step_avg:39.59ms
step:352/2330 train_time:13951ms step_avg:39.63ms
step:353/2330 train_time:13972ms step_avg:39.58ms
step:354/2330 train_time:14029ms step_avg:39.63ms
step:355/2330 train_time:14051ms step_avg:39.58ms
step:356/2330 train_time:14107ms step_avg:39.63ms
step:357/2330 train_time:14129ms step_avg:39.58ms
step:358/2330 train_time:14186ms step_avg:39.63ms
step:359/2330 train_time:14209ms step_avg:39.58ms
step:360/2330 train_time:14266ms step_avg:39.63ms
step:361/2330 train_time:14288ms step_avg:39.58ms
step:362/2330 train_time:14345ms step_avg:39.63ms
step:363/2330 train_time:14368ms step_avg:39.58ms
step:364/2330 train_time:14425ms step_avg:39.63ms
step:365/2330 train_time:14447ms step_avg:39.58ms
step:366/2330 train_time:14504ms step_avg:39.63ms
step:367/2330 train_time:14526ms step_avg:39.58ms
step:368/2330 train_time:14582ms step_avg:39.63ms
step:369/2330 train_time:14606ms step_avg:39.58ms
step:370/2330 train_time:14662ms step_avg:39.63ms
step:371/2330 train_time:14685ms step_avg:39.58ms
step:372/2330 train_time:14741ms step_avg:39.63ms
step:373/2330 train_time:14764ms step_avg:39.58ms
step:374/2330 train_time:14820ms step_avg:39.63ms
step:375/2330 train_time:14843ms step_avg:39.58ms
step:376/2330 train_time:14899ms step_avg:39.62ms
step:377/2330 train_time:14922ms step_avg:39.58ms
step:378/2330 train_time:14978ms step_avg:39.62ms
step:379/2330 train_time:15001ms step_avg:39.58ms
step:380/2330 train_time:15058ms step_avg:39.63ms
step:381/2330 train_time:15080ms step_avg:39.58ms
step:382/2330 train_time:15137ms step_avg:39.63ms
step:383/2330 train_time:15160ms step_avg:39.58ms
step:384/2330 train_time:15217ms step_avg:39.63ms
step:385/2330 train_time:15240ms step_avg:39.58ms
step:386/2330 train_time:15296ms step_avg:39.63ms
step:387/2330 train_time:15318ms step_avg:39.58ms
step:388/2330 train_time:15375ms step_avg:39.63ms
step:389/2330 train_time:15399ms step_avg:39.59ms
step:390/2330 train_time:15456ms step_avg:39.63ms
step:391/2330 train_time:15478ms step_avg:39.59ms
step:392/2330 train_time:15536ms step_avg:39.63ms
step:393/2330 train_time:15558ms step_avg:39.59ms
step:394/2330 train_time:15615ms step_avg:39.63ms
step:395/2330 train_time:15638ms step_avg:39.59ms
step:396/2330 train_time:15695ms step_avg:39.63ms
step:397/2330 train_time:15717ms step_avg:39.59ms
step:398/2330 train_time:15773ms step_avg:39.63ms
step:399/2330 train_time:15796ms step_avg:39.59ms
step:400/2330 train_time:15852ms step_avg:39.63ms
step:401/2330 train_time:15874ms step_avg:39.59ms
step:402/2330 train_time:15931ms step_avg:39.63ms
step:403/2330 train_time:15953ms step_avg:39.58ms
step:404/2330 train_time:16010ms step_avg:39.63ms
step:405/2330 train_time:16032ms step_avg:39.58ms
step:406/2330 train_time:16088ms step_avg:39.63ms
step:407/2330 train_time:16110ms step_avg:39.58ms
step:408/2330 train_time:16167ms step_avg:39.63ms
step:409/2330 train_time:16189ms step_avg:39.58ms
step:410/2330 train_time:16246ms step_avg:39.62ms
step:411/2330 train_time:16268ms step_avg:39.58ms
step:412/2330 train_time:16325ms step_avg:39.62ms
step:413/2330 train_time:16348ms step_avg:39.58ms
step:414/2330 train_time:16405ms step_avg:39.63ms
step:415/2330 train_time:16428ms step_avg:39.59ms
step:416/2330 train_time:16485ms step_avg:39.63ms
step:417/2330 train_time:16508ms step_avg:39.59ms
step:418/2330 train_time:16565ms step_avg:39.63ms
step:419/2330 train_time:16588ms step_avg:39.59ms
step:420/2330 train_time:16645ms step_avg:39.63ms
step:421/2330 train_time:16667ms step_avg:39.59ms
step:422/2330 train_time:16725ms step_avg:39.63ms
step:423/2330 train_time:16748ms step_avg:39.59ms
step:424/2330 train_time:16805ms step_avg:39.63ms
step:425/2330 train_time:16827ms step_avg:39.59ms
step:426/2330 train_time:16884ms step_avg:39.63ms
step:427/2330 train_time:16907ms step_avg:39.59ms
step:428/2330 train_time:16963ms step_avg:39.63ms
step:429/2330 train_time:16985ms step_avg:39.59ms
step:430/2330 train_time:17041ms step_avg:39.63ms
step:431/2330 train_time:17064ms step_avg:39.59ms
step:432/2330 train_time:17119ms step_avg:39.63ms
step:433/2330 train_time:17142ms step_avg:39.59ms
step:434/2330 train_time:17198ms step_avg:39.63ms
step:435/2330 train_time:17221ms step_avg:39.59ms
step:436/2330 train_time:17277ms step_avg:39.63ms
step:437/2330 train_time:17301ms step_avg:39.59ms
step:438/2330 train_time:17359ms step_avg:39.63ms
step:439/2330 train_time:17382ms step_avg:39.59ms
step:440/2330 train_time:17438ms step_avg:39.63ms
step:441/2330 train_time:17461ms step_avg:39.59ms
step:442/2330 train_time:17518ms step_avg:39.63ms
step:443/2330 train_time:17541ms step_avg:39.60ms
step:444/2330 train_time:17597ms step_avg:39.63ms
step:445/2330 train_time:17620ms step_avg:39.60ms
step:446/2330 train_time:17677ms step_avg:39.63ms
step:447/2330 train_time:17700ms step_avg:39.60ms
step:448/2330 train_time:17758ms step_avg:39.64ms
step:449/2330 train_time:17781ms step_avg:39.60ms
step:450/2330 train_time:17838ms step_avg:39.64ms
step:451/2330 train_time:17861ms step_avg:39.60ms
step:452/2330 train_time:17917ms step_avg:39.64ms
step:453/2330 train_time:17941ms step_avg:39.60ms
step:454/2330 train_time:17997ms step_avg:39.64ms
step:455/2330 train_time:18020ms step_avg:39.60ms
step:456/2330 train_time:18076ms step_avg:39.64ms
step:457/2330 train_time:18099ms step_avg:39.60ms
step:458/2330 train_time:18155ms step_avg:39.64ms
step:459/2330 train_time:18178ms step_avg:39.60ms
step:460/2330 train_time:18234ms step_avg:39.64ms
step:461/2330 train_time:18257ms step_avg:39.60ms
step:462/2330 train_time:18315ms step_avg:39.64ms
step:463/2330 train_time:18337ms step_avg:39.61ms
step:464/2330 train_time:18394ms step_avg:39.64ms
step:465/2330 train_time:18418ms step_avg:39.61ms
step:466/2330 train_time:18474ms step_avg:39.64ms
step:467/2330 train_time:18497ms step_avg:39.61ms
step:468/2330 train_time:18553ms step_avg:39.64ms
step:469/2330 train_time:18575ms step_avg:39.61ms
step:470/2330 train_time:18631ms step_avg:39.64ms
step:471/2330 train_time:18653ms step_avg:39.60ms
step:472/2330 train_time:18710ms step_avg:39.64ms
step:473/2330 train_time:18732ms step_avg:39.60ms
step:474/2330 train_time:18789ms step_avg:39.64ms
step:475/2330 train_time:18811ms step_avg:39.60ms
step:476/2330 train_time:18868ms step_avg:39.64ms
step:477/2330 train_time:18890ms step_avg:39.60ms
step:478/2330 train_time:18947ms step_avg:39.64ms
step:479/2330 train_time:18969ms step_avg:39.60ms
step:480/2330 train_time:19026ms step_avg:39.64ms
step:481/2330 train_time:19048ms step_avg:39.60ms
step:482/2330 train_time:19104ms step_avg:39.64ms
step:483/2330 train_time:19127ms step_avg:39.60ms
step:484/2330 train_time:19184ms step_avg:39.64ms
step:485/2330 train_time:19206ms step_avg:39.60ms
step:486/2330 train_time:19262ms step_avg:39.63ms
step:487/2330 train_time:19285ms step_avg:39.60ms
step:488/2330 train_time:19342ms step_avg:39.63ms
step:489/2330 train_time:19365ms step_avg:39.60ms
step:490/2330 train_time:19421ms step_avg:39.63ms
step:491/2330 train_time:19444ms step_avg:39.60ms
step:492/2330 train_time:19500ms step_avg:39.64ms
step:493/2330 train_time:19523ms step_avg:39.60ms
step:494/2330 train_time:19579ms step_avg:39.63ms
step:495/2330 train_time:19603ms step_avg:39.60ms
step:496/2330 train_time:19659ms step_avg:39.64ms
step:497/2330 train_time:19682ms step_avg:39.60ms
step:498/2330 train_time:19738ms step_avg:39.64ms
step:499/2330 train_time:19761ms step_avg:39.60ms
step:500/2330 train_time:19818ms step_avg:39.64ms
step:500/2330 val_loss:5.3634 train_time:19916ms step_avg:39.83ms
step:501/2330 train_time:19928ms step_avg:39.78ms
step:502/2330 train_time:19940ms step_avg:39.72ms
step:503/2330 train_time:19949ms step_avg:39.66ms
step:504/2330 train_time:19979ms step_avg:39.64ms
step:505/2330 train_time:20000ms step_avg:39.60ms
step:506/2330 train_time:20055ms step_avg:39.63ms
step:507/2330 train_time:20077ms step_avg:39.60ms
step:508/2330 train_time:20132ms step_avg:39.63ms
step:509/2330 train_time:20154ms step_avg:39.60ms
step:510/2330 train_time:20210ms step_avg:39.63ms
step:511/2330 train_time:20235ms step_avg:39.60ms
step:512/2330 train_time:20297ms step_avg:39.64ms
step:513/2330 train_time:20321ms step_avg:39.61ms
step:514/2330 train_time:20379ms step_avg:39.65ms
step:515/2330 train_time:20401ms step_avg:39.61ms
step:516/2330 train_time:20457ms step_avg:39.65ms
step:517/2330 train_time:20479ms step_avg:39.61ms
step:518/2330 train_time:20536ms step_avg:39.64ms
step:519/2330 train_time:20558ms step_avg:39.61ms
step:520/2330 train_time:20614ms step_avg:39.64ms
step:521/2330 train_time:20636ms step_avg:39.61ms
step:522/2330 train_time:20692ms step_avg:39.64ms
step:523/2330 train_time:20714ms step_avg:39.61ms
step:524/2330 train_time:20769ms step_avg:39.64ms
step:525/2330 train_time:20792ms step_avg:39.60ms
step:526/2330 train_time:20848ms step_avg:39.64ms
step:527/2330 train_time:20872ms step_avg:39.61ms
step:528/2330 train_time:20928ms step_avg:39.64ms
step:529/2330 train_time:20952ms step_avg:39.61ms
step:530/2330 train_time:21007ms step_avg:39.64ms
step:531/2330 train_time:21030ms step_avg:39.61ms
step:532/2330 train_time:21086ms step_avg:39.64ms
step:533/2330 train_time:21109ms step_avg:39.60ms
step:534/2330 train_time:21166ms step_avg:39.64ms
step:535/2330 train_time:21189ms step_avg:39.61ms
step:536/2330 train_time:21248ms step_avg:39.64ms
step:537/2330 train_time:21273ms step_avg:39.61ms
step:538/2330 train_time:21330ms step_avg:39.65ms
step:539/2330 train_time:21354ms step_avg:39.62ms
step:540/2330 train_time:21410ms step_avg:39.65ms
step:541/2330 train_time:21433ms step_avg:39.62ms
step:542/2330 train_time:21490ms step_avg:39.65ms
step:543/2330 train_time:21513ms step_avg:39.62ms
step:544/2330 train_time:21569ms step_avg:39.65ms
step:545/2330 train_time:21591ms step_avg:39.62ms
step:546/2330 train_time:21647ms step_avg:39.65ms
step:547/2330 train_time:21670ms step_avg:39.62ms
step:548/2330 train_time:21726ms step_avg:39.65ms
step:549/2330 train_time:21749ms step_avg:39.62ms
step:550/2330 train_time:21805ms step_avg:39.65ms
step:551/2330 train_time:21828ms step_avg:39.61ms
step:552/2330 train_time:21884ms step_avg:39.64ms
step:553/2330 train_time:21907ms step_avg:39.61ms
step:554/2330 train_time:21963ms step_avg:39.64ms
step:555/2330 train_time:21986ms step_avg:39.61ms
step:556/2330 train_time:22041ms step_avg:39.64ms
step:557/2330 train_time:22064ms step_avg:39.61ms
step:558/2330 train_time:22121ms step_avg:39.64ms
step:559/2330 train_time:22144ms step_avg:39.61ms
step:560/2330 train_time:22200ms step_avg:39.64ms
step:561/2330 train_time:22223ms step_avg:39.61ms
step:562/2330 train_time:22279ms step_avg:39.64ms
step:563/2330 train_time:22302ms step_avg:39.61ms
step:564/2330 train_time:22358ms step_avg:39.64ms
step:565/2330 train_time:22381ms step_avg:39.61ms
step:566/2330 train_time:22438ms step_avg:39.64ms
step:567/2330 train_time:22460ms step_avg:39.61ms
step:568/2330 train_time:22517ms step_avg:39.64ms
step:569/2330 train_time:22539ms step_avg:39.61ms
step:570/2330 train_time:22595ms step_avg:39.64ms
step:571/2330 train_time:22618ms step_avg:39.61ms
step:572/2330 train_time:22674ms step_avg:39.64ms
step:573/2330 train_time:22697ms step_avg:39.61ms
step:574/2330 train_time:22753ms step_avg:39.64ms
step:575/2330 train_time:22774ms step_avg:39.61ms
step:576/2330 train_time:22830ms step_avg:39.64ms
step:577/2330 train_time:22853ms step_avg:39.61ms
step:578/2330 train_time:22909ms step_avg:39.64ms
step:579/2330 train_time:22933ms step_avg:39.61ms
step:580/2330 train_time:22989ms step_avg:39.64ms
step:581/2330 train_time:23011ms step_avg:39.61ms
step:582/2330 train_time:23067ms step_avg:39.63ms
step:583/2330 train_time:23090ms step_avg:39.61ms
step:584/2330 train_time:23147ms step_avg:39.64ms
step:585/2330 train_time:23171ms step_avg:39.61ms
step:586/2330 train_time:23228ms step_avg:39.64ms
step:587/2330 train_time:23252ms step_avg:39.61ms
step:588/2330 train_time:23309ms step_avg:39.64ms
step:589/2330 train_time:23332ms step_avg:39.61ms
step:590/2330 train_time:23389ms step_avg:39.64ms
step:591/2330 train_time:23412ms step_avg:39.61ms
step:592/2330 train_time:23468ms step_avg:39.64ms
step:593/2330 train_time:23491ms step_avg:39.61ms
step:594/2330 train_time:23547ms step_avg:39.64ms
step:595/2330 train_time:23570ms step_avg:39.61ms
step:596/2330 train_time:23627ms step_avg:39.64ms
step:597/2330 train_time:23649ms step_avg:39.61ms
step:598/2330 train_time:23705ms step_avg:39.64ms
step:599/2330 train_time:23727ms step_avg:39.61ms
step:600/2330 train_time:23783ms step_avg:39.64ms
step:601/2330 train_time:23806ms step_avg:39.61ms
step:602/2330 train_time:23862ms step_avg:39.64ms
step:603/2330 train_time:23885ms step_avg:39.61ms
step:604/2330 train_time:23941ms step_avg:39.64ms
step:605/2330 train_time:23963ms step_avg:39.61ms
step:606/2330 train_time:24018ms step_avg:39.63ms
step:607/2330 train_time:24040ms step_avg:39.61ms
step:608/2330 train_time:24096ms step_avg:39.63ms
step:609/2330 train_time:24118ms step_avg:39.60ms
step:610/2330 train_time:24175ms step_avg:39.63ms
step:611/2330 train_time:24198ms step_avg:39.60ms
step:612/2330 train_time:24255ms step_avg:39.63ms
step:613/2330 train_time:24277ms step_avg:39.60ms
step:614/2330 train_time:24334ms step_avg:39.63ms
step:615/2330 train_time:24357ms step_avg:39.60ms
step:616/2330 train_time:24414ms step_avg:39.63ms
step:617/2330 train_time:24436ms step_avg:39.60ms
step:618/2330 train_time:24492ms step_avg:39.63ms
step:619/2330 train_time:24515ms step_avg:39.60ms
step:620/2330 train_time:24571ms step_avg:39.63ms
step:621/2330 train_time:24594ms step_avg:39.60ms
step:622/2330 train_time:24649ms step_avg:39.63ms
step:623/2330 train_time:24672ms step_avg:39.60ms
step:624/2330 train_time:24728ms step_avg:39.63ms
step:625/2330 train_time:24751ms step_avg:39.60ms
step:626/2330 train_time:24807ms step_avg:39.63ms
step:627/2330 train_time:24830ms step_avg:39.60ms
step:628/2330 train_time:24886ms step_avg:39.63ms
step:629/2330 train_time:24910ms step_avg:39.60ms
step:630/2330 train_time:24966ms step_avg:39.63ms
step:631/2330 train_time:24988ms step_avg:39.60ms
step:632/2330 train_time:25045ms step_avg:39.63ms
step:633/2330 train_time:25067ms step_avg:39.60ms
step:634/2330 train_time:25124ms step_avg:39.63ms
step:635/2330 train_time:25146ms step_avg:39.60ms
step:636/2330 train_time:25203ms step_avg:39.63ms
step:637/2330 train_time:25226ms step_avg:39.60ms
step:638/2330 train_time:25282ms step_avg:39.63ms
step:639/2330 train_time:25305ms step_avg:39.60ms
step:640/2330 train_time:25362ms step_avg:39.63ms
step:641/2330 train_time:25385ms step_avg:39.60ms
step:642/2330 train_time:25441ms step_avg:39.63ms
step:643/2330 train_time:25464ms step_avg:39.60ms
step:644/2330 train_time:25521ms step_avg:39.63ms
step:645/2330 train_time:25543ms step_avg:39.60ms
step:646/2330 train_time:25599ms step_avg:39.63ms
step:647/2330 train_time:25621ms step_avg:39.60ms
step:648/2330 train_time:25677ms step_avg:39.63ms
step:649/2330 train_time:25700ms step_avg:39.60ms
step:650/2330 train_time:25756ms step_avg:39.62ms
step:651/2330 train_time:25778ms step_avg:39.60ms
step:652/2330 train_time:25835ms step_avg:39.62ms
step:653/2330 train_time:25857ms step_avg:39.60ms
step:654/2330 train_time:25914ms step_avg:39.62ms
step:655/2330 train_time:25936ms step_avg:39.60ms
step:656/2330 train_time:25993ms step_avg:39.62ms
step:657/2330 train_time:26015ms step_avg:39.60ms
step:658/2330 train_time:26071ms step_avg:39.62ms
step:659/2330 train_time:26094ms step_avg:39.60ms
step:660/2330 train_time:26150ms step_avg:39.62ms
step:661/2330 train_time:26173ms step_avg:39.60ms
step:662/2330 train_time:26229ms step_avg:39.62ms
step:663/2330 train_time:26252ms step_avg:39.60ms
step:664/2330 train_time:26308ms step_avg:39.62ms
step:665/2330 train_time:26331ms step_avg:39.60ms
step:666/2330 train_time:26387ms step_avg:39.62ms
step:667/2330 train_time:26411ms step_avg:39.60ms
step:668/2330 train_time:26467ms step_avg:39.62ms
step:669/2330 train_time:26491ms step_avg:39.60ms
step:670/2330 train_time:26548ms step_avg:39.62ms
step:671/2330 train_time:26571ms step_avg:39.60ms
step:672/2330 train_time:26627ms step_avg:39.62ms
step:673/2330 train_time:26650ms step_avg:39.60ms
step:674/2330 train_time:26707ms step_avg:39.62ms
step:675/2330 train_time:26729ms step_avg:39.60ms
step:676/2330 train_time:26786ms step_avg:39.62ms
step:677/2330 train_time:26809ms step_avg:39.60ms
step:678/2330 train_time:26865ms step_avg:39.62ms
step:679/2330 train_time:26887ms step_avg:39.60ms
step:680/2330 train_time:26943ms step_avg:39.62ms
step:681/2330 train_time:26966ms step_avg:39.60ms
step:682/2330 train_time:27022ms step_avg:39.62ms
step:683/2330 train_time:27044ms step_avg:39.60ms
step:684/2330 train_time:27100ms step_avg:39.62ms
step:685/2330 train_time:27122ms step_avg:39.59ms
step:686/2330 train_time:27178ms step_avg:39.62ms
step:687/2330 train_time:27200ms step_avg:39.59ms
step:688/2330 train_time:27256ms step_avg:39.62ms
step:689/2330 train_time:27279ms step_avg:39.59ms
step:690/2330 train_time:27336ms step_avg:39.62ms
step:691/2330 train_time:27358ms step_avg:39.59ms
step:692/2330 train_time:27414ms step_avg:39.62ms
step:693/2330 train_time:27436ms step_avg:39.59ms
step:694/2330 train_time:27493ms step_avg:39.62ms
step:695/2330 train_time:27516ms step_avg:39.59ms
step:696/2330 train_time:27572ms step_avg:39.61ms
step:697/2330 train_time:27594ms step_avg:39.59ms
step:698/2330 train_time:27650ms step_avg:39.61ms
step:699/2330 train_time:27672ms step_avg:39.59ms
step:700/2330 train_time:27728ms step_avg:39.61ms
step:701/2330 train_time:27751ms step_avg:39.59ms
step:702/2330 train_time:27807ms step_avg:39.61ms
step:703/2330 train_time:27830ms step_avg:39.59ms
step:704/2330 train_time:27886ms step_avg:39.61ms
step:705/2330 train_time:27909ms step_avg:39.59ms
step:706/2330 train_time:27965ms step_avg:39.61ms
step:707/2330 train_time:27988ms step_avg:39.59ms
step:708/2330 train_time:28044ms step_avg:39.61ms
step:709/2330 train_time:28067ms step_avg:39.59ms
step:710/2330 train_time:28123ms step_avg:39.61ms
step:711/2330 train_time:28146ms step_avg:39.59ms
step:712/2330 train_time:28202ms step_avg:39.61ms
step:713/2330 train_time:28225ms step_avg:39.59ms
step:714/2330 train_time:28281ms step_avg:39.61ms
step:715/2330 train_time:28304ms step_avg:39.59ms
step:716/2330 train_time:28361ms step_avg:39.61ms
step:717/2330 train_time:28382ms step_avg:39.59ms
step:718/2330 train_time:28439ms step_avg:39.61ms
step:719/2330 train_time:28460ms step_avg:39.58ms
step:720/2330 train_time:28517ms step_avg:39.61ms
step:721/2330 train_time:28538ms step_avg:39.58ms
step:722/2330 train_time:28595ms step_avg:39.60ms
step:723/2330 train_time:28617ms step_avg:39.58ms
step:724/2330 train_time:28673ms step_avg:39.60ms
step:725/2330 train_time:28696ms step_avg:39.58ms
step:726/2330 train_time:28752ms step_avg:39.60ms
step:727/2330 train_time:28774ms step_avg:39.58ms
step:728/2330 train_time:28829ms step_avg:39.60ms
step:729/2330 train_time:28852ms step_avg:39.58ms
step:730/2330 train_time:28907ms step_avg:39.60ms
step:731/2330 train_time:28930ms step_avg:39.58ms
step:732/2330 train_time:28986ms step_avg:39.60ms
step:733/2330 train_time:29010ms step_avg:39.58ms
step:734/2330 train_time:29067ms step_avg:39.60ms
step:735/2330 train_time:29090ms step_avg:39.58ms
step:736/2330 train_time:29147ms step_avg:39.60ms
step:737/2330 train_time:29170ms step_avg:39.58ms
step:738/2330 train_time:29226ms step_avg:39.60ms
step:739/2330 train_time:29249ms step_avg:39.58ms
step:740/2330 train_time:29305ms step_avg:39.60ms
step:741/2330 train_time:29328ms step_avg:39.58ms
step:742/2330 train_time:29384ms step_avg:39.60ms
step:743/2330 train_time:29407ms step_avg:39.58ms
step:744/2330 train_time:29463ms step_avg:39.60ms
step:745/2330 train_time:29486ms step_avg:39.58ms
step:746/2330 train_time:29542ms step_avg:39.60ms
step:747/2330 train_time:29564ms step_avg:39.58ms
step:748/2330 train_time:29622ms step_avg:39.60ms
step:749/2330 train_time:29644ms step_avg:39.58ms
step:750/2330 train_time:29701ms step_avg:39.60ms
step:750/2330 val_loss:5.2720 train_time:29795ms step_avg:39.73ms
step:751/2330 train_time:29809ms step_avg:39.69ms
step:752/2330 train_time:29822ms step_avg:39.66ms
step:753/2330 train_time:29833ms step_avg:39.62ms
step:754/2330 train_time:29858ms step_avg:39.60ms
step:755/2330 train_time:29879ms step_avg:39.57ms
step:756/2330 train_time:29934ms step_avg:39.60ms
step:757/2330 train_time:29956ms step_avg:39.57ms
step:758/2330 train_time:30011ms step_avg:39.59ms
step:759/2330 train_time:30033ms step_avg:39.57ms
step:760/2330 train_time:30088ms step_avg:39.59ms
step:761/2330 train_time:30114ms step_avg:39.57ms
step:762/2330 train_time:30173ms step_avg:39.60ms
step:763/2330 train_time:30196ms step_avg:39.58ms
step:764/2330 train_time:30252ms step_avg:39.60ms
step:765/2330 train_time:30276ms step_avg:39.58ms
step:766/2330 train_time:30332ms step_avg:39.60ms
step:767/2330 train_time:30354ms step_avg:39.58ms
step:768/2330 train_time:30410ms step_avg:39.60ms
step:769/2330 train_time:30432ms step_avg:39.57ms
step:770/2330 train_time:30488ms step_avg:39.60ms
step:771/2330 train_time:30511ms step_avg:39.57ms
step:772/2330 train_time:30566ms step_avg:39.59ms
step:773/2330 train_time:30589ms step_avg:39.57ms
step:774/2330 train_time:30644ms step_avg:39.59ms
step:775/2330 train_time:30666ms step_avg:39.57ms
step:776/2330 train_time:30724ms step_avg:39.59ms
step:777/2330 train_time:30747ms step_avg:39.57ms
step:778/2330 train_time:30803ms step_avg:39.59ms
step:779/2330 train_time:30826ms step_avg:39.57ms
step:780/2330 train_time:30881ms step_avg:39.59ms
step:781/2330 train_time:30902ms step_avg:39.57ms
step:782/2330 train_time:30958ms step_avg:39.59ms
step:783/2330 train_time:30979ms step_avg:39.56ms
step:784/2330 train_time:31035ms step_avg:39.59ms
step:785/2330 train_time:31057ms step_avg:39.56ms
step:786/2330 train_time:31114ms step_avg:39.59ms
step:787/2330 train_time:31136ms step_avg:39.56ms
step:788/2330 train_time:31192ms step_avg:39.58ms
step:789/2330 train_time:31215ms step_avg:39.56ms
step:790/2330 train_time:31271ms step_avg:39.58ms
step:791/2330 train_time:31293ms step_avg:39.56ms
step:792/2330 train_time:31349ms step_avg:39.58ms
step:793/2330 train_time:31371ms step_avg:39.56ms
step:794/2330 train_time:31428ms step_avg:39.58ms
step:795/2330 train_time:31450ms step_avg:39.56ms
step:796/2330 train_time:31506ms step_avg:39.58ms
step:797/2330 train_time:31528ms step_avg:39.56ms
step:798/2330 train_time:31584ms step_avg:39.58ms
step:799/2330 train_time:31607ms step_avg:39.56ms
step:800/2330 train_time:31663ms step_avg:39.58ms
step:801/2330 train_time:31685ms step_avg:39.56ms
step:802/2330 train_time:31740ms step_avg:39.58ms
step:803/2330 train_time:31762ms step_avg:39.55ms
step:804/2330 train_time:31817ms step_avg:39.57ms
step:805/2330 train_time:31839ms step_avg:39.55ms
step:806/2330 train_time:31894ms step_avg:39.57ms
step:807/2330 train_time:31916ms step_avg:39.55ms
step:808/2330 train_time:31972ms step_avg:39.57ms
step:809/2330 train_time:31994ms step_avg:39.55ms
step:810/2330 train_time:32051ms step_avg:39.57ms
step:811/2330 train_time:32074ms step_avg:39.55ms
step:812/2330 train_time:32130ms step_avg:39.57ms
step:813/2330 train_time:32153ms step_avg:39.55ms
step:814/2330 train_time:32209ms step_avg:39.57ms
step:815/2330 train_time:32232ms step_avg:39.55ms
step:816/2330 train_time:32288ms step_avg:39.57ms
step:817/2330 train_time:32310ms step_avg:39.55ms
step:818/2330 train_time:32367ms step_avg:39.57ms
step:819/2330 train_time:32389ms step_avg:39.55ms
step:820/2330 train_time:32445ms step_avg:39.57ms
step:821/2330 train_time:32467ms step_avg:39.55ms
step:822/2330 train_time:32523ms step_avg:39.57ms
step:823/2330 train_time:32545ms step_avg:39.54ms
step:824/2330 train_time:32602ms step_avg:39.57ms
step:825/2330 train_time:32623ms step_avg:39.54ms
step:826/2330 train_time:32679ms step_avg:39.56ms
step:827/2330 train_time:32701ms step_avg:39.54ms
step:828/2330 train_time:32756ms step_avg:39.56ms
step:829/2330 train_time:32778ms step_avg:39.54ms
step:830/2330 train_time:32834ms step_avg:39.56ms
step:831/2330 train_time:32856ms step_avg:39.54ms
step:832/2330 train_time:32911ms step_avg:39.56ms
step:833/2330 train_time:32934ms step_avg:39.54ms
step:834/2330 train_time:32990ms step_avg:39.56ms
step:835/2330 train_time:33013ms step_avg:39.54ms
step:836/2330 train_time:33069ms step_avg:39.56ms
step:837/2330 train_time:33092ms step_avg:39.54ms
step:838/2330 train_time:33148ms step_avg:39.56ms
step:839/2330 train_time:33170ms step_avg:39.54ms
step:840/2330 train_time:33227ms step_avg:39.56ms
step:841/2330 train_time:33249ms step_avg:39.53ms
step:842/2330 train_time:33305ms step_avg:39.55ms
step:843/2330 train_time:33327ms step_avg:39.53ms
step:844/2330 train_time:33383ms step_avg:39.55ms
step:845/2330 train_time:33405ms step_avg:39.53ms
step:846/2330 train_time:33461ms step_avg:39.55ms
step:847/2330 train_time:33483ms step_avg:39.53ms
step:848/2330 train_time:33539ms step_avg:39.55ms
step:849/2330 train_time:33561ms step_avg:39.53ms
step:850/2330 train_time:33617ms step_avg:39.55ms
step:851/2330 train_time:33639ms step_avg:39.53ms
step:852/2330 train_time:33695ms step_avg:39.55ms
step:853/2330 train_time:33717ms step_avg:39.53ms
step:854/2330 train_time:33772ms step_avg:39.55ms
step:855/2330 train_time:33795ms step_avg:39.53ms
step:856/2330 train_time:33850ms step_avg:39.54ms
step:857/2330 train_time:33873ms step_avg:39.53ms
step:858/2330 train_time:33929ms step_avg:39.54ms
step:859/2330 train_time:33952ms step_avg:39.52ms
step:860/2330 train_time:34008ms step_avg:39.54ms
step:861/2330 train_time:34031ms step_avg:39.52ms
step:862/2330 train_time:34086ms step_avg:39.54ms
step:863/2330 train_time:34109ms step_avg:39.52ms
step:864/2330 train_time:34165ms step_avg:39.54ms
step:865/2330 train_time:34187ms step_avg:39.52ms
step:866/2330 train_time:34243ms step_avg:39.54ms
step:867/2330 train_time:34266ms step_avg:39.52ms
step:868/2330 train_time:34322ms step_avg:39.54ms
step:869/2330 train_time:34344ms step_avg:39.52ms
step:870/2330 train_time:34400ms step_avg:39.54ms
step:871/2330 train_time:34422ms step_avg:39.52ms
step:872/2330 train_time:34477ms step_avg:39.54ms
step:873/2330 train_time:34499ms step_avg:39.52ms
step:874/2330 train_time:34556ms step_avg:39.54ms
step:875/2330 train_time:34578ms step_avg:39.52ms
step:876/2330 train_time:34633ms step_avg:39.54ms
step:877/2330 train_time:34656ms step_avg:39.52ms
step:878/2330 train_time:34711ms step_avg:39.53ms
step:879/2330 train_time:34734ms step_avg:39.52ms
step:880/2330 train_time:34790ms step_avg:39.53ms
step:881/2330 train_time:34812ms step_avg:39.51ms
step:882/2330 train_time:34868ms step_avg:39.53ms
step:883/2330 train_time:34891ms step_avg:39.51ms
step:884/2330 train_time:34947ms step_avg:39.53ms
step:885/2330 train_time:34969ms step_avg:39.51ms
step:886/2330 train_time:35025ms step_avg:39.53ms
step:887/2330 train_time:35047ms step_avg:39.51ms
step:888/2330 train_time:35103ms step_avg:39.53ms
step:889/2330 train_time:35126ms step_avg:39.51ms
step:890/2330 train_time:35182ms step_avg:39.53ms
step:891/2330 train_time:35204ms step_avg:39.51ms
step:892/2330 train_time:35260ms step_avg:39.53ms
step:893/2330 train_time:35282ms step_avg:39.51ms
step:894/2330 train_time:35338ms step_avg:39.53ms
step:895/2330 train_time:35359ms step_avg:39.51ms
step:896/2330 train_time:35416ms step_avg:39.53ms
step:897/2330 train_time:35437ms step_avg:39.51ms
step:898/2330 train_time:35493ms step_avg:39.52ms
step:899/2330 train_time:35515ms step_avg:39.51ms
step:900/2330 train_time:35571ms step_avg:39.52ms
step:901/2330 train_time:35594ms step_avg:39.50ms
step:902/2330 train_time:35649ms step_avg:39.52ms
step:903/2330 train_time:35672ms step_avg:39.50ms
step:904/2330 train_time:35727ms step_avg:39.52ms
step:905/2330 train_time:35750ms step_avg:39.50ms
step:906/2330 train_time:35806ms step_avg:39.52ms
step:907/2330 train_time:35828ms step_avg:39.50ms
step:908/2330 train_time:35884ms step_avg:39.52ms
step:909/2330 train_time:35907ms step_avg:39.50ms
step:910/2330 train_time:35962ms step_avg:39.52ms
step:911/2330 train_time:35984ms step_avg:39.50ms
step:912/2330 train_time:36039ms step_avg:39.52ms
step:913/2330 train_time:36061ms step_avg:39.50ms
step:914/2330 train_time:36118ms step_avg:39.52ms
step:915/2330 train_time:36140ms step_avg:39.50ms
step:916/2330 train_time:36196ms step_avg:39.52ms
step:917/2330 train_time:36219ms step_avg:39.50ms
step:918/2330 train_time:36274ms step_avg:39.51ms
step:919/2330 train_time:36297ms step_avg:39.50ms
step:920/2330 train_time:36352ms step_avg:39.51ms
step:921/2330 train_time:36375ms step_avg:39.50ms
step:922/2330 train_time:36431ms step_avg:39.51ms
step:923/2330 train_time:36454ms step_avg:39.49ms
step:924/2330 train_time:36510ms step_avg:39.51ms
step:925/2330 train_time:36532ms step_avg:39.49ms
step:926/2330 train_time:36588ms step_avg:39.51ms
step:927/2330 train_time:36612ms step_avg:39.49ms
step:928/2330 train_time:36668ms step_avg:39.51ms
step:929/2330 train_time:36691ms step_avg:39.50ms
step:930/2330 train_time:36748ms step_avg:39.51ms
step:931/2330 train_time:36771ms step_avg:39.50ms
step:932/2330 train_time:36828ms step_avg:39.51ms
step:933/2330 train_time:36850ms step_avg:39.50ms
step:934/2330 train_time:36906ms step_avg:39.51ms
step:935/2330 train_time:36928ms step_avg:39.50ms
step:936/2330 train_time:36984ms step_avg:39.51ms
step:937/2330 train_time:37007ms step_avg:39.50ms
step:938/2330 train_time:37063ms step_avg:39.51ms
step:939/2330 train_time:37085ms step_avg:39.49ms
step:940/2330 train_time:37141ms step_avg:39.51ms
step:941/2330 train_time:37163ms step_avg:39.49ms
step:942/2330 train_time:37219ms step_avg:39.51ms
step:943/2330 train_time:37241ms step_avg:39.49ms
step:944/2330 train_time:37297ms step_avg:39.51ms
step:945/2330 train_time:37319ms step_avg:39.49ms
step:946/2330 train_time:37375ms step_avg:39.51ms
step:947/2330 train_time:37396ms step_avg:39.49ms
step:948/2330 train_time:37452ms step_avg:39.51ms
step:949/2330 train_time:37475ms step_avg:39.49ms
step:950/2330 train_time:37530ms step_avg:39.51ms
step:951/2330 train_time:37553ms step_avg:39.49ms
step:952/2330 train_time:37609ms step_avg:39.51ms
step:953/2330 train_time:37632ms step_avg:39.49ms
step:954/2330 train_time:37688ms step_avg:39.51ms
step:955/2330 train_time:37711ms step_avg:39.49ms
step:956/2330 train_time:37767ms step_avg:39.50ms
step:957/2330 train_time:37789ms step_avg:39.49ms
step:958/2330 train_time:37846ms step_avg:39.50ms
step:959/2330 train_time:37868ms step_avg:39.49ms
step:960/2330 train_time:37925ms step_avg:39.50ms
step:961/2330 train_time:37947ms step_avg:39.49ms
step:962/2330 train_time:38003ms step_avg:39.50ms
step:963/2330 train_time:38026ms step_avg:39.49ms
step:964/2330 train_time:38082ms step_avg:39.50ms
step:965/2330 train_time:38104ms step_avg:39.49ms
step:966/2330 train_time:38159ms step_avg:39.50ms
step:967/2330 train_time:38181ms step_avg:39.48ms
step:968/2330 train_time:38237ms step_avg:39.50ms
step:969/2330 train_time:38259ms step_avg:39.48ms
step:970/2330 train_time:38316ms step_avg:39.50ms
step:971/2330 train_time:38338ms step_avg:39.48ms
step:972/2330 train_time:38394ms step_avg:39.50ms
step:973/2330 train_time:38416ms step_avg:39.48ms
step:974/2330 train_time:38472ms step_avg:39.50ms
step:975/2330 train_time:38494ms step_avg:39.48ms
step:976/2330 train_time:38550ms step_avg:39.50ms
step:977/2330 train_time:38573ms step_avg:39.48ms
step:978/2330 train_time:38629ms step_avg:39.50ms
step:979/2330 train_time:38651ms step_avg:39.48ms
step:980/2330 train_time:38707ms step_avg:39.50ms
step:981/2330 train_time:38730ms step_avg:39.48ms
step:982/2330 train_time:38786ms step_avg:39.50ms
step:983/2330 train_time:38809ms step_avg:39.48ms
step:984/2330 train_time:38866ms step_avg:39.50ms
step:985/2330 train_time:38889ms step_avg:39.48ms
step:986/2330 train_time:38946ms step_avg:39.50ms
step:987/2330 train_time:38968ms step_avg:39.48ms
step:988/2330 train_time:39024ms step_avg:39.50ms
step:989/2330 train_time:39046ms step_avg:39.48ms
step:990/2330 train_time:39102ms step_avg:39.50ms
step:991/2330 train_time:39125ms step_avg:39.48ms
step:992/2330 train_time:39181ms step_avg:39.50ms
step:993/2330 train_time:39202ms step_avg:39.48ms
step:994/2330 train_time:39258ms step_avg:39.50ms
step:995/2330 train_time:39280ms step_avg:39.48ms
step:996/2330 train_time:39337ms step_avg:39.50ms
step:997/2330 train_time:39359ms step_avg:39.48ms
step:998/2330 train_time:39416ms step_avg:39.50ms
step:999/2330 train_time:39438ms step_avg:39.48ms
step:1000/2330 train_time:39494ms step_avg:39.49ms
step:1000/2330 val_loss:5.2341 train_time:39590ms step_avg:39.59ms
step:1001/2330 train_time:39603ms step_avg:39.56ms
step:1002/2330 train_time:39616ms step_avg:39.54ms
step:1003/2330 train_time:39626ms step_avg:39.51ms
step:1004/2330 train_time:39653ms step_avg:39.49ms
step:1005/2330 train_time:39674ms step_avg:39.48ms
step:1006/2330 train_time:39729ms step_avg:39.49ms
step:1007/2330 train_time:39750ms step_avg:39.47ms
step:1008/2330 train_time:39805ms step_avg:39.49ms
step:1009/2330 train_time:39827ms step_avg:39.47ms
step:1010/2330 train_time:39882ms step_avg:39.49ms
step:1011/2330 train_time:39907ms step_avg:39.47ms
step:1012/2330 train_time:39965ms step_avg:39.49ms
step:1013/2330 train_time:39989ms step_avg:39.48ms
step:1014/2330 train_time:40045ms step_avg:39.49ms
step:1015/2330 train_time:40068ms step_avg:39.48ms
step:1016/2330 train_time:40124ms step_avg:39.49ms
step:1017/2330 train_time:40146ms step_avg:39.48ms
step:1018/2330 train_time:40201ms step_avg:39.49ms
step:1019/2330 train_time:40223ms step_avg:39.47ms
step:1020/2330 train_time:40279ms step_avg:39.49ms
step:1021/2330 train_time:40301ms step_avg:39.47ms
step:1022/2330 train_time:40356ms step_avg:39.49ms
step:1023/2330 train_time:40378ms step_avg:39.47ms
step:1024/2330 train_time:40433ms step_avg:39.49ms
step:1025/2330 train_time:40455ms step_avg:39.47ms
step:1026/2330 train_time:40512ms step_avg:39.49ms
step:1027/2330 train_time:40535ms step_avg:39.47ms
step:1028/2330 train_time:40592ms step_avg:39.49ms
step:1029/2330 train_time:40613ms step_avg:39.47ms
step:1030/2330 train_time:40669ms step_avg:39.48ms
step:1031/2330 train_time:40691ms step_avg:39.47ms
step:1032/2330 train_time:40747ms step_avg:39.48ms
step:1033/2330 train_time:40768ms step_avg:39.47ms
step:1034/2330 train_time:40824ms step_avg:39.48ms
step:1035/2330 train_time:40847ms step_avg:39.47ms
step:1036/2330 train_time:40903ms step_avg:39.48ms
step:1037/2330 train_time:40927ms step_avg:39.47ms
step:1038/2330 train_time:40982ms step_avg:39.48ms
step:1039/2330 train_time:41005ms step_avg:39.47ms
step:1040/2330 train_time:41061ms step_avg:39.48ms
step:1041/2330 train_time:41084ms step_avg:39.47ms
step:1042/2330 train_time:41139ms step_avg:39.48ms
step:1043/2330 train_time:41162ms step_avg:39.46ms
step:1044/2330 train_time:41217ms step_avg:39.48ms
step:1045/2330 train_time:41240ms step_avg:39.46ms
step:1046/2330 train_time:41295ms step_avg:39.48ms
step:1047/2330 train_time:41317ms step_avg:39.46ms
step:1048/2330 train_time:41372ms step_avg:39.48ms
step:1049/2330 train_time:41393ms step_avg:39.46ms
step:1050/2330 train_time:41450ms step_avg:39.48ms
step:1051/2330 train_time:41472ms step_avg:39.46ms
step:1052/2330 train_time:41528ms step_avg:39.48ms
step:1053/2330 train_time:41550ms step_avg:39.46ms
step:1054/2330 train_time:41606ms step_avg:39.47ms
step:1055/2330 train_time:41628ms step_avg:39.46ms
step:1056/2330 train_time:41684ms step_avg:39.47ms
step:1057/2330 train_time:41707ms step_avg:39.46ms
step:1058/2330 train_time:41763ms step_avg:39.47ms
step:1059/2330 train_time:41785ms step_avg:39.46ms
step:1060/2330 train_time:41841ms step_avg:39.47ms
step:1061/2330 train_time:41864ms step_avg:39.46ms
step:1062/2330 train_time:41921ms step_avg:39.47ms
step:1063/2330 train_time:41943ms step_avg:39.46ms
step:1064/2330 train_time:41999ms step_avg:39.47ms
step:1065/2330 train_time:42022ms step_avg:39.46ms
step:1066/2330 train_time:42078ms step_avg:39.47ms
step:1067/2330 train_time:42100ms step_avg:39.46ms
step:1068/2330 train_time:42156ms step_avg:39.47ms
step:1069/2330 train_time:42178ms step_avg:39.46ms
step:1070/2330 train_time:42233ms step_avg:39.47ms
step:1071/2330 train_time:42255ms step_avg:39.45ms
step:1072/2330 train_time:42310ms step_avg:39.47ms
step:1073/2330 train_time:42332ms step_avg:39.45ms
step:1074/2330 train_time:42388ms step_avg:39.47ms
step:1075/2330 train_time:42410ms step_avg:39.45ms
step:1076/2330 train_time:42466ms step_avg:39.47ms
step:1077/2330 train_time:42489ms step_avg:39.45ms
step:1078/2330 train_time:42544ms step_avg:39.47ms
step:1079/2330 train_time:42567ms step_avg:39.45ms
step:1080/2330 train_time:42623ms step_avg:39.47ms
step:1081/2330 train_time:42645ms step_avg:39.45ms
step:1082/2330 train_time:42701ms step_avg:39.46ms
step:1083/2330 train_time:42723ms step_avg:39.45ms
step:1084/2330 train_time:42779ms step_avg:39.46ms
step:1085/2330 train_time:42802ms step_avg:39.45ms
step:1086/2330 train_time:42858ms step_avg:39.46ms
step:1087/2330 train_time:42881ms step_avg:39.45ms
step:1088/2330 train_time:42937ms step_avg:39.46ms
step:1089/2330 train_time:42960ms step_avg:39.45ms
step:1090/2330 train_time:43016ms step_avg:39.46ms
step:1091/2330 train_time:43038ms step_avg:39.45ms
step:1092/2330 train_time:43094ms step_avg:39.46ms
step:1093/2330 train_time:43115ms step_avg:39.45ms
step:1094/2330 train_time:43171ms step_avg:39.46ms
step:1095/2330 train_time:43193ms step_avg:39.45ms
step:1096/2330 train_time:43249ms step_avg:39.46ms
step:1097/2330 train_time:43271ms step_avg:39.44ms
step:1098/2330 train_time:43327ms step_avg:39.46ms
step:1099/2330 train_time:43349ms step_avg:39.44ms
step:1100/2330 train_time:43404ms step_avg:39.46ms
step:1101/2330 train_time:43427ms step_avg:39.44ms
step:1102/2330 train_time:43483ms step_avg:39.46ms
step:1103/2330 train_time:43506ms step_avg:39.44ms
step:1104/2330 train_time:43561ms step_avg:39.46ms
step:1105/2330 train_time:43584ms step_avg:39.44ms
step:1106/2330 train_time:43639ms step_avg:39.46ms
step:1107/2330 train_time:43662ms step_avg:39.44ms
step:1108/2330 train_time:43718ms step_avg:39.46ms
step:1109/2330 train_time:43740ms step_avg:39.44ms
step:1110/2330 train_time:43796ms step_avg:39.46ms
step:1111/2330 train_time:43819ms step_avg:39.44ms
step:1112/2330 train_time:43875ms step_avg:39.46ms
step:1113/2330 train_time:43898ms step_avg:39.44ms
step:1114/2330 train_time:43954ms step_avg:39.46ms
step:1115/2330 train_time:43976ms step_avg:39.44ms
step:1116/2330 train_time:44032ms step_avg:39.45ms
step:1117/2330 train_time:44054ms step_avg:39.44ms
step:1118/2330 train_time:44109ms step_avg:39.45ms
step:1119/2330 train_time:44131ms step_avg:39.44ms
step:1120/2330 train_time:44187ms step_avg:39.45ms
step:1121/2330 train_time:44209ms step_avg:39.44ms
step:1122/2330 train_time:44265ms step_avg:39.45ms
step:1123/2330 train_time:44287ms step_avg:39.44ms
step:1124/2330 train_time:44343ms step_avg:39.45ms
step:1125/2330 train_time:44366ms step_avg:39.44ms
step:1126/2330 train_time:44422ms step_avg:39.45ms
step:1127/2330 train_time:44445ms step_avg:39.44ms
step:1128/2330 train_time:44501ms step_avg:39.45ms
step:1129/2330 train_time:44524ms step_avg:39.44ms
step:1130/2330 train_time:44580ms step_avg:39.45ms
step:1131/2330 train_time:44602ms step_avg:39.44ms
step:1132/2330 train_time:44658ms step_avg:39.45ms
step:1133/2330 train_time:44681ms step_avg:39.44ms
step:1134/2330 train_time:44737ms step_avg:39.45ms
step:1135/2330 train_time:44759ms step_avg:39.44ms
step:1136/2330 train_time:44815ms step_avg:39.45ms
step:1137/2330 train_time:44838ms step_avg:39.43ms
step:1138/2330 train_time:44893ms step_avg:39.45ms
step:1139/2330 train_time:44915ms step_avg:39.43ms
step:1140/2330 train_time:44970ms step_avg:39.45ms
step:1141/2330 train_time:44993ms step_avg:39.43ms
step:1142/2330 train_time:45049ms step_avg:39.45ms
step:1143/2330 train_time:45070ms step_avg:39.43ms
step:1144/2330 train_time:45126ms step_avg:39.45ms
step:1145/2330 train_time:45149ms step_avg:39.43ms
step:1146/2330 train_time:45204ms step_avg:39.44ms
step:1147/2330 train_time:45226ms step_avg:39.43ms
step:1148/2330 train_time:45282ms step_avg:39.44ms
step:1149/2330 train_time:45304ms step_avg:39.43ms
step:1150/2330 train_time:45361ms step_avg:39.44ms
step:1151/2330 train_time:45384ms step_avg:39.43ms
step:1152/2330 train_time:45439ms step_avg:39.44ms
step:1153/2330 train_time:45462ms step_avg:39.43ms
step:1154/2330 train_time:45518ms step_avg:39.44ms
step:1155/2330 train_time:45540ms step_avg:39.43ms
step:1156/2330 train_time:45596ms step_avg:39.44ms
step:1157/2330 train_time:45618ms step_avg:39.43ms
step:1158/2330 train_time:45674ms step_avg:39.44ms
step:1159/2330 train_time:45696ms step_avg:39.43ms
step:1160/2330 train_time:45751ms step_avg:39.44ms
step:1161/2330 train_time:45773ms step_avg:39.43ms
step:1162/2330 train_time:45830ms step_avg:39.44ms
step:1163/2330 train_time:45852ms step_avg:39.43ms
step:1164/2330 train_time:45907ms step_avg:39.44ms
step:1165/2330 train_time:45929ms step_avg:39.42ms
step:1166/2330 train_time:45985ms step_avg:39.44ms
step:1167/2330 train_time:46008ms step_avg:39.42ms
step:1168/2330 train_time:46063ms step_avg:39.44ms
step:1169/2330 train_time:46086ms step_avg:39.42ms
step:1170/2330 train_time:46142ms step_avg:39.44ms
step:1171/2330 train_time:46164ms step_avg:39.42ms
step:1172/2330 train_time:46220ms step_avg:39.44ms
step:1173/2330 train_time:46242ms step_avg:39.42ms
step:1174/2330 train_time:46298ms step_avg:39.44ms
step:1175/2330 train_time:46320ms step_avg:39.42ms
step:1176/2330 train_time:46376ms step_avg:39.44ms
step:1177/2330 train_time:46399ms step_avg:39.42ms
step:1178/2330 train_time:46455ms step_avg:39.44ms
step:1179/2330 train_time:46477ms step_avg:39.42ms
step:1180/2330 train_time:46532ms step_avg:39.43ms
step:1181/2330 train_time:46555ms step_avg:39.42ms
step:1182/2330 train_time:46610ms step_avg:39.43ms
step:1183/2330 train_time:46632ms step_avg:39.42ms
step:1184/2330 train_time:46688ms step_avg:39.43ms
step:1185/2330 train_time:46709ms step_avg:39.42ms
step:1186/2330 train_time:46765ms step_avg:39.43ms
step:1187/2330 train_time:46788ms step_avg:39.42ms
step:1188/2330 train_time:46844ms step_avg:39.43ms
step:1189/2330 train_time:46866ms step_avg:39.42ms
step:1190/2330 train_time:46922ms step_avg:39.43ms
step:1191/2330 train_time:46944ms step_avg:39.42ms
step:1192/2330 train_time:47000ms step_avg:39.43ms
step:1193/2330 train_time:47022ms step_avg:39.42ms
step:1194/2330 train_time:47078ms step_avg:39.43ms
step:1195/2330 train_time:47102ms step_avg:39.42ms
step:1196/2330 train_time:47157ms step_avg:39.43ms
step:1197/2330 train_time:47179ms step_avg:39.41ms
step:1198/2330 train_time:47235ms step_avg:39.43ms
step:1199/2330 train_time:47257ms step_avg:39.41ms
step:1200/2330 train_time:47313ms step_avg:39.43ms
step:1201/2330 train_time:47335ms step_avg:39.41ms
step:1202/2330 train_time:47391ms step_avg:39.43ms
step:1203/2330 train_time:47413ms step_avg:39.41ms
step:1204/2330 train_time:47469ms step_avg:39.43ms
step:1205/2330 train_time:47491ms step_avg:39.41ms
step:1206/2330 train_time:47547ms step_avg:39.43ms
step:1207/2330 train_time:47569ms step_avg:39.41ms
step:1208/2330 train_time:47624ms step_avg:39.42ms
step:1209/2330 train_time:47646ms step_avg:39.41ms
step:1210/2330 train_time:47702ms step_avg:39.42ms
step:1211/2330 train_time:47724ms step_avg:39.41ms
step:1212/2330 train_time:47781ms step_avg:39.42ms
step:1213/2330 train_time:47803ms step_avg:39.41ms
step:1214/2330 train_time:47859ms step_avg:39.42ms
step:1215/2330 train_time:47881ms step_avg:39.41ms
step:1216/2330 train_time:47937ms step_avg:39.42ms
step:1217/2330 train_time:47960ms step_avg:39.41ms
step:1218/2330 train_time:48015ms step_avg:39.42ms
step:1219/2330 train_time:48039ms step_avg:39.41ms
step:1220/2330 train_time:48094ms step_avg:39.42ms
step:1221/2330 train_time:48116ms step_avg:39.41ms
step:1222/2330 train_time:48172ms step_avg:39.42ms
step:1223/2330 train_time:48194ms step_avg:39.41ms
step:1224/2330 train_time:48250ms step_avg:39.42ms
step:1225/2330 train_time:48272ms step_avg:39.41ms
step:1226/2330 train_time:48329ms step_avg:39.42ms
step:1227/2330 train_time:48351ms step_avg:39.41ms
step:1228/2330 train_time:48406ms step_avg:39.42ms
step:1229/2330 train_time:48428ms step_avg:39.40ms
step:1230/2330 train_time:48483ms step_avg:39.42ms
step:1231/2330 train_time:48506ms step_avg:39.40ms
step:1232/2330 train_time:48562ms step_avg:39.42ms
step:1233/2330 train_time:48585ms step_avg:39.40ms
step:1234/2330 train_time:48640ms step_avg:39.42ms
step:1235/2330 train_time:48663ms step_avg:39.40ms
step:1236/2330 train_time:48718ms step_avg:39.42ms
step:1237/2330 train_time:48741ms step_avg:39.40ms
step:1238/2330 train_time:48797ms step_avg:39.42ms
step:1239/2330 train_time:48819ms step_avg:39.40ms
step:1240/2330 train_time:48875ms step_avg:39.42ms
step:1241/2330 train_time:48897ms step_avg:39.40ms
step:1242/2330 train_time:48953ms step_avg:39.41ms
step:1243/2330 train_time:48975ms step_avg:39.40ms
step:1244/2330 train_time:49031ms step_avg:39.41ms
step:1245/2330 train_time:49053ms step_avg:39.40ms
step:1246/2330 train_time:49109ms step_avg:39.41ms
step:1247/2330 train_time:49130ms step_avg:39.40ms
step:1248/2330 train_time:49186ms step_avg:39.41ms
step:1249/2330 train_time:49208ms step_avg:39.40ms
step:1250/2330 train_time:49264ms step_avg:39.41ms
step:1250/2330 val_loss:5.1990 train_time:49360ms step_avg:39.49ms
step:1251/2330 train_time:49373ms step_avg:39.47ms
step:1252/2330 train_time:49384ms step_avg:39.44ms
step:1253/2330 train_time:49394ms step_avg:39.42ms
step:1254/2330 train_time:49423ms step_avg:39.41ms
step:1255/2330 train_time:49444ms step_avg:39.40ms
step:1256/2330 train_time:49499ms step_avg:39.41ms
step:1257/2330 train_time:49520ms step_avg:39.40ms
step:1258/2330 train_time:49574ms step_avg:39.41ms
step:1259/2330 train_time:49596ms step_avg:39.39ms
step:1260/2330 train_time:49651ms step_avg:39.41ms
step:1261/2330 train_time:49677ms step_avg:39.40ms
step:1262/2330 train_time:49737ms step_avg:39.41ms
step:1263/2330 train_time:49762ms step_avg:39.40ms
step:1264/2330 train_time:49821ms step_avg:39.42ms
step:1265/2330 train_time:49843ms step_avg:39.40ms
step:1266/2330 train_time:49900ms step_avg:39.42ms
step:1267/2330 train_time:49922ms step_avg:39.40ms
step:1268/2330 train_time:49978ms step_avg:39.41ms
step:1269/2330 train_time:49999ms step_avg:39.40ms
step:1270/2330 train_time:50054ms step_avg:39.41ms
step:1271/2330 train_time:50077ms step_avg:39.40ms
step:1272/2330 train_time:50132ms step_avg:39.41ms
step:1273/2330 train_time:50154ms step_avg:39.40ms
step:1274/2330 train_time:50209ms step_avg:39.41ms
step:1275/2330 train_time:50232ms step_avg:39.40ms
step:1276/2330 train_time:50288ms step_avg:39.41ms
step:1277/2330 train_time:50311ms step_avg:39.40ms
step:1278/2330 train_time:50367ms step_avg:39.41ms
step:1279/2330 train_time:50390ms step_avg:39.40ms
step:1280/2330 train_time:50446ms step_avg:39.41ms
step:1281/2330 train_time:50468ms step_avg:39.40ms
step:1282/2330 train_time:50523ms step_avg:39.41ms
step:1283/2330 train_time:50545ms step_avg:39.40ms
step:1284/2330 train_time:50601ms step_avg:39.41ms
step:1285/2330 train_time:50623ms step_avg:39.39ms
step:1286/2330 train_time:50679ms step_avg:39.41ms
step:1287/2330 train_time:50702ms step_avg:39.40ms
step:1288/2330 train_time:50759ms step_avg:39.41ms
step:1289/2330 train_time:50782ms step_avg:39.40ms
step:1290/2330 train_time:50839ms step_avg:39.41ms
step:1291/2330 train_time:50861ms step_avg:39.40ms
step:1292/2330 train_time:50917ms step_avg:39.41ms
step:1293/2330 train_time:50939ms step_avg:39.40ms
step:1294/2330 train_time:50995ms step_avg:39.41ms
step:1295/2330 train_time:51017ms step_avg:39.40ms
step:1296/2330 train_time:51073ms step_avg:39.41ms
step:1297/2330 train_time:51095ms step_avg:39.39ms
step:1298/2330 train_time:51150ms step_avg:39.41ms
step:1299/2330 train_time:51172ms step_avg:39.39ms
step:1300/2330 train_time:51228ms step_avg:39.41ms
step:1301/2330 train_time:51250ms step_avg:39.39ms
step:1302/2330 train_time:51306ms step_avg:39.41ms
step:1303/2330 train_time:51328ms step_avg:39.39ms
step:1304/2330 train_time:51383ms step_avg:39.40ms
step:1305/2330 train_time:51405ms step_avg:39.39ms
step:1306/2330 train_time:51461ms step_avg:39.40ms
step:1307/2330 train_time:51482ms step_avg:39.39ms
step:1308/2330 train_time:51538ms step_avg:39.40ms
step:1309/2330 train_time:51560ms step_avg:39.39ms
step:1310/2330 train_time:51616ms step_avg:39.40ms
step:1311/2330 train_time:51638ms step_avg:39.39ms
step:1312/2330 train_time:51694ms step_avg:39.40ms
step:1313/2330 train_time:51717ms step_avg:39.39ms
step:1314/2330 train_time:51773ms step_avg:39.40ms
step:1315/2330 train_time:51796ms step_avg:39.39ms
step:1316/2330 train_time:51853ms step_avg:39.40ms
step:1317/2330 train_time:51876ms step_avg:39.39ms
step:1318/2330 train_time:51932ms step_avg:39.40ms
step:1319/2330 train_time:51955ms step_avg:39.39ms
step:1320/2330 train_time:52011ms step_avg:39.40ms
step:1321/2330 train_time:52034ms step_avg:39.39ms
step:1322/2330 train_time:52089ms step_avg:39.40ms
step:1323/2330 train_time:52112ms step_avg:39.39ms
step:1324/2330 train_time:52168ms step_avg:39.40ms
step:1325/2330 train_time:52190ms step_avg:39.39ms
step:1326/2330 train_time:52246ms step_avg:39.40ms
step:1327/2330 train_time:52269ms step_avg:39.39ms
step:1328/2330 train_time:52324ms step_avg:39.40ms
step:1329/2330 train_time:52346ms step_avg:39.39ms
step:1330/2330 train_time:52401ms step_avg:39.40ms
step:1331/2330 train_time:52423ms step_avg:39.39ms
step:1332/2330 train_time:52479ms step_avg:39.40ms
step:1333/2330 train_time:52501ms step_avg:39.39ms
step:1334/2330 train_time:52557ms step_avg:39.40ms
step:1335/2330 train_time:52579ms step_avg:39.38ms
step:1336/2330 train_time:52635ms step_avg:39.40ms
step:1337/2330 train_time:52657ms step_avg:39.38ms
step:1338/2330 train_time:52713ms step_avg:39.40ms
step:1339/2330 train_time:52735ms step_avg:39.38ms
step:1340/2330 train_time:52792ms step_avg:39.40ms
step:1341/2330 train_time:52815ms step_avg:39.38ms
step:1342/2330 train_time:52872ms step_avg:39.40ms
step:1343/2330 train_time:52894ms step_avg:39.39ms
step:1344/2330 train_time:52950ms step_avg:39.40ms
step:1345/2330 train_time:52973ms step_avg:39.39ms
step:1346/2330 train_time:53030ms step_avg:39.40ms
step:1347/2330 train_time:53052ms step_avg:39.39ms
step:1348/2330 train_time:53108ms step_avg:39.40ms
step:1349/2330 train_time:53130ms step_avg:39.39ms
step:1350/2330 train_time:53186ms step_avg:39.40ms
step:1351/2330 train_time:53208ms step_avg:39.38ms
step:1352/2330 train_time:53263ms step_avg:39.40ms
step:1353/2330 train_time:53285ms step_avg:39.38ms
step:1354/2330 train_time:53340ms step_avg:39.39ms
step:1355/2330 train_time:53363ms step_avg:39.38ms
step:1356/2330 train_time:53418ms step_avg:39.39ms
step:1357/2330 train_time:53440ms step_avg:39.38ms
step:1358/2330 train_time:53495ms step_avg:39.39ms
step:1359/2330 train_time:53517ms step_avg:39.38ms
step:1360/2330 train_time:53573ms step_avg:39.39ms
step:1361/2330 train_time:53596ms step_avg:39.38ms
step:1362/2330 train_time:53651ms step_avg:39.39ms
step:1363/2330 train_time:53674ms step_avg:39.38ms
step:1364/2330 train_time:53730ms step_avg:39.39ms
step:1365/2330 train_time:53754ms step_avg:39.38ms
step:1366/2330 train_time:53810ms step_avg:39.39ms
step:1367/2330 train_time:53832ms step_avg:39.38ms
step:1368/2330 train_time:53889ms step_avg:39.39ms
step:1369/2330 train_time:53911ms step_avg:39.38ms
step:1370/2330 train_time:53967ms step_avg:39.39ms
step:1371/2330 train_time:53989ms step_avg:39.38ms
step:1372/2330 train_time:54045ms step_avg:39.39ms
step:1373/2330 train_time:54068ms step_avg:39.38ms
step:1374/2330 train_time:54123ms step_avg:39.39ms
step:1375/2330 train_time:54145ms step_avg:39.38ms
step:1376/2330 train_time:54200ms step_avg:39.39ms
step:1377/2330 train_time:54222ms step_avg:39.38ms
step:1378/2330 train_time:54278ms step_avg:39.39ms
step:1379/2330 train_time:54300ms step_avg:39.38ms
step:1380/2330 train_time:54355ms step_avg:39.39ms
step:1381/2330 train_time:54378ms step_avg:39.38ms
step:1382/2330 train_time:54433ms step_avg:39.39ms
step:1383/2330 train_time:54455ms step_avg:39.37ms
step:1384/2330 train_time:54511ms step_avg:39.39ms
step:1385/2330 train_time:54533ms step_avg:39.37ms
step:1386/2330 train_time:54589ms step_avg:39.39ms
step:1387/2330 train_time:54612ms step_avg:39.37ms
step:1388/2330 train_time:54667ms step_avg:39.39ms
step:1389/2330 train_time:54690ms step_avg:39.37ms
step:1390/2330 train_time:54746ms step_avg:39.39ms
step:1391/2330 train_time:54769ms step_avg:39.37ms
step:1392/2330 train_time:54825ms step_avg:39.39ms
step:1393/2330 train_time:54847ms step_avg:39.37ms
step:1394/2330 train_time:54903ms step_avg:39.39ms
step:1395/2330 train_time:54925ms step_avg:39.37ms
step:1396/2330 train_time:54982ms step_avg:39.39ms
step:1397/2330 train_time:55004ms step_avg:39.37ms
step:1398/2330 train_time:55059ms step_avg:39.38ms
step:1399/2330 train_time:55081ms step_avg:39.37ms
step:1400/2330 train_time:55137ms step_avg:39.38ms
step:1401/2330 train_time:55158ms step_avg:39.37ms
step:1402/2330 train_time:55214ms step_avg:39.38ms
step:1403/2330 train_time:55237ms step_avg:39.37ms
step:1404/2330 train_time:55292ms step_avg:39.38ms
step:1405/2330 train_time:55315ms step_avg:39.37ms
step:1406/2330 train_time:55371ms step_avg:39.38ms
step:1407/2330 train_time:55393ms step_avg:39.37ms
step:1408/2330 train_time:55449ms step_avg:39.38ms
step:1409/2330 train_time:55472ms step_avg:39.37ms
step:1410/2330 train_time:55528ms step_avg:39.38ms
step:1411/2330 train_time:55550ms step_avg:39.37ms
step:1412/2330 train_time:55606ms step_avg:39.38ms
step:1413/2330 train_time:55629ms step_avg:39.37ms
step:1414/2330 train_time:55685ms step_avg:39.38ms
step:1415/2330 train_time:55707ms step_avg:39.37ms
step:1416/2330 train_time:55763ms step_avg:39.38ms
step:1417/2330 train_time:55785ms step_avg:39.37ms
step:1418/2330 train_time:55841ms step_avg:39.38ms
step:1419/2330 train_time:55863ms step_avg:39.37ms
step:1420/2330 train_time:55919ms step_avg:39.38ms
step:1421/2330 train_time:55941ms step_avg:39.37ms
step:1422/2330 train_time:55997ms step_avg:39.38ms
step:1423/2330 train_time:56019ms step_avg:39.37ms
step:1424/2330 train_time:56075ms step_avg:39.38ms
step:1425/2330 train_time:56097ms step_avg:39.37ms
step:1426/2330 train_time:56153ms step_avg:39.38ms
step:1427/2330 train_time:56175ms step_avg:39.37ms
step:1428/2330 train_time:56231ms step_avg:39.38ms
step:1429/2330 train_time:56253ms step_avg:39.37ms
step:1430/2330 train_time:56309ms step_avg:39.38ms
step:1431/2330 train_time:56331ms step_avg:39.36ms
step:1432/2330 train_time:56387ms step_avg:39.38ms
step:1433/2330 train_time:56410ms step_avg:39.36ms
step:1434/2330 train_time:56465ms step_avg:39.38ms
step:1435/2330 train_time:56487ms step_avg:39.36ms
step:1436/2330 train_time:56543ms step_avg:39.38ms
step:1437/2330 train_time:56564ms step_avg:39.36ms
step:1438/2330 train_time:56620ms step_avg:39.37ms
step:1439/2330 train_time:56642ms step_avg:39.36ms
step:1440/2330 train_time:56698ms step_avg:39.37ms
step:1441/2330 train_time:56720ms step_avg:39.36ms
step:1442/2330 train_time:56776ms step_avg:39.37ms
step:1443/2330 train_time:56799ms step_avg:39.36ms
step:1444/2330 train_time:56854ms step_avg:39.37ms
step:1445/2330 train_time:56877ms step_avg:39.36ms
step:1446/2330 train_time:56933ms step_avg:39.37ms
step:1447/2330 train_time:56955ms step_avg:39.36ms
step:1448/2330 train_time:57011ms step_avg:39.37ms
step:1449/2330 train_time:57033ms step_avg:39.36ms
step:1450/2330 train_time:57089ms step_avg:39.37ms
step:1451/2330 train_time:57112ms step_avg:39.36ms
step:1452/2330 train_time:57168ms step_avg:39.37ms
step:1453/2330 train_time:57190ms step_avg:39.36ms
step:1454/2330 train_time:57246ms step_avg:39.37ms
step:1455/2330 train_time:57268ms step_avg:39.36ms
step:1456/2330 train_time:57324ms step_avg:39.37ms
step:1457/2330 train_time:57346ms step_avg:39.36ms
step:1458/2330 train_time:57402ms step_avg:39.37ms
step:1459/2330 train_time:57424ms step_avg:39.36ms
step:1460/2330 train_time:57479ms step_avg:39.37ms
step:1461/2330 train_time:57501ms step_avg:39.36ms
step:1462/2330 train_time:57557ms step_avg:39.37ms
step:1463/2330 train_time:57579ms step_avg:39.36ms
step:1464/2330 train_time:57634ms step_avg:39.37ms
step:1465/2330 train_time:57656ms step_avg:39.36ms
step:1466/2330 train_time:57712ms step_avg:39.37ms
step:1467/2330 train_time:57735ms step_avg:39.36ms
step:1468/2330 train_time:57791ms step_avg:39.37ms
step:1469/2330 train_time:57814ms step_avg:39.36ms
step:1470/2330 train_time:57871ms step_avg:39.37ms
step:1471/2330 train_time:57894ms step_avg:39.36ms
step:1472/2330 train_time:57950ms step_avg:39.37ms
step:1473/2330 train_time:57972ms step_avg:39.36ms
step:1474/2330 train_time:58028ms step_avg:39.37ms
step:1475/2330 train_time:58050ms step_avg:39.36ms
step:1476/2330 train_time:58106ms step_avg:39.37ms
step:1477/2330 train_time:58129ms step_avg:39.36ms
step:1478/2330 train_time:58186ms step_avg:39.37ms
step:1479/2330 train_time:58207ms step_avg:39.36ms
step:1480/2330 train_time:58263ms step_avg:39.37ms
step:1481/2330 train_time:58285ms step_avg:39.36ms
step:1482/2330 train_time:58341ms step_avg:39.37ms
step:1483/2330 train_time:58362ms step_avg:39.35ms
step:1484/2330 train_time:58418ms step_avg:39.37ms
step:1485/2330 train_time:58440ms step_avg:39.35ms
step:1486/2330 train_time:58495ms step_avg:39.36ms
step:1487/2330 train_time:58517ms step_avg:39.35ms
step:1488/2330 train_time:58573ms step_avg:39.36ms
step:1489/2330 train_time:58595ms step_avg:39.35ms
step:1490/2330 train_time:58651ms step_avg:39.36ms
step:1491/2330 train_time:58673ms step_avg:39.35ms
step:1492/2330 train_time:58729ms step_avg:39.36ms
step:1493/2330 train_time:58752ms step_avg:39.35ms
step:1494/2330 train_time:58808ms step_avg:39.36ms
step:1495/2330 train_time:58831ms step_avg:39.35ms
step:1496/2330 train_time:58886ms step_avg:39.36ms
step:1497/2330 train_time:58909ms step_avg:39.35ms
step:1498/2330 train_time:58964ms step_avg:39.36ms
step:1499/2330 train_time:58986ms step_avg:39.35ms
step:1500/2330 train_time:59041ms step_avg:39.36ms
step:1500/2330 val_loss:5.1619 train_time:59135ms step_avg:39.42ms
step:1501/2330 train_time:59147ms step_avg:39.40ms
step:1502/2330 train_time:59158ms step_avg:39.39ms
step:1503/2330 train_time:59167ms step_avg:39.37ms
step:1504/2330 train_time:59197ms step_avg:39.36ms
step:1505/2330 train_time:59219ms step_avg:39.35ms
step:1506/2330 train_time:59274ms step_avg:39.36ms
step:1507/2330 train_time:59295ms step_avg:39.35ms
step:1508/2330 train_time:59350ms step_avg:39.36ms
step:1509/2330 train_time:59372ms step_avg:39.34ms
step:1510/2330 train_time:59428ms step_avg:39.36ms
step:1511/2330 train_time:59454ms step_avg:39.35ms
step:1512/2330 train_time:59515ms step_avg:39.36ms
step:1513/2330 train_time:59538ms step_avg:39.35ms
step:1514/2330 train_time:59594ms step_avg:39.36ms
step:1515/2330 train_time:59615ms step_avg:39.35ms
step:1516/2330 train_time:59671ms step_avg:39.36ms
step:1517/2330 train_time:59693ms step_avg:39.35ms
step:1518/2330 train_time:59749ms step_avg:39.36ms
step:1519/2330 train_time:59770ms step_avg:39.35ms
step:1520/2330 train_time:59826ms step_avg:39.36ms
step:1521/2330 train_time:59848ms step_avg:39.35ms
step:1522/2330 train_time:59902ms step_avg:39.36ms
step:1523/2330 train_time:59925ms step_avg:39.35ms
step:1524/2330 train_time:59980ms step_avg:39.36ms
step:1525/2330 train_time:60002ms step_avg:39.35ms
step:1526/2330 train_time:60059ms step_avg:39.36ms
step:1527/2330 train_time:60081ms step_avg:39.35ms
step:1528/2330 train_time:60137ms step_avg:39.36ms
step:1529/2330 train_time:60160ms step_avg:39.35ms
step:1530/2330 train_time:60216ms step_avg:39.36ms
step:1531/2330 train_time:60236ms step_avg:39.34ms
step:1532/2330 train_time:60292ms step_avg:39.35ms
step:1533/2330 train_time:60313ms step_avg:39.34ms
step:1534/2330 train_time:60369ms step_avg:39.35ms
step:1535/2330 train_time:60391ms step_avg:39.34ms
step:1536/2330 train_time:60448ms step_avg:39.35ms
step:1537/2330 train_time:60472ms step_avg:39.34ms
step:1538/2330 train_time:60529ms step_avg:39.36ms
step:1539/2330 train_time:60551ms step_avg:39.34ms
step:1540/2330 train_time:60608ms step_avg:39.36ms
step:1541/2330 train_time:60630ms step_avg:39.34ms
step:1542/2330 train_time:60686ms step_avg:39.36ms
step:1543/2330 train_time:60708ms step_avg:39.34ms
step:1544/2330 train_time:60763ms step_avg:39.35ms
step:1545/2330 train_time:60786ms step_avg:39.34ms
step:1546/2330 train_time:60841ms step_avg:39.35ms
step:1547/2330 train_time:60863ms step_avg:39.34ms
step:1548/2330 train_time:60919ms step_avg:39.35ms
step:1549/2330 train_time:60941ms step_avg:39.34ms
step:1550/2330 train_time:60996ms step_avg:39.35ms
step:1551/2330 train_time:61018ms step_avg:39.34ms
step:1552/2330 train_time:61074ms step_avg:39.35ms
step:1553/2330 train_time:61095ms step_avg:39.34ms
step:1554/2330 train_time:61151ms step_avg:39.35ms
step:1555/2330 train_time:61173ms step_avg:39.34ms
step:1556/2330 train_time:61228ms step_avg:39.35ms
step:1557/2330 train_time:61250ms step_avg:39.34ms
step:1558/2330 train_time:61306ms step_avg:39.35ms
step:1559/2330 train_time:61328ms step_avg:39.34ms
step:1560/2330 train_time:61384ms step_avg:39.35ms
step:1561/2330 train_time:61407ms step_avg:39.34ms
step:1562/2330 train_time:61463ms step_avg:39.35ms
step:1563/2330 train_time:61486ms step_avg:39.34ms
step:1564/2330 train_time:61542ms step_avg:39.35ms
step:1565/2330 train_time:61565ms step_avg:39.34ms
step:1566/2330 train_time:61621ms step_avg:39.35ms
step:1567/2330 train_time:61644ms step_avg:39.34ms
step:1568/2330 train_time:61700ms step_avg:39.35ms
step:1569/2330 train_time:61723ms step_avg:39.34ms
step:1570/2330 train_time:61778ms step_avg:39.35ms
step:1571/2330 train_time:61800ms step_avg:39.34ms
step:1572/2330 train_time:61855ms step_avg:39.35ms
step:1573/2330 train_time:61877ms step_avg:39.34ms
step:1574/2330 train_time:61933ms step_avg:39.35ms
step:1575/2330 train_time:61955ms step_avg:39.34ms
step:1576/2330 train_time:62011ms step_avg:39.35ms
step:1577/2330 train_time:62033ms step_avg:39.34ms
step:1578/2330 train_time:62088ms step_avg:39.35ms
step:1579/2330 train_time:62110ms step_avg:39.34ms
step:1580/2330 train_time:62166ms step_avg:39.35ms
step:1581/2330 train_time:62187ms step_avg:39.33ms
step:1582/2330 train_time:62243ms step_avg:39.34ms
step:1583/2330 train_time:62266ms step_avg:39.33ms
step:1584/2330 train_time:62322ms step_avg:39.34ms
step:1585/2330 train_time:62345ms step_avg:39.33ms
step:1586/2330 train_time:62401ms step_avg:39.35ms
step:1587/2330 train_time:62424ms step_avg:39.33ms
step:1588/2330 train_time:62480ms step_avg:39.35ms
step:1589/2330 train_time:62503ms step_avg:39.33ms
step:1590/2330 train_time:62560ms step_avg:39.35ms
step:1591/2330 train_time:62582ms step_avg:39.33ms
step:1592/2330 train_time:62638ms step_avg:39.35ms
step:1593/2330 train_time:62661ms step_avg:39.34ms
step:1594/2330 train_time:62717ms step_avg:39.35ms
step:1595/2330 train_time:62740ms step_avg:39.34ms
step:1596/2330 train_time:62795ms step_avg:39.35ms
step:1597/2330 train_time:62818ms step_avg:39.33ms
step:1598/2330 train_time:62873ms step_avg:39.35ms
step:1599/2330 train_time:62896ms step_avg:39.33ms
step:1600/2330 train_time:62951ms step_avg:39.34ms
step:1601/2330 train_time:62973ms step_avg:39.33ms
step:1602/2330 train_time:63028ms step_avg:39.34ms
step:1603/2330 train_time:63050ms step_avg:39.33ms
step:1604/2330 train_time:63106ms step_avg:39.34ms
step:1605/2330 train_time:63128ms step_avg:39.33ms
step:1606/2330 train_time:63183ms step_avg:39.34ms
step:1607/2330 train_time:63205ms step_avg:39.33ms
step:1608/2330 train_time:63261ms step_avg:39.34ms
step:1609/2330 train_time:63283ms step_avg:39.33ms
step:1610/2330 train_time:63339ms step_avg:39.34ms
step:1611/2330 train_time:63361ms step_avg:39.33ms
step:1612/2330 train_time:63417ms step_avg:39.34ms
step:1613/2330 train_time:63440ms step_avg:39.33ms
step:1614/2330 train_time:63496ms step_avg:39.34ms
step:1615/2330 train_time:63519ms step_avg:39.33ms
step:1616/2330 train_time:63575ms step_avg:39.34ms
step:1617/2330 train_time:63597ms step_avg:39.33ms
step:1618/2330 train_time:63653ms step_avg:39.34ms
step:1619/2330 train_time:63675ms step_avg:39.33ms
step:1620/2330 train_time:63731ms step_avg:39.34ms
step:1621/2330 train_time:63753ms step_avg:39.33ms
step:1622/2330 train_time:63809ms step_avg:39.34ms
step:1623/2330 train_time:63831ms step_avg:39.33ms
step:1624/2330 train_time:63886ms step_avg:39.34ms
step:1625/2330 train_time:63908ms step_avg:39.33ms
step:1626/2330 train_time:63963ms step_avg:39.34ms
step:1627/2330 train_time:63986ms step_avg:39.33ms
step:1628/2330 train_time:64041ms step_avg:39.34ms
step:1629/2330 train_time:64064ms step_avg:39.33ms
step:1630/2330 train_time:64119ms step_avg:39.34ms
step:1631/2330 train_time:64142ms step_avg:39.33ms
step:1632/2330 train_time:64198ms step_avg:39.34ms
step:1633/2330 train_time:64220ms step_avg:39.33ms
step:1634/2330 train_time:64276ms step_avg:39.34ms
step:1635/2330 train_time:64298ms step_avg:39.33ms
step:1636/2330 train_time:64355ms step_avg:39.34ms
step:1637/2330 train_time:64377ms step_avg:39.33ms
step:1638/2330 train_time:64433ms step_avg:39.34ms
step:1639/2330 train_time:64455ms step_avg:39.33ms
step:1640/2330 train_time:64511ms step_avg:39.34ms
step:1641/2330 train_time:64532ms step_avg:39.33ms
step:1642/2330 train_time:64589ms step_avg:39.34ms
step:1643/2330 train_time:64611ms step_avg:39.32ms
step:1644/2330 train_time:64667ms step_avg:39.33ms
step:1645/2330 train_time:64689ms step_avg:39.32ms
step:1646/2330 train_time:64744ms step_avg:39.33ms
step:1647/2330 train_time:64767ms step_avg:39.32ms
step:1648/2330 train_time:64822ms step_avg:39.33ms
step:1649/2330 train_time:64845ms step_avg:39.32ms
step:1650/2330 train_time:64901ms step_avg:39.33ms
step:1651/2330 train_time:64923ms step_avg:39.32ms
step:1652/2330 train_time:64979ms step_avg:39.33ms
step:1653/2330 train_time:65001ms step_avg:39.32ms
step:1654/2330 train_time:65057ms step_avg:39.33ms
step:1655/2330 train_time:65079ms step_avg:39.32ms
step:1656/2330 train_time:65135ms step_avg:39.33ms
step:1657/2330 train_time:65157ms step_avg:39.32ms
step:1658/2330 train_time:65212ms step_avg:39.33ms
step:1659/2330 train_time:65234ms step_avg:39.32ms
step:1660/2330 train_time:65290ms step_avg:39.33ms
step:1661/2330 train_time:65312ms step_avg:39.32ms
step:1662/2330 train_time:65368ms step_avg:39.33ms
step:1663/2330 train_time:65390ms step_avg:39.32ms
step:1664/2330 train_time:65446ms step_avg:39.33ms
step:1665/2330 train_time:65468ms step_avg:39.32ms
step:1666/2330 train_time:65523ms step_avg:39.33ms
step:1667/2330 train_time:65545ms step_avg:39.32ms
step:1668/2330 train_time:65601ms step_avg:39.33ms
step:1669/2330 train_time:65624ms step_avg:39.32ms
step:1670/2330 train_time:65680ms step_avg:39.33ms
step:1671/2330 train_time:65703ms step_avg:39.32ms
step:1672/2330 train_time:65760ms step_avg:39.33ms
step:1673/2330 train_time:65782ms step_avg:39.32ms
step:1674/2330 train_time:65839ms step_avg:39.33ms
step:1675/2330 train_time:65862ms step_avg:39.32ms
step:1676/2330 train_time:65917ms step_avg:39.33ms
step:1677/2330 train_time:65941ms step_avg:39.32ms
step:1678/2330 train_time:65996ms step_avg:39.33ms
step:1679/2330 train_time:66019ms step_avg:39.32ms
step:1680/2330 train_time:66074ms step_avg:39.33ms
step:1681/2330 train_time:66096ms step_avg:39.32ms
step:1682/2330 train_time:66152ms step_avg:39.33ms
step:1683/2330 train_time:66174ms step_avg:39.32ms
step:1684/2330 train_time:66230ms step_avg:39.33ms
step:1685/2330 train_time:66251ms step_avg:39.32ms
step:1686/2330 train_time:66307ms step_avg:39.33ms
step:1687/2330 train_time:66328ms step_avg:39.32ms
step:1688/2330 train_time:66384ms step_avg:39.33ms
step:1689/2330 train_time:66406ms step_avg:39.32ms
step:1690/2330 train_time:66462ms step_avg:39.33ms
step:1691/2330 train_time:66485ms step_avg:39.32ms
step:1692/2330 train_time:66540ms step_avg:39.33ms
step:1693/2330 train_time:66563ms step_avg:39.32ms
step:1694/2330 train_time:66619ms step_avg:39.33ms
step:1695/2330 train_time:66641ms step_avg:39.32ms
step:1696/2330 train_time:66697ms step_avg:39.33ms
step:1697/2330 train_time:66720ms step_avg:39.32ms
step:1698/2330 train_time:66776ms step_avg:39.33ms
step:1699/2330 train_time:66799ms step_avg:39.32ms
step:1700/2330 train_time:66854ms step_avg:39.33ms
step:1701/2330 train_time:66877ms step_avg:39.32ms
step:1702/2330 train_time:66932ms step_avg:39.33ms
step:1703/2330 train_time:66954ms step_avg:39.32ms
step:1704/2330 train_time:67009ms step_avg:39.32ms
step:1705/2330 train_time:67031ms step_avg:39.31ms
step:1706/2330 train_time:67087ms step_avg:39.32ms
step:1707/2330 train_time:67109ms step_avg:39.31ms
step:1708/2330 train_time:67165ms step_avg:39.32ms
step:1709/2330 train_time:67187ms step_avg:39.31ms
step:1710/2330 train_time:67243ms step_avg:39.32ms
step:1711/2330 train_time:67265ms step_avg:39.31ms
step:1712/2330 train_time:67321ms step_avg:39.32ms
step:1713/2330 train_time:67344ms step_avg:39.31ms
step:1714/2330 train_time:67400ms step_avg:39.32ms
step:1715/2330 train_time:67422ms step_avg:39.31ms
step:1716/2330 train_time:67477ms step_avg:39.32ms
step:1717/2330 train_time:67500ms step_avg:39.31ms
step:1718/2330 train_time:67556ms step_avg:39.32ms
step:1719/2330 train_time:67579ms step_avg:39.31ms
step:1720/2330 train_time:67635ms step_avg:39.32ms
step:1721/2330 train_time:67657ms step_avg:39.31ms
step:1722/2330 train_time:67712ms step_avg:39.32ms
step:1723/2330 train_time:67734ms step_avg:39.31ms
step:1724/2330 train_time:67790ms step_avg:39.32ms
step:1725/2330 train_time:67812ms step_avg:39.31ms
step:1726/2330 train_time:67868ms step_avg:39.32ms
step:1727/2330 train_time:67890ms step_avg:39.31ms
step:1728/2330 train_time:67946ms step_avg:39.32ms
step:1729/2330 train_time:67968ms step_avg:39.31ms
step:1730/2330 train_time:68023ms step_avg:39.32ms
step:1731/2330 train_time:68046ms step_avg:39.31ms
step:1732/2330 train_time:68102ms step_avg:39.32ms
step:1733/2330 train_time:68125ms step_avg:39.31ms
step:1734/2330 train_time:68181ms step_avg:39.32ms
step:1735/2330 train_time:68203ms step_avg:39.31ms
step:1736/2330 train_time:68259ms step_avg:39.32ms
step:1737/2330 train_time:68281ms step_avg:39.31ms
step:1738/2330 train_time:68337ms step_avg:39.32ms
step:1739/2330 train_time:68359ms step_avg:39.31ms
step:1740/2330 train_time:68415ms step_avg:39.32ms
step:1741/2330 train_time:68438ms step_avg:39.31ms
step:1742/2330 train_time:68493ms step_avg:39.32ms
step:1743/2330 train_time:68515ms step_avg:39.31ms
step:1744/2330 train_time:68570ms step_avg:39.32ms
step:1745/2330 train_time:68592ms step_avg:39.31ms
step:1746/2330 train_time:68648ms step_avg:39.32ms
step:1747/2330 train_time:68670ms step_avg:39.31ms
step:1748/2330 train_time:68726ms step_avg:39.32ms
step:1749/2330 train_time:68748ms step_avg:39.31ms
step:1750/2330 train_time:68804ms step_avg:39.32ms
step:1750/2330 val_loss:5.1298 train_time:68900ms step_avg:39.37ms
step:1751/2330 train_time:68912ms step_avg:39.36ms
step:1752/2330 train_time:68923ms step_avg:39.34ms
step:1753/2330 train_time:68932ms step_avg:39.32ms
step:1754/2330 train_time:68962ms step_avg:39.32ms
step:1755/2330 train_time:68984ms step_avg:39.31ms
step:1756/2330 train_time:69039ms step_avg:39.32ms
step:1757/2330 train_time:69060ms step_avg:39.31ms
step:1758/2330 train_time:69115ms step_avg:39.31ms
step:1759/2330 train_time:69136ms step_avg:39.30ms
step:1760/2330 train_time:69191ms step_avg:39.31ms
step:1761/2330 train_time:69214ms step_avg:39.30ms
step:1762/2330 train_time:69275ms step_avg:39.32ms
step:1763/2330 train_time:69301ms step_avg:39.31ms
step:1764/2330 train_time:69359ms step_avg:39.32ms
step:1765/2330 train_time:69380ms step_avg:39.31ms
step:1766/2330 train_time:69436ms step_avg:39.32ms
step:1767/2330 train_time:69458ms step_avg:39.31ms
step:1768/2330 train_time:69514ms step_avg:39.32ms
step:1769/2330 train_time:69536ms step_avg:39.31ms
step:1770/2330 train_time:69591ms step_avg:39.32ms
step:1771/2330 train_time:69613ms step_avg:39.31ms
step:1772/2330 train_time:69668ms step_avg:39.32ms
step:1773/2330 train_time:69690ms step_avg:39.31ms
step:1774/2330 train_time:69745ms step_avg:39.32ms
step:1775/2330 train_time:69767ms step_avg:39.31ms
step:1776/2330 train_time:69823ms step_avg:39.31ms
step:1777/2330 train_time:69846ms step_avg:39.31ms
step:1778/2330 train_time:69902ms step_avg:39.32ms
step:1779/2330 train_time:69925ms step_avg:39.31ms
step:1780/2330 train_time:69980ms step_avg:39.31ms
step:1781/2330 train_time:70002ms step_avg:39.30ms
step:1782/2330 train_time:70057ms step_avg:39.31ms
step:1783/2330 train_time:70079ms step_avg:39.30ms
step:1784/2330 train_time:70134ms step_avg:39.31ms
step:1785/2330 train_time:70156ms step_avg:39.30ms
step:1786/2330 train_time:70213ms step_avg:39.31ms
step:1787/2330 train_time:70235ms step_avg:39.30ms
step:1788/2330 train_time:70292ms step_avg:39.31ms
step:1789/2330 train_time:70315ms step_avg:39.30ms
step:1790/2330 train_time:70371ms step_avg:39.31ms
step:1791/2330 train_time:70394ms step_avg:39.30ms
step:1792/2330 train_time:70449ms step_avg:39.31ms
step:1793/2330 train_time:70471ms step_avg:39.30ms
step:1794/2330 train_time:70527ms step_avg:39.31ms
step:1795/2330 train_time:70549ms step_avg:39.30ms
step:1796/2330 train_time:70605ms step_avg:39.31ms
step:1797/2330 train_time:70628ms step_avg:39.30ms
step:1798/2330 train_time:70683ms step_avg:39.31ms
step:1799/2330 train_time:70706ms step_avg:39.30ms
step:1800/2330 train_time:70762ms step_avg:39.31ms
step:1801/2330 train_time:70784ms step_avg:39.30ms
step:1802/2330 train_time:70840ms step_avg:39.31ms
step:1803/2330 train_time:70861ms step_avg:39.30ms
step:1804/2330 train_time:70917ms step_avg:39.31ms
step:1805/2330 train_time:70939ms step_avg:39.30ms
step:1806/2330 train_time:70994ms step_avg:39.31ms
step:1807/2330 train_time:71016ms step_avg:39.30ms
step:1808/2330 train_time:71071ms step_avg:39.31ms
step:1809/2330 train_time:71094ms step_avg:39.30ms
step:1810/2330 train_time:71149ms step_avg:39.31ms
step:1811/2330 train_time:71172ms step_avg:39.30ms
step:1812/2330 train_time:71228ms step_avg:39.31ms
step:1813/2330 train_time:71251ms step_avg:39.30ms
step:1814/2330 train_time:71307ms step_avg:39.31ms
step:1815/2330 train_time:71330ms step_avg:39.30ms
step:1816/2330 train_time:71387ms step_avg:39.31ms
step:1817/2330 train_time:71410ms step_avg:39.30ms
step:1818/2330 train_time:71466ms step_avg:39.31ms
step:1819/2330 train_time:71488ms step_avg:39.30ms
step:1820/2330 train_time:71543ms step_avg:39.31ms
step:1821/2330 train_time:71565ms step_avg:39.30ms
step:1822/2330 train_time:71620ms step_avg:39.31ms
step:1823/2330 train_time:71642ms step_avg:39.30ms
step:1824/2330 train_time:71698ms step_avg:39.31ms
step:1825/2330 train_time:71720ms step_avg:39.30ms
step:1826/2330 train_time:71775ms step_avg:39.31ms
step:1827/2330 train_time:71797ms step_avg:39.30ms
step:1828/2330 train_time:71853ms step_avg:39.31ms
step:1829/2330 train_time:71875ms step_avg:39.30ms
step:1830/2330 train_time:71930ms step_avg:39.31ms
step:1831/2330 train_time:71952ms step_avg:39.30ms
step:1832/2330 train_time:72007ms step_avg:39.31ms
step:1833/2330 train_time:72030ms step_avg:39.30ms
step:1834/2330 train_time:72085ms step_avg:39.30ms
step:1835/2330 train_time:72108ms step_avg:39.30ms
step:1836/2330 train_time:72163ms step_avg:39.30ms
step:1837/2330 train_time:72186ms step_avg:39.30ms
step:1838/2330 train_time:72242ms step_avg:39.30ms
step:1839/2330 train_time:72265ms step_avg:39.30ms
step:1840/2330 train_time:72321ms step_avg:39.31ms
step:1841/2330 train_time:72344ms step_avg:39.30ms
step:1842/2330 train_time:72400ms step_avg:39.30ms
step:1843/2330 train_time:72421ms step_avg:39.30ms
step:1844/2330 train_time:72477ms step_avg:39.30ms
step:1845/2330 train_time:72499ms step_avg:39.30ms
step:1846/2330 train_time:72555ms step_avg:39.30ms
step:1847/2330 train_time:72577ms step_avg:39.29ms
step:1848/2330 train_time:72632ms step_avg:39.30ms
step:1849/2330 train_time:72654ms step_avg:39.29ms
step:1850/2330 train_time:72709ms step_avg:39.30ms
step:1851/2330 train_time:72731ms step_avg:39.29ms
step:1852/2330 train_time:72787ms step_avg:39.30ms
step:1853/2330 train_time:72811ms step_avg:39.29ms
step:1854/2330 train_time:72867ms step_avg:39.30ms
step:1855/2330 train_time:72889ms step_avg:39.29ms
step:1856/2330 train_time:72945ms step_avg:39.30ms
step:1857/2330 train_time:72967ms step_avg:39.29ms
step:1858/2330 train_time:73023ms step_avg:39.30ms
step:1859/2330 train_time:73045ms step_avg:39.29ms
step:1860/2330 train_time:73101ms step_avg:39.30ms
step:1861/2330 train_time:73124ms step_avg:39.29ms
step:1862/2330 train_time:73179ms step_avg:39.30ms
step:1863/2330 train_time:73201ms step_avg:39.29ms
step:1864/2330 train_time:73257ms step_avg:39.30ms
step:1865/2330 train_time:73279ms step_avg:39.29ms
step:1866/2330 train_time:73335ms step_avg:39.30ms
step:1867/2330 train_time:73357ms step_avg:39.29ms
step:1868/2330 train_time:73414ms step_avg:39.30ms
step:1869/2330 train_time:73436ms step_avg:39.29ms
step:1870/2330 train_time:73491ms step_avg:39.30ms
step:1871/2330 train_time:73513ms step_avg:39.29ms
step:1872/2330 train_time:73569ms step_avg:39.30ms
step:1873/2330 train_time:73592ms step_avg:39.29ms
step:1874/2330 train_time:73647ms step_avg:39.30ms
step:1875/2330 train_time:73669ms step_avg:39.29ms
step:1876/2330 train_time:73725ms step_avg:39.30ms
step:1877/2330 train_time:73747ms step_avg:39.29ms
step:1878/2330 train_time:73803ms step_avg:39.30ms
step:1879/2330 train_time:73825ms step_avg:39.29ms
step:1880/2330 train_time:73880ms step_avg:39.30ms
step:1881/2330 train_time:73902ms step_avg:39.29ms
step:1882/2330 train_time:73957ms step_avg:39.30ms
step:1883/2330 train_time:73979ms step_avg:39.29ms
step:1884/2330 train_time:74035ms step_avg:39.30ms
step:1885/2330 train_time:74057ms step_avg:39.29ms
step:1886/2330 train_time:74112ms step_avg:39.30ms
step:1887/2330 train_time:74135ms step_avg:39.29ms
step:1888/2330 train_time:74190ms step_avg:39.30ms
step:1889/2330 train_time:74213ms step_avg:39.29ms
step:1890/2330 train_time:74268ms step_avg:39.30ms
step:1891/2330 train_time:74291ms step_avg:39.29ms
step:1892/2330 train_time:74347ms step_avg:39.30ms
step:1893/2330 train_time:74370ms step_avg:39.29ms
step:1894/2330 train_time:74427ms step_avg:39.30ms
step:1895/2330 train_time:74450ms step_avg:39.29ms
step:1896/2330 train_time:74506ms step_avg:39.30ms
step:1897/2330 train_time:74529ms step_avg:39.29ms
step:1898/2330 train_time:74585ms step_avg:39.30ms
step:1899/2330 train_time:74607ms step_avg:39.29ms
step:1900/2330 train_time:74663ms step_avg:39.30ms
step:1901/2330 train_time:74686ms step_avg:39.29ms
step:1902/2330 train_time:74742ms step_avg:39.30ms
step:1903/2330 train_time:74764ms step_avg:39.29ms
step:1904/2330 train_time:74820ms step_avg:39.30ms
step:1905/2330 train_time:74842ms step_avg:39.29ms
step:1906/2330 train_time:74897ms step_avg:39.30ms
step:1907/2330 train_time:74919ms step_avg:39.29ms
step:1908/2330 train_time:74974ms step_avg:39.29ms
step:1909/2330 train_time:74996ms step_avg:39.29ms
step:1910/2330 train_time:75052ms step_avg:39.29ms
step:1911/2330 train_time:75073ms step_avg:39.28ms
step:1912/2330 train_time:75129ms step_avg:39.29ms
step:1913/2330 train_time:75151ms step_avg:39.28ms
step:1914/2330 train_time:75207ms step_avg:39.29ms
step:1915/2330 train_time:75230ms step_avg:39.28ms
step:1916/2330 train_time:75286ms step_avg:39.29ms
step:1917/2330 train_time:75308ms step_avg:39.28ms
step:1918/2330 train_time:75364ms step_avg:39.29ms
step:1919/2330 train_time:75387ms step_avg:39.28ms
step:1920/2330 train_time:75443ms step_avg:39.29ms
step:1921/2330 train_time:75465ms step_avg:39.28ms
step:1922/2330 train_time:75521ms step_avg:39.29ms
step:1923/2330 train_time:75543ms step_avg:39.28ms
step:1924/2330 train_time:75599ms step_avg:39.29ms
step:1925/2330 train_time:75620ms step_avg:39.28ms
step:1926/2330 train_time:75676ms step_avg:39.29ms
step:1927/2330 train_time:75698ms step_avg:39.28ms
step:1928/2330 train_time:75755ms step_avg:39.29ms
step:1929/2330 train_time:75777ms step_avg:39.28ms
step:1930/2330 train_time:75832ms step_avg:39.29ms
step:1931/2330 train_time:75854ms step_avg:39.28ms
step:1932/2330 train_time:75910ms step_avg:39.29ms
step:1933/2330 train_time:75933ms step_avg:39.28ms
step:1934/2330 train_time:75988ms step_avg:39.29ms
step:1935/2330 train_time:76011ms step_avg:39.28ms
step:1936/2330 train_time:76066ms step_avg:39.29ms
step:1937/2330 train_time:76088ms step_avg:39.28ms
step:1938/2330 train_time:76144ms step_avg:39.29ms
step:1939/2330 train_time:76166ms step_avg:39.28ms
step:1940/2330 train_time:76222ms step_avg:39.29ms
step:1941/2330 train_time:76244ms step_avg:39.28ms
step:1942/2330 train_time:76299ms step_avg:39.29ms
step:1943/2330 train_time:76321ms step_avg:39.28ms
step:1944/2330 train_time:76377ms step_avg:39.29ms
step:1945/2330 train_time:76399ms step_avg:39.28ms
step:1946/2330 train_time:76455ms step_avg:39.29ms
step:1947/2330 train_time:76476ms step_avg:39.28ms
step:1948/2330 train_time:76533ms step_avg:39.29ms
step:1949/2330 train_time:76555ms step_avg:39.28ms
step:1950/2330 train_time:76610ms step_avg:39.29ms
step:1951/2330 train_time:76633ms step_avg:39.28ms
step:1952/2330 train_time:76690ms step_avg:39.29ms
step:1953/2330 train_time:76712ms step_avg:39.28ms
step:1954/2330 train_time:76768ms step_avg:39.29ms
step:1955/2330 train_time:76790ms step_avg:39.28ms
step:1956/2330 train_time:76846ms step_avg:39.29ms
step:1957/2330 train_time:76869ms step_avg:39.28ms
step:1958/2330 train_time:76925ms step_avg:39.29ms
step:1959/2330 train_time:76947ms step_avg:39.28ms
step:1960/2330 train_time:77003ms step_avg:39.29ms
step:1961/2330 train_time:77025ms step_avg:39.28ms
step:1962/2330 train_time:77081ms step_avg:39.29ms
step:1963/2330 train_time:77103ms step_avg:39.28ms
step:1964/2330 train_time:77159ms step_avg:39.29ms
step:1965/2330 train_time:77181ms step_avg:39.28ms
step:1966/2330 train_time:77236ms step_avg:39.29ms
step:1967/2330 train_time:77258ms step_avg:39.28ms
step:1968/2330 train_time:77314ms step_avg:39.29ms
step:1969/2330 train_time:77336ms step_avg:39.28ms
step:1970/2330 train_time:77392ms step_avg:39.29ms
step:1971/2330 train_time:77413ms step_avg:39.28ms
step:1972/2330 train_time:77469ms step_avg:39.28ms
step:1973/2330 train_time:77491ms step_avg:39.28ms
step:1974/2330 train_time:77547ms step_avg:39.28ms
step:1975/2330 train_time:77570ms step_avg:39.28ms
step:1976/2330 train_time:77625ms step_avg:39.28ms
step:1977/2330 train_time:77648ms step_avg:39.28ms
step:1978/2330 train_time:77704ms step_avg:39.28ms
step:1979/2330 train_time:77727ms step_avg:39.28ms
step:1980/2330 train_time:77783ms step_avg:39.28ms
step:1981/2330 train_time:77805ms step_avg:39.28ms
step:1982/2330 train_time:77861ms step_avg:39.28ms
step:1983/2330 train_time:77883ms step_avg:39.28ms
step:1984/2330 train_time:77939ms step_avg:39.28ms
step:1985/2330 train_time:77960ms step_avg:39.27ms
step:1986/2330 train_time:78015ms step_avg:39.28ms
step:1987/2330 train_time:78037ms step_avg:39.27ms
step:1988/2330 train_time:78094ms step_avg:39.28ms
step:1989/2330 train_time:78115ms step_avg:39.27ms
step:1990/2330 train_time:78171ms step_avg:39.28ms
step:1991/2330 train_time:78193ms step_avg:39.27ms
step:1992/2330 train_time:78248ms step_avg:39.28ms
step:1993/2330 train_time:78271ms step_avg:39.27ms
step:1994/2330 train_time:78327ms step_avg:39.28ms
step:1995/2330 train_time:78350ms step_avg:39.27ms
step:1996/2330 train_time:78405ms step_avg:39.28ms
step:1997/2330 train_time:78428ms step_avg:39.27ms
step:1998/2330 train_time:78484ms step_avg:39.28ms
step:1999/2330 train_time:78506ms step_avg:39.27ms
step:2000/2330 train_time:78562ms step_avg:39.28ms
step:2000/2330 val_loss:5.0990 train_time:78657ms step_avg:39.33ms
step:2001/2330 train_time:78669ms step_avg:39.31ms
step:2002/2330 train_time:78680ms step_avg:39.30ms
step:2003/2330 train_time:78690ms step_avg:39.29ms
step:2004/2330 train_time:78719ms step_avg:39.28ms
step:2005/2330 train_time:78740ms step_avg:39.27ms
step:2006/2330 train_time:78795ms step_avg:39.28ms
step:2007/2330 train_time:78817ms step_avg:39.27ms
step:2008/2330 train_time:78873ms step_avg:39.28ms
step:2009/2330 train_time:78896ms step_avg:39.27ms
step:2010/2330 train_time:78951ms step_avg:39.28ms
step:2011/2330 train_time:78974ms step_avg:39.27ms
step:2012/2330 train_time:79034ms step_avg:39.28ms
step:2013/2330 train_time:79056ms step_avg:39.27ms
step:2014/2330 train_time:79112ms step_avg:39.28ms
step:2015/2330 train_time:79134ms step_avg:39.27ms
step:2016/2330 train_time:79189ms step_avg:39.28ms
step:2017/2330 train_time:79211ms step_avg:39.27ms
step:2018/2330 train_time:79267ms step_avg:39.28ms
step:2019/2330 train_time:79289ms step_avg:39.27ms
step:2020/2330 train_time:79345ms step_avg:39.28ms
step:2021/2330 train_time:79367ms step_avg:39.27ms
step:2022/2330 train_time:79421ms step_avg:39.28ms
step:2023/2330 train_time:79443ms step_avg:39.27ms
step:2024/2330 train_time:79499ms step_avg:39.28ms
step:2025/2330 train_time:79521ms step_avg:39.27ms
step:2026/2330 train_time:79577ms step_avg:39.28ms
step:2027/2330 train_time:79599ms step_avg:39.27ms
step:2028/2330 train_time:79654ms step_avg:39.28ms
step:2029/2330 train_time:79676ms step_avg:39.27ms
step:2030/2330 train_time:79732ms step_avg:39.28ms
step:2031/2330 train_time:79753ms step_avg:39.27ms
step:2032/2330 train_time:79808ms step_avg:39.28ms
step:2033/2330 train_time:79830ms step_avg:39.27ms
step:2034/2330 train_time:79886ms step_avg:39.28ms
step:2035/2330 train_time:79908ms step_avg:39.27ms
step:2036/2330 train_time:79964ms step_avg:39.27ms
step:2037/2330 train_time:79986ms step_avg:39.27ms
step:2038/2330 train_time:80042ms step_avg:39.27ms
step:2039/2330 train_time:80065ms step_avg:39.27ms
step:2040/2330 train_time:80121ms step_avg:39.27ms
step:2041/2330 train_time:80144ms step_avg:39.27ms
step:2042/2330 train_time:80201ms step_avg:39.28ms
step:2043/2330 train_time:80224ms step_avg:39.27ms
step:2044/2330 train_time:80281ms step_avg:39.28ms
step:2045/2330 train_time:80303ms step_avg:39.27ms
step:2046/2330 train_time:80359ms step_avg:39.28ms
step:2047/2330 train_time:80381ms step_avg:39.27ms
step:2048/2330 train_time:80437ms step_avg:39.28ms
step:2049/2330 train_time:80459ms step_avg:39.27ms
step:2050/2330 train_time:80515ms step_avg:39.28ms
step:2051/2330 train_time:80537ms step_avg:39.27ms
step:2052/2330 train_time:80592ms step_avg:39.28ms
step:2053/2330 train_time:80615ms step_avg:39.27ms
step:2054/2330 train_time:80670ms step_avg:39.27ms
step:2055/2330 train_time:80692ms step_avg:39.27ms
step:2056/2330 train_time:80747ms step_avg:39.27ms
step:2057/2330 train_time:80769ms step_avg:39.27ms
step:2058/2330 train_time:80825ms step_avg:39.27ms
step:2059/2330 train_time:80847ms step_avg:39.27ms
step:2060/2330 train_time:80903ms step_avg:39.27ms
step:2061/2330 train_time:80926ms step_avg:39.27ms
step:2062/2330 train_time:80981ms step_avg:39.27ms
step:2063/2330 train_time:81004ms step_avg:39.27ms
step:2064/2330 train_time:81060ms step_avg:39.27ms
step:2065/2330 train_time:81083ms step_avg:39.27ms
step:2066/2330 train_time:81139ms step_avg:39.27ms
step:2067/2330 train_time:81162ms step_avg:39.27ms
step:2068/2330 train_time:81220ms step_avg:39.27ms
step:2069/2330 train_time:81243ms step_avg:39.27ms
step:2070/2330 train_time:81299ms step_avg:39.27ms
step:2071/2330 train_time:81321ms step_avg:39.27ms
step:2072/2330 train_time:81378ms step_avg:39.27ms
step:2073/2330 train_time:81401ms step_avg:39.27ms
step:2074/2330 train_time:81457ms step_avg:39.28ms
step:2075/2330 train_time:81479ms step_avg:39.27ms
step:2076/2330 train_time:81535ms step_avg:39.28ms
step:2077/2330 train_time:81558ms step_avg:39.27ms
step:2078/2330 train_time:81613ms step_avg:39.27ms
step:2079/2330 train_time:81636ms step_avg:39.27ms
step:2080/2330 train_time:81691ms step_avg:39.27ms
step:2081/2330 train_time:81712ms step_avg:39.27ms
step:2082/2330 train_time:81768ms step_avg:39.27ms
step:2083/2330 train_time:81789ms step_avg:39.27ms
step:2084/2330 train_time:81846ms step_avg:39.27ms
step:2085/2330 train_time:81867ms step_avg:39.26ms
step:2086/2330 train_time:81923ms step_avg:39.27ms
step:2087/2330 train_time:81945ms step_avg:39.26ms
step:2088/2330 train_time:82000ms step_avg:39.27ms
step:2089/2330 train_time:82023ms step_avg:39.26ms
step:2090/2330 train_time:82079ms step_avg:39.27ms
step:2091/2330 train_time:82101ms step_avg:39.26ms
step:2092/2330 train_time:82158ms step_avg:39.27ms
step:2093/2330 train_time:82181ms step_avg:39.26ms
step:2094/2330 train_time:82237ms step_avg:39.27ms
step:2095/2330 train_time:82259ms step_avg:39.26ms
step:2096/2330 train_time:82315ms step_avg:39.27ms
step:2097/2330 train_time:82336ms step_avg:39.26ms
step:2098/2330 train_time:82392ms step_avg:39.27ms
step:2099/2330 train_time:82414ms step_avg:39.26ms
step:2100/2330 train_time:82469ms step_avg:39.27ms
step:2101/2330 train_time:82491ms step_avg:39.26ms
step:2102/2330 train_time:82547ms step_avg:39.27ms
step:2103/2330 train_time:82570ms step_avg:39.26ms
step:2104/2330 train_time:82626ms step_avg:39.27ms
step:2105/2330 train_time:82648ms step_avg:39.26ms
step:2106/2330 train_time:82703ms step_avg:39.27ms
step:2107/2330 train_time:82726ms step_avg:39.26ms
step:2108/2330 train_time:82781ms step_avg:39.27ms
step:2109/2330 train_time:82804ms step_avg:39.26ms
step:2110/2330 train_time:82860ms step_avg:39.27ms
step:2111/2330 train_time:82882ms step_avg:39.26ms
step:2112/2330 train_time:82938ms step_avg:39.27ms
step:2113/2330 train_time:82960ms step_avg:39.26ms
step:2114/2330 train_time:83016ms step_avg:39.27ms
step:2115/2330 train_time:83038ms step_avg:39.26ms
step:2116/2330 train_time:83094ms step_avg:39.27ms
step:2117/2330 train_time:83116ms step_avg:39.26ms
step:2118/2330 train_time:83172ms step_avg:39.27ms
step:2119/2330 train_time:83194ms step_avg:39.26ms
step:2120/2330 train_time:83250ms step_avg:39.27ms
step:2121/2330 train_time:83272ms step_avg:39.26ms
step:2122/2330 train_time:83328ms step_avg:39.27ms
step:2123/2330 train_time:83349ms step_avg:39.26ms
step:2124/2330 train_time:83406ms step_avg:39.27ms
step:2125/2330 train_time:83428ms step_avg:39.26ms
step:2126/2330 train_time:83484ms step_avg:39.27ms
step:2127/2330 train_time:83507ms step_avg:39.26ms
step:2128/2330 train_time:83562ms step_avg:39.27ms
step:2129/2330 train_time:83585ms step_avg:39.26ms
step:2130/2330 train_time:83640ms step_avg:39.27ms
step:2131/2330 train_time:83663ms step_avg:39.26ms
step:2132/2330 train_time:83719ms step_avg:39.27ms
step:2133/2330 train_time:83741ms step_avg:39.26ms
step:2134/2330 train_time:83797ms step_avg:39.27ms
step:2135/2330 train_time:83819ms step_avg:39.26ms
step:2136/2330 train_time:83875ms step_avg:39.27ms
step:2137/2330 train_time:83897ms step_avg:39.26ms
step:2138/2330 train_time:83953ms step_avg:39.27ms
step:2139/2330 train_time:83974ms step_avg:39.26ms
step:2140/2330 train_time:84030ms step_avg:39.27ms
step:2141/2330 train_time:84052ms step_avg:39.26ms
step:2142/2330 train_time:84108ms step_avg:39.27ms
step:2143/2330 train_time:84130ms step_avg:39.26ms
step:2144/2330 train_time:84186ms step_avg:39.27ms
step:2145/2330 train_time:84209ms step_avg:39.26ms
step:2146/2330 train_time:84264ms step_avg:39.27ms
step:2147/2330 train_time:84286ms step_avg:39.26ms
step:2148/2330 train_time:84341ms step_avg:39.27ms
step:2149/2330 train_time:84364ms step_avg:39.26ms
step:2150/2330 train_time:84420ms step_avg:39.26ms
step:2151/2330 train_time:84443ms step_avg:39.26ms
step:2152/2330 train_time:84499ms step_avg:39.27ms
step:2153/2330 train_time:84521ms step_avg:39.26ms
step:2154/2330 train_time:84578ms step_avg:39.27ms
step:2155/2330 train_time:84600ms step_avg:39.26ms
step:2156/2330 train_time:84656ms step_avg:39.27ms
step:2157/2330 train_time:84678ms step_avg:39.26ms
step:2158/2330 train_time:84734ms step_avg:39.27ms
step:2159/2330 train_time:84757ms step_avg:39.26ms
step:2160/2330 train_time:84812ms step_avg:39.26ms
step:2161/2330 train_time:84834ms step_avg:39.26ms
step:2162/2330 train_time:84890ms step_avg:39.26ms
step:2163/2330 train_time:84912ms step_avg:39.26ms
step:2164/2330 train_time:84967ms step_avg:39.26ms
step:2165/2330 train_time:84989ms step_avg:39.26ms
step:2166/2330 train_time:85046ms step_avg:39.26ms
step:2167/2330 train_time:85067ms step_avg:39.26ms
step:2168/2330 train_time:85123ms step_avg:39.26ms
step:2169/2330 train_time:85145ms step_avg:39.26ms
step:2170/2330 train_time:85200ms step_avg:39.26ms
step:2171/2330 train_time:85223ms step_avg:39.26ms
step:2172/2330 train_time:85279ms step_avg:39.26ms
step:2173/2330 train_time:85302ms step_avg:39.26ms
step:2174/2330 train_time:85357ms step_avg:39.26ms
step:2175/2330 train_time:85380ms step_avg:39.26ms
step:2176/2330 train_time:85435ms step_avg:39.26ms
step:2177/2330 train_time:85458ms step_avg:39.26ms
step:2178/2330 train_time:85515ms step_avg:39.26ms
step:2179/2330 train_time:85538ms step_avg:39.26ms
step:2180/2330 train_time:85593ms step_avg:39.26ms
step:2181/2330 train_time:85615ms step_avg:39.26ms
step:2182/2330 train_time:85671ms step_avg:39.26ms
step:2183/2330 train_time:85692ms step_avg:39.25ms
step:2184/2330 train_time:85748ms step_avg:39.26ms
step:2185/2330 train_time:85769ms step_avg:39.25ms
step:2186/2330 train_time:85825ms step_avg:39.26ms
step:2187/2330 train_time:85846ms step_avg:39.25ms
step:2188/2330 train_time:85902ms step_avg:39.26ms
step:2189/2330 train_time:85925ms step_avg:39.25ms
step:2190/2330 train_time:85980ms step_avg:39.26ms
step:2191/2330 train_time:86003ms step_avg:39.25ms
step:2192/2330 train_time:86059ms step_avg:39.26ms
step:2193/2330 train_time:86081ms step_avg:39.25ms
step:2194/2330 train_time:86136ms step_avg:39.26ms
step:2195/2330 train_time:86158ms step_avg:39.25ms
step:2196/2330 train_time:86215ms step_avg:39.26ms
step:2197/2330 train_time:86237ms step_avg:39.25ms
step:2198/2330 train_time:86293ms step_avg:39.26ms
step:2199/2330 train_time:86315ms step_avg:39.25ms
step:2200/2330 train_time:86371ms step_avg:39.26ms
step:2201/2330 train_time:86393ms step_avg:39.25ms
step:2202/2330 train_time:86448ms step_avg:39.26ms
step:2203/2330 train_time:86470ms step_avg:39.25ms
step:2204/2330 train_time:86526ms step_avg:39.26ms
step:2205/2330 train_time:86548ms step_avg:39.25ms
step:2206/2330 train_time:86604ms step_avg:39.26ms
step:2207/2330 train_time:86626ms step_avg:39.25ms
step:2208/2330 train_time:86681ms step_avg:39.26ms
step:2209/2330 train_time:86703ms step_avg:39.25ms
step:2210/2330 train_time:86759ms step_avg:39.26ms
step:2211/2330 train_time:86781ms step_avg:39.25ms
step:2212/2330 train_time:86837ms step_avg:39.26ms
step:2213/2330 train_time:86859ms step_avg:39.25ms
step:2214/2330 train_time:86915ms step_avg:39.26ms
step:2215/2330 train_time:86938ms step_avg:39.25ms
step:2216/2330 train_time:86994ms step_avg:39.26ms
step:2217/2330 train_time:87016ms step_avg:39.25ms
step:2218/2330 train_time:87072ms step_avg:39.26ms
step:2219/2330 train_time:87094ms step_avg:39.25ms
step:2220/2330 train_time:87149ms step_avg:39.26ms
step:2221/2330 train_time:87171ms step_avg:39.25ms
step:2222/2330 train_time:87227ms step_avg:39.26ms
step:2223/2330 train_time:87249ms step_avg:39.25ms
step:2224/2330 train_time:87305ms step_avg:39.26ms
step:2225/2330 train_time:87327ms step_avg:39.25ms
step:2226/2330 train_time:87382ms step_avg:39.26ms
step:2227/2330 train_time:87405ms step_avg:39.25ms
step:2228/2330 train_time:87460ms step_avg:39.26ms
step:2229/2330 train_time:87483ms step_avg:39.25ms
step:2230/2330 train_time:87539ms step_avg:39.26ms
step:2231/2330 train_time:87562ms step_avg:39.25ms
step:2232/2330 train_time:87618ms step_avg:39.26ms
step:2233/2330 train_time:87640ms step_avg:39.25ms
step:2234/2330 train_time:87696ms step_avg:39.26ms
step:2235/2330 train_time:87718ms step_avg:39.25ms
step:2236/2330 train_time:87774ms step_avg:39.25ms
step:2237/2330 train_time:87796ms step_avg:39.25ms
step:2238/2330 train_time:87852ms step_avg:39.25ms
step:2239/2330 train_time:87873ms step_avg:39.25ms
step:2240/2330 train_time:87929ms step_avg:39.25ms
step:2241/2330 train_time:87951ms step_avg:39.25ms
step:2242/2330 train_time:88006ms step_avg:39.25ms
step:2243/2330 train_time:88028ms step_avg:39.25ms
step:2244/2330 train_time:88084ms step_avg:39.25ms
step:2245/2330 train_time:88106ms step_avg:39.25ms
step:2246/2330 train_time:88162ms step_avg:39.25ms
step:2247/2330 train_time:88184ms step_avg:39.25ms
step:2248/2330 train_time:88240ms step_avg:39.25ms
step:2249/2330 train_time:88262ms step_avg:39.24ms
step:2250/2330 train_time:88318ms step_avg:39.25ms
step:2250/2330 val_loss:5.0737 train_time:88413ms step_avg:39.29ms
step:2251/2330 train_time:88426ms step_avg:39.28ms
step:2252/2330 train_time:88437ms step_avg:39.27ms
step:2253/2330 train_time:88448ms step_avg:39.26ms
step:2254/2330 train_time:88475ms step_avg:39.25ms
step:2255/2330 train_time:88497ms step_avg:39.24ms
step:2256/2330 train_time:88552ms step_avg:39.25ms
step:2257/2330 train_time:88573ms step_avg:39.24ms
step:2258/2330 train_time:88628ms step_avg:39.25ms
step:2259/2330 train_time:88650ms step_avg:39.24ms
step:2260/2330 train_time:88705ms step_avg:39.25ms
step:2261/2330 train_time:88730ms step_avg:39.24ms
step:2262/2330 train_time:88788ms step_avg:39.25ms
step:2263/2330 train_time:88811ms step_avg:39.24ms
step:2264/2330 train_time:88867ms step_avg:39.25ms
step:2265/2330 train_time:88890ms step_avg:39.24ms
step:2266/2330 train_time:88946ms step_avg:39.25ms
step:2267/2330 train_time:88968ms step_avg:39.24ms
step:2268/2330 train_time:89024ms step_avg:39.25ms
step:2269/2330 train_time:89046ms step_avg:39.24ms
step:2270/2330 train_time:89101ms step_avg:39.25ms
step:2271/2330 train_time:89124ms step_avg:39.24ms
step:2272/2330 train_time:89178ms step_avg:39.25ms
step:2273/2330 train_time:89200ms step_avg:39.24ms
step:2274/2330 train_time:89255ms step_avg:39.25ms
step:2275/2330 train_time:89277ms step_avg:39.24ms
step:2276/2330 train_time:89333ms step_avg:39.25ms
step:2277/2330 train_time:89356ms step_avg:39.24ms
step:2278/2330 train_time:89411ms step_avg:39.25ms
step:2279/2330 train_time:89434ms step_avg:39.24ms
step:2280/2330 train_time:89489ms step_avg:39.25ms
step:2281/2330 train_time:89511ms step_avg:39.24ms
step:2282/2330 train_time:89567ms step_avg:39.25ms
step:2283/2330 train_time:89589ms step_avg:39.24ms
step:2284/2330 train_time:89644ms step_avg:39.25ms
step:2285/2330 train_time:89667ms step_avg:39.24ms
step:2286/2330 train_time:89723ms step_avg:39.25ms
step:2287/2330 train_time:89745ms step_avg:39.24ms
step:2288/2330 train_time:89801ms step_avg:39.25ms
step:2289/2330 train_time:89823ms step_avg:39.24ms
step:2290/2330 train_time:89879ms step_avg:39.25ms
step:2291/2330 train_time:89901ms step_avg:39.24ms
step:2292/2330 train_time:89957ms step_avg:39.25ms
step:2293/2330 train_time:89979ms step_avg:39.24ms
step:2294/2330 train_time:90035ms step_avg:39.25ms
step:2295/2330 train_time:90057ms step_avg:39.24ms
step:2296/2330 train_time:90113ms step_avg:39.25ms
step:2297/2330 train_time:90135ms step_avg:39.24ms
step:2298/2330 train_time:90190ms step_avg:39.25ms
step:2299/2330 train_time:90212ms step_avg:39.24ms
step:2300/2330 train_time:90268ms step_avg:39.25ms
step:2301/2330 train_time:90291ms step_avg:39.24ms
step:2302/2330 train_time:90346ms step_avg:39.25ms
step:2303/2330 train_time:90369ms step_avg:39.24ms
step:2304/2330 train_time:90424ms step_avg:39.25ms
step:2305/2330 train_time:90447ms step_avg:39.24ms
step:2306/2330 train_time:90503ms step_avg:39.25ms
step:2307/2330 train_time:90525ms step_avg:39.24ms
step:2308/2330 train_time:90581ms step_avg:39.25ms
step:2309/2330 train_time:90603ms step_avg:39.24ms
step:2310/2330 train_time:90659ms step_avg:39.25ms
step:2311/2330 train_time:90681ms step_avg:39.24ms
step:2312/2330 train_time:90737ms step_avg:39.25ms
step:2313/2330 train_time:90758ms step_avg:39.24ms
step:2314/2330 train_time:90815ms step_avg:39.25ms
step:2315/2330 train_time:90837ms step_avg:39.24ms
step:2316/2330 train_time:90893ms step_avg:39.25ms
step:2317/2330 train_time:90914ms step_avg:39.24ms
step:2318/2330 train_time:90970ms step_avg:39.24ms
step:2319/2330 train_time:90992ms step_avg:39.24ms
step:2320/2330 train_time:91048ms step_avg:39.24ms
step:2321/2330 train_time:91070ms step_avg:39.24ms
step:2322/2330 train_time:91126ms step_avg:39.24ms
step:2323/2330 train_time:91148ms step_avg:39.24ms
step:2324/2330 train_time:91204ms step_avg:39.24ms
step:2325/2330 train_time:91226ms step_avg:39.24ms
step:2326/2330 train_time:91281ms step_avg:39.24ms
step:2327/2330 train_time:91303ms step_avg:39.24ms
step:2328/2330 train_time:91359ms step_avg:39.24ms
step:2329/2330 train_time:91381ms step_avg:39.24ms
step:2330/2330 train_time:91436ms step_avg:39.24ms
step:2330/2330 val_loss:5.0669 train_time:91531ms step_avg:39.28ms
peak memory allocated: 29712 MiB reserved: 38888 MiB
