import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = divide_by_norm(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = divide_by_norm(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "gd_1e-1"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-1, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 03:33:36 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   31C    P0             111W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   28C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:82ms step_avg:82.31ms
step:2/2330 train_time:200ms step_avg:99.86ms
step:3/2330 train_time:221ms step_avg:73.79ms
step:4/2330 train_time:249ms step_avg:62.28ms
step:5/2330 train_time:305ms step_avg:61.07ms
step:6/2330 train_time:366ms step_avg:60.93ms
step:7/2330 train_time:423ms step_avg:60.43ms
step:8/2330 train_time:483ms step_avg:60.42ms
step:9/2330 train_time:542ms step_avg:60.20ms
step:10/2330 train_time:602ms step_avg:60.23ms
step:11/2330 train_time:660ms step_avg:60.02ms
step:12/2330 train_time:721ms step_avg:60.09ms
step:13/2330 train_time:779ms step_avg:59.90ms
step:14/2330 train_time:839ms step_avg:59.94ms
step:15/2330 train_time:897ms step_avg:59.79ms
step:16/2330 train_time:958ms step_avg:59.86ms
step:17/2330 train_time:1016ms step_avg:59.77ms
step:18/2330 train_time:1078ms step_avg:59.89ms
step:19/2330 train_time:1140ms step_avg:59.99ms
step:20/2330 train_time:1203ms step_avg:60.16ms
step:21/2330 train_time:1263ms step_avg:60.13ms
step:22/2330 train_time:1326ms step_avg:60.27ms
step:23/2330 train_time:1384ms step_avg:60.18ms
step:24/2330 train_time:1446ms step_avg:60.23ms
step:25/2330 train_time:1504ms step_avg:60.16ms
step:26/2330 train_time:1564ms step_avg:60.17ms
step:27/2330 train_time:1622ms step_avg:60.09ms
step:28/2330 train_time:1684ms step_avg:60.15ms
step:29/2330 train_time:1742ms step_avg:60.06ms
step:30/2330 train_time:1803ms step_avg:60.10ms
step:31/2330 train_time:1861ms step_avg:60.04ms
step:32/2330 train_time:1922ms step_avg:60.08ms
step:33/2330 train_time:1981ms step_avg:60.03ms
step:34/2330 train_time:2043ms step_avg:60.08ms
step:35/2330 train_time:2102ms step_avg:60.06ms
step:36/2330 train_time:2165ms step_avg:60.14ms
step:37/2330 train_time:2225ms step_avg:60.13ms
step:38/2330 train_time:2287ms step_avg:60.17ms
step:39/2330 train_time:2346ms step_avg:60.16ms
step:40/2330 train_time:2408ms step_avg:60.20ms
step:41/2330 train_time:2466ms step_avg:60.15ms
step:42/2330 train_time:2527ms step_avg:60.17ms
step:43/2330 train_time:2584ms step_avg:60.10ms
step:44/2330 train_time:2646ms step_avg:60.13ms
step:45/2330 train_time:2704ms step_avg:60.08ms
step:46/2330 train_time:2766ms step_avg:60.12ms
step:47/2330 train_time:2824ms step_avg:60.07ms
step:48/2330 train_time:2885ms step_avg:60.11ms
step:49/2330 train_time:2944ms step_avg:60.08ms
step:50/2330 train_time:3007ms step_avg:60.14ms
step:51/2330 train_time:3066ms step_avg:60.12ms
step:52/2330 train_time:3128ms step_avg:60.15ms
step:53/2330 train_time:3187ms step_avg:60.14ms
step:54/2330 train_time:3250ms step_avg:60.18ms
step:55/2330 train_time:3309ms step_avg:60.16ms
step:56/2330 train_time:3370ms step_avg:60.17ms
step:57/2330 train_time:3429ms step_avg:60.16ms
step:58/2330 train_time:3489ms step_avg:60.16ms
step:59/2330 train_time:3547ms step_avg:60.12ms
step:60/2330 train_time:3609ms step_avg:60.15ms
step:61/2330 train_time:3667ms step_avg:60.12ms
step:62/2330 train_time:3729ms step_avg:60.15ms
step:63/2330 train_time:3787ms step_avg:60.11ms
step:64/2330 train_time:3848ms step_avg:60.13ms
step:65/2330 train_time:3908ms step_avg:60.13ms
step:66/2330 train_time:3970ms step_avg:60.15ms
step:67/2330 train_time:4028ms step_avg:60.12ms
step:68/2330 train_time:4090ms step_avg:60.14ms
step:69/2330 train_time:4149ms step_avg:60.13ms
step:70/2330 train_time:4212ms step_avg:60.17ms
step:71/2330 train_time:4270ms step_avg:60.14ms
step:72/2330 train_time:4331ms step_avg:60.15ms
step:73/2330 train_time:4390ms step_avg:60.13ms
step:74/2330 train_time:4452ms step_avg:60.16ms
step:75/2330 train_time:4510ms step_avg:60.14ms
step:76/2330 train_time:4571ms step_avg:60.15ms
step:77/2330 train_time:4629ms step_avg:60.12ms
step:78/2330 train_time:4691ms step_avg:60.14ms
step:79/2330 train_time:4749ms step_avg:60.11ms
step:80/2330 train_time:4811ms step_avg:60.14ms
step:81/2330 train_time:4870ms step_avg:60.12ms
step:82/2330 train_time:4931ms step_avg:60.14ms
step:83/2330 train_time:4990ms step_avg:60.12ms
step:84/2330 train_time:5051ms step_avg:60.13ms
step:85/2330 train_time:5110ms step_avg:60.11ms
step:86/2330 train_time:5171ms step_avg:60.13ms
step:87/2330 train_time:5230ms step_avg:60.11ms
step:88/2330 train_time:5292ms step_avg:60.13ms
step:89/2330 train_time:5350ms step_avg:60.11ms
step:90/2330 train_time:5412ms step_avg:60.14ms
step:91/2330 train_time:5471ms step_avg:60.12ms
step:92/2330 train_time:5533ms step_avg:60.14ms
step:93/2330 train_time:5591ms step_avg:60.12ms
step:94/2330 train_time:5653ms step_avg:60.14ms
step:95/2330 train_time:5712ms step_avg:60.12ms
step:96/2330 train_time:5773ms step_avg:60.13ms
step:97/2330 train_time:5832ms step_avg:60.12ms
step:98/2330 train_time:5894ms step_avg:60.14ms
step:99/2330 train_time:5952ms step_avg:60.12ms
step:100/2330 train_time:6014ms step_avg:60.14ms
step:101/2330 train_time:6073ms step_avg:60.13ms
step:102/2330 train_time:6135ms step_avg:60.14ms
step:103/2330 train_time:6193ms step_avg:60.13ms
step:104/2330 train_time:6254ms step_avg:60.14ms
step:105/2330 train_time:6313ms step_avg:60.13ms
step:106/2330 train_time:6375ms step_avg:60.15ms
step:107/2330 train_time:6434ms step_avg:60.13ms
step:108/2330 train_time:6495ms step_avg:60.14ms
step:109/2330 train_time:6553ms step_avg:60.12ms
step:110/2330 train_time:6616ms step_avg:60.15ms
step:111/2330 train_time:6675ms step_avg:60.13ms
step:112/2330 train_time:6737ms step_avg:60.15ms
step:113/2330 train_time:6795ms step_avg:60.13ms
step:114/2330 train_time:6856ms step_avg:60.14ms
step:115/2330 train_time:6915ms step_avg:60.13ms
step:116/2330 train_time:6977ms step_avg:60.14ms
step:117/2330 train_time:7035ms step_avg:60.13ms
step:118/2330 train_time:7097ms step_avg:60.14ms
step:119/2330 train_time:7155ms step_avg:60.13ms
step:120/2330 train_time:7217ms step_avg:60.14ms
step:121/2330 train_time:7275ms step_avg:60.13ms
step:122/2330 train_time:7338ms step_avg:60.15ms
step:123/2330 train_time:7396ms step_avg:60.13ms
step:124/2330 train_time:7457ms step_avg:60.14ms
step:125/2330 train_time:7516ms step_avg:60.13ms
step:126/2330 train_time:7578ms step_avg:60.14ms
step:127/2330 train_time:7636ms step_avg:60.13ms
step:128/2330 train_time:7698ms step_avg:60.14ms
step:129/2330 train_time:7756ms step_avg:60.13ms
step:130/2330 train_time:7818ms step_avg:60.14ms
step:131/2330 train_time:7877ms step_avg:60.13ms
step:132/2330 train_time:7939ms step_avg:60.14ms
step:133/2330 train_time:7997ms step_avg:60.13ms
step:134/2330 train_time:8059ms step_avg:60.14ms
step:135/2330 train_time:8118ms step_avg:60.13ms
step:136/2330 train_time:8179ms step_avg:60.14ms
step:137/2330 train_time:8237ms step_avg:60.12ms
step:138/2330 train_time:8298ms step_avg:60.13ms
step:139/2330 train_time:8357ms step_avg:60.12ms
step:140/2330 train_time:8419ms step_avg:60.13ms
step:141/2330 train_time:8477ms step_avg:60.12ms
step:142/2330 train_time:8538ms step_avg:60.13ms
step:143/2330 train_time:8596ms step_avg:60.11ms
step:144/2330 train_time:8658ms step_avg:60.12ms
step:145/2330 train_time:8716ms step_avg:60.11ms
step:146/2330 train_time:8778ms step_avg:60.12ms
step:147/2330 train_time:8837ms step_avg:60.11ms
step:148/2330 train_time:8898ms step_avg:60.12ms
step:149/2330 train_time:8957ms step_avg:60.11ms
step:150/2330 train_time:9019ms step_avg:60.13ms
step:151/2330 train_time:9078ms step_avg:60.12ms
step:152/2330 train_time:9139ms step_avg:60.13ms
step:153/2330 train_time:9198ms step_avg:60.12ms
step:154/2330 train_time:9259ms step_avg:60.13ms
step:155/2330 train_time:9319ms step_avg:60.12ms
step:156/2330 train_time:9380ms step_avg:60.13ms
step:157/2330 train_time:9439ms step_avg:60.12ms
step:158/2330 train_time:9500ms step_avg:60.12ms
step:159/2330 train_time:9558ms step_avg:60.12ms
step:160/2330 train_time:9620ms step_avg:60.12ms
step:161/2330 train_time:9678ms step_avg:60.11ms
step:162/2330 train_time:9740ms step_avg:60.12ms
step:163/2330 train_time:9798ms step_avg:60.11ms
step:164/2330 train_time:9860ms step_avg:60.12ms
step:165/2330 train_time:9918ms step_avg:60.11ms
step:166/2330 train_time:9980ms step_avg:60.12ms
step:167/2330 train_time:10039ms step_avg:60.11ms
step:168/2330 train_time:10100ms step_avg:60.12ms
step:169/2330 train_time:10158ms step_avg:60.11ms
step:170/2330 train_time:10220ms step_avg:60.12ms
step:171/2330 train_time:10279ms step_avg:60.11ms
step:172/2330 train_time:10341ms step_avg:60.12ms
step:173/2330 train_time:10400ms step_avg:60.11ms
step:174/2330 train_time:10461ms step_avg:60.12ms
step:175/2330 train_time:10520ms step_avg:60.11ms
step:176/2330 train_time:10582ms step_avg:60.12ms
step:177/2330 train_time:10640ms step_avg:60.11ms
step:178/2330 train_time:10702ms step_avg:60.12ms
step:179/2330 train_time:10760ms step_avg:60.11ms
step:180/2330 train_time:10822ms step_avg:60.12ms
step:181/2330 train_time:10880ms step_avg:60.11ms
step:182/2330 train_time:10942ms step_avg:60.12ms
step:183/2330 train_time:11001ms step_avg:60.11ms
step:184/2330 train_time:11062ms step_avg:60.12ms
step:185/2330 train_time:11120ms step_avg:60.11ms
step:186/2330 train_time:11181ms step_avg:60.11ms
step:187/2330 train_time:11240ms step_avg:60.11ms
step:188/2330 train_time:11302ms step_avg:60.12ms
step:189/2330 train_time:11360ms step_avg:60.10ms
step:190/2330 train_time:11421ms step_avg:60.11ms
step:191/2330 train_time:11480ms step_avg:60.10ms
step:192/2330 train_time:11542ms step_avg:60.11ms
step:193/2330 train_time:11600ms step_avg:60.10ms
step:194/2330 train_time:11662ms step_avg:60.11ms
step:195/2330 train_time:11721ms step_avg:60.11ms
step:196/2330 train_time:11782ms step_avg:60.11ms
step:197/2330 train_time:11840ms step_avg:60.10ms
step:198/2330 train_time:11902ms step_avg:60.11ms
step:199/2330 train_time:11960ms step_avg:60.10ms
step:200/2330 train_time:12021ms step_avg:60.11ms
step:201/2330 train_time:12080ms step_avg:60.10ms
step:202/2330 train_time:12141ms step_avg:60.10ms
step:203/2330 train_time:12199ms step_avg:60.09ms
step:204/2330 train_time:12260ms step_avg:60.10ms
step:205/2330 train_time:12319ms step_avg:60.09ms
step:206/2330 train_time:12380ms step_avg:60.10ms
step:207/2330 train_time:12439ms step_avg:60.09ms
step:208/2330 train_time:12500ms step_avg:60.10ms
step:209/2330 train_time:12559ms step_avg:60.09ms
step:210/2330 train_time:12620ms step_avg:60.09ms
step:211/2330 train_time:12679ms step_avg:60.09ms
step:212/2330 train_time:12740ms step_avg:60.09ms
step:213/2330 train_time:12798ms step_avg:60.09ms
step:214/2330 train_time:12860ms step_avg:60.09ms
step:215/2330 train_time:12919ms step_avg:60.09ms
step:216/2330 train_time:12980ms step_avg:60.09ms
step:217/2330 train_time:13039ms step_avg:60.09ms
step:218/2330 train_time:13100ms step_avg:60.09ms
step:219/2330 train_time:13159ms step_avg:60.09ms
step:220/2330 train_time:13220ms step_avg:60.09ms
step:221/2330 train_time:13279ms step_avg:60.08ms
step:222/2330 train_time:13341ms step_avg:60.09ms
step:223/2330 train_time:13400ms step_avg:60.09ms
step:224/2330 train_time:13461ms step_avg:60.09ms
step:225/2330 train_time:13520ms step_avg:60.09ms
step:226/2330 train_time:13582ms step_avg:60.10ms
step:227/2330 train_time:13641ms step_avg:60.09ms
step:228/2330 train_time:13702ms step_avg:60.10ms
step:229/2330 train_time:13761ms step_avg:60.09ms
step:230/2330 train_time:13822ms step_avg:60.10ms
step:231/2330 train_time:13881ms step_avg:60.09ms
step:232/2330 train_time:13942ms step_avg:60.10ms
step:233/2330 train_time:14001ms step_avg:60.09ms
step:234/2330 train_time:14063ms step_avg:60.10ms
step:235/2330 train_time:14121ms step_avg:60.09ms
step:236/2330 train_time:14182ms step_avg:60.09ms
step:237/2330 train_time:14241ms step_avg:60.09ms
step:238/2330 train_time:14303ms step_avg:60.10ms
step:239/2330 train_time:14361ms step_avg:60.09ms
step:240/2330 train_time:14422ms step_avg:60.09ms
step:241/2330 train_time:14481ms step_avg:60.09ms
step:242/2330 train_time:14542ms step_avg:60.09ms
step:243/2330 train_time:14600ms step_avg:60.08ms
step:244/2330 train_time:14661ms step_avg:60.09ms
step:245/2330 train_time:14721ms step_avg:60.08ms
step:246/2330 train_time:14783ms step_avg:60.09ms
step:247/2330 train_time:14842ms step_avg:60.09ms
step:248/2330 train_time:14903ms step_avg:60.09ms
step:249/2330 train_time:14962ms step_avg:60.09ms
step:250/2330 train_time:15023ms step_avg:60.09ms
step:250/2330 val_loss:4.6143 train_time:15093ms step_avg:60.37ms
step:251/2330 train_time:15115ms step_avg:60.22ms
step:252/2330 train_time:15144ms step_avg:60.09ms
step:253/2330 train_time:15201ms step_avg:60.08ms
step:254/2330 train_time:15270ms step_avg:60.12ms
step:255/2330 train_time:15329ms step_avg:60.11ms
step:256/2330 train_time:15391ms step_avg:60.12ms
step:257/2330 train_time:15450ms step_avg:60.12ms
step:258/2330 train_time:15512ms step_avg:60.13ms
step:259/2330 train_time:15572ms step_avg:60.12ms
step:260/2330 train_time:15633ms step_avg:60.13ms
step:261/2330 train_time:15691ms step_avg:60.12ms
step:262/2330 train_time:15753ms step_avg:60.12ms
step:263/2330 train_time:15811ms step_avg:60.12ms
step:264/2330 train_time:15872ms step_avg:60.12ms
step:265/2330 train_time:15929ms step_avg:60.11ms
step:266/2330 train_time:15990ms step_avg:60.11ms
step:267/2330 train_time:16048ms step_avg:60.10ms
step:268/2330 train_time:16110ms step_avg:60.11ms
step:269/2330 train_time:16171ms step_avg:60.12ms
step:270/2330 train_time:16234ms step_avg:60.13ms
step:271/2330 train_time:16293ms step_avg:60.12ms
step:272/2330 train_time:16355ms step_avg:60.13ms
step:273/2330 train_time:16414ms step_avg:60.12ms
step:274/2330 train_time:16476ms step_avg:60.13ms
step:275/2330 train_time:16536ms step_avg:60.13ms
step:276/2330 train_time:16597ms step_avg:60.13ms
step:277/2330 train_time:16656ms step_avg:60.13ms
step:278/2330 train_time:16718ms step_avg:60.14ms
step:279/2330 train_time:16776ms step_avg:60.13ms
step:280/2330 train_time:16838ms step_avg:60.13ms
step:281/2330 train_time:16896ms step_avg:60.13ms
step:282/2330 train_time:16957ms step_avg:60.13ms
step:283/2330 train_time:17015ms step_avg:60.13ms
step:284/2330 train_time:17078ms step_avg:60.13ms
step:285/2330 train_time:17136ms step_avg:60.13ms
step:286/2330 train_time:17198ms step_avg:60.13ms
step:287/2330 train_time:17258ms step_avg:60.13ms
step:288/2330 train_time:17319ms step_avg:60.14ms
step:289/2330 train_time:17378ms step_avg:60.13ms
step:290/2330 train_time:17440ms step_avg:60.14ms
step:291/2330 train_time:17499ms step_avg:60.13ms
step:292/2330 train_time:17562ms step_avg:60.14ms
step:293/2330 train_time:17620ms step_avg:60.14ms
step:294/2330 train_time:17682ms step_avg:60.14ms
step:295/2330 train_time:17740ms step_avg:60.14ms
step:296/2330 train_time:17801ms step_avg:60.14ms
step:297/2330 train_time:17860ms step_avg:60.13ms
step:298/2330 train_time:17921ms step_avg:60.14ms
step:299/2330 train_time:17979ms step_avg:60.13ms
step:300/2330 train_time:18041ms step_avg:60.14ms
step:301/2330 train_time:18100ms step_avg:60.13ms
step:302/2330 train_time:18162ms step_avg:60.14ms
step:303/2330 train_time:18221ms step_avg:60.13ms
step:304/2330 train_time:18283ms step_avg:60.14ms
step:305/2330 train_time:18341ms step_avg:60.14ms
step:306/2330 train_time:18403ms step_avg:60.14ms
step:307/2330 train_time:18462ms step_avg:60.14ms
step:308/2330 train_time:18524ms step_avg:60.14ms
step:309/2330 train_time:18582ms step_avg:60.14ms
step:310/2330 train_time:18644ms step_avg:60.14ms
step:311/2330 train_time:18702ms step_avg:60.13ms
step:312/2330 train_time:18763ms step_avg:60.14ms
step:313/2330 train_time:18822ms step_avg:60.13ms
step:314/2330 train_time:18883ms step_avg:60.14ms
step:315/2330 train_time:18942ms step_avg:60.13ms
step:316/2330 train_time:19003ms step_avg:60.14ms
step:317/2330 train_time:19062ms step_avg:60.13ms
step:318/2330 train_time:19124ms step_avg:60.14ms
step:319/2330 train_time:19183ms step_avg:60.13ms
step:320/2330 train_time:19245ms step_avg:60.14ms
step:321/2330 train_time:19303ms step_avg:60.13ms
step:322/2330 train_time:19366ms step_avg:60.14ms
step:323/2330 train_time:19425ms step_avg:60.14ms
step:324/2330 train_time:19487ms step_avg:60.15ms
step:325/2330 train_time:19545ms step_avg:60.14ms
step:326/2330 train_time:19606ms step_avg:60.14ms
step:327/2330 train_time:19665ms step_avg:60.14ms
step:328/2330 train_time:19726ms step_avg:60.14ms
step:329/2330 train_time:19784ms step_avg:60.14ms
step:330/2330 train_time:19846ms step_avg:60.14ms
step:331/2330 train_time:19905ms step_avg:60.13ms
step:332/2330 train_time:19967ms step_avg:60.14ms
step:333/2330 train_time:20025ms step_avg:60.14ms
step:334/2330 train_time:20087ms step_avg:60.14ms
step:335/2330 train_time:20145ms step_avg:60.13ms
step:336/2330 train_time:20207ms step_avg:60.14ms
step:337/2330 train_time:20266ms step_avg:60.14ms
step:338/2330 train_time:20327ms step_avg:60.14ms
step:339/2330 train_time:20385ms step_avg:60.13ms
step:340/2330 train_time:20447ms step_avg:60.14ms
step:341/2330 train_time:20505ms step_avg:60.13ms
step:342/2330 train_time:20567ms step_avg:60.14ms
step:343/2330 train_time:20626ms step_avg:60.13ms
step:344/2330 train_time:20688ms step_avg:60.14ms
step:345/2330 train_time:20745ms step_avg:60.13ms
step:346/2330 train_time:20807ms step_avg:60.14ms
step:347/2330 train_time:20866ms step_avg:60.13ms
step:348/2330 train_time:20928ms step_avg:60.14ms
step:349/2330 train_time:20986ms step_avg:60.13ms
step:350/2330 train_time:21048ms step_avg:60.14ms
step:351/2330 train_time:21107ms step_avg:60.13ms
step:352/2330 train_time:21168ms step_avg:60.14ms
step:353/2330 train_time:21227ms step_avg:60.13ms
step:354/2330 train_time:21289ms step_avg:60.14ms
step:355/2330 train_time:21347ms step_avg:60.13ms
step:356/2330 train_time:21408ms step_avg:60.14ms
step:357/2330 train_time:21467ms step_avg:60.13ms
step:358/2330 train_time:21528ms step_avg:60.13ms
step:359/2330 train_time:21587ms step_avg:60.13ms
step:360/2330 train_time:21648ms step_avg:60.13ms
step:361/2330 train_time:21707ms step_avg:60.13ms
step:362/2330 train_time:21768ms step_avg:60.13ms
step:363/2330 train_time:21827ms step_avg:60.13ms
step:364/2330 train_time:21889ms step_avg:60.13ms
step:365/2330 train_time:21947ms step_avg:60.13ms
step:366/2330 train_time:22008ms step_avg:60.13ms
step:367/2330 train_time:22067ms step_avg:60.13ms
step:368/2330 train_time:22129ms step_avg:60.13ms
step:369/2330 train_time:22187ms step_avg:60.13ms
step:370/2330 train_time:22249ms step_avg:60.13ms
step:371/2330 train_time:22308ms step_avg:60.13ms
step:372/2330 train_time:22370ms step_avg:60.14ms
step:373/2330 train_time:22429ms step_avg:60.13ms
step:374/2330 train_time:22491ms step_avg:60.14ms
step:375/2330 train_time:22549ms step_avg:60.13ms
step:376/2330 train_time:22611ms step_avg:60.13ms
step:377/2330 train_time:22669ms step_avg:60.13ms
step:378/2330 train_time:22731ms step_avg:60.13ms
step:379/2330 train_time:22789ms step_avg:60.13ms
step:380/2330 train_time:22850ms step_avg:60.13ms
step:381/2330 train_time:22909ms step_avg:60.13ms
step:382/2330 train_time:22971ms step_avg:60.13ms
step:383/2330 train_time:23030ms step_avg:60.13ms
step:384/2330 train_time:23092ms step_avg:60.13ms
step:385/2330 train_time:23150ms step_avg:60.13ms
step:386/2330 train_time:23211ms step_avg:60.13ms
step:387/2330 train_time:23270ms step_avg:60.13ms
step:388/2330 train_time:23332ms step_avg:60.13ms
step:389/2330 train_time:23391ms step_avg:60.13ms
step:390/2330 train_time:23452ms step_avg:60.13ms
step:391/2330 train_time:23510ms step_avg:60.13ms
step:392/2330 train_time:23572ms step_avg:60.13ms
step:393/2330 train_time:23631ms step_avg:60.13ms
step:394/2330 train_time:23693ms step_avg:60.13ms
step:395/2330 train_time:23752ms step_avg:60.13ms
step:396/2330 train_time:23814ms step_avg:60.14ms
step:397/2330 train_time:23873ms step_avg:60.13ms
step:398/2330 train_time:23934ms step_avg:60.14ms
step:399/2330 train_time:23993ms step_avg:60.13ms
step:400/2330 train_time:24056ms step_avg:60.14ms
step:401/2330 train_time:24114ms step_avg:60.14ms
step:402/2330 train_time:24176ms step_avg:60.14ms
step:403/2330 train_time:24235ms step_avg:60.14ms
step:404/2330 train_time:24298ms step_avg:60.14ms
step:405/2330 train_time:24358ms step_avg:60.14ms
step:406/2330 train_time:24418ms step_avg:60.14ms
step:407/2330 train_time:24477ms step_avg:60.14ms
step:408/2330 train_time:24538ms step_avg:60.14ms
step:409/2330 train_time:24597ms step_avg:60.14ms
step:410/2330 train_time:24658ms step_avg:60.14ms
step:411/2330 train_time:24716ms step_avg:60.14ms
step:412/2330 train_time:24778ms step_avg:60.14ms
step:413/2330 train_time:24836ms step_avg:60.14ms
step:414/2330 train_time:24898ms step_avg:60.14ms
step:415/2330 train_time:24958ms step_avg:60.14ms
step:416/2330 train_time:25019ms step_avg:60.14ms
step:417/2330 train_time:25078ms step_avg:60.14ms
step:418/2330 train_time:25139ms step_avg:60.14ms
step:419/2330 train_time:25199ms step_avg:60.14ms
step:420/2330 train_time:25260ms step_avg:60.14ms
step:421/2330 train_time:25318ms step_avg:60.14ms
step:422/2330 train_time:25380ms step_avg:60.14ms
step:423/2330 train_time:25439ms step_avg:60.14ms
step:424/2330 train_time:25500ms step_avg:60.14ms
step:425/2330 train_time:25559ms step_avg:60.14ms
step:426/2330 train_time:25621ms step_avg:60.14ms
step:427/2330 train_time:25680ms step_avg:60.14ms
step:428/2330 train_time:25741ms step_avg:60.14ms
step:429/2330 train_time:25800ms step_avg:60.14ms
step:430/2330 train_time:25862ms step_avg:60.15ms
step:431/2330 train_time:25921ms step_avg:60.14ms
step:432/2330 train_time:25983ms step_avg:60.15ms
step:433/2330 train_time:26042ms step_avg:60.14ms
step:434/2330 train_time:26104ms step_avg:60.15ms
step:435/2330 train_time:26162ms step_avg:60.14ms
step:436/2330 train_time:26225ms step_avg:60.15ms
step:437/2330 train_time:26284ms step_avg:60.15ms
step:438/2330 train_time:26345ms step_avg:60.15ms
step:439/2330 train_time:26403ms step_avg:60.14ms
step:440/2330 train_time:26465ms step_avg:60.15ms
step:441/2330 train_time:26524ms step_avg:60.14ms
step:442/2330 train_time:26586ms step_avg:60.15ms
step:443/2330 train_time:26644ms step_avg:60.15ms
step:444/2330 train_time:26706ms step_avg:60.15ms
step:445/2330 train_time:26764ms step_avg:60.14ms
step:446/2330 train_time:26826ms step_avg:60.15ms
step:447/2330 train_time:26884ms step_avg:60.14ms
step:448/2330 train_time:26945ms step_avg:60.15ms
step:449/2330 train_time:27004ms step_avg:60.14ms
step:450/2330 train_time:27066ms step_avg:60.15ms
step:451/2330 train_time:27125ms step_avg:60.14ms
step:452/2330 train_time:27187ms step_avg:60.15ms
step:453/2330 train_time:27246ms step_avg:60.14ms
step:454/2330 train_time:27307ms step_avg:60.15ms
step:455/2330 train_time:27365ms step_avg:60.14ms
step:456/2330 train_time:27427ms step_avg:60.15ms
step:457/2330 train_time:27485ms step_avg:60.14ms
step:458/2330 train_time:27547ms step_avg:60.15ms
step:459/2330 train_time:27605ms step_avg:60.14ms
step:460/2330 train_time:27667ms step_avg:60.15ms
step:461/2330 train_time:27725ms step_avg:60.14ms
step:462/2330 train_time:27787ms step_avg:60.15ms
step:463/2330 train_time:27846ms step_avg:60.14ms
step:464/2330 train_time:27908ms step_avg:60.15ms
step:465/2330 train_time:27967ms step_avg:60.14ms
step:466/2330 train_time:28029ms step_avg:60.15ms
step:467/2330 train_time:28087ms step_avg:60.14ms
step:468/2330 train_time:28150ms step_avg:60.15ms
step:469/2330 train_time:28209ms step_avg:60.15ms
step:470/2330 train_time:28271ms step_avg:60.15ms
step:471/2330 train_time:28329ms step_avg:60.15ms
step:472/2330 train_time:28391ms step_avg:60.15ms
step:473/2330 train_time:28450ms step_avg:60.15ms
step:474/2330 train_time:28511ms step_avg:60.15ms
step:475/2330 train_time:28569ms step_avg:60.15ms
step:476/2330 train_time:28631ms step_avg:60.15ms
step:477/2330 train_time:28690ms step_avg:60.15ms
step:478/2330 train_time:28752ms step_avg:60.15ms
step:479/2330 train_time:28811ms step_avg:60.15ms
step:480/2330 train_time:28873ms step_avg:60.15ms
step:481/2330 train_time:28932ms step_avg:60.15ms
step:482/2330 train_time:28994ms step_avg:60.15ms
step:483/2330 train_time:29055ms step_avg:60.15ms
step:484/2330 train_time:29115ms step_avg:60.16ms
step:485/2330 train_time:29175ms step_avg:60.15ms
step:486/2330 train_time:29236ms step_avg:60.16ms
step:487/2330 train_time:29296ms step_avg:60.16ms
step:488/2330 train_time:29358ms step_avg:60.16ms
step:489/2330 train_time:29416ms step_avg:60.16ms
step:490/2330 train_time:29478ms step_avg:60.16ms
step:491/2330 train_time:29536ms step_avg:60.16ms
step:492/2330 train_time:29598ms step_avg:60.16ms
step:493/2330 train_time:29657ms step_avg:60.16ms
step:494/2330 train_time:29719ms step_avg:60.16ms
step:495/2330 train_time:29777ms step_avg:60.16ms
step:496/2330 train_time:29839ms step_avg:60.16ms
step:497/2330 train_time:29898ms step_avg:60.16ms
step:498/2330 train_time:29961ms step_avg:60.16ms
step:499/2330 train_time:30019ms step_avg:60.16ms
step:500/2330 train_time:30081ms step_avg:60.16ms
step:500/2330 val_loss:4.1112 train_time:30153ms step_avg:60.31ms
step:501/2330 train_time:30175ms step_avg:60.23ms
step:502/2330 train_time:30204ms step_avg:60.17ms
step:503/2330 train_time:30264ms step_avg:60.17ms
step:504/2330 train_time:30332ms step_avg:60.18ms
step:505/2330 train_time:30392ms step_avg:60.18ms
step:506/2330 train_time:30453ms step_avg:60.18ms
step:507/2330 train_time:30512ms step_avg:60.18ms
step:508/2330 train_time:30573ms step_avg:60.18ms
step:509/2330 train_time:30632ms step_avg:60.18ms
step:510/2330 train_time:30693ms step_avg:60.18ms
step:511/2330 train_time:30751ms step_avg:60.18ms
step:512/2330 train_time:30813ms step_avg:60.18ms
step:513/2330 train_time:30870ms step_avg:60.18ms
step:514/2330 train_time:30931ms step_avg:60.18ms
step:515/2330 train_time:30990ms step_avg:60.17ms
step:516/2330 train_time:31050ms step_avg:60.17ms
step:517/2330 train_time:31109ms step_avg:60.17ms
step:518/2330 train_time:31172ms step_avg:60.18ms
step:519/2330 train_time:31232ms step_avg:60.18ms
step:520/2330 train_time:31297ms step_avg:60.19ms
step:521/2330 train_time:31356ms step_avg:60.18ms
step:522/2330 train_time:31418ms step_avg:60.19ms
step:523/2330 train_time:31477ms step_avg:60.18ms
step:524/2330 train_time:31538ms step_avg:60.19ms
step:525/2330 train_time:31597ms step_avg:60.18ms
step:526/2330 train_time:31659ms step_avg:60.19ms
step:527/2330 train_time:31717ms step_avg:60.18ms
step:528/2330 train_time:31779ms step_avg:60.19ms
step:529/2330 train_time:31837ms step_avg:60.18ms
step:530/2330 train_time:31900ms step_avg:60.19ms
step:531/2330 train_time:31958ms step_avg:60.18ms
step:532/2330 train_time:32019ms step_avg:60.19ms
step:533/2330 train_time:32078ms step_avg:60.18ms
step:534/2330 train_time:32140ms step_avg:60.19ms
step:535/2330 train_time:32200ms step_avg:60.19ms
step:536/2330 train_time:32263ms step_avg:60.19ms
step:537/2330 train_time:32321ms step_avg:60.19ms
step:538/2330 train_time:32384ms step_avg:60.19ms
step:539/2330 train_time:32442ms step_avg:60.19ms
step:540/2330 train_time:32504ms step_avg:60.19ms
step:541/2330 train_time:32563ms step_avg:60.19ms
step:542/2330 train_time:32625ms step_avg:60.19ms
step:543/2330 train_time:32683ms step_avg:60.19ms
step:544/2330 train_time:32745ms step_avg:60.19ms
step:545/2330 train_time:32804ms step_avg:60.19ms
step:546/2330 train_time:32865ms step_avg:60.19ms
step:547/2330 train_time:32925ms step_avg:60.19ms
step:548/2330 train_time:32986ms step_avg:60.19ms
step:549/2330 train_time:33044ms step_avg:60.19ms
step:550/2330 train_time:33106ms step_avg:60.19ms
step:551/2330 train_time:33165ms step_avg:60.19ms
step:552/2330 train_time:33227ms step_avg:60.19ms
step:553/2330 train_time:33286ms step_avg:60.19ms
step:554/2330 train_time:33347ms step_avg:60.19ms
step:555/2330 train_time:33406ms step_avg:60.19ms
step:556/2330 train_time:33468ms step_avg:60.19ms
step:557/2330 train_time:33527ms step_avg:60.19ms
step:558/2330 train_time:33589ms step_avg:60.20ms
step:559/2330 train_time:33648ms step_avg:60.19ms
step:560/2330 train_time:33710ms step_avg:60.20ms
step:561/2330 train_time:33769ms step_avg:60.19ms
step:562/2330 train_time:33832ms step_avg:60.20ms
step:563/2330 train_time:33890ms step_avg:60.20ms
step:564/2330 train_time:33952ms step_avg:60.20ms
step:565/2330 train_time:34012ms step_avg:60.20ms
step:566/2330 train_time:34074ms step_avg:60.20ms
step:567/2330 train_time:34133ms step_avg:60.20ms
step:568/2330 train_time:34195ms step_avg:60.20ms
step:569/2330 train_time:34254ms step_avg:60.20ms
step:570/2330 train_time:34316ms step_avg:60.20ms
step:571/2330 train_time:34375ms step_avg:60.20ms
step:572/2330 train_time:34436ms step_avg:60.20ms
step:573/2330 train_time:34495ms step_avg:60.20ms
step:574/2330 train_time:34556ms step_avg:60.20ms
step:575/2330 train_time:34615ms step_avg:60.20ms
step:576/2330 train_time:34676ms step_avg:60.20ms
step:577/2330 train_time:34735ms step_avg:60.20ms
step:578/2330 train_time:34798ms step_avg:60.20ms
step:579/2330 train_time:34856ms step_avg:60.20ms
step:580/2330 train_time:34918ms step_avg:60.20ms
step:581/2330 train_time:34977ms step_avg:60.20ms
step:582/2330 train_time:35039ms step_avg:60.20ms
step:583/2330 train_time:35098ms step_avg:60.20ms
step:584/2330 train_time:35160ms step_avg:60.20ms
step:585/2330 train_time:35218ms step_avg:60.20ms
step:586/2330 train_time:35281ms step_avg:60.21ms
step:587/2330 train_time:35339ms step_avg:60.20ms
step:588/2330 train_time:35401ms step_avg:60.21ms
step:589/2330 train_time:35460ms step_avg:60.20ms
step:590/2330 train_time:35521ms step_avg:60.21ms
step:591/2330 train_time:35579ms step_avg:60.20ms
step:592/2330 train_time:35641ms step_avg:60.20ms
step:593/2330 train_time:35699ms step_avg:60.20ms
step:594/2330 train_time:35762ms step_avg:60.21ms
step:595/2330 train_time:35821ms step_avg:60.20ms
step:596/2330 train_time:35883ms step_avg:60.21ms
step:597/2330 train_time:35942ms step_avg:60.20ms
step:598/2330 train_time:36004ms step_avg:60.21ms
step:599/2330 train_time:36063ms step_avg:60.20ms
step:600/2330 train_time:36124ms step_avg:60.21ms
step:601/2330 train_time:36182ms step_avg:60.20ms
step:602/2330 train_time:36244ms step_avg:60.21ms
step:603/2330 train_time:36303ms step_avg:60.20ms
step:604/2330 train_time:36364ms step_avg:60.21ms
step:605/2330 train_time:36423ms step_avg:60.20ms
step:606/2330 train_time:36485ms step_avg:60.21ms
step:607/2330 train_time:36543ms step_avg:60.20ms
step:608/2330 train_time:36605ms step_avg:60.21ms
step:609/2330 train_time:36663ms step_avg:60.20ms
step:610/2330 train_time:36725ms step_avg:60.20ms
step:611/2330 train_time:36783ms step_avg:60.20ms
step:612/2330 train_time:36845ms step_avg:60.20ms
step:613/2330 train_time:36904ms step_avg:60.20ms
step:614/2330 train_time:36966ms step_avg:60.21ms
step:615/2330 train_time:37025ms step_avg:60.20ms
step:616/2330 train_time:37087ms step_avg:60.21ms
step:617/2330 train_time:37146ms step_avg:60.20ms
step:618/2330 train_time:37207ms step_avg:60.21ms
step:619/2330 train_time:37266ms step_avg:60.20ms
step:620/2330 train_time:37327ms step_avg:60.21ms
step:621/2330 train_time:37386ms step_avg:60.20ms
step:622/2330 train_time:37447ms step_avg:60.20ms
step:623/2330 train_time:37507ms step_avg:60.20ms
step:624/2330 train_time:37568ms step_avg:60.21ms
step:625/2330 train_time:37627ms step_avg:60.20ms
step:626/2330 train_time:37689ms step_avg:60.21ms
step:627/2330 train_time:37748ms step_avg:60.20ms
step:628/2330 train_time:37810ms step_avg:60.21ms
step:629/2330 train_time:37868ms step_avg:60.20ms
step:630/2330 train_time:37931ms step_avg:60.21ms
step:631/2330 train_time:37989ms step_avg:60.20ms
step:632/2330 train_time:38051ms step_avg:60.21ms
step:633/2330 train_time:38109ms step_avg:60.20ms
step:634/2330 train_time:38171ms step_avg:60.21ms
step:635/2330 train_time:38230ms step_avg:60.21ms
step:636/2330 train_time:38292ms step_avg:60.21ms
step:637/2330 train_time:38350ms step_avg:60.20ms
step:638/2330 train_time:38411ms step_avg:60.21ms
step:639/2330 train_time:38470ms step_avg:60.20ms
step:640/2330 train_time:38532ms step_avg:60.21ms
step:641/2330 train_time:38591ms step_avg:60.21ms
step:642/2330 train_time:38653ms step_avg:60.21ms
step:643/2330 train_time:38711ms step_avg:60.20ms
step:644/2330 train_time:38772ms step_avg:60.21ms
step:645/2330 train_time:38832ms step_avg:60.20ms
step:646/2330 train_time:38894ms step_avg:60.21ms
step:647/2330 train_time:38954ms step_avg:60.21ms
step:648/2330 train_time:39015ms step_avg:60.21ms
step:649/2330 train_time:39074ms step_avg:60.21ms
step:650/2330 train_time:39135ms step_avg:60.21ms
step:651/2330 train_time:39195ms step_avg:60.21ms
step:652/2330 train_time:39257ms step_avg:60.21ms
step:653/2330 train_time:39315ms step_avg:60.21ms
step:654/2330 train_time:39376ms step_avg:60.21ms
step:655/2330 train_time:39434ms step_avg:60.21ms
step:656/2330 train_time:39497ms step_avg:60.21ms
step:657/2330 train_time:39555ms step_avg:60.21ms
step:658/2330 train_time:39618ms step_avg:60.21ms
step:659/2330 train_time:39677ms step_avg:60.21ms
step:660/2330 train_time:39739ms step_avg:60.21ms
step:661/2330 train_time:39797ms step_avg:60.21ms
step:662/2330 train_time:39860ms step_avg:60.21ms
step:663/2330 train_time:39919ms step_avg:60.21ms
step:664/2330 train_time:39980ms step_avg:60.21ms
step:665/2330 train_time:40039ms step_avg:60.21ms
step:666/2330 train_time:40102ms step_avg:60.21ms
step:667/2330 train_time:40160ms step_avg:60.21ms
step:668/2330 train_time:40222ms step_avg:60.21ms
step:669/2330 train_time:40280ms step_avg:60.21ms
step:670/2330 train_time:40342ms step_avg:60.21ms
step:671/2330 train_time:40401ms step_avg:60.21ms
step:672/2330 train_time:40463ms step_avg:60.21ms
step:673/2330 train_time:40521ms step_avg:60.21ms
step:674/2330 train_time:40583ms step_avg:60.21ms
step:675/2330 train_time:40642ms step_avg:60.21ms
step:676/2330 train_time:40705ms step_avg:60.21ms
step:677/2330 train_time:40763ms step_avg:60.21ms
step:678/2330 train_time:40825ms step_avg:60.21ms
step:679/2330 train_time:40883ms step_avg:60.21ms
step:680/2330 train_time:40945ms step_avg:60.21ms
step:681/2330 train_time:41004ms step_avg:60.21ms
step:682/2330 train_time:41066ms step_avg:60.21ms
step:683/2330 train_time:41124ms step_avg:60.21ms
step:684/2330 train_time:41185ms step_avg:60.21ms
step:685/2330 train_time:41243ms step_avg:60.21ms
step:686/2330 train_time:41305ms step_avg:60.21ms
step:687/2330 train_time:41363ms step_avg:60.21ms
step:688/2330 train_time:41425ms step_avg:60.21ms
step:689/2330 train_time:41483ms step_avg:60.21ms
step:690/2330 train_time:41545ms step_avg:60.21ms
step:691/2330 train_time:41604ms step_avg:60.21ms
step:692/2330 train_time:41665ms step_avg:60.21ms
step:693/2330 train_time:41724ms step_avg:60.21ms
step:694/2330 train_time:41785ms step_avg:60.21ms
step:695/2330 train_time:41844ms step_avg:60.21ms
step:696/2330 train_time:41905ms step_avg:60.21ms
step:697/2330 train_time:41965ms step_avg:60.21ms
step:698/2330 train_time:42026ms step_avg:60.21ms
step:699/2330 train_time:42085ms step_avg:60.21ms
step:700/2330 train_time:42146ms step_avg:60.21ms
step:701/2330 train_time:42205ms step_avg:60.21ms
step:702/2330 train_time:42266ms step_avg:60.21ms
step:703/2330 train_time:42325ms step_avg:60.21ms
step:704/2330 train_time:42386ms step_avg:60.21ms
step:705/2330 train_time:42445ms step_avg:60.21ms
step:706/2330 train_time:42507ms step_avg:60.21ms
step:707/2330 train_time:42565ms step_avg:60.21ms
step:708/2330 train_time:42627ms step_avg:60.21ms
step:709/2330 train_time:42685ms step_avg:60.20ms
step:710/2330 train_time:42747ms step_avg:60.21ms
step:711/2330 train_time:42807ms step_avg:60.21ms
step:712/2330 train_time:42868ms step_avg:60.21ms
step:713/2330 train_time:42926ms step_avg:60.21ms
step:714/2330 train_time:42988ms step_avg:60.21ms
step:715/2330 train_time:43046ms step_avg:60.20ms
step:716/2330 train_time:43108ms step_avg:60.21ms
step:717/2330 train_time:43166ms step_avg:60.20ms
step:718/2330 train_time:43227ms step_avg:60.21ms
step:719/2330 train_time:43286ms step_avg:60.20ms
step:720/2330 train_time:43348ms step_avg:60.21ms
step:721/2330 train_time:43407ms step_avg:60.20ms
step:722/2330 train_time:43469ms step_avg:60.21ms
step:723/2330 train_time:43527ms step_avg:60.20ms
step:724/2330 train_time:43589ms step_avg:60.21ms
step:725/2330 train_time:43647ms step_avg:60.20ms
step:726/2330 train_time:43709ms step_avg:60.21ms
step:727/2330 train_time:43768ms step_avg:60.20ms
step:728/2330 train_time:43831ms step_avg:60.21ms
step:729/2330 train_time:43890ms step_avg:60.21ms
step:730/2330 train_time:43952ms step_avg:60.21ms
step:731/2330 train_time:44011ms step_avg:60.21ms
step:732/2330 train_time:44072ms step_avg:60.21ms
step:733/2330 train_time:44131ms step_avg:60.21ms
step:734/2330 train_time:44193ms step_avg:60.21ms
step:735/2330 train_time:44252ms step_avg:60.21ms
step:736/2330 train_time:44313ms step_avg:60.21ms
step:737/2330 train_time:44372ms step_avg:60.21ms
step:738/2330 train_time:44434ms step_avg:60.21ms
step:739/2330 train_time:44494ms step_avg:60.21ms
step:740/2330 train_time:44555ms step_avg:60.21ms
step:741/2330 train_time:44613ms step_avg:60.21ms
step:742/2330 train_time:44675ms step_avg:60.21ms
step:743/2330 train_time:44734ms step_avg:60.21ms
step:744/2330 train_time:44796ms step_avg:60.21ms
step:745/2330 train_time:44854ms step_avg:60.21ms
step:746/2330 train_time:44916ms step_avg:60.21ms
step:747/2330 train_time:44974ms step_avg:60.21ms
step:748/2330 train_time:45037ms step_avg:60.21ms
step:749/2330 train_time:45096ms step_avg:60.21ms
step:750/2330 train_time:45157ms step_avg:60.21ms
step:750/2330 val_loss:3.9401 train_time:45229ms step_avg:60.31ms
step:751/2330 train_time:45250ms step_avg:60.25ms
step:752/2330 train_time:45281ms step_avg:60.21ms
step:753/2330 train_time:45342ms step_avg:60.22ms
step:754/2330 train_time:45409ms step_avg:60.22ms
step:755/2330 train_time:45468ms step_avg:60.22ms
step:756/2330 train_time:45530ms step_avg:60.22ms
step:757/2330 train_time:45590ms step_avg:60.22ms
step:758/2330 train_time:45651ms step_avg:60.23ms
step:759/2330 train_time:45709ms step_avg:60.22ms
step:760/2330 train_time:45770ms step_avg:60.22ms
step:761/2330 train_time:45829ms step_avg:60.22ms
step:762/2330 train_time:45890ms step_avg:60.22ms
step:763/2330 train_time:45948ms step_avg:60.22ms
step:764/2330 train_time:46009ms step_avg:60.22ms
step:765/2330 train_time:46068ms step_avg:60.22ms
step:766/2330 train_time:46130ms step_avg:60.22ms
step:767/2330 train_time:46189ms step_avg:60.22ms
step:768/2330 train_time:46252ms step_avg:60.22ms
step:769/2330 train_time:46313ms step_avg:60.22ms
step:770/2330 train_time:46379ms step_avg:60.23ms
step:771/2330 train_time:46439ms step_avg:60.23ms
step:772/2330 train_time:46502ms step_avg:60.24ms
step:773/2330 train_time:46561ms step_avg:60.23ms
step:774/2330 train_time:46624ms step_avg:60.24ms
step:775/2330 train_time:46684ms step_avg:60.24ms
step:776/2330 train_time:46746ms step_avg:60.24ms
step:777/2330 train_time:46805ms step_avg:60.24ms
step:778/2330 train_time:46867ms step_avg:60.24ms
step:779/2330 train_time:46926ms step_avg:60.24ms
step:780/2330 train_time:46988ms step_avg:60.24ms
step:781/2330 train_time:47047ms step_avg:60.24ms
step:782/2330 train_time:47109ms step_avg:60.24ms
step:783/2330 train_time:47169ms step_avg:60.24ms
step:784/2330 train_time:47232ms step_avg:60.24ms
step:785/2330 train_time:47293ms step_avg:60.25ms
step:786/2330 train_time:47357ms step_avg:60.25ms
step:787/2330 train_time:47417ms step_avg:60.25ms
step:788/2330 train_time:47479ms step_avg:60.25ms
step:789/2330 train_time:47539ms step_avg:60.25ms
step:790/2330 train_time:47602ms step_avg:60.26ms
step:791/2330 train_time:47661ms step_avg:60.25ms
step:792/2330 train_time:47723ms step_avg:60.26ms
step:793/2330 train_time:47782ms step_avg:60.25ms
step:794/2330 train_time:47844ms step_avg:60.26ms
step:795/2330 train_time:47903ms step_avg:60.26ms
step:796/2330 train_time:47965ms step_avg:60.26ms
step:797/2330 train_time:48024ms step_avg:60.26ms
step:798/2330 train_time:48087ms step_avg:60.26ms
step:799/2330 train_time:48146ms step_avg:60.26ms
step:800/2330 train_time:48209ms step_avg:60.26ms
step:801/2330 train_time:48269ms step_avg:60.26ms
step:802/2330 train_time:48332ms step_avg:60.26ms
step:803/2330 train_time:48393ms step_avg:60.26ms
step:804/2330 train_time:48457ms step_avg:60.27ms
step:805/2330 train_time:48516ms step_avg:60.27ms
step:806/2330 train_time:48580ms step_avg:60.27ms
step:807/2330 train_time:48639ms step_avg:60.27ms
step:808/2330 train_time:48701ms step_avg:60.27ms
step:809/2330 train_time:48760ms step_avg:60.27ms
step:810/2330 train_time:48823ms step_avg:60.28ms
step:811/2330 train_time:48882ms step_avg:60.27ms
step:812/2330 train_time:48944ms step_avg:60.28ms
step:813/2330 train_time:49003ms step_avg:60.27ms
step:814/2330 train_time:49066ms step_avg:60.28ms
step:815/2330 train_time:49125ms step_avg:60.28ms
step:816/2330 train_time:49188ms step_avg:60.28ms
step:817/2330 train_time:49247ms step_avg:60.28ms
step:818/2330 train_time:49309ms step_avg:60.28ms
step:819/2330 train_time:49371ms step_avg:60.28ms
step:820/2330 train_time:49435ms step_avg:60.29ms
step:821/2330 train_time:49495ms step_avg:60.29ms
step:822/2330 train_time:49557ms step_avg:60.29ms
step:823/2330 train_time:49615ms step_avg:60.29ms
step:824/2330 train_time:49678ms step_avg:60.29ms
step:825/2330 train_time:49736ms step_avg:60.29ms
step:826/2330 train_time:49799ms step_avg:60.29ms
step:827/2330 train_time:49859ms step_avg:60.29ms
step:828/2330 train_time:49921ms step_avg:60.29ms
step:829/2330 train_time:49981ms step_avg:60.29ms
step:830/2330 train_time:50043ms step_avg:60.29ms
step:831/2330 train_time:50103ms step_avg:60.29ms
step:832/2330 train_time:50165ms step_avg:60.29ms
step:833/2330 train_time:50224ms step_avg:60.29ms
step:834/2330 train_time:50287ms step_avg:60.30ms
step:835/2330 train_time:50348ms step_avg:60.30ms
step:836/2330 train_time:50410ms step_avg:60.30ms
step:837/2330 train_time:50470ms step_avg:60.30ms
step:838/2330 train_time:50534ms step_avg:60.30ms
step:839/2330 train_time:50593ms step_avg:60.30ms
step:840/2330 train_time:50655ms step_avg:60.30ms
step:841/2330 train_time:50714ms step_avg:60.30ms
step:842/2330 train_time:50777ms step_avg:60.31ms
step:843/2330 train_time:50837ms step_avg:60.30ms
step:844/2330 train_time:50899ms step_avg:60.31ms
step:845/2330 train_time:50959ms step_avg:60.31ms
step:846/2330 train_time:51022ms step_avg:60.31ms
step:847/2330 train_time:51082ms step_avg:60.31ms
step:848/2330 train_time:51144ms step_avg:60.31ms
step:849/2330 train_time:51204ms step_avg:60.31ms
step:850/2330 train_time:51266ms step_avg:60.31ms
step:851/2330 train_time:51326ms step_avg:60.31ms
step:852/2330 train_time:51388ms step_avg:60.31ms
step:853/2330 train_time:51448ms step_avg:60.31ms
step:854/2330 train_time:51511ms step_avg:60.32ms
step:855/2330 train_time:51571ms step_avg:60.32ms
step:856/2330 train_time:51634ms step_avg:60.32ms
step:857/2330 train_time:51694ms step_avg:60.32ms
step:858/2330 train_time:51757ms step_avg:60.32ms
step:859/2330 train_time:51816ms step_avg:60.32ms
step:860/2330 train_time:51878ms step_avg:60.32ms
step:861/2330 train_time:51938ms step_avg:60.32ms
step:862/2330 train_time:52000ms step_avg:60.32ms
step:863/2330 train_time:52059ms step_avg:60.32ms
step:864/2330 train_time:52122ms step_avg:60.33ms
step:865/2330 train_time:52181ms step_avg:60.32ms
step:866/2330 train_time:52244ms step_avg:60.33ms
step:867/2330 train_time:52304ms step_avg:60.33ms
step:868/2330 train_time:52366ms step_avg:60.33ms
step:869/2330 train_time:52426ms step_avg:60.33ms
step:870/2330 train_time:52489ms step_avg:60.33ms
step:871/2330 train_time:52549ms step_avg:60.33ms
step:872/2330 train_time:52611ms step_avg:60.33ms
step:873/2330 train_time:52672ms step_avg:60.33ms
step:874/2330 train_time:52736ms step_avg:60.34ms
step:875/2330 train_time:52795ms step_avg:60.34ms
step:876/2330 train_time:52857ms step_avg:60.34ms
step:877/2330 train_time:52917ms step_avg:60.34ms
step:878/2330 train_time:52979ms step_avg:60.34ms
step:879/2330 train_time:53038ms step_avg:60.34ms
step:880/2330 train_time:53101ms step_avg:60.34ms
step:881/2330 train_time:53160ms step_avg:60.34ms
step:882/2330 train_time:53224ms step_avg:60.34ms
step:883/2330 train_time:53284ms step_avg:60.34ms
step:884/2330 train_time:53345ms step_avg:60.35ms
step:885/2330 train_time:53405ms step_avg:60.34ms
step:886/2330 train_time:53467ms step_avg:60.35ms
step:887/2330 train_time:53527ms step_avg:60.35ms
step:888/2330 train_time:53590ms step_avg:60.35ms
step:889/2330 train_time:53650ms step_avg:60.35ms
step:890/2330 train_time:53712ms step_avg:60.35ms
step:891/2330 train_time:53773ms step_avg:60.35ms
step:892/2330 train_time:53836ms step_avg:60.35ms
step:893/2330 train_time:53896ms step_avg:60.35ms
step:894/2330 train_time:53958ms step_avg:60.36ms
step:895/2330 train_time:54018ms step_avg:60.36ms
step:896/2330 train_time:54081ms step_avg:60.36ms
step:897/2330 train_time:54141ms step_avg:60.36ms
step:898/2330 train_time:54203ms step_avg:60.36ms
step:899/2330 train_time:54262ms step_avg:60.36ms
step:900/2330 train_time:54324ms step_avg:60.36ms
step:901/2330 train_time:54384ms step_avg:60.36ms
step:902/2330 train_time:54446ms step_avg:60.36ms
step:903/2330 train_time:54505ms step_avg:60.36ms
step:904/2330 train_time:54569ms step_avg:60.36ms
step:905/2330 train_time:54629ms step_avg:60.36ms
step:906/2330 train_time:54692ms step_avg:60.37ms
step:907/2330 train_time:54751ms step_avg:60.37ms
step:908/2330 train_time:54815ms step_avg:60.37ms
step:909/2330 train_time:54874ms step_avg:60.37ms
step:910/2330 train_time:54937ms step_avg:60.37ms
step:911/2330 train_time:54995ms step_avg:60.37ms
step:912/2330 train_time:55058ms step_avg:60.37ms
step:913/2330 train_time:55117ms step_avg:60.37ms
step:914/2330 train_time:55179ms step_avg:60.37ms
step:915/2330 train_time:55239ms step_avg:60.37ms
step:916/2330 train_time:55301ms step_avg:60.37ms
step:917/2330 train_time:55361ms step_avg:60.37ms
step:918/2330 train_time:55424ms step_avg:60.37ms
step:919/2330 train_time:55483ms step_avg:60.37ms
step:920/2330 train_time:55546ms step_avg:60.38ms
step:921/2330 train_time:55605ms step_avg:60.38ms
step:922/2330 train_time:55668ms step_avg:60.38ms
step:923/2330 train_time:55728ms step_avg:60.38ms
step:924/2330 train_time:55791ms step_avg:60.38ms
step:925/2330 train_time:55852ms step_avg:60.38ms
step:926/2330 train_time:55914ms step_avg:60.38ms
step:927/2330 train_time:55974ms step_avg:60.38ms
step:928/2330 train_time:56037ms step_avg:60.38ms
step:929/2330 train_time:56096ms step_avg:60.38ms
step:930/2330 train_time:56158ms step_avg:60.38ms
step:931/2330 train_time:56217ms step_avg:60.38ms
step:932/2330 train_time:56279ms step_avg:60.38ms
step:933/2330 train_time:56339ms step_avg:60.38ms
step:934/2330 train_time:56401ms step_avg:60.39ms
step:935/2330 train_time:56461ms step_avg:60.39ms
step:936/2330 train_time:56524ms step_avg:60.39ms
step:937/2330 train_time:56583ms step_avg:60.39ms
step:938/2330 train_time:56646ms step_avg:60.39ms
step:939/2330 train_time:56705ms step_avg:60.39ms
step:940/2330 train_time:56768ms step_avg:60.39ms
step:941/2330 train_time:56827ms step_avg:60.39ms
step:942/2330 train_time:56891ms step_avg:60.39ms
step:943/2330 train_time:56951ms step_avg:60.39ms
step:944/2330 train_time:57013ms step_avg:60.40ms
step:945/2330 train_time:57073ms step_avg:60.39ms
step:946/2330 train_time:57137ms step_avg:60.40ms
step:947/2330 train_time:57196ms step_avg:60.40ms
step:948/2330 train_time:57258ms step_avg:60.40ms
step:949/2330 train_time:57318ms step_avg:60.40ms
step:950/2330 train_time:57380ms step_avg:60.40ms
step:951/2330 train_time:57439ms step_avg:60.40ms
step:952/2330 train_time:57502ms step_avg:60.40ms
step:953/2330 train_time:57562ms step_avg:60.40ms
step:954/2330 train_time:57625ms step_avg:60.40ms
step:955/2330 train_time:57684ms step_avg:60.40ms
step:956/2330 train_time:57747ms step_avg:60.40ms
step:957/2330 train_time:57807ms step_avg:60.40ms
step:958/2330 train_time:57869ms step_avg:60.41ms
step:959/2330 train_time:57929ms step_avg:60.41ms
step:960/2330 train_time:57992ms step_avg:60.41ms
step:961/2330 train_time:58052ms step_avg:60.41ms
step:962/2330 train_time:58115ms step_avg:60.41ms
step:963/2330 train_time:58175ms step_avg:60.41ms
step:964/2330 train_time:58237ms step_avg:60.41ms
step:965/2330 train_time:58296ms step_avg:60.41ms
step:966/2330 train_time:58358ms step_avg:60.41ms
step:967/2330 train_time:58417ms step_avg:60.41ms
step:968/2330 train_time:58479ms step_avg:60.41ms
step:969/2330 train_time:58539ms step_avg:60.41ms
step:970/2330 train_time:58603ms step_avg:60.42ms
step:971/2330 train_time:58662ms step_avg:60.41ms
step:972/2330 train_time:58725ms step_avg:60.42ms
step:973/2330 train_time:58785ms step_avg:60.42ms
step:974/2330 train_time:58847ms step_avg:60.42ms
step:975/2330 train_time:58907ms step_avg:60.42ms
step:976/2330 train_time:58969ms step_avg:60.42ms
step:977/2330 train_time:59030ms step_avg:60.42ms
step:978/2330 train_time:59093ms step_avg:60.42ms
step:979/2330 train_time:59153ms step_avg:60.42ms
step:980/2330 train_time:59214ms step_avg:60.42ms
step:981/2330 train_time:59274ms step_avg:60.42ms
step:982/2330 train_time:59338ms step_avg:60.43ms
step:983/2330 train_time:59396ms step_avg:60.42ms
step:984/2330 train_time:59459ms step_avg:60.43ms
step:985/2330 train_time:59518ms step_avg:60.42ms
step:986/2330 train_time:59580ms step_avg:60.43ms
step:987/2330 train_time:59640ms step_avg:60.43ms
step:988/2330 train_time:59704ms step_avg:60.43ms
step:989/2330 train_time:59764ms step_avg:60.43ms
step:990/2330 train_time:59826ms step_avg:60.43ms
step:991/2330 train_time:59885ms step_avg:60.43ms
step:992/2330 train_time:59948ms step_avg:60.43ms
step:993/2330 train_time:60007ms step_avg:60.43ms
step:994/2330 train_time:60071ms step_avg:60.43ms
step:995/2330 train_time:60132ms step_avg:60.43ms
step:996/2330 train_time:60193ms step_avg:60.44ms
step:997/2330 train_time:60254ms step_avg:60.43ms
step:998/2330 train_time:60316ms step_avg:60.44ms
step:999/2330 train_time:60376ms step_avg:60.44ms
step:1000/2330 train_time:60439ms step_avg:60.44ms
step:1000/2330 val_loss:3.7691 train_time:60511ms step_avg:60.51ms
step:1001/2330 train_time:60532ms step_avg:60.47ms
step:1002/2330 train_time:60561ms step_avg:60.44ms
step:1003/2330 train_time:60624ms step_avg:60.44ms
step:1004/2330 train_time:60694ms step_avg:60.45ms
step:1005/2330 train_time:60757ms step_avg:60.46ms
step:1006/2330 train_time:60821ms step_avg:60.46ms
step:1007/2330 train_time:60879ms step_avg:60.46ms
step:1008/2330 train_time:60941ms step_avg:60.46ms
step:1009/2330 train_time:61000ms step_avg:60.46ms
step:1010/2330 train_time:61062ms step_avg:60.46ms
step:1011/2330 train_time:61121ms step_avg:60.46ms
step:1012/2330 train_time:61182ms step_avg:60.46ms
step:1013/2330 train_time:61240ms step_avg:60.45ms
step:1014/2330 train_time:61302ms step_avg:60.46ms
step:1015/2330 train_time:61361ms step_avg:60.45ms
step:1016/2330 train_time:61423ms step_avg:60.46ms
step:1017/2330 train_time:61484ms step_avg:60.46ms
step:1018/2330 train_time:61549ms step_avg:60.46ms
step:1019/2330 train_time:61611ms step_avg:60.46ms
step:1020/2330 train_time:61676ms step_avg:60.47ms
step:1021/2330 train_time:61736ms step_avg:60.47ms
step:1022/2330 train_time:61800ms step_avg:60.47ms
step:1023/2330 train_time:61859ms step_avg:60.47ms
step:1024/2330 train_time:61922ms step_avg:60.47ms
step:1025/2330 train_time:61980ms step_avg:60.47ms
step:1026/2330 train_time:62042ms step_avg:60.47ms
step:1027/2330 train_time:62101ms step_avg:60.47ms
step:1028/2330 train_time:62163ms step_avg:60.47ms
step:1029/2330 train_time:62221ms step_avg:60.47ms
step:1030/2330 train_time:62284ms step_avg:60.47ms
step:1031/2330 train_time:62343ms step_avg:60.47ms
step:1032/2330 train_time:62405ms step_avg:60.47ms
step:1033/2330 train_time:62465ms step_avg:60.47ms
step:1034/2330 train_time:62529ms step_avg:60.47ms
step:1035/2330 train_time:62590ms step_avg:60.47ms
step:1036/2330 train_time:62653ms step_avg:60.48ms
step:1037/2330 train_time:62712ms step_avg:60.47ms
step:1038/2330 train_time:62776ms step_avg:60.48ms
step:1039/2330 train_time:62837ms step_avg:60.48ms
step:1040/2330 train_time:62900ms step_avg:60.48ms
step:1041/2330 train_time:62959ms step_avg:60.48ms
step:1042/2330 train_time:63022ms step_avg:60.48ms
step:1043/2330 train_time:63080ms step_avg:60.48ms
step:1044/2330 train_time:63142ms step_avg:60.48ms
step:1045/2330 train_time:63201ms step_avg:60.48ms
step:1046/2330 train_time:63263ms step_avg:60.48ms
step:1047/2330 train_time:63322ms step_avg:60.48ms
step:1048/2330 train_time:63383ms step_avg:60.48ms
step:1049/2330 train_time:63443ms step_avg:60.48ms
step:1050/2330 train_time:63506ms step_avg:60.48ms
step:1051/2330 train_time:63566ms step_avg:60.48ms
step:1052/2330 train_time:63631ms step_avg:60.49ms
step:1053/2330 train_time:63691ms step_avg:60.49ms
step:1054/2330 train_time:63754ms step_avg:60.49ms
step:1055/2330 train_time:63814ms step_avg:60.49ms
step:1056/2330 train_time:63877ms step_avg:60.49ms
step:1057/2330 train_time:63937ms step_avg:60.49ms
step:1058/2330 train_time:63998ms step_avg:60.49ms
step:1059/2330 train_time:64058ms step_avg:60.49ms
step:1060/2330 train_time:64121ms step_avg:60.49ms
step:1061/2330 train_time:64180ms step_avg:60.49ms
step:1062/2330 train_time:64243ms step_avg:60.49ms
step:1063/2330 train_time:64301ms step_avg:60.49ms
step:1064/2330 train_time:64363ms step_avg:60.49ms
step:1065/2330 train_time:64423ms step_avg:60.49ms
step:1066/2330 train_time:64485ms step_avg:60.49ms
step:1067/2330 train_time:64545ms step_avg:60.49ms
step:1068/2330 train_time:64608ms step_avg:60.49ms
step:1069/2330 train_time:64668ms step_avg:60.49ms
step:1070/2330 train_time:64731ms step_avg:60.50ms
step:1071/2330 train_time:64791ms step_avg:60.50ms
step:1072/2330 train_time:64853ms step_avg:60.50ms
step:1073/2330 train_time:64913ms step_avg:60.50ms
step:1074/2330 train_time:64976ms step_avg:60.50ms
step:1075/2330 train_time:65035ms step_avg:60.50ms
step:1076/2330 train_time:65097ms step_avg:60.50ms
step:1077/2330 train_time:65157ms step_avg:60.50ms
step:1078/2330 train_time:65220ms step_avg:60.50ms
step:1079/2330 train_time:65279ms step_avg:60.50ms
step:1080/2330 train_time:65342ms step_avg:60.50ms
step:1081/2330 train_time:65401ms step_avg:60.50ms
step:1082/2330 train_time:65464ms step_avg:60.50ms
step:1083/2330 train_time:65523ms step_avg:60.50ms
step:1084/2330 train_time:65585ms step_avg:60.50ms
step:1085/2330 train_time:65645ms step_avg:60.50ms
step:1086/2330 train_time:65709ms step_avg:60.51ms
step:1087/2330 train_time:65768ms step_avg:60.50ms
step:1088/2330 train_time:65831ms step_avg:60.51ms
step:1089/2330 train_time:65890ms step_avg:60.51ms
step:1090/2330 train_time:65952ms step_avg:60.51ms
step:1091/2330 train_time:66011ms step_avg:60.50ms
step:1092/2330 train_time:66074ms step_avg:60.51ms
step:1093/2330 train_time:66133ms step_avg:60.51ms
step:1094/2330 train_time:66196ms step_avg:60.51ms
step:1095/2330 train_time:66256ms step_avg:60.51ms
step:1096/2330 train_time:66319ms step_avg:60.51ms
step:1097/2330 train_time:66377ms step_avg:60.51ms
step:1098/2330 train_time:66440ms step_avg:60.51ms
step:1099/2330 train_time:66500ms step_avg:60.51ms
step:1100/2330 train_time:66563ms step_avg:60.51ms
step:1101/2330 train_time:66623ms step_avg:60.51ms
step:1102/2330 train_time:66686ms step_avg:60.51ms
step:1103/2330 train_time:66744ms step_avg:60.51ms
step:1104/2330 train_time:66807ms step_avg:60.51ms
step:1105/2330 train_time:66867ms step_avg:60.51ms
step:1106/2330 train_time:66930ms step_avg:60.52ms
step:1107/2330 train_time:66990ms step_avg:60.51ms
step:1108/2330 train_time:67051ms step_avg:60.52ms
step:1109/2330 train_time:67111ms step_avg:60.51ms
step:1110/2330 train_time:67173ms step_avg:60.52ms
step:1111/2330 train_time:67232ms step_avg:60.52ms
step:1112/2330 train_time:67295ms step_avg:60.52ms
step:1113/2330 train_time:67356ms step_avg:60.52ms
step:1114/2330 train_time:67419ms step_avg:60.52ms
step:1115/2330 train_time:67479ms step_avg:60.52ms
step:1116/2330 train_time:67542ms step_avg:60.52ms
step:1117/2330 train_time:67601ms step_avg:60.52ms
step:1118/2330 train_time:67664ms step_avg:60.52ms
step:1119/2330 train_time:67725ms step_avg:60.52ms
step:1120/2330 train_time:67786ms step_avg:60.52ms
step:1121/2330 train_time:67845ms step_avg:60.52ms
step:1122/2330 train_time:67908ms step_avg:60.52ms
step:1123/2330 train_time:67968ms step_avg:60.52ms
step:1124/2330 train_time:68031ms step_avg:60.53ms
step:1125/2330 train_time:68090ms step_avg:60.52ms
step:1126/2330 train_time:68152ms step_avg:60.53ms
step:1127/2330 train_time:68211ms step_avg:60.52ms
step:1128/2330 train_time:68274ms step_avg:60.53ms
step:1129/2330 train_time:68334ms step_avg:60.53ms
step:1130/2330 train_time:68396ms step_avg:60.53ms
step:1131/2330 train_time:68457ms step_avg:60.53ms
step:1132/2330 train_time:68520ms step_avg:60.53ms
step:1133/2330 train_time:68580ms step_avg:60.53ms
step:1134/2330 train_time:68643ms step_avg:60.53ms
step:1135/2330 train_time:68703ms step_avg:60.53ms
step:1136/2330 train_time:68766ms step_avg:60.53ms
step:1137/2330 train_time:68826ms step_avg:60.53ms
step:1138/2330 train_time:68888ms step_avg:60.53ms
step:1139/2330 train_time:68947ms step_avg:60.53ms
step:1140/2330 train_time:69010ms step_avg:60.54ms
step:1141/2330 train_time:69069ms step_avg:60.53ms
step:1142/2330 train_time:69132ms step_avg:60.54ms
step:1143/2330 train_time:69191ms step_avg:60.53ms
step:1144/2330 train_time:69253ms step_avg:60.54ms
step:1145/2330 train_time:69312ms step_avg:60.53ms
step:1146/2330 train_time:69375ms step_avg:60.54ms
step:1147/2330 train_time:69436ms step_avg:60.54ms
step:1148/2330 train_time:69498ms step_avg:60.54ms
step:1149/2330 train_time:69558ms step_avg:60.54ms
step:1150/2330 train_time:69622ms step_avg:60.54ms
step:1151/2330 train_time:69681ms step_avg:60.54ms
step:1152/2330 train_time:69744ms step_avg:60.54ms
step:1153/2330 train_time:69803ms step_avg:60.54ms
step:1154/2330 train_time:69866ms step_avg:60.54ms
step:1155/2330 train_time:69925ms step_avg:60.54ms
step:1156/2330 train_time:69986ms step_avg:60.54ms
step:1157/2330 train_time:70046ms step_avg:60.54ms
step:1158/2330 train_time:70109ms step_avg:60.54ms
step:1159/2330 train_time:70168ms step_avg:60.54ms
step:1160/2330 train_time:70231ms step_avg:60.54ms
step:1161/2330 train_time:70291ms step_avg:60.54ms
step:1162/2330 train_time:70353ms step_avg:60.54ms
step:1163/2330 train_time:70412ms step_avg:60.54ms
step:1164/2330 train_time:70475ms step_avg:60.55ms
step:1165/2330 train_time:70535ms step_avg:60.55ms
step:1166/2330 train_time:70599ms step_avg:60.55ms
step:1167/2330 train_time:70659ms step_avg:60.55ms
step:1168/2330 train_time:70723ms step_avg:60.55ms
step:1169/2330 train_time:70782ms step_avg:60.55ms
step:1170/2330 train_time:70844ms step_avg:60.55ms
step:1171/2330 train_time:70903ms step_avg:60.55ms
step:1172/2330 train_time:70965ms step_avg:60.55ms
step:1173/2330 train_time:71025ms step_avg:60.55ms
step:1174/2330 train_time:71086ms step_avg:60.55ms
step:1175/2330 train_time:71145ms step_avg:60.55ms
step:1176/2330 train_time:71208ms step_avg:60.55ms
step:1177/2330 train_time:71267ms step_avg:60.55ms
step:1178/2330 train_time:71330ms step_avg:60.55ms
step:1179/2330 train_time:71390ms step_avg:60.55ms
step:1180/2330 train_time:71454ms step_avg:60.55ms
step:1181/2330 train_time:71514ms step_avg:60.55ms
step:1182/2330 train_time:71576ms step_avg:60.56ms
step:1183/2330 train_time:71635ms step_avg:60.55ms
step:1184/2330 train_time:71698ms step_avg:60.56ms
step:1185/2330 train_time:71758ms step_avg:60.56ms
step:1186/2330 train_time:71822ms step_avg:60.56ms
step:1187/2330 train_time:71882ms step_avg:60.56ms
step:1188/2330 train_time:71944ms step_avg:60.56ms
step:1189/2330 train_time:72002ms step_avg:60.56ms
step:1190/2330 train_time:72065ms step_avg:60.56ms
step:1191/2330 train_time:72125ms step_avg:60.56ms
step:1192/2330 train_time:72187ms step_avg:60.56ms
step:1193/2330 train_time:72246ms step_avg:60.56ms
step:1194/2330 train_time:72309ms step_avg:60.56ms
step:1195/2330 train_time:72369ms step_avg:60.56ms
step:1196/2330 train_time:72432ms step_avg:60.56ms
step:1197/2330 train_time:72491ms step_avg:60.56ms
step:1198/2330 train_time:72554ms step_avg:60.56ms
step:1199/2330 train_time:72613ms step_avg:60.56ms
step:1200/2330 train_time:72676ms step_avg:60.56ms
step:1201/2330 train_time:72737ms step_avg:60.56ms
step:1202/2330 train_time:72800ms step_avg:60.57ms
step:1203/2330 train_time:72860ms step_avg:60.57ms
step:1204/2330 train_time:72922ms step_avg:60.57ms
step:1205/2330 train_time:72981ms step_avg:60.56ms
step:1206/2330 train_time:73043ms step_avg:60.57ms
step:1207/2330 train_time:73102ms step_avg:60.57ms
step:1208/2330 train_time:73165ms step_avg:60.57ms
step:1209/2330 train_time:73225ms step_avg:60.57ms
step:1210/2330 train_time:73288ms step_avg:60.57ms
step:1211/2330 train_time:73347ms step_avg:60.57ms
step:1212/2330 train_time:73410ms step_avg:60.57ms
step:1213/2330 train_time:73469ms step_avg:60.57ms
step:1214/2330 train_time:73532ms step_avg:60.57ms
step:1215/2330 train_time:73592ms step_avg:60.57ms
step:1216/2330 train_time:73654ms step_avg:60.57ms
step:1217/2330 train_time:73714ms step_avg:60.57ms
step:1218/2330 train_time:73778ms step_avg:60.57ms
step:1219/2330 train_time:73837ms step_avg:60.57ms
step:1220/2330 train_time:73899ms step_avg:60.57ms
step:1221/2330 train_time:73959ms step_avg:60.57ms
step:1222/2330 train_time:74024ms step_avg:60.58ms
step:1223/2330 train_time:74082ms step_avg:60.57ms
step:1224/2330 train_time:74145ms step_avg:60.58ms
step:1225/2330 train_time:74203ms step_avg:60.57ms
step:1226/2330 train_time:74266ms step_avg:60.58ms
step:1227/2330 train_time:74326ms step_avg:60.58ms
step:1228/2330 train_time:74388ms step_avg:60.58ms
step:1229/2330 train_time:74448ms step_avg:60.58ms
step:1230/2330 train_time:74512ms step_avg:60.58ms
step:1231/2330 train_time:74570ms step_avg:60.58ms
step:1232/2330 train_time:74634ms step_avg:60.58ms
step:1233/2330 train_time:74693ms step_avg:60.58ms
step:1234/2330 train_time:74756ms step_avg:60.58ms
step:1235/2330 train_time:74816ms step_avg:60.58ms
step:1236/2330 train_time:74877ms step_avg:60.58ms
step:1237/2330 train_time:74937ms step_avg:60.58ms
step:1238/2330 train_time:75000ms step_avg:60.58ms
step:1239/2330 train_time:75060ms step_avg:60.58ms
step:1240/2330 train_time:75124ms step_avg:60.58ms
step:1241/2330 train_time:75182ms step_avg:60.58ms
step:1242/2330 train_time:75246ms step_avg:60.58ms
step:1243/2330 train_time:75305ms step_avg:60.58ms
step:1244/2330 train_time:75367ms step_avg:60.58ms
step:1245/2330 train_time:75427ms step_avg:60.58ms
step:1246/2330 train_time:75491ms step_avg:60.59ms
step:1247/2330 train_time:75550ms step_avg:60.59ms
step:1248/2330 train_time:75612ms step_avg:60.59ms
step:1249/2330 train_time:75672ms step_avg:60.59ms
step:1250/2330 train_time:75735ms step_avg:60.59ms
step:1250/2330 val_loss:3.7072 train_time:75807ms step_avg:60.65ms
step:1251/2330 train_time:75827ms step_avg:60.61ms
step:1252/2330 train_time:75860ms step_avg:60.59ms
step:1253/2330 train_time:75922ms step_avg:60.59ms
step:1254/2330 train_time:75985ms step_avg:60.59ms
step:1255/2330 train_time:76045ms step_avg:60.59ms
step:1256/2330 train_time:76107ms step_avg:60.59ms
step:1257/2330 train_time:76166ms step_avg:60.59ms
step:1258/2330 train_time:76227ms step_avg:60.59ms
step:1259/2330 train_time:76286ms step_avg:60.59ms
step:1260/2330 train_time:76348ms step_avg:60.59ms
step:1261/2330 train_time:76407ms step_avg:60.59ms
step:1262/2330 train_time:76469ms step_avg:60.59ms
step:1263/2330 train_time:76528ms step_avg:60.59ms
step:1264/2330 train_time:76589ms step_avg:60.59ms
step:1265/2330 train_time:76648ms step_avg:60.59ms
step:1266/2330 train_time:76710ms step_avg:60.59ms
step:1267/2330 train_time:76773ms step_avg:60.59ms
step:1268/2330 train_time:76838ms step_avg:60.60ms
step:1269/2330 train_time:76898ms step_avg:60.60ms
step:1270/2330 train_time:76961ms step_avg:60.60ms
step:1271/2330 train_time:77021ms step_avg:60.60ms
step:1272/2330 train_time:77083ms step_avg:60.60ms
step:1273/2330 train_time:77142ms step_avg:60.60ms
step:1274/2330 train_time:77204ms step_avg:60.60ms
step:1275/2330 train_time:77264ms step_avg:60.60ms
step:1276/2330 train_time:77326ms step_avg:60.60ms
step:1277/2330 train_time:77385ms step_avg:60.60ms
step:1278/2330 train_time:77447ms step_avg:60.60ms
step:1279/2330 train_time:77505ms step_avg:60.60ms
step:1280/2330 train_time:77568ms step_avg:60.60ms
step:1281/2330 train_time:77627ms step_avg:60.60ms
step:1282/2330 train_time:77689ms step_avg:60.60ms
step:1283/2330 train_time:77749ms step_avg:60.60ms
step:1284/2330 train_time:77813ms step_avg:60.60ms
step:1285/2330 train_time:77873ms step_avg:60.60ms
step:1286/2330 train_time:77936ms step_avg:60.60ms
step:1287/2330 train_time:77997ms step_avg:60.60ms
step:1288/2330 train_time:78059ms step_avg:60.60ms
step:1289/2330 train_time:78118ms step_avg:60.60ms
step:1290/2330 train_time:78181ms step_avg:60.61ms
step:1291/2330 train_time:78240ms step_avg:60.60ms
step:1292/2330 train_time:78302ms step_avg:60.61ms
step:1293/2330 train_time:78361ms step_avg:60.60ms
step:1294/2330 train_time:78423ms step_avg:60.61ms
step:1295/2330 train_time:78482ms step_avg:60.60ms
step:1296/2330 train_time:78545ms step_avg:60.61ms
step:1297/2330 train_time:78604ms step_avg:60.60ms
step:1298/2330 train_time:78666ms step_avg:60.61ms
step:1299/2330 train_time:78725ms step_avg:60.60ms
step:1300/2330 train_time:78788ms step_avg:60.61ms
step:1301/2330 train_time:78847ms step_avg:60.61ms
step:1302/2330 train_time:78909ms step_avg:60.61ms
step:1303/2330 train_time:78970ms step_avg:60.61ms
step:1304/2330 train_time:79034ms step_avg:60.61ms
step:1305/2330 train_time:79093ms step_avg:60.61ms
step:1306/2330 train_time:79156ms step_avg:60.61ms
step:1307/2330 train_time:79216ms step_avg:60.61ms
step:1308/2330 train_time:79278ms step_avg:60.61ms
step:1309/2330 train_time:79338ms step_avg:60.61ms
step:1310/2330 train_time:79401ms step_avg:60.61ms
step:1311/2330 train_time:79459ms step_avg:60.61ms
step:1312/2330 train_time:79521ms step_avg:60.61ms
step:1313/2330 train_time:79581ms step_avg:60.61ms
step:1314/2330 train_time:79643ms step_avg:60.61ms
step:1315/2330 train_time:79701ms step_avg:60.61ms
step:1316/2330 train_time:79764ms step_avg:60.61ms
step:1317/2330 train_time:79823ms step_avg:60.61ms
step:1318/2330 train_time:79886ms step_avg:60.61ms
step:1319/2330 train_time:79946ms step_avg:60.61ms
step:1320/2330 train_time:80009ms step_avg:60.61ms
step:1321/2330 train_time:80069ms step_avg:60.61ms
step:1322/2330 train_time:80132ms step_avg:60.61ms
step:1323/2330 train_time:80191ms step_avg:60.61ms
step:1324/2330 train_time:80254ms step_avg:60.61ms
step:1325/2330 train_time:80314ms step_avg:60.61ms
step:1326/2330 train_time:80377ms step_avg:60.62ms
step:1327/2330 train_time:80437ms step_avg:60.62ms
step:1328/2330 train_time:80500ms step_avg:60.62ms
step:1329/2330 train_time:80560ms step_avg:60.62ms
step:1330/2330 train_time:80622ms step_avg:60.62ms
step:1331/2330 train_time:80681ms step_avg:60.62ms
step:1332/2330 train_time:80743ms step_avg:60.62ms
step:1333/2330 train_time:80802ms step_avg:60.62ms
step:1334/2330 train_time:80865ms step_avg:60.62ms
step:1335/2330 train_time:80925ms step_avg:60.62ms
step:1336/2330 train_time:80988ms step_avg:60.62ms
step:1337/2330 train_time:81048ms step_avg:60.62ms
step:1338/2330 train_time:81111ms step_avg:60.62ms
step:1339/2330 train_time:81170ms step_avg:60.62ms
step:1340/2330 train_time:81233ms step_avg:60.62ms
step:1341/2330 train_time:81292ms step_avg:60.62ms
step:1342/2330 train_time:81355ms step_avg:60.62ms
step:1343/2330 train_time:81414ms step_avg:60.62ms
step:1344/2330 train_time:81477ms step_avg:60.62ms
step:1345/2330 train_time:81537ms step_avg:60.62ms
step:1346/2330 train_time:81599ms step_avg:60.62ms
step:1347/2330 train_time:81658ms step_avg:60.62ms
step:1348/2330 train_time:81721ms step_avg:60.62ms
step:1349/2330 train_time:81781ms step_avg:60.62ms
step:1350/2330 train_time:81844ms step_avg:60.63ms
step:1351/2330 train_time:81902ms step_avg:60.62ms
step:1352/2330 train_time:81965ms step_avg:60.63ms
step:1353/2330 train_time:82024ms step_avg:60.62ms
step:1354/2330 train_time:82087ms step_avg:60.63ms
step:1355/2330 train_time:82146ms step_avg:60.62ms
step:1356/2330 train_time:82209ms step_avg:60.63ms
step:1357/2330 train_time:82268ms step_avg:60.63ms
step:1358/2330 train_time:82332ms step_avg:60.63ms
step:1359/2330 train_time:82392ms step_avg:60.63ms
step:1360/2330 train_time:82455ms step_avg:60.63ms
step:1361/2330 train_time:82514ms step_avg:60.63ms
step:1362/2330 train_time:82577ms step_avg:60.63ms
step:1363/2330 train_time:82637ms step_avg:60.63ms
step:1364/2330 train_time:82699ms step_avg:60.63ms
step:1365/2330 train_time:82759ms step_avg:60.63ms
step:1366/2330 train_time:82822ms step_avg:60.63ms
step:1367/2330 train_time:82881ms step_avg:60.63ms
step:1368/2330 train_time:82945ms step_avg:60.63ms
step:1369/2330 train_time:83003ms step_avg:60.63ms
step:1370/2330 train_time:83065ms step_avg:60.63ms
step:1371/2330 train_time:83124ms step_avg:60.63ms
step:1372/2330 train_time:83188ms step_avg:60.63ms
step:1373/2330 train_time:83247ms step_avg:60.63ms
step:1374/2330 train_time:83310ms step_avg:60.63ms
step:1375/2330 train_time:83370ms step_avg:60.63ms
step:1376/2330 train_time:83433ms step_avg:60.63ms
step:1377/2330 train_time:83491ms step_avg:60.63ms
step:1378/2330 train_time:83554ms step_avg:60.63ms
step:1379/2330 train_time:83613ms step_avg:60.63ms
step:1380/2330 train_time:83676ms step_avg:60.63ms
step:1381/2330 train_time:83737ms step_avg:60.63ms
step:1382/2330 train_time:83799ms step_avg:60.64ms
step:1383/2330 train_time:83859ms step_avg:60.64ms
step:1384/2330 train_time:83922ms step_avg:60.64ms
step:1385/2330 train_time:83980ms step_avg:60.64ms
step:1386/2330 train_time:84043ms step_avg:60.64ms
step:1387/2330 train_time:84101ms step_avg:60.64ms
step:1388/2330 train_time:84164ms step_avg:60.64ms
step:1389/2330 train_time:84223ms step_avg:60.64ms
step:1390/2330 train_time:84286ms step_avg:60.64ms
step:1391/2330 train_time:84346ms step_avg:60.64ms
step:1392/2330 train_time:84409ms step_avg:60.64ms
step:1393/2330 train_time:84469ms step_avg:60.64ms
step:1394/2330 train_time:84531ms step_avg:60.64ms
step:1395/2330 train_time:84590ms step_avg:60.64ms
step:1396/2330 train_time:84653ms step_avg:60.64ms
step:1397/2330 train_time:84713ms step_avg:60.64ms
step:1398/2330 train_time:84776ms step_avg:60.64ms
step:1399/2330 train_time:84836ms step_avg:60.64ms
step:1400/2330 train_time:84899ms step_avg:60.64ms
step:1401/2330 train_time:84959ms step_avg:60.64ms
step:1402/2330 train_time:85022ms step_avg:60.64ms
step:1403/2330 train_time:85082ms step_avg:60.64ms
step:1404/2330 train_time:85144ms step_avg:60.64ms
step:1405/2330 train_time:85203ms step_avg:60.64ms
step:1406/2330 train_time:85266ms step_avg:60.64ms
step:1407/2330 train_time:85325ms step_avg:60.64ms
step:1408/2330 train_time:85387ms step_avg:60.64ms
step:1409/2330 train_time:85447ms step_avg:60.64ms
step:1410/2330 train_time:85509ms step_avg:60.64ms
step:1411/2330 train_time:85568ms step_avg:60.64ms
step:1412/2330 train_time:85631ms step_avg:60.65ms
step:1413/2330 train_time:85691ms step_avg:60.64ms
step:1414/2330 train_time:85754ms step_avg:60.65ms
step:1415/2330 train_time:85814ms step_avg:60.65ms
step:1416/2330 train_time:85876ms step_avg:60.65ms
step:1417/2330 train_time:85937ms step_avg:60.65ms
step:1418/2330 train_time:86000ms step_avg:60.65ms
step:1419/2330 train_time:86059ms step_avg:60.65ms
step:1420/2330 train_time:86122ms step_avg:60.65ms
step:1421/2330 train_time:86181ms step_avg:60.65ms
step:1422/2330 train_time:86245ms step_avg:60.65ms
step:1423/2330 train_time:86304ms step_avg:60.65ms
step:1424/2330 train_time:86366ms step_avg:60.65ms
step:1425/2330 train_time:86425ms step_avg:60.65ms
step:1426/2330 train_time:86488ms step_avg:60.65ms
step:1427/2330 train_time:86548ms step_avg:60.65ms
step:1428/2330 train_time:86611ms step_avg:60.65ms
step:1429/2330 train_time:86670ms step_avg:60.65ms
step:1430/2330 train_time:86734ms step_avg:60.65ms
step:1431/2330 train_time:86793ms step_avg:60.65ms
step:1432/2330 train_time:86856ms step_avg:60.65ms
step:1433/2330 train_time:86916ms step_avg:60.65ms
step:1434/2330 train_time:86978ms step_avg:60.65ms
step:1435/2330 train_time:87040ms step_avg:60.65ms
step:1436/2330 train_time:87102ms step_avg:60.66ms
step:1437/2330 train_time:87161ms step_avg:60.65ms
step:1438/2330 train_time:87224ms step_avg:60.66ms
step:1439/2330 train_time:87283ms step_avg:60.66ms
step:1440/2330 train_time:87346ms step_avg:60.66ms
step:1441/2330 train_time:87404ms step_avg:60.66ms
step:1442/2330 train_time:87466ms step_avg:60.66ms
step:1443/2330 train_time:87526ms step_avg:60.66ms
step:1444/2330 train_time:87588ms step_avg:60.66ms
step:1445/2330 train_time:87648ms step_avg:60.66ms
step:1446/2330 train_time:87710ms step_avg:60.66ms
step:1447/2330 train_time:87770ms step_avg:60.66ms
step:1448/2330 train_time:87832ms step_avg:60.66ms
step:1449/2330 train_time:87892ms step_avg:60.66ms
step:1450/2330 train_time:87956ms step_avg:60.66ms
step:1451/2330 train_time:88016ms step_avg:60.66ms
step:1452/2330 train_time:88078ms step_avg:60.66ms
step:1453/2330 train_time:88139ms step_avg:60.66ms
step:1454/2330 train_time:88202ms step_avg:60.66ms
step:1455/2330 train_time:88260ms step_avg:60.66ms
step:1456/2330 train_time:88323ms step_avg:60.66ms
step:1457/2330 train_time:88381ms step_avg:60.66ms
step:1458/2330 train_time:88445ms step_avg:60.66ms
step:1459/2330 train_time:88504ms step_avg:60.66ms
step:1460/2330 train_time:88566ms step_avg:60.66ms
step:1461/2330 train_time:88626ms step_avg:60.66ms
step:1462/2330 train_time:88689ms step_avg:60.66ms
step:1463/2330 train_time:88749ms step_avg:60.66ms
step:1464/2330 train_time:88811ms step_avg:60.66ms
step:1465/2330 train_time:88871ms step_avg:60.66ms
step:1466/2330 train_time:88935ms step_avg:60.66ms
step:1467/2330 train_time:88994ms step_avg:60.66ms
step:1468/2330 train_time:89057ms step_avg:60.67ms
step:1469/2330 train_time:89117ms step_avg:60.66ms
step:1470/2330 train_time:89179ms step_avg:60.67ms
step:1471/2330 train_time:89239ms step_avg:60.67ms
step:1472/2330 train_time:89302ms step_avg:60.67ms
step:1473/2330 train_time:89360ms step_avg:60.67ms
step:1474/2330 train_time:89422ms step_avg:60.67ms
step:1475/2330 train_time:89481ms step_avg:60.67ms
step:1476/2330 train_time:89544ms step_avg:60.67ms
step:1477/2330 train_time:89602ms step_avg:60.67ms
step:1478/2330 train_time:89665ms step_avg:60.67ms
step:1479/2330 train_time:89725ms step_avg:60.67ms
step:1480/2330 train_time:89789ms step_avg:60.67ms
step:1481/2330 train_time:89848ms step_avg:60.67ms
step:1482/2330 train_time:89911ms step_avg:60.67ms
step:1483/2330 train_time:89970ms step_avg:60.67ms
step:1484/2330 train_time:90033ms step_avg:60.67ms
step:1485/2330 train_time:90092ms step_avg:60.67ms
step:1486/2330 train_time:90156ms step_avg:60.67ms
step:1487/2330 train_time:90216ms step_avg:60.67ms
step:1488/2330 train_time:90279ms step_avg:60.67ms
step:1489/2330 train_time:90339ms step_avg:60.67ms
step:1490/2330 train_time:90401ms step_avg:60.67ms
step:1491/2330 train_time:90460ms step_avg:60.67ms
step:1492/2330 train_time:90523ms step_avg:60.67ms
step:1493/2330 train_time:90582ms step_avg:60.67ms
step:1494/2330 train_time:90644ms step_avg:60.67ms
step:1495/2330 train_time:90703ms step_avg:60.67ms
step:1496/2330 train_time:90766ms step_avg:60.67ms
step:1497/2330 train_time:90826ms step_avg:60.67ms
step:1498/2330 train_time:90888ms step_avg:60.67ms
step:1499/2330 train_time:90948ms step_avg:60.67ms
step:1500/2330 train_time:91010ms step_avg:60.67ms
step:1500/2330 val_loss:3.6188 train_time:91083ms step_avg:60.72ms
step:1501/2330 train_time:91103ms step_avg:60.70ms
step:1502/2330 train_time:91134ms step_avg:60.68ms
step:1503/2330 train_time:91196ms step_avg:60.68ms
step:1504/2330 train_time:91262ms step_avg:60.68ms
step:1505/2330 train_time:91321ms step_avg:60.68ms
step:1506/2330 train_time:91384ms step_avg:60.68ms
step:1507/2330 train_time:91443ms step_avg:60.68ms
step:1508/2330 train_time:91505ms step_avg:60.68ms
step:1509/2330 train_time:91564ms step_avg:60.68ms
step:1510/2330 train_time:91625ms step_avg:60.68ms
step:1511/2330 train_time:91684ms step_avg:60.68ms
step:1512/2330 train_time:91746ms step_avg:60.68ms
step:1513/2330 train_time:91804ms step_avg:60.68ms
step:1514/2330 train_time:91866ms step_avg:60.68ms
step:1515/2330 train_time:91925ms step_avg:60.68ms
step:1516/2330 train_time:91987ms step_avg:60.68ms
step:1517/2330 train_time:92047ms step_avg:60.68ms
step:1518/2330 train_time:92111ms step_avg:60.68ms
step:1519/2330 train_time:92173ms step_avg:60.68ms
step:1520/2330 train_time:92236ms step_avg:60.68ms
step:1521/2330 train_time:92298ms step_avg:60.68ms
step:1522/2330 train_time:92360ms step_avg:60.68ms
step:1523/2330 train_time:92420ms step_avg:60.68ms
step:1524/2330 train_time:92481ms step_avg:60.68ms
step:1525/2330 train_time:92540ms step_avg:60.68ms
step:1526/2330 train_time:92603ms step_avg:60.68ms
step:1527/2330 train_time:92663ms step_avg:60.68ms
step:1528/2330 train_time:92724ms step_avg:60.68ms
step:1529/2330 train_time:92784ms step_avg:60.68ms
step:1530/2330 train_time:92845ms step_avg:60.68ms
step:1531/2330 train_time:92903ms step_avg:60.68ms
step:1532/2330 train_time:92967ms step_avg:60.68ms
step:1533/2330 train_time:93027ms step_avg:60.68ms
step:1534/2330 train_time:93091ms step_avg:60.69ms
step:1535/2330 train_time:93151ms step_avg:60.68ms
step:1536/2330 train_time:93214ms step_avg:60.69ms
step:1537/2330 train_time:93276ms step_avg:60.69ms
step:1538/2330 train_time:93338ms step_avg:60.69ms
step:1539/2330 train_time:93399ms step_avg:60.69ms
step:1540/2330 train_time:93462ms step_avg:60.69ms
step:1541/2330 train_time:93521ms step_avg:60.69ms
step:1542/2330 train_time:93585ms step_avg:60.69ms
step:1543/2330 train_time:93643ms step_avg:60.69ms
step:1544/2330 train_time:93706ms step_avg:60.69ms
step:1545/2330 train_time:93765ms step_avg:60.69ms
step:1546/2330 train_time:93827ms step_avg:60.69ms
step:1547/2330 train_time:93887ms step_avg:60.69ms
step:1548/2330 train_time:93950ms step_avg:60.69ms
step:1549/2330 train_time:94010ms step_avg:60.69ms
step:1550/2330 train_time:94074ms step_avg:60.69ms
step:1551/2330 train_time:94133ms step_avg:60.69ms
step:1552/2330 train_time:94196ms step_avg:60.69ms
step:1553/2330 train_time:94257ms step_avg:60.69ms
step:1554/2330 train_time:94320ms step_avg:60.70ms
step:1555/2330 train_time:94380ms step_avg:60.69ms
step:1556/2330 train_time:94444ms step_avg:60.70ms
step:1557/2330 train_time:94504ms step_avg:60.70ms
step:1558/2330 train_time:94567ms step_avg:60.70ms
step:1559/2330 train_time:94626ms step_avg:60.70ms
step:1560/2330 train_time:94689ms step_avg:60.70ms
step:1561/2330 train_time:94748ms step_avg:60.70ms
step:1562/2330 train_time:94811ms step_avg:60.70ms
step:1563/2330 train_time:94871ms step_avg:60.70ms
step:1564/2330 train_time:94933ms step_avg:60.70ms
step:1565/2330 train_time:94993ms step_avg:60.70ms
step:1566/2330 train_time:95056ms step_avg:60.70ms
step:1567/2330 train_time:95116ms step_avg:60.70ms
step:1568/2330 train_time:95178ms step_avg:60.70ms
step:1569/2330 train_time:95238ms step_avg:60.70ms
step:1570/2330 train_time:95302ms step_avg:60.70ms
step:1571/2330 train_time:95362ms step_avg:60.70ms
step:1572/2330 train_time:95425ms step_avg:60.70ms
step:1573/2330 train_time:95484ms step_avg:60.70ms
step:1574/2330 train_time:95546ms step_avg:60.70ms
step:1575/2330 train_time:95606ms step_avg:60.70ms
step:1576/2330 train_time:95670ms step_avg:60.70ms
step:1577/2330 train_time:95729ms step_avg:60.70ms
step:1578/2330 train_time:95792ms step_avg:60.70ms
step:1579/2330 train_time:95852ms step_avg:60.70ms
step:1580/2330 train_time:95915ms step_avg:60.71ms
step:1581/2330 train_time:95976ms step_avg:60.71ms
step:1582/2330 train_time:96038ms step_avg:60.71ms
step:1583/2330 train_time:96098ms step_avg:60.71ms
step:1584/2330 train_time:96161ms step_avg:60.71ms
step:1585/2330 train_time:96221ms step_avg:60.71ms
step:1586/2330 train_time:96284ms step_avg:60.71ms
step:1587/2330 train_time:96344ms step_avg:60.71ms
step:1588/2330 train_time:96407ms step_avg:60.71ms
step:1589/2330 train_time:96467ms step_avg:60.71ms
step:1590/2330 train_time:96530ms step_avg:60.71ms
step:1591/2330 train_time:96590ms step_avg:60.71ms
step:1592/2330 train_time:96653ms step_avg:60.71ms
step:1593/2330 train_time:96713ms step_avg:60.71ms
step:1594/2330 train_time:96776ms step_avg:60.71ms
step:1595/2330 train_time:96836ms step_avg:60.71ms
step:1596/2330 train_time:96898ms step_avg:60.71ms
step:1597/2330 train_time:96958ms step_avg:60.71ms
step:1598/2330 train_time:97020ms step_avg:60.71ms
step:1599/2330 train_time:97080ms step_avg:60.71ms
step:1600/2330 train_time:97145ms step_avg:60.72ms
step:1601/2330 train_time:97204ms step_avg:60.71ms
step:1602/2330 train_time:97268ms step_avg:60.72ms
step:1603/2330 train_time:97328ms step_avg:60.72ms
step:1604/2330 train_time:97391ms step_avg:60.72ms
step:1605/2330 train_time:97451ms step_avg:60.72ms
step:1606/2330 train_time:97514ms step_avg:60.72ms
step:1607/2330 train_time:97575ms step_avg:60.72ms
step:1608/2330 train_time:97638ms step_avg:60.72ms
step:1609/2330 train_time:97698ms step_avg:60.72ms
step:1610/2330 train_time:97762ms step_avg:60.72ms
step:1611/2330 train_time:97820ms step_avg:60.72ms
step:1612/2330 train_time:97884ms step_avg:60.72ms
step:1613/2330 train_time:97944ms step_avg:60.72ms
step:1614/2330 train_time:98007ms step_avg:60.72ms
step:1615/2330 train_time:98067ms step_avg:60.72ms
step:1616/2330 train_time:98130ms step_avg:60.72ms
step:1617/2330 train_time:98190ms step_avg:60.72ms
step:1618/2330 train_time:98253ms step_avg:60.72ms
step:1619/2330 train_time:98313ms step_avg:60.72ms
step:1620/2330 train_time:98376ms step_avg:60.73ms
step:1621/2330 train_time:98436ms step_avg:60.73ms
step:1622/2330 train_time:98499ms step_avg:60.73ms
step:1623/2330 train_time:98558ms step_avg:60.73ms
step:1624/2330 train_time:98622ms step_avg:60.73ms
step:1625/2330 train_time:98681ms step_avg:60.73ms
step:1626/2330 train_time:98744ms step_avg:60.73ms
step:1627/2330 train_time:98804ms step_avg:60.73ms
step:1628/2330 train_time:98867ms step_avg:60.73ms
step:1629/2330 train_time:98926ms step_avg:60.73ms
step:1630/2330 train_time:98989ms step_avg:60.73ms
step:1631/2330 train_time:99049ms step_avg:60.73ms
step:1632/2330 train_time:99112ms step_avg:60.73ms
step:1633/2330 train_time:99172ms step_avg:60.73ms
step:1634/2330 train_time:99235ms step_avg:60.73ms
step:1635/2330 train_time:99294ms step_avg:60.73ms
step:1636/2330 train_time:99357ms step_avg:60.73ms
step:1637/2330 train_time:99417ms step_avg:60.73ms
step:1638/2330 train_time:99480ms step_avg:60.73ms
step:1639/2330 train_time:99540ms step_avg:60.73ms
step:1640/2330 train_time:99603ms step_avg:60.73ms
step:1641/2330 train_time:99663ms step_avg:60.73ms
step:1642/2330 train_time:99726ms step_avg:60.73ms
step:1643/2330 train_time:99787ms step_avg:60.73ms
step:1644/2330 train_time:99850ms step_avg:60.74ms
step:1645/2330 train_time:99909ms step_avg:60.74ms
step:1646/2330 train_time:99972ms step_avg:60.74ms
step:1647/2330 train_time:100032ms step_avg:60.74ms
step:1648/2330 train_time:100095ms step_avg:60.74ms
step:1649/2330 train_time:100156ms step_avg:60.74ms
step:1650/2330 train_time:100218ms step_avg:60.74ms
step:1651/2330 train_time:100277ms step_avg:60.74ms
step:1652/2330 train_time:100341ms step_avg:60.74ms
step:1653/2330 train_time:100400ms step_avg:60.74ms
step:1654/2330 train_time:100463ms step_avg:60.74ms
step:1655/2330 train_time:100523ms step_avg:60.74ms
step:1656/2330 train_time:100586ms step_avg:60.74ms
step:1657/2330 train_time:100647ms step_avg:60.74ms
step:1658/2330 train_time:100709ms step_avg:60.74ms
step:1659/2330 train_time:100770ms step_avg:60.74ms
step:1660/2330 train_time:100832ms step_avg:60.74ms
step:1661/2330 train_time:100892ms step_avg:60.74ms
step:1662/2330 train_time:100956ms step_avg:60.74ms
step:1663/2330 train_time:101016ms step_avg:60.74ms
step:1664/2330 train_time:101079ms step_avg:60.74ms
step:1665/2330 train_time:101139ms step_avg:60.74ms
step:1666/2330 train_time:101202ms step_avg:60.75ms
step:1667/2330 train_time:101262ms step_avg:60.74ms
step:1668/2330 train_time:101324ms step_avg:60.75ms
step:1669/2330 train_time:101384ms step_avg:60.75ms
step:1670/2330 train_time:101448ms step_avg:60.75ms
step:1671/2330 train_time:101508ms step_avg:60.75ms
step:1672/2330 train_time:101571ms step_avg:60.75ms
step:1673/2330 train_time:101631ms step_avg:60.75ms
step:1674/2330 train_time:101693ms step_avg:60.75ms
step:1675/2330 train_time:101754ms step_avg:60.75ms
step:1676/2330 train_time:101817ms step_avg:60.75ms
step:1677/2330 train_time:101877ms step_avg:60.75ms
step:1678/2330 train_time:101940ms step_avg:60.75ms
step:1679/2330 train_time:102000ms step_avg:60.75ms
step:1680/2330 train_time:102063ms step_avg:60.75ms
step:1681/2330 train_time:102123ms step_avg:60.75ms
step:1682/2330 train_time:102186ms step_avg:60.75ms
step:1683/2330 train_time:102246ms step_avg:60.75ms
step:1684/2330 train_time:102309ms step_avg:60.75ms
step:1685/2330 train_time:102369ms step_avg:60.75ms
step:1686/2330 train_time:102432ms step_avg:60.75ms
step:1687/2330 train_time:102492ms step_avg:60.75ms
step:1688/2330 train_time:102556ms step_avg:60.76ms
step:1689/2330 train_time:102616ms step_avg:60.76ms
step:1690/2330 train_time:102679ms step_avg:60.76ms
step:1691/2330 train_time:102739ms step_avg:60.76ms
step:1692/2330 train_time:102802ms step_avg:60.76ms
step:1693/2330 train_time:102862ms step_avg:60.76ms
step:1694/2330 train_time:102926ms step_avg:60.76ms
step:1695/2330 train_time:102986ms step_avg:60.76ms
step:1696/2330 train_time:103049ms step_avg:60.76ms
step:1697/2330 train_time:103109ms step_avg:60.76ms
step:1698/2330 train_time:103171ms step_avg:60.76ms
step:1699/2330 train_time:103232ms step_avg:60.76ms
step:1700/2330 train_time:103294ms step_avg:60.76ms
step:1701/2330 train_time:103354ms step_avg:60.76ms
step:1702/2330 train_time:103417ms step_avg:60.76ms
step:1703/2330 train_time:103477ms step_avg:60.76ms
step:1704/2330 train_time:103540ms step_avg:60.76ms
step:1705/2330 train_time:103599ms step_avg:60.76ms
step:1706/2330 train_time:103663ms step_avg:60.76ms
step:1707/2330 train_time:103722ms step_avg:60.76ms
step:1708/2330 train_time:103786ms step_avg:60.76ms
step:1709/2330 train_time:103846ms step_avg:60.76ms
step:1710/2330 train_time:103909ms step_avg:60.77ms
step:1711/2330 train_time:103968ms step_avg:60.76ms
step:1712/2330 train_time:104031ms step_avg:60.77ms
step:1713/2330 train_time:104091ms step_avg:60.77ms
step:1714/2330 train_time:104153ms step_avg:60.77ms
step:1715/2330 train_time:104212ms step_avg:60.77ms
step:1716/2330 train_time:104275ms step_avg:60.77ms
step:1717/2330 train_time:104334ms step_avg:60.77ms
step:1718/2330 train_time:104398ms step_avg:60.77ms
step:1719/2330 train_time:104457ms step_avg:60.77ms
step:1720/2330 train_time:104520ms step_avg:60.77ms
step:1721/2330 train_time:104580ms step_avg:60.77ms
step:1722/2330 train_time:104643ms step_avg:60.77ms
step:1723/2330 train_time:104704ms step_avg:60.77ms
step:1724/2330 train_time:104767ms step_avg:60.77ms
step:1725/2330 train_time:104826ms step_avg:60.77ms
step:1726/2330 train_time:104890ms step_avg:60.77ms
step:1727/2330 train_time:104950ms step_avg:60.77ms
step:1728/2330 train_time:105013ms step_avg:60.77ms
step:1729/2330 train_time:105073ms step_avg:60.77ms
step:1730/2330 train_time:105136ms step_avg:60.77ms
step:1731/2330 train_time:105197ms step_avg:60.77ms
step:1732/2330 train_time:105259ms step_avg:60.77ms
step:1733/2330 train_time:105319ms step_avg:60.77ms
step:1734/2330 train_time:105382ms step_avg:60.77ms
step:1735/2330 train_time:105442ms step_avg:60.77ms
step:1736/2330 train_time:105506ms step_avg:60.78ms
step:1737/2330 train_time:105566ms step_avg:60.77ms
step:1738/2330 train_time:105628ms step_avg:60.78ms
step:1739/2330 train_time:105689ms step_avg:60.78ms
step:1740/2330 train_time:105751ms step_avg:60.78ms
step:1741/2330 train_time:105812ms step_avg:60.78ms
step:1742/2330 train_time:105875ms step_avg:60.78ms
step:1743/2330 train_time:105935ms step_avg:60.78ms
step:1744/2330 train_time:105999ms step_avg:60.78ms
step:1745/2330 train_time:106059ms step_avg:60.78ms
step:1746/2330 train_time:106121ms step_avg:60.78ms
step:1747/2330 train_time:106181ms step_avg:60.78ms
step:1748/2330 train_time:106245ms step_avg:60.78ms
step:1749/2330 train_time:106305ms step_avg:60.78ms
step:1750/2330 train_time:106367ms step_avg:60.78ms
step:1750/2330 val_loss:3.5340 train_time:106441ms step_avg:60.82ms
step:1751/2330 train_time:106462ms step_avg:60.80ms
step:1752/2330 train_time:106492ms step_avg:60.78ms
step:1753/2330 train_time:106554ms step_avg:60.78ms
step:1754/2330 train_time:106627ms step_avg:60.79ms
step:1755/2330 train_time:106690ms step_avg:60.79ms
step:1756/2330 train_time:106753ms step_avg:60.79ms
step:1757/2330 train_time:106812ms step_avg:60.79ms
step:1758/2330 train_time:106874ms step_avg:60.79ms
step:1759/2330 train_time:106933ms step_avg:60.79ms
step:1760/2330 train_time:106996ms step_avg:60.79ms
step:1761/2330 train_time:107055ms step_avg:60.79ms
step:1762/2330 train_time:107117ms step_avg:60.79ms
step:1763/2330 train_time:107176ms step_avg:60.79ms
step:1764/2330 train_time:107238ms step_avg:60.79ms
step:1765/2330 train_time:107297ms step_avg:60.79ms
step:1766/2330 train_time:107362ms step_avg:60.79ms
step:1767/2330 train_time:107423ms step_avg:60.79ms
step:1768/2330 train_time:107486ms step_avg:60.80ms
step:1769/2330 train_time:107548ms step_avg:60.80ms
step:1770/2330 train_time:107613ms step_avg:60.80ms
step:1771/2330 train_time:107674ms step_avg:60.80ms
step:1772/2330 train_time:107738ms step_avg:60.80ms
step:1773/2330 train_time:107798ms step_avg:60.80ms
step:1774/2330 train_time:107860ms step_avg:60.80ms
step:1775/2330 train_time:107919ms step_avg:60.80ms
step:1776/2330 train_time:107982ms step_avg:60.80ms
step:1777/2330 train_time:108042ms step_avg:60.80ms
step:1778/2330 train_time:108104ms step_avg:60.80ms
step:1779/2330 train_time:108164ms step_avg:60.80ms
step:1780/2330 train_time:108226ms step_avg:60.80ms
step:1781/2330 train_time:108285ms step_avg:60.80ms
step:1782/2330 train_time:108348ms step_avg:60.80ms
step:1783/2330 train_time:108409ms step_avg:60.80ms
step:1784/2330 train_time:108472ms step_avg:60.80ms
step:1785/2330 train_time:108532ms step_avg:60.80ms
step:1786/2330 train_time:108596ms step_avg:60.80ms
step:1787/2330 train_time:108657ms step_avg:60.80ms
step:1788/2330 train_time:108721ms step_avg:60.81ms
step:1789/2330 train_time:108780ms step_avg:60.81ms
step:1790/2330 train_time:108844ms step_avg:60.81ms
step:1791/2330 train_time:108903ms step_avg:60.81ms
step:1792/2330 train_time:108966ms step_avg:60.81ms
step:1793/2330 train_time:109026ms step_avg:60.81ms
step:1794/2330 train_time:109088ms step_avg:60.81ms
step:1795/2330 train_time:109147ms step_avg:60.81ms
step:1796/2330 train_time:109210ms step_avg:60.81ms
step:1797/2330 train_time:109270ms step_avg:60.81ms
step:1798/2330 train_time:109332ms step_avg:60.81ms
step:1799/2330 train_time:109391ms step_avg:60.81ms
step:1800/2330 train_time:109455ms step_avg:60.81ms
step:1801/2330 train_time:109516ms step_avg:60.81ms
step:1802/2330 train_time:109579ms step_avg:60.81ms
step:1803/2330 train_time:109639ms step_avg:60.81ms
step:1804/2330 train_time:109702ms step_avg:60.81ms
step:1805/2330 train_time:109762ms step_avg:60.81ms
step:1806/2330 train_time:109827ms step_avg:60.81ms
step:1807/2330 train_time:109886ms step_avg:60.81ms
step:1808/2330 train_time:109949ms step_avg:60.81ms
step:1809/2330 train_time:110009ms step_avg:60.81ms
step:1810/2330 train_time:110072ms step_avg:60.81ms
step:1811/2330 train_time:110132ms step_avg:60.81ms
step:1812/2330 train_time:110194ms step_avg:60.81ms
step:1813/2330 train_time:110254ms step_avg:60.81ms
step:1814/2330 train_time:110317ms step_avg:60.81ms
step:1815/2330 train_time:110377ms step_avg:60.81ms
step:1816/2330 train_time:110439ms step_avg:60.81ms
step:1817/2330 train_time:110499ms step_avg:60.81ms
step:1818/2330 train_time:110563ms step_avg:60.82ms
step:1819/2330 train_time:110624ms step_avg:60.82ms
step:1820/2330 train_time:110685ms step_avg:60.82ms
step:1821/2330 train_time:110746ms step_avg:60.82ms
step:1822/2330 train_time:110808ms step_avg:60.82ms
step:1823/2330 train_time:110867ms step_avg:60.82ms
step:1824/2330 train_time:110930ms step_avg:60.82ms
step:1825/2330 train_time:110990ms step_avg:60.82ms
step:1826/2330 train_time:111052ms step_avg:60.82ms
step:1827/2330 train_time:111112ms step_avg:60.82ms
step:1828/2330 train_time:111174ms step_avg:60.82ms
step:1829/2330 train_time:111235ms step_avg:60.82ms
step:1830/2330 train_time:111297ms step_avg:60.82ms
step:1831/2330 train_time:111356ms step_avg:60.82ms
step:1832/2330 train_time:111419ms step_avg:60.82ms
step:1833/2330 train_time:111479ms step_avg:60.82ms
step:1834/2330 train_time:111542ms step_avg:60.82ms
step:1835/2330 train_time:111603ms step_avg:60.82ms
step:1836/2330 train_time:111666ms step_avg:60.82ms
step:1837/2330 train_time:111726ms step_avg:60.82ms
step:1838/2330 train_time:111788ms step_avg:60.82ms
step:1839/2330 train_time:111847ms step_avg:60.82ms
step:1840/2330 train_time:111910ms step_avg:60.82ms
step:1841/2330 train_time:111970ms step_avg:60.82ms
step:1842/2330 train_time:112033ms step_avg:60.82ms
step:1843/2330 train_time:112092ms step_avg:60.82ms
step:1844/2330 train_time:112155ms step_avg:60.82ms
step:1845/2330 train_time:112215ms step_avg:60.82ms
step:1846/2330 train_time:112278ms step_avg:60.82ms
step:1847/2330 train_time:112338ms step_avg:60.82ms
step:1848/2330 train_time:112400ms step_avg:60.82ms
step:1849/2330 train_time:112460ms step_avg:60.82ms
step:1850/2330 train_time:112525ms step_avg:60.82ms
step:1851/2330 train_time:112584ms step_avg:60.82ms
step:1852/2330 train_time:112647ms step_avg:60.82ms
step:1853/2330 train_time:112707ms step_avg:60.82ms
step:1854/2330 train_time:112770ms step_avg:60.83ms
step:1855/2330 train_time:112831ms step_avg:60.83ms
step:1856/2330 train_time:112895ms step_avg:60.83ms
step:1857/2330 train_time:112955ms step_avg:60.83ms
step:1858/2330 train_time:113018ms step_avg:60.83ms
step:1859/2330 train_time:113078ms step_avg:60.83ms
step:1860/2330 train_time:113140ms step_avg:60.83ms
step:1861/2330 train_time:113199ms step_avg:60.83ms
step:1862/2330 train_time:113263ms step_avg:60.83ms
step:1863/2330 train_time:113323ms step_avg:60.83ms
step:1864/2330 train_time:113386ms step_avg:60.83ms
step:1865/2330 train_time:113446ms step_avg:60.83ms
step:1866/2330 train_time:113510ms step_avg:60.83ms
step:1867/2330 train_time:113570ms step_avg:60.83ms
step:1868/2330 train_time:113634ms step_avg:60.83ms
step:1869/2330 train_time:113694ms step_avg:60.83ms
step:1870/2330 train_time:113757ms step_avg:60.83ms
step:1871/2330 train_time:113817ms step_avg:60.83ms
step:1872/2330 train_time:113879ms step_avg:60.83ms
step:1873/2330 train_time:113940ms step_avg:60.83ms
step:1874/2330 train_time:114002ms step_avg:60.83ms
step:1875/2330 train_time:114062ms step_avg:60.83ms
step:1876/2330 train_time:114126ms step_avg:60.83ms
step:1877/2330 train_time:114185ms step_avg:60.83ms
step:1878/2330 train_time:114249ms step_avg:60.84ms
step:1879/2330 train_time:114309ms step_avg:60.83ms
step:1880/2330 train_time:114372ms step_avg:60.84ms
step:1881/2330 train_time:114432ms step_avg:60.84ms
step:1882/2330 train_time:114494ms step_avg:60.84ms
step:1883/2330 train_time:114554ms step_avg:60.84ms
step:1884/2330 train_time:114617ms step_avg:60.84ms
step:1885/2330 train_time:114677ms step_avg:60.84ms
step:1886/2330 train_time:114740ms step_avg:60.84ms
step:1887/2330 train_time:114800ms step_avg:60.84ms
step:1888/2330 train_time:114862ms step_avg:60.84ms
step:1889/2330 train_time:114923ms step_avg:60.84ms
step:1890/2330 train_time:114985ms step_avg:60.84ms
step:1891/2330 train_time:115045ms step_avg:60.84ms
step:1892/2330 train_time:115108ms step_avg:60.84ms
step:1893/2330 train_time:115167ms step_avg:60.84ms
step:1894/2330 train_time:115230ms step_avg:60.84ms
step:1895/2330 train_time:115290ms step_avg:60.84ms
step:1896/2330 train_time:115353ms step_avg:60.84ms
step:1897/2330 train_time:115413ms step_avg:60.84ms
step:1898/2330 train_time:115476ms step_avg:60.84ms
step:1899/2330 train_time:115536ms step_avg:60.84ms
step:1900/2330 train_time:115599ms step_avg:60.84ms
step:1901/2330 train_time:115659ms step_avg:60.84ms
step:1902/2330 train_time:115723ms step_avg:60.84ms
step:1903/2330 train_time:115782ms step_avg:60.84ms
step:1904/2330 train_time:115845ms step_avg:60.84ms
step:1905/2330 train_time:115904ms step_avg:60.84ms
step:1906/2330 train_time:115967ms step_avg:60.84ms
step:1907/2330 train_time:116027ms step_avg:60.84ms
step:1908/2330 train_time:116091ms step_avg:60.84ms
step:1909/2330 train_time:116150ms step_avg:60.84ms
step:1910/2330 train_time:116213ms step_avg:60.84ms
step:1911/2330 train_time:116272ms step_avg:60.84ms
step:1912/2330 train_time:116335ms step_avg:60.84ms
step:1913/2330 train_time:116395ms step_avg:60.84ms
step:1914/2330 train_time:116457ms step_avg:60.85ms
step:1915/2330 train_time:116518ms step_avg:60.84ms
step:1916/2330 train_time:116581ms step_avg:60.85ms
step:1917/2330 train_time:116641ms step_avg:60.85ms
step:1918/2330 train_time:116704ms step_avg:60.85ms
step:1919/2330 train_time:116763ms step_avg:60.85ms
step:1920/2330 train_time:116826ms step_avg:60.85ms
step:1921/2330 train_time:116885ms step_avg:60.85ms
step:1922/2330 train_time:116949ms step_avg:60.85ms
step:1923/2330 train_time:117010ms step_avg:60.85ms
step:1924/2330 train_time:117073ms step_avg:60.85ms
step:1925/2330 train_time:117133ms step_avg:60.85ms
step:1926/2330 train_time:117196ms step_avg:60.85ms
step:1927/2330 train_time:117256ms step_avg:60.85ms
step:1928/2330 train_time:117320ms step_avg:60.85ms
step:1929/2330 train_time:117380ms step_avg:60.85ms
step:1930/2330 train_time:117442ms step_avg:60.85ms
step:1931/2330 train_time:117502ms step_avg:60.85ms
step:1932/2330 train_time:117565ms step_avg:60.85ms
step:1933/2330 train_time:117625ms step_avg:60.85ms
step:1934/2330 train_time:117688ms step_avg:60.85ms
step:1935/2330 train_time:117747ms step_avg:60.85ms
step:1936/2330 train_time:117810ms step_avg:60.85ms
step:1937/2330 train_time:117870ms step_avg:60.85ms
step:1938/2330 train_time:117933ms step_avg:60.85ms
step:1939/2330 train_time:117993ms step_avg:60.85ms
step:1940/2330 train_time:118056ms step_avg:60.85ms
step:1941/2330 train_time:118117ms step_avg:60.85ms
step:1942/2330 train_time:118179ms step_avg:60.85ms
step:1943/2330 train_time:118239ms step_avg:60.85ms
step:1944/2330 train_time:118302ms step_avg:60.85ms
step:1945/2330 train_time:118361ms step_avg:60.85ms
step:1946/2330 train_time:118425ms step_avg:60.86ms
step:1947/2330 train_time:118483ms step_avg:60.85ms
step:1948/2330 train_time:118548ms step_avg:60.86ms
step:1949/2330 train_time:118607ms step_avg:60.86ms
step:1950/2330 train_time:118669ms step_avg:60.86ms
step:1951/2330 train_time:118730ms step_avg:60.86ms
step:1952/2330 train_time:118792ms step_avg:60.86ms
step:1953/2330 train_time:118852ms step_avg:60.86ms
step:1954/2330 train_time:118916ms step_avg:60.86ms
step:1955/2330 train_time:118975ms step_avg:60.86ms
step:1956/2330 train_time:119038ms step_avg:60.86ms
step:1957/2330 train_time:119098ms step_avg:60.86ms
step:1958/2330 train_time:119161ms step_avg:60.86ms
step:1959/2330 train_time:119222ms step_avg:60.86ms
step:1960/2330 train_time:119284ms step_avg:60.86ms
step:1961/2330 train_time:119343ms step_avg:60.86ms
step:1962/2330 train_time:119407ms step_avg:60.86ms
step:1963/2330 train_time:119466ms step_avg:60.86ms
step:1964/2330 train_time:119530ms step_avg:60.86ms
step:1965/2330 train_time:119590ms step_avg:60.86ms
step:1966/2330 train_time:119653ms step_avg:60.86ms
step:1967/2330 train_time:119713ms step_avg:60.86ms
step:1968/2330 train_time:119776ms step_avg:60.86ms
step:1969/2330 train_time:119836ms step_avg:60.86ms
step:1970/2330 train_time:119899ms step_avg:60.86ms
step:1971/2330 train_time:119959ms step_avg:60.86ms
step:1972/2330 train_time:120023ms step_avg:60.86ms
step:1973/2330 train_time:120082ms step_avg:60.86ms
step:1974/2330 train_time:120146ms step_avg:60.86ms
step:1975/2330 train_time:120205ms step_avg:60.86ms
step:1976/2330 train_time:120268ms step_avg:60.86ms
step:1977/2330 train_time:120328ms step_avg:60.86ms
step:1978/2330 train_time:120390ms step_avg:60.86ms
step:1979/2330 train_time:120451ms step_avg:60.86ms
step:1980/2330 train_time:120513ms step_avg:60.87ms
step:1981/2330 train_time:120573ms step_avg:60.86ms
step:1982/2330 train_time:120637ms step_avg:60.87ms
step:1983/2330 train_time:120697ms step_avg:60.87ms
step:1984/2330 train_time:120760ms step_avg:60.87ms
step:1985/2330 train_time:120821ms step_avg:60.87ms
step:1986/2330 train_time:120883ms step_avg:60.87ms
step:1987/2330 train_time:120943ms step_avg:60.87ms
step:1988/2330 train_time:121005ms step_avg:60.87ms
step:1989/2330 train_time:121064ms step_avg:60.87ms
step:1990/2330 train_time:121128ms step_avg:60.87ms
step:1991/2330 train_time:121188ms step_avg:60.87ms
step:1992/2330 train_time:121250ms step_avg:60.87ms
step:1993/2330 train_time:121310ms step_avg:60.87ms
step:1994/2330 train_time:121372ms step_avg:60.87ms
step:1995/2330 train_time:121432ms step_avg:60.87ms
step:1996/2330 train_time:121495ms step_avg:60.87ms
step:1997/2330 train_time:121556ms step_avg:60.87ms
step:1998/2330 train_time:121619ms step_avg:60.87ms
step:1999/2330 train_time:121679ms step_avg:60.87ms
step:2000/2330 train_time:121741ms step_avg:60.87ms
step:2000/2330 val_loss:3.4817 train_time:121815ms step_avg:60.91ms
step:2001/2330 train_time:121835ms step_avg:60.89ms
step:2002/2330 train_time:121867ms step_avg:60.87ms
step:2003/2330 train_time:121932ms step_avg:60.87ms
step:2004/2330 train_time:121998ms step_avg:60.88ms
step:2005/2330 train_time:122058ms step_avg:60.88ms
step:2006/2330 train_time:122121ms step_avg:60.88ms
step:2007/2330 train_time:122180ms step_avg:60.88ms
step:2008/2330 train_time:122243ms step_avg:60.88ms
step:2009/2330 train_time:122301ms step_avg:60.88ms
step:2010/2330 train_time:122364ms step_avg:60.88ms
step:2011/2330 train_time:122423ms step_avg:60.88ms
step:2012/2330 train_time:122485ms step_avg:60.88ms
step:2013/2330 train_time:122545ms step_avg:60.88ms
step:2014/2330 train_time:122607ms step_avg:60.88ms
step:2015/2330 train_time:122666ms step_avg:60.88ms
step:2016/2330 train_time:122728ms step_avg:60.88ms
step:2017/2330 train_time:122789ms step_avg:60.88ms
step:2018/2330 train_time:122855ms step_avg:60.88ms
step:2019/2330 train_time:122916ms step_avg:60.88ms
step:2020/2330 train_time:122980ms step_avg:60.88ms
step:2021/2330 train_time:123042ms step_avg:60.88ms
step:2022/2330 train_time:123104ms step_avg:60.88ms
step:2023/2330 train_time:123164ms step_avg:60.88ms
step:2024/2330 train_time:123227ms step_avg:60.88ms
step:2025/2330 train_time:123287ms step_avg:60.88ms
step:2026/2330 train_time:123351ms step_avg:60.88ms
step:2027/2330 train_time:123410ms step_avg:60.88ms
step:2028/2330 train_time:123472ms step_avg:60.88ms
step:2029/2330 train_time:123531ms step_avg:60.88ms
step:2030/2330 train_time:123594ms step_avg:60.88ms
step:2031/2330 train_time:123653ms step_avg:60.88ms
step:2032/2330 train_time:123716ms step_avg:60.88ms
step:2033/2330 train_time:123776ms step_avg:60.88ms
step:2034/2330 train_time:123841ms step_avg:60.89ms
step:2035/2330 train_time:123901ms step_avg:60.88ms
step:2036/2330 train_time:123965ms step_avg:60.89ms
step:2037/2330 train_time:124026ms step_avg:60.89ms
step:2038/2330 train_time:124089ms step_avg:60.89ms
step:2039/2330 train_time:124149ms step_avg:60.89ms
step:2040/2330 train_time:124212ms step_avg:60.89ms
step:2041/2330 train_time:124272ms step_avg:60.89ms
step:2042/2330 train_time:124336ms step_avg:60.89ms
step:2043/2330 train_time:124396ms step_avg:60.89ms
step:2044/2330 train_time:124458ms step_avg:60.89ms
step:2045/2330 train_time:124517ms step_avg:60.89ms
step:2046/2330 train_time:124580ms step_avg:60.89ms
step:2047/2330 train_time:124640ms step_avg:60.89ms
step:2048/2330 train_time:124702ms step_avg:60.89ms
step:2049/2330 train_time:124762ms step_avg:60.89ms
step:2050/2330 train_time:124825ms step_avg:60.89ms
step:2051/2330 train_time:124886ms step_avg:60.89ms
step:2052/2330 train_time:124949ms step_avg:60.89ms
step:2053/2330 train_time:125009ms step_avg:60.89ms
step:2054/2330 train_time:125072ms step_avg:60.89ms
step:2055/2330 train_time:125132ms step_avg:60.89ms
step:2056/2330 train_time:125196ms step_avg:60.89ms
step:2057/2330 train_time:125256ms step_avg:60.89ms
step:2058/2330 train_time:125318ms step_avg:60.89ms
step:2059/2330 train_time:125378ms step_avg:60.89ms
step:2060/2330 train_time:125440ms step_avg:60.89ms
step:2061/2330 train_time:125500ms step_avg:60.89ms
step:2062/2330 train_time:125564ms step_avg:60.89ms
step:2063/2330 train_time:125624ms step_avg:60.89ms
step:2064/2330 train_time:125686ms step_avg:60.89ms
step:2065/2330 train_time:125746ms step_avg:60.89ms
step:2066/2330 train_time:125809ms step_avg:60.90ms
step:2067/2330 train_time:125870ms step_avg:60.90ms
step:2068/2330 train_time:125934ms step_avg:60.90ms
step:2069/2330 train_time:125994ms step_avg:60.90ms
step:2070/2330 train_time:126057ms step_avg:60.90ms
step:2071/2330 train_time:126117ms step_avg:60.90ms
step:2072/2330 train_time:126180ms step_avg:60.90ms
step:2073/2330 train_time:126240ms step_avg:60.90ms
step:2074/2330 train_time:126302ms step_avg:60.90ms
step:2075/2330 train_time:126362ms step_avg:60.90ms
step:2076/2330 train_time:126424ms step_avg:60.90ms
step:2077/2330 train_time:126484ms step_avg:60.90ms
step:2078/2330 train_time:126547ms step_avg:60.90ms
step:2079/2330 train_time:126607ms step_avg:60.90ms
step:2080/2330 train_time:126670ms step_avg:60.90ms
step:2081/2330 train_time:126730ms step_avg:60.90ms
step:2082/2330 train_time:126793ms step_avg:60.90ms
step:2083/2330 train_time:126853ms step_avg:60.90ms
step:2084/2330 train_time:126916ms step_avg:60.90ms
step:2085/2330 train_time:126977ms step_avg:60.90ms
step:2086/2330 train_time:127041ms step_avg:60.90ms
step:2087/2330 train_time:127101ms step_avg:60.90ms
step:2088/2330 train_time:127165ms step_avg:60.90ms
step:2089/2330 train_time:127225ms step_avg:60.90ms
step:2090/2330 train_time:127288ms step_avg:60.90ms
step:2091/2330 train_time:127348ms step_avg:60.90ms
step:2092/2330 train_time:127411ms step_avg:60.90ms
step:2093/2330 train_time:127471ms step_avg:60.90ms
step:2094/2330 train_time:127534ms step_avg:60.90ms
step:2095/2330 train_time:127594ms step_avg:60.90ms
step:2096/2330 train_time:127656ms step_avg:60.90ms
step:2097/2330 train_time:127716ms step_avg:60.90ms
step:2098/2330 train_time:127780ms step_avg:60.91ms
step:2099/2330 train_time:127841ms step_avg:60.91ms
step:2100/2330 train_time:127904ms step_avg:60.91ms
step:2101/2330 train_time:127964ms step_avg:60.91ms
step:2102/2330 train_time:128027ms step_avg:60.91ms
step:2103/2330 train_time:128087ms step_avg:60.91ms
step:2104/2330 train_time:128151ms step_avg:60.91ms
step:2105/2330 train_time:128211ms step_avg:60.91ms
step:2106/2330 train_time:128274ms step_avg:60.91ms
step:2107/2330 train_time:128333ms step_avg:60.91ms
step:2108/2330 train_time:128397ms step_avg:60.91ms
step:2109/2330 train_time:128456ms step_avg:60.91ms
step:2110/2330 train_time:128519ms step_avg:60.91ms
step:2111/2330 train_time:128579ms step_avg:60.91ms
step:2112/2330 train_time:128642ms step_avg:60.91ms
step:2113/2330 train_time:128701ms step_avg:60.91ms
step:2114/2330 train_time:128765ms step_avg:60.91ms
step:2115/2330 train_time:128824ms step_avg:60.91ms
step:2116/2330 train_time:128887ms step_avg:60.91ms
step:2117/2330 train_time:128947ms step_avg:60.91ms
step:2118/2330 train_time:129009ms step_avg:60.91ms
step:2119/2330 train_time:129069ms step_avg:60.91ms
step:2120/2330 train_time:129133ms step_avg:60.91ms
step:2121/2330 train_time:129192ms step_avg:60.91ms
step:2122/2330 train_time:129256ms step_avg:60.91ms
step:2123/2330 train_time:129316ms step_avg:60.91ms
step:2124/2330 train_time:129378ms step_avg:60.91ms
step:2125/2330 train_time:129439ms step_avg:60.91ms
step:2126/2330 train_time:129501ms step_avg:60.91ms
step:2127/2330 train_time:129561ms step_avg:60.91ms
step:2128/2330 train_time:129624ms step_avg:60.91ms
step:2129/2330 train_time:129684ms step_avg:60.91ms
step:2130/2330 train_time:129748ms step_avg:60.91ms
step:2131/2330 train_time:129807ms step_avg:60.91ms
step:2132/2330 train_time:129870ms step_avg:60.91ms
step:2133/2330 train_time:129930ms step_avg:60.91ms
step:2134/2330 train_time:129994ms step_avg:60.92ms
step:2135/2330 train_time:130053ms step_avg:60.91ms
step:2136/2330 train_time:130118ms step_avg:60.92ms
step:2137/2330 train_time:130177ms step_avg:60.92ms
step:2138/2330 train_time:130241ms step_avg:60.92ms
step:2139/2330 train_time:130300ms step_avg:60.92ms
step:2140/2330 train_time:130363ms step_avg:60.92ms
step:2141/2330 train_time:130423ms step_avg:60.92ms
step:2142/2330 train_time:130486ms step_avg:60.92ms
step:2143/2330 train_time:130546ms step_avg:60.92ms
step:2144/2330 train_time:130607ms step_avg:60.92ms
step:2145/2330 train_time:130668ms step_avg:60.92ms
step:2146/2330 train_time:130731ms step_avg:60.92ms
step:2147/2330 train_time:130791ms step_avg:60.92ms
step:2148/2330 train_time:130853ms step_avg:60.92ms
step:2149/2330 train_time:130913ms step_avg:60.92ms
step:2150/2330 train_time:130976ms step_avg:60.92ms
step:2151/2330 train_time:131037ms step_avg:60.92ms
step:2152/2330 train_time:131100ms step_avg:60.92ms
step:2153/2330 train_time:131160ms step_avg:60.92ms
step:2154/2330 train_time:131223ms step_avg:60.92ms
step:2155/2330 train_time:131282ms step_avg:60.92ms
step:2156/2330 train_time:131346ms step_avg:60.92ms
step:2157/2330 train_time:131406ms step_avg:60.92ms
step:2158/2330 train_time:131469ms step_avg:60.92ms
step:2159/2330 train_time:131530ms step_avg:60.92ms
step:2160/2330 train_time:131594ms step_avg:60.92ms
step:2161/2330 train_time:131654ms step_avg:60.92ms
step:2162/2330 train_time:131717ms step_avg:60.92ms
step:2163/2330 train_time:131777ms step_avg:60.92ms
step:2164/2330 train_time:131841ms step_avg:60.92ms
step:2165/2330 train_time:131900ms step_avg:60.92ms
step:2166/2330 train_time:131963ms step_avg:60.92ms
step:2167/2330 train_time:132023ms step_avg:60.92ms
step:2168/2330 train_time:132086ms step_avg:60.93ms
step:2169/2330 train_time:132146ms step_avg:60.92ms
step:2170/2330 train_time:132209ms step_avg:60.93ms
step:2171/2330 train_time:132270ms step_avg:60.93ms
step:2172/2330 train_time:132333ms step_avg:60.93ms
step:2173/2330 train_time:132392ms step_avg:60.93ms
step:2174/2330 train_time:132456ms step_avg:60.93ms
step:2175/2330 train_time:132515ms step_avg:60.93ms
step:2176/2330 train_time:132577ms step_avg:60.93ms
step:2177/2330 train_time:132638ms step_avg:60.93ms
step:2178/2330 train_time:132701ms step_avg:60.93ms
step:2179/2330 train_time:132760ms step_avg:60.93ms
step:2180/2330 train_time:132824ms step_avg:60.93ms
step:2181/2330 train_time:132884ms step_avg:60.93ms
step:2182/2330 train_time:132948ms step_avg:60.93ms
step:2183/2330 train_time:133007ms step_avg:60.93ms
step:2184/2330 train_time:133071ms step_avg:60.93ms
step:2185/2330 train_time:133131ms step_avg:60.93ms
step:2186/2330 train_time:133194ms step_avg:60.93ms
step:2187/2330 train_time:133256ms step_avg:60.93ms
step:2188/2330 train_time:133319ms step_avg:60.93ms
step:2189/2330 train_time:133378ms step_avg:60.93ms
step:2190/2330 train_time:133441ms step_avg:60.93ms
step:2191/2330 train_time:133500ms step_avg:60.93ms
step:2192/2330 train_time:133563ms step_avg:60.93ms
step:2193/2330 train_time:133624ms step_avg:60.93ms
step:2194/2330 train_time:133687ms step_avg:60.93ms
step:2195/2330 train_time:133747ms step_avg:60.93ms
step:2196/2330 train_time:133810ms step_avg:60.93ms
step:2197/2330 train_time:133870ms step_avg:60.93ms
step:2198/2330 train_time:133933ms step_avg:60.93ms
step:2199/2330 train_time:133994ms step_avg:60.93ms
step:2200/2330 train_time:134057ms step_avg:60.93ms
step:2201/2330 train_time:134117ms step_avg:60.93ms
step:2202/2330 train_time:134179ms step_avg:60.94ms
step:2203/2330 train_time:134240ms step_avg:60.94ms
step:2204/2330 train_time:134303ms step_avg:60.94ms
step:2205/2330 train_time:134363ms step_avg:60.94ms
step:2206/2330 train_time:134425ms step_avg:60.94ms
step:2207/2330 train_time:134485ms step_avg:60.94ms
step:2208/2330 train_time:134548ms step_avg:60.94ms
step:2209/2330 train_time:134609ms step_avg:60.94ms
step:2210/2330 train_time:134673ms step_avg:60.94ms
step:2211/2330 train_time:134733ms step_avg:60.94ms
step:2212/2330 train_time:134796ms step_avg:60.94ms
step:2213/2330 train_time:134855ms step_avg:60.94ms
step:2214/2330 train_time:134918ms step_avg:60.94ms
step:2215/2330 train_time:134978ms step_avg:60.94ms
step:2216/2330 train_time:135042ms step_avg:60.94ms
step:2217/2330 train_time:135101ms step_avg:60.94ms
step:2218/2330 train_time:135164ms step_avg:60.94ms
step:2219/2330 train_time:135223ms step_avg:60.94ms
step:2220/2330 train_time:135287ms step_avg:60.94ms
step:2221/2330 train_time:135347ms step_avg:60.94ms
step:2222/2330 train_time:135410ms step_avg:60.94ms
step:2223/2330 train_time:135471ms step_avg:60.94ms
step:2224/2330 train_time:135533ms step_avg:60.94ms
step:2225/2330 train_time:135593ms step_avg:60.94ms
step:2226/2330 train_time:135656ms step_avg:60.94ms
step:2227/2330 train_time:135716ms step_avg:60.94ms
step:2228/2330 train_time:135780ms step_avg:60.94ms
step:2229/2330 train_time:135840ms step_avg:60.94ms
step:2230/2330 train_time:135903ms step_avg:60.94ms
step:2231/2330 train_time:135962ms step_avg:60.94ms
step:2232/2330 train_time:136025ms step_avg:60.94ms
step:2233/2330 train_time:136085ms step_avg:60.94ms
step:2234/2330 train_time:136149ms step_avg:60.94ms
step:2235/2330 train_time:136209ms step_avg:60.94ms
step:2236/2330 train_time:136272ms step_avg:60.94ms
step:2237/2330 train_time:136332ms step_avg:60.94ms
step:2238/2330 train_time:136395ms step_avg:60.95ms
step:2239/2330 train_time:136455ms step_avg:60.94ms
step:2240/2330 train_time:136518ms step_avg:60.95ms
step:2241/2330 train_time:136577ms step_avg:60.94ms
step:2242/2330 train_time:136640ms step_avg:60.95ms
step:2243/2330 train_time:136700ms step_avg:60.95ms
step:2244/2330 train_time:136763ms step_avg:60.95ms
step:2245/2330 train_time:136823ms step_avg:60.95ms
step:2246/2330 train_time:136887ms step_avg:60.95ms
step:2247/2330 train_time:136948ms step_avg:60.95ms
step:2248/2330 train_time:137010ms step_avg:60.95ms
step:2249/2330 train_time:137070ms step_avg:60.95ms
step:2250/2330 train_time:137132ms step_avg:60.95ms
step:2250/2330 val_loss:3.4371 train_time:137206ms step_avg:60.98ms
step:2251/2330 train_time:137226ms step_avg:60.96ms
step:2252/2330 train_time:137258ms step_avg:60.95ms
step:2253/2330 train_time:137321ms step_avg:60.95ms
step:2254/2330 train_time:137386ms step_avg:60.95ms
step:2255/2330 train_time:137447ms step_avg:60.95ms
step:2256/2330 train_time:137510ms step_avg:60.95ms
step:2257/2330 train_time:137570ms step_avg:60.95ms
step:2258/2330 train_time:137632ms step_avg:60.95ms
step:2259/2330 train_time:137691ms step_avg:60.95ms
step:2260/2330 train_time:137753ms step_avg:60.95ms
step:2261/2330 train_time:137812ms step_avg:60.95ms
step:2262/2330 train_time:137875ms step_avg:60.95ms
step:2263/2330 train_time:137935ms step_avg:60.95ms
step:2264/2330 train_time:137998ms step_avg:60.95ms
step:2265/2330 train_time:138059ms step_avg:60.95ms
step:2266/2330 train_time:138121ms step_avg:60.95ms
step:2267/2330 train_time:138181ms step_avg:60.95ms
step:2268/2330 train_time:138246ms step_avg:60.96ms
step:2269/2330 train_time:138308ms step_avg:60.96ms
step:2270/2330 train_time:138372ms step_avg:60.96ms
step:2271/2330 train_time:138432ms step_avg:60.96ms
step:2272/2330 train_time:138495ms step_avg:60.96ms
step:2273/2330 train_time:138553ms step_avg:60.96ms
step:2274/2330 train_time:138616ms step_avg:60.96ms
step:2275/2330 train_time:138676ms step_avg:60.96ms
step:2276/2330 train_time:138738ms step_avg:60.96ms
step:2277/2330 train_time:138797ms step_avg:60.96ms
step:2278/2330 train_time:138860ms step_avg:60.96ms
step:2279/2330 train_time:138919ms step_avg:60.96ms
step:2280/2330 train_time:138982ms step_avg:60.96ms
step:2281/2330 train_time:139043ms step_avg:60.96ms
step:2282/2330 train_time:139106ms step_avg:60.96ms
step:2283/2330 train_time:139166ms step_avg:60.96ms
step:2284/2330 train_time:139229ms step_avg:60.96ms
step:2285/2330 train_time:139289ms step_avg:60.96ms
step:2286/2330 train_time:139353ms step_avg:60.96ms
step:2287/2330 train_time:139414ms step_avg:60.96ms
step:2288/2330 train_time:139478ms step_avg:60.96ms
step:2289/2330 train_time:139538ms step_avg:60.96ms
step:2290/2330 train_time:139601ms step_avg:60.96ms
step:2291/2330 train_time:139662ms step_avg:60.96ms
step:2292/2330 train_time:139724ms step_avg:60.96ms
step:2293/2330 train_time:139783ms step_avg:60.96ms
step:2294/2330 train_time:139847ms step_avg:60.96ms
step:2295/2330 train_time:139906ms step_avg:60.96ms
step:2296/2330 train_time:139969ms step_avg:60.96ms
step:2297/2330 train_time:140028ms step_avg:60.96ms
step:2298/2330 train_time:140090ms step_avg:60.96ms
step:2299/2330 train_time:140150ms step_avg:60.96ms
step:2300/2330 train_time:140214ms step_avg:60.96ms
step:2301/2330 train_time:140275ms step_avg:60.96ms
step:2302/2330 train_time:140339ms step_avg:60.96ms
step:2303/2330 train_time:140400ms step_avg:60.96ms
step:2304/2330 train_time:140464ms step_avg:60.97ms
step:2305/2330 train_time:140523ms step_avg:60.96ms
step:2306/2330 train_time:140586ms step_avg:60.97ms
step:2307/2330 train_time:140646ms step_avg:60.96ms
step:2308/2330 train_time:140709ms step_avg:60.97ms
step:2309/2330 train_time:140768ms step_avg:60.97ms
step:2310/2330 train_time:140830ms step_avg:60.97ms
step:2311/2330 train_time:140891ms step_avg:60.97ms
step:2312/2330 train_time:140954ms step_avg:60.97ms
step:2313/2330 train_time:141013ms step_avg:60.97ms
step:2314/2330 train_time:141076ms step_avg:60.97ms
step:2315/2330 train_time:141135ms step_avg:60.97ms
step:2316/2330 train_time:141199ms step_avg:60.97ms
step:2317/2330 train_time:141261ms step_avg:60.97ms
step:2318/2330 train_time:141324ms step_avg:60.97ms
step:2319/2330 train_time:141384ms step_avg:60.97ms
step:2320/2330 train_time:141447ms step_avg:60.97ms
step:2321/2330 train_time:141507ms step_avg:60.97ms
step:2322/2330 train_time:141571ms step_avg:60.97ms
step:2323/2330 train_time:141631ms step_avg:60.97ms
step:2324/2330 train_time:141694ms step_avg:60.97ms
step:2325/2330 train_time:141755ms step_avg:60.97ms
step:2326/2330 train_time:141818ms step_avg:60.97ms
step:2327/2330 train_time:141878ms step_avg:60.97ms
step:2328/2330 train_time:141940ms step_avg:60.97ms
step:2329/2330 train_time:141999ms step_avg:60.97ms
step:2330/2330 train_time:142062ms step_avg:60.97ms
step:2330/2330 val_loss:3.4224 train_time:142136ms step_avg:61.00ms
peak memory allocated: 29226 MiB reserved: 44076 MiB
