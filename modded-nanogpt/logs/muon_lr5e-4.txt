import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "muon_lr5e-4"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=5e-4, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Oct 28 22:21:05 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   32C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   30C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   30C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   28C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   31C    P0             115W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:91ms step_avg:91.09ms
step:2/2330 train_time:190ms step_avg:94.88ms
step:3/2330 train_time:212ms step_avg:70.60ms
step:4/2330 train_time:294ms step_avg:73.43ms
step:5/2330 train_time:351ms step_avg:70.22ms
step:6/2330 train_time:412ms step_avg:68.66ms
step:7/2330 train_time:470ms step_avg:67.18ms
step:8/2330 train_time:531ms step_avg:66.37ms
step:9/2330 train_time:590ms step_avg:65.55ms
step:10/2330 train_time:651ms step_avg:65.08ms
step:11/2330 train_time:709ms step_avg:64.48ms
step:12/2330 train_time:770ms step_avg:64.18ms
step:13/2330 train_time:829ms step_avg:63.79ms
step:14/2330 train_time:890ms step_avg:63.57ms
step:15/2330 train_time:949ms step_avg:63.24ms
step:16/2330 train_time:1010ms step_avg:63.09ms
step:17/2330 train_time:1068ms step_avg:62.84ms
step:18/2330 train_time:1131ms step_avg:62.85ms
step:19/2330 train_time:1199ms step_avg:63.09ms
step:20/2330 train_time:1263ms step_avg:63.16ms
step:21/2330 train_time:1323ms step_avg:63.01ms
step:22/2330 train_time:1385ms step_avg:62.94ms
step:23/2330 train_time:1444ms step_avg:62.76ms
step:24/2330 train_time:1506ms step_avg:62.73ms
step:25/2330 train_time:1565ms step_avg:62.59ms
step:26/2330 train_time:1626ms step_avg:62.56ms
step:27/2330 train_time:1686ms step_avg:62.45ms
step:28/2330 train_time:1747ms step_avg:62.40ms
step:29/2330 train_time:1806ms step_avg:62.29ms
step:30/2330 train_time:1867ms step_avg:62.25ms
step:31/2330 train_time:1927ms step_avg:62.16ms
step:32/2330 train_time:1988ms step_avg:62.13ms
step:33/2330 train_time:2047ms step_avg:62.03ms
step:34/2330 train_time:2109ms step_avg:62.02ms
step:35/2330 train_time:2170ms step_avg:61.99ms
step:36/2330 train_time:2232ms step_avg:62.01ms
step:37/2330 train_time:2293ms step_avg:61.96ms
step:38/2330 train_time:2355ms step_avg:61.96ms
step:39/2330 train_time:2414ms step_avg:61.91ms
step:40/2330 train_time:2477ms step_avg:61.92ms
step:41/2330 train_time:2537ms step_avg:61.87ms
step:42/2330 train_time:2598ms step_avg:61.87ms
step:43/2330 train_time:2658ms step_avg:61.80ms
step:44/2330 train_time:2719ms step_avg:61.79ms
step:45/2330 train_time:2779ms step_avg:61.75ms
step:46/2330 train_time:2841ms step_avg:61.76ms
step:47/2330 train_time:2901ms step_avg:61.73ms
step:48/2330 train_time:2964ms step_avg:61.75ms
step:49/2330 train_time:3024ms step_avg:61.71ms
step:50/2330 train_time:3086ms step_avg:61.72ms
step:51/2330 train_time:3145ms step_avg:61.68ms
step:52/2330 train_time:3207ms step_avg:61.68ms
step:53/2330 train_time:3267ms step_avg:61.64ms
step:54/2330 train_time:3329ms step_avg:61.64ms
step:55/2330 train_time:3388ms step_avg:61.60ms
step:56/2330 train_time:3449ms step_avg:61.59ms
step:57/2330 train_time:3508ms step_avg:61.55ms
step:58/2330 train_time:3570ms step_avg:61.55ms
step:59/2330 train_time:3630ms step_avg:61.52ms
step:60/2330 train_time:3691ms step_avg:61.52ms
step:61/2330 train_time:3751ms step_avg:61.49ms
step:62/2330 train_time:3813ms step_avg:61.50ms
step:63/2330 train_time:3872ms step_avg:61.47ms
step:64/2330 train_time:3935ms step_avg:61.48ms
step:65/2330 train_time:3995ms step_avg:61.46ms
step:66/2330 train_time:4057ms step_avg:61.46ms
step:67/2330 train_time:4117ms step_avg:61.45ms
step:68/2330 train_time:4179ms step_avg:61.45ms
step:69/2330 train_time:4239ms step_avg:61.43ms
step:70/2330 train_time:4301ms step_avg:61.44ms
step:71/2330 train_time:4361ms step_avg:61.42ms
step:72/2330 train_time:4423ms step_avg:61.43ms
step:73/2330 train_time:4483ms step_avg:61.41ms
step:74/2330 train_time:4545ms step_avg:61.41ms
step:75/2330 train_time:4605ms step_avg:61.40ms
step:76/2330 train_time:4666ms step_avg:61.40ms
step:77/2330 train_time:4726ms step_avg:61.37ms
step:78/2330 train_time:4788ms step_avg:61.38ms
step:79/2330 train_time:4847ms step_avg:61.35ms
step:80/2330 train_time:4908ms step_avg:61.35ms
step:81/2330 train_time:4968ms step_avg:61.34ms
step:82/2330 train_time:5030ms step_avg:61.34ms
step:83/2330 train_time:5089ms step_avg:61.32ms
step:84/2330 train_time:5151ms step_avg:61.32ms
step:85/2330 train_time:5212ms step_avg:61.32ms
step:86/2330 train_time:5274ms step_avg:61.33ms
step:87/2330 train_time:5334ms step_avg:61.31ms
step:88/2330 train_time:5397ms step_avg:61.33ms
step:89/2330 train_time:5457ms step_avg:61.31ms
step:90/2330 train_time:5519ms step_avg:61.32ms
step:91/2330 train_time:5578ms step_avg:61.30ms
step:92/2330 train_time:5640ms step_avg:61.31ms
step:93/2330 train_time:5700ms step_avg:61.29ms
step:94/2330 train_time:5761ms step_avg:61.29ms
step:95/2330 train_time:5821ms step_avg:61.27ms
step:96/2330 train_time:5884ms step_avg:61.29ms
step:97/2330 train_time:5944ms step_avg:61.28ms
step:98/2330 train_time:6006ms step_avg:61.28ms
step:99/2330 train_time:6065ms step_avg:61.27ms
step:100/2330 train_time:6128ms step_avg:61.28ms
step:101/2330 train_time:6188ms step_avg:61.26ms
step:102/2330 train_time:6250ms step_avg:61.27ms
step:103/2330 train_time:6309ms step_avg:61.25ms
step:104/2330 train_time:6371ms step_avg:61.26ms
step:105/2330 train_time:6430ms step_avg:61.24ms
step:106/2330 train_time:6492ms step_avg:61.25ms
step:107/2330 train_time:6552ms step_avg:61.23ms
step:108/2330 train_time:6614ms step_avg:61.24ms
step:109/2330 train_time:6674ms step_avg:61.23ms
step:110/2330 train_time:6737ms step_avg:61.24ms
step:111/2330 train_time:6797ms step_avg:61.24ms
step:112/2330 train_time:6859ms step_avg:61.25ms
step:113/2330 train_time:6919ms step_avg:61.23ms
step:114/2330 train_time:6982ms step_avg:61.24ms
step:115/2330 train_time:7042ms step_avg:61.24ms
step:116/2330 train_time:7105ms step_avg:61.25ms
step:117/2330 train_time:7165ms step_avg:61.24ms
step:118/2330 train_time:7227ms step_avg:61.25ms
step:119/2330 train_time:7287ms step_avg:61.23ms
step:120/2330 train_time:7348ms step_avg:61.23ms
step:121/2330 train_time:7408ms step_avg:61.22ms
step:122/2330 train_time:7469ms step_avg:61.22ms
step:123/2330 train_time:7529ms step_avg:61.21ms
step:124/2330 train_time:7590ms step_avg:61.21ms
step:125/2330 train_time:7649ms step_avg:61.19ms
step:126/2330 train_time:7711ms step_avg:61.20ms
step:127/2330 train_time:7771ms step_avg:61.19ms
step:128/2330 train_time:7834ms step_avg:61.20ms
step:129/2330 train_time:7894ms step_avg:61.19ms
step:130/2330 train_time:7956ms step_avg:61.20ms
step:131/2330 train_time:8017ms step_avg:61.20ms
step:132/2330 train_time:8079ms step_avg:61.21ms
step:133/2330 train_time:8140ms step_avg:61.20ms
step:134/2330 train_time:8202ms step_avg:61.21ms
step:135/2330 train_time:8262ms step_avg:61.20ms
step:136/2330 train_time:8325ms step_avg:61.21ms
step:137/2330 train_time:8385ms step_avg:61.21ms
step:138/2330 train_time:8446ms step_avg:61.21ms
step:139/2330 train_time:8506ms step_avg:61.19ms
step:140/2330 train_time:8567ms step_avg:61.20ms
step:141/2330 train_time:8627ms step_avg:61.18ms
step:142/2330 train_time:8688ms step_avg:61.19ms
step:143/2330 train_time:8748ms step_avg:61.17ms
step:144/2330 train_time:8810ms step_avg:61.18ms
step:145/2330 train_time:8869ms step_avg:61.17ms
step:146/2330 train_time:8931ms step_avg:61.17ms
step:147/2330 train_time:8991ms step_avg:61.17ms
step:148/2330 train_time:9054ms step_avg:61.17ms
step:149/2330 train_time:9114ms step_avg:61.17ms
step:150/2330 train_time:9176ms step_avg:61.17ms
step:151/2330 train_time:9236ms step_avg:61.17ms
step:152/2330 train_time:9299ms step_avg:61.18ms
step:153/2330 train_time:9359ms step_avg:61.17ms
step:154/2330 train_time:9421ms step_avg:61.18ms
step:155/2330 train_time:9481ms step_avg:61.17ms
step:156/2330 train_time:9543ms step_avg:61.17ms
step:157/2330 train_time:9603ms step_avg:61.17ms
step:158/2330 train_time:9665ms step_avg:61.17ms
step:159/2330 train_time:9725ms step_avg:61.17ms
step:160/2330 train_time:9788ms step_avg:61.17ms
step:161/2330 train_time:9847ms step_avg:61.16ms
step:162/2330 train_time:9908ms step_avg:61.16ms
step:163/2330 train_time:9969ms step_avg:61.16ms
step:164/2330 train_time:10030ms step_avg:61.16ms
step:165/2330 train_time:10090ms step_avg:61.15ms
step:166/2330 train_time:10152ms step_avg:61.16ms
step:167/2330 train_time:10213ms step_avg:61.15ms
step:168/2330 train_time:10275ms step_avg:61.16ms
step:169/2330 train_time:10335ms step_avg:61.15ms
step:170/2330 train_time:10398ms step_avg:61.16ms
step:171/2330 train_time:10457ms step_avg:61.15ms
step:172/2330 train_time:10520ms step_avg:61.16ms
step:173/2330 train_time:10579ms step_avg:61.15ms
step:174/2330 train_time:10641ms step_avg:61.15ms
step:175/2330 train_time:10701ms step_avg:61.15ms
step:176/2330 train_time:10763ms step_avg:61.16ms
step:177/2330 train_time:10824ms step_avg:61.15ms
step:178/2330 train_time:10887ms step_avg:61.16ms
step:179/2330 train_time:10946ms step_avg:61.15ms
step:180/2330 train_time:11008ms step_avg:61.15ms
step:181/2330 train_time:11067ms step_avg:61.15ms
step:182/2330 train_time:11129ms step_avg:61.15ms
step:183/2330 train_time:11189ms step_avg:61.14ms
step:184/2330 train_time:11250ms step_avg:61.14ms
step:185/2330 train_time:11310ms step_avg:61.13ms
step:186/2330 train_time:11372ms step_avg:61.14ms
step:187/2330 train_time:11432ms step_avg:61.13ms
step:188/2330 train_time:11494ms step_avg:61.14ms
step:189/2330 train_time:11554ms step_avg:61.13ms
step:190/2330 train_time:11617ms step_avg:61.14ms
step:191/2330 train_time:11677ms step_avg:61.14ms
step:192/2330 train_time:11739ms step_avg:61.14ms
step:193/2330 train_time:11799ms step_avg:61.14ms
step:194/2330 train_time:11861ms step_avg:61.14ms
step:195/2330 train_time:11921ms step_avg:61.13ms
step:196/2330 train_time:11984ms step_avg:61.14ms
step:197/2330 train_time:12043ms step_avg:61.13ms
step:198/2330 train_time:12106ms step_avg:61.14ms
step:199/2330 train_time:12165ms step_avg:61.13ms
step:200/2330 train_time:12227ms step_avg:61.14ms
step:201/2330 train_time:12287ms step_avg:61.13ms
step:202/2330 train_time:12348ms step_avg:61.13ms
step:203/2330 train_time:12407ms step_avg:61.12ms
step:204/2330 train_time:12469ms step_avg:61.12ms
step:205/2330 train_time:12529ms step_avg:61.12ms
step:206/2330 train_time:12591ms step_avg:61.12ms
step:207/2330 train_time:12651ms step_avg:61.12ms
step:208/2330 train_time:12713ms step_avg:61.12ms
step:209/2330 train_time:12774ms step_avg:61.12ms
step:210/2330 train_time:12836ms step_avg:61.12ms
step:211/2330 train_time:12896ms step_avg:61.12ms
step:212/2330 train_time:12958ms step_avg:61.12ms
step:213/2330 train_time:13019ms step_avg:61.12ms
step:214/2330 train_time:13081ms step_avg:61.13ms
step:215/2330 train_time:13141ms step_avg:61.12ms
step:216/2330 train_time:13203ms step_avg:61.12ms
step:217/2330 train_time:13263ms step_avg:61.12ms
step:218/2330 train_time:13326ms step_avg:61.13ms
step:219/2330 train_time:13386ms step_avg:61.12ms
step:220/2330 train_time:13447ms step_avg:61.12ms
step:221/2330 train_time:13507ms step_avg:61.12ms
step:222/2330 train_time:13568ms step_avg:61.12ms
step:223/2330 train_time:13628ms step_avg:61.11ms
step:224/2330 train_time:13690ms step_avg:61.11ms
step:225/2330 train_time:13749ms step_avg:61.11ms
step:226/2330 train_time:13812ms step_avg:61.11ms
step:227/2330 train_time:13872ms step_avg:61.11ms
step:228/2330 train_time:13934ms step_avg:61.11ms
step:229/2330 train_time:13995ms step_avg:61.12ms
step:230/2330 train_time:14057ms step_avg:61.12ms
step:231/2330 train_time:14117ms step_avg:61.11ms
step:232/2330 train_time:14179ms step_avg:61.12ms
step:233/2330 train_time:14239ms step_avg:61.11ms
step:234/2330 train_time:14301ms step_avg:61.12ms
step:235/2330 train_time:14361ms step_avg:61.11ms
step:236/2330 train_time:14423ms step_avg:61.12ms
step:237/2330 train_time:14483ms step_avg:61.11ms
step:238/2330 train_time:14545ms step_avg:61.12ms
step:239/2330 train_time:14605ms step_avg:61.11ms
step:240/2330 train_time:14667ms step_avg:61.11ms
step:241/2330 train_time:14727ms step_avg:61.11ms
step:242/2330 train_time:14789ms step_avg:61.11ms
step:243/2330 train_time:14849ms step_avg:61.11ms
step:244/2330 train_time:14910ms step_avg:61.11ms
step:245/2330 train_time:14970ms step_avg:61.10ms
step:246/2330 train_time:15032ms step_avg:61.11ms
step:247/2330 train_time:15093ms step_avg:61.10ms
step:248/2330 train_time:15155ms step_avg:61.11ms
step:249/2330 train_time:15215ms step_avg:61.10ms
step:250/2330 train_time:15277ms step_avg:61.11ms
step:250/2330 val_loss:5.0348 train_time:15341ms step_avg:61.37ms
step:251/2330 train_time:15364ms step_avg:61.21ms
step:252/2330 train_time:15401ms step_avg:61.11ms
step:253/2330 train_time:15466ms step_avg:61.13ms
step:254/2330 train_time:15529ms step_avg:61.14ms
step:255/2330 train_time:15589ms step_avg:61.13ms
step:256/2330 train_time:15651ms step_avg:61.14ms
step:257/2330 train_time:15711ms step_avg:61.13ms
step:258/2330 train_time:15773ms step_avg:61.14ms
step:259/2330 train_time:15833ms step_avg:61.13ms
step:260/2330 train_time:15895ms step_avg:61.13ms
step:261/2330 train_time:15955ms step_avg:61.13ms
step:262/2330 train_time:16016ms step_avg:61.13ms
step:263/2330 train_time:16076ms step_avg:61.12ms
step:264/2330 train_time:16137ms step_avg:61.12ms
step:265/2330 train_time:16196ms step_avg:61.12ms
step:266/2330 train_time:16258ms step_avg:61.12ms
step:267/2330 train_time:16319ms step_avg:61.12ms
step:268/2330 train_time:16382ms step_avg:61.13ms
step:269/2330 train_time:16445ms step_avg:61.13ms
step:270/2330 train_time:16507ms step_avg:61.14ms
step:271/2330 train_time:16567ms step_avg:61.13ms
step:272/2330 train_time:16629ms step_avg:61.14ms
step:273/2330 train_time:16689ms step_avg:61.13ms
step:274/2330 train_time:16751ms step_avg:61.13ms
step:275/2330 train_time:16810ms step_avg:61.13ms
step:276/2330 train_time:16873ms step_avg:61.13ms
step:277/2330 train_time:16932ms step_avg:61.13ms
step:278/2330 train_time:16994ms step_avg:61.13ms
step:279/2330 train_time:17053ms step_avg:61.12ms
step:280/2330 train_time:17114ms step_avg:61.12ms
step:281/2330 train_time:17174ms step_avg:61.12ms
step:282/2330 train_time:17236ms step_avg:61.12ms
step:283/2330 train_time:17297ms step_avg:61.12ms
step:284/2330 train_time:17360ms step_avg:61.13ms
step:285/2330 train_time:17420ms step_avg:61.12ms
step:286/2330 train_time:17482ms step_avg:61.13ms
step:287/2330 train_time:17543ms step_avg:61.12ms
step:288/2330 train_time:17605ms step_avg:61.13ms
step:289/2330 train_time:17665ms step_avg:61.12ms
step:290/2330 train_time:17727ms step_avg:61.13ms
step:291/2330 train_time:17787ms step_avg:61.12ms
step:292/2330 train_time:17849ms step_avg:61.13ms
step:293/2330 train_time:17908ms step_avg:61.12ms
step:294/2330 train_time:17970ms step_avg:61.12ms
step:295/2330 train_time:18030ms step_avg:61.12ms
step:296/2330 train_time:18091ms step_avg:61.12ms
step:297/2330 train_time:18152ms step_avg:61.12ms
step:298/2330 train_time:18214ms step_avg:61.12ms
step:299/2330 train_time:18274ms step_avg:61.12ms
step:300/2330 train_time:18337ms step_avg:61.12ms
step:301/2330 train_time:18397ms step_avg:61.12ms
step:302/2330 train_time:18460ms step_avg:61.12ms
step:303/2330 train_time:18519ms step_avg:61.12ms
step:304/2330 train_time:18581ms step_avg:61.12ms
step:305/2330 train_time:18642ms step_avg:61.12ms
step:306/2330 train_time:18704ms step_avg:61.12ms
step:307/2330 train_time:18764ms step_avg:61.12ms
step:308/2330 train_time:18825ms step_avg:61.12ms
step:309/2330 train_time:18885ms step_avg:61.12ms
step:310/2330 train_time:18947ms step_avg:61.12ms
step:311/2330 train_time:19007ms step_avg:61.11ms
step:312/2330 train_time:19068ms step_avg:61.12ms
step:313/2330 train_time:19128ms step_avg:61.11ms
step:314/2330 train_time:19189ms step_avg:61.11ms
step:315/2330 train_time:19249ms step_avg:61.11ms
step:316/2330 train_time:19312ms step_avg:61.11ms
step:317/2330 train_time:19372ms step_avg:61.11ms
step:318/2330 train_time:19434ms step_avg:61.11ms
step:319/2330 train_time:19494ms step_avg:61.11ms
step:320/2330 train_time:19557ms step_avg:61.12ms
step:321/2330 train_time:19618ms step_avg:61.12ms
step:322/2330 train_time:19680ms step_avg:61.12ms
step:323/2330 train_time:19740ms step_avg:61.11ms
step:324/2330 train_time:19802ms step_avg:61.12ms
step:325/2330 train_time:19862ms step_avg:61.11ms
step:326/2330 train_time:19925ms step_avg:61.12ms
step:327/2330 train_time:19985ms step_avg:61.12ms
step:328/2330 train_time:20047ms step_avg:61.12ms
step:329/2330 train_time:20106ms step_avg:61.11ms
step:330/2330 train_time:20168ms step_avg:61.12ms
step:331/2330 train_time:20227ms step_avg:61.11ms
step:332/2330 train_time:20288ms step_avg:61.11ms
step:333/2330 train_time:20347ms step_avg:61.10ms
step:334/2330 train_time:20409ms step_avg:61.10ms
step:335/2330 train_time:20470ms step_avg:61.10ms
step:336/2330 train_time:20533ms step_avg:61.11ms
step:337/2330 train_time:20593ms step_avg:61.11ms
step:338/2330 train_time:20657ms step_avg:61.12ms
step:339/2330 train_time:20718ms step_avg:61.11ms
step:340/2330 train_time:20779ms step_avg:61.12ms
step:341/2330 train_time:20839ms step_avg:61.11ms
step:342/2330 train_time:20901ms step_avg:61.11ms
step:343/2330 train_time:20961ms step_avg:61.11ms
step:344/2330 train_time:21023ms step_avg:61.11ms
step:345/2330 train_time:21084ms step_avg:61.11ms
step:346/2330 train_time:21146ms step_avg:61.11ms
step:347/2330 train_time:21205ms step_avg:61.11ms
step:348/2330 train_time:21266ms step_avg:61.11ms
step:349/2330 train_time:21327ms step_avg:61.11ms
step:350/2330 train_time:21388ms step_avg:61.11ms
step:351/2330 train_time:21447ms step_avg:61.10ms
step:352/2330 train_time:21509ms step_avg:61.11ms
step:353/2330 train_time:21569ms step_avg:61.10ms
step:354/2330 train_time:21632ms step_avg:61.11ms
step:355/2330 train_time:21693ms step_avg:61.11ms
step:356/2330 train_time:21756ms step_avg:61.11ms
step:357/2330 train_time:21816ms step_avg:61.11ms
step:358/2330 train_time:21879ms step_avg:61.11ms
step:359/2330 train_time:21938ms step_avg:61.11ms
step:360/2330 train_time:22000ms step_avg:61.11ms
step:361/2330 train_time:22061ms step_avg:61.11ms
step:362/2330 train_time:22124ms step_avg:61.12ms
step:363/2330 train_time:22183ms step_avg:61.11ms
step:364/2330 train_time:22245ms step_avg:61.11ms
step:365/2330 train_time:22306ms step_avg:61.11ms
step:366/2330 train_time:22367ms step_avg:61.11ms
step:367/2330 train_time:22427ms step_avg:61.11ms
step:368/2330 train_time:22488ms step_avg:61.11ms
step:369/2330 train_time:22549ms step_avg:61.11ms
step:370/2330 train_time:22610ms step_avg:61.11ms
step:371/2330 train_time:22670ms step_avg:61.10ms
step:372/2330 train_time:22732ms step_avg:61.11ms
step:373/2330 train_time:22793ms step_avg:61.11ms
step:374/2330 train_time:22855ms step_avg:61.11ms
step:375/2330 train_time:22916ms step_avg:61.11ms
step:376/2330 train_time:22979ms step_avg:61.11ms
step:377/2330 train_time:23039ms step_avg:61.11ms
step:378/2330 train_time:23101ms step_avg:61.11ms
step:379/2330 train_time:23161ms step_avg:61.11ms
step:380/2330 train_time:23223ms step_avg:61.11ms
step:381/2330 train_time:23284ms step_avg:61.11ms
step:382/2330 train_time:23346ms step_avg:61.12ms
step:383/2330 train_time:23405ms step_avg:61.11ms
step:384/2330 train_time:23468ms step_avg:61.11ms
step:385/2330 train_time:23527ms step_avg:61.11ms
step:386/2330 train_time:23588ms step_avg:61.11ms
step:387/2330 train_time:23648ms step_avg:61.11ms
step:388/2330 train_time:23710ms step_avg:61.11ms
step:389/2330 train_time:23770ms step_avg:61.10ms
step:390/2330 train_time:23832ms step_avg:61.11ms
step:391/2330 train_time:23893ms step_avg:61.11ms
step:392/2330 train_time:23956ms step_avg:61.11ms
step:393/2330 train_time:24018ms step_avg:61.11ms
step:394/2330 train_time:24079ms step_avg:61.12ms
step:395/2330 train_time:24140ms step_avg:61.11ms
step:396/2330 train_time:24201ms step_avg:61.11ms
step:397/2330 train_time:24261ms step_avg:61.11ms
step:398/2330 train_time:24323ms step_avg:61.11ms
step:399/2330 train_time:24383ms step_avg:61.11ms
step:400/2330 train_time:24445ms step_avg:61.11ms
step:401/2330 train_time:24505ms step_avg:61.11ms
step:402/2330 train_time:24567ms step_avg:61.11ms
step:403/2330 train_time:24627ms step_avg:61.11ms
step:404/2330 train_time:24689ms step_avg:61.11ms
step:405/2330 train_time:24749ms step_avg:61.11ms
step:406/2330 train_time:24810ms step_avg:61.11ms
step:407/2330 train_time:24870ms step_avg:61.11ms
step:408/2330 train_time:24933ms step_avg:61.11ms
step:409/2330 train_time:24995ms step_avg:61.11ms
step:410/2330 train_time:25058ms step_avg:61.12ms
step:411/2330 train_time:25118ms step_avg:61.12ms
step:412/2330 train_time:25180ms step_avg:61.12ms
step:413/2330 train_time:25240ms step_avg:61.11ms
step:414/2330 train_time:25302ms step_avg:61.12ms
step:415/2330 train_time:25362ms step_avg:61.11ms
step:416/2330 train_time:25423ms step_avg:61.11ms
step:417/2330 train_time:25484ms step_avg:61.11ms
step:418/2330 train_time:25546ms step_avg:61.11ms
step:419/2330 train_time:25605ms step_avg:61.11ms
step:420/2330 train_time:25668ms step_avg:61.11ms
step:421/2330 train_time:25728ms step_avg:61.11ms
step:422/2330 train_time:25790ms step_avg:61.11ms
step:423/2330 train_time:25850ms step_avg:61.11ms
step:424/2330 train_time:25912ms step_avg:61.11ms
step:425/2330 train_time:25973ms step_avg:61.11ms
step:426/2330 train_time:26036ms step_avg:61.12ms
step:427/2330 train_time:26097ms step_avg:61.12ms
step:428/2330 train_time:26160ms step_avg:61.12ms
step:429/2330 train_time:26220ms step_avg:61.12ms
step:430/2330 train_time:26281ms step_avg:61.12ms
step:431/2330 train_time:26341ms step_avg:61.12ms
step:432/2330 train_time:26403ms step_avg:61.12ms
step:433/2330 train_time:26463ms step_avg:61.12ms
step:434/2330 train_time:26525ms step_avg:61.12ms
step:435/2330 train_time:26585ms step_avg:61.11ms
step:436/2330 train_time:26648ms step_avg:61.12ms
step:437/2330 train_time:26707ms step_avg:61.11ms
step:438/2330 train_time:26769ms step_avg:61.12ms
step:439/2330 train_time:26829ms step_avg:61.11ms
step:440/2330 train_time:26891ms step_avg:61.12ms
step:441/2330 train_time:26951ms step_avg:61.11ms
step:442/2330 train_time:27012ms step_avg:61.11ms
step:443/2330 train_time:27073ms step_avg:61.11ms
step:444/2330 train_time:27136ms step_avg:61.12ms
step:445/2330 train_time:27197ms step_avg:61.12ms
step:446/2330 train_time:27260ms step_avg:61.12ms
step:447/2330 train_time:27319ms step_avg:61.12ms
step:448/2330 train_time:27381ms step_avg:61.12ms
step:449/2330 train_time:27442ms step_avg:61.12ms
step:450/2330 train_time:27503ms step_avg:61.12ms
step:451/2330 train_time:27563ms step_avg:61.12ms
step:452/2330 train_time:27625ms step_avg:61.12ms
step:453/2330 train_time:27686ms step_avg:61.12ms
step:454/2330 train_time:27748ms step_avg:61.12ms
step:455/2330 train_time:27808ms step_avg:61.12ms
step:456/2330 train_time:27870ms step_avg:61.12ms
step:457/2330 train_time:27929ms step_avg:61.11ms
step:458/2330 train_time:27991ms step_avg:61.12ms
step:459/2330 train_time:28051ms step_avg:61.11ms
step:460/2330 train_time:28114ms step_avg:61.12ms
step:461/2330 train_time:28174ms step_avg:61.12ms
step:462/2330 train_time:28237ms step_avg:61.12ms
step:463/2330 train_time:28297ms step_avg:61.12ms
step:464/2330 train_time:28360ms step_avg:61.12ms
step:465/2330 train_time:28421ms step_avg:61.12ms
step:466/2330 train_time:28483ms step_avg:61.12ms
step:467/2330 train_time:28543ms step_avg:61.12ms
step:468/2330 train_time:28605ms step_avg:61.12ms
step:469/2330 train_time:28665ms step_avg:61.12ms
step:470/2330 train_time:28727ms step_avg:61.12ms
step:471/2330 train_time:28787ms step_avg:61.12ms
step:472/2330 train_time:28850ms step_avg:61.12ms
step:473/2330 train_time:28909ms step_avg:61.12ms
step:474/2330 train_time:28971ms step_avg:61.12ms
step:475/2330 train_time:29031ms step_avg:61.12ms
step:476/2330 train_time:29094ms step_avg:61.12ms
step:477/2330 train_time:29153ms step_avg:61.12ms
step:478/2330 train_time:29216ms step_avg:61.12ms
step:479/2330 train_time:29277ms step_avg:61.12ms
step:480/2330 train_time:29339ms step_avg:61.12ms
step:481/2330 train_time:29399ms step_avg:61.12ms
step:482/2330 train_time:29463ms step_avg:61.13ms
step:483/2330 train_time:29522ms step_avg:61.12ms
step:484/2330 train_time:29584ms step_avg:61.12ms
step:485/2330 train_time:29644ms step_avg:61.12ms
step:486/2330 train_time:29706ms step_avg:61.12ms
step:487/2330 train_time:29765ms step_avg:61.12ms
step:488/2330 train_time:29828ms step_avg:61.12ms
step:489/2330 train_time:29888ms step_avg:61.12ms
step:490/2330 train_time:29950ms step_avg:61.12ms
step:491/2330 train_time:30009ms step_avg:61.12ms
step:492/2330 train_time:30071ms step_avg:61.12ms
step:493/2330 train_time:30131ms step_avg:61.12ms
step:494/2330 train_time:30193ms step_avg:61.12ms
step:495/2330 train_time:30254ms step_avg:61.12ms
step:496/2330 train_time:30317ms step_avg:61.12ms
step:497/2330 train_time:30378ms step_avg:61.12ms
step:498/2330 train_time:30441ms step_avg:61.13ms
step:499/2330 train_time:30502ms step_avg:61.13ms
step:500/2330 train_time:30564ms step_avg:61.13ms
step:500/2330 val_loss:4.4580 train_time:30628ms step_avg:61.26ms
step:501/2330 train_time:30651ms step_avg:61.18ms
step:502/2330 train_time:30688ms step_avg:61.13ms
step:503/2330 train_time:30754ms step_avg:61.14ms
step:504/2330 train_time:30819ms step_avg:61.15ms
step:505/2330 train_time:30880ms step_avg:61.15ms
step:506/2330 train_time:30942ms step_avg:61.15ms
step:507/2330 train_time:31001ms step_avg:61.15ms
step:508/2330 train_time:31062ms step_avg:61.15ms
step:509/2330 train_time:31122ms step_avg:61.14ms
step:510/2330 train_time:31183ms step_avg:61.14ms
step:511/2330 train_time:31242ms step_avg:61.14ms
step:512/2330 train_time:31303ms step_avg:61.14ms
step:513/2330 train_time:31363ms step_avg:61.14ms
step:514/2330 train_time:31424ms step_avg:61.14ms
step:515/2330 train_time:31483ms step_avg:61.13ms
step:516/2330 train_time:31545ms step_avg:61.13ms
step:517/2330 train_time:31606ms step_avg:61.13ms
step:518/2330 train_time:31669ms step_avg:61.14ms
step:519/2330 train_time:31731ms step_avg:61.14ms
step:520/2330 train_time:31795ms step_avg:61.14ms
step:521/2330 train_time:31856ms step_avg:61.14ms
step:522/2330 train_time:31918ms step_avg:61.15ms
step:523/2330 train_time:31979ms step_avg:61.15ms
step:524/2330 train_time:32040ms step_avg:61.15ms
step:525/2330 train_time:32101ms step_avg:61.14ms
step:526/2330 train_time:32162ms step_avg:61.14ms
step:527/2330 train_time:32221ms step_avg:61.14ms
step:528/2330 train_time:32282ms step_avg:61.14ms
step:529/2330 train_time:32342ms step_avg:61.14ms
step:530/2330 train_time:32404ms step_avg:61.14ms
step:531/2330 train_time:32464ms step_avg:61.14ms
step:532/2330 train_time:32526ms step_avg:61.14ms
step:533/2330 train_time:32586ms step_avg:61.14ms
step:534/2330 train_time:32647ms step_avg:61.14ms
step:535/2330 train_time:32709ms step_avg:61.14ms
step:536/2330 train_time:32772ms step_avg:61.14ms
step:537/2330 train_time:32832ms step_avg:61.14ms
step:538/2330 train_time:32896ms step_avg:61.15ms
step:539/2330 train_time:32956ms step_avg:61.14ms
step:540/2330 train_time:33018ms step_avg:61.14ms
step:541/2330 train_time:33078ms step_avg:61.14ms
step:542/2330 train_time:33140ms step_avg:61.14ms
step:543/2330 train_time:33200ms step_avg:61.14ms
step:544/2330 train_time:33262ms step_avg:61.14ms
step:545/2330 train_time:33321ms step_avg:61.14ms
step:546/2330 train_time:33383ms step_avg:61.14ms
step:547/2330 train_time:33442ms step_avg:61.14ms
step:548/2330 train_time:33504ms step_avg:61.14ms
step:549/2330 train_time:33563ms step_avg:61.14ms
step:550/2330 train_time:33626ms step_avg:61.14ms
step:551/2330 train_time:33687ms step_avg:61.14ms
step:552/2330 train_time:33749ms step_avg:61.14ms
step:553/2330 train_time:33810ms step_avg:61.14ms
step:554/2330 train_time:33872ms step_avg:61.14ms
step:555/2330 train_time:33932ms step_avg:61.14ms
step:556/2330 train_time:33995ms step_avg:61.14ms
step:557/2330 train_time:34055ms step_avg:61.14ms
step:558/2330 train_time:34117ms step_avg:61.14ms
step:559/2330 train_time:34177ms step_avg:61.14ms
step:560/2330 train_time:34239ms step_avg:61.14ms
step:561/2330 train_time:34300ms step_avg:61.14ms
step:562/2330 train_time:34362ms step_avg:61.14ms
step:563/2330 train_time:34422ms step_avg:61.14ms
step:564/2330 train_time:34484ms step_avg:61.14ms
step:565/2330 train_time:34544ms step_avg:61.14ms
step:566/2330 train_time:34606ms step_avg:61.14ms
step:567/2330 train_time:34667ms step_avg:61.14ms
step:568/2330 train_time:34729ms step_avg:61.14ms
step:569/2330 train_time:34789ms step_avg:61.14ms
step:570/2330 train_time:34852ms step_avg:61.14ms
step:571/2330 train_time:34912ms step_avg:61.14ms
step:572/2330 train_time:34975ms step_avg:61.15ms
step:573/2330 train_time:35035ms step_avg:61.14ms
step:574/2330 train_time:35097ms step_avg:61.15ms
step:575/2330 train_time:35157ms step_avg:61.14ms
step:576/2330 train_time:35220ms step_avg:61.15ms
step:577/2330 train_time:35279ms step_avg:61.14ms
step:578/2330 train_time:35341ms step_avg:61.14ms
step:579/2330 train_time:35401ms step_avg:61.14ms
step:580/2330 train_time:35463ms step_avg:61.14ms
step:581/2330 train_time:35522ms step_avg:61.14ms
step:582/2330 train_time:35584ms step_avg:61.14ms
step:583/2330 train_time:35644ms step_avg:61.14ms
step:584/2330 train_time:35707ms step_avg:61.14ms
step:585/2330 train_time:35767ms step_avg:61.14ms
step:586/2330 train_time:35829ms step_avg:61.14ms
step:587/2330 train_time:35889ms step_avg:61.14ms
step:588/2330 train_time:35951ms step_avg:61.14ms
step:589/2330 train_time:36011ms step_avg:61.14ms
step:590/2330 train_time:36074ms step_avg:61.14ms
step:591/2330 train_time:36135ms step_avg:61.14ms
step:592/2330 train_time:36197ms step_avg:61.14ms
step:593/2330 train_time:36257ms step_avg:61.14ms
step:594/2330 train_time:36319ms step_avg:61.14ms
step:595/2330 train_time:36380ms step_avg:61.14ms
step:596/2330 train_time:36442ms step_avg:61.14ms
step:597/2330 train_time:36502ms step_avg:61.14ms
step:598/2330 train_time:36564ms step_avg:61.14ms
step:599/2330 train_time:36625ms step_avg:61.14ms
step:600/2330 train_time:36686ms step_avg:61.14ms
step:601/2330 train_time:36746ms step_avg:61.14ms
step:602/2330 train_time:36808ms step_avg:61.14ms
step:603/2330 train_time:36867ms step_avg:61.14ms
step:604/2330 train_time:36929ms step_avg:61.14ms
step:605/2330 train_time:36989ms step_avg:61.14ms
step:606/2330 train_time:37052ms step_avg:61.14ms
step:607/2330 train_time:37113ms step_avg:61.14ms
step:608/2330 train_time:37175ms step_avg:61.14ms
step:609/2330 train_time:37236ms step_avg:61.14ms
step:610/2330 train_time:37299ms step_avg:61.15ms
step:611/2330 train_time:37359ms step_avg:61.14ms
step:612/2330 train_time:37421ms step_avg:61.14ms
step:613/2330 train_time:37481ms step_avg:61.14ms
step:614/2330 train_time:37543ms step_avg:61.14ms
step:615/2330 train_time:37603ms step_avg:61.14ms
step:616/2330 train_time:37665ms step_avg:61.14ms
step:617/2330 train_time:37725ms step_avg:61.14ms
step:618/2330 train_time:37787ms step_avg:61.14ms
step:619/2330 train_time:37847ms step_avg:61.14ms
step:620/2330 train_time:37909ms step_avg:61.14ms
step:621/2330 train_time:37969ms step_avg:61.14ms
step:622/2330 train_time:38031ms step_avg:61.14ms
step:623/2330 train_time:38091ms step_avg:61.14ms
step:624/2330 train_time:38154ms step_avg:61.14ms
step:625/2330 train_time:38215ms step_avg:61.14ms
step:626/2330 train_time:38277ms step_avg:61.15ms
step:627/2330 train_time:38338ms step_avg:61.14ms
step:628/2330 train_time:38400ms step_avg:61.15ms
step:629/2330 train_time:38460ms step_avg:61.14ms
step:630/2330 train_time:38522ms step_avg:61.15ms
step:631/2330 train_time:38582ms step_avg:61.14ms
step:632/2330 train_time:38643ms step_avg:61.14ms
step:633/2330 train_time:38703ms step_avg:61.14ms
step:634/2330 train_time:38766ms step_avg:61.14ms
step:635/2330 train_time:38826ms step_avg:61.14ms
step:636/2330 train_time:38888ms step_avg:61.14ms
step:637/2330 train_time:38947ms step_avg:61.14ms
step:638/2330 train_time:39009ms step_avg:61.14ms
step:639/2330 train_time:39070ms step_avg:61.14ms
step:640/2330 train_time:39133ms step_avg:61.14ms
step:641/2330 train_time:39194ms step_avg:61.14ms
step:642/2330 train_time:39256ms step_avg:61.15ms
step:643/2330 train_time:39317ms step_avg:61.15ms
step:644/2330 train_time:39379ms step_avg:61.15ms
step:645/2330 train_time:39440ms step_avg:61.15ms
step:646/2330 train_time:39502ms step_avg:61.15ms
step:647/2330 train_time:39562ms step_avg:61.15ms
step:648/2330 train_time:39624ms step_avg:61.15ms
step:649/2330 train_time:39684ms step_avg:61.15ms
step:650/2330 train_time:39746ms step_avg:61.15ms
step:651/2330 train_time:39805ms step_avg:61.14ms
step:652/2330 train_time:39867ms step_avg:61.15ms
step:653/2330 train_time:39927ms step_avg:61.14ms
step:654/2330 train_time:39989ms step_avg:61.15ms
step:655/2330 train_time:40050ms step_avg:61.14ms
step:656/2330 train_time:40112ms step_avg:61.15ms
step:657/2330 train_time:40171ms step_avg:61.14ms
step:658/2330 train_time:40234ms step_avg:61.15ms
step:659/2330 train_time:40295ms step_avg:61.15ms
step:660/2330 train_time:40357ms step_avg:61.15ms
step:661/2330 train_time:40417ms step_avg:61.15ms
step:662/2330 train_time:40479ms step_avg:61.15ms
step:663/2330 train_time:40540ms step_avg:61.15ms
step:664/2330 train_time:40602ms step_avg:61.15ms
step:665/2330 train_time:40661ms step_avg:61.14ms
step:666/2330 train_time:40724ms step_avg:61.15ms
step:667/2330 train_time:40783ms step_avg:61.14ms
step:668/2330 train_time:40845ms step_avg:61.14ms
step:669/2330 train_time:40905ms step_avg:61.14ms
step:670/2330 train_time:40966ms step_avg:61.14ms
step:671/2330 train_time:41026ms step_avg:61.14ms
step:672/2330 train_time:41089ms step_avg:61.14ms
step:673/2330 train_time:41150ms step_avg:61.14ms
step:674/2330 train_time:41212ms step_avg:61.15ms
step:675/2330 train_time:41273ms step_avg:61.15ms
step:676/2330 train_time:41336ms step_avg:61.15ms
step:677/2330 train_time:41395ms step_avg:61.15ms
step:678/2330 train_time:41457ms step_avg:61.15ms
step:679/2330 train_time:41518ms step_avg:61.15ms
step:680/2330 train_time:41580ms step_avg:61.15ms
step:681/2330 train_time:41641ms step_avg:61.15ms
step:682/2330 train_time:41703ms step_avg:61.15ms
step:683/2330 train_time:41763ms step_avg:61.15ms
step:684/2330 train_time:41824ms step_avg:61.15ms
step:685/2330 train_time:41884ms step_avg:61.15ms
step:686/2330 train_time:41947ms step_avg:61.15ms
step:687/2330 train_time:42006ms step_avg:61.14ms
step:688/2330 train_time:42068ms step_avg:61.15ms
step:689/2330 train_time:42128ms step_avg:61.14ms
step:690/2330 train_time:42191ms step_avg:61.15ms
step:691/2330 train_time:42252ms step_avg:61.15ms
step:692/2330 train_time:42314ms step_avg:61.15ms
step:693/2330 train_time:42374ms step_avg:61.15ms
step:694/2330 train_time:42436ms step_avg:61.15ms
step:695/2330 train_time:42496ms step_avg:61.14ms
step:696/2330 train_time:42558ms step_avg:61.15ms
step:697/2330 train_time:42618ms step_avg:61.15ms
step:698/2330 train_time:42681ms step_avg:61.15ms
step:699/2330 train_time:42741ms step_avg:61.15ms
step:700/2330 train_time:42803ms step_avg:61.15ms
step:701/2330 train_time:42863ms step_avg:61.15ms
step:702/2330 train_time:42925ms step_avg:61.15ms
step:703/2330 train_time:42985ms step_avg:61.14ms
step:704/2330 train_time:43046ms step_avg:61.15ms
step:705/2330 train_time:43107ms step_avg:61.14ms
step:706/2330 train_time:43169ms step_avg:61.15ms
step:707/2330 train_time:43229ms step_avg:61.14ms
step:708/2330 train_time:43293ms step_avg:61.15ms
step:709/2330 train_time:43353ms step_avg:61.15ms
step:710/2330 train_time:43416ms step_avg:61.15ms
step:711/2330 train_time:43476ms step_avg:61.15ms
step:712/2330 train_time:43538ms step_avg:61.15ms
step:713/2330 train_time:43599ms step_avg:61.15ms
step:714/2330 train_time:43660ms step_avg:61.15ms
step:715/2330 train_time:43720ms step_avg:61.15ms
step:716/2330 train_time:43783ms step_avg:61.15ms
step:717/2330 train_time:43843ms step_avg:61.15ms
step:718/2330 train_time:43904ms step_avg:61.15ms
step:719/2330 train_time:43964ms step_avg:61.15ms
step:720/2330 train_time:44026ms step_avg:61.15ms
step:721/2330 train_time:44087ms step_avg:61.15ms
step:722/2330 train_time:44148ms step_avg:61.15ms
step:723/2330 train_time:44208ms step_avg:61.15ms
step:724/2330 train_time:44271ms step_avg:61.15ms
step:725/2330 train_time:44332ms step_avg:61.15ms
step:726/2330 train_time:44395ms step_avg:61.15ms
step:727/2330 train_time:44456ms step_avg:61.15ms
step:728/2330 train_time:44518ms step_avg:61.15ms
step:729/2330 train_time:44579ms step_avg:61.15ms
step:730/2330 train_time:44641ms step_avg:61.15ms
step:731/2330 train_time:44701ms step_avg:61.15ms
step:732/2330 train_time:44763ms step_avg:61.15ms
step:733/2330 train_time:44824ms step_avg:61.15ms
step:734/2330 train_time:44886ms step_avg:61.15ms
step:735/2330 train_time:44946ms step_avg:61.15ms
step:736/2330 train_time:45008ms step_avg:61.15ms
step:737/2330 train_time:45068ms step_avg:61.15ms
step:738/2330 train_time:45130ms step_avg:61.15ms
step:739/2330 train_time:45190ms step_avg:61.15ms
step:740/2330 train_time:45252ms step_avg:61.15ms
step:741/2330 train_time:45313ms step_avg:61.15ms
step:742/2330 train_time:45376ms step_avg:61.15ms
step:743/2330 train_time:45436ms step_avg:61.15ms
step:744/2330 train_time:45499ms step_avg:61.15ms
step:745/2330 train_time:45559ms step_avg:61.15ms
step:746/2330 train_time:45622ms step_avg:61.15ms
step:747/2330 train_time:45682ms step_avg:61.15ms
step:748/2330 train_time:45743ms step_avg:61.15ms
step:749/2330 train_time:45803ms step_avg:61.15ms
step:750/2330 train_time:45865ms step_avg:61.15ms
step:750/2330 val_loss:4.1965 train_time:45929ms step_avg:61.24ms
step:751/2330 train_time:45952ms step_avg:61.19ms
step:752/2330 train_time:45988ms step_avg:61.15ms
step:753/2330 train_time:46055ms step_avg:61.16ms
step:754/2330 train_time:46122ms step_avg:61.17ms
step:755/2330 train_time:46183ms step_avg:61.17ms
step:756/2330 train_time:46245ms step_avg:61.17ms
step:757/2330 train_time:46305ms step_avg:61.17ms
step:758/2330 train_time:46367ms step_avg:61.17ms
step:759/2330 train_time:46427ms step_avg:61.17ms
step:760/2330 train_time:46488ms step_avg:61.17ms
step:761/2330 train_time:46547ms step_avg:61.17ms
step:762/2330 train_time:46608ms step_avg:61.17ms
step:763/2330 train_time:46668ms step_avg:61.16ms
step:764/2330 train_time:46729ms step_avg:61.16ms
step:765/2330 train_time:46788ms step_avg:61.16ms
step:766/2330 train_time:46850ms step_avg:61.16ms
step:767/2330 train_time:46911ms step_avg:61.16ms
step:768/2330 train_time:46975ms step_avg:61.16ms
step:769/2330 train_time:47037ms step_avg:61.17ms
step:770/2330 train_time:47101ms step_avg:61.17ms
step:771/2330 train_time:47164ms step_avg:61.17ms
step:772/2330 train_time:47228ms step_avg:61.18ms
step:773/2330 train_time:47288ms step_avg:61.17ms
step:774/2330 train_time:47351ms step_avg:61.18ms
step:775/2330 train_time:47411ms step_avg:61.18ms
step:776/2330 train_time:47474ms step_avg:61.18ms
step:777/2330 train_time:47535ms step_avg:61.18ms
step:778/2330 train_time:47597ms step_avg:61.18ms
step:779/2330 train_time:47658ms step_avg:61.18ms
step:780/2330 train_time:47722ms step_avg:61.18ms
step:781/2330 train_time:47782ms step_avg:61.18ms
step:782/2330 train_time:47845ms step_avg:61.18ms
step:783/2330 train_time:47906ms step_avg:61.18ms
step:784/2330 train_time:47970ms step_avg:61.19ms
step:785/2330 train_time:48031ms step_avg:61.19ms
step:786/2330 train_time:48093ms step_avg:61.19ms
step:787/2330 train_time:48154ms step_avg:61.19ms
step:788/2330 train_time:48218ms step_avg:61.19ms
step:789/2330 train_time:48280ms step_avg:61.19ms
step:790/2330 train_time:48343ms step_avg:61.19ms
step:791/2330 train_time:48403ms step_avg:61.19ms
step:792/2330 train_time:48466ms step_avg:61.19ms
step:793/2330 train_time:48528ms step_avg:61.19ms
step:794/2330 train_time:48590ms step_avg:61.20ms
step:795/2330 train_time:48650ms step_avg:61.20ms
step:796/2330 train_time:48712ms step_avg:61.20ms
step:797/2330 train_time:48773ms step_avg:61.20ms
step:798/2330 train_time:48836ms step_avg:61.20ms
step:799/2330 train_time:48897ms step_avg:61.20ms
step:800/2330 train_time:48961ms step_avg:61.20ms
step:801/2330 train_time:49023ms step_avg:61.20ms
step:802/2330 train_time:49085ms step_avg:61.20ms
step:803/2330 train_time:49147ms step_avg:61.20ms
step:804/2330 train_time:49210ms step_avg:61.21ms
step:805/2330 train_time:49271ms step_avg:61.21ms
step:806/2330 train_time:49334ms step_avg:61.21ms
step:807/2330 train_time:49395ms step_avg:61.21ms
step:808/2330 train_time:49458ms step_avg:61.21ms
step:809/2330 train_time:49521ms step_avg:61.21ms
step:810/2330 train_time:49584ms step_avg:61.21ms
step:811/2330 train_time:49644ms step_avg:61.21ms
step:812/2330 train_time:49707ms step_avg:61.22ms
step:813/2330 train_time:49768ms step_avg:61.21ms
step:814/2330 train_time:49830ms step_avg:61.22ms
step:815/2330 train_time:49891ms step_avg:61.22ms
step:816/2330 train_time:49954ms step_avg:61.22ms
step:817/2330 train_time:50014ms step_avg:61.22ms
step:818/2330 train_time:50077ms step_avg:61.22ms
step:819/2330 train_time:50139ms step_avg:61.22ms
step:820/2330 train_time:50202ms step_avg:61.22ms
step:821/2330 train_time:50263ms step_avg:61.22ms
step:822/2330 train_time:50326ms step_avg:61.22ms
step:823/2330 train_time:50387ms step_avg:61.22ms
step:824/2330 train_time:50450ms step_avg:61.23ms
step:825/2330 train_time:50510ms step_avg:61.22ms
step:826/2330 train_time:50573ms step_avg:61.23ms
step:827/2330 train_time:50634ms step_avg:61.23ms
step:828/2330 train_time:50696ms step_avg:61.23ms
step:829/2330 train_time:50757ms step_avg:61.23ms
step:830/2330 train_time:50821ms step_avg:61.23ms
step:831/2330 train_time:50881ms step_avg:61.23ms
step:832/2330 train_time:50944ms step_avg:61.23ms
step:833/2330 train_time:51005ms step_avg:61.23ms
step:834/2330 train_time:51069ms step_avg:61.23ms
step:835/2330 train_time:51129ms step_avg:61.23ms
step:836/2330 train_time:51192ms step_avg:61.23ms
step:837/2330 train_time:51252ms step_avg:61.23ms
step:838/2330 train_time:51314ms step_avg:61.23ms
step:839/2330 train_time:51376ms step_avg:61.23ms
step:840/2330 train_time:51440ms step_avg:61.24ms
step:841/2330 train_time:51501ms step_avg:61.24ms
step:842/2330 train_time:51564ms step_avg:61.24ms
step:843/2330 train_time:51625ms step_avg:61.24ms
step:844/2330 train_time:51688ms step_avg:61.24ms
step:845/2330 train_time:51749ms step_avg:61.24ms
step:846/2330 train_time:51812ms step_avg:61.24ms
step:847/2330 train_time:51871ms step_avg:61.24ms
step:848/2330 train_time:51935ms step_avg:61.24ms
step:849/2330 train_time:51996ms step_avg:61.24ms
step:850/2330 train_time:52060ms step_avg:61.25ms
step:851/2330 train_time:52121ms step_avg:61.25ms
step:852/2330 train_time:52184ms step_avg:61.25ms
step:853/2330 train_time:52245ms step_avg:61.25ms
step:854/2330 train_time:52309ms step_avg:61.25ms
step:855/2330 train_time:52370ms step_avg:61.25ms
step:856/2330 train_time:52433ms step_avg:61.25ms
step:857/2330 train_time:52493ms step_avg:61.25ms
step:858/2330 train_time:52557ms step_avg:61.26ms
step:859/2330 train_time:52618ms step_avg:61.26ms
step:860/2330 train_time:52682ms step_avg:61.26ms
step:861/2330 train_time:52742ms step_avg:61.26ms
step:862/2330 train_time:52805ms step_avg:61.26ms
step:863/2330 train_time:52866ms step_avg:61.26ms
step:864/2330 train_time:52929ms step_avg:61.26ms
step:865/2330 train_time:52990ms step_avg:61.26ms
step:866/2330 train_time:53053ms step_avg:61.26ms
step:867/2330 train_time:53114ms step_avg:61.26ms
step:868/2330 train_time:53178ms step_avg:61.26ms
step:869/2330 train_time:53239ms step_avg:61.26ms
step:870/2330 train_time:53301ms step_avg:61.27ms
step:871/2330 train_time:53363ms step_avg:61.27ms
step:872/2330 train_time:53426ms step_avg:61.27ms
step:873/2330 train_time:53488ms step_avg:61.27ms
step:874/2330 train_time:53551ms step_avg:61.27ms
step:875/2330 train_time:53612ms step_avg:61.27ms
step:876/2330 train_time:53674ms step_avg:61.27ms
step:877/2330 train_time:53735ms step_avg:61.27ms
step:878/2330 train_time:53798ms step_avg:61.27ms
step:879/2330 train_time:53860ms step_avg:61.27ms
step:880/2330 train_time:53923ms step_avg:61.28ms
step:881/2330 train_time:53984ms step_avg:61.28ms
step:882/2330 train_time:54047ms step_avg:61.28ms
step:883/2330 train_time:54109ms step_avg:61.28ms
step:884/2330 train_time:54172ms step_avg:61.28ms
step:885/2330 train_time:54232ms step_avg:61.28ms
step:886/2330 train_time:54294ms step_avg:61.28ms
step:887/2330 train_time:54355ms step_avg:61.28ms
step:888/2330 train_time:54419ms step_avg:61.28ms
step:889/2330 train_time:54480ms step_avg:61.28ms
step:890/2330 train_time:54543ms step_avg:61.28ms
step:891/2330 train_time:54604ms step_avg:61.28ms
step:892/2330 train_time:54667ms step_avg:61.29ms
step:893/2330 train_time:54727ms step_avg:61.28ms
step:894/2330 train_time:54790ms step_avg:61.29ms
step:895/2330 train_time:54850ms step_avg:61.29ms
step:896/2330 train_time:54914ms step_avg:61.29ms
step:897/2330 train_time:54975ms step_avg:61.29ms
step:898/2330 train_time:55040ms step_avg:61.29ms
step:899/2330 train_time:55101ms step_avg:61.29ms
step:900/2330 train_time:55164ms step_avg:61.29ms
step:901/2330 train_time:55226ms step_avg:61.29ms
step:902/2330 train_time:55289ms step_avg:61.30ms
step:903/2330 train_time:55350ms step_avg:61.30ms
step:904/2330 train_time:55412ms step_avg:61.30ms
step:905/2330 train_time:55473ms step_avg:61.30ms
step:906/2330 train_time:55535ms step_avg:61.30ms
step:907/2330 train_time:55596ms step_avg:61.30ms
step:908/2330 train_time:55660ms step_avg:61.30ms
step:909/2330 train_time:55722ms step_avg:61.30ms
step:910/2330 train_time:55784ms step_avg:61.30ms
step:911/2330 train_time:55845ms step_avg:61.30ms
step:912/2330 train_time:55908ms step_avg:61.30ms
step:913/2330 train_time:55969ms step_avg:61.30ms
step:914/2330 train_time:56032ms step_avg:61.30ms
step:915/2330 train_time:56093ms step_avg:61.30ms
step:916/2330 train_time:56156ms step_avg:61.31ms
step:917/2330 train_time:56217ms step_avg:61.31ms
step:918/2330 train_time:56281ms step_avg:61.31ms
step:919/2330 train_time:56342ms step_avg:61.31ms
step:920/2330 train_time:56405ms step_avg:61.31ms
step:921/2330 train_time:56465ms step_avg:61.31ms
step:922/2330 train_time:56528ms step_avg:61.31ms
step:923/2330 train_time:56590ms step_avg:61.31ms
step:924/2330 train_time:56653ms step_avg:61.31ms
step:925/2330 train_time:56713ms step_avg:61.31ms
step:926/2330 train_time:56775ms step_avg:61.31ms
step:927/2330 train_time:56836ms step_avg:61.31ms
step:928/2330 train_time:56900ms step_avg:61.31ms
step:929/2330 train_time:56961ms step_avg:61.31ms
step:930/2330 train_time:57024ms step_avg:61.32ms
step:931/2330 train_time:57085ms step_avg:61.32ms
step:932/2330 train_time:57147ms step_avg:61.32ms
step:933/2330 train_time:57208ms step_avg:61.32ms
step:934/2330 train_time:57271ms step_avg:61.32ms
step:935/2330 train_time:57332ms step_avg:61.32ms
step:936/2330 train_time:57394ms step_avg:61.32ms
step:937/2330 train_time:57456ms step_avg:61.32ms
step:938/2330 train_time:57520ms step_avg:61.32ms
step:939/2330 train_time:57581ms step_avg:61.32ms
step:940/2330 train_time:57644ms step_avg:61.32ms
step:941/2330 train_time:57705ms step_avg:61.32ms
step:942/2330 train_time:57768ms step_avg:61.33ms
step:943/2330 train_time:57830ms step_avg:61.33ms
step:944/2330 train_time:57893ms step_avg:61.33ms
step:945/2330 train_time:57953ms step_avg:61.33ms
step:946/2330 train_time:58016ms step_avg:61.33ms
step:947/2330 train_time:58077ms step_avg:61.33ms
step:948/2330 train_time:58140ms step_avg:61.33ms
step:949/2330 train_time:58202ms step_avg:61.33ms
step:950/2330 train_time:58265ms step_avg:61.33ms
step:951/2330 train_time:58326ms step_avg:61.33ms
step:952/2330 train_time:58388ms step_avg:61.33ms
step:953/2330 train_time:58457ms step_avg:61.34ms
step:954/2330 train_time:58513ms step_avg:61.33ms
step:955/2330 train_time:58573ms step_avg:61.33ms
step:956/2330 train_time:58637ms step_avg:61.34ms
step:957/2330 train_time:58697ms step_avg:61.33ms
step:958/2330 train_time:58761ms step_avg:61.34ms
step:959/2330 train_time:58823ms step_avg:61.34ms
step:960/2330 train_time:58885ms step_avg:61.34ms
step:961/2330 train_time:58946ms step_avg:61.34ms
step:962/2330 train_time:59010ms step_avg:61.34ms
step:963/2330 train_time:59070ms step_avg:61.34ms
step:964/2330 train_time:59133ms step_avg:61.34ms
step:965/2330 train_time:59194ms step_avg:61.34ms
step:966/2330 train_time:59257ms step_avg:61.34ms
step:967/2330 train_time:59318ms step_avg:61.34ms
step:968/2330 train_time:59381ms step_avg:61.34ms
step:969/2330 train_time:59442ms step_avg:61.34ms
step:970/2330 train_time:59504ms step_avg:61.34ms
step:971/2330 train_time:59566ms step_avg:61.35ms
step:972/2330 train_time:59629ms step_avg:61.35ms
step:973/2330 train_time:59690ms step_avg:61.35ms
step:974/2330 train_time:59753ms step_avg:61.35ms
step:975/2330 train_time:59813ms step_avg:61.35ms
step:976/2330 train_time:59876ms step_avg:61.35ms
step:977/2330 train_time:59937ms step_avg:61.35ms
step:978/2330 train_time:60000ms step_avg:61.35ms
step:979/2330 train_time:60062ms step_avg:61.35ms
step:980/2330 train_time:60125ms step_avg:61.35ms
step:981/2330 train_time:60186ms step_avg:61.35ms
step:982/2330 train_time:60250ms step_avg:61.35ms
step:983/2330 train_time:60310ms step_avg:61.35ms
step:984/2330 train_time:60372ms step_avg:61.35ms
step:985/2330 train_time:60433ms step_avg:61.35ms
step:986/2330 train_time:60495ms step_avg:61.35ms
step:987/2330 train_time:60556ms step_avg:61.35ms
step:988/2330 train_time:60620ms step_avg:61.36ms
step:989/2330 train_time:60681ms step_avg:61.36ms
step:990/2330 train_time:60744ms step_avg:61.36ms
step:991/2330 train_time:60804ms step_avg:61.36ms
step:992/2330 train_time:60868ms step_avg:61.36ms
step:993/2330 train_time:60929ms step_avg:61.36ms
step:994/2330 train_time:60991ms step_avg:61.36ms
step:995/2330 train_time:61052ms step_avg:61.36ms
step:996/2330 train_time:61114ms step_avg:61.36ms
step:997/2330 train_time:61176ms step_avg:61.36ms
step:998/2330 train_time:61239ms step_avg:61.36ms
step:999/2330 train_time:61300ms step_avg:61.36ms
step:1000/2330 train_time:61363ms step_avg:61.36ms
step:1000/2330 val_loss:3.9941 train_time:61428ms step_avg:61.43ms
step:1001/2330 train_time:61450ms step_avg:61.39ms
step:1002/2330 train_time:61488ms step_avg:61.37ms
step:1003/2330 train_time:61556ms step_avg:61.37ms
step:1004/2330 train_time:61622ms step_avg:61.38ms
step:1005/2330 train_time:61683ms step_avg:61.38ms
step:1006/2330 train_time:61746ms step_avg:61.38ms
step:1007/2330 train_time:61806ms step_avg:61.38ms
step:1008/2330 train_time:61869ms step_avg:61.38ms
step:1009/2330 train_time:61928ms step_avg:61.38ms
step:1010/2330 train_time:61990ms step_avg:61.38ms
step:1011/2330 train_time:62051ms step_avg:61.38ms
step:1012/2330 train_time:62112ms step_avg:61.38ms
step:1013/2330 train_time:62172ms step_avg:61.37ms
step:1014/2330 train_time:62234ms step_avg:61.37ms
step:1015/2330 train_time:62294ms step_avg:61.37ms
step:1016/2330 train_time:62359ms step_avg:61.38ms
step:1017/2330 train_time:62420ms step_avg:61.38ms
step:1018/2330 train_time:62485ms step_avg:61.38ms
step:1019/2330 train_time:62549ms step_avg:61.38ms
step:1020/2330 train_time:62612ms step_avg:61.38ms
step:1021/2330 train_time:62674ms step_avg:61.38ms
step:1022/2330 train_time:62736ms step_avg:61.39ms
step:1023/2330 train_time:62797ms step_avg:61.39ms
step:1024/2330 train_time:62861ms step_avg:61.39ms
step:1025/2330 train_time:62921ms step_avg:61.39ms
step:1026/2330 train_time:62984ms step_avg:61.39ms
step:1027/2330 train_time:63044ms step_avg:61.39ms
step:1028/2330 train_time:63106ms step_avg:61.39ms
step:1029/2330 train_time:63167ms step_avg:61.39ms
step:1030/2330 train_time:63229ms step_avg:61.39ms
step:1031/2330 train_time:63290ms step_avg:61.39ms
step:1032/2330 train_time:63354ms step_avg:61.39ms
step:1033/2330 train_time:63414ms step_avg:61.39ms
step:1034/2330 train_time:63478ms step_avg:61.39ms
step:1035/2330 train_time:63539ms step_avg:61.39ms
step:1036/2330 train_time:63603ms step_avg:61.39ms
step:1037/2330 train_time:63665ms step_avg:61.39ms
step:1038/2330 train_time:63729ms step_avg:61.40ms
step:1039/2330 train_time:63790ms step_avg:61.40ms
step:1040/2330 train_time:63853ms step_avg:61.40ms
step:1041/2330 train_time:63914ms step_avg:61.40ms
step:1042/2330 train_time:63976ms step_avg:61.40ms
step:1043/2330 train_time:64036ms step_avg:61.40ms
step:1044/2330 train_time:64099ms step_avg:61.40ms
step:1045/2330 train_time:64160ms step_avg:61.40ms
step:1046/2330 train_time:64223ms step_avg:61.40ms
step:1047/2330 train_time:64284ms step_avg:61.40ms
step:1048/2330 train_time:64347ms step_avg:61.40ms
step:1049/2330 train_time:64408ms step_avg:61.40ms
step:1050/2330 train_time:64471ms step_avg:61.40ms
step:1051/2330 train_time:64533ms step_avg:61.40ms
step:1052/2330 train_time:64595ms step_avg:61.40ms
step:1053/2330 train_time:64656ms step_avg:61.40ms
step:1054/2330 train_time:64720ms step_avg:61.40ms
step:1055/2330 train_time:64781ms step_avg:61.40ms
step:1056/2330 train_time:64845ms step_avg:61.41ms
step:1057/2330 train_time:64906ms step_avg:61.41ms
step:1058/2330 train_time:64968ms step_avg:61.41ms
step:1059/2330 train_time:65030ms step_avg:61.41ms
step:1060/2330 train_time:65093ms step_avg:61.41ms
step:1061/2330 train_time:65154ms step_avg:61.41ms
step:1062/2330 train_time:65216ms step_avg:61.41ms
step:1063/2330 train_time:65276ms step_avg:61.41ms
step:1064/2330 train_time:65339ms step_avg:61.41ms
step:1065/2330 train_time:65401ms step_avg:61.41ms
step:1066/2330 train_time:65464ms step_avg:61.41ms
step:1067/2330 train_time:65525ms step_avg:61.41ms
step:1068/2330 train_time:65589ms step_avg:61.41ms
step:1069/2330 train_time:65650ms step_avg:61.41ms
step:1070/2330 train_time:65713ms step_avg:61.41ms
step:1071/2330 train_time:65774ms step_avg:61.41ms
step:1072/2330 train_time:65836ms step_avg:61.41ms
step:1073/2330 train_time:65897ms step_avg:61.41ms
step:1074/2330 train_time:65961ms step_avg:61.42ms
step:1075/2330 train_time:66022ms step_avg:61.42ms
step:1076/2330 train_time:66085ms step_avg:61.42ms
step:1077/2330 train_time:66145ms step_avg:61.42ms
step:1078/2330 train_time:66209ms step_avg:61.42ms
step:1079/2330 train_time:66270ms step_avg:61.42ms
step:1080/2330 train_time:66333ms step_avg:61.42ms
step:1081/2330 train_time:66394ms step_avg:61.42ms
step:1082/2330 train_time:66457ms step_avg:61.42ms
step:1083/2330 train_time:66518ms step_avg:61.42ms
step:1084/2330 train_time:66581ms step_avg:61.42ms
step:1085/2330 train_time:66643ms step_avg:61.42ms
step:1086/2330 train_time:66706ms step_avg:61.42ms
step:1087/2330 train_time:66767ms step_avg:61.42ms
step:1088/2330 train_time:66830ms step_avg:61.42ms
step:1089/2330 train_time:66891ms step_avg:61.42ms
step:1090/2330 train_time:66955ms step_avg:61.43ms
step:1091/2330 train_time:67015ms step_avg:61.43ms
step:1092/2330 train_time:67077ms step_avg:61.43ms
step:1093/2330 train_time:67138ms step_avg:61.43ms
step:1094/2330 train_time:67202ms step_avg:61.43ms
step:1095/2330 train_time:67263ms step_avg:61.43ms
step:1096/2330 train_time:67326ms step_avg:61.43ms
step:1097/2330 train_time:67387ms step_avg:61.43ms
step:1098/2330 train_time:67450ms step_avg:61.43ms
step:1099/2330 train_time:67511ms step_avg:61.43ms
step:1100/2330 train_time:67574ms step_avg:61.43ms
step:1101/2330 train_time:67635ms step_avg:61.43ms
step:1102/2330 train_time:67698ms step_avg:61.43ms
step:1103/2330 train_time:67759ms step_avg:61.43ms
step:1104/2330 train_time:67823ms step_avg:61.43ms
step:1105/2330 train_time:67884ms step_avg:61.43ms
step:1106/2330 train_time:67948ms step_avg:61.44ms
step:1107/2330 train_time:68010ms step_avg:61.44ms
step:1108/2330 train_time:68072ms step_avg:61.44ms
step:1109/2330 train_time:68133ms step_avg:61.44ms
step:1110/2330 train_time:68196ms step_avg:61.44ms
step:1111/2330 train_time:68256ms step_avg:61.44ms
step:1112/2330 train_time:68320ms step_avg:61.44ms
step:1113/2330 train_time:68381ms step_avg:61.44ms
step:1114/2330 train_time:68445ms step_avg:61.44ms
step:1115/2330 train_time:68505ms step_avg:61.44ms
step:1116/2330 train_time:68568ms step_avg:61.44ms
step:1117/2330 train_time:68629ms step_avg:61.44ms
step:1118/2330 train_time:68692ms step_avg:61.44ms
step:1119/2330 train_time:68753ms step_avg:61.44ms
step:1120/2330 train_time:68816ms step_avg:61.44ms
step:1121/2330 train_time:68877ms step_avg:61.44ms
step:1122/2330 train_time:68941ms step_avg:61.44ms
step:1123/2330 train_time:69003ms step_avg:61.45ms
step:1124/2330 train_time:69066ms step_avg:61.45ms
step:1125/2330 train_time:69126ms step_avg:61.45ms
step:1126/2330 train_time:69189ms step_avg:61.45ms
step:1127/2330 train_time:69250ms step_avg:61.45ms
step:1128/2330 train_time:69313ms step_avg:61.45ms
step:1129/2330 train_time:69374ms step_avg:61.45ms
step:1130/2330 train_time:69436ms step_avg:61.45ms
step:1131/2330 train_time:69497ms step_avg:61.45ms
step:1132/2330 train_time:69561ms step_avg:61.45ms
step:1133/2330 train_time:69622ms step_avg:61.45ms
step:1134/2330 train_time:69686ms step_avg:61.45ms
step:1135/2330 train_time:69747ms step_avg:61.45ms
step:1136/2330 train_time:69810ms step_avg:61.45ms
step:1137/2330 train_time:69871ms step_avg:61.45ms
step:1138/2330 train_time:69933ms step_avg:61.45ms
step:1139/2330 train_time:69993ms step_avg:61.45ms
step:1140/2330 train_time:70056ms step_avg:61.45ms
step:1141/2330 train_time:70117ms step_avg:61.45ms
step:1142/2330 train_time:70180ms step_avg:61.45ms
step:1143/2330 train_time:70241ms step_avg:61.45ms
step:1144/2330 train_time:70304ms step_avg:61.45ms
step:1145/2330 train_time:70366ms step_avg:61.45ms
step:1146/2330 train_time:70428ms step_avg:61.46ms
step:1147/2330 train_time:70490ms step_avg:61.46ms
step:1148/2330 train_time:70554ms step_avg:61.46ms
step:1149/2330 train_time:70614ms step_avg:61.46ms
step:1150/2330 train_time:70677ms step_avg:61.46ms
step:1151/2330 train_time:70739ms step_avg:61.46ms
step:1152/2330 train_time:70803ms step_avg:61.46ms
step:1153/2330 train_time:70865ms step_avg:61.46ms
step:1154/2330 train_time:70928ms step_avg:61.46ms
step:1155/2330 train_time:70989ms step_avg:61.46ms
step:1156/2330 train_time:71052ms step_avg:61.46ms
step:1157/2330 train_time:71112ms step_avg:61.46ms
step:1158/2330 train_time:71175ms step_avg:61.46ms
step:1159/2330 train_time:71236ms step_avg:61.46ms
step:1160/2330 train_time:71299ms step_avg:61.46ms
step:1161/2330 train_time:71360ms step_avg:61.46ms
step:1162/2330 train_time:71424ms step_avg:61.47ms
step:1163/2330 train_time:71486ms step_avg:61.47ms
step:1164/2330 train_time:71549ms step_avg:61.47ms
step:1165/2330 train_time:71611ms step_avg:61.47ms
step:1166/2330 train_time:71674ms step_avg:61.47ms
step:1167/2330 train_time:71735ms step_avg:61.47ms
step:1168/2330 train_time:71798ms step_avg:61.47ms
step:1169/2330 train_time:71860ms step_avg:61.47ms
step:1170/2330 train_time:71922ms step_avg:61.47ms
step:1171/2330 train_time:71984ms step_avg:61.47ms
step:1172/2330 train_time:72047ms step_avg:61.47ms
step:1173/2330 train_time:72108ms step_avg:61.47ms
step:1174/2330 train_time:72172ms step_avg:61.48ms
step:1175/2330 train_time:72233ms step_avg:61.48ms
step:1176/2330 train_time:72296ms step_avg:61.48ms
step:1177/2330 train_time:72356ms step_avg:61.48ms
step:1178/2330 train_time:72419ms step_avg:61.48ms
step:1179/2330 train_time:72481ms step_avg:61.48ms
step:1180/2330 train_time:72544ms step_avg:61.48ms
step:1181/2330 train_time:72605ms step_avg:61.48ms
step:1182/2330 train_time:72668ms step_avg:61.48ms
step:1183/2330 train_time:72729ms step_avg:61.48ms
step:1184/2330 train_time:72793ms step_avg:61.48ms
step:1185/2330 train_time:72853ms step_avg:61.48ms
step:1186/2330 train_time:72916ms step_avg:61.48ms
step:1187/2330 train_time:72978ms step_avg:61.48ms
step:1188/2330 train_time:73041ms step_avg:61.48ms
step:1189/2330 train_time:73104ms step_avg:61.48ms
step:1190/2330 train_time:73167ms step_avg:61.48ms
step:1191/2330 train_time:73228ms step_avg:61.48ms
step:1192/2330 train_time:73291ms step_avg:61.49ms
step:1193/2330 train_time:73353ms step_avg:61.49ms
step:1194/2330 train_time:73415ms step_avg:61.49ms
step:1195/2330 train_time:73476ms step_avg:61.49ms
step:1196/2330 train_time:73540ms step_avg:61.49ms
step:1197/2330 train_time:73601ms step_avg:61.49ms
step:1198/2330 train_time:73665ms step_avg:61.49ms
step:1199/2330 train_time:73726ms step_avg:61.49ms
step:1200/2330 train_time:73789ms step_avg:61.49ms
step:1201/2330 train_time:73851ms step_avg:61.49ms
step:1202/2330 train_time:73914ms step_avg:61.49ms
step:1203/2330 train_time:73975ms step_avg:61.49ms
step:1204/2330 train_time:74037ms step_avg:61.49ms
step:1205/2330 train_time:74098ms step_avg:61.49ms
step:1206/2330 train_time:74161ms step_avg:61.49ms
step:1207/2330 train_time:74223ms step_avg:61.49ms
step:1208/2330 train_time:74285ms step_avg:61.49ms
step:1209/2330 train_time:74346ms step_avg:61.49ms
step:1210/2330 train_time:74409ms step_avg:61.50ms
step:1211/2330 train_time:74470ms step_avg:61.49ms
step:1212/2330 train_time:74534ms step_avg:61.50ms
step:1213/2330 train_time:74594ms step_avg:61.50ms
step:1214/2330 train_time:74657ms step_avg:61.50ms
step:1215/2330 train_time:74718ms step_avg:61.50ms
step:1216/2330 train_time:74782ms step_avg:61.50ms
step:1217/2330 train_time:74844ms step_avg:61.50ms
step:1218/2330 train_time:74906ms step_avg:61.50ms
step:1219/2330 train_time:74967ms step_avg:61.50ms
step:1220/2330 train_time:75029ms step_avg:61.50ms
step:1221/2330 train_time:75091ms step_avg:61.50ms
step:1222/2330 train_time:75155ms step_avg:61.50ms
step:1223/2330 train_time:75215ms step_avg:61.50ms
step:1224/2330 train_time:75277ms step_avg:61.50ms
step:1225/2330 train_time:75339ms step_avg:61.50ms
step:1226/2330 train_time:75402ms step_avg:61.50ms
step:1227/2330 train_time:75464ms step_avg:61.50ms
step:1228/2330 train_time:75527ms step_avg:61.50ms
step:1229/2330 train_time:75587ms step_avg:61.50ms
step:1230/2330 train_time:75651ms step_avg:61.50ms
step:1231/2330 train_time:75712ms step_avg:61.50ms
step:1232/2330 train_time:75775ms step_avg:61.51ms
step:1233/2330 train_time:75836ms step_avg:61.51ms
step:1234/2330 train_time:75899ms step_avg:61.51ms
step:1235/2330 train_time:75961ms step_avg:61.51ms
step:1236/2330 train_time:76025ms step_avg:61.51ms
step:1237/2330 train_time:76085ms step_avg:61.51ms
step:1238/2330 train_time:76148ms step_avg:61.51ms
step:1239/2330 train_time:76209ms step_avg:61.51ms
step:1240/2330 train_time:76272ms step_avg:61.51ms
step:1241/2330 train_time:76333ms step_avg:61.51ms
step:1242/2330 train_time:76395ms step_avg:61.51ms
step:1243/2330 train_time:76455ms step_avg:61.51ms
step:1244/2330 train_time:76518ms step_avg:61.51ms
step:1245/2330 train_time:76579ms step_avg:61.51ms
step:1246/2330 train_time:76643ms step_avg:61.51ms
step:1247/2330 train_time:76705ms step_avg:61.51ms
step:1248/2330 train_time:76768ms step_avg:61.51ms
step:1249/2330 train_time:76829ms step_avg:61.51ms
step:1250/2330 train_time:76892ms step_avg:61.51ms
step:1250/2330 val_loss:3.8793 train_time:76957ms step_avg:61.57ms
step:1251/2330 train_time:76979ms step_avg:61.53ms
step:1252/2330 train_time:77020ms step_avg:61.52ms
step:1253/2330 train_time:77086ms step_avg:61.52ms
step:1254/2330 train_time:77151ms step_avg:61.52ms
step:1255/2330 train_time:77211ms step_avg:61.52ms
step:1256/2330 train_time:77275ms step_avg:61.52ms
step:1257/2330 train_time:77335ms step_avg:61.52ms
step:1258/2330 train_time:77396ms step_avg:61.52ms
step:1259/2330 train_time:77456ms step_avg:61.52ms
step:1260/2330 train_time:77518ms step_avg:61.52ms
step:1261/2330 train_time:77578ms step_avg:61.52ms
step:1262/2330 train_time:77640ms step_avg:61.52ms
step:1263/2330 train_time:77701ms step_avg:61.52ms
step:1264/2330 train_time:77762ms step_avg:61.52ms
step:1265/2330 train_time:77822ms step_avg:61.52ms
step:1266/2330 train_time:77886ms step_avg:61.52ms
step:1267/2330 train_time:77948ms step_avg:61.52ms
step:1268/2330 train_time:78013ms step_avg:61.52ms
step:1269/2330 train_time:78076ms step_avg:61.53ms
step:1270/2330 train_time:78140ms step_avg:61.53ms
step:1271/2330 train_time:78201ms step_avg:61.53ms
step:1272/2330 train_time:78264ms step_avg:61.53ms
step:1273/2330 train_time:78325ms step_avg:61.53ms
step:1274/2330 train_time:78388ms step_avg:61.53ms
step:1275/2330 train_time:78449ms step_avg:61.53ms
step:1276/2330 train_time:78512ms step_avg:61.53ms
step:1277/2330 train_time:78573ms step_avg:61.53ms
step:1278/2330 train_time:78635ms step_avg:61.53ms
step:1279/2330 train_time:78695ms step_avg:61.53ms
step:1280/2330 train_time:78757ms step_avg:61.53ms
step:1281/2330 train_time:78817ms step_avg:61.53ms
step:1282/2330 train_time:78879ms step_avg:61.53ms
step:1283/2330 train_time:78940ms step_avg:61.53ms
step:1284/2330 train_time:79004ms step_avg:61.53ms
step:1285/2330 train_time:79066ms step_avg:61.53ms
step:1286/2330 train_time:79129ms step_avg:61.53ms
step:1287/2330 train_time:79192ms step_avg:61.53ms
step:1288/2330 train_time:79255ms step_avg:61.53ms
step:1289/2330 train_time:79315ms step_avg:61.53ms
step:1290/2330 train_time:79378ms step_avg:61.53ms
step:1291/2330 train_time:79439ms step_avg:61.53ms
step:1292/2330 train_time:79502ms step_avg:61.53ms
step:1293/2330 train_time:79563ms step_avg:61.53ms
step:1294/2330 train_time:79625ms step_avg:61.53ms
step:1295/2330 train_time:79686ms step_avg:61.53ms
step:1296/2330 train_time:79749ms step_avg:61.53ms
step:1297/2330 train_time:79810ms step_avg:61.53ms
step:1298/2330 train_time:79873ms step_avg:61.54ms
step:1299/2330 train_time:79934ms step_avg:61.53ms
step:1300/2330 train_time:79997ms step_avg:61.54ms
step:1301/2330 train_time:80058ms step_avg:61.54ms
step:1302/2330 train_time:80120ms step_avg:61.54ms
step:1303/2330 train_time:80181ms step_avg:61.54ms
step:1304/2330 train_time:80244ms step_avg:61.54ms
step:1305/2330 train_time:80305ms step_avg:61.54ms
step:1306/2330 train_time:80369ms step_avg:61.54ms
step:1307/2330 train_time:80430ms step_avg:61.54ms
step:1308/2330 train_time:80493ms step_avg:61.54ms
step:1309/2330 train_time:80553ms step_avg:61.54ms
step:1310/2330 train_time:80616ms step_avg:61.54ms
step:1311/2330 train_time:80676ms step_avg:61.54ms
step:1312/2330 train_time:80738ms step_avg:61.54ms
step:1313/2330 train_time:80799ms step_avg:61.54ms
step:1314/2330 train_time:80862ms step_avg:61.54ms
step:1315/2330 train_time:80923ms step_avg:61.54ms
step:1316/2330 train_time:80986ms step_avg:61.54ms
step:1317/2330 train_time:81047ms step_avg:61.54ms
step:1318/2330 train_time:81109ms step_avg:61.54ms
step:1319/2330 train_time:81171ms step_avg:61.54ms
step:1320/2330 train_time:81233ms step_avg:61.54ms
step:1321/2330 train_time:81294ms step_avg:61.54ms
step:1322/2330 train_time:81357ms step_avg:61.54ms
step:1323/2330 train_time:81417ms step_avg:61.54ms
step:1324/2330 train_time:81480ms step_avg:61.54ms
step:1325/2330 train_time:81541ms step_avg:61.54ms
step:1326/2330 train_time:81604ms step_avg:61.54ms
step:1327/2330 train_time:81664ms step_avg:61.54ms
step:1328/2330 train_time:81726ms step_avg:61.54ms
step:1329/2330 train_time:81788ms step_avg:61.54ms
step:1330/2330 train_time:81850ms step_avg:61.54ms
step:1331/2330 train_time:81911ms step_avg:61.54ms
step:1332/2330 train_time:81974ms step_avg:61.54ms
step:1333/2330 train_time:82035ms step_avg:61.54ms
step:1334/2330 train_time:82097ms step_avg:61.54ms
step:1335/2330 train_time:82158ms step_avg:61.54ms
step:1336/2330 train_time:82222ms step_avg:61.54ms
step:1337/2330 train_time:82284ms step_avg:61.54ms
step:1338/2330 train_time:82347ms step_avg:61.54ms
step:1339/2330 train_time:82408ms step_avg:61.54ms
step:1340/2330 train_time:82471ms step_avg:61.55ms
step:1341/2330 train_time:82533ms step_avg:61.55ms
step:1342/2330 train_time:82595ms step_avg:61.55ms
step:1343/2330 train_time:82656ms step_avg:61.55ms
step:1344/2330 train_time:82718ms step_avg:61.55ms
step:1345/2330 train_time:82779ms step_avg:61.55ms
step:1346/2330 train_time:82842ms step_avg:61.55ms
step:1347/2330 train_time:82902ms step_avg:61.55ms
step:1348/2330 train_time:82965ms step_avg:61.55ms
step:1349/2330 train_time:83026ms step_avg:61.55ms
step:1350/2330 train_time:83089ms step_avg:61.55ms
step:1351/2330 train_time:83150ms step_avg:61.55ms
step:1352/2330 train_time:83214ms step_avg:61.55ms
step:1353/2330 train_time:83274ms step_avg:61.55ms
step:1354/2330 train_time:83337ms step_avg:61.55ms
step:1355/2330 train_time:83398ms step_avg:61.55ms
step:1356/2330 train_time:83460ms step_avg:61.55ms
step:1357/2330 train_time:83521ms step_avg:61.55ms
step:1358/2330 train_time:83584ms step_avg:61.55ms
step:1359/2330 train_time:83645ms step_avg:61.55ms
step:1360/2330 train_time:83708ms step_avg:61.55ms
step:1361/2330 train_time:83770ms step_avg:61.55ms
step:1362/2330 train_time:83833ms step_avg:61.55ms
step:1363/2330 train_time:83894ms step_avg:61.55ms
step:1364/2330 train_time:83956ms step_avg:61.55ms
step:1365/2330 train_time:84016ms step_avg:61.55ms
step:1366/2330 train_time:84079ms step_avg:61.55ms
step:1367/2330 train_time:84140ms step_avg:61.55ms
step:1368/2330 train_time:84203ms step_avg:61.55ms
step:1369/2330 train_time:84264ms step_avg:61.55ms
step:1370/2330 train_time:84326ms step_avg:61.55ms
step:1371/2330 train_time:84387ms step_avg:61.55ms
step:1372/2330 train_time:84450ms step_avg:61.55ms
step:1373/2330 train_time:84511ms step_avg:61.55ms
step:1374/2330 train_time:84573ms step_avg:61.55ms
step:1375/2330 train_time:84633ms step_avg:61.55ms
step:1376/2330 train_time:84696ms step_avg:61.55ms
step:1377/2330 train_time:84757ms step_avg:61.55ms
step:1378/2330 train_time:84820ms step_avg:61.55ms
step:1379/2330 train_time:84880ms step_avg:61.55ms
step:1380/2330 train_time:84943ms step_avg:61.55ms
step:1381/2330 train_time:85004ms step_avg:61.55ms
step:1382/2330 train_time:85066ms step_avg:61.55ms
step:1383/2330 train_time:85127ms step_avg:61.55ms
step:1384/2330 train_time:85190ms step_avg:61.55ms
step:1385/2330 train_time:85251ms step_avg:61.55ms
step:1386/2330 train_time:85314ms step_avg:61.55ms
step:1387/2330 train_time:85375ms step_avg:61.55ms
step:1388/2330 train_time:85438ms step_avg:61.55ms
step:1389/2330 train_time:85499ms step_avg:61.55ms
step:1390/2330 train_time:85561ms step_avg:61.55ms
step:1391/2330 train_time:85622ms step_avg:61.55ms
step:1392/2330 train_time:85686ms step_avg:61.56ms
step:1393/2330 train_time:85747ms step_avg:61.56ms
step:1394/2330 train_time:85811ms step_avg:61.56ms
step:1395/2330 train_time:85872ms step_avg:61.56ms
step:1396/2330 train_time:85935ms step_avg:61.56ms
step:1397/2330 train_time:85995ms step_avg:61.56ms
step:1398/2330 train_time:86057ms step_avg:61.56ms
step:1399/2330 train_time:86118ms step_avg:61.56ms
step:1400/2330 train_time:86181ms step_avg:61.56ms
step:1401/2330 train_time:86242ms step_avg:61.56ms
step:1402/2330 train_time:86306ms step_avg:61.56ms
step:1403/2330 train_time:86367ms step_avg:61.56ms
step:1404/2330 train_time:86429ms step_avg:61.56ms
step:1405/2330 train_time:86490ms step_avg:61.56ms
step:1406/2330 train_time:86553ms step_avg:61.56ms
step:1407/2330 train_time:86614ms step_avg:61.56ms
step:1408/2330 train_time:86677ms step_avg:61.56ms
step:1409/2330 train_time:86737ms step_avg:61.56ms
step:1410/2330 train_time:86801ms step_avg:61.56ms
step:1411/2330 train_time:86861ms step_avg:61.56ms
step:1412/2330 train_time:86924ms step_avg:61.56ms
step:1413/2330 train_time:86984ms step_avg:61.56ms
step:1414/2330 train_time:87046ms step_avg:61.56ms
step:1415/2330 train_time:87107ms step_avg:61.56ms
step:1416/2330 train_time:87172ms step_avg:61.56ms
step:1417/2330 train_time:87233ms step_avg:61.56ms
step:1418/2330 train_time:87296ms step_avg:61.56ms
step:1419/2330 train_time:87356ms step_avg:61.56ms
step:1420/2330 train_time:87419ms step_avg:61.56ms
step:1421/2330 train_time:87480ms step_avg:61.56ms
step:1422/2330 train_time:87543ms step_avg:61.56ms
step:1423/2330 train_time:87604ms step_avg:61.56ms
step:1424/2330 train_time:87667ms step_avg:61.56ms
step:1425/2330 train_time:87729ms step_avg:61.56ms
step:1426/2330 train_time:87792ms step_avg:61.57ms
step:1427/2330 train_time:87854ms step_avg:61.57ms
step:1428/2330 train_time:87916ms step_avg:61.57ms
step:1429/2330 train_time:87977ms step_avg:61.57ms
step:1430/2330 train_time:88040ms step_avg:61.57ms
step:1431/2330 train_time:88101ms step_avg:61.57ms
step:1432/2330 train_time:88164ms step_avg:61.57ms
step:1433/2330 train_time:88225ms step_avg:61.57ms
step:1434/2330 train_time:88289ms step_avg:61.57ms
step:1435/2330 train_time:88350ms step_avg:61.57ms
step:1436/2330 train_time:88413ms step_avg:61.57ms
step:1437/2330 train_time:88474ms step_avg:61.57ms
step:1438/2330 train_time:88536ms step_avg:61.57ms
step:1439/2330 train_time:88597ms step_avg:61.57ms
step:1440/2330 train_time:88660ms step_avg:61.57ms
step:1441/2330 train_time:88721ms step_avg:61.57ms
step:1442/2330 train_time:88784ms step_avg:61.57ms
step:1443/2330 train_time:88846ms step_avg:61.57ms
step:1444/2330 train_time:88908ms step_avg:61.57ms
step:1445/2330 train_time:88970ms step_avg:61.57ms
step:1446/2330 train_time:89032ms step_avg:61.57ms
step:1447/2330 train_time:89093ms step_avg:61.57ms
step:1448/2330 train_time:89156ms step_avg:61.57ms
step:1449/2330 train_time:89217ms step_avg:61.57ms
step:1450/2330 train_time:89279ms step_avg:61.57ms
step:1451/2330 train_time:89340ms step_avg:61.57ms
step:1452/2330 train_time:89404ms step_avg:61.57ms
step:1453/2330 train_time:89465ms step_avg:61.57ms
step:1454/2330 train_time:89527ms step_avg:61.57ms
step:1455/2330 train_time:89588ms step_avg:61.57ms
step:1456/2330 train_time:89651ms step_avg:61.57ms
step:1457/2330 train_time:89713ms step_avg:61.57ms
step:1458/2330 train_time:89776ms step_avg:61.57ms
step:1459/2330 train_time:89836ms step_avg:61.57ms
step:1460/2330 train_time:89898ms step_avg:61.57ms
step:1461/2330 train_time:89959ms step_avg:61.57ms
step:1462/2330 train_time:90022ms step_avg:61.57ms
step:1463/2330 train_time:90083ms step_avg:61.57ms
step:1464/2330 train_time:90145ms step_avg:61.57ms
step:1465/2330 train_time:90206ms step_avg:61.57ms
step:1466/2330 train_time:90269ms step_avg:61.58ms
step:1467/2330 train_time:90331ms step_avg:61.58ms
step:1468/2330 train_time:90393ms step_avg:61.58ms
step:1469/2330 train_time:90454ms step_avg:61.57ms
step:1470/2330 train_time:90516ms step_avg:61.58ms
step:1471/2330 train_time:90576ms step_avg:61.57ms
step:1472/2330 train_time:90639ms step_avg:61.58ms
step:1473/2330 train_time:90700ms step_avg:61.58ms
step:1474/2330 train_time:90763ms step_avg:61.58ms
step:1475/2330 train_time:90825ms step_avg:61.58ms
step:1476/2330 train_time:90887ms step_avg:61.58ms
step:1477/2330 train_time:90949ms step_avg:61.58ms
step:1478/2330 train_time:91012ms step_avg:61.58ms
step:1479/2330 train_time:91073ms step_avg:61.58ms
step:1480/2330 train_time:91135ms step_avg:61.58ms
step:1481/2330 train_time:91196ms step_avg:61.58ms
step:1482/2330 train_time:91259ms step_avg:61.58ms
step:1483/2330 train_time:91319ms step_avg:61.58ms
step:1484/2330 train_time:91382ms step_avg:61.58ms
step:1485/2330 train_time:91443ms step_avg:61.58ms
step:1486/2330 train_time:91506ms step_avg:61.58ms
step:1487/2330 train_time:91567ms step_avg:61.58ms
step:1488/2330 train_time:91630ms step_avg:61.58ms
step:1489/2330 train_time:91690ms step_avg:61.58ms
step:1490/2330 train_time:91753ms step_avg:61.58ms
step:1491/2330 train_time:91814ms step_avg:61.58ms
step:1492/2330 train_time:91876ms step_avg:61.58ms
step:1493/2330 train_time:91937ms step_avg:61.58ms
step:1494/2330 train_time:92000ms step_avg:61.58ms
step:1495/2330 train_time:92061ms step_avg:61.58ms
step:1496/2330 train_time:92124ms step_avg:61.58ms
step:1497/2330 train_time:92185ms step_avg:61.58ms
step:1498/2330 train_time:92247ms step_avg:61.58ms
step:1499/2330 train_time:92308ms step_avg:61.58ms
step:1500/2330 train_time:92371ms step_avg:61.58ms
step:1500/2330 val_loss:3.7831 train_time:92436ms step_avg:61.62ms
step:1501/2330 train_time:92459ms step_avg:61.60ms
step:1502/2330 train_time:92498ms step_avg:61.58ms
step:1503/2330 train_time:92563ms step_avg:61.59ms
step:1504/2330 train_time:92628ms step_avg:61.59ms
step:1505/2330 train_time:92689ms step_avg:61.59ms
step:1506/2330 train_time:92752ms step_avg:61.59ms
step:1507/2330 train_time:92812ms step_avg:61.59ms
step:1508/2330 train_time:92874ms step_avg:61.59ms
step:1509/2330 train_time:92934ms step_avg:61.59ms
step:1510/2330 train_time:92997ms step_avg:61.59ms
step:1511/2330 train_time:93057ms step_avg:61.59ms
step:1512/2330 train_time:93119ms step_avg:61.59ms
step:1513/2330 train_time:93179ms step_avg:61.59ms
step:1514/2330 train_time:93241ms step_avg:61.59ms
step:1515/2330 train_time:93302ms step_avg:61.59ms
step:1516/2330 train_time:93364ms step_avg:61.59ms
step:1517/2330 train_time:93425ms step_avg:61.59ms
step:1518/2330 train_time:93489ms step_avg:61.59ms
step:1519/2330 train_time:93552ms step_avg:61.59ms
step:1520/2330 train_time:93617ms step_avg:61.59ms
step:1521/2330 train_time:93678ms step_avg:61.59ms
step:1522/2330 train_time:93742ms step_avg:61.59ms
step:1523/2330 train_time:93802ms step_avg:61.59ms
step:1524/2330 train_time:93865ms step_avg:61.59ms
step:1525/2330 train_time:93926ms step_avg:61.59ms
step:1526/2330 train_time:93988ms step_avg:61.59ms
step:1527/2330 train_time:94049ms step_avg:61.59ms
step:1528/2330 train_time:94112ms step_avg:61.59ms
step:1529/2330 train_time:94172ms step_avg:61.59ms
step:1530/2330 train_time:94236ms step_avg:61.59ms
step:1531/2330 train_time:94297ms step_avg:61.59ms
step:1532/2330 train_time:94360ms step_avg:61.59ms
step:1533/2330 train_time:94422ms step_avg:61.59ms
step:1534/2330 train_time:94484ms step_avg:61.59ms
step:1535/2330 train_time:94546ms step_avg:61.59ms
step:1536/2330 train_time:94610ms step_avg:61.60ms
step:1537/2330 train_time:94672ms step_avg:61.60ms
step:1538/2330 train_time:94736ms step_avg:61.60ms
step:1539/2330 train_time:94799ms step_avg:61.60ms
step:1540/2330 train_time:94862ms step_avg:61.60ms
step:1541/2330 train_time:94923ms step_avg:61.60ms
step:1542/2330 train_time:94986ms step_avg:61.60ms
step:1543/2330 train_time:95046ms step_avg:61.60ms
step:1544/2330 train_time:95110ms step_avg:61.60ms
step:1545/2330 train_time:95170ms step_avg:61.60ms
step:1546/2330 train_time:95234ms step_avg:61.60ms
step:1547/2330 train_time:95295ms step_avg:61.60ms
step:1548/2330 train_time:95358ms step_avg:61.60ms
step:1549/2330 train_time:95420ms step_avg:61.60ms
step:1550/2330 train_time:95484ms step_avg:61.60ms
step:1551/2330 train_time:95545ms step_avg:61.60ms
step:1552/2330 train_time:95609ms step_avg:61.60ms
step:1553/2330 train_time:95671ms step_avg:61.60ms
step:1554/2330 train_time:95735ms step_avg:61.61ms
step:1555/2330 train_time:95797ms step_avg:61.61ms
step:1556/2330 train_time:95861ms step_avg:61.61ms
step:1557/2330 train_time:95922ms step_avg:61.61ms
step:1558/2330 train_time:95985ms step_avg:61.61ms
step:1559/2330 train_time:96046ms step_avg:61.61ms
step:1560/2330 train_time:96109ms step_avg:61.61ms
step:1561/2330 train_time:96170ms step_avg:61.61ms
step:1562/2330 train_time:96233ms step_avg:61.61ms
step:1563/2330 train_time:96293ms step_avg:61.61ms
step:1564/2330 train_time:96357ms step_avg:61.61ms
step:1565/2330 train_time:96419ms step_avg:61.61ms
step:1566/2330 train_time:96482ms step_avg:61.61ms
step:1567/2330 train_time:96543ms step_avg:61.61ms
step:1568/2330 train_time:96607ms step_avg:61.61ms
step:1569/2330 train_time:96669ms step_avg:61.61ms
step:1570/2330 train_time:96732ms step_avg:61.61ms
step:1571/2330 train_time:96794ms step_avg:61.61ms
step:1572/2330 train_time:96858ms step_avg:61.61ms
step:1573/2330 train_time:96920ms step_avg:61.61ms
step:1574/2330 train_time:96984ms step_avg:61.62ms
step:1575/2330 train_time:97045ms step_avg:61.62ms
step:1576/2330 train_time:97108ms step_avg:61.62ms
step:1577/2330 train_time:97169ms step_avg:61.62ms
step:1578/2330 train_time:97233ms step_avg:61.62ms
step:1579/2330 train_time:97294ms step_avg:61.62ms
step:1580/2330 train_time:97357ms step_avg:61.62ms
step:1581/2330 train_time:97419ms step_avg:61.62ms
step:1582/2330 train_time:97482ms step_avg:61.62ms
step:1583/2330 train_time:97544ms step_avg:61.62ms
step:1584/2330 train_time:97608ms step_avg:61.62ms
step:1585/2330 train_time:97669ms step_avg:61.62ms
step:1586/2330 train_time:97733ms step_avg:61.62ms
step:1587/2330 train_time:97795ms step_avg:61.62ms
step:1588/2330 train_time:97859ms step_avg:61.62ms
step:1589/2330 train_time:97921ms step_avg:61.62ms
step:1590/2330 train_time:97985ms step_avg:61.63ms
step:1591/2330 train_time:98045ms step_avg:61.62ms
step:1592/2330 train_time:98108ms step_avg:61.63ms
step:1593/2330 train_time:98170ms step_avg:61.63ms
step:1594/2330 train_time:98233ms step_avg:61.63ms
step:1595/2330 train_time:98294ms step_avg:61.63ms
step:1596/2330 train_time:98358ms step_avg:61.63ms
step:1597/2330 train_time:98419ms step_avg:61.63ms
step:1598/2330 train_time:98483ms step_avg:61.63ms
step:1599/2330 train_time:98544ms step_avg:61.63ms
step:1600/2330 train_time:98608ms step_avg:61.63ms
step:1601/2330 train_time:98670ms step_avg:61.63ms
step:1602/2330 train_time:98734ms step_avg:61.63ms
step:1603/2330 train_time:98795ms step_avg:61.63ms
step:1604/2330 train_time:98859ms step_avg:61.63ms
step:1605/2330 train_time:98920ms step_avg:61.63ms
step:1606/2330 train_time:98984ms step_avg:61.63ms
step:1607/2330 train_time:99045ms step_avg:61.63ms
step:1608/2330 train_time:99108ms step_avg:61.63ms
step:1609/2330 train_time:99168ms step_avg:61.63ms
step:1610/2330 train_time:99232ms step_avg:61.63ms
step:1611/2330 train_time:99294ms step_avg:61.63ms
step:1612/2330 train_time:99357ms step_avg:61.64ms
step:1613/2330 train_time:99419ms step_avg:61.64ms
step:1614/2330 train_time:99482ms step_avg:61.64ms
step:1615/2330 train_time:99543ms step_avg:61.64ms
step:1616/2330 train_time:99606ms step_avg:61.64ms
step:1617/2330 train_time:99667ms step_avg:61.64ms
step:1618/2330 train_time:99731ms step_avg:61.64ms
step:1619/2330 train_time:99794ms step_avg:61.64ms
step:1620/2330 train_time:99858ms step_avg:61.64ms
step:1621/2330 train_time:99920ms step_avg:61.64ms
step:1622/2330 train_time:99983ms step_avg:61.64ms
step:1623/2330 train_time:100044ms step_avg:61.64ms
step:1624/2330 train_time:100107ms step_avg:61.64ms
step:1625/2330 train_time:100169ms step_avg:61.64ms
step:1626/2330 train_time:100232ms step_avg:61.64ms
step:1627/2330 train_time:100294ms step_avg:61.64ms
step:1628/2330 train_time:100357ms step_avg:61.64ms
step:1629/2330 train_time:100418ms step_avg:61.64ms
step:1630/2330 train_time:100482ms step_avg:61.65ms
step:1631/2330 train_time:100543ms step_avg:61.64ms
step:1632/2330 train_time:100606ms step_avg:61.65ms
step:1633/2330 train_time:100668ms step_avg:61.65ms
step:1634/2330 train_time:100732ms step_avg:61.65ms
step:1635/2330 train_time:100795ms step_avg:61.65ms
step:1636/2330 train_time:100858ms step_avg:61.65ms
step:1637/2330 train_time:100919ms step_avg:61.65ms
step:1638/2330 train_time:100983ms step_avg:61.65ms
step:1639/2330 train_time:101044ms step_avg:61.65ms
step:1640/2330 train_time:101107ms step_avg:61.65ms
step:1641/2330 train_time:101168ms step_avg:61.65ms
step:1642/2330 train_time:101232ms step_avg:61.65ms
step:1643/2330 train_time:101294ms step_avg:61.65ms
step:1644/2330 train_time:101357ms step_avg:61.65ms
step:1645/2330 train_time:101418ms step_avg:61.65ms
step:1646/2330 train_time:101483ms step_avg:61.65ms
step:1647/2330 train_time:101544ms step_avg:61.65ms
step:1648/2330 train_time:101607ms step_avg:61.65ms
step:1649/2330 train_time:101669ms step_avg:61.65ms
step:1650/2330 train_time:101732ms step_avg:61.66ms
step:1651/2330 train_time:101794ms step_avg:61.66ms
step:1652/2330 train_time:101857ms step_avg:61.66ms
step:1653/2330 train_time:101919ms step_avg:61.66ms
step:1654/2330 train_time:101983ms step_avg:61.66ms
step:1655/2330 train_time:102044ms step_avg:61.66ms
step:1656/2330 train_time:102107ms step_avg:61.66ms
step:1657/2330 train_time:102168ms step_avg:61.66ms
step:1658/2330 train_time:102232ms step_avg:61.66ms
step:1659/2330 train_time:102293ms step_avg:61.66ms
step:1660/2330 train_time:102356ms step_avg:61.66ms
step:1661/2330 train_time:102418ms step_avg:61.66ms
step:1662/2330 train_time:102482ms step_avg:61.66ms
step:1663/2330 train_time:102543ms step_avg:61.66ms
step:1664/2330 train_time:102606ms step_avg:61.66ms
step:1665/2330 train_time:102667ms step_avg:61.66ms
step:1666/2330 train_time:102730ms step_avg:61.66ms
step:1667/2330 train_time:102793ms step_avg:61.66ms
step:1668/2330 train_time:102857ms step_avg:61.67ms
step:1669/2330 train_time:102919ms step_avg:61.66ms
step:1670/2330 train_time:102982ms step_avg:61.67ms
step:1671/2330 train_time:103043ms step_avg:61.67ms
step:1672/2330 train_time:103106ms step_avg:61.67ms
step:1673/2330 train_time:103167ms step_avg:61.67ms
step:1674/2330 train_time:103231ms step_avg:61.67ms
step:1675/2330 train_time:103293ms step_avg:61.67ms
step:1676/2330 train_time:103356ms step_avg:61.67ms
step:1677/2330 train_time:103418ms step_avg:61.67ms
step:1678/2330 train_time:103481ms step_avg:61.67ms
step:1679/2330 train_time:103542ms step_avg:61.67ms
step:1680/2330 train_time:103605ms step_avg:61.67ms
step:1681/2330 train_time:103666ms step_avg:61.67ms
step:1682/2330 train_time:103730ms step_avg:61.67ms
step:1683/2330 train_time:103792ms step_avg:61.67ms
step:1684/2330 train_time:103856ms step_avg:61.67ms
step:1685/2330 train_time:103918ms step_avg:61.67ms
step:1686/2330 train_time:103981ms step_avg:61.67ms
step:1687/2330 train_time:104042ms step_avg:61.67ms
step:1688/2330 train_time:104105ms step_avg:61.67ms
step:1689/2330 train_time:104166ms step_avg:61.67ms
step:1690/2330 train_time:104230ms step_avg:61.67ms
step:1691/2330 train_time:104291ms step_avg:61.67ms
step:1692/2330 train_time:104355ms step_avg:61.68ms
step:1693/2330 train_time:104416ms step_avg:61.68ms
step:1694/2330 train_time:104480ms step_avg:61.68ms
step:1695/2330 train_time:104542ms step_avg:61.68ms
step:1696/2330 train_time:104605ms step_avg:61.68ms
step:1697/2330 train_time:104666ms step_avg:61.68ms
step:1698/2330 train_time:104730ms step_avg:61.68ms
step:1699/2330 train_time:104791ms step_avg:61.68ms
step:1700/2330 train_time:104855ms step_avg:61.68ms
step:1701/2330 train_time:104917ms step_avg:61.68ms
step:1702/2330 train_time:104981ms step_avg:61.68ms
step:1703/2330 train_time:105042ms step_avg:61.68ms
step:1704/2330 train_time:105106ms step_avg:61.68ms
step:1705/2330 train_time:105167ms step_avg:61.68ms
step:1706/2330 train_time:105230ms step_avg:61.68ms
step:1707/2330 train_time:105292ms step_avg:61.68ms
step:1708/2330 train_time:105355ms step_avg:61.68ms
step:1709/2330 train_time:105416ms step_avg:61.68ms
step:1710/2330 train_time:105480ms step_avg:61.68ms
step:1711/2330 train_time:105542ms step_avg:61.68ms
step:1712/2330 train_time:105605ms step_avg:61.69ms
step:1713/2330 train_time:105666ms step_avg:61.68ms
step:1714/2330 train_time:105730ms step_avg:61.69ms
step:1715/2330 train_time:105790ms step_avg:61.69ms
step:1716/2330 train_time:105854ms step_avg:61.69ms
step:1717/2330 train_time:105916ms step_avg:61.69ms
step:1718/2330 train_time:105980ms step_avg:61.69ms
step:1719/2330 train_time:106042ms step_avg:61.69ms
step:1720/2330 train_time:106105ms step_avg:61.69ms
step:1721/2330 train_time:106166ms step_avg:61.69ms
step:1722/2330 train_time:106229ms step_avg:61.69ms
step:1723/2330 train_time:106292ms step_avg:61.69ms
step:1724/2330 train_time:106355ms step_avg:61.69ms
step:1725/2330 train_time:106417ms step_avg:61.69ms
step:1726/2330 train_time:106481ms step_avg:61.69ms
step:1727/2330 train_time:106543ms step_avg:61.69ms
step:1728/2330 train_time:106606ms step_avg:61.69ms
step:1729/2330 train_time:106667ms step_avg:61.69ms
step:1730/2330 train_time:106730ms step_avg:61.69ms
step:1731/2330 train_time:106792ms step_avg:61.69ms
step:1732/2330 train_time:106855ms step_avg:61.69ms
step:1733/2330 train_time:106917ms step_avg:61.69ms
step:1734/2330 train_time:106981ms step_avg:61.70ms
step:1735/2330 train_time:107042ms step_avg:61.70ms
step:1736/2330 train_time:107105ms step_avg:61.70ms
step:1737/2330 train_time:107166ms step_avg:61.70ms
step:1738/2330 train_time:107229ms step_avg:61.70ms
step:1739/2330 train_time:107291ms step_avg:61.70ms
step:1740/2330 train_time:107354ms step_avg:61.70ms
step:1741/2330 train_time:107416ms step_avg:61.70ms
step:1742/2330 train_time:107481ms step_avg:61.70ms
step:1743/2330 train_time:107542ms step_avg:61.70ms
step:1744/2330 train_time:107605ms step_avg:61.70ms
step:1745/2330 train_time:107666ms step_avg:61.70ms
step:1746/2330 train_time:107729ms step_avg:61.70ms
step:1747/2330 train_time:107790ms step_avg:61.70ms
step:1748/2330 train_time:107854ms step_avg:61.70ms
step:1749/2330 train_time:107915ms step_avg:61.70ms
step:1750/2330 train_time:107979ms step_avg:61.70ms
step:1750/2330 val_loss:3.7005 train_time:108045ms step_avg:61.74ms
step:1751/2330 train_time:108067ms step_avg:61.72ms
step:1752/2330 train_time:108106ms step_avg:61.70ms
step:1753/2330 train_time:108174ms step_avg:61.71ms
step:1754/2330 train_time:108243ms step_avg:61.71ms
step:1755/2330 train_time:108305ms step_avg:61.71ms
step:1756/2330 train_time:108368ms step_avg:61.71ms
step:1757/2330 train_time:108429ms step_avg:61.71ms
step:1758/2330 train_time:108492ms step_avg:61.71ms
step:1759/2330 train_time:108552ms step_avg:61.71ms
step:1760/2330 train_time:108615ms step_avg:61.71ms
step:1761/2330 train_time:108675ms step_avg:61.71ms
step:1762/2330 train_time:108738ms step_avg:61.71ms
step:1763/2330 train_time:108798ms step_avg:61.71ms
step:1764/2330 train_time:108861ms step_avg:61.71ms
step:1765/2330 train_time:108920ms step_avg:61.71ms
step:1766/2330 train_time:108983ms step_avg:61.71ms
step:1767/2330 train_time:109046ms step_avg:61.71ms
step:1768/2330 train_time:109110ms step_avg:61.71ms
step:1769/2330 train_time:109174ms step_avg:61.72ms
step:1770/2330 train_time:109239ms step_avg:61.72ms
step:1771/2330 train_time:109301ms step_avg:61.72ms
step:1772/2330 train_time:109364ms step_avg:61.72ms
step:1773/2330 train_time:109425ms step_avg:61.72ms
step:1774/2330 train_time:109487ms step_avg:61.72ms
step:1775/2330 train_time:109549ms step_avg:61.72ms
step:1776/2330 train_time:109613ms step_avg:61.72ms
step:1777/2330 train_time:109674ms step_avg:61.72ms
step:1778/2330 train_time:109737ms step_avg:61.72ms
step:1779/2330 train_time:109797ms step_avg:61.72ms
step:1780/2330 train_time:109860ms step_avg:61.72ms
step:1781/2330 train_time:109920ms step_avg:61.72ms
step:1782/2330 train_time:109983ms step_avg:61.72ms
step:1783/2330 train_time:110045ms step_avg:61.72ms
step:1784/2330 train_time:110110ms step_avg:61.72ms
step:1785/2330 train_time:110173ms step_avg:61.72ms
step:1786/2330 train_time:110237ms step_avg:61.72ms
step:1787/2330 train_time:110299ms step_avg:61.72ms
step:1788/2330 train_time:110363ms step_avg:61.72ms
step:1789/2330 train_time:110425ms step_avg:61.72ms
step:1790/2330 train_time:110488ms step_avg:61.72ms
step:1791/2330 train_time:110549ms step_avg:61.72ms
step:1792/2330 train_time:110612ms step_avg:61.73ms
step:1793/2330 train_time:110673ms step_avg:61.72ms
step:1794/2330 train_time:110736ms step_avg:61.73ms
step:1795/2330 train_time:110797ms step_avg:61.73ms
step:1796/2330 train_time:110860ms step_avg:61.73ms
step:1797/2330 train_time:110920ms step_avg:61.73ms
step:1798/2330 train_time:110983ms step_avg:61.73ms
step:1799/2330 train_time:111044ms step_avg:61.73ms
step:1800/2330 train_time:111108ms step_avg:61.73ms
step:1801/2330 train_time:111171ms step_avg:61.73ms
step:1802/2330 train_time:111235ms step_avg:61.73ms
step:1803/2330 train_time:111297ms step_avg:61.73ms
step:1804/2330 train_time:111360ms step_avg:61.73ms
step:1805/2330 train_time:111422ms step_avg:61.73ms
step:1806/2330 train_time:111485ms step_avg:61.73ms
step:1807/2330 train_time:111547ms step_avg:61.73ms
step:1808/2330 train_time:111610ms step_avg:61.73ms
step:1809/2330 train_time:111671ms step_avg:61.73ms
step:1810/2330 train_time:111734ms step_avg:61.73ms
step:1811/2330 train_time:111795ms step_avg:61.73ms
step:1812/2330 train_time:111858ms step_avg:61.73ms
step:1813/2330 train_time:111920ms step_avg:61.73ms
step:1814/2330 train_time:111983ms step_avg:61.73ms
step:1815/2330 train_time:112044ms step_avg:61.73ms
step:1816/2330 train_time:112108ms step_avg:61.73ms
step:1817/2330 train_time:112170ms step_avg:61.73ms
step:1818/2330 train_time:112234ms step_avg:61.73ms
step:1819/2330 train_time:112295ms step_avg:61.73ms
step:1820/2330 train_time:112360ms step_avg:61.74ms
step:1821/2330 train_time:112422ms step_avg:61.74ms
step:1822/2330 train_time:112484ms step_avg:61.74ms
step:1823/2330 train_time:112546ms step_avg:61.74ms
step:1824/2330 train_time:112609ms step_avg:61.74ms
step:1825/2330 train_time:112671ms step_avg:61.74ms
step:1826/2330 train_time:112734ms step_avg:61.74ms
step:1827/2330 train_time:112795ms step_avg:61.74ms
step:1828/2330 train_time:112859ms step_avg:61.74ms
step:1829/2330 train_time:112920ms step_avg:61.74ms
step:1830/2330 train_time:112984ms step_avg:61.74ms
step:1831/2330 train_time:113044ms step_avg:61.74ms
step:1832/2330 train_time:113107ms step_avg:61.74ms
step:1833/2330 train_time:113168ms step_avg:61.74ms
step:1834/2330 train_time:113232ms step_avg:61.74ms
step:1835/2330 train_time:113294ms step_avg:61.74ms
step:1836/2330 train_time:113358ms step_avg:61.74ms
step:1837/2330 train_time:113419ms step_avg:61.74ms
step:1838/2330 train_time:113482ms step_avg:61.74ms
step:1839/2330 train_time:113543ms step_avg:61.74ms
step:1840/2330 train_time:113606ms step_avg:61.74ms
step:1841/2330 train_time:113668ms step_avg:61.74ms
step:1842/2330 train_time:113732ms step_avg:61.74ms
step:1843/2330 train_time:113793ms step_avg:61.74ms
step:1844/2330 train_time:113856ms step_avg:61.74ms
step:1845/2330 train_time:113918ms step_avg:61.74ms
step:1846/2330 train_time:113981ms step_avg:61.74ms
step:1847/2330 train_time:114042ms step_avg:61.74ms
step:1848/2330 train_time:114105ms step_avg:61.75ms
step:1849/2330 train_time:114166ms step_avg:61.74ms
step:1850/2330 train_time:114229ms step_avg:61.75ms
step:1851/2330 train_time:114292ms step_avg:61.75ms
step:1852/2330 train_time:114357ms step_avg:61.75ms
step:1853/2330 train_time:114418ms step_avg:61.75ms
step:1854/2330 train_time:114481ms step_avg:61.75ms
step:1855/2330 train_time:114542ms step_avg:61.75ms
step:1856/2330 train_time:114605ms step_avg:61.75ms
step:1857/2330 train_time:114667ms step_avg:61.75ms
step:1858/2330 train_time:114731ms step_avg:61.75ms
step:1859/2330 train_time:114792ms step_avg:61.75ms
step:1860/2330 train_time:114855ms step_avg:61.75ms
step:1861/2330 train_time:114918ms step_avg:61.75ms
step:1862/2330 train_time:114980ms step_avg:61.75ms
step:1863/2330 train_time:115041ms step_avg:61.75ms
step:1864/2330 train_time:115104ms step_avg:61.75ms
step:1865/2330 train_time:115165ms step_avg:61.75ms
step:1866/2330 train_time:115229ms step_avg:61.75ms
step:1867/2330 train_time:115292ms step_avg:61.75ms
step:1868/2330 train_time:115355ms step_avg:61.75ms
step:1869/2330 train_time:115417ms step_avg:61.75ms
step:1870/2330 train_time:115480ms step_avg:61.75ms
step:1871/2330 train_time:115541ms step_avg:61.75ms
step:1872/2330 train_time:115603ms step_avg:61.75ms
step:1873/2330 train_time:115664ms step_avg:61.75ms
step:1874/2330 train_time:115727ms step_avg:61.75ms
step:1875/2330 train_time:115789ms step_avg:61.75ms
step:1876/2330 train_time:115853ms step_avg:61.76ms
step:1877/2330 train_time:115914ms step_avg:61.75ms
step:1878/2330 train_time:115977ms step_avg:61.76ms
step:1879/2330 train_time:116038ms step_avg:61.76ms
step:1880/2330 train_time:116100ms step_avg:61.76ms
step:1881/2330 train_time:116162ms step_avg:61.76ms
step:1882/2330 train_time:116225ms step_avg:61.76ms
step:1883/2330 train_time:116287ms step_avg:61.76ms
step:1884/2330 train_time:116351ms step_avg:61.76ms
step:1885/2330 train_time:116413ms step_avg:61.76ms
step:1886/2330 train_time:116477ms step_avg:61.76ms
step:1887/2330 train_time:116538ms step_avg:61.76ms
step:1888/2330 train_time:116601ms step_avg:61.76ms
step:1889/2330 train_time:116662ms step_avg:61.76ms
step:1890/2330 train_time:116726ms step_avg:61.76ms
step:1891/2330 train_time:116788ms step_avg:61.76ms
step:1892/2330 train_time:116851ms step_avg:61.76ms
step:1893/2330 train_time:116913ms step_avg:61.76ms
step:1894/2330 train_time:116976ms step_avg:61.76ms
step:1895/2330 train_time:117037ms step_avg:61.76ms
step:1896/2330 train_time:117100ms step_avg:61.76ms
step:1897/2330 train_time:117161ms step_avg:61.76ms
step:1898/2330 train_time:117224ms step_avg:61.76ms
step:1899/2330 train_time:117286ms step_avg:61.76ms
step:1900/2330 train_time:117350ms step_avg:61.76ms
step:1901/2330 train_time:117412ms step_avg:61.76ms
step:1902/2330 train_time:117475ms step_avg:61.76ms
step:1903/2330 train_time:117537ms step_avg:61.76ms
step:1904/2330 train_time:117601ms step_avg:61.77ms
step:1905/2330 train_time:117662ms step_avg:61.76ms
step:1906/2330 train_time:117725ms step_avg:61.77ms
step:1907/2330 train_time:117786ms step_avg:61.77ms
step:1908/2330 train_time:117850ms step_avg:61.77ms
step:1909/2330 train_time:117911ms step_avg:61.77ms
step:1910/2330 train_time:117974ms step_avg:61.77ms
step:1911/2330 train_time:118036ms step_avg:61.77ms
step:1912/2330 train_time:118099ms step_avg:61.77ms
step:1913/2330 train_time:118161ms step_avg:61.77ms
step:1914/2330 train_time:118224ms step_avg:61.77ms
step:1915/2330 train_time:118285ms step_avg:61.77ms
step:1916/2330 train_time:118349ms step_avg:61.77ms
step:1917/2330 train_time:118411ms step_avg:61.77ms
step:1918/2330 train_time:118474ms step_avg:61.77ms
step:1919/2330 train_time:118537ms step_avg:61.77ms
step:1920/2330 train_time:118600ms step_avg:61.77ms
step:1921/2330 train_time:118661ms step_avg:61.77ms
step:1922/2330 train_time:118724ms step_avg:61.77ms
step:1923/2330 train_time:118785ms step_avg:61.77ms
step:1924/2330 train_time:118849ms step_avg:61.77ms
step:1925/2330 train_time:118911ms step_avg:61.77ms
step:1926/2330 train_time:118974ms step_avg:61.77ms
step:1927/2330 train_time:119036ms step_avg:61.77ms
step:1928/2330 train_time:119100ms step_avg:61.77ms
step:1929/2330 train_time:119161ms step_avg:61.77ms
step:1930/2330 train_time:119224ms step_avg:61.77ms
step:1931/2330 train_time:119285ms step_avg:61.77ms
step:1932/2330 train_time:119348ms step_avg:61.77ms
step:1933/2330 train_time:119410ms step_avg:61.77ms
step:1934/2330 train_time:119474ms step_avg:61.78ms
step:1935/2330 train_time:119536ms step_avg:61.78ms
step:1936/2330 train_time:119600ms step_avg:61.78ms
step:1937/2330 train_time:119661ms step_avg:61.78ms
step:1938/2330 train_time:119724ms step_avg:61.78ms
step:1939/2330 train_time:119785ms step_avg:61.78ms
step:1940/2330 train_time:119848ms step_avg:61.78ms
step:1941/2330 train_time:119910ms step_avg:61.78ms
step:1942/2330 train_time:119974ms step_avg:61.78ms
step:1943/2330 train_time:120036ms step_avg:61.78ms
step:1944/2330 train_time:120100ms step_avg:61.78ms
step:1945/2330 train_time:120161ms step_avg:61.78ms
step:1946/2330 train_time:120224ms step_avg:61.78ms
step:1947/2330 train_time:120285ms step_avg:61.78ms
step:1948/2330 train_time:120348ms step_avg:61.78ms
step:1949/2330 train_time:120410ms step_avg:61.78ms
step:1950/2330 train_time:120473ms step_avg:61.78ms
step:1951/2330 train_time:120535ms step_avg:61.78ms
step:1952/2330 train_time:120598ms step_avg:61.78ms
step:1953/2330 train_time:120659ms step_avg:61.78ms
step:1954/2330 train_time:120723ms step_avg:61.78ms
step:1955/2330 train_time:120784ms step_avg:61.78ms
step:1956/2330 train_time:120848ms step_avg:61.78ms
step:1957/2330 train_time:120910ms step_avg:61.78ms
step:1958/2330 train_time:120974ms step_avg:61.78ms
step:1959/2330 train_time:121036ms step_avg:61.78ms
step:1960/2330 train_time:121098ms step_avg:61.78ms
step:1961/2330 train_time:121160ms step_avg:61.78ms
step:1962/2330 train_time:121223ms step_avg:61.79ms
step:1963/2330 train_time:121284ms step_avg:61.78ms
step:1964/2330 train_time:121347ms step_avg:61.79ms
step:1965/2330 train_time:121410ms step_avg:61.79ms
step:1966/2330 train_time:121474ms step_avg:61.79ms
step:1967/2330 train_time:121536ms step_avg:61.79ms
step:1968/2330 train_time:121600ms step_avg:61.79ms
step:1969/2330 train_time:121661ms step_avg:61.79ms
step:1970/2330 train_time:121723ms step_avg:61.79ms
step:1971/2330 train_time:121785ms step_avg:61.79ms
step:1972/2330 train_time:121848ms step_avg:61.79ms
step:1973/2330 train_time:121910ms step_avg:61.79ms
step:1974/2330 train_time:121974ms step_avg:61.79ms
step:1975/2330 train_time:122036ms step_avg:61.79ms
step:1976/2330 train_time:122099ms step_avg:61.79ms
step:1977/2330 train_time:122161ms step_avg:61.79ms
step:1978/2330 train_time:122224ms step_avg:61.79ms
step:1979/2330 train_time:122285ms step_avg:61.79ms
step:1980/2330 train_time:122349ms step_avg:61.79ms
step:1981/2330 train_time:122412ms step_avg:61.79ms
step:1982/2330 train_time:122475ms step_avg:61.79ms
step:1983/2330 train_time:122536ms step_avg:61.79ms
step:1984/2330 train_time:122599ms step_avg:61.79ms
step:1985/2330 train_time:122660ms step_avg:61.79ms
step:1986/2330 train_time:122723ms step_avg:61.79ms
step:1987/2330 train_time:122785ms step_avg:61.79ms
step:1988/2330 train_time:122848ms step_avg:61.79ms
step:1989/2330 train_time:122910ms step_avg:61.79ms
step:1990/2330 train_time:122974ms step_avg:61.80ms
step:1991/2330 train_time:123036ms step_avg:61.80ms
step:1992/2330 train_time:123100ms step_avg:61.80ms
step:1993/2330 train_time:123161ms step_avg:61.80ms
step:1994/2330 train_time:123224ms step_avg:61.80ms
step:1995/2330 train_time:123285ms step_avg:61.80ms
step:1996/2330 train_time:123349ms step_avg:61.80ms
step:1997/2330 train_time:123410ms step_avg:61.80ms
step:1998/2330 train_time:123473ms step_avg:61.80ms
step:1999/2330 train_time:123535ms step_avg:61.80ms
step:2000/2330 train_time:123599ms step_avg:61.80ms
step:2000/2330 val_loss:3.6435 train_time:123665ms step_avg:61.83ms
step:2001/2330 train_time:123687ms step_avg:61.81ms
step:2002/2330 train_time:123727ms step_avg:61.80ms
step:2003/2330 train_time:123794ms step_avg:61.80ms
step:2004/2330 train_time:123859ms step_avg:61.81ms
step:2005/2330 train_time:123921ms step_avg:61.81ms
step:2006/2330 train_time:123984ms step_avg:61.81ms
step:2007/2330 train_time:124045ms step_avg:61.81ms
step:2008/2330 train_time:124108ms step_avg:61.81ms
step:2009/2330 train_time:124169ms step_avg:61.81ms
step:2010/2330 train_time:124231ms step_avg:61.81ms
step:2011/2330 train_time:124291ms step_avg:61.81ms
step:2012/2330 train_time:124354ms step_avg:61.81ms
step:2013/2330 train_time:124414ms step_avg:61.81ms
step:2014/2330 train_time:124477ms step_avg:61.81ms
step:2015/2330 train_time:124537ms step_avg:61.80ms
step:2016/2330 train_time:124601ms step_avg:61.81ms
step:2017/2330 train_time:124664ms step_avg:61.81ms
step:2018/2330 train_time:124731ms step_avg:61.81ms
step:2019/2330 train_time:124793ms step_avg:61.81ms
step:2020/2330 train_time:124857ms step_avg:61.81ms
step:2021/2330 train_time:124919ms step_avg:61.81ms
step:2022/2330 train_time:124983ms step_avg:61.81ms
step:2023/2330 train_time:125044ms step_avg:61.81ms
step:2024/2330 train_time:125108ms step_avg:61.81ms
step:2025/2330 train_time:125168ms step_avg:61.81ms
step:2026/2330 train_time:125231ms step_avg:61.81ms
step:2027/2330 train_time:125291ms step_avg:61.81ms
step:2028/2330 train_time:125354ms step_avg:61.81ms
step:2029/2330 train_time:125414ms step_avg:61.81ms
step:2030/2330 train_time:125477ms step_avg:61.81ms
step:2031/2330 train_time:125538ms step_avg:61.81ms
step:2032/2330 train_time:125602ms step_avg:61.81ms
step:2033/2330 train_time:125664ms step_avg:61.81ms
step:2034/2330 train_time:125731ms step_avg:61.81ms
step:2035/2330 train_time:125792ms step_avg:61.81ms
step:2036/2330 train_time:125856ms step_avg:61.82ms
step:2037/2330 train_time:125917ms step_avg:61.81ms
step:2038/2330 train_time:125982ms step_avg:61.82ms
step:2039/2330 train_time:126043ms step_avg:61.82ms
step:2040/2330 train_time:126106ms step_avg:61.82ms
step:2041/2330 train_time:126168ms step_avg:61.82ms
step:2042/2330 train_time:126231ms step_avg:61.82ms
step:2043/2330 train_time:126291ms step_avg:61.82ms
step:2044/2330 train_time:126354ms step_avg:61.82ms
step:2045/2330 train_time:126415ms step_avg:61.82ms
step:2046/2330 train_time:126477ms step_avg:61.82ms
step:2047/2330 train_time:126538ms step_avg:61.82ms
step:2048/2330 train_time:126601ms step_avg:61.82ms
step:2049/2330 train_time:126663ms step_avg:61.82ms
step:2050/2330 train_time:126727ms step_avg:61.82ms
step:2051/2330 train_time:126789ms step_avg:61.82ms
step:2052/2330 train_time:126852ms step_avg:61.82ms
step:2053/2330 train_time:126914ms step_avg:61.82ms
step:2054/2330 train_time:126978ms step_avg:61.82ms
step:2055/2330 train_time:127040ms step_avg:61.82ms
step:2056/2330 train_time:127103ms step_avg:61.82ms
step:2057/2330 train_time:127165ms step_avg:61.82ms
step:2058/2330 train_time:127229ms step_avg:61.82ms
step:2059/2330 train_time:127290ms step_avg:61.82ms
step:2060/2330 train_time:127353ms step_avg:61.82ms
step:2061/2330 train_time:127413ms step_avg:61.82ms
step:2062/2330 train_time:127476ms step_avg:61.82ms
step:2063/2330 train_time:127537ms step_avg:61.82ms
step:2064/2330 train_time:127600ms step_avg:61.82ms
step:2065/2330 train_time:127662ms step_avg:61.82ms
step:2066/2330 train_time:127726ms step_avg:61.82ms
step:2067/2330 train_time:127788ms step_avg:61.82ms
step:2068/2330 train_time:127852ms step_avg:61.82ms
step:2069/2330 train_time:127913ms step_avg:61.82ms
step:2070/2330 train_time:127977ms step_avg:61.82ms
step:2071/2330 train_time:128038ms step_avg:61.82ms
step:2072/2330 train_time:128102ms step_avg:61.83ms
step:2073/2330 train_time:128165ms step_avg:61.83ms
step:2074/2330 train_time:128229ms step_avg:61.83ms
step:2075/2330 train_time:128290ms step_avg:61.83ms
step:2076/2330 train_time:128353ms step_avg:61.83ms
step:2077/2330 train_time:128414ms step_avg:61.83ms
step:2078/2330 train_time:128477ms step_avg:61.83ms
step:2079/2330 train_time:128538ms step_avg:61.83ms
step:2080/2330 train_time:128600ms step_avg:61.83ms
step:2081/2330 train_time:128662ms step_avg:61.83ms
step:2082/2330 train_time:128725ms step_avg:61.83ms
step:2083/2330 train_time:128786ms step_avg:61.83ms
step:2084/2330 train_time:128850ms step_avg:61.83ms
step:2085/2330 train_time:128912ms step_avg:61.83ms
step:2086/2330 train_time:128976ms step_avg:61.83ms
step:2087/2330 train_time:129038ms step_avg:61.83ms
step:2088/2330 train_time:129102ms step_avg:61.83ms
step:2089/2330 train_time:129164ms step_avg:61.83ms
step:2090/2330 train_time:129228ms step_avg:61.83ms
step:2091/2330 train_time:129289ms step_avg:61.83ms
step:2092/2330 train_time:129352ms step_avg:61.83ms
step:2093/2330 train_time:129414ms step_avg:61.83ms
step:2094/2330 train_time:129477ms step_avg:61.83ms
step:2095/2330 train_time:129537ms step_avg:61.83ms
step:2096/2330 train_time:129601ms step_avg:61.83ms
step:2097/2330 train_time:129662ms step_avg:61.83ms
step:2098/2330 train_time:129726ms step_avg:61.83ms
step:2099/2330 train_time:129788ms step_avg:61.83ms
step:2100/2330 train_time:129851ms step_avg:61.83ms
step:2101/2330 train_time:129913ms step_avg:61.83ms
step:2102/2330 train_time:129977ms step_avg:61.83ms
step:2103/2330 train_time:130039ms step_avg:61.83ms
step:2104/2330 train_time:130102ms step_avg:61.84ms
step:2105/2330 train_time:130163ms step_avg:61.84ms
step:2106/2330 train_time:130228ms step_avg:61.84ms
step:2107/2330 train_time:130290ms step_avg:61.84ms
step:2108/2330 train_time:130353ms step_avg:61.84ms
step:2109/2330 train_time:130414ms step_avg:61.84ms
step:2110/2330 train_time:130477ms step_avg:61.84ms
step:2111/2330 train_time:130538ms step_avg:61.84ms
step:2112/2330 train_time:130601ms step_avg:61.84ms
step:2113/2330 train_time:130662ms step_avg:61.84ms
step:2114/2330 train_time:130726ms step_avg:61.84ms
step:2115/2330 train_time:130788ms step_avg:61.84ms
step:2116/2330 train_time:130851ms step_avg:61.84ms
step:2117/2330 train_time:130913ms step_avg:61.84ms
step:2118/2330 train_time:130976ms step_avg:61.84ms
step:2119/2330 train_time:131037ms step_avg:61.84ms
step:2120/2330 train_time:131101ms step_avg:61.84ms
step:2121/2330 train_time:131163ms step_avg:61.84ms
step:2122/2330 train_time:131226ms step_avg:61.84ms
step:2123/2330 train_time:131287ms step_avg:61.84ms
step:2124/2330 train_time:131351ms step_avg:61.84ms
step:2125/2330 train_time:131412ms step_avg:61.84ms
step:2126/2330 train_time:131476ms step_avg:61.84ms
step:2127/2330 train_time:131537ms step_avg:61.84ms
step:2128/2330 train_time:131600ms step_avg:61.84ms
step:2129/2330 train_time:131661ms step_avg:61.84ms
step:2130/2330 train_time:131725ms step_avg:61.84ms
step:2131/2330 train_time:131786ms step_avg:61.84ms
step:2132/2330 train_time:131851ms step_avg:61.84ms
step:2133/2330 train_time:131912ms step_avg:61.84ms
step:2134/2330 train_time:131976ms step_avg:61.84ms
step:2135/2330 train_time:132037ms step_avg:61.84ms
step:2136/2330 train_time:132100ms step_avg:61.84ms
step:2137/2330 train_time:132162ms step_avg:61.84ms
step:2138/2330 train_time:132225ms step_avg:61.85ms
step:2139/2330 train_time:132286ms step_avg:61.84ms
step:2140/2330 train_time:132350ms step_avg:61.85ms
step:2141/2330 train_time:132412ms step_avg:61.85ms
step:2142/2330 train_time:132475ms step_avg:61.85ms
step:2143/2330 train_time:132536ms step_avg:61.85ms
step:2144/2330 train_time:132600ms step_avg:61.85ms
step:2145/2330 train_time:132661ms step_avg:61.85ms
step:2146/2330 train_time:132725ms step_avg:61.85ms
step:2147/2330 train_time:132787ms step_avg:61.85ms
step:2148/2330 train_time:132851ms step_avg:61.85ms
step:2149/2330 train_time:132912ms step_avg:61.85ms
step:2150/2330 train_time:132975ms step_avg:61.85ms
step:2151/2330 train_time:133036ms step_avg:61.85ms
step:2152/2330 train_time:133100ms step_avg:61.85ms
step:2153/2330 train_time:133161ms step_avg:61.85ms
step:2154/2330 train_time:133225ms step_avg:61.85ms
step:2155/2330 train_time:133287ms step_avg:61.85ms
step:2156/2330 train_time:133350ms step_avg:61.85ms
step:2157/2330 train_time:133412ms step_avg:61.85ms
step:2158/2330 train_time:133475ms step_avg:61.85ms
step:2159/2330 train_time:133536ms step_avg:61.85ms
step:2160/2330 train_time:133599ms step_avg:61.85ms
step:2161/2330 train_time:133661ms step_avg:61.85ms
step:2162/2330 train_time:133725ms step_avg:61.85ms
step:2163/2330 train_time:133786ms step_avg:61.85ms
step:2164/2330 train_time:133850ms step_avg:61.85ms
step:2165/2330 train_time:133911ms step_avg:61.85ms
step:2166/2330 train_time:133974ms step_avg:61.85ms
step:2167/2330 train_time:134035ms step_avg:61.85ms
step:2168/2330 train_time:134098ms step_avg:61.85ms
step:2169/2330 train_time:134160ms step_avg:61.85ms
step:2170/2330 train_time:134224ms step_avg:61.85ms
step:2171/2330 train_time:134286ms step_avg:61.85ms
step:2172/2330 train_time:134349ms step_avg:61.85ms
step:2173/2330 train_time:134410ms step_avg:61.85ms
step:2174/2330 train_time:134473ms step_avg:61.86ms
step:2175/2330 train_time:134535ms step_avg:61.86ms
step:2176/2330 train_time:134598ms step_avg:61.86ms
step:2177/2330 train_time:134659ms step_avg:61.86ms
step:2178/2330 train_time:134722ms step_avg:61.86ms
step:2179/2330 train_time:134784ms step_avg:61.86ms
step:2180/2330 train_time:134848ms step_avg:61.86ms
step:2181/2330 train_time:134910ms step_avg:61.86ms
step:2182/2330 train_time:134973ms step_avg:61.86ms
step:2183/2330 train_time:135034ms step_avg:61.86ms
step:2184/2330 train_time:135097ms step_avg:61.86ms
step:2185/2330 train_time:135158ms step_avg:61.86ms
step:2186/2330 train_time:135221ms step_avg:61.86ms
step:2187/2330 train_time:135283ms step_avg:61.86ms
step:2188/2330 train_time:135346ms step_avg:61.86ms
step:2189/2330 train_time:135408ms step_avg:61.86ms
step:2190/2330 train_time:135472ms step_avg:61.86ms
step:2191/2330 train_time:135534ms step_avg:61.86ms
step:2192/2330 train_time:135598ms step_avg:61.86ms
step:2193/2330 train_time:135658ms step_avg:61.86ms
step:2194/2330 train_time:135721ms step_avg:61.86ms
step:2195/2330 train_time:135783ms step_avg:61.86ms
step:2196/2330 train_time:135847ms step_avg:61.86ms
step:2197/2330 train_time:135909ms step_avg:61.86ms
step:2198/2330 train_time:135972ms step_avg:61.86ms
step:2199/2330 train_time:136034ms step_avg:61.86ms
step:2200/2330 train_time:136097ms step_avg:61.86ms
step:2201/2330 train_time:136157ms step_avg:61.86ms
step:2202/2330 train_time:136221ms step_avg:61.86ms
step:2203/2330 train_time:136283ms step_avg:61.86ms
step:2204/2330 train_time:136347ms step_avg:61.86ms
step:2205/2330 train_time:136409ms step_avg:61.86ms
step:2206/2330 train_time:136473ms step_avg:61.86ms
step:2207/2330 train_time:136534ms step_avg:61.86ms
step:2208/2330 train_time:136597ms step_avg:61.86ms
step:2209/2330 train_time:136658ms step_avg:61.86ms
step:2210/2330 train_time:136722ms step_avg:61.86ms
step:2211/2330 train_time:136783ms step_avg:61.86ms
step:2212/2330 train_time:136847ms step_avg:61.87ms
step:2213/2330 train_time:136909ms step_avg:61.87ms
step:2214/2330 train_time:136973ms step_avg:61.87ms
step:2215/2330 train_time:137034ms step_avg:61.87ms
step:2216/2330 train_time:137097ms step_avg:61.87ms
step:2217/2330 train_time:137158ms step_avg:61.87ms
step:2218/2330 train_time:137222ms step_avg:61.87ms
step:2219/2330 train_time:137283ms step_avg:61.87ms
step:2220/2330 train_time:137348ms step_avg:61.87ms
step:2221/2330 train_time:137410ms step_avg:61.87ms
step:2222/2330 train_time:137474ms step_avg:61.87ms
step:2223/2330 train_time:137536ms step_avg:61.87ms
step:2224/2330 train_time:137599ms step_avg:61.87ms
step:2225/2330 train_time:137660ms step_avg:61.87ms
step:2226/2330 train_time:137723ms step_avg:61.87ms
step:2227/2330 train_time:137784ms step_avg:61.87ms
step:2228/2330 train_time:137848ms step_avg:61.87ms
step:2229/2330 train_time:137910ms step_avg:61.87ms
step:2230/2330 train_time:137973ms step_avg:61.87ms
step:2231/2330 train_time:138035ms step_avg:61.87ms
step:2232/2330 train_time:138097ms step_avg:61.87ms
step:2233/2330 train_time:138158ms step_avg:61.87ms
step:2234/2330 train_time:138221ms step_avg:61.87ms
step:2235/2330 train_time:138283ms step_avg:61.87ms
step:2236/2330 train_time:138347ms step_avg:61.87ms
step:2237/2330 train_time:138407ms step_avg:61.87ms
step:2238/2330 train_time:138471ms step_avg:61.87ms
step:2239/2330 train_time:138533ms step_avg:61.87ms
step:2240/2330 train_time:138595ms step_avg:61.87ms
step:2241/2330 train_time:138656ms step_avg:61.87ms
step:2242/2330 train_time:138719ms step_avg:61.87ms
step:2243/2330 train_time:138782ms step_avg:61.87ms
step:2244/2330 train_time:138845ms step_avg:61.87ms
step:2245/2330 train_time:138907ms step_avg:61.87ms
step:2246/2330 train_time:138971ms step_avg:61.87ms
step:2247/2330 train_time:139032ms step_avg:61.87ms
step:2248/2330 train_time:139095ms step_avg:61.87ms
step:2249/2330 train_time:139156ms step_avg:61.87ms
step:2250/2330 train_time:139219ms step_avg:61.87ms
step:2250/2330 val_loss:3.6043 train_time:139285ms step_avg:61.90ms
step:2251/2330 train_time:139308ms step_avg:61.89ms
step:2252/2330 train_time:139346ms step_avg:61.88ms
step:2253/2330 train_time:139414ms step_avg:61.88ms
step:2254/2330 train_time:139480ms step_avg:61.88ms
step:2255/2330 train_time:139541ms step_avg:61.88ms
step:2256/2330 train_time:139604ms step_avg:61.88ms
step:2257/2330 train_time:139665ms step_avg:61.88ms
step:2258/2330 train_time:139727ms step_avg:61.88ms
step:2259/2330 train_time:139787ms step_avg:61.88ms
step:2260/2330 train_time:139850ms step_avg:61.88ms
step:2261/2330 train_time:139910ms step_avg:61.88ms
step:2262/2330 train_time:139973ms step_avg:61.88ms
step:2263/2330 train_time:140035ms step_avg:61.88ms
step:2264/2330 train_time:140098ms step_avg:61.88ms
step:2265/2330 train_time:140158ms step_avg:61.88ms
step:2266/2330 train_time:140221ms step_avg:61.88ms
step:2267/2330 train_time:140283ms step_avg:61.88ms
step:2268/2330 train_time:140348ms step_avg:61.88ms
step:2269/2330 train_time:140412ms step_avg:61.88ms
step:2270/2330 train_time:140477ms step_avg:61.88ms
step:2271/2330 train_time:140538ms step_avg:61.88ms
step:2272/2330 train_time:140602ms step_avg:61.88ms
step:2273/2330 train_time:140663ms step_avg:61.88ms
step:2274/2330 train_time:140726ms step_avg:61.88ms
step:2275/2330 train_time:140787ms step_avg:61.88ms
step:2276/2330 train_time:140850ms step_avg:61.88ms
step:2277/2330 train_time:140910ms step_avg:61.88ms
step:2278/2330 train_time:140973ms step_avg:61.88ms
step:2279/2330 train_time:141033ms step_avg:61.88ms
step:2280/2330 train_time:141096ms step_avg:61.88ms
step:2281/2330 train_time:141157ms step_avg:61.88ms
step:2282/2330 train_time:141220ms step_avg:61.88ms
step:2283/2330 train_time:141281ms step_avg:61.88ms
step:2284/2330 train_time:141345ms step_avg:61.89ms
step:2285/2330 train_time:141407ms step_avg:61.89ms
step:2286/2330 train_time:141472ms step_avg:61.89ms
step:2287/2330 train_time:141535ms step_avg:61.89ms
step:2288/2330 train_time:141598ms step_avg:61.89ms
step:2289/2330 train_time:141660ms step_avg:61.89ms
step:2290/2330 train_time:141723ms step_avg:61.89ms
step:2291/2330 train_time:141784ms step_avg:61.89ms
step:2292/2330 train_time:141847ms step_avg:61.89ms
step:2293/2330 train_time:141907ms step_avg:61.89ms
step:2294/2330 train_time:141970ms step_avg:61.89ms
step:2295/2330 train_time:142031ms step_avg:61.89ms
step:2296/2330 train_time:142094ms step_avg:61.89ms
step:2297/2330 train_time:142155ms step_avg:61.89ms
step:2298/2330 train_time:142218ms step_avg:61.89ms
step:2299/2330 train_time:142280ms step_avg:61.89ms
step:2300/2330 train_time:142344ms step_avg:61.89ms
step:2301/2330 train_time:142405ms step_avg:61.89ms
step:2302/2330 train_time:142469ms step_avg:61.89ms
step:2303/2330 train_time:142531ms step_avg:61.89ms
step:2304/2330 train_time:142595ms step_avg:61.89ms
step:2305/2330 train_time:142657ms step_avg:61.89ms
step:2306/2330 train_time:142721ms step_avg:61.89ms
step:2307/2330 train_time:142783ms step_avg:61.89ms
step:2308/2330 train_time:142847ms step_avg:61.89ms
step:2309/2330 train_time:142908ms step_avg:61.89ms
step:2310/2330 train_time:142970ms step_avg:61.89ms
step:2311/2330 train_time:143032ms step_avg:61.89ms
step:2312/2330 train_time:143094ms step_avg:61.89ms
step:2313/2330 train_time:143156ms step_avg:61.89ms
step:2314/2330 train_time:143219ms step_avg:61.89ms
step:2315/2330 train_time:143280ms step_avg:61.89ms
step:2316/2330 train_time:143344ms step_avg:61.89ms
step:2317/2330 train_time:143405ms step_avg:61.89ms
step:2318/2330 train_time:143469ms step_avg:61.89ms
step:2319/2330 train_time:143531ms step_avg:61.89ms
step:2320/2330 train_time:143596ms step_avg:61.89ms
step:2321/2330 train_time:143658ms step_avg:61.89ms
step:2322/2330 train_time:143722ms step_avg:61.90ms
step:2323/2330 train_time:143783ms step_avg:61.90ms
step:2324/2330 train_time:143846ms step_avg:61.90ms
step:2325/2330 train_time:143907ms step_avg:61.90ms
step:2326/2330 train_time:143970ms step_avg:61.90ms
step:2327/2330 train_time:144030ms step_avg:61.90ms
step:2328/2330 train_time:144094ms step_avg:61.90ms
step:2329/2330 train_time:144156ms step_avg:61.90ms
step:2330/2330 train_time:144220ms step_avg:61.90ms
step:2330/2330 val_loss:3.5926 train_time:144286ms step_avg:61.93ms
peak memory allocated: 29512 MiB reserved: 44076 MiB
