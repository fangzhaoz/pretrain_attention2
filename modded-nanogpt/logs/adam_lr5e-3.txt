import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "adam_lr5e-3"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-1, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.Adam(
    hidden_matrix_params + gate_params,
    lr=5e-3,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 00:28:17 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   33C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   31C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   31C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             115W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:102ms step_avg:102.28ms
step:2/2330 train_time:213ms step_avg:106.66ms
step:3/2330 train_time:232ms step_avg:77.22ms
step:4/2330 train_time:254ms step_avg:63.55ms
step:5/2330 train_time:309ms step_avg:61.86ms
step:6/2330 train_time:368ms step_avg:61.38ms
step:7/2330 train_time:423ms step_avg:60.46ms
step:8/2330 train_time:483ms step_avg:60.36ms
step:9/2330 train_time:538ms step_avg:59.79ms
step:10/2330 train_time:597ms step_avg:59.71ms
step:11/2330 train_time:652ms step_avg:59.30ms
step:12/2330 train_time:712ms step_avg:59.30ms
step:13/2330 train_time:767ms step_avg:59.02ms
step:14/2330 train_time:826ms step_avg:59.00ms
step:15/2330 train_time:881ms step_avg:58.74ms
step:16/2330 train_time:941ms step_avg:58.81ms
step:17/2330 train_time:996ms step_avg:58.59ms
step:18/2330 train_time:1056ms step_avg:58.66ms
step:19/2330 train_time:1111ms step_avg:58.48ms
step:20/2330 train_time:1173ms step_avg:58.67ms
step:21/2330 train_time:1230ms step_avg:58.57ms
step:22/2330 train_time:1293ms step_avg:58.78ms
step:23/2330 train_time:1349ms step_avg:58.66ms
step:24/2330 train_time:1411ms step_avg:58.81ms
step:25/2330 train_time:1467ms step_avg:58.67ms
step:26/2330 train_time:1527ms step_avg:58.74ms
step:27/2330 train_time:1582ms step_avg:58.61ms
step:28/2330 train_time:1642ms step_avg:58.65ms
step:29/2330 train_time:1698ms step_avg:58.55ms
step:30/2330 train_time:1758ms step_avg:58.60ms
step:31/2330 train_time:1814ms step_avg:58.50ms
step:32/2330 train_time:1875ms step_avg:58.58ms
step:33/2330 train_time:1930ms step_avg:58.49ms
step:34/2330 train_time:1990ms step_avg:58.54ms
step:35/2330 train_time:2046ms step_avg:58.45ms
step:36/2330 train_time:2106ms step_avg:58.49ms
step:37/2330 train_time:2162ms step_avg:58.43ms
step:38/2330 train_time:2222ms step_avg:58.46ms
step:39/2330 train_time:2277ms step_avg:58.39ms
step:40/2330 train_time:2339ms step_avg:58.48ms
step:41/2330 train_time:2395ms step_avg:58.42ms
step:42/2330 train_time:2458ms step_avg:58.51ms
step:43/2330 train_time:2513ms step_avg:58.45ms
step:44/2330 train_time:2575ms step_avg:58.52ms
step:45/2330 train_time:2630ms step_avg:58.45ms
step:46/2330 train_time:2690ms step_avg:58.49ms
step:47/2330 train_time:2746ms step_avg:58.42ms
step:48/2330 train_time:2806ms step_avg:58.45ms
step:49/2330 train_time:2861ms step_avg:58.39ms
step:50/2330 train_time:2921ms step_avg:58.41ms
step:51/2330 train_time:2976ms step_avg:58.36ms
step:52/2330 train_time:3037ms step_avg:58.40ms
step:53/2330 train_time:3092ms step_avg:58.34ms
step:54/2330 train_time:3154ms step_avg:58.41ms
step:55/2330 train_time:3210ms step_avg:58.37ms
step:56/2330 train_time:3270ms step_avg:58.39ms
step:57/2330 train_time:3326ms step_avg:58.35ms
step:58/2330 train_time:3387ms step_avg:58.39ms
step:59/2330 train_time:3442ms step_avg:58.34ms
step:60/2330 train_time:3503ms step_avg:58.38ms
step:61/2330 train_time:3559ms step_avg:58.34ms
step:62/2330 train_time:3621ms step_avg:58.40ms
step:63/2330 train_time:3676ms step_avg:58.36ms
step:64/2330 train_time:3738ms step_avg:58.41ms
step:65/2330 train_time:3794ms step_avg:58.37ms
step:66/2330 train_time:3855ms step_avg:58.41ms
step:67/2330 train_time:3910ms step_avg:58.36ms
step:68/2330 train_time:3971ms step_avg:58.40ms
step:69/2330 train_time:4027ms step_avg:58.36ms
step:70/2330 train_time:4087ms step_avg:58.39ms
step:71/2330 train_time:4143ms step_avg:58.35ms
step:72/2330 train_time:4203ms step_avg:58.37ms
step:73/2330 train_time:4259ms step_avg:58.34ms
step:74/2330 train_time:4320ms step_avg:58.38ms
step:75/2330 train_time:4376ms step_avg:58.35ms
step:76/2330 train_time:4436ms step_avg:58.37ms
step:77/2330 train_time:4492ms step_avg:58.34ms
step:78/2330 train_time:4554ms step_avg:58.38ms
step:79/2330 train_time:4610ms step_avg:58.35ms
step:80/2330 train_time:4671ms step_avg:58.38ms
step:81/2330 train_time:4727ms step_avg:58.36ms
step:82/2330 train_time:4787ms step_avg:58.38ms
step:83/2330 train_time:4843ms step_avg:58.35ms
step:84/2330 train_time:4903ms step_avg:58.37ms
step:85/2330 train_time:4958ms step_avg:58.33ms
step:86/2330 train_time:5020ms step_avg:58.37ms
step:87/2330 train_time:5075ms step_avg:58.33ms
step:88/2330 train_time:5137ms step_avg:58.38ms
step:89/2330 train_time:5193ms step_avg:58.35ms
step:90/2330 train_time:5255ms step_avg:58.39ms
step:91/2330 train_time:5310ms step_avg:58.36ms
step:92/2330 train_time:5372ms step_avg:58.39ms
step:93/2330 train_time:5428ms step_avg:58.37ms
step:94/2330 train_time:5488ms step_avg:58.39ms
step:95/2330 train_time:5544ms step_avg:58.36ms
step:96/2330 train_time:5605ms step_avg:58.38ms
step:97/2330 train_time:5660ms step_avg:58.35ms
step:98/2330 train_time:5721ms step_avg:58.38ms
step:99/2330 train_time:5777ms step_avg:58.35ms
step:100/2330 train_time:5838ms step_avg:58.38ms
step:101/2330 train_time:5894ms step_avg:58.35ms
step:102/2330 train_time:5954ms step_avg:58.38ms
step:103/2330 train_time:6011ms step_avg:58.35ms
step:104/2330 train_time:6070ms step_avg:58.37ms
step:105/2330 train_time:6126ms step_avg:58.35ms
step:106/2330 train_time:6186ms step_avg:58.36ms
step:107/2330 train_time:6242ms step_avg:58.34ms
step:108/2330 train_time:6303ms step_avg:58.36ms
step:109/2330 train_time:6359ms step_avg:58.34ms
step:110/2330 train_time:6420ms step_avg:58.36ms
step:111/2330 train_time:6475ms step_avg:58.34ms
step:112/2330 train_time:6537ms step_avg:58.37ms
step:113/2330 train_time:6593ms step_avg:58.34ms
step:114/2330 train_time:6656ms step_avg:58.38ms
step:115/2330 train_time:6712ms step_avg:58.36ms
step:116/2330 train_time:6772ms step_avg:58.38ms
step:117/2330 train_time:6829ms step_avg:58.36ms
step:118/2330 train_time:6889ms step_avg:58.38ms
step:119/2330 train_time:6945ms step_avg:58.36ms
step:120/2330 train_time:7005ms step_avg:58.38ms
step:121/2330 train_time:7061ms step_avg:58.36ms
step:122/2330 train_time:7121ms step_avg:58.37ms
step:123/2330 train_time:7177ms step_avg:58.35ms
step:124/2330 train_time:7239ms step_avg:58.38ms
step:125/2330 train_time:7295ms step_avg:58.36ms
step:126/2330 train_time:7357ms step_avg:58.39ms
step:127/2330 train_time:7413ms step_avg:58.37ms
step:128/2330 train_time:7473ms step_avg:58.38ms
step:129/2330 train_time:7529ms step_avg:58.36ms
step:130/2330 train_time:7589ms step_avg:58.38ms
step:131/2330 train_time:7645ms step_avg:58.36ms
step:132/2330 train_time:7706ms step_avg:58.38ms
step:133/2330 train_time:7761ms step_avg:58.35ms
step:134/2330 train_time:7822ms step_avg:58.37ms
step:135/2330 train_time:7877ms step_avg:58.35ms
step:136/2330 train_time:7939ms step_avg:58.37ms
step:137/2330 train_time:7995ms step_avg:58.35ms
step:138/2330 train_time:8056ms step_avg:58.38ms
step:139/2330 train_time:8112ms step_avg:58.36ms
step:140/2330 train_time:8174ms step_avg:58.38ms
step:141/2330 train_time:8230ms step_avg:58.37ms
step:142/2330 train_time:8290ms step_avg:58.38ms
step:143/2330 train_time:8347ms step_avg:58.37ms
step:144/2330 train_time:8406ms step_avg:58.38ms
step:145/2330 train_time:8463ms step_avg:58.37ms
step:146/2330 train_time:8522ms step_avg:58.37ms
step:147/2330 train_time:8578ms step_avg:58.35ms
step:148/2330 train_time:8639ms step_avg:58.37ms
step:149/2330 train_time:8694ms step_avg:58.35ms
step:150/2330 train_time:8756ms step_avg:58.37ms
step:151/2330 train_time:8812ms step_avg:58.36ms
step:152/2330 train_time:8873ms step_avg:58.37ms
step:153/2330 train_time:8929ms step_avg:58.36ms
step:154/2330 train_time:8989ms step_avg:58.37ms
step:155/2330 train_time:9045ms step_avg:58.36ms
step:156/2330 train_time:9105ms step_avg:58.37ms
step:157/2330 train_time:9161ms step_avg:58.35ms
step:158/2330 train_time:9221ms step_avg:58.36ms
step:159/2330 train_time:9277ms step_avg:58.35ms
step:160/2330 train_time:9338ms step_avg:58.36ms
step:161/2330 train_time:9394ms step_avg:58.35ms
step:162/2330 train_time:9456ms step_avg:58.37ms
step:163/2330 train_time:9511ms step_avg:58.35ms
step:164/2330 train_time:9573ms step_avg:58.37ms
step:165/2330 train_time:9630ms step_avg:58.36ms
step:166/2330 train_time:9690ms step_avg:58.37ms
step:167/2330 train_time:9746ms step_avg:58.36ms
step:168/2330 train_time:9806ms step_avg:58.37ms
step:169/2330 train_time:9861ms step_avg:58.35ms
step:170/2330 train_time:9922ms step_avg:58.37ms
step:171/2330 train_time:9977ms step_avg:58.35ms
step:172/2330 train_time:10039ms step_avg:58.37ms
step:173/2330 train_time:10095ms step_avg:58.35ms
step:174/2330 train_time:10157ms step_avg:58.37ms
step:175/2330 train_time:10213ms step_avg:58.36ms
step:176/2330 train_time:10274ms step_avg:58.38ms
step:177/2330 train_time:10330ms step_avg:58.36ms
step:178/2330 train_time:10391ms step_avg:58.38ms
step:179/2330 train_time:10447ms step_avg:58.36ms
step:180/2330 train_time:10508ms step_avg:58.38ms
step:181/2330 train_time:10563ms step_avg:58.36ms
step:182/2330 train_time:10623ms step_avg:58.37ms
step:183/2330 train_time:10679ms step_avg:58.36ms
step:184/2330 train_time:10739ms step_avg:58.37ms
step:185/2330 train_time:10795ms step_avg:58.35ms
step:186/2330 train_time:10857ms step_avg:58.37ms
step:187/2330 train_time:10913ms step_avg:58.36ms
step:188/2330 train_time:10974ms step_avg:58.37ms
step:189/2330 train_time:11030ms step_avg:58.36ms
step:190/2330 train_time:11090ms step_avg:58.37ms
step:191/2330 train_time:11145ms step_avg:58.35ms
step:192/2330 train_time:11206ms step_avg:58.36ms
step:193/2330 train_time:11261ms step_avg:58.35ms
step:194/2330 train_time:11323ms step_avg:58.36ms
step:195/2330 train_time:11378ms step_avg:58.35ms
step:196/2330 train_time:11439ms step_avg:58.36ms
step:197/2330 train_time:11495ms step_avg:58.35ms
step:198/2330 train_time:11557ms step_avg:58.37ms
step:199/2330 train_time:11613ms step_avg:58.35ms
step:200/2330 train_time:11674ms step_avg:58.37ms
step:201/2330 train_time:11730ms step_avg:58.36ms
step:202/2330 train_time:11790ms step_avg:58.37ms
step:203/2330 train_time:11846ms step_avg:58.35ms
step:204/2330 train_time:11906ms step_avg:58.36ms
step:205/2330 train_time:11962ms step_avg:58.35ms
step:206/2330 train_time:12022ms step_avg:58.36ms
step:207/2330 train_time:12078ms step_avg:58.35ms
step:208/2330 train_time:12138ms step_avg:58.36ms
step:209/2330 train_time:12194ms step_avg:58.34ms
step:210/2330 train_time:12256ms step_avg:58.36ms
step:211/2330 train_time:12312ms step_avg:58.35ms
step:212/2330 train_time:12373ms step_avg:58.36ms
step:213/2330 train_time:12429ms step_avg:58.35ms
step:214/2330 train_time:12490ms step_avg:58.36ms
step:215/2330 train_time:12545ms step_avg:58.35ms
step:216/2330 train_time:12605ms step_avg:58.36ms
step:217/2330 train_time:12661ms step_avg:58.34ms
step:218/2330 train_time:12722ms step_avg:58.36ms
step:219/2330 train_time:12778ms step_avg:58.35ms
step:220/2330 train_time:12838ms step_avg:58.36ms
step:221/2330 train_time:12894ms step_avg:58.34ms
step:222/2330 train_time:12955ms step_avg:58.36ms
step:223/2330 train_time:13011ms step_avg:58.35ms
step:224/2330 train_time:13072ms step_avg:58.36ms
step:225/2330 train_time:13128ms step_avg:58.35ms
step:226/2330 train_time:13188ms step_avg:58.35ms
step:227/2330 train_time:13244ms step_avg:58.34ms
step:228/2330 train_time:13304ms step_avg:58.35ms
step:229/2330 train_time:13360ms step_avg:58.34ms
step:230/2330 train_time:13420ms step_avg:58.35ms
step:231/2330 train_time:13476ms step_avg:58.34ms
step:232/2330 train_time:13538ms step_avg:58.35ms
step:233/2330 train_time:13593ms step_avg:58.34ms
step:234/2330 train_time:13655ms step_avg:58.36ms
step:235/2330 train_time:13711ms step_avg:58.35ms
step:236/2330 train_time:13773ms step_avg:58.36ms
step:237/2330 train_time:13828ms step_avg:58.35ms
step:238/2330 train_time:13889ms step_avg:58.36ms
step:239/2330 train_time:13945ms step_avg:58.35ms
step:240/2330 train_time:14006ms step_avg:58.36ms
step:241/2330 train_time:14061ms step_avg:58.34ms
step:242/2330 train_time:14122ms step_avg:58.36ms
step:243/2330 train_time:14177ms step_avg:58.34ms
step:244/2330 train_time:14238ms step_avg:58.35ms
step:245/2330 train_time:14294ms step_avg:58.34ms
step:246/2330 train_time:14355ms step_avg:58.35ms
step:247/2330 train_time:14411ms step_avg:58.35ms
step:248/2330 train_time:14472ms step_avg:58.36ms
step:249/2330 train_time:14528ms step_avg:58.35ms
step:250/2330 train_time:14588ms step_avg:58.35ms
step:250/2330 val_loss:5.5642 train_time:14664ms step_avg:58.66ms
step:251/2330 train_time:14683ms step_avg:58.50ms
step:252/2330 train_time:14706ms step_avg:58.36ms
step:253/2330 train_time:14762ms step_avg:58.35ms
step:254/2330 train_time:14826ms step_avg:58.37ms
step:255/2330 train_time:14882ms step_avg:58.36ms
step:256/2330 train_time:14946ms step_avg:58.38ms
step:257/2330 train_time:15002ms step_avg:58.37ms
step:258/2330 train_time:15063ms step_avg:58.38ms
step:259/2330 train_time:15119ms step_avg:58.37ms
step:260/2330 train_time:15179ms step_avg:58.38ms
step:261/2330 train_time:15235ms step_avg:58.37ms
step:262/2330 train_time:15295ms step_avg:58.38ms
step:263/2330 train_time:15351ms step_avg:58.37ms
step:264/2330 train_time:15411ms step_avg:58.37ms
step:265/2330 train_time:15466ms step_avg:58.36ms
step:266/2330 train_time:15526ms step_avg:58.37ms
step:267/2330 train_time:15582ms step_avg:58.36ms
step:268/2330 train_time:15642ms step_avg:58.36ms
step:269/2330 train_time:15697ms step_avg:58.35ms
step:270/2330 train_time:15758ms step_avg:58.36ms
step:271/2330 train_time:15815ms step_avg:58.36ms
step:272/2330 train_time:15878ms step_avg:58.37ms
step:273/2330 train_time:15934ms step_avg:58.37ms
step:274/2330 train_time:15996ms step_avg:58.38ms
step:275/2330 train_time:16053ms step_avg:58.37ms
step:276/2330 train_time:16114ms step_avg:58.38ms
step:277/2330 train_time:16169ms step_avg:58.37ms
step:278/2330 train_time:16230ms step_avg:58.38ms
step:279/2330 train_time:16285ms step_avg:58.37ms
step:280/2330 train_time:16345ms step_avg:58.38ms
step:281/2330 train_time:16401ms step_avg:58.37ms
step:282/2330 train_time:16461ms step_avg:58.37ms
step:283/2330 train_time:16516ms step_avg:58.36ms
step:284/2330 train_time:16578ms step_avg:58.37ms
step:285/2330 train_time:16633ms step_avg:58.36ms
step:286/2330 train_time:16694ms step_avg:58.37ms
step:287/2330 train_time:16750ms step_avg:58.36ms
step:288/2330 train_time:16811ms step_avg:58.37ms
step:289/2330 train_time:16867ms step_avg:58.36ms
step:290/2330 train_time:16927ms step_avg:58.37ms
step:291/2330 train_time:16983ms step_avg:58.36ms
step:292/2330 train_time:17044ms step_avg:58.37ms
step:293/2330 train_time:17100ms step_avg:58.36ms
step:294/2330 train_time:17162ms step_avg:58.37ms
step:295/2330 train_time:17217ms step_avg:58.36ms
step:296/2330 train_time:17280ms step_avg:58.38ms
step:297/2330 train_time:17336ms step_avg:58.37ms
step:298/2330 train_time:17395ms step_avg:58.37ms
step:299/2330 train_time:17451ms step_avg:58.37ms
step:300/2330 train_time:17512ms step_avg:58.37ms
step:301/2330 train_time:17567ms step_avg:58.36ms
step:302/2330 train_time:17626ms step_avg:58.37ms
step:303/2330 train_time:17682ms step_avg:58.36ms
step:304/2330 train_time:17743ms step_avg:58.37ms
step:305/2330 train_time:17799ms step_avg:58.36ms
step:306/2330 train_time:17860ms step_avg:58.37ms
step:307/2330 train_time:17916ms step_avg:58.36ms
step:308/2330 train_time:17978ms step_avg:58.37ms
step:309/2330 train_time:18034ms step_avg:58.36ms
step:310/2330 train_time:18095ms step_avg:58.37ms
step:311/2330 train_time:18151ms step_avg:58.36ms
step:312/2330 train_time:18211ms step_avg:58.37ms
step:313/2330 train_time:18267ms step_avg:58.36ms
step:314/2330 train_time:18326ms step_avg:58.36ms
step:315/2330 train_time:18382ms step_avg:58.35ms
step:316/2330 train_time:18442ms step_avg:58.36ms
step:317/2330 train_time:18498ms step_avg:58.35ms
step:318/2330 train_time:18559ms step_avg:58.36ms
step:319/2330 train_time:18615ms step_avg:58.35ms
step:320/2330 train_time:18675ms step_avg:58.36ms
step:321/2330 train_time:18732ms step_avg:58.35ms
step:322/2330 train_time:18792ms step_avg:58.36ms
step:323/2330 train_time:18848ms step_avg:58.35ms
step:324/2330 train_time:18907ms step_avg:58.36ms
step:325/2330 train_time:18963ms step_avg:58.35ms
step:326/2330 train_time:19023ms step_avg:58.35ms
step:327/2330 train_time:19078ms step_avg:58.34ms
step:328/2330 train_time:19140ms step_avg:58.35ms
step:329/2330 train_time:19196ms step_avg:58.35ms
step:330/2330 train_time:19257ms step_avg:58.35ms
step:331/2330 train_time:19313ms step_avg:58.35ms
step:332/2330 train_time:19373ms step_avg:58.35ms
step:333/2330 train_time:19428ms step_avg:58.34ms
step:334/2330 train_time:19488ms step_avg:58.35ms
step:335/2330 train_time:19544ms step_avg:58.34ms
step:336/2330 train_time:19604ms step_avg:58.35ms
step:337/2330 train_time:19660ms step_avg:58.34ms
step:338/2330 train_time:19721ms step_avg:58.34ms
step:339/2330 train_time:19776ms step_avg:58.34ms
step:340/2330 train_time:19837ms step_avg:58.34ms
step:341/2330 train_time:19893ms step_avg:58.34ms
step:342/2330 train_time:19954ms step_avg:58.35ms
step:343/2330 train_time:20010ms step_avg:58.34ms
step:344/2330 train_time:20071ms step_avg:58.35ms
step:345/2330 train_time:20126ms step_avg:58.34ms
step:346/2330 train_time:20187ms step_avg:58.34ms
step:347/2330 train_time:20242ms step_avg:58.34ms
step:348/2330 train_time:20303ms step_avg:58.34ms
step:349/2330 train_time:20359ms step_avg:58.33ms
step:350/2330 train_time:20419ms step_avg:58.34ms
step:351/2330 train_time:20475ms step_avg:58.33ms
step:352/2330 train_time:20535ms step_avg:58.34ms
step:353/2330 train_time:20591ms step_avg:58.33ms
step:354/2330 train_time:20651ms step_avg:58.34ms
step:355/2330 train_time:20706ms step_avg:58.33ms
step:356/2330 train_time:20766ms step_avg:58.33ms
step:357/2330 train_time:20822ms step_avg:58.33ms
step:358/2330 train_time:20883ms step_avg:58.33ms
step:359/2330 train_time:20938ms step_avg:58.32ms
step:360/2330 train_time:21000ms step_avg:58.33ms
step:361/2330 train_time:21056ms step_avg:58.33ms
step:362/2330 train_time:21117ms step_avg:58.33ms
step:363/2330 train_time:21173ms step_avg:58.33ms
step:364/2330 train_time:21234ms step_avg:58.34ms
step:365/2330 train_time:21290ms step_avg:58.33ms
step:366/2330 train_time:21351ms step_avg:58.34ms
step:367/2330 train_time:21406ms step_avg:58.33ms
step:368/2330 train_time:21468ms step_avg:58.34ms
step:369/2330 train_time:21523ms step_avg:58.33ms
step:370/2330 train_time:21584ms step_avg:58.33ms
step:371/2330 train_time:21639ms step_avg:58.33ms
step:372/2330 train_time:21700ms step_avg:58.33ms
step:373/2330 train_time:21756ms step_avg:58.33ms
step:374/2330 train_time:21816ms step_avg:58.33ms
step:375/2330 train_time:21872ms step_avg:58.33ms
step:376/2330 train_time:21932ms step_avg:58.33ms
step:377/2330 train_time:21987ms step_avg:58.32ms
step:378/2330 train_time:22047ms step_avg:58.33ms
step:379/2330 train_time:22103ms step_avg:58.32ms
step:380/2330 train_time:22164ms step_avg:58.33ms
step:381/2330 train_time:22219ms step_avg:58.32ms
step:382/2330 train_time:22281ms step_avg:58.33ms
step:383/2330 train_time:22336ms step_avg:58.32ms
step:384/2330 train_time:22399ms step_avg:58.33ms
step:385/2330 train_time:22455ms step_avg:58.32ms
step:386/2330 train_time:22516ms step_avg:58.33ms
step:387/2330 train_time:22572ms step_avg:58.33ms
step:388/2330 train_time:22632ms step_avg:58.33ms
step:389/2330 train_time:22688ms step_avg:58.32ms
step:390/2330 train_time:22747ms step_avg:58.33ms
step:391/2330 train_time:22802ms step_avg:58.32ms
step:392/2330 train_time:22863ms step_avg:58.32ms
step:393/2330 train_time:22919ms step_avg:58.32ms
step:394/2330 train_time:22980ms step_avg:58.32ms
step:395/2330 train_time:23036ms step_avg:58.32ms
step:396/2330 train_time:23097ms step_avg:58.33ms
step:397/2330 train_time:23153ms step_avg:58.32ms
step:398/2330 train_time:23214ms step_avg:58.33ms
step:399/2330 train_time:23270ms step_avg:58.32ms
step:400/2330 train_time:23331ms step_avg:58.33ms
step:401/2330 train_time:23386ms step_avg:58.32ms
step:402/2330 train_time:23447ms step_avg:58.32ms
step:403/2330 train_time:23502ms step_avg:58.32ms
step:404/2330 train_time:23564ms step_avg:58.33ms
step:405/2330 train_time:23620ms step_avg:58.32ms
step:406/2330 train_time:23681ms step_avg:58.33ms
step:407/2330 train_time:23736ms step_avg:58.32ms
step:408/2330 train_time:23798ms step_avg:58.33ms
step:409/2330 train_time:23854ms step_avg:58.32ms
step:410/2330 train_time:23914ms step_avg:58.33ms
step:411/2330 train_time:23970ms step_avg:58.32ms
step:412/2330 train_time:24030ms step_avg:58.33ms
step:413/2330 train_time:24086ms step_avg:58.32ms
step:414/2330 train_time:24146ms step_avg:58.32ms
step:415/2330 train_time:24202ms step_avg:58.32ms
step:416/2330 train_time:24263ms step_avg:58.32ms
step:417/2330 train_time:24318ms step_avg:58.32ms
step:418/2330 train_time:24380ms step_avg:58.33ms
step:419/2330 train_time:24436ms step_avg:58.32ms
step:420/2330 train_time:24497ms step_avg:58.33ms
step:421/2330 train_time:24553ms step_avg:58.32ms
step:422/2330 train_time:24613ms step_avg:58.32ms
step:423/2330 train_time:24668ms step_avg:58.32ms
step:424/2330 train_time:24729ms step_avg:58.32ms
step:425/2330 train_time:24784ms step_avg:58.32ms
step:426/2330 train_time:24845ms step_avg:58.32ms
step:427/2330 train_time:24901ms step_avg:58.32ms
step:428/2330 train_time:24961ms step_avg:58.32ms
step:429/2330 train_time:25016ms step_avg:58.31ms
step:430/2330 train_time:25077ms step_avg:58.32ms
step:431/2330 train_time:25134ms step_avg:58.32ms
step:432/2330 train_time:25195ms step_avg:58.32ms
step:433/2330 train_time:25252ms step_avg:58.32ms
step:434/2330 train_time:25311ms step_avg:58.32ms
step:435/2330 train_time:25367ms step_avg:58.32ms
step:436/2330 train_time:25427ms step_avg:58.32ms
step:437/2330 train_time:25482ms step_avg:58.31ms
step:438/2330 train_time:25544ms step_avg:58.32ms
step:439/2330 train_time:25600ms step_avg:58.31ms
step:440/2330 train_time:25660ms step_avg:58.32ms
step:441/2330 train_time:25716ms step_avg:58.31ms
step:442/2330 train_time:25777ms step_avg:58.32ms
step:443/2330 train_time:25833ms step_avg:58.31ms
step:444/2330 train_time:25893ms step_avg:58.32ms
step:445/2330 train_time:25949ms step_avg:58.31ms
step:446/2330 train_time:26009ms step_avg:58.32ms
step:447/2330 train_time:26064ms step_avg:58.31ms
step:448/2330 train_time:26124ms step_avg:58.31ms
step:449/2330 train_time:26180ms step_avg:58.31ms
step:450/2330 train_time:26242ms step_avg:58.32ms
step:451/2330 train_time:26297ms step_avg:58.31ms
step:452/2330 train_time:26358ms step_avg:58.31ms
step:453/2330 train_time:26414ms step_avg:58.31ms
step:454/2330 train_time:26476ms step_avg:58.32ms
step:455/2330 train_time:26532ms step_avg:58.31ms
step:456/2330 train_time:26591ms step_avg:58.31ms
step:457/2330 train_time:26647ms step_avg:58.31ms
step:458/2330 train_time:26708ms step_avg:58.31ms
step:459/2330 train_time:26763ms step_avg:58.31ms
step:460/2330 train_time:26824ms step_avg:58.31ms
step:461/2330 train_time:26880ms step_avg:58.31ms
step:462/2330 train_time:26941ms step_avg:58.31ms
step:463/2330 train_time:26996ms step_avg:58.31ms
step:464/2330 train_time:27057ms step_avg:58.31ms
step:465/2330 train_time:27113ms step_avg:58.31ms
step:466/2330 train_time:27174ms step_avg:58.31ms
step:467/2330 train_time:27230ms step_avg:58.31ms
step:468/2330 train_time:27290ms step_avg:58.31ms
step:469/2330 train_time:27346ms step_avg:58.31ms
step:470/2330 train_time:27406ms step_avg:58.31ms
step:471/2330 train_time:27462ms step_avg:58.31ms
step:472/2330 train_time:27522ms step_avg:58.31ms
step:473/2330 train_time:27578ms step_avg:58.30ms
step:474/2330 train_time:27639ms step_avg:58.31ms
step:475/2330 train_time:27695ms step_avg:58.30ms
step:476/2330 train_time:27756ms step_avg:58.31ms
step:477/2330 train_time:27812ms step_avg:58.31ms
step:478/2330 train_time:27872ms step_avg:58.31ms
step:479/2330 train_time:27928ms step_avg:58.30ms
step:480/2330 train_time:27987ms step_avg:58.31ms
step:481/2330 train_time:28043ms step_avg:58.30ms
step:482/2330 train_time:28103ms step_avg:58.30ms
step:483/2330 train_time:28158ms step_avg:58.30ms
step:484/2330 train_time:28220ms step_avg:58.31ms
step:485/2330 train_time:28276ms step_avg:58.30ms
step:486/2330 train_time:28336ms step_avg:58.31ms
step:487/2330 train_time:28392ms step_avg:58.30ms
step:488/2330 train_time:28454ms step_avg:58.31ms
step:489/2330 train_time:28510ms step_avg:58.30ms
step:490/2330 train_time:28570ms step_avg:58.31ms
step:491/2330 train_time:28626ms step_avg:58.30ms
step:492/2330 train_time:28686ms step_avg:58.30ms
step:493/2330 train_time:28741ms step_avg:58.30ms
step:494/2330 train_time:28802ms step_avg:58.30ms
step:495/2330 train_time:28857ms step_avg:58.30ms
step:496/2330 train_time:28918ms step_avg:58.30ms
step:497/2330 train_time:28974ms step_avg:58.30ms
step:498/2330 train_time:29036ms step_avg:58.30ms
step:499/2330 train_time:29092ms step_avg:58.30ms
step:500/2330 train_time:29152ms step_avg:58.30ms
step:500/2330 val_loss:4.7869 train_time:29229ms step_avg:58.46ms
step:501/2330 train_time:29248ms step_avg:58.38ms
step:502/2330 train_time:29271ms step_avg:58.31ms
step:503/2330 train_time:29326ms step_avg:58.30ms
step:504/2330 train_time:29391ms step_avg:58.32ms
step:505/2330 train_time:29447ms step_avg:58.31ms
step:506/2330 train_time:29511ms step_avg:58.32ms
step:507/2330 train_time:29567ms step_avg:58.32ms
step:508/2330 train_time:29628ms step_avg:58.32ms
step:509/2330 train_time:29684ms step_avg:58.32ms
step:510/2330 train_time:29745ms step_avg:58.32ms
step:511/2330 train_time:29800ms step_avg:58.32ms
step:512/2330 train_time:29860ms step_avg:58.32ms
step:513/2330 train_time:29916ms step_avg:58.31ms
step:514/2330 train_time:29976ms step_avg:58.32ms
step:515/2330 train_time:30031ms step_avg:58.31ms
step:516/2330 train_time:30091ms step_avg:58.32ms
step:517/2330 train_time:30147ms step_avg:58.31ms
step:518/2330 train_time:30207ms step_avg:58.31ms
step:519/2330 train_time:30262ms step_avg:58.31ms
step:520/2330 train_time:30325ms step_avg:58.32ms
step:521/2330 train_time:30381ms step_avg:58.31ms
step:522/2330 train_time:30445ms step_avg:58.32ms
step:523/2330 train_time:30500ms step_avg:58.32ms
step:524/2330 train_time:30563ms step_avg:58.33ms
step:525/2330 train_time:30619ms step_avg:58.32ms
step:526/2330 train_time:30681ms step_avg:58.33ms
step:527/2330 train_time:30736ms step_avg:58.32ms
step:528/2330 train_time:30798ms step_avg:58.33ms
step:529/2330 train_time:30854ms step_avg:58.32ms
step:530/2330 train_time:30914ms step_avg:58.33ms
step:531/2330 train_time:30970ms step_avg:58.32ms
step:532/2330 train_time:31029ms step_avg:58.33ms
step:533/2330 train_time:31085ms step_avg:58.32ms
step:534/2330 train_time:31145ms step_avg:58.32ms
step:535/2330 train_time:31201ms step_avg:58.32ms
step:536/2330 train_time:31262ms step_avg:58.32ms
step:537/2330 train_time:31318ms step_avg:58.32ms
step:538/2330 train_time:31380ms step_avg:58.33ms
step:539/2330 train_time:31436ms step_avg:58.32ms
step:540/2330 train_time:31499ms step_avg:58.33ms
step:541/2330 train_time:31554ms step_avg:58.33ms
step:542/2330 train_time:31616ms step_avg:58.33ms
step:543/2330 train_time:31671ms step_avg:58.33ms
step:544/2330 train_time:31732ms step_avg:58.33ms
step:545/2330 train_time:31788ms step_avg:58.33ms
step:546/2330 train_time:31848ms step_avg:58.33ms
step:547/2330 train_time:31904ms step_avg:58.33ms
step:548/2330 train_time:31964ms step_avg:58.33ms
step:549/2330 train_time:32020ms step_avg:58.32ms
step:550/2330 train_time:32081ms step_avg:58.33ms
step:551/2330 train_time:32136ms step_avg:58.32ms
step:552/2330 train_time:32198ms step_avg:58.33ms
step:553/2330 train_time:32253ms step_avg:58.32ms
step:554/2330 train_time:32314ms step_avg:58.33ms
step:555/2330 train_time:32370ms step_avg:58.32ms
step:556/2330 train_time:32431ms step_avg:58.33ms
step:557/2330 train_time:32487ms step_avg:58.33ms
step:558/2330 train_time:32547ms step_avg:58.33ms
step:559/2330 train_time:32603ms step_avg:58.32ms
step:560/2330 train_time:32664ms step_avg:58.33ms
step:561/2330 train_time:32720ms step_avg:58.32ms
step:562/2330 train_time:32781ms step_avg:58.33ms
step:563/2330 train_time:32837ms step_avg:58.32ms
step:564/2330 train_time:32898ms step_avg:58.33ms
step:565/2330 train_time:32954ms step_avg:58.33ms
step:566/2330 train_time:33014ms step_avg:58.33ms
step:567/2330 train_time:33070ms step_avg:58.32ms
step:568/2330 train_time:33130ms step_avg:58.33ms
step:569/2330 train_time:33185ms step_avg:58.32ms
step:570/2330 train_time:33246ms step_avg:58.33ms
step:571/2330 train_time:33301ms step_avg:58.32ms
step:572/2330 train_time:33362ms step_avg:58.33ms
step:573/2330 train_time:33418ms step_avg:58.32ms
step:574/2330 train_time:33480ms step_avg:58.33ms
step:575/2330 train_time:33536ms step_avg:58.32ms
step:576/2330 train_time:33597ms step_avg:58.33ms
step:577/2330 train_time:33654ms step_avg:58.33ms
step:578/2330 train_time:33713ms step_avg:58.33ms
step:579/2330 train_time:33769ms step_avg:58.32ms
step:580/2330 train_time:33829ms step_avg:58.33ms
step:581/2330 train_time:33884ms step_avg:58.32ms
step:582/2330 train_time:33945ms step_avg:58.32ms
step:583/2330 train_time:34001ms step_avg:58.32ms
step:584/2330 train_time:34062ms step_avg:58.33ms
step:585/2330 train_time:34118ms step_avg:58.32ms
step:586/2330 train_time:34178ms step_avg:58.32ms
step:587/2330 train_time:34234ms step_avg:58.32ms
step:588/2330 train_time:34295ms step_avg:58.32ms
step:589/2330 train_time:34350ms step_avg:58.32ms
step:590/2330 train_time:34410ms step_avg:58.32ms
step:591/2330 train_time:34466ms step_avg:58.32ms
step:592/2330 train_time:34527ms step_avg:58.32ms
step:593/2330 train_time:34583ms step_avg:58.32ms
step:594/2330 train_time:34644ms step_avg:58.32ms
step:595/2330 train_time:34700ms step_avg:58.32ms
step:596/2330 train_time:34761ms step_avg:58.32ms
step:597/2330 train_time:34816ms step_avg:58.32ms
step:598/2330 train_time:34878ms step_avg:58.32ms
step:599/2330 train_time:34934ms step_avg:58.32ms
step:600/2330 train_time:34994ms step_avg:58.32ms
step:601/2330 train_time:35050ms step_avg:58.32ms
step:602/2330 train_time:35111ms step_avg:58.32ms
step:603/2330 train_time:35166ms step_avg:58.32ms
step:604/2330 train_time:35228ms step_avg:58.32ms
step:605/2330 train_time:35283ms step_avg:58.32ms
step:606/2330 train_time:35345ms step_avg:58.33ms
step:607/2330 train_time:35401ms step_avg:58.32ms
step:608/2330 train_time:35463ms step_avg:58.33ms
step:609/2330 train_time:35518ms step_avg:58.32ms
step:610/2330 train_time:35581ms step_avg:58.33ms
step:611/2330 train_time:35637ms step_avg:58.33ms
step:612/2330 train_time:35698ms step_avg:58.33ms
step:613/2330 train_time:35753ms step_avg:58.33ms
step:614/2330 train_time:35814ms step_avg:58.33ms
step:615/2330 train_time:35869ms step_avg:58.32ms
step:616/2330 train_time:35930ms step_avg:58.33ms
step:617/2330 train_time:35986ms step_avg:58.32ms
step:618/2330 train_time:36047ms step_avg:58.33ms
step:619/2330 train_time:36102ms step_avg:58.32ms
step:620/2330 train_time:36163ms step_avg:58.33ms
step:621/2330 train_time:36219ms step_avg:58.32ms
step:622/2330 train_time:36281ms step_avg:58.33ms
step:623/2330 train_time:36337ms step_avg:58.33ms
step:624/2330 train_time:36398ms step_avg:58.33ms
step:625/2330 train_time:36454ms step_avg:58.33ms
step:626/2330 train_time:36515ms step_avg:58.33ms
step:627/2330 train_time:36570ms step_avg:58.33ms
step:628/2330 train_time:36631ms step_avg:58.33ms
step:629/2330 train_time:36687ms step_avg:58.33ms
step:630/2330 train_time:36747ms step_avg:58.33ms
step:631/2330 train_time:36802ms step_avg:58.32ms
step:632/2330 train_time:36864ms step_avg:58.33ms
step:633/2330 train_time:36920ms step_avg:58.33ms
step:634/2330 train_time:36981ms step_avg:58.33ms
step:635/2330 train_time:37037ms step_avg:58.33ms
step:636/2330 train_time:37097ms step_avg:58.33ms
step:637/2330 train_time:37153ms step_avg:58.33ms
step:638/2330 train_time:37213ms step_avg:58.33ms
step:639/2330 train_time:37269ms step_avg:58.32ms
step:640/2330 train_time:37330ms step_avg:58.33ms
step:641/2330 train_time:37385ms step_avg:58.32ms
step:642/2330 train_time:37446ms step_avg:58.33ms
step:643/2330 train_time:37501ms step_avg:58.32ms
step:644/2330 train_time:37563ms step_avg:58.33ms
step:645/2330 train_time:37619ms step_avg:58.32ms
step:646/2330 train_time:37680ms step_avg:58.33ms
step:647/2330 train_time:37735ms step_avg:58.32ms
step:648/2330 train_time:37797ms step_avg:58.33ms
step:649/2330 train_time:37852ms step_avg:58.32ms
step:650/2330 train_time:37913ms step_avg:58.33ms
step:651/2330 train_time:37969ms step_avg:58.32ms
step:652/2330 train_time:38029ms step_avg:58.33ms
step:653/2330 train_time:38085ms step_avg:58.32ms
step:654/2330 train_time:38146ms step_avg:58.33ms
step:655/2330 train_time:38202ms step_avg:58.32ms
step:656/2330 train_time:38263ms step_avg:58.33ms
step:657/2330 train_time:38319ms step_avg:58.32ms
step:658/2330 train_time:38380ms step_avg:58.33ms
step:659/2330 train_time:38437ms step_avg:58.33ms
step:660/2330 train_time:38496ms step_avg:58.33ms
step:661/2330 train_time:38552ms step_avg:58.32ms
step:662/2330 train_time:38613ms step_avg:58.33ms
step:663/2330 train_time:38669ms step_avg:58.32ms
step:664/2330 train_time:38729ms step_avg:58.33ms
step:665/2330 train_time:38785ms step_avg:58.32ms
step:666/2330 train_time:38846ms step_avg:58.33ms
step:667/2330 train_time:38901ms step_avg:58.32ms
step:668/2330 train_time:38963ms step_avg:58.33ms
step:669/2330 train_time:39018ms step_avg:58.32ms
step:670/2330 train_time:39079ms step_avg:58.33ms
step:671/2330 train_time:39135ms step_avg:58.32ms
step:672/2330 train_time:39196ms step_avg:58.33ms
step:673/2330 train_time:39251ms step_avg:58.32ms
step:674/2330 train_time:39312ms step_avg:58.33ms
step:675/2330 train_time:39368ms step_avg:58.32ms
step:676/2330 train_time:39428ms step_avg:58.32ms
step:677/2330 train_time:39483ms step_avg:58.32ms
step:678/2330 train_time:39544ms step_avg:58.32ms
step:679/2330 train_time:39600ms step_avg:58.32ms
step:680/2330 train_time:39661ms step_avg:58.32ms
step:681/2330 train_time:39717ms step_avg:58.32ms
step:682/2330 train_time:39778ms step_avg:58.33ms
step:683/2330 train_time:39834ms step_avg:58.32ms
step:684/2330 train_time:39895ms step_avg:58.33ms
step:685/2330 train_time:39951ms step_avg:58.32ms
step:686/2330 train_time:40010ms step_avg:58.32ms
step:687/2330 train_time:40066ms step_avg:58.32ms
step:688/2330 train_time:40126ms step_avg:58.32ms
step:689/2330 train_time:40182ms step_avg:58.32ms
step:690/2330 train_time:40244ms step_avg:58.32ms
step:691/2330 train_time:40299ms step_avg:58.32ms
step:692/2330 train_time:40361ms step_avg:58.33ms
step:693/2330 train_time:40417ms step_avg:58.32ms
step:694/2330 train_time:40478ms step_avg:58.33ms
step:695/2330 train_time:40534ms step_avg:58.32ms
step:696/2330 train_time:40594ms step_avg:58.33ms
step:697/2330 train_time:40650ms step_avg:58.32ms
step:698/2330 train_time:40711ms step_avg:58.32ms
step:699/2330 train_time:40766ms step_avg:58.32ms
step:700/2330 train_time:40827ms step_avg:58.32ms
step:701/2330 train_time:40882ms step_avg:58.32ms
step:702/2330 train_time:40944ms step_avg:58.32ms
step:703/2330 train_time:41000ms step_avg:58.32ms
step:704/2330 train_time:41061ms step_avg:58.33ms
step:705/2330 train_time:41116ms step_avg:58.32ms
step:706/2330 train_time:41178ms step_avg:58.33ms
step:707/2330 train_time:41234ms step_avg:58.32ms
step:708/2330 train_time:41295ms step_avg:58.33ms
step:709/2330 train_time:41350ms step_avg:58.32ms
step:710/2330 train_time:41411ms step_avg:58.32ms
step:711/2330 train_time:41466ms step_avg:58.32ms
step:712/2330 train_time:41527ms step_avg:58.32ms
step:713/2330 train_time:41583ms step_avg:58.32ms
step:714/2330 train_time:41644ms step_avg:58.33ms
step:715/2330 train_time:41700ms step_avg:58.32ms
step:716/2330 train_time:41761ms step_avg:58.33ms
step:717/2330 train_time:41817ms step_avg:58.32ms
step:718/2330 train_time:41878ms step_avg:58.33ms
step:719/2330 train_time:41934ms step_avg:58.32ms
step:720/2330 train_time:41994ms step_avg:58.32ms
step:721/2330 train_time:42050ms step_avg:58.32ms
step:722/2330 train_time:42110ms step_avg:58.32ms
step:723/2330 train_time:42165ms step_avg:58.32ms
step:724/2330 train_time:42226ms step_avg:58.32ms
step:725/2330 train_time:42282ms step_avg:58.32ms
step:726/2330 train_time:42344ms step_avg:58.32ms
step:727/2330 train_time:42399ms step_avg:58.32ms
step:728/2330 train_time:42460ms step_avg:58.32ms
step:729/2330 train_time:42517ms step_avg:58.32ms
step:730/2330 train_time:42576ms step_avg:58.32ms
step:731/2330 train_time:42632ms step_avg:58.32ms
step:732/2330 train_time:42693ms step_avg:58.32ms
step:733/2330 train_time:42748ms step_avg:58.32ms
step:734/2330 train_time:42809ms step_avg:58.32ms
step:735/2330 train_time:42865ms step_avg:58.32ms
step:736/2330 train_time:42926ms step_avg:58.32ms
step:737/2330 train_time:42982ms step_avg:58.32ms
step:738/2330 train_time:43044ms step_avg:58.33ms
step:739/2330 train_time:43100ms step_avg:58.32ms
step:740/2330 train_time:43161ms step_avg:58.33ms
step:741/2330 train_time:43217ms step_avg:58.32ms
step:742/2330 train_time:43279ms step_avg:58.33ms
step:743/2330 train_time:43334ms step_avg:58.32ms
step:744/2330 train_time:43395ms step_avg:58.33ms
step:745/2330 train_time:43451ms step_avg:58.32ms
step:746/2330 train_time:43512ms step_avg:58.33ms
step:747/2330 train_time:43567ms step_avg:58.32ms
step:748/2330 train_time:43628ms step_avg:58.33ms
step:749/2330 train_time:43684ms step_avg:58.32ms
step:750/2330 train_time:43745ms step_avg:58.33ms
step:750/2330 val_loss:4.4847 train_time:43823ms step_avg:58.43ms
step:751/2330 train_time:43841ms step_avg:58.38ms
step:752/2330 train_time:43865ms step_avg:58.33ms
step:753/2330 train_time:43921ms step_avg:58.33ms
step:754/2330 train_time:43986ms step_avg:58.34ms
step:755/2330 train_time:44045ms step_avg:58.34ms
step:756/2330 train_time:44105ms step_avg:58.34ms
step:757/2330 train_time:44161ms step_avg:58.34ms
step:758/2330 train_time:44220ms step_avg:58.34ms
step:759/2330 train_time:44276ms step_avg:58.34ms
step:760/2330 train_time:44336ms step_avg:58.34ms
step:761/2330 train_time:44392ms step_avg:58.33ms
step:762/2330 train_time:44452ms step_avg:58.34ms
step:763/2330 train_time:44507ms step_avg:58.33ms
step:764/2330 train_time:44568ms step_avg:58.33ms
step:765/2330 train_time:44624ms step_avg:58.33ms
step:766/2330 train_time:44684ms step_avg:58.33ms
step:767/2330 train_time:44740ms step_avg:58.33ms
step:768/2330 train_time:44801ms step_avg:58.33ms
step:769/2330 train_time:44858ms step_avg:58.33ms
step:770/2330 train_time:44919ms step_avg:58.34ms
step:771/2330 train_time:44976ms step_avg:58.33ms
step:772/2330 train_time:45038ms step_avg:58.34ms
step:773/2330 train_time:45095ms step_avg:58.34ms
step:774/2330 train_time:45156ms step_avg:58.34ms
step:775/2330 train_time:45212ms step_avg:58.34ms
step:776/2330 train_time:45274ms step_avg:58.34ms
step:777/2330 train_time:45331ms step_avg:58.34ms
step:778/2330 train_time:45392ms step_avg:58.34ms
step:779/2330 train_time:45449ms step_avg:58.34ms
step:780/2330 train_time:45509ms step_avg:58.35ms
step:781/2330 train_time:45566ms step_avg:58.34ms
step:782/2330 train_time:45627ms step_avg:58.35ms
step:783/2330 train_time:45683ms step_avg:58.34ms
step:784/2330 train_time:45744ms step_avg:58.35ms
step:785/2330 train_time:45801ms step_avg:58.35ms
step:786/2330 train_time:45862ms step_avg:58.35ms
step:787/2330 train_time:45919ms step_avg:58.35ms
step:788/2330 train_time:45979ms step_avg:58.35ms
step:789/2330 train_time:46036ms step_avg:58.35ms
step:790/2330 train_time:46097ms step_avg:58.35ms
step:791/2330 train_time:46154ms step_avg:58.35ms
step:792/2330 train_time:46215ms step_avg:58.35ms
step:793/2330 train_time:46272ms step_avg:58.35ms
step:794/2330 train_time:46333ms step_avg:58.35ms
step:795/2330 train_time:46390ms step_avg:58.35ms
step:796/2330 train_time:46451ms step_avg:58.36ms
step:797/2330 train_time:46507ms step_avg:58.35ms
step:798/2330 train_time:46569ms step_avg:58.36ms
step:799/2330 train_time:46625ms step_avg:58.35ms
step:800/2330 train_time:46687ms step_avg:58.36ms
step:801/2330 train_time:46743ms step_avg:58.36ms
step:802/2330 train_time:46805ms step_avg:58.36ms
step:803/2330 train_time:46861ms step_avg:58.36ms
step:804/2330 train_time:46923ms step_avg:58.36ms
step:805/2330 train_time:46980ms step_avg:58.36ms
step:806/2330 train_time:47042ms step_avg:58.37ms
step:807/2330 train_time:47100ms step_avg:58.36ms
step:808/2330 train_time:47161ms step_avg:58.37ms
step:809/2330 train_time:47218ms step_avg:58.37ms
step:810/2330 train_time:47279ms step_avg:58.37ms
step:811/2330 train_time:47336ms step_avg:58.37ms
step:812/2330 train_time:47397ms step_avg:58.37ms
step:813/2330 train_time:47453ms step_avg:58.37ms
step:814/2330 train_time:47515ms step_avg:58.37ms
step:815/2330 train_time:47571ms step_avg:58.37ms
step:816/2330 train_time:47633ms step_avg:58.37ms
step:817/2330 train_time:47689ms step_avg:58.37ms
step:818/2330 train_time:47752ms step_avg:58.38ms
step:819/2330 train_time:47808ms step_avg:58.37ms
step:820/2330 train_time:47870ms step_avg:58.38ms
step:821/2330 train_time:47927ms step_avg:58.38ms
step:822/2330 train_time:47989ms step_avg:58.38ms
step:823/2330 train_time:48046ms step_avg:58.38ms
step:824/2330 train_time:48108ms step_avg:58.38ms
step:825/2330 train_time:48165ms step_avg:58.38ms
step:826/2330 train_time:48228ms step_avg:58.39ms
step:827/2330 train_time:48286ms step_avg:58.39ms
step:828/2330 train_time:48346ms step_avg:58.39ms
step:829/2330 train_time:48403ms step_avg:58.39ms
step:830/2330 train_time:48463ms step_avg:58.39ms
step:831/2330 train_time:48521ms step_avg:58.39ms
step:832/2330 train_time:48581ms step_avg:58.39ms
step:833/2330 train_time:48638ms step_avg:58.39ms
step:834/2330 train_time:48698ms step_avg:58.39ms
step:835/2330 train_time:48755ms step_avg:58.39ms
step:836/2330 train_time:48816ms step_avg:58.39ms
step:837/2330 train_time:48872ms step_avg:58.39ms
step:838/2330 train_time:48933ms step_avg:58.39ms
step:839/2330 train_time:48990ms step_avg:58.39ms
step:840/2330 train_time:49053ms step_avg:58.40ms
step:841/2330 train_time:49109ms step_avg:58.39ms
step:842/2330 train_time:49172ms step_avg:58.40ms
step:843/2330 train_time:49228ms step_avg:58.40ms
step:844/2330 train_time:49291ms step_avg:58.40ms
step:845/2330 train_time:49347ms step_avg:58.40ms
step:846/2330 train_time:49409ms step_avg:58.40ms
step:847/2330 train_time:49466ms step_avg:58.40ms
step:848/2330 train_time:49528ms step_avg:58.41ms
step:849/2330 train_time:49586ms step_avg:58.40ms
step:850/2330 train_time:49647ms step_avg:58.41ms
step:851/2330 train_time:49703ms step_avg:58.41ms
step:852/2330 train_time:49766ms step_avg:58.41ms
step:853/2330 train_time:49822ms step_avg:58.41ms
step:854/2330 train_time:49883ms step_avg:58.41ms
step:855/2330 train_time:49941ms step_avg:58.41ms
step:856/2330 train_time:50002ms step_avg:58.41ms
step:857/2330 train_time:50059ms step_avg:58.41ms
step:858/2330 train_time:50119ms step_avg:58.41ms
step:859/2330 train_time:50176ms step_avg:58.41ms
step:860/2330 train_time:50237ms step_avg:58.41ms
step:861/2330 train_time:50293ms step_avg:58.41ms
step:862/2330 train_time:50354ms step_avg:58.42ms
step:863/2330 train_time:50411ms step_avg:58.41ms
step:864/2330 train_time:50473ms step_avg:58.42ms
step:865/2330 train_time:50530ms step_avg:58.42ms
step:866/2330 train_time:50591ms step_avg:58.42ms
step:867/2330 train_time:50648ms step_avg:58.42ms
step:868/2330 train_time:50710ms step_avg:58.42ms
step:869/2330 train_time:50767ms step_avg:58.42ms
step:870/2330 train_time:50828ms step_avg:58.42ms
step:871/2330 train_time:50885ms step_avg:58.42ms
step:872/2330 train_time:50947ms step_avg:58.43ms
step:873/2330 train_time:51004ms step_avg:58.42ms
step:874/2330 train_time:51066ms step_avg:58.43ms
step:875/2330 train_time:51123ms step_avg:58.43ms
step:876/2330 train_time:51184ms step_avg:58.43ms
step:877/2330 train_time:51241ms step_avg:58.43ms
step:878/2330 train_time:51304ms step_avg:58.43ms
step:879/2330 train_time:51360ms step_avg:58.43ms
step:880/2330 train_time:51421ms step_avg:58.43ms
step:881/2330 train_time:51479ms step_avg:58.43ms
step:882/2330 train_time:51539ms step_avg:58.43ms
step:883/2330 train_time:51596ms step_avg:58.43ms
step:884/2330 train_time:51656ms step_avg:58.43ms
step:885/2330 train_time:51713ms step_avg:58.43ms
step:886/2330 train_time:51775ms step_avg:58.44ms
step:887/2330 train_time:51831ms step_avg:58.43ms
step:888/2330 train_time:51893ms step_avg:58.44ms
step:889/2330 train_time:51949ms step_avg:58.44ms
step:890/2330 train_time:52011ms step_avg:58.44ms
step:891/2330 train_time:52067ms step_avg:58.44ms
step:892/2330 train_time:52130ms step_avg:58.44ms
step:893/2330 train_time:52186ms step_avg:58.44ms
step:894/2330 train_time:52249ms step_avg:58.44ms
step:895/2330 train_time:52306ms step_avg:58.44ms
step:896/2330 train_time:52368ms step_avg:58.45ms
step:897/2330 train_time:52424ms step_avg:58.44ms
step:898/2330 train_time:52488ms step_avg:58.45ms
step:899/2330 train_time:52544ms step_avg:58.45ms
step:900/2330 train_time:52606ms step_avg:58.45ms
step:901/2330 train_time:52663ms step_avg:58.45ms
step:902/2330 train_time:52724ms step_avg:58.45ms
step:903/2330 train_time:52782ms step_avg:58.45ms
step:904/2330 train_time:52842ms step_avg:58.45ms
step:905/2330 train_time:52899ms step_avg:58.45ms
step:906/2330 train_time:52959ms step_avg:58.45ms
step:907/2330 train_time:53016ms step_avg:58.45ms
step:908/2330 train_time:53077ms step_avg:58.45ms
step:909/2330 train_time:53134ms step_avg:58.45ms
step:910/2330 train_time:53195ms step_avg:58.46ms
step:911/2330 train_time:53251ms step_avg:58.45ms
step:912/2330 train_time:53313ms step_avg:58.46ms
step:913/2330 train_time:53370ms step_avg:58.46ms
step:914/2330 train_time:53432ms step_avg:58.46ms
step:915/2330 train_time:53488ms step_avg:58.46ms
step:916/2330 train_time:53550ms step_avg:58.46ms
step:917/2330 train_time:53607ms step_avg:58.46ms
step:918/2330 train_time:53669ms step_avg:58.46ms
step:919/2330 train_time:53726ms step_avg:58.46ms
step:920/2330 train_time:53789ms step_avg:58.47ms
step:921/2330 train_time:53845ms step_avg:58.46ms
step:922/2330 train_time:53908ms step_avg:58.47ms
step:923/2330 train_time:53965ms step_avg:58.47ms
step:924/2330 train_time:54027ms step_avg:58.47ms
step:925/2330 train_time:54084ms step_avg:58.47ms
step:926/2330 train_time:54146ms step_avg:58.47ms
step:927/2330 train_time:54202ms step_avg:58.47ms
step:928/2330 train_time:54262ms step_avg:58.47ms
step:929/2330 train_time:54319ms step_avg:58.47ms
step:930/2330 train_time:54380ms step_avg:58.47ms
step:931/2330 train_time:54437ms step_avg:58.47ms
step:932/2330 train_time:54498ms step_avg:58.47ms
step:933/2330 train_time:54554ms step_avg:58.47ms
step:934/2330 train_time:54615ms step_avg:58.47ms
step:935/2330 train_time:54672ms step_avg:58.47ms
step:936/2330 train_time:54733ms step_avg:58.48ms
step:937/2330 train_time:54789ms step_avg:58.47ms
step:938/2330 train_time:54851ms step_avg:58.48ms
step:939/2330 train_time:54908ms step_avg:58.47ms
step:940/2330 train_time:54970ms step_avg:58.48ms
step:941/2330 train_time:55027ms step_avg:58.48ms
step:942/2330 train_time:55088ms step_avg:58.48ms
step:943/2330 train_time:55144ms step_avg:58.48ms
step:944/2330 train_time:55207ms step_avg:58.48ms
step:945/2330 train_time:55264ms step_avg:58.48ms
step:946/2330 train_time:55325ms step_avg:58.48ms
step:947/2330 train_time:55382ms step_avg:58.48ms
step:948/2330 train_time:55443ms step_avg:58.48ms
step:949/2330 train_time:55500ms step_avg:58.48ms
step:950/2330 train_time:55560ms step_avg:58.48ms
step:951/2330 train_time:55617ms step_avg:58.48ms
step:952/2330 train_time:55678ms step_avg:58.49ms
step:953/2330 train_time:55735ms step_avg:58.48ms
step:954/2330 train_time:55796ms step_avg:58.49ms
step:955/2330 train_time:55852ms step_avg:58.48ms
step:956/2330 train_time:55914ms step_avg:58.49ms
step:957/2330 train_time:55970ms step_avg:58.49ms
step:958/2330 train_time:56033ms step_avg:58.49ms
step:959/2330 train_time:56089ms step_avg:58.49ms
step:960/2330 train_time:56152ms step_avg:58.49ms
step:961/2330 train_time:56208ms step_avg:58.49ms
step:962/2330 train_time:56270ms step_avg:58.49ms
step:963/2330 train_time:56327ms step_avg:58.49ms
step:964/2330 train_time:56389ms step_avg:58.49ms
step:965/2330 train_time:56446ms step_avg:58.49ms
step:966/2330 train_time:56507ms step_avg:58.50ms
step:967/2330 train_time:56564ms step_avg:58.49ms
step:968/2330 train_time:56627ms step_avg:58.50ms
step:969/2330 train_time:56684ms step_avg:58.50ms
step:970/2330 train_time:56746ms step_avg:58.50ms
step:971/2330 train_time:56803ms step_avg:58.50ms
step:972/2330 train_time:56863ms step_avg:58.50ms
step:973/2330 train_time:56921ms step_avg:58.50ms
step:974/2330 train_time:56981ms step_avg:58.50ms
step:975/2330 train_time:57039ms step_avg:58.50ms
step:976/2330 train_time:57099ms step_avg:58.50ms
step:977/2330 train_time:57156ms step_avg:58.50ms
step:978/2330 train_time:57217ms step_avg:58.50ms
step:979/2330 train_time:57273ms step_avg:58.50ms
step:980/2330 train_time:57334ms step_avg:58.50ms
step:981/2330 train_time:57390ms step_avg:58.50ms
step:982/2330 train_time:57453ms step_avg:58.51ms
step:983/2330 train_time:57509ms step_avg:58.50ms
step:984/2330 train_time:57572ms step_avg:58.51ms
step:985/2330 train_time:57628ms step_avg:58.51ms
step:986/2330 train_time:57690ms step_avg:58.51ms
step:987/2330 train_time:57747ms step_avg:58.51ms
step:988/2330 train_time:57808ms step_avg:58.51ms
step:989/2330 train_time:57864ms step_avg:58.51ms
step:990/2330 train_time:57927ms step_avg:58.51ms
step:991/2330 train_time:57984ms step_avg:58.51ms
step:992/2330 train_time:58046ms step_avg:58.51ms
step:993/2330 train_time:58103ms step_avg:58.51ms
step:994/2330 train_time:58164ms step_avg:58.51ms
step:995/2330 train_time:58222ms step_avg:58.51ms
step:996/2330 train_time:58282ms step_avg:58.52ms
step:997/2330 train_time:58340ms step_avg:58.52ms
step:998/2330 train_time:58401ms step_avg:58.52ms
step:999/2330 train_time:58458ms step_avg:58.52ms
step:1000/2330 train_time:58518ms step_avg:58.52ms
step:1000/2330 val_loss:4.3124 train_time:58596ms step_avg:58.60ms
step:1001/2330 train_time:58614ms step_avg:58.56ms
step:1002/2330 train_time:58637ms step_avg:58.52ms
step:1003/2330 train_time:58693ms step_avg:58.52ms
step:1004/2330 train_time:58758ms step_avg:58.52ms
step:1005/2330 train_time:58814ms step_avg:58.52ms
step:1006/2330 train_time:58879ms step_avg:58.53ms
step:1007/2330 train_time:58935ms step_avg:58.53ms
step:1008/2330 train_time:58997ms step_avg:58.53ms
step:1009/2330 train_time:59054ms step_avg:58.53ms
step:1010/2330 train_time:59115ms step_avg:58.53ms
step:1011/2330 train_time:59172ms step_avg:58.53ms
step:1012/2330 train_time:59232ms step_avg:58.53ms
step:1013/2330 train_time:59288ms step_avg:58.53ms
step:1014/2330 train_time:59349ms step_avg:58.53ms
step:1015/2330 train_time:59405ms step_avg:58.53ms
step:1016/2330 train_time:59466ms step_avg:58.53ms
step:1017/2330 train_time:59523ms step_avg:58.53ms
step:1018/2330 train_time:59585ms step_avg:58.53ms
step:1019/2330 train_time:59642ms step_avg:58.53ms
step:1020/2330 train_time:59704ms step_avg:58.53ms
step:1021/2330 train_time:59762ms step_avg:58.53ms
step:1022/2330 train_time:59824ms step_avg:58.54ms
step:1023/2330 train_time:59880ms step_avg:58.53ms
step:1024/2330 train_time:59944ms step_avg:58.54ms
step:1025/2330 train_time:60000ms step_avg:58.54ms
step:1026/2330 train_time:60063ms step_avg:58.54ms
step:1027/2330 train_time:60119ms step_avg:58.54ms
step:1028/2330 train_time:60181ms step_avg:58.54ms
step:1029/2330 train_time:60238ms step_avg:58.54ms
step:1030/2330 train_time:60299ms step_avg:58.54ms
step:1031/2330 train_time:60356ms step_avg:58.54ms
step:1032/2330 train_time:60417ms step_avg:58.54ms
step:1033/2330 train_time:60474ms step_avg:58.54ms
step:1034/2330 train_time:60535ms step_avg:58.54ms
step:1035/2330 train_time:60592ms step_avg:58.54ms
step:1036/2330 train_time:60654ms step_avg:58.55ms
step:1037/2330 train_time:60712ms step_avg:58.55ms
step:1038/2330 train_time:60773ms step_avg:58.55ms
step:1039/2330 train_time:60830ms step_avg:58.55ms
step:1040/2330 train_time:60890ms step_avg:58.55ms
step:1041/2330 train_time:60947ms step_avg:58.55ms
step:1042/2330 train_time:61008ms step_avg:58.55ms
step:1043/2330 train_time:61065ms step_avg:58.55ms
step:1044/2330 train_time:61125ms step_avg:58.55ms
step:1045/2330 train_time:61181ms step_avg:58.55ms
step:1046/2330 train_time:61244ms step_avg:58.55ms
step:1047/2330 train_time:61300ms step_avg:58.55ms
step:1048/2330 train_time:61362ms step_avg:58.55ms
step:1049/2330 train_time:61419ms step_avg:58.55ms
step:1050/2330 train_time:61481ms step_avg:58.55ms
step:1051/2330 train_time:61538ms step_avg:58.55ms
step:1052/2330 train_time:61599ms step_avg:58.55ms
step:1053/2330 train_time:61656ms step_avg:58.55ms
step:1054/2330 train_time:61719ms step_avg:58.56ms
step:1055/2330 train_time:61776ms step_avg:58.56ms
step:1056/2330 train_time:61839ms step_avg:58.56ms
step:1057/2330 train_time:61896ms step_avg:58.56ms
step:1058/2330 train_time:61959ms step_avg:58.56ms
step:1059/2330 train_time:62017ms step_avg:58.56ms
step:1060/2330 train_time:62077ms step_avg:58.56ms
step:1061/2330 train_time:62134ms step_avg:58.56ms
step:1062/2330 train_time:62196ms step_avg:58.56ms
step:1063/2330 train_time:62253ms step_avg:58.56ms
step:1064/2330 train_time:62314ms step_avg:58.57ms
step:1065/2330 train_time:62370ms step_avg:58.56ms
step:1066/2330 train_time:62430ms step_avg:58.57ms
step:1067/2330 train_time:62487ms step_avg:58.56ms
step:1068/2330 train_time:62548ms step_avg:58.57ms
step:1069/2330 train_time:62604ms step_avg:58.56ms
step:1070/2330 train_time:62666ms step_avg:58.57ms
step:1071/2330 train_time:62722ms step_avg:58.56ms
step:1072/2330 train_time:62784ms step_avg:58.57ms
step:1073/2330 train_time:62840ms step_avg:58.56ms
step:1074/2330 train_time:62904ms step_avg:58.57ms
step:1075/2330 train_time:62961ms step_avg:58.57ms
step:1076/2330 train_time:63023ms step_avg:58.57ms
step:1077/2330 train_time:63080ms step_avg:58.57ms
step:1078/2330 train_time:63142ms step_avg:58.57ms
step:1079/2330 train_time:63198ms step_avg:58.57ms
step:1080/2330 train_time:63260ms step_avg:58.57ms
step:1081/2330 train_time:63317ms step_avg:58.57ms
step:1082/2330 train_time:63379ms step_avg:58.58ms
step:1083/2330 train_time:63436ms step_avg:58.57ms
step:1084/2330 train_time:63499ms step_avg:58.58ms
step:1085/2330 train_time:63556ms step_avg:58.58ms
step:1086/2330 train_time:63618ms step_avg:58.58ms
step:1087/2330 train_time:63675ms step_avg:58.58ms
step:1088/2330 train_time:63736ms step_avg:58.58ms
step:1089/2330 train_time:63793ms step_avg:58.58ms
step:1090/2330 train_time:63855ms step_avg:58.58ms
step:1091/2330 train_time:63912ms step_avg:58.58ms
step:1092/2330 train_time:63973ms step_avg:58.58ms
step:1093/2330 train_time:64030ms step_avg:58.58ms
step:1094/2330 train_time:64091ms step_avg:58.58ms
step:1095/2330 train_time:64148ms step_avg:58.58ms
step:1096/2330 train_time:64209ms step_avg:58.58ms
step:1097/2330 train_time:64265ms step_avg:58.58ms
step:1098/2330 train_time:64327ms step_avg:58.59ms
step:1099/2330 train_time:64383ms step_avg:58.58ms
step:1100/2330 train_time:64445ms step_avg:58.59ms
step:1101/2330 train_time:64501ms step_avg:58.58ms
step:1102/2330 train_time:64563ms step_avg:58.59ms
step:1103/2330 train_time:64619ms step_avg:58.58ms
step:1104/2330 train_time:64682ms step_avg:58.59ms
step:1105/2330 train_time:64738ms step_avg:58.59ms
step:1106/2330 train_time:64801ms step_avg:58.59ms
step:1107/2330 train_time:64858ms step_avg:58.59ms
step:1108/2330 train_time:64920ms step_avg:58.59ms
step:1109/2330 train_time:64977ms step_avg:58.59ms
step:1110/2330 train_time:65039ms step_avg:58.59ms
step:1111/2330 train_time:65097ms step_avg:58.59ms
step:1112/2330 train_time:65158ms step_avg:58.60ms
step:1113/2330 train_time:65216ms step_avg:58.59ms
step:1114/2330 train_time:65277ms step_avg:58.60ms
step:1115/2330 train_time:65333ms step_avg:58.60ms
step:1116/2330 train_time:65396ms step_avg:58.60ms
step:1117/2330 train_time:65453ms step_avg:58.60ms
step:1118/2330 train_time:65513ms step_avg:58.60ms
step:1119/2330 train_time:65570ms step_avg:58.60ms
step:1120/2330 train_time:65631ms step_avg:58.60ms
step:1121/2330 train_time:65688ms step_avg:58.60ms
step:1122/2330 train_time:65749ms step_avg:58.60ms
step:1123/2330 train_time:65806ms step_avg:58.60ms
step:1124/2330 train_time:65867ms step_avg:58.60ms
step:1125/2330 train_time:65923ms step_avg:58.60ms
step:1126/2330 train_time:65985ms step_avg:58.60ms
step:1127/2330 train_time:66042ms step_avg:58.60ms
step:1128/2330 train_time:66105ms step_avg:58.60ms
step:1129/2330 train_time:66161ms step_avg:58.60ms
step:1130/2330 train_time:66224ms step_avg:58.60ms
step:1131/2330 train_time:66279ms step_avg:58.60ms
step:1132/2330 train_time:66342ms step_avg:58.61ms
step:1133/2330 train_time:66399ms step_avg:58.60ms
step:1134/2330 train_time:66461ms step_avg:58.61ms
step:1135/2330 train_time:66518ms step_avg:58.61ms
step:1136/2330 train_time:66580ms step_avg:58.61ms
step:1137/2330 train_time:66637ms step_avg:58.61ms
step:1138/2330 train_time:66698ms step_avg:58.61ms
step:1139/2330 train_time:66755ms step_avg:58.61ms
step:1140/2330 train_time:66818ms step_avg:58.61ms
step:1141/2330 train_time:66875ms step_avg:58.61ms
step:1142/2330 train_time:66936ms step_avg:58.61ms
step:1143/2330 train_time:66993ms step_avg:58.61ms
step:1144/2330 train_time:67054ms step_avg:58.61ms
step:1145/2330 train_time:67112ms step_avg:58.61ms
step:1146/2330 train_time:67173ms step_avg:58.61ms
step:1147/2330 train_time:67230ms step_avg:58.61ms
step:1148/2330 train_time:67290ms step_avg:58.62ms
step:1149/2330 train_time:67348ms step_avg:58.61ms
step:1150/2330 train_time:67408ms step_avg:58.62ms
step:1151/2330 train_time:67465ms step_avg:58.61ms
step:1152/2330 train_time:67526ms step_avg:58.62ms
step:1153/2330 train_time:67582ms step_avg:58.61ms
step:1154/2330 train_time:67644ms step_avg:58.62ms
step:1155/2330 train_time:67701ms step_avg:58.62ms
step:1156/2330 train_time:67763ms step_avg:58.62ms
step:1157/2330 train_time:67819ms step_avg:58.62ms
step:1158/2330 train_time:67882ms step_avg:58.62ms
step:1159/2330 train_time:67938ms step_avg:58.62ms
step:1160/2330 train_time:68001ms step_avg:58.62ms
step:1161/2330 train_time:68058ms step_avg:58.62ms
step:1162/2330 train_time:68121ms step_avg:58.62ms
step:1163/2330 train_time:68178ms step_avg:58.62ms
step:1164/2330 train_time:68240ms step_avg:58.63ms
step:1165/2330 train_time:68297ms step_avg:58.62ms
step:1166/2330 train_time:68358ms step_avg:58.63ms
step:1167/2330 train_time:68416ms step_avg:58.63ms
step:1168/2330 train_time:68477ms step_avg:58.63ms
step:1169/2330 train_time:68535ms step_avg:58.63ms
step:1170/2330 train_time:68596ms step_avg:58.63ms
step:1171/2330 train_time:68654ms step_avg:58.63ms
step:1172/2330 train_time:68715ms step_avg:58.63ms
step:1173/2330 train_time:68772ms step_avg:58.63ms
step:1174/2330 train_time:68832ms step_avg:58.63ms
step:1175/2330 train_time:68888ms step_avg:58.63ms
step:1176/2330 train_time:68949ms step_avg:58.63ms
step:1177/2330 train_time:69006ms step_avg:58.63ms
step:1178/2330 train_time:69067ms step_avg:58.63ms
step:1179/2330 train_time:69124ms step_avg:58.63ms
step:1180/2330 train_time:69185ms step_avg:58.63ms
step:1181/2330 train_time:69241ms step_avg:58.63ms
step:1182/2330 train_time:69304ms step_avg:58.63ms
step:1183/2330 train_time:69360ms step_avg:58.63ms
step:1184/2330 train_time:69423ms step_avg:58.63ms
step:1185/2330 train_time:69479ms step_avg:58.63ms
step:1186/2330 train_time:69541ms step_avg:58.64ms
step:1187/2330 train_time:69598ms step_avg:58.63ms
step:1188/2330 train_time:69661ms step_avg:58.64ms
step:1189/2330 train_time:69719ms step_avg:58.64ms
step:1190/2330 train_time:69782ms step_avg:58.64ms
step:1191/2330 train_time:69838ms step_avg:58.64ms
step:1192/2330 train_time:69901ms step_avg:58.64ms
step:1193/2330 train_time:69958ms step_avg:58.64ms
step:1194/2330 train_time:70020ms step_avg:58.64ms
step:1195/2330 train_time:70076ms step_avg:58.64ms
step:1196/2330 train_time:70137ms step_avg:58.64ms
step:1197/2330 train_time:70194ms step_avg:58.64ms
step:1198/2330 train_time:70256ms step_avg:58.64ms
step:1199/2330 train_time:70314ms step_avg:58.64ms
step:1200/2330 train_time:70376ms step_avg:58.65ms
step:1201/2330 train_time:70432ms step_avg:58.64ms
step:1202/2330 train_time:70494ms step_avg:58.65ms
step:1203/2330 train_time:70550ms step_avg:58.65ms
step:1204/2330 train_time:70611ms step_avg:58.65ms
step:1205/2330 train_time:70668ms step_avg:58.65ms
step:1206/2330 train_time:70730ms step_avg:58.65ms
step:1207/2330 train_time:70786ms step_avg:58.65ms
step:1208/2330 train_time:70848ms step_avg:58.65ms
step:1209/2330 train_time:70905ms step_avg:58.65ms
step:1210/2330 train_time:70966ms step_avg:58.65ms
step:1211/2330 train_time:71023ms step_avg:58.65ms
step:1212/2330 train_time:71085ms step_avg:58.65ms
step:1213/2330 train_time:71141ms step_avg:58.65ms
step:1214/2330 train_time:71203ms step_avg:58.65ms
step:1215/2330 train_time:71259ms step_avg:58.65ms
step:1216/2330 train_time:71323ms step_avg:58.65ms
step:1217/2330 train_time:71379ms step_avg:58.65ms
step:1218/2330 train_time:71442ms step_avg:58.65ms
step:1219/2330 train_time:71498ms step_avg:58.65ms
step:1220/2330 train_time:71561ms step_avg:58.66ms
step:1221/2330 train_time:71617ms step_avg:58.65ms
step:1222/2330 train_time:71680ms step_avg:58.66ms
step:1223/2330 train_time:71737ms step_avg:58.66ms
step:1224/2330 train_time:71799ms step_avg:58.66ms
step:1225/2330 train_time:71857ms step_avg:58.66ms
step:1226/2330 train_time:71917ms step_avg:58.66ms
step:1227/2330 train_time:71975ms step_avg:58.66ms
step:1228/2330 train_time:72035ms step_avg:58.66ms
step:1229/2330 train_time:72092ms step_avg:58.66ms
step:1230/2330 train_time:72153ms step_avg:58.66ms
step:1231/2330 train_time:72209ms step_avg:58.66ms
step:1232/2330 train_time:72270ms step_avg:58.66ms
step:1233/2330 train_time:72327ms step_avg:58.66ms
step:1234/2330 train_time:72388ms step_avg:58.66ms
step:1235/2330 train_time:72445ms step_avg:58.66ms
step:1236/2330 train_time:72506ms step_avg:58.66ms
step:1237/2330 train_time:72562ms step_avg:58.66ms
step:1238/2330 train_time:72624ms step_avg:58.66ms
step:1239/2330 train_time:72681ms step_avg:58.66ms
step:1240/2330 train_time:72744ms step_avg:58.66ms
step:1241/2330 train_time:72800ms step_avg:58.66ms
step:1242/2330 train_time:72863ms step_avg:58.67ms
step:1243/2330 train_time:72920ms step_avg:58.66ms
step:1244/2330 train_time:72982ms step_avg:58.67ms
step:1245/2330 train_time:73039ms step_avg:58.67ms
step:1246/2330 train_time:73102ms step_avg:58.67ms
step:1247/2330 train_time:73159ms step_avg:58.67ms
step:1248/2330 train_time:73221ms step_avg:58.67ms
step:1249/2330 train_time:73277ms step_avg:58.67ms
step:1250/2330 train_time:73339ms step_avg:58.67ms
step:1250/2330 val_loss:4.2120 train_time:73417ms step_avg:58.73ms
step:1251/2330 train_time:73436ms step_avg:58.70ms
step:1252/2330 train_time:73460ms step_avg:58.67ms
step:1253/2330 train_time:73517ms step_avg:58.67ms
step:1254/2330 train_time:73585ms step_avg:58.68ms
step:1255/2330 train_time:73641ms step_avg:58.68ms
step:1256/2330 train_time:73706ms step_avg:58.68ms
step:1257/2330 train_time:73762ms step_avg:58.68ms
step:1258/2330 train_time:73824ms step_avg:58.68ms
step:1259/2330 train_time:73880ms step_avg:58.68ms
step:1260/2330 train_time:73941ms step_avg:58.68ms
step:1261/2330 train_time:73998ms step_avg:58.68ms
step:1262/2330 train_time:74059ms step_avg:58.68ms
step:1263/2330 train_time:74115ms step_avg:58.68ms
step:1264/2330 train_time:74177ms step_avg:58.68ms
step:1265/2330 train_time:74233ms step_avg:58.68ms
step:1266/2330 train_time:74293ms step_avg:58.68ms
step:1267/2330 train_time:74349ms step_avg:58.68ms
step:1268/2330 train_time:74410ms step_avg:58.68ms
step:1269/2330 train_time:74469ms step_avg:58.68ms
step:1270/2330 train_time:74531ms step_avg:58.69ms
step:1271/2330 train_time:74589ms step_avg:58.69ms
step:1272/2330 train_time:74652ms step_avg:58.69ms
step:1273/2330 train_time:74710ms step_avg:58.69ms
step:1274/2330 train_time:74773ms step_avg:58.69ms
step:1275/2330 train_time:74829ms step_avg:58.69ms
step:1276/2330 train_time:74891ms step_avg:58.69ms
step:1277/2330 train_time:74948ms step_avg:58.69ms
step:1278/2330 train_time:75010ms step_avg:58.69ms
step:1279/2330 train_time:75066ms step_avg:58.69ms
step:1280/2330 train_time:75127ms step_avg:58.69ms
step:1281/2330 train_time:75184ms step_avg:58.69ms
step:1282/2330 train_time:75245ms step_avg:58.69ms
step:1283/2330 train_time:75302ms step_avg:58.69ms
step:1284/2330 train_time:75362ms step_avg:58.69ms
step:1285/2330 train_time:75419ms step_avg:58.69ms
step:1286/2330 train_time:75480ms step_avg:58.69ms
step:1287/2330 train_time:75536ms step_avg:58.69ms
step:1288/2330 train_time:75600ms step_avg:58.70ms
step:1289/2330 train_time:75656ms step_avg:58.69ms
step:1290/2330 train_time:75720ms step_avg:58.70ms
step:1291/2330 train_time:75776ms step_avg:58.70ms
step:1292/2330 train_time:75839ms step_avg:58.70ms
step:1293/2330 train_time:75896ms step_avg:58.70ms
step:1294/2330 train_time:75958ms step_avg:58.70ms
step:1295/2330 train_time:76015ms step_avg:58.70ms
step:1296/2330 train_time:76077ms step_avg:58.70ms
step:1297/2330 train_time:76134ms step_avg:58.70ms
step:1298/2330 train_time:76196ms step_avg:58.70ms
step:1299/2330 train_time:76253ms step_avg:58.70ms
step:1300/2330 train_time:76314ms step_avg:58.70ms
step:1301/2330 train_time:76371ms step_avg:58.70ms
step:1302/2330 train_time:76432ms step_avg:58.70ms
step:1303/2330 train_time:76489ms step_avg:58.70ms
step:1304/2330 train_time:76550ms step_avg:58.70ms
step:1305/2330 train_time:76607ms step_avg:58.70ms
step:1306/2330 train_time:76669ms step_avg:58.71ms
step:1307/2330 train_time:76726ms step_avg:58.70ms
step:1308/2330 train_time:76786ms step_avg:58.71ms
step:1309/2330 train_time:76843ms step_avg:58.70ms
step:1310/2330 train_time:76905ms step_avg:58.71ms
step:1311/2330 train_time:76961ms step_avg:58.70ms
step:1312/2330 train_time:77022ms step_avg:58.71ms
step:1313/2330 train_time:77078ms step_avg:58.70ms
step:1314/2330 train_time:77140ms step_avg:58.71ms
step:1315/2330 train_time:77197ms step_avg:58.70ms
step:1316/2330 train_time:77259ms step_avg:58.71ms
step:1317/2330 train_time:77315ms step_avg:58.71ms
step:1318/2330 train_time:77377ms step_avg:58.71ms
step:1319/2330 train_time:77433ms step_avg:58.71ms
step:1320/2330 train_time:77496ms step_avg:58.71ms
step:1321/2330 train_time:77553ms step_avg:58.71ms
step:1322/2330 train_time:77616ms step_avg:58.71ms
step:1323/2330 train_time:77673ms step_avg:58.71ms
step:1324/2330 train_time:77735ms step_avg:58.71ms
step:1325/2330 train_time:77792ms step_avg:58.71ms
step:1326/2330 train_time:77856ms step_avg:58.71ms
step:1327/2330 train_time:77913ms step_avg:58.71ms
step:1328/2330 train_time:77974ms step_avg:58.72ms
step:1329/2330 train_time:78031ms step_avg:58.71ms
step:1330/2330 train_time:78092ms step_avg:58.72ms
step:1331/2330 train_time:78149ms step_avg:58.71ms
step:1332/2330 train_time:78210ms step_avg:58.72ms
step:1333/2330 train_time:78267ms step_avg:58.71ms
step:1334/2330 train_time:78328ms step_avg:58.72ms
step:1335/2330 train_time:78385ms step_avg:58.72ms
step:1336/2330 train_time:78446ms step_avg:58.72ms
step:1337/2330 train_time:78502ms step_avg:58.72ms
step:1338/2330 train_time:78564ms step_avg:58.72ms
step:1339/2330 train_time:78620ms step_avg:58.72ms
step:1340/2330 train_time:78681ms step_avg:58.72ms
step:1341/2330 train_time:78737ms step_avg:58.72ms
step:1342/2330 train_time:78800ms step_avg:58.72ms
step:1343/2330 train_time:78856ms step_avg:58.72ms
step:1344/2330 train_time:78919ms step_avg:58.72ms
step:1345/2330 train_time:78975ms step_avg:58.72ms
step:1346/2330 train_time:79037ms step_avg:58.72ms
step:1347/2330 train_time:79093ms step_avg:58.72ms
step:1348/2330 train_time:79156ms step_avg:58.72ms
step:1349/2330 train_time:79213ms step_avg:58.72ms
step:1350/2330 train_time:79275ms step_avg:58.72ms
step:1351/2330 train_time:79332ms step_avg:58.72ms
step:1352/2330 train_time:79394ms step_avg:58.72ms
step:1353/2330 train_time:79451ms step_avg:58.72ms
step:1354/2330 train_time:79512ms step_avg:58.72ms
step:1355/2330 train_time:79569ms step_avg:58.72ms
step:1356/2330 train_time:79631ms step_avg:58.72ms
step:1357/2330 train_time:79687ms step_avg:58.72ms
step:1358/2330 train_time:79749ms step_avg:58.73ms
step:1359/2330 train_time:79806ms step_avg:58.72ms
step:1360/2330 train_time:79869ms step_avg:58.73ms
step:1361/2330 train_time:79926ms step_avg:58.73ms
step:1362/2330 train_time:79987ms step_avg:58.73ms
step:1363/2330 train_time:80043ms step_avg:58.73ms
step:1364/2330 train_time:80105ms step_avg:58.73ms
step:1365/2330 train_time:80161ms step_avg:58.73ms
step:1366/2330 train_time:80223ms step_avg:58.73ms
step:1367/2330 train_time:80279ms step_avg:58.73ms
step:1368/2330 train_time:80341ms step_avg:58.73ms
step:1369/2330 train_time:80397ms step_avg:58.73ms
step:1370/2330 train_time:80459ms step_avg:58.73ms
step:1371/2330 train_time:80516ms step_avg:58.73ms
step:1372/2330 train_time:80579ms step_avg:58.73ms
step:1373/2330 train_time:80635ms step_avg:58.73ms
step:1374/2330 train_time:80698ms step_avg:58.73ms
step:1375/2330 train_time:80755ms step_avg:58.73ms
step:1376/2330 train_time:80817ms step_avg:58.73ms
step:1377/2330 train_time:80874ms step_avg:58.73ms
step:1378/2330 train_time:80936ms step_avg:58.73ms
step:1379/2330 train_time:80993ms step_avg:58.73ms
step:1380/2330 train_time:81055ms step_avg:58.74ms
step:1381/2330 train_time:81111ms step_avg:58.73ms
step:1382/2330 train_time:81174ms step_avg:58.74ms
step:1383/2330 train_time:81231ms step_avg:58.74ms
step:1384/2330 train_time:81292ms step_avg:58.74ms
step:1385/2330 train_time:81349ms step_avg:58.74ms
step:1386/2330 train_time:81410ms step_avg:58.74ms
step:1387/2330 train_time:81467ms step_avg:58.74ms
step:1388/2330 train_time:81527ms step_avg:58.74ms
step:1389/2330 train_time:81584ms step_avg:58.74ms
step:1390/2330 train_time:81645ms step_avg:58.74ms
step:1391/2330 train_time:81702ms step_avg:58.74ms
step:1392/2330 train_time:81763ms step_avg:58.74ms
step:1393/2330 train_time:81819ms step_avg:58.74ms
step:1394/2330 train_time:81882ms step_avg:58.74ms
step:1395/2330 train_time:81939ms step_avg:58.74ms
step:1396/2330 train_time:82001ms step_avg:58.74ms
step:1397/2330 train_time:82057ms step_avg:58.74ms
step:1398/2330 train_time:82120ms step_avg:58.74ms
step:1399/2330 train_time:82176ms step_avg:58.74ms
step:1400/2330 train_time:82239ms step_avg:58.74ms
step:1401/2330 train_time:82295ms step_avg:58.74ms
step:1402/2330 train_time:82358ms step_avg:58.74ms
step:1403/2330 train_time:82415ms step_avg:58.74ms
step:1404/2330 train_time:82477ms step_avg:58.74ms
step:1405/2330 train_time:82534ms step_avg:58.74ms
step:1406/2330 train_time:82596ms step_avg:58.75ms
step:1407/2330 train_time:82653ms step_avg:58.74ms
step:1408/2330 train_time:82714ms step_avg:58.75ms
step:1409/2330 train_time:82772ms step_avg:58.75ms
step:1410/2330 train_time:82834ms step_avg:58.75ms
step:1411/2330 train_time:82891ms step_avg:58.75ms
step:1412/2330 train_time:82952ms step_avg:58.75ms
step:1413/2330 train_time:83009ms step_avg:58.75ms
step:1414/2330 train_time:83071ms step_avg:58.75ms
step:1415/2330 train_time:83127ms step_avg:58.75ms
step:1416/2330 train_time:83189ms step_avg:58.75ms
step:1417/2330 train_time:83246ms step_avg:58.75ms
step:1418/2330 train_time:83308ms step_avg:58.75ms
step:1419/2330 train_time:83364ms step_avg:58.75ms
step:1420/2330 train_time:83426ms step_avg:58.75ms
step:1421/2330 train_time:83482ms step_avg:58.75ms
step:1422/2330 train_time:83543ms step_avg:58.75ms
step:1423/2330 train_time:83600ms step_avg:58.75ms
step:1424/2330 train_time:83662ms step_avg:58.75ms
step:1425/2330 train_time:83717ms step_avg:58.75ms
step:1426/2330 train_time:83780ms step_avg:58.75ms
step:1427/2330 train_time:83836ms step_avg:58.75ms
step:1428/2330 train_time:83898ms step_avg:58.75ms
step:1429/2330 train_time:83955ms step_avg:58.75ms
step:1430/2330 train_time:84017ms step_avg:58.75ms
step:1431/2330 train_time:84074ms step_avg:58.75ms
step:1432/2330 train_time:84136ms step_avg:58.75ms
step:1433/2330 train_time:84193ms step_avg:58.75ms
step:1434/2330 train_time:84255ms step_avg:58.76ms
step:1435/2330 train_time:84313ms step_avg:58.75ms
step:1436/2330 train_time:84374ms step_avg:58.76ms
step:1437/2330 train_time:84432ms step_avg:58.76ms
step:1438/2330 train_time:84493ms step_avg:58.76ms
step:1439/2330 train_time:84550ms step_avg:58.76ms
step:1440/2330 train_time:84612ms step_avg:58.76ms
step:1441/2330 train_time:84669ms step_avg:58.76ms
step:1442/2330 train_time:84730ms step_avg:58.76ms
step:1443/2330 train_time:84787ms step_avg:58.76ms
step:1444/2330 train_time:84849ms step_avg:58.76ms
step:1445/2330 train_time:84905ms step_avg:58.76ms
step:1446/2330 train_time:84967ms step_avg:58.76ms
step:1447/2330 train_time:85023ms step_avg:58.76ms
step:1448/2330 train_time:85085ms step_avg:58.76ms
step:1449/2330 train_time:85141ms step_avg:58.76ms
step:1450/2330 train_time:85203ms step_avg:58.76ms
step:1451/2330 train_time:85260ms step_avg:58.76ms
step:1452/2330 train_time:85321ms step_avg:58.76ms
step:1453/2330 train_time:85378ms step_avg:58.76ms
step:1454/2330 train_time:85440ms step_avg:58.76ms
step:1455/2330 train_time:85497ms step_avg:58.76ms
step:1456/2330 train_time:85559ms step_avg:58.76ms
step:1457/2330 train_time:85615ms step_avg:58.76ms
step:1458/2330 train_time:85677ms step_avg:58.76ms
step:1459/2330 train_time:85734ms step_avg:58.76ms
step:1460/2330 train_time:85796ms step_avg:58.76ms
step:1461/2330 train_time:85853ms step_avg:58.76ms
step:1462/2330 train_time:85916ms step_avg:58.77ms
step:1463/2330 train_time:85974ms step_avg:58.77ms
step:1464/2330 train_time:86035ms step_avg:58.77ms
step:1465/2330 train_time:86092ms step_avg:58.77ms
step:1466/2330 train_time:86155ms step_avg:58.77ms
step:1467/2330 train_time:86211ms step_avg:58.77ms
step:1468/2330 train_time:86272ms step_avg:58.77ms
step:1469/2330 train_time:86329ms step_avg:58.77ms
step:1470/2330 train_time:86391ms step_avg:58.77ms
step:1471/2330 train_time:86448ms step_avg:58.77ms
step:1472/2330 train_time:86508ms step_avg:58.77ms
step:1473/2330 train_time:86565ms step_avg:58.77ms
step:1474/2330 train_time:86625ms step_avg:58.77ms
step:1475/2330 train_time:86681ms step_avg:58.77ms
step:1476/2330 train_time:86743ms step_avg:58.77ms
step:1477/2330 train_time:86800ms step_avg:58.77ms
step:1478/2330 train_time:86862ms step_avg:58.77ms
step:1479/2330 train_time:86918ms step_avg:58.77ms
step:1480/2330 train_time:86981ms step_avg:58.77ms
step:1481/2330 train_time:87037ms step_avg:58.77ms
step:1482/2330 train_time:87099ms step_avg:58.77ms
step:1483/2330 train_time:87155ms step_avg:58.77ms
step:1484/2330 train_time:87216ms step_avg:58.77ms
step:1485/2330 train_time:87273ms step_avg:58.77ms
step:1486/2330 train_time:87335ms step_avg:58.77ms
step:1487/2330 train_time:87392ms step_avg:58.77ms
step:1488/2330 train_time:87455ms step_avg:58.77ms
step:1489/2330 train_time:87512ms step_avg:58.77ms
step:1490/2330 train_time:87574ms step_avg:58.77ms
step:1491/2330 train_time:87631ms step_avg:58.77ms
step:1492/2330 train_time:87693ms step_avg:58.78ms
step:1493/2330 train_time:87750ms step_avg:58.77ms
step:1494/2330 train_time:87810ms step_avg:58.78ms
step:1495/2330 train_time:87868ms step_avg:58.77ms
step:1496/2330 train_time:87930ms step_avg:58.78ms
step:1497/2330 train_time:87986ms step_avg:58.78ms
step:1498/2330 train_time:88048ms step_avg:58.78ms
step:1499/2330 train_time:88104ms step_avg:58.78ms
step:1500/2330 train_time:88165ms step_avg:58.78ms
step:1500/2330 val_loss:4.1243 train_time:88242ms step_avg:58.83ms
step:1501/2330 train_time:88264ms step_avg:58.80ms
step:1502/2330 train_time:88285ms step_avg:58.78ms
step:1503/2330 train_time:88344ms step_avg:58.78ms
step:1504/2330 train_time:88408ms step_avg:58.78ms
step:1505/2330 train_time:88465ms step_avg:58.78ms
step:1506/2330 train_time:88529ms step_avg:58.78ms
step:1507/2330 train_time:88586ms step_avg:58.78ms
step:1508/2330 train_time:88648ms step_avg:58.79ms
step:1509/2330 train_time:88704ms step_avg:58.78ms
step:1510/2330 train_time:88765ms step_avg:58.79ms
step:1511/2330 train_time:88822ms step_avg:58.78ms
step:1512/2330 train_time:88883ms step_avg:58.78ms
step:1513/2330 train_time:88939ms step_avg:58.78ms
step:1514/2330 train_time:88999ms step_avg:58.78ms
step:1515/2330 train_time:89055ms step_avg:58.78ms
step:1516/2330 train_time:89116ms step_avg:58.78ms
step:1517/2330 train_time:89172ms step_avg:58.78ms
step:1518/2330 train_time:89234ms step_avg:58.78ms
step:1519/2330 train_time:89291ms step_avg:58.78ms
step:1520/2330 train_time:89356ms step_avg:58.79ms
step:1521/2330 train_time:89412ms step_avg:58.79ms
step:1522/2330 train_time:89477ms step_avg:58.79ms
step:1523/2330 train_time:89533ms step_avg:58.79ms
step:1524/2330 train_time:89597ms step_avg:58.79ms
step:1525/2330 train_time:89653ms step_avg:58.79ms
step:1526/2330 train_time:89716ms step_avg:58.79ms
step:1527/2330 train_time:89772ms step_avg:58.79ms
step:1528/2330 train_time:89835ms step_avg:58.79ms
step:1529/2330 train_time:89892ms step_avg:58.79ms
step:1530/2330 train_time:89953ms step_avg:58.79ms
step:1531/2330 train_time:90010ms step_avg:58.79ms
step:1532/2330 train_time:90072ms step_avg:58.79ms
step:1533/2330 train_time:90129ms step_avg:58.79ms
step:1534/2330 train_time:90191ms step_avg:58.79ms
step:1535/2330 train_time:90249ms step_avg:58.79ms
step:1536/2330 train_time:90311ms step_avg:58.80ms
step:1537/2330 train_time:90368ms step_avg:58.80ms
step:1538/2330 train_time:90432ms step_avg:58.80ms
step:1539/2330 train_time:90489ms step_avg:58.80ms
step:1540/2330 train_time:90553ms step_avg:58.80ms
step:1541/2330 train_time:90611ms step_avg:58.80ms
step:1542/2330 train_time:90674ms step_avg:58.80ms
step:1543/2330 train_time:90730ms step_avg:58.80ms
step:1544/2330 train_time:90793ms step_avg:58.80ms
step:1545/2330 train_time:90850ms step_avg:58.80ms
step:1546/2330 train_time:90913ms step_avg:58.81ms
step:1547/2330 train_time:90970ms step_avg:58.80ms
step:1548/2330 train_time:91032ms step_avg:58.81ms
step:1549/2330 train_time:91089ms step_avg:58.81ms
step:1550/2330 train_time:91151ms step_avg:58.81ms
step:1551/2330 train_time:91209ms step_avg:58.81ms
step:1552/2330 train_time:91272ms step_avg:58.81ms
step:1553/2330 train_time:91330ms step_avg:58.81ms
step:1554/2330 train_time:91394ms step_avg:58.81ms
step:1555/2330 train_time:91451ms step_avg:58.81ms
step:1556/2330 train_time:91514ms step_avg:58.81ms
step:1557/2330 train_time:91571ms step_avg:58.81ms
step:1558/2330 train_time:91635ms step_avg:58.82ms
step:1559/2330 train_time:91691ms step_avg:58.81ms
step:1560/2330 train_time:91755ms step_avg:58.82ms
step:1561/2330 train_time:91812ms step_avg:58.82ms
step:1562/2330 train_time:91875ms step_avg:58.82ms
step:1563/2330 train_time:91932ms step_avg:58.82ms
step:1564/2330 train_time:91994ms step_avg:58.82ms
step:1565/2330 train_time:92050ms step_avg:58.82ms
step:1566/2330 train_time:92113ms step_avg:58.82ms
step:1567/2330 train_time:92169ms step_avg:58.82ms
step:1568/2330 train_time:92232ms step_avg:58.82ms
step:1569/2330 train_time:92289ms step_avg:58.82ms
step:1570/2330 train_time:92353ms step_avg:58.82ms
step:1571/2330 train_time:92411ms step_avg:58.82ms
step:1572/2330 train_time:92474ms step_avg:58.83ms
step:1573/2330 train_time:92531ms step_avg:58.82ms
step:1574/2330 train_time:92594ms step_avg:58.83ms
step:1575/2330 train_time:92651ms step_avg:58.83ms
step:1576/2330 train_time:92714ms step_avg:58.83ms
step:1577/2330 train_time:92772ms step_avg:58.83ms
step:1578/2330 train_time:92835ms step_avg:58.83ms
step:1579/2330 train_time:92891ms step_avg:58.83ms
step:1580/2330 train_time:92954ms step_avg:58.83ms
step:1581/2330 train_time:93011ms step_avg:58.83ms
step:1582/2330 train_time:93073ms step_avg:58.83ms
step:1583/2330 train_time:93130ms step_avg:58.83ms
step:1584/2330 train_time:93193ms step_avg:58.83ms
step:1585/2330 train_time:93250ms step_avg:58.83ms
step:1586/2330 train_time:93312ms step_avg:58.83ms
step:1587/2330 train_time:93369ms step_avg:58.83ms
step:1588/2330 train_time:93434ms step_avg:58.84ms
step:1589/2330 train_time:93491ms step_avg:58.84ms
step:1590/2330 train_time:93554ms step_avg:58.84ms
step:1591/2330 train_time:93611ms step_avg:58.84ms
step:1592/2330 train_time:93674ms step_avg:58.84ms
step:1593/2330 train_time:93731ms step_avg:58.84ms
step:1594/2330 train_time:93794ms step_avg:58.84ms
step:1595/2330 train_time:93852ms step_avg:58.84ms
step:1596/2330 train_time:93914ms step_avg:58.84ms
step:1597/2330 train_time:93971ms step_avg:58.84ms
step:1598/2330 train_time:94034ms step_avg:58.84ms
step:1599/2330 train_time:94090ms step_avg:58.84ms
step:1600/2330 train_time:94155ms step_avg:58.85ms
step:1601/2330 train_time:94212ms step_avg:58.85ms
step:1602/2330 train_time:94274ms step_avg:58.85ms
step:1603/2330 train_time:94331ms step_avg:58.85ms
step:1604/2330 train_time:94394ms step_avg:58.85ms
step:1605/2330 train_time:94451ms step_avg:58.85ms
step:1606/2330 train_time:94513ms step_avg:58.85ms
step:1607/2330 train_time:94570ms step_avg:58.85ms
step:1608/2330 train_time:94633ms step_avg:58.85ms
step:1609/2330 train_time:94691ms step_avg:58.85ms
step:1610/2330 train_time:94754ms step_avg:58.85ms
step:1611/2330 train_time:94811ms step_avg:58.85ms
step:1612/2330 train_time:94874ms step_avg:58.85ms
step:1613/2330 train_time:94931ms step_avg:58.85ms
step:1614/2330 train_time:94993ms step_avg:58.86ms
step:1615/2330 train_time:95051ms step_avg:58.86ms
step:1616/2330 train_time:95113ms step_avg:58.86ms
step:1617/2330 train_time:95170ms step_avg:58.86ms
step:1618/2330 train_time:95233ms step_avg:58.86ms
step:1619/2330 train_time:95290ms step_avg:58.86ms
step:1620/2330 train_time:95353ms step_avg:58.86ms
step:1621/2330 train_time:95410ms step_avg:58.86ms
step:1622/2330 train_time:95472ms step_avg:58.86ms
step:1623/2330 train_time:95529ms step_avg:58.86ms
step:1624/2330 train_time:95593ms step_avg:58.86ms
step:1625/2330 train_time:95651ms step_avg:58.86ms
step:1626/2330 train_time:95713ms step_avg:58.86ms
step:1627/2330 train_time:95770ms step_avg:58.86ms
step:1628/2330 train_time:95834ms step_avg:58.87ms
step:1629/2330 train_time:95891ms step_avg:58.87ms
step:1630/2330 train_time:95954ms step_avg:58.87ms
step:1631/2330 train_time:96011ms step_avg:58.87ms
step:1632/2330 train_time:96073ms step_avg:58.87ms
step:1633/2330 train_time:96130ms step_avg:58.87ms
step:1634/2330 train_time:96194ms step_avg:58.87ms
step:1635/2330 train_time:96251ms step_avg:58.87ms
step:1636/2330 train_time:96313ms step_avg:58.87ms
step:1637/2330 train_time:96370ms step_avg:58.87ms
step:1638/2330 train_time:96433ms step_avg:58.87ms
step:1639/2330 train_time:96490ms step_avg:58.87ms
step:1640/2330 train_time:96553ms step_avg:58.87ms
step:1641/2330 train_time:96610ms step_avg:58.87ms
step:1642/2330 train_time:96673ms step_avg:58.88ms
step:1643/2330 train_time:96730ms step_avg:58.87ms
step:1644/2330 train_time:96793ms step_avg:58.88ms
step:1645/2330 train_time:96850ms step_avg:58.88ms
step:1646/2330 train_time:96913ms step_avg:58.88ms
step:1647/2330 train_time:96970ms step_avg:58.88ms
step:1648/2330 train_time:97034ms step_avg:58.88ms
step:1649/2330 train_time:97091ms step_avg:58.88ms
step:1650/2330 train_time:97154ms step_avg:58.88ms
step:1651/2330 train_time:97211ms step_avg:58.88ms
step:1652/2330 train_time:97274ms step_avg:58.88ms
step:1653/2330 train_time:97331ms step_avg:58.88ms
step:1654/2330 train_time:97394ms step_avg:58.88ms
step:1655/2330 train_time:97451ms step_avg:58.88ms
step:1656/2330 train_time:97513ms step_avg:58.88ms
step:1657/2330 train_time:97570ms step_avg:58.88ms
step:1658/2330 train_time:97633ms step_avg:58.89ms
step:1659/2330 train_time:97690ms step_avg:58.88ms
step:1660/2330 train_time:97753ms step_avg:58.89ms
step:1661/2330 train_time:97810ms step_avg:58.89ms
step:1662/2330 train_time:97873ms step_avg:58.89ms
step:1663/2330 train_time:97930ms step_avg:58.89ms
step:1664/2330 train_time:97992ms step_avg:58.89ms
step:1665/2330 train_time:98049ms step_avg:58.89ms
step:1666/2330 train_time:98112ms step_avg:58.89ms
step:1667/2330 train_time:98170ms step_avg:58.89ms
step:1668/2330 train_time:98233ms step_avg:58.89ms
step:1669/2330 train_time:98290ms step_avg:58.89ms
step:1670/2330 train_time:98352ms step_avg:58.89ms
step:1671/2330 train_time:98409ms step_avg:58.89ms
step:1672/2330 train_time:98472ms step_avg:58.89ms
step:1673/2330 train_time:98530ms step_avg:58.89ms
step:1674/2330 train_time:98591ms step_avg:58.90ms
step:1675/2330 train_time:98648ms step_avg:58.89ms
step:1676/2330 train_time:98711ms step_avg:58.90ms
step:1677/2330 train_time:98768ms step_avg:58.90ms
step:1678/2330 train_time:98831ms step_avg:58.90ms
step:1679/2330 train_time:98888ms step_avg:58.90ms
step:1680/2330 train_time:98952ms step_avg:58.90ms
step:1681/2330 train_time:99010ms step_avg:58.90ms
step:1682/2330 train_time:99072ms step_avg:58.90ms
step:1683/2330 train_time:99129ms step_avg:58.90ms
step:1684/2330 train_time:99193ms step_avg:58.90ms
step:1685/2330 train_time:99250ms step_avg:58.90ms
step:1686/2330 train_time:99312ms step_avg:58.90ms
step:1687/2330 train_time:99369ms step_avg:58.90ms
step:1688/2330 train_time:99432ms step_avg:58.91ms
step:1689/2330 train_time:99488ms step_avg:58.90ms
step:1690/2330 train_time:99551ms step_avg:58.91ms
step:1691/2330 train_time:99608ms step_avg:58.90ms
step:1692/2330 train_time:99671ms step_avg:58.91ms
step:1693/2330 train_time:99729ms step_avg:58.91ms
step:1694/2330 train_time:99792ms step_avg:58.91ms
step:1695/2330 train_time:99849ms step_avg:58.91ms
step:1696/2330 train_time:99911ms step_avg:58.91ms
step:1697/2330 train_time:99968ms step_avg:58.91ms
step:1698/2330 train_time:100030ms step_avg:58.91ms
step:1699/2330 train_time:100087ms step_avg:58.91ms
step:1700/2330 train_time:100150ms step_avg:58.91ms
step:1701/2330 train_time:100206ms step_avg:58.91ms
step:1702/2330 train_time:100270ms step_avg:58.91ms
step:1703/2330 train_time:100327ms step_avg:58.91ms
step:1704/2330 train_time:100389ms step_avg:58.91ms
step:1705/2330 train_time:100446ms step_avg:58.91ms
step:1706/2330 train_time:100507ms step_avg:58.91ms
step:1707/2330 train_time:100565ms step_avg:58.91ms
step:1708/2330 train_time:100626ms step_avg:58.91ms
step:1709/2330 train_time:100684ms step_avg:58.91ms
step:1710/2330 train_time:100745ms step_avg:58.92ms
step:1711/2330 train_time:100802ms step_avg:58.91ms
step:1712/2330 train_time:100863ms step_avg:58.92ms
step:1713/2330 train_time:100919ms step_avg:58.91ms
step:1714/2330 train_time:100982ms step_avg:58.92ms
step:1715/2330 train_time:101039ms step_avg:58.91ms
step:1716/2330 train_time:101102ms step_avg:58.92ms
step:1717/2330 train_time:101159ms step_avg:58.92ms
step:1718/2330 train_time:101220ms step_avg:58.92ms
step:1719/2330 train_time:101277ms step_avg:58.92ms
step:1720/2330 train_time:101339ms step_avg:58.92ms
step:1721/2330 train_time:101396ms step_avg:58.92ms
step:1722/2330 train_time:101458ms step_avg:58.92ms
step:1723/2330 train_time:101515ms step_avg:58.92ms
step:1724/2330 train_time:101579ms step_avg:58.92ms
step:1725/2330 train_time:101636ms step_avg:58.92ms
step:1726/2330 train_time:101699ms step_avg:58.92ms
step:1727/2330 train_time:101755ms step_avg:58.92ms
step:1728/2330 train_time:101818ms step_avg:58.92ms
step:1729/2330 train_time:101875ms step_avg:58.92ms
step:1730/2330 train_time:101938ms step_avg:58.92ms
step:1731/2330 train_time:101994ms step_avg:58.92ms
step:1732/2330 train_time:102058ms step_avg:58.92ms
step:1733/2330 train_time:102114ms step_avg:58.92ms
step:1734/2330 train_time:102177ms step_avg:58.93ms
step:1735/2330 train_time:102234ms step_avg:58.92ms
step:1736/2330 train_time:102297ms step_avg:58.93ms
step:1737/2330 train_time:102354ms step_avg:58.93ms
step:1738/2330 train_time:102417ms step_avg:58.93ms
step:1739/2330 train_time:102474ms step_avg:58.93ms
step:1740/2330 train_time:102537ms step_avg:58.93ms
step:1741/2330 train_time:102594ms step_avg:58.93ms
step:1742/2330 train_time:102658ms step_avg:58.93ms
step:1743/2330 train_time:102714ms step_avg:58.93ms
step:1744/2330 train_time:102777ms step_avg:58.93ms
step:1745/2330 train_time:102834ms step_avg:58.93ms
step:1746/2330 train_time:102898ms step_avg:58.93ms
step:1747/2330 train_time:102954ms step_avg:58.93ms
step:1748/2330 train_time:103017ms step_avg:58.93ms
step:1749/2330 train_time:103074ms step_avg:58.93ms
step:1750/2330 train_time:103136ms step_avg:58.93ms
step:1750/2330 val_loss:4.0377 train_time:103216ms step_avg:58.98ms
step:1751/2330 train_time:103238ms step_avg:58.96ms
step:1752/2330 train_time:103260ms step_avg:58.94ms
step:1753/2330 train_time:103315ms step_avg:58.94ms
step:1754/2330 train_time:103378ms step_avg:58.94ms
step:1755/2330 train_time:103435ms step_avg:58.94ms
step:1756/2330 train_time:103499ms step_avg:58.94ms
step:1757/2330 train_time:103556ms step_avg:58.94ms
step:1758/2330 train_time:103618ms step_avg:58.94ms
step:1759/2330 train_time:103674ms step_avg:58.94ms
step:1760/2330 train_time:103736ms step_avg:58.94ms
step:1761/2330 train_time:103793ms step_avg:58.94ms
step:1762/2330 train_time:103855ms step_avg:58.94ms
step:1763/2330 train_time:103912ms step_avg:58.94ms
step:1764/2330 train_time:103973ms step_avg:58.94ms
step:1765/2330 train_time:104031ms step_avg:58.94ms
step:1766/2330 train_time:104091ms step_avg:58.94ms
step:1767/2330 train_time:104151ms step_avg:58.94ms
step:1768/2330 train_time:104213ms step_avg:58.94ms
step:1769/2330 train_time:104273ms step_avg:58.94ms
step:1770/2330 train_time:104333ms step_avg:58.95ms
step:1771/2330 train_time:104392ms step_avg:58.95ms
step:1772/2330 train_time:104452ms step_avg:58.95ms
step:1773/2330 train_time:104510ms step_avg:58.95ms
step:1774/2330 train_time:104571ms step_avg:58.95ms
step:1775/2330 train_time:104628ms step_avg:58.95ms
step:1776/2330 train_time:104689ms step_avg:58.95ms
step:1777/2330 train_time:104747ms step_avg:58.95ms
step:1778/2330 train_time:104808ms step_avg:58.95ms
step:1779/2330 train_time:104865ms step_avg:58.95ms
step:1780/2330 train_time:104927ms step_avg:58.95ms
step:1781/2330 train_time:104983ms step_avg:58.95ms
step:1782/2330 train_time:105045ms step_avg:58.95ms
step:1783/2330 train_time:105102ms step_avg:58.95ms
step:1784/2330 train_time:105164ms step_avg:58.95ms
step:1785/2330 train_time:105221ms step_avg:58.95ms
step:1786/2330 train_time:105283ms step_avg:58.95ms
step:1787/2330 train_time:105340ms step_avg:58.95ms
step:1788/2330 train_time:105404ms step_avg:58.95ms
step:1789/2330 train_time:105460ms step_avg:58.95ms
step:1790/2330 train_time:105523ms step_avg:58.95ms
step:1791/2330 train_time:105579ms step_avg:58.95ms
step:1792/2330 train_time:105642ms step_avg:58.95ms
step:1793/2330 train_time:105699ms step_avg:58.95ms
step:1794/2330 train_time:105761ms step_avg:58.95ms
step:1795/2330 train_time:105818ms step_avg:58.95ms
step:1796/2330 train_time:105880ms step_avg:58.95ms
step:1797/2330 train_time:105937ms step_avg:58.95ms
step:1798/2330 train_time:105999ms step_avg:58.95ms
step:1799/2330 train_time:106056ms step_avg:58.95ms
step:1800/2330 train_time:106118ms step_avg:58.95ms
step:1801/2330 train_time:106175ms step_avg:58.95ms
step:1802/2330 train_time:106238ms step_avg:58.96ms
step:1803/2330 train_time:106295ms step_avg:58.95ms
step:1804/2330 train_time:106358ms step_avg:58.96ms
step:1805/2330 train_time:106416ms step_avg:58.96ms
step:1806/2330 train_time:106478ms step_avg:58.96ms
step:1807/2330 train_time:106535ms step_avg:58.96ms
step:1808/2330 train_time:106598ms step_avg:58.96ms
step:1809/2330 train_time:106655ms step_avg:58.96ms
step:1810/2330 train_time:106717ms step_avg:58.96ms
step:1811/2330 train_time:106773ms step_avg:58.96ms
step:1812/2330 train_time:106836ms step_avg:58.96ms
step:1813/2330 train_time:106893ms step_avg:58.96ms
step:1814/2330 train_time:106955ms step_avg:58.96ms
step:1815/2330 train_time:107013ms step_avg:58.96ms
step:1816/2330 train_time:107075ms step_avg:58.96ms
step:1817/2330 train_time:107133ms step_avg:58.96ms
step:1818/2330 train_time:107194ms step_avg:58.96ms
step:1819/2330 train_time:107252ms step_avg:58.96ms
step:1820/2330 train_time:107314ms step_avg:58.96ms
step:1821/2330 train_time:107372ms step_avg:58.96ms
step:1822/2330 train_time:107433ms step_avg:58.96ms
step:1823/2330 train_time:107491ms step_avg:58.96ms
step:1824/2330 train_time:107552ms step_avg:58.96ms
step:1825/2330 train_time:107610ms step_avg:58.96ms
step:1826/2330 train_time:107671ms step_avg:58.97ms
step:1827/2330 train_time:107728ms step_avg:58.96ms
step:1828/2330 train_time:107790ms step_avg:58.97ms
step:1829/2330 train_time:107847ms step_avg:58.97ms
step:1830/2330 train_time:107908ms step_avg:58.97ms
step:1831/2330 train_time:107965ms step_avg:58.97ms
step:1832/2330 train_time:108027ms step_avg:58.97ms
step:1833/2330 train_time:108084ms step_avg:58.97ms
step:1834/2330 train_time:108145ms step_avg:58.97ms
step:1835/2330 train_time:108202ms step_avg:58.97ms
step:1836/2330 train_time:108265ms step_avg:58.97ms
step:1837/2330 train_time:108322ms step_avg:58.97ms
step:1838/2330 train_time:108383ms step_avg:58.97ms
step:1839/2330 train_time:108440ms step_avg:58.97ms
step:1840/2330 train_time:108504ms step_avg:58.97ms
step:1841/2330 train_time:108561ms step_avg:58.97ms
step:1842/2330 train_time:108623ms step_avg:58.97ms
step:1843/2330 train_time:108680ms step_avg:58.97ms
step:1844/2330 train_time:108743ms step_avg:58.97ms
step:1845/2330 train_time:108800ms step_avg:58.97ms
step:1846/2330 train_time:108862ms step_avg:58.97ms
step:1847/2330 train_time:108919ms step_avg:58.97ms
step:1848/2330 train_time:108981ms step_avg:58.97ms
step:1849/2330 train_time:109038ms step_avg:58.97ms
step:1850/2330 train_time:109100ms step_avg:58.97ms
step:1851/2330 train_time:109158ms step_avg:58.97ms
step:1852/2330 train_time:109219ms step_avg:58.97ms
step:1853/2330 train_time:109276ms step_avg:58.97ms
step:1854/2330 train_time:109337ms step_avg:58.97ms
step:1855/2330 train_time:109395ms step_avg:58.97ms
step:1856/2330 train_time:109457ms step_avg:58.97ms
step:1857/2330 train_time:109516ms step_avg:58.97ms
step:1858/2330 train_time:109577ms step_avg:58.98ms
step:1859/2330 train_time:109635ms step_avg:58.98ms
step:1860/2330 train_time:109698ms step_avg:58.98ms
step:1861/2330 train_time:109755ms step_avg:58.98ms
step:1862/2330 train_time:109817ms step_avg:58.98ms
step:1863/2330 train_time:109875ms step_avg:58.98ms
step:1864/2330 train_time:109936ms step_avg:58.98ms
step:1865/2330 train_time:109993ms step_avg:58.98ms
step:1866/2330 train_time:110055ms step_avg:58.98ms
step:1867/2330 train_time:110114ms step_avg:58.98ms
step:1868/2330 train_time:110175ms step_avg:58.98ms
step:1869/2330 train_time:110233ms step_avg:58.98ms
step:1870/2330 train_time:110294ms step_avg:58.98ms
step:1871/2330 train_time:110352ms step_avg:58.98ms
step:1872/2330 train_time:110413ms step_avg:58.98ms
step:1873/2330 train_time:110471ms step_avg:58.98ms
step:1874/2330 train_time:110533ms step_avg:58.98ms
step:1875/2330 train_time:110591ms step_avg:58.98ms
step:1876/2330 train_time:110652ms step_avg:58.98ms
step:1877/2330 train_time:110710ms step_avg:58.98ms
step:1878/2330 train_time:110771ms step_avg:58.98ms
step:1879/2330 train_time:110830ms step_avg:58.98ms
step:1880/2330 train_time:110891ms step_avg:58.98ms
step:1881/2330 train_time:110948ms step_avg:58.98ms
step:1882/2330 train_time:111009ms step_avg:58.98ms
step:1883/2330 train_time:111067ms step_avg:58.98ms
step:1884/2330 train_time:111128ms step_avg:58.98ms
step:1885/2330 train_time:111185ms step_avg:58.98ms
step:1886/2330 train_time:111247ms step_avg:58.99ms
step:1887/2330 train_time:111304ms step_avg:58.98ms
step:1888/2330 train_time:111366ms step_avg:58.99ms
step:1889/2330 train_time:111423ms step_avg:58.99ms
step:1890/2330 train_time:111484ms step_avg:58.99ms
step:1891/2330 train_time:111541ms step_avg:58.99ms
step:1892/2330 train_time:111604ms step_avg:58.99ms
step:1893/2330 train_time:111661ms step_avg:58.99ms
step:1894/2330 train_time:111723ms step_avg:58.99ms
step:1895/2330 train_time:111780ms step_avg:58.99ms
step:1896/2330 train_time:111842ms step_avg:58.99ms
step:1897/2330 train_time:111900ms step_avg:58.99ms
step:1898/2330 train_time:111962ms step_avg:58.99ms
step:1899/2330 train_time:112019ms step_avg:58.99ms
step:1900/2330 train_time:112081ms step_avg:58.99ms
step:1901/2330 train_time:112138ms step_avg:58.99ms
step:1902/2330 train_time:112201ms step_avg:58.99ms
step:1903/2330 train_time:112258ms step_avg:58.99ms
step:1904/2330 train_time:112320ms step_avg:58.99ms
step:1905/2330 train_time:112378ms step_avg:58.99ms
step:1906/2330 train_time:112439ms step_avg:58.99ms
step:1907/2330 train_time:112497ms step_avg:58.99ms
step:1908/2330 train_time:112559ms step_avg:58.99ms
step:1909/2330 train_time:112616ms step_avg:58.99ms
step:1910/2330 train_time:112678ms step_avg:58.99ms
step:1911/2330 train_time:112736ms step_avg:58.99ms
step:1912/2330 train_time:112797ms step_avg:58.99ms
step:1913/2330 train_time:112855ms step_avg:58.99ms
step:1914/2330 train_time:112917ms step_avg:59.00ms
step:1915/2330 train_time:112975ms step_avg:58.99ms
step:1916/2330 train_time:113036ms step_avg:59.00ms
step:1917/2330 train_time:113094ms step_avg:59.00ms
step:1918/2330 train_time:113155ms step_avg:59.00ms
step:1919/2330 train_time:113213ms step_avg:59.00ms
step:1920/2330 train_time:113275ms step_avg:59.00ms
step:1921/2330 train_time:113332ms step_avg:59.00ms
step:1922/2330 train_time:113393ms step_avg:59.00ms
step:1923/2330 train_time:113450ms step_avg:59.00ms
step:1924/2330 train_time:113512ms step_avg:59.00ms
step:1925/2330 train_time:113570ms step_avg:59.00ms
step:1926/2330 train_time:113632ms step_avg:59.00ms
step:1927/2330 train_time:113689ms step_avg:59.00ms
step:1928/2330 train_time:113750ms step_avg:59.00ms
step:1929/2330 train_time:113808ms step_avg:59.00ms
step:1930/2330 train_time:113869ms step_avg:59.00ms
step:1931/2330 train_time:113926ms step_avg:59.00ms
step:1932/2330 train_time:113988ms step_avg:59.00ms
step:1933/2330 train_time:114045ms step_avg:59.00ms
step:1934/2330 train_time:114108ms step_avg:59.00ms
step:1935/2330 train_time:114165ms step_avg:59.00ms
step:1936/2330 train_time:114227ms step_avg:59.00ms
step:1937/2330 train_time:114284ms step_avg:59.00ms
step:1938/2330 train_time:114345ms step_avg:59.00ms
step:1939/2330 train_time:114402ms step_avg:59.00ms
step:1940/2330 train_time:114464ms step_avg:59.00ms
step:1941/2330 train_time:114521ms step_avg:59.00ms
step:1942/2330 train_time:114583ms step_avg:59.00ms
step:1943/2330 train_time:114640ms step_avg:59.00ms
step:1944/2330 train_time:114703ms step_avg:59.00ms
step:1945/2330 train_time:114759ms step_avg:59.00ms
step:1946/2330 train_time:114823ms step_avg:59.00ms
step:1947/2330 train_time:114880ms step_avg:59.00ms
step:1948/2330 train_time:114943ms step_avg:59.01ms
step:1949/2330 train_time:115000ms step_avg:59.00ms
step:1950/2330 train_time:115062ms step_avg:59.01ms
step:1951/2330 train_time:115118ms step_avg:59.00ms
step:1952/2330 train_time:115181ms step_avg:59.01ms
step:1953/2330 train_time:115238ms step_avg:59.01ms
step:1954/2330 train_time:115300ms step_avg:59.01ms
step:1955/2330 train_time:115357ms step_avg:59.01ms
step:1956/2330 train_time:115419ms step_avg:59.01ms
step:1957/2330 train_time:115477ms step_avg:59.01ms
step:1958/2330 train_time:115538ms step_avg:59.01ms
step:1959/2330 train_time:115595ms step_avg:59.01ms
step:1960/2330 train_time:115658ms step_avg:59.01ms
step:1961/2330 train_time:115715ms step_avg:59.01ms
step:1962/2330 train_time:115778ms step_avg:59.01ms
step:1963/2330 train_time:115835ms step_avg:59.01ms
step:1964/2330 train_time:115897ms step_avg:59.01ms
step:1965/2330 train_time:115955ms step_avg:59.01ms
step:1966/2330 train_time:116018ms step_avg:59.01ms
step:1967/2330 train_time:116076ms step_avg:59.01ms
step:1968/2330 train_time:116137ms step_avg:59.01ms
step:1969/2330 train_time:116194ms step_avg:59.01ms
step:1970/2330 train_time:116256ms step_avg:59.01ms
step:1971/2330 train_time:116314ms step_avg:59.01ms
step:1972/2330 train_time:116375ms step_avg:59.01ms
step:1973/2330 train_time:116432ms step_avg:59.01ms
step:1974/2330 train_time:116494ms step_avg:59.01ms
step:1975/2330 train_time:116552ms step_avg:59.01ms
step:1976/2330 train_time:116614ms step_avg:59.01ms
step:1977/2330 train_time:116671ms step_avg:59.01ms
step:1978/2330 train_time:116733ms step_avg:59.02ms
step:1979/2330 train_time:116790ms step_avg:59.01ms
step:1980/2330 train_time:116852ms step_avg:59.02ms
step:1981/2330 train_time:116910ms step_avg:59.02ms
step:1982/2330 train_time:116971ms step_avg:59.02ms
step:1983/2330 train_time:117029ms step_avg:59.02ms
step:1984/2330 train_time:117090ms step_avg:59.02ms
step:1985/2330 train_time:117147ms step_avg:59.02ms
step:1986/2330 train_time:117209ms step_avg:59.02ms
step:1987/2330 train_time:117267ms step_avg:59.02ms
step:1988/2330 train_time:117328ms step_avg:59.02ms
step:1989/2330 train_time:117385ms step_avg:59.02ms
step:1990/2330 train_time:117446ms step_avg:59.02ms
step:1991/2330 train_time:117503ms step_avg:59.02ms
step:1992/2330 train_time:117565ms step_avg:59.02ms
step:1993/2330 train_time:117622ms step_avg:59.02ms
step:1994/2330 train_time:117684ms step_avg:59.02ms
step:1995/2330 train_time:117741ms step_avg:59.02ms
step:1996/2330 train_time:117804ms step_avg:59.02ms
step:1997/2330 train_time:117861ms step_avg:59.02ms
step:1998/2330 train_time:117923ms step_avg:59.02ms
step:1999/2330 train_time:117979ms step_avg:59.02ms
step:2000/2330 train_time:118042ms step_avg:59.02ms
step:2000/2330 val_loss:3.9772 train_time:118122ms step_avg:59.06ms
step:2001/2330 train_time:118142ms step_avg:59.04ms
step:2002/2330 train_time:118165ms step_avg:59.02ms
step:2003/2330 train_time:118223ms step_avg:59.02ms
step:2004/2330 train_time:118292ms step_avg:59.03ms
step:2005/2330 train_time:118349ms step_avg:59.03ms
step:2006/2330 train_time:118411ms step_avg:59.03ms
step:2007/2330 train_time:118468ms step_avg:59.03ms
step:2008/2330 train_time:118530ms step_avg:59.03ms
step:2009/2330 train_time:118586ms step_avg:59.03ms
step:2010/2330 train_time:118649ms step_avg:59.03ms
step:2011/2330 train_time:118706ms step_avg:59.03ms
step:2012/2330 train_time:118767ms step_avg:59.03ms
step:2013/2330 train_time:118823ms step_avg:59.03ms
step:2014/2330 train_time:118885ms step_avg:59.03ms
step:2015/2330 train_time:118941ms step_avg:59.03ms
step:2016/2330 train_time:119003ms step_avg:59.03ms
step:2017/2330 train_time:119060ms step_avg:59.03ms
step:2018/2330 train_time:119121ms step_avg:59.03ms
step:2019/2330 train_time:119179ms step_avg:59.03ms
step:2020/2330 train_time:119242ms step_avg:59.03ms
step:2021/2330 train_time:119301ms step_avg:59.03ms
step:2022/2330 train_time:119364ms step_avg:59.03ms
step:2023/2330 train_time:119422ms step_avg:59.03ms
step:2024/2330 train_time:119483ms step_avg:59.03ms
step:2025/2330 train_time:119541ms step_avg:59.03ms
step:2026/2330 train_time:119602ms step_avg:59.03ms
step:2027/2330 train_time:119659ms step_avg:59.03ms
step:2028/2330 train_time:119720ms step_avg:59.03ms
step:2029/2330 train_time:119777ms step_avg:59.03ms
step:2030/2330 train_time:119838ms step_avg:59.03ms
step:2031/2330 train_time:119896ms step_avg:59.03ms
step:2032/2330 train_time:119957ms step_avg:59.03ms
step:2033/2330 train_time:120013ms step_avg:59.03ms
step:2034/2330 train_time:120075ms step_avg:59.03ms
step:2035/2330 train_time:120131ms step_avg:59.03ms
step:2036/2330 train_time:120194ms step_avg:59.03ms
step:2037/2330 train_time:120250ms step_avg:59.03ms
step:2038/2330 train_time:120314ms step_avg:59.04ms
step:2039/2330 train_time:120371ms step_avg:59.03ms
step:2040/2330 train_time:120434ms step_avg:59.04ms
step:2041/2330 train_time:120491ms step_avg:59.04ms
step:2042/2330 train_time:120554ms step_avg:59.04ms
step:2043/2330 train_time:120611ms step_avg:59.04ms
step:2044/2330 train_time:120673ms step_avg:59.04ms
step:2045/2330 train_time:120730ms step_avg:59.04ms
step:2046/2330 train_time:120793ms step_avg:59.04ms
step:2047/2330 train_time:120849ms step_avg:59.04ms
step:2048/2330 train_time:120912ms step_avg:59.04ms
step:2049/2330 train_time:120968ms step_avg:59.04ms
step:2050/2330 train_time:121030ms step_avg:59.04ms
step:2051/2330 train_time:121087ms step_avg:59.04ms
step:2052/2330 train_time:121151ms step_avg:59.04ms
step:2053/2330 train_time:121208ms step_avg:59.04ms
step:2054/2330 train_time:121271ms step_avg:59.04ms
step:2055/2330 train_time:121327ms step_avg:59.04ms
step:2056/2330 train_time:121391ms step_avg:59.04ms
step:2057/2330 train_time:121448ms step_avg:59.04ms
step:2058/2330 train_time:121512ms step_avg:59.04ms
step:2059/2330 train_time:121569ms step_avg:59.04ms
step:2060/2330 train_time:121632ms step_avg:59.04ms
step:2061/2330 train_time:121689ms step_avg:59.04ms
step:2062/2330 train_time:121752ms step_avg:59.05ms
step:2063/2330 train_time:121809ms step_avg:59.04ms
step:2064/2330 train_time:121871ms step_avg:59.05ms
step:2065/2330 train_time:121928ms step_avg:59.05ms
step:2066/2330 train_time:121989ms step_avg:59.05ms
step:2067/2330 train_time:122046ms step_avg:59.04ms
step:2068/2330 train_time:122108ms step_avg:59.05ms
step:2069/2330 train_time:122165ms step_avg:59.05ms
step:2070/2330 train_time:122228ms step_avg:59.05ms
step:2071/2330 train_time:122285ms step_avg:59.05ms
step:2072/2330 train_time:122349ms step_avg:59.05ms
step:2073/2330 train_time:122406ms step_avg:59.05ms
step:2074/2330 train_time:122468ms step_avg:59.05ms
step:2075/2330 train_time:122526ms step_avg:59.05ms
step:2076/2330 train_time:122587ms step_avg:59.05ms
step:2077/2330 train_time:122644ms step_avg:59.05ms
step:2078/2330 train_time:122707ms step_avg:59.05ms
step:2079/2330 train_time:122764ms step_avg:59.05ms
step:2080/2330 train_time:122825ms step_avg:59.05ms
step:2081/2330 train_time:122883ms step_avg:59.05ms
step:2082/2330 train_time:122945ms step_avg:59.05ms
step:2083/2330 train_time:123002ms step_avg:59.05ms
step:2084/2330 train_time:123065ms step_avg:59.05ms
step:2085/2330 train_time:123122ms step_avg:59.05ms
step:2086/2330 train_time:123184ms step_avg:59.05ms
step:2087/2330 train_time:123242ms step_avg:59.05ms
step:2088/2330 train_time:123303ms step_avg:59.05ms
step:2089/2330 train_time:123361ms step_avg:59.05ms
step:2090/2330 train_time:123422ms step_avg:59.05ms
step:2091/2330 train_time:123479ms step_avg:59.05ms
step:2092/2330 train_time:123541ms step_avg:59.05ms
step:2093/2330 train_time:123598ms step_avg:59.05ms
step:2094/2330 train_time:123660ms step_avg:59.05ms
step:2095/2330 train_time:123717ms step_avg:59.05ms
step:2096/2330 train_time:123779ms step_avg:59.05ms
step:2097/2330 train_time:123836ms step_avg:59.05ms
step:2098/2330 train_time:123897ms step_avg:59.05ms
step:2099/2330 train_time:123954ms step_avg:59.05ms
step:2100/2330 train_time:124015ms step_avg:59.05ms
step:2101/2330 train_time:124072ms step_avg:59.05ms
step:2102/2330 train_time:124134ms step_avg:59.06ms
step:2103/2330 train_time:124191ms step_avg:59.05ms
step:2104/2330 train_time:124254ms step_avg:59.06ms
step:2105/2330 train_time:124310ms step_avg:59.05ms
step:2106/2330 train_time:124372ms step_avg:59.06ms
step:2107/2330 train_time:124429ms step_avg:59.06ms
step:2108/2330 train_time:124493ms step_avg:59.06ms
step:2109/2330 train_time:124550ms step_avg:59.06ms
step:2110/2330 train_time:124613ms step_avg:59.06ms
step:2111/2330 train_time:124669ms step_avg:59.06ms
step:2112/2330 train_time:124732ms step_avg:59.06ms
step:2113/2330 train_time:124789ms step_avg:59.06ms
step:2114/2330 train_time:124851ms step_avg:59.06ms
step:2115/2330 train_time:124908ms step_avg:59.06ms
step:2116/2330 train_time:124970ms step_avg:59.06ms
step:2117/2330 train_time:125027ms step_avg:59.06ms
step:2118/2330 train_time:125088ms step_avg:59.06ms
step:2119/2330 train_time:125146ms step_avg:59.06ms
step:2120/2330 train_time:125208ms step_avg:59.06ms
step:2121/2330 train_time:125265ms step_avg:59.06ms
step:2122/2330 train_time:125328ms step_avg:59.06ms
step:2123/2330 train_time:125385ms step_avg:59.06ms
step:2124/2330 train_time:125448ms step_avg:59.06ms
step:2125/2330 train_time:125505ms step_avg:59.06ms
step:2126/2330 train_time:125567ms step_avg:59.06ms
step:2127/2330 train_time:125624ms step_avg:59.06ms
step:2128/2330 train_time:125688ms step_avg:59.06ms
step:2129/2330 train_time:125745ms step_avg:59.06ms
step:2130/2330 train_time:125808ms step_avg:59.06ms
step:2131/2330 train_time:125865ms step_avg:59.06ms
step:2132/2330 train_time:125927ms step_avg:59.07ms
step:2133/2330 train_time:125984ms step_avg:59.06ms
step:2134/2330 train_time:126047ms step_avg:59.07ms
step:2135/2330 train_time:126104ms step_avg:59.06ms
step:2136/2330 train_time:126166ms step_avg:59.07ms
step:2137/2330 train_time:126223ms step_avg:59.07ms
step:2138/2330 train_time:126286ms step_avg:59.07ms
step:2139/2330 train_time:126344ms step_avg:59.07ms
step:2140/2330 train_time:126405ms step_avg:59.07ms
step:2141/2330 train_time:126463ms step_avg:59.07ms
step:2142/2330 train_time:126525ms step_avg:59.07ms
step:2143/2330 train_time:126583ms step_avg:59.07ms
step:2144/2330 train_time:126645ms step_avg:59.07ms
step:2145/2330 train_time:126702ms step_avg:59.07ms
step:2146/2330 train_time:126766ms step_avg:59.07ms
step:2147/2330 train_time:126824ms step_avg:59.07ms
step:2148/2330 train_time:126885ms step_avg:59.07ms
step:2149/2330 train_time:126943ms step_avg:59.07ms
step:2150/2330 train_time:127004ms step_avg:59.07ms
step:2151/2330 train_time:127062ms step_avg:59.07ms
step:2152/2330 train_time:127124ms step_avg:59.07ms
step:2153/2330 train_time:127181ms step_avg:59.07ms
step:2154/2330 train_time:127242ms step_avg:59.07ms
step:2155/2330 train_time:127299ms step_avg:59.07ms
step:2156/2330 train_time:127360ms step_avg:59.07ms
step:2157/2330 train_time:127418ms step_avg:59.07ms
step:2158/2330 train_time:127479ms step_avg:59.07ms
step:2159/2330 train_time:127536ms step_avg:59.07ms
step:2160/2330 train_time:127598ms step_avg:59.07ms
step:2161/2330 train_time:127654ms step_avg:59.07ms
step:2162/2330 train_time:127717ms step_avg:59.07ms
step:2163/2330 train_time:127774ms step_avg:59.07ms
step:2164/2330 train_time:127836ms step_avg:59.07ms
step:2165/2330 train_time:127893ms step_avg:59.07ms
step:2166/2330 train_time:127955ms step_avg:59.07ms
step:2167/2330 train_time:128012ms step_avg:59.07ms
step:2168/2330 train_time:128074ms step_avg:59.07ms
step:2169/2330 train_time:128131ms step_avg:59.07ms
step:2170/2330 train_time:128193ms step_avg:59.08ms
step:2171/2330 train_time:128250ms step_avg:59.07ms
step:2172/2330 train_time:128313ms step_avg:59.08ms
step:2173/2330 train_time:128370ms step_avg:59.07ms
step:2174/2330 train_time:128433ms step_avg:59.08ms
step:2175/2330 train_time:128490ms step_avg:59.08ms
step:2176/2330 train_time:128552ms step_avg:59.08ms
step:2177/2330 train_time:128609ms step_avg:59.08ms
step:2178/2330 train_time:128672ms step_avg:59.08ms
step:2179/2330 train_time:128728ms step_avg:59.08ms
step:2180/2330 train_time:128791ms step_avg:59.08ms
step:2181/2330 train_time:128848ms step_avg:59.08ms
step:2182/2330 train_time:128911ms step_avg:59.08ms
step:2183/2330 train_time:128967ms step_avg:59.08ms
step:2184/2330 train_time:129030ms step_avg:59.08ms
step:2185/2330 train_time:129088ms step_avg:59.08ms
step:2186/2330 train_time:129150ms step_avg:59.08ms
step:2187/2330 train_time:129207ms step_avg:59.08ms
step:2188/2330 train_time:129270ms step_avg:59.08ms
step:2189/2330 train_time:129327ms step_avg:59.08ms
step:2190/2330 train_time:129388ms step_avg:59.08ms
step:2191/2330 train_time:129445ms step_avg:59.08ms
step:2192/2330 train_time:129508ms step_avg:59.08ms
step:2193/2330 train_time:129564ms step_avg:59.08ms
step:2194/2330 train_time:129627ms step_avg:59.08ms
step:2195/2330 train_time:129684ms step_avg:59.08ms
step:2196/2330 train_time:129747ms step_avg:59.08ms
step:2197/2330 train_time:129804ms step_avg:59.08ms
step:2198/2330 train_time:129867ms step_avg:59.08ms
step:2199/2330 train_time:129924ms step_avg:59.08ms
step:2200/2330 train_time:129987ms step_avg:59.08ms
step:2201/2330 train_time:130044ms step_avg:59.08ms
step:2202/2330 train_time:130105ms step_avg:59.09ms
step:2203/2330 train_time:130163ms step_avg:59.08ms
step:2204/2330 train_time:130225ms step_avg:59.09ms
step:2205/2330 train_time:130283ms step_avg:59.09ms
step:2206/2330 train_time:130344ms step_avg:59.09ms
step:2207/2330 train_time:130402ms step_avg:59.09ms
step:2208/2330 train_time:130463ms step_avg:59.09ms
step:2209/2330 train_time:130520ms step_avg:59.09ms
step:2210/2330 train_time:130582ms step_avg:59.09ms
step:2211/2330 train_time:130640ms step_avg:59.09ms
step:2212/2330 train_time:130702ms step_avg:59.09ms
step:2213/2330 train_time:130759ms step_avg:59.09ms
step:2214/2330 train_time:130820ms step_avg:59.09ms
step:2215/2330 train_time:130877ms step_avg:59.09ms
step:2216/2330 train_time:130939ms step_avg:59.09ms
step:2217/2330 train_time:130996ms step_avg:59.09ms
step:2218/2330 train_time:131058ms step_avg:59.09ms
step:2219/2330 train_time:131115ms step_avg:59.09ms
step:2220/2330 train_time:131177ms step_avg:59.09ms
step:2221/2330 train_time:131235ms step_avg:59.09ms
step:2222/2330 train_time:131297ms step_avg:59.09ms
step:2223/2330 train_time:131353ms step_avg:59.09ms
step:2224/2330 train_time:131415ms step_avg:59.09ms
step:2225/2330 train_time:131472ms step_avg:59.09ms
step:2226/2330 train_time:131534ms step_avg:59.09ms
step:2227/2330 train_time:131591ms step_avg:59.09ms
step:2228/2330 train_time:131653ms step_avg:59.09ms
step:2229/2330 train_time:131710ms step_avg:59.09ms
step:2230/2330 train_time:131772ms step_avg:59.09ms
step:2231/2330 train_time:131829ms step_avg:59.09ms
step:2232/2330 train_time:131892ms step_avg:59.09ms
step:2233/2330 train_time:131948ms step_avg:59.09ms
step:2234/2330 train_time:132011ms step_avg:59.09ms
step:2235/2330 train_time:132068ms step_avg:59.09ms
step:2236/2330 train_time:132131ms step_avg:59.09ms
step:2237/2330 train_time:132188ms step_avg:59.09ms
step:2238/2330 train_time:132251ms step_avg:59.09ms
step:2239/2330 train_time:132308ms step_avg:59.09ms
step:2240/2330 train_time:132371ms step_avg:59.09ms
step:2241/2330 train_time:132428ms step_avg:59.09ms
step:2242/2330 train_time:132490ms step_avg:59.09ms
step:2243/2330 train_time:132546ms step_avg:59.09ms
step:2244/2330 train_time:132609ms step_avg:59.10ms
step:2245/2330 train_time:132666ms step_avg:59.09ms
step:2246/2330 train_time:132730ms step_avg:59.10ms
step:2247/2330 train_time:132787ms step_avg:59.10ms
step:2248/2330 train_time:132849ms step_avg:59.10ms
step:2249/2330 train_time:132906ms step_avg:59.10ms
step:2250/2330 train_time:132968ms step_avg:59.10ms
step:2250/2330 val_loss:3.9291 train_time:133048ms step_avg:59.13ms
step:2251/2330 train_time:133068ms step_avg:59.12ms
step:2252/2330 train_time:133091ms step_avg:59.10ms
step:2253/2330 train_time:133149ms step_avg:59.10ms
step:2254/2330 train_time:133213ms step_avg:59.10ms
step:2255/2330 train_time:133271ms step_avg:59.10ms
step:2256/2330 train_time:133332ms step_avg:59.10ms
step:2257/2330 train_time:133390ms step_avg:59.10ms
step:2258/2330 train_time:133451ms step_avg:59.10ms
step:2259/2330 train_time:133509ms step_avg:59.10ms
step:2260/2330 train_time:133570ms step_avg:59.10ms
step:2261/2330 train_time:133627ms step_avg:59.10ms
step:2262/2330 train_time:133688ms step_avg:59.10ms
step:2263/2330 train_time:133745ms step_avg:59.10ms
step:2264/2330 train_time:133805ms step_avg:59.10ms
step:2265/2330 train_time:133863ms step_avg:59.10ms
step:2266/2330 train_time:133923ms step_avg:59.10ms
step:2267/2330 train_time:133980ms step_avg:59.10ms
step:2268/2330 train_time:134042ms step_avg:59.10ms
step:2269/2330 train_time:134100ms step_avg:59.10ms
step:2270/2330 train_time:134164ms step_avg:59.10ms
step:2271/2330 train_time:134222ms step_avg:59.10ms
step:2272/2330 train_time:134284ms step_avg:59.10ms
step:2273/2330 train_time:134341ms step_avg:59.10ms
step:2274/2330 train_time:134404ms step_avg:59.10ms
step:2275/2330 train_time:134461ms step_avg:59.10ms
step:2276/2330 train_time:134523ms step_avg:59.10ms
step:2277/2330 train_time:134579ms step_avg:59.10ms
step:2278/2330 train_time:134641ms step_avg:59.11ms
step:2279/2330 train_time:134698ms step_avg:59.10ms
step:2280/2330 train_time:134760ms step_avg:59.11ms
step:2281/2330 train_time:134817ms step_avg:59.10ms
step:2282/2330 train_time:134879ms step_avg:59.11ms
step:2283/2330 train_time:134935ms step_avg:59.10ms
step:2284/2330 train_time:134997ms step_avg:59.11ms
step:2285/2330 train_time:135054ms step_avg:59.10ms
step:2286/2330 train_time:135118ms step_avg:59.11ms
step:2287/2330 train_time:135175ms step_avg:59.11ms
step:2288/2330 train_time:135239ms step_avg:59.11ms
step:2289/2330 train_time:135296ms step_avg:59.11ms
step:2290/2330 train_time:135359ms step_avg:59.11ms
step:2291/2330 train_time:135416ms step_avg:59.11ms
step:2292/2330 train_time:135478ms step_avg:59.11ms
step:2293/2330 train_time:135534ms step_avg:59.11ms
step:2294/2330 train_time:135597ms step_avg:59.11ms
step:2295/2330 train_time:135654ms step_avg:59.11ms
step:2296/2330 train_time:135718ms step_avg:59.11ms
step:2297/2330 train_time:135775ms step_avg:59.11ms
step:2298/2330 train_time:135836ms step_avg:59.11ms
step:2299/2330 train_time:135893ms step_avg:59.11ms
step:2300/2330 train_time:135955ms step_avg:59.11ms
step:2301/2330 train_time:136012ms step_avg:59.11ms
step:2302/2330 train_time:136075ms step_avg:59.11ms
step:2303/2330 train_time:136132ms step_avg:59.11ms
step:2304/2330 train_time:136194ms step_avg:59.11ms
step:2305/2330 train_time:136251ms step_avg:59.11ms
step:2306/2330 train_time:136314ms step_avg:59.11ms
step:2307/2330 train_time:136372ms step_avg:59.11ms
step:2308/2330 train_time:136433ms step_avg:59.11ms
step:2309/2330 train_time:136491ms step_avg:59.11ms
step:2310/2330 train_time:136552ms step_avg:59.11ms
step:2311/2330 train_time:136610ms step_avg:59.11ms
step:2312/2330 train_time:136671ms step_avg:59.11ms
step:2313/2330 train_time:136729ms step_avg:59.11ms
step:2314/2330 train_time:136790ms step_avg:59.11ms
step:2315/2330 train_time:136846ms step_avg:59.11ms
step:2316/2330 train_time:136909ms step_avg:59.11ms
step:2317/2330 train_time:136966ms step_avg:59.11ms
step:2318/2330 train_time:137028ms step_avg:59.11ms
step:2319/2330 train_time:137085ms step_avg:59.11ms
step:2320/2330 train_time:137147ms step_avg:59.12ms
step:2321/2330 train_time:137203ms step_avg:59.11ms
step:2322/2330 train_time:137266ms step_avg:59.12ms
step:2323/2330 train_time:137323ms step_avg:59.11ms
step:2324/2330 train_time:137384ms step_avg:59.12ms
step:2325/2330 train_time:137441ms step_avg:59.11ms
step:2326/2330 train_time:137503ms step_avg:59.12ms
step:2327/2330 train_time:137560ms step_avg:59.11ms
step:2328/2330 train_time:137622ms step_avg:59.12ms
step:2329/2330 train_time:137679ms step_avg:59.12ms
step:2330/2330 train_time:137742ms step_avg:59.12ms
step:2330/2330 val_loss:3.9148 train_time:137821ms step_avg:59.15ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
