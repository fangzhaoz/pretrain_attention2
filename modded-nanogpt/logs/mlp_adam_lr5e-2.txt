import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_adam_lr5e-2"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-1, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizer2 = torch.optim.Adam(
    hidden_matrix_params + gate_params,
    lr=5e-2,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 04:51:56 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   34C    P0             113W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   32C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   34C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:78ms step_avg:77.75ms
step:2/2330 train_time:159ms step_avg:79.39ms
step:3/2330 train_time:171ms step_avg:57.01ms
step:4/2330 train_time:183ms step_avg:45.73ms
step:5/2330 train_time:193ms step_avg:38.57ms
step:6/2330 train_time:204ms step_avg:33.97ms
step:7/2330 train_time:234ms step_avg:33.42ms
step:8/2330 train_time:274ms step_avg:34.21ms
step:9/2330 train_time:307ms step_avg:34.11ms
step:10/2330 train_time:347ms step_avg:34.68ms
step:11/2330 train_time:380ms step_avg:34.57ms
step:12/2330 train_time:420ms step_avg:35.00ms
step:13/2330 train_time:453ms step_avg:34.88ms
step:14/2330 train_time:493ms step_avg:35.23ms
step:15/2330 train_time:527ms step_avg:35.10ms
step:16/2330 train_time:566ms step_avg:35.39ms
step:17/2330 train_time:600ms step_avg:35.28ms
step:18/2330 train_time:640ms step_avg:35.56ms
step:19/2330 train_time:674ms step_avg:35.45ms
step:20/2330 train_time:714ms step_avg:35.68ms
step:21/2330 train_time:747ms step_avg:35.57ms
step:22/2330 train_time:787ms step_avg:35.77ms
step:23/2330 train_time:820ms step_avg:35.67ms
step:24/2330 train_time:860ms step_avg:35.85ms
step:25/2330 train_time:894ms step_avg:35.76ms
step:26/2330 train_time:934ms step_avg:35.92ms
step:27/2330 train_time:968ms step_avg:35.83ms
step:28/2330 train_time:1008ms step_avg:35.99ms
step:29/2330 train_time:1042ms step_avg:35.93ms
step:30/2330 train_time:1083ms step_avg:36.09ms
step:31/2330 train_time:1118ms step_avg:36.06ms
step:32/2330 train_time:1159ms step_avg:36.20ms
step:33/2330 train_time:1195ms step_avg:36.20ms
step:34/2330 train_time:1235ms step_avg:36.31ms
step:35/2330 train_time:1270ms step_avg:36.29ms
step:36/2330 train_time:1310ms step_avg:36.39ms
step:37/2330 train_time:1344ms step_avg:36.33ms
step:38/2330 train_time:1384ms step_avg:36.43ms
step:39/2330 train_time:1419ms step_avg:36.37ms
step:40/2330 train_time:1459ms step_avg:36.47ms
step:41/2330 train_time:1493ms step_avg:36.42ms
step:42/2330 train_time:1533ms step_avg:36.50ms
step:43/2330 train_time:1567ms step_avg:36.45ms
step:44/2330 train_time:1607ms step_avg:36.53ms
step:45/2330 train_time:1641ms step_avg:36.47ms
step:46/2330 train_time:1681ms step_avg:36.55ms
step:47/2330 train_time:1715ms step_avg:36.49ms
step:48/2330 train_time:1755ms step_avg:36.57ms
step:49/2330 train_time:1789ms step_avg:36.51ms
step:50/2330 train_time:1829ms step_avg:36.58ms
step:51/2330 train_time:1863ms step_avg:36.53ms
step:52/2330 train_time:1903ms step_avg:36.60ms
step:53/2330 train_time:1938ms step_avg:36.56ms
step:54/2330 train_time:1978ms step_avg:36.63ms
step:55/2330 train_time:2012ms step_avg:36.59ms
step:56/2330 train_time:2052ms step_avg:36.65ms
step:57/2330 train_time:2087ms step_avg:36.62ms
step:58/2330 train_time:2127ms step_avg:36.68ms
step:59/2330 train_time:2162ms step_avg:36.65ms
step:60/2330 train_time:2203ms step_avg:36.71ms
step:61/2330 train_time:2238ms step_avg:36.69ms
step:62/2330 train_time:2279ms step_avg:36.75ms
step:63/2330 train_time:2313ms step_avg:36.72ms
step:64/2330 train_time:2354ms step_avg:36.77ms
step:65/2330 train_time:2388ms step_avg:36.74ms
step:66/2330 train_time:2428ms step_avg:36.79ms
step:67/2330 train_time:2463ms step_avg:36.76ms
step:68/2330 train_time:2503ms step_avg:36.81ms
step:69/2330 train_time:2538ms step_avg:36.78ms
step:70/2330 train_time:2578ms step_avg:36.83ms
step:71/2330 train_time:2613ms step_avg:36.80ms
step:72/2330 train_time:2653ms step_avg:36.85ms
step:73/2330 train_time:2688ms step_avg:36.82ms
step:74/2330 train_time:2728ms step_avg:36.86ms
step:75/2330 train_time:2762ms step_avg:36.82ms
step:76/2330 train_time:2802ms step_avg:36.87ms
step:77/2330 train_time:2837ms step_avg:36.84ms
step:78/2330 train_time:2878ms step_avg:36.89ms
step:79/2330 train_time:2912ms step_avg:36.86ms
step:80/2330 train_time:2952ms step_avg:36.90ms
step:81/2330 train_time:2987ms step_avg:36.87ms
step:82/2330 train_time:3027ms step_avg:36.91ms
step:83/2330 train_time:3062ms step_avg:36.89ms
step:84/2330 train_time:3102ms step_avg:36.93ms
step:85/2330 train_time:3136ms step_avg:36.90ms
step:86/2330 train_time:3177ms step_avg:36.95ms
step:87/2330 train_time:3212ms step_avg:36.92ms
step:88/2330 train_time:3252ms step_avg:36.96ms
step:89/2330 train_time:3287ms step_avg:36.93ms
step:90/2330 train_time:3327ms step_avg:36.97ms
step:91/2330 train_time:3362ms step_avg:36.94ms
step:92/2330 train_time:3402ms step_avg:36.98ms
step:93/2330 train_time:3437ms step_avg:36.95ms
step:94/2330 train_time:3477ms step_avg:36.99ms
step:95/2330 train_time:3512ms step_avg:36.96ms
step:96/2330 train_time:3552ms step_avg:37.00ms
step:97/2330 train_time:3586ms step_avg:36.97ms
step:98/2330 train_time:3626ms step_avg:37.00ms
step:99/2330 train_time:3661ms step_avg:36.98ms
step:100/2330 train_time:3701ms step_avg:37.01ms
step:101/2330 train_time:3736ms step_avg:36.99ms
step:102/2330 train_time:3777ms step_avg:37.03ms
step:103/2330 train_time:3811ms step_avg:37.00ms
step:104/2330 train_time:3851ms step_avg:37.03ms
step:105/2330 train_time:3885ms step_avg:37.00ms
step:106/2330 train_time:3925ms step_avg:37.03ms
step:107/2330 train_time:3960ms step_avg:37.01ms
step:108/2330 train_time:4001ms step_avg:37.04ms
step:109/2330 train_time:4035ms step_avg:37.02ms
step:110/2330 train_time:4076ms step_avg:37.05ms
step:111/2330 train_time:4110ms step_avg:37.03ms
step:112/2330 train_time:4151ms step_avg:37.06ms
step:113/2330 train_time:4185ms step_avg:37.04ms
step:114/2330 train_time:4225ms step_avg:37.06ms
step:115/2330 train_time:4260ms step_avg:37.04ms
step:116/2330 train_time:4301ms step_avg:37.08ms
step:117/2330 train_time:4336ms step_avg:37.06ms
step:118/2330 train_time:4376ms step_avg:37.09ms
step:119/2330 train_time:4411ms step_avg:37.07ms
step:120/2330 train_time:4452ms step_avg:37.10ms
step:121/2330 train_time:4486ms step_avg:37.07ms
step:122/2330 train_time:4526ms step_avg:37.10ms
step:123/2330 train_time:4561ms step_avg:37.08ms
step:124/2330 train_time:4601ms step_avg:37.11ms
step:125/2330 train_time:4636ms step_avg:37.09ms
step:126/2330 train_time:4677ms step_avg:37.12ms
step:127/2330 train_time:4711ms step_avg:37.09ms
step:128/2330 train_time:4751ms step_avg:37.12ms
step:129/2330 train_time:4786ms step_avg:37.10ms
step:130/2330 train_time:4826ms step_avg:37.12ms
step:131/2330 train_time:4860ms step_avg:37.10ms
step:132/2330 train_time:4901ms step_avg:37.13ms
step:133/2330 train_time:4935ms step_avg:37.11ms
step:134/2330 train_time:4976ms step_avg:37.13ms
step:135/2330 train_time:5010ms step_avg:37.11ms
step:136/2330 train_time:5051ms step_avg:37.14ms
step:137/2330 train_time:5085ms step_avg:37.12ms
step:138/2330 train_time:5125ms step_avg:37.14ms
step:139/2330 train_time:5160ms step_avg:37.12ms
step:140/2330 train_time:5201ms step_avg:37.15ms
step:141/2330 train_time:5235ms step_avg:37.13ms
step:142/2330 train_time:5276ms step_avg:37.16ms
step:143/2330 train_time:5311ms step_avg:37.14ms
step:144/2330 train_time:5351ms step_avg:37.16ms
step:145/2330 train_time:5386ms step_avg:37.14ms
step:146/2330 train_time:5426ms step_avg:37.17ms
step:147/2330 train_time:5461ms step_avg:37.15ms
step:148/2330 train_time:5502ms step_avg:37.17ms
step:149/2330 train_time:5536ms step_avg:37.16ms
step:150/2330 train_time:5577ms step_avg:37.18ms
step:151/2330 train_time:5611ms step_avg:37.16ms
step:152/2330 train_time:5652ms step_avg:37.18ms
step:153/2330 train_time:5686ms step_avg:37.16ms
step:154/2330 train_time:5727ms step_avg:37.19ms
step:155/2330 train_time:5761ms step_avg:37.17ms
step:156/2330 train_time:5802ms step_avg:37.19ms
step:157/2330 train_time:5836ms step_avg:37.18ms
step:158/2330 train_time:5877ms step_avg:37.20ms
step:159/2330 train_time:5912ms step_avg:37.18ms
step:160/2330 train_time:5952ms step_avg:37.20ms
step:161/2330 train_time:5986ms step_avg:37.18ms
step:162/2330 train_time:6027ms step_avg:37.20ms
step:163/2330 train_time:6062ms step_avg:37.19ms
step:164/2330 train_time:6102ms step_avg:37.21ms
step:165/2330 train_time:6136ms step_avg:37.19ms
step:166/2330 train_time:6177ms step_avg:37.21ms
step:167/2330 train_time:6212ms step_avg:37.20ms
step:168/2330 train_time:6252ms step_avg:37.21ms
step:169/2330 train_time:6287ms step_avg:37.20ms
step:170/2330 train_time:6328ms step_avg:37.22ms
step:171/2330 train_time:6363ms step_avg:37.21ms
step:172/2330 train_time:6403ms step_avg:37.23ms
step:173/2330 train_time:6438ms step_avg:37.21ms
step:174/2330 train_time:6478ms step_avg:37.23ms
step:175/2330 train_time:6513ms step_avg:37.22ms
step:176/2330 train_time:6554ms step_avg:37.24ms
step:177/2330 train_time:6588ms step_avg:37.22ms
step:178/2330 train_time:6629ms step_avg:37.24ms
step:179/2330 train_time:6663ms step_avg:37.23ms
step:180/2330 train_time:6704ms step_avg:37.24ms
step:181/2330 train_time:6739ms step_avg:37.23ms
step:182/2330 train_time:6780ms step_avg:37.25ms
step:183/2330 train_time:6814ms step_avg:37.24ms
step:184/2330 train_time:6855ms step_avg:37.25ms
step:185/2330 train_time:6889ms step_avg:37.24ms
step:186/2330 train_time:6930ms step_avg:37.26ms
step:187/2330 train_time:6964ms step_avg:37.24ms
step:188/2330 train_time:7004ms step_avg:37.26ms
step:189/2330 train_time:7039ms step_avg:37.25ms
step:190/2330 train_time:7080ms step_avg:37.27ms
step:191/2330 train_time:7115ms step_avg:37.25ms
step:192/2330 train_time:7156ms step_avg:37.27ms
step:193/2330 train_time:7190ms step_avg:37.26ms
step:194/2330 train_time:7231ms step_avg:37.27ms
step:195/2330 train_time:7266ms step_avg:37.26ms
step:196/2330 train_time:7306ms step_avg:37.27ms
step:197/2330 train_time:7341ms step_avg:37.26ms
step:198/2330 train_time:7381ms step_avg:37.28ms
step:199/2330 train_time:7416ms step_avg:37.27ms
step:200/2330 train_time:7456ms step_avg:37.28ms
step:201/2330 train_time:7491ms step_avg:37.27ms
step:202/2330 train_time:7531ms step_avg:37.28ms
step:203/2330 train_time:7566ms step_avg:37.27ms
step:204/2330 train_time:7607ms step_avg:37.29ms
step:205/2330 train_time:7642ms step_avg:37.28ms
step:206/2330 train_time:7682ms step_avg:37.29ms
step:207/2330 train_time:7716ms step_avg:37.28ms
step:208/2330 train_time:7757ms step_avg:37.29ms
step:209/2330 train_time:7791ms step_avg:37.28ms
step:210/2330 train_time:7832ms step_avg:37.30ms
step:211/2330 train_time:7866ms step_avg:37.28ms
step:212/2330 train_time:7907ms step_avg:37.30ms
step:213/2330 train_time:7941ms step_avg:37.28ms
step:214/2330 train_time:7982ms step_avg:37.30ms
step:215/2330 train_time:8017ms step_avg:37.29ms
step:216/2330 train_time:8057ms step_avg:37.30ms
step:217/2330 train_time:8093ms step_avg:37.29ms
step:218/2330 train_time:8133ms step_avg:37.31ms
step:219/2330 train_time:8167ms step_avg:37.29ms
step:220/2330 train_time:8208ms step_avg:37.31ms
step:221/2330 train_time:8242ms step_avg:37.29ms
step:222/2330 train_time:8283ms step_avg:37.31ms
step:223/2330 train_time:8317ms step_avg:37.30ms
step:224/2330 train_time:8358ms step_avg:37.31ms
step:225/2330 train_time:8392ms step_avg:37.30ms
step:226/2330 train_time:8433ms step_avg:37.31ms
step:227/2330 train_time:8468ms step_avg:37.30ms
step:228/2330 train_time:8508ms step_avg:37.32ms
step:229/2330 train_time:8543ms step_avg:37.31ms
step:230/2330 train_time:8583ms step_avg:37.32ms
step:231/2330 train_time:8619ms step_avg:37.31ms
step:232/2330 train_time:8659ms step_avg:37.32ms
step:233/2330 train_time:8694ms step_avg:37.31ms
step:234/2330 train_time:8734ms step_avg:37.33ms
step:235/2330 train_time:8769ms step_avg:37.31ms
step:236/2330 train_time:8809ms step_avg:37.33ms
step:237/2330 train_time:8844ms step_avg:37.32ms
step:238/2330 train_time:8884ms step_avg:37.33ms
step:239/2330 train_time:8919ms step_avg:37.32ms
step:240/2330 train_time:8960ms step_avg:37.33ms
step:241/2330 train_time:8995ms step_avg:37.32ms
step:242/2330 train_time:9035ms step_avg:37.33ms
step:243/2330 train_time:9071ms step_avg:37.33ms
step:244/2330 train_time:9111ms step_avg:37.34ms
step:245/2330 train_time:9146ms step_avg:37.33ms
step:246/2330 train_time:9187ms step_avg:37.34ms
step:247/2330 train_time:9221ms step_avg:37.33ms
step:248/2330 train_time:9262ms step_avg:37.35ms
step:249/2330 train_time:9297ms step_avg:37.34ms
step:250/2330 train_time:9337ms step_avg:37.35ms
step:250/2330 val_loss:5.9986 train_time:9447ms step_avg:37.79ms
step:251/2330 train_time:9459ms step_avg:37.68ms
step:252/2330 train_time:9470ms step_avg:37.58ms
step:253/2330 train_time:9479ms step_avg:37.47ms
step:254/2330 train_time:9489ms step_avg:37.36ms
step:255/2330 train_time:9524ms step_avg:37.35ms
step:256/2330 train_time:9565ms step_avg:37.36ms
step:257/2330 train_time:9599ms step_avg:37.35ms
step:258/2330 train_time:9639ms step_avg:37.36ms
step:259/2330 train_time:9673ms step_avg:37.35ms
step:260/2330 train_time:9714ms step_avg:37.36ms
step:261/2330 train_time:9748ms step_avg:37.35ms
step:262/2330 train_time:9788ms step_avg:37.36ms
step:263/2330 train_time:9825ms step_avg:37.36ms
step:264/2330 train_time:9866ms step_avg:37.37ms
step:265/2330 train_time:9902ms step_avg:37.36ms
step:266/2330 train_time:9942ms step_avg:37.38ms
step:267/2330 train_time:9978ms step_avg:37.37ms
step:268/2330 train_time:10018ms step_avg:37.38ms
step:269/2330 train_time:10054ms step_avg:37.38ms
step:270/2330 train_time:10094ms step_avg:37.39ms
step:271/2330 train_time:10129ms step_avg:37.38ms
step:272/2330 train_time:10169ms step_avg:37.39ms
step:273/2330 train_time:10204ms step_avg:37.38ms
step:274/2330 train_time:10244ms step_avg:37.39ms
step:275/2330 train_time:10279ms step_avg:37.38ms
step:276/2330 train_time:10320ms step_avg:37.39ms
step:277/2330 train_time:10355ms step_avg:37.38ms
step:278/2330 train_time:10397ms step_avg:37.40ms
step:279/2330 train_time:10432ms step_avg:37.39ms
step:280/2330 train_time:10473ms step_avg:37.40ms
step:281/2330 train_time:10507ms step_avg:37.39ms
step:282/2330 train_time:10547ms step_avg:37.40ms
step:283/2330 train_time:10582ms step_avg:37.39ms
step:284/2330 train_time:10623ms step_avg:37.41ms
step:285/2330 train_time:10658ms step_avg:37.40ms
step:286/2330 train_time:10698ms step_avg:37.41ms
step:287/2330 train_time:10733ms step_avg:37.40ms
step:288/2330 train_time:10774ms step_avg:37.41ms
step:289/2330 train_time:10810ms step_avg:37.40ms
step:290/2330 train_time:10850ms step_avg:37.41ms
step:291/2330 train_time:10886ms step_avg:37.41ms
step:292/2330 train_time:10926ms step_avg:37.42ms
step:293/2330 train_time:10961ms step_avg:37.41ms
step:294/2330 train_time:11002ms step_avg:37.42ms
step:295/2330 train_time:11037ms step_avg:37.41ms
step:296/2330 train_time:11077ms step_avg:37.42ms
step:297/2330 train_time:11112ms step_avg:37.41ms
step:298/2330 train_time:11153ms step_avg:37.42ms
step:299/2330 train_time:11187ms step_avg:37.42ms
step:300/2330 train_time:11227ms step_avg:37.42ms
step:301/2330 train_time:11262ms step_avg:37.41ms
step:302/2330 train_time:11303ms step_avg:37.43ms
step:303/2330 train_time:11337ms step_avg:37.42ms
step:304/2330 train_time:11379ms step_avg:37.43ms
step:305/2330 train_time:11413ms step_avg:37.42ms
step:306/2330 train_time:11455ms step_avg:37.43ms
step:307/2330 train_time:11489ms step_avg:37.42ms
step:308/2330 train_time:11529ms step_avg:37.43ms
step:309/2330 train_time:11564ms step_avg:37.42ms
step:310/2330 train_time:11605ms step_avg:37.43ms
step:311/2330 train_time:11639ms step_avg:37.42ms
step:312/2330 train_time:11680ms step_avg:37.43ms
step:313/2330 train_time:11715ms step_avg:37.43ms
step:314/2330 train_time:11756ms step_avg:37.44ms
step:315/2330 train_time:11791ms step_avg:37.43ms
step:316/2330 train_time:11832ms step_avg:37.44ms
step:317/2330 train_time:11866ms step_avg:37.43ms
step:318/2330 train_time:11906ms step_avg:37.44ms
step:319/2330 train_time:11942ms step_avg:37.43ms
step:320/2330 train_time:11982ms step_avg:37.44ms
step:321/2330 train_time:12017ms step_avg:37.44ms
step:322/2330 train_time:12057ms step_avg:37.45ms
step:323/2330 train_time:12092ms step_avg:37.44ms
step:324/2330 train_time:12132ms step_avg:37.45ms
step:325/2330 train_time:12167ms step_avg:37.44ms
step:326/2330 train_time:12207ms step_avg:37.45ms
step:327/2330 train_time:12242ms step_avg:37.44ms
step:328/2330 train_time:12283ms step_avg:37.45ms
step:329/2330 train_time:12318ms step_avg:37.44ms
step:330/2330 train_time:12358ms step_avg:37.45ms
step:331/2330 train_time:12393ms step_avg:37.44ms
step:332/2330 train_time:12434ms step_avg:37.45ms
step:333/2330 train_time:12469ms step_avg:37.44ms
step:334/2330 train_time:12509ms step_avg:37.45ms
step:335/2330 train_time:12544ms step_avg:37.45ms
step:336/2330 train_time:12585ms step_avg:37.46ms
step:337/2330 train_time:12620ms step_avg:37.45ms
step:338/2330 train_time:12661ms step_avg:37.46ms
step:339/2330 train_time:12696ms step_avg:37.45ms
step:340/2330 train_time:12737ms step_avg:37.46ms
step:341/2330 train_time:12772ms step_avg:37.46ms
step:342/2330 train_time:12813ms step_avg:37.46ms
step:343/2330 train_time:12847ms step_avg:37.46ms
step:344/2330 train_time:12888ms step_avg:37.47ms
step:345/2330 train_time:12923ms step_avg:37.46ms
step:346/2330 train_time:12963ms step_avg:37.47ms
step:347/2330 train_time:12998ms step_avg:37.46ms
step:348/2330 train_time:13039ms step_avg:37.47ms
step:349/2330 train_time:13074ms step_avg:37.46ms
step:350/2330 train_time:13114ms step_avg:37.47ms
step:351/2330 train_time:13149ms step_avg:37.46ms
step:352/2330 train_time:13189ms step_avg:37.47ms
step:353/2330 train_time:13224ms step_avg:37.46ms
step:354/2330 train_time:13265ms step_avg:37.47ms
step:355/2330 train_time:13300ms step_avg:37.46ms
step:356/2330 train_time:13340ms step_avg:37.47ms
step:357/2330 train_time:13376ms step_avg:37.47ms
step:358/2330 train_time:13417ms step_avg:37.48ms
step:359/2330 train_time:13451ms step_avg:37.47ms
step:360/2330 train_time:13492ms step_avg:37.48ms
step:361/2330 train_time:13525ms step_avg:37.47ms
step:362/2330 train_time:13566ms step_avg:37.47ms
step:363/2330 train_time:13601ms step_avg:37.47ms
step:364/2330 train_time:13641ms step_avg:37.48ms
step:365/2330 train_time:13677ms step_avg:37.47ms
step:366/2330 train_time:13717ms step_avg:37.48ms
step:367/2330 train_time:13752ms step_avg:37.47ms
step:368/2330 train_time:13793ms step_avg:37.48ms
step:369/2330 train_time:13828ms step_avg:37.47ms
step:370/2330 train_time:13868ms step_avg:37.48ms
step:371/2330 train_time:13903ms step_avg:37.47ms
step:372/2330 train_time:13944ms step_avg:37.48ms
step:373/2330 train_time:13980ms step_avg:37.48ms
step:374/2330 train_time:14020ms step_avg:37.49ms
step:375/2330 train_time:14056ms step_avg:37.48ms
step:376/2330 train_time:14096ms step_avg:37.49ms
step:377/2330 train_time:14132ms step_avg:37.49ms
step:378/2330 train_time:14173ms step_avg:37.49ms
step:379/2330 train_time:14208ms step_avg:37.49ms
step:380/2330 train_time:14248ms step_avg:37.50ms
step:381/2330 train_time:14283ms step_avg:37.49ms
step:382/2330 train_time:14324ms step_avg:37.50ms
step:383/2330 train_time:14359ms step_avg:37.49ms
step:384/2330 train_time:14400ms step_avg:37.50ms
step:385/2330 train_time:14436ms step_avg:37.50ms
step:386/2330 train_time:14477ms step_avg:37.50ms
step:387/2330 train_time:14512ms step_avg:37.50ms
step:388/2330 train_time:14552ms step_avg:37.51ms
step:389/2330 train_time:14587ms step_avg:37.50ms
step:390/2330 train_time:14627ms step_avg:37.51ms
step:391/2330 train_time:14662ms step_avg:37.50ms
step:392/2330 train_time:14703ms step_avg:37.51ms
step:393/2330 train_time:14739ms step_avg:37.50ms
step:394/2330 train_time:14780ms step_avg:37.51ms
step:395/2330 train_time:14815ms step_avg:37.51ms
step:396/2330 train_time:14856ms step_avg:37.52ms
step:397/2330 train_time:14891ms step_avg:37.51ms
step:398/2330 train_time:14932ms step_avg:37.52ms
step:399/2330 train_time:14966ms step_avg:37.51ms
step:400/2330 train_time:15006ms step_avg:37.52ms
step:401/2330 train_time:15042ms step_avg:37.51ms
step:402/2330 train_time:15083ms step_avg:37.52ms
step:403/2330 train_time:15117ms step_avg:37.51ms
step:404/2330 train_time:15159ms step_avg:37.52ms
step:405/2330 train_time:15193ms step_avg:37.51ms
step:406/2330 train_time:15234ms step_avg:37.52ms
step:407/2330 train_time:15269ms step_avg:37.52ms
step:408/2330 train_time:15309ms step_avg:37.52ms
step:409/2330 train_time:15344ms step_avg:37.52ms
step:410/2330 train_time:15385ms step_avg:37.53ms
step:411/2330 train_time:15420ms step_avg:37.52ms
step:412/2330 train_time:15460ms step_avg:37.52ms
step:413/2330 train_time:15495ms step_avg:37.52ms
step:414/2330 train_time:15537ms step_avg:37.53ms
step:415/2330 train_time:15571ms step_avg:37.52ms
step:416/2330 train_time:15611ms step_avg:37.53ms
step:417/2330 train_time:15646ms step_avg:37.52ms
step:418/2330 train_time:15687ms step_avg:37.53ms
step:419/2330 train_time:15722ms step_avg:37.52ms
step:420/2330 train_time:15763ms step_avg:37.53ms
step:421/2330 train_time:15797ms step_avg:37.52ms
step:422/2330 train_time:15838ms step_avg:37.53ms
step:423/2330 train_time:15873ms step_avg:37.53ms
step:424/2330 train_time:15914ms step_avg:37.53ms
step:425/2330 train_time:15949ms step_avg:37.53ms
step:426/2330 train_time:15990ms step_avg:37.54ms
step:427/2330 train_time:16025ms step_avg:37.53ms
step:428/2330 train_time:16066ms step_avg:37.54ms
step:429/2330 train_time:16100ms step_avg:37.53ms
step:430/2330 train_time:16141ms step_avg:37.54ms
step:431/2330 train_time:16177ms step_avg:37.53ms
step:432/2330 train_time:16218ms step_avg:37.54ms
step:433/2330 train_time:16254ms step_avg:37.54ms
step:434/2330 train_time:16295ms step_avg:37.54ms
step:435/2330 train_time:16329ms step_avg:37.54ms
step:436/2330 train_time:16369ms step_avg:37.54ms
step:437/2330 train_time:16405ms step_avg:37.54ms
step:438/2330 train_time:16445ms step_avg:37.55ms
step:439/2330 train_time:16480ms step_avg:37.54ms
step:440/2330 train_time:16521ms step_avg:37.55ms
step:441/2330 train_time:16556ms step_avg:37.54ms
step:442/2330 train_time:16597ms step_avg:37.55ms
step:443/2330 train_time:16631ms step_avg:37.54ms
step:444/2330 train_time:16672ms step_avg:37.55ms
step:445/2330 train_time:16707ms step_avg:37.54ms
step:446/2330 train_time:16748ms step_avg:37.55ms
step:447/2330 train_time:16784ms step_avg:37.55ms
step:448/2330 train_time:16824ms step_avg:37.55ms
step:449/2330 train_time:16860ms step_avg:37.55ms
step:450/2330 train_time:16901ms step_avg:37.56ms
step:451/2330 train_time:16936ms step_avg:37.55ms
step:452/2330 train_time:16977ms step_avg:37.56ms
step:453/2330 train_time:17012ms step_avg:37.55ms
step:454/2330 train_time:17052ms step_avg:37.56ms
step:455/2330 train_time:17087ms step_avg:37.55ms
step:456/2330 train_time:17128ms step_avg:37.56ms
step:457/2330 train_time:17163ms step_avg:37.55ms
step:458/2330 train_time:17204ms step_avg:37.56ms
step:459/2330 train_time:17239ms step_avg:37.56ms
step:460/2330 train_time:17280ms step_avg:37.56ms
step:461/2330 train_time:17315ms step_avg:37.56ms
step:462/2330 train_time:17356ms step_avg:37.57ms
step:463/2330 train_time:17391ms step_avg:37.56ms
step:464/2330 train_time:17432ms step_avg:37.57ms
step:465/2330 train_time:17467ms step_avg:37.56ms
step:466/2330 train_time:17508ms step_avg:37.57ms
step:467/2330 train_time:17543ms step_avg:37.57ms
step:468/2330 train_time:17584ms step_avg:37.57ms
step:469/2330 train_time:17619ms step_avg:37.57ms
step:470/2330 train_time:17659ms step_avg:37.57ms
step:471/2330 train_time:17695ms step_avg:37.57ms
step:472/2330 train_time:17736ms step_avg:37.58ms
step:473/2330 train_time:17772ms step_avg:37.57ms
step:474/2330 train_time:17812ms step_avg:37.58ms
step:475/2330 train_time:17847ms step_avg:37.57ms
step:476/2330 train_time:17888ms step_avg:37.58ms
step:477/2330 train_time:17923ms step_avg:37.57ms
step:478/2330 train_time:17964ms step_avg:37.58ms
step:479/2330 train_time:17999ms step_avg:37.58ms
step:480/2330 train_time:18040ms step_avg:37.58ms
step:481/2330 train_time:18075ms step_avg:37.58ms
step:482/2330 train_time:18116ms step_avg:37.58ms
step:483/2330 train_time:18151ms step_avg:37.58ms
step:484/2330 train_time:18192ms step_avg:37.59ms
step:485/2330 train_time:18227ms step_avg:37.58ms
step:486/2330 train_time:18268ms step_avg:37.59ms
step:487/2330 train_time:18302ms step_avg:37.58ms
step:488/2330 train_time:18343ms step_avg:37.59ms
step:489/2330 train_time:18379ms step_avg:37.59ms
step:490/2330 train_time:18421ms step_avg:37.59ms
step:491/2330 train_time:18455ms step_avg:37.59ms
step:492/2330 train_time:18497ms step_avg:37.60ms
step:493/2330 train_time:18532ms step_avg:37.59ms
step:494/2330 train_time:18573ms step_avg:37.60ms
step:495/2330 train_time:18607ms step_avg:37.59ms
step:496/2330 train_time:18648ms step_avg:37.60ms
step:497/2330 train_time:18683ms step_avg:37.59ms
step:498/2330 train_time:18724ms step_avg:37.60ms
step:499/2330 train_time:18759ms step_avg:37.59ms
step:500/2330 train_time:18800ms step_avg:37.60ms
step:500/2330 val_loss:5.6913 train_time:18911ms step_avg:37.82ms
step:501/2330 train_time:18923ms step_avg:37.77ms
step:502/2330 train_time:18934ms step_avg:37.72ms
step:503/2330 train_time:18943ms step_avg:37.66ms
step:504/2330 train_time:18955ms step_avg:37.61ms
step:505/2330 train_time:18989ms step_avg:37.60ms
step:506/2330 train_time:19030ms step_avg:37.61ms
step:507/2330 train_time:19064ms step_avg:37.60ms
step:508/2330 train_time:19105ms step_avg:37.61ms
step:509/2330 train_time:19139ms step_avg:37.60ms
step:510/2330 train_time:19180ms step_avg:37.61ms
step:511/2330 train_time:19214ms step_avg:37.60ms
step:512/2330 train_time:19258ms step_avg:37.61ms
step:513/2330 train_time:19293ms step_avg:37.61ms
step:514/2330 train_time:19335ms step_avg:37.62ms
step:515/2330 train_time:19371ms step_avg:37.61ms
step:516/2330 train_time:19412ms step_avg:37.62ms
step:517/2330 train_time:19447ms step_avg:37.62ms
step:518/2330 train_time:19488ms step_avg:37.62ms
step:519/2330 train_time:19524ms step_avg:37.62ms
step:520/2330 train_time:19565ms step_avg:37.62ms
step:521/2330 train_time:19600ms step_avg:37.62ms
step:522/2330 train_time:19641ms step_avg:37.63ms
step:523/2330 train_time:19676ms step_avg:37.62ms
step:524/2330 train_time:19716ms step_avg:37.63ms
step:525/2330 train_time:19752ms step_avg:37.62ms
step:526/2330 train_time:19792ms step_avg:37.63ms
step:527/2330 train_time:19828ms step_avg:37.62ms
step:528/2330 train_time:19868ms step_avg:37.63ms
step:529/2330 train_time:19903ms step_avg:37.62ms
step:530/2330 train_time:19944ms step_avg:37.63ms
step:531/2330 train_time:19980ms step_avg:37.63ms
step:532/2330 train_time:20020ms step_avg:37.63ms
step:533/2330 train_time:20055ms step_avg:37.63ms
step:534/2330 train_time:20095ms step_avg:37.63ms
step:535/2330 train_time:20130ms step_avg:37.63ms
step:536/2330 train_time:20171ms step_avg:37.63ms
step:537/2330 train_time:20206ms step_avg:37.63ms
step:538/2330 train_time:20248ms step_avg:37.64ms
step:539/2330 train_time:20284ms step_avg:37.63ms
step:540/2330 train_time:20325ms step_avg:37.64ms
step:541/2330 train_time:20360ms step_avg:37.63ms
step:542/2330 train_time:20401ms step_avg:37.64ms
step:543/2330 train_time:20436ms step_avg:37.64ms
step:544/2330 train_time:20477ms step_avg:37.64ms
step:545/2330 train_time:20513ms step_avg:37.64ms
step:546/2330 train_time:20554ms step_avg:37.64ms
step:547/2330 train_time:20589ms step_avg:37.64ms
step:548/2330 train_time:20630ms step_avg:37.65ms
step:549/2330 train_time:20665ms step_avg:37.64ms
step:550/2330 train_time:20706ms step_avg:37.65ms
step:551/2330 train_time:20741ms step_avg:37.64ms
step:552/2330 train_time:20781ms step_avg:37.65ms
step:553/2330 train_time:20816ms step_avg:37.64ms
step:554/2330 train_time:20857ms step_avg:37.65ms
step:555/2330 train_time:20891ms step_avg:37.64ms
step:556/2330 train_time:20932ms step_avg:37.65ms
step:557/2330 train_time:20967ms step_avg:37.64ms
step:558/2330 train_time:21008ms step_avg:37.65ms
step:559/2330 train_time:21043ms step_avg:37.64ms
step:560/2330 train_time:21083ms step_avg:37.65ms
step:561/2330 train_time:21118ms step_avg:37.64ms
step:562/2330 train_time:21158ms step_avg:37.65ms
step:563/2330 train_time:21193ms step_avg:37.64ms
step:564/2330 train_time:21235ms step_avg:37.65ms
step:565/2330 train_time:21270ms step_avg:37.65ms
step:566/2330 train_time:21311ms step_avg:37.65ms
step:567/2330 train_time:21347ms step_avg:37.65ms
step:568/2330 train_time:21388ms step_avg:37.66ms
step:569/2330 train_time:21424ms step_avg:37.65ms
step:570/2330 train_time:21466ms step_avg:37.66ms
step:571/2330 train_time:21501ms step_avg:37.66ms
step:572/2330 train_time:21542ms step_avg:37.66ms
step:573/2330 train_time:21576ms step_avg:37.66ms
step:574/2330 train_time:21617ms step_avg:37.66ms
step:575/2330 train_time:21652ms step_avg:37.66ms
step:576/2330 train_time:21693ms step_avg:37.66ms
step:577/2330 train_time:21728ms step_avg:37.66ms
step:578/2330 train_time:21769ms step_avg:37.66ms
step:579/2330 train_time:21804ms step_avg:37.66ms
step:580/2330 train_time:21845ms step_avg:37.66ms
step:581/2330 train_time:21879ms step_avg:37.66ms
step:582/2330 train_time:21920ms step_avg:37.66ms
step:583/2330 train_time:21955ms step_avg:37.66ms
step:584/2330 train_time:21996ms step_avg:37.66ms
step:585/2330 train_time:22030ms step_avg:37.66ms
step:586/2330 train_time:22071ms step_avg:37.66ms
step:587/2330 train_time:22107ms step_avg:37.66ms
step:588/2330 train_time:22148ms step_avg:37.67ms
step:589/2330 train_time:22183ms step_avg:37.66ms
step:590/2330 train_time:22223ms step_avg:37.67ms
step:591/2330 train_time:22258ms step_avg:37.66ms
step:592/2330 train_time:22299ms step_avg:37.67ms
step:593/2330 train_time:22334ms step_avg:37.66ms
step:594/2330 train_time:22375ms step_avg:37.67ms
step:595/2330 train_time:22411ms step_avg:37.67ms
step:596/2330 train_time:22452ms step_avg:37.67ms
step:597/2330 train_time:22488ms step_avg:37.67ms
step:598/2330 train_time:22529ms step_avg:37.67ms
step:599/2330 train_time:22565ms step_avg:37.67ms
step:600/2330 train_time:22606ms step_avg:37.68ms
step:601/2330 train_time:22642ms step_avg:37.67ms
step:602/2330 train_time:22683ms step_avg:37.68ms
step:603/2330 train_time:22717ms step_avg:37.67ms
step:604/2330 train_time:22758ms step_avg:37.68ms
step:605/2330 train_time:22793ms step_avg:37.67ms
step:606/2330 train_time:22833ms step_avg:37.68ms
step:607/2330 train_time:22869ms step_avg:37.68ms
step:608/2330 train_time:22910ms step_avg:37.68ms
step:609/2330 train_time:22946ms step_avg:37.68ms
step:610/2330 train_time:22987ms step_avg:37.68ms
step:611/2330 train_time:23022ms step_avg:37.68ms
step:612/2330 train_time:23063ms step_avg:37.68ms
step:613/2330 train_time:23097ms step_avg:37.68ms
step:614/2330 train_time:23138ms step_avg:37.68ms
step:615/2330 train_time:23173ms step_avg:37.68ms
step:616/2330 train_time:23215ms step_avg:37.69ms
step:617/2330 train_time:23249ms step_avg:37.68ms
step:618/2330 train_time:23290ms step_avg:37.69ms
step:619/2330 train_time:23325ms step_avg:37.68ms
step:620/2330 train_time:23366ms step_avg:37.69ms
step:621/2330 train_time:23401ms step_avg:37.68ms
step:622/2330 train_time:23442ms step_avg:37.69ms
step:623/2330 train_time:23477ms step_avg:37.68ms
step:624/2330 train_time:23518ms step_avg:37.69ms
step:625/2330 train_time:23553ms step_avg:37.69ms
step:626/2330 train_time:23594ms step_avg:37.69ms
step:627/2330 train_time:23630ms step_avg:37.69ms
step:628/2330 train_time:23671ms step_avg:37.69ms
step:629/2330 train_time:23707ms step_avg:37.69ms
step:630/2330 train_time:23748ms step_avg:37.69ms
step:631/2330 train_time:23783ms step_avg:37.69ms
step:632/2330 train_time:23824ms step_avg:37.70ms
step:633/2330 train_time:23858ms step_avg:37.69ms
step:634/2330 train_time:23899ms step_avg:37.70ms
step:635/2330 train_time:23934ms step_avg:37.69ms
step:636/2330 train_time:23975ms step_avg:37.70ms
step:637/2330 train_time:24010ms step_avg:37.69ms
step:638/2330 train_time:24051ms step_avg:37.70ms
step:639/2330 train_time:24087ms step_avg:37.69ms
step:640/2330 train_time:24127ms step_avg:37.70ms
step:641/2330 train_time:24162ms step_avg:37.69ms
step:642/2330 train_time:24203ms step_avg:37.70ms
step:643/2330 train_time:24238ms step_avg:37.70ms
step:644/2330 train_time:24278ms step_avg:37.70ms
step:645/2330 train_time:24314ms step_avg:37.70ms
step:646/2330 train_time:24355ms step_avg:37.70ms
step:647/2330 train_time:24390ms step_avg:37.70ms
step:648/2330 train_time:24430ms step_avg:37.70ms
step:649/2330 train_time:24466ms step_avg:37.70ms
step:650/2330 train_time:24507ms step_avg:37.70ms
step:651/2330 train_time:24542ms step_avg:37.70ms
step:652/2330 train_time:24583ms step_avg:37.70ms
step:653/2330 train_time:24617ms step_avg:37.70ms
step:654/2330 train_time:24658ms step_avg:37.70ms
step:655/2330 train_time:24694ms step_avg:37.70ms
step:656/2330 train_time:24735ms step_avg:37.71ms
step:657/2330 train_time:24771ms step_avg:37.70ms
step:658/2330 train_time:24811ms step_avg:37.71ms
step:659/2330 train_time:24847ms step_avg:37.70ms
step:660/2330 train_time:24887ms step_avg:37.71ms
step:661/2330 train_time:24924ms step_avg:37.71ms
step:662/2330 train_time:24964ms step_avg:37.71ms
step:663/2330 train_time:25000ms step_avg:37.71ms
step:664/2330 train_time:25041ms step_avg:37.71ms
step:665/2330 train_time:25075ms step_avg:37.71ms
step:666/2330 train_time:25116ms step_avg:37.71ms
step:667/2330 train_time:25151ms step_avg:37.71ms
step:668/2330 train_time:25193ms step_avg:37.71ms
step:669/2330 train_time:25228ms step_avg:37.71ms
step:670/2330 train_time:25268ms step_avg:37.71ms
step:671/2330 train_time:25304ms step_avg:37.71ms
step:672/2330 train_time:25344ms step_avg:37.72ms
step:673/2330 train_time:25379ms step_avg:37.71ms
step:674/2330 train_time:25420ms step_avg:37.71ms
step:675/2330 train_time:25455ms step_avg:37.71ms
step:676/2330 train_time:25496ms step_avg:37.72ms
step:677/2330 train_time:25530ms step_avg:37.71ms
step:678/2330 train_time:25571ms step_avg:37.72ms
step:679/2330 train_time:25606ms step_avg:37.71ms
step:680/2330 train_time:25648ms step_avg:37.72ms
step:681/2330 train_time:25682ms step_avg:37.71ms
step:682/2330 train_time:25723ms step_avg:37.72ms
step:683/2330 train_time:25758ms step_avg:37.71ms
step:684/2330 train_time:25798ms step_avg:37.72ms
step:685/2330 train_time:25834ms step_avg:37.71ms
step:686/2330 train_time:25875ms step_avg:37.72ms
step:687/2330 train_time:25910ms step_avg:37.72ms
step:688/2330 train_time:25951ms step_avg:37.72ms
step:689/2330 train_time:25987ms step_avg:37.72ms
step:690/2330 train_time:26028ms step_avg:37.72ms
step:691/2330 train_time:26063ms step_avg:37.72ms
step:692/2330 train_time:26104ms step_avg:37.72ms
step:693/2330 train_time:26139ms step_avg:37.72ms
step:694/2330 train_time:26180ms step_avg:37.72ms
step:695/2330 train_time:26215ms step_avg:37.72ms
step:696/2330 train_time:26256ms step_avg:37.72ms
step:697/2330 train_time:26291ms step_avg:37.72ms
step:698/2330 train_time:26331ms step_avg:37.72ms
step:699/2330 train_time:26367ms step_avg:37.72ms
step:700/2330 train_time:26407ms step_avg:37.72ms
step:701/2330 train_time:26443ms step_avg:37.72ms
step:702/2330 train_time:26484ms step_avg:37.73ms
step:703/2330 train_time:26520ms step_avg:37.72ms
step:704/2330 train_time:26560ms step_avg:37.73ms
step:705/2330 train_time:26596ms step_avg:37.73ms
step:706/2330 train_time:26637ms step_avg:37.73ms
step:707/2330 train_time:26672ms step_avg:37.73ms
step:708/2330 train_time:26712ms step_avg:37.73ms
step:709/2330 train_time:26749ms step_avg:37.73ms
step:710/2330 train_time:26789ms step_avg:37.73ms
step:711/2330 train_time:26825ms step_avg:37.73ms
step:712/2330 train_time:26866ms step_avg:37.73ms
step:713/2330 train_time:26901ms step_avg:37.73ms
step:714/2330 train_time:26941ms step_avg:37.73ms
step:715/2330 train_time:26977ms step_avg:37.73ms
step:716/2330 train_time:27017ms step_avg:37.73ms
step:717/2330 train_time:27052ms step_avg:37.73ms
step:718/2330 train_time:27093ms step_avg:37.73ms
step:719/2330 train_time:27128ms step_avg:37.73ms
step:720/2330 train_time:27169ms step_avg:37.73ms
step:721/2330 train_time:27205ms step_avg:37.73ms
step:722/2330 train_time:27245ms step_avg:37.74ms
step:723/2330 train_time:27280ms step_avg:37.73ms
step:724/2330 train_time:27321ms step_avg:37.74ms
step:725/2330 train_time:27356ms step_avg:37.73ms
step:726/2330 train_time:27397ms step_avg:37.74ms
step:727/2330 train_time:27432ms step_avg:37.73ms
step:728/2330 train_time:27473ms step_avg:37.74ms
step:729/2330 train_time:27508ms step_avg:37.73ms
step:730/2330 train_time:27550ms step_avg:37.74ms
step:731/2330 train_time:27584ms step_avg:37.73ms
step:732/2330 train_time:27626ms step_avg:37.74ms
step:733/2330 train_time:27660ms step_avg:37.74ms
step:734/2330 train_time:27701ms step_avg:37.74ms
step:735/2330 train_time:27735ms step_avg:37.73ms
step:736/2330 train_time:27776ms step_avg:37.74ms
step:737/2330 train_time:27811ms step_avg:37.74ms
step:738/2330 train_time:27853ms step_avg:37.74ms
step:739/2330 train_time:27888ms step_avg:37.74ms
step:740/2330 train_time:27929ms step_avg:37.74ms
step:741/2330 train_time:27964ms step_avg:37.74ms
step:742/2330 train_time:28006ms step_avg:37.74ms
step:743/2330 train_time:28041ms step_avg:37.74ms
step:744/2330 train_time:28082ms step_avg:37.74ms
step:745/2330 train_time:28117ms step_avg:37.74ms
step:746/2330 train_time:28158ms step_avg:37.74ms
step:747/2330 train_time:28193ms step_avg:37.74ms
step:748/2330 train_time:28233ms step_avg:37.75ms
step:749/2330 train_time:28269ms step_avg:37.74ms
step:750/2330 train_time:28310ms step_avg:37.75ms
step:750/2330 val_loss:5.5710 train_time:28423ms step_avg:37.90ms
step:751/2330 train_time:28435ms step_avg:37.86ms
step:752/2330 train_time:28447ms step_avg:37.83ms
step:753/2330 train_time:28457ms step_avg:37.79ms
step:754/2330 train_time:28467ms step_avg:37.76ms
step:755/2330 train_time:28501ms step_avg:37.75ms
step:756/2330 train_time:28541ms step_avg:37.75ms
step:757/2330 train_time:28575ms step_avg:37.75ms
step:758/2330 train_time:28615ms step_avg:37.75ms
step:759/2330 train_time:28650ms step_avg:37.75ms
step:760/2330 train_time:28691ms step_avg:37.75ms
step:761/2330 train_time:28727ms step_avg:37.75ms
step:762/2330 train_time:28768ms step_avg:37.75ms
step:763/2330 train_time:28811ms step_avg:37.76ms
step:764/2330 train_time:28851ms step_avg:37.76ms
step:765/2330 train_time:28888ms step_avg:37.76ms
step:766/2330 train_time:28929ms step_avg:37.77ms
step:767/2330 train_time:28964ms step_avg:37.76ms
step:768/2330 train_time:29004ms step_avg:37.77ms
step:769/2330 train_time:29040ms step_avg:37.76ms
step:770/2330 train_time:29080ms step_avg:37.77ms
step:771/2330 train_time:29115ms step_avg:37.76ms
step:772/2330 train_time:29156ms step_avg:37.77ms
step:773/2330 train_time:29190ms step_avg:37.76ms
step:774/2330 train_time:29230ms step_avg:37.77ms
step:775/2330 train_time:29265ms step_avg:37.76ms
step:776/2330 train_time:29305ms step_avg:37.76ms
step:777/2330 train_time:29341ms step_avg:37.76ms
step:778/2330 train_time:29381ms step_avg:37.76ms
step:779/2330 train_time:29416ms step_avg:37.76ms
step:780/2330 train_time:29457ms step_avg:37.77ms
step:781/2330 train_time:29492ms step_avg:37.76ms
step:782/2330 train_time:29532ms step_avg:37.76ms
step:783/2330 train_time:29567ms step_avg:37.76ms
step:784/2330 train_time:29607ms step_avg:37.76ms
step:785/2330 train_time:29642ms step_avg:37.76ms
step:786/2330 train_time:29683ms step_avg:37.76ms
step:787/2330 train_time:29719ms step_avg:37.76ms
step:788/2330 train_time:29760ms step_avg:37.77ms
step:789/2330 train_time:29797ms step_avg:37.77ms
step:790/2330 train_time:29838ms step_avg:37.77ms
step:791/2330 train_time:29874ms step_avg:37.77ms
step:792/2330 train_time:29915ms step_avg:37.77ms
step:793/2330 train_time:29951ms step_avg:37.77ms
step:794/2330 train_time:29992ms step_avg:37.77ms
step:795/2330 train_time:30027ms step_avg:37.77ms
step:796/2330 train_time:30068ms step_avg:37.77ms
step:797/2330 train_time:30102ms step_avg:37.77ms
step:798/2330 train_time:30143ms step_avg:37.77ms
step:799/2330 train_time:30178ms step_avg:37.77ms
step:800/2330 train_time:30218ms step_avg:37.77ms
step:801/2330 train_time:30253ms step_avg:37.77ms
step:802/2330 train_time:30293ms step_avg:37.77ms
step:803/2330 train_time:30328ms step_avg:37.77ms
step:804/2330 train_time:30368ms step_avg:37.77ms
step:805/2330 train_time:30403ms step_avg:37.77ms
step:806/2330 train_time:30444ms step_avg:37.77ms
step:807/2330 train_time:30478ms step_avg:37.77ms
step:808/2330 train_time:30519ms step_avg:37.77ms
step:809/2330 train_time:30554ms step_avg:37.77ms
step:810/2330 train_time:30595ms step_avg:37.77ms
step:811/2330 train_time:30630ms step_avg:37.77ms
step:812/2330 train_time:30671ms step_avg:37.77ms
step:813/2330 train_time:30707ms step_avg:37.77ms
step:814/2330 train_time:30747ms step_avg:37.77ms
step:815/2330 train_time:30784ms step_avg:37.77ms
step:816/2330 train_time:30825ms step_avg:37.78ms
step:817/2330 train_time:30861ms step_avg:37.77ms
step:818/2330 train_time:30901ms step_avg:37.78ms
step:819/2330 train_time:30937ms step_avg:37.77ms
step:820/2330 train_time:30978ms step_avg:37.78ms
step:821/2330 train_time:31014ms step_avg:37.78ms
step:822/2330 train_time:31055ms step_avg:37.78ms
step:823/2330 train_time:31090ms step_avg:37.78ms
step:824/2330 train_time:31131ms step_avg:37.78ms
step:825/2330 train_time:31166ms step_avg:37.78ms
step:826/2330 train_time:31206ms step_avg:37.78ms
step:827/2330 train_time:31241ms step_avg:37.78ms
step:828/2330 train_time:31283ms step_avg:37.78ms
step:829/2330 train_time:31317ms step_avg:37.78ms
step:830/2330 train_time:31358ms step_avg:37.78ms
step:831/2330 train_time:31392ms step_avg:37.78ms
step:832/2330 train_time:31433ms step_avg:37.78ms
step:833/2330 train_time:31467ms step_avg:37.78ms
step:834/2330 train_time:31508ms step_avg:37.78ms
step:835/2330 train_time:31543ms step_avg:37.78ms
step:836/2330 train_time:31584ms step_avg:37.78ms
step:837/2330 train_time:31620ms step_avg:37.78ms
step:838/2330 train_time:31661ms step_avg:37.78ms
step:839/2330 train_time:31697ms step_avg:37.78ms
step:840/2330 train_time:31737ms step_avg:37.78ms
step:841/2330 train_time:31773ms step_avg:37.78ms
step:842/2330 train_time:31814ms step_avg:37.78ms
step:843/2330 train_time:31849ms step_avg:37.78ms
step:844/2330 train_time:31889ms step_avg:37.78ms
step:845/2330 train_time:31926ms step_avg:37.78ms
step:846/2330 train_time:31966ms step_avg:37.79ms
step:847/2330 train_time:32002ms step_avg:37.78ms
step:848/2330 train_time:32043ms step_avg:37.79ms
step:849/2330 train_time:32078ms step_avg:37.78ms
step:850/2330 train_time:32119ms step_avg:37.79ms
step:851/2330 train_time:32155ms step_avg:37.78ms
step:852/2330 train_time:32195ms step_avg:37.79ms
step:853/2330 train_time:32232ms step_avg:37.79ms
step:854/2330 train_time:32272ms step_avg:37.79ms
step:855/2330 train_time:32307ms step_avg:37.79ms
step:856/2330 train_time:32347ms step_avg:37.79ms
step:857/2330 train_time:32382ms step_avg:37.79ms
step:858/2330 train_time:32423ms step_avg:37.79ms
step:859/2330 train_time:32458ms step_avg:37.79ms
step:860/2330 train_time:32498ms step_avg:37.79ms
step:861/2330 train_time:32534ms step_avg:37.79ms
step:862/2330 train_time:32574ms step_avg:37.79ms
step:863/2330 train_time:32609ms step_avg:37.79ms
step:864/2330 train_time:32650ms step_avg:37.79ms
step:865/2330 train_time:32685ms step_avg:37.79ms
step:866/2330 train_time:32726ms step_avg:37.79ms
step:867/2330 train_time:32761ms step_avg:37.79ms
step:868/2330 train_time:32802ms step_avg:37.79ms
step:869/2330 train_time:32838ms step_avg:37.79ms
step:870/2330 train_time:32878ms step_avg:37.79ms
step:871/2330 train_time:32914ms step_avg:37.79ms
step:872/2330 train_time:32955ms step_avg:37.79ms
step:873/2330 train_time:32990ms step_avg:37.79ms
step:874/2330 train_time:33031ms step_avg:37.79ms
step:875/2330 train_time:33065ms step_avg:37.79ms
step:876/2330 train_time:33106ms step_avg:37.79ms
step:877/2330 train_time:33142ms step_avg:37.79ms
step:878/2330 train_time:33182ms step_avg:37.79ms
step:879/2330 train_time:33219ms step_avg:37.79ms
step:880/2330 train_time:33259ms step_avg:37.79ms
step:881/2330 train_time:33295ms step_avg:37.79ms
step:882/2330 train_time:33335ms step_avg:37.80ms
step:883/2330 train_time:33371ms step_avg:37.79ms
step:884/2330 train_time:33411ms step_avg:37.80ms
step:885/2330 train_time:33446ms step_avg:37.79ms
step:886/2330 train_time:33487ms step_avg:37.80ms
step:887/2330 train_time:33522ms step_avg:37.79ms
step:888/2330 train_time:33563ms step_avg:37.80ms
step:889/2330 train_time:33598ms step_avg:37.79ms
step:890/2330 train_time:33639ms step_avg:37.80ms
step:891/2330 train_time:33674ms step_avg:37.79ms
step:892/2330 train_time:33715ms step_avg:37.80ms
step:893/2330 train_time:33750ms step_avg:37.79ms
step:894/2330 train_time:33790ms step_avg:37.80ms
step:895/2330 train_time:33825ms step_avg:37.79ms
step:896/2330 train_time:33866ms step_avg:37.80ms
step:897/2330 train_time:33902ms step_avg:37.80ms
step:898/2330 train_time:33943ms step_avg:37.80ms
step:899/2330 train_time:33979ms step_avg:37.80ms
step:900/2330 train_time:34019ms step_avg:37.80ms
step:901/2330 train_time:34055ms step_avg:37.80ms
step:902/2330 train_time:34097ms step_avg:37.80ms
step:903/2330 train_time:34132ms step_avg:37.80ms
step:904/2330 train_time:34173ms step_avg:37.80ms
step:905/2330 train_time:34208ms step_avg:37.80ms
step:906/2330 train_time:34249ms step_avg:37.80ms
step:907/2330 train_time:34284ms step_avg:37.80ms
step:908/2330 train_time:34325ms step_avg:37.80ms
step:909/2330 train_time:34360ms step_avg:37.80ms
step:910/2330 train_time:34401ms step_avg:37.80ms
step:911/2330 train_time:34436ms step_avg:37.80ms
step:912/2330 train_time:34477ms step_avg:37.80ms
step:913/2330 train_time:34513ms step_avg:37.80ms
step:914/2330 train_time:34554ms step_avg:37.81ms
step:915/2330 train_time:34590ms step_avg:37.80ms
step:916/2330 train_time:34631ms step_avg:37.81ms
step:917/2330 train_time:34666ms step_avg:37.80ms
step:918/2330 train_time:34707ms step_avg:37.81ms
step:919/2330 train_time:34742ms step_avg:37.80ms
step:920/2330 train_time:34783ms step_avg:37.81ms
step:921/2330 train_time:34819ms step_avg:37.81ms
step:922/2330 train_time:34859ms step_avg:37.81ms
step:923/2330 train_time:34895ms step_avg:37.81ms
step:924/2330 train_time:34936ms step_avg:37.81ms
step:925/2330 train_time:34972ms step_avg:37.81ms
step:926/2330 train_time:35013ms step_avg:37.81ms
step:927/2330 train_time:35048ms step_avg:37.81ms
step:928/2330 train_time:35088ms step_avg:37.81ms
step:929/2330 train_time:35124ms step_avg:37.81ms
step:930/2330 train_time:35165ms step_avg:37.81ms
step:931/2330 train_time:35201ms step_avg:37.81ms
step:932/2330 train_time:35242ms step_avg:37.81ms
step:933/2330 train_time:35277ms step_avg:37.81ms
step:934/2330 train_time:35318ms step_avg:37.81ms
step:935/2330 train_time:35353ms step_avg:37.81ms
step:936/2330 train_time:35393ms step_avg:37.81ms
step:937/2330 train_time:35428ms step_avg:37.81ms
step:938/2330 train_time:35469ms step_avg:37.81ms
step:939/2330 train_time:35504ms step_avg:37.81ms
step:940/2330 train_time:35545ms step_avg:37.81ms
step:941/2330 train_time:35580ms step_avg:37.81ms
step:942/2330 train_time:35621ms step_avg:37.81ms
step:943/2330 train_time:35657ms step_avg:37.81ms
step:944/2330 train_time:35697ms step_avg:37.82ms
step:945/2330 train_time:35734ms step_avg:37.81ms
step:946/2330 train_time:35774ms step_avg:37.82ms
step:947/2330 train_time:35810ms step_avg:37.81ms
step:948/2330 train_time:35850ms step_avg:37.82ms
step:949/2330 train_time:35886ms step_avg:37.81ms
step:950/2330 train_time:35927ms step_avg:37.82ms
step:951/2330 train_time:35962ms step_avg:37.82ms
step:952/2330 train_time:36003ms step_avg:37.82ms
step:953/2330 train_time:36039ms step_avg:37.82ms
step:954/2330 train_time:36079ms step_avg:37.82ms
step:955/2330 train_time:36116ms step_avg:37.82ms
step:956/2330 train_time:36156ms step_avg:37.82ms
step:957/2330 train_time:36192ms step_avg:37.82ms
step:958/2330 train_time:36233ms step_avg:37.82ms
step:959/2330 train_time:36268ms step_avg:37.82ms
step:960/2330 train_time:36309ms step_avg:37.82ms
step:961/2330 train_time:36344ms step_avg:37.82ms
step:962/2330 train_time:36385ms step_avg:37.82ms
step:963/2330 train_time:36420ms step_avg:37.82ms
step:964/2330 train_time:36461ms step_avg:37.82ms
step:965/2330 train_time:36496ms step_avg:37.82ms
step:966/2330 train_time:36537ms step_avg:37.82ms
step:967/2330 train_time:36572ms step_avg:37.82ms
step:968/2330 train_time:36613ms step_avg:37.82ms
step:969/2330 train_time:36648ms step_avg:37.82ms
step:970/2330 train_time:36688ms step_avg:37.82ms
step:971/2330 train_time:36723ms step_avg:37.82ms
step:972/2330 train_time:36764ms step_avg:37.82ms
step:973/2330 train_time:36800ms step_avg:37.82ms
step:974/2330 train_time:36840ms step_avg:37.82ms
step:975/2330 train_time:36876ms step_avg:37.82ms
step:976/2330 train_time:36917ms step_avg:37.83ms
step:977/2330 train_time:36953ms step_avg:37.82ms
step:978/2330 train_time:36994ms step_avg:37.83ms
step:979/2330 train_time:37030ms step_avg:37.82ms
step:980/2330 train_time:37070ms step_avg:37.83ms
step:981/2330 train_time:37106ms step_avg:37.82ms
step:982/2330 train_time:37147ms step_avg:37.83ms
step:983/2330 train_time:37182ms step_avg:37.82ms
step:984/2330 train_time:37223ms step_avg:37.83ms
step:985/2330 train_time:37258ms step_avg:37.83ms
step:986/2330 train_time:37299ms step_avg:37.83ms
step:987/2330 train_time:37335ms step_avg:37.83ms
step:988/2330 train_time:37376ms step_avg:37.83ms
step:989/2330 train_time:37412ms step_avg:37.83ms
step:990/2330 train_time:37453ms step_avg:37.83ms
step:991/2330 train_time:37488ms step_avg:37.83ms
step:992/2330 train_time:37528ms step_avg:37.83ms
step:993/2330 train_time:37563ms step_avg:37.83ms
step:994/2330 train_time:37604ms step_avg:37.83ms
step:995/2330 train_time:37639ms step_avg:37.83ms
step:996/2330 train_time:37680ms step_avg:37.83ms
step:997/2330 train_time:37716ms step_avg:37.83ms
step:998/2330 train_time:37756ms step_avg:37.83ms
step:999/2330 train_time:37793ms step_avg:37.83ms
step:1000/2330 train_time:37833ms step_avg:37.83ms
step:1000/2330 val_loss:5.5012 train_time:37945ms step_avg:37.94ms
step:1001/2330 train_time:37957ms step_avg:37.92ms
step:1002/2330 train_time:37968ms step_avg:37.89ms
step:1003/2330 train_time:37979ms step_avg:37.87ms
step:1004/2330 train_time:37990ms step_avg:37.84ms
step:1005/2330 train_time:38022ms step_avg:37.83ms
step:1006/2330 train_time:38062ms step_avg:37.84ms
step:1007/2330 train_time:38097ms step_avg:37.83ms
step:1008/2330 train_time:38137ms step_avg:37.83ms
step:1009/2330 train_time:38172ms step_avg:37.83ms
step:1010/2330 train_time:38212ms step_avg:37.83ms
step:1011/2330 train_time:38247ms step_avg:37.83ms
step:1012/2330 train_time:38288ms step_avg:37.83ms
step:1013/2330 train_time:38326ms step_avg:37.83ms
step:1014/2330 train_time:38367ms step_avg:37.84ms
step:1015/2330 train_time:38404ms step_avg:37.84ms
step:1016/2330 train_time:38444ms step_avg:37.84ms
step:1017/2330 train_time:38480ms step_avg:37.84ms
step:1018/2330 train_time:38520ms step_avg:37.84ms
step:1019/2330 train_time:38555ms step_avg:37.84ms
step:1020/2330 train_time:38595ms step_avg:37.84ms
step:1021/2330 train_time:38630ms step_avg:37.84ms
step:1022/2330 train_time:38670ms step_avg:37.84ms
step:1023/2330 train_time:38705ms step_avg:37.83ms
step:1024/2330 train_time:38746ms step_avg:37.84ms
step:1025/2330 train_time:38782ms step_avg:37.84ms
step:1026/2330 train_time:38822ms step_avg:37.84ms
step:1027/2330 train_time:38858ms step_avg:37.84ms
step:1028/2330 train_time:38899ms step_avg:37.84ms
step:1029/2330 train_time:38935ms step_avg:37.84ms
step:1030/2330 train_time:38975ms step_avg:37.84ms
step:1031/2330 train_time:39011ms step_avg:37.84ms
step:1032/2330 train_time:39051ms step_avg:37.84ms
step:1033/2330 train_time:39086ms step_avg:37.84ms
step:1034/2330 train_time:39127ms step_avg:37.84ms
step:1035/2330 train_time:39162ms step_avg:37.84ms
step:1036/2330 train_time:39203ms step_avg:37.84ms
step:1037/2330 train_time:39239ms step_avg:37.84ms
step:1038/2330 train_time:39280ms step_avg:37.84ms
step:1039/2330 train_time:39316ms step_avg:37.84ms
step:1040/2330 train_time:39356ms step_avg:37.84ms
step:1041/2330 train_time:39393ms step_avg:37.84ms
step:1042/2330 train_time:39433ms step_avg:37.84ms
step:1043/2330 train_time:39468ms step_avg:37.84ms
step:1044/2330 train_time:39508ms step_avg:37.84ms
step:1045/2330 train_time:39544ms step_avg:37.84ms
step:1046/2330 train_time:39585ms step_avg:37.84ms
step:1047/2330 train_time:39620ms step_avg:37.84ms
step:1048/2330 train_time:39660ms step_avg:37.84ms
step:1049/2330 train_time:39694ms step_avg:37.84ms
step:1050/2330 train_time:39735ms step_avg:37.84ms
step:1051/2330 train_time:39770ms step_avg:37.84ms
step:1052/2330 train_time:39811ms step_avg:37.84ms
step:1053/2330 train_time:39846ms step_avg:37.84ms
step:1054/2330 train_time:39887ms step_avg:37.84ms
step:1055/2330 train_time:39922ms step_avg:37.84ms
step:1056/2330 train_time:39962ms step_avg:37.84ms
step:1057/2330 train_time:39998ms step_avg:37.84ms
step:1058/2330 train_time:40038ms step_avg:37.84ms
step:1059/2330 train_time:40075ms step_avg:37.84ms
step:1060/2330 train_time:40116ms step_avg:37.85ms
step:1061/2330 train_time:40151ms step_avg:37.84ms
step:1062/2330 train_time:40193ms step_avg:37.85ms
step:1063/2330 train_time:40228ms step_avg:37.84ms
step:1064/2330 train_time:40268ms step_avg:37.85ms
step:1065/2330 train_time:40304ms step_avg:37.84ms
step:1066/2330 train_time:40344ms step_avg:37.85ms
step:1067/2330 train_time:40381ms step_avg:37.85ms
step:1068/2330 train_time:40422ms step_avg:37.85ms
step:1069/2330 train_time:40457ms step_avg:37.85ms
step:1070/2330 train_time:40498ms step_avg:37.85ms
step:1071/2330 train_time:40532ms step_avg:37.85ms
step:1072/2330 train_time:40573ms step_avg:37.85ms
step:1073/2330 train_time:40607ms step_avg:37.84ms
step:1074/2330 train_time:40648ms step_avg:37.85ms
step:1075/2330 train_time:40683ms step_avg:37.84ms
step:1076/2330 train_time:40724ms step_avg:37.85ms
step:1077/2330 train_time:40759ms step_avg:37.84ms
step:1078/2330 train_time:40799ms step_avg:37.85ms
step:1079/2330 train_time:40835ms step_avg:37.85ms
step:1080/2330 train_time:40876ms step_avg:37.85ms
step:1081/2330 train_time:40912ms step_avg:37.85ms
step:1082/2330 train_time:40952ms step_avg:37.85ms
step:1083/2330 train_time:40988ms step_avg:37.85ms
step:1084/2330 train_time:41028ms step_avg:37.85ms
step:1085/2330 train_time:41063ms step_avg:37.85ms
step:1086/2330 train_time:41104ms step_avg:37.85ms
step:1087/2330 train_time:41140ms step_avg:37.85ms
step:1088/2330 train_time:41181ms step_avg:37.85ms
step:1089/2330 train_time:41216ms step_avg:37.85ms
step:1090/2330 train_time:41257ms step_avg:37.85ms
step:1091/2330 train_time:41292ms step_avg:37.85ms
step:1092/2330 train_time:41332ms step_avg:37.85ms
step:1093/2330 train_time:41368ms step_avg:37.85ms
step:1094/2330 train_time:41409ms step_avg:37.85ms
step:1095/2330 train_time:41445ms step_avg:37.85ms
step:1096/2330 train_time:41486ms step_avg:37.85ms
step:1097/2330 train_time:41521ms step_avg:37.85ms
step:1098/2330 train_time:41562ms step_avg:37.85ms
step:1099/2330 train_time:41597ms step_avg:37.85ms
step:1100/2330 train_time:41638ms step_avg:37.85ms
step:1101/2330 train_time:41673ms step_avg:37.85ms
step:1102/2330 train_time:41714ms step_avg:37.85ms
step:1103/2330 train_time:41748ms step_avg:37.85ms
step:1104/2330 train_time:41789ms step_avg:37.85ms
step:1105/2330 train_time:41824ms step_avg:37.85ms
step:1106/2330 train_time:41864ms step_avg:37.85ms
step:1107/2330 train_time:41900ms step_avg:37.85ms
step:1108/2330 train_time:41941ms step_avg:37.85ms
step:1109/2330 train_time:41976ms step_avg:37.85ms
step:1110/2330 train_time:42017ms step_avg:37.85ms
step:1111/2330 train_time:42052ms step_avg:37.85ms
step:1112/2330 train_time:42092ms step_avg:37.85ms
step:1113/2330 train_time:42127ms step_avg:37.85ms
step:1114/2330 train_time:42168ms step_avg:37.85ms
step:1115/2330 train_time:42203ms step_avg:37.85ms
step:1116/2330 train_time:42244ms step_avg:37.85ms
step:1117/2330 train_time:42280ms step_avg:37.85ms
step:1118/2330 train_time:42321ms step_avg:37.85ms
step:1119/2330 train_time:42356ms step_avg:37.85ms
step:1120/2330 train_time:42397ms step_avg:37.85ms
step:1121/2330 train_time:42432ms step_avg:37.85ms
step:1122/2330 train_time:42473ms step_avg:37.85ms
step:1123/2330 train_time:42508ms step_avg:37.85ms
step:1124/2330 train_time:42549ms step_avg:37.85ms
step:1125/2330 train_time:42583ms step_avg:37.85ms
step:1126/2330 train_time:42624ms step_avg:37.85ms
step:1127/2330 train_time:42660ms step_avg:37.85ms
step:1128/2330 train_time:42700ms step_avg:37.85ms
step:1129/2330 train_time:42736ms step_avg:37.85ms
step:1130/2330 train_time:42776ms step_avg:37.86ms
step:1131/2330 train_time:42812ms step_avg:37.85ms
step:1132/2330 train_time:42852ms step_avg:37.86ms
step:1133/2330 train_time:42888ms step_avg:37.85ms
step:1134/2330 train_time:42928ms step_avg:37.86ms
step:1135/2330 train_time:42963ms step_avg:37.85ms
step:1136/2330 train_time:43004ms step_avg:37.86ms
step:1137/2330 train_time:43040ms step_avg:37.85ms
step:1138/2330 train_time:43081ms step_avg:37.86ms
step:1139/2330 train_time:43117ms step_avg:37.85ms
step:1140/2330 train_time:43157ms step_avg:37.86ms
step:1141/2330 train_time:43194ms step_avg:37.86ms
step:1142/2330 train_time:43235ms step_avg:37.86ms
step:1143/2330 train_time:43271ms step_avg:37.86ms
step:1144/2330 train_time:43311ms step_avg:37.86ms
step:1145/2330 train_time:43346ms step_avg:37.86ms
step:1146/2330 train_time:43387ms step_avg:37.86ms
step:1147/2330 train_time:43423ms step_avg:37.86ms
step:1148/2330 train_time:43464ms step_avg:37.86ms
step:1149/2330 train_time:43499ms step_avg:37.86ms
step:1150/2330 train_time:43540ms step_avg:37.86ms
step:1151/2330 train_time:43576ms step_avg:37.86ms
step:1152/2330 train_time:43617ms step_avg:37.86ms
step:1153/2330 train_time:43653ms step_avg:37.86ms
step:1154/2330 train_time:43694ms step_avg:37.86ms
step:1155/2330 train_time:43729ms step_avg:37.86ms
step:1156/2330 train_time:43769ms step_avg:37.86ms
step:1157/2330 train_time:43805ms step_avg:37.86ms
step:1158/2330 train_time:43845ms step_avg:37.86ms
step:1159/2330 train_time:43881ms step_avg:37.86ms
step:1160/2330 train_time:43922ms step_avg:37.86ms
step:1161/2330 train_time:43958ms step_avg:37.86ms
step:1162/2330 train_time:43999ms step_avg:37.86ms
step:1163/2330 train_time:44034ms step_avg:37.86ms
step:1164/2330 train_time:44074ms step_avg:37.86ms
step:1165/2330 train_time:44109ms step_avg:37.86ms
step:1166/2330 train_time:44149ms step_avg:37.86ms
step:1167/2330 train_time:44185ms step_avg:37.86ms
step:1168/2330 train_time:44226ms step_avg:37.86ms
step:1169/2330 train_time:44262ms step_avg:37.86ms
step:1170/2330 train_time:44303ms step_avg:37.87ms
step:1171/2330 train_time:44339ms step_avg:37.86ms
step:1172/2330 train_time:44380ms step_avg:37.87ms
step:1173/2330 train_time:44415ms step_avg:37.86ms
step:1174/2330 train_time:44456ms step_avg:37.87ms
step:1175/2330 train_time:44491ms step_avg:37.86ms
step:1176/2330 train_time:44531ms step_avg:37.87ms
step:1177/2330 train_time:44566ms step_avg:37.86ms
step:1178/2330 train_time:44607ms step_avg:37.87ms
step:1179/2330 train_time:44642ms step_avg:37.86ms
step:1180/2330 train_time:44683ms step_avg:37.87ms
step:1181/2330 train_time:44719ms step_avg:37.87ms
step:1182/2330 train_time:44760ms step_avg:37.87ms
step:1183/2330 train_time:44795ms step_avg:37.87ms
step:1184/2330 train_time:44836ms step_avg:37.87ms
step:1185/2330 train_time:44872ms step_avg:37.87ms
step:1186/2330 train_time:44912ms step_avg:37.87ms
step:1187/2330 train_time:44948ms step_avg:37.87ms
step:1188/2330 train_time:44988ms step_avg:37.87ms
step:1189/2330 train_time:45023ms step_avg:37.87ms
step:1190/2330 train_time:45064ms step_avg:37.87ms
step:1191/2330 train_time:45099ms step_avg:37.87ms
step:1192/2330 train_time:45140ms step_avg:37.87ms
step:1193/2330 train_time:45176ms step_avg:37.87ms
step:1194/2330 train_time:45216ms step_avg:37.87ms
step:1195/2330 train_time:45251ms step_avg:37.87ms
step:1196/2330 train_time:45292ms step_avg:37.87ms
step:1197/2330 train_time:45328ms step_avg:37.87ms
step:1198/2330 train_time:45368ms step_avg:37.87ms
step:1199/2330 train_time:45404ms step_avg:37.87ms
step:1200/2330 train_time:45445ms step_avg:37.87ms
step:1201/2330 train_time:45481ms step_avg:37.87ms
step:1202/2330 train_time:45522ms step_avg:37.87ms
step:1203/2330 train_time:45556ms step_avg:37.87ms
step:1204/2330 train_time:45597ms step_avg:37.87ms
step:1205/2330 train_time:45633ms step_avg:37.87ms
step:1206/2330 train_time:45674ms step_avg:37.87ms
step:1207/2330 train_time:45709ms step_avg:37.87ms
step:1208/2330 train_time:45749ms step_avg:37.87ms
step:1209/2330 train_time:45786ms step_avg:37.87ms
step:1210/2330 train_time:45827ms step_avg:37.87ms
step:1211/2330 train_time:45861ms step_avg:37.87ms
step:1212/2330 train_time:45902ms step_avg:37.87ms
step:1213/2330 train_time:45937ms step_avg:37.87ms
step:1214/2330 train_time:45978ms step_avg:37.87ms
step:1215/2330 train_time:46014ms step_avg:37.87ms
step:1216/2330 train_time:46055ms step_avg:37.87ms
step:1217/2330 train_time:46091ms step_avg:37.87ms
step:1218/2330 train_time:46131ms step_avg:37.87ms
step:1219/2330 train_time:46167ms step_avg:37.87ms
step:1220/2330 train_time:46208ms step_avg:37.88ms
step:1221/2330 train_time:46243ms step_avg:37.87ms
step:1222/2330 train_time:46284ms step_avg:37.88ms
step:1223/2330 train_time:46319ms step_avg:37.87ms
step:1224/2330 train_time:46360ms step_avg:37.88ms
step:1225/2330 train_time:46396ms step_avg:37.87ms
step:1226/2330 train_time:46436ms step_avg:37.88ms
step:1227/2330 train_time:46472ms step_avg:37.87ms
step:1228/2330 train_time:46513ms step_avg:37.88ms
step:1229/2330 train_time:46547ms step_avg:37.87ms
step:1230/2330 train_time:46588ms step_avg:37.88ms
step:1231/2330 train_time:46623ms step_avg:37.87ms
step:1232/2330 train_time:46663ms step_avg:37.88ms
step:1233/2330 train_time:46699ms step_avg:37.87ms
step:1234/2330 train_time:46740ms step_avg:37.88ms
step:1235/2330 train_time:46776ms step_avg:37.88ms
step:1236/2330 train_time:46817ms step_avg:37.88ms
step:1237/2330 train_time:46853ms step_avg:37.88ms
step:1238/2330 train_time:46893ms step_avg:37.88ms
step:1239/2330 train_time:46928ms step_avg:37.88ms
step:1240/2330 train_time:46969ms step_avg:37.88ms
step:1241/2330 train_time:47004ms step_avg:37.88ms
step:1242/2330 train_time:47045ms step_avg:37.88ms
step:1243/2330 train_time:47081ms step_avg:37.88ms
step:1244/2330 train_time:47122ms step_avg:37.88ms
step:1245/2330 train_time:47157ms step_avg:37.88ms
step:1246/2330 train_time:47198ms step_avg:37.88ms
step:1247/2330 train_time:47234ms step_avg:37.88ms
step:1248/2330 train_time:47275ms step_avg:37.88ms
step:1249/2330 train_time:47310ms step_avg:37.88ms
step:1250/2330 train_time:47350ms step_avg:37.88ms
step:1250/2330 val_loss:5.4508 train_time:47462ms step_avg:37.97ms
step:1251/2330 train_time:47475ms step_avg:37.95ms
step:1252/2330 train_time:47486ms step_avg:37.93ms
step:1253/2330 train_time:47497ms step_avg:37.91ms
step:1254/2330 train_time:47508ms step_avg:37.89ms
step:1255/2330 train_time:47539ms step_avg:37.88ms
step:1256/2330 train_time:47579ms step_avg:37.88ms
step:1257/2330 train_time:47613ms step_avg:37.88ms
step:1258/2330 train_time:47654ms step_avg:37.88ms
step:1259/2330 train_time:47688ms step_avg:37.88ms
step:1260/2330 train_time:47728ms step_avg:37.88ms
step:1261/2330 train_time:47764ms step_avg:37.88ms
step:1262/2330 train_time:47805ms step_avg:37.88ms
step:1263/2330 train_time:47844ms step_avg:37.88ms
step:1264/2330 train_time:47885ms step_avg:37.88ms
step:1265/2330 train_time:47922ms step_avg:37.88ms
step:1266/2330 train_time:47962ms step_avg:37.89ms
step:1267/2330 train_time:47999ms step_avg:37.88ms
step:1268/2330 train_time:48039ms step_avg:37.89ms
step:1269/2330 train_time:48075ms step_avg:37.88ms
step:1270/2330 train_time:48114ms step_avg:37.89ms
step:1271/2330 train_time:48150ms step_avg:37.88ms
step:1272/2330 train_time:48190ms step_avg:37.89ms
step:1273/2330 train_time:48225ms step_avg:37.88ms
step:1274/2330 train_time:48265ms step_avg:37.88ms
step:1275/2330 train_time:48300ms step_avg:37.88ms
step:1276/2330 train_time:48341ms step_avg:37.88ms
step:1277/2330 train_time:48376ms step_avg:37.88ms
step:1278/2330 train_time:48417ms step_avg:37.89ms
step:1279/2330 train_time:48453ms step_avg:37.88ms
step:1280/2330 train_time:48494ms step_avg:37.89ms
step:1281/2330 train_time:48529ms step_avg:37.88ms
step:1282/2330 train_time:48571ms step_avg:37.89ms
step:1283/2330 train_time:48605ms step_avg:37.88ms
step:1284/2330 train_time:48646ms step_avg:37.89ms
step:1285/2330 train_time:48680ms step_avg:37.88ms
step:1286/2330 train_time:48721ms step_avg:37.89ms
step:1287/2330 train_time:48757ms step_avg:37.88ms
step:1288/2330 train_time:48797ms step_avg:37.89ms
step:1289/2330 train_time:48834ms step_avg:37.88ms
step:1290/2330 train_time:48874ms step_avg:37.89ms
step:1291/2330 train_time:48910ms step_avg:37.89ms
step:1292/2330 train_time:48951ms step_avg:37.89ms
step:1293/2330 train_time:48988ms step_avg:37.89ms
step:1294/2330 train_time:49028ms step_avg:37.89ms
step:1295/2330 train_time:49065ms step_avg:37.89ms
step:1296/2330 train_time:49105ms step_avg:37.89ms
step:1297/2330 train_time:49141ms step_avg:37.89ms
step:1298/2330 train_time:49181ms step_avg:37.89ms
step:1299/2330 train_time:49217ms step_avg:37.89ms
step:1300/2330 train_time:49257ms step_avg:37.89ms
step:1301/2330 train_time:49292ms step_avg:37.89ms
step:1302/2330 train_time:49333ms step_avg:37.89ms
step:1303/2330 train_time:49367ms step_avg:37.89ms
step:1304/2330 train_time:49409ms step_avg:37.89ms
step:1305/2330 train_time:49444ms step_avg:37.89ms
step:1306/2330 train_time:49485ms step_avg:37.89ms
step:1307/2330 train_time:49520ms step_avg:37.89ms
step:1308/2330 train_time:49562ms step_avg:37.89ms
step:1309/2330 train_time:49596ms step_avg:37.89ms
step:1310/2330 train_time:49637ms step_avg:37.89ms
step:1311/2330 train_time:49671ms step_avg:37.89ms
step:1312/2330 train_time:49712ms step_avg:37.89ms
step:1313/2330 train_time:49748ms step_avg:37.89ms
step:1314/2330 train_time:49789ms step_avg:37.89ms
step:1315/2330 train_time:49824ms step_avg:37.89ms
step:1316/2330 train_time:49865ms step_avg:37.89ms
step:1317/2330 train_time:49902ms step_avg:37.89ms
step:1318/2330 train_time:49942ms step_avg:37.89ms
step:1319/2330 train_time:49979ms step_avg:37.89ms
step:1320/2330 train_time:50020ms step_avg:37.89ms
step:1321/2330 train_time:50056ms step_avg:37.89ms
step:1322/2330 train_time:50096ms step_avg:37.89ms
step:1323/2330 train_time:50132ms step_avg:37.89ms
step:1324/2330 train_time:50172ms step_avg:37.89ms
step:1325/2330 train_time:50208ms step_avg:37.89ms
step:1326/2330 train_time:50249ms step_avg:37.90ms
step:1327/2330 train_time:50284ms step_avg:37.89ms
step:1328/2330 train_time:50325ms step_avg:37.90ms
step:1329/2330 train_time:50360ms step_avg:37.89ms
step:1330/2330 train_time:50401ms step_avg:37.90ms
step:1331/2330 train_time:50437ms step_avg:37.89ms
step:1332/2330 train_time:50477ms step_avg:37.90ms
step:1333/2330 train_time:50512ms step_avg:37.89ms
step:1334/2330 train_time:50553ms step_avg:37.90ms
step:1335/2330 train_time:50587ms step_avg:37.89ms
step:1336/2330 train_time:50628ms step_avg:37.90ms
step:1337/2330 train_time:50664ms step_avg:37.89ms
step:1338/2330 train_time:50705ms step_avg:37.90ms
step:1339/2330 train_time:50740ms step_avg:37.89ms
step:1340/2330 train_time:50781ms step_avg:37.90ms
step:1341/2330 train_time:50817ms step_avg:37.89ms
step:1342/2330 train_time:50858ms step_avg:37.90ms
step:1343/2330 train_time:50893ms step_avg:37.89ms
step:1344/2330 train_time:50933ms step_avg:37.90ms
step:1345/2330 train_time:50969ms step_avg:37.90ms
step:1346/2330 train_time:51010ms step_avg:37.90ms
step:1347/2330 train_time:51046ms step_avg:37.90ms
step:1348/2330 train_time:51087ms step_avg:37.90ms
step:1349/2330 train_time:51122ms step_avg:37.90ms
step:1350/2330 train_time:51163ms step_avg:37.90ms
step:1351/2330 train_time:51198ms step_avg:37.90ms
step:1352/2330 train_time:51239ms step_avg:37.90ms
step:1353/2330 train_time:51274ms step_avg:37.90ms
step:1354/2330 train_time:51314ms step_avg:37.90ms
step:1355/2330 train_time:51349ms step_avg:37.90ms
step:1356/2330 train_time:51389ms step_avg:37.90ms
step:1357/2330 train_time:51426ms step_avg:37.90ms
step:1358/2330 train_time:51467ms step_avg:37.90ms
step:1359/2330 train_time:51503ms step_avg:37.90ms
step:1360/2330 train_time:51543ms step_avg:37.90ms
step:1361/2330 train_time:51579ms step_avg:37.90ms
step:1362/2330 train_time:51620ms step_avg:37.90ms
step:1363/2330 train_time:51656ms step_avg:37.90ms
step:1364/2330 train_time:51696ms step_avg:37.90ms
step:1365/2330 train_time:51731ms step_avg:37.90ms
step:1366/2330 train_time:51772ms step_avg:37.90ms
step:1367/2330 train_time:51808ms step_avg:37.90ms
step:1368/2330 train_time:51849ms step_avg:37.90ms
step:1369/2330 train_time:51884ms step_avg:37.90ms
step:1370/2330 train_time:51925ms step_avg:37.90ms
step:1371/2330 train_time:51961ms step_avg:37.90ms
step:1372/2330 train_time:52001ms step_avg:37.90ms
step:1373/2330 train_time:52037ms step_avg:37.90ms
step:1374/2330 train_time:52078ms step_avg:37.90ms
step:1375/2330 train_time:52113ms step_avg:37.90ms
step:1376/2330 train_time:52153ms step_avg:37.90ms
step:1377/2330 train_time:52189ms step_avg:37.90ms
step:1378/2330 train_time:52229ms step_avg:37.90ms
step:1379/2330 train_time:52265ms step_avg:37.90ms
step:1380/2330 train_time:52305ms step_avg:37.90ms
step:1381/2330 train_time:52342ms step_avg:37.90ms
step:1382/2330 train_time:52383ms step_avg:37.90ms
step:1383/2330 train_time:52418ms step_avg:37.90ms
step:1384/2330 train_time:52459ms step_avg:37.90ms
step:1385/2330 train_time:52493ms step_avg:37.90ms
step:1386/2330 train_time:52534ms step_avg:37.90ms
step:1387/2330 train_time:52569ms step_avg:37.90ms
step:1388/2330 train_time:52610ms step_avg:37.90ms
step:1389/2330 train_time:52646ms step_avg:37.90ms
step:1390/2330 train_time:52687ms step_avg:37.90ms
step:1391/2330 train_time:52723ms step_avg:37.90ms
step:1392/2330 train_time:52764ms step_avg:37.90ms
step:1393/2330 train_time:52799ms step_avg:37.90ms
step:1394/2330 train_time:52840ms step_avg:37.91ms
step:1395/2330 train_time:52876ms step_avg:37.90ms
step:1396/2330 train_time:52916ms step_avg:37.91ms
step:1397/2330 train_time:52952ms step_avg:37.90ms
step:1398/2330 train_time:52993ms step_avg:37.91ms
step:1399/2330 train_time:53027ms step_avg:37.90ms
step:1400/2330 train_time:53068ms step_avg:37.91ms
step:1401/2330 train_time:53104ms step_avg:37.90ms
step:1402/2330 train_time:53145ms step_avg:37.91ms
step:1403/2330 train_time:53181ms step_avg:37.91ms
step:1404/2330 train_time:53222ms step_avg:37.91ms
step:1405/2330 train_time:53258ms step_avg:37.91ms
step:1406/2330 train_time:53298ms step_avg:37.91ms
step:1407/2330 train_time:53334ms step_avg:37.91ms
step:1408/2330 train_time:53374ms step_avg:37.91ms
step:1409/2330 train_time:53409ms step_avg:37.91ms
step:1410/2330 train_time:53450ms step_avg:37.91ms
step:1411/2330 train_time:53485ms step_avg:37.91ms
step:1412/2330 train_time:53525ms step_avg:37.91ms
step:1413/2330 train_time:53562ms step_avg:37.91ms
step:1414/2330 train_time:53603ms step_avg:37.91ms
step:1415/2330 train_time:53639ms step_avg:37.91ms
step:1416/2330 train_time:53680ms step_avg:37.91ms
step:1417/2330 train_time:53715ms step_avg:37.91ms
step:1418/2330 train_time:53756ms step_avg:37.91ms
step:1419/2330 train_time:53791ms step_avg:37.91ms
step:1420/2330 train_time:53832ms step_avg:37.91ms
step:1421/2330 train_time:53867ms step_avg:37.91ms
step:1422/2330 train_time:53909ms step_avg:37.91ms
step:1423/2330 train_time:53944ms step_avg:37.91ms
step:1424/2330 train_time:53985ms step_avg:37.91ms
step:1425/2330 train_time:54020ms step_avg:37.91ms
step:1426/2330 train_time:54060ms step_avg:37.91ms
step:1427/2330 train_time:54096ms step_avg:37.91ms
step:1428/2330 train_time:54136ms step_avg:37.91ms
step:1429/2330 train_time:54172ms step_avg:37.91ms
step:1430/2330 train_time:54213ms step_avg:37.91ms
step:1431/2330 train_time:54248ms step_avg:37.91ms
step:1432/2330 train_time:54289ms step_avg:37.91ms
step:1433/2330 train_time:54325ms step_avg:37.91ms
step:1434/2330 train_time:54365ms step_avg:37.91ms
step:1435/2330 train_time:54401ms step_avg:37.91ms
step:1436/2330 train_time:54442ms step_avg:37.91ms
step:1437/2330 train_time:54478ms step_avg:37.91ms
step:1438/2330 train_time:54519ms step_avg:37.91ms
step:1439/2330 train_time:54554ms step_avg:37.91ms
step:1440/2330 train_time:54595ms step_avg:37.91ms
step:1441/2330 train_time:54629ms step_avg:37.91ms
step:1442/2330 train_time:54671ms step_avg:37.91ms
step:1443/2330 train_time:54706ms step_avg:37.91ms
step:1444/2330 train_time:54747ms step_avg:37.91ms
step:1445/2330 train_time:54783ms step_avg:37.91ms
step:1446/2330 train_time:54823ms step_avg:37.91ms
step:1447/2330 train_time:54860ms step_avg:37.91ms
step:1448/2330 train_time:54901ms step_avg:37.91ms
step:1449/2330 train_time:54936ms step_avg:37.91ms
step:1450/2330 train_time:54977ms step_avg:37.91ms
step:1451/2330 train_time:55011ms step_avg:37.91ms
step:1452/2330 train_time:55053ms step_avg:37.92ms
step:1453/2330 train_time:55087ms step_avg:37.91ms
step:1454/2330 train_time:55128ms step_avg:37.91ms
step:1455/2330 train_time:55164ms step_avg:37.91ms
step:1456/2330 train_time:55204ms step_avg:37.92ms
step:1457/2330 train_time:55240ms step_avg:37.91ms
step:1458/2330 train_time:55280ms step_avg:37.92ms
step:1459/2330 train_time:55316ms step_avg:37.91ms
step:1460/2330 train_time:55356ms step_avg:37.92ms
step:1461/2330 train_time:55392ms step_avg:37.91ms
step:1462/2330 train_time:55432ms step_avg:37.92ms
step:1463/2330 train_time:55469ms step_avg:37.91ms
step:1464/2330 train_time:55510ms step_avg:37.92ms
step:1465/2330 train_time:55545ms step_avg:37.91ms
step:1466/2330 train_time:55585ms step_avg:37.92ms
step:1467/2330 train_time:55621ms step_avg:37.91ms
step:1468/2330 train_time:55661ms step_avg:37.92ms
step:1469/2330 train_time:55696ms step_avg:37.91ms
step:1470/2330 train_time:55737ms step_avg:37.92ms
step:1471/2330 train_time:55772ms step_avg:37.91ms
step:1472/2330 train_time:55813ms step_avg:37.92ms
step:1473/2330 train_time:55849ms step_avg:37.91ms
step:1474/2330 train_time:55890ms step_avg:37.92ms
step:1475/2330 train_time:55925ms step_avg:37.92ms
step:1476/2330 train_time:55965ms step_avg:37.92ms
step:1477/2330 train_time:56001ms step_avg:37.92ms
step:1478/2330 train_time:56042ms step_avg:37.92ms
step:1479/2330 train_time:56078ms step_avg:37.92ms
step:1480/2330 train_time:56119ms step_avg:37.92ms
step:1481/2330 train_time:56154ms step_avg:37.92ms
step:1482/2330 train_time:56194ms step_avg:37.92ms
step:1483/2330 train_time:56230ms step_avg:37.92ms
step:1484/2330 train_time:56271ms step_avg:37.92ms
step:1485/2330 train_time:56306ms step_avg:37.92ms
step:1486/2330 train_time:56347ms step_avg:37.92ms
step:1487/2330 train_time:56383ms step_avg:37.92ms
step:1488/2330 train_time:56423ms step_avg:37.92ms
step:1489/2330 train_time:56458ms step_avg:37.92ms
step:1490/2330 train_time:56499ms step_avg:37.92ms
step:1491/2330 train_time:56534ms step_avg:37.92ms
step:1492/2330 train_time:56574ms step_avg:37.92ms
step:1493/2330 train_time:56610ms step_avg:37.92ms
step:1494/2330 train_time:56651ms step_avg:37.92ms
step:1495/2330 train_time:56686ms step_avg:37.92ms
step:1496/2330 train_time:56727ms step_avg:37.92ms
step:1497/2330 train_time:56763ms step_avg:37.92ms
step:1498/2330 train_time:56804ms step_avg:37.92ms
step:1499/2330 train_time:56840ms step_avg:37.92ms
step:1500/2330 train_time:56881ms step_avg:37.92ms
step:1500/2330 val_loss:5.4005 train_time:56992ms step_avg:37.99ms
step:1501/2330 train_time:57004ms step_avg:37.98ms
step:1502/2330 train_time:57017ms step_avg:37.96ms
step:1503/2330 train_time:57027ms step_avg:37.94ms
step:1504/2330 train_time:57038ms step_avg:37.92ms
step:1505/2330 train_time:57071ms step_avg:37.92ms
step:1506/2330 train_time:57112ms step_avg:37.92ms
step:1507/2330 train_time:57146ms step_avg:37.92ms
step:1508/2330 train_time:57187ms step_avg:37.92ms
step:1509/2330 train_time:57221ms step_avg:37.92ms
step:1510/2330 train_time:57262ms step_avg:37.92ms
step:1511/2330 train_time:57299ms step_avg:37.92ms
step:1512/2330 train_time:57339ms step_avg:37.92ms
step:1513/2330 train_time:57379ms step_avg:37.92ms
step:1514/2330 train_time:57420ms step_avg:37.93ms
step:1515/2330 train_time:57455ms step_avg:37.92ms
step:1516/2330 train_time:57496ms step_avg:37.93ms
step:1517/2330 train_time:57532ms step_avg:37.92ms
step:1518/2330 train_time:57572ms step_avg:37.93ms
step:1519/2330 train_time:57608ms step_avg:37.93ms
step:1520/2330 train_time:57649ms step_avg:37.93ms
step:1521/2330 train_time:57683ms step_avg:37.92ms
step:1522/2330 train_time:57724ms step_avg:37.93ms
step:1523/2330 train_time:57758ms step_avg:37.92ms
step:1524/2330 train_time:57798ms step_avg:37.93ms
step:1525/2330 train_time:57833ms step_avg:37.92ms
step:1526/2330 train_time:57874ms step_avg:37.93ms
step:1527/2330 train_time:57909ms step_avg:37.92ms
step:1528/2330 train_time:57950ms step_avg:37.93ms
step:1529/2330 train_time:57986ms step_avg:37.92ms
step:1530/2330 train_time:58027ms step_avg:37.93ms
step:1531/2330 train_time:58062ms step_avg:37.92ms
step:1532/2330 train_time:58102ms step_avg:37.93ms
step:1533/2330 train_time:58137ms step_avg:37.92ms
step:1534/2330 train_time:58177ms step_avg:37.93ms
step:1535/2330 train_time:58213ms step_avg:37.92ms
step:1536/2330 train_time:58254ms step_avg:37.93ms
step:1537/2330 train_time:58291ms step_avg:37.92ms
step:1538/2330 train_time:58331ms step_avg:37.93ms
step:1539/2330 train_time:58368ms step_avg:37.93ms
step:1540/2330 train_time:58409ms step_avg:37.93ms
step:1541/2330 train_time:58444ms step_avg:37.93ms
step:1542/2330 train_time:58485ms step_avg:37.93ms
step:1543/2330 train_time:58521ms step_avg:37.93ms
step:1544/2330 train_time:58561ms step_avg:37.93ms
step:1545/2330 train_time:58597ms step_avg:37.93ms
step:1546/2330 train_time:58637ms step_avg:37.93ms
step:1547/2330 train_time:58672ms step_avg:37.93ms
step:1548/2330 train_time:58713ms step_avg:37.93ms
step:1549/2330 train_time:58748ms step_avg:37.93ms
step:1550/2330 train_time:58789ms step_avg:37.93ms
step:1551/2330 train_time:58823ms step_avg:37.93ms
step:1552/2330 train_time:58865ms step_avg:37.93ms
step:1553/2330 train_time:58899ms step_avg:37.93ms
step:1554/2330 train_time:58939ms step_avg:37.93ms
step:1555/2330 train_time:58974ms step_avg:37.93ms
step:1556/2330 train_time:59016ms step_avg:37.93ms
step:1557/2330 train_time:59050ms step_avg:37.93ms
step:1558/2330 train_time:59091ms step_avg:37.93ms
step:1559/2330 train_time:59126ms step_avg:37.93ms
step:1560/2330 train_time:59167ms step_avg:37.93ms
step:1561/2330 train_time:59202ms step_avg:37.93ms
step:1562/2330 train_time:59244ms step_avg:37.93ms
step:1563/2330 train_time:59279ms step_avg:37.93ms
step:1564/2330 train_time:59320ms step_avg:37.93ms
step:1565/2330 train_time:59355ms step_avg:37.93ms
step:1566/2330 train_time:59396ms step_avg:37.93ms
step:1567/2330 train_time:59432ms step_avg:37.93ms
step:1568/2330 train_time:59472ms step_avg:37.93ms
step:1569/2330 train_time:59508ms step_avg:37.93ms
step:1570/2330 train_time:59549ms step_avg:37.93ms
step:1571/2330 train_time:59584ms step_avg:37.93ms
step:1572/2330 train_time:59625ms step_avg:37.93ms
step:1573/2330 train_time:59661ms step_avg:37.93ms
step:1574/2330 train_time:59702ms step_avg:37.93ms
step:1575/2330 train_time:59736ms step_avg:37.93ms
step:1576/2330 train_time:59777ms step_avg:37.93ms
step:1577/2330 train_time:59812ms step_avg:37.93ms
step:1578/2330 train_time:59853ms step_avg:37.93ms
step:1579/2330 train_time:59888ms step_avg:37.93ms
step:1580/2330 train_time:59928ms step_avg:37.93ms
step:1581/2330 train_time:59963ms step_avg:37.93ms
step:1582/2330 train_time:60004ms step_avg:37.93ms
step:1583/2330 train_time:60038ms step_avg:37.93ms
step:1584/2330 train_time:60079ms step_avg:37.93ms
step:1585/2330 train_time:60114ms step_avg:37.93ms
step:1586/2330 train_time:60155ms step_avg:37.93ms
step:1587/2330 train_time:60189ms step_avg:37.93ms
step:1588/2330 train_time:60230ms step_avg:37.93ms
step:1589/2330 train_time:60266ms step_avg:37.93ms
step:1590/2330 train_time:60308ms step_avg:37.93ms
step:1591/2330 train_time:60343ms step_avg:37.93ms
step:1592/2330 train_time:60384ms step_avg:37.93ms
step:1593/2330 train_time:60419ms step_avg:37.93ms
step:1594/2330 train_time:60460ms step_avg:37.93ms
step:1595/2330 train_time:60495ms step_avg:37.93ms
step:1596/2330 train_time:60536ms step_avg:37.93ms
step:1597/2330 train_time:60571ms step_avg:37.93ms
step:1598/2330 train_time:60612ms step_avg:37.93ms
step:1599/2330 train_time:60647ms step_avg:37.93ms
step:1600/2330 train_time:60688ms step_avg:37.93ms
step:1601/2330 train_time:60723ms step_avg:37.93ms
step:1602/2330 train_time:60764ms step_avg:37.93ms
step:1603/2330 train_time:60799ms step_avg:37.93ms
step:1604/2330 train_time:60839ms step_avg:37.93ms
step:1605/2330 train_time:60874ms step_avg:37.93ms
step:1606/2330 train_time:60915ms step_avg:37.93ms
step:1607/2330 train_time:60950ms step_avg:37.93ms
step:1608/2330 train_time:60991ms step_avg:37.93ms
step:1609/2330 train_time:61025ms step_avg:37.93ms
step:1610/2330 train_time:61066ms step_avg:37.93ms
step:1611/2330 train_time:61101ms step_avg:37.93ms
step:1612/2330 train_time:61142ms step_avg:37.93ms
step:1613/2330 train_time:61177ms step_avg:37.93ms
step:1614/2330 train_time:61218ms step_avg:37.93ms
step:1615/2330 train_time:61252ms step_avg:37.93ms
step:1616/2330 train_time:61293ms step_avg:37.93ms
step:1617/2330 train_time:61329ms step_avg:37.93ms
step:1618/2330 train_time:61369ms step_avg:37.93ms
step:1619/2330 train_time:61406ms step_avg:37.93ms
step:1620/2330 train_time:61447ms step_avg:37.93ms
step:1621/2330 train_time:61482ms step_avg:37.93ms
step:1622/2330 train_time:61523ms step_avg:37.93ms
step:1623/2330 train_time:61559ms step_avg:37.93ms
step:1624/2330 train_time:61600ms step_avg:37.93ms
step:1625/2330 train_time:61635ms step_avg:37.93ms
step:1626/2330 train_time:61676ms step_avg:37.93ms
step:1627/2330 train_time:61711ms step_avg:37.93ms
step:1628/2330 train_time:61752ms step_avg:37.93ms
step:1629/2330 train_time:61787ms step_avg:37.93ms
step:1630/2330 train_time:61827ms step_avg:37.93ms
step:1631/2330 train_time:61863ms step_avg:37.93ms
step:1632/2330 train_time:61903ms step_avg:37.93ms
step:1633/2330 train_time:61938ms step_avg:37.93ms
step:1634/2330 train_time:61978ms step_avg:37.93ms
step:1635/2330 train_time:62013ms step_avg:37.93ms
step:1636/2330 train_time:62054ms step_avg:37.93ms
step:1637/2330 train_time:62089ms step_avg:37.93ms
step:1638/2330 train_time:62130ms step_avg:37.93ms
step:1639/2330 train_time:62166ms step_avg:37.93ms
step:1640/2330 train_time:62207ms step_avg:37.93ms
step:1641/2330 train_time:62242ms step_avg:37.93ms
step:1642/2330 train_time:62283ms step_avg:37.93ms
step:1643/2330 train_time:62318ms step_avg:37.93ms
step:1644/2330 train_time:62358ms step_avg:37.93ms
step:1645/2330 train_time:62394ms step_avg:37.93ms
step:1646/2330 train_time:62435ms step_avg:37.93ms
step:1647/2330 train_time:62470ms step_avg:37.93ms
step:1648/2330 train_time:62511ms step_avg:37.93ms
step:1649/2330 train_time:62546ms step_avg:37.93ms
step:1650/2330 train_time:62587ms step_avg:37.93ms
step:1651/2330 train_time:62622ms step_avg:37.93ms
step:1652/2330 train_time:62664ms step_avg:37.93ms
step:1653/2330 train_time:62698ms step_avg:37.93ms
step:1654/2330 train_time:62739ms step_avg:37.93ms
step:1655/2330 train_time:62774ms step_avg:37.93ms
step:1656/2330 train_time:62815ms step_avg:37.93ms
step:1657/2330 train_time:62850ms step_avg:37.93ms
step:1658/2330 train_time:62891ms step_avg:37.93ms
step:1659/2330 train_time:62926ms step_avg:37.93ms
step:1660/2330 train_time:62966ms step_avg:37.93ms
step:1661/2330 train_time:63001ms step_avg:37.93ms
step:1662/2330 train_time:63042ms step_avg:37.93ms
step:1663/2330 train_time:63076ms step_avg:37.93ms
step:1664/2330 train_time:63117ms step_avg:37.93ms
step:1665/2330 train_time:63151ms step_avg:37.93ms
step:1666/2330 train_time:63192ms step_avg:37.93ms
step:1667/2330 train_time:63228ms step_avg:37.93ms
step:1668/2330 train_time:63269ms step_avg:37.93ms
step:1669/2330 train_time:63304ms step_avg:37.93ms
step:1670/2330 train_time:63345ms step_avg:37.93ms
step:1671/2330 train_time:63381ms step_avg:37.93ms
step:1672/2330 train_time:63421ms step_avg:37.93ms
step:1673/2330 train_time:63456ms step_avg:37.93ms
step:1674/2330 train_time:63497ms step_avg:37.93ms
step:1675/2330 train_time:63532ms step_avg:37.93ms
step:1676/2330 train_time:63573ms step_avg:37.93ms
step:1677/2330 train_time:63609ms step_avg:37.93ms
step:1678/2330 train_time:63650ms step_avg:37.93ms
step:1679/2330 train_time:63685ms step_avg:37.93ms
step:1680/2330 train_time:63726ms step_avg:37.93ms
step:1681/2330 train_time:63761ms step_avg:37.93ms
step:1682/2330 train_time:63802ms step_avg:37.93ms
step:1683/2330 train_time:63837ms step_avg:37.93ms
step:1684/2330 train_time:63877ms step_avg:37.93ms
step:1685/2330 train_time:63912ms step_avg:37.93ms
step:1686/2330 train_time:63953ms step_avg:37.93ms
step:1687/2330 train_time:63988ms step_avg:37.93ms
step:1688/2330 train_time:64029ms step_avg:37.93ms
step:1689/2330 train_time:64065ms step_avg:37.93ms
step:1690/2330 train_time:64106ms step_avg:37.93ms
step:1691/2330 train_time:64141ms step_avg:37.93ms
step:1692/2330 train_time:64182ms step_avg:37.93ms
step:1693/2330 train_time:64217ms step_avg:37.93ms
step:1694/2330 train_time:64257ms step_avg:37.93ms
step:1695/2330 train_time:64293ms step_avg:37.93ms
step:1696/2330 train_time:64334ms step_avg:37.93ms
step:1697/2330 train_time:64370ms step_avg:37.93ms
step:1698/2330 train_time:64410ms step_avg:37.93ms
step:1699/2330 train_time:64446ms step_avg:37.93ms
step:1700/2330 train_time:64487ms step_avg:37.93ms
step:1701/2330 train_time:64522ms step_avg:37.93ms
step:1702/2330 train_time:64563ms step_avg:37.93ms
step:1703/2330 train_time:64598ms step_avg:37.93ms
step:1704/2330 train_time:64638ms step_avg:37.93ms
step:1705/2330 train_time:64674ms step_avg:37.93ms
step:1706/2330 train_time:64715ms step_avg:37.93ms
step:1707/2330 train_time:64750ms step_avg:37.93ms
step:1708/2330 train_time:64790ms step_avg:37.93ms
step:1709/2330 train_time:64827ms step_avg:37.93ms
step:1710/2330 train_time:64868ms step_avg:37.93ms
step:1711/2330 train_time:64903ms step_avg:37.93ms
step:1712/2330 train_time:64944ms step_avg:37.93ms
step:1713/2330 train_time:64978ms step_avg:37.93ms
step:1714/2330 train_time:65019ms step_avg:37.93ms
step:1715/2330 train_time:65053ms step_avg:37.93ms
step:1716/2330 train_time:65094ms step_avg:37.93ms
step:1717/2330 train_time:65129ms step_avg:37.93ms
step:1718/2330 train_time:65170ms step_avg:37.93ms
step:1719/2330 train_time:65206ms step_avg:37.93ms
step:1720/2330 train_time:65246ms step_avg:37.93ms
step:1721/2330 train_time:65282ms step_avg:37.93ms
step:1722/2330 train_time:65323ms step_avg:37.93ms
step:1723/2330 train_time:65357ms step_avg:37.93ms
step:1724/2330 train_time:65398ms step_avg:37.93ms
step:1725/2330 train_time:65432ms step_avg:37.93ms
step:1726/2330 train_time:65474ms step_avg:37.93ms
step:1727/2330 train_time:65509ms step_avg:37.93ms
step:1728/2330 train_time:65550ms step_avg:37.93ms
step:1729/2330 train_time:65585ms step_avg:37.93ms
step:1730/2330 train_time:65627ms step_avg:37.93ms
step:1731/2330 train_time:65661ms step_avg:37.93ms
step:1732/2330 train_time:65701ms step_avg:37.93ms
step:1733/2330 train_time:65736ms step_avg:37.93ms
step:1734/2330 train_time:65777ms step_avg:37.93ms
step:1735/2330 train_time:65813ms step_avg:37.93ms
step:1736/2330 train_time:65854ms step_avg:37.93ms
step:1737/2330 train_time:65890ms step_avg:37.93ms
step:1738/2330 train_time:65930ms step_avg:37.93ms
step:1739/2330 train_time:65965ms step_avg:37.93ms
step:1740/2330 train_time:66006ms step_avg:37.93ms
step:1741/2330 train_time:66042ms step_avg:37.93ms
step:1742/2330 train_time:66083ms step_avg:37.93ms
step:1743/2330 train_time:66117ms step_avg:37.93ms
step:1744/2330 train_time:66157ms step_avg:37.93ms
step:1745/2330 train_time:66193ms step_avg:37.93ms
step:1746/2330 train_time:66234ms step_avg:37.93ms
step:1747/2330 train_time:66269ms step_avg:37.93ms
step:1748/2330 train_time:66310ms step_avg:37.94ms
step:1749/2330 train_time:66345ms step_avg:37.93ms
step:1750/2330 train_time:66386ms step_avg:37.93ms
step:1750/2330 val_loss:5.3543 train_time:66497ms step_avg:38.00ms
step:1751/2330 train_time:66509ms step_avg:37.98ms
step:1752/2330 train_time:66520ms step_avg:37.97ms
step:1753/2330 train_time:66530ms step_avg:37.95ms
step:1754/2330 train_time:66540ms step_avg:37.94ms
step:1755/2330 train_time:66574ms step_avg:37.93ms
step:1756/2330 train_time:66614ms step_avg:37.94ms
step:1757/2330 train_time:66648ms step_avg:37.93ms
step:1758/2330 train_time:66688ms step_avg:37.93ms
step:1759/2330 train_time:66723ms step_avg:37.93ms
step:1760/2330 train_time:66764ms step_avg:37.93ms
step:1761/2330 train_time:66799ms step_avg:37.93ms
step:1762/2330 train_time:66841ms step_avg:37.93ms
step:1763/2330 train_time:66877ms step_avg:37.93ms
step:1764/2330 train_time:66919ms step_avg:37.94ms
step:1765/2330 train_time:66954ms step_avg:37.93ms
step:1766/2330 train_time:66995ms step_avg:37.94ms
step:1767/2330 train_time:67029ms step_avg:37.93ms
step:1768/2330 train_time:67070ms step_avg:37.94ms
step:1769/2330 train_time:67104ms step_avg:37.93ms
step:1770/2330 train_time:67145ms step_avg:37.93ms
step:1771/2330 train_time:67180ms step_avg:37.93ms
step:1772/2330 train_time:67221ms step_avg:37.93ms
step:1773/2330 train_time:67256ms step_avg:37.93ms
step:1774/2330 train_time:67296ms step_avg:37.93ms
step:1775/2330 train_time:67331ms step_avg:37.93ms
step:1776/2330 train_time:67371ms step_avg:37.93ms
step:1777/2330 train_time:67407ms step_avg:37.93ms
step:1778/2330 train_time:67447ms step_avg:37.93ms
step:1779/2330 train_time:67484ms step_avg:37.93ms
step:1780/2330 train_time:67524ms step_avg:37.93ms
step:1781/2330 train_time:67560ms step_avg:37.93ms
step:1782/2330 train_time:67601ms step_avg:37.94ms
step:1783/2330 train_time:67636ms step_avg:37.93ms
step:1784/2330 train_time:67676ms step_avg:37.94ms
step:1785/2330 train_time:67712ms step_avg:37.93ms
step:1786/2330 train_time:67752ms step_avg:37.94ms
step:1787/2330 train_time:67787ms step_avg:37.93ms
step:1788/2330 train_time:67828ms step_avg:37.94ms
step:1789/2330 train_time:67864ms step_avg:37.93ms
step:1790/2330 train_time:67905ms step_avg:37.94ms
step:1791/2330 train_time:67940ms step_avg:37.93ms
step:1792/2330 train_time:67981ms step_avg:37.94ms
step:1793/2330 train_time:68016ms step_avg:37.93ms
step:1794/2330 train_time:68058ms step_avg:37.94ms
step:1795/2330 train_time:68092ms step_avg:37.93ms
step:1796/2330 train_time:68133ms step_avg:37.94ms
step:1797/2330 train_time:68168ms step_avg:37.93ms
step:1798/2330 train_time:68208ms step_avg:37.94ms
step:1799/2330 train_time:68243ms step_avg:37.93ms
step:1800/2330 train_time:68284ms step_avg:37.94ms
step:1801/2330 train_time:68318ms step_avg:37.93ms
step:1802/2330 train_time:68359ms step_avg:37.94ms
step:1803/2330 train_time:68394ms step_avg:37.93ms
step:1804/2330 train_time:68435ms step_avg:37.94ms
step:1805/2330 train_time:68471ms step_avg:37.93ms
step:1806/2330 train_time:68511ms step_avg:37.94ms
step:1807/2330 train_time:68546ms step_avg:37.93ms
step:1808/2330 train_time:68587ms step_avg:37.94ms
step:1809/2330 train_time:68623ms step_avg:37.93ms
step:1810/2330 train_time:68664ms step_avg:37.94ms
step:1811/2330 train_time:68700ms step_avg:37.93ms
step:1812/2330 train_time:68741ms step_avg:37.94ms
step:1813/2330 train_time:68776ms step_avg:37.93ms
step:1814/2330 train_time:68817ms step_avg:37.94ms
step:1815/2330 train_time:68852ms step_avg:37.94ms
step:1816/2330 train_time:68893ms step_avg:37.94ms
step:1817/2330 train_time:68928ms step_avg:37.94ms
step:1818/2330 train_time:68969ms step_avg:37.94ms
step:1819/2330 train_time:69004ms step_avg:37.94ms
step:1820/2330 train_time:69045ms step_avg:37.94ms
step:1821/2330 train_time:69080ms step_avg:37.94ms
step:1822/2330 train_time:69121ms step_avg:37.94ms
step:1823/2330 train_time:69156ms step_avg:37.94ms
step:1824/2330 train_time:69197ms step_avg:37.94ms
step:1825/2330 train_time:69232ms step_avg:37.94ms
step:1826/2330 train_time:69273ms step_avg:37.94ms
step:1827/2330 train_time:69307ms step_avg:37.93ms
step:1828/2330 train_time:69347ms step_avg:37.94ms
step:1829/2330 train_time:69382ms step_avg:37.93ms
step:1830/2330 train_time:69423ms step_avg:37.94ms
step:1831/2330 train_time:69459ms step_avg:37.94ms
step:1832/2330 train_time:69500ms step_avg:37.94ms
step:1833/2330 train_time:69536ms step_avg:37.94ms
step:1834/2330 train_time:69577ms step_avg:37.94ms
step:1835/2330 train_time:69611ms step_avg:37.94ms
step:1836/2330 train_time:69652ms step_avg:37.94ms
step:1837/2330 train_time:69687ms step_avg:37.94ms
step:1838/2330 train_time:69728ms step_avg:37.94ms
step:1839/2330 train_time:69764ms step_avg:37.94ms
step:1840/2330 train_time:69804ms step_avg:37.94ms
step:1841/2330 train_time:69839ms step_avg:37.94ms
step:1842/2330 train_time:69880ms step_avg:37.94ms
step:1843/2330 train_time:69916ms step_avg:37.94ms
step:1844/2330 train_time:69957ms step_avg:37.94ms
step:1845/2330 train_time:69991ms step_avg:37.94ms
step:1846/2330 train_time:70032ms step_avg:37.94ms
step:1847/2330 train_time:70067ms step_avg:37.94ms
step:1848/2330 train_time:70108ms step_avg:37.94ms
step:1849/2330 train_time:70144ms step_avg:37.94ms
step:1850/2330 train_time:70185ms step_avg:37.94ms
step:1851/2330 train_time:70220ms step_avg:37.94ms
step:1852/2330 train_time:70261ms step_avg:37.94ms
step:1853/2330 train_time:70296ms step_avg:37.94ms
step:1854/2330 train_time:70336ms step_avg:37.94ms
step:1855/2330 train_time:70371ms step_avg:37.94ms
step:1856/2330 train_time:70412ms step_avg:37.94ms
step:1857/2330 train_time:70447ms step_avg:37.94ms
step:1858/2330 train_time:70488ms step_avg:37.94ms
step:1859/2330 train_time:70523ms step_avg:37.94ms
step:1860/2330 train_time:70564ms step_avg:37.94ms
step:1861/2330 train_time:70599ms step_avg:37.94ms
step:1862/2330 train_time:70640ms step_avg:37.94ms
step:1863/2330 train_time:70675ms step_avg:37.94ms
step:1864/2330 train_time:70716ms step_avg:37.94ms
step:1865/2330 train_time:70752ms step_avg:37.94ms
step:1866/2330 train_time:70792ms step_avg:37.94ms
step:1867/2330 train_time:70827ms step_avg:37.94ms
step:1868/2330 train_time:70868ms step_avg:37.94ms
step:1869/2330 train_time:70903ms step_avg:37.94ms
step:1870/2330 train_time:70944ms step_avg:37.94ms
step:1871/2330 train_time:70980ms step_avg:37.94ms
step:1872/2330 train_time:71020ms step_avg:37.94ms
step:1873/2330 train_time:71056ms step_avg:37.94ms
step:1874/2330 train_time:71097ms step_avg:37.94ms
step:1875/2330 train_time:71132ms step_avg:37.94ms
step:1876/2330 train_time:71173ms step_avg:37.94ms
step:1877/2330 train_time:71208ms step_avg:37.94ms
step:1878/2330 train_time:71249ms step_avg:37.94ms
step:1879/2330 train_time:71284ms step_avg:37.94ms
step:1880/2330 train_time:71325ms step_avg:37.94ms
step:1881/2330 train_time:71360ms step_avg:37.94ms
step:1882/2330 train_time:71401ms step_avg:37.94ms
step:1883/2330 train_time:71436ms step_avg:37.94ms
step:1884/2330 train_time:71477ms step_avg:37.94ms
step:1885/2330 train_time:71513ms step_avg:37.94ms
step:1886/2330 train_time:71554ms step_avg:37.94ms
step:1887/2330 train_time:71589ms step_avg:37.94ms
step:1888/2330 train_time:71629ms step_avg:37.94ms
step:1889/2330 train_time:71664ms step_avg:37.94ms
step:1890/2330 train_time:71706ms step_avg:37.94ms
step:1891/2330 train_time:71741ms step_avg:37.94ms
step:1892/2330 train_time:71782ms step_avg:37.94ms
step:1893/2330 train_time:71817ms step_avg:37.94ms
step:1894/2330 train_time:71858ms step_avg:37.94ms
step:1895/2330 train_time:71892ms step_avg:37.94ms
step:1896/2330 train_time:71934ms step_avg:37.94ms
step:1897/2330 train_time:71968ms step_avg:37.94ms
step:1898/2330 train_time:72009ms step_avg:37.94ms
step:1899/2330 train_time:72045ms step_avg:37.94ms
step:1900/2330 train_time:72086ms step_avg:37.94ms
step:1901/2330 train_time:72121ms step_avg:37.94ms
step:1902/2330 train_time:72164ms step_avg:37.94ms
step:1903/2330 train_time:72197ms step_avg:37.94ms
step:1904/2330 train_time:72239ms step_avg:37.94ms
step:1905/2330 train_time:72274ms step_avg:37.94ms
step:1906/2330 train_time:72315ms step_avg:37.94ms
step:1907/2330 train_time:72350ms step_avg:37.94ms
step:1908/2330 train_time:72391ms step_avg:37.94ms
step:1909/2330 train_time:72427ms step_avg:37.94ms
step:1910/2330 train_time:72468ms step_avg:37.94ms
step:1911/2330 train_time:72503ms step_avg:37.94ms
step:1912/2330 train_time:72543ms step_avg:37.94ms
step:1913/2330 train_time:72580ms step_avg:37.94ms
step:1914/2330 train_time:72620ms step_avg:37.94ms
step:1915/2330 train_time:72656ms step_avg:37.94ms
step:1916/2330 train_time:72697ms step_avg:37.94ms
step:1917/2330 train_time:72733ms step_avg:37.94ms
step:1918/2330 train_time:72774ms step_avg:37.94ms
step:1919/2330 train_time:72809ms step_avg:37.94ms
step:1920/2330 train_time:72850ms step_avg:37.94ms
step:1921/2330 train_time:72884ms step_avg:37.94ms
step:1922/2330 train_time:72926ms step_avg:37.94ms
step:1923/2330 train_time:72961ms step_avg:37.94ms
step:1924/2330 train_time:73001ms step_avg:37.94ms
step:1925/2330 train_time:73037ms step_avg:37.94ms
step:1926/2330 train_time:73078ms step_avg:37.94ms
step:1927/2330 train_time:73113ms step_avg:37.94ms
step:1928/2330 train_time:73154ms step_avg:37.94ms
step:1929/2330 train_time:73188ms step_avg:37.94ms
step:1930/2330 train_time:73228ms step_avg:37.94ms
step:1931/2330 train_time:73264ms step_avg:37.94ms
step:1932/2330 train_time:73306ms step_avg:37.94ms
step:1933/2330 train_time:73340ms step_avg:37.94ms
step:1934/2330 train_time:73381ms step_avg:37.94ms
step:1935/2330 train_time:73416ms step_avg:37.94ms
step:1936/2330 train_time:73456ms step_avg:37.94ms
step:1937/2330 train_time:73492ms step_avg:37.94ms
step:1938/2330 train_time:73532ms step_avg:37.94ms
step:1939/2330 train_time:73567ms step_avg:37.94ms
step:1940/2330 train_time:73608ms step_avg:37.94ms
step:1941/2330 train_time:73643ms step_avg:37.94ms
step:1942/2330 train_time:73684ms step_avg:37.94ms
step:1943/2330 train_time:73720ms step_avg:37.94ms
step:1944/2330 train_time:73761ms step_avg:37.94ms
step:1945/2330 train_time:73796ms step_avg:37.94ms
step:1946/2330 train_time:73837ms step_avg:37.94ms
step:1947/2330 train_time:73872ms step_avg:37.94ms
step:1948/2330 train_time:73913ms step_avg:37.94ms
step:1949/2330 train_time:73947ms step_avg:37.94ms
step:1950/2330 train_time:73988ms step_avg:37.94ms
step:1951/2330 train_time:74023ms step_avg:37.94ms
step:1952/2330 train_time:74064ms step_avg:37.94ms
step:1953/2330 train_time:74099ms step_avg:37.94ms
step:1954/2330 train_time:74140ms step_avg:37.94ms
step:1955/2330 train_time:74175ms step_avg:37.94ms
step:1956/2330 train_time:74216ms step_avg:37.94ms
step:1957/2330 train_time:74252ms step_avg:37.94ms
step:1958/2330 train_time:74293ms step_avg:37.94ms
step:1959/2330 train_time:74328ms step_avg:37.94ms
step:1960/2330 train_time:74369ms step_avg:37.94ms
step:1961/2330 train_time:74403ms step_avg:37.94ms
step:1962/2330 train_time:74444ms step_avg:37.94ms
step:1963/2330 train_time:74480ms step_avg:37.94ms
step:1964/2330 train_time:74521ms step_avg:37.94ms
step:1965/2330 train_time:74556ms step_avg:37.94ms
step:1966/2330 train_time:74597ms step_avg:37.94ms
step:1967/2330 train_time:74632ms step_avg:37.94ms
step:1968/2330 train_time:74673ms step_avg:37.94ms
step:1969/2330 train_time:74708ms step_avg:37.94ms
step:1970/2330 train_time:74749ms step_avg:37.94ms
step:1971/2330 train_time:74784ms step_avg:37.94ms
step:1972/2330 train_time:74825ms step_avg:37.94ms
step:1973/2330 train_time:74860ms step_avg:37.94ms
step:1974/2330 train_time:74901ms step_avg:37.94ms
step:1975/2330 train_time:74937ms step_avg:37.94ms
step:1976/2330 train_time:74978ms step_avg:37.94ms
step:1977/2330 train_time:75013ms step_avg:37.94ms
step:1978/2330 train_time:75054ms step_avg:37.94ms
step:1979/2330 train_time:75088ms step_avg:37.94ms
step:1980/2330 train_time:75128ms step_avg:37.94ms
step:1981/2330 train_time:75164ms step_avg:37.94ms
step:1982/2330 train_time:75204ms step_avg:37.94ms
step:1983/2330 train_time:75241ms step_avg:37.94ms
step:1984/2330 train_time:75281ms step_avg:37.94ms
step:1985/2330 train_time:75317ms step_avg:37.94ms
step:1986/2330 train_time:75357ms step_avg:37.94ms
step:1987/2330 train_time:75392ms step_avg:37.94ms
step:1988/2330 train_time:75433ms step_avg:37.94ms
step:1989/2330 train_time:75468ms step_avg:37.94ms
step:1990/2330 train_time:75509ms step_avg:37.94ms
step:1991/2330 train_time:75544ms step_avg:37.94ms
step:1992/2330 train_time:75585ms step_avg:37.94ms
step:1993/2330 train_time:75620ms step_avg:37.94ms
step:1994/2330 train_time:75661ms step_avg:37.94ms
step:1995/2330 train_time:75697ms step_avg:37.94ms
step:1996/2330 train_time:75737ms step_avg:37.94ms
step:1997/2330 train_time:75773ms step_avg:37.94ms
step:1998/2330 train_time:75813ms step_avg:37.94ms
step:1999/2330 train_time:75848ms step_avg:37.94ms
step:2000/2330 train_time:75889ms step_avg:37.94ms
step:2000/2330 val_loss:5.3161 train_time:76001ms step_avg:38.00ms
step:2001/2330 train_time:76012ms step_avg:37.99ms
step:2002/2330 train_time:76023ms step_avg:37.97ms
step:2003/2330 train_time:76033ms step_avg:37.96ms
step:2004/2330 train_time:76044ms step_avg:37.95ms
step:2005/2330 train_time:76078ms step_avg:37.94ms
step:2006/2330 train_time:76118ms step_avg:37.95ms
step:2007/2330 train_time:76152ms step_avg:37.94ms
step:2008/2330 train_time:76193ms step_avg:37.94ms
step:2009/2330 train_time:76227ms step_avg:37.94ms
step:2010/2330 train_time:76268ms step_avg:37.94ms
step:2011/2330 train_time:76303ms step_avg:37.94ms
step:2012/2330 train_time:76346ms step_avg:37.95ms
step:2013/2330 train_time:76383ms step_avg:37.94ms
step:2014/2330 train_time:76424ms step_avg:37.95ms
step:2015/2330 train_time:76460ms step_avg:37.95ms
step:2016/2330 train_time:76500ms step_avg:37.95ms
step:2017/2330 train_time:76537ms step_avg:37.95ms
step:2018/2330 train_time:76578ms step_avg:37.95ms
step:2019/2330 train_time:76612ms step_avg:37.95ms
step:2020/2330 train_time:76652ms step_avg:37.95ms
step:2021/2330 train_time:76687ms step_avg:37.94ms
step:2022/2330 train_time:76727ms step_avg:37.95ms
step:2023/2330 train_time:76762ms step_avg:37.94ms
step:2024/2330 train_time:76802ms step_avg:37.95ms
step:2025/2330 train_time:76837ms step_avg:37.94ms
step:2026/2330 train_time:76877ms step_avg:37.95ms
step:2027/2330 train_time:76912ms step_avg:37.94ms
step:2028/2330 train_time:76953ms step_avg:37.95ms
step:2029/2330 train_time:76989ms step_avg:37.94ms
step:2030/2330 train_time:77030ms step_avg:37.95ms
step:2031/2330 train_time:77065ms step_avg:37.94ms
step:2032/2330 train_time:77106ms step_avg:37.95ms
step:2033/2330 train_time:77141ms step_avg:37.94ms
step:2034/2330 train_time:77181ms step_avg:37.95ms
step:2035/2330 train_time:77216ms step_avg:37.94ms
step:2036/2330 train_time:77257ms step_avg:37.95ms
step:2037/2330 train_time:77293ms step_avg:37.94ms
step:2038/2330 train_time:77335ms step_avg:37.95ms
step:2039/2330 train_time:77370ms step_avg:37.95ms
step:2040/2330 train_time:77411ms step_avg:37.95ms
step:2041/2330 train_time:77447ms step_avg:37.95ms
step:2042/2330 train_time:77488ms step_avg:37.95ms
step:2043/2330 train_time:77524ms step_avg:37.95ms
step:2044/2330 train_time:77566ms step_avg:37.95ms
step:2045/2330 train_time:77601ms step_avg:37.95ms
step:2046/2330 train_time:77641ms step_avg:37.95ms
step:2047/2330 train_time:77676ms step_avg:37.95ms
step:2048/2330 train_time:77716ms step_avg:37.95ms
step:2049/2330 train_time:77751ms step_avg:37.95ms
step:2050/2330 train_time:77791ms step_avg:37.95ms
step:2051/2330 train_time:77826ms step_avg:37.95ms
step:2052/2330 train_time:77866ms step_avg:37.95ms
step:2053/2330 train_time:77901ms step_avg:37.94ms
step:2054/2330 train_time:77942ms step_avg:37.95ms
step:2055/2330 train_time:77977ms step_avg:37.94ms
step:2056/2330 train_time:78017ms step_avg:37.95ms
step:2057/2330 train_time:78053ms step_avg:37.94ms
step:2058/2330 train_time:78094ms step_avg:37.95ms
step:2059/2330 train_time:78129ms step_avg:37.95ms
step:2060/2330 train_time:78170ms step_avg:37.95ms
step:2061/2330 train_time:78205ms step_avg:37.95ms
step:2062/2330 train_time:78246ms step_avg:37.95ms
step:2063/2330 train_time:78281ms step_avg:37.94ms
step:2064/2330 train_time:78322ms step_avg:37.95ms
step:2065/2330 train_time:78356ms step_avg:37.94ms
step:2066/2330 train_time:78397ms step_avg:37.95ms
step:2067/2330 train_time:78433ms step_avg:37.95ms
step:2068/2330 train_time:78474ms step_avg:37.95ms
step:2069/2330 train_time:78510ms step_avg:37.95ms
step:2070/2330 train_time:78551ms step_avg:37.95ms
step:2071/2330 train_time:78587ms step_avg:37.95ms
step:2072/2330 train_time:78628ms step_avg:37.95ms
step:2073/2330 train_time:78663ms step_avg:37.95ms
step:2074/2330 train_time:78704ms step_avg:37.95ms
step:2075/2330 train_time:78739ms step_avg:37.95ms
step:2076/2330 train_time:78779ms step_avg:37.95ms
step:2077/2330 train_time:78814ms step_avg:37.95ms
step:2078/2330 train_time:78855ms step_avg:37.95ms
step:2079/2330 train_time:78890ms step_avg:37.95ms
step:2080/2330 train_time:78931ms step_avg:37.95ms
step:2081/2330 train_time:78966ms step_avg:37.95ms
step:2082/2330 train_time:79007ms step_avg:37.95ms
step:2083/2330 train_time:79042ms step_avg:37.95ms
step:2084/2330 train_time:79083ms step_avg:37.95ms
step:2085/2330 train_time:79118ms step_avg:37.95ms
step:2086/2330 train_time:79159ms step_avg:37.95ms
step:2087/2330 train_time:79194ms step_avg:37.95ms
step:2088/2330 train_time:79236ms step_avg:37.95ms
step:2089/2330 train_time:79271ms step_avg:37.95ms
step:2090/2330 train_time:79312ms step_avg:37.95ms
step:2091/2330 train_time:79348ms step_avg:37.95ms
step:2092/2330 train_time:79389ms step_avg:37.95ms
step:2093/2330 train_time:79424ms step_avg:37.95ms
step:2094/2330 train_time:79465ms step_avg:37.95ms
step:2095/2330 train_time:79500ms step_avg:37.95ms
step:2096/2330 train_time:79541ms step_avg:37.95ms
step:2097/2330 train_time:79577ms step_avg:37.95ms
step:2098/2330 train_time:79618ms step_avg:37.95ms
step:2099/2330 train_time:79654ms step_avg:37.95ms
step:2100/2330 train_time:79695ms step_avg:37.95ms
step:2101/2330 train_time:79731ms step_avg:37.95ms
step:2102/2330 train_time:79771ms step_avg:37.95ms
step:2103/2330 train_time:79807ms step_avg:37.95ms
step:2104/2330 train_time:79848ms step_avg:37.95ms
step:2105/2330 train_time:79883ms step_avg:37.95ms
step:2106/2330 train_time:79924ms step_avg:37.95ms
step:2107/2330 train_time:79959ms step_avg:37.95ms
step:2108/2330 train_time:79999ms step_avg:37.95ms
step:2109/2330 train_time:80034ms step_avg:37.95ms
step:2110/2330 train_time:80075ms step_avg:37.95ms
step:2111/2330 train_time:80110ms step_avg:37.95ms
step:2112/2330 train_time:80150ms step_avg:37.95ms
step:2113/2330 train_time:80185ms step_avg:37.95ms
step:2114/2330 train_time:80226ms step_avg:37.95ms
step:2115/2330 train_time:80261ms step_avg:37.95ms
step:2116/2330 train_time:80301ms step_avg:37.95ms
step:2117/2330 train_time:80337ms step_avg:37.95ms
step:2118/2330 train_time:80378ms step_avg:37.95ms
step:2119/2330 train_time:80413ms step_avg:37.95ms
step:2120/2330 train_time:80454ms step_avg:37.95ms
step:2121/2330 train_time:80491ms step_avg:37.95ms
step:2122/2330 train_time:80532ms step_avg:37.95ms
step:2123/2330 train_time:80567ms step_avg:37.95ms
step:2124/2330 train_time:80608ms step_avg:37.95ms
step:2125/2330 train_time:80644ms step_avg:37.95ms
step:2126/2330 train_time:80685ms step_avg:37.95ms
step:2127/2330 train_time:80719ms step_avg:37.95ms
step:2128/2330 train_time:80760ms step_avg:37.95ms
step:2129/2330 train_time:80795ms step_avg:37.95ms
step:2130/2330 train_time:80837ms step_avg:37.95ms
step:2131/2330 train_time:80872ms step_avg:37.95ms
step:2132/2330 train_time:80913ms step_avg:37.95ms
step:2133/2330 train_time:80948ms step_avg:37.95ms
step:2134/2330 train_time:80989ms step_avg:37.95ms
step:2135/2330 train_time:81024ms step_avg:37.95ms
step:2136/2330 train_time:81065ms step_avg:37.95ms
step:2137/2330 train_time:81100ms step_avg:37.95ms
step:2138/2330 train_time:81140ms step_avg:37.95ms
step:2139/2330 train_time:81175ms step_avg:37.95ms
step:2140/2330 train_time:81215ms step_avg:37.95ms
step:2141/2330 train_time:81252ms step_avg:37.95ms
step:2142/2330 train_time:81293ms step_avg:37.95ms
step:2143/2330 train_time:81328ms step_avg:37.95ms
step:2144/2330 train_time:81369ms step_avg:37.95ms
step:2145/2330 train_time:81405ms step_avg:37.95ms
step:2146/2330 train_time:81446ms step_avg:37.95ms
step:2147/2330 train_time:81482ms step_avg:37.95ms
step:2148/2330 train_time:81523ms step_avg:37.95ms
step:2149/2330 train_time:81558ms step_avg:37.95ms
step:2150/2330 train_time:81599ms step_avg:37.95ms
step:2151/2330 train_time:81634ms step_avg:37.95ms
step:2152/2330 train_time:81675ms step_avg:37.95ms
step:2153/2330 train_time:81710ms step_avg:37.95ms
step:2154/2330 train_time:81751ms step_avg:37.95ms
step:2155/2330 train_time:81787ms step_avg:37.95ms
step:2156/2330 train_time:81828ms step_avg:37.95ms
step:2157/2330 train_time:81863ms step_avg:37.95ms
step:2158/2330 train_time:81903ms step_avg:37.95ms
step:2159/2330 train_time:81939ms step_avg:37.95ms
step:2160/2330 train_time:81980ms step_avg:37.95ms
step:2161/2330 train_time:82015ms step_avg:37.95ms
step:2162/2330 train_time:82056ms step_avg:37.95ms
step:2163/2330 train_time:82092ms step_avg:37.95ms
step:2164/2330 train_time:82133ms step_avg:37.95ms
step:2165/2330 train_time:82168ms step_avg:37.95ms
step:2166/2330 train_time:82209ms step_avg:37.95ms
step:2167/2330 train_time:82243ms step_avg:37.95ms
step:2168/2330 train_time:82284ms step_avg:37.95ms
step:2169/2330 train_time:82319ms step_avg:37.95ms
step:2170/2330 train_time:82359ms step_avg:37.95ms
step:2171/2330 train_time:82396ms step_avg:37.95ms
step:2172/2330 train_time:82437ms step_avg:37.95ms
step:2173/2330 train_time:82473ms step_avg:37.95ms
step:2174/2330 train_time:82513ms step_avg:37.95ms
step:2175/2330 train_time:82549ms step_avg:37.95ms
step:2176/2330 train_time:82589ms step_avg:37.95ms
step:2177/2330 train_time:82624ms step_avg:37.95ms
step:2178/2330 train_time:82665ms step_avg:37.95ms
step:2179/2330 train_time:82700ms step_avg:37.95ms
step:2180/2330 train_time:82740ms step_avg:37.95ms
step:2181/2330 train_time:82775ms step_avg:37.95ms
step:2182/2330 train_time:82816ms step_avg:37.95ms
step:2183/2330 train_time:82852ms step_avg:37.95ms
step:2184/2330 train_time:82893ms step_avg:37.95ms
step:2185/2330 train_time:82928ms step_avg:37.95ms
step:2186/2330 train_time:82969ms step_avg:37.95ms
step:2187/2330 train_time:83004ms step_avg:37.95ms
step:2188/2330 train_time:83045ms step_avg:37.95ms
step:2189/2330 train_time:83079ms step_avg:37.95ms
step:2190/2330 train_time:83120ms step_avg:37.95ms
step:2191/2330 train_time:83156ms step_avg:37.95ms
step:2192/2330 train_time:83197ms step_avg:37.95ms
step:2193/2330 train_time:83233ms step_avg:37.95ms
step:2194/2330 train_time:83274ms step_avg:37.96ms
step:2195/2330 train_time:83309ms step_avg:37.95ms
step:2196/2330 train_time:83350ms step_avg:37.96ms
step:2197/2330 train_time:83386ms step_avg:37.95ms
step:2198/2330 train_time:83426ms step_avg:37.96ms
step:2199/2330 train_time:83462ms step_avg:37.95ms
step:2200/2330 train_time:83502ms step_avg:37.96ms
step:2201/2330 train_time:83537ms step_avg:37.95ms
step:2202/2330 train_time:83578ms step_avg:37.96ms
step:2203/2330 train_time:83613ms step_avg:37.95ms
step:2204/2330 train_time:83654ms step_avg:37.96ms
step:2205/2330 train_time:83689ms step_avg:37.95ms
step:2206/2330 train_time:83730ms step_avg:37.96ms
step:2207/2330 train_time:83766ms step_avg:37.95ms
step:2208/2330 train_time:83807ms step_avg:37.96ms
step:2209/2330 train_time:83843ms step_avg:37.96ms
step:2210/2330 train_time:83884ms step_avg:37.96ms
step:2211/2330 train_time:83918ms step_avg:37.95ms
step:2212/2330 train_time:83959ms step_avg:37.96ms
step:2213/2330 train_time:83994ms step_avg:37.95ms
step:2214/2330 train_time:84035ms step_avg:37.96ms
step:2215/2330 train_time:84070ms step_avg:37.95ms
step:2216/2330 train_time:84111ms step_avg:37.96ms
step:2217/2330 train_time:84146ms step_avg:37.95ms
step:2218/2330 train_time:84187ms step_avg:37.96ms
step:2219/2330 train_time:84222ms step_avg:37.96ms
step:2220/2330 train_time:84263ms step_avg:37.96ms
step:2221/2330 train_time:84298ms step_avg:37.95ms
step:2222/2330 train_time:84339ms step_avg:37.96ms
step:2223/2330 train_time:84374ms step_avg:37.95ms
step:2224/2330 train_time:84415ms step_avg:37.96ms
step:2225/2330 train_time:84450ms step_avg:37.95ms
step:2226/2330 train_time:84491ms step_avg:37.96ms
step:2227/2330 train_time:84527ms step_avg:37.96ms
step:2228/2330 train_time:84568ms step_avg:37.96ms
step:2229/2330 train_time:84603ms step_avg:37.96ms
step:2230/2330 train_time:84644ms step_avg:37.96ms
step:2231/2330 train_time:84680ms step_avg:37.96ms
step:2232/2330 train_time:84720ms step_avg:37.96ms
step:2233/2330 train_time:84756ms step_avg:37.96ms
step:2234/2330 train_time:84797ms step_avg:37.96ms
step:2235/2330 train_time:84833ms step_avg:37.96ms
step:2236/2330 train_time:84874ms step_avg:37.96ms
step:2237/2330 train_time:84909ms step_avg:37.96ms
step:2238/2330 train_time:84950ms step_avg:37.96ms
step:2239/2330 train_time:84985ms step_avg:37.96ms
step:2240/2330 train_time:85025ms step_avg:37.96ms
step:2241/2330 train_time:85060ms step_avg:37.96ms
step:2242/2330 train_time:85100ms step_avg:37.96ms
step:2243/2330 train_time:85137ms step_avg:37.96ms
step:2244/2330 train_time:85177ms step_avg:37.96ms
step:2245/2330 train_time:85212ms step_avg:37.96ms
step:2246/2330 train_time:85253ms step_avg:37.96ms
step:2247/2330 train_time:85289ms step_avg:37.96ms
step:2248/2330 train_time:85330ms step_avg:37.96ms
step:2249/2330 train_time:85365ms step_avg:37.96ms
step:2250/2330 train_time:85406ms step_avg:37.96ms
step:2250/2330 val_loss:5.2836 train_time:85516ms step_avg:38.01ms
step:2251/2330 train_time:85527ms step_avg:38.00ms
step:2252/2330 train_time:85538ms step_avg:37.98ms
step:2253/2330 train_time:85547ms step_avg:37.97ms
step:2254/2330 train_time:85559ms step_avg:37.96ms
step:2255/2330 train_time:85595ms step_avg:37.96ms
step:2256/2330 train_time:85635ms step_avg:37.96ms
step:2257/2330 train_time:85670ms step_avg:37.96ms
step:2258/2330 train_time:85710ms step_avg:37.96ms
step:2259/2330 train_time:85744ms step_avg:37.96ms
step:2260/2330 train_time:85785ms step_avg:37.96ms
step:2261/2330 train_time:85821ms step_avg:37.96ms
step:2262/2330 train_time:85862ms step_avg:37.96ms
step:2263/2330 train_time:85902ms step_avg:37.96ms
step:2264/2330 train_time:85942ms step_avg:37.96ms
step:2265/2330 train_time:85978ms step_avg:37.96ms
step:2266/2330 train_time:86019ms step_avg:37.96ms
step:2267/2330 train_time:86054ms step_avg:37.96ms
step:2268/2330 train_time:86094ms step_avg:37.96ms
step:2269/2330 train_time:86130ms step_avg:37.96ms
step:2270/2330 train_time:86171ms step_avg:37.96ms
step:2271/2330 train_time:86205ms step_avg:37.96ms
step:2272/2330 train_time:86246ms step_avg:37.96ms
step:2273/2330 train_time:86281ms step_avg:37.96ms
step:2274/2330 train_time:86321ms step_avg:37.96ms
step:2275/2330 train_time:86356ms step_avg:37.96ms
step:2276/2330 train_time:86397ms step_avg:37.96ms
step:2277/2330 train_time:86431ms step_avg:37.96ms
step:2278/2330 train_time:86473ms step_avg:37.96ms
step:2279/2330 train_time:86508ms step_avg:37.96ms
step:2280/2330 train_time:86549ms step_avg:37.96ms
step:2281/2330 train_time:86584ms step_avg:37.96ms
step:2282/2330 train_time:86625ms step_avg:37.96ms
step:2283/2330 train_time:86660ms step_avg:37.96ms
step:2284/2330 train_time:86700ms step_avg:37.96ms
step:2285/2330 train_time:86735ms step_avg:37.96ms
step:2286/2330 train_time:86777ms step_avg:37.96ms
step:2287/2330 train_time:86813ms step_avg:37.96ms
step:2288/2330 train_time:86854ms step_avg:37.96ms
step:2289/2330 train_time:86890ms step_avg:37.96ms
step:2290/2330 train_time:86932ms step_avg:37.96ms
step:2291/2330 train_time:86968ms step_avg:37.96ms
step:2292/2330 train_time:87009ms step_avg:37.96ms
step:2293/2330 train_time:87045ms step_avg:37.96ms
step:2294/2330 train_time:87086ms step_avg:37.96ms
step:2295/2330 train_time:87120ms step_avg:37.96ms
step:2296/2330 train_time:87161ms step_avg:37.96ms
step:2297/2330 train_time:87196ms step_avg:37.96ms
step:2298/2330 train_time:87237ms step_avg:37.96ms
step:2299/2330 train_time:87271ms step_avg:37.96ms
step:2300/2330 train_time:87312ms step_avg:37.96ms
step:2301/2330 train_time:87347ms step_avg:37.96ms
step:2302/2330 train_time:87388ms step_avg:37.96ms
step:2303/2330 train_time:87423ms step_avg:37.96ms
step:2304/2330 train_time:87464ms step_avg:37.96ms
step:2305/2330 train_time:87499ms step_avg:37.96ms
step:2306/2330 train_time:87539ms step_avg:37.96ms
step:2307/2330 train_time:87575ms step_avg:37.96ms
step:2308/2330 train_time:87616ms step_avg:37.96ms
step:2309/2330 train_time:87650ms step_avg:37.96ms
step:2310/2330 train_time:87691ms step_avg:37.96ms
step:2311/2330 train_time:87727ms step_avg:37.96ms
step:2312/2330 train_time:87768ms step_avg:37.96ms
step:2313/2330 train_time:87804ms step_avg:37.96ms
step:2314/2330 train_time:87845ms step_avg:37.96ms
step:2315/2330 train_time:87880ms step_avg:37.96ms
step:2316/2330 train_time:87921ms step_avg:37.96ms
step:2317/2330 train_time:87956ms step_avg:37.96ms
step:2318/2330 train_time:87997ms step_avg:37.96ms
step:2319/2330 train_time:88032ms step_avg:37.96ms
step:2320/2330 train_time:88074ms step_avg:37.96ms
step:2321/2330 train_time:88109ms step_avg:37.96ms
step:2322/2330 train_time:88150ms step_avg:37.96ms
step:2323/2330 train_time:88186ms step_avg:37.96ms
step:2324/2330 train_time:88227ms step_avg:37.96ms
step:2325/2330 train_time:88262ms step_avg:37.96ms
step:2326/2330 train_time:88303ms step_avg:37.96ms
step:2327/2330 train_time:88338ms step_avg:37.96ms
step:2328/2330 train_time:88379ms step_avg:37.96ms
step:2329/2330 train_time:88414ms step_avg:37.96ms
step:2330/2330 train_time:88455ms step_avg:37.96ms
step:2330/2330 val_loss:5.2751 train_time:88566ms step_avg:38.01ms
peak memory allocated: 27165 MiB reserved: 38888 MiB
