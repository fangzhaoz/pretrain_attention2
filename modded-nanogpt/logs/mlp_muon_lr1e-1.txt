import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_muon_lr1e-1"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-1, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 04:22:23 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   33C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   31C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   31C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             115W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:84ms step_avg:84.43ms
step:2/2330 train_time:168ms step_avg:83.83ms
step:3/2330 train_time:212ms step_avg:70.68ms
step:4/2330 train_time:225ms step_avg:56.17ms
step:5/2330 train_time:236ms step_avg:47.13ms
step:6/2330 train_time:260ms step_avg:43.31ms
step:7/2330 train_time:293ms step_avg:41.91ms
step:8/2330 train_time:337ms step_avg:42.07ms
step:9/2330 train_time:371ms step_avg:41.21ms
step:10/2330 train_time:415ms step_avg:41.46ms
step:11/2330 train_time:450ms step_avg:40.88ms
step:12/2330 train_time:493ms step_avg:41.11ms
step:13/2330 train_time:528ms step_avg:40.60ms
step:14/2330 train_time:572ms step_avg:40.83ms
step:15/2330 train_time:606ms step_avg:40.43ms
step:16/2330 train_time:651ms step_avg:40.70ms
step:17/2330 train_time:685ms step_avg:40.31ms
step:18/2330 train_time:730ms step_avg:40.55ms
step:19/2330 train_time:765ms step_avg:40.26ms
step:20/2330 train_time:809ms step_avg:40.44ms
step:21/2330 train_time:843ms step_avg:40.16ms
step:22/2330 train_time:888ms step_avg:40.37ms
step:23/2330 train_time:922ms step_avg:40.09ms
step:24/2330 train_time:966ms step_avg:40.23ms
step:25/2330 train_time:1000ms step_avg:40.02ms
step:26/2330 train_time:1045ms step_avg:40.19ms
step:27/2330 train_time:1083ms step_avg:40.12ms
step:28/2330 train_time:1135ms step_avg:40.53ms
step:29/2330 train_time:1173ms step_avg:40.44ms
step:30/2330 train_time:1220ms step_avg:40.68ms
step:31/2330 train_time:1256ms step_avg:40.53ms
step:32/2330 train_time:1301ms step_avg:40.66ms
step:33/2330 train_time:1336ms step_avg:40.49ms
step:34/2330 train_time:1380ms step_avg:40.59ms
step:35/2330 train_time:1415ms step_avg:40.44ms
step:36/2330 train_time:1460ms step_avg:40.54ms
step:37/2330 train_time:1494ms step_avg:40.39ms
step:38/2330 train_time:1538ms step_avg:40.48ms
step:39/2330 train_time:1573ms step_avg:40.34ms
step:40/2330 train_time:1617ms step_avg:40.42ms
step:41/2330 train_time:1652ms step_avg:40.29ms
step:42/2330 train_time:1696ms step_avg:40.39ms
step:43/2330 train_time:1731ms step_avg:40.25ms
step:44/2330 train_time:1774ms step_avg:40.32ms
step:45/2330 train_time:1809ms step_avg:40.21ms
step:46/2330 train_time:1854ms step_avg:40.30ms
step:47/2330 train_time:1889ms step_avg:40.20ms
step:48/2330 train_time:1933ms step_avg:40.28ms
step:49/2330 train_time:1969ms step_avg:40.18ms
step:50/2330 train_time:2014ms step_avg:40.28ms
step:51/2330 train_time:2051ms step_avg:40.21ms
step:52/2330 train_time:2097ms step_avg:40.33ms
step:53/2330 train_time:2134ms step_avg:40.26ms
step:54/2330 train_time:2180ms step_avg:40.36ms
step:55/2330 train_time:2216ms step_avg:40.29ms
step:56/2330 train_time:2262ms step_avg:40.39ms
step:57/2330 train_time:2298ms step_avg:40.31ms
step:58/2330 train_time:2342ms step_avg:40.37ms
step:59/2330 train_time:2376ms step_avg:40.28ms
step:60/2330 train_time:2421ms step_avg:40.35ms
step:61/2330 train_time:2456ms step_avg:40.26ms
step:62/2330 train_time:2500ms step_avg:40.32ms
step:63/2330 train_time:2534ms step_avg:40.23ms
step:64/2330 train_time:2579ms step_avg:40.29ms
step:65/2330 train_time:2613ms step_avg:40.20ms
step:66/2330 train_time:2657ms step_avg:40.26ms
step:67/2330 train_time:2691ms step_avg:40.17ms
step:68/2330 train_time:2735ms step_avg:40.23ms
step:69/2330 train_time:2770ms step_avg:40.15ms
step:70/2330 train_time:2814ms step_avg:40.20ms
step:71/2330 train_time:2849ms step_avg:40.13ms
step:72/2330 train_time:2894ms step_avg:40.19ms
step:73/2330 train_time:2929ms step_avg:40.12ms
step:74/2330 train_time:2973ms step_avg:40.17ms
step:75/2330 train_time:3008ms step_avg:40.11ms
step:76/2330 train_time:3055ms step_avg:40.20ms
step:77/2330 train_time:3091ms step_avg:40.14ms
step:78/2330 train_time:3137ms step_avg:40.22ms
step:79/2330 train_time:3173ms step_avg:40.17ms
step:80/2330 train_time:3218ms step_avg:40.23ms
step:81/2330 train_time:3254ms step_avg:40.17ms
step:82/2330 train_time:3299ms step_avg:40.24ms
step:83/2330 train_time:3334ms step_avg:40.17ms
step:84/2330 train_time:3378ms step_avg:40.22ms
step:85/2330 train_time:3414ms step_avg:40.16ms
step:86/2330 train_time:3458ms step_avg:40.21ms
step:87/2330 train_time:3494ms step_avg:40.16ms
step:88/2330 train_time:3537ms step_avg:40.19ms
step:89/2330 train_time:3573ms step_avg:40.14ms
step:90/2330 train_time:3616ms step_avg:40.18ms
step:91/2330 train_time:3651ms step_avg:40.12ms
step:92/2330 train_time:3695ms step_avg:40.16ms
step:93/2330 train_time:3729ms step_avg:40.10ms
step:94/2330 train_time:3773ms step_avg:40.14ms
step:95/2330 train_time:3808ms step_avg:40.08ms
step:96/2330 train_time:3853ms step_avg:40.13ms
step:97/2330 train_time:3888ms step_avg:40.08ms
step:98/2330 train_time:3931ms step_avg:40.11ms
step:99/2330 train_time:3967ms step_avg:40.07ms
step:100/2330 train_time:4011ms step_avg:40.11ms
step:101/2330 train_time:4048ms step_avg:40.08ms
step:102/2330 train_time:4094ms step_avg:40.14ms
step:103/2330 train_time:4130ms step_avg:40.09ms
step:104/2330 train_time:4175ms step_avg:40.15ms
step:105/2330 train_time:4211ms step_avg:40.11ms
step:106/2330 train_time:4256ms step_avg:40.15ms
step:107/2330 train_time:4292ms step_avg:40.11ms
step:108/2330 train_time:4337ms step_avg:40.16ms
step:109/2330 train_time:4372ms step_avg:40.11ms
step:110/2330 train_time:4416ms step_avg:40.15ms
step:111/2330 train_time:4452ms step_avg:40.11ms
step:112/2330 train_time:4496ms step_avg:40.14ms
step:113/2330 train_time:4530ms step_avg:40.09ms
step:114/2330 train_time:4575ms step_avg:40.13ms
step:115/2330 train_time:4610ms step_avg:40.08ms
step:116/2330 train_time:4654ms step_avg:40.12ms
step:117/2330 train_time:4689ms step_avg:40.07ms
step:118/2330 train_time:4733ms step_avg:40.11ms
step:119/2330 train_time:4768ms step_avg:40.07ms
step:120/2330 train_time:4812ms step_avg:40.10ms
step:121/2330 train_time:4847ms step_avg:40.06ms
step:122/2330 train_time:4892ms step_avg:40.10ms
step:123/2330 train_time:4927ms step_avg:40.05ms
step:124/2330 train_time:4971ms step_avg:40.09ms
step:125/2330 train_time:5006ms step_avg:40.04ms
step:126/2330 train_time:5051ms step_avg:40.09ms
step:127/2330 train_time:5088ms step_avg:40.06ms
step:128/2330 train_time:5133ms step_avg:40.10ms
step:129/2330 train_time:5168ms step_avg:40.06ms
step:130/2330 train_time:5213ms step_avg:40.10ms
step:131/2330 train_time:5249ms step_avg:40.07ms
step:132/2330 train_time:5295ms step_avg:40.11ms
step:133/2330 train_time:5330ms step_avg:40.08ms
step:134/2330 train_time:5375ms step_avg:40.11ms
step:135/2330 train_time:5410ms step_avg:40.08ms
step:136/2330 train_time:5455ms step_avg:40.11ms
step:137/2330 train_time:5491ms step_avg:40.08ms
step:138/2330 train_time:5535ms step_avg:40.11ms
step:139/2330 train_time:5570ms step_avg:40.07ms
step:140/2330 train_time:5614ms step_avg:40.10ms
step:141/2330 train_time:5649ms step_avg:40.06ms
step:142/2330 train_time:5694ms step_avg:40.10ms
step:143/2330 train_time:5729ms step_avg:40.06ms
step:144/2330 train_time:5773ms step_avg:40.09ms
step:145/2330 train_time:5808ms step_avg:40.06ms
step:146/2330 train_time:5852ms step_avg:40.08ms
step:147/2330 train_time:5887ms step_avg:40.05ms
step:148/2330 train_time:5933ms step_avg:40.08ms
step:149/2330 train_time:5969ms step_avg:40.06ms
step:150/2330 train_time:6014ms step_avg:40.09ms
step:151/2330 train_time:6049ms step_avg:40.06ms
step:152/2330 train_time:6095ms step_avg:40.10ms
step:153/2330 train_time:6130ms step_avg:40.06ms
step:154/2330 train_time:6174ms step_avg:40.09ms
step:155/2330 train_time:6210ms step_avg:40.06ms
step:156/2330 train_time:6255ms step_avg:40.10ms
step:157/2330 train_time:6290ms step_avg:40.07ms
step:158/2330 train_time:6336ms step_avg:40.10ms
step:159/2330 train_time:6371ms step_avg:40.07ms
step:160/2330 train_time:6415ms step_avg:40.09ms
step:161/2330 train_time:6450ms step_avg:40.06ms
step:162/2330 train_time:6496ms step_avg:40.10ms
step:163/2330 train_time:6531ms step_avg:40.07ms
step:164/2330 train_time:6575ms step_avg:40.09ms
step:165/2330 train_time:6610ms step_avg:40.06ms
step:166/2330 train_time:6654ms step_avg:40.09ms
step:167/2330 train_time:6689ms step_avg:40.06ms
step:168/2330 train_time:6734ms step_avg:40.08ms
step:169/2330 train_time:6770ms step_avg:40.06ms
step:170/2330 train_time:6814ms step_avg:40.08ms
step:171/2330 train_time:6849ms step_avg:40.05ms
step:172/2330 train_time:6894ms step_avg:40.08ms
step:173/2330 train_time:6930ms step_avg:40.06ms
step:174/2330 train_time:6975ms step_avg:40.09ms
step:175/2330 train_time:7010ms step_avg:40.06ms
step:176/2330 train_time:7056ms step_avg:40.09ms
step:177/2330 train_time:7090ms step_avg:40.06ms
step:178/2330 train_time:7135ms step_avg:40.08ms
step:179/2330 train_time:7171ms step_avg:40.06ms
step:180/2330 train_time:7216ms step_avg:40.09ms
step:181/2330 train_time:7252ms step_avg:40.07ms
step:182/2330 train_time:7297ms step_avg:40.09ms
step:183/2330 train_time:7331ms step_avg:40.06ms
step:184/2330 train_time:7376ms step_avg:40.08ms
step:185/2330 train_time:7411ms step_avg:40.06ms
step:186/2330 train_time:7457ms step_avg:40.09ms
step:187/2330 train_time:7491ms step_avg:40.06ms
step:188/2330 train_time:7536ms step_avg:40.08ms
step:189/2330 train_time:7571ms step_avg:40.06ms
step:190/2330 train_time:7615ms step_avg:40.08ms
step:191/2330 train_time:7651ms step_avg:40.06ms
step:192/2330 train_time:7695ms step_avg:40.08ms
step:193/2330 train_time:7730ms step_avg:40.05ms
step:194/2330 train_time:7775ms step_avg:40.08ms
step:195/2330 train_time:7810ms step_avg:40.05ms
step:196/2330 train_time:7855ms step_avg:40.08ms
step:197/2330 train_time:7890ms step_avg:40.05ms
step:198/2330 train_time:7935ms step_avg:40.08ms
step:199/2330 train_time:7970ms step_avg:40.05ms
step:200/2330 train_time:8015ms step_avg:40.08ms
step:201/2330 train_time:8050ms step_avg:40.05ms
step:202/2330 train_time:8095ms step_avg:40.07ms
step:203/2330 train_time:8130ms step_avg:40.05ms
step:204/2330 train_time:8176ms step_avg:40.08ms
step:205/2330 train_time:8211ms step_avg:40.05ms
step:206/2330 train_time:8257ms step_avg:40.08ms
step:207/2330 train_time:8292ms step_avg:40.06ms
step:208/2330 train_time:8337ms step_avg:40.08ms
step:209/2330 train_time:8371ms step_avg:40.05ms
step:210/2330 train_time:8416ms step_avg:40.08ms
step:211/2330 train_time:8452ms step_avg:40.05ms
step:212/2330 train_time:8496ms step_avg:40.07ms
step:213/2330 train_time:8531ms step_avg:40.05ms
step:214/2330 train_time:8575ms step_avg:40.07ms
step:215/2330 train_time:8611ms step_avg:40.05ms
step:216/2330 train_time:8655ms step_avg:40.07ms
step:217/2330 train_time:8691ms step_avg:40.05ms
step:218/2330 train_time:8735ms step_avg:40.07ms
step:219/2330 train_time:8770ms step_avg:40.05ms
step:220/2330 train_time:8814ms step_avg:40.06ms
step:221/2330 train_time:8849ms step_avg:40.04ms
step:222/2330 train_time:8894ms step_avg:40.06ms
step:223/2330 train_time:8930ms step_avg:40.04ms
step:224/2330 train_time:8974ms step_avg:40.06ms
step:225/2330 train_time:9009ms step_avg:40.04ms
step:226/2330 train_time:9054ms step_avg:40.06ms
step:227/2330 train_time:9089ms step_avg:40.04ms
step:228/2330 train_time:9135ms step_avg:40.07ms
step:229/2330 train_time:9171ms step_avg:40.05ms
step:230/2330 train_time:9216ms step_avg:40.07ms
step:231/2330 train_time:9251ms step_avg:40.05ms
step:232/2330 train_time:9296ms step_avg:40.07ms
step:233/2330 train_time:9331ms step_avg:40.05ms
step:234/2330 train_time:9376ms step_avg:40.07ms
step:235/2330 train_time:9411ms step_avg:40.05ms
step:236/2330 train_time:9457ms step_avg:40.07ms
step:237/2330 train_time:9491ms step_avg:40.05ms
step:238/2330 train_time:9536ms step_avg:40.07ms
step:239/2330 train_time:9571ms step_avg:40.05ms
step:240/2330 train_time:9616ms step_avg:40.07ms
step:241/2330 train_time:9650ms step_avg:40.04ms
step:242/2330 train_time:9696ms step_avg:40.06ms
step:243/2330 train_time:9730ms step_avg:40.04ms
step:244/2330 train_time:9775ms step_avg:40.06ms
step:245/2330 train_time:9810ms step_avg:40.04ms
step:246/2330 train_time:9854ms step_avg:40.06ms
step:247/2330 train_time:9889ms step_avg:40.04ms
step:248/2330 train_time:9934ms step_avg:40.06ms
step:249/2330 train_time:9969ms step_avg:40.04ms
step:250/2330 train_time:10014ms step_avg:40.06ms
step:250/2330 val_loss:5.4383 train_time:10103ms step_avg:40.41ms
step:251/2330 train_time:10116ms step_avg:40.30ms
step:252/2330 train_time:10128ms step_avg:40.19ms
step:253/2330 train_time:10139ms step_avg:40.07ms
step:254/2330 train_time:10175ms step_avg:40.06ms
step:255/2330 train_time:10209ms step_avg:40.04ms
step:256/2330 train_time:10253ms step_avg:40.05ms
step:257/2330 train_time:10288ms step_avg:40.03ms
step:258/2330 train_time:10331ms step_avg:40.04ms
step:259/2330 train_time:10365ms step_avg:40.02ms
step:260/2330 train_time:10410ms step_avg:40.04ms
step:261/2330 train_time:10447ms step_avg:40.03ms
step:262/2330 train_time:10494ms step_avg:40.05ms
step:263/2330 train_time:10531ms step_avg:40.04ms
step:264/2330 train_time:10577ms step_avg:40.06ms
step:265/2330 train_time:10614ms step_avg:40.05ms
step:266/2330 train_time:10658ms step_avg:40.07ms
step:267/2330 train_time:10694ms step_avg:40.05ms
step:268/2330 train_time:10738ms step_avg:40.07ms
step:269/2330 train_time:10772ms step_avg:40.05ms
step:270/2330 train_time:10817ms step_avg:40.06ms
step:271/2330 train_time:10852ms step_avg:40.05ms
step:272/2330 train_time:10896ms step_avg:40.06ms
step:273/2330 train_time:10931ms step_avg:40.04ms
step:274/2330 train_time:10974ms step_avg:40.05ms
step:275/2330 train_time:11009ms step_avg:40.03ms
step:276/2330 train_time:11055ms step_avg:40.05ms
step:277/2330 train_time:11090ms step_avg:40.04ms
step:278/2330 train_time:11133ms step_avg:40.05ms
step:279/2330 train_time:11169ms step_avg:40.03ms
step:280/2330 train_time:11212ms step_avg:40.04ms
step:281/2330 train_time:11247ms step_avg:40.03ms
step:282/2330 train_time:11292ms step_avg:40.04ms
step:283/2330 train_time:11326ms step_avg:40.02ms
step:284/2330 train_time:11371ms step_avg:40.04ms
step:285/2330 train_time:11407ms step_avg:40.02ms
step:286/2330 train_time:11452ms step_avg:40.04ms
step:287/2330 train_time:11488ms step_avg:40.03ms
step:288/2330 train_time:11534ms step_avg:40.05ms
step:289/2330 train_time:11569ms step_avg:40.03ms
step:290/2330 train_time:11613ms step_avg:40.05ms
step:291/2330 train_time:11649ms step_avg:40.03ms
step:292/2330 train_time:11694ms step_avg:40.05ms
step:293/2330 train_time:11729ms step_avg:40.03ms
step:294/2330 train_time:11774ms step_avg:40.05ms
step:295/2330 train_time:11809ms step_avg:40.03ms
step:296/2330 train_time:11853ms step_avg:40.04ms
step:297/2330 train_time:11887ms step_avg:40.02ms
step:298/2330 train_time:11932ms step_avg:40.04ms
step:299/2330 train_time:11967ms step_avg:40.02ms
step:300/2330 train_time:12011ms step_avg:40.04ms
step:301/2330 train_time:12046ms step_avg:40.02ms
step:302/2330 train_time:12091ms step_avg:40.04ms
step:303/2330 train_time:12125ms step_avg:40.02ms
step:304/2330 train_time:12169ms step_avg:40.03ms
step:305/2330 train_time:12204ms step_avg:40.01ms
step:306/2330 train_time:12249ms step_avg:40.03ms
step:307/2330 train_time:12284ms step_avg:40.01ms
step:308/2330 train_time:12329ms step_avg:40.03ms
step:309/2330 train_time:12364ms step_avg:40.01ms
step:310/2330 train_time:12409ms step_avg:40.03ms
step:311/2330 train_time:12444ms step_avg:40.01ms
step:312/2330 train_time:12490ms step_avg:40.03ms
step:313/2330 train_time:12526ms step_avg:40.02ms
step:314/2330 train_time:12571ms step_avg:40.03ms
step:315/2330 train_time:12606ms step_avg:40.02ms
step:316/2330 train_time:12651ms step_avg:40.03ms
step:317/2330 train_time:12687ms step_avg:40.02ms
step:318/2330 train_time:12732ms step_avg:40.04ms
step:319/2330 train_time:12766ms step_avg:40.02ms
step:320/2330 train_time:12811ms step_avg:40.03ms
step:321/2330 train_time:12845ms step_avg:40.02ms
step:322/2330 train_time:12890ms step_avg:40.03ms
step:323/2330 train_time:12925ms step_avg:40.01ms
step:324/2330 train_time:12969ms step_avg:40.03ms
step:325/2330 train_time:13003ms step_avg:40.01ms
step:326/2330 train_time:13049ms step_avg:40.03ms
step:327/2330 train_time:13083ms step_avg:40.01ms
step:328/2330 train_time:13127ms step_avg:40.02ms
step:329/2330 train_time:13163ms step_avg:40.01ms
step:330/2330 train_time:13207ms step_avg:40.02ms
step:331/2330 train_time:13243ms step_avg:40.01ms
step:332/2330 train_time:13288ms step_avg:40.02ms
step:333/2330 train_time:13323ms step_avg:40.01ms
step:334/2330 train_time:13368ms step_avg:40.02ms
step:335/2330 train_time:13403ms step_avg:40.01ms
step:336/2330 train_time:13447ms step_avg:40.02ms
step:337/2330 train_time:13483ms step_avg:40.01ms
step:338/2330 train_time:13528ms step_avg:40.02ms
step:339/2330 train_time:13564ms step_avg:40.01ms
step:340/2330 train_time:13609ms step_avg:40.03ms
step:341/2330 train_time:13645ms step_avg:40.02ms
step:342/2330 train_time:13690ms step_avg:40.03ms
step:343/2330 train_time:13726ms step_avg:40.02ms
step:344/2330 train_time:13769ms step_avg:40.03ms
step:345/2330 train_time:13805ms step_avg:40.01ms
step:346/2330 train_time:13850ms step_avg:40.03ms
step:347/2330 train_time:13885ms step_avg:40.02ms
step:348/2330 train_time:13930ms step_avg:40.03ms
step:349/2330 train_time:13964ms step_avg:40.01ms
step:350/2330 train_time:14008ms step_avg:40.02ms
step:351/2330 train_time:14044ms step_avg:40.01ms
step:352/2330 train_time:14088ms step_avg:40.02ms
step:353/2330 train_time:14123ms step_avg:40.01ms
step:354/2330 train_time:14167ms step_avg:40.02ms
step:355/2330 train_time:14203ms step_avg:40.01ms
step:356/2330 train_time:14247ms step_avg:40.02ms
step:357/2330 train_time:14283ms step_avg:40.01ms
step:358/2330 train_time:14328ms step_avg:40.02ms
step:359/2330 train_time:14362ms step_avg:40.01ms
step:360/2330 train_time:14408ms step_avg:40.02ms
step:361/2330 train_time:14443ms step_avg:40.01ms
step:362/2330 train_time:14488ms step_avg:40.02ms
step:363/2330 train_time:14523ms step_avg:40.01ms
step:364/2330 train_time:14569ms step_avg:40.02ms
step:365/2330 train_time:14604ms step_avg:40.01ms
step:366/2330 train_time:14649ms step_avg:40.03ms
step:367/2330 train_time:14685ms step_avg:40.01ms
step:368/2330 train_time:14729ms step_avg:40.03ms
step:369/2330 train_time:14764ms step_avg:40.01ms
step:370/2330 train_time:14809ms step_avg:40.03ms
step:371/2330 train_time:14845ms step_avg:40.01ms
step:372/2330 train_time:14889ms step_avg:40.02ms
step:373/2330 train_time:14924ms step_avg:40.01ms
step:374/2330 train_time:14968ms step_avg:40.02ms
step:375/2330 train_time:15004ms step_avg:40.01ms
step:376/2330 train_time:15048ms step_avg:40.02ms
step:377/2330 train_time:15084ms step_avg:40.01ms
step:378/2330 train_time:15129ms step_avg:40.02ms
step:379/2330 train_time:15163ms step_avg:40.01ms
step:380/2330 train_time:15207ms step_avg:40.02ms
step:381/2330 train_time:15243ms step_avg:40.01ms
step:382/2330 train_time:15287ms step_avg:40.02ms
step:383/2330 train_time:15323ms step_avg:40.01ms
step:384/2330 train_time:15367ms step_avg:40.02ms
step:385/2330 train_time:15403ms step_avg:40.01ms
step:386/2330 train_time:15448ms step_avg:40.02ms
step:387/2330 train_time:15483ms step_avg:40.01ms
step:388/2330 train_time:15528ms step_avg:40.02ms
step:389/2330 train_time:15563ms step_avg:40.01ms
step:390/2330 train_time:15608ms step_avg:40.02ms
step:391/2330 train_time:15643ms step_avg:40.01ms
step:392/2330 train_time:15689ms step_avg:40.02ms
step:393/2330 train_time:15724ms step_avg:40.01ms
step:394/2330 train_time:15769ms step_avg:40.02ms
step:395/2330 train_time:15804ms step_avg:40.01ms
step:396/2330 train_time:15849ms step_avg:40.02ms
step:397/2330 train_time:15884ms step_avg:40.01ms
step:398/2330 train_time:15929ms step_avg:40.02ms
step:399/2330 train_time:15964ms step_avg:40.01ms
step:400/2330 train_time:16009ms step_avg:40.02ms
step:401/2330 train_time:16044ms step_avg:40.01ms
step:402/2330 train_time:16089ms step_avg:40.02ms
step:403/2330 train_time:16124ms step_avg:40.01ms
step:404/2330 train_time:16168ms step_avg:40.02ms
step:405/2330 train_time:16203ms step_avg:40.01ms
step:406/2330 train_time:16248ms step_avg:40.02ms
step:407/2330 train_time:16283ms step_avg:40.01ms
step:408/2330 train_time:16328ms step_avg:40.02ms
step:409/2330 train_time:16363ms step_avg:40.01ms
step:410/2330 train_time:16408ms step_avg:40.02ms
step:411/2330 train_time:16443ms step_avg:40.01ms
step:412/2330 train_time:16488ms step_avg:40.02ms
step:413/2330 train_time:16524ms step_avg:40.01ms
step:414/2330 train_time:16568ms step_avg:40.02ms
step:415/2330 train_time:16604ms step_avg:40.01ms
step:416/2330 train_time:16648ms step_avg:40.02ms
step:417/2330 train_time:16684ms step_avg:40.01ms
step:418/2330 train_time:16729ms step_avg:40.02ms
step:419/2330 train_time:16764ms step_avg:40.01ms
step:420/2330 train_time:16809ms step_avg:40.02ms
step:421/2330 train_time:16844ms step_avg:40.01ms
step:422/2330 train_time:16889ms step_avg:40.02ms
step:423/2330 train_time:16925ms step_avg:40.01ms
step:424/2330 train_time:16969ms step_avg:40.02ms
step:425/2330 train_time:17004ms step_avg:40.01ms
step:426/2330 train_time:17049ms step_avg:40.02ms
step:427/2330 train_time:17084ms step_avg:40.01ms
step:428/2330 train_time:17128ms step_avg:40.02ms
step:429/2330 train_time:17163ms step_avg:40.01ms
step:430/2330 train_time:17208ms step_avg:40.02ms
step:431/2330 train_time:17243ms step_avg:40.01ms
step:432/2330 train_time:17288ms step_avg:40.02ms
step:433/2330 train_time:17324ms step_avg:40.01ms
step:434/2330 train_time:17368ms step_avg:40.02ms
step:435/2330 train_time:17404ms step_avg:40.01ms
step:436/2330 train_time:17449ms step_avg:40.02ms
step:437/2330 train_time:17483ms step_avg:40.01ms
step:438/2330 train_time:17528ms step_avg:40.02ms
step:439/2330 train_time:17564ms step_avg:40.01ms
step:440/2330 train_time:17609ms step_avg:40.02ms
step:441/2330 train_time:17645ms step_avg:40.01ms
step:442/2330 train_time:17690ms step_avg:40.02ms
step:443/2330 train_time:17725ms step_avg:40.01ms
step:444/2330 train_time:17770ms step_avg:40.02ms
step:445/2330 train_time:17805ms step_avg:40.01ms
step:446/2330 train_time:17850ms step_avg:40.02ms
step:447/2330 train_time:17885ms step_avg:40.01ms
step:448/2330 train_time:17929ms step_avg:40.02ms
step:449/2330 train_time:17965ms step_avg:40.01ms
step:450/2330 train_time:18009ms step_avg:40.02ms
step:451/2330 train_time:18044ms step_avg:40.01ms
step:452/2330 train_time:18088ms step_avg:40.02ms
step:453/2330 train_time:18123ms step_avg:40.01ms
step:454/2330 train_time:18168ms step_avg:40.02ms
step:455/2330 train_time:18203ms step_avg:40.01ms
step:456/2330 train_time:18248ms step_avg:40.02ms
step:457/2330 train_time:18282ms step_avg:40.00ms
step:458/2330 train_time:18327ms step_avg:40.02ms
step:459/2330 train_time:18362ms step_avg:40.00ms
step:460/2330 train_time:18406ms step_avg:40.01ms
step:461/2330 train_time:18442ms step_avg:40.00ms
step:462/2330 train_time:18488ms step_avg:40.02ms
step:463/2330 train_time:18523ms step_avg:40.01ms
step:464/2330 train_time:18567ms step_avg:40.02ms
step:465/2330 train_time:18603ms step_avg:40.01ms
step:466/2330 train_time:18648ms step_avg:40.02ms
step:467/2330 train_time:18683ms step_avg:40.01ms
step:468/2330 train_time:18727ms step_avg:40.02ms
step:469/2330 train_time:18763ms step_avg:40.01ms
step:470/2330 train_time:18808ms step_avg:40.02ms
step:471/2330 train_time:18843ms step_avg:40.01ms
step:472/2330 train_time:18888ms step_avg:40.02ms
step:473/2330 train_time:18924ms step_avg:40.01ms
step:474/2330 train_time:18968ms step_avg:40.02ms
step:475/2330 train_time:19003ms step_avg:40.01ms
step:476/2330 train_time:19048ms step_avg:40.02ms
step:477/2330 train_time:19084ms step_avg:40.01ms
step:478/2330 train_time:19128ms step_avg:40.02ms
step:479/2330 train_time:19163ms step_avg:40.01ms
step:480/2330 train_time:19208ms step_avg:40.02ms
step:481/2330 train_time:19244ms step_avg:40.01ms
step:482/2330 train_time:19288ms step_avg:40.02ms
step:483/2330 train_time:19324ms step_avg:40.01ms
step:484/2330 train_time:19368ms step_avg:40.02ms
step:485/2330 train_time:19403ms step_avg:40.01ms
step:486/2330 train_time:19449ms step_avg:40.02ms
step:487/2330 train_time:19484ms step_avg:40.01ms
step:488/2330 train_time:19529ms step_avg:40.02ms
step:489/2330 train_time:19564ms step_avg:40.01ms
step:490/2330 train_time:19608ms step_avg:40.02ms
step:491/2330 train_time:19644ms step_avg:40.01ms
step:492/2330 train_time:19689ms step_avg:40.02ms
step:493/2330 train_time:19725ms step_avg:40.01ms
step:494/2330 train_time:19769ms step_avg:40.02ms
step:495/2330 train_time:19804ms step_avg:40.01ms
step:496/2330 train_time:19849ms step_avg:40.02ms
step:497/2330 train_time:19885ms step_avg:40.01ms
step:498/2330 train_time:19929ms step_avg:40.02ms
step:499/2330 train_time:19964ms step_avg:40.01ms
step:500/2330 train_time:20009ms step_avg:40.02ms
step:500/2330 val_loss:5.3083 train_time:20095ms step_avg:40.19ms
step:501/2330 train_time:20108ms step_avg:40.14ms
step:502/2330 train_time:20120ms step_avg:40.08ms
step:503/2330 train_time:20131ms step_avg:40.02ms
step:504/2330 train_time:20168ms step_avg:40.02ms
step:505/2330 train_time:20203ms step_avg:40.00ms
step:506/2330 train_time:20246ms step_avg:40.01ms
step:507/2330 train_time:20281ms step_avg:40.00ms
step:508/2330 train_time:20324ms step_avg:40.01ms
step:509/2330 train_time:20359ms step_avg:40.00ms
step:510/2330 train_time:20403ms step_avg:40.01ms
step:511/2330 train_time:20442ms step_avg:40.00ms
step:512/2330 train_time:20488ms step_avg:40.02ms
step:513/2330 train_time:20525ms step_avg:40.01ms
step:514/2330 train_time:20572ms step_avg:40.02ms
step:515/2330 train_time:20609ms step_avg:40.02ms
step:516/2330 train_time:20653ms step_avg:40.03ms
step:517/2330 train_time:20690ms step_avg:40.02ms
step:518/2330 train_time:20734ms step_avg:40.03ms
step:519/2330 train_time:20769ms step_avg:40.02ms
step:520/2330 train_time:20814ms step_avg:40.03ms
step:521/2330 train_time:20849ms step_avg:40.02ms
step:522/2330 train_time:20894ms step_avg:40.03ms
step:523/2330 train_time:20929ms step_avg:40.02ms
step:524/2330 train_time:20973ms step_avg:40.03ms
step:525/2330 train_time:21010ms step_avg:40.02ms
step:526/2330 train_time:21056ms step_avg:40.03ms
step:527/2330 train_time:21092ms step_avg:40.02ms
step:528/2330 train_time:21137ms step_avg:40.03ms
step:529/2330 train_time:21172ms step_avg:40.02ms
step:530/2330 train_time:21217ms step_avg:40.03ms
step:531/2330 train_time:21252ms step_avg:40.02ms
step:532/2330 train_time:21296ms step_avg:40.03ms
step:533/2330 train_time:21333ms step_avg:40.02ms
step:534/2330 train_time:21378ms step_avg:40.03ms
step:535/2330 train_time:21414ms step_avg:40.03ms
step:536/2330 train_time:21460ms step_avg:40.04ms
step:537/2330 train_time:21496ms step_avg:40.03ms
step:538/2330 train_time:21542ms step_avg:40.04ms
step:539/2330 train_time:21577ms step_avg:40.03ms
step:540/2330 train_time:21622ms step_avg:40.04ms
step:541/2330 train_time:21656ms step_avg:40.03ms
step:542/2330 train_time:21701ms step_avg:40.04ms
step:543/2330 train_time:21736ms step_avg:40.03ms
step:544/2330 train_time:21781ms step_avg:40.04ms
step:545/2330 train_time:21816ms step_avg:40.03ms
step:546/2330 train_time:21860ms step_avg:40.04ms
step:547/2330 train_time:21896ms step_avg:40.03ms
step:548/2330 train_time:21940ms step_avg:40.04ms
step:549/2330 train_time:21977ms step_avg:40.03ms
step:550/2330 train_time:22021ms step_avg:40.04ms
step:551/2330 train_time:22056ms step_avg:40.03ms
step:552/2330 train_time:22101ms step_avg:40.04ms
step:553/2330 train_time:22137ms step_avg:40.03ms
step:554/2330 train_time:22181ms step_avg:40.04ms
step:555/2330 train_time:22216ms step_avg:40.03ms
step:556/2330 train_time:22260ms step_avg:40.04ms
step:557/2330 train_time:22295ms step_avg:40.03ms
step:558/2330 train_time:22341ms step_avg:40.04ms
step:559/2330 train_time:22376ms step_avg:40.03ms
step:560/2330 train_time:22420ms step_avg:40.04ms
step:561/2330 train_time:22456ms step_avg:40.03ms
step:562/2330 train_time:22501ms step_avg:40.04ms
step:563/2330 train_time:22537ms step_avg:40.03ms
step:564/2330 train_time:22582ms step_avg:40.04ms
step:565/2330 train_time:22617ms step_avg:40.03ms
step:566/2330 train_time:22662ms step_avg:40.04ms
step:567/2330 train_time:22697ms step_avg:40.03ms
step:568/2330 train_time:22742ms step_avg:40.04ms
step:569/2330 train_time:22777ms step_avg:40.03ms
step:570/2330 train_time:22821ms step_avg:40.04ms
step:571/2330 train_time:22857ms step_avg:40.03ms
step:572/2330 train_time:22901ms step_avg:40.04ms
step:573/2330 train_time:22936ms step_avg:40.03ms
step:574/2330 train_time:22981ms step_avg:40.04ms
step:575/2330 train_time:23016ms step_avg:40.03ms
step:576/2330 train_time:23060ms step_avg:40.04ms
step:577/2330 train_time:23096ms step_avg:40.03ms
step:578/2330 train_time:23140ms step_avg:40.03ms
step:579/2330 train_time:23176ms step_avg:40.03ms
step:580/2330 train_time:23220ms step_avg:40.03ms
step:581/2330 train_time:23255ms step_avg:40.03ms
step:582/2330 train_time:23300ms step_avg:40.03ms
step:583/2330 train_time:23335ms step_avg:40.03ms
step:584/2330 train_time:23380ms step_avg:40.03ms
step:585/2330 train_time:23415ms step_avg:40.03ms
step:586/2330 train_time:23461ms step_avg:40.04ms
step:587/2330 train_time:23496ms step_avg:40.03ms
step:588/2330 train_time:23541ms step_avg:40.04ms
step:589/2330 train_time:23576ms step_avg:40.03ms
step:590/2330 train_time:23621ms step_avg:40.03ms
step:591/2330 train_time:23656ms step_avg:40.03ms
step:592/2330 train_time:23701ms step_avg:40.04ms
step:593/2330 train_time:23736ms step_avg:40.03ms
step:594/2330 train_time:23781ms step_avg:40.03ms
step:595/2330 train_time:23816ms step_avg:40.03ms
step:596/2330 train_time:23860ms step_avg:40.03ms
step:597/2330 train_time:23896ms step_avg:40.03ms
step:598/2330 train_time:23940ms step_avg:40.03ms
step:599/2330 train_time:23975ms step_avg:40.03ms
step:600/2330 train_time:24020ms step_avg:40.03ms
step:601/2330 train_time:24055ms step_avg:40.02ms
step:602/2330 train_time:24100ms step_avg:40.03ms
step:603/2330 train_time:24135ms step_avg:40.02ms
step:604/2330 train_time:24179ms step_avg:40.03ms
step:605/2330 train_time:24215ms step_avg:40.02ms
step:606/2330 train_time:24259ms step_avg:40.03ms
step:607/2330 train_time:24295ms step_avg:40.02ms
step:608/2330 train_time:24340ms step_avg:40.03ms
step:609/2330 train_time:24375ms step_avg:40.02ms
step:610/2330 train_time:24420ms step_avg:40.03ms
step:611/2330 train_time:24455ms step_avg:40.03ms
step:612/2330 train_time:24501ms step_avg:40.03ms
step:613/2330 train_time:24536ms step_avg:40.03ms
step:614/2330 train_time:24581ms step_avg:40.03ms
step:615/2330 train_time:24616ms step_avg:40.03ms
step:616/2330 train_time:24661ms step_avg:40.03ms
step:617/2330 train_time:24696ms step_avg:40.03ms
step:618/2330 train_time:24742ms step_avg:40.04ms
step:619/2330 train_time:24777ms step_avg:40.03ms
step:620/2330 train_time:24821ms step_avg:40.03ms
step:621/2330 train_time:24857ms step_avg:40.03ms
step:622/2330 train_time:24901ms step_avg:40.03ms
step:623/2330 train_time:24936ms step_avg:40.03ms
step:624/2330 train_time:24981ms step_avg:40.03ms
step:625/2330 train_time:25017ms step_avg:40.03ms
step:626/2330 train_time:25060ms step_avg:40.03ms
step:627/2330 train_time:25096ms step_avg:40.03ms
step:628/2330 train_time:25140ms step_avg:40.03ms
step:629/2330 train_time:25175ms step_avg:40.02ms
step:630/2330 train_time:25220ms step_avg:40.03ms
step:631/2330 train_time:25255ms step_avg:40.02ms
step:632/2330 train_time:25300ms step_avg:40.03ms
step:633/2330 train_time:25336ms step_avg:40.02ms
step:634/2330 train_time:25380ms step_avg:40.03ms
step:635/2330 train_time:25415ms step_avg:40.02ms
step:636/2330 train_time:25461ms step_avg:40.03ms
step:637/2330 train_time:25497ms step_avg:40.03ms
step:638/2330 train_time:25541ms step_avg:40.03ms
step:639/2330 train_time:25576ms step_avg:40.03ms
step:640/2330 train_time:25621ms step_avg:40.03ms
step:641/2330 train_time:25656ms step_avg:40.03ms
step:642/2330 train_time:25701ms step_avg:40.03ms
step:643/2330 train_time:25737ms step_avg:40.03ms
step:644/2330 train_time:25782ms step_avg:40.03ms
step:645/2330 train_time:25817ms step_avg:40.03ms
step:646/2330 train_time:25861ms step_avg:40.03ms
step:647/2330 train_time:25896ms step_avg:40.03ms
step:648/2330 train_time:25940ms step_avg:40.03ms
step:649/2330 train_time:25975ms step_avg:40.02ms
step:650/2330 train_time:26020ms step_avg:40.03ms
step:651/2330 train_time:26056ms step_avg:40.02ms
step:652/2330 train_time:26100ms step_avg:40.03ms
step:653/2330 train_time:26136ms step_avg:40.02ms
step:654/2330 train_time:26180ms step_avg:40.03ms
step:655/2330 train_time:26215ms step_avg:40.02ms
step:656/2330 train_time:26260ms step_avg:40.03ms
step:657/2330 train_time:26295ms step_avg:40.02ms
step:658/2330 train_time:26340ms step_avg:40.03ms
step:659/2330 train_time:26376ms step_avg:40.02ms
step:660/2330 train_time:26421ms step_avg:40.03ms
step:661/2330 train_time:26456ms step_avg:40.02ms
step:662/2330 train_time:26501ms step_avg:40.03ms
step:663/2330 train_time:26536ms step_avg:40.02ms
step:664/2330 train_time:26581ms step_avg:40.03ms
step:665/2330 train_time:26616ms step_avg:40.02ms
step:666/2330 train_time:26661ms step_avg:40.03ms
step:667/2330 train_time:26696ms step_avg:40.02ms
step:668/2330 train_time:26742ms step_avg:40.03ms
step:669/2330 train_time:26777ms step_avg:40.03ms
step:670/2330 train_time:26821ms step_avg:40.03ms
step:671/2330 train_time:26856ms step_avg:40.02ms
step:672/2330 train_time:26900ms step_avg:40.03ms
step:673/2330 train_time:26936ms step_avg:40.02ms
step:674/2330 train_time:26981ms step_avg:40.03ms
step:675/2330 train_time:27016ms step_avg:40.02ms
step:676/2330 train_time:27060ms step_avg:40.03ms
step:677/2330 train_time:27096ms step_avg:40.02ms
step:678/2330 train_time:27140ms step_avg:40.03ms
step:679/2330 train_time:27175ms step_avg:40.02ms
step:680/2330 train_time:27220ms step_avg:40.03ms
step:681/2330 train_time:27255ms step_avg:40.02ms
step:682/2330 train_time:27300ms step_avg:40.03ms
step:683/2330 train_time:27335ms step_avg:40.02ms
step:684/2330 train_time:27381ms step_avg:40.03ms
step:685/2330 train_time:27416ms step_avg:40.02ms
step:686/2330 train_time:27461ms step_avg:40.03ms
step:687/2330 train_time:27496ms step_avg:40.02ms
step:688/2330 train_time:27541ms step_avg:40.03ms
step:689/2330 train_time:27576ms step_avg:40.02ms
step:690/2330 train_time:27621ms step_avg:40.03ms
step:691/2330 train_time:27656ms step_avg:40.02ms
step:692/2330 train_time:27701ms step_avg:40.03ms
step:693/2330 train_time:27736ms step_avg:40.02ms
step:694/2330 train_time:27781ms step_avg:40.03ms
step:695/2330 train_time:27817ms step_avg:40.02ms
step:696/2330 train_time:27861ms step_avg:40.03ms
step:697/2330 train_time:27896ms step_avg:40.02ms
step:698/2330 train_time:27941ms step_avg:40.03ms
step:699/2330 train_time:27976ms step_avg:40.02ms
step:700/2330 train_time:28021ms step_avg:40.03ms
step:701/2330 train_time:28055ms step_avg:40.02ms
step:702/2330 train_time:28100ms step_avg:40.03ms
step:703/2330 train_time:28135ms step_avg:40.02ms
step:704/2330 train_time:28180ms step_avg:40.03ms
step:705/2330 train_time:28214ms step_avg:40.02ms
step:706/2330 train_time:28260ms step_avg:40.03ms
step:707/2330 train_time:28295ms step_avg:40.02ms
step:708/2330 train_time:28340ms step_avg:40.03ms
step:709/2330 train_time:28375ms step_avg:40.02ms
step:710/2330 train_time:28419ms step_avg:40.03ms
step:711/2330 train_time:28454ms step_avg:40.02ms
step:712/2330 train_time:28500ms step_avg:40.03ms
step:713/2330 train_time:28535ms step_avg:40.02ms
step:714/2330 train_time:28580ms step_avg:40.03ms
step:715/2330 train_time:28615ms step_avg:40.02ms
step:716/2330 train_time:28660ms step_avg:40.03ms
step:717/2330 train_time:28696ms step_avg:40.02ms
step:718/2330 train_time:28741ms step_avg:40.03ms
step:719/2330 train_time:28776ms step_avg:40.02ms
step:720/2330 train_time:28821ms step_avg:40.03ms
step:721/2330 train_time:28856ms step_avg:40.02ms
step:722/2330 train_time:28901ms step_avg:40.03ms
step:723/2330 train_time:28935ms step_avg:40.02ms
step:724/2330 train_time:28980ms step_avg:40.03ms
step:725/2330 train_time:29014ms step_avg:40.02ms
step:726/2330 train_time:29059ms step_avg:40.03ms
step:727/2330 train_time:29094ms step_avg:40.02ms
step:728/2330 train_time:29139ms step_avg:40.03ms
step:729/2330 train_time:29175ms step_avg:40.02ms
step:730/2330 train_time:29219ms step_avg:40.03ms
step:731/2330 train_time:29254ms step_avg:40.02ms
step:732/2330 train_time:29299ms step_avg:40.03ms
step:733/2330 train_time:29334ms step_avg:40.02ms
step:734/2330 train_time:29379ms step_avg:40.03ms
step:735/2330 train_time:29414ms step_avg:40.02ms
step:736/2330 train_time:29459ms step_avg:40.03ms
step:737/2330 train_time:29495ms step_avg:40.02ms
step:738/2330 train_time:29539ms step_avg:40.03ms
step:739/2330 train_time:29575ms step_avg:40.02ms
step:740/2330 train_time:29619ms step_avg:40.03ms
step:741/2330 train_time:29654ms step_avg:40.02ms
step:742/2330 train_time:29699ms step_avg:40.03ms
step:743/2330 train_time:29735ms step_avg:40.02ms
step:744/2330 train_time:29779ms step_avg:40.03ms
step:745/2330 train_time:29815ms step_avg:40.02ms
step:746/2330 train_time:29860ms step_avg:40.03ms
step:747/2330 train_time:29896ms step_avg:40.02ms
step:748/2330 train_time:29941ms step_avg:40.03ms
step:749/2330 train_time:29975ms step_avg:40.02ms
step:750/2330 train_time:30020ms step_avg:40.03ms
step:750/2330 val_loss:5.2488 train_time:30107ms step_avg:40.14ms
step:751/2330 train_time:30120ms step_avg:40.11ms
step:752/2330 train_time:30133ms step_avg:40.07ms
step:753/2330 train_time:30144ms step_avg:40.03ms
step:754/2330 train_time:30179ms step_avg:40.03ms
step:755/2330 train_time:30213ms step_avg:40.02ms
step:756/2330 train_time:30257ms step_avg:40.02ms
step:757/2330 train_time:30291ms step_avg:40.01ms
step:758/2330 train_time:30335ms step_avg:40.02ms
step:759/2330 train_time:30369ms step_avg:40.01ms
step:760/2330 train_time:30414ms step_avg:40.02ms
step:761/2330 train_time:30454ms step_avg:40.02ms
step:762/2330 train_time:30501ms step_avg:40.03ms
step:763/2330 train_time:30537ms step_avg:40.02ms
step:764/2330 train_time:30582ms step_avg:40.03ms
step:765/2330 train_time:30619ms step_avg:40.02ms
step:766/2330 train_time:30663ms step_avg:40.03ms
step:767/2330 train_time:30698ms step_avg:40.02ms
step:768/2330 train_time:30742ms step_avg:40.03ms
step:769/2330 train_time:30776ms step_avg:40.02ms
step:770/2330 train_time:30820ms step_avg:40.03ms
step:771/2330 train_time:30855ms step_avg:40.02ms
step:772/2330 train_time:30898ms step_avg:40.02ms
step:773/2330 train_time:30933ms step_avg:40.02ms
step:774/2330 train_time:30978ms step_avg:40.02ms
step:775/2330 train_time:31013ms step_avg:40.02ms
step:776/2330 train_time:31058ms step_avg:40.02ms
step:777/2330 train_time:31093ms step_avg:40.02ms
step:778/2330 train_time:31137ms step_avg:40.02ms
step:779/2330 train_time:31172ms step_avg:40.02ms
step:780/2330 train_time:31216ms step_avg:40.02ms
step:781/2330 train_time:31251ms step_avg:40.01ms
step:782/2330 train_time:31295ms step_avg:40.02ms
step:783/2330 train_time:31330ms step_avg:40.01ms
step:784/2330 train_time:31375ms step_avg:40.02ms
step:785/2330 train_time:31411ms step_avg:40.01ms
step:786/2330 train_time:31457ms step_avg:40.02ms
step:787/2330 train_time:31493ms step_avg:40.02ms
step:788/2330 train_time:31539ms step_avg:40.02ms
step:789/2330 train_time:31574ms step_avg:40.02ms
step:790/2330 train_time:31619ms step_avg:40.02ms
step:791/2330 train_time:31654ms step_avg:40.02ms
step:792/2330 train_time:31698ms step_avg:40.02ms
step:793/2330 train_time:31733ms step_avg:40.02ms
step:794/2330 train_time:31777ms step_avg:40.02ms
step:795/2330 train_time:31812ms step_avg:40.02ms
step:796/2330 train_time:31856ms step_avg:40.02ms
step:797/2330 train_time:31891ms step_avg:40.01ms
step:798/2330 train_time:31936ms step_avg:40.02ms
step:799/2330 train_time:31971ms step_avg:40.01ms
step:800/2330 train_time:32015ms step_avg:40.02ms
step:801/2330 train_time:32050ms step_avg:40.01ms
step:802/2330 train_time:32095ms step_avg:40.02ms
step:803/2330 train_time:32130ms step_avg:40.01ms
step:804/2330 train_time:32174ms step_avg:40.02ms
step:805/2330 train_time:32209ms step_avg:40.01ms
step:806/2330 train_time:32254ms step_avg:40.02ms
step:807/2330 train_time:32289ms step_avg:40.01ms
step:808/2330 train_time:32334ms step_avg:40.02ms
step:809/2330 train_time:32369ms step_avg:40.01ms
step:810/2330 train_time:32414ms step_avg:40.02ms
step:811/2330 train_time:32450ms step_avg:40.01ms
step:812/2330 train_time:32495ms step_avg:40.02ms
step:813/2330 train_time:32530ms step_avg:40.01ms
step:814/2330 train_time:32575ms step_avg:40.02ms
step:815/2330 train_time:32610ms step_avg:40.01ms
step:816/2330 train_time:32655ms step_avg:40.02ms
step:817/2330 train_time:32691ms step_avg:40.01ms
step:818/2330 train_time:32736ms step_avg:40.02ms
step:819/2330 train_time:32771ms step_avg:40.01ms
step:820/2330 train_time:32815ms step_avg:40.02ms
step:821/2330 train_time:32850ms step_avg:40.01ms
step:822/2330 train_time:32895ms step_avg:40.02ms
step:823/2330 train_time:32930ms step_avg:40.01ms
step:824/2330 train_time:32973ms step_avg:40.02ms
step:825/2330 train_time:33009ms step_avg:40.01ms
step:826/2330 train_time:33053ms step_avg:40.02ms
step:827/2330 train_time:33088ms step_avg:40.01ms
step:828/2330 train_time:33132ms step_avg:40.01ms
step:829/2330 train_time:33167ms step_avg:40.01ms
step:830/2330 train_time:33212ms step_avg:40.01ms
step:831/2330 train_time:33247ms step_avg:40.01ms
step:832/2330 train_time:33292ms step_avg:40.01ms
step:833/2330 train_time:33327ms step_avg:40.01ms
step:834/2330 train_time:33373ms step_avg:40.02ms
step:835/2330 train_time:33408ms step_avg:40.01ms
step:836/2330 train_time:33454ms step_avg:40.02ms
step:837/2330 train_time:33489ms step_avg:40.01ms
step:838/2330 train_time:33535ms step_avg:40.02ms
step:839/2330 train_time:33569ms step_avg:40.01ms
step:840/2330 train_time:33614ms step_avg:40.02ms
step:841/2330 train_time:33650ms step_avg:40.01ms
step:842/2330 train_time:33695ms step_avg:40.02ms
step:843/2330 train_time:33731ms step_avg:40.01ms
step:844/2330 train_time:33775ms step_avg:40.02ms
step:845/2330 train_time:33810ms step_avg:40.01ms
step:846/2330 train_time:33855ms step_avg:40.02ms
step:847/2330 train_time:33890ms step_avg:40.01ms
step:848/2330 train_time:33935ms step_avg:40.02ms
step:849/2330 train_time:33970ms step_avg:40.01ms
step:850/2330 train_time:34015ms step_avg:40.02ms
step:851/2330 train_time:34050ms step_avg:40.01ms
step:852/2330 train_time:34094ms step_avg:40.02ms
step:853/2330 train_time:34129ms step_avg:40.01ms
step:854/2330 train_time:34173ms step_avg:40.02ms
step:855/2330 train_time:34208ms step_avg:40.01ms
step:856/2330 train_time:34253ms step_avg:40.02ms
step:857/2330 train_time:34289ms step_avg:40.01ms
step:858/2330 train_time:34333ms step_avg:40.02ms
step:859/2330 train_time:34369ms step_avg:40.01ms
step:860/2330 train_time:34414ms step_avg:40.02ms
step:861/2330 train_time:34450ms step_avg:40.01ms
step:862/2330 train_time:34495ms step_avg:40.02ms
step:863/2330 train_time:34529ms step_avg:40.01ms
step:864/2330 train_time:34575ms step_avg:40.02ms
step:865/2330 train_time:34610ms step_avg:40.01ms
step:866/2330 train_time:34655ms step_avg:40.02ms
step:867/2330 train_time:34691ms step_avg:40.01ms
step:868/2330 train_time:34735ms step_avg:40.02ms
step:869/2330 train_time:34771ms step_avg:40.01ms
step:870/2330 train_time:34815ms step_avg:40.02ms
step:871/2330 train_time:34850ms step_avg:40.01ms
step:872/2330 train_time:34896ms step_avg:40.02ms
step:873/2330 train_time:34930ms step_avg:40.01ms
step:874/2330 train_time:34975ms step_avg:40.02ms
step:875/2330 train_time:35010ms step_avg:40.01ms
step:876/2330 train_time:35054ms step_avg:40.02ms
step:877/2330 train_time:35089ms step_avg:40.01ms
step:878/2330 train_time:35133ms step_avg:40.02ms
step:879/2330 train_time:35169ms step_avg:40.01ms
step:880/2330 train_time:35214ms step_avg:40.02ms
step:881/2330 train_time:35249ms step_avg:40.01ms
step:882/2330 train_time:35294ms step_avg:40.02ms
step:883/2330 train_time:35329ms step_avg:40.01ms
step:884/2330 train_time:35374ms step_avg:40.02ms
step:885/2330 train_time:35408ms step_avg:40.01ms
step:886/2330 train_time:35454ms step_avg:40.02ms
step:887/2330 train_time:35489ms step_avg:40.01ms
step:888/2330 train_time:35535ms step_avg:40.02ms
step:889/2330 train_time:35570ms step_avg:40.01ms
step:890/2330 train_time:35615ms step_avg:40.02ms
step:891/2330 train_time:35649ms step_avg:40.01ms
step:892/2330 train_time:35695ms step_avg:40.02ms
step:893/2330 train_time:35730ms step_avg:40.01ms
step:894/2330 train_time:35775ms step_avg:40.02ms
step:895/2330 train_time:35810ms step_avg:40.01ms
step:896/2330 train_time:35855ms step_avg:40.02ms
step:897/2330 train_time:35891ms step_avg:40.01ms
step:898/2330 train_time:35935ms step_avg:40.02ms
step:899/2330 train_time:35970ms step_avg:40.01ms
step:900/2330 train_time:36015ms step_avg:40.02ms
step:901/2330 train_time:36050ms step_avg:40.01ms
step:902/2330 train_time:36094ms step_avg:40.02ms
step:903/2330 train_time:36129ms step_avg:40.01ms
step:904/2330 train_time:36174ms step_avg:40.02ms
step:905/2330 train_time:36209ms step_avg:40.01ms
step:906/2330 train_time:36253ms step_avg:40.01ms
step:907/2330 train_time:36289ms step_avg:40.01ms
step:908/2330 train_time:36334ms step_avg:40.02ms
step:909/2330 train_time:36368ms step_avg:40.01ms
step:910/2330 train_time:36413ms step_avg:40.01ms
step:911/2330 train_time:36449ms step_avg:40.01ms
step:912/2330 train_time:36495ms step_avg:40.02ms
step:913/2330 train_time:36530ms step_avg:40.01ms
step:914/2330 train_time:36575ms step_avg:40.02ms
step:915/2330 train_time:36610ms step_avg:40.01ms
step:916/2330 train_time:36655ms step_avg:40.02ms
step:917/2330 train_time:36690ms step_avg:40.01ms
step:918/2330 train_time:36735ms step_avg:40.02ms
step:919/2330 train_time:36771ms step_avg:40.01ms
step:920/2330 train_time:36815ms step_avg:40.02ms
step:921/2330 train_time:36850ms step_avg:40.01ms
step:922/2330 train_time:36895ms step_avg:40.02ms
step:923/2330 train_time:36930ms step_avg:40.01ms
step:924/2330 train_time:36975ms step_avg:40.02ms
step:925/2330 train_time:37009ms step_avg:40.01ms
step:926/2330 train_time:37054ms step_avg:40.02ms
step:927/2330 train_time:37089ms step_avg:40.01ms
step:928/2330 train_time:37133ms step_avg:40.01ms
step:929/2330 train_time:37169ms step_avg:40.01ms
step:930/2330 train_time:37213ms step_avg:40.01ms
step:931/2330 train_time:37248ms step_avg:40.01ms
step:932/2330 train_time:37293ms step_avg:40.01ms
step:933/2330 train_time:37329ms step_avg:40.01ms
step:934/2330 train_time:37373ms step_avg:40.01ms
step:935/2330 train_time:37408ms step_avg:40.01ms
step:936/2330 train_time:37452ms step_avg:40.01ms
step:937/2330 train_time:37488ms step_avg:40.01ms
step:938/2330 train_time:37533ms step_avg:40.01ms
step:939/2330 train_time:37568ms step_avg:40.01ms
step:940/2330 train_time:37614ms step_avg:40.01ms
step:941/2330 train_time:37649ms step_avg:40.01ms
step:942/2330 train_time:37695ms step_avg:40.02ms
step:943/2330 train_time:37730ms step_avg:40.01ms
step:944/2330 train_time:37775ms step_avg:40.02ms
step:945/2330 train_time:37810ms step_avg:40.01ms
step:946/2330 train_time:37856ms step_avg:40.02ms
step:947/2330 train_time:37891ms step_avg:40.01ms
step:948/2330 train_time:37935ms step_avg:40.02ms
step:949/2330 train_time:37970ms step_avg:40.01ms
step:950/2330 train_time:38015ms step_avg:40.02ms
step:951/2330 train_time:38050ms step_avg:40.01ms
step:952/2330 train_time:38096ms step_avg:40.02ms
step:953/2330 train_time:38131ms step_avg:40.01ms
step:954/2330 train_time:38175ms step_avg:40.02ms
step:955/2330 train_time:38210ms step_avg:40.01ms
step:956/2330 train_time:38255ms step_avg:40.02ms
step:957/2330 train_time:38290ms step_avg:40.01ms
step:958/2330 train_time:38335ms step_avg:40.02ms
step:959/2330 train_time:38370ms step_avg:40.01ms
step:960/2330 train_time:38414ms step_avg:40.01ms
step:961/2330 train_time:38449ms step_avg:40.01ms
step:962/2330 train_time:38494ms step_avg:40.02ms
step:963/2330 train_time:38529ms step_avg:40.01ms
step:964/2330 train_time:38574ms step_avg:40.01ms
step:965/2330 train_time:38610ms step_avg:40.01ms
step:966/2330 train_time:38655ms step_avg:40.02ms
step:967/2330 train_time:38690ms step_avg:40.01ms
step:968/2330 train_time:38735ms step_avg:40.01ms
step:969/2330 train_time:38770ms step_avg:40.01ms
step:970/2330 train_time:38815ms step_avg:40.02ms
step:971/2330 train_time:38851ms step_avg:40.01ms
step:972/2330 train_time:38895ms step_avg:40.02ms
step:973/2330 train_time:38930ms step_avg:40.01ms
step:974/2330 train_time:38975ms step_avg:40.02ms
step:975/2330 train_time:39011ms step_avg:40.01ms
step:976/2330 train_time:39055ms step_avg:40.02ms
step:977/2330 train_time:39090ms step_avg:40.01ms
step:978/2330 train_time:39134ms step_avg:40.01ms
step:979/2330 train_time:39169ms step_avg:40.01ms
step:980/2330 train_time:39213ms step_avg:40.01ms
step:981/2330 train_time:39248ms step_avg:40.01ms
step:982/2330 train_time:39294ms step_avg:40.01ms
step:983/2330 train_time:39328ms step_avg:40.01ms
step:984/2330 train_time:39373ms step_avg:40.01ms
step:985/2330 train_time:39409ms step_avg:40.01ms
step:986/2330 train_time:39454ms step_avg:40.01ms
step:987/2330 train_time:39489ms step_avg:40.01ms
step:988/2330 train_time:39534ms step_avg:40.01ms
step:989/2330 train_time:39569ms step_avg:40.01ms
step:990/2330 train_time:39613ms step_avg:40.01ms
step:991/2330 train_time:39649ms step_avg:40.01ms
step:992/2330 train_time:39694ms step_avg:40.01ms
step:993/2330 train_time:39730ms step_avg:40.01ms
step:994/2330 train_time:39774ms step_avg:40.01ms
step:995/2330 train_time:39809ms step_avg:40.01ms
step:996/2330 train_time:39855ms step_avg:40.01ms
step:997/2330 train_time:39890ms step_avg:40.01ms
step:998/2330 train_time:39935ms step_avg:40.02ms
step:999/2330 train_time:39970ms step_avg:40.01ms
step:1000/2330 train_time:40015ms step_avg:40.01ms
step:1000/2330 val_loss:5.2127 train_time:40102ms step_avg:40.10ms
step:1001/2330 train_time:40116ms step_avg:40.08ms
step:1002/2330 train_time:40129ms step_avg:40.05ms
step:1003/2330 train_time:40140ms step_avg:40.02ms
step:1004/2330 train_time:40175ms step_avg:40.01ms
step:1005/2330 train_time:40209ms step_avg:40.01ms
step:1006/2330 train_time:40253ms step_avg:40.01ms
step:1007/2330 train_time:40287ms step_avg:40.01ms
step:1008/2330 train_time:40331ms step_avg:40.01ms
step:1009/2330 train_time:40365ms step_avg:40.01ms
step:1010/2330 train_time:40413ms step_avg:40.01ms
step:1011/2330 train_time:40451ms step_avg:40.01ms
step:1012/2330 train_time:40498ms step_avg:40.02ms
step:1013/2330 train_time:40535ms step_avg:40.01ms
step:1014/2330 train_time:40580ms step_avg:40.02ms
step:1015/2330 train_time:40615ms step_avg:40.02ms
step:1016/2330 train_time:40659ms step_avg:40.02ms
step:1017/2330 train_time:40695ms step_avg:40.01ms
step:1018/2330 train_time:40739ms step_avg:40.02ms
step:1019/2330 train_time:40774ms step_avg:40.01ms
step:1020/2330 train_time:40818ms step_avg:40.02ms
step:1021/2330 train_time:40853ms step_avg:40.01ms
step:1022/2330 train_time:40896ms step_avg:40.02ms
step:1023/2330 train_time:40931ms step_avg:40.01ms
step:1024/2330 train_time:40975ms step_avg:40.02ms
step:1025/2330 train_time:41012ms step_avg:40.01ms
step:1026/2330 train_time:41058ms step_avg:40.02ms
step:1027/2330 train_time:41093ms step_avg:40.01ms
step:1028/2330 train_time:41137ms step_avg:40.02ms
step:1029/2330 train_time:41172ms step_avg:40.01ms
step:1030/2330 train_time:41215ms step_avg:40.01ms
step:1031/2330 train_time:41250ms step_avg:40.01ms
step:1032/2330 train_time:41294ms step_avg:40.01ms
step:1033/2330 train_time:41330ms step_avg:40.01ms
step:1034/2330 train_time:41375ms step_avg:40.01ms
step:1035/2330 train_time:41411ms step_avg:40.01ms
step:1036/2330 train_time:41457ms step_avg:40.02ms
step:1037/2330 train_time:41492ms step_avg:40.01ms
step:1038/2330 train_time:41538ms step_avg:40.02ms
step:1039/2330 train_time:41574ms step_avg:40.01ms
step:1040/2330 train_time:41619ms step_avg:40.02ms
step:1041/2330 train_time:41654ms step_avg:40.01ms
step:1042/2330 train_time:41698ms step_avg:40.02ms
step:1043/2330 train_time:41733ms step_avg:40.01ms
step:1044/2330 train_time:41777ms step_avg:40.02ms
step:1045/2330 train_time:41812ms step_avg:40.01ms
step:1046/2330 train_time:41856ms step_avg:40.02ms
step:1047/2330 train_time:41891ms step_avg:40.01ms
step:1048/2330 train_time:41935ms step_avg:40.01ms
step:1049/2330 train_time:41970ms step_avg:40.01ms
step:1050/2330 train_time:42015ms step_avg:40.01ms
step:1051/2330 train_time:42049ms step_avg:40.01ms
step:1052/2330 train_time:42094ms step_avg:40.01ms
step:1053/2330 train_time:42129ms step_avg:40.01ms
step:1054/2330 train_time:42173ms step_avg:40.01ms
step:1055/2330 train_time:42208ms step_avg:40.01ms
step:1056/2330 train_time:42253ms step_avg:40.01ms
step:1057/2330 train_time:42288ms step_avg:40.01ms
step:1058/2330 train_time:42333ms step_avg:40.01ms
step:1059/2330 train_time:42369ms step_avg:40.01ms
step:1060/2330 train_time:42414ms step_avg:40.01ms
step:1061/2330 train_time:42450ms step_avg:40.01ms
step:1062/2330 train_time:42495ms step_avg:40.01ms
step:1063/2330 train_time:42531ms step_avg:40.01ms
step:1064/2330 train_time:42576ms step_avg:40.02ms
step:1065/2330 train_time:42612ms step_avg:40.01ms
step:1066/2330 train_time:42657ms step_avg:40.02ms
step:1067/2330 train_time:42692ms step_avg:40.01ms
step:1068/2330 train_time:42737ms step_avg:40.02ms
step:1069/2330 train_time:42772ms step_avg:40.01ms
step:1070/2330 train_time:42816ms step_avg:40.02ms
step:1071/2330 train_time:42852ms step_avg:40.01ms
step:1072/2330 train_time:42896ms step_avg:40.02ms
step:1073/2330 train_time:42932ms step_avg:40.01ms
step:1074/2330 train_time:42976ms step_avg:40.01ms
step:1075/2330 train_time:43011ms step_avg:40.01ms
step:1076/2330 train_time:43055ms step_avg:40.01ms
step:1077/2330 train_time:43089ms step_avg:40.01ms
step:1078/2330 train_time:43134ms step_avg:40.01ms
step:1079/2330 train_time:43168ms step_avg:40.01ms
step:1080/2330 train_time:43213ms step_avg:40.01ms
step:1081/2330 train_time:43249ms step_avg:40.01ms
step:1082/2330 train_time:43294ms step_avg:40.01ms
step:1083/2330 train_time:43329ms step_avg:40.01ms
step:1084/2330 train_time:43374ms step_avg:40.01ms
step:1085/2330 train_time:43410ms step_avg:40.01ms
step:1086/2330 train_time:43455ms step_avg:40.01ms
step:1087/2330 train_time:43490ms step_avg:40.01ms
step:1088/2330 train_time:43536ms step_avg:40.01ms
step:1089/2330 train_time:43571ms step_avg:40.01ms
step:1090/2330 train_time:43616ms step_avg:40.01ms
step:1091/2330 train_time:43653ms step_avg:40.01ms
step:1092/2330 train_time:43697ms step_avg:40.02ms
step:1093/2330 train_time:43732ms step_avg:40.01ms
step:1094/2330 train_time:43777ms step_avg:40.02ms
step:1095/2330 train_time:43812ms step_avg:40.01ms
step:1096/2330 train_time:43858ms step_avg:40.02ms
step:1097/2330 train_time:43893ms step_avg:40.01ms
step:1098/2330 train_time:43937ms step_avg:40.02ms
step:1099/2330 train_time:43972ms step_avg:40.01ms
step:1100/2330 train_time:44016ms step_avg:40.01ms
step:1101/2330 train_time:44051ms step_avg:40.01ms
step:1102/2330 train_time:44095ms step_avg:40.01ms
step:1103/2330 train_time:44130ms step_avg:40.01ms
step:1104/2330 train_time:44174ms step_avg:40.01ms
step:1105/2330 train_time:44209ms step_avg:40.01ms
step:1106/2330 train_time:44254ms step_avg:40.01ms
step:1107/2330 train_time:44289ms step_avg:40.01ms
step:1108/2330 train_time:44334ms step_avg:40.01ms
step:1109/2330 train_time:44369ms step_avg:40.01ms
step:1110/2330 train_time:44414ms step_avg:40.01ms
step:1111/2330 train_time:44450ms step_avg:40.01ms
step:1112/2330 train_time:44495ms step_avg:40.01ms
step:1113/2330 train_time:44530ms step_avg:40.01ms
step:1114/2330 train_time:44575ms step_avg:40.01ms
step:1115/2330 train_time:44610ms step_avg:40.01ms
step:1116/2330 train_time:44655ms step_avg:40.01ms
step:1117/2330 train_time:44691ms step_avg:40.01ms
step:1118/2330 train_time:44735ms step_avg:40.01ms
step:1119/2330 train_time:44771ms step_avg:40.01ms
step:1120/2330 train_time:44816ms step_avg:40.01ms
step:1121/2330 train_time:44850ms step_avg:40.01ms
step:1122/2330 train_time:44895ms step_avg:40.01ms
step:1123/2330 train_time:44930ms step_avg:40.01ms
step:1124/2330 train_time:44974ms step_avg:40.01ms
step:1125/2330 train_time:45009ms step_avg:40.01ms
step:1126/2330 train_time:45054ms step_avg:40.01ms
step:1127/2330 train_time:45088ms step_avg:40.01ms
step:1128/2330 train_time:45132ms step_avg:40.01ms
step:1129/2330 train_time:45167ms step_avg:40.01ms
step:1130/2330 train_time:45212ms step_avg:40.01ms
step:1131/2330 train_time:45247ms step_avg:40.01ms
step:1132/2330 train_time:45292ms step_avg:40.01ms
step:1133/2330 train_time:45327ms step_avg:40.01ms
step:1134/2330 train_time:45372ms step_avg:40.01ms
step:1135/2330 train_time:45408ms step_avg:40.01ms
step:1136/2330 train_time:45454ms step_avg:40.01ms
step:1137/2330 train_time:45489ms step_avg:40.01ms
step:1138/2330 train_time:45534ms step_avg:40.01ms
step:1139/2330 train_time:45569ms step_avg:40.01ms
step:1140/2330 train_time:45615ms step_avg:40.01ms
step:1141/2330 train_time:45650ms step_avg:40.01ms
step:1142/2330 train_time:45694ms step_avg:40.01ms
step:1143/2330 train_time:45730ms step_avg:40.01ms
step:1144/2330 train_time:45774ms step_avg:40.01ms
step:1145/2330 train_time:45810ms step_avg:40.01ms
step:1146/2330 train_time:45854ms step_avg:40.01ms
step:1147/2330 train_time:45890ms step_avg:40.01ms
step:1148/2330 train_time:45935ms step_avg:40.01ms
step:1149/2330 train_time:45969ms step_avg:40.01ms
step:1150/2330 train_time:46014ms step_avg:40.01ms
step:1151/2330 train_time:46049ms step_avg:40.01ms
step:1152/2330 train_time:46094ms step_avg:40.01ms
step:1153/2330 train_time:46128ms step_avg:40.01ms
step:1154/2330 train_time:46173ms step_avg:40.01ms
step:1155/2330 train_time:46208ms step_avg:40.01ms
step:1156/2330 train_time:46252ms step_avg:40.01ms
step:1157/2330 train_time:46288ms step_avg:40.01ms
step:1158/2330 train_time:46332ms step_avg:40.01ms
step:1159/2330 train_time:46368ms step_avg:40.01ms
step:1160/2330 train_time:46413ms step_avg:40.01ms
step:1161/2330 train_time:46449ms step_avg:40.01ms
step:1162/2330 train_time:46494ms step_avg:40.01ms
step:1163/2330 train_time:46529ms step_avg:40.01ms
step:1164/2330 train_time:46574ms step_avg:40.01ms
step:1165/2330 train_time:46610ms step_avg:40.01ms
step:1166/2330 train_time:46654ms step_avg:40.01ms
step:1167/2330 train_time:46690ms step_avg:40.01ms
step:1168/2330 train_time:46735ms step_avg:40.01ms
step:1169/2330 train_time:46770ms step_avg:40.01ms
step:1170/2330 train_time:46816ms step_avg:40.01ms
step:1171/2330 train_time:46851ms step_avg:40.01ms
step:1172/2330 train_time:46895ms step_avg:40.01ms
step:1173/2330 train_time:46931ms step_avg:40.01ms
step:1174/2330 train_time:46974ms step_avg:40.01ms
step:1175/2330 train_time:47010ms step_avg:40.01ms
step:1176/2330 train_time:47055ms step_avg:40.01ms
step:1177/2330 train_time:47090ms step_avg:40.01ms
step:1178/2330 train_time:47134ms step_avg:40.01ms
step:1179/2330 train_time:47169ms step_avg:40.01ms
step:1180/2330 train_time:47213ms step_avg:40.01ms
step:1181/2330 train_time:47247ms step_avg:40.01ms
step:1182/2330 train_time:47293ms step_avg:40.01ms
step:1183/2330 train_time:47328ms step_avg:40.01ms
step:1184/2330 train_time:47373ms step_avg:40.01ms
step:1185/2330 train_time:47408ms step_avg:40.01ms
step:1186/2330 train_time:47453ms step_avg:40.01ms
step:1187/2330 train_time:47488ms step_avg:40.01ms
step:1188/2330 train_time:47534ms step_avg:40.01ms
step:1189/2330 train_time:47569ms step_avg:40.01ms
step:1190/2330 train_time:47614ms step_avg:40.01ms
step:1191/2330 train_time:47650ms step_avg:40.01ms
step:1192/2330 train_time:47694ms step_avg:40.01ms
step:1193/2330 train_time:47730ms step_avg:40.01ms
step:1194/2330 train_time:47775ms step_avg:40.01ms
step:1195/2330 train_time:47811ms step_avg:40.01ms
step:1196/2330 train_time:47856ms step_avg:40.01ms
step:1197/2330 train_time:47890ms step_avg:40.01ms
step:1198/2330 train_time:47934ms step_avg:40.01ms
step:1199/2330 train_time:47969ms step_avg:40.01ms
step:1200/2330 train_time:48014ms step_avg:40.01ms
step:1201/2330 train_time:48049ms step_avg:40.01ms
step:1202/2330 train_time:48094ms step_avg:40.01ms
step:1203/2330 train_time:48128ms step_avg:40.01ms
step:1204/2330 train_time:48173ms step_avg:40.01ms
step:1205/2330 train_time:48208ms step_avg:40.01ms
step:1206/2330 train_time:48252ms step_avg:40.01ms
step:1207/2330 train_time:48287ms step_avg:40.01ms
step:1208/2330 train_time:48332ms step_avg:40.01ms
step:1209/2330 train_time:48368ms step_avg:40.01ms
step:1210/2330 train_time:48413ms step_avg:40.01ms
step:1211/2330 train_time:48448ms step_avg:40.01ms
step:1212/2330 train_time:48493ms step_avg:40.01ms
step:1213/2330 train_time:48529ms step_avg:40.01ms
step:1214/2330 train_time:48573ms step_avg:40.01ms
step:1215/2330 train_time:48609ms step_avg:40.01ms
step:1216/2330 train_time:48654ms step_avg:40.01ms
step:1217/2330 train_time:48689ms step_avg:40.01ms
step:1218/2330 train_time:48734ms step_avg:40.01ms
step:1219/2330 train_time:48770ms step_avg:40.01ms
step:1220/2330 train_time:48815ms step_avg:40.01ms
step:1221/2330 train_time:48851ms step_avg:40.01ms
step:1222/2330 train_time:48895ms step_avg:40.01ms
step:1223/2330 train_time:48930ms step_avg:40.01ms
step:1224/2330 train_time:48974ms step_avg:40.01ms
step:1225/2330 train_time:49009ms step_avg:40.01ms
step:1226/2330 train_time:49053ms step_avg:40.01ms
step:1227/2330 train_time:49088ms step_avg:40.01ms
step:1228/2330 train_time:49133ms step_avg:40.01ms
step:1229/2330 train_time:49168ms step_avg:40.01ms
step:1230/2330 train_time:49212ms step_avg:40.01ms
step:1231/2330 train_time:49247ms step_avg:40.01ms
step:1232/2330 train_time:49292ms step_avg:40.01ms
step:1233/2330 train_time:49328ms step_avg:40.01ms
step:1234/2330 train_time:49373ms step_avg:40.01ms
step:1235/2330 train_time:49407ms step_avg:40.01ms
step:1236/2330 train_time:49452ms step_avg:40.01ms
step:1237/2330 train_time:49488ms step_avg:40.01ms
step:1238/2330 train_time:49532ms step_avg:40.01ms
step:1239/2330 train_time:49568ms step_avg:40.01ms
step:1240/2330 train_time:49613ms step_avg:40.01ms
step:1241/2330 train_time:49648ms step_avg:40.01ms
step:1242/2330 train_time:49693ms step_avg:40.01ms
step:1243/2330 train_time:49728ms step_avg:40.01ms
step:1244/2330 train_time:49774ms step_avg:40.01ms
step:1245/2330 train_time:49810ms step_avg:40.01ms
step:1246/2330 train_time:49855ms step_avg:40.01ms
step:1247/2330 train_time:49890ms step_avg:40.01ms
step:1248/2330 train_time:49935ms step_avg:40.01ms
step:1249/2330 train_time:49969ms step_avg:40.01ms
step:1250/2330 train_time:50014ms step_avg:40.01ms
step:1250/2330 val_loss:5.1878 train_time:50100ms step_avg:40.08ms
step:1251/2330 train_time:50114ms step_avg:40.06ms
step:1252/2330 train_time:50126ms step_avg:40.04ms
step:1253/2330 train_time:50137ms step_avg:40.01ms
step:1254/2330 train_time:50174ms step_avg:40.01ms
step:1255/2330 train_time:50207ms step_avg:40.01ms
step:1256/2330 train_time:50251ms step_avg:40.01ms
step:1257/2330 train_time:50285ms step_avg:40.00ms
step:1258/2330 train_time:50329ms step_avg:40.01ms
step:1259/2330 train_time:50363ms step_avg:40.00ms
step:1260/2330 train_time:50407ms step_avg:40.01ms
step:1261/2330 train_time:50445ms step_avg:40.00ms
step:1262/2330 train_time:50492ms step_avg:40.01ms
step:1263/2330 train_time:50530ms step_avg:40.01ms
step:1264/2330 train_time:50574ms step_avg:40.01ms
step:1265/2330 train_time:50609ms step_avg:40.01ms
step:1266/2330 train_time:50653ms step_avg:40.01ms
step:1267/2330 train_time:50688ms step_avg:40.01ms
step:1268/2330 train_time:50732ms step_avg:40.01ms
step:1269/2330 train_time:50899ms step_avg:40.11ms
step:1270/2330 train_time:50941ms step_avg:40.11ms
step:1271/2330 train_time:51064ms step_avg:40.18ms
step:1272/2330 train_time:51106ms step_avg:40.18ms
step:1273/2330 train_time:51140ms step_avg:40.17ms
step:1274/2330 train_time:51185ms step_avg:40.18ms
step:1275/2330 train_time:51219ms step_avg:40.17ms
step:1276/2330 train_time:51409ms step_avg:40.29ms
step:1277/2330 train_time:51442ms step_avg:40.28ms
step:1278/2330 train_time:51486ms step_avg:40.29ms
step:1279/2330 train_time:51520ms step_avg:40.28ms
step:1280/2330 train_time:51563ms step_avg:40.28ms
step:1281/2330 train_time:51598ms step_avg:40.28ms
step:1282/2330 train_time:51642ms step_avg:40.28ms
step:1283/2330 train_time:51676ms step_avg:40.28ms
step:1284/2330 train_time:51720ms step_avg:40.28ms
step:1285/2330 train_time:51755ms step_avg:40.28ms
step:1286/2330 train_time:51799ms step_avg:40.28ms
step:1287/2330 train_time:51833ms step_avg:40.27ms
step:1288/2330 train_time:51877ms step_avg:40.28ms
step:1289/2330 train_time:51911ms step_avg:40.27ms
step:1290/2330 train_time:51955ms step_avg:40.28ms
step:1291/2330 train_time:51990ms step_avg:40.27ms
step:1292/2330 train_time:52034ms step_avg:40.27ms
step:1293/2330 train_time:52068ms step_avg:40.27ms
step:1294/2330 train_time:52112ms step_avg:40.27ms
step:1295/2330 train_time:52147ms step_avg:40.27ms
step:1296/2330 train_time:52191ms step_avg:40.27ms
step:1297/2330 train_time:52225ms step_avg:40.27ms
step:1298/2330 train_time:52275ms step_avg:40.27ms
step:1299/2330 train_time:52314ms step_avg:40.27ms
step:1300/2330 train_time:52362ms step_avg:40.28ms
step:1301/2330 train_time:52400ms step_avg:40.28ms
step:1302/2330 train_time:52446ms step_avg:40.28ms
step:1303/2330 train_time:52482ms step_avg:40.28ms
step:1304/2330 train_time:52527ms step_avg:40.28ms
step:1305/2330 train_time:52563ms step_avg:40.28ms
step:1306/2330 train_time:52607ms step_avg:40.28ms
step:1307/2330 train_time:52641ms step_avg:40.28ms
step:1308/2330 train_time:52686ms step_avg:40.28ms
step:1309/2330 train_time:52721ms step_avg:40.28ms
step:1310/2330 train_time:52765ms step_avg:40.28ms
step:1311/2330 train_time:52800ms step_avg:40.27ms
step:1312/2330 train_time:52845ms step_avg:40.28ms
step:1313/2330 train_time:52879ms step_avg:40.27ms
step:1314/2330 train_time:52924ms step_avg:40.28ms
step:1315/2330 train_time:52959ms step_avg:40.27ms
step:1316/2330 train_time:53003ms step_avg:40.28ms
step:1317/2330 train_time:53038ms step_avg:40.27ms
step:1318/2330 train_time:53082ms step_avg:40.27ms
step:1319/2330 train_time:53116ms step_avg:40.27ms
step:1320/2330 train_time:53161ms step_avg:40.27ms
step:1321/2330 train_time:53197ms step_avg:40.27ms
step:1322/2330 train_time:53243ms step_avg:40.27ms
step:1323/2330 train_time:53279ms step_avg:40.27ms
step:1324/2330 train_time:53326ms step_avg:40.28ms
step:1325/2330 train_time:53364ms step_avg:40.27ms
step:1326/2330 train_time:53409ms step_avg:40.28ms
step:1327/2330 train_time:53444ms step_avg:40.27ms
step:1328/2330 train_time:53489ms step_avg:40.28ms
step:1329/2330 train_time:53524ms step_avg:40.27ms
step:1330/2330 train_time:53569ms step_avg:40.28ms
step:1331/2330 train_time:53604ms step_avg:40.27ms
step:1332/2330 train_time:53648ms step_avg:40.28ms
step:1333/2330 train_time:53683ms step_avg:40.27ms
step:1334/2330 train_time:53727ms step_avg:40.28ms
step:1335/2330 train_time:53762ms step_avg:40.27ms
step:1336/2330 train_time:53807ms step_avg:40.27ms
step:1337/2330 train_time:53842ms step_avg:40.27ms
step:1338/2330 train_time:53886ms step_avg:40.27ms
step:1339/2330 train_time:53921ms step_avg:40.27ms
step:1340/2330 train_time:53966ms step_avg:40.27ms
step:1341/2330 train_time:54000ms step_avg:40.27ms
step:1342/2330 train_time:54046ms step_avg:40.27ms
step:1343/2330 train_time:54081ms step_avg:40.27ms
step:1344/2330 train_time:54125ms step_avg:40.27ms
step:1345/2330 train_time:54162ms step_avg:40.27ms
step:1346/2330 train_time:54207ms step_avg:40.27ms
step:1347/2330 train_time:54243ms step_avg:40.27ms
step:1348/2330 train_time:54289ms step_avg:40.27ms
step:1349/2330 train_time:54324ms step_avg:40.27ms
step:1350/2330 train_time:54369ms step_avg:40.27ms
step:1351/2330 train_time:54405ms step_avg:40.27ms
step:1352/2330 train_time:54449ms step_avg:40.27ms
step:1353/2330 train_time:54484ms step_avg:40.27ms
step:1354/2330 train_time:54529ms step_avg:40.27ms
step:1355/2330 train_time:54565ms step_avg:40.27ms
step:1356/2330 train_time:54609ms step_avg:40.27ms
step:1357/2330 train_time:54645ms step_avg:40.27ms
step:1358/2330 train_time:54689ms step_avg:40.27ms
step:1359/2330 train_time:54724ms step_avg:40.27ms
step:1360/2330 train_time:54769ms step_avg:40.27ms
step:1361/2330 train_time:54803ms step_avg:40.27ms
step:1362/2330 train_time:54847ms step_avg:40.27ms
step:1363/2330 train_time:54882ms step_avg:40.27ms
step:1364/2330 train_time:54927ms step_avg:40.27ms
step:1365/2330 train_time:54962ms step_avg:40.26ms
step:1366/2330 train_time:55006ms step_avg:40.27ms
step:1367/2330 train_time:55041ms step_avg:40.26ms
step:1368/2330 train_time:55086ms step_avg:40.27ms
step:1369/2330 train_time:55121ms step_avg:40.26ms
step:1370/2330 train_time:55166ms step_avg:40.27ms
step:1371/2330 train_time:55202ms step_avg:40.26ms
step:1372/2330 train_time:55247ms step_avg:40.27ms
step:1373/2330 train_time:55283ms step_avg:40.26ms
step:1374/2330 train_time:55329ms step_avg:40.27ms
step:1375/2330 train_time:55365ms step_avg:40.27ms
step:1376/2330 train_time:55410ms step_avg:40.27ms
step:1377/2330 train_time:55445ms step_avg:40.27ms
step:1378/2330 train_time:55490ms step_avg:40.27ms
step:1379/2330 train_time:55525ms step_avg:40.26ms
step:1380/2330 train_time:55570ms step_avg:40.27ms
step:1381/2330 train_time:55606ms step_avg:40.26ms
step:1382/2330 train_time:55649ms step_avg:40.27ms
step:1383/2330 train_time:55684ms step_avg:40.26ms
step:1384/2330 train_time:55729ms step_avg:40.27ms
step:1385/2330 train_time:55763ms step_avg:40.26ms
step:1386/2330 train_time:55808ms step_avg:40.27ms
step:1387/2330 train_time:55843ms step_avg:40.26ms
step:1388/2330 train_time:55888ms step_avg:40.26ms
step:1389/2330 train_time:55922ms step_avg:40.26ms
step:1390/2330 train_time:55966ms step_avg:40.26ms
step:1391/2330 train_time:56002ms step_avg:40.26ms
step:1392/2330 train_time:56046ms step_avg:40.26ms
step:1393/2330 train_time:56082ms step_avg:40.26ms
step:1394/2330 train_time:56127ms step_avg:40.26ms
step:1395/2330 train_time:56162ms step_avg:40.26ms
step:1396/2330 train_time:56208ms step_avg:40.26ms
step:1397/2330 train_time:56243ms step_avg:40.26ms
step:1398/2330 train_time:56289ms step_avg:40.26ms
step:1399/2330 train_time:56324ms step_avg:40.26ms
step:1400/2330 train_time:56370ms step_avg:40.26ms
step:1401/2330 train_time:56406ms step_avg:40.26ms
step:1402/2330 train_time:56450ms step_avg:40.26ms
step:1403/2330 train_time:56485ms step_avg:40.26ms
step:1404/2330 train_time:56529ms step_avg:40.26ms
step:1405/2330 train_time:56564ms step_avg:40.26ms
step:1406/2330 train_time:56610ms step_avg:40.26ms
step:1407/2330 train_time:56644ms step_avg:40.26ms
step:1408/2330 train_time:56689ms step_avg:40.26ms
step:1409/2330 train_time:56723ms step_avg:40.26ms
step:1410/2330 train_time:56767ms step_avg:40.26ms
step:1411/2330 train_time:56803ms step_avg:40.26ms
step:1412/2330 train_time:56847ms step_avg:40.26ms
step:1413/2330 train_time:56882ms step_avg:40.26ms
step:1414/2330 train_time:56926ms step_avg:40.26ms
step:1415/2330 train_time:56961ms step_avg:40.26ms
step:1416/2330 train_time:57005ms step_avg:40.26ms
step:1417/2330 train_time:57041ms step_avg:40.25ms
step:1418/2330 train_time:57085ms step_avg:40.26ms
step:1419/2330 train_time:57120ms step_avg:40.25ms
step:1420/2330 train_time:57165ms step_avg:40.26ms
step:1421/2330 train_time:57201ms step_avg:40.25ms
step:1422/2330 train_time:57245ms step_avg:40.26ms
step:1423/2330 train_time:57281ms step_avg:40.25ms
step:1424/2330 train_time:57326ms step_avg:40.26ms
step:1425/2330 train_time:57363ms step_avg:40.25ms
step:1426/2330 train_time:57407ms step_avg:40.26ms
step:1427/2330 train_time:57443ms step_avg:40.25ms
step:1428/2330 train_time:57488ms step_avg:40.26ms
step:1429/2330 train_time:57523ms step_avg:40.25ms
step:1430/2330 train_time:57569ms step_avg:40.26ms
step:1431/2330 train_time:57604ms step_avg:40.25ms
step:1432/2330 train_time:57648ms step_avg:40.26ms
step:1433/2330 train_time:57683ms step_avg:40.25ms
step:1434/2330 train_time:57728ms step_avg:40.26ms
step:1435/2330 train_time:57762ms step_avg:40.25ms
step:1436/2330 train_time:57807ms step_avg:40.26ms
step:1437/2330 train_time:57841ms step_avg:40.25ms
step:1438/2330 train_time:57886ms step_avg:40.25ms
step:1439/2330 train_time:57921ms step_avg:40.25ms
step:1440/2330 train_time:57965ms step_avg:40.25ms
step:1441/2330 train_time:58000ms step_avg:40.25ms
step:1442/2330 train_time:58045ms step_avg:40.25ms
step:1443/2330 train_time:58081ms step_avg:40.25ms
step:1444/2330 train_time:58126ms step_avg:40.25ms
step:1445/2330 train_time:58161ms step_avg:40.25ms
step:1446/2330 train_time:58206ms step_avg:40.25ms
step:1447/2330 train_time:58241ms step_avg:40.25ms
step:1448/2330 train_time:58286ms step_avg:40.25ms
step:1449/2330 train_time:58323ms step_avg:40.25ms
step:1450/2330 train_time:58368ms step_avg:40.25ms
step:1451/2330 train_time:58403ms step_avg:40.25ms
step:1452/2330 train_time:58448ms step_avg:40.25ms
step:1453/2330 train_time:58484ms step_avg:40.25ms
step:1454/2330 train_time:58529ms step_avg:40.25ms
step:1455/2330 train_time:58565ms step_avg:40.25ms
step:1456/2330 train_time:58609ms step_avg:40.25ms
step:1457/2330 train_time:58644ms step_avg:40.25ms
step:1458/2330 train_time:58688ms step_avg:40.25ms
step:1459/2330 train_time:58723ms step_avg:40.25ms
step:1460/2330 train_time:58768ms step_avg:40.25ms
step:1461/2330 train_time:58804ms step_avg:40.25ms
step:1462/2330 train_time:58847ms step_avg:40.25ms
step:1463/2330 train_time:58882ms step_avg:40.25ms
step:1464/2330 train_time:58927ms step_avg:40.25ms
step:1465/2330 train_time:58962ms step_avg:40.25ms
step:1466/2330 train_time:59007ms step_avg:40.25ms
step:1467/2330 train_time:59042ms step_avg:40.25ms
step:1468/2330 train_time:59087ms step_avg:40.25ms
step:1469/2330 train_time:59122ms step_avg:40.25ms
step:1470/2330 train_time:59167ms step_avg:40.25ms
step:1471/2330 train_time:59202ms step_avg:40.25ms
step:1472/2330 train_time:59247ms step_avg:40.25ms
step:1473/2330 train_time:59282ms step_avg:40.25ms
step:1474/2330 train_time:59327ms step_avg:40.25ms
step:1475/2330 train_time:59363ms step_avg:40.25ms
step:1476/2330 train_time:59408ms step_avg:40.25ms
step:1477/2330 train_time:59443ms step_avg:40.25ms
step:1478/2330 train_time:59488ms step_avg:40.25ms
step:1479/2330 train_time:59524ms step_avg:40.25ms
step:1480/2330 train_time:59569ms step_avg:40.25ms
step:1481/2330 train_time:59604ms step_avg:40.25ms
step:1482/2330 train_time:59648ms step_avg:40.25ms
step:1483/2330 train_time:59684ms step_avg:40.25ms
step:1484/2330 train_time:59728ms step_avg:40.25ms
step:1485/2330 train_time:59763ms step_avg:40.24ms
step:1486/2330 train_time:59808ms step_avg:40.25ms
step:1487/2330 train_time:59842ms step_avg:40.24ms
step:1488/2330 train_time:59886ms step_avg:40.25ms
step:1489/2330 train_time:59922ms step_avg:40.24ms
step:1490/2330 train_time:59968ms step_avg:40.25ms
step:1491/2330 train_time:60003ms step_avg:40.24ms
step:1492/2330 train_time:60047ms step_avg:40.25ms
step:1493/2330 train_time:60082ms step_avg:40.24ms
step:1494/2330 train_time:60127ms step_avg:40.25ms
step:1495/2330 train_time:60162ms step_avg:40.24ms
step:1496/2330 train_time:60207ms step_avg:40.25ms
step:1497/2330 train_time:60243ms step_avg:40.24ms
step:1498/2330 train_time:60287ms step_avg:40.25ms
step:1499/2330 train_time:60323ms step_avg:40.24ms
step:1500/2330 train_time:60368ms step_avg:40.25ms
step:1500/2330 val_loss:5.1523 train_time:60455ms step_avg:40.30ms
step:1501/2330 train_time:60468ms step_avg:40.29ms
step:1502/2330 train_time:60481ms step_avg:40.27ms
step:1503/2330 train_time:60492ms step_avg:40.25ms
step:1504/2330 train_time:60528ms step_avg:40.24ms
step:1505/2330 train_time:60562ms step_avg:40.24ms
step:1506/2330 train_time:60606ms step_avg:40.24ms
step:1507/2330 train_time:60640ms step_avg:40.24ms
step:1508/2330 train_time:60683ms step_avg:40.24ms
step:1509/2330 train_time:60718ms step_avg:40.24ms
step:1510/2330 train_time:60763ms step_avg:40.24ms
step:1511/2330 train_time:60803ms step_avg:40.24ms
step:1512/2330 train_time:60851ms step_avg:40.25ms
step:1513/2330 train_time:60887ms step_avg:40.24ms
step:1514/2330 train_time:60932ms step_avg:40.25ms
step:1515/2330 train_time:60968ms step_avg:40.24ms
step:1516/2330 train_time:61011ms step_avg:40.24ms
step:1517/2330 train_time:61047ms step_avg:40.24ms
step:1518/2330 train_time:61091ms step_avg:40.24ms
step:1519/2330 train_time:61126ms step_avg:40.24ms
step:1520/2330 train_time:61170ms step_avg:40.24ms
step:1521/2330 train_time:61205ms step_avg:40.24ms
step:1522/2330 train_time:61249ms step_avg:40.24ms
step:1523/2330 train_time:61284ms step_avg:40.24ms
step:1524/2330 train_time:61329ms step_avg:40.24ms
step:1525/2330 train_time:61363ms step_avg:40.24ms
step:1526/2330 train_time:61408ms step_avg:40.24ms
step:1527/2330 train_time:61443ms step_avg:40.24ms
step:1528/2330 train_time:61487ms step_avg:40.24ms
step:1529/2330 train_time:61523ms step_avg:40.24ms
step:1530/2330 train_time:61566ms step_avg:40.24ms
step:1531/2330 train_time:61600ms step_avg:40.24ms
step:1532/2330 train_time:61644ms step_avg:40.24ms
step:1533/2330 train_time:61679ms step_avg:40.23ms
step:1534/2330 train_time:61724ms step_avg:40.24ms
step:1535/2330 train_time:61761ms step_avg:40.24ms
step:1536/2330 train_time:61806ms step_avg:40.24ms
step:1537/2330 train_time:61842ms step_avg:40.24ms
step:1538/2330 train_time:61887ms step_avg:40.24ms
step:1539/2330 train_time:61923ms step_avg:40.24ms
step:1540/2330 train_time:61968ms step_avg:40.24ms
step:1541/2330 train_time:62003ms step_avg:40.24ms
step:1542/2330 train_time:62047ms step_avg:40.24ms
step:1543/2330 train_time:62082ms step_avg:40.23ms
step:1544/2330 train_time:62126ms step_avg:40.24ms
step:1545/2330 train_time:62162ms step_avg:40.23ms
step:1546/2330 train_time:62205ms step_avg:40.24ms
step:1547/2330 train_time:62241ms step_avg:40.23ms
step:1548/2330 train_time:62285ms step_avg:40.24ms
step:1549/2330 train_time:62320ms step_avg:40.23ms
step:1550/2330 train_time:62365ms step_avg:40.24ms
step:1551/2330 train_time:62400ms step_avg:40.23ms
step:1552/2330 train_time:62444ms step_avg:40.23ms
step:1553/2330 train_time:62479ms step_avg:40.23ms
step:1554/2330 train_time:62523ms step_avg:40.23ms
step:1555/2330 train_time:62557ms step_avg:40.23ms
step:1556/2330 train_time:62602ms step_avg:40.23ms
step:1557/2330 train_time:62637ms step_avg:40.23ms
step:1558/2330 train_time:62682ms step_avg:40.23ms
step:1559/2330 train_time:62718ms step_avg:40.23ms
step:1560/2330 train_time:62763ms step_avg:40.23ms
step:1561/2330 train_time:62799ms step_avg:40.23ms
step:1562/2330 train_time:62844ms step_avg:40.23ms
step:1563/2330 train_time:62880ms step_avg:40.23ms
step:1564/2330 train_time:62926ms step_avg:40.23ms
step:1565/2330 train_time:62961ms step_avg:40.23ms
step:1566/2330 train_time:63005ms step_avg:40.23ms
step:1567/2330 train_time:63041ms step_avg:40.23ms
step:1568/2330 train_time:63085ms step_avg:40.23ms
step:1569/2330 train_time:63120ms step_avg:40.23ms
step:1570/2330 train_time:63166ms step_avg:40.23ms
step:1571/2330 train_time:63201ms step_avg:40.23ms
step:1572/2330 train_time:63245ms step_avg:40.23ms
step:1573/2330 train_time:63280ms step_avg:40.23ms
step:1574/2330 train_time:63325ms step_avg:40.23ms
step:1575/2330 train_time:63360ms step_avg:40.23ms
step:1576/2330 train_time:63404ms step_avg:40.23ms
step:1577/2330 train_time:63439ms step_avg:40.23ms
step:1578/2330 train_time:63483ms step_avg:40.23ms
step:1579/2330 train_time:63519ms step_avg:40.23ms
step:1580/2330 train_time:63563ms step_avg:40.23ms
step:1581/2330 train_time:63598ms step_avg:40.23ms
step:1582/2330 train_time:63642ms step_avg:40.23ms
step:1583/2330 train_time:63678ms step_avg:40.23ms
step:1584/2330 train_time:63723ms step_avg:40.23ms
step:1585/2330 train_time:63759ms step_avg:40.23ms
step:1586/2330 train_time:63803ms step_avg:40.23ms
step:1587/2330 train_time:63839ms step_avg:40.23ms
step:1588/2330 train_time:63885ms step_avg:40.23ms
step:1589/2330 train_time:63920ms step_avg:40.23ms
step:1590/2330 train_time:63965ms step_avg:40.23ms
step:1591/2330 train_time:64000ms step_avg:40.23ms
step:1592/2330 train_time:64045ms step_avg:40.23ms
step:1593/2330 train_time:64080ms step_avg:40.23ms
step:1594/2330 train_time:64125ms step_avg:40.23ms
step:1595/2330 train_time:64160ms step_avg:40.23ms
step:1596/2330 train_time:64204ms step_avg:40.23ms
step:1597/2330 train_time:64240ms step_avg:40.23ms
step:1598/2330 train_time:64285ms step_avg:40.23ms
step:1599/2330 train_time:64320ms step_avg:40.22ms
step:1600/2330 train_time:64364ms step_avg:40.23ms
step:1601/2330 train_time:64399ms step_avg:40.22ms
step:1602/2330 train_time:64443ms step_avg:40.23ms
step:1603/2330 train_time:64478ms step_avg:40.22ms
step:1604/2330 train_time:64522ms step_avg:40.23ms
step:1605/2330 train_time:64557ms step_avg:40.22ms
step:1606/2330 train_time:64602ms step_avg:40.23ms
step:1607/2330 train_time:64637ms step_avg:40.22ms
step:1608/2330 train_time:64681ms step_avg:40.22ms
step:1609/2330 train_time:64717ms step_avg:40.22ms
step:1610/2330 train_time:64763ms step_avg:40.23ms
step:1611/2330 train_time:64798ms step_avg:40.22ms
step:1612/2330 train_time:64843ms step_avg:40.23ms
step:1613/2330 train_time:64879ms step_avg:40.22ms
step:1614/2330 train_time:64925ms step_avg:40.23ms
step:1615/2330 train_time:64960ms step_avg:40.22ms
step:1616/2330 train_time:65006ms step_avg:40.23ms
step:1617/2330 train_time:65040ms step_avg:40.22ms
step:1618/2330 train_time:65085ms step_avg:40.23ms
step:1619/2330 train_time:65121ms step_avg:40.22ms
step:1620/2330 train_time:65166ms step_avg:40.23ms
step:1621/2330 train_time:65201ms step_avg:40.22ms
step:1622/2330 train_time:65245ms step_avg:40.22ms
step:1623/2330 train_time:65281ms step_avg:40.22ms
step:1624/2330 train_time:65325ms step_avg:40.22ms
step:1625/2330 train_time:65361ms step_avg:40.22ms
step:1626/2330 train_time:65405ms step_avg:40.22ms
step:1627/2330 train_time:65440ms step_avg:40.22ms
step:1628/2330 train_time:65484ms step_avg:40.22ms
step:1629/2330 train_time:65519ms step_avg:40.22ms
step:1630/2330 train_time:65564ms step_avg:40.22ms
step:1631/2330 train_time:65599ms step_avg:40.22ms
step:1632/2330 train_time:65644ms step_avg:40.22ms
step:1633/2330 train_time:65679ms step_avg:40.22ms
step:1634/2330 train_time:65724ms step_avg:40.22ms
step:1635/2330 train_time:65759ms step_avg:40.22ms
step:1636/2330 train_time:65804ms step_avg:40.22ms
step:1637/2330 train_time:65839ms step_avg:40.22ms
step:1638/2330 train_time:65884ms step_avg:40.22ms
step:1639/2330 train_time:65920ms step_avg:40.22ms
step:1640/2330 train_time:65964ms step_avg:40.22ms
step:1641/2330 train_time:66000ms step_avg:40.22ms
step:1642/2330 train_time:66045ms step_avg:40.22ms
step:1643/2330 train_time:66080ms step_avg:40.22ms
step:1644/2330 train_time:66126ms step_avg:40.22ms
step:1645/2330 train_time:66161ms step_avg:40.22ms
step:1646/2330 train_time:66206ms step_avg:40.22ms
step:1647/2330 train_time:66240ms step_avg:40.22ms
step:1648/2330 train_time:66285ms step_avg:40.22ms
step:1649/2330 train_time:66320ms step_avg:40.22ms
step:1650/2330 train_time:66364ms step_avg:40.22ms
step:1651/2330 train_time:66399ms step_avg:40.22ms
step:1652/2330 train_time:66443ms step_avg:40.22ms
step:1653/2330 train_time:66479ms step_avg:40.22ms
step:1654/2330 train_time:66523ms step_avg:40.22ms
step:1655/2330 train_time:66558ms step_avg:40.22ms
step:1656/2330 train_time:66603ms step_avg:40.22ms
step:1657/2330 train_time:66638ms step_avg:40.22ms
step:1658/2330 train_time:66682ms step_avg:40.22ms
step:1659/2330 train_time:66718ms step_avg:40.22ms
step:1660/2330 train_time:66763ms step_avg:40.22ms
step:1661/2330 train_time:66798ms step_avg:40.22ms
step:1662/2330 train_time:66843ms step_avg:40.22ms
step:1663/2330 train_time:66879ms step_avg:40.22ms
step:1664/2330 train_time:66924ms step_avg:40.22ms
step:1665/2330 train_time:66959ms step_avg:40.22ms
step:1666/2330 train_time:67004ms step_avg:40.22ms
step:1667/2330 train_time:67039ms step_avg:40.22ms
step:1668/2330 train_time:67084ms step_avg:40.22ms
step:1669/2330 train_time:67120ms step_avg:40.22ms
step:1670/2330 train_time:67165ms step_avg:40.22ms
step:1671/2330 train_time:67200ms step_avg:40.22ms
step:1672/2330 train_time:67244ms step_avg:40.22ms
step:1673/2330 train_time:67279ms step_avg:40.21ms
step:1674/2330 train_time:67324ms step_avg:40.22ms
step:1675/2330 train_time:67359ms step_avg:40.21ms
step:1676/2330 train_time:67404ms step_avg:40.22ms
step:1677/2330 train_time:67439ms step_avg:40.21ms
step:1678/2330 train_time:67483ms step_avg:40.22ms
step:1679/2330 train_time:67518ms step_avg:40.21ms
step:1680/2330 train_time:67563ms step_avg:40.22ms
step:1681/2330 train_time:67598ms step_avg:40.21ms
step:1682/2330 train_time:67642ms step_avg:40.22ms
step:1683/2330 train_time:67677ms step_avg:40.21ms
step:1684/2330 train_time:67721ms step_avg:40.21ms
step:1685/2330 train_time:67757ms step_avg:40.21ms
step:1686/2330 train_time:67801ms step_avg:40.21ms
step:1687/2330 train_time:67838ms step_avg:40.21ms
step:1688/2330 train_time:67883ms step_avg:40.21ms
step:1689/2330 train_time:67918ms step_avg:40.21ms
step:1690/2330 train_time:67963ms step_avg:40.21ms
step:1691/2330 train_time:67998ms step_avg:40.21ms
step:1692/2330 train_time:68044ms step_avg:40.22ms
step:1693/2330 train_time:68079ms step_avg:40.21ms
step:1694/2330 train_time:68124ms step_avg:40.22ms
step:1695/2330 train_time:68159ms step_avg:40.21ms
step:1696/2330 train_time:68203ms step_avg:40.21ms
step:1697/2330 train_time:68239ms step_avg:40.21ms
step:1698/2330 train_time:68283ms step_avg:40.21ms
step:1699/2330 train_time:68319ms step_avg:40.21ms
step:1700/2330 train_time:68364ms step_avg:40.21ms
step:1701/2330 train_time:68399ms step_avg:40.21ms
step:1702/2330 train_time:68444ms step_avg:40.21ms
step:1703/2330 train_time:68479ms step_avg:40.21ms
step:1704/2330 train_time:68523ms step_avg:40.21ms
step:1705/2330 train_time:68558ms step_avg:40.21ms
step:1706/2330 train_time:68603ms step_avg:40.21ms
step:1707/2330 train_time:68639ms step_avg:40.21ms
step:1708/2330 train_time:68684ms step_avg:40.21ms
step:1709/2330 train_time:68719ms step_avg:40.21ms
step:1710/2330 train_time:68764ms step_avg:40.21ms
step:1711/2330 train_time:68799ms step_avg:40.21ms
step:1712/2330 train_time:68843ms step_avg:40.21ms
step:1713/2330 train_time:68879ms step_avg:40.21ms
step:1714/2330 train_time:68924ms step_avg:40.21ms
step:1715/2330 train_time:68958ms step_avg:40.21ms
step:1716/2330 train_time:69004ms step_avg:40.21ms
step:1717/2330 train_time:69039ms step_avg:40.21ms
step:1718/2330 train_time:69084ms step_avg:40.21ms
step:1719/2330 train_time:69119ms step_avg:40.21ms
step:1720/2330 train_time:69164ms step_avg:40.21ms
step:1721/2330 train_time:69199ms step_avg:40.21ms
step:1722/2330 train_time:69244ms step_avg:40.21ms
step:1723/2330 train_time:69280ms step_avg:40.21ms
step:1724/2330 train_time:69324ms step_avg:40.21ms
step:1725/2330 train_time:69360ms step_avg:40.21ms
step:1726/2330 train_time:69404ms step_avg:40.21ms
step:1727/2330 train_time:69439ms step_avg:40.21ms
step:1728/2330 train_time:69484ms step_avg:40.21ms
step:1729/2330 train_time:69520ms step_avg:40.21ms
step:1730/2330 train_time:69564ms step_avg:40.21ms
step:1731/2330 train_time:69599ms step_avg:40.21ms
step:1732/2330 train_time:69643ms step_avg:40.21ms
step:1733/2330 train_time:69678ms step_avg:40.21ms
step:1734/2330 train_time:69723ms step_avg:40.21ms
step:1735/2330 train_time:69759ms step_avg:40.21ms
step:1736/2330 train_time:69803ms step_avg:40.21ms
step:1737/2330 train_time:69838ms step_avg:40.21ms
step:1738/2330 train_time:69883ms step_avg:40.21ms
step:1739/2330 train_time:69919ms step_avg:40.21ms
step:1740/2330 train_time:69964ms step_avg:40.21ms
step:1741/2330 train_time:69998ms step_avg:40.21ms
step:1742/2330 train_time:70043ms step_avg:40.21ms
step:1743/2330 train_time:70078ms step_avg:40.21ms
step:1744/2330 train_time:70123ms step_avg:40.21ms
step:1745/2330 train_time:70159ms step_avg:40.21ms
step:1746/2330 train_time:70203ms step_avg:40.21ms
step:1747/2330 train_time:70239ms step_avg:40.21ms
step:1748/2330 train_time:70283ms step_avg:40.21ms
step:1749/2330 train_time:70319ms step_avg:40.21ms
step:1750/2330 train_time:70364ms step_avg:40.21ms
step:1750/2330 val_loss:5.1182 train_time:70451ms step_avg:40.26ms
step:1751/2330 train_time:70464ms step_avg:40.24ms
step:1752/2330 train_time:70477ms step_avg:40.23ms
step:1753/2330 train_time:70488ms step_avg:40.21ms
step:1754/2330 train_time:70523ms step_avg:40.21ms
step:1755/2330 train_time:70557ms step_avg:40.20ms
step:1756/2330 train_time:70601ms step_avg:40.21ms
step:1757/2330 train_time:70635ms step_avg:40.20ms
step:1758/2330 train_time:70679ms step_avg:40.20ms
step:1759/2330 train_time:70714ms step_avg:40.20ms
step:1760/2330 train_time:70760ms step_avg:40.20ms
step:1761/2330 train_time:70798ms step_avg:40.20ms
step:1762/2330 train_time:70846ms step_avg:40.21ms
step:1763/2330 train_time:70881ms step_avg:40.20ms
step:1764/2330 train_time:70926ms step_avg:40.21ms
step:1765/2330 train_time:70960ms step_avg:40.20ms
step:1766/2330 train_time:71004ms step_avg:40.21ms
step:1767/2330 train_time:71039ms step_avg:40.20ms
step:1768/2330 train_time:71083ms step_avg:40.21ms
step:1769/2330 train_time:71117ms step_avg:40.20ms
step:1770/2330 train_time:71161ms step_avg:40.20ms
step:1771/2330 train_time:71196ms step_avg:40.20ms
step:1772/2330 train_time:71239ms step_avg:40.20ms
step:1773/2330 train_time:71274ms step_avg:40.20ms
step:1774/2330 train_time:71318ms step_avg:40.20ms
step:1775/2330 train_time:71357ms step_avg:40.20ms
step:1776/2330 train_time:71408ms step_avg:40.21ms
step:1777/2330 train_time:71446ms step_avg:40.21ms
step:1778/2330 train_time:71490ms step_avg:40.21ms
step:1779/2330 train_time:71525ms step_avg:40.21ms
step:1780/2330 train_time:71570ms step_avg:40.21ms
step:1781/2330 train_time:71604ms step_avg:40.20ms
step:1782/2330 train_time:71649ms step_avg:40.21ms
step:1783/2330 train_time:71684ms step_avg:40.20ms
step:1784/2330 train_time:71728ms step_avg:40.21ms
step:1785/2330 train_time:71764ms step_avg:40.20ms
step:1786/2330 train_time:71809ms step_avg:40.21ms
step:1787/2330 train_time:71845ms step_avg:40.20ms
step:1788/2330 train_time:71889ms step_avg:40.21ms
step:1789/2330 train_time:71925ms step_avg:40.20ms
step:1790/2330 train_time:71969ms step_avg:40.21ms
step:1791/2330 train_time:72005ms step_avg:40.20ms
step:1792/2330 train_time:72049ms step_avg:40.21ms
step:1793/2330 train_time:72084ms step_avg:40.20ms
step:1794/2330 train_time:72128ms step_avg:40.21ms
step:1795/2330 train_time:72164ms step_avg:40.20ms
step:1796/2330 train_time:72208ms step_avg:40.20ms
step:1797/2330 train_time:72242ms step_avg:40.20ms
step:1798/2330 train_time:72287ms step_avg:40.20ms
step:1799/2330 train_time:72323ms step_avg:40.20ms
step:1800/2330 train_time:72369ms step_avg:40.20ms
step:1801/2330 train_time:72404ms step_avg:40.20ms
step:1802/2330 train_time:72449ms step_avg:40.20ms
step:1803/2330 train_time:72484ms step_avg:40.20ms
step:1804/2330 train_time:72528ms step_avg:40.20ms
step:1805/2330 train_time:72563ms step_avg:40.20ms
step:1806/2330 train_time:72609ms step_avg:40.20ms
step:1807/2330 train_time:72644ms step_avg:40.20ms
step:1808/2330 train_time:72688ms step_avg:40.20ms
step:1809/2330 train_time:72724ms step_avg:40.20ms
step:1810/2330 train_time:72770ms step_avg:40.20ms
step:1811/2330 train_time:72805ms step_avg:40.20ms
step:1812/2330 train_time:72849ms step_avg:40.20ms
step:1813/2330 train_time:72884ms step_avg:40.20ms
step:1814/2330 train_time:72930ms step_avg:40.20ms
step:1815/2330 train_time:72965ms step_avg:40.20ms
step:1816/2330 train_time:73010ms step_avg:40.20ms
step:1817/2330 train_time:73045ms step_avg:40.20ms
step:1818/2330 train_time:73090ms step_avg:40.20ms
step:1819/2330 train_time:73126ms step_avg:40.20ms
step:1820/2330 train_time:73170ms step_avg:40.20ms
step:1821/2330 train_time:73205ms step_avg:40.20ms
step:1822/2330 train_time:73250ms step_avg:40.20ms
step:1823/2330 train_time:73286ms step_avg:40.20ms
step:1824/2330 train_time:73330ms step_avg:40.20ms
step:1825/2330 train_time:73366ms step_avg:40.20ms
step:1826/2330 train_time:73410ms step_avg:40.20ms
step:1827/2330 train_time:73445ms step_avg:40.20ms
step:1828/2330 train_time:73490ms step_avg:40.20ms
step:1829/2330 train_time:73525ms step_avg:40.20ms
step:1830/2330 train_time:73570ms step_avg:40.20ms
step:1831/2330 train_time:73605ms step_avg:40.20ms
step:1832/2330 train_time:73649ms step_avg:40.20ms
step:1833/2330 train_time:73684ms step_avg:40.20ms
step:1834/2330 train_time:73729ms step_avg:40.20ms
step:1835/2330 train_time:73765ms step_avg:40.20ms
step:1836/2330 train_time:73809ms step_avg:40.20ms
step:1837/2330 train_time:73844ms step_avg:40.20ms
step:1838/2330 train_time:73889ms step_avg:40.20ms
step:1839/2330 train_time:73925ms step_avg:40.20ms
step:1840/2330 train_time:73969ms step_avg:40.20ms
step:1841/2330 train_time:74004ms step_avg:40.20ms
step:1842/2330 train_time:74049ms step_avg:40.20ms
step:1843/2330 train_time:74083ms step_avg:40.20ms
step:1844/2330 train_time:74128ms step_avg:40.20ms
step:1845/2330 train_time:74164ms step_avg:40.20ms
step:1846/2330 train_time:74209ms step_avg:40.20ms
step:1847/2330 train_time:74244ms step_avg:40.20ms
step:1848/2330 train_time:74289ms step_avg:40.20ms
step:1849/2330 train_time:74324ms step_avg:40.20ms
step:1850/2330 train_time:74369ms step_avg:40.20ms
step:1851/2330 train_time:74405ms step_avg:40.20ms
step:1852/2330 train_time:74449ms step_avg:40.20ms
step:1853/2330 train_time:74485ms step_avg:40.20ms
step:1854/2330 train_time:74529ms step_avg:40.20ms
step:1855/2330 train_time:74565ms step_avg:40.20ms
step:1856/2330 train_time:74608ms step_avg:40.20ms
step:1857/2330 train_time:74644ms step_avg:40.20ms
step:1858/2330 train_time:74689ms step_avg:40.20ms
step:1859/2330 train_time:74724ms step_avg:40.20ms
step:1860/2330 train_time:74769ms step_avg:40.20ms
step:1861/2330 train_time:74805ms step_avg:40.20ms
step:1862/2330 train_time:74849ms step_avg:40.20ms
step:1863/2330 train_time:74884ms step_avg:40.20ms
step:1864/2330 train_time:74928ms step_avg:40.20ms
step:1865/2330 train_time:74964ms step_avg:40.20ms
step:1866/2330 train_time:75009ms step_avg:40.20ms
step:1867/2330 train_time:75044ms step_avg:40.20ms
step:1868/2330 train_time:75089ms step_avg:40.20ms
step:1869/2330 train_time:75125ms step_avg:40.20ms
step:1870/2330 train_time:75169ms step_avg:40.20ms
step:1871/2330 train_time:75204ms step_avg:40.19ms
step:1872/2330 train_time:75248ms step_avg:40.20ms
step:1873/2330 train_time:75284ms step_avg:40.19ms
step:1874/2330 train_time:75329ms step_avg:40.20ms
step:1875/2330 train_time:75365ms step_avg:40.19ms
step:1876/2330 train_time:75409ms step_avg:40.20ms
step:1877/2330 train_time:75443ms step_avg:40.19ms
step:1878/2330 train_time:75488ms step_avg:40.20ms
step:1879/2330 train_time:75524ms step_avg:40.19ms
step:1880/2330 train_time:75568ms step_avg:40.20ms
step:1881/2330 train_time:75604ms step_avg:40.19ms
step:1882/2330 train_time:75648ms step_avg:40.20ms
step:1883/2330 train_time:75683ms step_avg:40.19ms
step:1884/2330 train_time:75728ms step_avg:40.20ms
step:1885/2330 train_time:75763ms step_avg:40.19ms
step:1886/2330 train_time:75807ms step_avg:40.19ms
step:1887/2330 train_time:75842ms step_avg:40.19ms
step:1888/2330 train_time:75887ms step_avg:40.19ms
step:1889/2330 train_time:75923ms step_avg:40.19ms
step:1890/2330 train_time:75968ms step_avg:40.19ms
step:1891/2330 train_time:76003ms step_avg:40.19ms
step:1892/2330 train_time:76047ms step_avg:40.19ms
step:1893/2330 train_time:76083ms step_avg:40.19ms
step:1894/2330 train_time:76127ms step_avg:40.19ms
step:1895/2330 train_time:76162ms step_avg:40.19ms
step:1896/2330 train_time:76206ms step_avg:40.19ms
step:1897/2330 train_time:76241ms step_avg:40.19ms
step:1898/2330 train_time:76286ms step_avg:40.19ms
step:1899/2330 train_time:76320ms step_avg:40.19ms
step:1900/2330 train_time:76365ms step_avg:40.19ms
step:1901/2330 train_time:76399ms step_avg:40.19ms
step:1902/2330 train_time:76444ms step_avg:40.19ms
step:1903/2330 train_time:76479ms step_avg:40.19ms
step:1904/2330 train_time:76523ms step_avg:40.19ms
step:1905/2330 train_time:76559ms step_avg:40.19ms
step:1906/2330 train_time:76603ms step_avg:40.19ms
step:1907/2330 train_time:76638ms step_avg:40.19ms
step:1908/2330 train_time:76682ms step_avg:40.19ms
step:1909/2330 train_time:76718ms step_avg:40.19ms
step:1910/2330 train_time:76763ms step_avg:40.19ms
step:1911/2330 train_time:76798ms step_avg:40.19ms
step:1912/2330 train_time:76843ms step_avg:40.19ms
step:1913/2330 train_time:76878ms step_avg:40.19ms
step:1914/2330 train_time:76922ms step_avg:40.19ms
step:1915/2330 train_time:76957ms step_avg:40.19ms
step:1916/2330 train_time:77002ms step_avg:40.19ms
step:1917/2330 train_time:77038ms step_avg:40.19ms
step:1918/2330 train_time:77083ms step_avg:40.19ms
step:1919/2330 train_time:77118ms step_avg:40.19ms
step:1920/2330 train_time:77163ms step_avg:40.19ms
step:1921/2330 train_time:77199ms step_avg:40.19ms
step:1922/2330 train_time:77243ms step_avg:40.19ms
step:1923/2330 train_time:77278ms step_avg:40.19ms
step:1924/2330 train_time:77323ms step_avg:40.19ms
step:1925/2330 train_time:77358ms step_avg:40.19ms
step:1926/2330 train_time:77402ms step_avg:40.19ms
step:1927/2330 train_time:77438ms step_avg:40.19ms
step:1928/2330 train_time:77483ms step_avg:40.19ms
step:1929/2330 train_time:77517ms step_avg:40.19ms
step:1930/2330 train_time:77562ms step_avg:40.19ms
step:1931/2330 train_time:77597ms step_avg:40.19ms
step:1932/2330 train_time:77642ms step_avg:40.19ms
step:1933/2330 train_time:77678ms step_avg:40.19ms
step:1934/2330 train_time:77722ms step_avg:40.19ms
step:1935/2330 train_time:77758ms step_avg:40.18ms
step:1936/2330 train_time:77802ms step_avg:40.19ms
step:1937/2330 train_time:77838ms step_avg:40.18ms
step:1938/2330 train_time:77882ms step_avg:40.19ms
step:1939/2330 train_time:77918ms step_avg:40.18ms
step:1940/2330 train_time:77962ms step_avg:40.19ms
step:1941/2330 train_time:77998ms step_avg:40.18ms
step:1942/2330 train_time:78042ms step_avg:40.19ms
step:1943/2330 train_time:78078ms step_avg:40.18ms
step:1944/2330 train_time:78122ms step_avg:40.19ms
step:1945/2330 train_time:78157ms step_avg:40.18ms
step:1946/2330 train_time:78202ms step_avg:40.19ms
step:1947/2330 train_time:78237ms step_avg:40.18ms
step:1948/2330 train_time:78282ms step_avg:40.19ms
step:1949/2330 train_time:78317ms step_avg:40.18ms
step:1950/2330 train_time:78362ms step_avg:40.19ms
step:1951/2330 train_time:78397ms step_avg:40.18ms
step:1952/2330 train_time:78441ms step_avg:40.19ms
step:1953/2330 train_time:78476ms step_avg:40.18ms
step:1954/2330 train_time:78521ms step_avg:40.18ms
step:1955/2330 train_time:78557ms step_avg:40.18ms
step:1956/2330 train_time:78602ms step_avg:40.18ms
step:1957/2330 train_time:78637ms step_avg:40.18ms
step:1958/2330 train_time:78682ms step_avg:40.18ms
step:1959/2330 train_time:78717ms step_avg:40.18ms
step:1960/2330 train_time:78763ms step_avg:40.18ms
step:1961/2330 train_time:78798ms step_avg:40.18ms
step:1962/2330 train_time:78843ms step_avg:40.18ms
step:1963/2330 train_time:78878ms step_avg:40.18ms
step:1964/2330 train_time:78922ms step_avg:40.18ms
step:1965/2330 train_time:78958ms step_avg:40.18ms
step:1966/2330 train_time:79003ms step_avg:40.18ms
step:1967/2330 train_time:79038ms step_avg:40.18ms
step:1968/2330 train_time:79083ms step_avg:40.18ms
step:1969/2330 train_time:79118ms step_avg:40.18ms
step:1970/2330 train_time:79162ms step_avg:40.18ms
step:1971/2330 train_time:79198ms step_avg:40.18ms
step:1972/2330 train_time:79242ms step_avg:40.18ms
step:1973/2330 train_time:79277ms step_avg:40.18ms
step:1974/2330 train_time:79321ms step_avg:40.18ms
step:1975/2330 train_time:79357ms step_avg:40.18ms
step:1976/2330 train_time:79402ms step_avg:40.18ms
step:1977/2330 train_time:79437ms step_avg:40.18ms
step:1978/2330 train_time:79481ms step_avg:40.18ms
step:1979/2330 train_time:79517ms step_avg:40.18ms
step:1980/2330 train_time:79562ms step_avg:40.18ms
step:1981/2330 train_time:79597ms step_avg:40.18ms
step:1982/2330 train_time:79641ms step_avg:40.18ms
step:1983/2330 train_time:79676ms step_avg:40.18ms
step:1984/2330 train_time:79722ms step_avg:40.18ms
step:1985/2330 train_time:79757ms step_avg:40.18ms
step:1986/2330 train_time:79802ms step_avg:40.18ms
step:1987/2330 train_time:79838ms step_avg:40.18ms
step:1988/2330 train_time:79882ms step_avg:40.18ms
step:1989/2330 train_time:79918ms step_avg:40.18ms
step:1990/2330 train_time:79962ms step_avg:40.18ms
step:1991/2330 train_time:79998ms step_avg:40.18ms
step:1992/2330 train_time:80042ms step_avg:40.18ms
step:1993/2330 train_time:80078ms step_avg:40.18ms
step:1994/2330 train_time:80122ms step_avg:40.18ms
step:1995/2330 train_time:80158ms step_avg:40.18ms
step:1996/2330 train_time:80203ms step_avg:40.18ms
step:1997/2330 train_time:80238ms step_avg:40.18ms
step:1998/2330 train_time:80282ms step_avg:40.18ms
step:1999/2330 train_time:80318ms step_avg:40.18ms
step:2000/2330 train_time:80362ms step_avg:40.18ms
step:2000/2330 val_loss:5.0892 train_time:80449ms step_avg:40.22ms
step:2001/2330 train_time:80462ms step_avg:40.21ms
step:2002/2330 train_time:80475ms step_avg:40.20ms
step:2003/2330 train_time:80485ms step_avg:40.18ms
step:2004/2330 train_time:80523ms step_avg:40.18ms
step:2005/2330 train_time:80557ms step_avg:40.18ms
step:2006/2330 train_time:80601ms step_avg:40.18ms
step:2007/2330 train_time:80635ms step_avg:40.18ms
step:2008/2330 train_time:80679ms step_avg:40.18ms
step:2009/2330 train_time:80713ms step_avg:40.18ms
step:2010/2330 train_time:80758ms step_avg:40.18ms
step:2011/2330 train_time:80796ms step_avg:40.18ms
step:2012/2330 train_time:80842ms step_avg:40.18ms
step:2013/2330 train_time:80879ms step_avg:40.18ms
step:2014/2330 train_time:80924ms step_avg:40.18ms
step:2015/2330 train_time:80959ms step_avg:40.18ms
step:2016/2330 train_time:81004ms step_avg:40.18ms
step:2017/2330 train_time:81038ms step_avg:40.18ms
step:2018/2330 train_time:81083ms step_avg:40.18ms
step:2019/2330 train_time:81118ms step_avg:40.18ms
step:2020/2330 train_time:81163ms step_avg:40.18ms
step:2021/2330 train_time:81198ms step_avg:40.18ms
step:2022/2330 train_time:81242ms step_avg:40.18ms
step:2023/2330 train_time:81277ms step_avg:40.18ms
step:2024/2330 train_time:81321ms step_avg:40.18ms
step:2025/2330 train_time:81356ms step_avg:40.18ms
step:2026/2330 train_time:81402ms step_avg:40.18ms
step:2027/2330 train_time:81438ms step_avg:40.18ms
step:2028/2330 train_time:81482ms step_avg:40.18ms
step:2029/2330 train_time:81517ms step_avg:40.18ms
step:2030/2330 train_time:81561ms step_avg:40.18ms
step:2031/2330 train_time:81596ms step_avg:40.18ms
step:2032/2330 train_time:81639ms step_avg:40.18ms
step:2033/2330 train_time:81674ms step_avg:40.17ms
step:2034/2330 train_time:81719ms step_avg:40.18ms
step:2035/2330 train_time:81756ms step_avg:40.17ms
step:2036/2330 train_time:81801ms step_avg:40.18ms
step:2037/2330 train_time:81837ms step_avg:40.18ms
step:2038/2330 train_time:81881ms step_avg:40.18ms
step:2039/2330 train_time:81917ms step_avg:40.17ms
step:2040/2330 train_time:81962ms step_avg:40.18ms
step:2041/2330 train_time:81997ms step_avg:40.17ms
step:2042/2330 train_time:82041ms step_avg:40.18ms
step:2043/2330 train_time:82076ms step_avg:40.17ms
step:2044/2330 train_time:82120ms step_avg:40.18ms
step:2045/2330 train_time:82155ms step_avg:40.17ms
step:2046/2330 train_time:82199ms step_avg:40.18ms
step:2047/2330 train_time:82234ms step_avg:40.17ms
step:2048/2330 train_time:82278ms step_avg:40.17ms
step:2049/2330 train_time:82313ms step_avg:40.17ms
step:2050/2330 train_time:82358ms step_avg:40.17ms
step:2051/2330 train_time:82394ms step_avg:40.17ms
step:2052/2330 train_time:82438ms step_avg:40.17ms
step:2053/2330 train_time:82474ms step_avg:40.17ms
step:2054/2330 train_time:82518ms step_avg:40.17ms
step:2055/2330 train_time:82554ms step_avg:40.17ms
step:2056/2330 train_time:82598ms step_avg:40.17ms
step:2057/2330 train_time:82633ms step_avg:40.17ms
step:2058/2330 train_time:82678ms step_avg:40.17ms
step:2059/2330 train_time:82713ms step_avg:40.17ms
step:2060/2330 train_time:82758ms step_avg:40.17ms
step:2061/2330 train_time:82794ms step_avg:40.17ms
step:2062/2330 train_time:82839ms step_avg:40.17ms
step:2063/2330 train_time:82874ms step_avg:40.17ms
step:2064/2330 train_time:82918ms step_avg:40.17ms
step:2065/2330 train_time:82955ms step_avg:40.17ms
step:2066/2330 train_time:82999ms step_avg:40.17ms
step:2067/2330 train_time:83035ms step_avg:40.17ms
step:2068/2330 train_time:83079ms step_avg:40.17ms
step:2069/2330 train_time:83113ms step_avg:40.17ms
step:2070/2330 train_time:83158ms step_avg:40.17ms
step:2071/2330 train_time:83193ms step_avg:40.17ms
step:2072/2330 train_time:83236ms step_avg:40.17ms
step:2073/2330 train_time:83272ms step_avg:40.17ms
step:2074/2330 train_time:83316ms step_avg:40.17ms
step:2075/2330 train_time:83352ms step_avg:40.17ms
step:2076/2330 train_time:83397ms step_avg:40.17ms
step:2077/2330 train_time:83432ms step_avg:40.17ms
step:2078/2330 train_time:83477ms step_avg:40.17ms
step:2079/2330 train_time:83512ms step_avg:40.17ms
step:2080/2330 train_time:83557ms step_avg:40.17ms
step:2081/2330 train_time:83592ms step_avg:40.17ms
step:2082/2330 train_time:83637ms step_avg:40.17ms
step:2083/2330 train_time:83672ms step_avg:40.17ms
step:2084/2330 train_time:83717ms step_avg:40.17ms
step:2085/2330 train_time:83753ms step_avg:40.17ms
step:2086/2330 train_time:83798ms step_avg:40.17ms
step:2087/2330 train_time:83833ms step_avg:40.17ms
step:2088/2330 train_time:83878ms step_avg:40.17ms
step:2089/2330 train_time:83913ms step_avg:40.17ms
step:2090/2330 train_time:83958ms step_avg:40.17ms
step:2091/2330 train_time:83993ms step_avg:40.17ms
step:2092/2330 train_time:84038ms step_avg:40.17ms
step:2093/2330 train_time:84072ms step_avg:40.17ms
step:2094/2330 train_time:84117ms step_avg:40.17ms
step:2095/2330 train_time:84152ms step_avg:40.17ms
step:2096/2330 train_time:84197ms step_avg:40.17ms
step:2097/2330 train_time:84232ms step_avg:40.17ms
step:2098/2330 train_time:84276ms step_avg:40.17ms
step:2099/2330 train_time:84311ms step_avg:40.17ms
step:2100/2330 train_time:84356ms step_avg:40.17ms
step:2101/2330 train_time:84392ms step_avg:40.17ms
step:2102/2330 train_time:84436ms step_avg:40.17ms
step:2103/2330 train_time:84471ms step_avg:40.17ms
step:2104/2330 train_time:84516ms step_avg:40.17ms
step:2105/2330 train_time:84552ms step_avg:40.17ms
step:2106/2330 train_time:84596ms step_avg:40.17ms
step:2107/2330 train_time:84631ms step_avg:40.17ms
step:2108/2330 train_time:84677ms step_avg:40.17ms
step:2109/2330 train_time:84712ms step_avg:40.17ms
step:2110/2330 train_time:84757ms step_avg:40.17ms
step:2111/2330 train_time:84792ms step_avg:40.17ms
step:2112/2330 train_time:84837ms step_avg:40.17ms
step:2113/2330 train_time:84872ms step_avg:40.17ms
step:2114/2330 train_time:84917ms step_avg:40.17ms
step:2115/2330 train_time:84953ms step_avg:40.17ms
step:2116/2330 train_time:84997ms step_avg:40.17ms
step:2117/2330 train_time:85033ms step_avg:40.17ms
step:2118/2330 train_time:85077ms step_avg:40.17ms
step:2119/2330 train_time:85112ms step_avg:40.17ms
step:2120/2330 train_time:85157ms step_avg:40.17ms
step:2121/2330 train_time:85193ms step_avg:40.17ms
step:2122/2330 train_time:85237ms step_avg:40.17ms
step:2123/2330 train_time:85272ms step_avg:40.17ms
step:2124/2330 train_time:85317ms step_avg:40.17ms
step:2125/2330 train_time:85352ms step_avg:40.17ms
step:2126/2330 train_time:85396ms step_avg:40.17ms
step:2127/2330 train_time:85432ms step_avg:40.17ms
step:2128/2330 train_time:85477ms step_avg:40.17ms
step:2129/2330 train_time:85511ms step_avg:40.17ms
step:2130/2330 train_time:85556ms step_avg:40.17ms
step:2131/2330 train_time:85591ms step_avg:40.16ms
step:2132/2330 train_time:85636ms step_avg:40.17ms
step:2133/2330 train_time:85672ms step_avg:40.16ms
step:2134/2330 train_time:85716ms step_avg:40.17ms
step:2135/2330 train_time:85751ms step_avg:40.16ms
step:2136/2330 train_time:85796ms step_avg:40.17ms
step:2137/2330 train_time:85831ms step_avg:40.16ms
step:2138/2330 train_time:85876ms step_avg:40.17ms
step:2139/2330 train_time:85912ms step_avg:40.16ms
step:2140/2330 train_time:85956ms step_avg:40.17ms
step:2141/2330 train_time:85993ms step_avg:40.16ms
step:2142/2330 train_time:86037ms step_avg:40.17ms
step:2143/2330 train_time:86072ms step_avg:40.16ms
step:2144/2330 train_time:86116ms step_avg:40.17ms
step:2145/2330 train_time:86152ms step_avg:40.16ms
step:2146/2330 train_time:86198ms step_avg:40.17ms
step:2147/2330 train_time:86232ms step_avg:40.16ms
step:2148/2330 train_time:86277ms step_avg:40.17ms
step:2149/2330 train_time:86312ms step_avg:40.16ms
step:2150/2330 train_time:86357ms step_avg:40.17ms
step:2151/2330 train_time:86392ms step_avg:40.16ms
step:2152/2330 train_time:86437ms step_avg:40.17ms
step:2153/2330 train_time:86472ms step_avg:40.16ms
step:2154/2330 train_time:86516ms step_avg:40.17ms
step:2155/2330 train_time:86551ms step_avg:40.16ms
step:2156/2330 train_time:86596ms step_avg:40.17ms
step:2157/2330 train_time:86632ms step_avg:40.16ms
step:2158/2330 train_time:86677ms step_avg:40.17ms
step:2159/2330 train_time:86713ms step_avg:40.16ms
step:2160/2330 train_time:86757ms step_avg:40.17ms
step:2161/2330 train_time:86793ms step_avg:40.16ms
step:2162/2330 train_time:86837ms step_avg:40.17ms
step:2163/2330 train_time:86873ms step_avg:40.16ms
step:2164/2330 train_time:86917ms step_avg:40.16ms
step:2165/2330 train_time:86953ms step_avg:40.16ms
step:2166/2330 train_time:86997ms step_avg:40.16ms
step:2167/2330 train_time:87032ms step_avg:40.16ms
step:2168/2330 train_time:87077ms step_avg:40.16ms
step:2169/2330 train_time:87113ms step_avg:40.16ms
step:2170/2330 train_time:87158ms step_avg:40.17ms
step:2171/2330 train_time:87194ms step_avg:40.16ms
step:2172/2330 train_time:87238ms step_avg:40.16ms
step:2173/2330 train_time:87273ms step_avg:40.16ms
step:2174/2330 train_time:87318ms step_avg:40.16ms
step:2175/2330 train_time:87353ms step_avg:40.16ms
step:2176/2330 train_time:87397ms step_avg:40.16ms
step:2177/2330 train_time:87433ms step_avg:40.16ms
step:2178/2330 train_time:87477ms step_avg:40.16ms
step:2179/2330 train_time:87512ms step_avg:40.16ms
step:2180/2330 train_time:87557ms step_avg:40.16ms
step:2181/2330 train_time:87593ms step_avg:40.16ms
step:2182/2330 train_time:87637ms step_avg:40.16ms
step:2183/2330 train_time:87672ms step_avg:40.16ms
step:2184/2330 train_time:87717ms step_avg:40.16ms
step:2185/2330 train_time:87753ms step_avg:40.16ms
step:2186/2330 train_time:87798ms step_avg:40.16ms
step:2187/2330 train_time:87833ms step_avg:40.16ms
step:2188/2330 train_time:87878ms step_avg:40.16ms
step:2189/2330 train_time:87913ms step_avg:40.16ms
step:2190/2330 train_time:87957ms step_avg:40.16ms
step:2191/2330 train_time:87993ms step_avg:40.16ms
step:2192/2330 train_time:88037ms step_avg:40.16ms
step:2193/2330 train_time:88073ms step_avg:40.16ms
step:2194/2330 train_time:88117ms step_avg:40.16ms
step:2195/2330 train_time:88153ms step_avg:40.16ms
step:2196/2330 train_time:88198ms step_avg:40.16ms
step:2197/2330 train_time:88233ms step_avg:40.16ms
step:2198/2330 train_time:88277ms step_avg:40.16ms
step:2199/2330 train_time:88312ms step_avg:40.16ms
step:2200/2330 train_time:88358ms step_avg:40.16ms
step:2201/2330 train_time:88393ms step_avg:40.16ms
step:2202/2330 train_time:88438ms step_avg:40.16ms
step:2203/2330 train_time:88473ms step_avg:40.16ms
step:2204/2330 train_time:88517ms step_avg:40.16ms
step:2205/2330 train_time:88552ms step_avg:40.16ms
step:2206/2330 train_time:88597ms step_avg:40.16ms
step:2207/2330 train_time:88632ms step_avg:40.16ms
step:2208/2330 train_time:88676ms step_avg:40.16ms
step:2209/2330 train_time:88712ms step_avg:40.16ms
step:2210/2330 train_time:88757ms step_avg:40.16ms
step:2211/2330 train_time:88791ms step_avg:40.16ms
step:2212/2330 train_time:88836ms step_avg:40.16ms
step:2213/2330 train_time:88872ms step_avg:40.16ms
step:2214/2330 train_time:88917ms step_avg:40.16ms
step:2215/2330 train_time:88952ms step_avg:40.16ms
step:2216/2330 train_time:88998ms step_avg:40.16ms
step:2217/2330 train_time:89033ms step_avg:40.16ms
step:2218/2330 train_time:89078ms step_avg:40.16ms
step:2219/2330 train_time:89113ms step_avg:40.16ms
step:2220/2330 train_time:89158ms step_avg:40.16ms
step:2221/2330 train_time:89193ms step_avg:40.16ms
step:2222/2330 train_time:89238ms step_avg:40.16ms
step:2223/2330 train_time:89273ms step_avg:40.16ms
step:2224/2330 train_time:89318ms step_avg:40.16ms
step:2225/2330 train_time:89353ms step_avg:40.16ms
step:2226/2330 train_time:89398ms step_avg:40.16ms
step:2227/2330 train_time:89433ms step_avg:40.16ms
step:2228/2330 train_time:89478ms step_avg:40.16ms
step:2229/2330 train_time:89512ms step_avg:40.16ms
step:2230/2330 train_time:89557ms step_avg:40.16ms
step:2231/2330 train_time:89591ms step_avg:40.16ms
step:2232/2330 train_time:89636ms step_avg:40.16ms
step:2233/2330 train_time:89671ms step_avg:40.16ms
step:2234/2330 train_time:89716ms step_avg:40.16ms
step:2235/2330 train_time:89751ms step_avg:40.16ms
step:2236/2330 train_time:89796ms step_avg:40.16ms
step:2237/2330 train_time:89831ms step_avg:40.16ms
step:2238/2330 train_time:89876ms step_avg:40.16ms
step:2239/2330 train_time:89912ms step_avg:40.16ms
step:2240/2330 train_time:89957ms step_avg:40.16ms
step:2241/2330 train_time:89992ms step_avg:40.16ms
step:2242/2330 train_time:90037ms step_avg:40.16ms
step:2243/2330 train_time:90073ms step_avg:40.16ms
step:2244/2330 train_time:90117ms step_avg:40.16ms
step:2245/2330 train_time:90153ms step_avg:40.16ms
step:2246/2330 train_time:90198ms step_avg:40.16ms
step:2247/2330 train_time:90233ms step_avg:40.16ms
step:2248/2330 train_time:90277ms step_avg:40.16ms
step:2249/2330 train_time:90313ms step_avg:40.16ms
step:2250/2330 train_time:90358ms step_avg:40.16ms
step:2250/2330 val_loss:5.0643 train_time:90444ms step_avg:40.20ms
step:2251/2330 train_time:90458ms step_avg:40.19ms
step:2252/2330 train_time:90469ms step_avg:40.17ms
step:2253/2330 train_time:90480ms step_avg:40.16ms
step:2254/2330 train_time:90517ms step_avg:40.16ms
step:2255/2330 train_time:90552ms step_avg:40.16ms
step:2256/2330 train_time:90595ms step_avg:40.16ms
step:2257/2330 train_time:90630ms step_avg:40.15ms
step:2258/2330 train_time:90674ms step_avg:40.16ms
step:2259/2330 train_time:90708ms step_avg:40.15ms
step:2260/2330 train_time:90756ms step_avg:40.16ms
step:2261/2330 train_time:90794ms step_avg:40.16ms
step:2262/2330 train_time:90840ms step_avg:40.16ms
step:2263/2330 train_time:90875ms step_avg:40.16ms
step:2264/2330 train_time:90921ms step_avg:40.16ms
step:2265/2330 train_time:90956ms step_avg:40.16ms
step:2266/2330 train_time:91000ms step_avg:40.16ms
step:2267/2330 train_time:91035ms step_avg:40.16ms
step:2268/2330 train_time:91079ms step_avg:40.16ms
step:2269/2330 train_time:91113ms step_avg:40.16ms
step:2270/2330 train_time:91158ms step_avg:40.16ms
step:2271/2330 train_time:91193ms step_avg:40.16ms
step:2272/2330 train_time:91237ms step_avg:40.16ms
step:2273/2330 train_time:91272ms step_avg:40.15ms
step:2274/2330 train_time:91316ms step_avg:40.16ms
step:2275/2330 train_time:91352ms step_avg:40.15ms
step:2276/2330 train_time:91400ms step_avg:40.16ms
step:2277/2330 train_time:91436ms step_avg:40.16ms
step:2278/2330 train_time:91481ms step_avg:40.16ms
step:2279/2330 train_time:91516ms step_avg:40.16ms
step:2280/2330 train_time:91560ms step_avg:40.16ms
step:2281/2330 train_time:91595ms step_avg:40.16ms
step:2282/2330 train_time:91639ms step_avg:40.16ms
step:2283/2330 train_time:91674ms step_avg:40.15ms
step:2284/2330 train_time:91719ms step_avg:40.16ms
step:2285/2330 train_time:91755ms step_avg:40.16ms
step:2286/2330 train_time:91801ms step_avg:40.16ms
step:2287/2330 train_time:91836ms step_avg:40.16ms
step:2288/2330 train_time:91881ms step_avg:40.16ms
step:2289/2330 train_time:91917ms step_avg:40.16ms
step:2290/2330 train_time:91961ms step_avg:40.16ms
step:2291/2330 train_time:91996ms step_avg:40.16ms
step:2292/2330 train_time:92040ms step_avg:40.16ms
step:2293/2330 train_time:92074ms step_avg:40.15ms
step:2294/2330 train_time:92118ms step_avg:40.16ms
step:2295/2330 train_time:92153ms step_avg:40.15ms
step:2296/2330 train_time:92197ms step_avg:40.16ms
step:2297/2330 train_time:92231ms step_avg:40.15ms
step:2298/2330 train_time:92276ms step_avg:40.15ms
step:2299/2330 train_time:92311ms step_avg:40.15ms
step:2300/2330 train_time:92357ms step_avg:40.16ms
step:2301/2330 train_time:92392ms step_avg:40.15ms
step:2302/2330 train_time:92437ms step_avg:40.16ms
step:2303/2330 train_time:92473ms step_avg:40.15ms
step:2304/2330 train_time:92518ms step_avg:40.16ms
step:2305/2330 train_time:92553ms step_avg:40.15ms
step:2306/2330 train_time:92597ms step_avg:40.16ms
step:2307/2330 train_time:92633ms step_avg:40.15ms
step:2308/2330 train_time:92677ms step_avg:40.15ms
step:2309/2330 train_time:92713ms step_avg:40.15ms
step:2310/2330 train_time:92758ms step_avg:40.16ms
step:2311/2330 train_time:92794ms step_avg:40.15ms
step:2312/2330 train_time:92838ms step_avg:40.15ms
step:2313/2330 train_time:92874ms step_avg:40.15ms
step:2314/2330 train_time:92919ms step_avg:40.15ms
step:2315/2330 train_time:92953ms step_avg:40.15ms
step:2316/2330 train_time:92998ms step_avg:40.15ms
step:2317/2330 train_time:93033ms step_avg:40.15ms
step:2318/2330 train_time:93078ms step_avg:40.15ms
step:2319/2330 train_time:93113ms step_avg:40.15ms
step:2320/2330 train_time:93157ms step_avg:40.15ms
step:2321/2330 train_time:93192ms step_avg:40.15ms
step:2322/2330 train_time:93237ms step_avg:40.15ms
step:2323/2330 train_time:93271ms step_avg:40.15ms
step:2324/2330 train_time:93317ms step_avg:40.15ms
step:2325/2330 train_time:93352ms step_avg:40.15ms
step:2326/2330 train_time:93397ms step_avg:40.15ms
step:2327/2330 train_time:93432ms step_avg:40.15ms
step:2328/2330 train_time:93477ms step_avg:40.15ms
step:2329/2330 train_time:93511ms step_avg:40.15ms
step:2330/2330 train_time:93557ms step_avg:40.15ms
step:2330/2330 val_loss:5.0571 train_time:93644ms step_avg:40.19ms
peak memory allocated: 29226 MiB reserved: 38888 MiB
