import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = divide_by_norm(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = divide_by_norm(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_gd_lr7e-4"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=7e-4, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 05:09:17 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   32C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   31C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   31C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   32C    P0             115W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.05ms
step:1/2330 train_time:62ms step_avg:61.76ms
step:2/2330 train_time:148ms step_avg:74.12ms
step:3/2330 train_time:168ms step_avg:55.90ms
step:4/2330 train_time:179ms step_avg:44.81ms
step:5/2330 train_time:189ms step_avg:37.87ms
step:6/2330 train_time:211ms step_avg:35.20ms
step:7/2330 train_time:232ms step_avg:33.19ms
step:8/2330 train_time:287ms step_avg:35.88ms
step:9/2330 train_time:309ms step_avg:34.33ms
step:10/2330 train_time:364ms step_avg:36.42ms
step:11/2330 train_time:386ms step_avg:35.09ms
step:12/2330 train_time:442ms step_avg:36.80ms
step:13/2330 train_time:463ms step_avg:35.65ms
step:14/2330 train_time:519ms step_avg:37.06ms
step:15/2330 train_time:541ms step_avg:36.07ms
step:16/2330 train_time:597ms step_avg:37.29ms
step:17/2330 train_time:618ms step_avg:36.35ms
step:18/2330 train_time:673ms step_avg:37.40ms
step:19/2330 train_time:695ms step_avg:36.60ms
step:20/2330 train_time:751ms step_avg:37.54ms
step:21/2330 train_time:773ms step_avg:36.82ms
step:22/2330 train_time:829ms step_avg:37.67ms
step:23/2330 train_time:851ms step_avg:37.00ms
step:24/2330 train_time:906ms step_avg:37.77ms
step:25/2330 train_time:929ms step_avg:37.16ms
step:26/2330 train_time:988ms step_avg:38.01ms
step:27/2330 train_time:1015ms step_avg:37.58ms
step:28/2330 train_time:1075ms step_avg:38.39ms
step:29/2330 train_time:1101ms step_avg:37.95ms
step:30/2330 train_time:1159ms step_avg:38.64ms
step:31/2330 train_time:1182ms step_avg:38.13ms
step:32/2330 train_time:1240ms step_avg:38.75ms
step:33/2330 train_time:1262ms step_avg:38.25ms
step:34/2330 train_time:1319ms step_avg:38.80ms
step:35/2330 train_time:1342ms step_avg:38.33ms
step:36/2330 train_time:1398ms step_avg:38.83ms
step:37/2330 train_time:1420ms step_avg:38.37ms
step:38/2330 train_time:1476ms step_avg:38.85ms
step:39/2330 train_time:1499ms step_avg:38.43ms
step:40/2330 train_time:1555ms step_avg:38.88ms
step:41/2330 train_time:1576ms step_avg:38.45ms
step:42/2330 train_time:1632ms step_avg:38.85ms
step:43/2330 train_time:1654ms step_avg:38.47ms
step:44/2330 train_time:1710ms step_avg:38.86ms
step:45/2330 train_time:1732ms step_avg:38.49ms
step:46/2330 train_time:1788ms step_avg:38.87ms
step:47/2330 train_time:1811ms step_avg:38.52ms
step:48/2330 train_time:1866ms step_avg:38.88ms
step:49/2330 train_time:1889ms step_avg:38.55ms
step:50/2330 train_time:1947ms step_avg:38.94ms
step:51/2330 train_time:1972ms step_avg:38.66ms
step:52/2330 train_time:2030ms step_avg:39.05ms
step:53/2330 train_time:2055ms step_avg:38.78ms
step:54/2330 train_time:2113ms step_avg:39.13ms
step:55/2330 train_time:2138ms step_avg:38.86ms
step:56/2330 train_time:2195ms step_avg:39.20ms
step:57/2330 train_time:2219ms step_avg:38.92ms
step:58/2330 train_time:2276ms step_avg:39.24ms
step:59/2330 train_time:2297ms step_avg:38.94ms
step:60/2330 train_time:2354ms step_avg:39.23ms
step:61/2330 train_time:2376ms step_avg:38.95ms
step:62/2330 train_time:2432ms step_avg:39.22ms
step:63/2330 train_time:2454ms step_avg:38.96ms
step:64/2330 train_time:2510ms step_avg:39.23ms
step:65/2330 train_time:2533ms step_avg:38.97ms
step:66/2330 train_time:2589ms step_avg:39.23ms
step:67/2330 train_time:2612ms step_avg:38.98ms
step:68/2330 train_time:2668ms step_avg:39.23ms
step:69/2330 train_time:2690ms step_avg:38.99ms
step:70/2330 train_time:2746ms step_avg:39.23ms
step:71/2330 train_time:2768ms step_avg:38.99ms
step:72/2330 train_time:2824ms step_avg:39.22ms
step:73/2330 train_time:2847ms step_avg:38.99ms
step:74/2330 train_time:2903ms step_avg:39.23ms
step:75/2330 train_time:2926ms step_avg:39.02ms
step:76/2330 train_time:2986ms step_avg:39.28ms
step:77/2330 train_time:3009ms step_avg:39.08ms
step:78/2330 train_time:3067ms step_avg:39.33ms
step:79/2330 train_time:3092ms step_avg:39.14ms
step:80/2330 train_time:3150ms step_avg:39.37ms
step:81/2330 train_time:3175ms step_avg:39.20ms
step:82/2330 train_time:3232ms step_avg:39.41ms
step:83/2330 train_time:3256ms step_avg:39.22ms
step:84/2330 train_time:3311ms step_avg:39.42ms
step:85/2330 train_time:3334ms step_avg:39.23ms
step:86/2330 train_time:3391ms step_avg:39.44ms
step:87/2330 train_time:3415ms step_avg:39.25ms
step:88/2330 train_time:3471ms step_avg:39.44ms
step:89/2330 train_time:3494ms step_avg:39.26ms
step:90/2330 train_time:3550ms step_avg:39.44ms
step:91/2330 train_time:3572ms step_avg:39.26ms
step:92/2330 train_time:3628ms step_avg:39.44ms
step:93/2330 train_time:3651ms step_avg:39.26ms
step:94/2330 train_time:3707ms step_avg:39.43ms
step:95/2330 train_time:3730ms step_avg:39.26ms
step:96/2330 train_time:3786ms step_avg:39.44ms
step:97/2330 train_time:3810ms step_avg:39.28ms
step:98/2330 train_time:3867ms step_avg:39.46ms
step:99/2330 train_time:3890ms step_avg:39.30ms
step:100/2330 train_time:3947ms step_avg:39.47ms
step:101/2330 train_time:3971ms step_avg:39.31ms
step:102/2330 train_time:4029ms step_avg:39.50ms
step:103/2330 train_time:4053ms step_avg:39.35ms
step:104/2330 train_time:4111ms step_avg:39.53ms
step:105/2330 train_time:4134ms step_avg:39.38ms
step:106/2330 train_time:4191ms step_avg:39.54ms
step:107/2330 train_time:4215ms step_avg:39.39ms
step:108/2330 train_time:4271ms step_avg:39.55ms
step:109/2330 train_time:4294ms step_avg:39.40ms
step:110/2330 train_time:4351ms step_avg:39.55ms
step:111/2330 train_time:4374ms step_avg:39.41ms
step:112/2330 train_time:4431ms step_avg:39.56ms
step:113/2330 train_time:4454ms step_avg:39.42ms
step:114/2330 train_time:4510ms step_avg:39.56ms
step:115/2330 train_time:4533ms step_avg:39.42ms
step:116/2330 train_time:4589ms step_avg:39.56ms
step:117/2330 train_time:4613ms step_avg:39.43ms
step:118/2330 train_time:4669ms step_avg:39.57ms
step:119/2330 train_time:4693ms step_avg:39.44ms
step:120/2330 train_time:4749ms step_avg:39.57ms
step:121/2330 train_time:4772ms step_avg:39.44ms
step:122/2330 train_time:4829ms step_avg:39.58ms
step:123/2330 train_time:4852ms step_avg:39.45ms
step:124/2330 train_time:4908ms step_avg:39.58ms
step:125/2330 train_time:4932ms step_avg:39.46ms
step:126/2330 train_time:4989ms step_avg:39.59ms
step:127/2330 train_time:5014ms step_avg:39.48ms
step:128/2330 train_time:5070ms step_avg:39.61ms
step:129/2330 train_time:5094ms step_avg:39.49ms
step:130/2330 train_time:5151ms step_avg:39.62ms
step:131/2330 train_time:5175ms step_avg:39.50ms
step:132/2330 train_time:5232ms step_avg:39.63ms
step:133/2330 train_time:5254ms step_avg:39.51ms
step:134/2330 train_time:5311ms step_avg:39.63ms
step:135/2330 train_time:5335ms step_avg:39.51ms
step:136/2330 train_time:5391ms step_avg:39.64ms
step:137/2330 train_time:5415ms step_avg:39.53ms
step:138/2330 train_time:5471ms step_avg:39.64ms
step:139/2330 train_time:5494ms step_avg:39.53ms
step:140/2330 train_time:5550ms step_avg:39.64ms
step:141/2330 train_time:5574ms step_avg:39.53ms
step:142/2330 train_time:5629ms step_avg:39.64ms
step:143/2330 train_time:5652ms step_avg:39.52ms
step:144/2330 train_time:5708ms step_avg:39.64ms
step:145/2330 train_time:5731ms step_avg:39.52ms
step:146/2330 train_time:5788ms step_avg:39.64ms
step:147/2330 train_time:5811ms step_avg:39.53ms
step:148/2330 train_time:5868ms step_avg:39.65ms
step:149/2330 train_time:5891ms step_avg:39.54ms
step:150/2330 train_time:5949ms step_avg:39.66ms
step:151/2330 train_time:5973ms step_avg:39.55ms
step:152/2330 train_time:6030ms step_avg:39.67ms
step:153/2330 train_time:6053ms step_avg:39.56ms
step:154/2330 train_time:6110ms step_avg:39.67ms
step:155/2330 train_time:6133ms step_avg:39.57ms
step:156/2330 train_time:6190ms step_avg:39.68ms
step:157/2330 train_time:6214ms step_avg:39.58ms
step:158/2330 train_time:6270ms step_avg:39.69ms
step:159/2330 train_time:6294ms step_avg:39.59ms
step:160/2330 train_time:6351ms step_avg:39.69ms
step:161/2330 train_time:6374ms step_avg:39.59ms
step:162/2330 train_time:6431ms step_avg:39.70ms
step:163/2330 train_time:6454ms step_avg:39.60ms
step:164/2330 train_time:6510ms step_avg:39.70ms
step:165/2330 train_time:6534ms step_avg:39.60ms
step:166/2330 train_time:6590ms step_avg:39.70ms
step:167/2330 train_time:6613ms step_avg:39.60ms
step:168/2330 train_time:6670ms step_avg:39.70ms
step:169/2330 train_time:6692ms step_avg:39.60ms
step:170/2330 train_time:6749ms step_avg:39.70ms
step:171/2330 train_time:6772ms step_avg:39.60ms
step:172/2330 train_time:6829ms step_avg:39.70ms
step:173/2330 train_time:6852ms step_avg:39.61ms
step:174/2330 train_time:6909ms step_avg:39.70ms
step:175/2330 train_time:6932ms step_avg:39.61ms
step:176/2330 train_time:6989ms step_avg:39.71ms
step:177/2330 train_time:7013ms step_avg:39.62ms
step:178/2330 train_time:7070ms step_avg:39.72ms
step:179/2330 train_time:7093ms step_avg:39.63ms
step:180/2330 train_time:7150ms step_avg:39.72ms
step:181/2330 train_time:7173ms step_avg:39.63ms
step:182/2330 train_time:7231ms step_avg:39.73ms
step:183/2330 train_time:7254ms step_avg:39.64ms
step:184/2330 train_time:7311ms step_avg:39.73ms
step:185/2330 train_time:7334ms step_avg:39.65ms
step:186/2330 train_time:7391ms step_avg:39.74ms
step:187/2330 train_time:7414ms step_avg:39.65ms
step:188/2330 train_time:7471ms step_avg:39.74ms
step:189/2330 train_time:7494ms step_avg:39.65ms
step:190/2330 train_time:7551ms step_avg:39.74ms
step:191/2330 train_time:7574ms step_avg:39.65ms
step:192/2330 train_time:7631ms step_avg:39.74ms
step:193/2330 train_time:7653ms step_avg:39.66ms
step:194/2330 train_time:7710ms step_avg:39.74ms
step:195/2330 train_time:7733ms step_avg:39.66ms
step:196/2330 train_time:7790ms step_avg:39.74ms
step:197/2330 train_time:7813ms step_avg:39.66ms
step:198/2330 train_time:7869ms step_avg:39.74ms
step:199/2330 train_time:7893ms step_avg:39.66ms
step:200/2330 train_time:7949ms step_avg:39.75ms
step:201/2330 train_time:7973ms step_avg:39.67ms
step:202/2330 train_time:8029ms step_avg:39.75ms
step:203/2330 train_time:8053ms step_avg:39.67ms
step:204/2330 train_time:8109ms step_avg:39.75ms
step:205/2330 train_time:8132ms step_avg:39.67ms
step:206/2330 train_time:8189ms step_avg:39.75ms
step:207/2330 train_time:8213ms step_avg:39.68ms
step:208/2330 train_time:8269ms step_avg:39.76ms
step:209/2330 train_time:8293ms step_avg:39.68ms
step:210/2330 train_time:8350ms step_avg:39.76ms
step:211/2330 train_time:8373ms step_avg:39.68ms
step:212/2330 train_time:8431ms step_avg:39.77ms
step:213/2330 train_time:8454ms step_avg:39.69ms
step:214/2330 train_time:8510ms step_avg:39.77ms
step:215/2330 train_time:8533ms step_avg:39.69ms
step:216/2330 train_time:8589ms step_avg:39.77ms
step:217/2330 train_time:8613ms step_avg:39.69ms
step:218/2330 train_time:8669ms step_avg:39.77ms
step:219/2330 train_time:8692ms step_avg:39.69ms
step:220/2330 train_time:8749ms step_avg:39.77ms
step:221/2330 train_time:8772ms step_avg:39.69ms
step:222/2330 train_time:8829ms step_avg:39.77ms
step:223/2330 train_time:8853ms step_avg:39.70ms
step:224/2330 train_time:8910ms step_avg:39.78ms
step:225/2330 train_time:8934ms step_avg:39.71ms
step:226/2330 train_time:8991ms step_avg:39.78ms
step:227/2330 train_time:9015ms step_avg:39.71ms
step:228/2330 train_time:9072ms step_avg:39.79ms
step:229/2330 train_time:9096ms step_avg:39.72ms
step:230/2330 train_time:9152ms step_avg:39.79ms
step:231/2330 train_time:9175ms step_avg:39.72ms
step:232/2330 train_time:9232ms step_avg:39.79ms
step:233/2330 train_time:9256ms step_avg:39.72ms
step:234/2330 train_time:9313ms step_avg:39.80ms
step:235/2330 train_time:9336ms step_avg:39.73ms
step:236/2330 train_time:9393ms step_avg:39.80ms
step:237/2330 train_time:9416ms step_avg:39.73ms
step:238/2330 train_time:9471ms step_avg:39.80ms
step:239/2330 train_time:9494ms step_avg:39.73ms
step:240/2330 train_time:9551ms step_avg:39.80ms
step:241/2330 train_time:9574ms step_avg:39.72ms
step:242/2330 train_time:9630ms step_avg:39.79ms
step:243/2330 train_time:9653ms step_avg:39.72ms
step:244/2330 train_time:9709ms step_avg:39.79ms
step:245/2330 train_time:9732ms step_avg:39.72ms
step:246/2330 train_time:9789ms step_avg:39.79ms
step:247/2330 train_time:9812ms step_avg:39.72ms
step:248/2330 train_time:9869ms step_avg:39.79ms
step:249/2330 train_time:9892ms step_avg:39.73ms
step:250/2330 train_time:9949ms step_avg:39.80ms
step:250/2330 val_loss:5.7444 train_time:10048ms step_avg:40.19ms
step:251/2330 train_time:10060ms step_avg:40.08ms
step:252/2330 train_time:10072ms step_avg:39.97ms
step:253/2330 train_time:10082ms step_avg:39.85ms
step:254/2330 train_time:10111ms step_avg:39.81ms
step:255/2330 train_time:10133ms step_avg:39.74ms
step:256/2330 train_time:10189ms step_avg:39.80ms
step:257/2330 train_time:10211ms step_avg:39.73ms
step:258/2330 train_time:10268ms step_avg:39.80ms
step:259/2330 train_time:10290ms step_avg:39.73ms
step:260/2330 train_time:10346ms step_avg:39.79ms
step:261/2330 train_time:10371ms step_avg:39.74ms
step:262/2330 train_time:10432ms step_avg:39.82ms
step:263/2330 train_time:10457ms step_avg:39.76ms
step:264/2330 train_time:10515ms step_avg:39.83ms
step:265/2330 train_time:10540ms step_avg:39.77ms
step:266/2330 train_time:10596ms step_avg:39.83ms
step:267/2330 train_time:10619ms step_avg:39.77ms
step:268/2330 train_time:10676ms step_avg:39.83ms
step:269/2330 train_time:10698ms step_avg:39.77ms
step:270/2330 train_time:10755ms step_avg:39.83ms
step:271/2330 train_time:10778ms step_avg:39.77ms
step:272/2330 train_time:10834ms step_avg:39.83ms
step:273/2330 train_time:10857ms step_avg:39.77ms
step:274/2330 train_time:10915ms step_avg:39.84ms
step:275/2330 train_time:10940ms step_avg:39.78ms
step:276/2330 train_time:11002ms step_avg:39.86ms
step:277/2330 train_time:11027ms step_avg:39.81ms
step:278/2330 train_time:11085ms step_avg:39.87ms
step:279/2330 train_time:11108ms step_avg:39.81ms
step:280/2330 train_time:11167ms step_avg:39.88ms
step:281/2330 train_time:11189ms step_avg:39.82ms
step:282/2330 train_time:11245ms step_avg:39.88ms
step:283/2330 train_time:11267ms step_avg:39.81ms
step:284/2330 train_time:11324ms step_avg:39.87ms
step:285/2330 train_time:11346ms step_avg:39.81ms
step:286/2330 train_time:11403ms step_avg:39.87ms
step:287/2330 train_time:11426ms step_avg:39.81ms
step:288/2330 train_time:11482ms step_avg:39.87ms
step:289/2330 train_time:11506ms step_avg:39.81ms
step:290/2330 train_time:11561ms step_avg:39.87ms
step:291/2330 train_time:11585ms step_avg:39.81ms
step:292/2330 train_time:11640ms step_avg:39.86ms
step:293/2330 train_time:11663ms step_avg:39.81ms
step:294/2330 train_time:11719ms step_avg:39.86ms
step:295/2330 train_time:11742ms step_avg:39.80ms
step:296/2330 train_time:11798ms step_avg:39.86ms
step:297/2330 train_time:11821ms step_avg:39.80ms
step:298/2330 train_time:11879ms step_avg:39.86ms
step:299/2330 train_time:11903ms step_avg:39.81ms
step:300/2330 train_time:11960ms step_avg:39.87ms
step:301/2330 train_time:11984ms step_avg:39.81ms
step:302/2330 train_time:12042ms step_avg:39.87ms
step:303/2330 train_time:12067ms step_avg:39.82ms
step:304/2330 train_time:12123ms step_avg:39.88ms
step:305/2330 train_time:12147ms step_avg:39.83ms
step:306/2330 train_time:12203ms step_avg:39.88ms
step:307/2330 train_time:12227ms step_avg:39.83ms
step:308/2330 train_time:12282ms step_avg:39.88ms
step:309/2330 train_time:12306ms step_avg:39.82ms
step:310/2330 train_time:12363ms step_avg:39.88ms
step:311/2330 train_time:12386ms step_avg:39.83ms
step:312/2330 train_time:12442ms step_avg:39.88ms
step:313/2330 train_time:12465ms step_avg:39.83ms
step:314/2330 train_time:12523ms step_avg:39.88ms
step:315/2330 train_time:12546ms step_avg:39.83ms
step:316/2330 train_time:12602ms step_avg:39.88ms
step:317/2330 train_time:12625ms step_avg:39.83ms
step:318/2330 train_time:12681ms step_avg:39.88ms
step:319/2330 train_time:12703ms step_avg:39.82ms
step:320/2330 train_time:12759ms step_avg:39.87ms
step:321/2330 train_time:12782ms step_avg:39.82ms
step:322/2330 train_time:12839ms step_avg:39.87ms
step:323/2330 train_time:12862ms step_avg:39.82ms
step:324/2330 train_time:12919ms step_avg:39.87ms
step:325/2330 train_time:12944ms step_avg:39.83ms
step:326/2330 train_time:13001ms step_avg:39.88ms
step:327/2330 train_time:13025ms step_avg:39.83ms
step:328/2330 train_time:13082ms step_avg:39.88ms
step:329/2330 train_time:13106ms step_avg:39.83ms
step:330/2330 train_time:13162ms step_avg:39.89ms
step:331/2330 train_time:13186ms step_avg:39.84ms
step:332/2330 train_time:13244ms step_avg:39.89ms
step:333/2330 train_time:13267ms step_avg:39.84ms
step:334/2330 train_time:13324ms step_avg:39.89ms
step:335/2330 train_time:13347ms step_avg:39.84ms
step:336/2330 train_time:13403ms step_avg:39.89ms
step:337/2330 train_time:13427ms step_avg:39.84ms
step:338/2330 train_time:13483ms step_avg:39.89ms
step:339/2330 train_time:13506ms step_avg:39.84ms
step:340/2330 train_time:13562ms step_avg:39.89ms
step:341/2330 train_time:13585ms step_avg:39.84ms
step:342/2330 train_time:13641ms step_avg:39.89ms
step:343/2330 train_time:13664ms step_avg:39.84ms
step:344/2330 train_time:13720ms step_avg:39.89ms
step:345/2330 train_time:13744ms step_avg:39.84ms
step:346/2330 train_time:13801ms step_avg:39.89ms
step:347/2330 train_time:13824ms step_avg:39.84ms
step:348/2330 train_time:13881ms step_avg:39.89ms
step:349/2330 train_time:13904ms step_avg:39.84ms
step:350/2330 train_time:13962ms step_avg:39.89ms
step:351/2330 train_time:13985ms step_avg:39.84ms
step:352/2330 train_time:14041ms step_avg:39.89ms
step:353/2330 train_time:14065ms step_avg:39.85ms
step:354/2330 train_time:14122ms step_avg:39.89ms
step:355/2330 train_time:14145ms step_avg:39.85ms
step:356/2330 train_time:14202ms step_avg:39.89ms
step:357/2330 train_time:14226ms step_avg:39.85ms
step:358/2330 train_time:14283ms step_avg:39.90ms
step:359/2330 train_time:14306ms step_avg:39.85ms
step:360/2330 train_time:14362ms step_avg:39.90ms
step:361/2330 train_time:14386ms step_avg:39.85ms
step:362/2330 train_time:14442ms step_avg:39.90ms
step:363/2330 train_time:14465ms step_avg:39.85ms
step:364/2330 train_time:14522ms step_avg:39.90ms
step:365/2330 train_time:14546ms step_avg:39.85ms
step:366/2330 train_time:14601ms step_avg:39.89ms
step:367/2330 train_time:14624ms step_avg:39.85ms
step:368/2330 train_time:14681ms step_avg:39.89ms
step:369/2330 train_time:14704ms step_avg:39.85ms
step:370/2330 train_time:14761ms step_avg:39.89ms
step:371/2330 train_time:14785ms step_avg:39.85ms
step:372/2330 train_time:14841ms step_avg:39.89ms
step:373/2330 train_time:14864ms step_avg:39.85ms
step:374/2330 train_time:14921ms step_avg:39.89ms
step:375/2330 train_time:14945ms step_avg:39.85ms
step:376/2330 train_time:15001ms step_avg:39.90ms
step:377/2330 train_time:15025ms step_avg:39.85ms
step:378/2330 train_time:15082ms step_avg:39.90ms
step:379/2330 train_time:15105ms step_avg:39.86ms
step:380/2330 train_time:15162ms step_avg:39.90ms
step:381/2330 train_time:15185ms step_avg:39.86ms
step:382/2330 train_time:15242ms step_avg:39.90ms
step:383/2330 train_time:15265ms step_avg:39.86ms
step:384/2330 train_time:15322ms step_avg:39.90ms
step:385/2330 train_time:15345ms step_avg:39.86ms
step:386/2330 train_time:15402ms step_avg:39.90ms
step:387/2330 train_time:15425ms step_avg:39.86ms
step:388/2330 train_time:15482ms step_avg:39.90ms
step:389/2330 train_time:15506ms step_avg:39.86ms
step:390/2330 train_time:15562ms step_avg:39.90ms
step:391/2330 train_time:15585ms step_avg:39.86ms
step:392/2330 train_time:15642ms step_avg:39.90ms
step:393/2330 train_time:15665ms step_avg:39.86ms
step:394/2330 train_time:15721ms step_avg:39.90ms
step:395/2330 train_time:15745ms step_avg:39.86ms
step:396/2330 train_time:15801ms step_avg:39.90ms
step:397/2330 train_time:15825ms step_avg:39.86ms
step:398/2330 train_time:15882ms step_avg:39.90ms
step:399/2330 train_time:15905ms step_avg:39.86ms
step:400/2330 train_time:15961ms step_avg:39.90ms
step:401/2330 train_time:15985ms step_avg:39.86ms
step:402/2330 train_time:16041ms step_avg:39.90ms
step:403/2330 train_time:16065ms step_avg:39.86ms
step:404/2330 train_time:16121ms step_avg:39.90ms
step:405/2330 train_time:16144ms step_avg:39.86ms
step:406/2330 train_time:16201ms step_avg:39.90ms
step:407/2330 train_time:16225ms step_avg:39.86ms
step:408/2330 train_time:16282ms step_avg:39.91ms
step:409/2330 train_time:16305ms step_avg:39.87ms
step:410/2330 train_time:16361ms step_avg:39.91ms
step:411/2330 train_time:16385ms step_avg:39.87ms
step:412/2330 train_time:16441ms step_avg:39.91ms
step:413/2330 train_time:16465ms step_avg:39.87ms
step:414/2330 train_time:16521ms step_avg:39.91ms
step:415/2330 train_time:16544ms step_avg:39.86ms
step:416/2330 train_time:16601ms step_avg:39.91ms
step:417/2330 train_time:16624ms step_avg:39.87ms
step:418/2330 train_time:16681ms step_avg:39.91ms
step:419/2330 train_time:16704ms step_avg:39.87ms
step:420/2330 train_time:16760ms step_avg:39.91ms
step:421/2330 train_time:16784ms step_avg:39.87ms
step:422/2330 train_time:16841ms step_avg:39.91ms
step:423/2330 train_time:16864ms step_avg:39.87ms
step:424/2330 train_time:16921ms step_avg:39.91ms
step:425/2330 train_time:16944ms step_avg:39.87ms
step:426/2330 train_time:17000ms step_avg:39.91ms
step:427/2330 train_time:17024ms step_avg:39.87ms
step:428/2330 train_time:17081ms step_avg:39.91ms
step:429/2330 train_time:17105ms step_avg:39.87ms
step:430/2330 train_time:17161ms step_avg:39.91ms
step:431/2330 train_time:17184ms step_avg:39.87ms
step:432/2330 train_time:17241ms step_avg:39.91ms
step:433/2330 train_time:17264ms step_avg:39.87ms
step:434/2330 train_time:17321ms step_avg:39.91ms
step:435/2330 train_time:17346ms step_avg:39.87ms
step:436/2330 train_time:17402ms step_avg:39.91ms
step:437/2330 train_time:17426ms step_avg:39.88ms
step:438/2330 train_time:17482ms step_avg:39.91ms
step:439/2330 train_time:17504ms step_avg:39.87ms
step:440/2330 train_time:17561ms step_avg:39.91ms
step:441/2330 train_time:17585ms step_avg:39.87ms
step:442/2330 train_time:17640ms step_avg:39.91ms
step:443/2330 train_time:17664ms step_avg:39.87ms
step:444/2330 train_time:17721ms step_avg:39.91ms
step:445/2330 train_time:17745ms step_avg:39.88ms
step:446/2330 train_time:17801ms step_avg:39.91ms
step:447/2330 train_time:17825ms step_avg:39.88ms
step:448/2330 train_time:17881ms step_avg:39.91ms
step:449/2330 train_time:17905ms step_avg:39.88ms
step:450/2330 train_time:17961ms step_avg:39.91ms
step:451/2330 train_time:17985ms step_avg:39.88ms
step:452/2330 train_time:18041ms step_avg:39.91ms
step:453/2330 train_time:18064ms step_avg:39.88ms
step:454/2330 train_time:18121ms step_avg:39.91ms
step:455/2330 train_time:18144ms step_avg:39.88ms
step:456/2330 train_time:18201ms step_avg:39.92ms
step:457/2330 train_time:18224ms step_avg:39.88ms
step:458/2330 train_time:18281ms step_avg:39.92ms
step:459/2330 train_time:18305ms step_avg:39.88ms
step:460/2330 train_time:18361ms step_avg:39.92ms
step:461/2330 train_time:18385ms step_avg:39.88ms
step:462/2330 train_time:18442ms step_avg:39.92ms
step:463/2330 train_time:18465ms step_avg:39.88ms
step:464/2330 train_time:18521ms step_avg:39.92ms
step:465/2330 train_time:18544ms step_avg:39.88ms
step:466/2330 train_time:18600ms step_avg:39.91ms
step:467/2330 train_time:18623ms step_avg:39.88ms
step:468/2330 train_time:18680ms step_avg:39.91ms
step:469/2330 train_time:18703ms step_avg:39.88ms
step:470/2330 train_time:18760ms step_avg:39.91ms
step:471/2330 train_time:18783ms step_avg:39.88ms
step:472/2330 train_time:18840ms step_avg:39.92ms
step:473/2330 train_time:18863ms step_avg:39.88ms
step:474/2330 train_time:18920ms step_avg:39.91ms
step:475/2330 train_time:18943ms step_avg:39.88ms
step:476/2330 train_time:19000ms step_avg:39.92ms
step:477/2330 train_time:19024ms step_avg:39.88ms
step:478/2330 train_time:19080ms step_avg:39.92ms
step:479/2330 train_time:19104ms step_avg:39.88ms
step:480/2330 train_time:19160ms step_avg:39.92ms
step:481/2330 train_time:19183ms step_avg:39.88ms
step:482/2330 train_time:19240ms step_avg:39.92ms
step:483/2330 train_time:19264ms step_avg:39.88ms
step:484/2330 train_time:19321ms step_avg:39.92ms
step:485/2330 train_time:19344ms step_avg:39.89ms
step:486/2330 train_time:19400ms step_avg:39.92ms
step:487/2330 train_time:19424ms step_avg:39.88ms
step:488/2330 train_time:19480ms step_avg:39.92ms
step:489/2330 train_time:19504ms step_avg:39.89ms
step:490/2330 train_time:19561ms step_avg:39.92ms
step:491/2330 train_time:19584ms step_avg:39.89ms
step:492/2330 train_time:19641ms step_avg:39.92ms
step:493/2330 train_time:19664ms step_avg:39.89ms
step:494/2330 train_time:19721ms step_avg:39.92ms
step:495/2330 train_time:19745ms step_avg:39.89ms
step:496/2330 train_time:19801ms step_avg:39.92ms
step:497/2330 train_time:19825ms step_avg:39.89ms
step:498/2330 train_time:19881ms step_avg:39.92ms
step:499/2330 train_time:19905ms step_avg:39.89ms
step:500/2330 train_time:19962ms step_avg:39.92ms
step:500/2330 val_loss:5.6431 train_time:20060ms step_avg:40.12ms
step:501/2330 train_time:20072ms step_avg:40.06ms
step:502/2330 train_time:20083ms step_avg:40.01ms
step:503/2330 train_time:20093ms step_avg:39.95ms
step:504/2330 train_time:20123ms step_avg:39.93ms
step:505/2330 train_time:20145ms step_avg:39.89ms
step:506/2330 train_time:20201ms step_avg:39.92ms
step:507/2330 train_time:20224ms step_avg:39.89ms
step:508/2330 train_time:20281ms step_avg:39.92ms
step:509/2330 train_time:20303ms step_avg:39.89ms
step:510/2330 train_time:20362ms step_avg:39.93ms
step:511/2330 train_time:20389ms step_avg:39.90ms
step:512/2330 train_time:20451ms step_avg:39.94ms
step:513/2330 train_time:20474ms step_avg:39.91ms
step:514/2330 train_time:20530ms step_avg:39.94ms
step:515/2330 train_time:20553ms step_avg:39.91ms
step:516/2330 train_time:20609ms step_avg:39.94ms
step:517/2330 train_time:20634ms step_avg:39.91ms
step:518/2330 train_time:20690ms step_avg:39.94ms
step:519/2330 train_time:20713ms step_avg:39.91ms
step:520/2330 train_time:20769ms step_avg:39.94ms
step:521/2330 train_time:20792ms step_avg:39.91ms
step:522/2330 train_time:20847ms step_avg:39.94ms
step:523/2330 train_time:20870ms step_avg:39.91ms
step:524/2330 train_time:20926ms step_avg:39.94ms
step:525/2330 train_time:20949ms step_avg:39.90ms
step:526/2330 train_time:21007ms step_avg:39.94ms
step:527/2330 train_time:21032ms step_avg:39.91ms
step:528/2330 train_time:21088ms step_avg:39.94ms
step:529/2330 train_time:21111ms step_avg:39.91ms
step:530/2330 train_time:21167ms step_avg:39.94ms
step:531/2330 train_time:21191ms step_avg:39.91ms
step:532/2330 train_time:21247ms step_avg:39.94ms
step:533/2330 train_time:21270ms step_avg:39.91ms
step:534/2330 train_time:21327ms step_avg:39.94ms
step:535/2330 train_time:21353ms step_avg:39.91ms
step:536/2330 train_time:21410ms step_avg:39.94ms
step:537/2330 train_time:21434ms step_avg:39.92ms
step:538/2330 train_time:21491ms step_avg:39.95ms
step:539/2330 train_time:21514ms step_avg:39.91ms
step:540/2330 train_time:21571ms step_avg:39.95ms
step:541/2330 train_time:21594ms step_avg:39.92ms
step:542/2330 train_time:21651ms step_avg:39.95ms
step:543/2330 train_time:21674ms step_avg:39.91ms
step:544/2330 train_time:21730ms step_avg:39.95ms
step:545/2330 train_time:21753ms step_avg:39.91ms
step:546/2330 train_time:21809ms step_avg:39.94ms
step:547/2330 train_time:21831ms step_avg:39.91ms
step:548/2330 train_time:21888ms step_avg:39.94ms
step:549/2330 train_time:21911ms step_avg:39.91ms
step:550/2330 train_time:21968ms step_avg:39.94ms
step:551/2330 train_time:21993ms step_avg:39.91ms
step:552/2330 train_time:22049ms step_avg:39.94ms
step:553/2330 train_time:22072ms step_avg:39.91ms
step:554/2330 train_time:22128ms step_avg:39.94ms
step:555/2330 train_time:22152ms step_avg:39.91ms
step:556/2330 train_time:22209ms step_avg:39.94ms
step:557/2330 train_time:22233ms step_avg:39.91ms
step:558/2330 train_time:22290ms step_avg:39.95ms
step:559/2330 train_time:22313ms step_avg:39.92ms
step:560/2330 train_time:22370ms step_avg:39.95ms
step:561/2330 train_time:22395ms step_avg:39.92ms
step:562/2330 train_time:22451ms step_avg:39.95ms
step:563/2330 train_time:22475ms step_avg:39.92ms
step:564/2330 train_time:22531ms step_avg:39.95ms
step:565/2330 train_time:22555ms step_avg:39.92ms
step:566/2330 train_time:22613ms step_avg:39.95ms
step:567/2330 train_time:22635ms step_avg:39.92ms
step:568/2330 train_time:22693ms step_avg:39.95ms
step:569/2330 train_time:22715ms step_avg:39.92ms
step:570/2330 train_time:22772ms step_avg:39.95ms
step:571/2330 train_time:22795ms step_avg:39.92ms
step:572/2330 train_time:22852ms step_avg:39.95ms
step:573/2330 train_time:22874ms step_avg:39.92ms
step:574/2330 train_time:22930ms step_avg:39.95ms
step:575/2330 train_time:22953ms step_avg:39.92ms
step:576/2330 train_time:23010ms step_avg:39.95ms
step:577/2330 train_time:23033ms step_avg:39.92ms
step:578/2330 train_time:23089ms step_avg:39.95ms
step:579/2330 train_time:23113ms step_avg:39.92ms
step:580/2330 train_time:23169ms step_avg:39.95ms
step:581/2330 train_time:23193ms step_avg:39.92ms
step:582/2330 train_time:23249ms step_avg:39.95ms
step:583/2330 train_time:23273ms step_avg:39.92ms
step:584/2330 train_time:23330ms step_avg:39.95ms
step:585/2330 train_time:23353ms step_avg:39.92ms
step:586/2330 train_time:23409ms step_avg:39.95ms
step:587/2330 train_time:23433ms step_avg:39.92ms
step:588/2330 train_time:23490ms step_avg:39.95ms
step:589/2330 train_time:23514ms step_avg:39.92ms
step:590/2330 train_time:23570ms step_avg:39.95ms
step:591/2330 train_time:23594ms step_avg:39.92ms
step:592/2330 train_time:23650ms step_avg:39.95ms
step:593/2330 train_time:23673ms step_avg:39.92ms
step:594/2330 train_time:23730ms step_avg:39.95ms
step:595/2330 train_time:23754ms step_avg:39.92ms
step:596/2330 train_time:23811ms step_avg:39.95ms
step:597/2330 train_time:23834ms step_avg:39.92ms
step:598/2330 train_time:23890ms step_avg:39.95ms
step:599/2330 train_time:23914ms step_avg:39.92ms
step:600/2330 train_time:23971ms step_avg:39.95ms
step:601/2330 train_time:23995ms step_avg:39.92ms
step:602/2330 train_time:24052ms step_avg:39.95ms
step:603/2330 train_time:24076ms step_avg:39.93ms
step:604/2330 train_time:24134ms step_avg:39.96ms
step:605/2330 train_time:24157ms step_avg:39.93ms
step:606/2330 train_time:24214ms step_avg:39.96ms
step:607/2330 train_time:24237ms step_avg:39.93ms
step:608/2330 train_time:24295ms step_avg:39.96ms
step:609/2330 train_time:24317ms step_avg:39.93ms
step:610/2330 train_time:24375ms step_avg:39.96ms
step:611/2330 train_time:24397ms step_avg:39.93ms
step:612/2330 train_time:24454ms step_avg:39.96ms
step:613/2330 train_time:24478ms step_avg:39.93ms
step:614/2330 train_time:24535ms step_avg:39.96ms
step:615/2330 train_time:24558ms step_avg:39.93ms
step:616/2330 train_time:24615ms step_avg:39.96ms
step:617/2330 train_time:24637ms step_avg:39.93ms
step:618/2330 train_time:24695ms step_avg:39.96ms
step:619/2330 train_time:24718ms step_avg:39.93ms
step:620/2330 train_time:24776ms step_avg:39.96ms
step:621/2330 train_time:24799ms step_avg:39.93ms
step:622/2330 train_time:24857ms step_avg:39.96ms
step:623/2330 train_time:24880ms step_avg:39.94ms
step:624/2330 train_time:24939ms step_avg:39.97ms
step:625/2330 train_time:24961ms step_avg:39.94ms
step:626/2330 train_time:25019ms step_avg:39.97ms
step:627/2330 train_time:25042ms step_avg:39.94ms
step:628/2330 train_time:25101ms step_avg:39.97ms
step:629/2330 train_time:25124ms step_avg:39.94ms
step:630/2330 train_time:25182ms step_avg:39.97ms
step:631/2330 train_time:25205ms step_avg:39.94ms
step:632/2330 train_time:25262ms step_avg:39.97ms
step:633/2330 train_time:25285ms step_avg:39.94ms
step:634/2330 train_time:25343ms step_avg:39.97ms
step:635/2330 train_time:25365ms step_avg:39.95ms
step:636/2330 train_time:25423ms step_avg:39.97ms
step:637/2330 train_time:25446ms step_avg:39.95ms
step:638/2330 train_time:25502ms step_avg:39.97ms
step:639/2330 train_time:25527ms step_avg:39.95ms
step:640/2330 train_time:25584ms step_avg:39.98ms
step:641/2330 train_time:25607ms step_avg:39.95ms
step:642/2330 train_time:25664ms step_avg:39.98ms
step:643/2330 train_time:25687ms step_avg:39.95ms
step:644/2330 train_time:25745ms step_avg:39.98ms
step:645/2330 train_time:25769ms step_avg:39.95ms
step:646/2330 train_time:25825ms step_avg:39.98ms
step:647/2330 train_time:25849ms step_avg:39.95ms
step:648/2330 train_time:25906ms step_avg:39.98ms
step:649/2330 train_time:25929ms step_avg:39.95ms
step:650/2330 train_time:25986ms step_avg:39.98ms
step:651/2330 train_time:26009ms step_avg:39.95ms
step:652/2330 train_time:26065ms step_avg:39.98ms
step:653/2330 train_time:26089ms step_avg:39.95ms
step:654/2330 train_time:26146ms step_avg:39.98ms
step:655/2330 train_time:26170ms step_avg:39.95ms
step:656/2330 train_time:26226ms step_avg:39.98ms
step:657/2330 train_time:26250ms step_avg:39.95ms
step:658/2330 train_time:26307ms step_avg:39.98ms
step:659/2330 train_time:26331ms step_avg:39.96ms
step:660/2330 train_time:26387ms step_avg:39.98ms
step:661/2330 train_time:26410ms step_avg:39.95ms
step:662/2330 train_time:26467ms step_avg:39.98ms
step:663/2330 train_time:26491ms step_avg:39.96ms
step:664/2330 train_time:26548ms step_avg:39.98ms
step:665/2330 train_time:26572ms step_avg:39.96ms
step:666/2330 train_time:26629ms step_avg:39.98ms
step:667/2330 train_time:26652ms step_avg:39.96ms
step:668/2330 train_time:26709ms step_avg:39.98ms
step:669/2330 train_time:26733ms step_avg:39.96ms
step:670/2330 train_time:26789ms step_avg:39.98ms
step:671/2330 train_time:26813ms step_avg:39.96ms
step:672/2330 train_time:26871ms step_avg:39.99ms
step:673/2330 train_time:26894ms step_avg:39.96ms
step:674/2330 train_time:26950ms step_avg:39.99ms
step:675/2330 train_time:26974ms step_avg:39.96ms
step:676/2330 train_time:27031ms step_avg:39.99ms
step:677/2330 train_time:27055ms step_avg:39.96ms
step:678/2330 train_time:27112ms step_avg:39.99ms
step:679/2330 train_time:27136ms step_avg:39.96ms
step:680/2330 train_time:27194ms step_avg:39.99ms
step:681/2330 train_time:27216ms step_avg:39.97ms
step:682/2330 train_time:27275ms step_avg:39.99ms
step:683/2330 train_time:27297ms step_avg:39.97ms
step:684/2330 train_time:27354ms step_avg:39.99ms
step:685/2330 train_time:27377ms step_avg:39.97ms
step:686/2330 train_time:27435ms step_avg:39.99ms
step:687/2330 train_time:27458ms step_avg:39.97ms
step:688/2330 train_time:27515ms step_avg:39.99ms
step:689/2330 train_time:27538ms step_avg:39.97ms
step:690/2330 train_time:27595ms step_avg:39.99ms
step:691/2330 train_time:27618ms step_avg:39.97ms
step:692/2330 train_time:27677ms step_avg:40.00ms
step:693/2330 train_time:27700ms step_avg:39.97ms
step:694/2330 train_time:27758ms step_avg:40.00ms
step:695/2330 train_time:27782ms step_avg:39.97ms
step:696/2330 train_time:27839ms step_avg:40.00ms
step:697/2330 train_time:27863ms step_avg:39.98ms
step:698/2330 train_time:27921ms step_avg:40.00ms
step:699/2330 train_time:27945ms step_avg:39.98ms
step:700/2330 train_time:28002ms step_avg:40.00ms
step:701/2330 train_time:28025ms step_avg:39.98ms
step:702/2330 train_time:28083ms step_avg:40.00ms
step:703/2330 train_time:28107ms step_avg:39.98ms
step:704/2330 train_time:28164ms step_avg:40.00ms
step:705/2330 train_time:28188ms step_avg:39.98ms
step:706/2330 train_time:28246ms step_avg:40.01ms
step:707/2330 train_time:28269ms step_avg:39.98ms
step:708/2330 train_time:28325ms step_avg:40.01ms
step:709/2330 train_time:28349ms step_avg:39.98ms
step:710/2330 train_time:28406ms step_avg:40.01ms
step:711/2330 train_time:28430ms step_avg:39.99ms
step:712/2330 train_time:28487ms step_avg:40.01ms
step:713/2330 train_time:28511ms step_avg:39.99ms
step:714/2330 train_time:28568ms step_avg:40.01ms
step:715/2330 train_time:28591ms step_avg:39.99ms
step:716/2330 train_time:28649ms step_avg:40.01ms
step:717/2330 train_time:28673ms step_avg:39.99ms
step:718/2330 train_time:28729ms step_avg:40.01ms
step:719/2330 train_time:28752ms step_avg:39.99ms
step:720/2330 train_time:28809ms step_avg:40.01ms
step:721/2330 train_time:28833ms step_avg:39.99ms
step:722/2330 train_time:28890ms step_avg:40.01ms
step:723/2330 train_time:28914ms step_avg:39.99ms
step:724/2330 train_time:28971ms step_avg:40.01ms
step:725/2330 train_time:28995ms step_avg:39.99ms
step:726/2330 train_time:29051ms step_avg:40.02ms
step:727/2330 train_time:29075ms step_avg:39.99ms
step:728/2330 train_time:29132ms step_avg:40.02ms
step:729/2330 train_time:29155ms step_avg:39.99ms
step:730/2330 train_time:29212ms step_avg:40.02ms
step:731/2330 train_time:29236ms step_avg:39.99ms
step:732/2330 train_time:29293ms step_avg:40.02ms
step:733/2330 train_time:29316ms step_avg:39.99ms
step:734/2330 train_time:29373ms step_avg:40.02ms
step:735/2330 train_time:29396ms step_avg:39.99ms
step:736/2330 train_time:29453ms step_avg:40.02ms
step:737/2330 train_time:29477ms step_avg:40.00ms
step:738/2330 train_time:29535ms step_avg:40.02ms
step:739/2330 train_time:29557ms step_avg:40.00ms
step:740/2330 train_time:29615ms step_avg:40.02ms
step:741/2330 train_time:29637ms step_avg:40.00ms
step:742/2330 train_time:29694ms step_avg:40.02ms
step:743/2330 train_time:29717ms step_avg:40.00ms
step:744/2330 train_time:29776ms step_avg:40.02ms
step:745/2330 train_time:29798ms step_avg:40.00ms
step:746/2330 train_time:29855ms step_avg:40.02ms
step:747/2330 train_time:29878ms step_avg:40.00ms
step:748/2330 train_time:29936ms step_avg:40.02ms
step:749/2330 train_time:29960ms step_avg:40.00ms
step:750/2330 train_time:30017ms step_avg:40.02ms
step:750/2330 val_loss:5.5660 train_time:30116ms step_avg:40.15ms
step:751/2330 train_time:30128ms step_avg:40.12ms
step:752/2330 train_time:30139ms step_avg:40.08ms
step:753/2330 train_time:30150ms step_avg:40.04ms
step:754/2330 train_time:30179ms step_avg:40.03ms
step:755/2330 train_time:30201ms step_avg:40.00ms
step:756/2330 train_time:30258ms step_avg:40.02ms
step:757/2330 train_time:30280ms step_avg:40.00ms
step:758/2330 train_time:30337ms step_avg:40.02ms
step:759/2330 train_time:30360ms step_avg:40.00ms
step:760/2330 train_time:30420ms step_avg:40.03ms
step:761/2330 train_time:30446ms step_avg:40.01ms
step:762/2330 train_time:30507ms step_avg:40.04ms
step:763/2330 train_time:30531ms step_avg:40.01ms
step:764/2330 train_time:30589ms step_avg:40.04ms
step:765/2330 train_time:30613ms step_avg:40.02ms
step:766/2330 train_time:30670ms step_avg:40.04ms
step:767/2330 train_time:30692ms step_avg:40.02ms
step:768/2330 train_time:30749ms step_avg:40.04ms
step:769/2330 train_time:30772ms step_avg:40.02ms
step:770/2330 train_time:30828ms step_avg:40.04ms
step:771/2330 train_time:30851ms step_avg:40.01ms
step:772/2330 train_time:30907ms step_avg:40.04ms
step:773/2330 train_time:30930ms step_avg:40.01ms
step:774/2330 train_time:30987ms step_avg:40.03ms
step:775/2330 train_time:31010ms step_avg:40.01ms
step:776/2330 train_time:31068ms step_avg:40.04ms
step:777/2330 train_time:31092ms step_avg:40.02ms
step:778/2330 train_time:31149ms step_avg:40.04ms
step:779/2330 train_time:31172ms step_avg:40.02ms
step:780/2330 train_time:31228ms step_avg:40.04ms
step:781/2330 train_time:31252ms step_avg:40.02ms
step:782/2330 train_time:31308ms step_avg:40.04ms
step:783/2330 train_time:31332ms step_avg:40.02ms
step:784/2330 train_time:31388ms step_avg:40.04ms
step:785/2330 train_time:31413ms step_avg:40.02ms
step:786/2330 train_time:31469ms step_avg:40.04ms
step:787/2330 train_time:31493ms step_avg:40.02ms
step:788/2330 train_time:31549ms step_avg:40.04ms
step:789/2330 train_time:31573ms step_avg:40.02ms
step:790/2330 train_time:31630ms step_avg:40.04ms
step:791/2330 train_time:31653ms step_avg:40.02ms
step:792/2330 train_time:31710ms step_avg:40.04ms
step:793/2330 train_time:31733ms step_avg:40.02ms
step:794/2330 train_time:31790ms step_avg:40.04ms
step:795/2330 train_time:31812ms step_avg:40.02ms
step:796/2330 train_time:31869ms step_avg:40.04ms
step:797/2330 train_time:31892ms step_avg:40.01ms
step:798/2330 train_time:31948ms step_avg:40.03ms
step:799/2330 train_time:31972ms step_avg:40.01ms
step:800/2330 train_time:32028ms step_avg:40.04ms
step:801/2330 train_time:32052ms step_avg:40.02ms
step:802/2330 train_time:32109ms step_avg:40.04ms
step:803/2330 train_time:32132ms step_avg:40.02ms
step:804/2330 train_time:32189ms step_avg:40.04ms
step:805/2330 train_time:32213ms step_avg:40.02ms
step:806/2330 train_time:32269ms step_avg:40.04ms
step:807/2330 train_time:32292ms step_avg:40.01ms
step:808/2330 train_time:32349ms step_avg:40.04ms
step:809/2330 train_time:32373ms step_avg:40.02ms
step:810/2330 train_time:32430ms step_avg:40.04ms
step:811/2330 train_time:32453ms step_avg:40.02ms
step:812/2330 train_time:32510ms step_avg:40.04ms
step:813/2330 train_time:32533ms step_avg:40.02ms
step:814/2330 train_time:32592ms step_avg:40.04ms
step:815/2330 train_time:32614ms step_avg:40.02ms
step:816/2330 train_time:32672ms step_avg:40.04ms
step:817/2330 train_time:32694ms step_avg:40.02ms
step:818/2330 train_time:32752ms step_avg:40.04ms
step:819/2330 train_time:32774ms step_avg:40.02ms
step:820/2330 train_time:32832ms step_avg:40.04ms
step:821/2330 train_time:32854ms step_avg:40.02ms
step:822/2330 train_time:32912ms step_avg:40.04ms
step:823/2330 train_time:32935ms step_avg:40.02ms
step:824/2330 train_time:32992ms step_avg:40.04ms
step:825/2330 train_time:33015ms step_avg:40.02ms
step:826/2330 train_time:33072ms step_avg:40.04ms
step:827/2330 train_time:33094ms step_avg:40.02ms
step:828/2330 train_time:33151ms step_avg:40.04ms
step:829/2330 train_time:33174ms step_avg:40.02ms
step:830/2330 train_time:33231ms step_avg:40.04ms
step:831/2330 train_time:33254ms step_avg:40.02ms
step:832/2330 train_time:33310ms step_avg:40.04ms
step:833/2330 train_time:33332ms step_avg:40.01ms
step:834/2330 train_time:33389ms step_avg:40.04ms
step:835/2330 train_time:33413ms step_avg:40.02ms
step:836/2330 train_time:33471ms step_avg:40.04ms
step:837/2330 train_time:33494ms step_avg:40.02ms
step:838/2330 train_time:33553ms step_avg:40.04ms
step:839/2330 train_time:33576ms step_avg:40.02ms
step:840/2330 train_time:33634ms step_avg:40.04ms
step:841/2330 train_time:33657ms step_avg:40.02ms
step:842/2330 train_time:33714ms step_avg:40.04ms
step:843/2330 train_time:33736ms step_avg:40.02ms
step:844/2330 train_time:33794ms step_avg:40.04ms
step:845/2330 train_time:33817ms step_avg:40.02ms
step:846/2330 train_time:33875ms step_avg:40.04ms
step:847/2330 train_time:33898ms step_avg:40.02ms
step:848/2330 train_time:33956ms step_avg:40.04ms
step:849/2330 train_time:33978ms step_avg:40.02ms
step:850/2330 train_time:34037ms step_avg:40.04ms
step:851/2330 train_time:34060ms step_avg:40.02ms
step:852/2330 train_time:34118ms step_avg:40.04ms
step:853/2330 train_time:34141ms step_avg:40.02ms
step:854/2330 train_time:34199ms step_avg:40.05ms
step:855/2330 train_time:34221ms step_avg:40.02ms
step:856/2330 train_time:34278ms step_avg:40.04ms
step:857/2330 train_time:34301ms step_avg:40.03ms
step:858/2330 train_time:34360ms step_avg:40.05ms
step:859/2330 train_time:34383ms step_avg:40.03ms
step:860/2330 train_time:34441ms step_avg:40.05ms
step:861/2330 train_time:34465ms step_avg:40.03ms
step:862/2330 train_time:34523ms step_avg:40.05ms
step:863/2330 train_time:34547ms step_avg:40.03ms
step:864/2330 train_time:34605ms step_avg:40.05ms
step:865/2330 train_time:34628ms step_avg:40.03ms
step:866/2330 train_time:34685ms step_avg:40.05ms
step:867/2330 train_time:34708ms step_avg:40.03ms
step:868/2330 train_time:34765ms step_avg:40.05ms
step:869/2330 train_time:34789ms step_avg:40.03ms
step:870/2330 train_time:34846ms step_avg:40.05ms
step:871/2330 train_time:34868ms step_avg:40.03ms
step:872/2330 train_time:34925ms step_avg:40.05ms
step:873/2330 train_time:34949ms step_avg:40.03ms
step:874/2330 train_time:35005ms step_avg:40.05ms
step:875/2330 train_time:35029ms step_avg:40.03ms
step:876/2330 train_time:35087ms step_avg:40.05ms
step:877/2330 train_time:35110ms step_avg:40.03ms
step:878/2330 train_time:35167ms step_avg:40.05ms
step:879/2330 train_time:35190ms step_avg:40.03ms
step:880/2330 train_time:35247ms step_avg:40.05ms
step:881/2330 train_time:35271ms step_avg:40.04ms
step:882/2330 train_time:35328ms step_avg:40.05ms
step:883/2330 train_time:35352ms step_avg:40.04ms
step:884/2330 train_time:35408ms step_avg:40.05ms
step:885/2330 train_time:35431ms step_avg:40.04ms
step:886/2330 train_time:35489ms step_avg:40.06ms
step:887/2330 train_time:35513ms step_avg:40.04ms
step:888/2330 train_time:35569ms step_avg:40.06ms
step:889/2330 train_time:35592ms step_avg:40.04ms
step:890/2330 train_time:35649ms step_avg:40.05ms
step:891/2330 train_time:35672ms step_avg:40.04ms
step:892/2330 train_time:35729ms step_avg:40.05ms
step:893/2330 train_time:35752ms step_avg:40.04ms
step:894/2330 train_time:35808ms step_avg:40.05ms
step:895/2330 train_time:35832ms step_avg:40.04ms
step:896/2330 train_time:35888ms step_avg:40.05ms
step:897/2330 train_time:35912ms step_avg:40.04ms
step:898/2330 train_time:35968ms step_avg:40.05ms
step:899/2330 train_time:35992ms step_avg:40.04ms
step:900/2330 train_time:36048ms step_avg:40.05ms
step:901/2330 train_time:36072ms step_avg:40.04ms
step:902/2330 train_time:36128ms step_avg:40.05ms
step:903/2330 train_time:36152ms step_avg:40.04ms
step:904/2330 train_time:36209ms step_avg:40.05ms
step:905/2330 train_time:36233ms step_avg:40.04ms
step:906/2330 train_time:36290ms step_avg:40.05ms
step:907/2330 train_time:36313ms step_avg:40.04ms
step:908/2330 train_time:36369ms step_avg:40.05ms
step:909/2330 train_time:36392ms step_avg:40.04ms
step:910/2330 train_time:36449ms step_avg:40.05ms
step:911/2330 train_time:36473ms step_avg:40.04ms
step:912/2330 train_time:36529ms step_avg:40.05ms
step:913/2330 train_time:36552ms step_avg:40.04ms
step:914/2330 train_time:36609ms step_avg:40.05ms
step:915/2330 train_time:36632ms step_avg:40.03ms
step:916/2330 train_time:36689ms step_avg:40.05ms
step:917/2330 train_time:36713ms step_avg:40.04ms
step:918/2330 train_time:36770ms step_avg:40.05ms
step:919/2330 train_time:36793ms step_avg:40.04ms
step:920/2330 train_time:36850ms step_avg:40.05ms
step:921/2330 train_time:36873ms step_avg:40.04ms
step:922/2330 train_time:36929ms step_avg:40.05ms
step:923/2330 train_time:36953ms step_avg:40.04ms
step:924/2330 train_time:37010ms step_avg:40.05ms
step:925/2330 train_time:37033ms step_avg:40.04ms
step:926/2330 train_time:37090ms step_avg:40.05ms
step:927/2330 train_time:37113ms step_avg:40.04ms
step:928/2330 train_time:37170ms step_avg:40.05ms
step:929/2330 train_time:37193ms step_avg:40.04ms
step:930/2330 train_time:37250ms step_avg:40.05ms
step:931/2330 train_time:37273ms step_avg:40.04ms
step:932/2330 train_time:37330ms step_avg:40.05ms
step:933/2330 train_time:37353ms step_avg:40.04ms
step:934/2330 train_time:37411ms step_avg:40.05ms
step:935/2330 train_time:37434ms step_avg:40.04ms
step:936/2330 train_time:37491ms step_avg:40.05ms
step:937/2330 train_time:37514ms step_avg:40.04ms
step:938/2330 train_time:37572ms step_avg:40.06ms
step:939/2330 train_time:37594ms step_avg:40.04ms
step:940/2330 train_time:37651ms step_avg:40.05ms
step:941/2330 train_time:37673ms step_avg:40.04ms
step:942/2330 train_time:37731ms step_avg:40.05ms
step:943/2330 train_time:37754ms step_avg:40.04ms
step:944/2330 train_time:37811ms step_avg:40.05ms
step:945/2330 train_time:37835ms step_avg:40.04ms
step:946/2330 train_time:37893ms step_avg:40.06ms
step:947/2330 train_time:37916ms step_avg:40.04ms
step:948/2330 train_time:37973ms step_avg:40.06ms
step:949/2330 train_time:37996ms step_avg:40.04ms
step:950/2330 train_time:38053ms step_avg:40.06ms
step:951/2330 train_time:38076ms step_avg:40.04ms
step:952/2330 train_time:38134ms step_avg:40.06ms
step:953/2330 train_time:38156ms step_avg:40.04ms
step:954/2330 train_time:38214ms step_avg:40.06ms
step:955/2330 train_time:38236ms step_avg:40.04ms
step:956/2330 train_time:38294ms step_avg:40.06ms
step:957/2330 train_time:38316ms step_avg:40.04ms
step:958/2330 train_time:38374ms step_avg:40.06ms
step:959/2330 train_time:38396ms step_avg:40.04ms
step:960/2330 train_time:38454ms step_avg:40.06ms
step:961/2330 train_time:38477ms step_avg:40.04ms
step:962/2330 train_time:38535ms step_avg:40.06ms
step:963/2330 train_time:38558ms step_avg:40.04ms
step:964/2330 train_time:38616ms step_avg:40.06ms
step:965/2330 train_time:38639ms step_avg:40.04ms
step:966/2330 train_time:38698ms step_avg:40.06ms
step:967/2330 train_time:38720ms step_avg:40.04ms
step:968/2330 train_time:38777ms step_avg:40.06ms
step:969/2330 train_time:38800ms step_avg:40.04ms
step:970/2330 train_time:38858ms step_avg:40.06ms
step:971/2330 train_time:38882ms step_avg:40.04ms
step:972/2330 train_time:38940ms step_avg:40.06ms
step:973/2330 train_time:38963ms step_avg:40.04ms
step:974/2330 train_time:39020ms step_avg:40.06ms
step:975/2330 train_time:39044ms step_avg:40.04ms
step:976/2330 train_time:39102ms step_avg:40.06ms
step:977/2330 train_time:39125ms step_avg:40.05ms
step:978/2330 train_time:39183ms step_avg:40.06ms
step:979/2330 train_time:39206ms step_avg:40.05ms
step:980/2330 train_time:39263ms step_avg:40.06ms
step:981/2330 train_time:39287ms step_avg:40.05ms
step:982/2330 train_time:39345ms step_avg:40.07ms
step:983/2330 train_time:39368ms step_avg:40.05ms
step:984/2330 train_time:39425ms step_avg:40.07ms
step:985/2330 train_time:39448ms step_avg:40.05ms
step:986/2330 train_time:39506ms step_avg:40.07ms
step:987/2330 train_time:39529ms step_avg:40.05ms
step:988/2330 train_time:39586ms step_avg:40.07ms
step:989/2330 train_time:39610ms step_avg:40.05ms
step:990/2330 train_time:39667ms step_avg:40.07ms
step:991/2330 train_time:39691ms step_avg:40.05ms
step:992/2330 train_time:39747ms step_avg:40.07ms
step:993/2330 train_time:39771ms step_avg:40.05ms
step:994/2330 train_time:39828ms step_avg:40.07ms
step:995/2330 train_time:39851ms step_avg:40.05ms
step:996/2330 train_time:39908ms step_avg:40.07ms
step:997/2330 train_time:39931ms step_avg:40.05ms
step:998/2330 train_time:39988ms step_avg:40.07ms
step:999/2330 train_time:40012ms step_avg:40.05ms
step:1000/2330 train_time:40069ms step_avg:40.07ms
step:1000/2330 val_loss:5.5243 train_time:40167ms step_avg:40.17ms
step:1001/2330 train_time:40179ms step_avg:40.14ms
step:1002/2330 train_time:40191ms step_avg:40.11ms
step:1003/2330 train_time:40200ms step_avg:40.08ms
step:1004/2330 train_time:40231ms step_avg:40.07ms
step:1005/2330 train_time:40253ms step_avg:40.05ms
step:1006/2330 train_time:40309ms step_avg:40.07ms
step:1007/2330 train_time:40331ms step_avg:40.05ms
step:1008/2330 train_time:40387ms step_avg:40.07ms
step:1009/2330 train_time:40409ms step_avg:40.05ms
step:1010/2330 train_time:40466ms step_avg:40.07ms
step:1011/2330 train_time:40494ms step_avg:40.05ms
step:1012/2330 train_time:40555ms step_avg:40.07ms
step:1013/2330 train_time:40580ms step_avg:40.06ms
step:1014/2330 train_time:40638ms step_avg:40.08ms
step:1015/2330 train_time:40662ms step_avg:40.06ms
step:1016/2330 train_time:40720ms step_avg:40.08ms
step:1017/2330 train_time:40742ms step_avg:40.06ms
step:1018/2330 train_time:40799ms step_avg:40.08ms
step:1019/2330 train_time:40822ms step_avg:40.06ms
step:1020/2330 train_time:40879ms step_avg:40.08ms
step:1021/2330 train_time:40902ms step_avg:40.06ms
step:1022/2330 train_time:40959ms step_avg:40.08ms
step:1023/2330 train_time:40981ms step_avg:40.06ms
step:1024/2330 train_time:41039ms step_avg:40.08ms
step:1025/2330 train_time:41061ms step_avg:40.06ms
step:1026/2330 train_time:41121ms step_avg:40.08ms
step:1027/2330 train_time:41144ms step_avg:40.06ms
step:1028/2330 train_time:41202ms step_avg:40.08ms
step:1029/2330 train_time:41225ms step_avg:40.06ms
step:1030/2330 train_time:41282ms step_avg:40.08ms
step:1031/2330 train_time:41304ms step_avg:40.06ms
step:1032/2330 train_time:41361ms step_avg:40.08ms
step:1033/2330 train_time:41384ms step_avg:40.06ms
step:1034/2330 train_time:41443ms step_avg:40.08ms
step:1035/2330 train_time:41469ms step_avg:40.07ms
step:1036/2330 train_time:41528ms step_avg:40.08ms
step:1037/2330 train_time:41552ms step_avg:40.07ms
step:1038/2330 train_time:41610ms step_avg:40.09ms
step:1039/2330 train_time:41634ms step_avg:40.07ms
step:1040/2330 train_time:41691ms step_avg:40.09ms
step:1041/2330 train_time:41715ms step_avg:40.07ms
step:1042/2330 train_time:41771ms step_avg:40.09ms
step:1043/2330 train_time:41795ms step_avg:40.07ms
step:1044/2330 train_time:41851ms step_avg:40.09ms
step:1045/2330 train_time:41875ms step_avg:40.07ms
step:1046/2330 train_time:41931ms step_avg:40.09ms
step:1047/2330 train_time:41954ms step_avg:40.07ms
step:1048/2330 train_time:42011ms step_avg:40.09ms
step:1049/2330 train_time:42034ms step_avg:40.07ms
step:1050/2330 train_time:42091ms step_avg:40.09ms
step:1051/2330 train_time:42115ms step_avg:40.07ms
step:1052/2330 train_time:42171ms step_avg:40.09ms
step:1053/2330 train_time:42195ms step_avg:40.07ms
step:1054/2330 train_time:42251ms step_avg:40.09ms
step:1055/2330 train_time:42275ms step_avg:40.07ms
step:1056/2330 train_time:42332ms step_avg:40.09ms
step:1057/2330 train_time:42356ms step_avg:40.07ms
step:1058/2330 train_time:42413ms step_avg:40.09ms
step:1059/2330 train_time:42437ms step_avg:40.07ms
step:1060/2330 train_time:42495ms step_avg:40.09ms
step:1061/2330 train_time:42518ms step_avg:40.07ms
step:1062/2330 train_time:42577ms step_avg:40.09ms
step:1063/2330 train_time:42600ms step_avg:40.07ms
step:1064/2330 train_time:42658ms step_avg:40.09ms
step:1065/2330 train_time:42681ms step_avg:40.08ms
step:1066/2330 train_time:42739ms step_avg:40.09ms
step:1067/2330 train_time:42761ms step_avg:40.08ms
step:1068/2330 train_time:42820ms step_avg:40.09ms
step:1069/2330 train_time:42842ms step_avg:40.08ms
step:1070/2330 train_time:42901ms step_avg:40.09ms
step:1071/2330 train_time:42923ms step_avg:40.08ms
step:1072/2330 train_time:42981ms step_avg:40.09ms
step:1073/2330 train_time:43004ms step_avg:40.08ms
step:1074/2330 train_time:43062ms step_avg:40.10ms
step:1075/2330 train_time:43085ms step_avg:40.08ms
step:1076/2330 train_time:43143ms step_avg:40.10ms
step:1077/2330 train_time:43166ms step_avg:40.08ms
step:1078/2330 train_time:43223ms step_avg:40.10ms
step:1079/2330 train_time:43246ms step_avg:40.08ms
step:1080/2330 train_time:43304ms step_avg:40.10ms
step:1081/2330 train_time:43328ms step_avg:40.08ms
step:1082/2330 train_time:43386ms step_avg:40.10ms
step:1083/2330 train_time:43409ms step_avg:40.08ms
step:1084/2330 train_time:43467ms step_avg:40.10ms
step:1085/2330 train_time:43491ms step_avg:40.08ms
step:1086/2330 train_time:43549ms step_avg:40.10ms
step:1087/2330 train_time:43573ms step_avg:40.09ms
step:1088/2330 train_time:43630ms step_avg:40.10ms
step:1089/2330 train_time:43653ms step_avg:40.09ms
step:1090/2330 train_time:43710ms step_avg:40.10ms
step:1091/2330 train_time:43733ms step_avg:40.09ms
step:1092/2330 train_time:43791ms step_avg:40.10ms
step:1093/2330 train_time:43815ms step_avg:40.09ms
step:1094/2330 train_time:43871ms step_avg:40.10ms
step:1095/2330 train_time:43894ms step_avg:40.09ms
step:1096/2330 train_time:43950ms step_avg:40.10ms
step:1097/2330 train_time:43973ms step_avg:40.09ms
step:1098/2330 train_time:44031ms step_avg:40.10ms
step:1099/2330 train_time:44055ms step_avg:40.09ms
step:1100/2330 train_time:44111ms step_avg:40.10ms
step:1101/2330 train_time:44135ms step_avg:40.09ms
step:1102/2330 train_time:44191ms step_avg:40.10ms
step:1103/2330 train_time:44215ms step_avg:40.09ms
step:1104/2330 train_time:44271ms step_avg:40.10ms
step:1105/2330 train_time:44295ms step_avg:40.09ms
step:1106/2330 train_time:44352ms step_avg:40.10ms
step:1107/2330 train_time:44375ms step_avg:40.09ms
step:1108/2330 train_time:44432ms step_avg:40.10ms
step:1109/2330 train_time:44455ms step_avg:40.09ms
step:1110/2330 train_time:44512ms step_avg:40.10ms
step:1111/2330 train_time:44535ms step_avg:40.09ms
step:1112/2330 train_time:44592ms step_avg:40.10ms
step:1113/2330 train_time:44616ms step_avg:40.09ms
step:1114/2330 train_time:44673ms step_avg:40.10ms
step:1115/2330 train_time:44697ms step_avg:40.09ms
step:1116/2330 train_time:44754ms step_avg:40.10ms
step:1117/2330 train_time:44777ms step_avg:40.09ms
step:1118/2330 train_time:44834ms step_avg:40.10ms
step:1119/2330 train_time:44857ms step_avg:40.09ms
step:1120/2330 train_time:44914ms step_avg:40.10ms
step:1121/2330 train_time:44937ms step_avg:40.09ms
step:1122/2330 train_time:44995ms step_avg:40.10ms
step:1123/2330 train_time:45018ms step_avg:40.09ms
step:1124/2330 train_time:45076ms step_avg:40.10ms
step:1125/2330 train_time:45098ms step_avg:40.09ms
step:1126/2330 train_time:45156ms step_avg:40.10ms
step:1127/2330 train_time:45179ms step_avg:40.09ms
step:1128/2330 train_time:45237ms step_avg:40.10ms
step:1129/2330 train_time:45259ms step_avg:40.09ms
step:1130/2330 train_time:45316ms step_avg:40.10ms
step:1131/2330 train_time:45340ms step_avg:40.09ms
step:1132/2330 train_time:45397ms step_avg:40.10ms
step:1133/2330 train_time:45421ms step_avg:40.09ms
step:1134/2330 train_time:45478ms step_avg:40.10ms
step:1135/2330 train_time:45501ms step_avg:40.09ms
step:1136/2330 train_time:45560ms step_avg:40.11ms
step:1137/2330 train_time:45584ms step_avg:40.09ms
step:1138/2330 train_time:45642ms step_avg:40.11ms
step:1139/2330 train_time:45666ms step_avg:40.09ms
step:1140/2330 train_time:45724ms step_avg:40.11ms
step:1141/2330 train_time:45748ms step_avg:40.09ms
step:1142/2330 train_time:45806ms step_avg:40.11ms
step:1143/2330 train_time:45829ms step_avg:40.10ms
step:1144/2330 train_time:45886ms step_avg:40.11ms
step:1145/2330 train_time:45909ms step_avg:40.10ms
step:1146/2330 train_time:45966ms step_avg:40.11ms
step:1147/2330 train_time:45989ms step_avg:40.10ms
step:1148/2330 train_time:46047ms step_avg:40.11ms
step:1149/2330 train_time:46071ms step_avg:40.10ms
step:1150/2330 train_time:46128ms step_avg:40.11ms
step:1151/2330 train_time:46152ms step_avg:40.10ms
step:1152/2330 train_time:46209ms step_avg:40.11ms
step:1153/2330 train_time:46232ms step_avg:40.10ms
step:1154/2330 train_time:46289ms step_avg:40.11ms
step:1155/2330 train_time:46312ms step_avg:40.10ms
step:1156/2330 train_time:46369ms step_avg:40.11ms
step:1157/2330 train_time:46393ms step_avg:40.10ms
step:1158/2330 train_time:46449ms step_avg:40.11ms
step:1159/2330 train_time:46473ms step_avg:40.10ms
step:1160/2330 train_time:46530ms step_avg:40.11ms
step:1161/2330 train_time:46554ms step_avg:40.10ms
step:1162/2330 train_time:46611ms step_avg:40.11ms
step:1163/2330 train_time:46635ms step_avg:40.10ms
step:1164/2330 train_time:46692ms step_avg:40.11ms
step:1165/2330 train_time:46715ms step_avg:40.10ms
step:1166/2330 train_time:46772ms step_avg:40.11ms
step:1167/2330 train_time:46796ms step_avg:40.10ms
step:1168/2330 train_time:46852ms step_avg:40.11ms
step:1169/2330 train_time:46875ms step_avg:40.10ms
step:1170/2330 train_time:46932ms step_avg:40.11ms
step:1171/2330 train_time:46956ms step_avg:40.10ms
step:1172/2330 train_time:47013ms step_avg:40.11ms
step:1173/2330 train_time:47036ms step_avg:40.10ms
step:1174/2330 train_time:47092ms step_avg:40.11ms
step:1175/2330 train_time:47115ms step_avg:40.10ms
step:1176/2330 train_time:47173ms step_avg:40.11ms
step:1177/2330 train_time:47197ms step_avg:40.10ms
step:1178/2330 train_time:47253ms step_avg:40.11ms
step:1179/2330 train_time:47276ms step_avg:40.10ms
step:1180/2330 train_time:47333ms step_avg:40.11ms
step:1181/2330 train_time:47356ms step_avg:40.10ms
step:1182/2330 train_time:47413ms step_avg:40.11ms
step:1183/2330 train_time:47437ms step_avg:40.10ms
step:1184/2330 train_time:47495ms step_avg:40.11ms
step:1185/2330 train_time:47518ms step_avg:40.10ms
step:1186/2330 train_time:47576ms step_avg:40.11ms
step:1187/2330 train_time:47599ms step_avg:40.10ms
step:1188/2330 train_time:47657ms step_avg:40.12ms
step:1189/2330 train_time:47680ms step_avg:40.10ms
step:1190/2330 train_time:47737ms step_avg:40.12ms
step:1191/2330 train_time:47760ms step_avg:40.10ms
step:1192/2330 train_time:47818ms step_avg:40.12ms
step:1193/2330 train_time:47840ms step_avg:40.10ms
step:1194/2330 train_time:47898ms step_avg:40.12ms
step:1195/2330 train_time:47921ms step_avg:40.10ms
step:1196/2330 train_time:47979ms step_avg:40.12ms
step:1197/2330 train_time:48001ms step_avg:40.10ms
step:1198/2330 train_time:48059ms step_avg:40.12ms
step:1199/2330 train_time:48083ms step_avg:40.10ms
step:1200/2330 train_time:48142ms step_avg:40.12ms
step:1201/2330 train_time:48165ms step_avg:40.10ms
step:1202/2330 train_time:48223ms step_avg:40.12ms
step:1203/2330 train_time:48246ms step_avg:40.10ms
step:1204/2330 train_time:48304ms step_avg:40.12ms
step:1205/2330 train_time:48327ms step_avg:40.11ms
step:1206/2330 train_time:48386ms step_avg:40.12ms
step:1207/2330 train_time:48408ms step_avg:40.11ms
step:1208/2330 train_time:48466ms step_avg:40.12ms
step:1209/2330 train_time:48489ms step_avg:40.11ms
step:1210/2330 train_time:48548ms step_avg:40.12ms
step:1211/2330 train_time:48571ms step_avg:40.11ms
step:1212/2330 train_time:48628ms step_avg:40.12ms
step:1213/2330 train_time:48651ms step_avg:40.11ms
step:1214/2330 train_time:48708ms step_avg:40.12ms
step:1215/2330 train_time:48731ms step_avg:40.11ms
step:1216/2330 train_time:48789ms step_avg:40.12ms
step:1217/2330 train_time:48813ms step_avg:40.11ms
step:1218/2330 train_time:48870ms step_avg:40.12ms
step:1219/2330 train_time:48894ms step_avg:40.11ms
step:1220/2330 train_time:48951ms step_avg:40.12ms
step:1221/2330 train_time:48975ms step_avg:40.11ms
step:1222/2330 train_time:49031ms step_avg:40.12ms
step:1223/2330 train_time:49055ms step_avg:40.11ms
step:1224/2330 train_time:49112ms step_avg:40.12ms
step:1225/2330 train_time:49136ms step_avg:40.11ms
step:1226/2330 train_time:49192ms step_avg:40.12ms
step:1227/2330 train_time:49216ms step_avg:40.11ms
step:1228/2330 train_time:49273ms step_avg:40.12ms
step:1229/2330 train_time:49296ms step_avg:40.11ms
step:1230/2330 train_time:49353ms step_avg:40.12ms
step:1231/2330 train_time:49376ms step_avg:40.11ms
step:1232/2330 train_time:49433ms step_avg:40.12ms
step:1233/2330 train_time:49457ms step_avg:40.11ms
step:1234/2330 train_time:49515ms step_avg:40.13ms
step:1235/2330 train_time:49538ms step_avg:40.11ms
step:1236/2330 train_time:49596ms step_avg:40.13ms
step:1237/2330 train_time:49618ms step_avg:40.11ms
step:1238/2330 train_time:49676ms step_avg:40.13ms
step:1239/2330 train_time:49699ms step_avg:40.11ms
step:1240/2330 train_time:49757ms step_avg:40.13ms
step:1241/2330 train_time:49780ms step_avg:40.11ms
step:1242/2330 train_time:49837ms step_avg:40.13ms
step:1243/2330 train_time:49859ms step_avg:40.11ms
step:1244/2330 train_time:49917ms step_avg:40.13ms
step:1245/2330 train_time:49940ms step_avg:40.11ms
step:1246/2330 train_time:49997ms step_avg:40.13ms
step:1247/2330 train_time:50020ms step_avg:40.11ms
step:1248/2330 train_time:50078ms step_avg:40.13ms
step:1249/2330 train_time:50100ms step_avg:40.11ms
step:1250/2330 train_time:50158ms step_avg:40.13ms
step:1250/2330 val_loss:5.5310 train_time:50255ms step_avg:40.20ms
step:1251/2330 train_time:50267ms step_avg:40.18ms
step:1252/2330 train_time:50278ms step_avg:40.16ms
step:1253/2330 train_time:50288ms step_avg:40.13ms
step:1254/2330 train_time:50319ms step_avg:40.13ms
step:1255/2330 train_time:50341ms step_avg:40.11ms
step:1256/2330 train_time:50397ms step_avg:40.12ms
step:1257/2330 train_time:50419ms step_avg:40.11ms
step:1258/2330 train_time:50475ms step_avg:40.12ms
step:1259/2330 train_time:50498ms step_avg:40.11ms
step:1260/2330 train_time:50557ms step_avg:40.12ms
step:1261/2330 train_time:50584ms step_avg:40.11ms
step:1262/2330 train_time:50644ms step_avg:40.13ms
step:1263/2330 train_time:50668ms step_avg:40.12ms
step:1264/2330 train_time:50727ms step_avg:40.13ms
step:1265/2330 train_time:50749ms step_avg:40.12ms
step:1266/2330 train_time:50806ms step_avg:40.13ms
step:1267/2330 train_time:50830ms step_avg:40.12ms
step:1268/2330 train_time:50887ms step_avg:40.13ms
step:1269/2330 train_time:50922ms step_avg:40.13ms
step:1270/2330 train_time:50968ms step_avg:40.13ms
step:1271/2330 train_time:50993ms step_avg:40.12ms
step:1272/2330 train_time:51048ms step_avg:40.13ms
step:1273/2330 train_time:51071ms step_avg:40.12ms
step:1274/2330 train_time:51128ms step_avg:40.13ms
step:1275/2330 train_time:51151ms step_avg:40.12ms
step:1276/2330 train_time:51210ms step_avg:40.13ms
step:1277/2330 train_time:51234ms step_avg:40.12ms
step:1278/2330 train_time:51292ms step_avg:40.13ms
step:1279/2330 train_time:51315ms step_avg:40.12ms
step:1280/2330 train_time:51372ms step_avg:40.13ms
step:1281/2330 train_time:51395ms step_avg:40.12ms
step:1282/2330 train_time:51452ms step_avg:40.13ms
step:1283/2330 train_time:51476ms step_avg:40.12ms
step:1284/2330 train_time:51535ms step_avg:40.14ms
step:1285/2330 train_time:51559ms step_avg:40.12ms
step:1286/2330 train_time:51618ms step_avg:40.14ms
step:1287/2330 train_time:51643ms step_avg:40.13ms
step:1288/2330 train_time:51700ms step_avg:40.14ms
step:1289/2330 train_time:51724ms step_avg:40.13ms
step:1290/2330 train_time:51781ms step_avg:40.14ms
step:1291/2330 train_time:51805ms step_avg:40.13ms
step:1292/2330 train_time:51862ms step_avg:40.14ms
step:1293/2330 train_time:51885ms step_avg:40.13ms
step:1294/2330 train_time:51943ms step_avg:40.14ms
step:1295/2330 train_time:51965ms step_avg:40.13ms
step:1296/2330 train_time:52022ms step_avg:40.14ms
step:1297/2330 train_time:52044ms step_avg:40.13ms
step:1298/2330 train_time:52102ms step_avg:40.14ms
step:1299/2330 train_time:52126ms step_avg:40.13ms
step:1300/2330 train_time:52184ms step_avg:40.14ms
step:1301/2330 train_time:52207ms step_avg:40.13ms
step:1302/2330 train_time:52264ms step_avg:40.14ms
step:1303/2330 train_time:52287ms step_avg:40.13ms
step:1304/2330 train_time:52346ms step_avg:40.14ms
step:1305/2330 train_time:52369ms step_avg:40.13ms
step:1306/2330 train_time:52426ms step_avg:40.14ms
step:1307/2330 train_time:52450ms step_avg:40.13ms
step:1308/2330 train_time:52507ms step_avg:40.14ms
step:1309/2330 train_time:52531ms step_avg:40.13ms
step:1310/2330 train_time:52591ms step_avg:40.15ms
step:1311/2330 train_time:52614ms step_avg:40.13ms
step:1312/2330 train_time:52672ms step_avg:40.15ms
step:1313/2330 train_time:52696ms step_avg:40.13ms
step:1314/2330 train_time:52753ms step_avg:40.15ms
step:1315/2330 train_time:52777ms step_avg:40.13ms
step:1316/2330 train_time:52834ms step_avg:40.15ms
step:1317/2330 train_time:52857ms step_avg:40.13ms
step:1318/2330 train_time:52914ms step_avg:40.15ms
step:1319/2330 train_time:52938ms step_avg:40.13ms
step:1320/2330 train_time:52995ms step_avg:40.15ms
step:1321/2330 train_time:53019ms step_avg:40.14ms
step:1322/2330 train_time:53076ms step_avg:40.15ms
step:1323/2330 train_time:53099ms step_avg:40.14ms
step:1324/2330 train_time:53156ms step_avg:40.15ms
step:1325/2330 train_time:53181ms step_avg:40.14ms
step:1326/2330 train_time:53237ms step_avg:40.15ms
step:1327/2330 train_time:53260ms step_avg:40.14ms
step:1328/2330 train_time:53316ms step_avg:40.15ms
step:1329/2330 train_time:53340ms step_avg:40.14ms
step:1330/2330 train_time:53397ms step_avg:40.15ms
step:1331/2330 train_time:53421ms step_avg:40.14ms
step:1332/2330 train_time:53477ms step_avg:40.15ms
step:1333/2330 train_time:53501ms step_avg:40.14ms
step:1334/2330 train_time:53559ms step_avg:40.15ms
step:1335/2330 train_time:53583ms step_avg:40.14ms
step:1336/2330 train_time:53641ms step_avg:40.15ms
step:1337/2330 train_time:53665ms step_avg:40.14ms
step:1338/2330 train_time:53723ms step_avg:40.15ms
step:1339/2330 train_time:53745ms step_avg:40.14ms
step:1340/2330 train_time:53803ms step_avg:40.15ms
step:1341/2330 train_time:53827ms step_avg:40.14ms
step:1342/2330 train_time:53885ms step_avg:40.15ms
step:1343/2330 train_time:53907ms step_avg:40.14ms
step:1344/2330 train_time:53965ms step_avg:40.15ms
step:1345/2330 train_time:53988ms step_avg:40.14ms
step:1346/2330 train_time:54045ms step_avg:40.15ms
step:1347/2330 train_time:54068ms step_avg:40.14ms
step:1348/2330 train_time:54126ms step_avg:40.15ms
step:1349/2330 train_time:54150ms step_avg:40.14ms
step:1350/2330 train_time:54208ms step_avg:40.15ms
step:1351/2330 train_time:54230ms step_avg:40.14ms
step:1352/2330 train_time:54287ms step_avg:40.15ms
step:1353/2330 train_time:54310ms step_avg:40.14ms
step:1354/2330 train_time:54368ms step_avg:40.15ms
step:1355/2330 train_time:54392ms step_avg:40.14ms
step:1356/2330 train_time:54450ms step_avg:40.16ms
step:1357/2330 train_time:54474ms step_avg:40.14ms
step:1358/2330 train_time:54532ms step_avg:40.16ms
step:1359/2330 train_time:54556ms step_avg:40.14ms
step:1360/2330 train_time:54613ms step_avg:40.16ms
step:1361/2330 train_time:54637ms step_avg:40.14ms
step:1362/2330 train_time:54694ms step_avg:40.16ms
step:1363/2330 train_time:54717ms step_avg:40.14ms
step:1364/2330 train_time:54774ms step_avg:40.16ms
step:1365/2330 train_time:54797ms step_avg:40.14ms
step:1366/2330 train_time:54855ms step_avg:40.16ms
step:1367/2330 train_time:54879ms step_avg:40.15ms
step:1368/2330 train_time:54936ms step_avg:40.16ms
step:1369/2330 train_time:54960ms step_avg:40.15ms
step:1370/2330 train_time:55018ms step_avg:40.16ms
step:1371/2330 train_time:55042ms step_avg:40.15ms
step:1372/2330 train_time:55099ms step_avg:40.16ms
step:1373/2330 train_time:55122ms step_avg:40.15ms
step:1374/2330 train_time:55179ms step_avg:40.16ms
step:1375/2330 train_time:55203ms step_avg:40.15ms
step:1376/2330 train_time:55260ms step_avg:40.16ms
step:1377/2330 train_time:55284ms step_avg:40.15ms
step:1378/2330 train_time:55341ms step_avg:40.16ms
step:1379/2330 train_time:55364ms step_avg:40.15ms
step:1380/2330 train_time:55421ms step_avg:40.16ms
step:1381/2330 train_time:55444ms step_avg:40.15ms
step:1382/2330 train_time:55502ms step_avg:40.16ms
step:1383/2330 train_time:55525ms step_avg:40.15ms
step:1384/2330 train_time:55583ms step_avg:40.16ms
step:1385/2330 train_time:55606ms step_avg:40.15ms
step:1386/2330 train_time:55664ms step_avg:40.16ms
step:1387/2330 train_time:55687ms step_avg:40.15ms
step:1388/2330 train_time:55744ms step_avg:40.16ms
step:1389/2330 train_time:55768ms step_avg:40.15ms
step:1390/2330 train_time:55826ms step_avg:40.16ms
step:1391/2330 train_time:55850ms step_avg:40.15ms
step:1392/2330 train_time:55909ms step_avg:40.16ms
step:1393/2330 train_time:55932ms step_avg:40.15ms
step:1394/2330 train_time:55990ms step_avg:40.16ms
step:1395/2330 train_time:56013ms step_avg:40.15ms
step:1396/2330 train_time:56071ms step_avg:40.17ms
step:1397/2330 train_time:56095ms step_avg:40.15ms
step:1398/2330 train_time:56153ms step_avg:40.17ms
step:1399/2330 train_time:56176ms step_avg:40.15ms
step:1400/2330 train_time:56233ms step_avg:40.17ms
step:1401/2330 train_time:56257ms step_avg:40.16ms
step:1402/2330 train_time:56315ms step_avg:40.17ms
step:1403/2330 train_time:56339ms step_avg:40.16ms
step:1404/2330 train_time:56395ms step_avg:40.17ms
step:1405/2330 train_time:56419ms step_avg:40.16ms
step:1406/2330 train_time:56475ms step_avg:40.17ms
step:1407/2330 train_time:56499ms step_avg:40.16ms
step:1408/2330 train_time:56556ms step_avg:40.17ms
step:1409/2330 train_time:56580ms step_avg:40.16ms
step:1410/2330 train_time:56636ms step_avg:40.17ms
step:1411/2330 train_time:56660ms step_avg:40.16ms
step:1412/2330 train_time:56717ms step_avg:40.17ms
step:1413/2330 train_time:56740ms step_avg:40.16ms
step:1414/2330 train_time:56797ms step_avg:40.17ms
step:1415/2330 train_time:56821ms step_avg:40.16ms
step:1416/2330 train_time:56878ms step_avg:40.17ms
step:1417/2330 train_time:56901ms step_avg:40.16ms
step:1418/2330 train_time:56958ms step_avg:40.17ms
step:1419/2330 train_time:56981ms step_avg:40.16ms
step:1420/2330 train_time:57038ms step_avg:40.17ms
step:1421/2330 train_time:57062ms step_avg:40.16ms
step:1422/2330 train_time:57119ms step_avg:40.17ms
step:1423/2330 train_time:57143ms step_avg:40.16ms
step:1424/2330 train_time:57201ms step_avg:40.17ms
step:1425/2330 train_time:57223ms step_avg:40.16ms
step:1426/2330 train_time:57281ms step_avg:40.17ms
step:1427/2330 train_time:57303ms step_avg:40.16ms
step:1428/2330 train_time:57361ms step_avg:40.17ms
step:1429/2330 train_time:57383ms step_avg:40.16ms
step:1430/2330 train_time:57441ms step_avg:40.17ms
step:1431/2330 train_time:57463ms step_avg:40.16ms
step:1432/2330 train_time:57520ms step_avg:40.17ms
step:1433/2330 train_time:57543ms step_avg:40.16ms
step:1434/2330 train_time:57600ms step_avg:40.17ms
step:1435/2330 train_time:57623ms step_avg:40.16ms
step:1436/2330 train_time:57680ms step_avg:40.17ms
step:1437/2330 train_time:57703ms step_avg:40.16ms
step:1438/2330 train_time:57761ms step_avg:40.17ms
step:1439/2330 train_time:57784ms step_avg:40.16ms
step:1440/2330 train_time:57842ms step_avg:40.17ms
step:1441/2330 train_time:57865ms step_avg:40.16ms
step:1442/2330 train_time:57923ms step_avg:40.17ms
step:1443/2330 train_time:57945ms step_avg:40.16ms
step:1444/2330 train_time:58003ms step_avg:40.17ms
step:1445/2330 train_time:58026ms step_avg:40.16ms
step:1446/2330 train_time:58084ms step_avg:40.17ms
step:1447/2330 train_time:58107ms step_avg:40.16ms
step:1448/2330 train_time:58165ms step_avg:40.17ms
step:1449/2330 train_time:58189ms step_avg:40.16ms
step:1450/2330 train_time:58247ms step_avg:40.17ms
step:1451/2330 train_time:58270ms step_avg:40.16ms
step:1452/2330 train_time:58327ms step_avg:40.17ms
step:1453/2330 train_time:58351ms step_avg:40.16ms
step:1454/2330 train_time:58409ms step_avg:40.17ms
step:1455/2330 train_time:58433ms step_avg:40.16ms
step:1456/2330 train_time:58490ms step_avg:40.17ms
step:1457/2330 train_time:58513ms step_avg:40.16ms
step:1458/2330 train_time:58572ms step_avg:40.17ms
step:1459/2330 train_time:58595ms step_avg:40.16ms
step:1460/2330 train_time:58652ms step_avg:40.17ms
step:1461/2330 train_time:58676ms step_avg:40.16ms
step:1462/2330 train_time:58733ms step_avg:40.17ms
step:1463/2330 train_time:58757ms step_avg:40.16ms
step:1464/2330 train_time:58814ms step_avg:40.17ms
step:1465/2330 train_time:58838ms step_avg:40.16ms
step:1466/2330 train_time:58895ms step_avg:40.17ms
step:1467/2330 train_time:58919ms step_avg:40.16ms
step:1468/2330 train_time:58976ms step_avg:40.17ms
step:1469/2330 train_time:59000ms step_avg:40.16ms
step:1470/2330 train_time:59056ms step_avg:40.17ms
step:1471/2330 train_time:59079ms step_avg:40.16ms
step:1472/2330 train_time:59137ms step_avg:40.17ms
step:1473/2330 train_time:59161ms step_avg:40.16ms
step:1474/2330 train_time:59218ms step_avg:40.17ms
step:1475/2330 train_time:59241ms step_avg:40.16ms
step:1476/2330 train_time:59298ms step_avg:40.17ms
step:1477/2330 train_time:59322ms step_avg:40.16ms
step:1478/2330 train_time:59378ms step_avg:40.17ms
step:1479/2330 train_time:59401ms step_avg:40.16ms
step:1480/2330 train_time:59458ms step_avg:40.17ms
step:1481/2330 train_time:59481ms step_avg:40.16ms
step:1482/2330 train_time:59538ms step_avg:40.17ms
step:1483/2330 train_time:59562ms step_avg:40.16ms
step:1484/2330 train_time:59618ms step_avg:40.17ms
step:1485/2330 train_time:59642ms step_avg:40.16ms
step:1486/2330 train_time:59699ms step_avg:40.17ms
step:1487/2330 train_time:59723ms step_avg:40.16ms
step:1488/2330 train_time:59780ms step_avg:40.17ms
step:1489/2330 train_time:59803ms step_avg:40.16ms
step:1490/2330 train_time:59860ms step_avg:40.17ms
step:1491/2330 train_time:59883ms step_avg:40.16ms
step:1492/2330 train_time:59940ms step_avg:40.17ms
step:1493/2330 train_time:59963ms step_avg:40.16ms
step:1494/2330 train_time:60021ms step_avg:40.17ms
step:1495/2330 train_time:60044ms step_avg:40.16ms
step:1496/2330 train_time:60101ms step_avg:40.17ms
step:1497/2330 train_time:60124ms step_avg:40.16ms
step:1498/2330 train_time:60182ms step_avg:40.17ms
step:1499/2330 train_time:60204ms step_avg:40.16ms
step:1500/2330 train_time:60260ms step_avg:40.17ms
step:1500/2330 val_loss:5.5890 train_time:60358ms step_avg:40.24ms
step:1501/2330 train_time:60371ms step_avg:40.22ms
step:1502/2330 train_time:60382ms step_avg:40.20ms
step:1503/2330 train_time:60391ms step_avg:40.18ms
step:1504/2330 train_time:60422ms step_avg:40.17ms
step:1505/2330 train_time:60444ms step_avg:40.16ms
step:1506/2330 train_time:60499ms step_avg:40.17ms
step:1507/2330 train_time:60522ms step_avg:40.16ms
step:1508/2330 train_time:60579ms step_avg:40.17ms
step:1509/2330 train_time:60602ms step_avg:40.16ms
step:1510/2330 train_time:60660ms step_avg:40.17ms
step:1511/2330 train_time:60688ms step_avg:40.16ms
step:1512/2330 train_time:60748ms step_avg:40.18ms
step:1513/2330 train_time:60774ms step_avg:40.17ms
step:1514/2330 train_time:60833ms step_avg:40.18ms
step:1515/2330 train_time:60856ms step_avg:40.17ms
step:1516/2330 train_time:60913ms step_avg:40.18ms
step:1517/2330 train_time:60936ms step_avg:40.17ms
step:1518/2330 train_time:60993ms step_avg:40.18ms
step:1519/2330 train_time:61015ms step_avg:40.17ms
step:1520/2330 train_time:61073ms step_avg:40.18ms
step:1521/2330 train_time:61096ms step_avg:40.17ms
step:1522/2330 train_time:61154ms step_avg:40.18ms
step:1523/2330 train_time:61176ms step_avg:40.17ms
step:1524/2330 train_time:61233ms step_avg:40.18ms
step:1525/2330 train_time:61256ms step_avg:40.17ms
step:1526/2330 train_time:61316ms step_avg:40.18ms
step:1527/2330 train_time:61340ms step_avg:40.17ms
step:1528/2330 train_time:61398ms step_avg:40.18ms
step:1529/2330 train_time:61422ms step_avg:40.17ms
step:1530/2330 train_time:61479ms step_avg:40.18ms
step:1531/2330 train_time:61500ms step_avg:40.17ms
step:1532/2330 train_time:61557ms step_avg:40.18ms
step:1533/2330 train_time:61580ms step_avg:40.17ms
step:1534/2330 train_time:61640ms step_avg:40.18ms
step:1535/2330 train_time:61665ms step_avg:40.17ms
step:1536/2330 train_time:61724ms step_avg:40.18ms
step:1537/2330 train_time:61748ms step_avg:40.17ms
step:1538/2330 train_time:61805ms step_avg:40.19ms
step:1539/2330 train_time:61830ms step_avg:40.18ms
step:1540/2330 train_time:61886ms step_avg:40.19ms
step:1541/2330 train_time:61910ms step_avg:40.18ms
step:1542/2330 train_time:61967ms step_avg:40.19ms
step:1543/2330 train_time:61990ms step_avg:40.18ms
step:1544/2330 train_time:62047ms step_avg:40.19ms
step:1545/2330 train_time:62071ms step_avg:40.18ms
step:1546/2330 train_time:62127ms step_avg:40.19ms
step:1547/2330 train_time:62150ms step_avg:40.17ms
step:1548/2330 train_time:62206ms step_avg:40.18ms
step:1549/2330 train_time:62231ms step_avg:40.17ms
step:1550/2330 train_time:62288ms step_avg:40.19ms
step:1551/2330 train_time:62311ms step_avg:40.17ms
step:1552/2330 train_time:62369ms step_avg:40.19ms
step:1553/2330 train_time:62393ms step_avg:40.18ms
step:1554/2330 train_time:62451ms step_avg:40.19ms
step:1555/2330 train_time:62473ms step_avg:40.18ms
step:1556/2330 train_time:62531ms step_avg:40.19ms
step:1557/2330 train_time:62553ms step_avg:40.18ms
step:1558/2330 train_time:62613ms step_avg:40.19ms
step:1559/2330 train_time:62636ms step_avg:40.18ms
step:1560/2330 train_time:62696ms step_avg:40.19ms
step:1561/2330 train_time:62719ms step_avg:40.18ms
step:1562/2330 train_time:62779ms step_avg:40.19ms
step:1563/2330 train_time:62802ms step_avg:40.18ms
step:1564/2330 train_time:62860ms step_avg:40.19ms
step:1565/2330 train_time:62883ms step_avg:40.18ms
step:1566/2330 train_time:62940ms step_avg:40.19ms
step:1567/2330 train_time:62964ms step_avg:40.18ms
step:1568/2330 train_time:63021ms step_avg:40.19ms
step:1569/2330 train_time:63045ms step_avg:40.18ms
step:1570/2330 train_time:63102ms step_avg:40.19ms
step:1571/2330 train_time:63125ms step_avg:40.18ms
step:1572/2330 train_time:63183ms step_avg:40.19ms
step:1573/2330 train_time:63207ms step_avg:40.18ms
step:1574/2330 train_time:63263ms step_avg:40.19ms
step:1575/2330 train_time:63286ms step_avg:40.18ms
step:1576/2330 train_time:63343ms step_avg:40.19ms
step:1577/2330 train_time:63367ms step_avg:40.18ms
step:1578/2330 train_time:63424ms step_avg:40.19ms
step:1579/2330 train_time:63447ms step_avg:40.18ms
step:1580/2330 train_time:63504ms step_avg:40.19ms
step:1581/2330 train_time:63528ms step_avg:40.18ms
step:1582/2330 train_time:63586ms step_avg:40.19ms
step:1583/2330 train_time:63610ms step_avg:40.18ms
step:1584/2330 train_time:63667ms step_avg:40.19ms
step:1585/2330 train_time:63690ms step_avg:40.18ms
step:1586/2330 train_time:63747ms step_avg:40.19ms
step:1587/2330 train_time:63770ms step_avg:40.18ms
step:1588/2330 train_time:63828ms step_avg:40.19ms
step:1589/2330 train_time:63852ms step_avg:40.18ms
step:1590/2330 train_time:63909ms step_avg:40.19ms
step:1591/2330 train_time:63932ms step_avg:40.18ms
step:1592/2330 train_time:63990ms step_avg:40.19ms
step:1593/2330 train_time:64012ms step_avg:40.18ms
step:1594/2330 train_time:64071ms step_avg:40.20ms
step:1595/2330 train_time:64094ms step_avg:40.18ms
step:1596/2330 train_time:64152ms step_avg:40.20ms
step:1597/2330 train_time:64174ms step_avg:40.18ms
step:1598/2330 train_time:64232ms step_avg:40.20ms
step:1599/2330 train_time:64254ms step_avg:40.18ms
step:1600/2330 train_time:64312ms step_avg:40.20ms
step:1601/2330 train_time:64335ms step_avg:40.18ms
step:1602/2330 train_time:64393ms step_avg:40.20ms
step:1603/2330 train_time:64416ms step_avg:40.18ms
step:1604/2330 train_time:64474ms step_avg:40.20ms
step:1605/2330 train_time:64496ms step_avg:40.18ms
step:1606/2330 train_time:64555ms step_avg:40.20ms
step:1607/2330 train_time:64577ms step_avg:40.19ms
step:1608/2330 train_time:64635ms step_avg:40.20ms
step:1609/2330 train_time:64657ms step_avg:40.18ms
step:1610/2330 train_time:64715ms step_avg:40.20ms
step:1611/2330 train_time:64738ms step_avg:40.19ms
step:1612/2330 train_time:64796ms step_avg:40.20ms
step:1613/2330 train_time:64821ms step_avg:40.19ms
step:1614/2330 train_time:64879ms step_avg:40.20ms
step:1615/2330 train_time:64903ms step_avg:40.19ms
step:1616/2330 train_time:64960ms step_avg:40.20ms
step:1617/2330 train_time:64984ms step_avg:40.19ms
step:1618/2330 train_time:65041ms step_avg:40.20ms
step:1619/2330 train_time:65066ms step_avg:40.19ms
step:1620/2330 train_time:65123ms step_avg:40.20ms
step:1621/2330 train_time:65146ms step_avg:40.19ms
step:1622/2330 train_time:65203ms step_avg:40.20ms
step:1623/2330 train_time:65227ms step_avg:40.19ms
step:1624/2330 train_time:65284ms step_avg:40.20ms
step:1625/2330 train_time:65308ms step_avg:40.19ms
step:1626/2330 train_time:65365ms step_avg:40.20ms
step:1627/2330 train_time:65389ms step_avg:40.19ms
step:1628/2330 train_time:65446ms step_avg:40.20ms
step:1629/2330 train_time:65469ms step_avg:40.19ms
step:1630/2330 train_time:65527ms step_avg:40.20ms
step:1631/2330 train_time:65550ms step_avg:40.19ms
step:1632/2330 train_time:65606ms step_avg:40.20ms
step:1633/2330 train_time:65630ms step_avg:40.19ms
step:1634/2330 train_time:65686ms step_avg:40.20ms
step:1635/2330 train_time:65710ms step_avg:40.19ms
step:1636/2330 train_time:65767ms step_avg:40.20ms
step:1637/2330 train_time:65791ms step_avg:40.19ms
step:1638/2330 train_time:65850ms step_avg:40.20ms
step:1639/2330 train_time:65873ms step_avg:40.19ms
step:1640/2330 train_time:65931ms step_avg:40.20ms
step:1641/2330 train_time:65955ms step_avg:40.19ms
step:1642/2330 train_time:66013ms step_avg:40.20ms
step:1643/2330 train_time:66036ms step_avg:40.19ms
step:1644/2330 train_time:66094ms step_avg:40.20ms
step:1645/2330 train_time:66116ms step_avg:40.19ms
step:1646/2330 train_time:66175ms step_avg:40.20ms
step:1647/2330 train_time:66198ms step_avg:40.19ms
step:1648/2330 train_time:66256ms step_avg:40.20ms
step:1649/2330 train_time:66279ms step_avg:40.19ms
step:1650/2330 train_time:66337ms step_avg:40.20ms
step:1651/2330 train_time:66360ms step_avg:40.19ms
step:1652/2330 train_time:66419ms step_avg:40.20ms
step:1653/2330 train_time:66442ms step_avg:40.19ms
step:1654/2330 train_time:66499ms step_avg:40.21ms
step:1655/2330 train_time:66523ms step_avg:40.19ms
step:1656/2330 train_time:66580ms step_avg:40.21ms
step:1657/2330 train_time:66604ms step_avg:40.20ms
step:1658/2330 train_time:66661ms step_avg:40.21ms
step:1659/2330 train_time:66684ms step_avg:40.20ms
step:1660/2330 train_time:66742ms step_avg:40.21ms
step:1661/2330 train_time:66765ms step_avg:40.20ms
step:1662/2330 train_time:66822ms step_avg:40.21ms
step:1663/2330 train_time:66846ms step_avg:40.20ms
step:1664/2330 train_time:66903ms step_avg:40.21ms
step:1665/2330 train_time:66926ms step_avg:40.20ms
step:1666/2330 train_time:66983ms step_avg:40.21ms
step:1667/2330 train_time:67007ms step_avg:40.20ms
step:1668/2330 train_time:67064ms step_avg:40.21ms
step:1669/2330 train_time:67087ms step_avg:40.20ms
step:1670/2330 train_time:67144ms step_avg:40.21ms
step:1671/2330 train_time:67168ms step_avg:40.20ms
step:1672/2330 train_time:67224ms step_avg:40.21ms
step:1673/2330 train_time:67248ms step_avg:40.20ms
step:1674/2330 train_time:67305ms step_avg:40.21ms
step:1675/2330 train_time:67329ms step_avg:40.20ms
step:1676/2330 train_time:67385ms step_avg:40.21ms
step:1677/2330 train_time:67409ms step_avg:40.20ms
step:1678/2330 train_time:67465ms step_avg:40.21ms
step:1679/2330 train_time:67490ms step_avg:40.20ms
step:1680/2330 train_time:67547ms step_avg:40.21ms
step:1681/2330 train_time:67570ms step_avg:40.20ms
step:1682/2330 train_time:67627ms step_avg:40.21ms
step:1683/2330 train_time:67651ms step_avg:40.20ms
step:1684/2330 train_time:67708ms step_avg:40.21ms
step:1685/2330 train_time:67731ms step_avg:40.20ms
step:1686/2330 train_time:67789ms step_avg:40.21ms
step:1687/2330 train_time:67812ms step_avg:40.20ms
step:1688/2330 train_time:67870ms step_avg:40.21ms
step:1689/2330 train_time:67892ms step_avg:40.20ms
step:1690/2330 train_time:67951ms step_avg:40.21ms
step:1691/2330 train_time:67973ms step_avg:40.20ms
step:1692/2330 train_time:68031ms step_avg:40.21ms
step:1693/2330 train_time:68053ms step_avg:40.20ms
step:1694/2330 train_time:68111ms step_avg:40.21ms
step:1695/2330 train_time:68134ms step_avg:40.20ms
step:1696/2330 train_time:68190ms step_avg:40.21ms
step:1697/2330 train_time:68213ms step_avg:40.20ms
step:1698/2330 train_time:68272ms step_avg:40.21ms
step:1699/2330 train_time:68294ms step_avg:40.20ms
step:1700/2330 train_time:68352ms step_avg:40.21ms
step:1701/2330 train_time:68375ms step_avg:40.20ms
step:1702/2330 train_time:68433ms step_avg:40.21ms
step:1703/2330 train_time:68456ms step_avg:40.20ms
step:1704/2330 train_time:68514ms step_avg:40.21ms
step:1705/2330 train_time:68536ms step_avg:40.20ms
step:1706/2330 train_time:68594ms step_avg:40.21ms
step:1707/2330 train_time:68618ms step_avg:40.20ms
step:1708/2330 train_time:68677ms step_avg:40.21ms
step:1709/2330 train_time:68700ms step_avg:40.20ms
step:1710/2330 train_time:68758ms step_avg:40.21ms
step:1711/2330 train_time:68781ms step_avg:40.20ms
step:1712/2330 train_time:68840ms step_avg:40.21ms
step:1713/2330 train_time:68863ms step_avg:40.20ms
step:1714/2330 train_time:68920ms step_avg:40.21ms
step:1715/2330 train_time:68943ms step_avg:40.20ms
step:1716/2330 train_time:69001ms step_avg:40.21ms
step:1717/2330 train_time:69025ms step_avg:40.20ms
step:1718/2330 train_time:69082ms step_avg:40.21ms
step:1719/2330 train_time:69105ms step_avg:40.20ms
step:1720/2330 train_time:69162ms step_avg:40.21ms
step:1721/2330 train_time:69185ms step_avg:40.20ms
step:1722/2330 train_time:69243ms step_avg:40.21ms
step:1723/2330 train_time:69266ms step_avg:40.20ms
step:1724/2330 train_time:69324ms step_avg:40.21ms
step:1725/2330 train_time:69347ms step_avg:40.20ms
step:1726/2330 train_time:69404ms step_avg:40.21ms
step:1727/2330 train_time:69428ms step_avg:40.20ms
step:1728/2330 train_time:69486ms step_avg:40.21ms
step:1729/2330 train_time:69510ms step_avg:40.20ms
step:1730/2330 train_time:69567ms step_avg:40.21ms
step:1731/2330 train_time:69591ms step_avg:40.20ms
step:1732/2330 train_time:69648ms step_avg:40.21ms
step:1733/2330 train_time:69671ms step_avg:40.20ms
step:1734/2330 train_time:69728ms step_avg:40.21ms
step:1735/2330 train_time:69751ms step_avg:40.20ms
step:1736/2330 train_time:69808ms step_avg:40.21ms
step:1737/2330 train_time:69830ms step_avg:40.20ms
step:1738/2330 train_time:69887ms step_avg:40.21ms
step:1739/2330 train_time:69911ms step_avg:40.20ms
step:1740/2330 train_time:69968ms step_avg:40.21ms
step:1741/2330 train_time:69991ms step_avg:40.20ms
step:1742/2330 train_time:70049ms step_avg:40.21ms
step:1743/2330 train_time:70072ms step_avg:40.20ms
step:1744/2330 train_time:70130ms step_avg:40.21ms
step:1745/2330 train_time:70153ms step_avg:40.20ms
step:1746/2330 train_time:70212ms step_avg:40.21ms
step:1747/2330 train_time:70234ms step_avg:40.20ms
step:1748/2330 train_time:70292ms step_avg:40.21ms
step:1749/2330 train_time:70315ms step_avg:40.20ms
step:1750/2330 train_time:70373ms step_avg:40.21ms
step:1750/2330 val_loss:5.3645 train_time:70470ms step_avg:40.27ms
step:1751/2330 train_time:70483ms step_avg:40.25ms
step:1752/2330 train_time:70495ms step_avg:40.24ms
step:1753/2330 train_time:70506ms step_avg:40.22ms
step:1754/2330 train_time:70533ms step_avg:40.21ms
step:1755/2330 train_time:70555ms step_avg:40.20ms
step:1756/2330 train_time:70611ms step_avg:40.21ms
step:1757/2330 train_time:70633ms step_avg:40.20ms
step:1758/2330 train_time:70690ms step_avg:40.21ms
step:1759/2330 train_time:70712ms step_avg:40.20ms
step:1760/2330 train_time:70769ms step_avg:40.21ms
step:1761/2330 train_time:70798ms step_avg:40.20ms
step:1762/2330 train_time:70861ms step_avg:40.22ms
step:1763/2330 train_time:70886ms step_avg:40.21ms
step:1764/2330 train_time:70945ms step_avg:40.22ms
step:1765/2330 train_time:70968ms step_avg:40.21ms
step:1766/2330 train_time:71026ms step_avg:40.22ms
step:1767/2330 train_time:71049ms step_avg:40.21ms
step:1768/2330 train_time:71105ms step_avg:40.22ms
step:1769/2330 train_time:71129ms step_avg:40.21ms
step:1770/2330 train_time:71187ms step_avg:40.22ms
step:1771/2330 train_time:71209ms step_avg:40.21ms
step:1772/2330 train_time:71266ms step_avg:40.22ms
step:1773/2330 train_time:71289ms step_avg:40.21ms
step:1774/2330 train_time:71346ms step_avg:40.22ms
step:1775/2330 train_time:71369ms step_avg:40.21ms
step:1776/2330 train_time:71428ms step_avg:40.22ms
step:1777/2330 train_time:71451ms step_avg:40.21ms
step:1778/2330 train_time:71508ms step_avg:40.22ms
step:1779/2330 train_time:71531ms step_avg:40.21ms
step:1780/2330 train_time:71588ms step_avg:40.22ms
step:1781/2330 train_time:71611ms step_avg:40.21ms
step:1782/2330 train_time:71668ms step_avg:40.22ms
step:1783/2330 train_time:71691ms step_avg:40.21ms
step:1784/2330 train_time:71751ms step_avg:40.22ms
step:1785/2330 train_time:71776ms step_avg:40.21ms
step:1786/2330 train_time:71834ms step_avg:40.22ms
step:1787/2330 train_time:71860ms step_avg:40.21ms
step:1788/2330 train_time:71918ms step_avg:40.22ms
step:1789/2330 train_time:71942ms step_avg:40.21ms
step:1790/2330 train_time:71999ms step_avg:40.22ms
step:1791/2330 train_time:72021ms step_avg:40.21ms
step:1792/2330 train_time:72079ms step_avg:40.22ms
step:1793/2330 train_time:72101ms step_avg:40.21ms
step:1794/2330 train_time:72159ms step_avg:40.22ms
step:1795/2330 train_time:72182ms step_avg:40.21ms
step:1796/2330 train_time:72240ms step_avg:40.22ms
step:1797/2330 train_time:72262ms step_avg:40.21ms
step:1798/2330 train_time:72318ms step_avg:40.22ms
step:1799/2330 train_time:72341ms step_avg:40.21ms
step:1800/2330 train_time:72399ms step_avg:40.22ms
step:1801/2330 train_time:72422ms step_avg:40.21ms
step:1802/2330 train_time:72480ms step_avg:40.22ms
step:1803/2330 train_time:72502ms step_avg:40.21ms
step:1804/2330 train_time:72559ms step_avg:40.22ms
step:1805/2330 train_time:72582ms step_avg:40.21ms
step:1806/2330 train_time:72639ms step_avg:40.22ms
step:1807/2330 train_time:72662ms step_avg:40.21ms
step:1808/2330 train_time:72720ms step_avg:40.22ms
step:1809/2330 train_time:72744ms step_avg:40.21ms
step:1810/2330 train_time:72802ms step_avg:40.22ms
step:1811/2330 train_time:72826ms step_avg:40.21ms
step:1812/2330 train_time:72884ms step_avg:40.22ms
step:1813/2330 train_time:72908ms step_avg:40.21ms
step:1814/2330 train_time:72966ms step_avg:40.22ms
step:1815/2330 train_time:72990ms step_avg:40.21ms
step:1816/2330 train_time:73048ms step_avg:40.22ms
step:1817/2330 train_time:73072ms step_avg:40.22ms
step:1818/2330 train_time:73130ms step_avg:40.23ms
step:1819/2330 train_time:73153ms step_avg:40.22ms
step:1820/2330 train_time:73211ms step_avg:40.23ms
step:1821/2330 train_time:73234ms step_avg:40.22ms
step:1822/2330 train_time:73291ms step_avg:40.23ms
step:1823/2330 train_time:73314ms step_avg:40.22ms
step:1824/2330 train_time:73371ms step_avg:40.23ms
step:1825/2330 train_time:73395ms step_avg:40.22ms
step:1826/2330 train_time:73451ms step_avg:40.23ms
step:1827/2330 train_time:73474ms step_avg:40.22ms
step:1828/2330 train_time:73531ms step_avg:40.23ms
step:1829/2330 train_time:73555ms step_avg:40.22ms
step:1830/2330 train_time:73612ms step_avg:40.23ms
step:1831/2330 train_time:73636ms step_avg:40.22ms
step:1832/2330 train_time:73692ms step_avg:40.23ms
step:1833/2330 train_time:73716ms step_avg:40.22ms
step:1834/2330 train_time:73773ms step_avg:40.23ms
step:1835/2330 train_time:73797ms step_avg:40.22ms
step:1836/2330 train_time:73855ms step_avg:40.23ms
step:1837/2330 train_time:73879ms step_avg:40.22ms
step:1838/2330 train_time:73936ms step_avg:40.23ms
step:1839/2330 train_time:73960ms step_avg:40.22ms
step:1840/2330 train_time:74018ms step_avg:40.23ms
step:1841/2330 train_time:74042ms step_avg:40.22ms
step:1842/2330 train_time:74099ms step_avg:40.23ms
step:1843/2330 train_time:74121ms step_avg:40.22ms
step:1844/2330 train_time:74179ms step_avg:40.23ms
step:1845/2330 train_time:74202ms step_avg:40.22ms
step:1846/2330 train_time:74260ms step_avg:40.23ms
step:1847/2330 train_time:74282ms step_avg:40.22ms
step:1848/2330 train_time:74340ms step_avg:40.23ms
step:1849/2330 train_time:74363ms step_avg:40.22ms
step:1850/2330 train_time:74420ms step_avg:40.23ms
step:1851/2330 train_time:74442ms step_avg:40.22ms
step:1852/2330 train_time:74500ms step_avg:40.23ms
step:1853/2330 train_time:74522ms step_avg:40.22ms
step:1854/2330 train_time:74580ms step_avg:40.23ms
step:1855/2330 train_time:74603ms step_avg:40.22ms
step:1856/2330 train_time:74660ms step_avg:40.23ms
step:1857/2330 train_time:74683ms step_avg:40.22ms
step:1858/2330 train_time:74742ms step_avg:40.23ms
step:1859/2330 train_time:74765ms step_avg:40.22ms
step:1860/2330 train_time:74823ms step_avg:40.23ms
step:1861/2330 train_time:74846ms step_avg:40.22ms
step:1862/2330 train_time:74905ms step_avg:40.23ms
step:1863/2330 train_time:74928ms step_avg:40.22ms
step:1864/2330 train_time:74986ms step_avg:40.23ms
step:1865/2330 train_time:75009ms step_avg:40.22ms
step:1866/2330 train_time:75068ms step_avg:40.23ms
step:1867/2330 train_time:75092ms step_avg:40.22ms
step:1868/2330 train_time:75149ms step_avg:40.23ms
step:1869/2330 train_time:75173ms step_avg:40.22ms
step:1870/2330 train_time:75230ms step_avg:40.23ms
step:1871/2330 train_time:75253ms step_avg:40.22ms
step:1872/2330 train_time:75311ms step_avg:40.23ms
step:1873/2330 train_time:75334ms step_avg:40.22ms
step:1874/2330 train_time:75391ms step_avg:40.23ms
step:1875/2330 train_time:75414ms step_avg:40.22ms
step:1876/2330 train_time:75471ms step_avg:40.23ms
step:1877/2330 train_time:75494ms step_avg:40.22ms
step:1878/2330 train_time:75552ms step_avg:40.23ms
step:1879/2330 train_time:75575ms step_avg:40.22ms
step:1880/2330 train_time:75632ms step_avg:40.23ms
step:1881/2330 train_time:75655ms step_avg:40.22ms
step:1882/2330 train_time:75712ms step_avg:40.23ms
step:1883/2330 train_time:75736ms step_avg:40.22ms
step:1884/2330 train_time:75794ms step_avg:40.23ms
step:1885/2330 train_time:75818ms step_avg:40.22ms
step:1886/2330 train_time:75875ms step_avg:40.23ms
step:1887/2330 train_time:75898ms step_avg:40.22ms
step:1888/2330 train_time:75955ms step_avg:40.23ms
step:1889/2330 train_time:75979ms step_avg:40.22ms
step:1890/2330 train_time:76036ms step_avg:40.23ms
step:1891/2330 train_time:76059ms step_avg:40.22ms
step:1892/2330 train_time:76116ms step_avg:40.23ms
step:1893/2330 train_time:76139ms step_avg:40.22ms
step:1894/2330 train_time:76196ms step_avg:40.23ms
step:1895/2330 train_time:76220ms step_avg:40.22ms
step:1896/2330 train_time:76278ms step_avg:40.23ms
step:1897/2330 train_time:76301ms step_avg:40.22ms
step:1898/2330 train_time:76359ms step_avg:40.23ms
step:1899/2330 train_time:76382ms step_avg:40.22ms
step:1900/2330 train_time:76439ms step_avg:40.23ms
step:1901/2330 train_time:76462ms step_avg:40.22ms
step:1902/2330 train_time:76520ms step_avg:40.23ms
step:1903/2330 train_time:76542ms step_avg:40.22ms
step:1904/2330 train_time:76600ms step_avg:40.23ms
step:1905/2330 train_time:76623ms step_avg:40.22ms
step:1906/2330 train_time:76680ms step_avg:40.23ms
step:1907/2330 train_time:76702ms step_avg:40.22ms
step:1908/2330 train_time:76761ms step_avg:40.23ms
step:1909/2330 train_time:76783ms step_avg:40.22ms
step:1910/2330 train_time:76842ms step_avg:40.23ms
step:1911/2330 train_time:76864ms step_avg:40.22ms
step:1912/2330 train_time:76922ms step_avg:40.23ms
step:1913/2330 train_time:76944ms step_avg:40.22ms
step:1914/2330 train_time:77002ms step_avg:40.23ms
step:1915/2330 train_time:77025ms step_avg:40.22ms
step:1916/2330 train_time:77083ms step_avg:40.23ms
step:1917/2330 train_time:77106ms step_avg:40.22ms
step:1918/2330 train_time:77164ms step_avg:40.23ms
step:1919/2330 train_time:77187ms step_avg:40.22ms
step:1920/2330 train_time:77246ms step_avg:40.23ms
step:1921/2330 train_time:77269ms step_avg:40.22ms
step:1922/2330 train_time:77326ms step_avg:40.23ms
step:1923/2330 train_time:77350ms step_avg:40.22ms
step:1924/2330 train_time:77407ms step_avg:40.23ms
step:1925/2330 train_time:77430ms step_avg:40.22ms
step:1926/2330 train_time:77488ms step_avg:40.23ms
step:1927/2330 train_time:77511ms step_avg:40.22ms
step:1928/2330 train_time:77569ms step_avg:40.23ms
step:1929/2330 train_time:77592ms step_avg:40.22ms
step:1930/2330 train_time:77650ms step_avg:40.23ms
step:1931/2330 train_time:77674ms step_avg:40.22ms
step:1932/2330 train_time:77731ms step_avg:40.23ms
step:1933/2330 train_time:77755ms step_avg:40.23ms
step:1934/2330 train_time:77813ms step_avg:40.23ms
step:1935/2330 train_time:77836ms step_avg:40.23ms
step:1936/2330 train_time:77893ms step_avg:40.23ms
step:1937/2330 train_time:77917ms step_avg:40.23ms
step:1938/2330 train_time:77973ms step_avg:40.23ms
step:1939/2330 train_time:77997ms step_avg:40.23ms
step:1940/2330 train_time:78054ms step_avg:40.23ms
step:1941/2330 train_time:78078ms step_avg:40.23ms
step:1942/2330 train_time:78135ms step_avg:40.23ms
step:1943/2330 train_time:78160ms step_avg:40.23ms
step:1944/2330 train_time:78217ms step_avg:40.24ms
step:1945/2330 train_time:78242ms step_avg:40.23ms
step:1946/2330 train_time:78299ms step_avg:40.24ms
step:1947/2330 train_time:78323ms step_avg:40.23ms
step:1948/2330 train_time:78380ms step_avg:40.24ms
step:1949/2330 train_time:78403ms step_avg:40.23ms
step:1950/2330 train_time:78460ms step_avg:40.24ms
step:1951/2330 train_time:78483ms step_avg:40.23ms
step:1952/2330 train_time:78542ms step_avg:40.24ms
step:1953/2330 train_time:78564ms step_avg:40.23ms
step:1954/2330 train_time:78622ms step_avg:40.24ms
step:1955/2330 train_time:78646ms step_avg:40.23ms
step:1956/2330 train_time:78705ms step_avg:40.24ms
step:1957/2330 train_time:78728ms step_avg:40.23ms
step:1958/2330 train_time:78786ms step_avg:40.24ms
step:1959/2330 train_time:78809ms step_avg:40.23ms
step:1960/2330 train_time:78868ms step_avg:40.24ms
step:1961/2330 train_time:78891ms step_avg:40.23ms
step:1962/2330 train_time:78949ms step_avg:40.24ms
step:1963/2330 train_time:78972ms step_avg:40.23ms
step:1964/2330 train_time:79030ms step_avg:40.24ms
step:1965/2330 train_time:79054ms step_avg:40.23ms
step:1966/2330 train_time:79111ms step_avg:40.24ms
step:1967/2330 train_time:79135ms step_avg:40.23ms
step:1968/2330 train_time:79192ms step_avg:40.24ms
step:1969/2330 train_time:79216ms step_avg:40.23ms
step:1970/2330 train_time:79274ms step_avg:40.24ms
step:1971/2330 train_time:79298ms step_avg:40.23ms
step:1972/2330 train_time:79355ms step_avg:40.24ms
step:1973/2330 train_time:79379ms step_avg:40.23ms
step:1974/2330 train_time:79436ms step_avg:40.24ms
step:1975/2330 train_time:79459ms step_avg:40.23ms
step:1976/2330 train_time:79516ms step_avg:40.24ms
step:1977/2330 train_time:79539ms step_avg:40.23ms
step:1978/2330 train_time:79596ms step_avg:40.24ms
step:1979/2330 train_time:79619ms step_avg:40.23ms
step:1980/2330 train_time:79677ms step_avg:40.24ms
step:1981/2330 train_time:79701ms step_avg:40.23ms
step:1982/2330 train_time:79759ms step_avg:40.24ms
step:1983/2330 train_time:79782ms step_avg:40.23ms
step:1984/2330 train_time:79840ms step_avg:40.24ms
step:1985/2330 train_time:79864ms step_avg:40.23ms
step:1986/2330 train_time:79921ms step_avg:40.24ms
step:1987/2330 train_time:79945ms step_avg:40.23ms
step:1988/2330 train_time:80004ms step_avg:40.24ms
step:1989/2330 train_time:80027ms step_avg:40.23ms
step:1990/2330 train_time:80085ms step_avg:40.24ms
step:1991/2330 train_time:80107ms step_avg:40.23ms
step:1992/2330 train_time:80165ms step_avg:40.24ms
step:1993/2330 train_time:80190ms step_avg:40.24ms
step:1994/2330 train_time:80248ms step_avg:40.24ms
step:1995/2330 train_time:80272ms step_avg:40.24ms
step:1996/2330 train_time:80330ms step_avg:40.25ms
step:1997/2330 train_time:80354ms step_avg:40.24ms
step:1998/2330 train_time:80411ms step_avg:40.25ms
step:1999/2330 train_time:80434ms step_avg:40.24ms
step:2000/2330 train_time:80491ms step_avg:40.25ms
step:2000/2330 val_loss:5.3201 train_time:80590ms step_avg:40.30ms
step:2001/2330 train_time:80603ms step_avg:40.28ms
step:2002/2330 train_time:80616ms step_avg:40.27ms
step:2003/2330 train_time:80626ms step_avg:40.25ms
step:2004/2330 train_time:80654ms step_avg:40.25ms
step:2005/2330 train_time:80675ms step_avg:40.24ms
step:2006/2330 train_time:80732ms step_avg:40.25ms
step:2007/2330 train_time:80753ms step_avg:40.24ms
step:2008/2330 train_time:80810ms step_avg:40.24ms
step:2009/2330 train_time:80832ms step_avg:40.23ms
step:2010/2330 train_time:80889ms step_avg:40.24ms
step:2011/2330 train_time:80916ms step_avg:40.24ms
step:2012/2330 train_time:80978ms step_avg:40.25ms
step:2013/2330 train_time:81003ms step_avg:40.24ms
step:2014/2330 train_time:81061ms step_avg:40.25ms
step:2015/2330 train_time:81085ms step_avg:40.24ms
step:2016/2330 train_time:81143ms step_avg:40.25ms
step:2017/2330 train_time:81167ms step_avg:40.24ms
step:2018/2330 train_time:81223ms step_avg:40.25ms
step:2019/2330 train_time:81246ms step_avg:40.24ms
step:2020/2330 train_time:81303ms step_avg:40.25ms
step:2021/2330 train_time:81326ms step_avg:40.24ms
step:2022/2330 train_time:81383ms step_avg:40.25ms
step:2023/2330 train_time:81406ms step_avg:40.24ms
step:2024/2330 train_time:81463ms step_avg:40.25ms
step:2025/2330 train_time:81487ms step_avg:40.24ms
step:2026/2330 train_time:81545ms step_avg:40.25ms
step:2027/2330 train_time:81568ms step_avg:40.24ms
step:2028/2330 train_time:81625ms step_avg:40.25ms
step:2029/2330 train_time:81648ms step_avg:40.24ms
step:2030/2330 train_time:81705ms step_avg:40.25ms
step:2031/2330 train_time:81728ms step_avg:40.24ms
step:2032/2330 train_time:81785ms step_avg:40.25ms
step:2033/2330 train_time:81808ms step_avg:40.24ms
step:2034/2330 train_time:81867ms step_avg:40.25ms
step:2035/2330 train_time:81892ms step_avg:40.24ms
step:2036/2330 train_time:81950ms step_avg:40.25ms
step:2037/2330 train_time:81975ms step_avg:40.24ms
step:2038/2330 train_time:82033ms step_avg:40.25ms
step:2039/2330 train_time:82057ms step_avg:40.24ms
step:2040/2330 train_time:82115ms step_avg:40.25ms
step:2041/2330 train_time:82138ms step_avg:40.24ms
step:2042/2330 train_time:82195ms step_avg:40.25ms
step:2043/2330 train_time:82218ms step_avg:40.24ms
step:2044/2330 train_time:82275ms step_avg:40.25ms
step:2045/2330 train_time:82298ms step_avg:40.24ms
step:2046/2330 train_time:82356ms step_avg:40.25ms
step:2047/2330 train_time:82379ms step_avg:40.24ms
step:2048/2330 train_time:82438ms step_avg:40.25ms
step:2049/2330 train_time:82460ms step_avg:40.24ms
step:2050/2330 train_time:82518ms step_avg:40.25ms
step:2051/2330 train_time:82541ms step_avg:40.24ms
step:2052/2330 train_time:82599ms step_avg:40.25ms
step:2053/2330 train_time:82622ms step_avg:40.24ms
step:2054/2330 train_time:82679ms step_avg:40.25ms
step:2055/2330 train_time:82702ms step_avg:40.24ms
step:2056/2330 train_time:82760ms step_avg:40.25ms
step:2057/2330 train_time:82783ms step_avg:40.24ms
step:2058/2330 train_time:82841ms step_avg:40.25ms
step:2059/2330 train_time:82865ms step_avg:40.25ms
step:2060/2330 train_time:82924ms step_avg:40.25ms
step:2061/2330 train_time:82948ms step_avg:40.25ms
step:2062/2330 train_time:83006ms step_avg:40.25ms
step:2063/2330 train_time:83030ms step_avg:40.25ms
step:2064/2330 train_time:83087ms step_avg:40.26ms
step:2065/2330 train_time:83111ms step_avg:40.25ms
step:2066/2330 train_time:83168ms step_avg:40.26ms
step:2067/2330 train_time:83192ms step_avg:40.25ms
step:2068/2330 train_time:83249ms step_avg:40.26ms
step:2069/2330 train_time:83273ms step_avg:40.25ms
step:2070/2330 train_time:83329ms step_avg:40.26ms
step:2071/2330 train_time:83353ms step_avg:40.25ms
step:2072/2330 train_time:83409ms step_avg:40.26ms
step:2073/2330 train_time:83433ms step_avg:40.25ms
step:2074/2330 train_time:83490ms step_avg:40.26ms
step:2075/2330 train_time:83513ms step_avg:40.25ms
step:2076/2330 train_time:83569ms step_avg:40.25ms
step:2077/2330 train_time:83593ms step_avg:40.25ms
step:2078/2330 train_time:83651ms step_avg:40.26ms
step:2079/2330 train_time:83674ms step_avg:40.25ms
step:2080/2330 train_time:83732ms step_avg:40.26ms
step:2081/2330 train_time:83755ms step_avg:40.25ms
step:2082/2330 train_time:83814ms step_avg:40.26ms
step:2083/2330 train_time:83837ms step_avg:40.25ms
step:2084/2330 train_time:83895ms step_avg:40.26ms
step:2085/2330 train_time:83918ms step_avg:40.25ms
step:2086/2330 train_time:83977ms step_avg:40.26ms
step:2087/2330 train_time:84001ms step_avg:40.25ms
step:2088/2330 train_time:84060ms step_avg:40.26ms
step:2089/2330 train_time:84083ms step_avg:40.25ms
step:2090/2330 train_time:84142ms step_avg:40.26ms
step:2091/2330 train_time:84165ms step_avg:40.25ms
step:2092/2330 train_time:84223ms step_avg:40.26ms
step:2093/2330 train_time:84246ms step_avg:40.25ms
step:2094/2330 train_time:84303ms step_avg:40.26ms
step:2095/2330 train_time:84326ms step_avg:40.25ms
step:2096/2330 train_time:84384ms step_avg:40.26ms
step:2097/2330 train_time:84407ms step_avg:40.25ms
step:2098/2330 train_time:84465ms step_avg:40.26ms
step:2099/2330 train_time:84488ms step_avg:40.25ms
step:2100/2330 train_time:84544ms step_avg:40.26ms
step:2101/2330 train_time:84568ms step_avg:40.25ms
step:2102/2330 train_time:84625ms step_avg:40.26ms
step:2103/2330 train_time:84649ms step_avg:40.25ms
step:2104/2330 train_time:84706ms step_avg:40.26ms
step:2105/2330 train_time:84729ms step_avg:40.25ms
step:2106/2330 train_time:84786ms step_avg:40.26ms
step:2107/2330 train_time:84810ms step_avg:40.25ms
step:2108/2330 train_time:84869ms step_avg:40.26ms
step:2109/2330 train_time:84892ms step_avg:40.25ms
step:2110/2330 train_time:84949ms step_avg:40.26ms
step:2111/2330 train_time:84972ms step_avg:40.25ms
step:2112/2330 train_time:85030ms step_avg:40.26ms
step:2113/2330 train_time:85054ms step_avg:40.25ms
step:2114/2330 train_time:85110ms step_avg:40.26ms
step:2115/2330 train_time:85134ms step_avg:40.25ms
step:2116/2330 train_time:85191ms step_avg:40.26ms
step:2117/2330 train_time:85213ms step_avg:40.25ms
step:2118/2330 train_time:85270ms step_avg:40.26ms
step:2119/2330 train_time:85294ms step_avg:40.25ms
step:2120/2330 train_time:85352ms step_avg:40.26ms
step:2121/2330 train_time:85374ms step_avg:40.25ms
step:2122/2330 train_time:85432ms step_avg:40.26ms
step:2123/2330 train_time:85455ms step_avg:40.25ms
step:2124/2330 train_time:85513ms step_avg:40.26ms
step:2125/2330 train_time:85536ms step_avg:40.25ms
step:2126/2330 train_time:85593ms step_avg:40.26ms
step:2127/2330 train_time:85615ms step_avg:40.25ms
step:2128/2330 train_time:85673ms step_avg:40.26ms
step:2129/2330 train_time:85696ms step_avg:40.25ms
step:2130/2330 train_time:85754ms step_avg:40.26ms
step:2131/2330 train_time:85777ms step_avg:40.25ms
step:2132/2330 train_time:85835ms step_avg:40.26ms
step:2133/2330 train_time:85858ms step_avg:40.25ms
step:2134/2330 train_time:85915ms step_avg:40.26ms
step:2135/2330 train_time:85937ms step_avg:40.25ms
step:2136/2330 train_time:85995ms step_avg:40.26ms
step:2137/2330 train_time:86018ms step_avg:40.25ms
step:2138/2330 train_time:86076ms step_avg:40.26ms
step:2139/2330 train_time:86098ms step_avg:40.25ms
step:2140/2330 train_time:86157ms step_avg:40.26ms
step:2141/2330 train_time:86179ms step_avg:40.25ms
step:2142/2330 train_time:86237ms step_avg:40.26ms
step:2143/2330 train_time:86260ms step_avg:40.25ms
step:2144/2330 train_time:86317ms step_avg:40.26ms
step:2145/2330 train_time:86341ms step_avg:40.25ms
step:2146/2330 train_time:86398ms step_avg:40.26ms
step:2147/2330 train_time:86422ms step_avg:40.25ms
step:2148/2330 train_time:86479ms step_avg:40.26ms
step:2149/2330 train_time:86503ms step_avg:40.25ms
step:2150/2330 train_time:86562ms step_avg:40.26ms
step:2151/2330 train_time:86585ms step_avg:40.25ms
step:2152/2330 train_time:86643ms step_avg:40.26ms
step:2153/2330 train_time:86666ms step_avg:40.25ms
step:2154/2330 train_time:86724ms step_avg:40.26ms
step:2155/2330 train_time:86748ms step_avg:40.25ms
step:2156/2330 train_time:86805ms step_avg:40.26ms
step:2157/2330 train_time:86828ms step_avg:40.25ms
step:2158/2330 train_time:86886ms step_avg:40.26ms
step:2159/2330 train_time:86910ms step_avg:40.25ms
step:2160/2330 train_time:86966ms step_avg:40.26ms
step:2161/2330 train_time:86991ms step_avg:40.25ms
step:2162/2330 train_time:87048ms step_avg:40.26ms
step:2163/2330 train_time:87072ms step_avg:40.26ms
step:2164/2330 train_time:87129ms step_avg:40.26ms
step:2165/2330 train_time:87153ms step_avg:40.26ms
step:2166/2330 train_time:87210ms step_avg:40.26ms
step:2167/2330 train_time:87233ms step_avg:40.26ms
step:2168/2330 train_time:87290ms step_avg:40.26ms
step:2169/2330 train_time:87314ms step_avg:40.26ms
step:2170/2330 train_time:87371ms step_avg:40.26ms
step:2171/2330 train_time:87394ms step_avg:40.26ms
step:2172/2330 train_time:87451ms step_avg:40.26ms
step:2173/2330 train_time:87474ms step_avg:40.26ms
step:2174/2330 train_time:87532ms step_avg:40.26ms
step:2175/2330 train_time:87555ms step_avg:40.26ms
step:2176/2330 train_time:87614ms step_avg:40.26ms
step:2177/2330 train_time:87637ms step_avg:40.26ms
step:2178/2330 train_time:87695ms step_avg:40.26ms
step:2179/2330 train_time:87718ms step_avg:40.26ms
step:2180/2330 train_time:87775ms step_avg:40.26ms
step:2181/2330 train_time:87799ms step_avg:40.26ms
step:2182/2330 train_time:87857ms step_avg:40.26ms
step:2183/2330 train_time:87880ms step_avg:40.26ms
step:2184/2330 train_time:87938ms step_avg:40.26ms
step:2185/2330 train_time:87961ms step_avg:40.26ms
step:2186/2330 train_time:88020ms step_avg:40.27ms
step:2187/2330 train_time:88042ms step_avg:40.26ms
step:2188/2330 train_time:88100ms step_avg:40.27ms
step:2189/2330 train_time:88123ms step_avg:40.26ms
step:2190/2330 train_time:88181ms step_avg:40.27ms
step:2191/2330 train_time:88205ms step_avg:40.26ms
step:2192/2330 train_time:88263ms step_avg:40.27ms
step:2193/2330 train_time:88287ms step_avg:40.26ms
step:2194/2330 train_time:88343ms step_avg:40.27ms
step:2195/2330 train_time:88366ms step_avg:40.26ms
step:2196/2330 train_time:88424ms step_avg:40.27ms
step:2197/2330 train_time:88448ms step_avg:40.26ms
step:2198/2330 train_time:88506ms step_avg:40.27ms
step:2199/2330 train_time:88530ms step_avg:40.26ms
step:2200/2330 train_time:88587ms step_avg:40.27ms
step:2201/2330 train_time:88611ms step_avg:40.26ms
step:2202/2330 train_time:88668ms step_avg:40.27ms
step:2203/2330 train_time:88692ms step_avg:40.26ms
step:2204/2330 train_time:88748ms step_avg:40.27ms
step:2205/2330 train_time:88772ms step_avg:40.26ms
step:2206/2330 train_time:88828ms step_avg:40.27ms
step:2207/2330 train_time:88853ms step_avg:40.26ms
step:2208/2330 train_time:88910ms step_avg:40.27ms
step:2209/2330 train_time:88934ms step_avg:40.26ms
step:2210/2330 train_time:88991ms step_avg:40.27ms
step:2211/2330 train_time:89014ms step_avg:40.26ms
step:2212/2330 train_time:89071ms step_avg:40.27ms
step:2213/2330 train_time:89094ms step_avg:40.26ms
step:2214/2330 train_time:89152ms step_avg:40.27ms
step:2215/2330 train_time:89174ms step_avg:40.26ms
step:2216/2330 train_time:89232ms step_avg:40.27ms
step:2217/2330 train_time:89254ms step_avg:40.26ms
step:2218/2330 train_time:89313ms step_avg:40.27ms
step:2219/2330 train_time:89336ms step_avg:40.26ms
step:2220/2330 train_time:89394ms step_avg:40.27ms
step:2221/2330 train_time:89417ms step_avg:40.26ms
step:2222/2330 train_time:89474ms step_avg:40.27ms
step:2223/2330 train_time:89496ms step_avg:40.26ms
step:2224/2330 train_time:89555ms step_avg:40.27ms
step:2225/2330 train_time:89577ms step_avg:40.26ms
step:2226/2330 train_time:89635ms step_avg:40.27ms
step:2227/2330 train_time:89658ms step_avg:40.26ms
step:2228/2330 train_time:89716ms step_avg:40.27ms
step:2229/2330 train_time:89738ms step_avg:40.26ms
step:2230/2330 train_time:89796ms step_avg:40.27ms
step:2231/2330 train_time:89819ms step_avg:40.26ms
step:2232/2330 train_time:89877ms step_avg:40.27ms
step:2233/2330 train_time:89900ms step_avg:40.26ms
step:2234/2330 train_time:89959ms step_avg:40.27ms
step:2235/2330 train_time:89982ms step_avg:40.26ms
step:2236/2330 train_time:90041ms step_avg:40.27ms
step:2237/2330 train_time:90064ms step_avg:40.26ms
step:2238/2330 train_time:90122ms step_avg:40.27ms
step:2239/2330 train_time:90145ms step_avg:40.26ms
step:2240/2330 train_time:90202ms step_avg:40.27ms
step:2241/2330 train_time:90225ms step_avg:40.26ms
step:2242/2330 train_time:90283ms step_avg:40.27ms
step:2243/2330 train_time:90307ms step_avg:40.26ms
step:2244/2330 train_time:90364ms step_avg:40.27ms
step:2245/2330 train_time:90388ms step_avg:40.26ms
step:2246/2330 train_time:90445ms step_avg:40.27ms
step:2247/2330 train_time:90469ms step_avg:40.26ms
step:2248/2330 train_time:90526ms step_avg:40.27ms
step:2249/2330 train_time:90550ms step_avg:40.26ms
step:2250/2330 train_time:90607ms step_avg:40.27ms
step:2250/2330 val_loss:5.2883 train_time:90705ms step_avg:40.31ms
step:2251/2330 train_time:90718ms step_avg:40.30ms
step:2252/2330 train_time:90731ms step_avg:40.29ms
step:2253/2330 train_time:90742ms step_avg:40.28ms
step:2254/2330 train_time:90768ms step_avg:40.27ms
step:2255/2330 train_time:90790ms step_avg:40.26ms
step:2256/2330 train_time:90847ms step_avg:40.27ms
step:2257/2330 train_time:90870ms step_avg:40.26ms
step:2258/2330 train_time:90926ms step_avg:40.27ms
step:2259/2330 train_time:90949ms step_avg:40.26ms
step:2260/2330 train_time:91009ms step_avg:40.27ms
step:2261/2330 train_time:91036ms step_avg:40.26ms
step:2262/2330 train_time:91096ms step_avg:40.27ms
step:2263/2330 train_time:91121ms step_avg:40.27ms
step:2264/2330 train_time:91178ms step_avg:40.27ms
step:2265/2330 train_time:91201ms step_avg:40.27ms
step:2266/2330 train_time:91258ms step_avg:40.27ms
step:2267/2330 train_time:91281ms step_avg:40.27ms
step:2268/2330 train_time:91338ms step_avg:40.27ms
step:2269/2330 train_time:91360ms step_avg:40.26ms
step:2270/2330 train_time:91417ms step_avg:40.27ms
step:2271/2330 train_time:91439ms step_avg:40.26ms
step:2272/2330 train_time:91495ms step_avg:40.27ms
step:2273/2330 train_time:91518ms step_avg:40.26ms
step:2274/2330 train_time:91574ms step_avg:40.27ms
step:2275/2330 train_time:91597ms step_avg:40.26ms
step:2276/2330 train_time:91656ms step_avg:40.27ms
step:2277/2330 train_time:91679ms step_avg:40.26ms
step:2278/2330 train_time:91736ms step_avg:40.27ms
step:2279/2330 train_time:91760ms step_avg:40.26ms
step:2280/2330 train_time:91816ms step_avg:40.27ms
step:2281/2330 train_time:91840ms step_avg:40.26ms
step:2282/2330 train_time:91897ms step_avg:40.27ms
step:2283/2330 train_time:91920ms step_avg:40.26ms
step:2284/2330 train_time:91978ms step_avg:40.27ms
step:2285/2330 train_time:92002ms step_avg:40.26ms
step:2286/2330 train_time:92060ms step_avg:40.27ms
step:2287/2330 train_time:92084ms step_avg:40.26ms
step:2288/2330 train_time:92142ms step_avg:40.27ms
step:2289/2330 train_time:92164ms step_avg:40.26ms
step:2290/2330 train_time:92222ms step_avg:40.27ms
step:2291/2330 train_time:92245ms step_avg:40.26ms
step:2292/2330 train_time:92302ms step_avg:40.27ms
step:2293/2330 train_time:92325ms step_avg:40.26ms
step:2294/2330 train_time:92383ms step_avg:40.27ms
step:2295/2330 train_time:92405ms step_avg:40.26ms
step:2296/2330 train_time:92462ms step_avg:40.27ms
step:2297/2330 train_time:92485ms step_avg:40.26ms
step:2298/2330 train_time:92542ms step_avg:40.27ms
step:2299/2330 train_time:92565ms step_avg:40.26ms
step:2300/2330 train_time:92623ms step_avg:40.27ms
step:2301/2330 train_time:92646ms step_avg:40.26ms
step:2302/2330 train_time:92704ms step_avg:40.27ms
step:2303/2330 train_time:92727ms step_avg:40.26ms
step:2304/2330 train_time:92785ms step_avg:40.27ms
step:2305/2330 train_time:92807ms step_avg:40.26ms
step:2306/2330 train_time:92865ms step_avg:40.27ms
step:2307/2330 train_time:92888ms step_avg:40.26ms
step:2308/2330 train_time:92947ms step_avg:40.27ms
step:2309/2330 train_time:92970ms step_avg:40.26ms
step:2310/2330 train_time:93028ms step_avg:40.27ms
step:2311/2330 train_time:93052ms step_avg:40.26ms
step:2312/2330 train_time:93109ms step_avg:40.27ms
step:2313/2330 train_time:93132ms step_avg:40.26ms
step:2314/2330 train_time:93190ms step_avg:40.27ms
step:2315/2330 train_time:93214ms step_avg:40.27ms
step:2316/2330 train_time:93272ms step_avg:40.27ms
step:2317/2330 train_time:93295ms step_avg:40.27ms
step:2318/2330 train_time:93352ms step_avg:40.27ms
step:2319/2330 train_time:93375ms step_avg:40.27ms
step:2320/2330 train_time:93433ms step_avg:40.27ms
step:2321/2330 train_time:93456ms step_avg:40.27ms
step:2322/2330 train_time:93513ms step_avg:40.27ms
step:2323/2330 train_time:93537ms step_avg:40.27ms
step:2324/2330 train_time:93595ms step_avg:40.27ms
step:2325/2330 train_time:93618ms step_avg:40.27ms
step:2326/2330 train_time:93675ms step_avg:40.27ms
step:2327/2330 train_time:93699ms step_avg:40.27ms
step:2328/2330 train_time:93755ms step_avg:40.27ms
step:2329/2330 train_time:93779ms step_avg:40.27ms
step:2330/2330 train_time:93836ms step_avg:40.27ms
step:2330/2330 val_loss:5.2795 train_time:93935ms step_avg:40.32ms
peak memory allocated: 29494 MiB reserved: 38888 MiB
