import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = divide_by_norm(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = divide_by_norm(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_gd_lr3e-4"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=3e-4, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 05:03:54 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   33C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   31C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   31C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:91ms step_avg:90.88ms
step:2/2330 train_time:159ms step_avg:79.48ms
step:3/2330 train_time:172ms step_avg:57.36ms
step:4/2330 train_time:190ms step_avg:47.58ms
step:5/2330 train_time:201ms step_avg:40.28ms
step:6/2330 train_time:219ms step_avg:36.53ms
step:7/2330 train_time:240ms step_avg:34.33ms
step:8/2330 train_time:295ms step_avg:36.89ms
step:9/2330 train_time:317ms step_avg:35.18ms
step:10/2330 train_time:372ms step_avg:37.23ms
step:11/2330 train_time:394ms step_avg:35.84ms
step:12/2330 train_time:449ms step_avg:37.46ms
step:13/2330 train_time:471ms step_avg:36.27ms
step:14/2330 train_time:526ms step_avg:37.60ms
step:15/2330 train_time:548ms step_avg:36.55ms
step:16/2330 train_time:604ms step_avg:37.75ms
step:17/2330 train_time:625ms step_avg:36.79ms
step:18/2330 train_time:680ms step_avg:37.79ms
step:19/2330 train_time:703ms step_avg:36.98ms
step:20/2330 train_time:758ms step_avg:37.91ms
step:21/2330 train_time:780ms step_avg:37.15ms
step:22/2330 train_time:836ms step_avg:37.99ms
step:23/2330 train_time:858ms step_avg:37.31ms
step:24/2330 train_time:914ms step_avg:38.08ms
step:25/2330 train_time:937ms step_avg:37.47ms
step:26/2330 train_time:997ms step_avg:38.34ms
step:27/2330 train_time:1023ms step_avg:37.89ms
step:28/2330 train_time:1084ms step_avg:38.72ms
step:29/2330 train_time:1109ms step_avg:38.24ms
step:30/2330 train_time:1168ms step_avg:38.93ms
step:31/2330 train_time:1191ms step_avg:38.41ms
step:32/2330 train_time:1248ms step_avg:39.01ms
step:33/2330 train_time:1271ms step_avg:38.51ms
step:34/2330 train_time:1327ms step_avg:39.03ms
step:35/2330 train_time:1349ms step_avg:38.54ms
step:36/2330 train_time:1405ms step_avg:39.04ms
step:37/2330 train_time:1427ms step_avg:38.57ms
step:38/2330 train_time:1482ms step_avg:39.01ms
step:39/2330 train_time:1505ms step_avg:38.58ms
step:40/2330 train_time:1560ms step_avg:39.00ms
step:41/2330 train_time:1582ms step_avg:38.59ms
step:42/2330 train_time:1638ms step_avg:39.00ms
step:43/2330 train_time:1660ms step_avg:38.60ms
step:44/2330 train_time:1716ms step_avg:39.00ms
step:45/2330 train_time:1738ms step_avg:38.62ms
step:46/2330 train_time:1794ms step_avg:38.99ms
step:47/2330 train_time:1815ms step_avg:38.63ms
step:48/2330 train_time:1872ms step_avg:39.00ms
step:49/2330 train_time:1894ms step_avg:38.66ms
step:50/2330 train_time:1953ms step_avg:39.06ms
step:51/2330 train_time:1976ms step_avg:38.74ms
step:52/2330 train_time:2036ms step_avg:39.16ms
step:53/2330 train_time:2061ms step_avg:38.90ms
step:54/2330 train_time:2120ms step_avg:39.27ms
step:55/2330 train_time:2144ms step_avg:38.99ms
step:56/2330 train_time:2201ms step_avg:39.31ms
step:57/2330 train_time:2225ms step_avg:39.04ms
step:58/2330 train_time:2282ms step_avg:39.34ms
step:59/2330 train_time:2305ms step_avg:39.07ms
step:60/2330 train_time:2361ms step_avg:39.36ms
step:61/2330 train_time:2385ms step_avg:39.09ms
step:62/2330 train_time:2440ms step_avg:39.36ms
step:63/2330 train_time:2463ms step_avg:39.10ms
step:64/2330 train_time:2519ms step_avg:39.36ms
step:65/2330 train_time:2541ms step_avg:39.09ms
step:66/2330 train_time:2597ms step_avg:39.35ms
step:67/2330 train_time:2619ms step_avg:39.09ms
step:68/2330 train_time:2675ms step_avg:39.34ms
step:69/2330 train_time:2698ms step_avg:39.10ms
step:70/2330 train_time:2755ms step_avg:39.35ms
step:71/2330 train_time:2777ms step_avg:39.11ms
step:72/2330 train_time:2833ms step_avg:39.35ms
step:73/2330 train_time:2856ms step_avg:39.13ms
step:74/2330 train_time:2914ms step_avg:39.38ms
step:75/2330 train_time:2938ms step_avg:39.17ms
step:76/2330 train_time:2996ms step_avg:39.42ms
step:77/2330 train_time:3020ms step_avg:39.22ms
step:78/2330 train_time:3078ms step_avg:39.47ms
step:79/2330 train_time:3103ms step_avg:39.28ms
step:80/2330 train_time:3161ms step_avg:39.51ms
step:81/2330 train_time:3185ms step_avg:39.32ms
step:82/2330 train_time:3241ms step_avg:39.53ms
step:83/2330 train_time:3265ms step_avg:39.34ms
step:84/2330 train_time:3321ms step_avg:39.54ms
step:85/2330 train_time:3344ms step_avg:39.34ms
step:86/2330 train_time:3400ms step_avg:39.54ms
step:87/2330 train_time:3423ms step_avg:39.34ms
step:88/2330 train_time:3479ms step_avg:39.54ms
step:89/2330 train_time:3502ms step_avg:39.35ms
step:90/2330 train_time:3559ms step_avg:39.54ms
step:91/2330 train_time:3581ms step_avg:39.35ms
step:92/2330 train_time:3637ms step_avg:39.53ms
step:93/2330 train_time:3660ms step_avg:39.35ms
step:94/2330 train_time:3717ms step_avg:39.54ms
step:95/2330 train_time:3740ms step_avg:39.36ms
step:96/2330 train_time:3796ms step_avg:39.54ms
step:97/2330 train_time:3819ms step_avg:39.37ms
step:98/2330 train_time:3877ms step_avg:39.56ms
step:99/2330 train_time:3900ms step_avg:39.39ms
step:100/2330 train_time:3957ms step_avg:39.57ms
step:101/2330 train_time:3981ms step_avg:39.41ms
step:102/2330 train_time:4038ms step_avg:39.59ms
step:103/2330 train_time:4062ms step_avg:39.44ms
step:104/2330 train_time:4120ms step_avg:39.62ms
step:105/2330 train_time:4143ms step_avg:39.46ms
step:106/2330 train_time:4200ms step_avg:39.63ms
step:107/2330 train_time:4223ms step_avg:39.47ms
step:108/2330 train_time:4280ms step_avg:39.63ms
step:109/2330 train_time:4303ms step_avg:39.48ms
step:110/2330 train_time:4359ms step_avg:39.63ms
step:111/2330 train_time:4382ms step_avg:39.48ms
step:112/2330 train_time:4439ms step_avg:39.63ms
step:113/2330 train_time:4462ms step_avg:39.49ms
step:114/2330 train_time:4518ms step_avg:39.63ms
step:115/2330 train_time:4541ms step_avg:39.49ms
step:116/2330 train_time:4597ms step_avg:39.63ms
step:117/2330 train_time:4620ms step_avg:39.48ms
step:118/2330 train_time:4677ms step_avg:39.63ms
step:119/2330 train_time:4700ms step_avg:39.49ms
step:120/2330 train_time:4756ms step_avg:39.64ms
step:121/2330 train_time:4780ms step_avg:39.50ms
step:122/2330 train_time:4836ms step_avg:39.64ms
step:123/2330 train_time:4859ms step_avg:39.51ms
step:124/2330 train_time:4916ms step_avg:39.65ms
step:125/2330 train_time:4939ms step_avg:39.52ms
step:126/2330 train_time:4997ms step_avg:39.66ms
step:127/2330 train_time:5021ms step_avg:39.54ms
step:128/2330 train_time:5079ms step_avg:39.68ms
step:129/2330 train_time:5103ms step_avg:39.56ms
step:130/2330 train_time:5159ms step_avg:39.69ms
step:131/2330 train_time:5183ms step_avg:39.57ms
step:132/2330 train_time:5240ms step_avg:39.70ms
step:133/2330 train_time:5263ms step_avg:39.57ms
step:134/2330 train_time:5320ms step_avg:39.70ms
step:135/2330 train_time:5342ms step_avg:39.57ms
step:136/2330 train_time:5399ms step_avg:39.70ms
step:137/2330 train_time:5422ms step_avg:39.58ms
step:138/2330 train_time:5479ms step_avg:39.70ms
step:139/2330 train_time:5501ms step_avg:39.58ms
step:140/2330 train_time:5558ms step_avg:39.70ms
step:141/2330 train_time:5581ms step_avg:39.58ms
step:142/2330 train_time:5637ms step_avg:39.70ms
step:143/2330 train_time:5660ms step_avg:39.58ms
step:144/2330 train_time:5717ms step_avg:39.70ms
step:145/2330 train_time:5740ms step_avg:39.58ms
step:146/2330 train_time:5796ms step_avg:39.70ms
step:147/2330 train_time:5819ms step_avg:39.59ms
step:148/2330 train_time:5876ms step_avg:39.70ms
step:149/2330 train_time:5899ms step_avg:39.59ms
step:150/2330 train_time:5957ms step_avg:39.72ms
step:151/2330 train_time:5981ms step_avg:39.61ms
step:152/2330 train_time:6038ms step_avg:39.73ms
step:153/2330 train_time:6062ms step_avg:39.62ms
step:154/2330 train_time:6119ms step_avg:39.73ms
step:155/2330 train_time:6142ms step_avg:39.63ms
step:156/2330 train_time:6199ms step_avg:39.74ms
step:157/2330 train_time:6224ms step_avg:39.64ms
step:158/2330 train_time:6281ms step_avg:39.75ms
step:159/2330 train_time:6304ms step_avg:39.65ms
step:160/2330 train_time:6361ms step_avg:39.75ms
step:161/2330 train_time:6384ms step_avg:39.65ms
step:162/2330 train_time:6440ms step_avg:39.75ms
step:163/2330 train_time:6463ms step_avg:39.65ms
step:164/2330 train_time:6520ms step_avg:39.75ms
step:165/2330 train_time:6543ms step_avg:39.66ms
step:166/2330 train_time:6601ms step_avg:39.76ms
step:167/2330 train_time:6624ms step_avg:39.67ms
step:168/2330 train_time:6680ms step_avg:39.76ms
step:169/2330 train_time:6703ms step_avg:39.67ms
step:170/2330 train_time:6761ms step_avg:39.77ms
step:171/2330 train_time:6785ms step_avg:39.68ms
step:172/2330 train_time:6841ms step_avg:39.77ms
step:173/2330 train_time:6864ms step_avg:39.68ms
step:174/2330 train_time:6921ms step_avg:39.77ms
step:175/2330 train_time:6945ms step_avg:39.68ms
step:176/2330 train_time:7001ms step_avg:39.78ms
step:177/2330 train_time:7024ms step_avg:39.68ms
step:178/2330 train_time:7081ms step_avg:39.78ms
step:179/2330 train_time:7104ms step_avg:39.69ms
step:180/2330 train_time:7161ms step_avg:39.78ms
step:181/2330 train_time:7184ms step_avg:39.69ms
step:182/2330 train_time:7241ms step_avg:39.78ms
step:183/2330 train_time:7265ms step_avg:39.70ms
step:184/2330 train_time:7322ms step_avg:39.79ms
step:185/2330 train_time:7345ms step_avg:39.70ms
step:186/2330 train_time:7401ms step_avg:39.79ms
step:187/2330 train_time:7424ms step_avg:39.70ms
step:188/2330 train_time:7481ms step_avg:39.79ms
step:189/2330 train_time:7504ms step_avg:39.70ms
step:190/2330 train_time:7560ms step_avg:39.79ms
step:191/2330 train_time:7584ms step_avg:39.70ms
step:192/2330 train_time:7640ms step_avg:39.79ms
step:193/2330 train_time:7664ms step_avg:39.71ms
step:194/2330 train_time:7720ms step_avg:39.80ms
step:195/2330 train_time:7743ms step_avg:39.71ms
step:196/2330 train_time:7800ms step_avg:39.80ms
step:197/2330 train_time:7824ms step_avg:39.72ms
step:198/2330 train_time:7881ms step_avg:39.80ms
step:199/2330 train_time:7905ms step_avg:39.72ms
step:200/2330 train_time:7961ms step_avg:39.81ms
step:201/2330 train_time:7985ms step_avg:39.72ms
step:202/2330 train_time:8041ms step_avg:39.81ms
step:203/2330 train_time:8064ms step_avg:39.72ms
step:204/2330 train_time:8120ms step_avg:39.81ms
step:205/2330 train_time:8144ms step_avg:39.72ms
step:206/2330 train_time:8201ms step_avg:39.81ms
step:207/2330 train_time:8224ms step_avg:39.73ms
step:208/2330 train_time:8281ms step_avg:39.81ms
step:209/2330 train_time:8304ms step_avg:39.73ms
step:210/2330 train_time:8360ms step_avg:39.81ms
step:211/2330 train_time:8384ms step_avg:39.73ms
step:212/2330 train_time:8440ms step_avg:39.81ms
step:213/2330 train_time:8463ms step_avg:39.73ms
step:214/2330 train_time:8520ms step_avg:39.81ms
step:215/2330 train_time:8543ms step_avg:39.73ms
step:216/2330 train_time:8600ms step_avg:39.81ms
step:217/2330 train_time:8623ms step_avg:39.74ms
step:218/2330 train_time:8679ms step_avg:39.81ms
step:219/2330 train_time:8702ms step_avg:39.73ms
step:220/2330 train_time:8759ms step_avg:39.81ms
step:221/2330 train_time:8782ms step_avg:39.74ms
step:222/2330 train_time:8838ms step_avg:39.81ms
step:223/2330 train_time:8862ms step_avg:39.74ms
step:224/2330 train_time:8919ms step_avg:39.82ms
step:225/2330 train_time:8942ms step_avg:39.74ms
step:226/2330 train_time:8999ms step_avg:39.82ms
step:227/2330 train_time:9022ms step_avg:39.75ms
step:228/2330 train_time:9079ms step_avg:39.82ms
step:229/2330 train_time:9102ms step_avg:39.75ms
step:230/2330 train_time:9159ms step_avg:39.82ms
step:231/2330 train_time:9182ms step_avg:39.75ms
step:232/2330 train_time:9239ms step_avg:39.82ms
step:233/2330 train_time:9263ms step_avg:39.76ms
step:234/2330 train_time:9319ms step_avg:39.83ms
step:235/2330 train_time:9343ms step_avg:39.76ms
step:236/2330 train_time:9399ms step_avg:39.83ms
step:237/2330 train_time:9422ms step_avg:39.75ms
step:238/2330 train_time:9478ms step_avg:39.83ms
step:239/2330 train_time:9502ms step_avg:39.76ms
step:240/2330 train_time:9559ms step_avg:39.83ms
step:241/2330 train_time:9582ms step_avg:39.76ms
step:242/2330 train_time:9638ms step_avg:39.83ms
step:243/2330 train_time:9662ms step_avg:39.76ms
step:244/2330 train_time:9718ms step_avg:39.83ms
step:245/2330 train_time:9741ms step_avg:39.76ms
step:246/2330 train_time:9798ms step_avg:39.83ms
step:247/2330 train_time:9821ms step_avg:39.76ms
step:248/2330 train_time:9878ms step_avg:39.83ms
step:249/2330 train_time:9902ms step_avg:39.77ms
step:250/2330 train_time:9959ms step_avg:39.83ms
step:250/2330 val_loss:5.7458 train_time:10055ms step_avg:40.22ms
step:251/2330 train_time:10068ms step_avg:40.11ms
step:252/2330 train_time:10080ms step_avg:40.00ms
step:253/2330 train_time:10090ms step_avg:39.88ms
step:254/2330 train_time:10118ms step_avg:39.84ms
step:255/2330 train_time:10140ms step_avg:39.77ms
step:256/2330 train_time:10196ms step_avg:39.83ms
step:257/2330 train_time:10219ms step_avg:39.76ms
step:258/2330 train_time:10275ms step_avg:39.83ms
step:259/2330 train_time:10297ms step_avg:39.76ms
step:260/2330 train_time:10356ms step_avg:39.83ms
step:261/2330 train_time:10382ms step_avg:39.78ms
step:262/2330 train_time:10445ms step_avg:39.86ms
step:263/2330 train_time:10468ms step_avg:39.80ms
step:264/2330 train_time:10526ms step_avg:39.87ms
step:265/2330 train_time:10549ms step_avg:39.81ms
step:266/2330 train_time:10606ms step_avg:39.87ms
step:267/2330 train_time:10629ms step_avg:39.81ms
step:268/2330 train_time:10686ms step_avg:39.87ms
step:269/2330 train_time:10708ms step_avg:39.81ms
step:270/2330 train_time:10764ms step_avg:39.87ms
step:271/2330 train_time:10787ms step_avg:39.80ms
step:272/2330 train_time:10843ms step_avg:39.86ms
step:273/2330 train_time:10865ms step_avg:39.80ms
step:274/2330 train_time:10922ms step_avg:39.86ms
step:275/2330 train_time:10945ms step_avg:39.80ms
step:276/2330 train_time:11003ms step_avg:39.86ms
step:277/2330 train_time:11025ms step_avg:39.80ms
step:278/2330 train_time:11082ms step_avg:39.86ms
step:279/2330 train_time:11105ms step_avg:39.80ms
step:280/2330 train_time:11162ms step_avg:39.87ms
step:281/2330 train_time:11184ms step_avg:39.80ms
step:282/2330 train_time:11241ms step_avg:39.86ms
step:283/2330 train_time:11264ms step_avg:39.80ms
step:284/2330 train_time:11323ms step_avg:39.87ms
step:285/2330 train_time:11348ms step_avg:39.82ms
step:286/2330 train_time:11407ms step_avg:39.88ms
step:287/2330 train_time:11431ms step_avg:39.83ms
step:288/2330 train_time:11488ms step_avg:39.89ms
step:289/2330 train_time:11512ms step_avg:39.84ms
step:290/2330 train_time:11570ms step_avg:39.90ms
step:291/2330 train_time:11594ms step_avg:39.84ms
step:292/2330 train_time:11650ms step_avg:39.90ms
step:293/2330 train_time:11674ms step_avg:39.84ms
step:294/2330 train_time:11729ms step_avg:39.90ms
step:295/2330 train_time:11752ms step_avg:39.84ms
step:296/2330 train_time:11808ms step_avg:39.89ms
step:297/2330 train_time:11830ms step_avg:39.83ms
step:298/2330 train_time:11885ms step_avg:39.88ms
step:299/2330 train_time:11909ms step_avg:39.83ms
step:300/2330 train_time:11965ms step_avg:39.88ms
step:301/2330 train_time:11989ms step_avg:39.83ms
step:302/2330 train_time:12045ms step_avg:39.89ms
step:303/2330 train_time:12068ms step_avg:39.83ms
step:304/2330 train_time:12124ms step_avg:39.88ms
step:305/2330 train_time:12148ms step_avg:39.83ms
step:306/2330 train_time:12204ms step_avg:39.88ms
step:307/2330 train_time:12228ms step_avg:39.83ms
step:308/2330 train_time:12284ms step_avg:39.88ms
step:309/2330 train_time:12308ms step_avg:39.83ms
step:310/2330 train_time:12366ms step_avg:39.89ms
step:311/2330 train_time:12391ms step_avg:39.84ms
step:312/2330 train_time:12448ms step_avg:39.90ms
step:313/2330 train_time:12472ms step_avg:39.85ms
step:314/2330 train_time:12529ms step_avg:39.90ms
step:315/2330 train_time:12552ms step_avg:39.85ms
step:316/2330 train_time:12609ms step_avg:39.90ms
step:317/2330 train_time:12632ms step_avg:39.85ms
step:318/2330 train_time:12688ms step_avg:39.90ms
step:319/2330 train_time:12711ms step_avg:39.85ms
step:320/2330 train_time:12767ms step_avg:39.90ms
step:321/2330 train_time:12791ms step_avg:39.85ms
step:322/2330 train_time:12848ms step_avg:39.90ms
step:323/2330 train_time:12872ms step_avg:39.85ms
step:324/2330 train_time:12929ms step_avg:39.90ms
step:325/2330 train_time:12952ms step_avg:39.85ms
step:326/2330 train_time:13008ms step_avg:39.90ms
step:327/2330 train_time:13031ms step_avg:39.85ms
step:328/2330 train_time:13088ms step_avg:39.90ms
step:329/2330 train_time:13111ms step_avg:39.85ms
step:330/2330 train_time:13167ms step_avg:39.90ms
step:331/2330 train_time:13190ms step_avg:39.85ms
step:332/2330 train_time:13247ms step_avg:39.90ms
step:333/2330 train_time:13270ms step_avg:39.85ms
step:334/2330 train_time:13327ms step_avg:39.90ms
step:335/2330 train_time:13350ms step_avg:39.85ms
step:336/2330 train_time:13408ms step_avg:39.90ms
step:337/2330 train_time:13432ms step_avg:39.86ms
step:338/2330 train_time:13488ms step_avg:39.91ms
step:339/2330 train_time:13511ms step_avg:39.86ms
step:340/2330 train_time:13568ms step_avg:39.91ms
step:341/2330 train_time:13592ms step_avg:39.86ms
step:342/2330 train_time:13649ms step_avg:39.91ms
step:343/2330 train_time:13672ms step_avg:39.86ms
step:344/2330 train_time:13728ms step_avg:39.91ms
step:345/2330 train_time:13752ms step_avg:39.86ms
step:346/2330 train_time:13808ms step_avg:39.91ms
step:347/2330 train_time:13831ms step_avg:39.86ms
step:348/2330 train_time:13887ms step_avg:39.91ms
step:349/2330 train_time:13910ms step_avg:39.86ms
step:350/2330 train_time:13967ms step_avg:39.91ms
step:351/2330 train_time:13990ms step_avg:39.86ms
step:352/2330 train_time:14047ms step_avg:39.91ms
step:353/2330 train_time:14070ms step_avg:39.86ms
step:354/2330 train_time:14126ms step_avg:39.90ms
step:355/2330 train_time:14149ms step_avg:39.86ms
step:356/2330 train_time:14205ms step_avg:39.90ms
step:357/2330 train_time:14229ms step_avg:39.86ms
step:358/2330 train_time:14287ms step_avg:39.91ms
step:359/2330 train_time:14310ms step_avg:39.86ms
step:360/2330 train_time:14367ms step_avg:39.91ms
step:361/2330 train_time:14390ms step_avg:39.86ms
step:362/2330 train_time:14447ms step_avg:39.91ms
step:363/2330 train_time:14470ms step_avg:39.86ms
step:364/2330 train_time:14527ms step_avg:39.91ms
step:365/2330 train_time:14551ms step_avg:39.87ms
step:366/2330 train_time:14609ms step_avg:39.91ms
step:367/2330 train_time:14631ms step_avg:39.87ms
step:368/2330 train_time:14688ms step_avg:39.91ms
step:369/2330 train_time:14711ms step_avg:39.87ms
step:370/2330 train_time:14768ms step_avg:39.91ms
step:371/2330 train_time:14791ms step_avg:39.87ms
step:372/2330 train_time:14847ms step_avg:39.91ms
step:373/2330 train_time:14870ms step_avg:39.87ms
step:374/2330 train_time:14927ms step_avg:39.91ms
step:375/2330 train_time:14950ms step_avg:39.87ms
step:376/2330 train_time:15006ms step_avg:39.91ms
step:377/2330 train_time:15029ms step_avg:39.87ms
step:378/2330 train_time:15086ms step_avg:39.91ms
step:379/2330 train_time:15109ms step_avg:39.87ms
step:380/2330 train_time:15166ms step_avg:39.91ms
step:381/2330 train_time:15189ms step_avg:39.87ms
step:382/2330 train_time:15246ms step_avg:39.91ms
step:383/2330 train_time:15269ms step_avg:39.87ms
step:384/2330 train_time:15326ms step_avg:39.91ms
step:385/2330 train_time:15349ms step_avg:39.87ms
step:386/2330 train_time:15406ms step_avg:39.91ms
step:387/2330 train_time:15430ms step_avg:39.87ms
step:388/2330 train_time:15487ms step_avg:39.91ms
step:389/2330 train_time:15510ms step_avg:39.87ms
step:390/2330 train_time:15568ms step_avg:39.92ms
step:391/2330 train_time:15591ms step_avg:39.87ms
step:392/2330 train_time:15648ms step_avg:39.92ms
step:393/2330 train_time:15671ms step_avg:39.87ms
step:394/2330 train_time:15728ms step_avg:39.92ms
step:395/2330 train_time:15751ms step_avg:39.88ms
step:396/2330 train_time:15808ms step_avg:39.92ms
step:397/2330 train_time:15831ms step_avg:39.88ms
step:398/2330 train_time:15887ms step_avg:39.92ms
step:399/2330 train_time:15910ms step_avg:39.88ms
step:400/2330 train_time:15967ms step_avg:39.92ms
step:401/2330 train_time:15992ms step_avg:39.88ms
step:402/2330 train_time:16049ms step_avg:39.92ms
step:403/2330 train_time:16073ms step_avg:39.88ms
step:404/2330 train_time:16129ms step_avg:39.92ms
step:405/2330 train_time:16153ms step_avg:39.88ms
step:406/2330 train_time:16210ms step_avg:39.92ms
step:407/2330 train_time:16232ms step_avg:39.88ms
step:408/2330 train_time:16289ms step_avg:39.92ms
step:409/2330 train_time:16312ms step_avg:39.88ms
step:410/2330 train_time:16368ms step_avg:39.92ms
step:411/2330 train_time:16392ms step_avg:39.88ms
step:412/2330 train_time:16448ms step_avg:39.92ms
step:413/2330 train_time:16472ms step_avg:39.88ms
step:414/2330 train_time:16529ms step_avg:39.93ms
step:415/2330 train_time:16553ms step_avg:39.89ms
step:416/2330 train_time:16609ms step_avg:39.93ms
step:417/2330 train_time:16632ms step_avg:39.89ms
step:418/2330 train_time:16689ms step_avg:39.92ms
step:419/2330 train_time:16712ms step_avg:39.89ms
step:420/2330 train_time:16769ms step_avg:39.93ms
step:421/2330 train_time:16792ms step_avg:39.89ms
step:422/2330 train_time:16848ms step_avg:39.93ms
step:423/2330 train_time:16871ms step_avg:39.89ms
step:424/2330 train_time:16928ms step_avg:39.92ms
step:425/2330 train_time:16952ms step_avg:39.89ms
step:426/2330 train_time:17009ms step_avg:39.93ms
step:427/2330 train_time:17032ms step_avg:39.89ms
step:428/2330 train_time:17089ms step_avg:39.93ms
step:429/2330 train_time:17112ms step_avg:39.89ms
step:430/2330 train_time:17169ms step_avg:39.93ms
step:431/2330 train_time:17193ms step_avg:39.89ms
step:432/2330 train_time:17249ms step_avg:39.93ms
step:433/2330 train_time:17273ms step_avg:39.89ms
step:434/2330 train_time:17329ms step_avg:39.93ms
step:435/2330 train_time:17352ms step_avg:39.89ms
step:436/2330 train_time:17409ms step_avg:39.93ms
step:437/2330 train_time:17432ms step_avg:39.89ms
step:438/2330 train_time:17489ms step_avg:39.93ms
step:439/2330 train_time:17512ms step_avg:39.89ms
step:440/2330 train_time:17569ms step_avg:39.93ms
step:441/2330 train_time:17593ms step_avg:39.89ms
step:442/2330 train_time:17649ms step_avg:39.93ms
step:443/2330 train_time:17672ms step_avg:39.89ms
step:444/2330 train_time:17729ms step_avg:39.93ms
step:445/2330 train_time:17753ms step_avg:39.89ms
step:446/2330 train_time:17810ms step_avg:39.93ms
step:447/2330 train_time:17833ms step_avg:39.89ms
step:448/2330 train_time:17889ms step_avg:39.93ms
step:449/2330 train_time:17913ms step_avg:39.90ms
step:450/2330 train_time:17970ms step_avg:39.93ms
step:451/2330 train_time:17993ms step_avg:39.90ms
step:452/2330 train_time:18050ms step_avg:39.93ms
step:453/2330 train_time:18073ms step_avg:39.90ms
step:454/2330 train_time:18130ms step_avg:39.93ms
step:455/2330 train_time:18154ms step_avg:39.90ms
step:456/2330 train_time:18209ms step_avg:39.93ms
step:457/2330 train_time:18233ms step_avg:39.90ms
step:458/2330 train_time:18288ms step_avg:39.93ms
step:459/2330 train_time:18311ms step_avg:39.89ms
step:460/2330 train_time:18368ms step_avg:39.93ms
step:461/2330 train_time:18393ms step_avg:39.90ms
step:462/2330 train_time:18449ms step_avg:39.93ms
step:463/2330 train_time:18472ms step_avg:39.90ms
step:464/2330 train_time:18529ms step_avg:39.93ms
step:465/2330 train_time:18552ms step_avg:39.90ms
step:466/2330 train_time:18609ms step_avg:39.93ms
step:467/2330 train_time:18632ms step_avg:39.90ms
step:468/2330 train_time:18688ms step_avg:39.93ms
step:469/2330 train_time:18712ms step_avg:39.90ms
step:470/2330 train_time:18768ms step_avg:39.93ms
step:471/2330 train_time:18791ms step_avg:39.90ms
step:472/2330 train_time:18848ms step_avg:39.93ms
step:473/2330 train_time:18871ms step_avg:39.90ms
step:474/2330 train_time:18928ms step_avg:39.93ms
step:475/2330 train_time:18952ms step_avg:39.90ms
step:476/2330 train_time:19009ms step_avg:39.94ms
step:477/2330 train_time:19032ms step_avg:39.90ms
step:478/2330 train_time:19089ms step_avg:39.93ms
step:479/2330 train_time:19112ms step_avg:39.90ms
step:480/2330 train_time:19169ms step_avg:39.94ms
step:481/2330 train_time:19192ms step_avg:39.90ms
step:482/2330 train_time:19248ms step_avg:39.93ms
step:483/2330 train_time:19272ms step_avg:39.90ms
step:484/2330 train_time:19329ms step_avg:39.94ms
step:485/2330 train_time:19352ms step_avg:39.90ms
step:486/2330 train_time:19409ms step_avg:39.94ms
step:487/2330 train_time:19432ms step_avg:39.90ms
step:488/2330 train_time:19488ms step_avg:39.93ms
step:489/2330 train_time:19511ms step_avg:39.90ms
step:490/2330 train_time:19568ms step_avg:39.93ms
step:491/2330 train_time:19591ms step_avg:39.90ms
step:492/2330 train_time:19649ms step_avg:39.94ms
step:493/2330 train_time:19672ms step_avg:39.90ms
step:494/2330 train_time:19728ms step_avg:39.94ms
step:495/2330 train_time:19751ms step_avg:39.90ms
step:496/2330 train_time:19808ms step_avg:39.94ms
step:497/2330 train_time:19831ms step_avg:39.90ms
step:498/2330 train_time:19888ms step_avg:39.94ms
step:499/2330 train_time:19911ms step_avg:39.90ms
step:500/2330 train_time:19968ms step_avg:39.94ms
step:500/2330 val_loss:5.6823 train_time:20068ms step_avg:40.14ms
step:501/2330 train_time:20081ms step_avg:40.08ms
step:502/2330 train_time:20092ms step_avg:40.02ms
step:503/2330 train_time:20103ms step_avg:39.97ms
step:504/2330 train_time:20130ms step_avg:39.94ms
step:505/2330 train_time:20152ms step_avg:39.91ms
step:506/2330 train_time:20207ms step_avg:39.94ms
step:507/2330 train_time:20230ms step_avg:39.90ms
step:508/2330 train_time:20286ms step_avg:39.93ms
step:509/2330 train_time:20308ms step_avg:39.90ms
step:510/2330 train_time:20366ms step_avg:39.93ms
step:511/2330 train_time:20393ms step_avg:39.91ms
step:512/2330 train_time:20453ms step_avg:39.95ms
step:513/2330 train_time:20476ms step_avg:39.91ms
step:514/2330 train_time:20533ms step_avg:39.95ms
step:515/2330 train_time:20556ms step_avg:39.91ms
step:516/2330 train_time:20612ms step_avg:39.95ms
step:517/2330 train_time:20635ms step_avg:39.91ms
step:518/2330 train_time:20691ms step_avg:39.94ms
step:519/2330 train_time:20714ms step_avg:39.91ms
step:520/2330 train_time:20770ms step_avg:39.94ms
step:521/2330 train_time:20793ms step_avg:39.91ms
step:522/2330 train_time:20849ms step_avg:39.94ms
step:523/2330 train_time:20873ms step_avg:39.91ms
step:524/2330 train_time:20930ms step_avg:39.94ms
step:525/2330 train_time:20953ms step_avg:39.91ms
step:526/2330 train_time:21011ms step_avg:39.94ms
step:527/2330 train_time:21035ms step_avg:39.91ms
step:528/2330 train_time:21092ms step_avg:39.95ms
step:529/2330 train_time:21115ms step_avg:39.91ms
step:530/2330 train_time:21171ms step_avg:39.95ms
step:531/2330 train_time:21195ms step_avg:39.91ms
step:532/2330 train_time:21250ms step_avg:39.94ms
step:533/2330 train_time:21274ms step_avg:39.91ms
step:534/2330 train_time:21331ms step_avg:39.95ms
step:535/2330 train_time:21356ms step_avg:39.92ms
step:536/2330 train_time:21412ms step_avg:39.95ms
step:537/2330 train_time:21436ms step_avg:39.92ms
step:538/2330 train_time:21493ms step_avg:39.95ms
step:539/2330 train_time:21516ms step_avg:39.92ms
step:540/2330 train_time:21572ms step_avg:39.95ms
step:541/2330 train_time:21596ms step_avg:39.92ms
step:542/2330 train_time:21652ms step_avg:39.95ms
step:543/2330 train_time:21675ms step_avg:39.92ms
step:544/2330 train_time:21731ms step_avg:39.95ms
step:545/2330 train_time:21754ms step_avg:39.92ms
step:546/2330 train_time:21810ms step_avg:39.95ms
step:547/2330 train_time:21833ms step_avg:39.91ms
step:548/2330 train_time:21889ms step_avg:39.94ms
step:549/2330 train_time:21913ms step_avg:39.91ms
step:550/2330 train_time:21971ms step_avg:39.95ms
step:551/2330 train_time:21995ms step_avg:39.92ms
step:552/2330 train_time:22051ms step_avg:39.95ms
step:553/2330 train_time:22074ms step_avg:39.92ms
step:554/2330 train_time:22131ms step_avg:39.95ms
step:555/2330 train_time:22154ms step_avg:39.92ms
step:556/2330 train_time:22211ms step_avg:39.95ms
step:557/2330 train_time:22234ms step_avg:39.92ms
step:558/2330 train_time:22291ms step_avg:39.95ms
step:559/2330 train_time:22315ms step_avg:39.92ms
step:560/2330 train_time:22372ms step_avg:39.95ms
step:561/2330 train_time:22395ms step_avg:39.92ms
step:562/2330 train_time:22452ms step_avg:39.95ms
step:563/2330 train_time:22476ms step_avg:39.92ms
step:564/2330 train_time:22533ms step_avg:39.95ms
step:565/2330 train_time:22556ms step_avg:39.92ms
step:566/2330 train_time:22612ms step_avg:39.95ms
step:567/2330 train_time:22636ms step_avg:39.92ms
step:568/2330 train_time:22692ms step_avg:39.95ms
step:569/2330 train_time:22717ms step_avg:39.92ms
step:570/2330 train_time:22773ms step_avg:39.95ms
step:571/2330 train_time:22797ms step_avg:39.92ms
step:572/2330 train_time:22854ms step_avg:39.95ms
step:573/2330 train_time:22878ms step_avg:39.93ms
step:574/2330 train_time:22935ms step_avg:39.96ms
step:575/2330 train_time:22957ms step_avg:39.93ms
step:576/2330 train_time:23014ms step_avg:39.95ms
step:577/2330 train_time:23037ms step_avg:39.93ms
step:578/2330 train_time:23094ms step_avg:39.96ms
step:579/2330 train_time:23118ms step_avg:39.93ms
step:580/2330 train_time:23174ms step_avg:39.95ms
step:581/2330 train_time:23197ms step_avg:39.93ms
step:582/2330 train_time:23254ms step_avg:39.96ms
step:583/2330 train_time:23277ms step_avg:39.93ms
step:584/2330 train_time:23333ms step_avg:39.95ms
step:585/2330 train_time:23357ms step_avg:39.93ms
step:586/2330 train_time:23414ms step_avg:39.95ms
step:587/2330 train_time:23437ms step_avg:39.93ms
step:588/2330 train_time:23493ms step_avg:39.95ms
step:589/2330 train_time:23516ms step_avg:39.93ms
step:590/2330 train_time:23572ms step_avg:39.95ms
step:591/2330 train_time:23595ms step_avg:39.92ms
step:592/2330 train_time:23652ms step_avg:39.95ms
step:593/2330 train_time:23675ms step_avg:39.92ms
step:594/2330 train_time:23731ms step_avg:39.95ms
step:595/2330 train_time:23756ms step_avg:39.93ms
step:596/2330 train_time:23812ms step_avg:39.95ms
step:597/2330 train_time:23835ms step_avg:39.93ms
step:598/2330 train_time:23891ms step_avg:39.95ms
step:599/2330 train_time:23915ms step_avg:39.93ms
step:600/2330 train_time:23972ms step_avg:39.95ms
step:601/2330 train_time:23996ms step_avg:39.93ms
step:602/2330 train_time:24052ms step_avg:39.95ms
step:603/2330 train_time:24075ms step_avg:39.93ms
step:604/2330 train_time:24132ms step_avg:39.95ms
step:605/2330 train_time:24155ms step_avg:39.93ms
step:606/2330 train_time:24211ms step_avg:39.95ms
step:607/2330 train_time:24235ms step_avg:39.93ms
step:608/2330 train_time:24291ms step_avg:39.95ms
step:609/2330 train_time:24314ms step_avg:39.93ms
step:610/2330 train_time:24371ms step_avg:39.95ms
step:611/2330 train_time:24394ms step_avg:39.92ms
step:612/2330 train_time:24451ms step_avg:39.95ms
step:613/2330 train_time:24474ms step_avg:39.92ms
step:614/2330 train_time:24530ms step_avg:39.95ms
step:615/2330 train_time:24554ms step_avg:39.92ms
step:616/2330 train_time:24611ms step_avg:39.95ms
step:617/2330 train_time:24634ms step_avg:39.93ms
step:618/2330 train_time:24691ms step_avg:39.95ms
step:619/2330 train_time:24714ms step_avg:39.93ms
step:620/2330 train_time:24772ms step_avg:39.95ms
step:621/2330 train_time:24795ms step_avg:39.93ms
step:622/2330 train_time:24851ms step_avg:39.95ms
step:623/2330 train_time:24874ms step_avg:39.93ms
step:624/2330 train_time:24931ms step_avg:39.95ms
step:625/2330 train_time:24954ms step_avg:39.93ms
step:626/2330 train_time:25010ms step_avg:39.95ms
step:627/2330 train_time:25034ms step_avg:39.93ms
step:628/2330 train_time:25090ms step_avg:39.95ms
step:629/2330 train_time:25114ms step_avg:39.93ms
step:630/2330 train_time:25171ms step_avg:39.95ms
step:631/2330 train_time:25195ms step_avg:39.93ms
step:632/2330 train_time:25251ms step_avg:39.95ms
step:633/2330 train_time:25274ms step_avg:39.93ms
step:634/2330 train_time:25330ms step_avg:39.95ms
step:635/2330 train_time:25354ms step_avg:39.93ms
step:636/2330 train_time:25410ms step_avg:39.95ms
step:637/2330 train_time:25433ms step_avg:39.93ms
step:638/2330 train_time:25490ms step_avg:39.95ms
step:639/2330 train_time:25514ms step_avg:39.93ms
step:640/2330 train_time:25571ms step_avg:39.95ms
step:641/2330 train_time:25595ms step_avg:39.93ms
step:642/2330 train_time:25651ms step_avg:39.96ms
step:643/2330 train_time:25675ms step_avg:39.93ms
step:644/2330 train_time:25732ms step_avg:39.96ms
step:645/2330 train_time:25756ms step_avg:39.93ms
step:646/2330 train_time:25811ms step_avg:39.96ms
step:647/2330 train_time:25835ms step_avg:39.93ms
step:648/2330 train_time:25891ms step_avg:39.96ms
step:649/2330 train_time:25915ms step_avg:39.93ms
step:650/2330 train_time:25972ms step_avg:39.96ms
step:651/2330 train_time:25995ms step_avg:39.93ms
step:652/2330 train_time:26051ms step_avg:39.96ms
step:653/2330 train_time:26075ms step_avg:39.93ms
step:654/2330 train_time:26131ms step_avg:39.96ms
step:655/2330 train_time:26154ms step_avg:39.93ms
step:656/2330 train_time:26211ms step_avg:39.96ms
step:657/2330 train_time:26234ms step_avg:39.93ms
step:658/2330 train_time:26290ms step_avg:39.96ms
step:659/2330 train_time:26314ms step_avg:39.93ms
step:660/2330 train_time:26371ms step_avg:39.96ms
step:661/2330 train_time:26394ms step_avg:39.93ms
step:662/2330 train_time:26450ms step_avg:39.96ms
step:663/2330 train_time:26474ms step_avg:39.93ms
step:664/2330 train_time:26532ms step_avg:39.96ms
step:665/2330 train_time:26555ms step_avg:39.93ms
step:666/2330 train_time:26612ms step_avg:39.96ms
step:667/2330 train_time:26635ms step_avg:39.93ms
step:668/2330 train_time:26692ms step_avg:39.96ms
step:669/2330 train_time:26715ms step_avg:39.93ms
step:670/2330 train_time:26772ms step_avg:39.96ms
step:671/2330 train_time:26795ms step_avg:39.93ms
step:672/2330 train_time:26851ms step_avg:39.96ms
step:673/2330 train_time:26874ms step_avg:39.93ms
step:674/2330 train_time:26931ms step_avg:39.96ms
step:675/2330 train_time:26955ms step_avg:39.93ms
step:676/2330 train_time:27012ms step_avg:39.96ms
step:677/2330 train_time:27036ms step_avg:39.93ms
step:678/2330 train_time:27091ms step_avg:39.96ms
step:679/2330 train_time:27114ms step_avg:39.93ms
step:680/2330 train_time:27171ms step_avg:39.96ms
step:681/2330 train_time:27194ms step_avg:39.93ms
step:682/2330 train_time:27250ms step_avg:39.96ms
step:683/2330 train_time:27274ms step_avg:39.93ms
step:684/2330 train_time:27331ms step_avg:39.96ms
step:685/2330 train_time:27354ms step_avg:39.93ms
step:686/2330 train_time:27411ms step_avg:39.96ms
step:687/2330 train_time:27435ms step_avg:39.93ms
step:688/2330 train_time:27491ms step_avg:39.96ms
step:689/2330 train_time:27514ms step_avg:39.93ms
step:690/2330 train_time:27571ms step_avg:39.96ms
step:691/2330 train_time:27595ms step_avg:39.93ms
step:692/2330 train_time:27652ms step_avg:39.96ms
step:693/2330 train_time:27675ms step_avg:39.94ms
step:694/2330 train_time:27731ms step_avg:39.96ms
step:695/2330 train_time:27756ms step_avg:39.94ms
step:696/2330 train_time:27812ms step_avg:39.96ms
step:697/2330 train_time:27835ms step_avg:39.94ms
step:698/2330 train_time:27892ms step_avg:39.96ms
step:699/2330 train_time:27915ms step_avg:39.94ms
step:700/2330 train_time:27972ms step_avg:39.96ms
step:701/2330 train_time:27995ms step_avg:39.94ms
step:702/2330 train_time:28051ms step_avg:39.96ms
step:703/2330 train_time:28075ms step_avg:39.94ms
step:704/2330 train_time:28131ms step_avg:39.96ms
step:705/2330 train_time:28154ms step_avg:39.93ms
step:706/2330 train_time:28210ms step_avg:39.96ms
step:707/2330 train_time:28234ms step_avg:39.94ms
step:708/2330 train_time:28291ms step_avg:39.96ms
step:709/2330 train_time:28314ms step_avg:39.94ms
step:710/2330 train_time:28372ms step_avg:39.96ms
step:711/2330 train_time:28395ms step_avg:39.94ms
step:712/2330 train_time:28451ms step_avg:39.96ms
step:713/2330 train_time:28473ms step_avg:39.93ms
step:714/2330 train_time:28530ms step_avg:39.96ms
step:715/2330 train_time:28553ms step_avg:39.93ms
step:716/2330 train_time:28610ms step_avg:39.96ms
step:717/2330 train_time:28634ms step_avg:39.94ms
step:718/2330 train_time:28690ms step_avg:39.96ms
step:719/2330 train_time:28715ms step_avg:39.94ms
step:720/2330 train_time:28772ms step_avg:39.96ms
step:721/2330 train_time:28795ms step_avg:39.94ms
step:722/2330 train_time:28851ms step_avg:39.96ms
step:723/2330 train_time:28875ms step_avg:39.94ms
step:724/2330 train_time:28931ms step_avg:39.96ms
step:725/2330 train_time:28955ms step_avg:39.94ms
step:726/2330 train_time:29011ms step_avg:39.96ms
step:727/2330 train_time:29034ms step_avg:39.94ms
step:728/2330 train_time:29090ms step_avg:39.96ms
step:729/2330 train_time:29114ms step_avg:39.94ms
step:730/2330 train_time:29170ms step_avg:39.96ms
step:731/2330 train_time:29193ms step_avg:39.94ms
step:732/2330 train_time:29250ms step_avg:39.96ms
step:733/2330 train_time:29274ms step_avg:39.94ms
step:734/2330 train_time:29330ms step_avg:39.96ms
step:735/2330 train_time:29353ms step_avg:39.94ms
step:736/2330 train_time:29410ms step_avg:39.96ms
step:737/2330 train_time:29434ms step_avg:39.94ms
step:738/2330 train_time:29491ms step_avg:39.96ms
step:739/2330 train_time:29514ms step_avg:39.94ms
step:740/2330 train_time:29571ms step_avg:39.96ms
step:741/2330 train_time:29594ms step_avg:39.94ms
step:742/2330 train_time:29651ms step_avg:39.96ms
step:743/2330 train_time:29674ms step_avg:39.94ms
step:744/2330 train_time:29731ms step_avg:39.96ms
step:745/2330 train_time:29755ms step_avg:39.94ms
step:746/2330 train_time:29811ms step_avg:39.96ms
step:747/2330 train_time:29834ms step_avg:39.94ms
step:748/2330 train_time:29891ms step_avg:39.96ms
step:749/2330 train_time:29915ms step_avg:39.94ms
step:750/2330 train_time:29971ms step_avg:39.96ms
step:750/2330 val_loss:5.6571 train_time:30068ms step_avg:40.09ms
step:751/2330 train_time:30081ms step_avg:40.05ms
step:752/2330 train_time:30092ms step_avg:40.02ms
step:753/2330 train_time:30103ms step_avg:39.98ms
step:754/2330 train_time:30131ms step_avg:39.96ms
step:755/2330 train_time:30153ms step_avg:39.94ms
step:756/2330 train_time:30208ms step_avg:39.96ms
step:757/2330 train_time:30231ms step_avg:39.94ms
step:758/2330 train_time:30287ms step_avg:39.96ms
step:759/2330 train_time:30310ms step_avg:39.93ms
step:760/2330 train_time:30370ms step_avg:39.96ms
step:761/2330 train_time:30396ms step_avg:39.94ms
step:762/2330 train_time:30457ms step_avg:39.97ms
step:763/2330 train_time:30481ms step_avg:39.95ms
step:764/2330 train_time:30539ms step_avg:39.97ms
step:765/2330 train_time:30563ms step_avg:39.95ms
step:766/2330 train_time:30620ms step_avg:39.97ms
step:767/2330 train_time:30643ms step_avg:39.95ms
step:768/2330 train_time:30701ms step_avg:39.97ms
step:769/2330 train_time:30723ms step_avg:39.95ms
step:770/2330 train_time:30780ms step_avg:39.97ms
step:771/2330 train_time:30802ms step_avg:39.95ms
step:772/2330 train_time:30859ms step_avg:39.97ms
step:773/2330 train_time:30882ms step_avg:39.95ms
step:774/2330 train_time:30938ms step_avg:39.97ms
step:775/2330 train_time:30962ms step_avg:39.95ms
step:776/2330 train_time:31021ms step_avg:39.98ms
step:777/2330 train_time:31043ms step_avg:39.95ms
step:778/2330 train_time:31101ms step_avg:39.98ms
step:779/2330 train_time:31124ms step_avg:39.95ms
step:780/2330 train_time:31180ms step_avg:39.97ms
step:781/2330 train_time:31204ms step_avg:39.95ms
step:782/2330 train_time:31261ms step_avg:39.98ms
step:783/2330 train_time:31284ms step_avg:39.95ms
step:784/2330 train_time:31343ms step_avg:39.98ms
step:785/2330 train_time:31366ms step_avg:39.96ms
step:786/2330 train_time:31425ms step_avg:39.98ms
step:787/2330 train_time:31449ms step_avg:39.96ms
step:788/2330 train_time:31507ms step_avg:39.98ms
step:789/2330 train_time:31530ms step_avg:39.96ms
step:790/2330 train_time:31588ms step_avg:39.98ms
step:791/2330 train_time:31611ms step_avg:39.96ms
step:792/2330 train_time:31668ms step_avg:39.99ms
step:793/2330 train_time:31692ms step_avg:39.97ms
step:794/2330 train_time:31749ms step_avg:39.99ms
step:795/2330 train_time:31772ms step_avg:39.96ms
step:796/2330 train_time:31829ms step_avg:39.99ms
step:797/2330 train_time:31852ms step_avg:39.96ms
step:798/2330 train_time:31908ms step_avg:39.99ms
step:799/2330 train_time:31932ms step_avg:39.96ms
step:800/2330 train_time:31989ms step_avg:39.99ms
step:801/2330 train_time:32012ms step_avg:39.96ms
step:802/2330 train_time:32069ms step_avg:39.99ms
step:803/2330 train_time:32092ms step_avg:39.97ms
step:804/2330 train_time:32148ms step_avg:39.99ms
step:805/2330 train_time:32171ms step_avg:39.96ms
step:806/2330 train_time:32229ms step_avg:39.99ms
step:807/2330 train_time:32252ms step_avg:39.97ms
step:808/2330 train_time:32310ms step_avg:39.99ms
step:809/2330 train_time:32334ms step_avg:39.97ms
step:810/2330 train_time:32391ms step_avg:39.99ms
step:811/2330 train_time:32414ms step_avg:39.97ms
step:812/2330 train_time:32471ms step_avg:39.99ms
step:813/2330 train_time:32494ms step_avg:39.97ms
step:814/2330 train_time:32551ms step_avg:39.99ms
step:815/2330 train_time:32575ms step_avg:39.97ms
step:816/2330 train_time:32632ms step_avg:39.99ms
step:817/2330 train_time:32655ms step_avg:39.97ms
step:818/2330 train_time:32712ms step_avg:39.99ms
step:819/2330 train_time:32735ms step_avg:39.97ms
step:820/2330 train_time:32792ms step_avg:39.99ms
step:821/2330 train_time:32815ms step_avg:39.97ms
step:822/2330 train_time:32872ms step_avg:39.99ms
step:823/2330 train_time:32896ms step_avg:39.97ms
step:824/2330 train_time:32952ms step_avg:39.99ms
step:825/2330 train_time:32975ms step_avg:39.97ms
step:826/2330 train_time:33031ms step_avg:39.99ms
step:827/2330 train_time:33055ms step_avg:39.97ms
step:828/2330 train_time:33111ms step_avg:39.99ms
step:829/2330 train_time:33134ms step_avg:39.97ms
step:830/2330 train_time:33191ms step_avg:39.99ms
step:831/2330 train_time:33214ms step_avg:39.97ms
step:832/2330 train_time:33271ms step_avg:39.99ms
step:833/2330 train_time:33294ms step_avg:39.97ms
step:834/2330 train_time:33351ms step_avg:39.99ms
step:835/2330 train_time:33374ms step_avg:39.97ms
step:836/2330 train_time:33431ms step_avg:39.99ms
step:837/2330 train_time:33455ms step_avg:39.97ms
step:838/2330 train_time:33512ms step_avg:39.99ms
step:839/2330 train_time:33535ms step_avg:39.97ms
step:840/2330 train_time:33591ms step_avg:39.99ms
step:841/2330 train_time:33614ms step_avg:39.97ms
step:842/2330 train_time:33671ms step_avg:39.99ms
step:843/2330 train_time:33695ms step_avg:39.97ms
step:844/2330 train_time:33751ms step_avg:39.99ms
step:845/2330 train_time:33775ms step_avg:39.97ms
step:846/2330 train_time:33831ms step_avg:39.99ms
step:847/2330 train_time:33855ms step_avg:39.97ms
step:848/2330 train_time:33911ms step_avg:39.99ms
step:849/2330 train_time:33935ms step_avg:39.97ms
step:850/2330 train_time:33992ms step_avg:39.99ms
step:851/2330 train_time:34015ms step_avg:39.97ms
step:852/2330 train_time:34072ms step_avg:39.99ms
step:853/2330 train_time:34095ms step_avg:39.97ms
step:854/2330 train_time:34152ms step_avg:39.99ms
step:855/2330 train_time:34176ms step_avg:39.97ms
step:856/2330 train_time:34232ms step_avg:39.99ms
step:857/2330 train_time:34256ms step_avg:39.97ms
step:858/2330 train_time:34312ms step_avg:39.99ms
step:859/2330 train_time:34336ms step_avg:39.97ms
step:860/2330 train_time:34393ms step_avg:39.99ms
step:861/2330 train_time:34416ms step_avg:39.97ms
step:862/2330 train_time:34473ms step_avg:39.99ms
step:863/2330 train_time:34497ms step_avg:39.97ms
step:864/2330 train_time:34553ms step_avg:39.99ms
step:865/2330 train_time:34577ms step_avg:39.97ms
step:866/2330 train_time:34632ms step_avg:39.99ms
step:867/2330 train_time:34656ms step_avg:39.97ms
step:868/2330 train_time:34712ms step_avg:39.99ms
step:869/2330 train_time:34735ms step_avg:39.97ms
step:870/2330 train_time:34792ms step_avg:39.99ms
step:871/2330 train_time:34815ms step_avg:39.97ms
step:872/2330 train_time:34871ms step_avg:39.99ms
step:873/2330 train_time:34895ms step_avg:39.97ms
step:874/2330 train_time:34951ms step_avg:39.99ms
step:875/2330 train_time:34975ms step_avg:39.97ms
step:876/2330 train_time:35031ms step_avg:39.99ms
step:877/2330 train_time:35055ms step_avg:39.97ms
step:878/2330 train_time:35111ms step_avg:39.99ms
step:879/2330 train_time:35134ms step_avg:39.97ms
step:880/2330 train_time:35191ms step_avg:39.99ms
step:881/2330 train_time:35215ms step_avg:39.97ms
step:882/2330 train_time:35272ms step_avg:39.99ms
step:883/2330 train_time:35296ms step_avg:39.97ms
step:884/2330 train_time:35352ms step_avg:39.99ms
step:885/2330 train_time:35376ms step_avg:39.97ms
step:886/2330 train_time:35432ms step_avg:39.99ms
step:887/2330 train_time:35456ms step_avg:39.97ms
step:888/2330 train_time:35512ms step_avg:39.99ms
step:889/2330 train_time:35535ms step_avg:39.97ms
step:890/2330 train_time:35592ms step_avg:39.99ms
step:891/2330 train_time:35615ms step_avg:39.97ms
step:892/2330 train_time:35672ms step_avg:39.99ms
step:893/2330 train_time:35695ms step_avg:39.97ms
step:894/2330 train_time:35751ms step_avg:39.99ms
step:895/2330 train_time:35775ms step_avg:39.97ms
step:896/2330 train_time:35833ms step_avg:39.99ms
step:897/2330 train_time:35856ms step_avg:39.97ms
step:898/2330 train_time:35912ms step_avg:39.99ms
step:899/2330 train_time:35936ms step_avg:39.97ms
step:900/2330 train_time:35993ms step_avg:39.99ms
step:901/2330 train_time:36016ms step_avg:39.97ms
step:902/2330 train_time:36072ms step_avg:39.99ms
step:903/2330 train_time:36096ms step_avg:39.97ms
step:904/2330 train_time:36152ms step_avg:39.99ms
step:905/2330 train_time:36176ms step_avg:39.97ms
step:906/2330 train_time:36233ms step_avg:39.99ms
step:907/2330 train_time:36256ms step_avg:39.97ms
step:908/2330 train_time:36312ms step_avg:39.99ms
step:909/2330 train_time:36335ms step_avg:39.97ms
step:910/2330 train_time:36391ms step_avg:39.99ms
step:911/2330 train_time:36414ms step_avg:39.97ms
step:912/2330 train_time:36471ms step_avg:39.99ms
step:913/2330 train_time:36494ms step_avg:39.97ms
step:914/2330 train_time:36550ms step_avg:39.99ms
step:915/2330 train_time:36573ms step_avg:39.97ms
step:916/2330 train_time:36631ms step_avg:39.99ms
step:917/2330 train_time:36655ms step_avg:39.97ms
step:918/2330 train_time:36711ms step_avg:39.99ms
step:919/2330 train_time:36735ms step_avg:39.97ms
step:920/2330 train_time:36791ms step_avg:39.99ms
step:921/2330 train_time:36815ms step_avg:39.97ms
step:922/2330 train_time:36871ms step_avg:39.99ms
step:923/2330 train_time:36895ms step_avg:39.97ms
step:924/2330 train_time:36951ms step_avg:39.99ms
step:925/2330 train_time:36974ms step_avg:39.97ms
step:926/2330 train_time:37032ms step_avg:39.99ms
step:927/2330 train_time:37055ms step_avg:39.97ms
step:928/2330 train_time:37112ms step_avg:39.99ms
step:929/2330 train_time:37135ms step_avg:39.97ms
step:930/2330 train_time:37191ms step_avg:39.99ms
step:931/2330 train_time:37214ms step_avg:39.97ms
step:932/2330 train_time:37271ms step_avg:39.99ms
step:933/2330 train_time:37295ms step_avg:39.97ms
step:934/2330 train_time:37352ms step_avg:39.99ms
step:935/2330 train_time:37375ms step_avg:39.97ms
step:936/2330 train_time:37433ms step_avg:39.99ms
step:937/2330 train_time:37456ms step_avg:39.97ms
step:938/2330 train_time:37512ms step_avg:39.99ms
step:939/2330 train_time:37536ms step_avg:39.97ms
step:940/2330 train_time:37592ms step_avg:39.99ms
step:941/2330 train_time:37616ms step_avg:39.97ms
step:942/2330 train_time:37672ms step_avg:39.99ms
step:943/2330 train_time:37696ms step_avg:39.97ms
step:944/2330 train_time:37753ms step_avg:39.99ms
step:945/2330 train_time:37777ms step_avg:39.98ms
step:946/2330 train_time:37833ms step_avg:39.99ms
step:947/2330 train_time:37856ms step_avg:39.97ms
step:948/2330 train_time:37913ms step_avg:39.99ms
step:949/2330 train_time:37936ms step_avg:39.97ms
step:950/2330 train_time:37993ms step_avg:39.99ms
step:951/2330 train_time:38017ms step_avg:39.98ms
step:952/2330 train_time:38073ms step_avg:39.99ms
step:953/2330 train_time:38096ms step_avg:39.97ms
step:954/2330 train_time:38153ms step_avg:39.99ms
step:955/2330 train_time:38176ms step_avg:39.97ms
step:956/2330 train_time:38232ms step_avg:39.99ms
step:957/2330 train_time:38255ms step_avg:39.97ms
step:958/2330 train_time:38312ms step_avg:39.99ms
step:959/2330 train_time:38336ms step_avg:39.97ms
step:960/2330 train_time:38392ms step_avg:39.99ms
step:961/2330 train_time:38416ms step_avg:39.97ms
step:962/2330 train_time:38472ms step_avg:39.99ms
step:963/2330 train_time:38496ms step_avg:39.97ms
step:964/2330 train_time:38552ms step_avg:39.99ms
step:965/2330 train_time:38575ms step_avg:39.97ms
step:966/2330 train_time:38632ms step_avg:39.99ms
step:967/2330 train_time:38656ms step_avg:39.97ms
step:968/2330 train_time:38712ms step_avg:39.99ms
step:969/2330 train_time:38735ms step_avg:39.97ms
step:970/2330 train_time:38792ms step_avg:39.99ms
step:971/2330 train_time:38815ms step_avg:39.97ms
step:972/2330 train_time:38872ms step_avg:39.99ms
step:973/2330 train_time:38895ms step_avg:39.97ms
step:974/2330 train_time:38951ms step_avg:39.99ms
step:975/2330 train_time:38975ms step_avg:39.97ms
step:976/2330 train_time:39032ms step_avg:39.99ms
step:977/2330 train_time:39056ms step_avg:39.98ms
step:978/2330 train_time:39112ms step_avg:39.99ms
step:979/2330 train_time:39135ms step_avg:39.97ms
step:980/2330 train_time:39192ms step_avg:39.99ms
step:981/2330 train_time:39216ms step_avg:39.98ms
step:982/2330 train_time:39272ms step_avg:39.99ms
step:983/2330 train_time:39295ms step_avg:39.97ms
step:984/2330 train_time:39351ms step_avg:39.99ms
step:985/2330 train_time:39375ms step_avg:39.97ms
step:986/2330 train_time:39432ms step_avg:39.99ms
step:987/2330 train_time:39456ms step_avg:39.98ms
step:988/2330 train_time:39511ms step_avg:39.99ms
step:989/2330 train_time:39535ms step_avg:39.97ms
step:990/2330 train_time:39592ms step_avg:39.99ms
step:991/2330 train_time:39616ms step_avg:39.98ms
step:992/2330 train_time:39672ms step_avg:39.99ms
step:993/2330 train_time:39695ms step_avg:39.97ms
step:994/2330 train_time:39751ms step_avg:39.99ms
step:995/2330 train_time:39775ms step_avg:39.97ms
step:996/2330 train_time:39832ms step_avg:39.99ms
step:997/2330 train_time:39855ms step_avg:39.97ms
step:998/2330 train_time:39911ms step_avg:39.99ms
step:999/2330 train_time:39935ms step_avg:39.97ms
step:1000/2330 train_time:39992ms step_avg:39.99ms
step:1000/2330 val_loss:5.6648 train_time:40091ms step_avg:40.09ms
step:1001/2330 train_time:40104ms step_avg:40.06ms
step:1002/2330 train_time:40115ms step_avg:40.03ms
step:1003/2330 train_time:40125ms step_avg:40.00ms
step:1004/2330 train_time:40153ms step_avg:39.99ms
step:1005/2330 train_time:40175ms step_avg:39.98ms
step:1006/2330 train_time:40231ms step_avg:39.99ms
step:1007/2330 train_time:40254ms step_avg:39.97ms
step:1008/2330 train_time:40310ms step_avg:39.99ms
step:1009/2330 train_time:40332ms step_avg:39.97ms
step:1010/2330 train_time:40393ms step_avg:39.99ms
step:1011/2330 train_time:40421ms step_avg:39.98ms
step:1012/2330 train_time:40481ms step_avg:40.00ms
step:1013/2330 train_time:40506ms step_avg:39.99ms
step:1014/2330 train_time:40563ms step_avg:40.00ms
step:1015/2330 train_time:40586ms step_avg:39.99ms
step:1016/2330 train_time:40643ms step_avg:40.00ms
step:1017/2330 train_time:40665ms step_avg:39.99ms
step:1018/2330 train_time:40722ms step_avg:40.00ms
step:1019/2330 train_time:40743ms step_avg:39.98ms
step:1020/2330 train_time:40800ms step_avg:40.00ms
step:1021/2330 train_time:40822ms step_avg:39.98ms
step:1022/2330 train_time:40878ms step_avg:40.00ms
step:1023/2330 train_time:40900ms step_avg:39.98ms
step:1024/2330 train_time:40956ms step_avg:40.00ms
step:1025/2330 train_time:40980ms step_avg:39.98ms
step:1026/2330 train_time:41039ms step_avg:40.00ms
step:1027/2330 train_time:41063ms step_avg:39.98ms
step:1028/2330 train_time:41121ms step_avg:40.00ms
step:1029/2330 train_time:41145ms step_avg:39.99ms
step:1030/2330 train_time:41202ms step_avg:40.00ms
step:1031/2330 train_time:41224ms step_avg:39.98ms
step:1032/2330 train_time:41281ms step_avg:40.00ms
step:1033/2330 train_time:41303ms step_avg:39.98ms
step:1034/2330 train_time:41364ms step_avg:40.00ms
step:1035/2330 train_time:41387ms step_avg:39.99ms
step:1036/2330 train_time:41445ms step_avg:40.00ms
step:1037/2330 train_time:41467ms step_avg:39.99ms
step:1038/2330 train_time:41525ms step_avg:40.00ms
step:1039/2330 train_time:41549ms step_avg:39.99ms
step:1040/2330 train_time:41606ms step_avg:40.01ms
step:1041/2330 train_time:41629ms step_avg:39.99ms
step:1042/2330 train_time:41686ms step_avg:40.01ms
step:1043/2330 train_time:41709ms step_avg:39.99ms
step:1044/2330 train_time:41766ms step_avg:40.01ms
step:1045/2330 train_time:41789ms step_avg:39.99ms
step:1046/2330 train_time:41846ms step_avg:40.01ms
step:1047/2330 train_time:41869ms step_avg:39.99ms
step:1048/2330 train_time:41928ms step_avg:40.01ms
step:1049/2330 train_time:41950ms step_avg:39.99ms
step:1050/2330 train_time:42009ms step_avg:40.01ms
step:1051/2330 train_time:42032ms step_avg:39.99ms
step:1052/2330 train_time:42090ms step_avg:40.01ms
step:1053/2330 train_time:42113ms step_avg:39.99ms
step:1054/2330 train_time:42171ms step_avg:40.01ms
step:1055/2330 train_time:42194ms step_avg:39.99ms
step:1056/2330 train_time:42251ms step_avg:40.01ms
step:1057/2330 train_time:42275ms step_avg:40.00ms
step:1058/2330 train_time:42334ms step_avg:40.01ms
step:1059/2330 train_time:42358ms step_avg:40.00ms
step:1060/2330 train_time:42416ms step_avg:40.01ms
step:1061/2330 train_time:42440ms step_avg:40.00ms
step:1062/2330 train_time:42496ms step_avg:40.02ms
step:1063/2330 train_time:42520ms step_avg:40.00ms
step:1064/2330 train_time:42577ms step_avg:40.02ms
step:1065/2330 train_time:42600ms step_avg:40.00ms
step:1066/2330 train_time:42657ms step_avg:40.02ms
step:1067/2330 train_time:42681ms step_avg:40.00ms
step:1068/2330 train_time:42738ms step_avg:40.02ms
step:1069/2330 train_time:42762ms step_avg:40.00ms
step:1070/2330 train_time:42818ms step_avg:40.02ms
step:1071/2330 train_time:42841ms step_avg:40.00ms
step:1072/2330 train_time:42898ms step_avg:40.02ms
step:1073/2330 train_time:42921ms step_avg:40.00ms
step:1074/2330 train_time:42978ms step_avg:40.02ms
step:1075/2330 train_time:43002ms step_avg:40.00ms
step:1076/2330 train_time:43059ms step_avg:40.02ms
step:1077/2330 train_time:43082ms step_avg:40.00ms
step:1078/2330 train_time:43138ms step_avg:40.02ms
step:1079/2330 train_time:43161ms step_avg:40.00ms
step:1080/2330 train_time:43219ms step_avg:40.02ms
step:1081/2330 train_time:43242ms step_avg:40.00ms
step:1082/2330 train_time:43300ms step_avg:40.02ms
step:1083/2330 train_time:43323ms step_avg:40.00ms
step:1084/2330 train_time:43381ms step_avg:40.02ms
step:1085/2330 train_time:43404ms step_avg:40.00ms
step:1086/2330 train_time:43462ms step_avg:40.02ms
step:1087/2330 train_time:43486ms step_avg:40.01ms
step:1088/2330 train_time:43543ms step_avg:40.02ms
step:1089/2330 train_time:43565ms step_avg:40.00ms
step:1090/2330 train_time:43624ms step_avg:40.02ms
step:1091/2330 train_time:43646ms step_avg:40.01ms
step:1092/2330 train_time:43703ms step_avg:40.02ms
step:1093/2330 train_time:43726ms step_avg:40.01ms
step:1094/2330 train_time:43784ms step_avg:40.02ms
step:1095/2330 train_time:43807ms step_avg:40.01ms
step:1096/2330 train_time:43865ms step_avg:40.02ms
step:1097/2330 train_time:43888ms step_avg:40.01ms
step:1098/2330 train_time:43946ms step_avg:40.02ms
step:1099/2330 train_time:43969ms step_avg:40.01ms
step:1100/2330 train_time:44027ms step_avg:40.02ms
step:1101/2330 train_time:44050ms step_avg:40.01ms
step:1102/2330 train_time:44108ms step_avg:40.03ms
step:1103/2330 train_time:44130ms step_avg:40.01ms
step:1104/2330 train_time:44188ms step_avg:40.03ms
step:1105/2330 train_time:44211ms step_avg:40.01ms
step:1106/2330 train_time:44270ms step_avg:40.03ms
step:1107/2330 train_time:44293ms step_avg:40.01ms
step:1108/2330 train_time:44350ms step_avg:40.03ms
step:1109/2330 train_time:44374ms step_avg:40.01ms
step:1110/2330 train_time:44431ms step_avg:40.03ms
step:1111/2330 train_time:44455ms step_avg:40.01ms
step:1112/2330 train_time:44514ms step_avg:40.03ms
step:1113/2330 train_time:44537ms step_avg:40.02ms
step:1114/2330 train_time:44594ms step_avg:40.03ms
step:1115/2330 train_time:44617ms step_avg:40.02ms
step:1116/2330 train_time:44674ms step_avg:40.03ms
step:1117/2330 train_time:44697ms step_avg:40.02ms
step:1118/2330 train_time:44755ms step_avg:40.03ms
step:1119/2330 train_time:44778ms step_avg:40.02ms
step:1120/2330 train_time:44836ms step_avg:40.03ms
step:1121/2330 train_time:44859ms step_avg:40.02ms
step:1122/2330 train_time:44916ms step_avg:40.03ms
step:1123/2330 train_time:44941ms step_avg:40.02ms
step:1124/2330 train_time:44997ms step_avg:40.03ms
step:1125/2330 train_time:45021ms step_avg:40.02ms
step:1126/2330 train_time:45078ms step_avg:40.03ms
step:1127/2330 train_time:45101ms step_avg:40.02ms
step:1128/2330 train_time:45158ms step_avg:40.03ms
step:1129/2330 train_time:45181ms step_avg:40.02ms
step:1130/2330 train_time:45238ms step_avg:40.03ms
step:1131/2330 train_time:45261ms step_avg:40.02ms
step:1132/2330 train_time:45317ms step_avg:40.03ms
step:1133/2330 train_time:45341ms step_avg:40.02ms
step:1134/2330 train_time:45398ms step_avg:40.03ms
step:1135/2330 train_time:45421ms step_avg:40.02ms
step:1136/2330 train_time:45478ms step_avg:40.03ms
step:1137/2330 train_time:45502ms step_avg:40.02ms
step:1138/2330 train_time:45558ms step_avg:40.03ms
step:1139/2330 train_time:45582ms step_avg:40.02ms
step:1140/2330 train_time:45638ms step_avg:40.03ms
step:1141/2330 train_time:45661ms step_avg:40.02ms
step:1142/2330 train_time:45719ms step_avg:40.03ms
step:1143/2330 train_time:45743ms step_avg:40.02ms
step:1144/2330 train_time:45800ms step_avg:40.04ms
step:1145/2330 train_time:45824ms step_avg:40.02ms
step:1146/2330 train_time:45880ms step_avg:40.04ms
step:1147/2330 train_time:45903ms step_avg:40.02ms
step:1148/2330 train_time:45960ms step_avg:40.04ms
step:1149/2330 train_time:45982ms step_avg:40.02ms
step:1150/2330 train_time:46040ms step_avg:40.03ms
step:1151/2330 train_time:46062ms step_avg:40.02ms
step:1152/2330 train_time:46119ms step_avg:40.03ms
step:1153/2330 train_time:46142ms step_avg:40.02ms
step:1154/2330 train_time:46198ms step_avg:40.03ms
step:1155/2330 train_time:46221ms step_avg:40.02ms
step:1156/2330 train_time:46277ms step_avg:40.03ms
step:1157/2330 train_time:46300ms step_avg:40.02ms
step:1158/2330 train_time:46357ms step_avg:40.03ms
step:1159/2330 train_time:46380ms step_avg:40.02ms
step:1160/2330 train_time:46437ms step_avg:40.03ms
step:1161/2330 train_time:46460ms step_avg:40.02ms
step:1162/2330 train_time:46516ms step_avg:40.03ms
step:1163/2330 train_time:46540ms step_avg:40.02ms
step:1164/2330 train_time:46596ms step_avg:40.03ms
step:1165/2330 train_time:46620ms step_avg:40.02ms
step:1166/2330 train_time:46677ms step_avg:40.03ms
step:1167/2330 train_time:46702ms step_avg:40.02ms
step:1168/2330 train_time:46758ms step_avg:40.03ms
step:1169/2330 train_time:46781ms step_avg:40.02ms
step:1170/2330 train_time:46837ms step_avg:40.03ms
step:1171/2330 train_time:46861ms step_avg:40.02ms
step:1172/2330 train_time:46917ms step_avg:40.03ms
step:1173/2330 train_time:46941ms step_avg:40.02ms
step:1174/2330 train_time:46997ms step_avg:40.03ms
step:1175/2330 train_time:47022ms step_avg:40.02ms
step:1176/2330 train_time:47079ms step_avg:40.03ms
step:1177/2330 train_time:47103ms step_avg:40.02ms
step:1178/2330 train_time:47159ms step_avg:40.03ms
step:1179/2330 train_time:47182ms step_avg:40.02ms
step:1180/2330 train_time:47238ms step_avg:40.03ms
step:1181/2330 train_time:47261ms step_avg:40.02ms
step:1182/2330 train_time:47317ms step_avg:40.03ms
step:1183/2330 train_time:47340ms step_avg:40.02ms
step:1184/2330 train_time:47397ms step_avg:40.03ms
step:1185/2330 train_time:47420ms step_avg:40.02ms
step:1186/2330 train_time:47477ms step_avg:40.03ms
step:1187/2330 train_time:47501ms step_avg:40.02ms
step:1188/2330 train_time:47557ms step_avg:40.03ms
step:1189/2330 train_time:47580ms step_avg:40.02ms
step:1190/2330 train_time:47637ms step_avg:40.03ms
step:1191/2330 train_time:47661ms step_avg:40.02ms
step:1192/2330 train_time:47718ms step_avg:40.03ms
step:1193/2330 train_time:47741ms step_avg:40.02ms
step:1194/2330 train_time:47798ms step_avg:40.03ms
step:1195/2330 train_time:47822ms step_avg:40.02ms
step:1196/2330 train_time:47879ms step_avg:40.03ms
step:1197/2330 train_time:47902ms step_avg:40.02ms
step:1198/2330 train_time:47960ms step_avg:40.03ms
step:1199/2330 train_time:47983ms step_avg:40.02ms
step:1200/2330 train_time:48040ms step_avg:40.03ms
step:1201/2330 train_time:48063ms step_avg:40.02ms
step:1202/2330 train_time:48119ms step_avg:40.03ms
step:1203/2330 train_time:48142ms step_avg:40.02ms
step:1204/2330 train_time:48199ms step_avg:40.03ms
step:1205/2330 train_time:48222ms step_avg:40.02ms
step:1206/2330 train_time:48278ms step_avg:40.03ms
step:1207/2330 train_time:48301ms step_avg:40.02ms
step:1208/2330 train_time:48358ms step_avg:40.03ms
step:1209/2330 train_time:48382ms step_avg:40.02ms
step:1210/2330 train_time:48439ms step_avg:40.03ms
step:1211/2330 train_time:48462ms step_avg:40.02ms
step:1212/2330 train_time:48518ms step_avg:40.03ms
step:1213/2330 train_time:48542ms step_avg:40.02ms
step:1214/2330 train_time:48598ms step_avg:40.03ms
step:1215/2330 train_time:48622ms step_avg:40.02ms
step:1216/2330 train_time:48678ms step_avg:40.03ms
step:1217/2330 train_time:48701ms step_avg:40.02ms
step:1218/2330 train_time:48758ms step_avg:40.03ms
step:1219/2330 train_time:48781ms step_avg:40.02ms
step:1220/2330 train_time:48837ms step_avg:40.03ms
step:1221/2330 train_time:48861ms step_avg:40.02ms
step:1222/2330 train_time:48918ms step_avg:40.03ms
step:1223/2330 train_time:48942ms step_avg:40.02ms
step:1224/2330 train_time:48998ms step_avg:40.03ms
step:1225/2330 train_time:49021ms step_avg:40.02ms
step:1226/2330 train_time:49078ms step_avg:40.03ms
step:1227/2330 train_time:49100ms step_avg:40.02ms
step:1228/2330 train_time:49157ms step_avg:40.03ms
step:1229/2330 train_time:49181ms step_avg:40.02ms
step:1230/2330 train_time:49237ms step_avg:40.03ms
step:1231/2330 train_time:49260ms step_avg:40.02ms
step:1232/2330 train_time:49316ms step_avg:40.03ms
step:1233/2330 train_time:49339ms step_avg:40.02ms
step:1234/2330 train_time:49397ms step_avg:40.03ms
step:1235/2330 train_time:49420ms step_avg:40.02ms
step:1236/2330 train_time:49477ms step_avg:40.03ms
step:1237/2330 train_time:49500ms step_avg:40.02ms
step:1238/2330 train_time:49557ms step_avg:40.03ms
step:1239/2330 train_time:49580ms step_avg:40.02ms
step:1240/2330 train_time:49637ms step_avg:40.03ms
step:1241/2330 train_time:49661ms step_avg:40.02ms
step:1242/2330 train_time:49717ms step_avg:40.03ms
step:1243/2330 train_time:49741ms step_avg:40.02ms
step:1244/2330 train_time:49797ms step_avg:40.03ms
step:1245/2330 train_time:49821ms step_avg:40.02ms
step:1246/2330 train_time:49878ms step_avg:40.03ms
step:1247/2330 train_time:49903ms step_avg:40.02ms
step:1248/2330 train_time:49959ms step_avg:40.03ms
step:1249/2330 train_time:49982ms step_avg:40.02ms
step:1250/2330 train_time:50039ms step_avg:40.03ms
step:1250/2330 val_loss:5.5768 train_time:50138ms step_avg:40.11ms
step:1251/2330 train_time:50150ms step_avg:40.09ms
step:1252/2330 train_time:50163ms step_avg:40.07ms
step:1253/2330 train_time:50174ms step_avg:40.04ms
step:1254/2330 train_time:50202ms step_avg:40.03ms
step:1255/2330 train_time:50224ms step_avg:40.02ms
step:1256/2330 train_time:50280ms step_avg:40.03ms
step:1257/2330 train_time:50301ms step_avg:40.02ms
step:1258/2330 train_time:50358ms step_avg:40.03ms
step:1259/2330 train_time:50381ms step_avg:40.02ms
step:1260/2330 train_time:50441ms step_avg:40.03ms
step:1261/2330 train_time:50467ms step_avg:40.02ms
step:1262/2330 train_time:50527ms step_avg:40.04ms
step:1263/2330 train_time:50550ms step_avg:40.02ms
step:1264/2330 train_time:50609ms step_avg:40.04ms
step:1265/2330 train_time:50631ms step_avg:40.02ms
step:1266/2330 train_time:50689ms step_avg:40.04ms
step:1267/2330 train_time:50711ms step_avg:40.02ms
step:1268/2330 train_time:50768ms step_avg:40.04ms
step:1269/2330 train_time:50791ms step_avg:40.02ms
step:1270/2330 train_time:50847ms step_avg:40.04ms
step:1271/2330 train_time:50870ms step_avg:40.02ms
step:1272/2330 train_time:50926ms step_avg:40.04ms
step:1273/2330 train_time:50948ms step_avg:40.02ms
step:1274/2330 train_time:51004ms step_avg:40.03ms
step:1275/2330 train_time:51026ms step_avg:40.02ms
step:1276/2330 train_time:51084ms step_avg:40.03ms
step:1277/2330 train_time:51107ms step_avg:40.02ms
step:1278/2330 train_time:51164ms step_avg:40.03ms
step:1279/2330 train_time:51187ms step_avg:40.02ms
step:1280/2330 train_time:51243ms step_avg:40.03ms
step:1281/2330 train_time:51265ms step_avg:40.02ms
step:1282/2330 train_time:51321ms step_avg:40.03ms
step:1283/2330 train_time:51345ms step_avg:40.02ms
step:1284/2330 train_time:51403ms step_avg:40.03ms
step:1285/2330 train_time:51427ms step_avg:40.02ms
step:1286/2330 train_time:51484ms step_avg:40.03ms
step:1287/2330 train_time:51508ms step_avg:40.02ms
step:1288/2330 train_time:51565ms step_avg:40.04ms
step:1289/2330 train_time:51589ms step_avg:40.02ms
step:1290/2330 train_time:51645ms step_avg:40.04ms
step:1291/2330 train_time:51669ms step_avg:40.02ms
step:1292/2330 train_time:51727ms step_avg:40.04ms
step:1293/2330 train_time:51751ms step_avg:40.02ms
step:1294/2330 train_time:51808ms step_avg:40.04ms
step:1295/2330 train_time:51830ms step_avg:40.02ms
step:1296/2330 train_time:51887ms step_avg:40.04ms
step:1297/2330 train_time:51909ms step_avg:40.02ms
step:1298/2330 train_time:51966ms step_avg:40.04ms
step:1299/2330 train_time:51989ms step_avg:40.02ms
step:1300/2330 train_time:52046ms step_avg:40.04ms
step:1301/2330 train_time:52069ms step_avg:40.02ms
step:1302/2330 train_time:52126ms step_avg:40.04ms
step:1303/2330 train_time:52149ms step_avg:40.02ms
step:1304/2330 train_time:52206ms step_avg:40.04ms
step:1305/2330 train_time:52230ms step_avg:40.02ms
step:1306/2330 train_time:52287ms step_avg:40.04ms
step:1307/2330 train_time:52309ms step_avg:40.02ms
step:1308/2330 train_time:52368ms step_avg:40.04ms
step:1309/2330 train_time:52390ms step_avg:40.02ms
step:1310/2330 train_time:52448ms step_avg:40.04ms
step:1311/2330 train_time:52471ms step_avg:40.02ms
step:1312/2330 train_time:52530ms step_avg:40.04ms
step:1313/2330 train_time:52553ms step_avg:40.02ms
step:1314/2330 train_time:52611ms step_avg:40.04ms
step:1315/2330 train_time:52633ms step_avg:40.03ms
step:1316/2330 train_time:52692ms step_avg:40.04ms
step:1317/2330 train_time:52714ms step_avg:40.03ms
step:1318/2330 train_time:52772ms step_avg:40.04ms
step:1319/2330 train_time:52795ms step_avg:40.03ms
step:1320/2330 train_time:52852ms step_avg:40.04ms
step:1321/2330 train_time:52875ms step_avg:40.03ms
step:1322/2330 train_time:52933ms step_avg:40.04ms
step:1323/2330 train_time:52956ms step_avg:40.03ms
step:1324/2330 train_time:53014ms step_avg:40.04ms
step:1325/2330 train_time:53037ms step_avg:40.03ms
step:1326/2330 train_time:53095ms step_avg:40.04ms
step:1327/2330 train_time:53117ms step_avg:40.03ms
step:1328/2330 train_time:53175ms step_avg:40.04ms
step:1329/2330 train_time:53198ms step_avg:40.03ms
step:1330/2330 train_time:53256ms step_avg:40.04ms
step:1331/2330 train_time:53279ms step_avg:40.03ms
step:1332/2330 train_time:53336ms step_avg:40.04ms
step:1333/2330 train_time:53360ms step_avg:40.03ms
step:1334/2330 train_time:53418ms step_avg:40.04ms
step:1335/2330 train_time:53442ms step_avg:40.03ms
step:1336/2330 train_time:53499ms step_avg:40.04ms
step:1337/2330 train_time:53523ms step_avg:40.03ms
step:1338/2330 train_time:53580ms step_avg:40.05ms
step:1339/2330 train_time:53604ms step_avg:40.03ms
step:1340/2330 train_time:53661ms step_avg:40.05ms
step:1341/2330 train_time:53685ms step_avg:40.03ms
step:1342/2330 train_time:53743ms step_avg:40.05ms
step:1343/2330 train_time:53767ms step_avg:40.03ms
step:1344/2330 train_time:53823ms step_avg:40.05ms
step:1345/2330 train_time:53846ms step_avg:40.03ms
step:1346/2330 train_time:53903ms step_avg:40.05ms
step:1347/2330 train_time:53926ms step_avg:40.03ms
step:1348/2330 train_time:53983ms step_avg:40.05ms
step:1349/2330 train_time:54007ms step_avg:40.03ms
step:1350/2330 train_time:54063ms step_avg:40.05ms
step:1351/2330 train_time:54087ms step_avg:40.03ms
step:1352/2330 train_time:54143ms step_avg:40.05ms
step:1353/2330 train_time:54167ms step_avg:40.03ms
step:1354/2330 train_time:54224ms step_avg:40.05ms
step:1355/2330 train_time:54247ms step_avg:40.03ms
step:1356/2330 train_time:54303ms step_avg:40.05ms
step:1357/2330 train_time:54326ms step_avg:40.03ms
step:1358/2330 train_time:54382ms step_avg:40.05ms
step:1359/2330 train_time:54406ms step_avg:40.03ms
step:1360/2330 train_time:54462ms step_avg:40.05ms
step:1361/2330 train_time:54486ms step_avg:40.03ms
step:1362/2330 train_time:54544ms step_avg:40.05ms
step:1363/2330 train_time:54568ms step_avg:40.04ms
step:1364/2330 train_time:54624ms step_avg:40.05ms
step:1365/2330 train_time:54648ms step_avg:40.04ms
step:1366/2330 train_time:54705ms step_avg:40.05ms
step:1367/2330 train_time:54728ms step_avg:40.04ms
step:1368/2330 train_time:54785ms step_avg:40.05ms
step:1369/2330 train_time:54808ms step_avg:40.04ms
step:1370/2330 train_time:54865ms step_avg:40.05ms
step:1371/2330 train_time:54889ms step_avg:40.04ms
step:1372/2330 train_time:54945ms step_avg:40.05ms
step:1373/2330 train_time:54968ms step_avg:40.03ms
step:1374/2330 train_time:55025ms step_avg:40.05ms
step:1375/2330 train_time:55048ms step_avg:40.03ms
step:1376/2330 train_time:55104ms step_avg:40.05ms
step:1377/2330 train_time:55127ms step_avg:40.03ms
step:1378/2330 train_time:55183ms step_avg:40.05ms
step:1379/2330 train_time:55207ms step_avg:40.03ms
step:1380/2330 train_time:55263ms step_avg:40.05ms
step:1381/2330 train_time:55287ms step_avg:40.03ms
step:1382/2330 train_time:55344ms step_avg:40.05ms
step:1383/2330 train_time:55367ms step_avg:40.03ms
step:1384/2330 train_time:55424ms step_avg:40.05ms
step:1385/2330 train_time:55448ms step_avg:40.03ms
step:1386/2330 train_time:55504ms step_avg:40.05ms
step:1387/2330 train_time:55527ms step_avg:40.03ms
step:1388/2330 train_time:55584ms step_avg:40.05ms
step:1389/2330 train_time:55607ms step_avg:40.03ms
step:1390/2330 train_time:55664ms step_avg:40.05ms
step:1391/2330 train_time:55687ms step_avg:40.03ms
step:1392/2330 train_time:55744ms step_avg:40.05ms
step:1393/2330 train_time:55768ms step_avg:40.03ms
step:1394/2330 train_time:55824ms step_avg:40.05ms
step:1395/2330 train_time:55848ms step_avg:40.03ms
step:1396/2330 train_time:55904ms step_avg:40.05ms
step:1397/2330 train_time:55927ms step_avg:40.03ms
step:1398/2330 train_time:55984ms step_avg:40.05ms
step:1399/2330 train_time:56007ms step_avg:40.03ms
step:1400/2330 train_time:56063ms step_avg:40.05ms
step:1401/2330 train_time:56087ms step_avg:40.03ms
step:1402/2330 train_time:56143ms step_avg:40.04ms
step:1403/2330 train_time:56167ms step_avg:40.03ms
step:1404/2330 train_time:56224ms step_avg:40.05ms
step:1405/2330 train_time:56247ms step_avg:40.03ms
step:1406/2330 train_time:56304ms step_avg:40.05ms
step:1407/2330 train_time:56326ms step_avg:40.03ms
step:1408/2330 train_time:56383ms step_avg:40.04ms
step:1409/2330 train_time:56406ms step_avg:40.03ms
step:1410/2330 train_time:56463ms step_avg:40.04ms
step:1411/2330 train_time:56486ms step_avg:40.03ms
step:1412/2330 train_time:56543ms step_avg:40.04ms
step:1413/2330 train_time:56565ms step_avg:40.03ms
step:1414/2330 train_time:56622ms step_avg:40.04ms
step:1415/2330 train_time:56645ms step_avg:40.03ms
step:1416/2330 train_time:56702ms step_avg:40.04ms
step:1417/2330 train_time:56726ms step_avg:40.03ms
step:1418/2330 train_time:56782ms step_avg:40.04ms
step:1419/2330 train_time:56805ms step_avg:40.03ms
step:1420/2330 train_time:56862ms step_avg:40.04ms
step:1421/2330 train_time:56887ms step_avg:40.03ms
step:1422/2330 train_time:56944ms step_avg:40.04ms
step:1423/2330 train_time:56967ms step_avg:40.03ms
step:1424/2330 train_time:57023ms step_avg:40.04ms
step:1425/2330 train_time:57047ms step_avg:40.03ms
step:1426/2330 train_time:57103ms step_avg:40.04ms
step:1427/2330 train_time:57126ms step_avg:40.03ms
step:1428/2330 train_time:57183ms step_avg:40.04ms
step:1429/2330 train_time:57207ms step_avg:40.03ms
step:1430/2330 train_time:57263ms step_avg:40.04ms
step:1431/2330 train_time:57287ms step_avg:40.03ms
step:1432/2330 train_time:57343ms step_avg:40.04ms
step:1433/2330 train_time:57367ms step_avg:40.03ms
step:1434/2330 train_time:57424ms step_avg:40.04ms
step:1435/2330 train_time:57447ms step_avg:40.03ms
step:1436/2330 train_time:57503ms step_avg:40.04ms
step:1437/2330 train_time:57526ms step_avg:40.03ms
step:1438/2330 train_time:57583ms step_avg:40.04ms
step:1439/2330 train_time:57607ms step_avg:40.03ms
step:1440/2330 train_time:57663ms step_avg:40.04ms
step:1441/2330 train_time:57686ms step_avg:40.03ms
step:1442/2330 train_time:57743ms step_avg:40.04ms
step:1443/2330 train_time:57766ms step_avg:40.03ms
step:1444/2330 train_time:57823ms step_avg:40.04ms
step:1445/2330 train_time:57847ms step_avg:40.03ms
step:1446/2330 train_time:57904ms step_avg:40.04ms
step:1447/2330 train_time:57928ms step_avg:40.03ms
step:1448/2330 train_time:57984ms step_avg:40.04ms
step:1449/2330 train_time:58007ms step_avg:40.03ms
step:1450/2330 train_time:58064ms step_avg:40.04ms
step:1451/2330 train_time:58088ms step_avg:40.03ms
step:1452/2330 train_time:58144ms step_avg:40.04ms
step:1453/2330 train_time:58167ms step_avg:40.03ms
step:1454/2330 train_time:58224ms step_avg:40.04ms
step:1455/2330 train_time:58247ms step_avg:40.03ms
step:1456/2330 train_time:58304ms step_avg:40.04ms
step:1457/2330 train_time:58327ms step_avg:40.03ms
step:1458/2330 train_time:58384ms step_avg:40.04ms
step:1459/2330 train_time:58408ms step_avg:40.03ms
step:1460/2330 train_time:58464ms step_avg:40.04ms
step:1461/2330 train_time:58487ms step_avg:40.03ms
step:1462/2330 train_time:58544ms step_avg:40.04ms
step:1463/2330 train_time:58567ms step_avg:40.03ms
step:1464/2330 train_time:58624ms step_avg:40.04ms
step:1465/2330 train_time:58648ms step_avg:40.03ms
step:1466/2330 train_time:58704ms step_avg:40.04ms
step:1467/2330 train_time:58727ms step_avg:40.03ms
step:1468/2330 train_time:58784ms step_avg:40.04ms
step:1469/2330 train_time:58807ms step_avg:40.03ms
step:1470/2330 train_time:58864ms step_avg:40.04ms
step:1471/2330 train_time:58886ms step_avg:40.03ms
step:1472/2330 train_time:58943ms step_avg:40.04ms
step:1473/2330 train_time:58967ms step_avg:40.03ms
step:1474/2330 train_time:59023ms step_avg:40.04ms
step:1475/2330 train_time:59047ms step_avg:40.03ms
step:1476/2330 train_time:59104ms step_avg:40.04ms
step:1477/2330 train_time:59127ms step_avg:40.03ms
step:1478/2330 train_time:59183ms step_avg:40.04ms
step:1479/2330 train_time:59207ms step_avg:40.03ms
step:1480/2330 train_time:59264ms step_avg:40.04ms
step:1481/2330 train_time:59287ms step_avg:40.03ms
step:1482/2330 train_time:59344ms step_avg:40.04ms
step:1483/2330 train_time:59367ms step_avg:40.03ms
step:1484/2330 train_time:59424ms step_avg:40.04ms
step:1485/2330 train_time:59447ms step_avg:40.03ms
step:1486/2330 train_time:59504ms step_avg:40.04ms
step:1487/2330 train_time:59528ms step_avg:40.03ms
step:1488/2330 train_time:59584ms step_avg:40.04ms
step:1489/2330 train_time:59608ms step_avg:40.03ms
step:1490/2330 train_time:59664ms step_avg:40.04ms
step:1491/2330 train_time:59688ms step_avg:40.03ms
step:1492/2330 train_time:59744ms step_avg:40.04ms
step:1493/2330 train_time:59768ms step_avg:40.03ms
step:1494/2330 train_time:59824ms step_avg:40.04ms
step:1495/2330 train_time:59847ms step_avg:40.03ms
step:1496/2330 train_time:59904ms step_avg:40.04ms
step:1497/2330 train_time:59927ms step_avg:40.03ms
step:1498/2330 train_time:59984ms step_avg:40.04ms
step:1499/2330 train_time:60007ms step_avg:40.03ms
step:1500/2330 train_time:60064ms step_avg:40.04ms
step:1500/2330 val_loss:5.5482 train_time:60162ms step_avg:40.11ms
step:1501/2330 train_time:60174ms step_avg:40.09ms
step:1502/2330 train_time:60186ms step_avg:40.07ms
step:1503/2330 train_time:60196ms step_avg:40.05ms
step:1504/2330 train_time:60225ms step_avg:40.04ms
step:1505/2330 train_time:60247ms step_avg:40.03ms
step:1506/2330 train_time:60303ms step_avg:40.04ms
step:1507/2330 train_time:60325ms step_avg:40.03ms
step:1508/2330 train_time:60381ms step_avg:40.04ms
step:1509/2330 train_time:60404ms step_avg:40.03ms
step:1510/2330 train_time:60462ms step_avg:40.04ms
step:1511/2330 train_time:60490ms step_avg:40.03ms
step:1512/2330 train_time:60551ms step_avg:40.05ms
step:1513/2330 train_time:60576ms step_avg:40.04ms
step:1514/2330 train_time:60634ms step_avg:40.05ms
step:1515/2330 train_time:60657ms step_avg:40.04ms
step:1516/2330 train_time:60714ms step_avg:40.05ms
step:1517/2330 train_time:60737ms step_avg:40.04ms
step:1518/2330 train_time:60794ms step_avg:40.05ms
step:1519/2330 train_time:60816ms step_avg:40.04ms
step:1520/2330 train_time:60874ms step_avg:40.05ms
step:1521/2330 train_time:60896ms step_avg:40.04ms
step:1522/2330 train_time:60953ms step_avg:40.05ms
step:1523/2330 train_time:60976ms step_avg:40.04ms
step:1524/2330 train_time:61032ms step_avg:40.05ms
step:1525/2330 train_time:61055ms step_avg:40.04ms
step:1526/2330 train_time:61114ms step_avg:40.05ms
step:1527/2330 train_time:61137ms step_avg:40.04ms
step:1528/2330 train_time:61196ms step_avg:40.05ms
step:1529/2330 train_time:61219ms step_avg:40.04ms
step:1530/2330 train_time:61277ms step_avg:40.05ms
step:1531/2330 train_time:61298ms step_avg:40.04ms
step:1532/2330 train_time:61356ms step_avg:40.05ms
step:1533/2330 train_time:61379ms step_avg:40.04ms
step:1534/2330 train_time:61438ms step_avg:40.05ms
step:1535/2330 train_time:61462ms step_avg:40.04ms
step:1536/2330 train_time:61521ms step_avg:40.05ms
step:1537/2330 train_time:61546ms step_avg:40.04ms
step:1538/2330 train_time:61604ms step_avg:40.05ms
step:1539/2330 train_time:61627ms step_avg:40.04ms
step:1540/2330 train_time:61684ms step_avg:40.05ms
step:1541/2330 train_time:61707ms step_avg:40.04ms
step:1542/2330 train_time:61763ms step_avg:40.05ms
step:1543/2330 train_time:61787ms step_avg:40.04ms
step:1544/2330 train_time:61844ms step_avg:40.05ms
step:1545/2330 train_time:61867ms step_avg:40.04ms
step:1546/2330 train_time:61924ms step_avg:40.05ms
step:1547/2330 train_time:61947ms step_avg:40.04ms
step:1548/2330 train_time:62003ms step_avg:40.05ms
step:1549/2330 train_time:62027ms step_avg:40.04ms
step:1550/2330 train_time:62084ms step_avg:40.05ms
step:1551/2330 train_time:62107ms step_avg:40.04ms
step:1552/2330 train_time:62164ms step_avg:40.05ms
step:1553/2330 train_time:62187ms step_avg:40.04ms
step:1554/2330 train_time:62244ms step_avg:40.05ms
step:1555/2330 train_time:62267ms step_avg:40.04ms
step:1556/2330 train_time:62324ms step_avg:40.05ms
step:1557/2330 train_time:62349ms step_avg:40.04ms
step:1558/2330 train_time:62406ms step_avg:40.06ms
step:1559/2330 train_time:62429ms step_avg:40.04ms
step:1560/2330 train_time:62487ms step_avg:40.06ms
step:1561/2330 train_time:62511ms step_avg:40.05ms
step:1562/2330 train_time:62568ms step_avg:40.06ms
step:1563/2330 train_time:62591ms step_avg:40.05ms
step:1564/2330 train_time:62648ms step_avg:40.06ms
step:1565/2330 train_time:62671ms step_avg:40.05ms
step:1566/2330 train_time:62728ms step_avg:40.06ms
step:1567/2330 train_time:62751ms step_avg:40.05ms
step:1568/2330 train_time:62807ms step_avg:40.06ms
step:1569/2330 train_time:62830ms step_avg:40.04ms
step:1570/2330 train_time:62887ms step_avg:40.06ms
step:1571/2330 train_time:62911ms step_avg:40.04ms
step:1572/2330 train_time:62967ms step_avg:40.06ms
step:1573/2330 train_time:62990ms step_avg:40.04ms
step:1574/2330 train_time:63047ms step_avg:40.06ms
step:1575/2330 train_time:63070ms step_avg:40.04ms
step:1576/2330 train_time:63127ms step_avg:40.06ms
step:1577/2330 train_time:63150ms step_avg:40.04ms
step:1578/2330 train_time:63206ms step_avg:40.05ms
step:1579/2330 train_time:63229ms step_avg:40.04ms
step:1580/2330 train_time:63285ms step_avg:40.05ms
step:1581/2330 train_time:63309ms step_avg:40.04ms
step:1582/2330 train_time:63366ms step_avg:40.05ms
step:1583/2330 train_time:63390ms step_avg:40.04ms
step:1584/2330 train_time:63446ms step_avg:40.05ms
step:1585/2330 train_time:63470ms step_avg:40.04ms
step:1586/2330 train_time:63527ms step_avg:40.05ms
step:1587/2330 train_time:63551ms step_avg:40.04ms
step:1588/2330 train_time:63607ms step_avg:40.06ms
step:1589/2330 train_time:63631ms step_avg:40.04ms
step:1590/2330 train_time:63688ms step_avg:40.06ms
step:1591/2330 train_time:63712ms step_avg:40.05ms
step:1592/2330 train_time:63769ms step_avg:40.06ms
step:1593/2330 train_time:63792ms step_avg:40.05ms
step:1594/2330 train_time:63849ms step_avg:40.06ms
step:1595/2330 train_time:63871ms step_avg:40.04ms
step:1596/2330 train_time:63929ms step_avg:40.06ms
step:1597/2330 train_time:63953ms step_avg:40.05ms
step:1598/2330 train_time:64010ms step_avg:40.06ms
step:1599/2330 train_time:64034ms step_avg:40.05ms
step:1600/2330 train_time:64092ms step_avg:40.06ms
step:1601/2330 train_time:64114ms step_avg:40.05ms
step:1602/2330 train_time:64172ms step_avg:40.06ms
step:1603/2330 train_time:64194ms step_avg:40.05ms
step:1604/2330 train_time:64252ms step_avg:40.06ms
step:1605/2330 train_time:64274ms step_avg:40.05ms
step:1606/2330 train_time:64332ms step_avg:40.06ms
step:1607/2330 train_time:64355ms step_avg:40.05ms
step:1608/2330 train_time:64412ms step_avg:40.06ms
step:1609/2330 train_time:64435ms step_avg:40.05ms
step:1610/2330 train_time:64494ms step_avg:40.06ms
step:1611/2330 train_time:64517ms step_avg:40.05ms
step:1612/2330 train_time:64574ms step_avg:40.06ms
step:1613/2330 train_time:64597ms step_avg:40.05ms
step:1614/2330 train_time:64655ms step_avg:40.06ms
step:1615/2330 train_time:64679ms step_avg:40.05ms
step:1616/2330 train_time:64737ms step_avg:40.06ms
step:1617/2330 train_time:64760ms step_avg:40.05ms
step:1618/2330 train_time:64817ms step_avg:40.06ms
step:1619/2330 train_time:64841ms step_avg:40.05ms
step:1620/2330 train_time:64898ms step_avg:40.06ms
step:1621/2330 train_time:64921ms step_avg:40.05ms
step:1622/2330 train_time:64979ms step_avg:40.06ms
step:1623/2330 train_time:65003ms step_avg:40.05ms
step:1624/2330 train_time:65060ms step_avg:40.06ms
step:1625/2330 train_time:65083ms step_avg:40.05ms
step:1626/2330 train_time:65141ms step_avg:40.06ms
step:1627/2330 train_time:65165ms step_avg:40.05ms
step:1628/2330 train_time:65222ms step_avg:40.06ms
step:1629/2330 train_time:65246ms step_avg:40.05ms
step:1630/2330 train_time:65303ms step_avg:40.06ms
step:1631/2330 train_time:65327ms step_avg:40.05ms
step:1632/2330 train_time:65383ms step_avg:40.06ms
step:1633/2330 train_time:65407ms step_avg:40.05ms
step:1634/2330 train_time:65465ms step_avg:40.06ms
step:1635/2330 train_time:65489ms step_avg:40.05ms
step:1636/2330 train_time:65546ms step_avg:40.06ms
step:1637/2330 train_time:65570ms step_avg:40.05ms
step:1638/2330 train_time:65626ms step_avg:40.06ms
step:1639/2330 train_time:65651ms step_avg:40.06ms
step:1640/2330 train_time:65708ms step_avg:40.07ms
step:1641/2330 train_time:65731ms step_avg:40.06ms
step:1642/2330 train_time:65788ms step_avg:40.07ms
step:1643/2330 train_time:65811ms step_avg:40.06ms
step:1644/2330 train_time:65867ms step_avg:40.07ms
step:1645/2330 train_time:65891ms step_avg:40.06ms
step:1646/2330 train_time:65947ms step_avg:40.06ms
step:1647/2330 train_time:65969ms step_avg:40.05ms
step:1648/2330 train_time:66026ms step_avg:40.06ms
step:1649/2330 train_time:66050ms step_avg:40.05ms
step:1650/2330 train_time:66106ms step_avg:40.06ms
step:1651/2330 train_time:66129ms step_avg:40.05ms
step:1652/2330 train_time:66187ms step_avg:40.06ms
step:1653/2330 train_time:66210ms step_avg:40.05ms
step:1654/2330 train_time:66267ms step_avg:40.06ms
step:1655/2330 train_time:66290ms step_avg:40.05ms
step:1656/2330 train_time:66348ms step_avg:40.07ms
step:1657/2330 train_time:66372ms step_avg:40.06ms
step:1658/2330 train_time:66430ms step_avg:40.07ms
step:1659/2330 train_time:66453ms step_avg:40.06ms
step:1660/2330 train_time:66510ms step_avg:40.07ms
step:1661/2330 train_time:66533ms step_avg:40.06ms
step:1662/2330 train_time:66590ms step_avg:40.07ms
step:1663/2330 train_time:66612ms step_avg:40.06ms
step:1664/2330 train_time:66670ms step_avg:40.07ms
step:1665/2330 train_time:66692ms step_avg:40.06ms
step:1666/2330 train_time:66749ms step_avg:40.07ms
step:1667/2330 train_time:66771ms step_avg:40.05ms
step:1668/2330 train_time:66827ms step_avg:40.06ms
step:1669/2330 train_time:66851ms step_avg:40.05ms
step:1670/2330 train_time:66907ms step_avg:40.06ms
step:1671/2330 train_time:66931ms step_avg:40.05ms
step:1672/2330 train_time:66988ms step_avg:40.06ms
step:1673/2330 train_time:67011ms step_avg:40.05ms
step:1674/2330 train_time:67068ms step_avg:40.06ms
step:1675/2330 train_time:67092ms step_avg:40.05ms
step:1676/2330 train_time:67148ms step_avg:40.06ms
step:1677/2330 train_time:67171ms step_avg:40.05ms
step:1678/2330 train_time:67227ms step_avg:40.06ms
step:1679/2330 train_time:67251ms step_avg:40.05ms
step:1680/2330 train_time:67309ms step_avg:40.06ms
step:1681/2330 train_time:67332ms step_avg:40.05ms
step:1682/2330 train_time:67389ms step_avg:40.07ms
step:1683/2330 train_time:67413ms step_avg:40.06ms
step:1684/2330 train_time:67470ms step_avg:40.07ms
step:1685/2330 train_time:67493ms step_avg:40.05ms
step:1686/2330 train_time:67551ms step_avg:40.07ms
step:1687/2330 train_time:67574ms step_avg:40.06ms
step:1688/2330 train_time:67631ms step_avg:40.07ms
step:1689/2330 train_time:67653ms step_avg:40.06ms
step:1690/2330 train_time:67710ms step_avg:40.07ms
step:1691/2330 train_time:67732ms step_avg:40.05ms
step:1692/2330 train_time:67790ms step_avg:40.07ms
step:1693/2330 train_time:67813ms step_avg:40.05ms
step:1694/2330 train_time:67870ms step_avg:40.06ms
step:1695/2330 train_time:67892ms step_avg:40.05ms
step:1696/2330 train_time:67950ms step_avg:40.06ms
step:1697/2330 train_time:67973ms step_avg:40.05ms
step:1698/2330 train_time:68030ms step_avg:40.06ms
step:1699/2330 train_time:68052ms step_avg:40.05ms
step:1700/2330 train_time:68110ms step_avg:40.06ms
step:1701/2330 train_time:68132ms step_avg:40.05ms
step:1702/2330 train_time:68190ms step_avg:40.06ms
step:1703/2330 train_time:68212ms step_avg:40.05ms
step:1704/2330 train_time:68270ms step_avg:40.06ms
step:1705/2330 train_time:68293ms step_avg:40.05ms
step:1706/2330 train_time:68351ms step_avg:40.07ms
step:1707/2330 train_time:68373ms step_avg:40.05ms
step:1708/2330 train_time:68431ms step_avg:40.06ms
step:1709/2330 train_time:68453ms step_avg:40.05ms
step:1710/2330 train_time:68511ms step_avg:40.07ms
step:1711/2330 train_time:68533ms step_avg:40.05ms
step:1712/2330 train_time:68591ms step_avg:40.06ms
step:1713/2330 train_time:68613ms step_avg:40.05ms
step:1714/2330 train_time:68671ms step_avg:40.06ms
step:1715/2330 train_time:68693ms step_avg:40.05ms
step:1716/2330 train_time:68750ms step_avg:40.06ms
step:1717/2330 train_time:68772ms step_avg:40.05ms
step:1718/2330 train_time:68829ms step_avg:40.06ms
step:1719/2330 train_time:68852ms step_avg:40.05ms
step:1720/2330 train_time:68908ms step_avg:40.06ms
step:1721/2330 train_time:68932ms step_avg:40.05ms
step:1722/2330 train_time:68989ms step_avg:40.06ms
step:1723/2330 train_time:69012ms step_avg:40.05ms
step:1724/2330 train_time:69069ms step_avg:40.06ms
step:1725/2330 train_time:69092ms step_avg:40.05ms
step:1726/2330 train_time:69149ms step_avg:40.06ms
step:1727/2330 train_time:69173ms step_avg:40.05ms
step:1728/2330 train_time:69229ms step_avg:40.06ms
step:1729/2330 train_time:69252ms step_avg:40.05ms
step:1730/2330 train_time:69309ms step_avg:40.06ms
step:1731/2330 train_time:69332ms step_avg:40.05ms
step:1732/2330 train_time:69388ms step_avg:40.06ms
step:1733/2330 train_time:69411ms step_avg:40.05ms
step:1734/2330 train_time:69468ms step_avg:40.06ms
step:1735/2330 train_time:69491ms step_avg:40.05ms
step:1736/2330 train_time:69547ms step_avg:40.06ms
step:1737/2330 train_time:69570ms step_avg:40.05ms
step:1738/2330 train_time:69626ms step_avg:40.06ms
step:1739/2330 train_time:69649ms step_avg:40.05ms
step:1740/2330 train_time:69705ms step_avg:40.06ms
step:1741/2330 train_time:69729ms step_avg:40.05ms
step:1742/2330 train_time:69785ms step_avg:40.06ms
step:1743/2330 train_time:69809ms step_avg:40.05ms
step:1744/2330 train_time:69865ms step_avg:40.06ms
step:1745/2330 train_time:69888ms step_avg:40.05ms
step:1746/2330 train_time:69945ms step_avg:40.06ms
step:1747/2330 train_time:69968ms step_avg:40.05ms
step:1748/2330 train_time:70025ms step_avg:40.06ms
step:1749/2330 train_time:70049ms step_avg:40.05ms
step:1750/2330 train_time:70106ms step_avg:40.06ms
step:1750/2330 val_loss:5.5014 train_time:70204ms step_avg:40.12ms
step:1751/2330 train_time:70217ms step_avg:40.10ms
step:1752/2330 train_time:70228ms step_avg:40.08ms
step:1753/2330 train_time:70239ms step_avg:40.07ms
step:1754/2330 train_time:70266ms step_avg:40.06ms
step:1755/2330 train_time:70288ms step_avg:40.05ms
step:1756/2330 train_time:70344ms step_avg:40.06ms
step:1757/2330 train_time:70367ms step_avg:40.05ms
step:1758/2330 train_time:70423ms step_avg:40.06ms
step:1759/2330 train_time:70445ms step_avg:40.05ms
step:1760/2330 train_time:70504ms step_avg:40.06ms
step:1761/2330 train_time:70530ms step_avg:40.05ms
step:1762/2330 train_time:70591ms step_avg:40.06ms
step:1763/2330 train_time:70617ms step_avg:40.05ms
step:1764/2330 train_time:70675ms step_avg:40.07ms
step:1765/2330 train_time:70698ms step_avg:40.06ms
step:1766/2330 train_time:70755ms step_avg:40.07ms
step:1767/2330 train_time:70778ms step_avg:40.06ms
step:1768/2330 train_time:70835ms step_avg:40.07ms
step:1769/2330 train_time:70857ms step_avg:40.06ms
step:1770/2330 train_time:70914ms step_avg:40.06ms
step:1771/2330 train_time:70935ms step_avg:40.05ms
step:1772/2330 train_time:70992ms step_avg:40.06ms
step:1773/2330 train_time:71014ms step_avg:40.05ms
step:1774/2330 train_time:71070ms step_avg:40.06ms
step:1775/2330 train_time:71093ms step_avg:40.05ms
step:1776/2330 train_time:71151ms step_avg:40.06ms
step:1777/2330 train_time:71175ms step_avg:40.05ms
step:1778/2330 train_time:71233ms step_avg:40.06ms
step:1779/2330 train_time:71257ms step_avg:40.05ms
step:1780/2330 train_time:71314ms step_avg:40.06ms
step:1781/2330 train_time:71336ms step_avg:40.05ms
step:1782/2330 train_time:71393ms step_avg:40.06ms
step:1783/2330 train_time:71417ms step_avg:40.05ms
step:1784/2330 train_time:71475ms step_avg:40.06ms
step:1785/2330 train_time:71499ms step_avg:40.06ms
step:1786/2330 train_time:71558ms step_avg:40.07ms
step:1787/2330 train_time:71583ms step_avg:40.06ms
step:1788/2330 train_time:71642ms step_avg:40.07ms
step:1789/2330 train_time:71666ms step_avg:40.06ms
step:1790/2330 train_time:71724ms step_avg:40.07ms
step:1791/2330 train_time:71748ms step_avg:40.06ms
step:1792/2330 train_time:71806ms step_avg:40.07ms
step:1793/2330 train_time:71829ms step_avg:40.06ms
step:1794/2330 train_time:71886ms step_avg:40.07ms
step:1795/2330 train_time:71909ms step_avg:40.06ms
step:1796/2330 train_time:71966ms step_avg:40.07ms
step:1797/2330 train_time:71989ms step_avg:40.06ms
step:1798/2330 train_time:72046ms step_avg:40.07ms
step:1799/2330 train_time:72069ms step_avg:40.06ms
step:1800/2330 train_time:72127ms step_avg:40.07ms
step:1801/2330 train_time:72150ms step_avg:40.06ms
step:1802/2330 train_time:72207ms step_avg:40.07ms
step:1803/2330 train_time:72230ms step_avg:40.06ms
step:1804/2330 train_time:72286ms step_avg:40.07ms
step:1805/2330 train_time:72310ms step_avg:40.06ms
step:1806/2330 train_time:72367ms step_avg:40.07ms
step:1807/2330 train_time:72390ms step_avg:40.06ms
step:1808/2330 train_time:72449ms step_avg:40.07ms
step:1809/2330 train_time:72473ms step_avg:40.06ms
step:1810/2330 train_time:72531ms step_avg:40.07ms
step:1811/2330 train_time:72555ms step_avg:40.06ms
step:1812/2330 train_time:72612ms step_avg:40.07ms
step:1813/2330 train_time:72637ms step_avg:40.06ms
step:1814/2330 train_time:72693ms step_avg:40.07ms
step:1815/2330 train_time:72716ms step_avg:40.06ms
step:1816/2330 train_time:72772ms step_avg:40.07ms
step:1817/2330 train_time:72795ms step_avg:40.06ms
step:1818/2330 train_time:72852ms step_avg:40.07ms
step:1819/2330 train_time:72875ms step_avg:40.06ms
step:1820/2330 train_time:72932ms step_avg:40.07ms
step:1821/2330 train_time:72955ms step_avg:40.06ms
step:1822/2330 train_time:73011ms step_avg:40.07ms
step:1823/2330 train_time:73034ms step_avg:40.06ms
step:1824/2330 train_time:73091ms step_avg:40.07ms
step:1825/2330 train_time:73114ms step_avg:40.06ms
step:1826/2330 train_time:73171ms step_avg:40.07ms
step:1827/2330 train_time:73194ms step_avg:40.06ms
step:1828/2330 train_time:73251ms step_avg:40.07ms
step:1829/2330 train_time:73274ms step_avg:40.06ms
step:1830/2330 train_time:73331ms step_avg:40.07ms
step:1831/2330 train_time:73354ms step_avg:40.06ms
step:1832/2330 train_time:73411ms step_avg:40.07ms
step:1833/2330 train_time:73434ms step_avg:40.06ms
step:1834/2330 train_time:73491ms step_avg:40.07ms
step:1835/2330 train_time:73515ms step_avg:40.06ms
step:1836/2330 train_time:73572ms step_avg:40.07ms
step:1837/2330 train_time:73595ms step_avg:40.06ms
step:1838/2330 train_time:73652ms step_avg:40.07ms
step:1839/2330 train_time:73675ms step_avg:40.06ms
step:1840/2330 train_time:73732ms step_avg:40.07ms
step:1841/2330 train_time:73756ms step_avg:40.06ms
step:1842/2330 train_time:73813ms step_avg:40.07ms
step:1843/2330 train_time:73837ms step_avg:40.06ms
step:1844/2330 train_time:73894ms step_avg:40.07ms
step:1845/2330 train_time:73917ms step_avg:40.06ms
step:1846/2330 train_time:73974ms step_avg:40.07ms
step:1847/2330 train_time:73997ms step_avg:40.06ms
step:1848/2330 train_time:74054ms step_avg:40.07ms
step:1849/2330 train_time:74077ms step_avg:40.06ms
step:1850/2330 train_time:74134ms step_avg:40.07ms
step:1851/2330 train_time:74157ms step_avg:40.06ms
step:1852/2330 train_time:74214ms step_avg:40.07ms
step:1853/2330 train_time:74237ms step_avg:40.06ms
step:1854/2330 train_time:74295ms step_avg:40.07ms
step:1855/2330 train_time:74318ms step_avg:40.06ms
step:1856/2330 train_time:74377ms step_avg:40.07ms
step:1857/2330 train_time:74399ms step_avg:40.06ms
step:1858/2330 train_time:74456ms step_avg:40.07ms
step:1859/2330 train_time:74480ms step_avg:40.06ms
step:1860/2330 train_time:74539ms step_avg:40.07ms
step:1861/2330 train_time:74562ms step_avg:40.07ms
step:1862/2330 train_time:74622ms step_avg:40.08ms
step:1863/2330 train_time:74645ms step_avg:40.07ms
step:1864/2330 train_time:74703ms step_avg:40.08ms
step:1865/2330 train_time:74727ms step_avg:40.07ms
step:1866/2330 train_time:74785ms step_avg:40.08ms
step:1867/2330 train_time:74808ms step_avg:40.07ms
step:1868/2330 train_time:74865ms step_avg:40.08ms
step:1869/2330 train_time:74889ms step_avg:40.07ms
step:1870/2330 train_time:74946ms step_avg:40.08ms
step:1871/2330 train_time:74970ms step_avg:40.07ms
step:1872/2330 train_time:75027ms step_avg:40.08ms
step:1873/2330 train_time:75051ms step_avg:40.07ms
step:1874/2330 train_time:75108ms step_avg:40.08ms
step:1875/2330 train_time:75132ms step_avg:40.07ms
step:1876/2330 train_time:75188ms step_avg:40.08ms
step:1877/2330 train_time:75212ms step_avg:40.07ms
step:1878/2330 train_time:75268ms step_avg:40.08ms
step:1879/2330 train_time:75292ms step_avg:40.07ms
step:1880/2330 train_time:75350ms step_avg:40.08ms
step:1881/2330 train_time:75373ms step_avg:40.07ms
step:1882/2330 train_time:75430ms step_avg:40.08ms
step:1883/2330 train_time:75453ms step_avg:40.07ms
step:1884/2330 train_time:75510ms step_avg:40.08ms
step:1885/2330 train_time:75533ms step_avg:40.07ms
step:1886/2330 train_time:75590ms step_avg:40.08ms
step:1887/2330 train_time:75614ms step_avg:40.07ms
step:1888/2330 train_time:75670ms step_avg:40.08ms
step:1889/2330 train_time:75694ms step_avg:40.07ms
step:1890/2330 train_time:75750ms step_avg:40.08ms
step:1891/2330 train_time:75773ms step_avg:40.07ms
step:1892/2330 train_time:75830ms step_avg:40.08ms
step:1893/2330 train_time:75853ms step_avg:40.07ms
step:1894/2330 train_time:75910ms step_avg:40.08ms
step:1895/2330 train_time:75933ms step_avg:40.07ms
step:1896/2330 train_time:75990ms step_avg:40.08ms
step:1897/2330 train_time:76013ms step_avg:40.07ms
step:1898/2330 train_time:76070ms step_avg:40.08ms
step:1899/2330 train_time:76094ms step_avg:40.07ms
step:1900/2330 train_time:76151ms step_avg:40.08ms
step:1901/2330 train_time:76174ms step_avg:40.07ms
step:1902/2330 train_time:76231ms step_avg:40.08ms
step:1903/2330 train_time:76255ms step_avg:40.07ms
step:1904/2330 train_time:76311ms step_avg:40.08ms
step:1905/2330 train_time:76334ms step_avg:40.07ms
step:1906/2330 train_time:76391ms step_avg:40.08ms
step:1907/2330 train_time:76414ms step_avg:40.07ms
step:1908/2330 train_time:76470ms step_avg:40.08ms
step:1909/2330 train_time:76494ms step_avg:40.07ms
step:1910/2330 train_time:76551ms step_avg:40.08ms
step:1911/2330 train_time:76575ms step_avg:40.07ms
step:1912/2330 train_time:76631ms step_avg:40.08ms
step:1913/2330 train_time:76655ms step_avg:40.07ms
step:1914/2330 train_time:76711ms step_avg:40.08ms
step:1915/2330 train_time:76735ms step_avg:40.07ms
step:1916/2330 train_time:76792ms step_avg:40.08ms
step:1917/2330 train_time:76816ms step_avg:40.07ms
step:1918/2330 train_time:76873ms step_avg:40.08ms
step:1919/2330 train_time:76897ms step_avg:40.07ms
step:1920/2330 train_time:76953ms step_avg:40.08ms
step:1921/2330 train_time:76977ms step_avg:40.07ms
step:1922/2330 train_time:77035ms step_avg:40.08ms
step:1923/2330 train_time:77058ms step_avg:40.07ms
step:1924/2330 train_time:77115ms step_avg:40.08ms
step:1925/2330 train_time:77137ms step_avg:40.07ms
step:1926/2330 train_time:77194ms step_avg:40.08ms
step:1927/2330 train_time:77216ms step_avg:40.07ms
step:1928/2330 train_time:77273ms step_avg:40.08ms
step:1929/2330 train_time:77295ms step_avg:40.07ms
step:1930/2330 train_time:77353ms step_avg:40.08ms
step:1931/2330 train_time:77376ms step_avg:40.07ms
step:1932/2330 train_time:77433ms step_avg:40.08ms
step:1933/2330 train_time:77456ms step_avg:40.07ms
step:1934/2330 train_time:77512ms step_avg:40.08ms
step:1935/2330 train_time:77535ms step_avg:40.07ms
step:1936/2330 train_time:77592ms step_avg:40.08ms
step:1937/2330 train_time:77615ms step_avg:40.07ms
step:1938/2330 train_time:77672ms step_avg:40.08ms
step:1939/2330 train_time:77695ms step_avg:40.07ms
step:1940/2330 train_time:77751ms step_avg:40.08ms
step:1941/2330 train_time:77775ms step_avg:40.07ms
step:1942/2330 train_time:77831ms step_avg:40.08ms
step:1943/2330 train_time:77855ms step_avg:40.07ms
step:1944/2330 train_time:77911ms step_avg:40.08ms
step:1945/2330 train_time:77934ms step_avg:40.07ms
step:1946/2330 train_time:77990ms step_avg:40.08ms
step:1947/2330 train_time:78015ms step_avg:40.07ms
step:1948/2330 train_time:78071ms step_avg:40.08ms
step:1949/2330 train_time:78094ms step_avg:40.07ms
step:1950/2330 train_time:78151ms step_avg:40.08ms
step:1951/2330 train_time:78174ms step_avg:40.07ms
step:1952/2330 train_time:78230ms step_avg:40.08ms
step:1953/2330 train_time:78253ms step_avg:40.07ms
step:1954/2330 train_time:78311ms step_avg:40.08ms
step:1955/2330 train_time:78334ms step_avg:40.07ms
step:1956/2330 train_time:78391ms step_avg:40.08ms
step:1957/2330 train_time:78415ms step_avg:40.07ms
step:1958/2330 train_time:78472ms step_avg:40.08ms
step:1959/2330 train_time:78495ms step_avg:40.07ms
step:1960/2330 train_time:78551ms step_avg:40.08ms
step:1961/2330 train_time:78574ms step_avg:40.07ms
step:1962/2330 train_time:78630ms step_avg:40.08ms
step:1963/2330 train_time:78654ms step_avg:40.07ms
step:1964/2330 train_time:78711ms step_avg:40.08ms
step:1965/2330 train_time:78734ms step_avg:40.07ms
step:1966/2330 train_time:78791ms step_avg:40.08ms
step:1967/2330 train_time:78814ms step_avg:40.07ms
step:1968/2330 train_time:78871ms step_avg:40.08ms
step:1969/2330 train_time:78894ms step_avg:40.07ms
step:1970/2330 train_time:78951ms step_avg:40.08ms
step:1971/2330 train_time:78975ms step_avg:40.07ms
step:1972/2330 train_time:79031ms step_avg:40.08ms
step:1973/2330 train_time:79054ms step_avg:40.07ms
step:1974/2330 train_time:79110ms step_avg:40.08ms
step:1975/2330 train_time:79134ms step_avg:40.07ms
step:1976/2330 train_time:79190ms step_avg:40.08ms
step:1977/2330 train_time:79213ms step_avg:40.07ms
step:1978/2330 train_time:79270ms step_avg:40.08ms
step:1979/2330 train_time:79293ms step_avg:40.07ms
step:1980/2330 train_time:79350ms step_avg:40.08ms
step:1981/2330 train_time:79375ms step_avg:40.07ms
step:1982/2330 train_time:79432ms step_avg:40.08ms
step:1983/2330 train_time:79455ms step_avg:40.07ms
step:1984/2330 train_time:79512ms step_avg:40.08ms
step:1985/2330 train_time:79534ms step_avg:40.07ms
step:1986/2330 train_time:79591ms step_avg:40.08ms
step:1987/2330 train_time:79615ms step_avg:40.07ms
step:1988/2330 train_time:79671ms step_avg:40.08ms
step:1989/2330 train_time:79694ms step_avg:40.07ms
step:1990/2330 train_time:79751ms step_avg:40.08ms
step:1991/2330 train_time:79775ms step_avg:40.07ms
step:1992/2330 train_time:79833ms step_avg:40.08ms
step:1993/2330 train_time:79856ms step_avg:40.07ms
step:1994/2330 train_time:79914ms step_avg:40.08ms
step:1995/2330 train_time:79937ms step_avg:40.07ms
step:1996/2330 train_time:79994ms step_avg:40.08ms
step:1997/2330 train_time:80016ms step_avg:40.07ms
step:1998/2330 train_time:80074ms step_avg:40.08ms
step:1999/2330 train_time:80097ms step_avg:40.07ms
step:2000/2330 train_time:80155ms step_avg:40.08ms
step:2000/2330 val_loss:5.4204 train_time:80253ms step_avg:40.13ms
step:2001/2330 train_time:80265ms step_avg:40.11ms
step:2002/2330 train_time:80278ms step_avg:40.10ms
step:2003/2330 train_time:80289ms step_avg:40.08ms
step:2004/2330 train_time:80317ms step_avg:40.08ms
step:2005/2330 train_time:80340ms step_avg:40.07ms
step:2006/2330 train_time:80396ms step_avg:40.08ms
step:2007/2330 train_time:80419ms step_avg:40.07ms
step:2008/2330 train_time:80475ms step_avg:40.08ms
step:2009/2330 train_time:80497ms step_avg:40.07ms
step:2010/2330 train_time:80558ms step_avg:40.08ms
step:2011/2330 train_time:80582ms step_avg:40.07ms
step:2012/2330 train_time:80643ms step_avg:40.08ms
step:2013/2330 train_time:80666ms step_avg:40.07ms
step:2014/2330 train_time:80724ms step_avg:40.08ms
step:2015/2330 train_time:80747ms step_avg:40.07ms
step:2016/2330 train_time:80804ms step_avg:40.08ms
step:2017/2330 train_time:80827ms step_avg:40.07ms
step:2018/2330 train_time:80884ms step_avg:40.08ms
step:2019/2330 train_time:80906ms step_avg:40.07ms
step:2020/2330 train_time:80963ms step_avg:40.08ms
step:2021/2330 train_time:80986ms step_avg:40.07ms
step:2022/2330 train_time:81043ms step_avg:40.08ms
step:2023/2330 train_time:81066ms step_avg:40.07ms
step:2024/2330 train_time:81123ms step_avg:40.08ms
step:2025/2330 train_time:81147ms step_avg:40.07ms
step:2026/2330 train_time:81207ms step_avg:40.08ms
step:2027/2330 train_time:81231ms step_avg:40.07ms
step:2028/2330 train_time:81288ms step_avg:40.08ms
step:2029/2330 train_time:81312ms step_avg:40.07ms
step:2030/2330 train_time:81368ms step_avg:40.08ms
step:2031/2330 train_time:81392ms step_avg:40.07ms
step:2032/2330 train_time:81449ms step_avg:40.08ms
step:2033/2330 train_time:81472ms step_avg:40.07ms
step:2034/2330 train_time:81530ms step_avg:40.08ms
step:2035/2330 train_time:81554ms step_avg:40.08ms
step:2036/2330 train_time:81611ms step_avg:40.08ms
step:2037/2330 train_time:81636ms step_avg:40.08ms
step:2038/2330 train_time:81692ms step_avg:40.08ms
step:2039/2330 train_time:81716ms step_avg:40.08ms
step:2040/2330 train_time:81772ms step_avg:40.08ms
step:2041/2330 train_time:81795ms step_avg:40.08ms
step:2042/2330 train_time:81850ms step_avg:40.08ms
step:2043/2330 train_time:81874ms step_avg:40.08ms
step:2044/2330 train_time:81931ms step_avg:40.08ms
step:2045/2330 train_time:81954ms step_avg:40.08ms
step:2046/2330 train_time:82010ms step_avg:40.08ms
step:2047/2330 train_time:82033ms step_avg:40.07ms
step:2048/2330 train_time:82089ms step_avg:40.08ms
step:2049/2330 train_time:82112ms step_avg:40.07ms
step:2050/2330 train_time:82170ms step_avg:40.08ms
step:2051/2330 train_time:82194ms step_avg:40.08ms
step:2052/2330 train_time:82250ms step_avg:40.08ms
step:2053/2330 train_time:82275ms step_avg:40.08ms
step:2054/2330 train_time:82332ms step_avg:40.08ms
step:2055/2330 train_time:82355ms step_avg:40.08ms
step:2056/2330 train_time:82412ms step_avg:40.08ms
step:2057/2330 train_time:82436ms step_avg:40.08ms
step:2058/2330 train_time:82492ms step_avg:40.08ms
step:2059/2330 train_time:82515ms step_avg:40.08ms
step:2060/2330 train_time:82572ms step_avg:40.08ms
step:2061/2330 train_time:82596ms step_avg:40.08ms
step:2062/2330 train_time:82652ms step_avg:40.08ms
step:2063/2330 train_time:82675ms step_avg:40.08ms
step:2064/2330 train_time:82731ms step_avg:40.08ms
step:2065/2330 train_time:82754ms step_avg:40.07ms
step:2066/2330 train_time:82811ms step_avg:40.08ms
step:2067/2330 train_time:82834ms step_avg:40.07ms
step:2068/2330 train_time:82890ms step_avg:40.08ms
step:2069/2330 train_time:82913ms step_avg:40.07ms
step:2070/2330 train_time:82970ms step_avg:40.08ms
step:2071/2330 train_time:82993ms step_avg:40.07ms
step:2072/2330 train_time:83050ms step_avg:40.08ms
step:2073/2330 train_time:83073ms step_avg:40.07ms
step:2074/2330 train_time:83130ms step_avg:40.08ms
step:2075/2330 train_time:83154ms step_avg:40.07ms
step:2076/2330 train_time:83211ms step_avg:40.08ms
step:2077/2330 train_time:83234ms step_avg:40.07ms
step:2078/2330 train_time:83291ms step_avg:40.08ms
step:2079/2330 train_time:83314ms step_avg:40.07ms
step:2080/2330 train_time:83371ms step_avg:40.08ms
step:2081/2330 train_time:83395ms step_avg:40.07ms
step:2082/2330 train_time:83451ms step_avg:40.08ms
step:2083/2330 train_time:83475ms step_avg:40.07ms
step:2084/2330 train_time:83532ms step_avg:40.08ms
step:2085/2330 train_time:83555ms step_avg:40.07ms
step:2086/2330 train_time:83611ms step_avg:40.08ms
step:2087/2330 train_time:83635ms step_avg:40.07ms
step:2088/2330 train_time:83691ms step_avg:40.08ms
step:2089/2330 train_time:83715ms step_avg:40.07ms
step:2090/2330 train_time:83772ms step_avg:40.08ms
step:2091/2330 train_time:83795ms step_avg:40.07ms
step:2092/2330 train_time:83852ms step_avg:40.08ms
step:2093/2330 train_time:83876ms step_avg:40.07ms
step:2094/2330 train_time:83932ms step_avg:40.08ms
step:2095/2330 train_time:83955ms step_avg:40.07ms
step:2096/2330 train_time:84012ms step_avg:40.08ms
step:2097/2330 train_time:84035ms step_avg:40.07ms
step:2098/2330 train_time:84091ms step_avg:40.08ms
step:2099/2330 train_time:84115ms step_avg:40.07ms
step:2100/2330 train_time:84172ms step_avg:40.08ms
step:2101/2330 train_time:84195ms step_avg:40.07ms
step:2102/2330 train_time:84252ms step_avg:40.08ms
step:2103/2330 train_time:84276ms step_avg:40.07ms
step:2104/2330 train_time:84333ms step_avg:40.08ms
step:2105/2330 train_time:84356ms step_avg:40.07ms
step:2106/2330 train_time:84413ms step_avg:40.08ms
step:2107/2330 train_time:84437ms step_avg:40.07ms
step:2108/2330 train_time:84494ms step_avg:40.08ms
step:2109/2330 train_time:84517ms step_avg:40.07ms
step:2110/2330 train_time:84574ms step_avg:40.08ms
step:2111/2330 train_time:84597ms step_avg:40.07ms
step:2112/2330 train_time:84654ms step_avg:40.08ms
step:2113/2330 train_time:84676ms step_avg:40.07ms
step:2114/2330 train_time:84733ms step_avg:40.08ms
step:2115/2330 train_time:84756ms step_avg:40.07ms
step:2116/2330 train_time:84812ms step_avg:40.08ms
step:2117/2330 train_time:84835ms step_avg:40.07ms
step:2118/2330 train_time:84892ms step_avg:40.08ms
step:2119/2330 train_time:84915ms step_avg:40.07ms
step:2120/2330 train_time:84971ms step_avg:40.08ms
step:2121/2330 train_time:84994ms step_avg:40.07ms
step:2122/2330 train_time:85051ms step_avg:40.08ms
step:2123/2330 train_time:85074ms step_avg:40.07ms
step:2124/2330 train_time:85130ms step_avg:40.08ms
step:2125/2330 train_time:85155ms step_avg:40.07ms
step:2126/2330 train_time:85211ms step_avg:40.08ms
step:2127/2330 train_time:85235ms step_avg:40.07ms
step:2128/2330 train_time:85292ms step_avg:40.08ms
step:2129/2330 train_time:85315ms step_avg:40.07ms
step:2130/2330 train_time:85372ms step_avg:40.08ms
step:2131/2330 train_time:85396ms step_avg:40.07ms
step:2132/2330 train_time:85452ms step_avg:40.08ms
step:2133/2330 train_time:85475ms step_avg:40.07ms
step:2134/2330 train_time:85532ms step_avg:40.08ms
step:2135/2330 train_time:85555ms step_avg:40.07ms
step:2136/2330 train_time:85611ms step_avg:40.08ms
step:2137/2330 train_time:85635ms step_avg:40.07ms
step:2138/2330 train_time:85693ms step_avg:40.08ms
step:2139/2330 train_time:85716ms step_avg:40.07ms
step:2140/2330 train_time:85773ms step_avg:40.08ms
step:2141/2330 train_time:85796ms step_avg:40.07ms
step:2142/2330 train_time:85852ms step_avg:40.08ms
step:2143/2330 train_time:85875ms step_avg:40.07ms
step:2144/2330 train_time:85932ms step_avg:40.08ms
step:2145/2330 train_time:85955ms step_avg:40.07ms
step:2146/2330 train_time:86011ms step_avg:40.08ms
step:2147/2330 train_time:86035ms step_avg:40.07ms
step:2148/2330 train_time:86091ms step_avg:40.08ms
step:2149/2330 train_time:86114ms step_avg:40.07ms
step:2150/2330 train_time:86171ms step_avg:40.08ms
step:2151/2330 train_time:86194ms step_avg:40.07ms
step:2152/2330 train_time:86250ms step_avg:40.08ms
step:2153/2330 train_time:86274ms step_avg:40.07ms
step:2154/2330 train_time:86330ms step_avg:40.08ms
step:2155/2330 train_time:86354ms step_avg:40.07ms
step:2156/2330 train_time:86410ms step_avg:40.08ms
step:2157/2330 train_time:86434ms step_avg:40.07ms
step:2158/2330 train_time:86490ms step_avg:40.08ms
step:2159/2330 train_time:86514ms step_avg:40.07ms
step:2160/2330 train_time:86570ms step_avg:40.08ms
step:2161/2330 train_time:86594ms step_avg:40.07ms
step:2162/2330 train_time:86651ms step_avg:40.08ms
step:2163/2330 train_time:86674ms step_avg:40.07ms
step:2164/2330 train_time:86731ms step_avg:40.08ms
step:2165/2330 train_time:86755ms step_avg:40.07ms
step:2166/2330 train_time:86812ms step_avg:40.08ms
step:2167/2330 train_time:86835ms step_avg:40.07ms
step:2168/2330 train_time:86891ms step_avg:40.08ms
step:2169/2330 train_time:86914ms step_avg:40.07ms
step:2170/2330 train_time:86971ms step_avg:40.08ms
step:2171/2330 train_time:86994ms step_avg:40.07ms
step:2172/2330 train_time:87051ms step_avg:40.08ms
step:2173/2330 train_time:87074ms step_avg:40.07ms
step:2174/2330 train_time:87130ms step_avg:40.08ms
step:2175/2330 train_time:87154ms step_avg:40.07ms
step:2176/2330 train_time:87211ms step_avg:40.08ms
step:2177/2330 train_time:87235ms step_avg:40.07ms
step:2178/2330 train_time:87292ms step_avg:40.08ms
step:2179/2330 train_time:87316ms step_avg:40.07ms
step:2180/2330 train_time:87372ms step_avg:40.08ms
step:2181/2330 train_time:87395ms step_avg:40.07ms
step:2182/2330 train_time:87452ms step_avg:40.08ms
step:2183/2330 train_time:87476ms step_avg:40.07ms
step:2184/2330 train_time:87532ms step_avg:40.08ms
step:2185/2330 train_time:87555ms step_avg:40.07ms
step:2186/2330 train_time:87612ms step_avg:40.08ms
step:2187/2330 train_time:87635ms step_avg:40.07ms
step:2188/2330 train_time:87692ms step_avg:40.08ms
step:2189/2330 train_time:87715ms step_avg:40.07ms
step:2190/2330 train_time:87772ms step_avg:40.08ms
step:2191/2330 train_time:87796ms step_avg:40.07ms
step:2192/2330 train_time:87852ms step_avg:40.08ms
step:2193/2330 train_time:87876ms step_avg:40.07ms
step:2194/2330 train_time:87932ms step_avg:40.08ms
step:2195/2330 train_time:87956ms step_avg:40.07ms
step:2196/2330 train_time:88012ms step_avg:40.08ms
step:2197/2330 train_time:88036ms step_avg:40.07ms
step:2198/2330 train_time:88094ms step_avg:40.08ms
step:2199/2330 train_time:88116ms step_avg:40.07ms
step:2200/2330 train_time:88173ms step_avg:40.08ms
step:2201/2330 train_time:88195ms step_avg:40.07ms
step:2202/2330 train_time:88252ms step_avg:40.08ms
step:2203/2330 train_time:88276ms step_avg:40.07ms
step:2204/2330 train_time:88332ms step_avg:40.08ms
step:2205/2330 train_time:88355ms step_avg:40.07ms
step:2206/2330 train_time:88412ms step_avg:40.08ms
step:2207/2330 train_time:88435ms step_avg:40.07ms
step:2208/2330 train_time:88492ms step_avg:40.08ms
step:2209/2330 train_time:88515ms step_avg:40.07ms
step:2210/2330 train_time:88572ms step_avg:40.08ms
step:2211/2330 train_time:88595ms step_avg:40.07ms
step:2212/2330 train_time:88653ms step_avg:40.08ms
step:2213/2330 train_time:88676ms step_avg:40.07ms
step:2214/2330 train_time:88733ms step_avg:40.08ms
step:2215/2330 train_time:88757ms step_avg:40.07ms
step:2216/2330 train_time:88813ms step_avg:40.08ms
step:2217/2330 train_time:88837ms step_avg:40.07ms
step:2218/2330 train_time:88894ms step_avg:40.08ms
step:2219/2330 train_time:88916ms step_avg:40.07ms
step:2220/2330 train_time:88973ms step_avg:40.08ms
step:2221/2330 train_time:88996ms step_avg:40.07ms
step:2222/2330 train_time:89052ms step_avg:40.08ms
step:2223/2330 train_time:89076ms step_avg:40.07ms
step:2224/2330 train_time:89133ms step_avg:40.08ms
step:2225/2330 train_time:89156ms step_avg:40.07ms
step:2226/2330 train_time:89213ms step_avg:40.08ms
step:2227/2330 train_time:89237ms step_avg:40.07ms
step:2228/2330 train_time:89295ms step_avg:40.08ms
step:2229/2330 train_time:89317ms step_avg:40.07ms
step:2230/2330 train_time:89375ms step_avg:40.08ms
step:2231/2330 train_time:89397ms step_avg:40.07ms
step:2232/2330 train_time:89455ms step_avg:40.08ms
step:2233/2330 train_time:89477ms step_avg:40.07ms
step:2234/2330 train_time:89534ms step_avg:40.08ms
step:2235/2330 train_time:89557ms step_avg:40.07ms
step:2236/2330 train_time:89614ms step_avg:40.08ms
step:2237/2330 train_time:89636ms step_avg:40.07ms
step:2238/2330 train_time:89693ms step_avg:40.08ms
step:2239/2330 train_time:89716ms step_avg:40.07ms
step:2240/2330 train_time:89773ms step_avg:40.08ms
step:2241/2330 train_time:89795ms step_avg:40.07ms
step:2242/2330 train_time:89851ms step_avg:40.08ms
step:2243/2330 train_time:89875ms step_avg:40.07ms
step:2244/2330 train_time:89931ms step_avg:40.08ms
step:2245/2330 train_time:89955ms step_avg:40.07ms
step:2246/2330 train_time:90013ms step_avg:40.08ms
step:2247/2330 train_time:90036ms step_avg:40.07ms
step:2248/2330 train_time:90093ms step_avg:40.08ms
step:2249/2330 train_time:90116ms step_avg:40.07ms
step:2250/2330 train_time:90173ms step_avg:40.08ms
step:2250/2330 val_loss:5.3772 train_time:90270ms step_avg:40.12ms
step:2251/2330 train_time:90283ms step_avg:40.11ms
step:2252/2330 train_time:90295ms step_avg:40.10ms
step:2253/2330 train_time:90306ms step_avg:40.08ms
step:2254/2330 train_time:90333ms step_avg:40.08ms
step:2255/2330 train_time:90355ms step_avg:40.07ms
step:2256/2330 train_time:90411ms step_avg:40.08ms
step:2257/2330 train_time:90432ms step_avg:40.07ms
step:2258/2330 train_time:90488ms step_avg:40.07ms
step:2259/2330 train_time:90510ms step_avg:40.07ms
step:2260/2330 train_time:90567ms step_avg:40.07ms
step:2261/2330 train_time:90594ms step_avg:40.07ms
step:2262/2330 train_time:90655ms step_avg:40.08ms
step:2263/2330 train_time:90681ms step_avg:40.07ms
step:2264/2330 train_time:90737ms step_avg:40.08ms
step:2265/2330 train_time:90760ms step_avg:40.07ms
step:2266/2330 train_time:90817ms step_avg:40.08ms
step:2267/2330 train_time:90840ms step_avg:40.07ms
step:2268/2330 train_time:90897ms step_avg:40.08ms
step:2269/2330 train_time:90920ms step_avg:40.07ms
step:2270/2330 train_time:90977ms step_avg:40.08ms
step:2271/2330 train_time:91000ms step_avg:40.07ms
step:2272/2330 train_time:91056ms step_avg:40.08ms
step:2273/2330 train_time:91079ms step_avg:40.07ms
step:2274/2330 train_time:91136ms step_avg:40.08ms
step:2275/2330 train_time:91159ms step_avg:40.07ms
step:2276/2330 train_time:91220ms step_avg:40.08ms
step:2277/2330 train_time:91243ms step_avg:40.07ms
step:2278/2330 train_time:91301ms step_avg:40.08ms
step:2279/2330 train_time:91324ms step_avg:40.07ms
step:2280/2330 train_time:91381ms step_avg:40.08ms
step:2281/2330 train_time:91404ms step_avg:40.07ms
step:2282/2330 train_time:91461ms step_avg:40.08ms
step:2283/2330 train_time:91485ms step_avg:40.07ms
step:2284/2330 train_time:91543ms step_avg:40.08ms
step:2285/2330 train_time:91567ms step_avg:40.07ms
step:2286/2330 train_time:91626ms step_avg:40.08ms
step:2287/2330 train_time:91650ms step_avg:40.07ms
step:2288/2330 train_time:91707ms step_avg:40.08ms
step:2289/2330 train_time:91731ms step_avg:40.07ms
step:2290/2330 train_time:91787ms step_avg:40.08ms
step:2291/2330 train_time:91812ms step_avg:40.07ms
step:2292/2330 train_time:91869ms step_avg:40.08ms
step:2293/2330 train_time:91893ms step_avg:40.08ms
step:2294/2330 train_time:91949ms step_avg:40.08ms
step:2295/2330 train_time:91972ms step_avg:40.08ms
step:2296/2330 train_time:92028ms step_avg:40.08ms
step:2297/2330 train_time:92051ms step_avg:40.07ms
step:2298/2330 train_time:92108ms step_avg:40.08ms
step:2299/2330 train_time:92132ms step_avg:40.07ms
step:2300/2330 train_time:92189ms step_avg:40.08ms
step:2301/2330 train_time:92213ms step_avg:40.07ms
step:2302/2330 train_time:92269ms step_avg:40.08ms
step:2303/2330 train_time:92292ms step_avg:40.07ms
step:2304/2330 train_time:92349ms step_avg:40.08ms
step:2305/2330 train_time:92373ms step_avg:40.07ms
step:2306/2330 train_time:92430ms step_avg:40.08ms
step:2307/2330 train_time:92453ms step_avg:40.08ms
step:2308/2330 train_time:92510ms step_avg:40.08ms
step:2309/2330 train_time:92534ms step_avg:40.08ms
step:2310/2330 train_time:92592ms step_avg:40.08ms
step:2311/2330 train_time:92615ms step_avg:40.08ms
step:2312/2330 train_time:92672ms step_avg:40.08ms
step:2313/2330 train_time:92694ms step_avg:40.08ms
step:2314/2330 train_time:92752ms step_avg:40.08ms
step:2315/2330 train_time:92774ms step_avg:40.08ms
step:2316/2330 train_time:92832ms step_avg:40.08ms
step:2317/2330 train_time:92855ms step_avg:40.08ms
step:2318/2330 train_time:92912ms step_avg:40.08ms
step:2319/2330 train_time:92935ms step_avg:40.08ms
step:2320/2330 train_time:92992ms step_avg:40.08ms
step:2321/2330 train_time:93014ms step_avg:40.08ms
step:2322/2330 train_time:93070ms step_avg:40.08ms
step:2323/2330 train_time:93094ms step_avg:40.07ms
step:2324/2330 train_time:93150ms step_avg:40.08ms
step:2325/2330 train_time:93173ms step_avg:40.07ms
step:2326/2330 train_time:93230ms step_avg:40.08ms
step:2327/2330 train_time:93253ms step_avg:40.07ms
step:2328/2330 train_time:93310ms step_avg:40.08ms
step:2329/2330 train_time:93333ms step_avg:40.07ms
step:2330/2330 train_time:93390ms step_avg:40.08ms
step:2330/2330 val_loss:5.3677 train_time:93488ms step_avg:40.12ms
peak memory allocated: 29712 MiB reserved: 38888 MiB
