import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = divide_by_norm(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = divide_by_norm(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_gd_lr7e-3"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=7e-3, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 05:21:00 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   34C    P0             113W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   32C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:60ms step_avg:60.38ms
step:2/2330 train_time:141ms step_avg:70.56ms
step:3/2330 train_time:153ms step_avg:50.91ms
step:4/2330 train_time:166ms step_avg:41.39ms
step:5/2330 train_time:176ms step_avg:35.29ms
step:6/2330 train_time:201ms step_avg:33.52ms
step:7/2330 train_time:222ms step_avg:31.76ms
step:8/2330 train_time:277ms step_avg:34.62ms
step:9/2330 train_time:299ms step_avg:33.18ms
step:10/2330 train_time:354ms step_avg:35.37ms
step:11/2330 train_time:375ms step_avg:34.11ms
step:12/2330 train_time:431ms step_avg:35.88ms
step:13/2330 train_time:452ms step_avg:34.78ms
step:14/2330 train_time:507ms step_avg:36.20ms
step:15/2330 train_time:529ms step_avg:35.29ms
step:16/2330 train_time:585ms step_avg:36.57ms
step:17/2330 train_time:608ms step_avg:35.74ms
step:18/2330 train_time:663ms step_avg:36.84ms
step:19/2330 train_time:686ms step_avg:36.08ms
step:20/2330 train_time:741ms step_avg:37.06ms
step:21/2330 train_time:764ms step_avg:36.37ms
step:22/2330 train_time:820ms step_avg:37.27ms
step:23/2330 train_time:842ms step_avg:36.63ms
step:24/2330 train_time:899ms step_avg:37.44ms
step:25/2330 train_time:921ms step_avg:36.83ms
step:26/2330 train_time:980ms step_avg:37.70ms
step:27/2330 train_time:1007ms step_avg:37.29ms
step:28/2330 train_time:1070ms step_avg:38.20ms
step:29/2330 train_time:1094ms step_avg:37.74ms
step:30/2330 train_time:1153ms step_avg:38.44ms
step:31/2330 train_time:1176ms step_avg:37.94ms
step:32/2330 train_time:1234ms step_avg:38.55ms
step:33/2330 train_time:1256ms step_avg:38.07ms
step:34/2330 train_time:1313ms step_avg:38.61ms
step:35/2330 train_time:1335ms step_avg:38.13ms
step:36/2330 train_time:1391ms step_avg:38.64ms
step:37/2330 train_time:1413ms step_avg:38.18ms
step:38/2330 train_time:1468ms step_avg:38.64ms
step:39/2330 train_time:1491ms step_avg:38.22ms
step:40/2330 train_time:1547ms step_avg:38.67ms
step:41/2330 train_time:1570ms step_avg:38.28ms
step:42/2330 train_time:1625ms step_avg:38.70ms
step:43/2330 train_time:1648ms step_avg:38.32ms
step:44/2330 train_time:1704ms step_avg:38.73ms
step:45/2330 train_time:1727ms step_avg:38.37ms
step:46/2330 train_time:1783ms step_avg:38.75ms
step:47/2330 train_time:1806ms step_avg:38.42ms
step:48/2330 train_time:1862ms step_avg:38.79ms
step:49/2330 train_time:1885ms step_avg:38.47ms
step:50/2330 train_time:1943ms step_avg:38.87ms
step:51/2330 train_time:1968ms step_avg:38.59ms
step:52/2330 train_time:2027ms step_avg:38.98ms
step:53/2330 train_time:2052ms step_avg:38.73ms
step:54/2330 train_time:2111ms step_avg:39.08ms
step:55/2330 train_time:2135ms step_avg:38.82ms
step:56/2330 train_time:2192ms step_avg:39.15ms
step:57/2330 train_time:2216ms step_avg:38.88ms
step:58/2330 train_time:2273ms step_avg:39.18ms
step:59/2330 train_time:2295ms step_avg:38.89ms
step:60/2330 train_time:2352ms step_avg:39.19ms
step:61/2330 train_time:2373ms step_avg:38.91ms
step:62/2330 train_time:2430ms step_avg:39.19ms
step:63/2330 train_time:2452ms step_avg:38.92ms
step:64/2330 train_time:2508ms step_avg:39.19ms
step:65/2330 train_time:2531ms step_avg:38.93ms
step:66/2330 train_time:2586ms step_avg:39.18ms
step:67/2330 train_time:2609ms step_avg:38.94ms
step:68/2330 train_time:2666ms step_avg:39.21ms
step:69/2330 train_time:2689ms step_avg:38.98ms
step:70/2330 train_time:2746ms step_avg:39.22ms
step:71/2330 train_time:2769ms step_avg:39.00ms
step:72/2330 train_time:2825ms step_avg:39.24ms
step:73/2330 train_time:2848ms step_avg:39.02ms
step:74/2330 train_time:2905ms step_avg:39.26ms
step:75/2330 train_time:2930ms step_avg:39.06ms
step:76/2330 train_time:2988ms step_avg:39.31ms
step:77/2330 train_time:3012ms step_avg:39.11ms
step:78/2330 train_time:3069ms step_avg:39.35ms
step:79/2330 train_time:3094ms step_avg:39.16ms
step:80/2330 train_time:3151ms step_avg:39.38ms
step:81/2330 train_time:3174ms step_avg:39.19ms
step:82/2330 train_time:3231ms step_avg:39.40ms
step:83/2330 train_time:3254ms step_avg:39.21ms
step:84/2330 train_time:3310ms step_avg:39.41ms
step:85/2330 train_time:3333ms step_avg:39.21ms
step:86/2330 train_time:3389ms step_avg:39.41ms
step:87/2330 train_time:3412ms step_avg:39.22ms
step:88/2330 train_time:3468ms step_avg:39.41ms
step:89/2330 train_time:3491ms step_avg:39.22ms
step:90/2330 train_time:3547ms step_avg:39.41ms
step:91/2330 train_time:3570ms step_avg:39.23ms
step:92/2330 train_time:3627ms step_avg:39.43ms
step:93/2330 train_time:3650ms step_avg:39.25ms
step:94/2330 train_time:3706ms step_avg:39.43ms
step:95/2330 train_time:3729ms step_avg:39.26ms
step:96/2330 train_time:3786ms step_avg:39.44ms
step:97/2330 train_time:3809ms step_avg:39.27ms
step:98/2330 train_time:3866ms step_avg:39.45ms
step:99/2330 train_time:3890ms step_avg:39.29ms
step:100/2330 train_time:3947ms step_avg:39.47ms
step:101/2330 train_time:3970ms step_avg:39.31ms
step:102/2330 train_time:4028ms step_avg:39.49ms
step:103/2330 train_time:4051ms step_avg:39.33ms
step:104/2330 train_time:4109ms step_avg:39.51ms
step:105/2330 train_time:4133ms step_avg:39.36ms
step:106/2330 train_time:4189ms step_avg:39.52ms
step:107/2330 train_time:4212ms step_avg:39.37ms
step:108/2330 train_time:4269ms step_avg:39.52ms
step:109/2330 train_time:4292ms step_avg:39.38ms
step:110/2330 train_time:4348ms step_avg:39.53ms
step:111/2330 train_time:4372ms step_avg:39.39ms
step:112/2330 train_time:4427ms step_avg:39.53ms
step:113/2330 train_time:4451ms step_avg:39.39ms
step:114/2330 train_time:4507ms step_avg:39.54ms
step:115/2330 train_time:4530ms step_avg:39.40ms
step:116/2330 train_time:4586ms step_avg:39.54ms
step:117/2330 train_time:4609ms step_avg:39.40ms
step:118/2330 train_time:4666ms step_avg:39.54ms
step:119/2330 train_time:4689ms step_avg:39.40ms
step:120/2330 train_time:4746ms step_avg:39.55ms
step:121/2330 train_time:4769ms step_avg:39.41ms
step:122/2330 train_time:4826ms step_avg:39.55ms
step:123/2330 train_time:4850ms step_avg:39.43ms
step:124/2330 train_time:4906ms step_avg:39.56ms
step:125/2330 train_time:4930ms step_avg:39.44ms
step:126/2330 train_time:4987ms step_avg:39.58ms
step:127/2330 train_time:5010ms step_avg:39.45ms
step:128/2330 train_time:5067ms step_avg:39.59ms
step:129/2330 train_time:5092ms step_avg:39.47ms
step:130/2330 train_time:5149ms step_avg:39.61ms
step:131/2330 train_time:5173ms step_avg:39.49ms
step:132/2330 train_time:5230ms step_avg:39.62ms
step:133/2330 train_time:5254ms step_avg:39.50ms
step:134/2330 train_time:5310ms step_avg:39.63ms
step:135/2330 train_time:5333ms step_avg:39.50ms
step:136/2330 train_time:5389ms step_avg:39.62ms
step:137/2330 train_time:5412ms step_avg:39.50ms
step:138/2330 train_time:5468ms step_avg:39.62ms
step:139/2330 train_time:5492ms step_avg:39.51ms
step:140/2330 train_time:5548ms step_avg:39.63ms
step:141/2330 train_time:5572ms step_avg:39.52ms
step:142/2330 train_time:5629ms step_avg:39.64ms
step:143/2330 train_time:5653ms step_avg:39.53ms
step:144/2330 train_time:5708ms step_avg:39.64ms
step:145/2330 train_time:5732ms step_avg:39.53ms
step:146/2330 train_time:5789ms step_avg:39.65ms
step:147/2330 train_time:5813ms step_avg:39.54ms
step:148/2330 train_time:5870ms step_avg:39.66ms
step:149/2330 train_time:5894ms step_avg:39.56ms
step:150/2330 train_time:5950ms step_avg:39.67ms
step:151/2330 train_time:5973ms step_avg:39.56ms
step:152/2330 train_time:6030ms step_avg:39.67ms
step:153/2330 train_time:6053ms step_avg:39.56ms
step:154/2330 train_time:6109ms step_avg:39.67ms
step:155/2330 train_time:6133ms step_avg:39.57ms
step:156/2330 train_time:6190ms step_avg:39.68ms
step:157/2330 train_time:6213ms step_avg:39.57ms
step:158/2330 train_time:6269ms step_avg:39.68ms
step:159/2330 train_time:6293ms step_avg:39.58ms
step:160/2330 train_time:6349ms step_avg:39.68ms
step:161/2330 train_time:6372ms step_avg:39.58ms
step:162/2330 train_time:6429ms step_avg:39.68ms
step:163/2330 train_time:6452ms step_avg:39.58ms
step:164/2330 train_time:6509ms step_avg:39.69ms
step:165/2330 train_time:6533ms step_avg:39.59ms
step:166/2330 train_time:6589ms step_avg:39.69ms
step:167/2330 train_time:6613ms step_avg:39.60ms
step:168/2330 train_time:6670ms step_avg:39.70ms
step:169/2330 train_time:6693ms step_avg:39.60ms
step:170/2330 train_time:6750ms step_avg:39.71ms
step:171/2330 train_time:6773ms step_avg:39.61ms
step:172/2330 train_time:6830ms step_avg:39.71ms
step:173/2330 train_time:6853ms step_avg:39.61ms
step:174/2330 train_time:6909ms step_avg:39.71ms
step:175/2330 train_time:6933ms step_avg:39.62ms
step:176/2330 train_time:6990ms step_avg:39.71ms
step:177/2330 train_time:7013ms step_avg:39.62ms
step:178/2330 train_time:7069ms step_avg:39.72ms
step:179/2330 train_time:7093ms step_avg:39.63ms
step:180/2330 train_time:7150ms step_avg:39.72ms
step:181/2330 train_time:7173ms step_avg:39.63ms
step:182/2330 train_time:7230ms step_avg:39.72ms
step:183/2330 train_time:7252ms step_avg:39.63ms
step:184/2330 train_time:7309ms step_avg:39.72ms
step:185/2330 train_time:7332ms step_avg:39.63ms
step:186/2330 train_time:7389ms step_avg:39.73ms
step:187/2330 train_time:7412ms step_avg:39.64ms
step:188/2330 train_time:7469ms step_avg:39.73ms
step:189/2330 train_time:7493ms step_avg:39.64ms
step:190/2330 train_time:7550ms step_avg:39.74ms
step:191/2330 train_time:7573ms step_avg:39.65ms
step:192/2330 train_time:7629ms step_avg:39.74ms
step:193/2330 train_time:7652ms step_avg:39.65ms
step:194/2330 train_time:7709ms step_avg:39.73ms
step:195/2330 train_time:7731ms step_avg:39.65ms
step:196/2330 train_time:7788ms step_avg:39.73ms
step:197/2330 train_time:7811ms step_avg:39.65ms
step:198/2330 train_time:7868ms step_avg:39.74ms
step:199/2330 train_time:7891ms step_avg:39.65ms
step:200/2330 train_time:7948ms step_avg:39.74ms
step:201/2330 train_time:7971ms step_avg:39.66ms
step:202/2330 train_time:8027ms step_avg:39.74ms
step:203/2330 train_time:8050ms step_avg:39.66ms
step:204/2330 train_time:8108ms step_avg:39.74ms
step:205/2330 train_time:8131ms step_avg:39.67ms
step:206/2330 train_time:8188ms step_avg:39.75ms
step:207/2330 train_time:8211ms step_avg:39.67ms
step:208/2330 train_time:8268ms step_avg:39.75ms
step:209/2330 train_time:8291ms step_avg:39.67ms
step:210/2330 train_time:8347ms step_avg:39.75ms
step:211/2330 train_time:8371ms step_avg:39.67ms
step:212/2330 train_time:8427ms step_avg:39.75ms
step:213/2330 train_time:8450ms step_avg:39.67ms
step:214/2330 train_time:8507ms step_avg:39.75ms
step:215/2330 train_time:8530ms step_avg:39.68ms
step:216/2330 train_time:8587ms step_avg:39.75ms
step:217/2330 train_time:8610ms step_avg:39.68ms
step:218/2330 train_time:8667ms step_avg:39.76ms
step:219/2330 train_time:8690ms step_avg:39.68ms
step:220/2330 train_time:8747ms step_avg:39.76ms
step:221/2330 train_time:8770ms step_avg:39.68ms
step:222/2330 train_time:8826ms step_avg:39.76ms
step:223/2330 train_time:8850ms step_avg:39.69ms
step:224/2330 train_time:8907ms step_avg:39.76ms
step:225/2330 train_time:8930ms step_avg:39.69ms
step:226/2330 train_time:8986ms step_avg:39.76ms
step:227/2330 train_time:9010ms step_avg:39.69ms
step:228/2330 train_time:9067ms step_avg:39.77ms
step:229/2330 train_time:9090ms step_avg:39.70ms
step:230/2330 train_time:9147ms step_avg:39.77ms
step:231/2330 train_time:9170ms step_avg:39.70ms
step:232/2330 train_time:9227ms step_avg:39.77ms
step:233/2330 train_time:9250ms step_avg:39.70ms
step:234/2330 train_time:9307ms step_avg:39.77ms
step:235/2330 train_time:9330ms step_avg:39.70ms
step:236/2330 train_time:9387ms step_avg:39.78ms
step:237/2330 train_time:9410ms step_avg:39.71ms
step:238/2330 train_time:9467ms step_avg:39.78ms
step:239/2330 train_time:9491ms step_avg:39.71ms
step:240/2330 train_time:9547ms step_avg:39.78ms
step:241/2330 train_time:9570ms step_avg:39.71ms
step:242/2330 train_time:9627ms step_avg:39.78ms
step:243/2330 train_time:9651ms step_avg:39.72ms
step:244/2330 train_time:9708ms step_avg:39.79ms
step:245/2330 train_time:9732ms step_avg:39.72ms
step:246/2330 train_time:9788ms step_avg:39.79ms
step:247/2330 train_time:9812ms step_avg:39.73ms
step:248/2330 train_time:9869ms step_avg:39.79ms
step:249/2330 train_time:9893ms step_avg:39.73ms
step:250/2330 train_time:9950ms step_avg:39.80ms
step:250/2330 val_loss:5.6181 train_time:10049ms step_avg:40.19ms
step:251/2330 train_time:10061ms step_avg:40.08ms
step:252/2330 train_time:10073ms step_avg:39.97ms
step:253/2330 train_time:10083ms step_avg:39.86ms
step:254/2330 train_time:10111ms step_avg:39.81ms
step:255/2330 train_time:10134ms step_avg:39.74ms
step:256/2330 train_time:10190ms step_avg:39.81ms
step:257/2330 train_time:10213ms step_avg:39.74ms
step:258/2330 train_time:10270ms step_avg:39.81ms
step:259/2330 train_time:10292ms step_avg:39.74ms
step:260/2330 train_time:10350ms step_avg:39.81ms
step:261/2330 train_time:10379ms step_avg:39.77ms
step:262/2330 train_time:10441ms step_avg:39.85ms
step:263/2330 train_time:10466ms step_avg:39.80ms
step:264/2330 train_time:10524ms step_avg:39.86ms
step:265/2330 train_time:10547ms step_avg:39.80ms
step:266/2330 train_time:10604ms step_avg:39.86ms
step:267/2330 train_time:10626ms step_avg:39.80ms
step:268/2330 train_time:10683ms step_avg:39.86ms
step:269/2330 train_time:10706ms step_avg:39.80ms
step:270/2330 train_time:10762ms step_avg:39.86ms
step:271/2330 train_time:10784ms step_avg:39.79ms
step:272/2330 train_time:10840ms step_avg:39.85ms
step:273/2330 train_time:10863ms step_avg:39.79ms
step:274/2330 train_time:10919ms step_avg:39.85ms
step:275/2330 train_time:10942ms step_avg:39.79ms
step:276/2330 train_time:11001ms step_avg:39.86ms
step:277/2330 train_time:11024ms step_avg:39.80ms
step:278/2330 train_time:11081ms step_avg:39.86ms
step:279/2330 train_time:11104ms step_avg:39.80ms
step:280/2330 train_time:11160ms step_avg:39.86ms
step:281/2330 train_time:11183ms step_avg:39.80ms
step:282/2330 train_time:11239ms step_avg:39.85ms
step:283/2330 train_time:11262ms step_avg:39.79ms
step:284/2330 train_time:11320ms step_avg:39.86ms
step:285/2330 train_time:11345ms step_avg:39.81ms
step:286/2330 train_time:11402ms step_avg:39.87ms
step:287/2330 train_time:11426ms step_avg:39.81ms
step:288/2330 train_time:11485ms step_avg:39.88ms
step:289/2330 train_time:11507ms step_avg:39.82ms
step:290/2330 train_time:11564ms step_avg:39.88ms
step:291/2330 train_time:11586ms step_avg:39.82ms
step:292/2330 train_time:11643ms step_avg:39.87ms
step:293/2330 train_time:11666ms step_avg:39.81ms
step:294/2330 train_time:11722ms step_avg:39.87ms
step:295/2330 train_time:11744ms step_avg:39.81ms
step:296/2330 train_time:11801ms step_avg:39.87ms
step:297/2330 train_time:11824ms step_avg:39.81ms
step:298/2330 train_time:11881ms step_avg:39.87ms
step:299/2330 train_time:11904ms step_avg:39.81ms
step:300/2330 train_time:11961ms step_avg:39.87ms
step:301/2330 train_time:11984ms step_avg:39.81ms
step:302/2330 train_time:12040ms step_avg:39.87ms
step:303/2330 train_time:12063ms step_avg:39.81ms
step:304/2330 train_time:12120ms step_avg:39.87ms
step:305/2330 train_time:12143ms step_avg:39.81ms
step:306/2330 train_time:12200ms step_avg:39.87ms
step:307/2330 train_time:12223ms step_avg:39.82ms
step:308/2330 train_time:12280ms step_avg:39.87ms
step:309/2330 train_time:12303ms step_avg:39.82ms
step:310/2330 train_time:12360ms step_avg:39.87ms
step:311/2330 train_time:12384ms step_avg:39.82ms
step:312/2330 train_time:12440ms step_avg:39.87ms
step:313/2330 train_time:12464ms step_avg:39.82ms
step:314/2330 train_time:12520ms step_avg:39.87ms
step:315/2330 train_time:12544ms step_avg:39.82ms
step:316/2330 train_time:12601ms step_avg:39.88ms
step:317/2330 train_time:12625ms step_avg:39.83ms
step:318/2330 train_time:12682ms step_avg:39.88ms
step:319/2330 train_time:12705ms step_avg:39.83ms
step:320/2330 train_time:12762ms step_avg:39.88ms
step:321/2330 train_time:12785ms step_avg:39.83ms
step:322/2330 train_time:12841ms step_avg:39.88ms
step:323/2330 train_time:12864ms step_avg:39.83ms
step:324/2330 train_time:12920ms step_avg:39.88ms
step:325/2330 train_time:12943ms step_avg:39.82ms
step:326/2330 train_time:13000ms step_avg:39.88ms
step:327/2330 train_time:13023ms step_avg:39.83ms
step:328/2330 train_time:13080ms step_avg:39.88ms
step:329/2330 train_time:13103ms step_avg:39.83ms
step:330/2330 train_time:13159ms step_avg:39.88ms
step:331/2330 train_time:13183ms step_avg:39.83ms
step:332/2330 train_time:13239ms step_avg:39.88ms
step:333/2330 train_time:13263ms step_avg:39.83ms
step:334/2330 train_time:13320ms step_avg:39.88ms
step:335/2330 train_time:13344ms step_avg:39.83ms
step:336/2330 train_time:13402ms step_avg:39.89ms
step:337/2330 train_time:13425ms step_avg:39.84ms
step:338/2330 train_time:13482ms step_avg:39.89ms
step:339/2330 train_time:13506ms step_avg:39.84ms
step:340/2330 train_time:13563ms step_avg:39.89ms
step:341/2330 train_time:13586ms step_avg:39.84ms
step:342/2330 train_time:13643ms step_avg:39.89ms
step:343/2330 train_time:13665ms step_avg:39.84ms
step:344/2330 train_time:13723ms step_avg:39.89ms
step:345/2330 train_time:13745ms step_avg:39.84ms
step:346/2330 train_time:13803ms step_avg:39.89ms
step:347/2330 train_time:13825ms step_avg:39.84ms
step:348/2330 train_time:13883ms step_avg:39.89ms
step:349/2330 train_time:13905ms step_avg:39.84ms
step:350/2330 train_time:13963ms step_avg:39.90ms
step:351/2330 train_time:13985ms step_avg:39.84ms
step:352/2330 train_time:14042ms step_avg:39.89ms
step:353/2330 train_time:14065ms step_avg:39.84ms
step:354/2330 train_time:14123ms step_avg:39.90ms
step:355/2330 train_time:14146ms step_avg:39.85ms
step:356/2330 train_time:14204ms step_avg:39.90ms
step:357/2330 train_time:14226ms step_avg:39.85ms
step:358/2330 train_time:14283ms step_avg:39.90ms
step:359/2330 train_time:14306ms step_avg:39.85ms
step:360/2330 train_time:14363ms step_avg:39.90ms
step:361/2330 train_time:14386ms step_avg:39.85ms
step:362/2330 train_time:14443ms step_avg:39.90ms
step:363/2330 train_time:14466ms step_avg:39.85ms
step:364/2330 train_time:14523ms step_avg:39.90ms
step:365/2330 train_time:14546ms step_avg:39.85ms
step:366/2330 train_time:14604ms step_avg:39.90ms
step:367/2330 train_time:14627ms step_avg:39.85ms
step:368/2330 train_time:14684ms step_avg:39.90ms
step:369/2330 train_time:14707ms step_avg:39.86ms
step:370/2330 train_time:14764ms step_avg:39.90ms
step:371/2330 train_time:14786ms step_avg:39.85ms
step:372/2330 train_time:14843ms step_avg:39.90ms
step:373/2330 train_time:14866ms step_avg:39.85ms
step:374/2330 train_time:14923ms step_avg:39.90ms
step:375/2330 train_time:14946ms step_avg:39.85ms
step:376/2330 train_time:15003ms step_avg:39.90ms
step:377/2330 train_time:15025ms step_avg:39.85ms
step:378/2330 train_time:15083ms step_avg:39.90ms
step:379/2330 train_time:15105ms step_avg:39.85ms
step:380/2330 train_time:15162ms step_avg:39.90ms
step:381/2330 train_time:15185ms step_avg:39.86ms
step:382/2330 train_time:15242ms step_avg:39.90ms
step:383/2330 train_time:15265ms step_avg:39.86ms
step:384/2330 train_time:15322ms step_avg:39.90ms
step:385/2330 train_time:15345ms step_avg:39.86ms
step:386/2330 train_time:15403ms step_avg:39.90ms
step:387/2330 train_time:15426ms step_avg:39.86ms
step:388/2330 train_time:15484ms step_avg:39.91ms
step:389/2330 train_time:15506ms step_avg:39.86ms
step:390/2330 train_time:15564ms step_avg:39.91ms
step:391/2330 train_time:15586ms step_avg:39.86ms
step:392/2330 train_time:15643ms step_avg:39.91ms
step:393/2330 train_time:15667ms step_avg:39.86ms
step:394/2330 train_time:15724ms step_avg:39.91ms
step:395/2330 train_time:15747ms step_avg:39.86ms
step:396/2330 train_time:15804ms step_avg:39.91ms
step:397/2330 train_time:15827ms step_avg:39.87ms
step:398/2330 train_time:15884ms step_avg:39.91ms
step:399/2330 train_time:15906ms step_avg:39.86ms
step:400/2330 train_time:15964ms step_avg:39.91ms
step:401/2330 train_time:15986ms step_avg:39.87ms
step:402/2330 train_time:16043ms step_avg:39.91ms
step:403/2330 train_time:16066ms step_avg:39.87ms
step:404/2330 train_time:16124ms step_avg:39.91ms
step:405/2330 train_time:16146ms step_avg:39.87ms
step:406/2330 train_time:16204ms step_avg:39.91ms
step:407/2330 train_time:16226ms step_avg:39.87ms
step:408/2330 train_time:16283ms step_avg:39.91ms
step:409/2330 train_time:16306ms step_avg:39.87ms
step:410/2330 train_time:16364ms step_avg:39.91ms
step:411/2330 train_time:16386ms step_avg:39.87ms
step:412/2330 train_time:16443ms step_avg:39.91ms
step:413/2330 train_time:16466ms step_avg:39.87ms
step:414/2330 train_time:16524ms step_avg:39.91ms
step:415/2330 train_time:16547ms step_avg:39.87ms
step:416/2330 train_time:16604ms step_avg:39.91ms
step:417/2330 train_time:16627ms step_avg:39.87ms
step:418/2330 train_time:16685ms step_avg:39.92ms
step:419/2330 train_time:16707ms step_avg:39.87ms
step:420/2330 train_time:16764ms step_avg:39.92ms
step:421/2330 train_time:16787ms step_avg:39.87ms
step:422/2330 train_time:16844ms step_avg:39.92ms
step:423/2330 train_time:16867ms step_avg:39.87ms
step:424/2330 train_time:16925ms step_avg:39.92ms
step:425/2330 train_time:16947ms step_avg:39.87ms
step:426/2330 train_time:17004ms step_avg:39.92ms
step:427/2330 train_time:17026ms step_avg:39.87ms
step:428/2330 train_time:17083ms step_avg:39.91ms
step:429/2330 train_time:17106ms step_avg:39.87ms
step:430/2330 train_time:17164ms step_avg:39.92ms
step:431/2330 train_time:17187ms step_avg:39.88ms
step:432/2330 train_time:17244ms step_avg:39.92ms
step:433/2330 train_time:17267ms step_avg:39.88ms
step:434/2330 train_time:17324ms step_avg:39.92ms
step:435/2330 train_time:17347ms step_avg:39.88ms
step:436/2330 train_time:17404ms step_avg:39.92ms
step:437/2330 train_time:17427ms step_avg:39.88ms
step:438/2330 train_time:17484ms step_avg:39.92ms
step:439/2330 train_time:17506ms step_avg:39.88ms
step:440/2330 train_time:17564ms step_avg:39.92ms
step:441/2330 train_time:17586ms step_avg:39.88ms
step:442/2330 train_time:17643ms step_avg:39.92ms
step:443/2330 train_time:17665ms step_avg:39.88ms
step:444/2330 train_time:17724ms step_avg:39.92ms
step:445/2330 train_time:17746ms step_avg:39.88ms
step:446/2330 train_time:17803ms step_avg:39.92ms
step:447/2330 train_time:17825ms step_avg:39.88ms
step:448/2330 train_time:17883ms step_avg:39.92ms
step:449/2330 train_time:17906ms step_avg:39.88ms
step:450/2330 train_time:17963ms step_avg:39.92ms
step:451/2330 train_time:17986ms step_avg:39.88ms
step:452/2330 train_time:18042ms step_avg:39.92ms
step:453/2330 train_time:18065ms step_avg:39.88ms
step:454/2330 train_time:18122ms step_avg:39.92ms
step:455/2330 train_time:18145ms step_avg:39.88ms
step:456/2330 train_time:18202ms step_avg:39.92ms
step:457/2330 train_time:18225ms step_avg:39.88ms
step:458/2330 train_time:18282ms step_avg:39.92ms
step:459/2330 train_time:18305ms step_avg:39.88ms
step:460/2330 train_time:18362ms step_avg:39.92ms
step:461/2330 train_time:18385ms step_avg:39.88ms
step:462/2330 train_time:18442ms step_avg:39.92ms
step:463/2330 train_time:18464ms step_avg:39.88ms
step:464/2330 train_time:18521ms step_avg:39.92ms
step:465/2330 train_time:18544ms step_avg:39.88ms
step:466/2330 train_time:18601ms step_avg:39.92ms
step:467/2330 train_time:18624ms step_avg:39.88ms
step:468/2330 train_time:18680ms step_avg:39.92ms
step:469/2330 train_time:18704ms step_avg:39.88ms
step:470/2330 train_time:18761ms step_avg:39.92ms
step:471/2330 train_time:18784ms step_avg:39.88ms
step:472/2330 train_time:18840ms step_avg:39.92ms
step:473/2330 train_time:18864ms step_avg:39.88ms
step:474/2330 train_time:18920ms step_avg:39.92ms
step:475/2330 train_time:18943ms step_avg:39.88ms
step:476/2330 train_time:19000ms step_avg:39.92ms
step:477/2330 train_time:19024ms step_avg:39.88ms
step:478/2330 train_time:19080ms step_avg:39.92ms
step:479/2330 train_time:19104ms step_avg:39.88ms
step:480/2330 train_time:19160ms step_avg:39.92ms
step:481/2330 train_time:19184ms step_avg:39.88ms
step:482/2330 train_time:19240ms step_avg:39.92ms
step:483/2330 train_time:19264ms step_avg:39.88ms
step:484/2330 train_time:19321ms step_avg:39.92ms
step:485/2330 train_time:19344ms step_avg:39.89ms
step:486/2330 train_time:19402ms step_avg:39.92ms
step:487/2330 train_time:19425ms step_avg:39.89ms
step:488/2330 train_time:19482ms step_avg:39.92ms
step:489/2330 train_time:19504ms step_avg:39.89ms
step:490/2330 train_time:19562ms step_avg:39.92ms
step:491/2330 train_time:19585ms step_avg:39.89ms
step:492/2330 train_time:19641ms step_avg:39.92ms
step:493/2330 train_time:19664ms step_avg:39.89ms
step:494/2330 train_time:19722ms step_avg:39.92ms
step:495/2330 train_time:19744ms step_avg:39.89ms
step:496/2330 train_time:19802ms step_avg:39.92ms
step:497/2330 train_time:19824ms step_avg:39.89ms
step:498/2330 train_time:19882ms step_avg:39.92ms
step:499/2330 train_time:19904ms step_avg:39.89ms
step:500/2330 train_time:19961ms step_avg:39.92ms
step:500/2330 val_loss:5.4514 train_time:20059ms step_avg:40.12ms
step:501/2330 train_time:20070ms step_avg:40.06ms
step:502/2330 train_time:20082ms step_avg:40.00ms
step:503/2330 train_time:20092ms step_avg:39.94ms
step:504/2330 train_time:20122ms step_avg:39.92ms
step:505/2330 train_time:20144ms step_avg:39.89ms
step:506/2330 train_time:20201ms step_avg:39.92ms
step:507/2330 train_time:20224ms step_avg:39.89ms
step:508/2330 train_time:20281ms step_avg:39.92ms
step:509/2330 train_time:20304ms step_avg:39.89ms
step:510/2330 train_time:20362ms step_avg:39.93ms
step:511/2330 train_time:20390ms step_avg:39.90ms
step:512/2330 train_time:20450ms step_avg:39.94ms
step:513/2330 train_time:20474ms step_avg:39.91ms
step:514/2330 train_time:20531ms step_avg:39.94ms
step:515/2330 train_time:20553ms step_avg:39.91ms
step:516/2330 train_time:20610ms step_avg:39.94ms
step:517/2330 train_time:20633ms step_avg:39.91ms
step:518/2330 train_time:20690ms step_avg:39.94ms
step:519/2330 train_time:20712ms step_avg:39.91ms
step:520/2330 train_time:20769ms step_avg:39.94ms
step:521/2330 train_time:20791ms step_avg:39.91ms
step:522/2330 train_time:20848ms step_avg:39.94ms
step:523/2330 train_time:20870ms step_avg:39.90ms
step:524/2330 train_time:20927ms step_avg:39.94ms
step:525/2330 train_time:20950ms step_avg:39.90ms
step:526/2330 train_time:21009ms step_avg:39.94ms
step:527/2330 train_time:21033ms step_avg:39.91ms
step:528/2330 train_time:21092ms step_avg:39.95ms
step:529/2330 train_time:21115ms step_avg:39.91ms
step:530/2330 train_time:21173ms step_avg:39.95ms
step:531/2330 train_time:21195ms step_avg:39.91ms
step:532/2330 train_time:21252ms step_avg:39.95ms
step:533/2330 train_time:21274ms step_avg:39.91ms
step:534/2330 train_time:21332ms step_avg:39.95ms
step:535/2330 train_time:21355ms step_avg:39.92ms
step:536/2330 train_time:21413ms step_avg:39.95ms
step:537/2330 train_time:21437ms step_avg:39.92ms
step:538/2330 train_time:21494ms step_avg:39.95ms
step:539/2330 train_time:21518ms step_avg:39.92ms
step:540/2330 train_time:21576ms step_avg:39.96ms
step:541/2330 train_time:21599ms step_avg:39.92ms
step:542/2330 train_time:21656ms step_avg:39.96ms
step:543/2330 train_time:21679ms step_avg:39.92ms
step:544/2330 train_time:21736ms step_avg:39.96ms
step:545/2330 train_time:21759ms step_avg:39.92ms
step:546/2330 train_time:21816ms step_avg:39.96ms
step:547/2330 train_time:21839ms step_avg:39.93ms
step:548/2330 train_time:21897ms step_avg:39.96ms
step:549/2330 train_time:21920ms step_avg:39.93ms
step:550/2330 train_time:21977ms step_avg:39.96ms
step:551/2330 train_time:22000ms step_avg:39.93ms
step:552/2330 train_time:22058ms step_avg:39.96ms
step:553/2330 train_time:22081ms step_avg:39.93ms
step:554/2330 train_time:22139ms step_avg:39.96ms
step:555/2330 train_time:22162ms step_avg:39.93ms
step:556/2330 train_time:22219ms step_avg:39.96ms
step:557/2330 train_time:22243ms step_avg:39.93ms
step:558/2330 train_time:22301ms step_avg:39.97ms
step:559/2330 train_time:22325ms step_avg:39.94ms
step:560/2330 train_time:22382ms step_avg:39.97ms
step:561/2330 train_time:22406ms step_avg:39.94ms
step:562/2330 train_time:22463ms step_avg:39.97ms
step:563/2330 train_time:22488ms step_avg:39.94ms
step:564/2330 train_time:22546ms step_avg:39.97ms
step:565/2330 train_time:22569ms step_avg:39.95ms
step:566/2330 train_time:22626ms step_avg:39.97ms
step:567/2330 train_time:22650ms step_avg:39.95ms
step:568/2330 train_time:22706ms step_avg:39.98ms
step:569/2330 train_time:22730ms step_avg:39.95ms
step:570/2330 train_time:22785ms step_avg:39.97ms
step:571/2330 train_time:22808ms step_avg:39.94ms
step:572/2330 train_time:22865ms step_avg:39.97ms
step:573/2330 train_time:22888ms step_avg:39.94ms
step:574/2330 train_time:22945ms step_avg:39.97ms
step:575/2330 train_time:22969ms step_avg:39.95ms
step:576/2330 train_time:23026ms step_avg:39.97ms
step:577/2330 train_time:23050ms step_avg:39.95ms
step:578/2330 train_time:23106ms step_avg:39.98ms
step:579/2330 train_time:23130ms step_avg:39.95ms
step:580/2330 train_time:23188ms step_avg:39.98ms
step:581/2330 train_time:23212ms step_avg:39.95ms
step:582/2330 train_time:23269ms step_avg:39.98ms
step:583/2330 train_time:23292ms step_avg:39.95ms
step:584/2330 train_time:23350ms step_avg:39.98ms
step:585/2330 train_time:23372ms step_avg:39.95ms
step:586/2330 train_time:23430ms step_avg:39.98ms
step:587/2330 train_time:23453ms step_avg:39.95ms
step:588/2330 train_time:23511ms step_avg:39.99ms
step:589/2330 train_time:23534ms step_avg:39.96ms
step:590/2330 train_time:23592ms step_avg:39.99ms
step:591/2330 train_time:23614ms step_avg:39.96ms
step:592/2330 train_time:23671ms step_avg:39.98ms
step:593/2330 train_time:23693ms step_avg:39.96ms
step:594/2330 train_time:23750ms step_avg:39.98ms
step:595/2330 train_time:23773ms step_avg:39.95ms
step:596/2330 train_time:23830ms step_avg:39.98ms
step:597/2330 train_time:23853ms step_avg:39.95ms
step:598/2330 train_time:23910ms step_avg:39.98ms
step:599/2330 train_time:23933ms step_avg:39.96ms
step:600/2330 train_time:23991ms step_avg:39.98ms
step:601/2330 train_time:24013ms step_avg:39.96ms
step:602/2330 train_time:24071ms step_avg:39.99ms
step:603/2330 train_time:24094ms step_avg:39.96ms
step:604/2330 train_time:24151ms step_avg:39.98ms
step:605/2330 train_time:24173ms step_avg:39.96ms
step:606/2330 train_time:24231ms step_avg:39.99ms
step:607/2330 train_time:24254ms step_avg:39.96ms
step:608/2330 train_time:24311ms step_avg:39.99ms
step:609/2330 train_time:24334ms step_avg:39.96ms
step:610/2330 train_time:24392ms step_avg:39.99ms
step:611/2330 train_time:24414ms step_avg:39.96ms
step:612/2330 train_time:24472ms step_avg:39.99ms
step:613/2330 train_time:24494ms step_avg:39.96ms
step:614/2330 train_time:24552ms step_avg:39.99ms
step:615/2330 train_time:24575ms step_avg:39.96ms
step:616/2330 train_time:24632ms step_avg:39.99ms
step:617/2330 train_time:24655ms step_avg:39.96ms
step:618/2330 train_time:24712ms step_avg:39.99ms
step:619/2330 train_time:24734ms step_avg:39.96ms
step:620/2330 train_time:24791ms step_avg:39.99ms
step:621/2330 train_time:24814ms step_avg:39.96ms
step:622/2330 train_time:24871ms step_avg:39.99ms
step:623/2330 train_time:24894ms step_avg:39.96ms
step:624/2330 train_time:24951ms step_avg:39.99ms
step:625/2330 train_time:24973ms step_avg:39.96ms
step:626/2330 train_time:25031ms step_avg:39.99ms
step:627/2330 train_time:25053ms step_avg:39.96ms
step:628/2330 train_time:25110ms step_avg:39.98ms
step:629/2330 train_time:25134ms step_avg:39.96ms
step:630/2330 train_time:25191ms step_avg:39.99ms
step:631/2330 train_time:25213ms step_avg:39.96ms
step:632/2330 train_time:25271ms step_avg:39.99ms
step:633/2330 train_time:25293ms step_avg:39.96ms
step:634/2330 train_time:25351ms step_avg:39.99ms
step:635/2330 train_time:25374ms step_avg:39.96ms
step:636/2330 train_time:25432ms step_avg:39.99ms
step:637/2330 train_time:25454ms step_avg:39.96ms
step:638/2330 train_time:25511ms step_avg:39.99ms
step:639/2330 train_time:25534ms step_avg:39.96ms
step:640/2330 train_time:25591ms step_avg:39.99ms
step:641/2330 train_time:25613ms step_avg:39.96ms
step:642/2330 train_time:25671ms step_avg:39.99ms
step:643/2330 train_time:25693ms step_avg:39.96ms
step:644/2330 train_time:25751ms step_avg:39.99ms
step:645/2330 train_time:25773ms step_avg:39.96ms
step:646/2330 train_time:25830ms step_avg:39.99ms
step:647/2330 train_time:25853ms step_avg:39.96ms
step:648/2330 train_time:25910ms step_avg:39.98ms
step:649/2330 train_time:25933ms step_avg:39.96ms
step:650/2330 train_time:25991ms step_avg:39.99ms
step:651/2330 train_time:26013ms step_avg:39.96ms
step:652/2330 train_time:26071ms step_avg:39.99ms
step:653/2330 train_time:26093ms step_avg:39.96ms
step:654/2330 train_time:26151ms step_avg:39.99ms
step:655/2330 train_time:26173ms step_avg:39.96ms
step:656/2330 train_time:26231ms step_avg:39.99ms
step:657/2330 train_time:26253ms step_avg:39.96ms
step:658/2330 train_time:26311ms step_avg:39.99ms
step:659/2330 train_time:26333ms step_avg:39.96ms
step:660/2330 train_time:26391ms step_avg:39.99ms
step:661/2330 train_time:26413ms step_avg:39.96ms
step:662/2330 train_time:26471ms step_avg:39.99ms
step:663/2330 train_time:26494ms step_avg:39.96ms
step:664/2330 train_time:26551ms step_avg:39.99ms
step:665/2330 train_time:26574ms step_avg:39.96ms
step:666/2330 train_time:26632ms step_avg:39.99ms
step:667/2330 train_time:26654ms step_avg:39.96ms
step:668/2330 train_time:26711ms step_avg:39.99ms
step:669/2330 train_time:26733ms step_avg:39.96ms
step:670/2330 train_time:26792ms step_avg:39.99ms
step:671/2330 train_time:26814ms step_avg:39.96ms
step:672/2330 train_time:26871ms step_avg:39.99ms
step:673/2330 train_time:26893ms step_avg:39.96ms
step:674/2330 train_time:26951ms step_avg:39.99ms
step:675/2330 train_time:26974ms step_avg:39.96ms
step:676/2330 train_time:27031ms step_avg:39.99ms
step:677/2330 train_time:27054ms step_avg:39.96ms
step:678/2330 train_time:27112ms step_avg:39.99ms
step:679/2330 train_time:27134ms step_avg:39.96ms
step:680/2330 train_time:27191ms step_avg:39.99ms
step:681/2330 train_time:27214ms step_avg:39.96ms
step:682/2330 train_time:27272ms step_avg:39.99ms
step:683/2330 train_time:27295ms step_avg:39.96ms
step:684/2330 train_time:27352ms step_avg:39.99ms
step:685/2330 train_time:27374ms step_avg:39.96ms
step:686/2330 train_time:27432ms step_avg:39.99ms
step:687/2330 train_time:27455ms step_avg:39.96ms
step:688/2330 train_time:27513ms step_avg:39.99ms
step:689/2330 train_time:27536ms step_avg:39.96ms
step:690/2330 train_time:27594ms step_avg:39.99ms
step:691/2330 train_time:27616ms step_avg:39.97ms
step:692/2330 train_time:27674ms step_avg:39.99ms
step:693/2330 train_time:27696ms step_avg:39.97ms
step:694/2330 train_time:27754ms step_avg:39.99ms
step:695/2330 train_time:27777ms step_avg:39.97ms
step:696/2330 train_time:27835ms step_avg:39.99ms
step:697/2330 train_time:27858ms step_avg:39.97ms
step:698/2330 train_time:27915ms step_avg:39.99ms
step:699/2330 train_time:27938ms step_avg:39.97ms
step:700/2330 train_time:27996ms step_avg:39.99ms
step:701/2330 train_time:28019ms step_avg:39.97ms
step:702/2330 train_time:28078ms step_avg:40.00ms
step:703/2330 train_time:28101ms step_avg:39.97ms
step:704/2330 train_time:28159ms step_avg:40.00ms
step:705/2330 train_time:28183ms step_avg:39.98ms
step:706/2330 train_time:28241ms step_avg:40.00ms
step:707/2330 train_time:28264ms step_avg:39.98ms
step:708/2330 train_time:28321ms step_avg:40.00ms
step:709/2330 train_time:28345ms step_avg:39.98ms
step:710/2330 train_time:28403ms step_avg:40.00ms
step:711/2330 train_time:28426ms step_avg:39.98ms
step:712/2330 train_time:28483ms step_avg:40.00ms
step:713/2330 train_time:28507ms step_avg:39.98ms
step:714/2330 train_time:28564ms step_avg:40.01ms
step:715/2330 train_time:28587ms step_avg:39.98ms
step:716/2330 train_time:28644ms step_avg:40.01ms
step:717/2330 train_time:28667ms step_avg:39.98ms
step:718/2330 train_time:28724ms step_avg:40.01ms
step:719/2330 train_time:28747ms step_avg:39.98ms
step:720/2330 train_time:28804ms step_avg:40.01ms
step:721/2330 train_time:28827ms step_avg:39.98ms
step:722/2330 train_time:28883ms step_avg:40.00ms
step:723/2330 train_time:28907ms step_avg:39.98ms
step:724/2330 train_time:28963ms step_avg:40.00ms
step:725/2330 train_time:28987ms step_avg:39.98ms
step:726/2330 train_time:29044ms step_avg:40.00ms
step:727/2330 train_time:29067ms step_avg:39.98ms
step:728/2330 train_time:29124ms step_avg:40.01ms
step:729/2330 train_time:29148ms step_avg:39.98ms
step:730/2330 train_time:29205ms step_avg:40.01ms
step:731/2330 train_time:29229ms step_avg:39.98ms
step:732/2330 train_time:29285ms step_avg:40.01ms
step:733/2330 train_time:29309ms step_avg:39.98ms
step:734/2330 train_time:29365ms step_avg:40.01ms
step:735/2330 train_time:29389ms step_avg:39.99ms
step:736/2330 train_time:29446ms step_avg:40.01ms
step:737/2330 train_time:29470ms step_avg:39.99ms
step:738/2330 train_time:29526ms step_avg:40.01ms
step:739/2330 train_time:29550ms step_avg:39.99ms
step:740/2330 train_time:29607ms step_avg:40.01ms
step:741/2330 train_time:29630ms step_avg:39.99ms
step:742/2330 train_time:29688ms step_avg:40.01ms
step:743/2330 train_time:29711ms step_avg:39.99ms
step:744/2330 train_time:29769ms step_avg:40.01ms
step:745/2330 train_time:29791ms step_avg:39.99ms
step:746/2330 train_time:29849ms step_avg:40.01ms
step:747/2330 train_time:29871ms step_avg:39.99ms
step:748/2330 train_time:29928ms step_avg:40.01ms
step:749/2330 train_time:29951ms step_avg:39.99ms
step:750/2330 train_time:30008ms step_avg:40.01ms
step:750/2330 val_loss:5.5055 train_time:30107ms step_avg:40.14ms
step:751/2330 train_time:30119ms step_avg:40.11ms
step:752/2330 train_time:30130ms step_avg:40.07ms
step:753/2330 train_time:30141ms step_avg:40.03ms
step:754/2330 train_time:30170ms step_avg:40.01ms
step:755/2330 train_time:30192ms step_avg:39.99ms
step:756/2330 train_time:30248ms step_avg:40.01ms
step:757/2330 train_time:30270ms step_avg:39.99ms
step:758/2330 train_time:30326ms step_avg:40.01ms
step:759/2330 train_time:30348ms step_avg:39.98ms
step:760/2330 train_time:30406ms step_avg:40.01ms
step:761/2330 train_time:30433ms step_avg:39.99ms
step:762/2330 train_time:30493ms step_avg:40.02ms
step:763/2330 train_time:30518ms step_avg:40.00ms
step:764/2330 train_time:30576ms step_avg:40.02ms
step:765/2330 train_time:30600ms step_avg:40.00ms
step:766/2330 train_time:30658ms step_avg:40.02ms
step:767/2330 train_time:30680ms step_avg:40.00ms
step:768/2330 train_time:30739ms step_avg:40.02ms
step:769/2330 train_time:30761ms step_avg:40.00ms
step:770/2330 train_time:30818ms step_avg:40.02ms
step:771/2330 train_time:30841ms step_avg:40.00ms
step:772/2330 train_time:30898ms step_avg:40.02ms
step:773/2330 train_time:30921ms step_avg:40.00ms
step:774/2330 train_time:30978ms step_avg:40.02ms
step:775/2330 train_time:31001ms step_avg:40.00ms
step:776/2330 train_time:31059ms step_avg:40.02ms
step:777/2330 train_time:31084ms step_avg:40.00ms
step:778/2330 train_time:31141ms step_avg:40.03ms
step:779/2330 train_time:31164ms step_avg:40.01ms
step:780/2330 train_time:31221ms step_avg:40.03ms
step:781/2330 train_time:31244ms step_avg:40.01ms
step:782/2330 train_time:31301ms step_avg:40.03ms
step:783/2330 train_time:31325ms step_avg:40.01ms
step:784/2330 train_time:31383ms step_avg:40.03ms
step:785/2330 train_time:31408ms step_avg:40.01ms
step:786/2330 train_time:31467ms step_avg:40.03ms
step:787/2330 train_time:31491ms step_avg:40.01ms
step:788/2330 train_time:31549ms step_avg:40.04ms
step:789/2330 train_time:31572ms step_avg:40.02ms
step:790/2330 train_time:31630ms step_avg:40.04ms
step:791/2330 train_time:31653ms step_avg:40.02ms
step:792/2330 train_time:31711ms step_avg:40.04ms
step:793/2330 train_time:31733ms step_avg:40.02ms
step:794/2330 train_time:31790ms step_avg:40.04ms
step:795/2330 train_time:31813ms step_avg:40.02ms
step:796/2330 train_time:31870ms step_avg:40.04ms
step:797/2330 train_time:31893ms step_avg:40.02ms
step:798/2330 train_time:31951ms step_avg:40.04ms
step:799/2330 train_time:31973ms step_avg:40.02ms
step:800/2330 train_time:32031ms step_avg:40.04ms
step:801/2330 train_time:32053ms step_avg:40.02ms
step:802/2330 train_time:32111ms step_avg:40.04ms
step:803/2330 train_time:32133ms step_avg:40.02ms
step:804/2330 train_time:32191ms step_avg:40.04ms
step:805/2330 train_time:32214ms step_avg:40.02ms
step:806/2330 train_time:32271ms step_avg:40.04ms
step:807/2330 train_time:32294ms step_avg:40.02ms
step:808/2330 train_time:32352ms step_avg:40.04ms
step:809/2330 train_time:32375ms step_avg:40.02ms
step:810/2330 train_time:32433ms step_avg:40.04ms
step:811/2330 train_time:32456ms step_avg:40.02ms
step:812/2330 train_time:32513ms step_avg:40.04ms
step:813/2330 train_time:32535ms step_avg:40.02ms
step:814/2330 train_time:32592ms step_avg:40.04ms
step:815/2330 train_time:32614ms step_avg:40.02ms
step:816/2330 train_time:32672ms step_avg:40.04ms
step:817/2330 train_time:32695ms step_avg:40.02ms
step:818/2330 train_time:32752ms step_avg:40.04ms
step:819/2330 train_time:32774ms step_avg:40.02ms
step:820/2330 train_time:32832ms step_avg:40.04ms
step:821/2330 train_time:32854ms step_avg:40.02ms
step:822/2330 train_time:32911ms step_avg:40.04ms
step:823/2330 train_time:32934ms step_avg:40.02ms
step:824/2330 train_time:32991ms step_avg:40.04ms
step:825/2330 train_time:33014ms step_avg:40.02ms
step:826/2330 train_time:33071ms step_avg:40.04ms
step:827/2330 train_time:33094ms step_avg:40.02ms
step:828/2330 train_time:33151ms step_avg:40.04ms
step:829/2330 train_time:33174ms step_avg:40.02ms
step:830/2330 train_time:33232ms step_avg:40.04ms
step:831/2330 train_time:33254ms step_avg:40.02ms
step:832/2330 train_time:33312ms step_avg:40.04ms
step:833/2330 train_time:33334ms step_avg:40.02ms
step:834/2330 train_time:33393ms step_avg:40.04ms
step:835/2330 train_time:33416ms step_avg:40.02ms
step:836/2330 train_time:33475ms step_avg:40.04ms
step:837/2330 train_time:33497ms step_avg:40.02ms
step:838/2330 train_time:33554ms step_avg:40.04ms
step:839/2330 train_time:33577ms step_avg:40.02ms
step:840/2330 train_time:33635ms step_avg:40.04ms
step:841/2330 train_time:33657ms step_avg:40.02ms
step:842/2330 train_time:33716ms step_avg:40.04ms
step:843/2330 train_time:33738ms step_avg:40.02ms
step:844/2330 train_time:33796ms step_avg:40.04ms
step:845/2330 train_time:33818ms step_avg:40.02ms
step:846/2330 train_time:33876ms step_avg:40.04ms
step:847/2330 train_time:33900ms step_avg:40.02ms
step:848/2330 train_time:33958ms step_avg:40.04ms
step:849/2330 train_time:33980ms step_avg:40.02ms
step:850/2330 train_time:34038ms step_avg:40.04ms
step:851/2330 train_time:34061ms step_avg:40.02ms
step:852/2330 train_time:34118ms step_avg:40.04ms
step:853/2330 train_time:34141ms step_avg:40.02ms
step:854/2330 train_time:34198ms step_avg:40.04ms
step:855/2330 train_time:34222ms step_avg:40.03ms
step:856/2330 train_time:34280ms step_avg:40.05ms
step:857/2330 train_time:34304ms step_avg:40.03ms
step:858/2330 train_time:34361ms step_avg:40.05ms
step:859/2330 train_time:34384ms step_avg:40.03ms
step:860/2330 train_time:34442ms step_avg:40.05ms
step:861/2330 train_time:34465ms step_avg:40.03ms
step:862/2330 train_time:34522ms step_avg:40.05ms
step:863/2330 train_time:34545ms step_avg:40.03ms
step:864/2330 train_time:34602ms step_avg:40.05ms
step:865/2330 train_time:34625ms step_avg:40.03ms
step:866/2330 train_time:34682ms step_avg:40.05ms
step:867/2330 train_time:34706ms step_avg:40.03ms
step:868/2330 train_time:34763ms step_avg:40.05ms
step:869/2330 train_time:34786ms step_avg:40.03ms
step:870/2330 train_time:34843ms step_avg:40.05ms
step:871/2330 train_time:34867ms step_avg:40.03ms
step:872/2330 train_time:34924ms step_avg:40.05ms
step:873/2330 train_time:34948ms step_avg:40.03ms
step:874/2330 train_time:35005ms step_avg:40.05ms
step:875/2330 train_time:35028ms step_avg:40.03ms
step:876/2330 train_time:35085ms step_avg:40.05ms
step:877/2330 train_time:35109ms step_avg:40.03ms
step:878/2330 train_time:35166ms step_avg:40.05ms
step:879/2330 train_time:35190ms step_avg:40.03ms
step:880/2330 train_time:35247ms step_avg:40.05ms
step:881/2330 train_time:35270ms step_avg:40.03ms
step:882/2330 train_time:35326ms step_avg:40.05ms
step:883/2330 train_time:35350ms step_avg:40.03ms
step:884/2330 train_time:35406ms step_avg:40.05ms
step:885/2330 train_time:35429ms step_avg:40.03ms
step:886/2330 train_time:35486ms step_avg:40.05ms
step:887/2330 train_time:35510ms step_avg:40.03ms
step:888/2330 train_time:35567ms step_avg:40.05ms
step:889/2330 train_time:35590ms step_avg:40.03ms
step:890/2330 train_time:35647ms step_avg:40.05ms
step:891/2330 train_time:35670ms step_avg:40.03ms
step:892/2330 train_time:35728ms step_avg:40.05ms
step:893/2330 train_time:35751ms step_avg:40.03ms
step:894/2330 train_time:35808ms step_avg:40.05ms
step:895/2330 train_time:35831ms step_avg:40.03ms
step:896/2330 train_time:35888ms step_avg:40.05ms
step:897/2330 train_time:35911ms step_avg:40.03ms
step:898/2330 train_time:35968ms step_avg:40.05ms
step:899/2330 train_time:35991ms step_avg:40.03ms
step:900/2330 train_time:36049ms step_avg:40.05ms
step:901/2330 train_time:36072ms step_avg:40.04ms
step:902/2330 train_time:36130ms step_avg:40.06ms
step:903/2330 train_time:36152ms step_avg:40.04ms
step:904/2330 train_time:36210ms step_avg:40.06ms
step:905/2330 train_time:36232ms step_avg:40.04ms
step:906/2330 train_time:36290ms step_avg:40.06ms
step:907/2330 train_time:36312ms step_avg:40.04ms
step:908/2330 train_time:36369ms step_avg:40.05ms
step:909/2330 train_time:36392ms step_avg:40.03ms
step:910/2330 train_time:36450ms step_avg:40.05ms
step:911/2330 train_time:36472ms step_avg:40.04ms
step:912/2330 train_time:36530ms step_avg:40.05ms
step:913/2330 train_time:36552ms step_avg:40.03ms
step:914/2330 train_time:36609ms step_avg:40.05ms
step:915/2330 train_time:36633ms step_avg:40.04ms
step:916/2330 train_time:36690ms step_avg:40.05ms
step:917/2330 train_time:36712ms step_avg:40.04ms
step:918/2330 train_time:36770ms step_avg:40.05ms
step:919/2330 train_time:36793ms step_avg:40.04ms
step:920/2330 train_time:36852ms step_avg:40.06ms
step:921/2330 train_time:36874ms step_avg:40.04ms
step:922/2330 train_time:36933ms step_avg:40.06ms
step:923/2330 train_time:36955ms step_avg:40.04ms
step:924/2330 train_time:37012ms step_avg:40.06ms
step:925/2330 train_time:37034ms step_avg:40.04ms
step:926/2330 train_time:37092ms step_avg:40.06ms
step:927/2330 train_time:37114ms step_avg:40.04ms
step:928/2330 train_time:37171ms step_avg:40.06ms
step:929/2330 train_time:37194ms step_avg:40.04ms
step:930/2330 train_time:37251ms step_avg:40.06ms
step:931/2330 train_time:37274ms step_avg:40.04ms
step:932/2330 train_time:37332ms step_avg:40.06ms
step:933/2330 train_time:37354ms step_avg:40.04ms
step:934/2330 train_time:37411ms step_avg:40.05ms
step:935/2330 train_time:37433ms step_avg:40.04ms
step:936/2330 train_time:37491ms step_avg:40.05ms
step:937/2330 train_time:37513ms step_avg:40.04ms
step:938/2330 train_time:37570ms step_avg:40.05ms
step:939/2330 train_time:37593ms step_avg:40.04ms
step:940/2330 train_time:37651ms step_avg:40.05ms
step:941/2330 train_time:37673ms step_avg:40.04ms
step:942/2330 train_time:37731ms step_avg:40.05ms
step:943/2330 train_time:37753ms step_avg:40.04ms
step:944/2330 train_time:37811ms step_avg:40.05ms
step:945/2330 train_time:37834ms step_avg:40.04ms
step:946/2330 train_time:37893ms step_avg:40.06ms
step:947/2330 train_time:37915ms step_avg:40.04ms
step:948/2330 train_time:37971ms step_avg:40.05ms
step:949/2330 train_time:37994ms step_avg:40.04ms
step:950/2330 train_time:38051ms step_avg:40.05ms
step:951/2330 train_time:38074ms step_avg:40.04ms
step:952/2330 train_time:38131ms step_avg:40.05ms
step:953/2330 train_time:38154ms step_avg:40.04ms
step:954/2330 train_time:38212ms step_avg:40.05ms
step:955/2330 train_time:38234ms step_avg:40.04ms
step:956/2330 train_time:38291ms step_avg:40.05ms
step:957/2330 train_time:38314ms step_avg:40.04ms
step:958/2330 train_time:38371ms step_avg:40.05ms
step:959/2330 train_time:38393ms step_avg:40.03ms
step:960/2330 train_time:38451ms step_avg:40.05ms
step:961/2330 train_time:38474ms step_avg:40.03ms
step:962/2330 train_time:38532ms step_avg:40.05ms
step:963/2330 train_time:38554ms step_avg:40.03ms
step:964/2330 train_time:38612ms step_avg:40.05ms
step:965/2330 train_time:38634ms step_avg:40.03ms
step:966/2330 train_time:38691ms step_avg:40.05ms
step:967/2330 train_time:38714ms step_avg:40.03ms
step:968/2330 train_time:38771ms step_avg:40.05ms
step:969/2330 train_time:38794ms step_avg:40.03ms
step:970/2330 train_time:38852ms step_avg:40.05ms
step:971/2330 train_time:38874ms step_avg:40.04ms
step:972/2330 train_time:38932ms step_avg:40.05ms
step:973/2330 train_time:38954ms step_avg:40.03ms
step:974/2330 train_time:39011ms step_avg:40.05ms
step:975/2330 train_time:39034ms step_avg:40.03ms
step:976/2330 train_time:39092ms step_avg:40.05ms
step:977/2330 train_time:39114ms step_avg:40.03ms
step:978/2330 train_time:39171ms step_avg:40.05ms
step:979/2330 train_time:39194ms step_avg:40.03ms
step:980/2330 train_time:39251ms step_avg:40.05ms
step:981/2330 train_time:39273ms step_avg:40.03ms
step:982/2330 train_time:39331ms step_avg:40.05ms
step:983/2330 train_time:39353ms step_avg:40.03ms
step:984/2330 train_time:39410ms step_avg:40.05ms
step:985/2330 train_time:39432ms step_avg:40.03ms
step:986/2330 train_time:39490ms step_avg:40.05ms
step:987/2330 train_time:39513ms step_avg:40.03ms
step:988/2330 train_time:39570ms step_avg:40.05ms
step:989/2330 train_time:39593ms step_avg:40.03ms
step:990/2330 train_time:39652ms step_avg:40.05ms
step:991/2330 train_time:39674ms step_avg:40.03ms
step:992/2330 train_time:39731ms step_avg:40.05ms
step:993/2330 train_time:39754ms step_avg:40.03ms
step:994/2330 train_time:39812ms step_avg:40.05ms
step:995/2330 train_time:39835ms step_avg:40.03ms
step:996/2330 train_time:39893ms step_avg:40.05ms
step:997/2330 train_time:39916ms step_avg:40.04ms
step:998/2330 train_time:39972ms step_avg:40.05ms
step:999/2330 train_time:39995ms step_avg:40.03ms
step:1000/2330 train_time:40052ms step_avg:40.05ms
step:1000/2330 val_loss:5.3316 train_time:40148ms step_avg:40.15ms
step:1001/2330 train_time:40161ms step_avg:40.12ms
step:1002/2330 train_time:40174ms step_avg:40.09ms
step:1003/2330 train_time:40185ms step_avg:40.06ms
step:1004/2330 train_time:40211ms step_avg:40.05ms
step:1005/2330 train_time:40233ms step_avg:40.03ms
step:1006/2330 train_time:40291ms step_avg:40.05ms
step:1007/2330 train_time:40313ms step_avg:40.03ms
step:1008/2330 train_time:40369ms step_avg:40.05ms
step:1009/2330 train_time:40391ms step_avg:40.03ms
step:1010/2330 train_time:40449ms step_avg:40.05ms
step:1011/2330 train_time:40475ms step_avg:40.03ms
step:1012/2330 train_time:40537ms step_avg:40.06ms
step:1013/2330 train_time:40561ms step_avg:40.04ms
step:1014/2330 train_time:40620ms step_avg:40.06ms
step:1015/2330 train_time:40644ms step_avg:40.04ms
step:1016/2330 train_time:40701ms step_avg:40.06ms
step:1017/2330 train_time:40724ms step_avg:40.04ms
step:1018/2330 train_time:40781ms step_avg:40.06ms
step:1019/2330 train_time:40804ms step_avg:40.04ms
step:1020/2330 train_time:40861ms step_avg:40.06ms
step:1021/2330 train_time:40884ms step_avg:40.04ms
step:1022/2330 train_time:40940ms step_avg:40.06ms
step:1023/2330 train_time:40963ms step_avg:40.04ms
step:1024/2330 train_time:41019ms step_avg:40.06ms
step:1025/2330 train_time:41042ms step_avg:40.04ms
step:1026/2330 train_time:41101ms step_avg:40.06ms
step:1027/2330 train_time:41126ms step_avg:40.04ms
step:1028/2330 train_time:41183ms step_avg:40.06ms
step:1029/2330 train_time:41206ms step_avg:40.05ms
step:1030/2330 train_time:41263ms step_avg:40.06ms
step:1031/2330 train_time:41286ms step_avg:40.04ms
step:1032/2330 train_time:41342ms step_avg:40.06ms
step:1033/2330 train_time:41365ms step_avg:40.04ms
step:1034/2330 train_time:41422ms step_avg:40.06ms
step:1035/2330 train_time:41447ms step_avg:40.05ms
step:1036/2330 train_time:41506ms step_avg:40.06ms
step:1037/2330 train_time:41530ms step_avg:40.05ms
step:1038/2330 train_time:41588ms step_avg:40.07ms
step:1039/2330 train_time:41612ms step_avg:40.05ms
step:1040/2330 train_time:41669ms step_avg:40.07ms
step:1041/2330 train_time:41692ms step_avg:40.05ms
step:1042/2330 train_time:41749ms step_avg:40.07ms
step:1043/2330 train_time:41772ms step_avg:40.05ms
step:1044/2330 train_time:41830ms step_avg:40.07ms
step:1045/2330 train_time:41853ms step_avg:40.05ms
step:1046/2330 train_time:41910ms step_avg:40.07ms
step:1047/2330 train_time:41933ms step_avg:40.05ms
step:1048/2330 train_time:41990ms step_avg:40.07ms
step:1049/2330 train_time:42013ms step_avg:40.05ms
step:1050/2330 train_time:42071ms step_avg:40.07ms
step:1051/2330 train_time:42093ms step_avg:40.05ms
step:1052/2330 train_time:42151ms step_avg:40.07ms
step:1053/2330 train_time:42173ms step_avg:40.05ms
step:1054/2330 train_time:42231ms step_avg:40.07ms
step:1055/2330 train_time:42253ms step_avg:40.05ms
step:1056/2330 train_time:42310ms step_avg:40.07ms
step:1057/2330 train_time:42333ms step_avg:40.05ms
step:1058/2330 train_time:42391ms step_avg:40.07ms
step:1059/2330 train_time:42414ms step_avg:40.05ms
step:1060/2330 train_time:42473ms step_avg:40.07ms
step:1061/2330 train_time:42496ms step_avg:40.05ms
step:1062/2330 train_time:42556ms step_avg:40.07ms
step:1063/2330 train_time:42579ms step_avg:40.06ms
step:1064/2330 train_time:42637ms step_avg:40.07ms
step:1065/2330 train_time:42661ms step_avg:40.06ms
step:1066/2330 train_time:42718ms step_avg:40.07ms
step:1067/2330 train_time:42742ms step_avg:40.06ms
step:1068/2330 train_time:42799ms step_avg:40.07ms
step:1069/2330 train_time:42823ms step_avg:40.06ms
step:1070/2330 train_time:42879ms step_avg:40.07ms
step:1071/2330 train_time:42902ms step_avg:40.06ms
step:1072/2330 train_time:42959ms step_avg:40.07ms
step:1073/2330 train_time:42983ms step_avg:40.06ms
step:1074/2330 train_time:43040ms step_avg:40.07ms
step:1075/2330 train_time:43063ms step_avg:40.06ms
step:1076/2330 train_time:43120ms step_avg:40.07ms
step:1077/2330 train_time:43143ms step_avg:40.06ms
step:1078/2330 train_time:43200ms step_avg:40.07ms
step:1079/2330 train_time:43223ms step_avg:40.06ms
step:1080/2330 train_time:43280ms step_avg:40.07ms
step:1081/2330 train_time:43304ms step_avg:40.06ms
step:1082/2330 train_time:43361ms step_avg:40.08ms
step:1083/2330 train_time:43386ms step_avg:40.06ms
step:1084/2330 train_time:43443ms step_avg:40.08ms
step:1085/2330 train_time:43467ms step_avg:40.06ms
step:1086/2330 train_time:43524ms step_avg:40.08ms
step:1087/2330 train_time:43548ms step_avg:40.06ms
step:1088/2330 train_time:43605ms step_avg:40.08ms
step:1089/2330 train_time:43629ms step_avg:40.06ms
step:1090/2330 train_time:43686ms step_avg:40.08ms
step:1091/2330 train_time:43708ms step_avg:40.06ms
step:1092/2330 train_time:43766ms step_avg:40.08ms
step:1093/2330 train_time:43788ms step_avg:40.06ms
step:1094/2330 train_time:43846ms step_avg:40.08ms
step:1095/2330 train_time:43868ms step_avg:40.06ms
step:1096/2330 train_time:43925ms step_avg:40.08ms
step:1097/2330 train_time:43947ms step_avg:40.06ms
step:1098/2330 train_time:44005ms step_avg:40.08ms
step:1099/2330 train_time:44027ms step_avg:40.06ms
step:1100/2330 train_time:44084ms step_avg:40.08ms
step:1101/2330 train_time:44107ms step_avg:40.06ms
step:1102/2330 train_time:44164ms step_avg:40.08ms
step:1103/2330 train_time:44186ms step_avg:40.06ms
step:1104/2330 train_time:44244ms step_avg:40.08ms
step:1105/2330 train_time:44267ms step_avg:40.06ms
step:1106/2330 train_time:44325ms step_avg:40.08ms
step:1107/2330 train_time:44347ms step_avg:40.06ms
step:1108/2330 train_time:44405ms step_avg:40.08ms
step:1109/2330 train_time:44428ms step_avg:40.06ms
step:1110/2330 train_time:44486ms step_avg:40.08ms
step:1111/2330 train_time:44509ms step_avg:40.06ms
step:1112/2330 train_time:44566ms step_avg:40.08ms
step:1113/2330 train_time:44588ms step_avg:40.06ms
step:1114/2330 train_time:44646ms step_avg:40.08ms
step:1115/2330 train_time:44668ms step_avg:40.06ms
step:1116/2330 train_time:44725ms step_avg:40.08ms
step:1117/2330 train_time:44748ms step_avg:40.06ms
step:1118/2330 train_time:44806ms step_avg:40.08ms
step:1119/2330 train_time:44829ms step_avg:40.06ms
step:1120/2330 train_time:44887ms step_avg:40.08ms
step:1121/2330 train_time:44909ms step_avg:40.06ms
step:1122/2330 train_time:44967ms step_avg:40.08ms
step:1123/2330 train_time:44989ms step_avg:40.06ms
step:1124/2330 train_time:45047ms step_avg:40.08ms
step:1125/2330 train_time:45070ms step_avg:40.06ms
step:1126/2330 train_time:45127ms step_avg:40.08ms
step:1127/2330 train_time:45150ms step_avg:40.06ms
step:1128/2330 train_time:45207ms step_avg:40.08ms
step:1129/2330 train_time:45230ms step_avg:40.06ms
step:1130/2330 train_time:45287ms step_avg:40.08ms
step:1131/2330 train_time:45310ms step_avg:40.06ms
step:1132/2330 train_time:45368ms step_avg:40.08ms
step:1133/2330 train_time:45390ms step_avg:40.06ms
step:1134/2330 train_time:45448ms step_avg:40.08ms
step:1135/2330 train_time:45470ms step_avg:40.06ms
step:1136/2330 train_time:45527ms step_avg:40.08ms
step:1137/2330 train_time:45550ms step_avg:40.06ms
step:1138/2330 train_time:45607ms step_avg:40.08ms
step:1139/2330 train_time:45629ms step_avg:40.06ms
step:1140/2330 train_time:45688ms step_avg:40.08ms
step:1141/2330 train_time:45710ms step_avg:40.06ms
step:1142/2330 train_time:45767ms step_avg:40.08ms
step:1143/2330 train_time:45789ms step_avg:40.06ms
step:1144/2330 train_time:45847ms step_avg:40.08ms
step:1145/2330 train_time:45870ms step_avg:40.06ms
step:1146/2330 train_time:45927ms step_avg:40.08ms
step:1147/2330 train_time:45949ms step_avg:40.06ms
step:1148/2330 train_time:46007ms step_avg:40.08ms
step:1149/2330 train_time:46029ms step_avg:40.06ms
step:1150/2330 train_time:46087ms step_avg:40.08ms
step:1151/2330 train_time:46110ms step_avg:40.06ms
step:1152/2330 train_time:46168ms step_avg:40.08ms
step:1153/2330 train_time:46190ms step_avg:40.06ms
step:1154/2330 train_time:46248ms step_avg:40.08ms
step:1155/2330 train_time:46270ms step_avg:40.06ms
step:1156/2330 train_time:46328ms step_avg:40.08ms
step:1157/2330 train_time:46350ms step_avg:40.06ms
step:1158/2330 train_time:46407ms step_avg:40.07ms
step:1159/2330 train_time:46429ms step_avg:40.06ms
step:1160/2330 train_time:46487ms step_avg:40.07ms
step:1161/2330 train_time:46510ms step_avg:40.06ms
step:1162/2330 train_time:46566ms step_avg:40.07ms
step:1163/2330 train_time:46589ms step_avg:40.06ms
step:1164/2330 train_time:46646ms step_avg:40.07ms
step:1165/2330 train_time:46669ms step_avg:40.06ms
step:1166/2330 train_time:46726ms step_avg:40.07ms
step:1167/2330 train_time:46749ms step_avg:40.06ms
step:1168/2330 train_time:46806ms step_avg:40.07ms
step:1169/2330 train_time:46828ms step_avg:40.06ms
step:1170/2330 train_time:46886ms step_avg:40.07ms
step:1171/2330 train_time:46909ms step_avg:40.06ms
step:1172/2330 train_time:46967ms step_avg:40.07ms
step:1173/2330 train_time:46989ms step_avg:40.06ms
step:1174/2330 train_time:47047ms step_avg:40.07ms
step:1175/2330 train_time:47070ms step_avg:40.06ms
step:1176/2330 train_time:47128ms step_avg:40.07ms
step:1177/2330 train_time:47150ms step_avg:40.06ms
step:1178/2330 train_time:47207ms step_avg:40.07ms
step:1179/2330 train_time:47230ms step_avg:40.06ms
step:1180/2330 train_time:47287ms step_avg:40.07ms
step:1181/2330 train_time:47310ms step_avg:40.06ms
step:1182/2330 train_time:47367ms step_avg:40.07ms
step:1183/2330 train_time:47389ms step_avg:40.06ms
step:1184/2330 train_time:47447ms step_avg:40.07ms
step:1185/2330 train_time:47469ms step_avg:40.06ms
step:1186/2330 train_time:47526ms step_avg:40.07ms
step:1187/2330 train_time:47548ms step_avg:40.06ms
step:1188/2330 train_time:47606ms step_avg:40.07ms
step:1189/2330 train_time:47628ms step_avg:40.06ms
step:1190/2330 train_time:47685ms step_avg:40.07ms
step:1191/2330 train_time:47708ms step_avg:40.06ms
step:1192/2330 train_time:47765ms step_avg:40.07ms
step:1193/2330 train_time:47788ms step_avg:40.06ms
step:1194/2330 train_time:47845ms step_avg:40.07ms
step:1195/2330 train_time:47868ms step_avg:40.06ms
step:1196/2330 train_time:47926ms step_avg:40.07ms
step:1197/2330 train_time:47949ms step_avg:40.06ms
step:1198/2330 train_time:48006ms step_avg:40.07ms
step:1199/2330 train_time:48029ms step_avg:40.06ms
step:1200/2330 train_time:48087ms step_avg:40.07ms
step:1201/2330 train_time:48110ms step_avg:40.06ms
step:1202/2330 train_time:48167ms step_avg:40.07ms
step:1203/2330 train_time:48189ms step_avg:40.06ms
step:1204/2330 train_time:48247ms step_avg:40.07ms
step:1205/2330 train_time:48269ms step_avg:40.06ms
step:1206/2330 train_time:48326ms step_avg:40.07ms
step:1207/2330 train_time:48349ms step_avg:40.06ms
step:1208/2330 train_time:48406ms step_avg:40.07ms
step:1209/2330 train_time:48428ms step_avg:40.06ms
step:1210/2330 train_time:48486ms step_avg:40.07ms
step:1211/2330 train_time:48509ms step_avg:40.06ms
step:1212/2330 train_time:48567ms step_avg:40.07ms
step:1213/2330 train_time:48589ms step_avg:40.06ms
step:1214/2330 train_time:48646ms step_avg:40.07ms
step:1215/2330 train_time:48669ms step_avg:40.06ms
step:1216/2330 train_time:48726ms step_avg:40.07ms
step:1217/2330 train_time:48749ms step_avg:40.06ms
step:1218/2330 train_time:48806ms step_avg:40.07ms
step:1219/2330 train_time:48828ms step_avg:40.06ms
step:1220/2330 train_time:48886ms step_avg:40.07ms
step:1221/2330 train_time:48909ms step_avg:40.06ms
step:1222/2330 train_time:48966ms step_avg:40.07ms
step:1223/2330 train_time:48988ms step_avg:40.06ms
step:1224/2330 train_time:49047ms step_avg:40.07ms
step:1225/2330 train_time:49070ms step_avg:40.06ms
step:1226/2330 train_time:49127ms step_avg:40.07ms
step:1227/2330 train_time:49150ms step_avg:40.06ms
step:1228/2330 train_time:49207ms step_avg:40.07ms
step:1229/2330 train_time:49230ms step_avg:40.06ms
step:1230/2330 train_time:49287ms step_avg:40.07ms
step:1231/2330 train_time:49309ms step_avg:40.06ms
step:1232/2330 train_time:49367ms step_avg:40.07ms
step:1233/2330 train_time:49389ms step_avg:40.06ms
step:1234/2330 train_time:49448ms step_avg:40.07ms
step:1235/2330 train_time:49470ms step_avg:40.06ms
step:1236/2330 train_time:49527ms step_avg:40.07ms
step:1237/2330 train_time:49549ms step_avg:40.06ms
step:1238/2330 train_time:49606ms step_avg:40.07ms
step:1239/2330 train_time:49628ms step_avg:40.06ms
step:1240/2330 train_time:49686ms step_avg:40.07ms
step:1241/2330 train_time:49709ms step_avg:40.06ms
step:1242/2330 train_time:49766ms step_avg:40.07ms
step:1243/2330 train_time:49788ms step_avg:40.05ms
step:1244/2330 train_time:49846ms step_avg:40.07ms
step:1245/2330 train_time:49869ms step_avg:40.06ms
step:1246/2330 train_time:49926ms step_avg:40.07ms
step:1247/2330 train_time:49949ms step_avg:40.06ms
step:1248/2330 train_time:50006ms step_avg:40.07ms
step:1249/2330 train_time:50029ms step_avg:40.06ms
step:1250/2330 train_time:50087ms step_avg:40.07ms
step:1250/2330 val_loss:5.2695 train_time:50184ms step_avg:40.15ms
step:1251/2330 train_time:50197ms step_avg:40.13ms
step:1252/2330 train_time:50208ms step_avg:40.10ms
step:1253/2330 train_time:50218ms step_avg:40.08ms
step:1254/2330 train_time:50247ms step_avg:40.07ms
step:1255/2330 train_time:50269ms step_avg:40.06ms
step:1256/2330 train_time:50325ms step_avg:40.07ms
step:1257/2330 train_time:50347ms step_avg:40.05ms
step:1258/2330 train_time:50403ms step_avg:40.07ms
step:1259/2330 train_time:50426ms step_avg:40.05ms
step:1260/2330 train_time:50485ms step_avg:40.07ms
step:1261/2330 train_time:50513ms step_avg:40.06ms
step:1262/2330 train_time:50575ms step_avg:40.08ms
step:1263/2330 train_time:50600ms step_avg:40.06ms
step:1264/2330 train_time:50658ms step_avg:40.08ms
step:1265/2330 train_time:50681ms step_avg:40.06ms
step:1266/2330 train_time:50737ms step_avg:40.08ms
step:1267/2330 train_time:50760ms step_avg:40.06ms
step:1268/2330 train_time:50816ms step_avg:40.08ms
step:1269/2330 train_time:50839ms step_avg:40.06ms
step:1270/2330 train_time:50896ms step_avg:40.08ms
step:1271/2330 train_time:50918ms step_avg:40.06ms
step:1272/2330 train_time:50975ms step_avg:40.07ms
step:1273/2330 train_time:50997ms step_avg:40.06ms
step:1274/2330 train_time:51054ms step_avg:40.07ms
step:1275/2330 train_time:51077ms step_avg:40.06ms
step:1276/2330 train_time:51136ms step_avg:40.08ms
step:1277/2330 train_time:51161ms step_avg:40.06ms
step:1278/2330 train_time:51220ms step_avg:40.08ms
step:1279/2330 train_time:51243ms step_avg:40.06ms
step:1280/2330 train_time:51300ms step_avg:40.08ms
step:1281/2330 train_time:51323ms step_avg:40.07ms
step:1282/2330 train_time:51380ms step_avg:40.08ms
step:1283/2330 train_time:51403ms step_avg:40.06ms
step:1284/2330 train_time:51462ms step_avg:40.08ms
step:1285/2330 train_time:51485ms step_avg:40.07ms
step:1286/2330 train_time:51544ms step_avg:40.08ms
step:1287/2330 train_time:51569ms step_avg:40.07ms
step:1288/2330 train_time:51627ms step_avg:40.08ms
step:1289/2330 train_time:51652ms step_avg:40.07ms
step:1290/2330 train_time:51710ms step_avg:40.08ms
step:1291/2330 train_time:51733ms step_avg:40.07ms
step:1292/2330 train_time:51789ms step_avg:40.08ms
step:1293/2330 train_time:51813ms step_avg:40.07ms
step:1294/2330 train_time:51869ms step_avg:40.08ms
step:1295/2330 train_time:51893ms step_avg:40.07ms
step:1296/2330 train_time:51950ms step_avg:40.08ms
step:1297/2330 train_time:51973ms step_avg:40.07ms
step:1298/2330 train_time:52030ms step_avg:40.08ms
step:1299/2330 train_time:52053ms step_avg:40.07ms
step:1300/2330 train_time:52110ms step_avg:40.08ms
step:1301/2330 train_time:52135ms step_avg:40.07ms
step:1302/2330 train_time:52191ms step_avg:40.09ms
step:1303/2330 train_time:52215ms step_avg:40.07ms
step:1304/2330 train_time:52272ms step_avg:40.09ms
step:1305/2330 train_time:52296ms step_avg:40.07ms
step:1306/2330 train_time:52352ms step_avg:40.09ms
step:1307/2330 train_time:52376ms step_avg:40.07ms
step:1308/2330 train_time:52433ms step_avg:40.09ms
step:1309/2330 train_time:52457ms step_avg:40.07ms
step:1310/2330 train_time:52515ms step_avg:40.09ms
step:1311/2330 train_time:52540ms step_avg:40.08ms
step:1312/2330 train_time:52598ms step_avg:40.09ms
step:1313/2330 train_time:52621ms step_avg:40.08ms
step:1314/2330 train_time:52678ms step_avg:40.09ms
step:1315/2330 train_time:52701ms step_avg:40.08ms
step:1316/2330 train_time:52758ms step_avg:40.09ms
step:1317/2330 train_time:52780ms step_avg:40.08ms
step:1318/2330 train_time:52837ms step_avg:40.09ms
step:1319/2330 train_time:52860ms step_avg:40.08ms
step:1320/2330 train_time:52917ms step_avg:40.09ms
step:1321/2330 train_time:52941ms step_avg:40.08ms
step:1322/2330 train_time:52998ms step_avg:40.09ms
step:1323/2330 train_time:53021ms step_avg:40.08ms
step:1324/2330 train_time:53079ms step_avg:40.09ms
step:1325/2330 train_time:53101ms step_avg:40.08ms
step:1326/2330 train_time:53159ms step_avg:40.09ms
step:1327/2330 train_time:53182ms step_avg:40.08ms
step:1328/2330 train_time:53240ms step_avg:40.09ms
step:1329/2330 train_time:53262ms step_avg:40.08ms
step:1330/2330 train_time:53319ms step_avg:40.09ms
step:1331/2330 train_time:53342ms step_avg:40.08ms
step:1332/2330 train_time:53400ms step_avg:40.09ms
step:1333/2330 train_time:53422ms step_avg:40.08ms
step:1334/2330 train_time:53479ms step_avg:40.09ms
step:1335/2330 train_time:53501ms step_avg:40.08ms
step:1336/2330 train_time:53559ms step_avg:40.09ms
step:1337/2330 train_time:53582ms step_avg:40.08ms
step:1338/2330 train_time:53639ms step_avg:40.09ms
step:1339/2330 train_time:53661ms step_avg:40.08ms
step:1340/2330 train_time:53719ms step_avg:40.09ms
step:1341/2330 train_time:53741ms step_avg:40.08ms
step:1342/2330 train_time:53799ms step_avg:40.09ms
step:1343/2330 train_time:53821ms step_avg:40.08ms
step:1344/2330 train_time:53878ms step_avg:40.09ms
step:1345/2330 train_time:53902ms step_avg:40.08ms
step:1346/2330 train_time:53959ms step_avg:40.09ms
step:1347/2330 train_time:53983ms step_avg:40.08ms
step:1348/2330 train_time:54041ms step_avg:40.09ms
step:1349/2330 train_time:54063ms step_avg:40.08ms
step:1350/2330 train_time:54120ms step_avg:40.09ms
step:1351/2330 train_time:54143ms step_avg:40.08ms
step:1352/2330 train_time:54200ms step_avg:40.09ms
step:1353/2330 train_time:54223ms step_avg:40.08ms
step:1354/2330 train_time:54280ms step_avg:40.09ms
step:1355/2330 train_time:54303ms step_avg:40.08ms
step:1356/2330 train_time:54361ms step_avg:40.09ms
step:1357/2330 train_time:54383ms step_avg:40.08ms
step:1358/2330 train_time:54440ms step_avg:40.09ms
step:1359/2330 train_time:54463ms step_avg:40.08ms
step:1360/2330 train_time:54520ms step_avg:40.09ms
step:1361/2330 train_time:54543ms step_avg:40.08ms
step:1362/2330 train_time:54600ms step_avg:40.09ms
step:1363/2330 train_time:54623ms step_avg:40.08ms
step:1364/2330 train_time:54681ms step_avg:40.09ms
step:1365/2330 train_time:54703ms step_avg:40.08ms
step:1366/2330 train_time:54760ms step_avg:40.09ms
step:1367/2330 train_time:54782ms step_avg:40.07ms
step:1368/2330 train_time:54840ms step_avg:40.09ms
step:1369/2330 train_time:54863ms step_avg:40.08ms
step:1370/2330 train_time:54921ms step_avg:40.09ms
step:1371/2330 train_time:54944ms step_avg:40.08ms
step:1372/2330 train_time:55002ms step_avg:40.09ms
step:1373/2330 train_time:55026ms step_avg:40.08ms
step:1374/2330 train_time:55084ms step_avg:40.09ms
step:1375/2330 train_time:55106ms step_avg:40.08ms
step:1376/2330 train_time:55163ms step_avg:40.09ms
step:1377/2330 train_time:55186ms step_avg:40.08ms
step:1378/2330 train_time:55245ms step_avg:40.09ms
step:1379/2330 train_time:55267ms step_avg:40.08ms
step:1380/2330 train_time:55324ms step_avg:40.09ms
step:1381/2330 train_time:55347ms step_avg:40.08ms
step:1382/2330 train_time:55406ms step_avg:40.09ms
step:1383/2330 train_time:55429ms step_avg:40.08ms
step:1384/2330 train_time:55486ms step_avg:40.09ms
step:1385/2330 train_time:55509ms step_avg:40.08ms
step:1386/2330 train_time:55566ms step_avg:40.09ms
step:1387/2330 train_time:55589ms step_avg:40.08ms
step:1388/2330 train_time:55648ms step_avg:40.09ms
step:1389/2330 train_time:55672ms step_avg:40.08ms
step:1390/2330 train_time:55729ms step_avg:40.09ms
step:1391/2330 train_time:55753ms step_avg:40.08ms
step:1392/2330 train_time:55810ms step_avg:40.09ms
step:1393/2330 train_time:55834ms step_avg:40.08ms
step:1394/2330 train_time:55891ms step_avg:40.09ms
step:1395/2330 train_time:55915ms step_avg:40.08ms
step:1396/2330 train_time:55971ms step_avg:40.09ms
step:1397/2330 train_time:55995ms step_avg:40.08ms
step:1398/2330 train_time:56052ms step_avg:40.09ms
step:1399/2330 train_time:56076ms step_avg:40.08ms
step:1400/2330 train_time:56133ms step_avg:40.09ms
step:1401/2330 train_time:56157ms step_avg:40.08ms
step:1402/2330 train_time:56214ms step_avg:40.10ms
step:1403/2330 train_time:56238ms step_avg:40.08ms
step:1404/2330 train_time:56295ms step_avg:40.10ms
step:1405/2330 train_time:56319ms step_avg:40.08ms
step:1406/2330 train_time:56378ms step_avg:40.10ms
step:1407/2330 train_time:56400ms step_avg:40.09ms
step:1408/2330 train_time:56457ms step_avg:40.10ms
step:1409/2330 train_time:56481ms step_avg:40.09ms
step:1410/2330 train_time:56538ms step_avg:40.10ms
step:1411/2330 train_time:56561ms step_avg:40.09ms
step:1412/2330 train_time:56619ms step_avg:40.10ms
step:1413/2330 train_time:56641ms step_avg:40.09ms
step:1414/2330 train_time:56699ms step_avg:40.10ms
step:1415/2330 train_time:56721ms step_avg:40.09ms
step:1416/2330 train_time:56779ms step_avg:40.10ms
step:1417/2330 train_time:56801ms step_avg:40.09ms
step:1418/2330 train_time:56859ms step_avg:40.10ms
step:1419/2330 train_time:56881ms step_avg:40.09ms
step:1420/2330 train_time:56939ms step_avg:40.10ms
step:1421/2330 train_time:56962ms step_avg:40.09ms
step:1422/2330 train_time:57020ms step_avg:40.10ms
step:1423/2330 train_time:57042ms step_avg:40.09ms
step:1424/2330 train_time:57099ms step_avg:40.10ms
step:1425/2330 train_time:57121ms step_avg:40.09ms
step:1426/2330 train_time:57179ms step_avg:40.10ms
step:1427/2330 train_time:57201ms step_avg:40.08ms
step:1428/2330 train_time:57258ms step_avg:40.10ms
step:1429/2330 train_time:57281ms step_avg:40.08ms
step:1430/2330 train_time:57339ms step_avg:40.10ms
step:1431/2330 train_time:57361ms step_avg:40.08ms
step:1432/2330 train_time:57418ms step_avg:40.10ms
step:1433/2330 train_time:57441ms step_avg:40.08ms
step:1434/2330 train_time:57498ms step_avg:40.10ms
step:1435/2330 train_time:57521ms step_avg:40.08ms
step:1436/2330 train_time:57579ms step_avg:40.10ms
step:1437/2330 train_time:57602ms step_avg:40.08ms
step:1438/2330 train_time:57660ms step_avg:40.10ms
step:1439/2330 train_time:57682ms step_avg:40.08ms
step:1440/2330 train_time:57740ms step_avg:40.10ms
step:1441/2330 train_time:57762ms step_avg:40.08ms
step:1442/2330 train_time:57819ms step_avg:40.10ms
step:1443/2330 train_time:57842ms step_avg:40.08ms
step:1444/2330 train_time:57900ms step_avg:40.10ms
step:1445/2330 train_time:57923ms step_avg:40.08ms
step:1446/2330 train_time:57981ms step_avg:40.10ms
step:1447/2330 train_time:58003ms step_avg:40.09ms
step:1448/2330 train_time:58060ms step_avg:40.10ms
step:1449/2330 train_time:58082ms step_avg:40.08ms
step:1450/2330 train_time:58140ms step_avg:40.10ms
step:1451/2330 train_time:58162ms step_avg:40.08ms
step:1452/2330 train_time:58220ms step_avg:40.10ms
step:1453/2330 train_time:58243ms step_avg:40.08ms
step:1454/2330 train_time:58300ms step_avg:40.10ms
step:1455/2330 train_time:58322ms step_avg:40.08ms
step:1456/2330 train_time:58379ms step_avg:40.10ms
step:1457/2330 train_time:58402ms step_avg:40.08ms
step:1458/2330 train_time:58460ms step_avg:40.10ms
step:1459/2330 train_time:58482ms step_avg:40.08ms
step:1460/2330 train_time:58539ms step_avg:40.10ms
step:1461/2330 train_time:58562ms step_avg:40.08ms
step:1462/2330 train_time:58620ms step_avg:40.10ms
step:1463/2330 train_time:58642ms step_avg:40.08ms
step:1464/2330 train_time:58700ms step_avg:40.10ms
step:1465/2330 train_time:58722ms step_avg:40.08ms
step:1466/2330 train_time:58779ms step_avg:40.09ms
step:1467/2330 train_time:58802ms step_avg:40.08ms
step:1468/2330 train_time:58860ms step_avg:40.10ms
step:1469/2330 train_time:58882ms step_avg:40.08ms
step:1470/2330 train_time:58940ms step_avg:40.10ms
step:1471/2330 train_time:58962ms step_avg:40.08ms
step:1472/2330 train_time:59019ms step_avg:40.09ms
step:1473/2330 train_time:59042ms step_avg:40.08ms
step:1474/2330 train_time:59100ms step_avg:40.09ms
step:1475/2330 train_time:59122ms step_avg:40.08ms
step:1476/2330 train_time:59180ms step_avg:40.09ms
step:1477/2330 train_time:59202ms step_avg:40.08ms
step:1478/2330 train_time:59260ms step_avg:40.09ms
step:1479/2330 train_time:59282ms step_avg:40.08ms
step:1480/2330 train_time:59339ms step_avg:40.09ms
step:1481/2330 train_time:59362ms step_avg:40.08ms
step:1482/2330 train_time:59420ms step_avg:40.09ms
step:1483/2330 train_time:59442ms step_avg:40.08ms
step:1484/2330 train_time:59500ms step_avg:40.09ms
step:1485/2330 train_time:59522ms step_avg:40.08ms
step:1486/2330 train_time:59580ms step_avg:40.09ms
step:1487/2330 train_time:59602ms step_avg:40.08ms
step:1488/2330 train_time:59660ms step_avg:40.09ms
step:1489/2330 train_time:59683ms step_avg:40.08ms
step:1490/2330 train_time:59739ms step_avg:40.09ms
step:1491/2330 train_time:59762ms step_avg:40.08ms
step:1492/2330 train_time:59819ms step_avg:40.09ms
step:1493/2330 train_time:59842ms step_avg:40.08ms
step:1494/2330 train_time:59899ms step_avg:40.09ms
step:1495/2330 train_time:59921ms step_avg:40.08ms
step:1496/2330 train_time:59979ms step_avg:40.09ms
step:1497/2330 train_time:60002ms step_avg:40.08ms
step:1498/2330 train_time:60060ms step_avg:40.09ms
step:1499/2330 train_time:60082ms step_avg:40.08ms
step:1500/2330 train_time:60139ms step_avg:40.09ms
step:1500/2330 val_loss:5.2900 train_time:60236ms step_avg:40.16ms
step:1501/2330 train_time:60249ms step_avg:40.14ms
step:1502/2330 train_time:60262ms step_avg:40.12ms
step:1503/2330 train_time:60273ms step_avg:40.10ms
step:1504/2330 train_time:60301ms step_avg:40.09ms
step:1505/2330 train_time:60323ms step_avg:40.08ms
step:1506/2330 train_time:60380ms step_avg:40.09ms
step:1507/2330 train_time:60402ms step_avg:40.08ms
step:1508/2330 train_time:60459ms step_avg:40.09ms
step:1509/2330 train_time:60482ms step_avg:40.08ms
step:1510/2330 train_time:60541ms step_avg:40.09ms
step:1511/2330 train_time:60568ms step_avg:40.08ms
step:1512/2330 train_time:60630ms step_avg:40.10ms
step:1513/2330 train_time:60654ms step_avg:40.09ms
step:1514/2330 train_time:60712ms step_avg:40.10ms
step:1515/2330 train_time:60735ms step_avg:40.09ms
step:1516/2330 train_time:60793ms step_avg:40.10ms
step:1517/2330 train_time:60815ms step_avg:40.09ms
step:1518/2330 train_time:60872ms step_avg:40.10ms
step:1519/2330 train_time:60894ms step_avg:40.09ms
step:1520/2330 train_time:60951ms step_avg:40.10ms
step:1521/2330 train_time:60973ms step_avg:40.09ms
step:1522/2330 train_time:61029ms step_avg:40.10ms
step:1523/2330 train_time:61052ms step_avg:40.09ms
step:1524/2330 train_time:61108ms step_avg:40.10ms
step:1525/2330 train_time:61131ms step_avg:40.09ms
step:1526/2330 train_time:61189ms step_avg:40.10ms
step:1527/2330 train_time:61214ms step_avg:40.09ms
step:1528/2330 train_time:61272ms step_avg:40.10ms
step:1529/2330 train_time:61296ms step_avg:40.09ms
step:1530/2330 train_time:61354ms step_avg:40.10ms
step:1531/2330 train_time:61375ms step_avg:40.09ms
step:1532/2330 train_time:61432ms step_avg:40.10ms
step:1533/2330 train_time:61455ms step_avg:40.09ms
step:1534/2330 train_time:61514ms step_avg:40.10ms
step:1535/2330 train_time:61537ms step_avg:40.09ms
step:1536/2330 train_time:61596ms step_avg:40.10ms
step:1537/2330 train_time:61618ms step_avg:40.09ms
step:1538/2330 train_time:61675ms step_avg:40.10ms
step:1539/2330 train_time:61698ms step_avg:40.09ms
step:1540/2330 train_time:61755ms step_avg:40.10ms
step:1541/2330 train_time:61778ms step_avg:40.09ms
step:1542/2330 train_time:61835ms step_avg:40.10ms
step:1543/2330 train_time:61858ms step_avg:40.09ms
step:1544/2330 train_time:61915ms step_avg:40.10ms
step:1545/2330 train_time:61938ms step_avg:40.09ms
step:1546/2330 train_time:61994ms step_avg:40.10ms
step:1547/2330 train_time:62016ms step_avg:40.09ms
step:1548/2330 train_time:62074ms step_avg:40.10ms
step:1549/2330 train_time:62096ms step_avg:40.09ms
step:1550/2330 train_time:62154ms step_avg:40.10ms
step:1551/2330 train_time:62176ms step_avg:40.09ms
step:1552/2330 train_time:62233ms step_avg:40.10ms
step:1553/2330 train_time:62257ms step_avg:40.09ms
step:1554/2330 train_time:62314ms step_avg:40.10ms
step:1555/2330 train_time:62337ms step_avg:40.09ms
step:1556/2330 train_time:62394ms step_avg:40.10ms
step:1557/2330 train_time:62416ms step_avg:40.09ms
step:1558/2330 train_time:62474ms step_avg:40.10ms
step:1559/2330 train_time:62497ms step_avg:40.09ms
step:1560/2330 train_time:62555ms step_avg:40.10ms
step:1561/2330 train_time:62578ms step_avg:40.09ms
step:1562/2330 train_time:62637ms step_avg:40.10ms
step:1563/2330 train_time:62659ms step_avg:40.09ms
step:1564/2330 train_time:62717ms step_avg:40.10ms
step:1565/2330 train_time:62740ms step_avg:40.09ms
step:1566/2330 train_time:62800ms step_avg:40.10ms
step:1567/2330 train_time:62822ms step_avg:40.09ms
step:1568/2330 train_time:62880ms step_avg:40.10ms
step:1569/2330 train_time:62903ms step_avg:40.09ms
step:1570/2330 train_time:62960ms step_avg:40.10ms
step:1571/2330 train_time:62984ms step_avg:40.09ms
step:1572/2330 train_time:63041ms step_avg:40.10ms
step:1573/2330 train_time:63065ms step_avg:40.09ms
step:1574/2330 train_time:63122ms step_avg:40.10ms
step:1575/2330 train_time:63145ms step_avg:40.09ms
step:1576/2330 train_time:63203ms step_avg:40.10ms
step:1577/2330 train_time:63226ms step_avg:40.09ms
step:1578/2330 train_time:63283ms step_avg:40.10ms
step:1579/2330 train_time:63306ms step_avg:40.09ms
step:1580/2330 train_time:63363ms step_avg:40.10ms
step:1581/2330 train_time:63387ms step_avg:40.09ms
step:1582/2330 train_time:63445ms step_avg:40.10ms
step:1583/2330 train_time:63468ms step_avg:40.09ms
step:1584/2330 train_time:63525ms step_avg:40.10ms
step:1585/2330 train_time:63549ms step_avg:40.09ms
step:1586/2330 train_time:63606ms step_avg:40.10ms
step:1587/2330 train_time:63630ms step_avg:40.09ms
step:1588/2330 train_time:63687ms step_avg:40.11ms
step:1589/2330 train_time:63711ms step_avg:40.10ms
step:1590/2330 train_time:63768ms step_avg:40.11ms
step:1591/2330 train_time:63793ms step_avg:40.10ms
step:1592/2330 train_time:63849ms step_avg:40.11ms
step:1593/2330 train_time:63872ms step_avg:40.10ms
step:1594/2330 train_time:63930ms step_avg:40.11ms
step:1595/2330 train_time:63954ms step_avg:40.10ms
step:1596/2330 train_time:64011ms step_avg:40.11ms
step:1597/2330 train_time:64035ms step_avg:40.10ms
step:1598/2330 train_time:64093ms step_avg:40.11ms
step:1599/2330 train_time:64116ms step_avg:40.10ms
step:1600/2330 train_time:64173ms step_avg:40.11ms
step:1601/2330 train_time:64196ms step_avg:40.10ms
step:1602/2330 train_time:64254ms step_avg:40.11ms
step:1603/2330 train_time:64276ms step_avg:40.10ms
step:1604/2330 train_time:64334ms step_avg:40.11ms
step:1605/2330 train_time:64356ms step_avg:40.10ms
step:1606/2330 train_time:64413ms step_avg:40.11ms
step:1607/2330 train_time:64436ms step_avg:40.10ms
step:1608/2330 train_time:64495ms step_avg:40.11ms
step:1609/2330 train_time:64517ms step_avg:40.10ms
step:1610/2330 train_time:64574ms step_avg:40.11ms
step:1611/2330 train_time:64597ms step_avg:40.10ms
step:1612/2330 train_time:64655ms step_avg:40.11ms
step:1613/2330 train_time:64677ms step_avg:40.10ms
step:1614/2330 train_time:64735ms step_avg:40.11ms
step:1615/2330 train_time:64757ms step_avg:40.10ms
step:1616/2330 train_time:64815ms step_avg:40.11ms
step:1617/2330 train_time:64838ms step_avg:40.10ms
step:1618/2330 train_time:64895ms step_avg:40.11ms
step:1619/2330 train_time:64918ms step_avg:40.10ms
step:1620/2330 train_time:64975ms step_avg:40.11ms
step:1621/2330 train_time:64998ms step_avg:40.10ms
step:1622/2330 train_time:65056ms step_avg:40.11ms
step:1623/2330 train_time:65078ms step_avg:40.10ms
step:1624/2330 train_time:65135ms step_avg:40.11ms
step:1625/2330 train_time:65158ms step_avg:40.10ms
step:1626/2330 train_time:65215ms step_avg:40.11ms
step:1627/2330 train_time:65237ms step_avg:40.10ms
step:1628/2330 train_time:65294ms step_avg:40.11ms
step:1629/2330 train_time:65317ms step_avg:40.10ms
step:1630/2330 train_time:65375ms step_avg:40.11ms
step:1631/2330 train_time:65398ms step_avg:40.10ms
step:1632/2330 train_time:65455ms step_avg:40.11ms
step:1633/2330 train_time:65477ms step_avg:40.10ms
step:1634/2330 train_time:65534ms step_avg:40.11ms
step:1635/2330 train_time:65557ms step_avg:40.10ms
step:1636/2330 train_time:65614ms step_avg:40.11ms
step:1637/2330 train_time:65637ms step_avg:40.10ms
step:1638/2330 train_time:65694ms step_avg:40.11ms
step:1639/2330 train_time:65717ms step_avg:40.10ms
step:1640/2330 train_time:65775ms step_avg:40.11ms
step:1641/2330 train_time:65797ms step_avg:40.10ms
step:1642/2330 train_time:65855ms step_avg:40.11ms
step:1643/2330 train_time:65877ms step_avg:40.10ms
step:1644/2330 train_time:65935ms step_avg:40.11ms
step:1645/2330 train_time:65957ms step_avg:40.10ms
step:1646/2330 train_time:66014ms step_avg:40.11ms
step:1647/2330 train_time:66037ms step_avg:40.10ms
step:1648/2330 train_time:66094ms step_avg:40.11ms
step:1649/2330 train_time:66117ms step_avg:40.10ms
step:1650/2330 train_time:66173ms step_avg:40.11ms
step:1651/2330 train_time:66196ms step_avg:40.09ms
step:1652/2330 train_time:66254ms step_avg:40.11ms
step:1653/2330 train_time:66276ms step_avg:40.09ms
step:1654/2330 train_time:66334ms step_avg:40.11ms
step:1655/2330 train_time:66357ms step_avg:40.09ms
step:1656/2330 train_time:66414ms step_avg:40.10ms
step:1657/2330 train_time:66436ms step_avg:40.09ms
step:1658/2330 train_time:66493ms step_avg:40.10ms
step:1659/2330 train_time:66516ms step_avg:40.09ms
step:1660/2330 train_time:66573ms step_avg:40.10ms
step:1661/2330 train_time:66595ms step_avg:40.09ms
step:1662/2330 train_time:66653ms step_avg:40.10ms
step:1663/2330 train_time:66675ms step_avg:40.09ms
step:1664/2330 train_time:66733ms step_avg:40.10ms
step:1665/2330 train_time:66756ms step_avg:40.09ms
step:1666/2330 train_time:66813ms step_avg:40.10ms
step:1667/2330 train_time:66836ms step_avg:40.09ms
step:1668/2330 train_time:66893ms step_avg:40.10ms
step:1669/2330 train_time:66915ms step_avg:40.09ms
step:1670/2330 train_time:66972ms step_avg:40.10ms
step:1671/2330 train_time:66994ms step_avg:40.09ms
step:1672/2330 train_time:67053ms step_avg:40.10ms
step:1673/2330 train_time:67075ms step_avg:40.09ms
step:1674/2330 train_time:67133ms step_avg:40.10ms
step:1675/2330 train_time:67156ms step_avg:40.09ms
step:1676/2330 train_time:67213ms step_avg:40.10ms
step:1677/2330 train_time:67235ms step_avg:40.09ms
step:1678/2330 train_time:67292ms step_avg:40.10ms
step:1679/2330 train_time:67315ms step_avg:40.09ms
step:1680/2330 train_time:67373ms step_avg:40.10ms
step:1681/2330 train_time:67396ms step_avg:40.09ms
step:1682/2330 train_time:67454ms step_avg:40.10ms
step:1683/2330 train_time:67476ms step_avg:40.09ms
step:1684/2330 train_time:67533ms step_avg:40.10ms
step:1685/2330 train_time:67556ms step_avg:40.09ms
step:1686/2330 train_time:67614ms step_avg:40.10ms
step:1687/2330 train_time:67636ms step_avg:40.09ms
step:1688/2330 train_time:67694ms step_avg:40.10ms
step:1689/2330 train_time:67716ms step_avg:40.09ms
step:1690/2330 train_time:67773ms step_avg:40.10ms
step:1691/2330 train_time:67796ms step_avg:40.09ms
step:1692/2330 train_time:67853ms step_avg:40.10ms
step:1693/2330 train_time:67875ms step_avg:40.09ms
step:1694/2330 train_time:67933ms step_avg:40.10ms
step:1695/2330 train_time:67956ms step_avg:40.09ms
step:1696/2330 train_time:68013ms step_avg:40.10ms
step:1697/2330 train_time:68035ms step_avg:40.09ms
step:1698/2330 train_time:68092ms step_avg:40.10ms
step:1699/2330 train_time:68115ms step_avg:40.09ms
step:1700/2330 train_time:68173ms step_avg:40.10ms
step:1701/2330 train_time:68195ms step_avg:40.09ms
step:1702/2330 train_time:68252ms step_avg:40.10ms
step:1703/2330 train_time:68275ms step_avg:40.09ms
step:1704/2330 train_time:68333ms step_avg:40.10ms
step:1705/2330 train_time:68355ms step_avg:40.09ms
step:1706/2330 train_time:68413ms step_avg:40.10ms
step:1707/2330 train_time:68436ms step_avg:40.09ms
step:1708/2330 train_time:68494ms step_avg:40.10ms
step:1709/2330 train_time:68516ms step_avg:40.09ms
step:1710/2330 train_time:68574ms step_avg:40.10ms
step:1711/2330 train_time:68596ms step_avg:40.09ms
step:1712/2330 train_time:68654ms step_avg:40.10ms
step:1713/2330 train_time:68676ms step_avg:40.09ms
step:1714/2330 train_time:68733ms step_avg:40.10ms
step:1715/2330 train_time:68755ms step_avg:40.09ms
step:1716/2330 train_time:68813ms step_avg:40.10ms
step:1717/2330 train_time:68836ms step_avg:40.09ms
step:1718/2330 train_time:68894ms step_avg:40.10ms
step:1719/2330 train_time:68916ms step_avg:40.09ms
step:1720/2330 train_time:68974ms step_avg:40.10ms
step:1721/2330 train_time:68996ms step_avg:40.09ms
step:1722/2330 train_time:69054ms step_avg:40.10ms
step:1723/2330 train_time:69076ms step_avg:40.09ms
step:1724/2330 train_time:69134ms step_avg:40.10ms
step:1725/2330 train_time:69156ms step_avg:40.09ms
step:1726/2330 train_time:69213ms step_avg:40.10ms
step:1727/2330 train_time:69235ms step_avg:40.09ms
step:1728/2330 train_time:69292ms step_avg:40.10ms
step:1729/2330 train_time:69314ms step_avg:40.09ms
step:1730/2330 train_time:69372ms step_avg:40.10ms
step:1731/2330 train_time:69394ms step_avg:40.09ms
step:1732/2330 train_time:69452ms step_avg:40.10ms
step:1733/2330 train_time:69474ms step_avg:40.09ms
step:1734/2330 train_time:69532ms step_avg:40.10ms
step:1735/2330 train_time:69555ms step_avg:40.09ms
step:1736/2330 train_time:69612ms step_avg:40.10ms
step:1737/2330 train_time:69635ms step_avg:40.09ms
step:1738/2330 train_time:69692ms step_avg:40.10ms
step:1739/2330 train_time:69715ms step_avg:40.09ms
step:1740/2330 train_time:69772ms step_avg:40.10ms
step:1741/2330 train_time:69794ms step_avg:40.09ms
step:1742/2330 train_time:69851ms step_avg:40.10ms
step:1743/2330 train_time:69874ms step_avg:40.09ms
step:1744/2330 train_time:69932ms step_avg:40.10ms
step:1745/2330 train_time:69954ms step_avg:40.09ms
step:1746/2330 train_time:70012ms step_avg:40.10ms
step:1747/2330 train_time:70034ms step_avg:40.09ms
step:1748/2330 train_time:70092ms step_avg:40.10ms
step:1749/2330 train_time:70114ms step_avg:40.09ms
step:1750/2330 train_time:70172ms step_avg:40.10ms
step:1750/2330 val_loss:5.2266 train_time:70268ms step_avg:40.15ms
step:1751/2330 train_time:70282ms step_avg:40.14ms
step:1752/2330 train_time:70295ms step_avg:40.12ms
step:1753/2330 train_time:70306ms step_avg:40.11ms
step:1754/2330 train_time:70330ms step_avg:40.10ms
step:1755/2330 train_time:70353ms step_avg:40.09ms
step:1756/2330 train_time:70409ms step_avg:40.10ms
step:1757/2330 train_time:70431ms step_avg:40.09ms
step:1758/2330 train_time:70488ms step_avg:40.10ms
step:1759/2330 train_time:70510ms step_avg:40.09ms
step:1760/2330 train_time:70570ms step_avg:40.10ms
step:1761/2330 train_time:70598ms step_avg:40.09ms
step:1762/2330 train_time:70659ms step_avg:40.10ms
step:1763/2330 train_time:70683ms step_avg:40.09ms
step:1764/2330 train_time:70741ms step_avg:40.10ms
step:1765/2330 train_time:70764ms step_avg:40.09ms
step:1766/2330 train_time:70821ms step_avg:40.10ms
step:1767/2330 train_time:70844ms step_avg:40.09ms
step:1768/2330 train_time:70901ms step_avg:40.10ms
step:1769/2330 train_time:70923ms step_avg:40.09ms
step:1770/2330 train_time:70980ms step_avg:40.10ms
step:1771/2330 train_time:71002ms step_avg:40.09ms
step:1772/2330 train_time:71058ms step_avg:40.10ms
step:1773/2330 train_time:71080ms step_avg:40.09ms
step:1774/2330 train_time:71136ms step_avg:40.10ms
step:1775/2330 train_time:71161ms step_avg:40.09ms
step:1776/2330 train_time:71222ms step_avg:40.10ms
step:1777/2330 train_time:71246ms step_avg:40.09ms
step:1778/2330 train_time:71304ms step_avg:40.10ms
step:1779/2330 train_time:71327ms step_avg:40.09ms
step:1780/2330 train_time:71384ms step_avg:40.10ms
step:1781/2330 train_time:71407ms step_avg:40.09ms
step:1782/2330 train_time:71464ms step_avg:40.10ms
step:1783/2330 train_time:71486ms step_avg:40.09ms
step:1784/2330 train_time:71545ms step_avg:40.10ms
step:1785/2330 train_time:71570ms step_avg:40.10ms
step:1786/2330 train_time:71629ms step_avg:40.11ms
step:1787/2330 train_time:71653ms step_avg:40.10ms
step:1788/2330 train_time:71710ms step_avg:40.11ms
step:1789/2330 train_time:71734ms step_avg:40.10ms
step:1790/2330 train_time:71791ms step_avg:40.11ms
step:1791/2330 train_time:71815ms step_avg:40.10ms
step:1792/2330 train_time:71872ms step_avg:40.11ms
step:1793/2330 train_time:71895ms step_avg:40.10ms
step:1794/2330 train_time:71951ms step_avg:40.11ms
step:1795/2330 train_time:71974ms step_avg:40.10ms
step:1796/2330 train_time:72031ms step_avg:40.11ms
step:1797/2330 train_time:72053ms step_avg:40.10ms
step:1798/2330 train_time:72111ms step_avg:40.11ms
step:1799/2330 train_time:72134ms step_avg:40.10ms
step:1800/2330 train_time:72191ms step_avg:40.11ms
step:1801/2330 train_time:72215ms step_avg:40.10ms
step:1802/2330 train_time:72272ms step_avg:40.11ms
step:1803/2330 train_time:72296ms step_avg:40.10ms
step:1804/2330 train_time:72353ms step_avg:40.11ms
step:1805/2330 train_time:72377ms step_avg:40.10ms
step:1806/2330 train_time:72434ms step_avg:40.11ms
step:1807/2330 train_time:72457ms step_avg:40.10ms
step:1808/2330 train_time:72514ms step_avg:40.11ms
step:1809/2330 train_time:72538ms step_avg:40.10ms
step:1810/2330 train_time:72596ms step_avg:40.11ms
step:1811/2330 train_time:72619ms step_avg:40.10ms
step:1812/2330 train_time:72677ms step_avg:40.11ms
step:1813/2330 train_time:72701ms step_avg:40.10ms
step:1814/2330 train_time:72758ms step_avg:40.11ms
step:1815/2330 train_time:72781ms step_avg:40.10ms
step:1816/2330 train_time:72838ms step_avg:40.11ms
step:1817/2330 train_time:72860ms step_avg:40.10ms
step:1818/2330 train_time:72918ms step_avg:40.11ms
step:1819/2330 train_time:72940ms step_avg:40.10ms
step:1820/2330 train_time:72998ms step_avg:40.11ms
step:1821/2330 train_time:73020ms step_avg:40.10ms
step:1822/2330 train_time:73078ms step_avg:40.11ms
step:1823/2330 train_time:73100ms step_avg:40.10ms
step:1824/2330 train_time:73158ms step_avg:40.11ms
step:1825/2330 train_time:73180ms step_avg:40.10ms
step:1826/2330 train_time:73238ms step_avg:40.11ms
step:1827/2330 train_time:73260ms step_avg:40.10ms
step:1828/2330 train_time:73318ms step_avg:40.11ms
step:1829/2330 train_time:73340ms step_avg:40.10ms
step:1830/2330 train_time:73398ms step_avg:40.11ms
step:1831/2330 train_time:73420ms step_avg:40.10ms
step:1832/2330 train_time:73478ms step_avg:40.11ms
step:1833/2330 train_time:73501ms step_avg:40.10ms
step:1834/2330 train_time:73558ms step_avg:40.11ms
step:1835/2330 train_time:73581ms step_avg:40.10ms
step:1836/2330 train_time:73637ms step_avg:40.11ms
step:1837/2330 train_time:73661ms step_avg:40.10ms
step:1838/2330 train_time:73719ms step_avg:40.11ms
step:1839/2330 train_time:73742ms step_avg:40.10ms
step:1840/2330 train_time:73800ms step_avg:40.11ms
step:1841/2330 train_time:73822ms step_avg:40.10ms
step:1842/2330 train_time:73880ms step_avg:40.11ms
step:1843/2330 train_time:73902ms step_avg:40.10ms
step:1844/2330 train_time:73960ms step_avg:40.11ms
step:1845/2330 train_time:73982ms step_avg:40.10ms
step:1846/2330 train_time:74039ms step_avg:40.11ms
step:1847/2330 train_time:74062ms step_avg:40.10ms
step:1848/2330 train_time:74120ms step_avg:40.11ms
step:1849/2330 train_time:74142ms step_avg:40.10ms
step:1850/2330 train_time:74200ms step_avg:40.11ms
step:1851/2330 train_time:74222ms step_avg:40.10ms
step:1852/2330 train_time:74279ms step_avg:40.11ms
step:1853/2330 train_time:74302ms step_avg:40.10ms
step:1854/2330 train_time:74360ms step_avg:40.11ms
step:1855/2330 train_time:74382ms step_avg:40.10ms
step:1856/2330 train_time:74439ms step_avg:40.11ms
step:1857/2330 train_time:74462ms step_avg:40.10ms
step:1858/2330 train_time:74519ms step_avg:40.11ms
step:1859/2330 train_time:74542ms step_avg:40.10ms
step:1860/2330 train_time:74599ms step_avg:40.11ms
step:1861/2330 train_time:74621ms step_avg:40.10ms
step:1862/2330 train_time:74680ms step_avg:40.11ms
step:1863/2330 train_time:74702ms step_avg:40.10ms
step:1864/2330 train_time:74759ms step_avg:40.11ms
step:1865/2330 train_time:74782ms step_avg:40.10ms
step:1866/2330 train_time:74839ms step_avg:40.11ms
step:1867/2330 train_time:74862ms step_avg:40.10ms
step:1868/2330 train_time:74920ms step_avg:40.11ms
step:1869/2330 train_time:74942ms step_avg:40.10ms
step:1870/2330 train_time:75000ms step_avg:40.11ms
step:1871/2330 train_time:75022ms step_avg:40.10ms
step:1872/2330 train_time:75080ms step_avg:40.11ms
step:1873/2330 train_time:75103ms step_avg:40.10ms
step:1874/2330 train_time:75161ms step_avg:40.11ms
step:1875/2330 train_time:75183ms step_avg:40.10ms
step:1876/2330 train_time:75241ms step_avg:40.11ms
step:1877/2330 train_time:75264ms step_avg:40.10ms
step:1878/2330 train_time:75321ms step_avg:40.11ms
step:1879/2330 train_time:75343ms step_avg:40.10ms
step:1880/2330 train_time:75401ms step_avg:40.11ms
step:1881/2330 train_time:75423ms step_avg:40.10ms
step:1882/2330 train_time:75480ms step_avg:40.11ms
step:1883/2330 train_time:75503ms step_avg:40.10ms
step:1884/2330 train_time:75560ms step_avg:40.11ms
step:1885/2330 train_time:75583ms step_avg:40.10ms
step:1886/2330 train_time:75641ms step_avg:40.11ms
step:1887/2330 train_time:75663ms step_avg:40.10ms
step:1888/2330 train_time:75720ms step_avg:40.11ms
step:1889/2330 train_time:75743ms step_avg:40.10ms
step:1890/2330 train_time:75801ms step_avg:40.11ms
step:1891/2330 train_time:75823ms step_avg:40.10ms
step:1892/2330 train_time:75880ms step_avg:40.11ms
step:1893/2330 train_time:75903ms step_avg:40.10ms
step:1894/2330 train_time:75960ms step_avg:40.11ms
step:1895/2330 train_time:75982ms step_avg:40.10ms
step:1896/2330 train_time:76039ms step_avg:40.11ms
step:1897/2330 train_time:76062ms step_avg:40.10ms
step:1898/2330 train_time:76120ms step_avg:40.11ms
step:1899/2330 train_time:76142ms step_avg:40.10ms
step:1900/2330 train_time:76201ms step_avg:40.11ms
step:1901/2330 train_time:76223ms step_avg:40.10ms
step:1902/2330 train_time:76280ms step_avg:40.11ms
step:1903/2330 train_time:76303ms step_avg:40.10ms
step:1904/2330 train_time:76361ms step_avg:40.11ms
step:1905/2330 train_time:76383ms step_avg:40.10ms
step:1906/2330 train_time:76441ms step_avg:40.11ms
step:1907/2330 train_time:76463ms step_avg:40.10ms
step:1908/2330 train_time:76521ms step_avg:40.11ms
step:1909/2330 train_time:76543ms step_avg:40.10ms
step:1910/2330 train_time:76600ms step_avg:40.10ms
step:1911/2330 train_time:76623ms step_avg:40.10ms
step:1912/2330 train_time:76680ms step_avg:40.10ms
step:1913/2330 train_time:76702ms step_avg:40.10ms
step:1914/2330 train_time:76761ms step_avg:40.10ms
step:1915/2330 train_time:76783ms step_avg:40.10ms
step:1916/2330 train_time:76840ms step_avg:40.10ms
step:1917/2330 train_time:76862ms step_avg:40.10ms
step:1918/2330 train_time:76919ms step_avg:40.10ms
step:1919/2330 train_time:76942ms step_avg:40.09ms
step:1920/2330 train_time:77000ms step_avg:40.10ms
step:1921/2330 train_time:77022ms step_avg:40.09ms
step:1922/2330 train_time:77079ms step_avg:40.10ms
step:1923/2330 train_time:77101ms step_avg:40.09ms
step:1924/2330 train_time:77159ms step_avg:40.10ms
step:1925/2330 train_time:77182ms step_avg:40.09ms
step:1926/2330 train_time:77240ms step_avg:40.10ms
step:1927/2330 train_time:77262ms step_avg:40.09ms
step:1928/2330 train_time:77319ms step_avg:40.10ms
step:1929/2330 train_time:77342ms step_avg:40.09ms
step:1930/2330 train_time:77399ms step_avg:40.10ms
step:1931/2330 train_time:77422ms step_avg:40.09ms
step:1932/2330 train_time:77480ms step_avg:40.10ms
step:1933/2330 train_time:77502ms step_avg:40.09ms
step:1934/2330 train_time:77559ms step_avg:40.10ms
step:1935/2330 train_time:77581ms step_avg:40.09ms
step:1936/2330 train_time:77639ms step_avg:40.10ms
step:1937/2330 train_time:77662ms step_avg:40.09ms
step:1938/2330 train_time:77719ms step_avg:40.10ms
step:1939/2330 train_time:77742ms step_avg:40.09ms
step:1940/2330 train_time:77799ms step_avg:40.10ms
step:1941/2330 train_time:77821ms step_avg:40.09ms
step:1942/2330 train_time:77878ms step_avg:40.10ms
step:1943/2330 train_time:77902ms step_avg:40.09ms
step:1944/2330 train_time:77959ms step_avg:40.10ms
step:1945/2330 train_time:77981ms step_avg:40.09ms
step:1946/2330 train_time:78038ms step_avg:40.10ms
step:1947/2330 train_time:78061ms step_avg:40.09ms
step:1948/2330 train_time:78119ms step_avg:40.10ms
step:1949/2330 train_time:78141ms step_avg:40.09ms
step:1950/2330 train_time:78198ms step_avg:40.10ms
step:1951/2330 train_time:78220ms step_avg:40.09ms
step:1952/2330 train_time:78277ms step_avg:40.10ms
step:1953/2330 train_time:78300ms step_avg:40.09ms
step:1954/2330 train_time:78358ms step_avg:40.10ms
step:1955/2330 train_time:78380ms step_avg:40.09ms
step:1956/2330 train_time:78437ms step_avg:40.10ms
step:1957/2330 train_time:78460ms step_avg:40.09ms
step:1958/2330 train_time:78518ms step_avg:40.10ms
step:1959/2330 train_time:78540ms step_avg:40.09ms
step:1960/2330 train_time:78597ms step_avg:40.10ms
step:1961/2330 train_time:78620ms step_avg:40.09ms
step:1962/2330 train_time:78677ms step_avg:40.10ms
step:1963/2330 train_time:78700ms step_avg:40.09ms
step:1964/2330 train_time:78756ms step_avg:40.10ms
step:1965/2330 train_time:78780ms step_avg:40.09ms
step:1966/2330 train_time:78837ms step_avg:40.10ms
step:1967/2330 train_time:78860ms step_avg:40.09ms
step:1968/2330 train_time:78917ms step_avg:40.10ms
step:1969/2330 train_time:78940ms step_avg:40.09ms
step:1970/2330 train_time:78997ms step_avg:40.10ms
step:1971/2330 train_time:79020ms step_avg:40.09ms
step:1972/2330 train_time:79078ms step_avg:40.10ms
step:1973/2330 train_time:79100ms step_avg:40.09ms
step:1974/2330 train_time:79158ms step_avg:40.10ms
step:1975/2330 train_time:79180ms step_avg:40.09ms
step:1976/2330 train_time:79238ms step_avg:40.10ms
step:1977/2330 train_time:79261ms step_avg:40.09ms
step:1978/2330 train_time:79319ms step_avg:40.10ms
step:1979/2330 train_time:79342ms step_avg:40.09ms
step:1980/2330 train_time:79399ms step_avg:40.10ms
step:1981/2330 train_time:79421ms step_avg:40.09ms
step:1982/2330 train_time:79478ms step_avg:40.10ms
step:1983/2330 train_time:79500ms step_avg:40.09ms
step:1984/2330 train_time:79557ms step_avg:40.10ms
step:1985/2330 train_time:79581ms step_avg:40.09ms
step:1986/2330 train_time:79639ms step_avg:40.10ms
step:1987/2330 train_time:79662ms step_avg:40.09ms
step:1988/2330 train_time:79719ms step_avg:40.10ms
step:1989/2330 train_time:79741ms step_avg:40.09ms
step:1990/2330 train_time:79800ms step_avg:40.10ms
step:1991/2330 train_time:79822ms step_avg:40.09ms
step:1992/2330 train_time:79879ms step_avg:40.10ms
step:1993/2330 train_time:79901ms step_avg:40.09ms
step:1994/2330 train_time:79959ms step_avg:40.10ms
step:1995/2330 train_time:79981ms step_avg:40.09ms
step:1996/2330 train_time:80038ms step_avg:40.10ms
step:1997/2330 train_time:80061ms step_avg:40.09ms
step:1998/2330 train_time:80118ms step_avg:40.10ms
step:1999/2330 train_time:80141ms step_avg:40.09ms
step:2000/2330 train_time:80198ms step_avg:40.10ms
step:2000/2330 val_loss:5.1557 train_time:80295ms step_avg:40.15ms
step:2001/2330 train_time:80309ms step_avg:40.13ms
step:2002/2330 train_time:80321ms step_avg:40.12ms
step:2003/2330 train_time:80332ms step_avg:40.11ms
step:2004/2330 train_time:80359ms step_avg:40.10ms
step:2005/2330 train_time:80381ms step_avg:40.09ms
step:2006/2330 train_time:80437ms step_avg:40.10ms
step:2007/2330 train_time:80460ms step_avg:40.09ms
step:2008/2330 train_time:80517ms step_avg:40.10ms
step:2009/2330 train_time:80541ms step_avg:40.09ms
step:2010/2330 train_time:80600ms step_avg:40.10ms
step:2011/2330 train_time:80628ms step_avg:40.09ms
step:2012/2330 train_time:80690ms step_avg:40.10ms
step:2013/2330 train_time:80712ms step_avg:40.10ms
step:2014/2330 train_time:80772ms step_avg:40.11ms
step:2015/2330 train_time:80794ms step_avg:40.10ms
step:2016/2330 train_time:80851ms step_avg:40.10ms
step:2017/2330 train_time:80874ms step_avg:40.10ms
step:2018/2330 train_time:80931ms step_avg:40.10ms
step:2019/2330 train_time:80954ms step_avg:40.10ms
step:2020/2330 train_time:81010ms step_avg:40.10ms
step:2021/2330 train_time:81033ms step_avg:40.10ms
step:2022/2330 train_time:81089ms step_avg:40.10ms
step:2023/2330 train_time:81111ms step_avg:40.09ms
step:2024/2330 train_time:81167ms step_avg:40.10ms
step:2025/2330 train_time:81189ms step_avg:40.09ms
step:2026/2330 train_time:81250ms step_avg:40.10ms
step:2027/2330 train_time:81273ms step_avg:40.10ms
step:2028/2330 train_time:81331ms step_avg:40.10ms
step:2029/2330 train_time:81353ms step_avg:40.10ms
step:2030/2330 train_time:81410ms step_avg:40.10ms
step:2031/2330 train_time:81433ms step_avg:40.09ms
step:2032/2330 train_time:81490ms step_avg:40.10ms
step:2033/2330 train_time:81513ms step_avg:40.09ms
step:2034/2330 train_time:81571ms step_avg:40.10ms
step:2035/2330 train_time:81594ms step_avg:40.10ms
step:2036/2330 train_time:81653ms step_avg:40.10ms
step:2037/2330 train_time:81676ms step_avg:40.10ms
step:2038/2330 train_time:81734ms step_avg:40.11ms
step:2039/2330 train_time:81758ms step_avg:40.10ms
step:2040/2330 train_time:81816ms step_avg:40.11ms
step:2041/2330 train_time:81839ms step_avg:40.10ms
step:2042/2330 train_time:81895ms step_avg:40.11ms
step:2043/2330 train_time:81918ms step_avg:40.10ms
step:2044/2330 train_time:81975ms step_avg:40.11ms
step:2045/2330 train_time:81998ms step_avg:40.10ms
step:2046/2330 train_time:82055ms step_avg:40.11ms
step:2047/2330 train_time:82078ms step_avg:40.10ms
step:2048/2330 train_time:82135ms step_avg:40.10ms
step:2049/2330 train_time:82158ms step_avg:40.10ms
step:2050/2330 train_time:82215ms step_avg:40.10ms
step:2051/2330 train_time:82239ms step_avg:40.10ms
step:2052/2330 train_time:82296ms step_avg:40.11ms
step:2053/2330 train_time:82319ms step_avg:40.10ms
step:2054/2330 train_time:82376ms step_avg:40.10ms
step:2055/2330 train_time:82398ms step_avg:40.10ms
step:2056/2330 train_time:82455ms step_avg:40.10ms
step:2057/2330 train_time:82478ms step_avg:40.10ms
step:2058/2330 train_time:82536ms step_avg:40.10ms
step:2059/2330 train_time:82559ms step_avg:40.10ms
step:2060/2330 train_time:82618ms step_avg:40.11ms
step:2061/2330 train_time:82642ms step_avg:40.10ms
step:2062/2330 train_time:82700ms step_avg:40.11ms
step:2063/2330 train_time:82723ms step_avg:40.10ms
step:2064/2330 train_time:82780ms step_avg:40.11ms
step:2065/2330 train_time:82803ms step_avg:40.10ms
step:2066/2330 train_time:82860ms step_avg:40.11ms
step:2067/2330 train_time:82884ms step_avg:40.10ms
step:2068/2330 train_time:82940ms step_avg:40.11ms
step:2069/2330 train_time:82964ms step_avg:40.10ms
step:2070/2330 train_time:83020ms step_avg:40.11ms
step:2071/2330 train_time:83044ms step_avg:40.10ms
step:2072/2330 train_time:83102ms step_avg:40.11ms
step:2073/2330 train_time:83126ms step_avg:40.10ms
step:2074/2330 train_time:83182ms step_avg:40.11ms
step:2075/2330 train_time:83205ms step_avg:40.10ms
step:2076/2330 train_time:83262ms step_avg:40.11ms
step:2077/2330 train_time:83285ms step_avg:40.10ms
step:2078/2330 train_time:83341ms step_avg:40.11ms
step:2079/2330 train_time:83364ms step_avg:40.10ms
step:2080/2330 train_time:83421ms step_avg:40.11ms
step:2081/2330 train_time:83445ms step_avg:40.10ms
step:2082/2330 train_time:83501ms step_avg:40.11ms
step:2083/2330 train_time:83525ms step_avg:40.10ms
step:2084/2330 train_time:83581ms step_avg:40.11ms
step:2085/2330 train_time:83605ms step_avg:40.10ms
step:2086/2330 train_time:83662ms step_avg:40.11ms
step:2087/2330 train_time:83686ms step_avg:40.10ms
step:2088/2330 train_time:83743ms step_avg:40.11ms
step:2089/2330 train_time:83767ms step_avg:40.10ms
step:2090/2330 train_time:83824ms step_avg:40.11ms
step:2091/2330 train_time:83846ms step_avg:40.10ms
step:2092/2330 train_time:83904ms step_avg:40.11ms
step:2093/2330 train_time:83927ms step_avg:40.10ms
step:2094/2330 train_time:83985ms step_avg:40.11ms
step:2095/2330 train_time:84007ms step_avg:40.10ms
step:2096/2330 train_time:84065ms step_avg:40.11ms
step:2097/2330 train_time:84087ms step_avg:40.10ms
step:2098/2330 train_time:84144ms step_avg:40.11ms
step:2099/2330 train_time:84167ms step_avg:40.10ms
step:2100/2330 train_time:84225ms step_avg:40.11ms
step:2101/2330 train_time:84248ms step_avg:40.10ms
step:2102/2330 train_time:84305ms step_avg:40.11ms
step:2103/2330 train_time:84328ms step_avg:40.10ms
step:2104/2330 train_time:84386ms step_avg:40.11ms
step:2105/2330 train_time:84409ms step_avg:40.10ms
step:2106/2330 train_time:84467ms step_avg:40.11ms
step:2107/2330 train_time:84489ms step_avg:40.10ms
step:2108/2330 train_time:84547ms step_avg:40.11ms
step:2109/2330 train_time:84569ms step_avg:40.10ms
step:2110/2330 train_time:84627ms step_avg:40.11ms
step:2111/2330 train_time:84649ms step_avg:40.10ms
step:2112/2330 train_time:84706ms step_avg:40.11ms
step:2113/2330 train_time:84730ms step_avg:40.10ms
step:2114/2330 train_time:84787ms step_avg:40.11ms
step:2115/2330 train_time:84810ms step_avg:40.10ms
step:2116/2330 train_time:84868ms step_avg:40.11ms
step:2117/2330 train_time:84891ms step_avg:40.10ms
step:2118/2330 train_time:84949ms step_avg:40.11ms
step:2119/2330 train_time:84971ms step_avg:40.10ms
step:2120/2330 train_time:85029ms step_avg:40.11ms
step:2121/2330 train_time:85051ms step_avg:40.10ms
step:2122/2330 train_time:85109ms step_avg:40.11ms
step:2123/2330 train_time:85132ms step_avg:40.10ms
step:2124/2330 train_time:85190ms step_avg:40.11ms
step:2125/2330 train_time:85212ms step_avg:40.10ms
step:2126/2330 train_time:85270ms step_avg:40.11ms
step:2127/2330 train_time:85292ms step_avg:40.10ms
step:2128/2330 train_time:85349ms step_avg:40.11ms
step:2129/2330 train_time:85372ms step_avg:40.10ms
step:2130/2330 train_time:85430ms step_avg:40.11ms
step:2131/2330 train_time:85452ms step_avg:40.10ms
step:2132/2330 train_time:85510ms step_avg:40.11ms
step:2133/2330 train_time:85532ms step_avg:40.10ms
step:2134/2330 train_time:85589ms step_avg:40.11ms
step:2135/2330 train_time:85612ms step_avg:40.10ms
step:2136/2330 train_time:85669ms step_avg:40.11ms
step:2137/2330 train_time:85692ms step_avg:40.10ms
step:2138/2330 train_time:85750ms step_avg:40.11ms
step:2139/2330 train_time:85772ms step_avg:40.10ms
step:2140/2330 train_time:85829ms step_avg:40.11ms
step:2141/2330 train_time:85851ms step_avg:40.10ms
step:2142/2330 train_time:85908ms step_avg:40.11ms
step:2143/2330 train_time:85931ms step_avg:40.10ms
step:2144/2330 train_time:85988ms step_avg:40.11ms
step:2145/2330 train_time:86011ms step_avg:40.10ms
step:2146/2330 train_time:86067ms step_avg:40.11ms
step:2147/2330 train_time:86090ms step_avg:40.10ms
step:2148/2330 train_time:86147ms step_avg:40.11ms
step:2149/2330 train_time:86170ms step_avg:40.10ms
step:2150/2330 train_time:86227ms step_avg:40.11ms
step:2151/2330 train_time:86250ms step_avg:40.10ms
step:2152/2330 train_time:86307ms step_avg:40.11ms
step:2153/2330 train_time:86329ms step_avg:40.10ms
step:2154/2330 train_time:86387ms step_avg:40.11ms
step:2155/2330 train_time:86410ms step_avg:40.10ms
step:2156/2330 train_time:86467ms step_avg:40.11ms
step:2157/2330 train_time:86489ms step_avg:40.10ms
step:2158/2330 train_time:86547ms step_avg:40.11ms
step:2159/2330 train_time:86569ms step_avg:40.10ms
step:2160/2330 train_time:86627ms step_avg:40.10ms
step:2161/2330 train_time:86649ms step_avg:40.10ms
step:2162/2330 train_time:86706ms step_avg:40.10ms
step:2163/2330 train_time:86730ms step_avg:40.10ms
step:2164/2330 train_time:86788ms step_avg:40.11ms
step:2165/2330 train_time:86811ms step_avg:40.10ms
step:2166/2330 train_time:86868ms step_avg:40.11ms
step:2167/2330 train_time:86890ms step_avg:40.10ms
step:2168/2330 train_time:86948ms step_avg:40.11ms
step:2169/2330 train_time:86971ms step_avg:40.10ms
step:2170/2330 train_time:87028ms step_avg:40.10ms
step:2171/2330 train_time:87050ms step_avg:40.10ms
step:2172/2330 train_time:87108ms step_avg:40.10ms
step:2173/2330 train_time:87131ms step_avg:40.10ms
step:2174/2330 train_time:87188ms step_avg:40.10ms
step:2175/2330 train_time:87210ms step_avg:40.10ms
step:2176/2330 train_time:87268ms step_avg:40.10ms
step:2177/2330 train_time:87290ms step_avg:40.10ms
step:2178/2330 train_time:87347ms step_avg:40.10ms
step:2179/2330 train_time:87370ms step_avg:40.10ms
step:2180/2330 train_time:87427ms step_avg:40.10ms
step:2181/2330 train_time:87450ms step_avg:40.10ms
step:2182/2330 train_time:87508ms step_avg:40.10ms
step:2183/2330 train_time:87530ms step_avg:40.10ms
step:2184/2330 train_time:87587ms step_avg:40.10ms
step:2185/2330 train_time:87610ms step_avg:40.10ms
step:2186/2330 train_time:87668ms step_avg:40.10ms
step:2187/2330 train_time:87690ms step_avg:40.10ms
step:2188/2330 train_time:87748ms step_avg:40.10ms
step:2189/2330 train_time:87771ms step_avg:40.10ms
step:2190/2330 train_time:87828ms step_avg:40.10ms
step:2191/2330 train_time:87851ms step_avg:40.10ms
step:2192/2330 train_time:87909ms step_avg:40.10ms
step:2193/2330 train_time:87931ms step_avg:40.10ms
step:2194/2330 train_time:87988ms step_avg:40.10ms
step:2195/2330 train_time:88010ms step_avg:40.10ms
step:2196/2330 train_time:88068ms step_avg:40.10ms
step:2197/2330 train_time:88090ms step_avg:40.10ms
step:2198/2330 train_time:88148ms step_avg:40.10ms
step:2199/2330 train_time:88170ms step_avg:40.10ms
step:2200/2330 train_time:88227ms step_avg:40.10ms
step:2201/2330 train_time:88250ms step_avg:40.10ms
step:2202/2330 train_time:88307ms step_avg:40.10ms
step:2203/2330 train_time:88329ms step_avg:40.10ms
step:2204/2330 train_time:88387ms step_avg:40.10ms
step:2205/2330 train_time:88410ms step_avg:40.10ms
step:2206/2330 train_time:88468ms step_avg:40.10ms
step:2207/2330 train_time:88490ms step_avg:40.09ms
step:2208/2330 train_time:88547ms step_avg:40.10ms
step:2209/2330 train_time:88569ms step_avg:40.09ms
step:2210/2330 train_time:88626ms step_avg:40.10ms
step:2211/2330 train_time:88649ms step_avg:40.09ms
step:2212/2330 train_time:88706ms step_avg:40.10ms
step:2213/2330 train_time:88729ms step_avg:40.09ms
step:2214/2330 train_time:88787ms step_avg:40.10ms
step:2215/2330 train_time:88810ms step_avg:40.09ms
step:2216/2330 train_time:88867ms step_avg:40.10ms
step:2217/2330 train_time:88890ms step_avg:40.09ms
step:2218/2330 train_time:88947ms step_avg:40.10ms
step:2219/2330 train_time:88969ms step_avg:40.09ms
step:2220/2330 train_time:89027ms step_avg:40.10ms
step:2221/2330 train_time:89050ms step_avg:40.09ms
step:2222/2330 train_time:89107ms step_avg:40.10ms
step:2223/2330 train_time:89130ms step_avg:40.09ms
step:2224/2330 train_time:89187ms step_avg:40.10ms
step:2225/2330 train_time:89209ms step_avg:40.09ms
step:2226/2330 train_time:89267ms step_avg:40.10ms
step:2227/2330 train_time:89289ms step_avg:40.09ms
step:2228/2330 train_time:89347ms step_avg:40.10ms
step:2229/2330 train_time:89369ms step_avg:40.09ms
step:2230/2330 train_time:89426ms step_avg:40.10ms
step:2231/2330 train_time:89448ms step_avg:40.09ms
step:2232/2330 train_time:89507ms step_avg:40.10ms
step:2233/2330 train_time:89529ms step_avg:40.09ms
step:2234/2330 train_time:89587ms step_avg:40.10ms
step:2235/2330 train_time:89610ms step_avg:40.09ms
step:2236/2330 train_time:89667ms step_avg:40.10ms
step:2237/2330 train_time:89689ms step_avg:40.09ms
step:2238/2330 train_time:89747ms step_avg:40.10ms
step:2239/2330 train_time:89770ms step_avg:40.09ms
step:2240/2330 train_time:89828ms step_avg:40.10ms
step:2241/2330 train_time:89850ms step_avg:40.09ms
step:2242/2330 train_time:89907ms step_avg:40.10ms
step:2243/2330 train_time:89930ms step_avg:40.09ms
step:2244/2330 train_time:89987ms step_avg:40.10ms
step:2245/2330 train_time:90010ms step_avg:40.09ms
step:2246/2330 train_time:90067ms step_avg:40.10ms
step:2247/2330 train_time:90090ms step_avg:40.09ms
step:2248/2330 train_time:90147ms step_avg:40.10ms
step:2249/2330 train_time:90170ms step_avg:40.09ms
step:2250/2330 train_time:90227ms step_avg:40.10ms
step:2250/2330 val_loss:5.1280 train_time:90324ms step_avg:40.14ms
step:2251/2330 train_time:90337ms step_avg:40.13ms
step:2252/2330 train_time:90349ms step_avg:40.12ms
step:2253/2330 train_time:90359ms step_avg:40.11ms
step:2254/2330 train_time:90389ms step_avg:40.10ms
step:2255/2330 train_time:90410ms step_avg:40.09ms
step:2256/2330 train_time:90467ms step_avg:40.10ms
step:2257/2330 train_time:90488ms step_avg:40.09ms
step:2258/2330 train_time:90545ms step_avg:40.10ms
step:2259/2330 train_time:90567ms step_avg:40.09ms
step:2260/2330 train_time:90626ms step_avg:40.10ms
step:2261/2330 train_time:90653ms step_avg:40.09ms
step:2262/2330 train_time:90715ms step_avg:40.10ms
step:2263/2330 train_time:90740ms step_avg:40.10ms
step:2264/2330 train_time:90798ms step_avg:40.11ms
step:2265/2330 train_time:90823ms step_avg:40.10ms
step:2266/2330 train_time:90879ms step_avg:40.11ms
step:2267/2330 train_time:90903ms step_avg:40.10ms
step:2268/2330 train_time:90959ms step_avg:40.11ms
step:2269/2330 train_time:90982ms step_avg:40.10ms
step:2270/2330 train_time:91038ms step_avg:40.10ms
step:2271/2330 train_time:91061ms step_avg:40.10ms
step:2272/2330 train_time:91117ms step_avg:40.10ms
step:2273/2330 train_time:91140ms step_avg:40.10ms
step:2274/2330 train_time:91196ms step_avg:40.10ms
step:2275/2330 train_time:91219ms step_avg:40.10ms
step:2276/2330 train_time:91277ms step_avg:40.10ms
step:2277/2330 train_time:91301ms step_avg:40.10ms
step:2278/2330 train_time:91358ms step_avg:40.10ms
step:2279/2330 train_time:91382ms step_avg:40.10ms
step:2280/2330 train_time:91438ms step_avg:40.10ms
step:2281/2330 train_time:91461ms step_avg:40.10ms
step:2282/2330 train_time:91518ms step_avg:40.10ms
step:2283/2330 train_time:91543ms step_avg:40.10ms
step:2284/2330 train_time:91601ms step_avg:40.11ms
step:2285/2330 train_time:91626ms step_avg:40.10ms
step:2286/2330 train_time:91684ms step_avg:40.11ms
step:2287/2330 train_time:91708ms step_avg:40.10ms
step:2288/2330 train_time:91765ms step_avg:40.11ms
step:2289/2330 train_time:91789ms step_avg:40.10ms
step:2290/2330 train_time:91846ms step_avg:40.11ms
step:2291/2330 train_time:91869ms step_avg:40.10ms
step:2292/2330 train_time:91926ms step_avg:40.11ms
step:2293/2330 train_time:91948ms step_avg:40.10ms
step:2294/2330 train_time:92006ms step_avg:40.11ms
step:2295/2330 train_time:92028ms step_avg:40.10ms
step:2296/2330 train_time:92085ms step_avg:40.11ms
step:2297/2330 train_time:92107ms step_avg:40.10ms
step:2298/2330 train_time:92164ms step_avg:40.11ms
step:2299/2330 train_time:92187ms step_avg:40.10ms
step:2300/2330 train_time:92243ms step_avg:40.11ms
step:2301/2330 train_time:92267ms step_avg:40.10ms
step:2302/2330 train_time:92324ms step_avg:40.11ms
step:2303/2330 train_time:92347ms step_avg:40.10ms
step:2304/2330 train_time:92404ms step_avg:40.11ms
step:2305/2330 train_time:92427ms step_avg:40.10ms
step:2306/2330 train_time:92485ms step_avg:40.11ms
step:2307/2330 train_time:92507ms step_avg:40.10ms
step:2308/2330 train_time:92565ms step_avg:40.11ms
step:2309/2330 train_time:92588ms step_avg:40.10ms
step:2310/2330 train_time:92646ms step_avg:40.11ms
step:2311/2330 train_time:92668ms step_avg:40.10ms
step:2312/2330 train_time:92727ms step_avg:40.11ms
step:2313/2330 train_time:92749ms step_avg:40.10ms
step:2314/2330 train_time:92807ms step_avg:40.11ms
step:2315/2330 train_time:92829ms step_avg:40.10ms
step:2316/2330 train_time:92887ms step_avg:40.11ms
step:2317/2330 train_time:92909ms step_avg:40.10ms
step:2318/2330 train_time:92966ms step_avg:40.11ms
step:2319/2330 train_time:92988ms step_avg:40.10ms
step:2320/2330 train_time:93046ms step_avg:40.11ms
step:2321/2330 train_time:93068ms step_avg:40.10ms
step:2322/2330 train_time:93125ms step_avg:40.11ms
step:2323/2330 train_time:93148ms step_avg:40.10ms
step:2324/2330 train_time:93206ms step_avg:40.11ms
step:2325/2330 train_time:93228ms step_avg:40.10ms
step:2326/2330 train_time:93285ms step_avg:40.11ms
step:2327/2330 train_time:93308ms step_avg:40.10ms
step:2328/2330 train_time:93365ms step_avg:40.11ms
step:2329/2330 train_time:93388ms step_avg:40.10ms
step:2330/2330 train_time:93445ms step_avg:40.11ms
step:2330/2330 val_loss:5.1208 train_time:93543ms step_avg:40.15ms
peak memory allocated: 29712 MiB reserved: 38888 MiB
