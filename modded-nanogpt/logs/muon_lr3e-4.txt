import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "muon_lr3e-4"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=3e-4, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Oct 28 22:16:44 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   31C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   31C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   32C    P0             115W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:91ms step_avg:91.47ms
step:2/2330 train_time:232ms step_avg:116.04ms
step:3/2330 train_time:254ms step_avg:84.66ms
step:4/2330 train_time:289ms step_avg:72.26ms
step:5/2330 train_time:346ms step_avg:69.27ms
step:6/2330 train_time:407ms step_avg:67.80ms
step:7/2330 train_time:465ms step_avg:66.45ms
step:8/2330 train_time:526ms step_avg:65.77ms
step:9/2330 train_time:584ms step_avg:64.94ms
step:10/2330 train_time:645ms step_avg:64.49ms
step:11/2330 train_time:704ms step_avg:63.96ms
step:12/2330 train_time:765ms step_avg:63.72ms
step:13/2330 train_time:823ms step_avg:63.32ms
step:14/2330 train_time:884ms step_avg:63.16ms
step:15/2330 train_time:943ms step_avg:62.83ms
step:16/2330 train_time:1003ms step_avg:62.71ms
step:17/2330 train_time:1063ms step_avg:62.50ms
step:18/2330 train_time:1125ms step_avg:62.53ms
step:19/2330 train_time:1188ms step_avg:62.52ms
step:20/2330 train_time:1251ms step_avg:62.57ms
step:21/2330 train_time:1313ms step_avg:62.52ms
step:22/2330 train_time:1375ms step_avg:62.50ms
step:23/2330 train_time:1434ms step_avg:62.36ms
step:24/2330 train_time:1496ms step_avg:62.33ms
step:25/2330 train_time:1556ms step_avg:62.23ms
step:26/2330 train_time:1617ms step_avg:62.21ms
step:27/2330 train_time:1676ms step_avg:62.09ms
step:28/2330 train_time:1738ms step_avg:62.06ms
step:29/2330 train_time:1797ms step_avg:61.95ms
step:30/2330 train_time:1858ms step_avg:61.94ms
step:31/2330 train_time:1918ms step_avg:61.87ms
step:32/2330 train_time:1979ms step_avg:61.86ms
step:33/2330 train_time:2040ms step_avg:61.81ms
step:34/2330 train_time:2101ms step_avg:61.80ms
step:35/2330 train_time:2162ms step_avg:61.77ms
step:36/2330 train_time:2225ms step_avg:61.80ms
step:37/2330 train_time:2286ms step_avg:61.79ms
step:38/2330 train_time:2348ms step_avg:61.80ms
step:39/2330 train_time:2408ms step_avg:61.75ms
step:40/2330 train_time:2470ms step_avg:61.74ms
step:41/2330 train_time:2529ms step_avg:61.68ms
step:42/2330 train_time:2591ms step_avg:61.68ms
step:43/2330 train_time:2650ms step_avg:61.62ms
step:44/2330 train_time:2711ms step_avg:61.61ms
step:45/2330 train_time:2771ms step_avg:61.57ms
step:46/2330 train_time:2832ms step_avg:61.57ms
step:47/2330 train_time:2892ms step_avg:61.53ms
step:48/2330 train_time:2954ms step_avg:61.54ms
step:49/2330 train_time:3014ms step_avg:61.51ms
step:50/2330 train_time:3077ms step_avg:61.53ms
step:51/2330 train_time:3137ms step_avg:61.52ms
step:52/2330 train_time:3200ms step_avg:61.54ms
step:53/2330 train_time:3260ms step_avg:61.51ms
step:54/2330 train_time:3322ms step_avg:61.52ms
step:55/2330 train_time:3382ms step_avg:61.49ms
step:56/2330 train_time:3444ms step_avg:61.49ms
step:57/2330 train_time:3503ms step_avg:61.46ms
step:58/2330 train_time:3565ms step_avg:61.47ms
step:59/2330 train_time:3625ms step_avg:61.44ms
step:60/2330 train_time:3686ms step_avg:61.44ms
step:61/2330 train_time:3746ms step_avg:61.40ms
step:62/2330 train_time:3807ms step_avg:61.41ms
step:63/2330 train_time:3867ms step_avg:61.39ms
step:64/2330 train_time:3929ms step_avg:61.39ms
step:65/2330 train_time:3988ms step_avg:61.36ms
step:66/2330 train_time:4050ms step_avg:61.36ms
step:67/2330 train_time:4110ms step_avg:61.35ms
step:68/2330 train_time:4174ms step_avg:61.38ms
step:69/2330 train_time:4234ms step_avg:61.37ms
step:70/2330 train_time:4296ms step_avg:61.38ms
step:71/2330 train_time:4356ms step_avg:61.35ms
step:72/2330 train_time:4418ms step_avg:61.36ms
step:73/2330 train_time:4477ms step_avg:61.34ms
step:74/2330 train_time:4540ms step_avg:61.35ms
step:75/2330 train_time:4600ms step_avg:61.34ms
step:76/2330 train_time:4662ms step_avg:61.35ms
step:77/2330 train_time:4722ms step_avg:61.33ms
step:78/2330 train_time:4784ms step_avg:61.33ms
step:79/2330 train_time:4843ms step_avg:61.31ms
step:80/2330 train_time:4905ms step_avg:61.31ms
step:81/2330 train_time:4966ms step_avg:61.31ms
step:82/2330 train_time:5027ms step_avg:61.31ms
step:83/2330 train_time:5086ms step_avg:61.28ms
step:84/2330 train_time:5148ms step_avg:61.28ms
step:85/2330 train_time:5207ms step_avg:61.26ms
step:86/2330 train_time:5269ms step_avg:61.27ms
step:87/2330 train_time:5329ms step_avg:61.25ms
step:88/2330 train_time:5391ms step_avg:61.26ms
step:89/2330 train_time:5450ms step_avg:61.23ms
step:90/2330 train_time:5513ms step_avg:61.25ms
step:91/2330 train_time:5573ms step_avg:61.24ms
step:92/2330 train_time:5635ms step_avg:61.25ms
step:93/2330 train_time:5695ms step_avg:61.24ms
step:94/2330 train_time:5758ms step_avg:61.25ms
step:95/2330 train_time:5818ms step_avg:61.24ms
step:96/2330 train_time:5879ms step_avg:61.24ms
step:97/2330 train_time:5939ms step_avg:61.23ms
step:98/2330 train_time:6001ms step_avg:61.24ms
step:99/2330 train_time:6061ms step_avg:61.22ms
step:100/2330 train_time:6123ms step_avg:61.23ms
step:101/2330 train_time:6184ms step_avg:61.22ms
step:102/2330 train_time:6245ms step_avg:61.23ms
step:103/2330 train_time:6304ms step_avg:61.21ms
step:104/2330 train_time:6366ms step_avg:61.21ms
step:105/2330 train_time:6425ms step_avg:61.19ms
step:106/2330 train_time:6487ms step_avg:61.20ms
step:107/2330 train_time:6547ms step_avg:61.19ms
step:108/2330 train_time:6609ms step_avg:61.19ms
step:109/2330 train_time:6669ms step_avg:61.19ms
step:110/2330 train_time:6731ms step_avg:61.19ms
step:111/2330 train_time:6791ms step_avg:61.18ms
step:112/2330 train_time:6853ms step_avg:61.19ms
step:113/2330 train_time:6913ms step_avg:61.18ms
step:114/2330 train_time:6976ms step_avg:61.19ms
step:115/2330 train_time:7036ms step_avg:61.18ms
step:116/2330 train_time:7099ms step_avg:61.19ms
step:117/2330 train_time:7158ms step_avg:61.18ms
step:118/2330 train_time:7220ms step_avg:61.19ms
step:119/2330 train_time:7280ms step_avg:61.18ms
step:120/2330 train_time:7342ms step_avg:61.18ms
step:121/2330 train_time:7402ms step_avg:61.17ms
step:122/2330 train_time:7464ms step_avg:61.18ms
step:123/2330 train_time:7524ms step_avg:61.17ms
step:124/2330 train_time:7586ms step_avg:61.18ms
step:125/2330 train_time:7646ms step_avg:61.17ms
step:126/2330 train_time:7709ms step_avg:61.18ms
step:127/2330 train_time:7768ms step_avg:61.17ms
step:128/2330 train_time:7830ms step_avg:61.17ms
step:129/2330 train_time:7890ms step_avg:61.16ms
step:130/2330 train_time:7951ms step_avg:61.16ms
step:131/2330 train_time:8011ms step_avg:61.15ms
step:132/2330 train_time:8073ms step_avg:61.16ms
step:133/2330 train_time:8133ms step_avg:61.15ms
step:134/2330 train_time:8195ms step_avg:61.16ms
step:135/2330 train_time:8256ms step_avg:61.15ms
step:136/2330 train_time:8317ms step_avg:61.16ms
step:137/2330 train_time:8377ms step_avg:61.15ms
step:138/2330 train_time:8439ms step_avg:61.15ms
step:139/2330 train_time:8499ms step_avg:61.14ms
step:140/2330 train_time:8562ms step_avg:61.16ms
step:141/2330 train_time:8622ms step_avg:61.15ms
step:142/2330 train_time:8684ms step_avg:61.16ms
step:143/2330 train_time:8744ms step_avg:61.15ms
step:144/2330 train_time:8807ms step_avg:61.16ms
step:145/2330 train_time:8866ms step_avg:61.14ms
step:146/2330 train_time:8928ms step_avg:61.15ms
step:147/2330 train_time:8988ms step_avg:61.14ms
step:148/2330 train_time:9049ms step_avg:61.14ms
step:149/2330 train_time:9109ms step_avg:61.13ms
step:150/2330 train_time:9171ms step_avg:61.14ms
step:151/2330 train_time:9231ms step_avg:61.13ms
step:152/2330 train_time:9294ms step_avg:61.14ms
step:153/2330 train_time:9354ms step_avg:61.14ms
step:154/2330 train_time:9417ms step_avg:61.15ms
step:155/2330 train_time:9477ms step_avg:61.14ms
step:156/2330 train_time:9539ms step_avg:61.15ms
step:157/2330 train_time:9599ms step_avg:61.14ms
step:158/2330 train_time:9661ms step_avg:61.15ms
step:159/2330 train_time:9722ms step_avg:61.14ms
step:160/2330 train_time:9784ms step_avg:61.15ms
step:161/2330 train_time:9844ms step_avg:61.14ms
step:162/2330 train_time:9906ms step_avg:61.15ms
step:163/2330 train_time:9966ms step_avg:61.14ms
step:164/2330 train_time:10027ms step_avg:61.14ms
step:165/2330 train_time:10087ms step_avg:61.13ms
step:166/2330 train_time:10148ms step_avg:61.14ms
step:167/2330 train_time:10208ms step_avg:61.13ms
step:168/2330 train_time:10270ms step_avg:61.13ms
step:169/2330 train_time:10330ms step_avg:61.13ms
step:170/2330 train_time:10392ms step_avg:61.13ms
step:171/2330 train_time:10452ms step_avg:61.12ms
step:172/2330 train_time:10515ms step_avg:61.13ms
step:173/2330 train_time:10576ms step_avg:61.13ms
step:174/2330 train_time:10638ms step_avg:61.14ms
step:175/2330 train_time:10699ms step_avg:61.13ms
step:176/2330 train_time:10760ms step_avg:61.14ms
step:177/2330 train_time:10821ms step_avg:61.14ms
step:178/2330 train_time:10884ms step_avg:61.15ms
step:179/2330 train_time:10944ms step_avg:61.14ms
step:180/2330 train_time:11006ms step_avg:61.14ms
step:181/2330 train_time:11065ms step_avg:61.13ms
step:182/2330 train_time:11127ms step_avg:61.14ms
step:183/2330 train_time:11187ms step_avg:61.13ms
step:184/2330 train_time:11248ms step_avg:61.13ms
step:185/2330 train_time:11308ms step_avg:61.12ms
step:186/2330 train_time:11370ms step_avg:61.13ms
step:187/2330 train_time:11430ms step_avg:61.12ms
step:188/2330 train_time:11493ms step_avg:61.13ms
step:189/2330 train_time:11553ms step_avg:61.13ms
step:190/2330 train_time:11617ms step_avg:61.14ms
step:191/2330 train_time:11678ms step_avg:61.14ms
step:192/2330 train_time:11740ms step_avg:61.14ms
step:193/2330 train_time:11800ms step_avg:61.14ms
step:194/2330 train_time:11862ms step_avg:61.15ms
step:195/2330 train_time:11922ms step_avg:61.14ms
step:196/2330 train_time:11984ms step_avg:61.14ms
step:197/2330 train_time:12044ms step_avg:61.13ms
step:198/2330 train_time:12105ms step_avg:61.14ms
step:199/2330 train_time:12165ms step_avg:61.13ms
step:200/2330 train_time:12227ms step_avg:61.14ms
step:201/2330 train_time:12287ms step_avg:61.13ms
step:202/2330 train_time:12349ms step_avg:61.13ms
step:203/2330 train_time:12409ms step_avg:61.13ms
step:204/2330 train_time:12471ms step_avg:61.13ms
step:205/2330 train_time:12531ms step_avg:61.13ms
step:206/2330 train_time:12594ms step_avg:61.13ms
step:207/2330 train_time:12654ms step_avg:61.13ms
step:208/2330 train_time:12717ms step_avg:61.14ms
step:209/2330 train_time:12778ms step_avg:61.14ms
step:210/2330 train_time:12840ms step_avg:61.14ms
step:211/2330 train_time:12900ms step_avg:61.14ms
step:212/2330 train_time:12962ms step_avg:61.14ms
step:213/2330 train_time:13022ms step_avg:61.14ms
step:214/2330 train_time:13084ms step_avg:61.14ms
step:215/2330 train_time:13144ms step_avg:61.13ms
step:216/2330 train_time:13206ms step_avg:61.14ms
step:217/2330 train_time:13265ms step_avg:61.13ms
step:218/2330 train_time:13327ms step_avg:61.13ms
step:219/2330 train_time:13386ms step_avg:61.13ms
step:220/2330 train_time:13449ms step_avg:61.13ms
step:221/2330 train_time:13508ms step_avg:61.12ms
step:222/2330 train_time:13570ms step_avg:61.13ms
step:223/2330 train_time:13630ms step_avg:61.12ms
step:224/2330 train_time:13693ms step_avg:61.13ms
step:225/2330 train_time:13753ms step_avg:61.12ms
step:226/2330 train_time:13816ms step_avg:61.13ms
step:227/2330 train_time:13877ms step_avg:61.13ms
step:228/2330 train_time:13939ms step_avg:61.14ms
step:229/2330 train_time:13999ms step_avg:61.13ms
step:230/2330 train_time:14061ms step_avg:61.13ms
step:231/2330 train_time:14121ms step_avg:61.13ms
step:232/2330 train_time:14183ms step_avg:61.13ms
step:233/2330 train_time:14243ms step_avg:61.13ms
step:234/2330 train_time:14305ms step_avg:61.13ms
step:235/2330 train_time:14367ms step_avg:61.14ms
step:236/2330 train_time:14428ms step_avg:61.14ms
step:237/2330 train_time:14488ms step_avg:61.13ms
step:238/2330 train_time:14550ms step_avg:61.13ms
step:239/2330 train_time:14610ms step_avg:61.13ms
step:240/2330 train_time:14671ms step_avg:61.13ms
step:241/2330 train_time:14731ms step_avg:61.12ms
step:242/2330 train_time:14793ms step_avg:61.13ms
step:243/2330 train_time:14854ms step_avg:61.13ms
step:244/2330 train_time:14916ms step_avg:61.13ms
step:245/2330 train_time:14977ms step_avg:61.13ms
step:246/2330 train_time:15039ms step_avg:61.13ms
step:247/2330 train_time:15099ms step_avg:61.13ms
step:248/2330 train_time:15161ms step_avg:61.13ms
step:249/2330 train_time:15221ms step_avg:61.13ms
step:250/2330 train_time:15284ms step_avg:61.13ms
step:250/2330 val_loss:5.2496 train_time:15348ms step_avg:61.39ms
step:251/2330 train_time:15370ms step_avg:61.24ms
step:252/2330 train_time:15408ms step_avg:61.14ms
step:253/2330 train_time:15474ms step_avg:61.16ms
step:254/2330 train_time:15543ms step_avg:61.19ms
step:255/2330 train_time:15603ms step_avg:61.19ms
step:256/2330 train_time:15665ms step_avg:61.19ms
step:257/2330 train_time:15724ms step_avg:61.18ms
step:258/2330 train_time:15785ms step_avg:61.18ms
step:259/2330 train_time:15844ms step_avg:61.18ms
step:260/2330 train_time:15906ms step_avg:61.18ms
step:261/2330 train_time:15965ms step_avg:61.17ms
step:262/2330 train_time:16026ms step_avg:61.17ms
step:263/2330 train_time:16086ms step_avg:61.16ms
step:264/2330 train_time:16147ms step_avg:61.16ms
step:265/2330 train_time:16206ms step_avg:61.15ms
step:266/2330 train_time:16268ms step_avg:61.16ms
step:267/2330 train_time:16328ms step_avg:61.15ms
step:268/2330 train_time:16391ms step_avg:61.16ms
step:269/2330 train_time:16454ms step_avg:61.17ms
step:270/2330 train_time:16517ms step_avg:61.18ms
step:271/2330 train_time:16578ms step_avg:61.17ms
step:272/2330 train_time:16640ms step_avg:61.18ms
step:273/2330 train_time:16700ms step_avg:61.17ms
step:274/2330 train_time:16762ms step_avg:61.17ms
step:275/2330 train_time:16821ms step_avg:61.17ms
step:276/2330 train_time:16882ms step_avg:61.17ms
step:277/2330 train_time:16942ms step_avg:61.16ms
step:278/2330 train_time:17003ms step_avg:61.16ms
step:279/2330 train_time:17063ms step_avg:61.16ms
step:280/2330 train_time:17124ms step_avg:61.16ms
step:281/2330 train_time:17183ms step_avg:61.15ms
step:282/2330 train_time:17245ms step_avg:61.15ms
step:283/2330 train_time:17304ms step_avg:61.15ms
step:284/2330 train_time:17366ms step_avg:61.15ms
step:285/2330 train_time:17426ms step_avg:61.14ms
step:286/2330 train_time:17489ms step_avg:61.15ms
step:287/2330 train_time:17550ms step_avg:61.15ms
step:288/2330 train_time:17612ms step_avg:61.15ms
step:289/2330 train_time:17673ms step_avg:61.15ms
step:290/2330 train_time:17735ms step_avg:61.16ms
step:291/2330 train_time:17795ms step_avg:61.15ms
step:292/2330 train_time:17857ms step_avg:61.16ms
step:293/2330 train_time:17917ms step_avg:61.15ms
step:294/2330 train_time:17979ms step_avg:61.15ms
step:295/2330 train_time:18039ms step_avg:61.15ms
step:296/2330 train_time:18101ms step_avg:61.15ms
step:297/2330 train_time:18161ms step_avg:61.15ms
step:298/2330 train_time:18223ms step_avg:61.15ms
step:299/2330 train_time:18283ms step_avg:61.15ms
step:300/2330 train_time:18344ms step_avg:61.15ms
step:301/2330 train_time:18404ms step_avg:61.14ms
step:302/2330 train_time:18466ms step_avg:61.14ms
step:303/2330 train_time:18526ms step_avg:61.14ms
step:304/2330 train_time:18588ms step_avg:61.15ms
step:305/2330 train_time:18650ms step_avg:61.15ms
step:306/2330 train_time:18712ms step_avg:61.15ms
step:307/2330 train_time:18773ms step_avg:61.15ms
step:308/2330 train_time:18835ms step_avg:61.15ms
step:309/2330 train_time:18895ms step_avg:61.15ms
step:310/2330 train_time:18958ms step_avg:61.15ms
step:311/2330 train_time:19017ms step_avg:61.15ms
step:312/2330 train_time:19079ms step_avg:61.15ms
step:313/2330 train_time:19140ms step_avg:61.15ms
step:314/2330 train_time:19202ms step_avg:61.15ms
step:315/2330 train_time:19262ms step_avg:61.15ms
step:316/2330 train_time:19323ms step_avg:61.15ms
step:317/2330 train_time:19384ms step_avg:61.15ms
step:318/2330 train_time:19445ms step_avg:61.15ms
step:319/2330 train_time:19505ms step_avg:61.14ms
step:320/2330 train_time:19567ms step_avg:61.15ms
step:321/2330 train_time:19627ms step_avg:61.14ms
step:322/2330 train_time:19688ms step_avg:61.14ms
step:323/2330 train_time:19749ms step_avg:61.14ms
step:324/2330 train_time:19811ms step_avg:61.14ms
step:325/2330 train_time:19871ms step_avg:61.14ms
step:326/2330 train_time:19934ms step_avg:61.15ms
step:327/2330 train_time:19995ms step_avg:61.15ms
step:328/2330 train_time:20058ms step_avg:61.15ms
step:329/2330 train_time:20118ms step_avg:61.15ms
step:330/2330 train_time:20180ms step_avg:61.15ms
step:331/2330 train_time:20239ms step_avg:61.15ms
step:332/2330 train_time:20301ms step_avg:61.15ms
step:333/2330 train_time:20362ms step_avg:61.15ms
step:334/2330 train_time:20424ms step_avg:61.15ms
step:335/2330 train_time:20484ms step_avg:61.15ms
step:336/2330 train_time:20546ms step_avg:61.15ms
step:337/2330 train_time:20605ms step_avg:61.14ms
step:338/2330 train_time:20667ms step_avg:61.15ms
step:339/2330 train_time:20727ms step_avg:61.14ms
step:340/2330 train_time:20790ms step_avg:61.15ms
step:341/2330 train_time:20850ms step_avg:61.14ms
step:342/2330 train_time:20912ms step_avg:61.15ms
step:343/2330 train_time:20973ms step_avg:61.15ms
step:344/2330 train_time:21036ms step_avg:61.15ms
step:345/2330 train_time:21096ms step_avg:61.15ms
step:346/2330 train_time:21158ms step_avg:61.15ms
step:347/2330 train_time:21218ms step_avg:61.15ms
step:348/2330 train_time:21280ms step_avg:61.15ms
step:349/2330 train_time:21341ms step_avg:61.15ms
step:350/2330 train_time:21403ms step_avg:61.15ms
step:351/2330 train_time:21463ms step_avg:61.15ms
step:352/2330 train_time:21524ms step_avg:61.15ms
step:353/2330 train_time:21584ms step_avg:61.14ms
step:354/2330 train_time:21646ms step_avg:61.15ms
step:355/2330 train_time:21706ms step_avg:61.14ms
step:356/2330 train_time:21768ms step_avg:61.15ms
step:357/2330 train_time:21828ms step_avg:61.14ms
step:358/2330 train_time:21890ms step_avg:61.15ms
step:359/2330 train_time:21952ms step_avg:61.15ms
step:360/2330 train_time:22015ms step_avg:61.15ms
step:361/2330 train_time:22075ms step_avg:61.15ms
step:362/2330 train_time:22137ms step_avg:61.15ms
step:363/2330 train_time:22196ms step_avg:61.15ms
step:364/2330 train_time:22260ms step_avg:61.15ms
step:365/2330 train_time:22320ms step_avg:61.15ms
step:366/2330 train_time:22381ms step_avg:61.15ms
step:367/2330 train_time:22442ms step_avg:61.15ms
step:368/2330 train_time:22503ms step_avg:61.15ms
step:369/2330 train_time:22563ms step_avg:61.15ms
step:370/2330 train_time:22624ms step_avg:61.15ms
step:371/2330 train_time:22685ms step_avg:61.14ms
step:372/2330 train_time:22747ms step_avg:61.15ms
step:373/2330 train_time:22807ms step_avg:61.14ms
step:374/2330 train_time:22869ms step_avg:61.15ms
step:375/2330 train_time:22928ms step_avg:61.14ms
step:376/2330 train_time:22991ms step_avg:61.15ms
step:377/2330 train_time:23052ms step_avg:61.15ms
step:378/2330 train_time:23114ms step_avg:61.15ms
step:379/2330 train_time:23175ms step_avg:61.15ms
step:380/2330 train_time:23237ms step_avg:61.15ms
step:381/2330 train_time:23296ms step_avg:61.15ms
step:382/2330 train_time:23359ms step_avg:61.15ms
step:383/2330 train_time:23419ms step_avg:61.15ms
step:384/2330 train_time:23482ms step_avg:61.15ms
step:385/2330 train_time:23542ms step_avg:61.15ms
step:386/2330 train_time:23603ms step_avg:61.15ms
step:387/2330 train_time:23663ms step_avg:61.15ms
step:388/2330 train_time:23725ms step_avg:61.15ms
step:389/2330 train_time:23785ms step_avg:61.14ms
step:390/2330 train_time:23847ms step_avg:61.15ms
step:391/2330 train_time:23907ms step_avg:61.14ms
step:392/2330 train_time:23969ms step_avg:61.15ms
step:393/2330 train_time:24030ms step_avg:61.15ms
step:394/2330 train_time:24093ms step_avg:61.15ms
step:395/2330 train_time:24155ms step_avg:61.15ms
step:396/2330 train_time:24217ms step_avg:61.15ms
step:397/2330 train_time:24277ms step_avg:61.15ms
step:398/2330 train_time:24338ms step_avg:61.15ms
step:399/2330 train_time:24398ms step_avg:61.15ms
step:400/2330 train_time:24461ms step_avg:61.15ms
step:401/2330 train_time:24521ms step_avg:61.15ms
step:402/2330 train_time:24583ms step_avg:61.15ms
step:403/2330 train_time:24643ms step_avg:61.15ms
step:404/2330 train_time:24704ms step_avg:61.15ms
step:405/2330 train_time:24764ms step_avg:61.14ms
step:406/2330 train_time:24826ms step_avg:61.15ms
step:407/2330 train_time:24886ms step_avg:61.14ms
step:408/2330 train_time:24948ms step_avg:61.15ms
step:409/2330 train_time:25008ms step_avg:61.14ms
step:410/2330 train_time:25070ms step_avg:61.15ms
step:411/2330 train_time:25133ms step_avg:61.15ms
step:412/2330 train_time:25195ms step_avg:61.15ms
step:413/2330 train_time:25256ms step_avg:61.15ms
step:414/2330 train_time:25317ms step_avg:61.15ms
step:415/2330 train_time:25378ms step_avg:61.15ms
step:416/2330 train_time:25440ms step_avg:61.15ms
step:417/2330 train_time:25499ms step_avg:61.15ms
step:418/2330 train_time:25562ms step_avg:61.15ms
step:419/2330 train_time:25622ms step_avg:61.15ms
step:420/2330 train_time:25683ms step_avg:61.15ms
step:421/2330 train_time:25743ms step_avg:61.15ms
step:422/2330 train_time:25805ms step_avg:61.15ms
step:423/2330 train_time:25865ms step_avg:61.15ms
step:424/2330 train_time:25926ms step_avg:61.15ms
step:425/2330 train_time:25987ms step_avg:61.15ms
step:426/2330 train_time:26049ms step_avg:61.15ms
step:427/2330 train_time:26109ms step_avg:61.15ms
step:428/2330 train_time:26172ms step_avg:61.15ms
step:429/2330 train_time:26233ms step_avg:61.15ms
step:430/2330 train_time:26296ms step_avg:61.15ms
step:431/2330 train_time:26356ms step_avg:61.15ms
step:432/2330 train_time:26418ms step_avg:61.15ms
step:433/2330 train_time:26478ms step_avg:61.15ms
step:434/2330 train_time:26540ms step_avg:61.15ms
step:435/2330 train_time:26601ms step_avg:61.15ms
step:436/2330 train_time:26663ms step_avg:61.15ms
step:437/2330 train_time:26722ms step_avg:61.15ms
step:438/2330 train_time:26784ms step_avg:61.15ms
step:439/2330 train_time:26845ms step_avg:61.15ms
step:440/2330 train_time:26907ms step_avg:61.15ms
step:441/2330 train_time:26966ms step_avg:61.15ms
step:442/2330 train_time:27029ms step_avg:61.15ms
step:443/2330 train_time:27089ms step_avg:61.15ms
step:444/2330 train_time:27151ms step_avg:61.15ms
step:445/2330 train_time:27211ms step_avg:61.15ms
step:446/2330 train_time:27274ms step_avg:61.15ms
step:447/2330 train_time:27334ms step_avg:61.15ms
step:448/2330 train_time:27396ms step_avg:61.15ms
step:449/2330 train_time:27457ms step_avg:61.15ms
step:450/2330 train_time:27519ms step_avg:61.15ms
step:451/2330 train_time:27579ms step_avg:61.15ms
step:452/2330 train_time:27641ms step_avg:61.15ms
step:453/2330 train_time:27701ms step_avg:61.15ms
step:454/2330 train_time:27763ms step_avg:61.15ms
step:455/2330 train_time:27823ms step_avg:61.15ms
step:456/2330 train_time:27885ms step_avg:61.15ms
step:457/2330 train_time:27946ms step_avg:61.15ms
step:458/2330 train_time:28008ms step_avg:61.15ms
step:459/2330 train_time:28067ms step_avg:61.15ms
step:460/2330 train_time:28129ms step_avg:61.15ms
step:461/2330 train_time:28189ms step_avg:61.15ms
step:462/2330 train_time:28252ms step_avg:61.15ms
step:463/2330 train_time:28313ms step_avg:61.15ms
step:464/2330 train_time:28375ms step_avg:61.15ms
step:465/2330 train_time:28436ms step_avg:61.15ms
step:466/2330 train_time:28498ms step_avg:61.15ms
step:467/2330 train_time:28557ms step_avg:61.15ms
step:468/2330 train_time:28619ms step_avg:61.15ms
step:469/2330 train_time:28680ms step_avg:61.15ms
step:470/2330 train_time:28743ms step_avg:61.15ms
step:471/2330 train_time:28802ms step_avg:61.15ms
step:472/2330 train_time:28864ms step_avg:61.15ms
step:473/2330 train_time:28924ms step_avg:61.15ms
step:474/2330 train_time:28986ms step_avg:61.15ms
step:475/2330 train_time:29046ms step_avg:61.15ms
step:476/2330 train_time:29107ms step_avg:61.15ms
step:477/2330 train_time:29167ms step_avg:61.15ms
step:478/2330 train_time:29230ms step_avg:61.15ms
step:479/2330 train_time:29290ms step_avg:61.15ms
step:480/2330 train_time:29353ms step_avg:61.15ms
step:481/2330 train_time:29414ms step_avg:61.15ms
step:482/2330 train_time:29476ms step_avg:61.15ms
step:483/2330 train_time:29538ms step_avg:61.16ms
step:484/2330 train_time:29600ms step_avg:61.16ms
step:485/2330 train_time:29660ms step_avg:61.16ms
step:486/2330 train_time:29722ms step_avg:61.16ms
step:487/2330 train_time:29782ms step_avg:61.15ms
step:488/2330 train_time:29844ms step_avg:61.16ms
step:489/2330 train_time:29903ms step_avg:61.15ms
step:490/2330 train_time:29966ms step_avg:61.15ms
step:491/2330 train_time:30026ms step_avg:61.15ms
step:492/2330 train_time:30088ms step_avg:61.15ms
step:493/2330 train_time:30148ms step_avg:61.15ms
step:494/2330 train_time:30210ms step_avg:61.15ms
step:495/2330 train_time:30270ms step_avg:61.15ms
step:496/2330 train_time:30333ms step_avg:61.16ms
step:497/2330 train_time:30394ms step_avg:61.16ms
step:498/2330 train_time:30458ms step_avg:61.16ms
step:499/2330 train_time:30518ms step_avg:61.16ms
step:500/2330 train_time:30580ms step_avg:61.16ms
step:500/2330 val_loss:4.7961 train_time:30644ms step_avg:61.29ms
step:501/2330 train_time:30667ms step_avg:61.21ms
step:502/2330 train_time:30706ms step_avg:61.17ms
step:503/2330 train_time:30773ms step_avg:61.18ms
step:504/2330 train_time:30836ms step_avg:61.18ms
step:505/2330 train_time:30897ms step_avg:61.18ms
step:506/2330 train_time:30960ms step_avg:61.19ms
step:507/2330 train_time:31019ms step_avg:61.18ms
step:508/2330 train_time:31082ms step_avg:61.18ms
step:509/2330 train_time:31142ms step_avg:61.18ms
step:510/2330 train_time:31203ms step_avg:61.18ms
step:511/2330 train_time:31263ms step_avg:61.18ms
step:512/2330 train_time:31325ms step_avg:61.18ms
step:513/2330 train_time:31384ms step_avg:61.18ms
step:514/2330 train_time:31445ms step_avg:61.18ms
step:515/2330 train_time:31504ms step_avg:61.17ms
step:516/2330 train_time:31567ms step_avg:61.18ms
step:517/2330 train_time:31627ms step_avg:61.17ms
step:518/2330 train_time:31691ms step_avg:61.18ms
step:519/2330 train_time:31753ms step_avg:61.18ms
step:520/2330 train_time:31816ms step_avg:61.18ms
step:521/2330 train_time:31878ms step_avg:61.19ms
step:522/2330 train_time:31940ms step_avg:61.19ms
step:523/2330 train_time:32000ms step_avg:61.19ms
step:524/2330 train_time:32062ms step_avg:61.19ms
step:525/2330 train_time:32123ms step_avg:61.19ms
step:526/2330 train_time:32184ms step_avg:61.19ms
step:527/2330 train_time:32244ms step_avg:61.18ms
step:528/2330 train_time:32306ms step_avg:61.19ms
step:529/2330 train_time:32366ms step_avg:61.18ms
step:530/2330 train_time:32427ms step_avg:61.18ms
step:531/2330 train_time:32487ms step_avg:61.18ms
step:532/2330 train_time:32548ms step_avg:61.18ms
step:533/2330 train_time:32609ms step_avg:61.18ms
step:534/2330 train_time:32671ms step_avg:61.18ms
step:535/2330 train_time:32732ms step_avg:61.18ms
step:536/2330 train_time:32795ms step_avg:61.18ms
step:537/2330 train_time:32855ms step_avg:61.18ms
step:538/2330 train_time:32918ms step_avg:61.19ms
step:539/2330 train_time:32978ms step_avg:61.18ms
step:540/2330 train_time:33040ms step_avg:61.18ms
step:541/2330 train_time:33100ms step_avg:61.18ms
step:542/2330 train_time:33162ms step_avg:61.18ms
step:543/2330 train_time:33222ms step_avg:61.18ms
step:544/2330 train_time:33285ms step_avg:61.19ms
step:545/2330 train_time:33344ms step_avg:61.18ms
step:546/2330 train_time:33407ms step_avg:61.18ms
step:547/2330 train_time:33467ms step_avg:61.18ms
step:548/2330 train_time:33529ms step_avg:61.18ms
step:549/2330 train_time:33589ms step_avg:61.18ms
step:550/2330 train_time:33651ms step_avg:61.18ms
step:551/2330 train_time:33711ms step_avg:61.18ms
step:552/2330 train_time:33775ms step_avg:61.19ms
step:553/2330 train_time:33835ms step_avg:61.18ms
step:554/2330 train_time:33897ms step_avg:61.19ms
step:555/2330 train_time:33958ms step_avg:61.19ms
step:556/2330 train_time:34020ms step_avg:61.19ms
step:557/2330 train_time:34081ms step_avg:61.19ms
step:558/2330 train_time:34142ms step_avg:61.19ms
step:559/2330 train_time:34202ms step_avg:61.19ms
step:560/2330 train_time:34265ms step_avg:61.19ms
step:561/2330 train_time:34324ms step_avg:61.18ms
step:562/2330 train_time:34386ms step_avg:61.19ms
step:563/2330 train_time:34446ms step_avg:61.18ms
step:564/2330 train_time:34509ms step_avg:61.19ms
step:565/2330 train_time:34569ms step_avg:61.18ms
step:566/2330 train_time:34631ms step_avg:61.19ms
step:567/2330 train_time:34691ms step_avg:61.18ms
step:568/2330 train_time:34753ms step_avg:61.19ms
step:569/2330 train_time:34815ms step_avg:61.19ms
step:570/2330 train_time:34877ms step_avg:61.19ms
step:571/2330 train_time:34938ms step_avg:61.19ms
step:572/2330 train_time:35000ms step_avg:61.19ms
step:573/2330 train_time:35059ms step_avg:61.19ms
step:574/2330 train_time:35122ms step_avg:61.19ms
step:575/2330 train_time:35182ms step_avg:61.19ms
step:576/2330 train_time:35244ms step_avg:61.19ms
step:577/2330 train_time:35303ms step_avg:61.18ms
step:578/2330 train_time:35366ms step_avg:61.19ms
step:579/2330 train_time:35426ms step_avg:61.18ms
step:580/2330 train_time:35487ms step_avg:61.18ms
step:581/2330 train_time:35548ms step_avg:61.18ms
step:582/2330 train_time:35610ms step_avg:61.19ms
step:583/2330 train_time:35670ms step_avg:61.18ms
step:584/2330 train_time:35732ms step_avg:61.18ms
step:585/2330 train_time:35793ms step_avg:61.18ms
step:586/2330 train_time:35855ms step_avg:61.19ms
step:587/2330 train_time:35916ms step_avg:61.18ms
step:588/2330 train_time:35979ms step_avg:61.19ms
step:589/2330 train_time:36038ms step_avg:61.19ms
step:590/2330 train_time:36100ms step_avg:61.19ms
step:591/2330 train_time:36160ms step_avg:61.19ms
step:592/2330 train_time:36223ms step_avg:61.19ms
step:593/2330 train_time:36283ms step_avg:61.19ms
step:594/2330 train_time:36346ms step_avg:61.19ms
step:595/2330 train_time:36406ms step_avg:61.19ms
step:596/2330 train_time:36468ms step_avg:61.19ms
step:597/2330 train_time:36528ms step_avg:61.19ms
step:598/2330 train_time:36590ms step_avg:61.19ms
step:599/2330 train_time:36650ms step_avg:61.19ms
step:600/2330 train_time:36713ms step_avg:61.19ms
step:601/2330 train_time:36773ms step_avg:61.19ms
step:602/2330 train_time:36835ms step_avg:61.19ms
step:603/2330 train_time:36895ms step_avg:61.19ms
step:604/2330 train_time:36958ms step_avg:61.19ms
step:605/2330 train_time:37019ms step_avg:61.19ms
step:606/2330 train_time:37081ms step_avg:61.19ms
step:607/2330 train_time:37141ms step_avg:61.19ms
step:608/2330 train_time:37203ms step_avg:61.19ms
step:609/2330 train_time:37263ms step_avg:61.19ms
step:610/2330 train_time:37325ms step_avg:61.19ms
step:611/2330 train_time:37385ms step_avg:61.19ms
step:612/2330 train_time:37447ms step_avg:61.19ms
step:613/2330 train_time:37507ms step_avg:61.19ms
step:614/2330 train_time:37569ms step_avg:61.19ms
step:615/2330 train_time:37629ms step_avg:61.19ms
step:616/2330 train_time:37691ms step_avg:61.19ms
step:617/2330 train_time:37751ms step_avg:61.19ms
step:618/2330 train_time:37815ms step_avg:61.19ms
step:619/2330 train_time:37875ms step_avg:61.19ms
step:620/2330 train_time:37938ms step_avg:61.19ms
step:621/2330 train_time:37999ms step_avg:61.19ms
step:622/2330 train_time:38060ms step_avg:61.19ms
step:623/2330 train_time:38120ms step_avg:61.19ms
step:624/2330 train_time:38182ms step_avg:61.19ms
step:625/2330 train_time:38242ms step_avg:61.19ms
step:626/2330 train_time:38304ms step_avg:61.19ms
step:627/2330 train_time:38364ms step_avg:61.19ms
step:628/2330 train_time:38426ms step_avg:61.19ms
step:629/2330 train_time:38487ms step_avg:61.19ms
step:630/2330 train_time:38548ms step_avg:61.19ms
step:631/2330 train_time:38609ms step_avg:61.19ms
step:632/2330 train_time:38671ms step_avg:61.19ms
step:633/2330 train_time:38731ms step_avg:61.19ms
step:634/2330 train_time:38794ms step_avg:61.19ms
step:635/2330 train_time:38855ms step_avg:61.19ms
step:636/2330 train_time:38917ms step_avg:61.19ms
step:637/2330 train_time:38977ms step_avg:61.19ms
step:638/2330 train_time:39039ms step_avg:61.19ms
step:639/2330 train_time:39100ms step_avg:61.19ms
step:640/2330 train_time:39161ms step_avg:61.19ms
step:641/2330 train_time:39222ms step_avg:61.19ms
step:642/2330 train_time:39285ms step_avg:61.19ms
step:643/2330 train_time:39345ms step_avg:61.19ms
step:644/2330 train_time:39407ms step_avg:61.19ms
step:645/2330 train_time:39467ms step_avg:61.19ms
step:646/2330 train_time:39530ms step_avg:61.19ms
step:647/2330 train_time:39590ms step_avg:61.19ms
step:648/2330 train_time:39651ms step_avg:61.19ms
step:649/2330 train_time:39711ms step_avg:61.19ms
step:650/2330 train_time:39773ms step_avg:61.19ms
step:651/2330 train_time:39834ms step_avg:61.19ms
step:652/2330 train_time:39896ms step_avg:61.19ms
step:653/2330 train_time:39956ms step_avg:61.19ms
step:654/2330 train_time:40018ms step_avg:61.19ms
step:655/2330 train_time:40078ms step_avg:61.19ms
step:656/2330 train_time:40139ms step_avg:61.19ms
step:657/2330 train_time:40200ms step_avg:61.19ms
step:658/2330 train_time:40261ms step_avg:61.19ms
step:659/2330 train_time:40322ms step_avg:61.19ms
step:660/2330 train_time:40384ms step_avg:61.19ms
step:661/2330 train_time:40446ms step_avg:61.19ms
step:662/2330 train_time:40508ms step_avg:61.19ms
step:663/2330 train_time:40568ms step_avg:61.19ms
step:664/2330 train_time:40630ms step_avg:61.19ms
step:665/2330 train_time:40690ms step_avg:61.19ms
step:666/2330 train_time:40752ms step_avg:61.19ms
step:667/2330 train_time:40812ms step_avg:61.19ms
step:668/2330 train_time:40875ms step_avg:61.19ms
step:669/2330 train_time:40935ms step_avg:61.19ms
step:670/2330 train_time:40997ms step_avg:61.19ms
step:671/2330 train_time:41058ms step_avg:61.19ms
step:672/2330 train_time:41120ms step_avg:61.19ms
step:673/2330 train_time:41180ms step_avg:61.19ms
step:674/2330 train_time:41242ms step_avg:61.19ms
step:675/2330 train_time:41301ms step_avg:61.19ms
step:676/2330 train_time:41364ms step_avg:61.19ms
step:677/2330 train_time:41425ms step_avg:61.19ms
step:678/2330 train_time:41488ms step_avg:61.19ms
step:679/2330 train_time:41548ms step_avg:61.19ms
step:680/2330 train_time:41611ms step_avg:61.19ms
step:681/2330 train_time:41672ms step_avg:61.19ms
step:682/2330 train_time:41734ms step_avg:61.19ms
step:683/2330 train_time:41794ms step_avg:61.19ms
step:684/2330 train_time:41856ms step_avg:61.19ms
step:685/2330 train_time:41916ms step_avg:61.19ms
step:686/2330 train_time:41978ms step_avg:61.19ms
step:687/2330 train_time:42038ms step_avg:61.19ms
step:688/2330 train_time:42101ms step_avg:61.19ms
step:689/2330 train_time:42160ms step_avg:61.19ms
step:690/2330 train_time:42222ms step_avg:61.19ms
step:691/2330 train_time:42282ms step_avg:61.19ms
step:692/2330 train_time:42345ms step_avg:61.19ms
step:693/2330 train_time:42405ms step_avg:61.19ms
step:694/2330 train_time:42468ms step_avg:61.19ms
step:695/2330 train_time:42528ms step_avg:61.19ms
step:696/2330 train_time:42590ms step_avg:61.19ms
step:697/2330 train_time:42650ms step_avg:61.19ms
step:698/2330 train_time:42712ms step_avg:61.19ms
step:699/2330 train_time:42772ms step_avg:61.19ms
step:700/2330 train_time:42834ms step_avg:61.19ms
step:701/2330 train_time:42894ms step_avg:61.19ms
step:702/2330 train_time:42957ms step_avg:61.19ms
step:703/2330 train_time:43017ms step_avg:61.19ms
step:704/2330 train_time:43079ms step_avg:61.19ms
step:705/2330 train_time:43139ms step_avg:61.19ms
step:706/2330 train_time:43201ms step_avg:61.19ms
step:707/2330 train_time:43261ms step_avg:61.19ms
step:708/2330 train_time:43323ms step_avg:61.19ms
step:709/2330 train_time:43383ms step_avg:61.19ms
step:710/2330 train_time:43446ms step_avg:61.19ms
step:711/2330 train_time:43507ms step_avg:61.19ms
step:712/2330 train_time:43569ms step_avg:61.19ms
step:713/2330 train_time:43630ms step_avg:61.19ms
step:714/2330 train_time:43692ms step_avg:61.19ms
step:715/2330 train_time:43752ms step_avg:61.19ms
step:716/2330 train_time:43814ms step_avg:61.19ms
step:717/2330 train_time:43875ms step_avg:61.19ms
step:718/2330 train_time:43937ms step_avg:61.19ms
step:719/2330 train_time:43997ms step_avg:61.19ms
step:720/2330 train_time:44059ms step_avg:61.19ms
step:721/2330 train_time:44119ms step_avg:61.19ms
step:722/2330 train_time:44181ms step_avg:61.19ms
step:723/2330 train_time:44241ms step_avg:61.19ms
step:724/2330 train_time:44303ms step_avg:61.19ms
step:725/2330 train_time:44363ms step_avg:61.19ms
step:726/2330 train_time:44425ms step_avg:61.19ms
step:727/2330 train_time:44485ms step_avg:61.19ms
step:728/2330 train_time:44548ms step_avg:61.19ms
step:729/2330 train_time:44608ms step_avg:61.19ms
step:730/2330 train_time:44671ms step_avg:61.19ms
step:731/2330 train_time:44731ms step_avg:61.19ms
step:732/2330 train_time:44793ms step_avg:61.19ms
step:733/2330 train_time:44853ms step_avg:61.19ms
step:734/2330 train_time:44915ms step_avg:61.19ms
step:735/2330 train_time:44976ms step_avg:61.19ms
step:736/2330 train_time:45038ms step_avg:61.19ms
step:737/2330 train_time:45097ms step_avg:61.19ms
step:738/2330 train_time:45159ms step_avg:61.19ms
step:739/2330 train_time:45219ms step_avg:61.19ms
step:740/2330 train_time:45282ms step_avg:61.19ms
step:741/2330 train_time:45341ms step_avg:61.19ms
step:742/2330 train_time:45404ms step_avg:61.19ms
step:743/2330 train_time:45465ms step_avg:61.19ms
step:744/2330 train_time:45527ms step_avg:61.19ms
step:745/2330 train_time:45588ms step_avg:61.19ms
step:746/2330 train_time:45650ms step_avg:61.19ms
step:747/2330 train_time:45710ms step_avg:61.19ms
step:748/2330 train_time:45772ms step_avg:61.19ms
step:749/2330 train_time:45833ms step_avg:61.19ms
step:750/2330 train_time:45895ms step_avg:61.19ms
step:750/2330 val_loss:4.3325 train_time:45959ms step_avg:61.28ms
step:751/2330 train_time:45982ms step_avg:61.23ms
step:752/2330 train_time:46021ms step_avg:61.20ms
step:753/2330 train_time:46086ms step_avg:61.20ms
step:754/2330 train_time:46151ms step_avg:61.21ms
step:755/2330 train_time:46211ms step_avg:61.21ms
step:756/2330 train_time:46273ms step_avg:61.21ms
step:757/2330 train_time:46332ms step_avg:61.20ms
step:758/2330 train_time:46394ms step_avg:61.21ms
step:759/2330 train_time:46453ms step_avg:61.20ms
step:760/2330 train_time:46514ms step_avg:61.20ms
step:761/2330 train_time:46573ms step_avg:61.20ms
step:762/2330 train_time:46635ms step_avg:61.20ms
step:763/2330 train_time:46694ms step_avg:61.20ms
step:764/2330 train_time:46755ms step_avg:61.20ms
step:765/2330 train_time:46814ms step_avg:61.19ms
step:766/2330 train_time:46877ms step_avg:61.20ms
step:767/2330 train_time:46938ms step_avg:61.20ms
step:768/2330 train_time:47002ms step_avg:61.20ms
step:769/2330 train_time:47064ms step_avg:61.20ms
step:770/2330 train_time:47129ms step_avg:61.21ms
step:771/2330 train_time:47190ms step_avg:61.21ms
step:772/2330 train_time:47253ms step_avg:61.21ms
step:773/2330 train_time:47314ms step_avg:61.21ms
step:774/2330 train_time:47377ms step_avg:61.21ms
step:775/2330 train_time:47437ms step_avg:61.21ms
step:776/2330 train_time:47500ms step_avg:61.21ms
step:777/2330 train_time:47561ms step_avg:61.21ms
step:778/2330 train_time:47623ms step_avg:61.21ms
step:779/2330 train_time:47683ms step_avg:61.21ms
step:780/2330 train_time:47746ms step_avg:61.21ms
step:781/2330 train_time:47806ms step_avg:61.21ms
step:782/2330 train_time:47869ms step_avg:61.21ms
step:783/2330 train_time:47930ms step_avg:61.21ms
step:784/2330 train_time:47993ms step_avg:61.22ms
step:785/2330 train_time:48055ms step_avg:61.22ms
step:786/2330 train_time:48118ms step_avg:61.22ms
step:787/2330 train_time:48180ms step_avg:61.22ms
step:788/2330 train_time:48244ms step_avg:61.22ms
step:789/2330 train_time:48305ms step_avg:61.22ms
step:790/2330 train_time:48368ms step_avg:61.23ms
step:791/2330 train_time:48430ms step_avg:61.23ms
step:792/2330 train_time:48492ms step_avg:61.23ms
step:793/2330 train_time:48553ms step_avg:61.23ms
step:794/2330 train_time:48615ms step_avg:61.23ms
step:795/2330 train_time:48676ms step_avg:61.23ms
step:796/2330 train_time:48738ms step_avg:61.23ms
step:797/2330 train_time:48798ms step_avg:61.23ms
step:798/2330 train_time:48861ms step_avg:61.23ms
step:799/2330 train_time:48923ms step_avg:61.23ms
step:800/2330 train_time:48986ms step_avg:61.23ms
step:801/2330 train_time:49047ms step_avg:61.23ms
step:802/2330 train_time:49111ms step_avg:61.24ms
step:803/2330 train_time:49172ms step_avg:61.24ms
step:804/2330 train_time:49236ms step_avg:61.24ms
step:805/2330 train_time:49296ms step_avg:61.24ms
step:806/2330 train_time:49359ms step_avg:61.24ms
step:807/2330 train_time:49420ms step_avg:61.24ms
step:808/2330 train_time:49483ms step_avg:61.24ms
step:809/2330 train_time:49544ms step_avg:61.24ms
step:810/2330 train_time:49607ms step_avg:61.24ms
step:811/2330 train_time:49668ms step_avg:61.24ms
step:812/2330 train_time:49731ms step_avg:61.25ms
step:813/2330 train_time:49792ms step_avg:61.25ms
step:814/2330 train_time:49856ms step_avg:61.25ms
step:815/2330 train_time:49916ms step_avg:61.25ms
step:816/2330 train_time:49978ms step_avg:61.25ms
step:817/2330 train_time:50040ms step_avg:61.25ms
step:818/2330 train_time:50105ms step_avg:61.25ms
step:819/2330 train_time:50167ms step_avg:61.25ms
step:820/2330 train_time:50230ms step_avg:61.26ms
step:821/2330 train_time:50291ms step_avg:61.26ms
step:822/2330 train_time:50355ms step_avg:61.26ms
step:823/2330 train_time:50415ms step_avg:61.26ms
step:824/2330 train_time:50477ms step_avg:61.26ms
step:825/2330 train_time:50538ms step_avg:61.26ms
step:826/2330 train_time:50602ms step_avg:61.26ms
step:827/2330 train_time:50663ms step_avg:61.26ms
step:828/2330 train_time:50727ms step_avg:61.26ms
step:829/2330 train_time:50787ms step_avg:61.26ms
step:830/2330 train_time:50850ms step_avg:61.27ms
step:831/2330 train_time:50911ms step_avg:61.26ms
step:832/2330 train_time:50974ms step_avg:61.27ms
step:833/2330 train_time:51034ms step_avg:61.27ms
step:834/2330 train_time:51097ms step_avg:61.27ms
step:835/2330 train_time:51158ms step_avg:61.27ms
step:836/2330 train_time:51222ms step_avg:61.27ms
step:837/2330 train_time:51283ms step_avg:61.27ms
step:838/2330 train_time:51346ms step_avg:61.27ms
step:839/2330 train_time:51407ms step_avg:61.27ms
step:840/2330 train_time:51470ms step_avg:61.27ms
step:841/2330 train_time:51531ms step_avg:61.27ms
step:842/2330 train_time:51593ms step_avg:61.27ms
step:843/2330 train_time:51654ms step_avg:61.27ms
step:844/2330 train_time:51717ms step_avg:61.28ms
step:845/2330 train_time:51778ms step_avg:61.28ms
step:846/2330 train_time:51841ms step_avg:61.28ms
step:847/2330 train_time:51903ms step_avg:61.28ms
step:848/2330 train_time:51966ms step_avg:61.28ms
step:849/2330 train_time:52027ms step_avg:61.28ms
step:850/2330 train_time:52090ms step_avg:61.28ms
step:851/2330 train_time:52151ms step_avg:61.28ms
step:852/2330 train_time:52215ms step_avg:61.28ms
step:853/2330 train_time:52275ms step_avg:61.28ms
step:854/2330 train_time:52338ms step_avg:61.29ms
step:855/2330 train_time:52399ms step_avg:61.29ms
step:856/2330 train_time:52462ms step_avg:61.29ms
step:857/2330 train_time:52523ms step_avg:61.29ms
step:858/2330 train_time:52586ms step_avg:61.29ms
step:859/2330 train_time:52647ms step_avg:61.29ms
step:860/2330 train_time:52711ms step_avg:61.29ms
step:861/2330 train_time:52772ms step_avg:61.29ms
step:862/2330 train_time:52834ms step_avg:61.29ms
step:863/2330 train_time:52895ms step_avg:61.29ms
step:864/2330 train_time:52958ms step_avg:61.29ms
step:865/2330 train_time:53019ms step_avg:61.29ms
step:866/2330 train_time:53082ms step_avg:61.30ms
step:867/2330 train_time:53144ms step_avg:61.30ms
step:868/2330 train_time:53208ms step_avg:61.30ms
step:869/2330 train_time:53268ms step_avg:61.30ms
step:870/2330 train_time:53331ms step_avg:61.30ms
step:871/2330 train_time:53392ms step_avg:61.30ms
step:872/2330 train_time:53456ms step_avg:61.30ms
step:873/2330 train_time:53517ms step_avg:61.30ms
step:874/2330 train_time:53579ms step_avg:61.30ms
step:875/2330 train_time:53640ms step_avg:61.30ms
step:876/2330 train_time:53704ms step_avg:61.31ms
step:877/2330 train_time:53766ms step_avg:61.31ms
step:878/2330 train_time:53829ms step_avg:61.31ms
step:879/2330 train_time:53890ms step_avg:61.31ms
step:880/2330 train_time:53953ms step_avg:61.31ms
step:881/2330 train_time:54014ms step_avg:61.31ms
step:882/2330 train_time:54077ms step_avg:61.31ms
step:883/2330 train_time:54137ms step_avg:61.31ms
step:884/2330 train_time:54200ms step_avg:61.31ms
step:885/2330 train_time:54261ms step_avg:61.31ms
step:886/2330 train_time:54325ms step_avg:61.31ms
step:887/2330 train_time:54386ms step_avg:61.31ms
step:888/2330 train_time:54449ms step_avg:61.32ms
step:889/2330 train_time:54511ms step_avg:61.32ms
step:890/2330 train_time:54573ms step_avg:61.32ms
step:891/2330 train_time:54634ms step_avg:61.32ms
step:892/2330 train_time:54697ms step_avg:61.32ms
step:893/2330 train_time:54759ms step_avg:61.32ms
step:894/2330 train_time:54823ms step_avg:61.32ms
step:895/2330 train_time:54884ms step_avg:61.32ms
step:896/2330 train_time:54948ms step_avg:61.33ms
step:897/2330 train_time:55008ms step_avg:61.32ms
step:898/2330 train_time:55071ms step_avg:61.33ms
step:899/2330 train_time:55132ms step_avg:61.33ms
step:900/2330 train_time:55195ms step_avg:61.33ms
step:901/2330 train_time:55256ms step_avg:61.33ms
step:902/2330 train_time:55320ms step_avg:61.33ms
step:903/2330 train_time:55381ms step_avg:61.33ms
step:904/2330 train_time:55444ms step_avg:61.33ms
step:905/2330 train_time:55506ms step_avg:61.33ms
step:906/2330 train_time:55569ms step_avg:61.33ms
step:907/2330 train_time:55630ms step_avg:61.33ms
step:908/2330 train_time:55693ms step_avg:61.34ms
step:909/2330 train_time:55754ms step_avg:61.34ms
step:910/2330 train_time:55818ms step_avg:61.34ms
step:911/2330 train_time:55879ms step_avg:61.34ms
step:912/2330 train_time:55943ms step_avg:61.34ms
step:913/2330 train_time:56004ms step_avg:61.34ms
step:914/2330 train_time:56067ms step_avg:61.34ms
step:915/2330 train_time:56128ms step_avg:61.34ms
step:916/2330 train_time:56191ms step_avg:61.34ms
step:917/2330 train_time:56252ms step_avg:61.34ms
step:918/2330 train_time:56315ms step_avg:61.35ms
step:919/2330 train_time:56375ms step_avg:61.34ms
step:920/2330 train_time:56438ms step_avg:61.35ms
step:921/2330 train_time:56500ms step_avg:61.35ms
step:922/2330 train_time:56562ms step_avg:61.35ms
step:923/2330 train_time:56624ms step_avg:61.35ms
step:924/2330 train_time:56687ms step_avg:61.35ms
step:925/2330 train_time:56748ms step_avg:61.35ms
step:926/2330 train_time:56811ms step_avg:61.35ms
step:927/2330 train_time:56873ms step_avg:61.35ms
step:928/2330 train_time:56936ms step_avg:61.35ms
step:929/2330 train_time:56997ms step_avg:61.35ms
step:930/2330 train_time:57060ms step_avg:61.35ms
step:931/2330 train_time:57121ms step_avg:61.35ms
step:932/2330 train_time:57184ms step_avg:61.36ms
step:933/2330 train_time:57245ms step_avg:61.36ms
step:934/2330 train_time:57308ms step_avg:61.36ms
step:935/2330 train_time:57369ms step_avg:61.36ms
step:936/2330 train_time:57432ms step_avg:61.36ms
step:937/2330 train_time:57493ms step_avg:61.36ms
step:938/2330 train_time:57556ms step_avg:61.36ms
step:939/2330 train_time:57616ms step_avg:61.36ms
step:940/2330 train_time:57679ms step_avg:61.36ms
step:941/2330 train_time:57740ms step_avg:61.36ms
step:942/2330 train_time:57804ms step_avg:61.36ms
step:943/2330 train_time:57865ms step_avg:61.36ms
step:944/2330 train_time:57928ms step_avg:61.36ms
step:945/2330 train_time:57989ms step_avg:61.36ms
step:946/2330 train_time:58053ms step_avg:61.37ms
step:947/2330 train_time:58114ms step_avg:61.37ms
step:948/2330 train_time:58176ms step_avg:61.37ms
step:949/2330 train_time:58237ms step_avg:61.37ms
step:950/2330 train_time:58300ms step_avg:61.37ms
step:951/2330 train_time:58362ms step_avg:61.37ms
step:952/2330 train_time:58426ms step_avg:61.37ms
step:953/2330 train_time:58487ms step_avg:61.37ms
step:954/2330 train_time:58550ms step_avg:61.37ms
step:955/2330 train_time:58611ms step_avg:61.37ms
step:956/2330 train_time:58674ms step_avg:61.37ms
step:957/2330 train_time:58735ms step_avg:61.37ms
step:958/2330 train_time:58798ms step_avg:61.38ms
step:959/2330 train_time:58859ms step_avg:61.38ms
step:960/2330 train_time:58923ms step_avg:61.38ms
step:961/2330 train_time:58985ms step_avg:61.38ms
step:962/2330 train_time:59048ms step_avg:61.38ms
step:963/2330 train_time:59108ms step_avg:61.38ms
step:964/2330 train_time:59171ms step_avg:61.38ms
step:965/2330 train_time:59232ms step_avg:61.38ms
step:966/2330 train_time:59295ms step_avg:61.38ms
step:967/2330 train_time:59357ms step_avg:61.38ms
step:968/2330 train_time:59420ms step_avg:61.38ms
step:969/2330 train_time:59481ms step_avg:61.38ms
step:970/2330 train_time:59545ms step_avg:61.39ms
step:971/2330 train_time:59606ms step_avg:61.39ms
step:972/2330 train_time:59669ms step_avg:61.39ms
step:973/2330 train_time:59729ms step_avg:61.39ms
step:974/2330 train_time:59793ms step_avg:61.39ms
step:975/2330 train_time:59854ms step_avg:61.39ms
step:976/2330 train_time:59917ms step_avg:61.39ms
step:977/2330 train_time:59977ms step_avg:61.39ms
step:978/2330 train_time:60041ms step_avg:61.39ms
step:979/2330 train_time:60102ms step_avg:61.39ms
step:980/2330 train_time:60166ms step_avg:61.39ms
step:981/2330 train_time:60228ms step_avg:61.39ms
step:982/2330 train_time:60291ms step_avg:61.40ms
step:983/2330 train_time:60352ms step_avg:61.40ms
step:984/2330 train_time:60415ms step_avg:61.40ms
step:985/2330 train_time:60475ms step_avg:61.40ms
step:986/2330 train_time:60538ms step_avg:61.40ms
step:987/2330 train_time:60600ms step_avg:61.40ms
step:988/2330 train_time:60663ms step_avg:61.40ms
step:989/2330 train_time:60725ms step_avg:61.40ms
step:990/2330 train_time:60788ms step_avg:61.40ms
step:991/2330 train_time:60849ms step_avg:61.40ms
step:992/2330 train_time:60913ms step_avg:61.40ms
step:993/2330 train_time:60973ms step_avg:61.40ms
step:994/2330 train_time:61036ms step_avg:61.40ms
step:995/2330 train_time:61097ms step_avg:61.40ms
step:996/2330 train_time:61160ms step_avg:61.41ms
step:997/2330 train_time:61221ms step_avg:61.41ms
step:998/2330 train_time:61285ms step_avg:61.41ms
step:999/2330 train_time:61346ms step_avg:61.41ms
step:1000/2330 train_time:61409ms step_avg:61.41ms
step:1000/2330 val_loss:4.1282 train_time:61475ms step_avg:61.48ms
step:1001/2330 train_time:61497ms step_avg:61.44ms
step:1002/2330 train_time:61536ms step_avg:61.41ms
step:1003/2330 train_time:61601ms step_avg:61.42ms
step:1004/2330 train_time:61666ms step_avg:61.42ms
step:1005/2330 train_time:61728ms step_avg:61.42ms
step:1006/2330 train_time:61792ms step_avg:61.42ms
step:1007/2330 train_time:61852ms step_avg:61.42ms
step:1008/2330 train_time:61914ms step_avg:61.42ms
step:1009/2330 train_time:61974ms step_avg:61.42ms
step:1010/2330 train_time:62036ms step_avg:61.42ms
step:1011/2330 train_time:62096ms step_avg:61.42ms
step:1012/2330 train_time:62158ms step_avg:61.42ms
step:1013/2330 train_time:62218ms step_avg:61.42ms
step:1014/2330 train_time:62280ms step_avg:61.42ms
step:1015/2330 train_time:62340ms step_avg:61.42ms
step:1016/2330 train_time:62408ms step_avg:61.42ms
step:1017/2330 train_time:62471ms step_avg:61.43ms
step:1018/2330 train_time:62536ms step_avg:61.43ms
step:1019/2330 train_time:62598ms step_avg:61.43ms
step:1020/2330 train_time:62661ms step_avg:61.43ms
step:1021/2330 train_time:62723ms step_avg:61.43ms
step:1022/2330 train_time:62786ms step_avg:61.43ms
step:1023/2330 train_time:62846ms step_avg:61.43ms
step:1024/2330 train_time:62908ms step_avg:61.43ms
step:1025/2330 train_time:62968ms step_avg:61.43ms
step:1026/2330 train_time:63031ms step_avg:61.43ms
step:1027/2330 train_time:63092ms step_avg:61.43ms
step:1028/2330 train_time:63155ms step_avg:61.43ms
step:1029/2330 train_time:63214ms step_avg:61.43ms
step:1030/2330 train_time:63277ms step_avg:61.43ms
step:1031/2330 train_time:63338ms step_avg:61.43ms
step:1032/2330 train_time:63401ms step_avg:61.44ms
step:1033/2330 train_time:63463ms step_avg:61.44ms
step:1034/2330 train_time:63527ms step_avg:61.44ms
step:1035/2330 train_time:63589ms step_avg:61.44ms
step:1036/2330 train_time:63653ms step_avg:61.44ms
step:1037/2330 train_time:63714ms step_avg:61.44ms
step:1038/2330 train_time:63777ms step_avg:61.44ms
step:1039/2330 train_time:63837ms step_avg:61.44ms
step:1040/2330 train_time:63901ms step_avg:61.44ms
step:1041/2330 train_time:63962ms step_avg:61.44ms
step:1042/2330 train_time:64024ms step_avg:61.44ms
step:1043/2330 train_time:64084ms step_avg:61.44ms
step:1044/2330 train_time:64147ms step_avg:61.44ms
step:1045/2330 train_time:64207ms step_avg:61.44ms
step:1046/2330 train_time:64271ms step_avg:61.44ms
step:1047/2330 train_time:64333ms step_avg:61.45ms
step:1048/2330 train_time:64396ms step_avg:61.45ms
step:1049/2330 train_time:64457ms step_avg:61.45ms
step:1050/2330 train_time:64522ms step_avg:61.45ms
step:1051/2330 train_time:64583ms step_avg:61.45ms
step:1052/2330 train_time:64646ms step_avg:61.45ms
step:1053/2330 train_time:64708ms step_avg:61.45ms
step:1054/2330 train_time:64771ms step_avg:61.45ms
step:1055/2330 train_time:64833ms step_avg:61.45ms
step:1056/2330 train_time:64896ms step_avg:61.45ms
step:1057/2330 train_time:64957ms step_avg:61.45ms
step:1058/2330 train_time:65021ms step_avg:61.46ms
step:1059/2330 train_time:65081ms step_avg:61.46ms
step:1060/2330 train_time:65144ms step_avg:61.46ms
step:1061/2330 train_time:65204ms step_avg:61.46ms
step:1062/2330 train_time:65267ms step_avg:61.46ms
step:1063/2330 train_time:65328ms step_avg:61.46ms
step:1064/2330 train_time:65392ms step_avg:61.46ms
step:1065/2330 train_time:65453ms step_avg:61.46ms
step:1066/2330 train_time:65516ms step_avg:61.46ms
step:1067/2330 train_time:65577ms step_avg:61.46ms
step:1068/2330 train_time:65641ms step_avg:61.46ms
step:1069/2330 train_time:65702ms step_avg:61.46ms
step:1070/2330 train_time:65765ms step_avg:61.46ms
step:1071/2330 train_time:65826ms step_avg:61.46ms
step:1072/2330 train_time:65889ms step_avg:61.46ms
step:1073/2330 train_time:65951ms step_avg:61.46ms
step:1074/2330 train_time:66014ms step_avg:61.47ms
step:1075/2330 train_time:66075ms step_avg:61.46ms
step:1076/2330 train_time:66138ms step_avg:61.47ms
step:1077/2330 train_time:66198ms step_avg:61.47ms
step:1078/2330 train_time:66262ms step_avg:61.47ms
step:1079/2330 train_time:66323ms step_avg:61.47ms
step:1080/2330 train_time:66385ms step_avg:61.47ms
step:1081/2330 train_time:66446ms step_avg:61.47ms
step:1082/2330 train_time:66510ms step_avg:61.47ms
step:1083/2330 train_time:66572ms step_avg:61.47ms
step:1084/2330 train_time:66636ms step_avg:61.47ms
step:1085/2330 train_time:66696ms step_avg:61.47ms
step:1086/2330 train_time:66760ms step_avg:61.47ms
step:1087/2330 train_time:66821ms step_avg:61.47ms
step:1088/2330 train_time:66884ms step_avg:61.47ms
step:1089/2330 train_time:66944ms step_avg:61.47ms
step:1090/2330 train_time:67007ms step_avg:61.47ms
step:1091/2330 train_time:67068ms step_avg:61.47ms
step:1092/2330 train_time:67131ms step_avg:61.48ms
step:1093/2330 train_time:67192ms step_avg:61.47ms
step:1094/2330 train_time:67254ms step_avg:61.48ms
step:1095/2330 train_time:67315ms step_avg:61.47ms
step:1096/2330 train_time:67378ms step_avg:61.48ms
step:1097/2330 train_time:67439ms step_avg:61.48ms
step:1098/2330 train_time:67501ms step_avg:61.48ms
step:1099/2330 train_time:67562ms step_avg:61.48ms
step:1100/2330 train_time:67625ms step_avg:61.48ms
step:1101/2330 train_time:67687ms step_avg:61.48ms
step:1102/2330 train_time:67751ms step_avg:61.48ms
step:1103/2330 train_time:67812ms step_avg:61.48ms
step:1104/2330 train_time:67875ms step_avg:61.48ms
step:1105/2330 train_time:67935ms step_avg:61.48ms
step:1106/2330 train_time:67999ms step_avg:61.48ms
step:1107/2330 train_time:68059ms step_avg:61.48ms
step:1108/2330 train_time:68122ms step_avg:61.48ms
step:1109/2330 train_time:68183ms step_avg:61.48ms
step:1110/2330 train_time:68246ms step_avg:61.48ms
step:1111/2330 train_time:68306ms step_avg:61.48ms
step:1112/2330 train_time:68370ms step_avg:61.48ms
step:1113/2330 train_time:68432ms step_avg:61.48ms
step:1114/2330 train_time:68494ms step_avg:61.48ms
step:1115/2330 train_time:68555ms step_avg:61.48ms
step:1116/2330 train_time:68618ms step_avg:61.49ms
step:1117/2330 train_time:68679ms step_avg:61.49ms
step:1118/2330 train_time:68743ms step_avg:61.49ms
step:1119/2330 train_time:68804ms step_avg:61.49ms
step:1120/2330 train_time:68867ms step_avg:61.49ms
step:1121/2330 train_time:68928ms step_avg:61.49ms
step:1122/2330 train_time:68992ms step_avg:61.49ms
step:1123/2330 train_time:69053ms step_avg:61.49ms
step:1124/2330 train_time:69116ms step_avg:61.49ms
step:1125/2330 train_time:69177ms step_avg:61.49ms
step:1126/2330 train_time:69240ms step_avg:61.49ms
step:1127/2330 train_time:69301ms step_avg:61.49ms
step:1128/2330 train_time:69364ms step_avg:61.49ms
step:1129/2330 train_time:69424ms step_avg:61.49ms
step:1130/2330 train_time:69487ms step_avg:61.49ms
step:1131/2330 train_time:69549ms step_avg:61.49ms
step:1132/2330 train_time:69612ms step_avg:61.49ms
step:1133/2330 train_time:69673ms step_avg:61.49ms
step:1134/2330 train_time:69737ms step_avg:61.50ms
step:1135/2330 train_time:69799ms step_avg:61.50ms
step:1136/2330 train_time:69861ms step_avg:61.50ms
step:1137/2330 train_time:69922ms step_avg:61.50ms
step:1138/2330 train_time:69985ms step_avg:61.50ms
step:1139/2330 train_time:70046ms step_avg:61.50ms
step:1140/2330 train_time:70109ms step_avg:61.50ms
step:1141/2330 train_time:70171ms step_avg:61.50ms
step:1142/2330 train_time:70234ms step_avg:61.50ms
step:1143/2330 train_time:70295ms step_avg:61.50ms
step:1144/2330 train_time:70357ms step_avg:61.50ms
step:1145/2330 train_time:70419ms step_avg:61.50ms
step:1146/2330 train_time:70482ms step_avg:61.50ms
step:1147/2330 train_time:70543ms step_avg:61.50ms
step:1148/2330 train_time:70605ms step_avg:61.50ms
step:1149/2330 train_time:70666ms step_avg:61.50ms
step:1150/2330 train_time:70730ms step_avg:61.50ms
step:1151/2330 train_time:70791ms step_avg:61.50ms
step:1152/2330 train_time:70856ms step_avg:61.51ms
step:1153/2330 train_time:70916ms step_avg:61.51ms
step:1154/2330 train_time:70979ms step_avg:61.51ms
step:1155/2330 train_time:71040ms step_avg:61.51ms
step:1156/2330 train_time:71103ms step_avg:61.51ms
step:1157/2330 train_time:71164ms step_avg:61.51ms
step:1158/2330 train_time:71227ms step_avg:61.51ms
step:1159/2330 train_time:71288ms step_avg:61.51ms
step:1160/2330 train_time:71351ms step_avg:61.51ms
step:1161/2330 train_time:71413ms step_avg:61.51ms
step:1162/2330 train_time:71476ms step_avg:61.51ms
step:1163/2330 train_time:71537ms step_avg:61.51ms
step:1164/2330 train_time:71600ms step_avg:61.51ms
step:1165/2330 train_time:71662ms step_avg:61.51ms
step:1166/2330 train_time:71724ms step_avg:61.51ms
step:1167/2330 train_time:71785ms step_avg:61.51ms
step:1168/2330 train_time:71850ms step_avg:61.52ms
step:1169/2330 train_time:71911ms step_avg:61.51ms
step:1170/2330 train_time:71973ms step_avg:61.52ms
step:1171/2330 train_time:72034ms step_avg:61.52ms
step:1172/2330 train_time:72097ms step_avg:61.52ms
step:1173/2330 train_time:72158ms step_avg:61.52ms
step:1174/2330 train_time:72222ms step_avg:61.52ms
step:1175/2330 train_time:72283ms step_avg:61.52ms
step:1176/2330 train_time:72346ms step_avg:61.52ms
step:1177/2330 train_time:72407ms step_avg:61.52ms
step:1178/2330 train_time:72470ms step_avg:61.52ms
step:1179/2330 train_time:72532ms step_avg:61.52ms
step:1180/2330 train_time:72595ms step_avg:61.52ms
step:1181/2330 train_time:72657ms step_avg:61.52ms
step:1182/2330 train_time:72721ms step_avg:61.52ms
step:1183/2330 train_time:72782ms step_avg:61.52ms
step:1184/2330 train_time:72844ms step_avg:61.52ms
step:1185/2330 train_time:72905ms step_avg:61.52ms
step:1186/2330 train_time:72968ms step_avg:61.52ms
step:1187/2330 train_time:73030ms step_avg:61.52ms
step:1188/2330 train_time:73094ms step_avg:61.53ms
step:1189/2330 train_time:73156ms step_avg:61.53ms
step:1190/2330 train_time:73219ms step_avg:61.53ms
step:1191/2330 train_time:73279ms step_avg:61.53ms
step:1192/2330 train_time:73342ms step_avg:61.53ms
step:1193/2330 train_time:73403ms step_avg:61.53ms
step:1194/2330 train_time:73465ms step_avg:61.53ms
step:1195/2330 train_time:73526ms step_avg:61.53ms
step:1196/2330 train_time:73590ms step_avg:61.53ms
step:1197/2330 train_time:73651ms step_avg:61.53ms
step:1198/2330 train_time:73714ms step_avg:61.53ms
step:1199/2330 train_time:73774ms step_avg:61.53ms
step:1200/2330 train_time:73838ms step_avg:61.53ms
step:1201/2330 train_time:73898ms step_avg:61.53ms
step:1202/2330 train_time:73961ms step_avg:61.53ms
step:1203/2330 train_time:74022ms step_avg:61.53ms
step:1204/2330 train_time:74084ms step_avg:61.53ms
step:1205/2330 train_time:74146ms step_avg:61.53ms
step:1206/2330 train_time:74209ms step_avg:61.53ms
step:1207/2330 train_time:74270ms step_avg:61.53ms
step:1208/2330 train_time:74333ms step_avg:61.53ms
step:1209/2330 train_time:74394ms step_avg:61.53ms
step:1210/2330 train_time:74457ms step_avg:61.54ms
step:1211/2330 train_time:74518ms step_avg:61.53ms
step:1212/2330 train_time:74582ms step_avg:61.54ms
step:1213/2330 train_time:74642ms step_avg:61.54ms
step:1214/2330 train_time:74705ms step_avg:61.54ms
step:1215/2330 train_time:74765ms step_avg:61.54ms
step:1216/2330 train_time:74829ms step_avg:61.54ms
step:1217/2330 train_time:74891ms step_avg:61.54ms
step:1218/2330 train_time:74954ms step_avg:61.54ms
step:1219/2330 train_time:75015ms step_avg:61.54ms
step:1220/2330 train_time:75078ms step_avg:61.54ms
step:1221/2330 train_time:75139ms step_avg:61.54ms
step:1222/2330 train_time:75201ms step_avg:61.54ms
step:1223/2330 train_time:75262ms step_avg:61.54ms
step:1224/2330 train_time:75324ms step_avg:61.54ms
step:1225/2330 train_time:75385ms step_avg:61.54ms
step:1226/2330 train_time:75449ms step_avg:61.54ms
step:1227/2330 train_time:75510ms step_avg:61.54ms
step:1228/2330 train_time:75574ms step_avg:61.54ms
step:1229/2330 train_time:75635ms step_avg:61.54ms
step:1230/2330 train_time:75698ms step_avg:61.54ms
step:1231/2330 train_time:75759ms step_avg:61.54ms
step:1232/2330 train_time:75823ms step_avg:61.54ms
step:1233/2330 train_time:75884ms step_avg:61.54ms
step:1234/2330 train_time:75946ms step_avg:61.54ms
step:1235/2330 train_time:76007ms step_avg:61.54ms
step:1236/2330 train_time:76070ms step_avg:61.55ms
step:1237/2330 train_time:76132ms step_avg:61.55ms
step:1238/2330 train_time:76196ms step_avg:61.55ms
step:1239/2330 train_time:76256ms step_avg:61.55ms
step:1240/2330 train_time:76319ms step_avg:61.55ms
step:1241/2330 train_time:76380ms step_avg:61.55ms
step:1242/2330 train_time:76443ms step_avg:61.55ms
step:1243/2330 train_time:76505ms step_avg:61.55ms
step:1244/2330 train_time:76568ms step_avg:61.55ms
step:1245/2330 train_time:76629ms step_avg:61.55ms
step:1246/2330 train_time:76693ms step_avg:61.55ms
step:1247/2330 train_time:76755ms step_avg:61.55ms
step:1248/2330 train_time:76818ms step_avg:61.55ms
step:1249/2330 train_time:76879ms step_avg:61.55ms
step:1250/2330 train_time:76942ms step_avg:61.55ms
step:1250/2330 val_loss:3.9673 train_time:77007ms step_avg:61.61ms
step:1251/2330 train_time:77030ms step_avg:61.58ms
step:1252/2330 train_time:77070ms step_avg:61.56ms
step:1253/2330 train_time:77137ms step_avg:61.56ms
step:1254/2330 train_time:77202ms step_avg:61.56ms
step:1255/2330 train_time:77263ms step_avg:61.56ms
step:1256/2330 train_time:77325ms step_avg:61.56ms
step:1257/2330 train_time:77385ms step_avg:61.56ms
step:1258/2330 train_time:77447ms step_avg:61.56ms
step:1259/2330 train_time:77507ms step_avg:61.56ms
step:1260/2330 train_time:77569ms step_avg:61.56ms
step:1261/2330 train_time:77630ms step_avg:61.56ms
step:1262/2330 train_time:77692ms step_avg:61.56ms
step:1263/2330 train_time:77751ms step_avg:61.56ms
step:1264/2330 train_time:77814ms step_avg:61.56ms
step:1265/2330 train_time:77874ms step_avg:61.56ms
step:1266/2330 train_time:77937ms step_avg:61.56ms
step:1267/2330 train_time:77998ms step_avg:61.56ms
step:1268/2330 train_time:78064ms step_avg:61.56ms
step:1269/2330 train_time:78127ms step_avg:61.57ms
step:1270/2330 train_time:78191ms step_avg:61.57ms
step:1271/2330 train_time:78253ms step_avg:61.57ms
step:1272/2330 train_time:78316ms step_avg:61.57ms
step:1273/2330 train_time:78378ms step_avg:61.57ms
step:1274/2330 train_time:78441ms step_avg:61.57ms
step:1275/2330 train_time:78501ms step_avg:61.57ms
step:1276/2330 train_time:78565ms step_avg:61.57ms
step:1277/2330 train_time:78625ms step_avg:61.57ms
step:1278/2330 train_time:78687ms step_avg:61.57ms
step:1279/2330 train_time:78747ms step_avg:61.57ms
step:1280/2330 train_time:78810ms step_avg:61.57ms
step:1281/2330 train_time:78871ms step_avg:61.57ms
step:1282/2330 train_time:78934ms step_avg:61.57ms
step:1283/2330 train_time:78997ms step_avg:61.57ms
step:1284/2330 train_time:79061ms step_avg:61.57ms
step:1285/2330 train_time:79123ms step_avg:61.57ms
step:1286/2330 train_time:79187ms step_avg:61.58ms
step:1287/2330 train_time:79249ms step_avg:61.58ms
step:1288/2330 train_time:79312ms step_avg:61.58ms
step:1289/2330 train_time:79373ms step_avg:61.58ms
step:1290/2330 train_time:79436ms step_avg:61.58ms
step:1291/2330 train_time:79497ms step_avg:61.58ms
step:1292/2330 train_time:79560ms step_avg:61.58ms
step:1293/2330 train_time:79620ms step_avg:61.58ms
step:1294/2330 train_time:79683ms step_avg:61.58ms
step:1295/2330 train_time:79744ms step_avg:61.58ms
step:1296/2330 train_time:79807ms step_avg:61.58ms
step:1297/2330 train_time:79867ms step_avg:61.58ms
step:1298/2330 train_time:79930ms step_avg:61.58ms
step:1299/2330 train_time:79991ms step_avg:61.58ms
step:1300/2330 train_time:80055ms step_avg:61.58ms
step:1301/2330 train_time:80117ms step_avg:61.58ms
step:1302/2330 train_time:80181ms step_avg:61.58ms
step:1303/2330 train_time:80244ms step_avg:61.58ms
step:1304/2330 train_time:80306ms step_avg:61.58ms
step:1305/2330 train_time:80367ms step_avg:61.58ms
step:1306/2330 train_time:80431ms step_avg:61.59ms
step:1307/2330 train_time:80492ms step_avg:61.59ms
step:1308/2330 train_time:80555ms step_avg:61.59ms
step:1309/2330 train_time:80616ms step_avg:61.59ms
step:1310/2330 train_time:80680ms step_avg:61.59ms
step:1311/2330 train_time:80740ms step_avg:61.59ms
step:1312/2330 train_time:80804ms step_avg:61.59ms
step:1313/2330 train_time:80864ms step_avg:61.59ms
step:1314/2330 train_time:80926ms step_avg:61.59ms
step:1315/2330 train_time:80987ms step_avg:61.59ms
step:1316/2330 train_time:81049ms step_avg:61.59ms
step:1317/2330 train_time:81111ms step_avg:61.59ms
step:1318/2330 train_time:81175ms step_avg:61.59ms
step:1319/2330 train_time:81237ms step_avg:61.59ms
step:1320/2330 train_time:81299ms step_avg:61.59ms
step:1321/2330 train_time:81361ms step_avg:61.59ms
step:1322/2330 train_time:81424ms step_avg:61.59ms
step:1323/2330 train_time:81484ms step_avg:61.59ms
step:1324/2330 train_time:81547ms step_avg:61.59ms
step:1325/2330 train_time:81608ms step_avg:61.59ms
step:1326/2330 train_time:81672ms step_avg:61.59ms
step:1327/2330 train_time:81733ms step_avg:61.59ms
step:1328/2330 train_time:81796ms step_avg:61.59ms
step:1329/2330 train_time:81857ms step_avg:61.59ms
step:1330/2330 train_time:81920ms step_avg:61.59ms
step:1331/2330 train_time:81981ms step_avg:61.59ms
step:1332/2330 train_time:82044ms step_avg:61.59ms
step:1333/2330 train_time:82105ms step_avg:61.59ms
step:1334/2330 train_time:82168ms step_avg:61.59ms
step:1335/2330 train_time:82229ms step_avg:61.59ms
step:1336/2330 train_time:82293ms step_avg:61.60ms
step:1337/2330 train_time:82355ms step_avg:61.60ms
step:1338/2330 train_time:82418ms step_avg:61.60ms
step:1339/2330 train_time:82478ms step_avg:61.60ms
step:1340/2330 train_time:82541ms step_avg:61.60ms
step:1341/2330 train_time:82602ms step_avg:61.60ms
step:1342/2330 train_time:82665ms step_avg:61.60ms
step:1343/2330 train_time:82726ms step_avg:61.60ms
step:1344/2330 train_time:82789ms step_avg:61.60ms
step:1345/2330 train_time:82849ms step_avg:61.60ms
step:1346/2330 train_time:82913ms step_avg:61.60ms
step:1347/2330 train_time:82974ms step_avg:61.60ms
step:1348/2330 train_time:83037ms step_avg:61.60ms
step:1349/2330 train_time:83098ms step_avg:61.60ms
step:1350/2330 train_time:83161ms step_avg:61.60ms
step:1351/2330 train_time:83223ms step_avg:61.60ms
step:1352/2330 train_time:83286ms step_avg:61.60ms
step:1353/2330 train_time:83346ms step_avg:61.60ms
step:1354/2330 train_time:83409ms step_avg:61.60ms
step:1355/2330 train_time:83470ms step_avg:61.60ms
step:1356/2330 train_time:83534ms step_avg:61.60ms
step:1357/2330 train_time:83595ms step_avg:61.60ms
step:1358/2330 train_time:83659ms step_avg:61.60ms
step:1359/2330 train_time:83720ms step_avg:61.60ms
step:1360/2330 train_time:83782ms step_avg:61.60ms
step:1361/2330 train_time:83843ms step_avg:61.60ms
step:1362/2330 train_time:83907ms step_avg:61.61ms
step:1363/2330 train_time:83967ms step_avg:61.60ms
step:1364/2330 train_time:84029ms step_avg:61.60ms
step:1365/2330 train_time:84090ms step_avg:61.60ms
step:1366/2330 train_time:84154ms step_avg:61.61ms
step:1367/2330 train_time:84216ms step_avg:61.61ms
step:1368/2330 train_time:84279ms step_avg:61.61ms
step:1369/2330 train_time:84339ms step_avg:61.61ms
step:1370/2330 train_time:84403ms step_avg:61.61ms
step:1371/2330 train_time:84464ms step_avg:61.61ms
step:1372/2330 train_time:84526ms step_avg:61.61ms
step:1373/2330 train_time:84588ms step_avg:61.61ms
step:1374/2330 train_time:84651ms step_avg:61.61ms
step:1375/2330 train_time:84712ms step_avg:61.61ms
step:1376/2330 train_time:84775ms step_avg:61.61ms
step:1377/2330 train_time:84836ms step_avg:61.61ms
step:1378/2330 train_time:84899ms step_avg:61.61ms
step:1379/2330 train_time:84960ms step_avg:61.61ms
step:1380/2330 train_time:85023ms step_avg:61.61ms
step:1381/2330 train_time:85084ms step_avg:61.61ms
step:1382/2330 train_time:85146ms step_avg:61.61ms
step:1383/2330 train_time:85207ms step_avg:61.61ms
step:1384/2330 train_time:85271ms step_avg:61.61ms
step:1385/2330 train_time:85332ms step_avg:61.61ms
step:1386/2330 train_time:85395ms step_avg:61.61ms
step:1387/2330 train_time:85457ms step_avg:61.61ms
step:1388/2330 train_time:85520ms step_avg:61.61ms
step:1389/2330 train_time:85582ms step_avg:61.61ms
step:1390/2330 train_time:85645ms step_avg:61.61ms
step:1391/2330 train_time:85706ms step_avg:61.61ms
step:1392/2330 train_time:85769ms step_avg:61.62ms
step:1393/2330 train_time:85831ms step_avg:61.62ms
step:1394/2330 train_time:85894ms step_avg:61.62ms
step:1395/2330 train_time:85956ms step_avg:61.62ms
step:1396/2330 train_time:86020ms step_avg:61.62ms
step:1397/2330 train_time:86081ms step_avg:61.62ms
step:1398/2330 train_time:86144ms step_avg:61.62ms
step:1399/2330 train_time:86205ms step_avg:61.62ms
step:1400/2330 train_time:86267ms step_avg:61.62ms
step:1401/2330 train_time:86328ms step_avg:61.62ms
step:1402/2330 train_time:86391ms step_avg:61.62ms
step:1403/2330 train_time:86452ms step_avg:61.62ms
step:1404/2330 train_time:86515ms step_avg:61.62ms
step:1405/2330 train_time:86576ms step_avg:61.62ms
step:1406/2330 train_time:86639ms step_avg:61.62ms
step:1407/2330 train_time:86700ms step_avg:61.62ms
step:1408/2330 train_time:86763ms step_avg:61.62ms
step:1409/2330 train_time:86824ms step_avg:61.62ms
step:1410/2330 train_time:86887ms step_avg:61.62ms
step:1411/2330 train_time:86948ms step_avg:61.62ms
step:1412/2330 train_time:87011ms step_avg:61.62ms
step:1413/2330 train_time:87072ms step_avg:61.62ms
step:1414/2330 train_time:87136ms step_avg:61.62ms
step:1415/2330 train_time:87197ms step_avg:61.62ms
step:1416/2330 train_time:87260ms step_avg:61.62ms
step:1417/2330 train_time:87321ms step_avg:61.62ms
step:1418/2330 train_time:87384ms step_avg:61.63ms
step:1419/2330 train_time:87445ms step_avg:61.62ms
step:1420/2330 train_time:87507ms step_avg:61.62ms
step:1421/2330 train_time:87569ms step_avg:61.62ms
step:1422/2330 train_time:87632ms step_avg:61.63ms
step:1423/2330 train_time:87694ms step_avg:61.63ms
step:1424/2330 train_time:87757ms step_avg:61.63ms
step:1425/2330 train_time:87818ms step_avg:61.63ms
step:1426/2330 train_time:87881ms step_avg:61.63ms
step:1427/2330 train_time:87943ms step_avg:61.63ms
step:1428/2330 train_time:88006ms step_avg:61.63ms
step:1429/2330 train_time:88067ms step_avg:61.63ms
step:1430/2330 train_time:88129ms step_avg:61.63ms
step:1431/2330 train_time:88190ms step_avg:61.63ms
step:1432/2330 train_time:88253ms step_avg:61.63ms
step:1433/2330 train_time:88315ms step_avg:61.63ms
step:1434/2330 train_time:88379ms step_avg:61.63ms
step:1435/2330 train_time:88439ms step_avg:61.63ms
step:1436/2330 train_time:88502ms step_avg:61.63ms
step:1437/2330 train_time:88563ms step_avg:61.63ms
step:1438/2330 train_time:88626ms step_avg:61.63ms
step:1439/2330 train_time:88686ms step_avg:61.63ms
step:1440/2330 train_time:88749ms step_avg:61.63ms
step:1441/2330 train_time:88810ms step_avg:61.63ms
step:1442/2330 train_time:88875ms step_avg:61.63ms
step:1443/2330 train_time:88936ms step_avg:61.63ms
step:1444/2330 train_time:88999ms step_avg:61.63ms
step:1445/2330 train_time:89061ms step_avg:61.63ms
step:1446/2330 train_time:89124ms step_avg:61.63ms
step:1447/2330 train_time:89185ms step_avg:61.63ms
step:1448/2330 train_time:89248ms step_avg:61.64ms
step:1449/2330 train_time:89309ms step_avg:61.63ms
step:1450/2330 train_time:89373ms step_avg:61.64ms
step:1451/2330 train_time:89434ms step_avg:61.64ms
step:1452/2330 train_time:89497ms step_avg:61.64ms
step:1453/2330 train_time:89558ms step_avg:61.64ms
step:1454/2330 train_time:89621ms step_avg:61.64ms
step:1455/2330 train_time:89683ms step_avg:61.64ms
step:1456/2330 train_time:89745ms step_avg:61.64ms
step:1457/2330 train_time:89806ms step_avg:61.64ms
step:1458/2330 train_time:89869ms step_avg:61.64ms
step:1459/2330 train_time:89931ms step_avg:61.64ms
step:1460/2330 train_time:89996ms step_avg:61.64ms
step:1461/2330 train_time:90057ms step_avg:61.64ms
step:1462/2330 train_time:90120ms step_avg:61.64ms
step:1463/2330 train_time:90181ms step_avg:61.64ms
step:1464/2330 train_time:90245ms step_avg:61.64ms
step:1465/2330 train_time:90306ms step_avg:61.64ms
step:1466/2330 train_time:90369ms step_avg:61.64ms
step:1467/2330 train_time:90430ms step_avg:61.64ms
step:1468/2330 train_time:90493ms step_avg:61.64ms
step:1469/2330 train_time:90555ms step_avg:61.64ms
step:1470/2330 train_time:90618ms step_avg:61.64ms
step:1471/2330 train_time:90679ms step_avg:61.64ms
step:1472/2330 train_time:90742ms step_avg:61.65ms
step:1473/2330 train_time:90803ms step_avg:61.64ms
step:1474/2330 train_time:90867ms step_avg:61.65ms
step:1475/2330 train_time:90927ms step_avg:61.65ms
step:1476/2330 train_time:90990ms step_avg:61.65ms
step:1477/2330 train_time:91052ms step_avg:61.65ms
step:1478/2330 train_time:91115ms step_avg:61.65ms
step:1479/2330 train_time:91177ms step_avg:61.65ms
step:1480/2330 train_time:91239ms step_avg:61.65ms
step:1481/2330 train_time:91300ms step_avg:61.65ms
step:1482/2330 train_time:91364ms step_avg:61.65ms
step:1483/2330 train_time:91424ms step_avg:61.65ms
step:1484/2330 train_time:91486ms step_avg:61.65ms
step:1485/2330 train_time:91547ms step_avg:61.65ms
step:1486/2330 train_time:91610ms step_avg:61.65ms
step:1487/2330 train_time:91672ms step_avg:61.65ms
step:1488/2330 train_time:91735ms step_avg:61.65ms
step:1489/2330 train_time:91797ms step_avg:61.65ms
step:1490/2330 train_time:91860ms step_avg:61.65ms
step:1491/2330 train_time:91922ms step_avg:61.65ms
step:1492/2330 train_time:91985ms step_avg:61.65ms
step:1493/2330 train_time:92045ms step_avg:61.65ms
step:1494/2330 train_time:92108ms step_avg:61.65ms
step:1495/2330 train_time:92169ms step_avg:61.65ms
step:1496/2330 train_time:92233ms step_avg:61.65ms
step:1497/2330 train_time:92293ms step_avg:61.65ms
step:1498/2330 train_time:92357ms step_avg:61.65ms
step:1499/2330 train_time:92417ms step_avg:61.65ms
step:1500/2330 train_time:92480ms step_avg:61.65ms
step:1500/2330 val_loss:3.8703 train_time:92546ms step_avg:61.70ms
step:1501/2330 train_time:92570ms step_avg:61.67ms
step:1502/2330 train_time:92608ms step_avg:61.66ms
step:1503/2330 train_time:92675ms step_avg:61.66ms
step:1504/2330 train_time:92741ms step_avg:61.66ms
step:1505/2330 train_time:92802ms step_avg:61.66ms
step:1506/2330 train_time:92866ms step_avg:61.66ms
step:1507/2330 train_time:92927ms step_avg:61.66ms
step:1508/2330 train_time:92989ms step_avg:61.66ms
step:1509/2330 train_time:93049ms step_avg:61.66ms
step:1510/2330 train_time:93111ms step_avg:61.66ms
step:1511/2330 train_time:93171ms step_avg:61.66ms
step:1512/2330 train_time:93233ms step_avg:61.66ms
step:1513/2330 train_time:93293ms step_avg:61.66ms
step:1514/2330 train_time:93355ms step_avg:61.66ms
step:1515/2330 train_time:93415ms step_avg:61.66ms
step:1516/2330 train_time:93479ms step_avg:61.66ms
step:1517/2330 train_time:93541ms step_avg:61.66ms
step:1518/2330 train_time:93607ms step_avg:61.66ms
step:1519/2330 train_time:93670ms step_avg:61.67ms
step:1520/2330 train_time:93734ms step_avg:61.67ms
step:1521/2330 train_time:93796ms step_avg:61.67ms
step:1522/2330 train_time:93859ms step_avg:61.67ms
step:1523/2330 train_time:93920ms step_avg:61.67ms
step:1524/2330 train_time:93983ms step_avg:61.67ms
step:1525/2330 train_time:94044ms step_avg:61.67ms
step:1526/2330 train_time:94107ms step_avg:61.67ms
step:1527/2330 train_time:94167ms step_avg:61.67ms
step:1528/2330 train_time:94229ms step_avg:61.67ms
step:1529/2330 train_time:94289ms step_avg:61.67ms
step:1530/2330 train_time:94352ms step_avg:61.67ms
step:1531/2330 train_time:94413ms step_avg:61.67ms
step:1532/2330 train_time:94477ms step_avg:61.67ms
step:1533/2330 train_time:94540ms step_avg:61.67ms
step:1534/2330 train_time:94604ms step_avg:61.67ms
step:1535/2330 train_time:94666ms step_avg:61.67ms
step:1536/2330 train_time:94730ms step_avg:61.67ms
step:1537/2330 train_time:94793ms step_avg:61.67ms
step:1538/2330 train_time:94858ms step_avg:61.68ms
step:1539/2330 train_time:94919ms step_avg:61.68ms
step:1540/2330 train_time:94983ms step_avg:61.68ms
step:1541/2330 train_time:95045ms step_avg:61.68ms
step:1542/2330 train_time:95108ms step_avg:61.68ms
step:1543/2330 train_time:95169ms step_avg:61.68ms
step:1544/2330 train_time:95232ms step_avg:61.68ms
step:1545/2330 train_time:95292ms step_avg:61.68ms
step:1546/2330 train_time:95355ms step_avg:61.68ms
step:1547/2330 train_time:95416ms step_avg:61.68ms
step:1548/2330 train_time:95480ms step_avg:61.68ms
step:1549/2330 train_time:95541ms step_avg:61.68ms
step:1550/2330 train_time:95606ms step_avg:61.68ms
step:1551/2330 train_time:95668ms step_avg:61.68ms
step:1552/2330 train_time:95731ms step_avg:61.68ms
step:1553/2330 train_time:95794ms step_avg:61.68ms
step:1554/2330 train_time:95858ms step_avg:61.68ms
step:1555/2330 train_time:95921ms step_avg:61.69ms
step:1556/2330 train_time:95985ms step_avg:61.69ms
step:1557/2330 train_time:96047ms step_avg:61.69ms
step:1558/2330 train_time:96110ms step_avg:61.69ms
step:1559/2330 train_time:96171ms step_avg:61.69ms
step:1560/2330 train_time:96234ms step_avg:61.69ms
step:1561/2330 train_time:96296ms step_avg:61.69ms
step:1562/2330 train_time:96359ms step_avg:61.69ms
step:1563/2330 train_time:96420ms step_avg:61.69ms
step:1564/2330 train_time:96482ms step_avg:61.69ms
step:1565/2330 train_time:96544ms step_avg:61.69ms
step:1566/2330 train_time:96608ms step_avg:61.69ms
step:1567/2330 train_time:96669ms step_avg:61.69ms
step:1568/2330 train_time:96733ms step_avg:61.69ms
step:1569/2330 train_time:96794ms step_avg:61.69ms
step:1570/2330 train_time:96859ms step_avg:61.69ms
step:1571/2330 train_time:96921ms step_avg:61.69ms
step:1572/2330 train_time:96985ms step_avg:61.70ms
step:1573/2330 train_time:97046ms step_avg:61.69ms
step:1574/2330 train_time:97110ms step_avg:61.70ms
step:1575/2330 train_time:97171ms step_avg:61.70ms
step:1576/2330 train_time:97234ms step_avg:61.70ms
step:1577/2330 train_time:97295ms step_avg:61.70ms
step:1578/2330 train_time:97358ms step_avg:61.70ms
step:1579/2330 train_time:97420ms step_avg:61.70ms
step:1580/2330 train_time:97484ms step_avg:61.70ms
step:1581/2330 train_time:97545ms step_avg:61.70ms
step:1582/2330 train_time:97610ms step_avg:61.70ms
step:1583/2330 train_time:97671ms step_avg:61.70ms
step:1584/2330 train_time:97734ms step_avg:61.70ms
step:1585/2330 train_time:97797ms step_avg:61.70ms
step:1586/2330 train_time:97861ms step_avg:61.70ms
step:1587/2330 train_time:97923ms step_avg:61.70ms
step:1588/2330 train_time:97987ms step_avg:61.70ms
step:1589/2330 train_time:98048ms step_avg:61.70ms
step:1590/2330 train_time:98112ms step_avg:61.71ms
step:1591/2330 train_time:98174ms step_avg:61.71ms
step:1592/2330 train_time:98237ms step_avg:61.71ms
step:1593/2330 train_time:98299ms step_avg:61.71ms
step:1594/2330 train_time:98362ms step_avg:61.71ms
step:1595/2330 train_time:98424ms step_avg:61.71ms
step:1596/2330 train_time:98487ms step_avg:61.71ms
step:1597/2330 train_time:98548ms step_avg:61.71ms
step:1598/2330 train_time:98612ms step_avg:61.71ms
step:1599/2330 train_time:98673ms step_avg:61.71ms
step:1600/2330 train_time:98737ms step_avg:61.71ms
step:1601/2330 train_time:98799ms step_avg:61.71ms
step:1602/2330 train_time:98862ms step_avg:61.71ms
step:1603/2330 train_time:98924ms step_avg:61.71ms
step:1604/2330 train_time:98988ms step_avg:61.71ms
step:1605/2330 train_time:99049ms step_avg:61.71ms
step:1606/2330 train_time:99113ms step_avg:61.71ms
step:1607/2330 train_time:99174ms step_avg:61.71ms
step:1608/2330 train_time:99237ms step_avg:61.71ms
step:1609/2330 train_time:99299ms step_avg:61.71ms
step:1610/2330 train_time:99362ms step_avg:61.72ms
step:1611/2330 train_time:99423ms step_avg:61.72ms
step:1612/2330 train_time:99487ms step_avg:61.72ms
step:1613/2330 train_time:99548ms step_avg:61.72ms
step:1614/2330 train_time:99611ms step_avg:61.72ms
step:1615/2330 train_time:99672ms step_avg:61.72ms
step:1616/2330 train_time:99736ms step_avg:61.72ms
step:1617/2330 train_time:99798ms step_avg:61.72ms
step:1618/2330 train_time:99861ms step_avg:61.72ms
step:1619/2330 train_time:99923ms step_avg:61.72ms
step:1620/2330 train_time:99987ms step_avg:61.72ms
step:1621/2330 train_time:100049ms step_avg:61.72ms
step:1622/2330 train_time:100112ms step_avg:61.72ms
step:1623/2330 train_time:100174ms step_avg:61.72ms
step:1624/2330 train_time:100238ms step_avg:61.72ms
step:1625/2330 train_time:100300ms step_avg:61.72ms
step:1626/2330 train_time:100363ms step_avg:61.72ms
step:1627/2330 train_time:100425ms step_avg:61.72ms
step:1628/2330 train_time:100488ms step_avg:61.72ms
step:1629/2330 train_time:100548ms step_avg:61.72ms
step:1630/2330 train_time:100612ms step_avg:61.72ms
step:1631/2330 train_time:100674ms step_avg:61.73ms
step:1632/2330 train_time:100738ms step_avg:61.73ms
step:1633/2330 train_time:100801ms step_avg:61.73ms
step:1634/2330 train_time:100863ms step_avg:61.73ms
step:1635/2330 train_time:100925ms step_avg:61.73ms
step:1636/2330 train_time:100989ms step_avg:61.73ms
step:1637/2330 train_time:101050ms step_avg:61.73ms
step:1638/2330 train_time:101114ms step_avg:61.73ms
step:1639/2330 train_time:101175ms step_avg:61.73ms
step:1640/2330 train_time:101240ms step_avg:61.73ms
step:1641/2330 train_time:101302ms step_avg:61.73ms
step:1642/2330 train_time:101365ms step_avg:61.73ms
step:1643/2330 train_time:101427ms step_avg:61.73ms
step:1644/2330 train_time:101490ms step_avg:61.73ms
step:1645/2330 train_time:101551ms step_avg:61.73ms
step:1646/2330 train_time:101614ms step_avg:61.73ms
step:1647/2330 train_time:101676ms step_avg:61.73ms
step:1648/2330 train_time:101740ms step_avg:61.74ms
step:1649/2330 train_time:101801ms step_avg:61.74ms
step:1650/2330 train_time:101865ms step_avg:61.74ms
step:1651/2330 train_time:101926ms step_avg:61.74ms
step:1652/2330 train_time:101990ms step_avg:61.74ms
step:1653/2330 train_time:102051ms step_avg:61.74ms
step:1654/2330 train_time:102115ms step_avg:61.74ms
step:1655/2330 train_time:102177ms step_avg:61.74ms
step:1656/2330 train_time:102241ms step_avg:61.74ms
step:1657/2330 train_time:102303ms step_avg:61.74ms
step:1658/2330 train_time:102366ms step_avg:61.74ms
step:1659/2330 train_time:102427ms step_avg:61.74ms
step:1660/2330 train_time:102491ms step_avg:61.74ms
step:1661/2330 train_time:102552ms step_avg:61.74ms
step:1662/2330 train_time:102614ms step_avg:61.74ms
step:1663/2330 train_time:102676ms step_avg:61.74ms
step:1664/2330 train_time:102741ms step_avg:61.74ms
step:1665/2330 train_time:102803ms step_avg:61.74ms
step:1666/2330 train_time:102865ms step_avg:61.74ms
step:1667/2330 train_time:102927ms step_avg:61.74ms
step:1668/2330 train_time:102990ms step_avg:61.74ms
step:1669/2330 train_time:103051ms step_avg:61.74ms
step:1670/2330 train_time:103115ms step_avg:61.75ms
step:1671/2330 train_time:103176ms step_avg:61.75ms
step:1672/2330 train_time:103240ms step_avg:61.75ms
step:1673/2330 train_time:103301ms step_avg:61.75ms
step:1674/2330 train_time:103365ms step_avg:61.75ms
step:1675/2330 train_time:103427ms step_avg:61.75ms
step:1676/2330 train_time:103491ms step_avg:61.75ms
step:1677/2330 train_time:103551ms step_avg:61.75ms
step:1678/2330 train_time:103615ms step_avg:61.75ms
step:1679/2330 train_time:103677ms step_avg:61.75ms
step:1680/2330 train_time:103742ms step_avg:61.75ms
step:1681/2330 train_time:103803ms step_avg:61.75ms
step:1682/2330 train_time:103867ms step_avg:61.75ms
step:1683/2330 train_time:103928ms step_avg:61.75ms
step:1684/2330 train_time:103992ms step_avg:61.75ms
step:1685/2330 train_time:104052ms step_avg:61.75ms
step:1686/2330 train_time:104116ms step_avg:61.75ms
step:1687/2330 train_time:104179ms step_avg:61.75ms
step:1688/2330 train_time:104242ms step_avg:61.75ms
step:1689/2330 train_time:104305ms step_avg:61.76ms
step:1690/2330 train_time:104368ms step_avg:61.76ms
step:1691/2330 train_time:104429ms step_avg:61.76ms
step:1692/2330 train_time:104492ms step_avg:61.76ms
step:1693/2330 train_time:104554ms step_avg:61.76ms
step:1694/2330 train_time:104618ms step_avg:61.76ms
step:1695/2330 train_time:104680ms step_avg:61.76ms
step:1696/2330 train_time:104743ms step_avg:61.76ms
step:1697/2330 train_time:104805ms step_avg:61.76ms
step:1698/2330 train_time:104868ms step_avg:61.76ms
step:1699/2330 train_time:104929ms step_avg:61.76ms
step:1700/2330 train_time:104993ms step_avg:61.76ms
step:1701/2330 train_time:105054ms step_avg:61.76ms
step:1702/2330 train_time:105118ms step_avg:61.76ms
step:1703/2330 train_time:105178ms step_avg:61.76ms
step:1704/2330 train_time:105242ms step_avg:61.76ms
step:1705/2330 train_time:105304ms step_avg:61.76ms
step:1706/2330 train_time:105367ms step_avg:61.76ms
step:1707/2330 train_time:105429ms step_avg:61.76ms
step:1708/2330 train_time:105492ms step_avg:61.76ms
step:1709/2330 train_time:105553ms step_avg:61.76ms
step:1710/2330 train_time:105617ms step_avg:61.76ms
step:1711/2330 train_time:105679ms step_avg:61.76ms
step:1712/2330 train_time:105744ms step_avg:61.77ms
step:1713/2330 train_time:105806ms step_avg:61.77ms
step:1714/2330 train_time:105869ms step_avg:61.77ms
step:1715/2330 train_time:105930ms step_avg:61.77ms
step:1716/2330 train_time:105993ms step_avg:61.77ms
step:1717/2330 train_time:106054ms step_avg:61.77ms
step:1718/2330 train_time:106117ms step_avg:61.77ms
step:1719/2330 train_time:106179ms step_avg:61.77ms
step:1720/2330 train_time:106243ms step_avg:61.77ms
step:1721/2330 train_time:106305ms step_avg:61.77ms
step:1722/2330 train_time:106369ms step_avg:61.77ms
step:1723/2330 train_time:106430ms step_avg:61.77ms
step:1724/2330 train_time:106493ms step_avg:61.77ms
step:1725/2330 train_time:106555ms step_avg:61.77ms
step:1726/2330 train_time:106619ms step_avg:61.77ms
step:1727/2330 train_time:106681ms step_avg:61.77ms
step:1728/2330 train_time:106746ms step_avg:61.77ms
step:1729/2330 train_time:106808ms step_avg:61.77ms
step:1730/2330 train_time:106871ms step_avg:61.78ms
step:1731/2330 train_time:106932ms step_avg:61.77ms
step:1732/2330 train_time:106996ms step_avg:61.78ms
step:1733/2330 train_time:107057ms step_avg:61.78ms
step:1734/2330 train_time:107121ms step_avg:61.78ms
step:1735/2330 train_time:107183ms step_avg:61.78ms
step:1736/2330 train_time:107247ms step_avg:61.78ms
step:1737/2330 train_time:107309ms step_avg:61.78ms
step:1738/2330 train_time:107372ms step_avg:61.78ms
step:1739/2330 train_time:107433ms step_avg:61.78ms
step:1740/2330 train_time:107497ms step_avg:61.78ms
step:1741/2330 train_time:107559ms step_avg:61.78ms
step:1742/2330 train_time:107622ms step_avg:61.78ms
step:1743/2330 train_time:107684ms step_avg:61.78ms
step:1744/2330 train_time:107748ms step_avg:61.78ms
step:1745/2330 train_time:107809ms step_avg:61.78ms
step:1746/2330 train_time:107873ms step_avg:61.78ms
step:1747/2330 train_time:107934ms step_avg:61.78ms
step:1748/2330 train_time:107998ms step_avg:61.78ms
step:1749/2330 train_time:108060ms step_avg:61.78ms
step:1750/2330 train_time:108123ms step_avg:61.78ms
step:1750/2330 val_loss:3.7684 train_time:108189ms step_avg:61.82ms
step:1751/2330 train_time:108212ms step_avg:61.80ms
step:1752/2330 train_time:108250ms step_avg:61.79ms
step:1753/2330 train_time:108316ms step_avg:61.79ms
step:1754/2330 train_time:108381ms step_avg:61.79ms
step:1755/2330 train_time:108443ms step_avg:61.79ms
step:1756/2330 train_time:108506ms step_avg:61.79ms
step:1757/2330 train_time:108566ms step_avg:61.79ms
step:1758/2330 train_time:108629ms step_avg:61.79ms
step:1759/2330 train_time:108689ms step_avg:61.79ms
step:1760/2330 train_time:108752ms step_avg:61.79ms
step:1761/2330 train_time:108813ms step_avg:61.79ms
step:1762/2330 train_time:108875ms step_avg:61.79ms
step:1763/2330 train_time:108936ms step_avg:61.79ms
step:1764/2330 train_time:108999ms step_avg:61.79ms
step:1765/2330 train_time:109059ms step_avg:61.79ms
step:1766/2330 train_time:109124ms step_avg:61.79ms
step:1767/2330 train_time:109187ms step_avg:61.79ms
step:1768/2330 train_time:109251ms step_avg:61.79ms
step:1769/2330 train_time:109314ms step_avg:61.79ms
step:1770/2330 train_time:109379ms step_avg:61.80ms
step:1771/2330 train_time:109440ms step_avg:61.80ms
step:1772/2330 train_time:109503ms step_avg:61.80ms
step:1773/2330 train_time:109565ms step_avg:61.80ms
step:1774/2330 train_time:109628ms step_avg:61.80ms
step:1775/2330 train_time:109688ms step_avg:61.80ms
step:1776/2330 train_time:109752ms step_avg:61.80ms
step:1777/2330 train_time:109813ms step_avg:61.80ms
step:1778/2330 train_time:109876ms step_avg:61.80ms
step:1779/2330 train_time:109937ms step_avg:61.80ms
step:1780/2330 train_time:109999ms step_avg:61.80ms
step:1781/2330 train_time:110060ms step_avg:61.80ms
step:1782/2330 train_time:110124ms step_avg:61.80ms
step:1783/2330 train_time:110186ms step_avg:61.80ms
step:1784/2330 train_time:110250ms step_avg:61.80ms
step:1785/2330 train_time:110312ms step_avg:61.80ms
step:1786/2330 train_time:110376ms step_avg:61.80ms
step:1787/2330 train_time:110437ms step_avg:61.80ms
step:1788/2330 train_time:110502ms step_avg:61.80ms
step:1789/2330 train_time:110563ms step_avg:61.80ms
step:1790/2330 train_time:110627ms step_avg:61.80ms
step:1791/2330 train_time:110688ms step_avg:61.80ms
step:1792/2330 train_time:110750ms step_avg:61.80ms
step:1793/2330 train_time:110811ms step_avg:61.80ms
step:1794/2330 train_time:110874ms step_avg:61.80ms
step:1795/2330 train_time:110936ms step_avg:61.80ms
step:1796/2330 train_time:110999ms step_avg:61.80ms
step:1797/2330 train_time:111060ms step_avg:61.80ms
step:1798/2330 train_time:111124ms step_avg:61.80ms
step:1799/2330 train_time:111185ms step_avg:61.80ms
step:1800/2330 train_time:111248ms step_avg:61.80ms
step:1801/2330 train_time:111311ms step_avg:61.80ms
step:1802/2330 train_time:111375ms step_avg:61.81ms
step:1803/2330 train_time:111437ms step_avg:61.81ms
step:1804/2330 train_time:111501ms step_avg:61.81ms
step:1805/2330 train_time:111563ms step_avg:61.81ms
step:1806/2330 train_time:111627ms step_avg:61.81ms
step:1807/2330 train_time:111687ms step_avg:61.81ms
step:1808/2330 train_time:111750ms step_avg:61.81ms
step:1809/2330 train_time:111811ms step_avg:61.81ms
step:1810/2330 train_time:111876ms step_avg:61.81ms
step:1811/2330 train_time:111937ms step_avg:61.81ms
step:1812/2330 train_time:112001ms step_avg:61.81ms
step:1813/2330 train_time:112062ms step_avg:61.81ms
step:1814/2330 train_time:112126ms step_avg:61.81ms
step:1815/2330 train_time:112188ms step_avg:61.81ms
step:1816/2330 train_time:112251ms step_avg:61.81ms
step:1817/2330 train_time:112313ms step_avg:61.81ms
step:1818/2330 train_time:112377ms step_avg:61.81ms
step:1819/2330 train_time:112438ms step_avg:61.81ms
step:1820/2330 train_time:112502ms step_avg:61.81ms
step:1821/2330 train_time:112564ms step_avg:61.81ms
step:1822/2330 train_time:112627ms step_avg:61.82ms
step:1823/2330 train_time:112688ms step_avg:61.81ms
step:1824/2330 train_time:112751ms step_avg:61.82ms
step:1825/2330 train_time:112812ms step_avg:61.81ms
step:1826/2330 train_time:112875ms step_avg:61.82ms
step:1827/2330 train_time:112936ms step_avg:61.82ms
step:1828/2330 train_time:113000ms step_avg:61.82ms
step:1829/2330 train_time:113062ms step_avg:61.82ms
step:1830/2330 train_time:113125ms step_avg:61.82ms
step:1831/2330 train_time:113186ms step_avg:61.82ms
step:1832/2330 train_time:113249ms step_avg:61.82ms
step:1833/2330 train_time:113311ms step_avg:61.82ms
step:1834/2330 train_time:113375ms step_avg:61.82ms
step:1835/2330 train_time:113437ms step_avg:61.82ms
step:1836/2330 train_time:113501ms step_avg:61.82ms
step:1837/2330 train_time:113562ms step_avg:61.82ms
step:1838/2330 train_time:113626ms step_avg:61.82ms
step:1839/2330 train_time:113687ms step_avg:61.82ms
step:1840/2330 train_time:113750ms step_avg:61.82ms
step:1841/2330 train_time:113810ms step_avg:61.82ms
step:1842/2330 train_time:113874ms step_avg:61.82ms
step:1843/2330 train_time:113936ms step_avg:61.82ms
step:1844/2330 train_time:113999ms step_avg:61.82ms
step:1845/2330 train_time:114061ms step_avg:61.82ms
step:1846/2330 train_time:114123ms step_avg:61.82ms
step:1847/2330 train_time:114184ms step_avg:61.82ms
step:1848/2330 train_time:114247ms step_avg:61.82ms
step:1849/2330 train_time:114308ms step_avg:61.82ms
step:1850/2330 train_time:114373ms step_avg:61.82ms
step:1851/2330 train_time:114434ms step_avg:61.82ms
step:1852/2330 train_time:114498ms step_avg:61.82ms
step:1853/2330 train_time:114560ms step_avg:61.82ms
step:1854/2330 train_time:114623ms step_avg:61.82ms
step:1855/2330 train_time:114684ms step_avg:61.82ms
step:1856/2330 train_time:114747ms step_avg:61.82ms
step:1857/2330 train_time:114808ms step_avg:61.82ms
step:1858/2330 train_time:114871ms step_avg:61.83ms
step:1859/2330 train_time:114933ms step_avg:61.83ms
step:1860/2330 train_time:114996ms step_avg:61.83ms
step:1861/2330 train_time:115057ms step_avg:61.83ms
step:1862/2330 train_time:115120ms step_avg:61.83ms
step:1863/2330 train_time:115181ms step_avg:61.83ms
step:1864/2330 train_time:115244ms step_avg:61.83ms
step:1865/2330 train_time:115305ms step_avg:61.83ms
step:1866/2330 train_time:115368ms step_avg:61.83ms
step:1867/2330 train_time:115430ms step_avg:61.83ms
step:1868/2330 train_time:115495ms step_avg:61.83ms
step:1869/2330 train_time:115556ms step_avg:61.83ms
step:1870/2330 train_time:115619ms step_avg:61.83ms
step:1871/2330 train_time:115680ms step_avg:61.83ms
step:1872/2330 train_time:115743ms step_avg:61.83ms
step:1873/2330 train_time:115803ms step_avg:61.83ms
step:1874/2330 train_time:115866ms step_avg:61.83ms
step:1875/2330 train_time:115927ms step_avg:61.83ms
step:1876/2330 train_time:115990ms step_avg:61.83ms
step:1877/2330 train_time:116051ms step_avg:61.83ms
step:1878/2330 train_time:116115ms step_avg:61.83ms
step:1879/2330 train_time:116176ms step_avg:61.83ms
step:1880/2330 train_time:116240ms step_avg:61.83ms
step:1881/2330 train_time:116301ms step_avg:61.83ms
step:1882/2330 train_time:116365ms step_avg:61.83ms
step:1883/2330 train_time:116427ms step_avg:61.83ms
step:1884/2330 train_time:116490ms step_avg:61.83ms
step:1885/2330 train_time:116553ms step_avg:61.83ms
step:1886/2330 train_time:116616ms step_avg:61.83ms
step:1887/2330 train_time:116677ms step_avg:61.83ms
step:1888/2330 train_time:116741ms step_avg:61.83ms
step:1889/2330 train_time:116803ms step_avg:61.83ms
step:1890/2330 train_time:116866ms step_avg:61.83ms
step:1891/2330 train_time:116926ms step_avg:61.83ms
step:1892/2330 train_time:116989ms step_avg:61.83ms
step:1893/2330 train_time:117051ms step_avg:61.83ms
step:1894/2330 train_time:117115ms step_avg:61.83ms
step:1895/2330 train_time:117176ms step_avg:61.83ms
step:1896/2330 train_time:117239ms step_avg:61.84ms
step:1897/2330 train_time:117301ms step_avg:61.83ms
step:1898/2330 train_time:117364ms step_avg:61.84ms
step:1899/2330 train_time:117426ms step_avg:61.84ms
step:1900/2330 train_time:117489ms step_avg:61.84ms
step:1901/2330 train_time:117551ms step_avg:61.84ms
step:1902/2330 train_time:117615ms step_avg:61.84ms
step:1903/2330 train_time:117676ms step_avg:61.84ms
step:1904/2330 train_time:117740ms step_avg:61.84ms
step:1905/2330 train_time:117801ms step_avg:61.84ms
step:1906/2330 train_time:117864ms step_avg:61.84ms
step:1907/2330 train_time:117925ms step_avg:61.84ms
step:1908/2330 train_time:117988ms step_avg:61.84ms
step:1909/2330 train_time:118049ms step_avg:61.84ms
step:1910/2330 train_time:118113ms step_avg:61.84ms
step:1911/2330 train_time:118174ms step_avg:61.84ms
step:1912/2330 train_time:118238ms step_avg:61.84ms
step:1913/2330 train_time:118299ms step_avg:61.84ms
step:1914/2330 train_time:118363ms step_avg:61.84ms
step:1915/2330 train_time:118425ms step_avg:61.84ms
step:1916/2330 train_time:118488ms step_avg:61.84ms
step:1917/2330 train_time:118549ms step_avg:61.84ms
step:1918/2330 train_time:118613ms step_avg:61.84ms
step:1919/2330 train_time:118675ms step_avg:61.84ms
step:1920/2330 train_time:118738ms step_avg:61.84ms
step:1921/2330 train_time:118799ms step_avg:61.84ms
step:1922/2330 train_time:118863ms step_avg:61.84ms
step:1923/2330 train_time:118924ms step_avg:61.84ms
step:1924/2330 train_time:118987ms step_avg:61.84ms
step:1925/2330 train_time:119048ms step_avg:61.84ms
step:1926/2330 train_time:119112ms step_avg:61.84ms
step:1927/2330 train_time:119174ms step_avg:61.84ms
step:1928/2330 train_time:119237ms step_avg:61.84ms
step:1929/2330 train_time:119298ms step_avg:61.84ms
step:1930/2330 train_time:119362ms step_avg:61.85ms
step:1931/2330 train_time:119423ms step_avg:61.85ms
step:1932/2330 train_time:119486ms step_avg:61.85ms
step:1933/2330 train_time:119547ms step_avg:61.85ms
step:1934/2330 train_time:119611ms step_avg:61.85ms
step:1935/2330 train_time:119672ms step_avg:61.85ms
step:1936/2330 train_time:119736ms step_avg:61.85ms
step:1937/2330 train_time:119798ms step_avg:61.85ms
step:1938/2330 train_time:119862ms step_avg:61.85ms
step:1939/2330 train_time:119924ms step_avg:61.85ms
step:1940/2330 train_time:119987ms step_avg:61.85ms
step:1941/2330 train_time:120048ms step_avg:61.85ms
step:1942/2330 train_time:120111ms step_avg:61.85ms
step:1943/2330 train_time:120172ms step_avg:61.85ms
step:1944/2330 train_time:120236ms step_avg:61.85ms
step:1945/2330 train_time:120297ms step_avg:61.85ms
step:1946/2330 train_time:120360ms step_avg:61.85ms
step:1947/2330 train_time:120422ms step_avg:61.85ms
step:1948/2330 train_time:120485ms step_avg:61.85ms
step:1949/2330 train_time:120546ms step_avg:61.85ms
step:1950/2330 train_time:120609ms step_avg:61.85ms
step:1951/2330 train_time:120670ms step_avg:61.85ms
step:1952/2330 train_time:120734ms step_avg:61.85ms
step:1953/2330 train_time:120796ms step_avg:61.85ms
step:1954/2330 train_time:120860ms step_avg:61.85ms
step:1955/2330 train_time:120921ms step_avg:61.85ms
step:1956/2330 train_time:120984ms step_avg:61.85ms
step:1957/2330 train_time:121045ms step_avg:61.85ms
step:1958/2330 train_time:121108ms step_avg:61.85ms
step:1959/2330 train_time:121171ms step_avg:61.85ms
step:1960/2330 train_time:121234ms step_avg:61.85ms
step:1961/2330 train_time:121296ms step_avg:61.85ms
step:1962/2330 train_time:121360ms step_avg:61.86ms
step:1963/2330 train_time:121422ms step_avg:61.86ms
step:1964/2330 train_time:121486ms step_avg:61.86ms
step:1965/2330 train_time:121546ms step_avg:61.86ms
step:1966/2330 train_time:121610ms step_avg:61.86ms
step:1967/2330 train_time:121670ms step_avg:61.86ms
step:1968/2330 train_time:121734ms step_avg:61.86ms
step:1969/2330 train_time:121797ms step_avg:61.86ms
step:1970/2330 train_time:121860ms step_avg:61.86ms
step:1971/2330 train_time:121921ms step_avg:61.86ms
step:1972/2330 train_time:121984ms step_avg:61.86ms
step:1973/2330 train_time:122045ms step_avg:61.86ms
step:1974/2330 train_time:122108ms step_avg:61.86ms
step:1975/2330 train_time:122169ms step_avg:61.86ms
step:1976/2330 train_time:122234ms step_avg:61.86ms
step:1977/2330 train_time:122297ms step_avg:61.86ms
step:1978/2330 train_time:122360ms step_avg:61.86ms
step:1979/2330 train_time:122421ms step_avg:61.86ms
step:1980/2330 train_time:122485ms step_avg:61.86ms
step:1981/2330 train_time:122545ms step_avg:61.86ms
step:1982/2330 train_time:122609ms step_avg:61.86ms
step:1983/2330 train_time:122671ms step_avg:61.86ms
step:1984/2330 train_time:122735ms step_avg:61.86ms
step:1985/2330 train_time:122796ms step_avg:61.86ms
step:1986/2330 train_time:122859ms step_avg:61.86ms
step:1987/2330 train_time:122920ms step_avg:61.86ms
step:1988/2330 train_time:122984ms step_avg:61.86ms
step:1989/2330 train_time:123044ms step_avg:61.86ms
step:1990/2330 train_time:123107ms step_avg:61.86ms
step:1991/2330 train_time:123169ms step_avg:61.86ms
step:1992/2330 train_time:123232ms step_avg:61.86ms
step:1993/2330 train_time:123294ms step_avg:61.86ms
step:1994/2330 train_time:123357ms step_avg:61.86ms
step:1995/2330 train_time:123418ms step_avg:61.86ms
step:1996/2330 train_time:123483ms step_avg:61.87ms
step:1997/2330 train_time:123544ms step_avg:61.86ms
step:1998/2330 train_time:123608ms step_avg:61.87ms
step:1999/2330 train_time:123669ms step_avg:61.87ms
step:2000/2330 train_time:123732ms step_avg:61.87ms
step:2000/2330 val_loss:3.7129 train_time:123797ms step_avg:61.90ms
step:2001/2330 train_time:123820ms step_avg:61.88ms
step:2002/2330 train_time:123859ms step_avg:61.87ms
step:2003/2330 train_time:123926ms step_avg:61.87ms
step:2004/2330 train_time:123993ms step_avg:61.87ms
step:2005/2330 train_time:124055ms step_avg:61.87ms
step:2006/2330 train_time:124117ms step_avg:61.87ms
step:2007/2330 train_time:124178ms step_avg:61.87ms
step:2008/2330 train_time:124240ms step_avg:61.87ms
step:2009/2330 train_time:124301ms step_avg:61.87ms
step:2010/2330 train_time:124363ms step_avg:61.87ms
step:2011/2330 train_time:124424ms step_avg:61.87ms
step:2012/2330 train_time:124486ms step_avg:61.87ms
step:2013/2330 train_time:124547ms step_avg:61.87ms
step:2014/2330 train_time:124611ms step_avg:61.87ms
step:2015/2330 train_time:124671ms step_avg:61.87ms
step:2016/2330 train_time:124734ms step_avg:61.87ms
step:2017/2330 train_time:124795ms step_avg:61.87ms
step:2018/2330 train_time:124859ms step_avg:61.87ms
step:2019/2330 train_time:124923ms step_avg:61.87ms
step:2020/2330 train_time:124988ms step_avg:61.88ms
step:2021/2330 train_time:125050ms step_avg:61.88ms
step:2022/2330 train_time:125114ms step_avg:61.88ms
step:2023/2330 train_time:125177ms step_avg:61.88ms
step:2024/2330 train_time:125240ms step_avg:61.88ms
step:2025/2330 train_time:125301ms step_avg:61.88ms
step:2026/2330 train_time:125363ms step_avg:61.88ms
step:2027/2330 train_time:125424ms step_avg:61.88ms
step:2028/2330 train_time:125487ms step_avg:61.88ms
step:2029/2330 train_time:125548ms step_avg:61.88ms
step:2030/2330 train_time:125610ms step_avg:61.88ms
step:2031/2330 train_time:125670ms step_avg:61.88ms
step:2032/2330 train_time:125733ms step_avg:61.88ms
step:2033/2330 train_time:125796ms step_avg:61.88ms
step:2034/2330 train_time:125860ms step_avg:61.88ms
step:2035/2330 train_time:125921ms step_avg:61.88ms
step:2036/2330 train_time:125985ms step_avg:61.88ms
step:2037/2330 train_time:126048ms step_avg:61.88ms
step:2038/2330 train_time:126113ms step_avg:61.88ms
step:2039/2330 train_time:126175ms step_avg:61.88ms
step:2040/2330 train_time:126238ms step_avg:61.88ms
step:2041/2330 train_time:126300ms step_avg:61.88ms
step:2042/2330 train_time:126363ms step_avg:61.88ms
step:2043/2330 train_time:126424ms step_avg:61.88ms
step:2044/2330 train_time:126487ms step_avg:61.88ms
step:2045/2330 train_time:126548ms step_avg:61.88ms
step:2046/2330 train_time:126612ms step_avg:61.88ms
step:2047/2330 train_time:126672ms step_avg:61.88ms
step:2048/2330 train_time:126735ms step_avg:61.88ms
step:2049/2330 train_time:126796ms step_avg:61.88ms
step:2050/2330 train_time:126860ms step_avg:61.88ms
step:2051/2330 train_time:126923ms step_avg:61.88ms
step:2052/2330 train_time:126988ms step_avg:61.88ms
step:2053/2330 train_time:127050ms step_avg:61.89ms
step:2054/2330 train_time:127114ms step_avg:61.89ms
step:2055/2330 train_time:127176ms step_avg:61.89ms
step:2056/2330 train_time:127240ms step_avg:61.89ms
step:2057/2330 train_time:127302ms step_avg:61.89ms
step:2058/2330 train_time:127365ms step_avg:61.89ms
step:2059/2330 train_time:127425ms step_avg:61.89ms
step:2060/2330 train_time:127488ms step_avg:61.89ms
step:2061/2330 train_time:127549ms step_avg:61.89ms
step:2062/2330 train_time:127613ms step_avg:61.89ms
step:2063/2330 train_time:127673ms step_avg:61.89ms
step:2064/2330 train_time:127736ms step_avg:61.89ms
step:2065/2330 train_time:127798ms step_avg:61.89ms
step:2066/2330 train_time:127861ms step_avg:61.89ms
step:2067/2330 train_time:127923ms step_avg:61.89ms
step:2068/2330 train_time:127988ms step_avg:61.89ms
step:2069/2330 train_time:128050ms step_avg:61.89ms
step:2070/2330 train_time:128114ms step_avg:61.89ms
step:2071/2330 train_time:128176ms step_avg:61.89ms
step:2072/2330 train_time:128241ms step_avg:61.89ms
step:2073/2330 train_time:128303ms step_avg:61.89ms
step:2074/2330 train_time:128365ms step_avg:61.89ms
step:2075/2330 train_time:128427ms step_avg:61.89ms
step:2076/2330 train_time:128490ms step_avg:61.89ms
step:2077/2330 train_time:128550ms step_avg:61.89ms
step:2078/2330 train_time:128613ms step_avg:61.89ms
step:2079/2330 train_time:128674ms step_avg:61.89ms
step:2080/2330 train_time:128738ms step_avg:61.89ms
step:2081/2330 train_time:128800ms step_avg:61.89ms
step:2082/2330 train_time:128863ms step_avg:61.89ms
step:2083/2330 train_time:128926ms step_avg:61.89ms
step:2084/2330 train_time:128990ms step_avg:61.90ms
step:2085/2330 train_time:129051ms step_avg:61.90ms
step:2086/2330 train_time:129115ms step_avg:61.90ms
step:2087/2330 train_time:129177ms step_avg:61.90ms
step:2088/2330 train_time:129241ms step_avg:61.90ms
step:2089/2330 train_time:129302ms step_avg:61.90ms
step:2090/2330 train_time:129365ms step_avg:61.90ms
step:2091/2330 train_time:129426ms step_avg:61.90ms
step:2092/2330 train_time:129489ms step_avg:61.90ms
step:2093/2330 train_time:129550ms step_avg:61.90ms
step:2094/2330 train_time:129614ms step_avg:61.90ms
step:2095/2330 train_time:129676ms step_avg:61.90ms
step:2096/2330 train_time:129739ms step_avg:61.90ms
step:2097/2330 train_time:129799ms step_avg:61.90ms
step:2098/2330 train_time:129863ms step_avg:61.90ms
step:2099/2330 train_time:129925ms step_avg:61.90ms
step:2100/2330 train_time:129990ms step_avg:61.90ms
step:2101/2330 train_time:130051ms step_avg:61.90ms
step:2102/2330 train_time:130115ms step_avg:61.90ms
step:2103/2330 train_time:130177ms step_avg:61.90ms
step:2104/2330 train_time:130241ms step_avg:61.90ms
step:2105/2330 train_time:130301ms step_avg:61.90ms
step:2106/2330 train_time:130364ms step_avg:61.90ms
step:2107/2330 train_time:130426ms step_avg:61.90ms
step:2108/2330 train_time:130489ms step_avg:61.90ms
step:2109/2330 train_time:130550ms step_avg:61.90ms
step:2110/2330 train_time:130613ms step_avg:61.90ms
step:2111/2330 train_time:130675ms step_avg:61.90ms
step:2112/2330 train_time:130738ms step_avg:61.90ms
step:2113/2330 train_time:130800ms step_avg:61.90ms
step:2114/2330 train_time:130863ms step_avg:61.90ms
step:2115/2330 train_time:130925ms step_avg:61.90ms
step:2116/2330 train_time:130990ms step_avg:61.90ms
step:2117/2330 train_time:131051ms step_avg:61.90ms
step:2118/2330 train_time:131115ms step_avg:61.91ms
step:2119/2330 train_time:131177ms step_avg:61.91ms
step:2120/2330 train_time:131240ms step_avg:61.91ms
step:2121/2330 train_time:131301ms step_avg:61.91ms
step:2122/2330 train_time:131364ms step_avg:61.91ms
step:2123/2330 train_time:131425ms step_avg:61.91ms
step:2124/2330 train_time:131488ms step_avg:61.91ms
step:2125/2330 train_time:131549ms step_avg:61.91ms
step:2126/2330 train_time:131613ms step_avg:61.91ms
step:2127/2330 train_time:131674ms step_avg:61.91ms
step:2128/2330 train_time:131739ms step_avg:61.91ms
step:2129/2330 train_time:131800ms step_avg:61.91ms
step:2130/2330 train_time:131863ms step_avg:61.91ms
step:2131/2330 train_time:131925ms step_avg:61.91ms
step:2132/2330 train_time:131989ms step_avg:61.91ms
step:2133/2330 train_time:132051ms step_avg:61.91ms
step:2134/2330 train_time:132115ms step_avg:61.91ms
step:2135/2330 train_time:132177ms step_avg:61.91ms
step:2136/2330 train_time:132240ms step_avg:61.91ms
step:2137/2330 train_time:132301ms step_avg:61.91ms
step:2138/2330 train_time:132364ms step_avg:61.91ms
step:2139/2330 train_time:132425ms step_avg:61.91ms
step:2140/2330 train_time:132489ms step_avg:61.91ms
step:2141/2330 train_time:132550ms step_avg:61.91ms
step:2142/2330 train_time:132614ms step_avg:61.91ms
step:2143/2330 train_time:132675ms step_avg:61.91ms
step:2144/2330 train_time:132738ms step_avg:61.91ms
step:2145/2330 train_time:132800ms step_avg:61.91ms
step:2146/2330 train_time:132863ms step_avg:61.91ms
step:2147/2330 train_time:132924ms step_avg:61.91ms
step:2148/2330 train_time:132987ms step_avg:61.91ms
step:2149/2330 train_time:133048ms step_avg:61.91ms
step:2150/2330 train_time:133113ms step_avg:61.91ms
step:2151/2330 train_time:133174ms step_avg:61.91ms
step:2152/2330 train_time:133238ms step_avg:61.91ms
step:2153/2330 train_time:133298ms step_avg:61.91ms
step:2154/2330 train_time:133361ms step_avg:61.91ms
step:2155/2330 train_time:133423ms step_avg:61.91ms
step:2156/2330 train_time:133487ms step_avg:61.91ms
step:2157/2330 train_time:133549ms step_avg:61.91ms
step:2158/2330 train_time:133613ms step_avg:61.92ms
step:2159/2330 train_time:133675ms step_avg:61.92ms
step:2160/2330 train_time:133738ms step_avg:61.92ms
step:2161/2330 train_time:133799ms step_avg:61.92ms
step:2162/2330 train_time:133862ms step_avg:61.92ms
step:2163/2330 train_time:133923ms step_avg:61.92ms
step:2164/2330 train_time:133986ms step_avg:61.92ms
step:2165/2330 train_time:134048ms step_avg:61.92ms
step:2166/2330 train_time:134111ms step_avg:61.92ms
step:2167/2330 train_time:134173ms step_avg:61.92ms
step:2168/2330 train_time:134237ms step_avg:61.92ms
step:2169/2330 train_time:134299ms step_avg:61.92ms
step:2170/2330 train_time:134362ms step_avg:61.92ms
step:2171/2330 train_time:134423ms step_avg:61.92ms
step:2172/2330 train_time:134488ms step_avg:61.92ms
step:2173/2330 train_time:134549ms step_avg:61.92ms
step:2174/2330 train_time:134612ms step_avg:61.92ms
step:2175/2330 train_time:134674ms step_avg:61.92ms
step:2176/2330 train_time:134737ms step_avg:61.92ms
step:2177/2330 train_time:134798ms step_avg:61.92ms
step:2178/2330 train_time:134861ms step_avg:61.92ms
step:2179/2330 train_time:134923ms step_avg:61.92ms
step:2180/2330 train_time:134987ms step_avg:61.92ms
step:2181/2330 train_time:135049ms step_avg:61.92ms
step:2182/2330 train_time:135113ms step_avg:61.92ms
step:2183/2330 train_time:135174ms step_avg:61.92ms
step:2184/2330 train_time:135238ms step_avg:61.92ms
step:2185/2330 train_time:135299ms step_avg:61.92ms
step:2186/2330 train_time:135361ms step_avg:61.92ms
step:2187/2330 train_time:135423ms step_avg:61.92ms
step:2188/2330 train_time:135487ms step_avg:61.92ms
step:2189/2330 train_time:135549ms step_avg:61.92ms
step:2190/2330 train_time:135613ms step_avg:61.92ms
step:2191/2330 train_time:135675ms step_avg:61.92ms
step:2192/2330 train_time:135737ms step_avg:61.92ms
step:2193/2330 train_time:135799ms step_avg:61.92ms
step:2194/2330 train_time:135862ms step_avg:61.92ms
step:2195/2330 train_time:135922ms step_avg:61.92ms
step:2196/2330 train_time:135987ms step_avg:61.92ms
step:2197/2330 train_time:136048ms step_avg:61.92ms
step:2198/2330 train_time:136113ms step_avg:61.93ms
step:2199/2330 train_time:136175ms step_avg:61.93ms
step:2200/2330 train_time:136238ms step_avg:61.93ms
step:2201/2330 train_time:136299ms step_avg:61.93ms
step:2202/2330 train_time:136362ms step_avg:61.93ms
step:2203/2330 train_time:136423ms step_avg:61.93ms
step:2204/2330 train_time:136487ms step_avg:61.93ms
step:2205/2330 train_time:136549ms step_avg:61.93ms
step:2206/2330 train_time:136613ms step_avg:61.93ms
step:2207/2330 train_time:136675ms step_avg:61.93ms
step:2208/2330 train_time:136738ms step_avg:61.93ms
step:2209/2330 train_time:136799ms step_avg:61.93ms
step:2210/2330 train_time:136863ms step_avg:61.93ms
step:2211/2330 train_time:136924ms step_avg:61.93ms
step:2212/2330 train_time:136987ms step_avg:61.93ms
step:2213/2330 train_time:137049ms step_avg:61.93ms
step:2214/2330 train_time:137113ms step_avg:61.93ms
step:2215/2330 train_time:137174ms step_avg:61.93ms
step:2216/2330 train_time:137238ms step_avg:61.93ms
step:2217/2330 train_time:137300ms step_avg:61.93ms
step:2218/2330 train_time:137362ms step_avg:61.93ms
step:2219/2330 train_time:137423ms step_avg:61.93ms
step:2220/2330 train_time:137487ms step_avg:61.93ms
step:2221/2330 train_time:137549ms step_avg:61.93ms
step:2222/2330 train_time:137613ms step_avg:61.93ms
step:2223/2330 train_time:137675ms step_avg:61.93ms
step:2224/2330 train_time:137738ms step_avg:61.93ms
step:2225/2330 train_time:137799ms step_avg:61.93ms
step:2226/2330 train_time:137862ms step_avg:61.93ms
step:2227/2330 train_time:137923ms step_avg:61.93ms
step:2228/2330 train_time:137987ms step_avg:61.93ms
step:2229/2330 train_time:138049ms step_avg:61.93ms
step:2230/2330 train_time:138113ms step_avg:61.93ms
step:2231/2330 train_time:138174ms step_avg:61.93ms
step:2232/2330 train_time:138238ms step_avg:61.93ms
step:2233/2330 train_time:138299ms step_avg:61.93ms
step:2234/2330 train_time:138362ms step_avg:61.93ms
step:2235/2330 train_time:138423ms step_avg:61.93ms
step:2236/2330 train_time:138487ms step_avg:61.94ms
step:2237/2330 train_time:138549ms step_avg:61.93ms
step:2238/2330 train_time:138613ms step_avg:61.94ms
step:2239/2330 train_time:138674ms step_avg:61.94ms
step:2240/2330 train_time:138738ms step_avg:61.94ms
step:2241/2330 train_time:138799ms step_avg:61.94ms
step:2242/2330 train_time:138862ms step_avg:61.94ms
step:2243/2330 train_time:138923ms step_avg:61.94ms
step:2244/2330 train_time:138986ms step_avg:61.94ms
step:2245/2330 train_time:139047ms step_avg:61.94ms
step:2246/2330 train_time:139111ms step_avg:61.94ms
step:2247/2330 train_time:139172ms step_avg:61.94ms
step:2248/2330 train_time:139236ms step_avg:61.94ms
step:2249/2330 train_time:139298ms step_avg:61.94ms
step:2250/2330 train_time:139360ms step_avg:61.94ms
step:2250/2330 val_loss:3.6733 train_time:139425ms step_avg:61.97ms
step:2251/2330 train_time:139450ms step_avg:61.95ms
step:2252/2330 train_time:139486ms step_avg:61.94ms
step:2253/2330 train_time:139551ms step_avg:61.94ms
step:2254/2330 train_time:139616ms step_avg:61.94ms
step:2255/2330 train_time:139678ms step_avg:61.94ms
step:2256/2330 train_time:139743ms step_avg:61.94ms
step:2257/2330 train_time:139803ms step_avg:61.94ms
step:2258/2330 train_time:139866ms step_avg:61.94ms
step:2259/2330 train_time:139927ms step_avg:61.94ms
step:2260/2330 train_time:139989ms step_avg:61.94ms
step:2261/2330 train_time:140049ms step_avg:61.94ms
step:2262/2330 train_time:140112ms step_avg:61.94ms
step:2263/2330 train_time:140172ms step_avg:61.94ms
step:2264/2330 train_time:140235ms step_avg:61.94ms
step:2265/2330 train_time:140295ms step_avg:61.94ms
step:2266/2330 train_time:140359ms step_avg:61.94ms
step:2267/2330 train_time:140421ms step_avg:61.94ms
step:2268/2330 train_time:140486ms step_avg:61.94ms
step:2269/2330 train_time:140549ms step_avg:61.94ms
step:2270/2330 train_time:140614ms step_avg:61.94ms
step:2271/2330 train_time:140676ms step_avg:61.94ms
step:2272/2330 train_time:140740ms step_avg:61.95ms
step:2273/2330 train_time:140801ms step_avg:61.94ms
step:2274/2330 train_time:140864ms step_avg:61.95ms
step:2275/2330 train_time:140924ms step_avg:61.94ms
step:2276/2330 train_time:140987ms step_avg:61.95ms
step:2277/2330 train_time:141048ms step_avg:61.94ms
step:2278/2330 train_time:141111ms step_avg:61.95ms
step:2279/2330 train_time:141172ms step_avg:61.94ms
step:2280/2330 train_time:141235ms step_avg:61.95ms
step:2281/2330 train_time:141296ms step_avg:61.94ms
step:2282/2330 train_time:141359ms step_avg:61.95ms
step:2283/2330 train_time:141420ms step_avg:61.94ms
step:2284/2330 train_time:141484ms step_avg:61.95ms
step:2285/2330 train_time:141545ms step_avg:61.95ms
step:2286/2330 train_time:141611ms step_avg:61.95ms
step:2287/2330 train_time:141673ms step_avg:61.95ms
step:2288/2330 train_time:141737ms step_avg:61.95ms
step:2289/2330 train_time:141799ms step_avg:61.95ms
step:2290/2330 train_time:141863ms step_avg:61.95ms
step:2291/2330 train_time:141923ms step_avg:61.95ms
step:2292/2330 train_time:141986ms step_avg:61.95ms
step:2293/2330 train_time:142047ms step_avg:61.95ms
step:2294/2330 train_time:142110ms step_avg:61.95ms
step:2295/2330 train_time:142171ms step_avg:61.95ms
step:2296/2330 train_time:142234ms step_avg:61.95ms
step:2297/2330 train_time:142295ms step_avg:61.95ms
step:2298/2330 train_time:142358ms step_avg:61.95ms
step:2299/2330 train_time:142419ms step_avg:61.95ms
step:2300/2330 train_time:142482ms step_avg:61.95ms
step:2301/2330 train_time:142544ms step_avg:61.95ms
step:2302/2330 train_time:142609ms step_avg:61.95ms
step:2303/2330 train_time:142671ms step_avg:61.95ms
step:2304/2330 train_time:142736ms step_avg:61.95ms
step:2305/2330 train_time:142797ms step_avg:61.95ms
step:2306/2330 train_time:142862ms step_avg:61.95ms
step:2307/2330 train_time:142923ms step_avg:61.95ms
step:2308/2330 train_time:142986ms step_avg:61.95ms
step:2309/2330 train_time:143046ms step_avg:61.95ms
step:2310/2330 train_time:143109ms step_avg:61.95ms
step:2311/2330 train_time:143171ms step_avg:61.95ms
step:2312/2330 train_time:143234ms step_avg:61.95ms
step:2313/2330 train_time:143295ms step_avg:61.95ms
step:2314/2330 train_time:143358ms step_avg:61.95ms
step:2315/2330 train_time:143419ms step_avg:61.95ms
step:2316/2330 train_time:143482ms step_avg:61.95ms
step:2317/2330 train_time:143544ms step_avg:61.95ms
step:2318/2330 train_time:143608ms step_avg:61.95ms
step:2319/2330 train_time:143672ms step_avg:61.95ms
step:2320/2330 train_time:143735ms step_avg:61.95ms
step:2321/2330 train_time:143798ms step_avg:61.96ms
step:2322/2330 train_time:143861ms step_avg:61.96ms
step:2323/2330 train_time:143923ms step_avg:61.96ms
step:2324/2330 train_time:143986ms step_avg:61.96ms
step:2325/2330 train_time:144047ms step_avg:61.96ms
step:2326/2330 train_time:144111ms step_avg:61.96ms
step:2327/2330 train_time:144172ms step_avg:61.96ms
step:2328/2330 train_time:144235ms step_avg:61.96ms
step:2329/2330 train_time:144296ms step_avg:61.96ms
step:2330/2330 train_time:144359ms step_avg:61.96ms
step:2330/2330 val_loss:3.6620 train_time:144425ms step_avg:61.99ms
peak memory allocated: 29226 MiB reserved: 44076 MiB
