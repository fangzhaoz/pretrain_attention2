import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_adam_lr3e-4"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-1, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizer2 = torch.optim.Adam(
    hidden_matrix_params + gate_params,
    lr=3e-4,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 04:28:47 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   34C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   32C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:76ms step_avg:76.10ms
step:2/2330 train_time:137ms step_avg:68.37ms
step:3/2330 train_time:149ms step_avg:49.68ms
step:4/2330 train_time:161ms step_avg:40.15ms
step:5/2330 train_time:171ms step_avg:34.16ms
step:6/2330 train_time:182ms step_avg:30.35ms
step:7/2330 train_time:213ms step_avg:30.37ms
step:8/2330 train_time:253ms step_avg:31.64ms
step:9/2330 train_time:288ms step_avg:32.01ms
step:10/2330 train_time:329ms step_avg:32.88ms
step:11/2330 train_time:364ms step_avg:33.12ms
step:12/2330 train_time:405ms step_avg:33.78ms
step:13/2330 train_time:440ms step_avg:33.85ms
step:14/2330 train_time:481ms step_avg:34.35ms
step:15/2330 train_time:516ms step_avg:34.39ms
step:16/2330 train_time:557ms step_avg:34.80ms
step:17/2330 train_time:592ms step_avg:34.81ms
step:18/2330 train_time:633ms step_avg:35.15ms
step:19/2330 train_time:667ms step_avg:35.13ms
step:20/2330 train_time:708ms step_avg:35.41ms
step:21/2330 train_time:744ms step_avg:35.41ms
step:22/2330 train_time:785ms step_avg:35.66ms
step:23/2330 train_time:819ms step_avg:35.63ms
step:24/2330 train_time:861ms step_avg:35.87ms
step:25/2330 train_time:896ms step_avg:35.85ms
step:26/2330 train_time:938ms step_avg:36.07ms
step:27/2330 train_time:975ms step_avg:36.12ms
step:28/2330 train_time:1021ms step_avg:36.45ms
step:29/2330 train_time:1061ms step_avg:36.57ms
step:30/2330 train_time:1105ms step_avg:36.83ms
step:31/2330 train_time:1144ms step_avg:36.89ms
step:32/2330 train_time:1186ms step_avg:37.07ms
step:33/2330 train_time:1222ms step_avg:37.03ms
step:34/2330 train_time:1264ms step_avg:37.17ms
step:35/2330 train_time:1300ms step_avg:37.13ms
step:36/2330 train_time:1341ms step_avg:37.25ms
step:37/2330 train_time:1377ms step_avg:37.21ms
step:38/2330 train_time:1418ms step_avg:37.32ms
step:39/2330 train_time:1453ms step_avg:37.26ms
step:40/2330 train_time:1494ms step_avg:37.36ms
step:41/2330 train_time:1530ms step_avg:37.31ms
step:42/2330 train_time:1570ms step_avg:37.39ms
step:43/2330 train_time:1606ms step_avg:37.35ms
step:44/2330 train_time:1647ms step_avg:37.44ms
step:45/2330 train_time:1682ms step_avg:37.38ms
step:46/2330 train_time:1723ms step_avg:37.46ms
step:47/2330 train_time:1758ms step_avg:37.41ms
step:48/2330 train_time:1800ms step_avg:37.49ms
step:49/2330 train_time:1835ms step_avg:37.45ms
step:50/2330 train_time:1876ms step_avg:37.53ms
step:51/2330 train_time:1912ms step_avg:37.50ms
step:52/2330 train_time:1954ms step_avg:37.58ms
step:53/2330 train_time:1994ms step_avg:37.62ms
step:54/2330 train_time:2036ms step_avg:37.69ms
step:55/2330 train_time:2076ms step_avg:37.74ms
step:56/2330 train_time:2118ms step_avg:37.82ms
step:57/2330 train_time:2156ms step_avg:37.82ms
step:58/2330 train_time:2198ms step_avg:37.89ms
step:59/2330 train_time:2235ms step_avg:37.88ms
step:60/2330 train_time:2277ms step_avg:37.95ms
step:61/2330 train_time:2313ms step_avg:37.92ms
step:62/2330 train_time:2354ms step_avg:37.98ms
step:63/2330 train_time:2391ms step_avg:37.95ms
step:64/2330 train_time:2431ms step_avg:37.99ms
step:65/2330 train_time:2467ms step_avg:37.95ms
step:66/2330 train_time:2508ms step_avg:38.00ms
step:67/2330 train_time:2543ms step_avg:37.96ms
step:68/2330 train_time:2585ms step_avg:38.01ms
step:69/2330 train_time:2620ms step_avg:37.97ms
step:70/2330 train_time:2661ms step_avg:38.02ms
step:71/2330 train_time:2697ms step_avg:37.98ms
step:72/2330 train_time:2738ms step_avg:38.03ms
step:73/2330 train_time:2773ms step_avg:37.99ms
step:74/2330 train_time:2814ms step_avg:38.03ms
step:75/2330 train_time:2851ms step_avg:38.01ms
step:76/2330 train_time:2892ms step_avg:38.05ms
step:77/2330 train_time:2928ms step_avg:38.03ms
step:78/2330 train_time:2970ms step_avg:38.07ms
step:79/2330 train_time:3008ms step_avg:38.07ms
step:80/2330 train_time:3050ms step_avg:38.12ms
step:81/2330 train_time:3087ms step_avg:38.12ms
step:82/2330 train_time:3129ms step_avg:38.15ms
step:83/2330 train_time:3165ms step_avg:38.14ms
step:84/2330 train_time:3207ms step_avg:38.17ms
step:85/2330 train_time:3243ms step_avg:38.16ms
step:86/2330 train_time:3285ms step_avg:38.20ms
step:87/2330 train_time:3321ms step_avg:38.18ms
step:88/2330 train_time:3363ms step_avg:38.21ms
step:89/2330 train_time:3398ms step_avg:38.18ms
step:90/2330 train_time:3440ms step_avg:38.22ms
step:91/2330 train_time:3476ms step_avg:38.19ms
step:92/2330 train_time:3517ms step_avg:38.23ms
step:93/2330 train_time:3553ms step_avg:38.20ms
step:94/2330 train_time:3594ms step_avg:38.23ms
step:95/2330 train_time:3629ms step_avg:38.20ms
step:96/2330 train_time:3670ms step_avg:38.23ms
step:97/2330 train_time:3705ms step_avg:38.20ms
step:98/2330 train_time:3747ms step_avg:38.23ms
step:99/2330 train_time:3782ms step_avg:38.20ms
step:100/2330 train_time:3823ms step_avg:38.23ms
step:101/2330 train_time:3859ms step_avg:38.21ms
step:102/2330 train_time:3901ms step_avg:38.25ms
step:103/2330 train_time:3938ms step_avg:38.23ms
step:104/2330 train_time:3980ms step_avg:38.27ms
step:105/2330 train_time:4017ms step_avg:38.25ms
step:106/2330 train_time:4059ms step_avg:38.30ms
step:107/2330 train_time:4096ms step_avg:38.28ms
step:108/2330 train_time:4138ms step_avg:38.32ms
step:109/2330 train_time:4175ms step_avg:38.30ms
step:110/2330 train_time:4217ms step_avg:38.34ms
step:111/2330 train_time:4253ms step_avg:38.32ms
step:112/2330 train_time:4295ms step_avg:38.35ms
step:113/2330 train_time:4331ms step_avg:38.33ms
step:114/2330 train_time:4372ms step_avg:38.35ms
step:115/2330 train_time:4408ms step_avg:38.33ms
step:116/2330 train_time:4449ms step_avg:38.35ms
step:117/2330 train_time:4484ms step_avg:38.33ms
step:118/2330 train_time:4526ms step_avg:38.35ms
step:119/2330 train_time:4561ms step_avg:38.33ms
step:120/2330 train_time:4602ms step_avg:38.35ms
step:121/2330 train_time:4638ms step_avg:38.33ms
step:122/2330 train_time:4679ms step_avg:38.36ms
step:123/2330 train_time:4714ms step_avg:38.32ms
step:124/2330 train_time:4755ms step_avg:38.35ms
step:125/2330 train_time:4790ms step_avg:38.32ms
step:126/2330 train_time:4831ms step_avg:38.34ms
step:127/2330 train_time:4867ms step_avg:38.32ms
step:128/2330 train_time:4908ms step_avg:38.34ms
step:129/2330 train_time:4945ms step_avg:38.33ms
step:130/2330 train_time:4986ms step_avg:38.36ms
step:131/2330 train_time:5022ms step_avg:38.34ms
step:132/2330 train_time:5064ms step_avg:38.36ms
step:133/2330 train_time:5100ms step_avg:38.35ms
step:134/2330 train_time:5142ms step_avg:38.38ms
step:135/2330 train_time:5178ms step_avg:38.36ms
step:136/2330 train_time:5221ms step_avg:38.39ms
step:137/2330 train_time:5257ms step_avg:38.37ms
step:138/2330 train_time:5299ms step_avg:38.40ms
step:139/2330 train_time:5336ms step_avg:38.39ms
step:140/2330 train_time:5378ms step_avg:38.41ms
step:141/2330 train_time:5413ms step_avg:38.39ms
step:142/2330 train_time:5455ms step_avg:38.41ms
step:143/2330 train_time:5490ms step_avg:38.39ms
step:144/2330 train_time:5531ms step_avg:38.41ms
step:145/2330 train_time:5568ms step_avg:38.40ms
step:146/2330 train_time:5609ms step_avg:38.42ms
step:147/2330 train_time:5645ms step_avg:38.40ms
step:148/2330 train_time:5686ms step_avg:38.42ms
step:149/2330 train_time:5721ms step_avg:38.40ms
step:150/2330 train_time:5762ms step_avg:38.42ms
step:151/2330 train_time:5798ms step_avg:38.40ms
step:152/2330 train_time:5840ms step_avg:38.42ms
step:153/2330 train_time:5876ms step_avg:38.40ms
step:154/2330 train_time:5917ms step_avg:38.42ms
step:155/2330 train_time:5953ms step_avg:38.41ms
step:156/2330 train_time:5994ms step_avg:38.43ms
step:157/2330 train_time:6030ms step_avg:38.41ms
step:158/2330 train_time:6071ms step_avg:38.43ms
step:159/2330 train_time:6109ms step_avg:38.42ms
step:160/2330 train_time:6150ms step_avg:38.44ms
step:161/2330 train_time:6188ms step_avg:38.43ms
step:162/2330 train_time:6229ms step_avg:38.45ms
step:163/2330 train_time:6265ms step_avg:38.44ms
step:164/2330 train_time:6307ms step_avg:38.46ms
step:165/2330 train_time:6343ms step_avg:38.44ms
step:166/2330 train_time:6384ms step_avg:38.46ms
step:167/2330 train_time:6420ms step_avg:38.44ms
step:168/2330 train_time:6462ms step_avg:38.46ms
step:169/2330 train_time:6498ms step_avg:38.45ms
step:170/2330 train_time:6540ms step_avg:38.47ms
step:171/2330 train_time:6576ms step_avg:38.46ms
step:172/2330 train_time:6617ms step_avg:38.47ms
step:173/2330 train_time:6653ms step_avg:38.46ms
step:174/2330 train_time:6694ms step_avg:38.47ms
step:175/2330 train_time:6729ms step_avg:38.45ms
step:176/2330 train_time:6770ms step_avg:38.47ms
step:177/2330 train_time:6806ms step_avg:38.45ms
step:178/2330 train_time:6847ms step_avg:38.47ms
step:179/2330 train_time:6882ms step_avg:38.45ms
step:180/2330 train_time:6924ms step_avg:38.46ms
step:181/2330 train_time:6960ms step_avg:38.45ms
step:182/2330 train_time:7002ms step_avg:38.47ms
step:183/2330 train_time:7038ms step_avg:38.46ms
step:184/2330 train_time:7080ms step_avg:38.48ms
step:185/2330 train_time:7116ms step_avg:38.46ms
step:186/2330 train_time:7158ms step_avg:38.48ms
step:187/2330 train_time:7194ms step_avg:38.47ms
step:188/2330 train_time:7236ms step_avg:38.49ms
step:189/2330 train_time:7272ms step_avg:38.48ms
step:190/2330 train_time:7313ms step_avg:38.49ms
step:191/2330 train_time:7350ms step_avg:38.48ms
step:192/2330 train_time:7391ms step_avg:38.49ms
step:193/2330 train_time:7428ms step_avg:38.49ms
step:194/2330 train_time:7469ms step_avg:38.50ms
step:195/2330 train_time:7506ms step_avg:38.49ms
step:196/2330 train_time:7547ms step_avg:38.51ms
step:197/2330 train_time:7582ms step_avg:38.49ms
step:198/2330 train_time:7623ms step_avg:38.50ms
step:199/2330 train_time:7659ms step_avg:38.49ms
step:200/2330 train_time:7701ms step_avg:38.50ms
step:201/2330 train_time:7736ms step_avg:38.49ms
step:202/2330 train_time:7778ms step_avg:38.50ms
step:203/2330 train_time:7813ms step_avg:38.49ms
step:204/2330 train_time:7855ms step_avg:38.50ms
step:205/2330 train_time:7890ms step_avg:38.49ms
step:206/2330 train_time:7931ms step_avg:38.50ms
step:207/2330 train_time:7968ms step_avg:38.49ms
step:208/2330 train_time:8009ms step_avg:38.50ms
step:209/2330 train_time:8045ms step_avg:38.49ms
step:210/2330 train_time:8086ms step_avg:38.50ms
step:211/2330 train_time:8122ms step_avg:38.49ms
step:212/2330 train_time:8164ms step_avg:38.51ms
step:213/2330 train_time:8199ms step_avg:38.49ms
step:214/2330 train_time:8241ms step_avg:38.51ms
step:215/2330 train_time:8277ms step_avg:38.50ms
step:216/2330 train_time:8319ms step_avg:38.51ms
step:217/2330 train_time:8356ms step_avg:38.51ms
step:218/2330 train_time:8397ms step_avg:38.52ms
step:219/2330 train_time:8434ms step_avg:38.51ms
step:220/2330 train_time:8476ms step_avg:38.53ms
step:221/2330 train_time:8511ms step_avg:38.51ms
step:222/2330 train_time:8552ms step_avg:38.52ms
step:223/2330 train_time:8587ms step_avg:38.51ms
step:224/2330 train_time:8628ms step_avg:38.52ms
step:225/2330 train_time:8664ms step_avg:38.51ms
step:226/2330 train_time:8705ms step_avg:38.52ms
step:227/2330 train_time:8740ms step_avg:38.50ms
step:228/2330 train_time:8782ms step_avg:38.52ms
step:229/2330 train_time:8817ms step_avg:38.50ms
step:230/2330 train_time:8859ms step_avg:38.52ms
step:231/2330 train_time:8895ms step_avg:38.50ms
step:232/2330 train_time:8936ms step_avg:38.52ms
step:233/2330 train_time:8972ms step_avg:38.51ms
step:234/2330 train_time:9013ms step_avg:38.52ms
step:235/2330 train_time:9051ms step_avg:38.51ms
step:236/2330 train_time:9091ms step_avg:38.52ms
step:237/2330 train_time:9128ms step_avg:38.51ms
step:238/2330 train_time:9168ms step_avg:38.52ms
step:239/2330 train_time:9205ms step_avg:38.51ms
step:240/2330 train_time:9246ms step_avg:38.52ms
step:241/2330 train_time:9282ms step_avg:38.51ms
step:242/2330 train_time:9323ms step_avg:38.52ms
step:243/2330 train_time:9360ms step_avg:38.52ms
step:244/2330 train_time:9401ms step_avg:38.53ms
step:245/2330 train_time:9438ms step_avg:38.52ms
step:246/2330 train_time:9480ms step_avg:38.54ms
step:247/2330 train_time:9516ms step_avg:38.53ms
step:248/2330 train_time:9557ms step_avg:38.54ms
step:249/2330 train_time:9592ms step_avg:38.52ms
step:250/2330 train_time:9634ms step_avg:38.54ms
step:250/2330 val_loss:5.5664 train_time:9748ms step_avg:38.99ms
step:251/2330 train_time:9759ms step_avg:38.88ms
step:252/2330 train_time:9771ms step_avg:38.77ms
step:253/2330 train_time:9780ms step_avg:38.66ms
step:254/2330 train_time:9790ms step_avg:38.54ms
step:255/2330 train_time:9825ms step_avg:38.53ms
step:256/2330 train_time:9866ms step_avg:38.54ms
step:257/2330 train_time:9900ms step_avg:38.52ms
step:258/2330 train_time:9941ms step_avg:38.53ms
step:259/2330 train_time:9976ms step_avg:38.52ms
step:260/2330 train_time:10017ms step_avg:38.53ms
step:261/2330 train_time:10056ms step_avg:38.53ms
step:262/2330 train_time:10098ms step_avg:38.54ms
step:263/2330 train_time:10142ms step_avg:38.56ms
step:264/2330 train_time:10183ms step_avg:38.57ms
step:265/2330 train_time:10221ms step_avg:38.57ms
step:266/2330 train_time:10262ms step_avg:38.58ms
step:267/2330 train_time:10298ms step_avg:38.57ms
step:268/2330 train_time:10340ms step_avg:38.58ms
step:269/2330 train_time:10375ms step_avg:38.57ms
step:270/2330 train_time:10416ms step_avg:38.58ms
step:271/2330 train_time:10451ms step_avg:38.56ms
step:272/2330 train_time:10492ms step_avg:38.57ms
step:273/2330 train_time:10527ms step_avg:38.56ms
step:274/2330 train_time:10568ms step_avg:38.57ms
step:275/2330 train_time:10604ms step_avg:38.56ms
step:276/2330 train_time:10645ms step_avg:38.57ms
step:277/2330 train_time:10681ms step_avg:38.56ms
step:278/2330 train_time:10723ms step_avg:38.57ms
step:279/2330 train_time:10759ms step_avg:38.56ms
step:280/2330 train_time:10800ms step_avg:38.57ms
step:281/2330 train_time:10836ms step_avg:38.56ms
step:282/2330 train_time:10877ms step_avg:38.57ms
step:283/2330 train_time:10911ms step_avg:38.56ms
step:284/2330 train_time:10952ms step_avg:38.57ms
step:285/2330 train_time:10989ms step_avg:38.56ms
step:286/2330 train_time:11030ms step_avg:38.57ms
step:287/2330 train_time:11069ms step_avg:38.57ms
step:288/2330 train_time:11110ms step_avg:38.58ms
step:289/2330 train_time:11148ms step_avg:38.57ms
step:290/2330 train_time:11188ms step_avg:38.58ms
step:291/2330 train_time:11225ms step_avg:38.57ms
step:292/2330 train_time:11266ms step_avg:38.58ms
step:293/2330 train_time:11302ms step_avg:38.57ms
step:294/2330 train_time:11343ms step_avg:38.58ms
step:295/2330 train_time:11379ms step_avg:38.57ms
step:296/2330 train_time:11421ms step_avg:38.58ms
step:297/2330 train_time:11457ms step_avg:38.58ms
step:298/2330 train_time:11498ms step_avg:38.59ms
step:299/2330 train_time:11534ms step_avg:38.58ms
step:300/2330 train_time:11575ms step_avg:38.58ms
step:301/2330 train_time:11611ms step_avg:38.57ms
step:302/2330 train_time:11652ms step_avg:38.58ms
step:303/2330 train_time:11688ms step_avg:38.57ms
step:304/2330 train_time:11729ms step_avg:38.58ms
step:305/2330 train_time:11763ms step_avg:38.57ms
step:306/2330 train_time:11805ms step_avg:38.58ms
step:307/2330 train_time:11841ms step_avg:38.57ms
step:308/2330 train_time:11883ms step_avg:38.58ms
step:309/2330 train_time:11919ms step_avg:38.57ms
step:310/2330 train_time:11961ms step_avg:38.58ms
step:311/2330 train_time:11997ms step_avg:38.58ms
step:312/2330 train_time:12039ms step_avg:38.59ms
step:313/2330 train_time:12076ms step_avg:38.58ms
step:314/2330 train_time:12118ms step_avg:38.59ms
step:315/2330 train_time:12156ms step_avg:38.59ms
step:316/2330 train_time:12197ms step_avg:38.60ms
step:317/2330 train_time:12236ms step_avg:38.60ms
step:318/2330 train_time:12276ms step_avg:38.60ms
step:319/2330 train_time:12313ms step_avg:38.60ms
step:320/2330 train_time:12354ms step_avg:38.61ms
step:321/2330 train_time:12391ms step_avg:38.60ms
step:322/2330 train_time:12431ms step_avg:38.61ms
step:323/2330 train_time:12467ms step_avg:38.60ms
step:324/2330 train_time:12508ms step_avg:38.60ms
step:325/2330 train_time:12542ms step_avg:38.59ms
step:326/2330 train_time:12584ms step_avg:38.60ms
step:327/2330 train_time:12619ms step_avg:38.59ms
step:328/2330 train_time:12661ms step_avg:38.60ms
step:329/2330 train_time:12696ms step_avg:38.59ms
step:330/2330 train_time:12738ms step_avg:38.60ms
step:331/2330 train_time:12773ms step_avg:38.59ms
step:332/2330 train_time:12813ms step_avg:38.59ms
step:333/2330 train_time:12849ms step_avg:38.59ms
step:334/2330 train_time:12891ms step_avg:38.59ms
step:335/2330 train_time:12927ms step_avg:38.59ms
step:336/2330 train_time:12968ms step_avg:38.60ms
step:337/2330 train_time:13003ms step_avg:38.59ms
step:338/2330 train_time:13045ms step_avg:38.60ms
step:339/2330 train_time:13081ms step_avg:38.59ms
step:340/2330 train_time:13123ms step_avg:38.60ms
step:341/2330 train_time:13159ms step_avg:38.59ms
step:342/2330 train_time:13202ms step_avg:38.60ms
step:343/2330 train_time:13239ms step_avg:38.60ms
step:344/2330 train_time:13281ms step_avg:38.61ms
step:345/2330 train_time:13316ms step_avg:38.60ms
step:346/2330 train_time:13358ms step_avg:38.61ms
step:347/2330 train_time:13395ms step_avg:38.60ms
step:348/2330 train_time:13436ms step_avg:38.61ms
step:349/2330 train_time:13473ms step_avg:38.60ms
step:350/2330 train_time:13513ms step_avg:38.61ms
step:351/2330 train_time:13550ms step_avg:38.60ms
step:352/2330 train_time:13591ms step_avg:38.61ms
step:353/2330 train_time:13627ms step_avg:38.60ms
step:354/2330 train_time:13667ms step_avg:38.61ms
step:355/2330 train_time:13703ms step_avg:38.60ms
step:356/2330 train_time:13744ms step_avg:38.61ms
step:357/2330 train_time:13780ms step_avg:38.60ms
step:358/2330 train_time:13822ms step_avg:38.61ms
step:359/2330 train_time:13857ms step_avg:38.60ms
step:360/2330 train_time:13898ms step_avg:38.61ms
step:361/2330 train_time:13934ms step_avg:38.60ms
step:362/2330 train_time:13975ms step_avg:38.61ms
step:363/2330 train_time:14012ms step_avg:38.60ms
step:364/2330 train_time:14053ms step_avg:38.61ms
step:365/2330 train_time:14090ms step_avg:38.60ms
step:366/2330 train_time:14131ms step_avg:38.61ms
step:367/2330 train_time:14166ms step_avg:38.60ms
step:368/2330 train_time:14208ms step_avg:38.61ms
step:369/2330 train_time:14244ms step_avg:38.60ms
step:370/2330 train_time:14285ms step_avg:38.61ms
step:371/2330 train_time:14322ms step_avg:38.60ms
step:372/2330 train_time:14364ms step_avg:38.61ms
step:373/2330 train_time:14399ms step_avg:38.60ms
step:374/2330 train_time:14441ms step_avg:38.61ms
step:375/2330 train_time:14476ms step_avg:38.60ms
step:376/2330 train_time:14518ms step_avg:38.61ms
step:377/2330 train_time:14554ms step_avg:38.60ms
step:378/2330 train_time:14595ms step_avg:38.61ms
step:379/2330 train_time:14632ms step_avg:38.61ms
step:380/2330 train_time:14672ms step_avg:38.61ms
step:381/2330 train_time:14709ms step_avg:38.61ms
step:382/2330 train_time:14750ms step_avg:38.61ms
step:383/2330 train_time:14786ms step_avg:38.60ms
step:384/2330 train_time:14826ms step_avg:38.61ms
step:385/2330 train_time:14863ms step_avg:38.61ms
step:386/2330 train_time:14905ms step_avg:38.61ms
step:387/2330 train_time:14940ms step_avg:38.60ms
step:388/2330 train_time:14982ms step_avg:38.61ms
step:389/2330 train_time:15018ms step_avg:38.61ms
step:390/2330 train_time:15060ms step_avg:38.61ms
step:391/2330 train_time:15095ms step_avg:38.61ms
step:392/2330 train_time:15136ms step_avg:38.61ms
step:393/2330 train_time:15173ms step_avg:38.61ms
step:394/2330 train_time:15213ms step_avg:38.61ms
step:395/2330 train_time:15251ms step_avg:38.61ms
step:396/2330 train_time:15292ms step_avg:38.62ms
step:397/2330 train_time:15328ms step_avg:38.61ms
step:398/2330 train_time:15369ms step_avg:38.62ms
step:399/2330 train_time:15404ms step_avg:38.61ms
step:400/2330 train_time:15446ms step_avg:38.61ms
step:401/2330 train_time:15481ms step_avg:38.61ms
step:402/2330 train_time:15523ms step_avg:38.61ms
step:403/2330 train_time:15559ms step_avg:38.61ms
step:404/2330 train_time:15601ms step_avg:38.62ms
step:405/2330 train_time:15637ms step_avg:38.61ms
step:406/2330 train_time:15678ms step_avg:38.62ms
step:407/2330 train_time:15714ms step_avg:38.61ms
step:408/2330 train_time:15755ms step_avg:38.62ms
step:409/2330 train_time:15793ms step_avg:38.61ms
step:410/2330 train_time:15833ms step_avg:38.62ms
step:411/2330 train_time:15870ms step_avg:38.61ms
step:412/2330 train_time:15911ms step_avg:38.62ms
step:413/2330 train_time:15947ms step_avg:38.61ms
step:414/2330 train_time:15988ms step_avg:38.62ms
step:415/2330 train_time:16023ms step_avg:38.61ms
step:416/2330 train_time:16065ms step_avg:38.62ms
step:417/2330 train_time:16101ms step_avg:38.61ms
step:418/2330 train_time:16143ms step_avg:38.62ms
step:419/2330 train_time:16180ms step_avg:38.61ms
step:420/2330 train_time:16221ms step_avg:38.62ms
step:421/2330 train_time:16258ms step_avg:38.62ms
step:422/2330 train_time:16299ms step_avg:38.62ms
step:423/2330 train_time:16334ms step_avg:38.62ms
step:424/2330 train_time:16376ms step_avg:38.62ms
step:425/2330 train_time:16411ms step_avg:38.61ms
step:426/2330 train_time:16452ms step_avg:38.62ms
step:427/2330 train_time:16488ms step_avg:38.61ms
step:428/2330 train_time:16530ms step_avg:38.62ms
step:429/2330 train_time:16565ms step_avg:38.61ms
step:430/2330 train_time:16606ms step_avg:38.62ms
step:431/2330 train_time:16642ms step_avg:38.61ms
step:432/2330 train_time:16684ms step_avg:38.62ms
step:433/2330 train_time:16719ms step_avg:38.61ms
step:434/2330 train_time:16761ms step_avg:38.62ms
step:435/2330 train_time:16796ms step_avg:38.61ms
step:436/2330 train_time:16837ms step_avg:38.62ms
step:437/2330 train_time:16874ms step_avg:38.61ms
step:438/2330 train_time:16915ms step_avg:38.62ms
step:439/2330 train_time:16952ms step_avg:38.61ms
step:440/2330 train_time:16993ms step_avg:38.62ms
step:441/2330 train_time:17029ms step_avg:38.62ms
step:442/2330 train_time:17070ms step_avg:38.62ms
step:443/2330 train_time:17107ms step_avg:38.62ms
step:444/2330 train_time:17148ms step_avg:38.62ms
step:445/2330 train_time:17183ms step_avg:38.61ms
step:446/2330 train_time:17224ms step_avg:38.62ms
step:447/2330 train_time:17260ms step_avg:38.61ms
step:448/2330 train_time:17302ms step_avg:38.62ms
step:449/2330 train_time:17337ms step_avg:38.61ms
step:450/2330 train_time:17378ms step_avg:38.62ms
step:451/2330 train_time:17414ms step_avg:38.61ms
step:452/2330 train_time:17455ms step_avg:38.62ms
step:453/2330 train_time:17491ms step_avg:38.61ms
step:454/2330 train_time:17532ms step_avg:38.62ms
step:455/2330 train_time:17568ms step_avg:38.61ms
step:456/2330 train_time:17609ms step_avg:38.62ms
step:457/2330 train_time:17646ms step_avg:38.61ms
step:458/2330 train_time:17687ms step_avg:38.62ms
step:459/2330 train_time:17722ms step_avg:38.61ms
step:460/2330 train_time:17764ms step_avg:38.62ms
step:461/2330 train_time:17800ms step_avg:38.61ms
step:462/2330 train_time:17842ms step_avg:38.62ms
step:463/2330 train_time:17877ms step_avg:38.61ms
step:464/2330 train_time:17919ms step_avg:38.62ms
step:465/2330 train_time:17955ms step_avg:38.61ms
step:466/2330 train_time:17996ms step_avg:38.62ms
step:467/2330 train_time:18032ms step_avg:38.61ms
step:468/2330 train_time:18073ms step_avg:38.62ms
step:469/2330 train_time:18110ms step_avg:38.61ms
step:470/2330 train_time:18151ms step_avg:38.62ms
step:471/2330 train_time:18188ms step_avg:38.61ms
step:472/2330 train_time:18228ms step_avg:38.62ms
step:473/2330 train_time:18265ms step_avg:38.61ms
step:474/2330 train_time:18305ms step_avg:38.62ms
step:475/2330 train_time:18341ms step_avg:38.61ms
step:476/2330 train_time:18383ms step_avg:38.62ms
step:477/2330 train_time:18419ms step_avg:38.61ms
step:478/2330 train_time:18460ms step_avg:38.62ms
step:479/2330 train_time:18496ms step_avg:38.61ms
step:480/2330 train_time:18537ms step_avg:38.62ms
step:481/2330 train_time:18574ms step_avg:38.61ms
step:482/2330 train_time:18614ms step_avg:38.62ms
step:483/2330 train_time:18652ms step_avg:38.62ms
step:484/2330 train_time:18693ms step_avg:38.62ms
step:485/2330 train_time:18730ms step_avg:38.62ms
step:486/2330 train_time:18771ms step_avg:38.62ms
step:487/2330 train_time:18806ms step_avg:38.62ms
step:488/2330 train_time:18848ms step_avg:38.62ms
step:489/2330 train_time:18883ms step_avg:38.62ms
step:490/2330 train_time:18924ms step_avg:38.62ms
step:491/2330 train_time:18961ms step_avg:38.62ms
step:492/2330 train_time:19003ms step_avg:38.62ms
step:493/2330 train_time:19039ms step_avg:38.62ms
step:494/2330 train_time:19081ms step_avg:38.63ms
step:495/2330 train_time:19117ms step_avg:38.62ms
step:496/2330 train_time:19158ms step_avg:38.63ms
step:497/2330 train_time:19195ms step_avg:38.62ms
step:498/2330 train_time:19235ms step_avg:38.62ms
step:499/2330 train_time:19272ms step_avg:38.62ms
step:500/2330 train_time:19312ms step_avg:38.62ms
step:500/2330 val_loss:5.4217 train_time:19425ms step_avg:38.85ms
step:501/2330 train_time:19436ms step_avg:38.80ms
step:502/2330 train_time:19447ms step_avg:38.74ms
step:503/2330 train_time:19456ms step_avg:38.68ms
step:504/2330 train_time:19467ms step_avg:38.63ms
step:505/2330 train_time:19504ms step_avg:38.62ms
step:506/2330 train_time:19544ms step_avg:38.63ms
step:507/2330 train_time:19579ms step_avg:38.62ms
step:508/2330 train_time:19620ms step_avg:38.62ms
step:509/2330 train_time:19655ms step_avg:38.62ms
step:510/2330 train_time:19696ms step_avg:38.62ms
step:511/2330 train_time:19733ms step_avg:38.62ms
step:512/2330 train_time:19775ms step_avg:38.62ms
step:513/2330 train_time:19817ms step_avg:38.63ms
step:514/2330 train_time:19858ms step_avg:38.63ms
step:515/2330 train_time:19898ms step_avg:38.64ms
step:516/2330 train_time:19939ms step_avg:38.64ms
step:517/2330 train_time:19976ms step_avg:38.64ms
step:518/2330 train_time:20017ms step_avg:38.64ms
step:519/2330 train_time:20053ms step_avg:38.64ms
step:520/2330 train_time:20094ms step_avg:38.64ms
step:521/2330 train_time:20129ms step_avg:38.64ms
step:522/2330 train_time:20170ms step_avg:38.64ms
step:523/2330 train_time:20206ms step_avg:38.63ms
step:524/2330 train_time:20247ms step_avg:38.64ms
step:525/2330 train_time:20282ms step_avg:38.63ms
step:526/2330 train_time:20324ms step_avg:38.64ms
step:527/2330 train_time:20361ms step_avg:38.63ms
step:528/2330 train_time:20403ms step_avg:38.64ms
step:529/2330 train_time:20439ms step_avg:38.64ms
step:530/2330 train_time:20480ms step_avg:38.64ms
step:531/2330 train_time:20516ms step_avg:38.64ms
step:532/2330 train_time:20557ms step_avg:38.64ms
step:533/2330 train_time:20592ms step_avg:38.63ms
step:534/2330 train_time:20633ms step_avg:38.64ms
step:535/2330 train_time:20669ms step_avg:38.63ms
step:536/2330 train_time:20711ms step_avg:38.64ms
step:537/2330 train_time:20748ms step_avg:38.64ms
step:538/2330 train_time:20789ms step_avg:38.64ms
step:539/2330 train_time:20826ms step_avg:38.64ms
step:540/2330 train_time:20868ms step_avg:38.64ms
step:541/2330 train_time:20903ms step_avg:38.64ms
step:542/2330 train_time:20945ms step_avg:38.64ms
step:543/2330 train_time:20981ms step_avg:38.64ms
step:544/2330 train_time:21022ms step_avg:38.64ms
step:545/2330 train_time:21058ms step_avg:38.64ms
step:546/2330 train_time:21100ms step_avg:38.64ms
step:547/2330 train_time:21135ms step_avg:38.64ms
step:548/2330 train_time:21176ms step_avg:38.64ms
step:549/2330 train_time:21211ms step_avg:38.64ms
step:550/2330 train_time:21252ms step_avg:38.64ms
step:551/2330 train_time:21289ms step_avg:38.64ms
step:552/2330 train_time:21330ms step_avg:38.64ms
step:553/2330 train_time:21366ms step_avg:38.64ms
step:554/2330 train_time:21407ms step_avg:38.64ms
step:555/2330 train_time:21443ms step_avg:38.64ms
step:556/2330 train_time:21484ms step_avg:38.64ms
step:557/2330 train_time:21520ms step_avg:38.64ms
step:558/2330 train_time:21562ms step_avg:38.64ms
step:559/2330 train_time:21598ms step_avg:38.64ms
step:560/2330 train_time:21641ms step_avg:38.64ms
step:561/2330 train_time:21676ms step_avg:38.64ms
step:562/2330 train_time:21718ms step_avg:38.64ms
step:563/2330 train_time:21755ms step_avg:38.64ms
step:564/2330 train_time:21797ms step_avg:38.65ms
step:565/2330 train_time:21834ms step_avg:38.64ms
step:566/2330 train_time:21875ms step_avg:38.65ms
step:567/2330 train_time:21912ms step_avg:38.65ms
step:568/2330 train_time:21953ms step_avg:38.65ms
step:569/2330 train_time:21990ms step_avg:38.65ms
step:570/2330 train_time:22031ms step_avg:38.65ms
step:571/2330 train_time:22068ms step_avg:38.65ms
step:572/2330 train_time:22109ms step_avg:38.65ms
step:573/2330 train_time:22145ms step_avg:38.65ms
step:574/2330 train_time:22186ms step_avg:38.65ms
step:575/2330 train_time:22221ms step_avg:38.65ms
step:576/2330 train_time:22263ms step_avg:38.65ms
step:577/2330 train_time:22299ms step_avg:38.65ms
step:578/2330 train_time:22341ms step_avg:38.65ms
step:579/2330 train_time:22376ms step_avg:38.65ms
step:580/2330 train_time:22418ms step_avg:38.65ms
step:581/2330 train_time:22453ms step_avg:38.65ms
step:582/2330 train_time:22494ms step_avg:38.65ms
step:583/2330 train_time:22531ms step_avg:38.65ms
step:584/2330 train_time:22572ms step_avg:38.65ms
step:585/2330 train_time:22610ms step_avg:38.65ms
step:586/2330 train_time:22650ms step_avg:38.65ms
step:587/2330 train_time:22687ms step_avg:38.65ms
step:588/2330 train_time:22729ms step_avg:38.65ms
step:589/2330 train_time:22765ms step_avg:38.65ms
step:590/2330 train_time:22807ms step_avg:38.66ms
step:591/2330 train_time:22843ms step_avg:38.65ms
step:592/2330 train_time:22884ms step_avg:38.66ms
step:593/2330 train_time:22921ms step_avg:38.65ms
step:594/2330 train_time:22962ms step_avg:38.66ms
step:595/2330 train_time:22999ms step_avg:38.65ms
step:596/2330 train_time:23041ms step_avg:38.66ms
step:597/2330 train_time:23077ms step_avg:38.66ms
step:598/2330 train_time:23119ms step_avg:38.66ms
step:599/2330 train_time:23155ms step_avg:38.66ms
step:600/2330 train_time:23197ms step_avg:38.66ms
step:601/2330 train_time:23232ms step_avg:38.66ms
step:602/2330 train_time:23273ms step_avg:38.66ms
step:603/2330 train_time:23310ms step_avg:38.66ms
step:604/2330 train_time:23351ms step_avg:38.66ms
step:605/2330 train_time:23387ms step_avg:38.66ms
step:606/2330 train_time:23429ms step_avg:38.66ms
step:607/2330 train_time:23464ms step_avg:38.65ms
step:608/2330 train_time:23505ms step_avg:38.66ms
step:609/2330 train_time:23540ms step_avg:38.65ms
step:610/2330 train_time:23582ms step_avg:38.66ms
step:611/2330 train_time:23618ms step_avg:38.65ms
step:612/2330 train_time:23659ms step_avg:38.66ms
step:613/2330 train_time:23696ms step_avg:38.66ms
step:614/2330 train_time:23737ms step_avg:38.66ms
step:615/2330 train_time:23773ms step_avg:38.66ms
step:616/2330 train_time:23814ms step_avg:38.66ms
step:617/2330 train_time:23852ms step_avg:38.66ms
step:618/2330 train_time:23893ms step_avg:38.66ms
step:619/2330 train_time:23930ms step_avg:38.66ms
step:620/2330 train_time:23971ms step_avg:38.66ms
step:621/2330 train_time:24008ms step_avg:38.66ms
step:622/2330 train_time:24049ms step_avg:38.66ms
step:623/2330 train_time:24085ms step_avg:38.66ms
step:624/2330 train_time:24126ms step_avg:38.66ms
step:625/2330 train_time:24162ms step_avg:38.66ms
step:626/2330 train_time:24203ms step_avg:38.66ms
step:627/2330 train_time:24239ms step_avg:38.66ms
step:628/2330 train_time:24281ms step_avg:38.66ms
step:629/2330 train_time:24317ms step_avg:38.66ms
step:630/2330 train_time:24358ms step_avg:38.66ms
step:631/2330 train_time:24395ms step_avg:38.66ms
step:632/2330 train_time:24436ms step_avg:38.67ms
step:633/2330 train_time:24472ms step_avg:38.66ms
step:634/2330 train_time:24513ms step_avg:38.66ms
step:635/2330 train_time:24551ms step_avg:38.66ms
step:636/2330 train_time:24592ms step_avg:38.67ms
step:637/2330 train_time:24628ms step_avg:38.66ms
step:638/2330 train_time:24669ms step_avg:38.67ms
step:639/2330 train_time:24705ms step_avg:38.66ms
step:640/2330 train_time:24746ms step_avg:38.67ms
step:641/2330 train_time:24782ms step_avg:38.66ms
step:642/2330 train_time:24824ms step_avg:38.67ms
step:643/2330 train_time:24859ms step_avg:38.66ms
step:644/2330 train_time:24901ms step_avg:38.67ms
step:645/2330 train_time:24938ms step_avg:38.66ms
step:646/2330 train_time:24979ms step_avg:38.67ms
step:647/2330 train_time:25015ms step_avg:38.66ms
step:648/2330 train_time:25057ms step_avg:38.67ms
step:649/2330 train_time:25094ms step_avg:38.67ms
step:650/2330 train_time:25136ms step_avg:38.67ms
step:651/2330 train_time:25171ms step_avg:38.67ms
step:652/2330 train_time:25212ms step_avg:38.67ms
step:653/2330 train_time:25249ms step_avg:38.67ms
step:654/2330 train_time:25290ms step_avg:38.67ms
step:655/2330 train_time:25326ms step_avg:38.67ms
step:656/2330 train_time:25367ms step_avg:38.67ms
step:657/2330 train_time:25403ms step_avg:38.67ms
step:658/2330 train_time:25445ms step_avg:38.67ms
step:659/2330 train_time:25480ms step_avg:38.67ms
step:660/2330 train_time:25522ms step_avg:38.67ms
step:661/2330 train_time:25558ms step_avg:38.67ms
step:662/2330 train_time:25600ms step_avg:38.67ms
step:663/2330 train_time:25635ms step_avg:38.67ms
step:664/2330 train_time:25677ms step_avg:38.67ms
step:665/2330 train_time:25713ms step_avg:38.67ms
step:666/2330 train_time:25754ms step_avg:38.67ms
step:667/2330 train_time:25790ms step_avg:38.67ms
step:668/2330 train_time:25831ms step_avg:38.67ms
step:669/2330 train_time:25867ms step_avg:38.67ms
step:670/2330 train_time:25909ms step_avg:38.67ms
step:671/2330 train_time:25945ms step_avg:38.67ms
step:672/2330 train_time:25986ms step_avg:38.67ms
step:673/2330 train_time:26022ms step_avg:38.67ms
step:674/2330 train_time:26063ms step_avg:38.67ms
step:675/2330 train_time:26100ms step_avg:38.67ms
step:676/2330 train_time:26141ms step_avg:38.67ms
step:677/2330 train_time:26177ms step_avg:38.67ms
step:678/2330 train_time:26219ms step_avg:38.67ms
step:679/2330 train_time:26255ms step_avg:38.67ms
step:680/2330 train_time:26297ms step_avg:38.67ms
step:681/2330 train_time:26332ms step_avg:38.67ms
step:682/2330 train_time:26374ms step_avg:38.67ms
step:683/2330 train_time:26410ms step_avg:38.67ms
step:684/2330 train_time:26451ms step_avg:38.67ms
step:685/2330 train_time:26487ms step_avg:38.67ms
step:686/2330 train_time:26529ms step_avg:38.67ms
step:687/2330 train_time:26564ms step_avg:38.67ms
step:688/2330 train_time:26606ms step_avg:38.67ms
step:689/2330 train_time:26641ms step_avg:38.67ms
step:690/2330 train_time:26683ms step_avg:38.67ms
step:691/2330 train_time:26720ms step_avg:38.67ms
step:692/2330 train_time:26761ms step_avg:38.67ms
step:693/2330 train_time:26797ms step_avg:38.67ms
step:694/2330 train_time:26839ms step_avg:38.67ms
step:695/2330 train_time:26875ms step_avg:38.67ms
step:696/2330 train_time:26917ms step_avg:38.67ms
step:697/2330 train_time:26953ms step_avg:38.67ms
step:698/2330 train_time:26994ms step_avg:38.67ms
step:699/2330 train_time:27031ms step_avg:38.67ms
step:700/2330 train_time:27072ms step_avg:38.67ms
step:701/2330 train_time:27110ms step_avg:38.67ms
step:702/2330 train_time:27151ms step_avg:38.68ms
step:703/2330 train_time:27188ms step_avg:38.67ms
step:704/2330 train_time:27228ms step_avg:38.68ms
step:705/2330 train_time:27265ms step_avg:38.67ms
step:706/2330 train_time:27306ms step_avg:38.68ms
step:707/2330 train_time:27342ms step_avg:38.67ms
step:708/2330 train_time:27383ms step_avg:38.68ms
step:709/2330 train_time:27419ms step_avg:38.67ms
step:710/2330 train_time:27460ms step_avg:38.68ms
step:711/2330 train_time:27496ms step_avg:38.67ms
step:712/2330 train_time:27538ms step_avg:38.68ms
step:713/2330 train_time:27573ms step_avg:38.67ms
step:714/2330 train_time:27615ms step_avg:38.68ms
step:715/2330 train_time:27651ms step_avg:38.67ms
step:716/2330 train_time:27691ms step_avg:38.67ms
step:717/2330 train_time:27729ms step_avg:38.67ms
step:718/2330 train_time:27770ms step_avg:38.68ms
step:719/2330 train_time:27807ms step_avg:38.67ms
step:720/2330 train_time:27848ms step_avg:38.68ms
step:721/2330 train_time:27884ms step_avg:38.67ms
step:722/2330 train_time:27925ms step_avg:38.68ms
step:723/2330 train_time:27962ms step_avg:38.67ms
step:724/2330 train_time:28003ms step_avg:38.68ms
step:725/2330 train_time:28039ms step_avg:38.67ms
step:726/2330 train_time:28081ms step_avg:38.68ms
step:727/2330 train_time:28117ms step_avg:38.68ms
step:728/2330 train_time:28159ms step_avg:38.68ms
step:729/2330 train_time:28194ms step_avg:38.67ms
step:730/2330 train_time:28236ms step_avg:38.68ms
step:731/2330 train_time:28271ms step_avg:38.67ms
step:732/2330 train_time:28312ms step_avg:38.68ms
step:733/2330 train_time:28349ms step_avg:38.68ms
step:734/2330 train_time:28390ms step_avg:38.68ms
step:735/2330 train_time:28427ms step_avg:38.68ms
step:736/2330 train_time:28468ms step_avg:38.68ms
step:737/2330 train_time:28504ms step_avg:38.68ms
step:738/2330 train_time:28546ms step_avg:38.68ms
step:739/2330 train_time:28581ms step_avg:38.67ms
step:740/2330 train_time:28622ms step_avg:38.68ms
step:741/2330 train_time:28658ms step_avg:38.67ms
step:742/2330 train_time:28700ms step_avg:38.68ms
step:743/2330 train_time:28736ms step_avg:38.68ms
step:744/2330 train_time:28778ms step_avg:38.68ms
step:745/2330 train_time:28814ms step_avg:38.68ms
step:746/2330 train_time:28856ms step_avg:38.68ms
step:747/2330 train_time:28892ms step_avg:38.68ms
step:748/2330 train_time:28933ms step_avg:38.68ms
step:749/2330 train_time:28970ms step_avg:38.68ms
step:750/2330 train_time:29011ms step_avg:38.68ms
step:750/2330 val_loss:5.3572 train_time:29125ms step_avg:38.83ms
step:751/2330 train_time:29137ms step_avg:38.80ms
step:752/2330 train_time:29149ms step_avg:38.76ms
step:753/2330 train_time:29158ms step_avg:38.72ms
step:754/2330 train_time:29169ms step_avg:38.69ms
step:755/2330 train_time:29204ms step_avg:38.68ms
step:756/2330 train_time:29245ms step_avg:38.68ms
step:757/2330 train_time:29279ms step_avg:38.68ms
step:758/2330 train_time:29320ms step_avg:38.68ms
step:759/2330 train_time:29355ms step_avg:38.68ms
step:760/2330 train_time:29396ms step_avg:38.68ms
step:761/2330 train_time:29436ms step_avg:38.68ms
step:762/2330 train_time:29479ms step_avg:38.69ms
step:763/2330 train_time:29520ms step_avg:38.69ms
step:764/2330 train_time:29563ms step_avg:38.70ms
step:765/2330 train_time:29599ms step_avg:38.69ms
step:766/2330 train_time:29642ms step_avg:38.70ms
step:767/2330 train_time:29678ms step_avg:38.69ms
step:768/2330 train_time:29719ms step_avg:38.70ms
step:769/2330 train_time:29754ms step_avg:38.69ms
step:770/2330 train_time:29795ms step_avg:38.70ms
step:771/2330 train_time:29831ms step_avg:38.69ms
step:772/2330 train_time:29872ms step_avg:38.69ms
step:773/2330 train_time:29909ms step_avg:38.69ms
step:774/2330 train_time:29950ms step_avg:38.69ms
step:775/2330 train_time:29985ms step_avg:38.69ms
step:776/2330 train_time:30026ms step_avg:38.69ms
step:777/2330 train_time:30062ms step_avg:38.69ms
step:778/2330 train_time:30104ms step_avg:38.69ms
step:779/2330 train_time:30139ms step_avg:38.69ms
step:780/2330 train_time:30181ms step_avg:38.69ms
step:781/2330 train_time:30216ms step_avg:38.69ms
step:782/2330 train_time:30257ms step_avg:38.69ms
step:783/2330 train_time:30292ms step_avg:38.69ms
step:784/2330 train_time:30333ms step_avg:38.69ms
step:785/2330 train_time:30370ms step_avg:38.69ms
step:786/2330 train_time:30411ms step_avg:38.69ms
step:787/2330 train_time:30451ms step_avg:38.69ms
step:788/2330 train_time:30492ms step_avg:38.70ms
step:789/2330 train_time:30530ms step_avg:38.69ms
step:790/2330 train_time:30570ms step_avg:38.70ms
step:791/2330 train_time:30608ms step_avg:38.70ms
step:792/2330 train_time:30649ms step_avg:38.70ms
step:793/2330 train_time:30685ms step_avg:38.70ms
step:794/2330 train_time:30727ms step_avg:38.70ms
step:795/2330 train_time:30762ms step_avg:38.69ms
step:796/2330 train_time:30804ms step_avg:38.70ms
step:797/2330 train_time:30840ms step_avg:38.70ms
step:798/2330 train_time:30881ms step_avg:38.70ms
step:799/2330 train_time:30918ms step_avg:38.70ms
step:800/2330 train_time:30960ms step_avg:38.70ms
step:801/2330 train_time:30996ms step_avg:38.70ms
step:802/2330 train_time:31037ms step_avg:38.70ms
step:803/2330 train_time:31073ms step_avg:38.70ms
step:804/2330 train_time:31114ms step_avg:38.70ms
step:805/2330 train_time:31150ms step_avg:38.70ms
step:806/2330 train_time:31191ms step_avg:38.70ms
step:807/2330 train_time:31227ms step_avg:38.69ms
step:808/2330 train_time:31268ms step_avg:38.70ms
step:809/2330 train_time:31303ms step_avg:38.69ms
step:810/2330 train_time:31344ms step_avg:38.70ms
step:811/2330 train_time:31381ms step_avg:38.69ms
step:812/2330 train_time:31423ms step_avg:38.70ms
step:813/2330 train_time:31460ms step_avg:38.70ms
step:814/2330 train_time:31503ms step_avg:38.70ms
step:815/2330 train_time:31539ms step_avg:38.70ms
step:816/2330 train_time:31582ms step_avg:38.70ms
step:817/2330 train_time:31617ms step_avg:38.70ms
step:818/2330 train_time:31659ms step_avg:38.70ms
step:819/2330 train_time:31695ms step_avg:38.70ms
step:820/2330 train_time:31737ms step_avg:38.70ms
step:821/2330 train_time:31773ms step_avg:38.70ms
step:822/2330 train_time:31815ms step_avg:38.70ms
step:823/2330 train_time:31852ms step_avg:38.70ms
step:824/2330 train_time:31893ms step_avg:38.70ms
step:825/2330 train_time:31930ms step_avg:38.70ms
step:826/2330 train_time:31971ms step_avg:38.71ms
step:827/2330 train_time:32007ms step_avg:38.70ms
step:828/2330 train_time:32048ms step_avg:38.71ms
step:829/2330 train_time:32084ms step_avg:38.70ms
step:830/2330 train_time:32125ms step_avg:38.70ms
step:831/2330 train_time:32161ms step_avg:38.70ms
step:832/2330 train_time:32203ms step_avg:38.71ms
step:833/2330 train_time:32238ms step_avg:38.70ms
step:834/2330 train_time:32280ms step_avg:38.71ms
step:835/2330 train_time:32316ms step_avg:38.70ms
step:836/2330 train_time:32357ms step_avg:38.71ms
step:837/2330 train_time:32394ms step_avg:38.70ms
step:838/2330 train_time:32435ms step_avg:38.71ms
step:839/2330 train_time:32472ms step_avg:38.70ms
step:840/2330 train_time:32513ms step_avg:38.71ms
step:841/2330 train_time:32551ms step_avg:38.71ms
step:842/2330 train_time:32592ms step_avg:38.71ms
step:843/2330 train_time:32629ms step_avg:38.71ms
step:844/2330 train_time:32670ms step_avg:38.71ms
step:845/2330 train_time:32706ms step_avg:38.71ms
step:846/2330 train_time:32747ms step_avg:38.71ms
step:847/2330 train_time:32783ms step_avg:38.71ms
step:848/2330 train_time:32825ms step_avg:38.71ms
step:849/2330 train_time:32860ms step_avg:38.70ms
step:850/2330 train_time:32902ms step_avg:38.71ms
step:851/2330 train_time:32938ms step_avg:38.71ms
step:852/2330 train_time:32980ms step_avg:38.71ms
step:853/2330 train_time:33016ms step_avg:38.71ms
step:854/2330 train_time:33058ms step_avg:38.71ms
step:855/2330 train_time:33093ms step_avg:38.71ms
step:856/2330 train_time:33135ms step_avg:38.71ms
step:857/2330 train_time:33170ms step_avg:38.70ms
step:858/2330 train_time:33211ms step_avg:38.71ms
step:859/2330 train_time:33247ms step_avg:38.70ms
step:860/2330 train_time:33288ms step_avg:38.71ms
step:861/2330 train_time:33325ms step_avg:38.70ms
step:862/2330 train_time:33366ms step_avg:38.71ms
step:863/2330 train_time:33401ms step_avg:38.70ms
step:864/2330 train_time:33443ms step_avg:38.71ms
step:865/2330 train_time:33480ms step_avg:38.71ms
step:866/2330 train_time:33522ms step_avg:38.71ms
step:867/2330 train_time:33559ms step_avg:38.71ms
step:868/2330 train_time:33600ms step_avg:38.71ms
step:869/2330 train_time:33637ms step_avg:38.71ms
step:870/2330 train_time:33678ms step_avg:38.71ms
step:871/2330 train_time:33715ms step_avg:38.71ms
step:872/2330 train_time:33757ms step_avg:38.71ms
step:873/2330 train_time:33792ms step_avg:38.71ms
step:874/2330 train_time:33834ms step_avg:38.71ms
step:875/2330 train_time:33870ms step_avg:38.71ms
step:876/2330 train_time:33911ms step_avg:38.71ms
step:877/2330 train_time:33949ms step_avg:38.71ms
step:878/2330 train_time:33990ms step_avg:38.71ms
step:879/2330 train_time:34027ms step_avg:38.71ms
step:880/2330 train_time:34068ms step_avg:38.71ms
step:881/2330 train_time:34104ms step_avg:38.71ms
step:882/2330 train_time:34145ms step_avg:38.71ms
step:883/2330 train_time:34180ms step_avg:38.71ms
step:884/2330 train_time:34222ms step_avg:38.71ms
step:885/2330 train_time:34258ms step_avg:38.71ms
step:886/2330 train_time:34299ms step_avg:38.71ms
step:887/2330 train_time:34335ms step_avg:38.71ms
step:888/2330 train_time:34376ms step_avg:38.71ms
step:889/2330 train_time:34413ms step_avg:38.71ms
step:890/2330 train_time:34455ms step_avg:38.71ms
step:891/2330 train_time:34491ms step_avg:38.71ms
step:892/2330 train_time:34532ms step_avg:38.71ms
step:893/2330 train_time:34570ms step_avg:38.71ms
step:894/2330 train_time:34611ms step_avg:38.71ms
step:895/2330 train_time:34648ms step_avg:38.71ms
step:896/2330 train_time:34689ms step_avg:38.72ms
step:897/2330 train_time:34726ms step_avg:38.71ms
step:898/2330 train_time:34767ms step_avg:38.72ms
step:899/2330 train_time:34802ms step_avg:38.71ms
step:900/2330 train_time:34844ms step_avg:38.72ms
step:901/2330 train_time:34880ms step_avg:38.71ms
step:902/2330 train_time:34922ms step_avg:38.72ms
step:903/2330 train_time:34957ms step_avg:38.71ms
step:904/2330 train_time:34999ms step_avg:38.72ms
step:905/2330 train_time:35034ms step_avg:38.71ms
step:906/2330 train_time:35076ms step_avg:38.72ms
step:907/2330 train_time:35113ms step_avg:38.71ms
step:908/2330 train_time:35154ms step_avg:38.72ms
step:909/2330 train_time:35191ms step_avg:38.71ms
step:910/2330 train_time:35231ms step_avg:38.72ms
step:911/2330 train_time:35268ms step_avg:38.71ms
step:912/2330 train_time:35309ms step_avg:38.72ms
step:913/2330 train_time:35345ms step_avg:38.71ms
step:914/2330 train_time:35386ms step_avg:38.72ms
step:915/2330 train_time:35422ms step_avg:38.71ms
step:916/2330 train_time:35464ms step_avg:38.72ms
step:917/2330 train_time:35500ms step_avg:38.71ms
step:918/2330 train_time:35542ms step_avg:38.72ms
step:919/2330 train_time:35578ms step_avg:38.71ms
step:920/2330 train_time:35620ms step_avg:38.72ms
step:921/2330 train_time:35656ms step_avg:38.71ms
step:922/2330 train_time:35698ms step_avg:38.72ms
step:923/2330 train_time:35733ms step_avg:38.71ms
step:924/2330 train_time:35775ms step_avg:38.72ms
step:925/2330 train_time:35811ms step_avg:38.72ms
step:926/2330 train_time:35852ms step_avg:38.72ms
step:927/2330 train_time:35890ms step_avg:38.72ms
step:928/2330 train_time:35931ms step_avg:38.72ms
step:929/2330 train_time:35968ms step_avg:38.72ms
step:930/2330 train_time:36009ms step_avg:38.72ms
step:931/2330 train_time:36046ms step_avg:38.72ms
step:932/2330 train_time:36087ms step_avg:38.72ms
step:933/2330 train_time:36123ms step_avg:38.72ms
step:934/2330 train_time:36164ms step_avg:38.72ms
step:935/2330 train_time:36200ms step_avg:38.72ms
step:936/2330 train_time:36241ms step_avg:38.72ms
step:937/2330 train_time:36278ms step_avg:38.72ms
step:938/2330 train_time:36320ms step_avg:38.72ms
step:939/2330 train_time:36356ms step_avg:38.72ms
step:940/2330 train_time:36398ms step_avg:38.72ms
step:941/2330 train_time:36433ms step_avg:38.72ms
step:942/2330 train_time:36475ms step_avg:38.72ms
step:943/2330 train_time:36512ms step_avg:38.72ms
step:944/2330 train_time:36553ms step_avg:38.72ms
step:945/2330 train_time:36591ms step_avg:38.72ms
step:946/2330 train_time:36631ms step_avg:38.72ms
step:947/2330 train_time:36669ms step_avg:38.72ms
step:948/2330 train_time:36710ms step_avg:38.72ms
step:949/2330 train_time:36747ms step_avg:38.72ms
step:950/2330 train_time:36788ms step_avg:38.72ms
step:951/2330 train_time:36824ms step_avg:38.72ms
step:952/2330 train_time:36865ms step_avg:38.72ms
step:953/2330 train_time:36901ms step_avg:38.72ms
step:954/2330 train_time:36942ms step_avg:38.72ms
step:955/2330 train_time:36979ms step_avg:38.72ms
step:956/2330 train_time:37021ms step_avg:38.72ms
step:957/2330 train_time:37057ms step_avg:38.72ms
step:958/2330 train_time:37098ms step_avg:38.72ms
step:959/2330 train_time:37134ms step_avg:38.72ms
step:960/2330 train_time:37176ms step_avg:38.72ms
step:961/2330 train_time:37212ms step_avg:38.72ms
step:962/2330 train_time:37252ms step_avg:38.72ms
step:963/2330 train_time:37290ms step_avg:38.72ms
step:964/2330 train_time:37331ms step_avg:38.72ms
step:965/2330 train_time:37367ms step_avg:38.72ms
step:966/2330 train_time:37408ms step_avg:38.73ms
step:967/2330 train_time:37446ms step_avg:38.72ms
step:968/2330 train_time:37487ms step_avg:38.73ms
step:969/2330 train_time:37523ms step_avg:38.72ms
step:970/2330 train_time:37564ms step_avg:38.73ms
step:971/2330 train_time:37600ms step_avg:38.72ms
step:972/2330 train_time:37642ms step_avg:38.73ms
step:973/2330 train_time:37679ms step_avg:38.72ms
step:974/2330 train_time:37721ms step_avg:38.73ms
step:975/2330 train_time:37757ms step_avg:38.73ms
step:976/2330 train_time:37799ms step_avg:38.73ms
step:977/2330 train_time:37835ms step_avg:38.73ms
step:978/2330 train_time:37876ms step_avg:38.73ms
step:979/2330 train_time:37913ms step_avg:38.73ms
step:980/2330 train_time:37954ms step_avg:38.73ms
step:981/2330 train_time:37992ms step_avg:38.73ms
step:982/2330 train_time:38032ms step_avg:38.73ms
step:983/2330 train_time:38069ms step_avg:38.73ms
step:984/2330 train_time:38110ms step_avg:38.73ms
step:985/2330 train_time:38146ms step_avg:38.73ms
step:986/2330 train_time:38187ms step_avg:38.73ms
step:987/2330 train_time:38223ms step_avg:38.73ms
step:988/2330 train_time:38264ms step_avg:38.73ms
step:989/2330 train_time:38300ms step_avg:38.73ms
step:990/2330 train_time:38342ms step_avg:38.73ms
step:991/2330 train_time:38378ms step_avg:38.73ms
step:992/2330 train_time:38420ms step_avg:38.73ms
step:993/2330 train_time:38456ms step_avg:38.73ms
step:994/2330 train_time:38498ms step_avg:38.73ms
step:995/2330 train_time:38534ms step_avg:38.73ms
step:996/2330 train_time:38576ms step_avg:38.73ms
step:997/2330 train_time:38612ms step_avg:38.73ms
step:998/2330 train_time:38654ms step_avg:38.73ms
step:999/2330 train_time:38691ms step_avg:38.73ms
step:1000/2330 train_time:38731ms step_avg:38.73ms
step:1000/2330 val_loss:5.3199 train_time:38848ms step_avg:38.85ms
step:1001/2330 train_time:38859ms step_avg:38.82ms
step:1002/2330 train_time:38870ms step_avg:38.79ms
step:1003/2330 train_time:38879ms step_avg:38.76ms
step:1004/2330 train_time:38890ms step_avg:38.74ms
step:1005/2330 train_time:38925ms step_avg:38.73ms
step:1006/2330 train_time:38966ms step_avg:38.73ms
step:1007/2330 train_time:39001ms step_avg:38.73ms
step:1008/2330 train_time:39042ms step_avg:38.73ms
step:1009/2330 train_time:39077ms step_avg:38.73ms
step:1010/2330 train_time:39118ms step_avg:38.73ms
step:1011/2330 train_time:39158ms step_avg:38.73ms
step:1012/2330 train_time:39201ms step_avg:38.74ms
step:1013/2330 train_time:39243ms step_avg:38.74ms
step:1014/2330 train_time:39284ms step_avg:38.74ms
step:1015/2330 train_time:39322ms step_avg:38.74ms
step:1016/2330 train_time:39363ms step_avg:38.74ms
step:1017/2330 train_time:39400ms step_avg:38.74ms
step:1018/2330 train_time:39441ms step_avg:38.74ms
step:1019/2330 train_time:39477ms step_avg:38.74ms
step:1020/2330 train_time:39518ms step_avg:38.74ms
step:1021/2330 train_time:39555ms step_avg:38.74ms
step:1022/2330 train_time:39595ms step_avg:38.74ms
step:1023/2330 train_time:39631ms step_avg:38.74ms
step:1024/2330 train_time:39672ms step_avg:38.74ms
step:1025/2330 train_time:39708ms step_avg:38.74ms
step:1026/2330 train_time:39749ms step_avg:38.74ms
step:1027/2330 train_time:39787ms step_avg:38.74ms
step:1028/2330 train_time:39828ms step_avg:38.74ms
step:1029/2330 train_time:39864ms step_avg:38.74ms
step:1030/2330 train_time:39905ms step_avg:38.74ms
step:1031/2330 train_time:39941ms step_avg:38.74ms
step:1032/2330 train_time:39982ms step_avg:38.74ms
step:1033/2330 train_time:40017ms step_avg:38.74ms
step:1034/2330 train_time:40059ms step_avg:38.74ms
step:1035/2330 train_time:40095ms step_avg:38.74ms
step:1036/2330 train_time:40137ms step_avg:38.74ms
step:1037/2330 train_time:40173ms step_avg:38.74ms
step:1038/2330 train_time:40215ms step_avg:38.74ms
step:1039/2330 train_time:40252ms step_avg:38.74ms
step:1040/2330 train_time:40293ms step_avg:38.74ms
step:1041/2330 train_time:40331ms step_avg:38.74ms
step:1042/2330 train_time:40372ms step_avg:38.75ms
step:1043/2330 train_time:40408ms step_avg:38.74ms
step:1044/2330 train_time:40450ms step_avg:38.74ms
step:1045/2330 train_time:40485ms step_avg:38.74ms
step:1046/2330 train_time:40527ms step_avg:38.74ms
step:1047/2330 train_time:40563ms step_avg:38.74ms
step:1048/2330 train_time:40604ms step_avg:38.74ms
step:1049/2330 train_time:40640ms step_avg:38.74ms
step:1050/2330 train_time:40682ms step_avg:38.74ms
step:1051/2330 train_time:40718ms step_avg:38.74ms
step:1052/2330 train_time:40759ms step_avg:38.74ms
step:1053/2330 train_time:40795ms step_avg:38.74ms
step:1054/2330 train_time:40836ms step_avg:38.74ms
step:1055/2330 train_time:40872ms step_avg:38.74ms
step:1056/2330 train_time:40913ms step_avg:38.74ms
step:1057/2330 train_time:40948ms step_avg:38.74ms
step:1058/2330 train_time:40989ms step_avg:38.74ms
step:1059/2330 train_time:41025ms step_avg:38.74ms
step:1060/2330 train_time:41066ms step_avg:38.74ms
step:1061/2330 train_time:41103ms step_avg:38.74ms
step:1062/2330 train_time:41145ms step_avg:38.74ms
step:1063/2330 train_time:41182ms step_avg:38.74ms
step:1064/2330 train_time:41225ms step_avg:38.74ms
step:1065/2330 train_time:41261ms step_avg:38.74ms
step:1066/2330 train_time:41303ms step_avg:38.75ms
step:1067/2330 train_time:41339ms step_avg:38.74ms
step:1068/2330 train_time:41382ms step_avg:38.75ms
step:1069/2330 train_time:41417ms step_avg:38.74ms
step:1070/2330 train_time:41458ms step_avg:38.75ms
step:1071/2330 train_time:41495ms step_avg:38.74ms
step:1072/2330 train_time:41536ms step_avg:38.75ms
step:1073/2330 train_time:41573ms step_avg:38.74ms
step:1074/2330 train_time:41614ms step_avg:38.75ms
step:1075/2330 train_time:41650ms step_avg:38.74ms
step:1076/2330 train_time:41691ms step_avg:38.75ms
step:1077/2330 train_time:41727ms step_avg:38.74ms
step:1078/2330 train_time:41768ms step_avg:38.75ms
step:1079/2330 train_time:41803ms step_avg:38.74ms
step:1080/2330 train_time:41844ms step_avg:38.74ms
step:1081/2330 train_time:41880ms step_avg:38.74ms
step:1082/2330 train_time:41922ms step_avg:38.74ms
step:1083/2330 train_time:41958ms step_avg:38.74ms
step:1084/2330 train_time:41999ms step_avg:38.74ms
step:1085/2330 train_time:42036ms step_avg:38.74ms
step:1086/2330 train_time:42077ms step_avg:38.74ms
step:1087/2330 train_time:42115ms step_avg:38.74ms
step:1088/2330 train_time:42155ms step_avg:38.75ms
step:1089/2330 train_time:42193ms step_avg:38.74ms
step:1090/2330 train_time:42233ms step_avg:38.75ms
step:1091/2330 train_time:42272ms step_avg:38.75ms
step:1092/2330 train_time:42313ms step_avg:38.75ms
step:1093/2330 train_time:42350ms step_avg:38.75ms
step:1094/2330 train_time:42392ms step_avg:38.75ms
step:1095/2330 train_time:42427ms step_avg:38.75ms
step:1096/2330 train_time:42468ms step_avg:38.75ms
step:1097/2330 train_time:42505ms step_avg:38.75ms
step:1098/2330 train_time:42546ms step_avg:38.75ms
step:1099/2330 train_time:42582ms step_avg:38.75ms
step:1100/2330 train_time:42624ms step_avg:38.75ms
step:1101/2330 train_time:42660ms step_avg:38.75ms
step:1102/2330 train_time:42701ms step_avg:38.75ms
step:1103/2330 train_time:42737ms step_avg:38.75ms
step:1104/2330 train_time:42779ms step_avg:38.75ms
step:1105/2330 train_time:42815ms step_avg:38.75ms
step:1106/2330 train_time:42855ms step_avg:38.75ms
step:1107/2330 train_time:42892ms step_avg:38.75ms
step:1108/2330 train_time:42933ms step_avg:38.75ms
step:1109/2330 train_time:42970ms step_avg:38.75ms
step:1110/2330 train_time:43011ms step_avg:38.75ms
step:1111/2330 train_time:43047ms step_avg:38.75ms
step:1112/2330 train_time:43089ms step_avg:38.75ms
step:1113/2330 train_time:43124ms step_avg:38.75ms
step:1114/2330 train_time:43166ms step_avg:38.75ms
step:1115/2330 train_time:43202ms step_avg:38.75ms
step:1116/2330 train_time:43244ms step_avg:38.75ms
step:1117/2330 train_time:43281ms step_avg:38.75ms
step:1118/2330 train_time:43323ms step_avg:38.75ms
step:1119/2330 train_time:43359ms step_avg:38.75ms
step:1120/2330 train_time:43400ms step_avg:38.75ms
step:1121/2330 train_time:43437ms step_avg:38.75ms
step:1122/2330 train_time:43478ms step_avg:38.75ms
step:1123/2330 train_time:43515ms step_avg:38.75ms
step:1124/2330 train_time:43555ms step_avg:38.75ms
step:1125/2330 train_time:43593ms step_avg:38.75ms
step:1126/2330 train_time:43634ms step_avg:38.75ms
step:1127/2330 train_time:43670ms step_avg:38.75ms
step:1128/2330 train_time:43711ms step_avg:38.75ms
step:1129/2330 train_time:43747ms step_avg:38.75ms
step:1130/2330 train_time:43788ms step_avg:38.75ms
step:1131/2330 train_time:43824ms step_avg:38.75ms
step:1132/2330 train_time:43865ms step_avg:38.75ms
step:1133/2330 train_time:43902ms step_avg:38.75ms
step:1134/2330 train_time:43943ms step_avg:38.75ms
step:1135/2330 train_time:43980ms step_avg:38.75ms
step:1136/2330 train_time:44022ms step_avg:38.75ms
step:1137/2330 train_time:44057ms step_avg:38.75ms
step:1138/2330 train_time:44099ms step_avg:38.75ms
step:1139/2330 train_time:44136ms step_avg:38.75ms
step:1140/2330 train_time:44177ms step_avg:38.75ms
step:1141/2330 train_time:44214ms step_avg:38.75ms
step:1142/2330 train_time:44255ms step_avg:38.75ms
step:1143/2330 train_time:44293ms step_avg:38.75ms
step:1144/2330 train_time:44334ms step_avg:38.75ms
step:1145/2330 train_time:44371ms step_avg:38.75ms
step:1146/2330 train_time:44412ms step_avg:38.75ms
step:1147/2330 train_time:44448ms step_avg:38.75ms
step:1148/2330 train_time:44489ms step_avg:38.75ms
step:1149/2330 train_time:44525ms step_avg:38.75ms
step:1150/2330 train_time:44566ms step_avg:38.75ms
step:1151/2330 train_time:44603ms step_avg:38.75ms
step:1152/2330 train_time:44644ms step_avg:38.75ms
step:1153/2330 train_time:44680ms step_avg:38.75ms
step:1154/2330 train_time:44722ms step_avg:38.75ms
step:1155/2330 train_time:44758ms step_avg:38.75ms
step:1156/2330 train_time:44799ms step_avg:38.75ms
step:1157/2330 train_time:44835ms step_avg:38.75ms
step:1158/2330 train_time:44875ms step_avg:38.75ms
step:1159/2330 train_time:44913ms step_avg:38.75ms
step:1160/2330 train_time:44954ms step_avg:38.75ms
step:1161/2330 train_time:44991ms step_avg:38.75ms
step:1162/2330 train_time:45032ms step_avg:38.75ms
step:1163/2330 train_time:45068ms step_avg:38.75ms
step:1164/2330 train_time:45110ms step_avg:38.75ms
step:1165/2330 train_time:45147ms step_avg:38.75ms
step:1166/2330 train_time:45189ms step_avg:38.76ms
step:1167/2330 train_time:45225ms step_avg:38.75ms
step:1168/2330 train_time:45266ms step_avg:38.76ms
step:1169/2330 train_time:45303ms step_avg:38.75ms
step:1170/2330 train_time:45345ms step_avg:38.76ms
step:1171/2330 train_time:45381ms step_avg:38.75ms
step:1172/2330 train_time:45423ms step_avg:38.76ms
step:1173/2330 train_time:45460ms step_avg:38.76ms
step:1174/2330 train_time:45502ms step_avg:38.76ms
step:1175/2330 train_time:45537ms step_avg:38.76ms
step:1176/2330 train_time:45579ms step_avg:38.76ms
step:1177/2330 train_time:45616ms step_avg:38.76ms
step:1178/2330 train_time:45656ms step_avg:38.76ms
step:1179/2330 train_time:45694ms step_avg:38.76ms
step:1180/2330 train_time:45734ms step_avg:38.76ms
step:1181/2330 train_time:45771ms step_avg:38.76ms
step:1182/2330 train_time:45812ms step_avg:38.76ms
step:1183/2330 train_time:45847ms step_avg:38.76ms
step:1184/2330 train_time:45888ms step_avg:38.76ms
step:1185/2330 train_time:45924ms step_avg:38.75ms
step:1186/2330 train_time:45965ms step_avg:38.76ms
step:1187/2330 train_time:46001ms step_avg:38.75ms
step:1188/2330 train_time:46043ms step_avg:38.76ms
step:1189/2330 train_time:46080ms step_avg:38.75ms
step:1190/2330 train_time:46121ms step_avg:38.76ms
step:1191/2330 train_time:46158ms step_avg:38.76ms
step:1192/2330 train_time:46200ms step_avg:38.76ms
step:1193/2330 train_time:46236ms step_avg:38.76ms
step:1194/2330 train_time:46277ms step_avg:38.76ms
step:1195/2330 train_time:46316ms step_avg:38.76ms
step:1196/2330 train_time:46357ms step_avg:38.76ms
step:1197/2330 train_time:46395ms step_avg:38.76ms
step:1198/2330 train_time:46435ms step_avg:38.76ms
step:1199/2330 train_time:46472ms step_avg:38.76ms
step:1200/2330 train_time:46513ms step_avg:38.76ms
step:1201/2330 train_time:46550ms step_avg:38.76ms
step:1202/2330 train_time:46591ms step_avg:38.76ms
step:1203/2330 train_time:46628ms step_avg:38.76ms
step:1204/2330 train_time:46669ms step_avg:38.76ms
step:1205/2330 train_time:46705ms step_avg:38.76ms
step:1206/2330 train_time:46746ms step_avg:38.76ms
step:1207/2330 train_time:46783ms step_avg:38.76ms
step:1208/2330 train_time:46825ms step_avg:38.76ms
step:1209/2330 train_time:46860ms step_avg:38.76ms
step:1210/2330 train_time:46902ms step_avg:38.76ms
step:1211/2330 train_time:46937ms step_avg:38.76ms
step:1212/2330 train_time:46979ms step_avg:38.76ms
step:1213/2330 train_time:47016ms step_avg:38.76ms
step:1214/2330 train_time:47056ms step_avg:38.76ms
step:1215/2330 train_time:47094ms step_avg:38.76ms
step:1216/2330 train_time:47135ms step_avg:38.76ms
step:1217/2330 train_time:47172ms step_avg:38.76ms
step:1218/2330 train_time:47213ms step_avg:38.76ms
step:1219/2330 train_time:47250ms step_avg:38.76ms
step:1220/2330 train_time:47292ms step_avg:38.76ms
step:1221/2330 train_time:47329ms step_avg:38.76ms
step:1222/2330 train_time:47369ms step_avg:38.76ms
step:1223/2330 train_time:47405ms step_avg:38.76ms
step:1224/2330 train_time:47446ms step_avg:38.76ms
step:1225/2330 train_time:47483ms step_avg:38.76ms
step:1226/2330 train_time:47524ms step_avg:38.76ms
step:1227/2330 train_time:47561ms step_avg:38.76ms
step:1228/2330 train_time:47603ms step_avg:38.76ms
step:1229/2330 train_time:47640ms step_avg:38.76ms
step:1230/2330 train_time:47681ms step_avg:38.77ms
step:1231/2330 train_time:47718ms step_avg:38.76ms
step:1232/2330 train_time:47759ms step_avg:38.77ms
step:1233/2330 train_time:47795ms step_avg:38.76ms
step:1234/2330 train_time:47836ms step_avg:38.77ms
step:1235/2330 train_time:47873ms step_avg:38.76ms
step:1236/2330 train_time:47914ms step_avg:38.77ms
step:1237/2330 train_time:47950ms step_avg:38.76ms
step:1238/2330 train_time:47992ms step_avg:38.77ms
step:1239/2330 train_time:48028ms step_avg:38.76ms
step:1240/2330 train_time:48069ms step_avg:38.77ms
step:1241/2330 train_time:48106ms step_avg:38.76ms
step:1242/2330 train_time:48147ms step_avg:38.77ms
step:1243/2330 train_time:48184ms step_avg:38.76ms
step:1244/2330 train_time:48226ms step_avg:38.77ms
step:1245/2330 train_time:48262ms step_avg:38.76ms
step:1246/2330 train_time:48304ms step_avg:38.77ms
step:1247/2330 train_time:48340ms step_avg:38.77ms
step:1248/2330 train_time:48382ms step_avg:38.77ms
step:1249/2330 train_time:48417ms step_avg:38.76ms
step:1250/2330 train_time:48459ms step_avg:38.77ms
step:1250/2330 val_loss:5.2923 train_time:48573ms step_avg:38.86ms
step:1251/2330 train_time:48586ms step_avg:38.84ms
step:1252/2330 train_time:48598ms step_avg:38.82ms
step:1253/2330 train_time:48609ms step_avg:38.79ms
step:1254/2330 train_time:48621ms step_avg:38.77ms
step:1255/2330 train_time:48651ms step_avg:38.77ms
step:1256/2330 train_time:48692ms step_avg:38.77ms
step:1257/2330 train_time:48726ms step_avg:38.76ms
step:1258/2330 train_time:48767ms step_avg:38.77ms
step:1259/2330 train_time:48803ms step_avg:38.76ms
step:1260/2330 train_time:48843ms step_avg:38.76ms
step:1261/2330 train_time:48885ms step_avg:38.77ms
step:1262/2330 train_time:48927ms step_avg:38.77ms
step:1263/2330 train_time:48970ms step_avg:38.77ms
step:1264/2330 train_time:49012ms step_avg:38.77ms
step:1265/2330 train_time:49049ms step_avg:38.77ms
step:1266/2330 train_time:49091ms step_avg:38.78ms
step:1267/2330 train_time:49126ms step_avg:38.77ms
step:1268/2330 train_time:49168ms step_avg:38.78ms
step:1269/2330 train_time:49203ms step_avg:38.77ms
step:1270/2330 train_time:49245ms step_avg:38.78ms
step:1271/2330 train_time:49281ms step_avg:38.77ms
step:1272/2330 train_time:49321ms step_avg:38.77ms
step:1273/2330 train_time:49357ms step_avg:38.77ms
step:1274/2330 train_time:49398ms step_avg:38.77ms
step:1275/2330 train_time:49434ms step_avg:38.77ms
step:1276/2330 train_time:49475ms step_avg:38.77ms
step:1277/2330 train_time:49510ms step_avg:38.77ms
step:1278/2330 train_time:49552ms step_avg:38.77ms
step:1279/2330 train_time:49588ms step_avg:38.77ms
step:1280/2330 train_time:49629ms step_avg:38.77ms
step:1281/2330 train_time:49664ms step_avg:38.77ms
step:1282/2330 train_time:49705ms step_avg:38.77ms
step:1283/2330 train_time:49741ms step_avg:38.77ms
step:1284/2330 train_time:49782ms step_avg:38.77ms
step:1285/2330 train_time:49820ms step_avg:38.77ms
step:1286/2330 train_time:49861ms step_avg:38.77ms
step:1287/2330 train_time:49902ms step_avg:38.77ms
step:1288/2330 train_time:49943ms step_avg:38.78ms
step:1289/2330 train_time:49983ms step_avg:38.78ms
step:1290/2330 train_time:50024ms step_avg:38.78ms
step:1291/2330 train_time:50062ms step_avg:38.78ms
step:1292/2330 train_time:50103ms step_avg:38.78ms
step:1293/2330 train_time:50140ms step_avg:38.78ms
step:1294/2330 train_time:50181ms step_avg:38.78ms
step:1295/2330 train_time:50217ms step_avg:38.78ms
step:1296/2330 train_time:50259ms step_avg:38.78ms
step:1297/2330 train_time:50295ms step_avg:38.78ms
step:1298/2330 train_time:50336ms step_avg:38.78ms
step:1299/2330 train_time:50371ms step_avg:38.78ms
step:1300/2330 train_time:50412ms step_avg:38.78ms
step:1301/2330 train_time:50449ms step_avg:38.78ms
step:1302/2330 train_time:50491ms step_avg:38.78ms
step:1303/2330 train_time:50526ms step_avg:38.78ms
step:1304/2330 train_time:50568ms step_avg:38.78ms
step:1305/2330 train_time:50603ms step_avg:38.78ms
step:1306/2330 train_time:50645ms step_avg:38.78ms
step:1307/2330 train_time:50681ms step_avg:38.78ms
step:1308/2330 train_time:50721ms step_avg:38.78ms
step:1309/2330 train_time:50759ms step_avg:38.78ms
step:1310/2330 train_time:50800ms step_avg:38.78ms
step:1311/2330 train_time:50837ms step_avg:38.78ms
step:1312/2330 train_time:50879ms step_avg:38.78ms
step:1313/2330 train_time:50916ms step_avg:38.78ms
step:1314/2330 train_time:50958ms step_avg:38.78ms
step:1315/2330 train_time:50995ms step_avg:38.78ms
step:1316/2330 train_time:51036ms step_avg:38.78ms
step:1317/2330 train_time:51073ms step_avg:38.78ms
step:1318/2330 train_time:51114ms step_avg:38.78ms
step:1319/2330 train_time:51150ms step_avg:38.78ms
step:1320/2330 train_time:51192ms step_avg:38.78ms
step:1321/2330 train_time:51228ms step_avg:38.78ms
step:1322/2330 train_time:51269ms step_avg:38.78ms
step:1323/2330 train_time:51305ms step_avg:38.78ms
step:1324/2330 train_time:51346ms step_avg:38.78ms
step:1325/2330 train_time:51382ms step_avg:38.78ms
step:1326/2330 train_time:51423ms step_avg:38.78ms
step:1327/2330 train_time:51459ms step_avg:38.78ms
step:1328/2330 train_time:51500ms step_avg:38.78ms
step:1329/2330 train_time:51536ms step_avg:38.78ms
step:1330/2330 train_time:51577ms step_avg:38.78ms
step:1331/2330 train_time:51613ms step_avg:38.78ms
step:1332/2330 train_time:51655ms step_avg:38.78ms
step:1333/2330 train_time:51691ms step_avg:38.78ms
step:1334/2330 train_time:51732ms step_avg:38.78ms
step:1335/2330 train_time:51768ms step_avg:38.78ms
step:1336/2330 train_time:51810ms step_avg:38.78ms
step:1337/2330 train_time:51846ms step_avg:38.78ms
step:1338/2330 train_time:51889ms step_avg:38.78ms
step:1339/2330 train_time:51925ms step_avg:38.78ms
step:1340/2330 train_time:51967ms step_avg:38.78ms
step:1341/2330 train_time:52004ms step_avg:38.78ms
step:1342/2330 train_time:52045ms step_avg:38.78ms
step:1343/2330 train_time:52083ms step_avg:38.78ms
step:1344/2330 train_time:52123ms step_avg:38.78ms
step:1345/2330 train_time:52161ms step_avg:38.78ms
step:1346/2330 train_time:52202ms step_avg:38.78ms
step:1347/2330 train_time:52239ms step_avg:38.78ms
step:1348/2330 train_time:52280ms step_avg:38.78ms
step:1349/2330 train_time:52316ms step_avg:38.78ms
step:1350/2330 train_time:52357ms step_avg:38.78ms
step:1351/2330 train_time:52394ms step_avg:38.78ms
step:1352/2330 train_time:52435ms step_avg:38.78ms
step:1353/2330 train_time:52471ms step_avg:38.78ms
step:1354/2330 train_time:52512ms step_avg:38.78ms
step:1355/2330 train_time:52548ms step_avg:38.78ms
step:1356/2330 train_time:52590ms step_avg:38.78ms
step:1357/2330 train_time:52626ms step_avg:38.78ms
step:1358/2330 train_time:52667ms step_avg:38.78ms
step:1359/2330 train_time:52704ms step_avg:38.78ms
step:1360/2330 train_time:52745ms step_avg:38.78ms
step:1361/2330 train_time:52782ms step_avg:38.78ms
step:1362/2330 train_time:52823ms step_avg:38.78ms
step:1363/2330 train_time:52861ms step_avg:38.78ms
step:1364/2330 train_time:52902ms step_avg:38.78ms
step:1365/2330 train_time:52939ms step_avg:38.78ms
step:1366/2330 train_time:52980ms step_avg:38.78ms
step:1367/2330 train_time:53017ms step_avg:38.78ms
step:1368/2330 train_time:53058ms step_avg:38.79ms
step:1369/2330 train_time:53095ms step_avg:38.78ms
step:1370/2330 train_time:53136ms step_avg:38.79ms
step:1371/2330 train_time:53173ms step_avg:38.78ms
step:1372/2330 train_time:53214ms step_avg:38.79ms
step:1373/2330 train_time:53251ms step_avg:38.78ms
step:1374/2330 train_time:53292ms step_avg:38.79ms
step:1375/2330 train_time:53328ms step_avg:38.78ms
step:1376/2330 train_time:53370ms step_avg:38.79ms
step:1377/2330 train_time:53406ms step_avg:38.78ms
step:1378/2330 train_time:53448ms step_avg:38.79ms
step:1379/2330 train_time:53483ms step_avg:38.78ms
step:1380/2330 train_time:53524ms step_avg:38.79ms
step:1381/2330 train_time:53561ms step_avg:38.78ms
step:1382/2330 train_time:53602ms step_avg:38.79ms
step:1383/2330 train_time:53639ms step_avg:38.78ms
step:1384/2330 train_time:53680ms step_avg:38.79ms
step:1385/2330 train_time:53716ms step_avg:38.78ms
step:1386/2330 train_time:53757ms step_avg:38.79ms
step:1387/2330 train_time:53793ms step_avg:38.78ms
step:1388/2330 train_time:53834ms step_avg:38.79ms
step:1389/2330 train_time:53871ms step_avg:38.78ms
step:1390/2330 train_time:53913ms step_avg:38.79ms
step:1391/2330 train_time:53949ms step_avg:38.78ms
step:1392/2330 train_time:53992ms step_avg:38.79ms
step:1393/2330 train_time:54027ms step_avg:38.78ms
step:1394/2330 train_time:54069ms step_avg:38.79ms
step:1395/2330 train_time:54105ms step_avg:38.79ms
step:1396/2330 train_time:54147ms step_avg:38.79ms
step:1397/2330 train_time:54184ms step_avg:38.79ms
step:1398/2330 train_time:54225ms step_avg:38.79ms
step:1399/2330 train_time:54262ms step_avg:38.79ms
step:1400/2330 train_time:54303ms step_avg:38.79ms
step:1401/2330 train_time:54340ms step_avg:38.79ms
step:1402/2330 train_time:54381ms step_avg:38.79ms
step:1403/2330 train_time:54418ms step_avg:38.79ms
step:1404/2330 train_time:54459ms step_avg:38.79ms
step:1405/2330 train_time:54495ms step_avg:38.79ms
step:1406/2330 train_time:54536ms step_avg:38.79ms
step:1407/2330 train_time:54572ms step_avg:38.79ms
step:1408/2330 train_time:54613ms step_avg:38.79ms
step:1409/2330 train_time:54649ms step_avg:38.79ms
step:1410/2330 train_time:54691ms step_avg:38.79ms
step:1411/2330 train_time:54726ms step_avg:38.79ms
step:1412/2330 train_time:54768ms step_avg:38.79ms
step:1413/2330 train_time:54804ms step_avg:38.79ms
step:1414/2330 train_time:54845ms step_avg:38.79ms
step:1415/2330 train_time:54882ms step_avg:38.79ms
step:1416/2330 train_time:54923ms step_avg:38.79ms
step:1417/2330 train_time:54960ms step_avg:38.79ms
step:1418/2330 train_time:55001ms step_avg:38.79ms
step:1419/2330 train_time:55038ms step_avg:38.79ms
step:1420/2330 train_time:55080ms step_avg:38.79ms
step:1421/2330 train_time:55116ms step_avg:38.79ms
step:1422/2330 train_time:55158ms step_avg:38.79ms
step:1423/2330 train_time:55194ms step_avg:38.79ms
step:1424/2330 train_time:55235ms step_avg:38.79ms
step:1425/2330 train_time:55272ms step_avg:38.79ms
step:1426/2330 train_time:55313ms step_avg:38.79ms
step:1427/2330 train_time:55350ms step_avg:38.79ms
step:1428/2330 train_time:55391ms step_avg:38.79ms
step:1429/2330 train_time:55427ms step_avg:38.79ms
step:1430/2330 train_time:55469ms step_avg:38.79ms
step:1431/2330 train_time:55505ms step_avg:38.79ms
step:1432/2330 train_time:55546ms step_avg:38.79ms
step:1433/2330 train_time:55582ms step_avg:38.79ms
step:1434/2330 train_time:55623ms step_avg:38.79ms
step:1435/2330 train_time:55660ms step_avg:38.79ms
step:1436/2330 train_time:55701ms step_avg:38.79ms
step:1437/2330 train_time:55738ms step_avg:38.79ms
step:1438/2330 train_time:55779ms step_avg:38.79ms
step:1439/2330 train_time:55816ms step_avg:38.79ms
step:1440/2330 train_time:55857ms step_avg:38.79ms
step:1441/2330 train_time:55894ms step_avg:38.79ms
step:1442/2330 train_time:55935ms step_avg:38.79ms
step:1443/2330 train_time:55971ms step_avg:38.79ms
step:1444/2330 train_time:56013ms step_avg:38.79ms
step:1445/2330 train_time:56049ms step_avg:38.79ms
step:1446/2330 train_time:56092ms step_avg:38.79ms
step:1447/2330 train_time:56128ms step_avg:38.79ms
step:1448/2330 train_time:56170ms step_avg:38.79ms
step:1449/2330 train_time:56206ms step_avg:38.79ms
step:1450/2330 train_time:56248ms step_avg:38.79ms
step:1451/2330 train_time:56284ms step_avg:38.79ms
step:1452/2330 train_time:56326ms step_avg:38.79ms
step:1453/2330 train_time:56363ms step_avg:38.79ms
step:1454/2330 train_time:56404ms step_avg:38.79ms
step:1455/2330 train_time:56441ms step_avg:38.79ms
step:1456/2330 train_time:56482ms step_avg:38.79ms
step:1457/2330 train_time:56519ms step_avg:38.79ms
step:1458/2330 train_time:56560ms step_avg:38.79ms
step:1459/2330 train_time:56596ms step_avg:38.79ms
step:1460/2330 train_time:56637ms step_avg:38.79ms
step:1461/2330 train_time:56674ms step_avg:38.79ms
step:1462/2330 train_time:56715ms step_avg:38.79ms
step:1463/2330 train_time:56750ms step_avg:38.79ms
step:1464/2330 train_time:56792ms step_avg:38.79ms
step:1465/2330 train_time:56828ms step_avg:38.79ms
step:1466/2330 train_time:56870ms step_avg:38.79ms
step:1467/2330 train_time:56906ms step_avg:38.79ms
step:1468/2330 train_time:56947ms step_avg:38.79ms
step:1469/2330 train_time:56984ms step_avg:38.79ms
step:1470/2330 train_time:57025ms step_avg:38.79ms
step:1471/2330 train_time:57063ms step_avg:38.79ms
step:1472/2330 train_time:57103ms step_avg:38.79ms
step:1473/2330 train_time:57142ms step_avg:38.79ms
step:1474/2330 train_time:57182ms step_avg:38.79ms
step:1475/2330 train_time:57220ms step_avg:38.79ms
step:1476/2330 train_time:57261ms step_avg:38.79ms
step:1477/2330 train_time:57298ms step_avg:38.79ms
step:1478/2330 train_time:57339ms step_avg:38.80ms
step:1479/2330 train_time:57377ms step_avg:38.79ms
step:1480/2330 train_time:57418ms step_avg:38.80ms
step:1481/2330 train_time:57454ms step_avg:38.79ms
step:1482/2330 train_time:57496ms step_avg:38.80ms
step:1483/2330 train_time:57531ms step_avg:38.79ms
step:1484/2330 train_time:57572ms step_avg:38.80ms
step:1485/2330 train_time:57609ms step_avg:38.79ms
step:1486/2330 train_time:57650ms step_avg:38.80ms
step:1487/2330 train_time:57686ms step_avg:38.79ms
step:1488/2330 train_time:57727ms step_avg:38.80ms
step:1489/2330 train_time:57764ms step_avg:38.79ms
step:1490/2330 train_time:57805ms step_avg:38.80ms
step:1491/2330 train_time:57843ms step_avg:38.79ms
step:1492/2330 train_time:57883ms step_avg:38.80ms
step:1493/2330 train_time:57921ms step_avg:38.79ms
step:1494/2330 train_time:57961ms step_avg:38.80ms
step:1495/2330 train_time:57998ms step_avg:38.79ms
step:1496/2330 train_time:58039ms step_avg:38.80ms
step:1497/2330 train_time:58076ms step_avg:38.80ms
step:1498/2330 train_time:58118ms step_avg:38.80ms
step:1499/2330 train_time:58153ms step_avg:38.79ms
step:1500/2330 train_time:58194ms step_avg:38.80ms
step:1500/2330 val_loss:5.2513 train_time:58309ms step_avg:38.87ms
step:1501/2330 train_time:58321ms step_avg:38.85ms
step:1502/2330 train_time:58332ms step_avg:38.84ms
step:1503/2330 train_time:58341ms step_avg:38.82ms
step:1504/2330 train_time:58351ms step_avg:38.80ms
step:1505/2330 train_time:58387ms step_avg:38.80ms
step:1506/2330 train_time:58429ms step_avg:38.80ms
step:1507/2330 train_time:58464ms step_avg:38.79ms
step:1508/2330 train_time:58505ms step_avg:38.80ms
step:1509/2330 train_time:58540ms step_avg:38.79ms
step:1510/2330 train_time:58580ms step_avg:38.79ms
step:1511/2330 train_time:58621ms step_avg:38.80ms
step:1512/2330 train_time:58664ms step_avg:38.80ms
step:1513/2330 train_time:58705ms step_avg:38.80ms
step:1514/2330 train_time:58746ms step_avg:38.80ms
step:1515/2330 train_time:58784ms step_avg:38.80ms
step:1516/2330 train_time:58825ms step_avg:38.80ms
step:1517/2330 train_time:58862ms step_avg:38.80ms
step:1518/2330 train_time:58903ms step_avg:38.80ms
step:1519/2330 train_time:58939ms step_avg:38.80ms
step:1520/2330 train_time:58980ms step_avg:38.80ms
step:1521/2330 train_time:59017ms step_avg:38.80ms
step:1522/2330 train_time:59058ms step_avg:38.80ms
step:1523/2330 train_time:59093ms step_avg:38.80ms
step:1524/2330 train_time:59134ms step_avg:38.80ms
step:1525/2330 train_time:59169ms step_avg:38.80ms
step:1526/2330 train_time:59211ms step_avg:38.80ms
step:1527/2330 train_time:59246ms step_avg:38.80ms
step:1528/2330 train_time:59288ms step_avg:38.80ms
step:1529/2330 train_time:59325ms step_avg:38.80ms
step:1530/2330 train_time:59367ms step_avg:38.80ms
step:1531/2330 train_time:59402ms step_avg:38.80ms
step:1532/2330 train_time:59443ms step_avg:38.80ms
step:1533/2330 train_time:59479ms step_avg:38.80ms
step:1534/2330 train_time:59520ms step_avg:38.80ms
step:1535/2330 train_time:59558ms step_avg:38.80ms
step:1536/2330 train_time:59599ms step_avg:38.80ms
step:1537/2330 train_time:59639ms step_avg:38.80ms
step:1538/2330 train_time:59680ms step_avg:38.80ms
step:1539/2330 train_time:59719ms step_avg:38.80ms
step:1540/2330 train_time:59759ms step_avg:38.80ms
step:1541/2330 train_time:59797ms step_avg:38.80ms
step:1542/2330 train_time:59839ms step_avg:38.81ms
step:1543/2330 train_time:59875ms step_avg:38.80ms
step:1544/2330 train_time:59916ms step_avg:38.81ms
step:1545/2330 train_time:59951ms step_avg:38.80ms
step:1546/2330 train_time:59992ms step_avg:38.80ms
step:1547/2330 train_time:60028ms step_avg:38.80ms
step:1548/2330 train_time:60069ms step_avg:38.80ms
step:1549/2330 train_time:60106ms step_avg:38.80ms
step:1550/2330 train_time:60147ms step_avg:38.80ms
step:1551/2330 train_time:60182ms step_avg:38.80ms
step:1552/2330 train_time:60223ms step_avg:38.80ms
step:1553/2330 train_time:60260ms step_avg:38.80ms
step:1554/2330 train_time:60301ms step_avg:38.80ms
step:1555/2330 train_time:60338ms step_avg:38.80ms
step:1556/2330 train_time:60378ms step_avg:38.80ms
step:1557/2330 train_time:60414ms step_avg:38.80ms
step:1558/2330 train_time:60455ms step_avg:38.80ms
step:1559/2330 train_time:60491ms step_avg:38.80ms
step:1560/2330 train_time:60533ms step_avg:38.80ms
step:1561/2330 train_time:60569ms step_avg:38.80ms
step:1562/2330 train_time:60611ms step_avg:38.80ms
step:1563/2330 train_time:60648ms step_avg:38.80ms
step:1564/2330 train_time:60691ms step_avg:38.80ms
step:1565/2330 train_time:60728ms step_avg:38.80ms
step:1566/2330 train_time:60770ms step_avg:38.81ms
step:1567/2330 train_time:60806ms step_avg:38.80ms
step:1568/2330 train_time:60848ms step_avg:38.81ms
step:1569/2330 train_time:60884ms step_avg:38.80ms
step:1570/2330 train_time:60926ms step_avg:38.81ms
step:1571/2330 train_time:60962ms step_avg:38.80ms
step:1572/2330 train_time:61004ms step_avg:38.81ms
step:1573/2330 train_time:61040ms step_avg:38.80ms
step:1574/2330 train_time:61080ms step_avg:38.81ms
step:1575/2330 train_time:61117ms step_avg:38.80ms
step:1576/2330 train_time:61158ms step_avg:38.81ms
step:1577/2330 train_time:61194ms step_avg:38.80ms
step:1578/2330 train_time:61235ms step_avg:38.81ms
step:1579/2330 train_time:61271ms step_avg:38.80ms
step:1580/2330 train_time:61312ms step_avg:38.81ms
step:1581/2330 train_time:61348ms step_avg:38.80ms
step:1582/2330 train_time:61390ms step_avg:38.81ms
step:1583/2330 train_time:61426ms step_avg:38.80ms
step:1584/2330 train_time:61468ms step_avg:38.81ms
step:1585/2330 train_time:61504ms step_avg:38.80ms
step:1586/2330 train_time:61546ms step_avg:38.81ms
step:1587/2330 train_time:61582ms step_avg:38.80ms
step:1588/2330 train_time:61624ms step_avg:38.81ms
step:1589/2330 train_time:61661ms step_avg:38.81ms
step:1590/2330 train_time:61703ms step_avg:38.81ms
step:1591/2330 train_time:61740ms step_avg:38.81ms
step:1592/2330 train_time:61781ms step_avg:38.81ms
step:1593/2330 train_time:61819ms step_avg:38.81ms
step:1594/2330 train_time:61859ms step_avg:38.81ms
step:1595/2330 train_time:61897ms step_avg:38.81ms
step:1596/2330 train_time:61938ms step_avg:38.81ms
step:1597/2330 train_time:61975ms step_avg:38.81ms
step:1598/2330 train_time:62016ms step_avg:38.81ms
step:1599/2330 train_time:62052ms step_avg:38.81ms
step:1600/2330 train_time:62092ms step_avg:38.81ms
step:1601/2330 train_time:62130ms step_avg:38.81ms
step:1602/2330 train_time:62170ms step_avg:38.81ms
step:1603/2330 train_time:62207ms step_avg:38.81ms
step:1604/2330 train_time:62248ms step_avg:38.81ms
step:1605/2330 train_time:62284ms step_avg:38.81ms
step:1606/2330 train_time:62325ms step_avg:38.81ms
step:1607/2330 train_time:62361ms step_avg:38.81ms
step:1608/2330 train_time:62402ms step_avg:38.81ms
step:1609/2330 train_time:62439ms step_avg:38.81ms
step:1610/2330 train_time:62480ms step_avg:38.81ms
step:1611/2330 train_time:62518ms step_avg:38.81ms
step:1612/2330 train_time:62559ms step_avg:38.81ms
step:1613/2330 train_time:62596ms step_avg:38.81ms
step:1614/2330 train_time:62637ms step_avg:38.81ms
step:1615/2330 train_time:62674ms step_avg:38.81ms
step:1616/2330 train_time:62716ms step_avg:38.81ms
step:1617/2330 train_time:62751ms step_avg:38.81ms
step:1618/2330 train_time:62792ms step_avg:38.81ms
step:1619/2330 train_time:62830ms step_avg:38.81ms
step:1620/2330 train_time:62872ms step_avg:38.81ms
step:1621/2330 train_time:62908ms step_avg:38.81ms
step:1622/2330 train_time:62949ms step_avg:38.81ms
step:1623/2330 train_time:62985ms step_avg:38.81ms
step:1624/2330 train_time:63027ms step_avg:38.81ms
step:1625/2330 train_time:63063ms step_avg:38.81ms
step:1626/2330 train_time:63104ms step_avg:38.81ms
step:1627/2330 train_time:63141ms step_avg:38.81ms
step:1628/2330 train_time:63182ms step_avg:38.81ms
step:1629/2330 train_time:63218ms step_avg:38.81ms
step:1630/2330 train_time:63259ms step_avg:38.81ms
step:1631/2330 train_time:63295ms step_avg:38.81ms
step:1632/2330 train_time:63336ms step_avg:38.81ms
step:1633/2330 train_time:63371ms step_avg:38.81ms
step:1634/2330 train_time:63413ms step_avg:38.81ms
step:1635/2330 train_time:63448ms step_avg:38.81ms
step:1636/2330 train_time:63489ms step_avg:38.81ms
step:1637/2330 train_time:63526ms step_avg:38.81ms
step:1638/2330 train_time:63568ms step_avg:38.81ms
step:1639/2330 train_time:63605ms step_avg:38.81ms
step:1640/2330 train_time:63647ms step_avg:38.81ms
step:1641/2330 train_time:63684ms step_avg:38.81ms
step:1642/2330 train_time:63725ms step_avg:38.81ms
step:1643/2330 train_time:63762ms step_avg:38.81ms
step:1644/2330 train_time:63803ms step_avg:38.81ms
step:1645/2330 train_time:63840ms step_avg:38.81ms
step:1646/2330 train_time:63880ms step_avg:38.81ms
step:1647/2330 train_time:63919ms step_avg:38.81ms
step:1648/2330 train_time:63960ms step_avg:38.81ms
step:1649/2330 train_time:63998ms step_avg:38.81ms
step:1650/2330 train_time:64039ms step_avg:38.81ms
step:1651/2330 train_time:64075ms step_avg:38.81ms
step:1652/2330 train_time:64116ms step_avg:38.81ms
step:1653/2330 train_time:64152ms step_avg:38.81ms
step:1654/2330 train_time:64193ms step_avg:38.81ms
step:1655/2330 train_time:64229ms step_avg:38.81ms
step:1656/2330 train_time:64270ms step_avg:38.81ms
step:1657/2330 train_time:64306ms step_avg:38.81ms
step:1658/2330 train_time:64348ms step_avg:38.81ms
step:1659/2330 train_time:64383ms step_avg:38.81ms
step:1660/2330 train_time:64425ms step_avg:38.81ms
step:1661/2330 train_time:64462ms step_avg:38.81ms
step:1662/2330 train_time:64503ms step_avg:38.81ms
step:1663/2330 train_time:64540ms step_avg:38.81ms
step:1664/2330 train_time:64581ms step_avg:38.81ms
step:1665/2330 train_time:64619ms step_avg:38.81ms
step:1666/2330 train_time:64660ms step_avg:38.81ms
step:1667/2330 train_time:64697ms step_avg:38.81ms
step:1668/2330 train_time:64738ms step_avg:38.81ms
step:1669/2330 train_time:64774ms step_avg:38.81ms
step:1670/2330 train_time:64816ms step_avg:38.81ms
step:1671/2330 train_time:64851ms step_avg:38.81ms
step:1672/2330 train_time:64893ms step_avg:38.81ms
step:1673/2330 train_time:64930ms step_avg:38.81ms
step:1674/2330 train_time:64972ms step_avg:38.81ms
step:1675/2330 train_time:65008ms step_avg:38.81ms
step:1676/2330 train_time:65050ms step_avg:38.81ms
step:1677/2330 train_time:65086ms step_avg:38.81ms
step:1678/2330 train_time:65128ms step_avg:38.81ms
step:1679/2330 train_time:65164ms step_avg:38.81ms
step:1680/2330 train_time:65205ms step_avg:38.81ms
step:1681/2330 train_time:65241ms step_avg:38.81ms
step:1682/2330 train_time:65282ms step_avg:38.81ms
step:1683/2330 train_time:65319ms step_avg:38.81ms
step:1684/2330 train_time:65359ms step_avg:38.81ms
step:1685/2330 train_time:65396ms step_avg:38.81ms
step:1686/2330 train_time:65437ms step_avg:38.81ms
step:1687/2330 train_time:65474ms step_avg:38.81ms
step:1688/2330 train_time:65516ms step_avg:38.81ms
step:1689/2330 train_time:65552ms step_avg:38.81ms
step:1690/2330 train_time:65593ms step_avg:38.81ms
step:1691/2330 train_time:65629ms step_avg:38.81ms
step:1692/2330 train_time:65671ms step_avg:38.81ms
step:1693/2330 train_time:65707ms step_avg:38.81ms
step:1694/2330 train_time:65749ms step_avg:38.81ms
step:1695/2330 train_time:65785ms step_avg:38.81ms
step:1696/2330 train_time:65827ms step_avg:38.81ms
step:1697/2330 train_time:65864ms step_avg:38.81ms
step:1698/2330 train_time:65906ms step_avg:38.81ms
step:1699/2330 train_time:65942ms step_avg:38.81ms
step:1700/2330 train_time:65983ms step_avg:38.81ms
step:1701/2330 train_time:66021ms step_avg:38.81ms
step:1702/2330 train_time:66061ms step_avg:38.81ms
step:1703/2330 train_time:66099ms step_avg:38.81ms
step:1704/2330 train_time:66139ms step_avg:38.81ms
step:1705/2330 train_time:66176ms step_avg:38.81ms
step:1706/2330 train_time:66218ms step_avg:38.81ms
step:1707/2330 train_time:66254ms step_avg:38.81ms
step:1708/2330 train_time:66295ms step_avg:38.81ms
step:1709/2330 train_time:66330ms step_avg:38.81ms
step:1710/2330 train_time:66372ms step_avg:38.81ms
step:1711/2330 train_time:66409ms step_avg:38.81ms
step:1712/2330 train_time:66450ms step_avg:38.81ms
step:1713/2330 train_time:66486ms step_avg:38.81ms
step:1714/2330 train_time:66528ms step_avg:38.81ms
step:1715/2330 train_time:66564ms step_avg:38.81ms
step:1716/2330 train_time:66605ms step_avg:38.81ms
step:1717/2330 train_time:66642ms step_avg:38.81ms
step:1718/2330 train_time:66684ms step_avg:38.81ms
step:1719/2330 train_time:66720ms step_avg:38.81ms
step:1720/2330 train_time:66761ms step_avg:38.81ms
step:1721/2330 train_time:66799ms step_avg:38.81ms
step:1722/2330 train_time:66840ms step_avg:38.82ms
step:1723/2330 train_time:66877ms step_avg:38.81ms
step:1724/2330 train_time:66918ms step_avg:38.82ms
step:1725/2330 train_time:66955ms step_avg:38.81ms
step:1726/2330 train_time:66996ms step_avg:38.82ms
step:1727/2330 train_time:67032ms step_avg:38.81ms
step:1728/2330 train_time:67074ms step_avg:38.82ms
step:1729/2330 train_time:67110ms step_avg:38.81ms
step:1730/2330 train_time:67151ms step_avg:38.82ms
step:1731/2330 train_time:67187ms step_avg:38.81ms
step:1732/2330 train_time:67229ms step_avg:38.82ms
step:1733/2330 train_time:67265ms step_avg:38.81ms
step:1734/2330 train_time:67307ms step_avg:38.82ms
step:1735/2330 train_time:67343ms step_avg:38.81ms
step:1736/2330 train_time:67384ms step_avg:38.82ms
step:1737/2330 train_time:67421ms step_avg:38.81ms
step:1738/2330 train_time:67462ms step_avg:38.82ms
step:1739/2330 train_time:67499ms step_avg:38.81ms
step:1740/2330 train_time:67540ms step_avg:38.82ms
step:1741/2330 train_time:67577ms step_avg:38.82ms
step:1742/2330 train_time:67618ms step_avg:38.82ms
step:1743/2330 train_time:67655ms step_avg:38.82ms
step:1744/2330 train_time:67696ms step_avg:38.82ms
step:1745/2330 train_time:67733ms step_avg:38.82ms
step:1746/2330 train_time:67774ms step_avg:38.82ms
step:1747/2330 train_time:67810ms step_avg:38.81ms
step:1748/2330 train_time:67851ms step_avg:38.82ms
step:1749/2330 train_time:67887ms step_avg:38.81ms
step:1750/2330 train_time:67929ms step_avg:38.82ms
step:1750/2330 val_loss:5.2133 train_time:68044ms step_avg:38.88ms
step:1751/2330 train_time:68055ms step_avg:38.87ms
step:1752/2330 train_time:68066ms step_avg:38.85ms
step:1753/2330 train_time:68075ms step_avg:38.83ms
step:1754/2330 train_time:68087ms step_avg:38.82ms
step:1755/2330 train_time:68121ms step_avg:38.82ms
step:1756/2330 train_time:68162ms step_avg:38.82ms
step:1757/2330 train_time:68197ms step_avg:38.81ms
step:1758/2330 train_time:68238ms step_avg:38.82ms
step:1759/2330 train_time:68273ms step_avg:38.81ms
step:1760/2330 train_time:68313ms step_avg:38.81ms
step:1761/2330 train_time:68351ms step_avg:38.81ms
step:1762/2330 train_time:68394ms step_avg:38.82ms
step:1763/2330 train_time:68435ms step_avg:38.82ms
step:1764/2330 train_time:68476ms step_avg:38.82ms
step:1765/2330 train_time:68516ms step_avg:38.82ms
step:1766/2330 train_time:68556ms step_avg:38.82ms
step:1767/2330 train_time:68595ms step_avg:38.82ms
step:1768/2330 train_time:68635ms step_avg:38.82ms
step:1769/2330 train_time:68674ms step_avg:38.82ms
step:1770/2330 train_time:68714ms step_avg:38.82ms
step:1771/2330 train_time:68751ms step_avg:38.82ms
step:1772/2330 train_time:68792ms step_avg:38.82ms
step:1773/2330 train_time:68828ms step_avg:38.82ms
step:1774/2330 train_time:68869ms step_avg:38.82ms
step:1775/2330 train_time:68904ms step_avg:38.82ms
step:1776/2330 train_time:68945ms step_avg:38.82ms
step:1777/2330 train_time:68981ms step_avg:38.82ms
step:1778/2330 train_time:69022ms step_avg:38.82ms
step:1779/2330 train_time:69059ms step_avg:38.82ms
step:1780/2330 train_time:69100ms step_avg:38.82ms
step:1781/2330 train_time:69136ms step_avg:38.82ms
step:1782/2330 train_time:69177ms step_avg:38.82ms
step:1783/2330 train_time:69213ms step_avg:38.82ms
step:1784/2330 train_time:69253ms step_avg:38.82ms
step:1785/2330 train_time:69290ms step_avg:38.82ms
step:1786/2330 train_time:69331ms step_avg:38.82ms
step:1787/2330 train_time:69369ms step_avg:38.82ms
step:1788/2330 train_time:69411ms step_avg:38.82ms
step:1789/2330 train_time:69449ms step_avg:38.82ms
step:1790/2330 train_time:69491ms step_avg:38.82ms
step:1791/2330 train_time:69528ms step_avg:38.82ms
step:1792/2330 train_time:69571ms step_avg:38.82ms
step:1793/2330 train_time:69606ms step_avg:38.82ms
step:1794/2330 train_time:69648ms step_avg:38.82ms
step:1795/2330 train_time:69684ms step_avg:38.82ms
step:1796/2330 train_time:69727ms step_avg:38.82ms
step:1797/2330 train_time:69762ms step_avg:38.82ms
step:1798/2330 train_time:69804ms step_avg:38.82ms
step:1799/2330 train_time:69839ms step_avg:38.82ms
step:1800/2330 train_time:69881ms step_avg:38.82ms
step:1801/2330 train_time:69917ms step_avg:38.82ms
step:1802/2330 train_time:69957ms step_avg:38.82ms
step:1803/2330 train_time:69994ms step_avg:38.82ms
step:1804/2330 train_time:70035ms step_avg:38.82ms
step:1805/2330 train_time:70072ms step_avg:38.82ms
step:1806/2330 train_time:70113ms step_avg:38.82ms
step:1807/2330 train_time:70149ms step_avg:38.82ms
step:1808/2330 train_time:70189ms step_avg:38.82ms
step:1809/2330 train_time:70225ms step_avg:38.82ms
step:1810/2330 train_time:70266ms step_avg:38.82ms
step:1811/2330 train_time:70302ms step_avg:38.82ms
step:1812/2330 train_time:70344ms step_avg:38.82ms
step:1813/2330 train_time:70380ms step_avg:38.82ms
step:1814/2330 train_time:70422ms step_avg:38.82ms
step:1815/2330 train_time:70460ms step_avg:38.82ms
step:1816/2330 train_time:70502ms step_avg:38.82ms
step:1817/2330 train_time:70539ms step_avg:38.82ms
step:1818/2330 train_time:70581ms step_avg:38.82ms
step:1819/2330 train_time:70617ms step_avg:38.82ms
step:1820/2330 train_time:70659ms step_avg:38.82ms
step:1821/2330 train_time:70695ms step_avg:38.82ms
step:1822/2330 train_time:70737ms step_avg:38.82ms
step:1823/2330 train_time:70774ms step_avg:38.82ms
step:1824/2330 train_time:70814ms step_avg:38.82ms
step:1825/2330 train_time:70851ms step_avg:38.82ms
step:1826/2330 train_time:70892ms step_avg:38.82ms
step:1827/2330 train_time:70928ms step_avg:38.82ms
step:1828/2330 train_time:70970ms step_avg:38.82ms
step:1829/2330 train_time:71004ms step_avg:38.82ms
step:1830/2330 train_time:71046ms step_avg:38.82ms
step:1831/2330 train_time:71081ms step_avg:38.82ms
step:1832/2330 train_time:71123ms step_avg:38.82ms
step:1833/2330 train_time:71158ms step_avg:38.82ms
step:1834/2330 train_time:71200ms step_avg:38.82ms
step:1835/2330 train_time:71236ms step_avg:38.82ms
step:1836/2330 train_time:71277ms step_avg:38.82ms
step:1837/2330 train_time:71314ms step_avg:38.82ms
step:1838/2330 train_time:71354ms step_avg:38.82ms
step:1839/2330 train_time:71393ms step_avg:38.82ms
step:1840/2330 train_time:71434ms step_avg:38.82ms
step:1841/2330 train_time:71472ms step_avg:38.82ms
step:1842/2330 train_time:71513ms step_avg:38.82ms
step:1843/2330 train_time:71550ms step_avg:38.82ms
step:1844/2330 train_time:71591ms step_avg:38.82ms
step:1845/2330 train_time:71627ms step_avg:38.82ms
step:1846/2330 train_time:71669ms step_avg:38.82ms
step:1847/2330 train_time:71704ms step_avg:38.82ms
step:1848/2330 train_time:71745ms step_avg:38.82ms
step:1849/2330 train_time:71781ms step_avg:38.82ms
step:1850/2330 train_time:71824ms step_avg:38.82ms
step:1851/2330 train_time:71860ms step_avg:38.82ms
step:1852/2330 train_time:71902ms step_avg:38.82ms
step:1853/2330 train_time:71937ms step_avg:38.82ms
step:1854/2330 train_time:71978ms step_avg:38.82ms
step:1855/2330 train_time:72013ms step_avg:38.82ms
step:1856/2330 train_time:72054ms step_avg:38.82ms
step:1857/2330 train_time:72090ms step_avg:38.82ms
step:1858/2330 train_time:72131ms step_avg:38.82ms
step:1859/2330 train_time:72167ms step_avg:38.82ms
step:1860/2330 train_time:72208ms step_avg:38.82ms
step:1861/2330 train_time:72244ms step_avg:38.82ms
step:1862/2330 train_time:72285ms step_avg:38.82ms
step:1863/2330 train_time:72322ms step_avg:38.82ms
step:1864/2330 train_time:72364ms step_avg:38.82ms
step:1865/2330 train_time:72401ms step_avg:38.82ms
step:1866/2330 train_time:72443ms step_avg:38.82ms
step:1867/2330 train_time:72480ms step_avg:38.82ms
step:1868/2330 train_time:72521ms step_avg:38.82ms
step:1869/2330 train_time:72557ms step_avg:38.82ms
step:1870/2330 train_time:72599ms step_avg:38.82ms
step:1871/2330 train_time:72635ms step_avg:38.82ms
step:1872/2330 train_time:72676ms step_avg:38.82ms
step:1873/2330 train_time:72713ms step_avg:38.82ms
step:1874/2330 train_time:72754ms step_avg:38.82ms
step:1875/2330 train_time:72790ms step_avg:38.82ms
step:1876/2330 train_time:72831ms step_avg:38.82ms
step:1877/2330 train_time:72868ms step_avg:38.82ms
step:1878/2330 train_time:72909ms step_avg:38.82ms
step:1879/2330 train_time:72945ms step_avg:38.82ms
step:1880/2330 train_time:72987ms step_avg:38.82ms
step:1881/2330 train_time:73022ms step_avg:38.82ms
step:1882/2330 train_time:73064ms step_avg:38.82ms
step:1883/2330 train_time:73099ms step_avg:38.82ms
step:1884/2330 train_time:73141ms step_avg:38.82ms
step:1885/2330 train_time:73177ms step_avg:38.82ms
step:1886/2330 train_time:73219ms step_avg:38.82ms
step:1887/2330 train_time:73255ms step_avg:38.82ms
step:1888/2330 train_time:73295ms step_avg:38.82ms
step:1889/2330 train_time:73334ms step_avg:38.82ms
step:1890/2330 train_time:73375ms step_avg:38.82ms
step:1891/2330 train_time:73413ms step_avg:38.82ms
step:1892/2330 train_time:73453ms step_avg:38.82ms
step:1893/2330 train_time:73491ms step_avg:38.82ms
step:1894/2330 train_time:73532ms step_avg:38.82ms
step:1895/2330 train_time:73569ms step_avg:38.82ms
step:1896/2330 train_time:73610ms step_avg:38.82ms
step:1897/2330 train_time:73647ms step_avg:38.82ms
step:1898/2330 train_time:73688ms step_avg:38.82ms
step:1899/2330 train_time:73724ms step_avg:38.82ms
step:1900/2330 train_time:73765ms step_avg:38.82ms
step:1901/2330 train_time:73802ms step_avg:38.82ms
step:1902/2330 train_time:73845ms step_avg:38.82ms
step:1903/2330 train_time:73880ms step_avg:38.82ms
step:1904/2330 train_time:73921ms step_avg:38.82ms
step:1905/2330 train_time:73958ms step_avg:38.82ms
step:1906/2330 train_time:73999ms step_avg:38.82ms
step:1907/2330 train_time:74035ms step_avg:38.82ms
step:1908/2330 train_time:74076ms step_avg:38.82ms
step:1909/2330 train_time:74113ms step_avg:38.82ms
step:1910/2330 train_time:74154ms step_avg:38.82ms
step:1911/2330 train_time:74191ms step_avg:38.82ms
step:1912/2330 train_time:74232ms step_avg:38.82ms
step:1913/2330 train_time:74269ms step_avg:38.82ms
step:1914/2330 train_time:74310ms step_avg:38.82ms
step:1915/2330 train_time:74346ms step_avg:38.82ms
step:1916/2330 train_time:74388ms step_avg:38.82ms
step:1917/2330 train_time:74424ms step_avg:38.82ms
step:1918/2330 train_time:74466ms step_avg:38.82ms
step:1919/2330 train_time:74502ms step_avg:38.82ms
step:1920/2330 train_time:74544ms step_avg:38.82ms
step:1921/2330 train_time:74581ms step_avg:38.82ms
step:1922/2330 train_time:74623ms step_avg:38.83ms
step:1923/2330 train_time:74659ms step_avg:38.82ms
step:1924/2330 train_time:74701ms step_avg:38.83ms
step:1925/2330 train_time:74737ms step_avg:38.82ms
step:1926/2330 train_time:74779ms step_avg:38.83ms
step:1927/2330 train_time:74815ms step_avg:38.82ms
step:1928/2330 train_time:74856ms step_avg:38.83ms
step:1929/2330 train_time:74894ms step_avg:38.83ms
step:1930/2330 train_time:74934ms step_avg:38.83ms
step:1931/2330 train_time:74972ms step_avg:38.83ms
step:1932/2330 train_time:75013ms step_avg:38.83ms
step:1933/2330 train_time:75049ms step_avg:38.83ms
step:1934/2330 train_time:75090ms step_avg:38.83ms
step:1935/2330 train_time:75126ms step_avg:38.82ms
step:1936/2330 train_time:75168ms step_avg:38.83ms
step:1937/2330 train_time:75203ms step_avg:38.82ms
step:1938/2330 train_time:75245ms step_avg:38.83ms
step:1939/2330 train_time:75280ms step_avg:38.82ms
step:1940/2330 train_time:75323ms step_avg:38.83ms
step:1941/2330 train_time:75358ms step_avg:38.82ms
step:1942/2330 train_time:75400ms step_avg:38.83ms
step:1943/2330 train_time:75436ms step_avg:38.82ms
step:1944/2330 train_time:75478ms step_avg:38.83ms
step:1945/2330 train_time:75514ms step_avg:38.82ms
step:1946/2330 train_time:75555ms step_avg:38.83ms
step:1947/2330 train_time:75593ms step_avg:38.83ms
step:1948/2330 train_time:75634ms step_avg:38.83ms
step:1949/2330 train_time:75671ms step_avg:38.83ms
step:1950/2330 train_time:75712ms step_avg:38.83ms
step:1951/2330 train_time:75749ms step_avg:38.83ms
step:1952/2330 train_time:75790ms step_avg:38.83ms
step:1953/2330 train_time:75826ms step_avg:38.83ms
step:1954/2330 train_time:75868ms step_avg:38.83ms
step:1955/2330 train_time:75903ms step_avg:38.83ms
step:1956/2330 train_time:75945ms step_avg:38.83ms
step:1957/2330 train_time:75981ms step_avg:38.83ms
step:1958/2330 train_time:76023ms step_avg:38.83ms
step:1959/2330 train_time:76060ms step_avg:38.83ms
step:1960/2330 train_time:76101ms step_avg:38.83ms
step:1961/2330 train_time:76138ms step_avg:38.83ms
step:1962/2330 train_time:76179ms step_avg:38.83ms
step:1963/2330 train_time:76216ms step_avg:38.83ms
step:1964/2330 train_time:76257ms step_avg:38.83ms
step:1965/2330 train_time:76295ms step_avg:38.83ms
step:1966/2330 train_time:76336ms step_avg:38.83ms
step:1967/2330 train_time:76373ms step_avg:38.83ms
step:1968/2330 train_time:76414ms step_avg:38.83ms
step:1969/2330 train_time:76451ms step_avg:38.83ms
step:1970/2330 train_time:76492ms step_avg:38.83ms
step:1971/2330 train_time:76529ms step_avg:38.83ms
step:1972/2330 train_time:76570ms step_avg:38.83ms
step:1973/2330 train_time:76607ms step_avg:38.83ms
step:1974/2330 train_time:76648ms step_avg:38.83ms
step:1975/2330 train_time:76685ms step_avg:38.83ms
step:1976/2330 train_time:76727ms step_avg:38.83ms
step:1977/2330 train_time:76762ms step_avg:38.83ms
step:1978/2330 train_time:76804ms step_avg:38.83ms
step:1979/2330 train_time:76840ms step_avg:38.83ms
step:1980/2330 train_time:76882ms step_avg:38.83ms
step:1981/2330 train_time:76918ms step_avg:38.83ms
step:1982/2330 train_time:76960ms step_avg:38.83ms
step:1983/2330 train_time:76996ms step_avg:38.83ms
step:1984/2330 train_time:77037ms step_avg:38.83ms
step:1985/2330 train_time:77075ms step_avg:38.83ms
step:1986/2330 train_time:77116ms step_avg:38.83ms
step:1987/2330 train_time:77153ms step_avg:38.83ms
step:1988/2330 train_time:77194ms step_avg:38.83ms
step:1989/2330 train_time:77231ms step_avg:38.83ms
step:1990/2330 train_time:77273ms step_avg:38.83ms
step:1991/2330 train_time:77309ms step_avg:38.83ms
step:1992/2330 train_time:77350ms step_avg:38.83ms
step:1993/2330 train_time:77386ms step_avg:38.83ms
step:1994/2330 train_time:77428ms step_avg:38.83ms
step:1995/2330 train_time:77464ms step_avg:38.83ms
step:1996/2330 train_time:77506ms step_avg:38.83ms
step:1997/2330 train_time:77542ms step_avg:38.83ms
step:1998/2330 train_time:77584ms step_avg:38.83ms
step:1999/2330 train_time:77620ms step_avg:38.83ms
step:2000/2330 train_time:77663ms step_avg:38.83ms
step:2000/2330 val_loss:5.1809 train_time:77777ms step_avg:38.89ms
step:2001/2330 train_time:77788ms step_avg:38.87ms
step:2002/2330 train_time:77800ms step_avg:38.86ms
step:2003/2330 train_time:77809ms step_avg:38.85ms
step:2004/2330 train_time:77820ms step_avg:38.83ms
step:2005/2330 train_time:77855ms step_avg:38.83ms
step:2006/2330 train_time:77896ms step_avg:38.83ms
step:2007/2330 train_time:77931ms step_avg:38.83ms
step:2008/2330 train_time:77972ms step_avg:38.83ms
step:2009/2330 train_time:78008ms step_avg:38.83ms
step:2010/2330 train_time:78049ms step_avg:38.83ms
step:2011/2330 train_time:78089ms step_avg:38.83ms
step:2012/2330 train_time:78132ms step_avg:38.83ms
step:2013/2330 train_time:78175ms step_avg:38.84ms
step:2014/2330 train_time:78217ms step_avg:38.84ms
step:2015/2330 train_time:78253ms step_avg:38.83ms
step:2016/2330 train_time:78295ms step_avg:38.84ms
step:2017/2330 train_time:78330ms step_avg:38.83ms
step:2018/2330 train_time:78371ms step_avg:38.84ms
step:2019/2330 train_time:78407ms step_avg:38.83ms
step:2020/2330 train_time:78448ms step_avg:38.84ms
step:2021/2330 train_time:78485ms step_avg:38.83ms
step:2022/2330 train_time:78525ms step_avg:38.84ms
step:2023/2330 train_time:78561ms step_avg:38.83ms
step:2024/2330 train_time:78602ms step_avg:38.84ms
step:2025/2330 train_time:78638ms step_avg:38.83ms
step:2026/2330 train_time:78679ms step_avg:38.83ms
step:2027/2330 train_time:78715ms step_avg:38.83ms
step:2028/2330 train_time:78756ms step_avg:38.83ms
step:2029/2330 train_time:78792ms step_avg:38.83ms
step:2030/2330 train_time:78834ms step_avg:38.83ms
step:2031/2330 train_time:78869ms step_avg:38.83ms
step:2032/2330 train_time:78911ms step_avg:38.83ms
step:2033/2330 train_time:78946ms step_avg:38.83ms
step:2034/2330 train_time:78988ms step_avg:38.83ms
step:2035/2330 train_time:79025ms step_avg:38.83ms
step:2036/2330 train_time:79066ms step_avg:38.83ms
step:2037/2330 train_time:79106ms step_avg:38.83ms
step:2038/2330 train_time:79148ms step_avg:38.84ms
step:2039/2330 train_time:79187ms step_avg:38.84ms
step:2040/2330 train_time:79227ms step_avg:38.84ms
step:2041/2330 train_time:79265ms step_avg:38.84ms
step:2042/2330 train_time:79306ms step_avg:38.84ms
step:2043/2330 train_time:79343ms step_avg:38.84ms
step:2044/2330 train_time:79384ms step_avg:38.84ms
step:2045/2330 train_time:79421ms step_avg:38.84ms
step:2046/2330 train_time:79462ms step_avg:38.84ms
step:2047/2330 train_time:79497ms step_avg:38.84ms
step:2048/2330 train_time:79538ms step_avg:38.84ms
step:2049/2330 train_time:79574ms step_avg:38.84ms
step:2050/2330 train_time:79615ms step_avg:38.84ms
step:2051/2330 train_time:79651ms step_avg:38.84ms
step:2052/2330 train_time:79692ms step_avg:38.84ms
step:2053/2330 train_time:79729ms step_avg:38.84ms
step:2054/2330 train_time:79770ms step_avg:38.84ms
step:2055/2330 train_time:79806ms step_avg:38.84ms
step:2056/2330 train_time:79847ms step_avg:38.84ms
step:2057/2330 train_time:79884ms step_avg:38.84ms
step:2058/2330 train_time:79925ms step_avg:38.84ms
step:2059/2330 train_time:79962ms step_avg:38.84ms
step:2060/2330 train_time:80003ms step_avg:38.84ms
step:2061/2330 train_time:80041ms step_avg:38.84ms
step:2062/2330 train_time:80082ms step_avg:38.84ms
step:2063/2330 train_time:80120ms step_avg:38.84ms
step:2064/2330 train_time:80162ms step_avg:38.84ms
step:2065/2330 train_time:80198ms step_avg:38.84ms
step:2066/2330 train_time:80240ms step_avg:38.84ms
step:2067/2330 train_time:80276ms step_avg:38.84ms
step:2068/2330 train_time:80318ms step_avg:38.84ms
step:2069/2330 train_time:80354ms step_avg:38.84ms
step:2070/2330 train_time:80395ms step_avg:38.84ms
step:2071/2330 train_time:80431ms step_avg:38.84ms
step:2072/2330 train_time:80473ms step_avg:38.84ms
step:2073/2330 train_time:80508ms step_avg:38.84ms
step:2074/2330 train_time:80550ms step_avg:38.84ms
step:2075/2330 train_time:80585ms step_avg:38.84ms
step:2076/2330 train_time:80626ms step_avg:38.84ms
step:2077/2330 train_time:80662ms step_avg:38.84ms
step:2078/2330 train_time:80703ms step_avg:38.84ms
step:2079/2330 train_time:80739ms step_avg:38.84ms
step:2080/2330 train_time:80780ms step_avg:38.84ms
step:2081/2330 train_time:80815ms step_avg:38.83ms
step:2082/2330 train_time:80857ms step_avg:38.84ms
step:2083/2330 train_time:80893ms step_avg:38.83ms
step:2084/2330 train_time:80934ms step_avg:38.84ms
step:2085/2330 train_time:80970ms step_avg:38.83ms
step:2086/2330 train_time:81013ms step_avg:38.84ms
step:2087/2330 train_time:81049ms step_avg:38.84ms
step:2088/2330 train_time:81091ms step_avg:38.84ms
step:2089/2330 train_time:81128ms step_avg:38.84ms
step:2090/2330 train_time:81170ms step_avg:38.84ms
step:2091/2330 train_time:81206ms step_avg:38.84ms
step:2092/2330 train_time:81248ms step_avg:38.84ms
step:2093/2330 train_time:81285ms step_avg:38.84ms
step:2094/2330 train_time:81326ms step_avg:38.84ms
step:2095/2330 train_time:81364ms step_avg:38.84ms
step:2096/2330 train_time:81405ms step_avg:38.84ms
step:2097/2330 train_time:81441ms step_avg:38.84ms
step:2098/2330 train_time:81482ms step_avg:38.84ms
step:2099/2330 train_time:81518ms step_avg:38.84ms
step:2100/2330 train_time:81559ms step_avg:38.84ms
step:2101/2330 train_time:81595ms step_avg:38.84ms
step:2102/2330 train_time:81636ms step_avg:38.84ms
step:2103/2330 train_time:81672ms step_avg:38.84ms
step:2104/2330 train_time:81714ms step_avg:38.84ms
step:2105/2330 train_time:81750ms step_avg:38.84ms
step:2106/2330 train_time:81791ms step_avg:38.84ms
step:2107/2330 train_time:81828ms step_avg:38.84ms
step:2108/2330 train_time:81869ms step_avg:38.84ms
step:2109/2330 train_time:81905ms step_avg:38.84ms
step:2110/2330 train_time:81946ms step_avg:38.84ms
step:2111/2330 train_time:81984ms step_avg:38.84ms
step:2112/2330 train_time:82024ms step_avg:38.84ms
step:2113/2330 train_time:82063ms step_avg:38.84ms
step:2114/2330 train_time:82104ms step_avg:38.84ms
step:2115/2330 train_time:82141ms step_avg:38.84ms
step:2116/2330 train_time:82182ms step_avg:38.84ms
step:2117/2330 train_time:82219ms step_avg:38.84ms
step:2118/2330 train_time:82261ms step_avg:38.84ms
step:2119/2330 train_time:82297ms step_avg:38.84ms
step:2120/2330 train_time:82339ms step_avg:38.84ms
step:2121/2330 train_time:82374ms step_avg:38.84ms
step:2122/2330 train_time:82415ms step_avg:38.84ms
step:2123/2330 train_time:82452ms step_avg:38.84ms
step:2124/2330 train_time:82494ms step_avg:38.84ms
step:2125/2330 train_time:82529ms step_avg:38.84ms
step:2126/2330 train_time:82571ms step_avg:38.84ms
step:2127/2330 train_time:82607ms step_avg:38.84ms
step:2128/2330 train_time:82648ms step_avg:38.84ms
step:2129/2330 train_time:82685ms step_avg:38.84ms
step:2130/2330 train_time:82725ms step_avg:38.84ms
step:2131/2330 train_time:82763ms step_avg:38.84ms
step:2132/2330 train_time:82803ms step_avg:38.84ms
step:2133/2330 train_time:82840ms step_avg:38.84ms
step:2134/2330 train_time:82882ms step_avg:38.84ms
step:2135/2330 train_time:82919ms step_avg:38.84ms
step:2136/2330 train_time:82960ms step_avg:38.84ms
step:2137/2330 train_time:82996ms step_avg:38.84ms
step:2138/2330 train_time:83038ms step_avg:38.84ms
step:2139/2330 train_time:83074ms step_avg:38.84ms
step:2140/2330 train_time:83115ms step_avg:38.84ms
step:2141/2330 train_time:83152ms step_avg:38.84ms
step:2142/2330 train_time:83194ms step_avg:38.84ms
step:2143/2330 train_time:83230ms step_avg:38.84ms
step:2144/2330 train_time:83272ms step_avg:38.84ms
step:2145/2330 train_time:83308ms step_avg:38.84ms
step:2146/2330 train_time:83350ms step_avg:38.84ms
step:2147/2330 train_time:83387ms step_avg:38.84ms
step:2148/2330 train_time:83429ms step_avg:38.84ms
step:2149/2330 train_time:83465ms step_avg:38.84ms
step:2150/2330 train_time:83506ms step_avg:38.84ms
step:2151/2330 train_time:83543ms step_avg:38.84ms
step:2152/2330 train_time:83584ms step_avg:38.84ms
step:2153/2330 train_time:83621ms step_avg:38.84ms
step:2154/2330 train_time:83662ms step_avg:38.84ms
step:2155/2330 train_time:83699ms step_avg:38.84ms
step:2156/2330 train_time:83740ms step_avg:38.84ms
step:2157/2330 train_time:83776ms step_avg:38.84ms
step:2158/2330 train_time:83817ms step_avg:38.84ms
step:2159/2330 train_time:83853ms step_avg:38.84ms
step:2160/2330 train_time:83895ms step_avg:38.84ms
step:2161/2330 train_time:83931ms step_avg:38.84ms
step:2162/2330 train_time:83973ms step_avg:38.84ms
step:2163/2330 train_time:84009ms step_avg:38.84ms
step:2164/2330 train_time:84051ms step_avg:38.84ms
step:2165/2330 train_time:84088ms step_avg:38.84ms
step:2166/2330 train_time:84130ms step_avg:38.84ms
step:2167/2330 train_time:84167ms step_avg:38.84ms
step:2168/2330 train_time:84208ms step_avg:38.84ms
step:2169/2330 train_time:84245ms step_avg:38.84ms
step:2170/2330 train_time:84286ms step_avg:38.84ms
step:2171/2330 train_time:84324ms step_avg:38.84ms
step:2172/2330 train_time:84365ms step_avg:38.84ms
step:2173/2330 train_time:84402ms step_avg:38.84ms
step:2174/2330 train_time:84443ms step_avg:38.84ms
step:2175/2330 train_time:84479ms step_avg:38.84ms
step:2176/2330 train_time:84520ms step_avg:38.84ms
step:2177/2330 train_time:84555ms step_avg:38.84ms
step:2178/2330 train_time:84597ms step_avg:38.84ms
step:2179/2330 train_time:84632ms step_avg:38.84ms
step:2180/2330 train_time:84674ms step_avg:38.84ms
step:2181/2330 train_time:84710ms step_avg:38.84ms
step:2182/2330 train_time:84751ms step_avg:38.84ms
step:2183/2330 train_time:84788ms step_avg:38.84ms
step:2184/2330 train_time:84830ms step_avg:38.84ms
step:2185/2330 train_time:84866ms step_avg:38.84ms
step:2186/2330 train_time:84907ms step_avg:38.84ms
step:2187/2330 train_time:84945ms step_avg:38.84ms
step:2188/2330 train_time:84986ms step_avg:38.84ms
step:2189/2330 train_time:85023ms step_avg:38.84ms
step:2190/2330 train_time:85064ms step_avg:38.84ms
step:2191/2330 train_time:85101ms step_avg:38.84ms
step:2192/2330 train_time:85143ms step_avg:38.84ms
step:2193/2330 train_time:85180ms step_avg:38.84ms
step:2194/2330 train_time:85220ms step_avg:38.84ms
step:2195/2330 train_time:85258ms step_avg:38.84ms
step:2196/2330 train_time:85299ms step_avg:38.84ms
step:2197/2330 train_time:85335ms step_avg:38.84ms
step:2198/2330 train_time:85377ms step_avg:38.84ms
step:2199/2330 train_time:85413ms step_avg:38.84ms
step:2200/2330 train_time:85454ms step_avg:38.84ms
step:2201/2330 train_time:85491ms step_avg:38.84ms
step:2202/2330 train_time:85532ms step_avg:38.84ms
step:2203/2330 train_time:85569ms step_avg:38.84ms
step:2204/2330 train_time:85610ms step_avg:38.84ms
step:2205/2330 train_time:85647ms step_avg:38.84ms
step:2206/2330 train_time:85689ms step_avg:38.84ms
step:2207/2330 train_time:85727ms step_avg:38.84ms
step:2208/2330 train_time:85767ms step_avg:38.84ms
step:2209/2330 train_time:85804ms step_avg:38.84ms
step:2210/2330 train_time:85845ms step_avg:38.84ms
step:2211/2330 train_time:85883ms step_avg:38.84ms
step:2212/2330 train_time:85924ms step_avg:38.84ms
step:2213/2330 train_time:85961ms step_avg:38.84ms
step:2214/2330 train_time:86002ms step_avg:38.84ms
step:2215/2330 train_time:86038ms step_avg:38.84ms
step:2216/2330 train_time:86080ms step_avg:38.84ms
step:2217/2330 train_time:86116ms step_avg:38.84ms
step:2218/2330 train_time:86157ms step_avg:38.84ms
step:2219/2330 train_time:86194ms step_avg:38.84ms
step:2220/2330 train_time:86236ms step_avg:38.84ms
step:2221/2330 train_time:86272ms step_avg:38.84ms
step:2222/2330 train_time:86314ms step_avg:38.85ms
step:2223/2330 train_time:86350ms step_avg:38.84ms
step:2224/2330 train_time:86392ms step_avg:38.85ms
step:2225/2330 train_time:86428ms step_avg:38.84ms
step:2226/2330 train_time:86470ms step_avg:38.85ms
step:2227/2330 train_time:86506ms step_avg:38.84ms
step:2228/2330 train_time:86547ms step_avg:38.84ms
step:2229/2330 train_time:86584ms step_avg:38.84ms
step:2230/2330 train_time:86625ms step_avg:38.85ms
step:2231/2330 train_time:86662ms step_avg:38.84ms
step:2232/2330 train_time:86703ms step_avg:38.85ms
step:2233/2330 train_time:86741ms step_avg:38.84ms
step:2234/2330 train_time:86782ms step_avg:38.85ms
step:2235/2330 train_time:86819ms step_avg:38.85ms
step:2236/2330 train_time:86860ms step_avg:38.85ms
step:2237/2330 train_time:86896ms step_avg:38.84ms
step:2238/2330 train_time:86937ms step_avg:38.85ms
step:2239/2330 train_time:86973ms step_avg:38.84ms
step:2240/2330 train_time:87015ms step_avg:38.85ms
step:2241/2330 train_time:87051ms step_avg:38.84ms
step:2242/2330 train_time:87092ms step_avg:38.85ms
step:2243/2330 train_time:87128ms step_avg:38.84ms
step:2244/2330 train_time:87170ms step_avg:38.85ms
step:2245/2330 train_time:87206ms step_avg:38.84ms
step:2246/2330 train_time:87247ms step_avg:38.85ms
step:2247/2330 train_time:87284ms step_avg:38.84ms
step:2248/2330 train_time:87325ms step_avg:38.85ms
step:2249/2330 train_time:87363ms step_avg:38.85ms
step:2250/2330 train_time:87403ms step_avg:38.85ms
step:2250/2330 val_loss:5.1536 train_time:87517ms step_avg:38.90ms
step:2251/2330 train_time:87529ms step_avg:38.88ms
step:2252/2330 train_time:87540ms step_avg:38.87ms
step:2253/2330 train_time:87550ms step_avg:38.86ms
step:2254/2330 train_time:87560ms step_avg:38.85ms
step:2255/2330 train_time:87595ms step_avg:38.84ms
step:2256/2330 train_time:87635ms step_avg:38.85ms
step:2257/2330 train_time:87670ms step_avg:38.84ms
step:2258/2330 train_time:87711ms step_avg:38.84ms
step:2259/2330 train_time:87746ms step_avg:38.84ms
step:2260/2330 train_time:87787ms step_avg:38.84ms
step:2261/2330 train_time:87825ms step_avg:38.84ms
step:2262/2330 train_time:87869ms step_avg:38.85ms
step:2263/2330 train_time:87908ms step_avg:38.85ms
step:2264/2330 train_time:87950ms step_avg:38.85ms
step:2265/2330 train_time:87986ms step_avg:38.85ms
step:2266/2330 train_time:88027ms step_avg:38.85ms
step:2267/2330 train_time:88064ms step_avg:38.85ms
step:2268/2330 train_time:88106ms step_avg:38.85ms
step:2269/2330 train_time:88141ms step_avg:38.85ms
step:2270/2330 train_time:88183ms step_avg:38.85ms
step:2271/2330 train_time:88217ms step_avg:38.85ms
step:2272/2330 train_time:88259ms step_avg:38.85ms
step:2273/2330 train_time:88294ms step_avg:38.84ms
step:2274/2330 train_time:88335ms step_avg:38.85ms
step:2275/2330 train_time:88370ms step_avg:38.84ms
step:2276/2330 train_time:88411ms step_avg:38.85ms
step:2277/2330 train_time:88449ms step_avg:38.84ms
step:2278/2330 train_time:88490ms step_avg:38.85ms
step:2279/2330 train_time:88526ms step_avg:38.84ms
step:2280/2330 train_time:88567ms step_avg:38.85ms
step:2281/2330 train_time:88603ms step_avg:38.84ms
step:2282/2330 train_time:88644ms step_avg:38.85ms
step:2283/2330 train_time:88680ms step_avg:38.84ms
step:2284/2330 train_time:88721ms step_avg:38.84ms
step:2285/2330 train_time:88757ms step_avg:38.84ms
step:2286/2330 train_time:88799ms step_avg:38.84ms
step:2287/2330 train_time:88836ms step_avg:38.84ms
step:2288/2330 train_time:88878ms step_avg:38.85ms
step:2289/2330 train_time:88917ms step_avg:38.85ms
step:2290/2330 train_time:88959ms step_avg:38.85ms
step:2291/2330 train_time:88997ms step_avg:38.85ms
step:2292/2330 train_time:89038ms step_avg:38.85ms
step:2293/2330 train_time:89075ms step_avg:38.85ms
step:2294/2330 train_time:89115ms step_avg:38.85ms
step:2295/2330 train_time:89152ms step_avg:38.85ms
step:2296/2330 train_time:89193ms step_avg:38.85ms
step:2297/2330 train_time:89229ms step_avg:38.85ms
step:2298/2330 train_time:89270ms step_avg:38.85ms
step:2299/2330 train_time:89305ms step_avg:38.85ms
step:2300/2330 train_time:89347ms step_avg:38.85ms
step:2301/2330 train_time:89382ms step_avg:38.84ms
step:2302/2330 train_time:89424ms step_avg:38.85ms
step:2303/2330 train_time:89459ms step_avg:38.84ms
step:2304/2330 train_time:89501ms step_avg:38.85ms
step:2305/2330 train_time:89536ms step_avg:38.84ms
step:2306/2330 train_time:89577ms step_avg:38.85ms
step:2307/2330 train_time:89614ms step_avg:38.84ms
step:2308/2330 train_time:89655ms step_avg:38.85ms
step:2309/2330 train_time:89691ms step_avg:38.84ms
step:2310/2330 train_time:89733ms step_avg:38.85ms
step:2311/2330 train_time:89770ms step_avg:38.84ms
step:2312/2330 train_time:89812ms step_avg:38.85ms
step:2313/2330 train_time:89848ms step_avg:38.84ms
step:2314/2330 train_time:89889ms step_avg:38.85ms
step:2315/2330 train_time:89926ms step_avg:38.84ms
step:2316/2330 train_time:89967ms step_avg:38.85ms
step:2317/2330 train_time:90005ms step_avg:38.85ms
step:2318/2330 train_time:90046ms step_avg:38.85ms
step:2319/2330 train_time:90083ms step_avg:38.85ms
step:2320/2330 train_time:90124ms step_avg:38.85ms
step:2321/2330 train_time:90160ms step_avg:38.85ms
step:2322/2330 train_time:90202ms step_avg:38.85ms
step:2323/2330 train_time:90237ms step_avg:38.85ms
step:2324/2330 train_time:90279ms step_avg:38.85ms
step:2325/2330 train_time:90315ms step_avg:38.85ms
step:2326/2330 train_time:90356ms step_avg:38.85ms
step:2327/2330 train_time:90392ms step_avg:38.84ms
step:2328/2330 train_time:90433ms step_avg:38.85ms
step:2329/2330 train_time:90469ms step_avg:38.84ms
step:2330/2330 train_time:90511ms step_avg:38.85ms
step:2330/2330 val_loss:5.1464 train_time:90623ms step_avg:38.89ms
peak memory allocated: 27165 MiB reserved: 38888 MiB
