import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = divide_by_norm(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = divide_by_norm(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_gd_lr5e-2"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=5e-2, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 05:29:25 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   31C    P0             111W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   29C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   30C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   28C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   31C    P0             115W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:73ms step_avg:72.87ms
step:2/2330 train_time:169ms step_avg:84.69ms
step:3/2330 train_time:181ms step_avg:60.30ms
step:4/2330 train_time:192ms step_avg:48.02ms
step:5/2330 train_time:202ms step_avg:40.37ms
step:6/2330 train_time:230ms step_avg:38.28ms
step:7/2330 train_time:251ms step_avg:35.84ms
step:8/2330 train_time:306ms step_avg:38.20ms
step:9/2330 train_time:328ms step_avg:36.46ms
step:10/2330 train_time:383ms step_avg:38.31ms
step:11/2330 train_time:406ms step_avg:36.87ms
step:12/2330 train_time:462ms step_avg:38.47ms
step:13/2330 train_time:484ms step_avg:37.23ms
step:14/2330 train_time:539ms step_avg:38.53ms
step:15/2330 train_time:561ms step_avg:37.41ms
step:16/2330 train_time:617ms step_avg:38.57ms
step:17/2330 train_time:639ms step_avg:37.57ms
step:18/2330 train_time:694ms step_avg:38.55ms
step:19/2330 train_time:716ms step_avg:37.66ms
step:20/2330 train_time:771ms step_avg:38.54ms
step:21/2330 train_time:793ms step_avg:37.75ms
step:22/2330 train_time:848ms step_avg:38.56ms
step:23/2330 train_time:871ms step_avg:37.85ms
step:24/2330 train_time:926ms step_avg:38.57ms
step:25/2330 train_time:948ms step_avg:37.91ms
step:26/2330 train_time:1006ms step_avg:38.69ms
step:27/2330 train_time:1032ms step_avg:38.24ms
step:28/2330 train_time:1093ms step_avg:39.04ms
step:29/2330 train_time:1120ms step_avg:38.61ms
step:30/2330 train_time:1178ms step_avg:39.27ms
step:31/2330 train_time:1201ms step_avg:38.76ms
step:32/2330 train_time:1258ms step_avg:39.32ms
step:33/2330 train_time:1281ms step_avg:38.80ms
step:34/2330 train_time:1337ms step_avg:39.32ms
step:35/2330 train_time:1359ms step_avg:38.84ms
step:36/2330 train_time:1416ms step_avg:39.32ms
step:37/2330 train_time:1437ms step_avg:38.84ms
step:38/2330 train_time:1493ms step_avg:39.29ms
step:39/2330 train_time:1515ms step_avg:38.85ms
step:40/2330 train_time:1571ms step_avg:39.27ms
step:41/2330 train_time:1593ms step_avg:38.85ms
step:42/2330 train_time:1648ms step_avg:39.25ms
step:43/2330 train_time:1670ms step_avg:38.84ms
step:44/2330 train_time:1726ms step_avg:39.23ms
step:45/2330 train_time:1749ms step_avg:38.86ms
step:46/2330 train_time:1805ms step_avg:39.23ms
step:47/2330 train_time:1826ms step_avg:38.86ms
step:48/2330 train_time:1882ms step_avg:39.21ms
step:49/2330 train_time:1905ms step_avg:38.89ms
step:50/2330 train_time:1963ms step_avg:39.26ms
step:51/2330 train_time:1987ms step_avg:38.97ms
step:52/2330 train_time:2045ms step_avg:39.33ms
step:53/2330 train_time:2069ms step_avg:39.04ms
step:54/2330 train_time:2127ms step_avg:39.39ms
step:55/2330 train_time:2152ms step_avg:39.13ms
step:56/2330 train_time:2209ms step_avg:39.44ms
step:57/2330 train_time:2232ms step_avg:39.16ms
step:58/2330 train_time:2288ms step_avg:39.46ms
step:59/2330 train_time:2312ms step_avg:39.18ms
step:60/2330 train_time:2368ms step_avg:39.46ms
step:61/2330 train_time:2391ms step_avg:39.19ms
step:62/2330 train_time:2447ms step_avg:39.47ms
step:63/2330 train_time:2470ms step_avg:39.20ms
step:64/2330 train_time:2526ms step_avg:39.46ms
step:65/2330 train_time:2548ms step_avg:39.21ms
step:66/2330 train_time:2604ms step_avg:39.46ms
step:67/2330 train_time:2627ms step_avg:39.21ms
step:68/2330 train_time:2683ms step_avg:39.46ms
step:69/2330 train_time:2706ms step_avg:39.21ms
step:70/2330 train_time:2762ms step_avg:39.45ms
step:71/2330 train_time:2784ms step_avg:39.21ms
step:72/2330 train_time:2840ms step_avg:39.44ms
step:73/2330 train_time:2862ms step_avg:39.20ms
step:74/2330 train_time:2918ms step_avg:39.43ms
step:75/2330 train_time:2940ms step_avg:39.20ms
step:76/2330 train_time:2998ms step_avg:39.45ms
step:77/2330 train_time:3021ms step_avg:39.23ms
step:78/2330 train_time:3079ms step_avg:39.48ms
step:79/2330 train_time:3102ms step_avg:39.26ms
step:80/2330 train_time:3160ms step_avg:39.50ms
step:81/2330 train_time:3182ms step_avg:39.29ms
step:82/2330 train_time:3240ms step_avg:39.52ms
step:83/2330 train_time:3263ms step_avg:39.31ms
step:84/2330 train_time:3319ms step_avg:39.51ms
step:85/2330 train_time:3341ms step_avg:39.31ms
step:86/2330 train_time:3398ms step_avg:39.52ms
step:87/2330 train_time:3421ms step_avg:39.32ms
step:88/2330 train_time:3477ms step_avg:39.52ms
step:89/2330 train_time:3499ms step_avg:39.32ms
step:90/2330 train_time:3556ms step_avg:39.51ms
step:91/2330 train_time:3578ms step_avg:39.32ms
step:92/2330 train_time:3634ms step_avg:39.50ms
step:93/2330 train_time:3656ms step_avg:39.31ms
step:94/2330 train_time:3712ms step_avg:39.49ms
step:95/2330 train_time:3735ms step_avg:39.31ms
step:96/2330 train_time:3791ms step_avg:39.49ms
step:97/2330 train_time:3814ms step_avg:39.32ms
step:98/2330 train_time:3870ms step_avg:39.48ms
step:99/2330 train_time:3893ms step_avg:39.32ms
step:100/2330 train_time:3949ms step_avg:39.49ms
step:101/2330 train_time:3972ms step_avg:39.32ms
step:102/2330 train_time:4029ms step_avg:39.50ms
step:103/2330 train_time:4053ms step_avg:39.35ms
step:104/2330 train_time:4110ms step_avg:39.52ms
step:105/2330 train_time:4134ms step_avg:39.38ms
step:106/2330 train_time:4191ms step_avg:39.54ms
step:107/2330 train_time:4214ms step_avg:39.38ms
step:108/2330 train_time:4270ms step_avg:39.54ms
step:109/2330 train_time:4294ms step_avg:39.39ms
step:110/2330 train_time:4350ms step_avg:39.55ms
step:111/2330 train_time:4373ms step_avg:39.40ms
step:112/2330 train_time:4429ms step_avg:39.55ms
step:113/2330 train_time:4452ms step_avg:39.40ms
step:114/2330 train_time:4508ms step_avg:39.54ms
step:115/2330 train_time:4531ms step_avg:39.40ms
step:116/2330 train_time:4587ms step_avg:39.54ms
step:117/2330 train_time:4609ms step_avg:39.40ms
step:118/2330 train_time:4666ms step_avg:39.54ms
step:119/2330 train_time:4688ms step_avg:39.40ms
step:120/2330 train_time:4745ms step_avg:39.54ms
step:121/2330 train_time:4767ms step_avg:39.40ms
step:122/2330 train_time:4824ms step_avg:39.54ms
step:123/2330 train_time:4847ms step_avg:39.41ms
step:124/2330 train_time:4903ms step_avg:39.54ms
step:125/2330 train_time:4925ms step_avg:39.40ms
step:126/2330 train_time:4982ms step_avg:39.54ms
step:127/2330 train_time:5006ms step_avg:39.41ms
step:128/2330 train_time:5063ms step_avg:39.56ms
step:129/2330 train_time:5087ms step_avg:39.43ms
step:130/2330 train_time:5144ms step_avg:39.57ms
step:131/2330 train_time:5168ms step_avg:39.45ms
step:132/2330 train_time:5225ms step_avg:39.58ms
step:133/2330 train_time:5249ms step_avg:39.47ms
step:134/2330 train_time:5306ms step_avg:39.59ms
step:135/2330 train_time:5329ms step_avg:39.47ms
step:136/2330 train_time:5385ms step_avg:39.60ms
step:137/2330 train_time:5408ms step_avg:39.47ms
step:138/2330 train_time:5464ms step_avg:39.60ms
step:139/2330 train_time:5487ms step_avg:39.48ms
step:140/2330 train_time:5543ms step_avg:39.60ms
step:141/2330 train_time:5565ms step_avg:39.47ms
step:142/2330 train_time:5621ms step_avg:39.59ms
step:143/2330 train_time:5645ms step_avg:39.47ms
step:144/2330 train_time:5701ms step_avg:39.59ms
step:145/2330 train_time:5723ms step_avg:39.47ms
step:146/2330 train_time:5779ms step_avg:39.58ms
step:147/2330 train_time:5801ms step_avg:39.46ms
step:148/2330 train_time:5857ms step_avg:39.58ms
step:149/2330 train_time:5879ms step_avg:39.46ms
step:150/2330 train_time:5936ms step_avg:39.57ms
step:151/2330 train_time:5958ms step_avg:39.46ms
step:152/2330 train_time:6016ms step_avg:39.58ms
step:153/2330 train_time:6038ms step_avg:39.47ms
step:154/2330 train_time:6095ms step_avg:39.58ms
step:155/2330 train_time:6118ms step_avg:39.47ms
step:156/2330 train_time:6176ms step_avg:39.59ms
step:157/2330 train_time:6198ms step_avg:39.48ms
step:158/2330 train_time:6255ms step_avg:39.59ms
step:159/2330 train_time:6278ms step_avg:39.49ms
step:160/2330 train_time:6335ms step_avg:39.60ms
step:161/2330 train_time:6357ms step_avg:39.49ms
step:162/2330 train_time:6414ms step_avg:39.59ms
step:163/2330 train_time:6437ms step_avg:39.49ms
step:164/2330 train_time:6493ms step_avg:39.59ms
step:165/2330 train_time:6515ms step_avg:39.49ms
step:166/2330 train_time:6571ms step_avg:39.59ms
step:167/2330 train_time:6594ms step_avg:39.48ms
step:168/2330 train_time:6650ms step_avg:39.58ms
step:169/2330 train_time:6673ms step_avg:39.49ms
step:170/2330 train_time:6729ms step_avg:39.58ms
step:171/2330 train_time:6752ms step_avg:39.48ms
step:172/2330 train_time:6808ms step_avg:39.58ms
step:173/2330 train_time:6831ms step_avg:39.49ms
step:174/2330 train_time:6887ms step_avg:39.58ms
step:175/2330 train_time:6910ms step_avg:39.49ms
step:176/2330 train_time:6967ms step_avg:39.58ms
step:177/2330 train_time:6991ms step_avg:39.49ms
step:178/2330 train_time:7048ms step_avg:39.59ms
step:179/2330 train_time:7071ms step_avg:39.50ms
step:180/2330 train_time:7128ms step_avg:39.60ms
step:181/2330 train_time:7151ms step_avg:39.51ms
step:182/2330 train_time:7208ms step_avg:39.60ms
step:183/2330 train_time:7231ms step_avg:39.51ms
step:184/2330 train_time:7287ms step_avg:39.60ms
step:185/2330 train_time:7310ms step_avg:39.52ms
step:186/2330 train_time:7367ms step_avg:39.61ms
step:187/2330 train_time:7391ms step_avg:39.52ms
step:188/2330 train_time:7447ms step_avg:39.61ms
step:189/2330 train_time:7470ms step_avg:39.53ms
step:190/2330 train_time:7527ms step_avg:39.61ms
step:191/2330 train_time:7549ms step_avg:39.53ms
step:192/2330 train_time:7606ms step_avg:39.61ms
step:193/2330 train_time:7629ms step_avg:39.53ms
step:194/2330 train_time:7686ms step_avg:39.62ms
step:195/2330 train_time:7709ms step_avg:39.54ms
step:196/2330 train_time:7766ms step_avg:39.62ms
step:197/2330 train_time:7789ms step_avg:39.54ms
step:198/2330 train_time:7845ms step_avg:39.62ms
step:199/2330 train_time:7868ms step_avg:39.54ms
step:200/2330 train_time:7925ms step_avg:39.63ms
step:201/2330 train_time:7949ms step_avg:39.55ms
step:202/2330 train_time:8006ms step_avg:39.63ms
step:203/2330 train_time:8029ms step_avg:39.55ms
step:204/2330 train_time:8086ms step_avg:39.64ms
step:205/2330 train_time:8110ms step_avg:39.56ms
step:206/2330 train_time:8167ms step_avg:39.65ms
step:207/2330 train_time:8190ms step_avg:39.56ms
step:208/2330 train_time:8246ms step_avg:39.64ms
step:209/2330 train_time:8269ms step_avg:39.56ms
step:210/2330 train_time:8325ms step_avg:39.64ms
step:211/2330 train_time:8349ms step_avg:39.57ms
step:212/2330 train_time:8406ms step_avg:39.65ms
step:213/2330 train_time:8429ms step_avg:39.57ms
step:214/2330 train_time:8486ms step_avg:39.65ms
step:215/2330 train_time:8509ms step_avg:39.58ms
step:216/2330 train_time:8565ms step_avg:39.65ms
step:217/2330 train_time:8588ms step_avg:39.57ms
step:218/2330 train_time:8644ms step_avg:39.65ms
step:219/2330 train_time:8666ms step_avg:39.57ms
step:220/2330 train_time:8723ms step_avg:39.65ms
step:221/2330 train_time:8745ms step_avg:39.57ms
step:222/2330 train_time:8802ms step_avg:39.65ms
step:223/2330 train_time:8824ms step_avg:39.57ms
step:224/2330 train_time:8881ms step_avg:39.65ms
step:225/2330 train_time:8903ms step_avg:39.57ms
step:226/2330 train_time:8961ms step_avg:39.65ms
step:227/2330 train_time:8983ms step_avg:39.57ms
step:228/2330 train_time:9040ms step_avg:39.65ms
step:229/2330 train_time:9062ms step_avg:39.57ms
step:230/2330 train_time:9120ms step_avg:39.65ms
step:231/2330 train_time:9142ms step_avg:39.58ms
step:232/2330 train_time:9198ms step_avg:39.65ms
step:233/2330 train_time:9220ms step_avg:39.57ms
step:234/2330 train_time:9277ms step_avg:39.65ms
step:235/2330 train_time:9300ms step_avg:39.57ms
step:236/2330 train_time:9357ms step_avg:39.65ms
step:237/2330 train_time:9379ms step_avg:39.57ms
step:238/2330 train_time:9436ms step_avg:39.65ms
step:239/2330 train_time:9458ms step_avg:39.57ms
step:240/2330 train_time:9515ms step_avg:39.64ms
step:241/2330 train_time:9537ms step_avg:39.57ms
step:242/2330 train_time:9593ms step_avg:39.64ms
step:243/2330 train_time:9616ms step_avg:39.57ms
step:244/2330 train_time:9672ms step_avg:39.64ms
step:245/2330 train_time:9694ms step_avg:39.57ms
step:246/2330 train_time:9751ms step_avg:39.64ms
step:247/2330 train_time:9773ms step_avg:39.57ms
step:248/2330 train_time:9829ms step_avg:39.63ms
step:249/2330 train_time:9853ms step_avg:39.57ms
step:250/2330 train_time:9909ms step_avg:39.64ms
step:250/2330 val_loss:5.5399 train_time:10006ms step_avg:40.03ms
step:251/2330 train_time:10019ms step_avg:39.91ms
step:252/2330 train_time:10030ms step_avg:39.80ms
step:253/2330 train_time:10040ms step_avg:39.69ms
step:254/2330 train_time:10068ms step_avg:39.64ms
step:255/2330 train_time:10090ms step_avg:39.57ms
step:256/2330 train_time:10146ms step_avg:39.63ms
step:257/2330 train_time:10168ms step_avg:39.56ms
step:258/2330 train_time:10224ms step_avg:39.63ms
step:259/2330 train_time:10246ms step_avg:39.56ms
step:260/2330 train_time:10302ms step_avg:39.62ms
step:261/2330 train_time:10329ms step_avg:39.58ms
step:262/2330 train_time:10390ms step_avg:39.66ms
step:263/2330 train_time:10415ms step_avg:39.60ms
step:264/2330 train_time:10472ms step_avg:39.67ms
step:265/2330 train_time:10495ms step_avg:39.60ms
step:266/2330 train_time:10551ms step_avg:39.67ms
step:267/2330 train_time:10574ms step_avg:39.60ms
step:268/2330 train_time:10629ms step_avg:39.66ms
step:269/2330 train_time:10652ms step_avg:39.60ms
step:270/2330 train_time:10708ms step_avg:39.66ms
step:271/2330 train_time:10731ms step_avg:39.60ms
step:272/2330 train_time:10786ms step_avg:39.65ms
step:273/2330 train_time:10809ms step_avg:39.59ms
step:274/2330 train_time:10865ms step_avg:39.65ms
step:275/2330 train_time:10887ms step_avg:39.59ms
step:276/2330 train_time:10944ms step_avg:39.65ms
step:277/2330 train_time:10967ms step_avg:39.59ms
step:278/2330 train_time:11023ms step_avg:39.65ms
step:279/2330 train_time:11045ms step_avg:39.59ms
step:280/2330 train_time:11101ms step_avg:39.65ms
step:281/2330 train_time:11123ms step_avg:39.58ms
step:282/2330 train_time:11179ms step_avg:39.64ms
step:283/2330 train_time:11201ms step_avg:39.58ms
step:284/2330 train_time:11258ms step_avg:39.64ms
step:285/2330 train_time:11281ms step_avg:39.58ms
step:286/2330 train_time:11340ms step_avg:39.65ms
step:287/2330 train_time:11362ms step_avg:39.59ms
step:288/2330 train_time:11419ms step_avg:39.65ms
step:289/2330 train_time:11441ms step_avg:39.59ms
step:290/2330 train_time:11497ms step_avg:39.65ms
step:291/2330 train_time:11520ms step_avg:39.59ms
step:292/2330 train_time:11577ms step_avg:39.65ms
step:293/2330 train_time:11599ms step_avg:39.59ms
step:294/2330 train_time:11655ms step_avg:39.64ms
step:295/2330 train_time:11677ms step_avg:39.58ms
step:296/2330 train_time:11734ms step_avg:39.64ms
step:297/2330 train_time:11756ms step_avg:39.58ms
step:298/2330 train_time:11813ms step_avg:39.64ms
step:299/2330 train_time:11835ms step_avg:39.58ms
step:300/2330 train_time:11892ms step_avg:39.64ms
step:301/2330 train_time:11914ms step_avg:39.58ms
step:302/2330 train_time:11970ms step_avg:39.64ms
step:303/2330 train_time:11993ms step_avg:39.58ms
step:304/2330 train_time:12049ms step_avg:39.63ms
step:305/2330 train_time:12071ms step_avg:39.58ms
step:306/2330 train_time:12127ms step_avg:39.63ms
step:307/2330 train_time:12150ms step_avg:39.58ms
step:308/2330 train_time:12206ms step_avg:39.63ms
step:309/2330 train_time:12230ms step_avg:39.58ms
step:310/2330 train_time:12287ms step_avg:39.63ms
step:311/2330 train_time:12310ms step_avg:39.58ms
step:312/2330 train_time:12367ms step_avg:39.64ms
step:313/2330 train_time:12391ms step_avg:39.59ms
step:314/2330 train_time:12448ms step_avg:39.64ms
step:315/2330 train_time:12472ms step_avg:39.59ms
step:316/2330 train_time:12528ms step_avg:39.65ms
step:317/2330 train_time:12552ms step_avg:39.60ms
step:318/2330 train_time:12609ms step_avg:39.65ms
step:319/2330 train_time:12632ms step_avg:39.60ms
step:320/2330 train_time:12688ms step_avg:39.65ms
step:321/2330 train_time:12711ms step_avg:39.60ms
step:322/2330 train_time:12766ms step_avg:39.65ms
step:323/2330 train_time:12789ms step_avg:39.60ms
step:324/2330 train_time:12845ms step_avg:39.65ms
step:325/2330 train_time:12868ms step_avg:39.59ms
step:326/2330 train_time:12924ms step_avg:39.65ms
step:327/2330 train_time:12947ms step_avg:39.59ms
step:328/2330 train_time:13003ms step_avg:39.64ms
step:329/2330 train_time:13026ms step_avg:39.59ms
step:330/2330 train_time:13082ms step_avg:39.64ms
step:331/2330 train_time:13105ms step_avg:39.59ms
step:332/2330 train_time:13161ms step_avg:39.64ms
step:333/2330 train_time:13184ms step_avg:39.59ms
step:334/2330 train_time:13241ms step_avg:39.64ms
step:335/2330 train_time:13263ms step_avg:39.59ms
step:336/2330 train_time:13320ms step_avg:39.64ms
step:337/2330 train_time:13343ms step_avg:39.59ms
step:338/2330 train_time:13400ms step_avg:39.65ms
step:339/2330 train_time:13424ms step_avg:39.60ms
step:340/2330 train_time:13481ms step_avg:39.65ms
step:341/2330 train_time:13504ms step_avg:39.60ms
step:342/2330 train_time:13561ms step_avg:39.65ms
step:343/2330 train_time:13584ms step_avg:39.60ms
step:344/2330 train_time:13642ms step_avg:39.66ms
step:345/2330 train_time:13664ms step_avg:39.61ms
step:346/2330 train_time:13720ms step_avg:39.65ms
step:347/2330 train_time:13743ms step_avg:39.60ms
step:348/2330 train_time:13799ms step_avg:39.65ms
step:349/2330 train_time:13821ms step_avg:39.60ms
step:350/2330 train_time:13878ms step_avg:39.65ms
step:351/2330 train_time:13900ms step_avg:39.60ms
step:352/2330 train_time:13956ms step_avg:39.65ms
step:353/2330 train_time:13978ms step_avg:39.60ms
step:354/2330 train_time:14035ms step_avg:39.65ms
step:355/2330 train_time:14056ms step_avg:39.60ms
step:356/2330 train_time:14113ms step_avg:39.64ms
step:357/2330 train_time:14136ms step_avg:39.60ms
step:358/2330 train_time:14193ms step_avg:39.65ms
step:359/2330 train_time:14215ms step_avg:39.60ms
step:360/2330 train_time:14272ms step_avg:39.64ms
step:361/2330 train_time:14294ms step_avg:39.60ms
step:362/2330 train_time:14351ms step_avg:39.64ms
step:363/2330 train_time:14374ms step_avg:39.60ms
step:364/2330 train_time:14430ms step_avg:39.64ms
step:365/2330 train_time:14453ms step_avg:39.60ms
step:366/2330 train_time:14509ms step_avg:39.64ms
step:367/2330 train_time:14533ms step_avg:39.60ms
step:368/2330 train_time:14589ms step_avg:39.64ms
step:369/2330 train_time:14612ms step_avg:39.60ms
step:370/2330 train_time:14668ms step_avg:39.64ms
step:371/2330 train_time:14691ms step_avg:39.60ms
step:372/2330 train_time:14747ms step_avg:39.64ms
step:373/2330 train_time:14771ms step_avg:39.60ms
step:374/2330 train_time:14827ms step_avg:39.64ms
step:375/2330 train_time:14850ms step_avg:39.60ms
step:376/2330 train_time:14907ms step_avg:39.65ms
step:377/2330 train_time:14930ms step_avg:39.60ms
step:378/2330 train_time:14986ms step_avg:39.65ms
step:379/2330 train_time:15009ms step_avg:39.60ms
step:380/2330 train_time:15065ms step_avg:39.64ms
step:381/2330 train_time:15088ms step_avg:39.60ms
step:382/2330 train_time:15144ms step_avg:39.64ms
step:383/2330 train_time:15168ms step_avg:39.60ms
step:384/2330 train_time:15224ms step_avg:39.65ms
step:385/2330 train_time:15247ms step_avg:39.60ms
step:386/2330 train_time:15304ms step_avg:39.65ms
step:387/2330 train_time:15327ms step_avg:39.60ms
step:388/2330 train_time:15384ms step_avg:39.65ms
step:389/2330 train_time:15407ms step_avg:39.61ms
step:390/2330 train_time:15464ms step_avg:39.65ms
step:391/2330 train_time:15486ms step_avg:39.61ms
step:392/2330 train_time:15544ms step_avg:39.65ms
step:393/2330 train_time:15566ms step_avg:39.61ms
step:394/2330 train_time:15623ms step_avg:39.65ms
step:395/2330 train_time:15645ms step_avg:39.61ms
step:396/2330 train_time:15703ms step_avg:39.65ms
step:397/2330 train_time:15725ms step_avg:39.61ms
step:398/2330 train_time:15782ms step_avg:39.65ms
step:399/2330 train_time:15804ms step_avg:39.61ms
step:400/2330 train_time:15860ms step_avg:39.65ms
step:401/2330 train_time:15883ms step_avg:39.61ms
step:402/2330 train_time:15939ms step_avg:39.65ms
step:403/2330 train_time:15961ms step_avg:39.61ms
step:404/2330 train_time:16018ms step_avg:39.65ms
step:405/2330 train_time:16041ms step_avg:39.61ms
step:406/2330 train_time:16096ms step_avg:39.65ms
step:407/2330 train_time:16119ms step_avg:39.60ms
step:408/2330 train_time:16176ms step_avg:39.65ms
step:409/2330 train_time:16198ms step_avg:39.60ms
step:410/2330 train_time:16254ms step_avg:39.64ms
step:411/2330 train_time:16276ms step_avg:39.60ms
step:412/2330 train_time:16333ms step_avg:39.64ms
step:413/2330 train_time:16355ms step_avg:39.60ms
step:414/2330 train_time:16413ms step_avg:39.64ms
step:415/2330 train_time:16435ms step_avg:39.60ms
step:416/2330 train_time:16492ms step_avg:39.65ms
step:417/2330 train_time:16515ms step_avg:39.60ms
step:418/2330 train_time:16572ms step_avg:39.65ms
step:419/2330 train_time:16594ms step_avg:39.60ms
step:420/2330 train_time:16651ms step_avg:39.64ms
step:421/2330 train_time:16673ms step_avg:39.60ms
step:422/2330 train_time:16729ms step_avg:39.64ms
step:423/2330 train_time:16752ms step_avg:39.60ms
step:424/2330 train_time:16808ms step_avg:39.64ms
step:425/2330 train_time:16831ms step_avg:39.60ms
step:426/2330 train_time:16887ms step_avg:39.64ms
step:427/2330 train_time:16910ms step_avg:39.60ms
step:428/2330 train_time:16966ms step_avg:39.64ms
step:429/2330 train_time:16989ms step_avg:39.60ms
step:430/2330 train_time:17046ms step_avg:39.64ms
step:431/2330 train_time:17069ms step_avg:39.60ms
step:432/2330 train_time:17126ms step_avg:39.64ms
step:433/2330 train_time:17149ms step_avg:39.60ms
step:434/2330 train_time:17205ms step_avg:39.64ms
step:435/2330 train_time:17228ms step_avg:39.60ms
step:436/2330 train_time:17285ms step_avg:39.64ms
step:437/2330 train_time:17307ms step_avg:39.60ms
step:438/2330 train_time:17364ms step_avg:39.64ms
step:439/2330 train_time:17386ms step_avg:39.60ms
step:440/2330 train_time:17443ms step_avg:39.64ms
step:441/2330 train_time:17465ms step_avg:39.60ms
step:442/2330 train_time:17522ms step_avg:39.64ms
step:443/2330 train_time:17544ms step_avg:39.60ms
step:444/2330 train_time:17601ms step_avg:39.64ms
step:445/2330 train_time:17624ms step_avg:39.60ms
step:446/2330 train_time:17681ms step_avg:39.64ms
step:447/2330 train_time:17703ms step_avg:39.60ms
step:448/2330 train_time:17759ms step_avg:39.64ms
step:449/2330 train_time:17782ms step_avg:39.60ms
step:450/2330 train_time:17838ms step_avg:39.64ms
step:451/2330 train_time:17861ms step_avg:39.60ms
step:452/2330 train_time:17917ms step_avg:39.64ms
step:453/2330 train_time:17939ms step_avg:39.60ms
step:454/2330 train_time:17997ms step_avg:39.64ms
step:455/2330 train_time:18019ms step_avg:39.60ms
step:456/2330 train_time:18076ms step_avg:39.64ms
step:457/2330 train_time:18098ms step_avg:39.60ms
step:458/2330 train_time:18154ms step_avg:39.64ms
step:459/2330 train_time:18177ms step_avg:39.60ms
step:460/2330 train_time:18234ms step_avg:39.64ms
step:461/2330 train_time:18255ms step_avg:39.60ms
step:462/2330 train_time:18312ms step_avg:39.64ms
step:463/2330 train_time:18334ms step_avg:39.60ms
step:464/2330 train_time:18391ms step_avg:39.64ms
step:465/2330 train_time:18413ms step_avg:39.60ms
step:466/2330 train_time:18469ms step_avg:39.63ms
step:467/2330 train_time:18492ms step_avg:39.60ms
step:468/2330 train_time:18548ms step_avg:39.63ms
step:469/2330 train_time:18572ms step_avg:39.60ms
step:470/2330 train_time:18628ms step_avg:39.63ms
step:471/2330 train_time:18651ms step_avg:39.60ms
step:472/2330 train_time:18707ms step_avg:39.63ms
step:473/2330 train_time:18730ms step_avg:39.60ms
step:474/2330 train_time:18787ms step_avg:39.63ms
step:475/2330 train_time:18810ms step_avg:39.60ms
step:476/2330 train_time:18866ms step_avg:39.63ms
step:477/2330 train_time:18889ms step_avg:39.60ms
step:478/2330 train_time:18946ms step_avg:39.64ms
step:479/2330 train_time:18969ms step_avg:39.60ms
step:480/2330 train_time:19025ms step_avg:39.64ms
step:481/2330 train_time:19049ms step_avg:39.60ms
step:482/2330 train_time:19106ms step_avg:39.64ms
step:483/2330 train_time:19129ms step_avg:39.60ms
step:484/2330 train_time:19185ms step_avg:39.64ms
step:485/2330 train_time:19208ms step_avg:39.60ms
step:486/2330 train_time:19264ms step_avg:39.64ms
step:487/2330 train_time:19287ms step_avg:39.60ms
step:488/2330 train_time:19344ms step_avg:39.64ms
step:489/2330 train_time:19367ms step_avg:39.60ms
step:490/2330 train_time:19423ms step_avg:39.64ms
step:491/2330 train_time:19446ms step_avg:39.61ms
step:492/2330 train_time:19503ms step_avg:39.64ms
step:493/2330 train_time:19526ms step_avg:39.61ms
step:494/2330 train_time:19583ms step_avg:39.64ms
step:495/2330 train_time:19606ms step_avg:39.61ms
step:496/2330 train_time:19663ms step_avg:39.64ms
step:497/2330 train_time:19685ms step_avg:39.61ms
step:498/2330 train_time:19742ms step_avg:39.64ms
step:499/2330 train_time:19764ms step_avg:39.61ms
step:500/2330 train_time:19821ms step_avg:39.64ms
step:500/2330 val_loss:5.3541 train_time:19916ms step_avg:39.83ms
step:501/2330 train_time:19929ms step_avg:39.78ms
step:502/2330 train_time:19939ms step_avg:39.72ms
step:503/2330 train_time:19950ms step_avg:39.66ms
step:504/2330 train_time:19979ms step_avg:39.64ms
step:505/2330 train_time:20000ms step_avg:39.60ms
step:506/2330 train_time:20055ms step_avg:39.64ms
step:507/2330 train_time:20077ms step_avg:39.60ms
step:508/2330 train_time:20133ms step_avg:39.63ms
step:509/2330 train_time:20154ms step_avg:39.60ms
step:510/2330 train_time:20210ms step_avg:39.63ms
step:511/2330 train_time:20236ms step_avg:39.60ms
step:512/2330 train_time:20296ms step_avg:39.64ms
step:513/2330 train_time:20321ms step_avg:39.61ms
step:514/2330 train_time:20378ms step_avg:39.65ms
step:515/2330 train_time:20400ms step_avg:39.61ms
step:516/2330 train_time:20456ms step_avg:39.64ms
step:517/2330 train_time:20478ms step_avg:39.61ms
step:518/2330 train_time:20534ms step_avg:39.64ms
step:519/2330 train_time:20556ms step_avg:39.61ms
step:520/2330 train_time:20612ms step_avg:39.64ms
step:521/2330 train_time:20636ms step_avg:39.61ms
step:522/2330 train_time:20691ms step_avg:39.64ms
step:523/2330 train_time:20714ms step_avg:39.61ms
step:524/2330 train_time:20769ms step_avg:39.64ms
step:525/2330 train_time:20792ms step_avg:39.60ms
step:526/2330 train_time:20849ms step_avg:39.64ms
step:527/2330 train_time:20873ms step_avg:39.61ms
step:528/2330 train_time:20929ms step_avg:39.64ms
step:529/2330 train_time:20952ms step_avg:39.61ms
step:530/2330 train_time:21007ms step_avg:39.64ms
step:531/2330 train_time:21030ms step_avg:39.61ms
step:532/2330 train_time:21086ms step_avg:39.64ms
step:533/2330 train_time:21109ms step_avg:39.60ms
step:534/2330 train_time:21166ms step_avg:39.64ms
step:535/2330 train_time:21190ms step_avg:39.61ms
step:536/2330 train_time:21247ms step_avg:39.64ms
step:537/2330 train_time:21272ms step_avg:39.61ms
step:538/2330 train_time:21329ms step_avg:39.65ms
step:539/2330 train_time:21354ms step_avg:39.62ms
step:540/2330 train_time:21411ms step_avg:39.65ms
step:541/2330 train_time:21436ms step_avg:39.62ms
step:542/2330 train_time:21492ms step_avg:39.65ms
step:543/2330 train_time:21514ms step_avg:39.62ms
step:544/2330 train_time:21570ms step_avg:39.65ms
step:545/2330 train_time:21592ms step_avg:39.62ms
step:546/2330 train_time:21648ms step_avg:39.65ms
step:547/2330 train_time:21670ms step_avg:39.62ms
step:548/2330 train_time:21726ms step_avg:39.65ms
step:549/2330 train_time:21749ms step_avg:39.62ms
step:550/2330 train_time:21805ms step_avg:39.65ms
step:551/2330 train_time:21828ms step_avg:39.62ms
step:552/2330 train_time:21885ms step_avg:39.65ms
step:553/2330 train_time:21907ms step_avg:39.62ms
step:554/2330 train_time:21963ms step_avg:39.65ms
step:555/2330 train_time:21986ms step_avg:39.61ms
step:556/2330 train_time:22042ms step_avg:39.64ms
step:557/2330 train_time:22065ms step_avg:39.61ms
step:558/2330 train_time:22121ms step_avg:39.64ms
step:559/2330 train_time:22144ms step_avg:39.61ms
step:560/2330 train_time:22202ms step_avg:39.65ms
step:561/2330 train_time:22224ms step_avg:39.62ms
step:562/2330 train_time:22281ms step_avg:39.65ms
step:563/2330 train_time:22304ms step_avg:39.62ms
step:564/2330 train_time:22361ms step_avg:39.65ms
step:565/2330 train_time:22384ms step_avg:39.62ms
step:566/2330 train_time:22440ms step_avg:39.65ms
step:567/2330 train_time:22463ms step_avg:39.62ms
step:568/2330 train_time:22520ms step_avg:39.65ms
step:569/2330 train_time:22542ms step_avg:39.62ms
step:570/2330 train_time:22598ms step_avg:39.64ms
step:571/2330 train_time:22620ms step_avg:39.62ms
step:572/2330 train_time:22676ms step_avg:39.64ms
step:573/2330 train_time:22699ms step_avg:39.61ms
step:574/2330 train_time:22755ms step_avg:39.64ms
step:575/2330 train_time:22778ms step_avg:39.61ms
step:576/2330 train_time:22835ms step_avg:39.64ms
step:577/2330 train_time:22857ms step_avg:39.61ms
step:578/2330 train_time:22914ms step_avg:39.64ms
step:579/2330 train_time:22936ms step_avg:39.61ms
step:580/2330 train_time:22993ms step_avg:39.64ms
step:581/2330 train_time:23015ms step_avg:39.61ms
step:582/2330 train_time:23072ms step_avg:39.64ms
step:583/2330 train_time:23095ms step_avg:39.61ms
step:584/2330 train_time:23151ms step_avg:39.64ms
step:585/2330 train_time:23174ms step_avg:39.61ms
step:586/2330 train_time:23231ms step_avg:39.64ms
step:587/2330 train_time:23255ms step_avg:39.62ms
step:588/2330 train_time:23311ms step_avg:39.64ms
step:589/2330 train_time:23334ms step_avg:39.62ms
step:590/2330 train_time:23390ms step_avg:39.64ms
step:591/2330 train_time:23414ms step_avg:39.62ms
step:592/2330 train_time:23471ms step_avg:39.65ms
step:593/2330 train_time:23494ms step_avg:39.62ms
step:594/2330 train_time:23550ms step_avg:39.65ms
step:595/2330 train_time:23573ms step_avg:39.62ms
step:596/2330 train_time:23628ms step_avg:39.65ms
step:597/2330 train_time:23652ms step_avg:39.62ms
step:598/2330 train_time:23708ms step_avg:39.65ms
step:599/2330 train_time:23731ms step_avg:39.62ms
step:600/2330 train_time:23787ms step_avg:39.65ms
step:601/2330 train_time:23811ms step_avg:39.62ms
step:602/2330 train_time:23867ms step_avg:39.65ms
step:603/2330 train_time:23890ms step_avg:39.62ms
step:604/2330 train_time:23945ms step_avg:39.64ms
step:605/2330 train_time:23968ms step_avg:39.62ms
step:606/2330 train_time:24024ms step_avg:39.64ms
step:607/2330 train_time:24047ms step_avg:39.62ms
step:608/2330 train_time:24103ms step_avg:39.64ms
step:609/2330 train_time:24126ms step_avg:39.62ms
step:610/2330 train_time:24183ms step_avg:39.64ms
step:611/2330 train_time:24206ms step_avg:39.62ms
step:612/2330 train_time:24263ms step_avg:39.65ms
step:613/2330 train_time:24286ms step_avg:39.62ms
step:614/2330 train_time:24344ms step_avg:39.65ms
step:615/2330 train_time:24367ms step_avg:39.62ms
step:616/2330 train_time:24424ms step_avg:39.65ms
step:617/2330 train_time:24448ms step_avg:39.62ms
step:618/2330 train_time:24505ms step_avg:39.65ms
step:619/2330 train_time:24527ms step_avg:39.62ms
step:620/2330 train_time:24584ms step_avg:39.65ms
step:621/2330 train_time:24606ms step_avg:39.62ms
step:622/2330 train_time:24663ms step_avg:39.65ms
step:623/2330 train_time:24685ms step_avg:39.62ms
step:624/2330 train_time:24742ms step_avg:39.65ms
step:625/2330 train_time:24764ms step_avg:39.62ms
step:626/2330 train_time:24820ms step_avg:39.65ms
step:627/2330 train_time:24842ms step_avg:39.62ms
step:628/2330 train_time:24899ms step_avg:39.65ms
step:629/2330 train_time:24921ms step_avg:39.62ms
step:630/2330 train_time:24978ms step_avg:39.65ms
step:631/2330 train_time:25000ms step_avg:39.62ms
step:632/2330 train_time:25057ms step_avg:39.65ms
step:633/2330 train_time:25079ms step_avg:39.62ms
step:634/2330 train_time:25135ms step_avg:39.64ms
step:635/2330 train_time:25157ms step_avg:39.62ms
step:636/2330 train_time:25213ms step_avg:39.64ms
step:637/2330 train_time:25236ms step_avg:39.62ms
step:638/2330 train_time:25293ms step_avg:39.64ms
step:639/2330 train_time:25316ms step_avg:39.62ms
step:640/2330 train_time:25373ms step_avg:39.65ms
step:641/2330 train_time:25395ms step_avg:39.62ms
step:642/2330 train_time:25452ms step_avg:39.64ms
step:643/2330 train_time:25474ms step_avg:39.62ms
step:644/2330 train_time:25531ms step_avg:39.64ms
step:645/2330 train_time:25554ms step_avg:39.62ms
step:646/2330 train_time:25610ms step_avg:39.64ms
step:647/2330 train_time:25633ms step_avg:39.62ms
step:648/2330 train_time:25689ms step_avg:39.64ms
step:649/2330 train_time:25712ms step_avg:39.62ms
step:650/2330 train_time:25768ms step_avg:39.64ms
step:651/2330 train_time:25791ms step_avg:39.62ms
step:652/2330 train_time:25848ms step_avg:39.64ms
step:653/2330 train_time:25871ms step_avg:39.62ms
step:654/2330 train_time:25927ms step_avg:39.64ms
step:655/2330 train_time:25949ms step_avg:39.62ms
step:656/2330 train_time:26006ms step_avg:39.64ms
step:657/2330 train_time:26029ms step_avg:39.62ms
step:658/2330 train_time:26085ms step_avg:39.64ms
step:659/2330 train_time:26108ms step_avg:39.62ms
step:660/2330 train_time:26164ms step_avg:39.64ms
step:661/2330 train_time:26186ms step_avg:39.62ms
step:662/2330 train_time:26243ms step_avg:39.64ms
step:663/2330 train_time:26266ms step_avg:39.62ms
step:664/2330 train_time:26322ms step_avg:39.64ms
step:665/2330 train_time:26346ms step_avg:39.62ms
step:666/2330 train_time:26404ms step_avg:39.65ms
step:667/2330 train_time:26426ms step_avg:39.62ms
step:668/2330 train_time:26483ms step_avg:39.65ms
step:669/2330 train_time:26506ms step_avg:39.62ms
step:670/2330 train_time:26562ms step_avg:39.65ms
step:671/2330 train_time:26585ms step_avg:39.62ms
step:672/2330 train_time:26642ms step_avg:39.65ms
step:673/2330 train_time:26664ms step_avg:39.62ms
step:674/2330 train_time:26721ms step_avg:39.65ms
step:675/2330 train_time:26743ms step_avg:39.62ms
step:676/2330 train_time:26799ms step_avg:39.64ms
step:677/2330 train_time:26821ms step_avg:39.62ms
step:678/2330 train_time:26878ms step_avg:39.64ms
step:679/2330 train_time:26900ms step_avg:39.62ms
step:680/2330 train_time:26957ms step_avg:39.64ms
step:681/2330 train_time:26979ms step_avg:39.62ms
step:682/2330 train_time:27035ms step_avg:39.64ms
step:683/2330 train_time:27058ms step_avg:39.62ms
step:684/2330 train_time:27114ms step_avg:39.64ms
step:685/2330 train_time:27136ms step_avg:39.61ms
step:686/2330 train_time:27192ms step_avg:39.64ms
step:687/2330 train_time:27215ms step_avg:39.61ms
step:688/2330 train_time:27272ms step_avg:39.64ms
step:689/2330 train_time:27295ms step_avg:39.62ms
step:690/2330 train_time:27351ms step_avg:39.64ms
step:691/2330 train_time:27374ms step_avg:39.61ms
step:692/2330 train_time:27430ms step_avg:39.64ms
step:693/2330 train_time:27453ms step_avg:39.61ms
step:694/2330 train_time:27509ms step_avg:39.64ms
step:695/2330 train_time:27532ms step_avg:39.61ms
step:696/2330 train_time:27589ms step_avg:39.64ms
step:697/2330 train_time:27611ms step_avg:39.61ms
step:698/2330 train_time:27668ms step_avg:39.64ms
step:699/2330 train_time:27692ms step_avg:39.62ms
step:700/2330 train_time:27748ms step_avg:39.64ms
step:701/2330 train_time:27772ms step_avg:39.62ms
step:702/2330 train_time:27828ms step_avg:39.64ms
step:703/2330 train_time:27851ms step_avg:39.62ms
step:704/2330 train_time:27908ms step_avg:39.64ms
step:705/2330 train_time:27930ms step_avg:39.62ms
step:706/2330 train_time:27987ms step_avg:39.64ms
step:707/2330 train_time:28009ms step_avg:39.62ms
step:708/2330 train_time:28066ms step_avg:39.64ms
step:709/2330 train_time:28089ms step_avg:39.62ms
step:710/2330 train_time:28146ms step_avg:39.64ms
step:711/2330 train_time:28169ms step_avg:39.62ms
step:712/2330 train_time:28225ms step_avg:39.64ms
step:713/2330 train_time:28249ms step_avg:39.62ms
step:714/2330 train_time:28306ms step_avg:39.64ms
step:715/2330 train_time:28329ms step_avg:39.62ms
step:716/2330 train_time:28385ms step_avg:39.64ms
step:717/2330 train_time:28408ms step_avg:39.62ms
step:718/2330 train_time:28465ms step_avg:39.65ms
step:719/2330 train_time:28488ms step_avg:39.62ms
step:720/2330 train_time:28545ms step_avg:39.65ms
step:721/2330 train_time:28567ms step_avg:39.62ms
step:722/2330 train_time:28624ms step_avg:39.65ms
step:723/2330 train_time:28647ms step_avg:39.62ms
step:724/2330 train_time:28704ms step_avg:39.65ms
step:725/2330 train_time:28726ms step_avg:39.62ms
step:726/2330 train_time:28783ms step_avg:39.65ms
step:727/2330 train_time:28805ms step_avg:39.62ms
step:728/2330 train_time:28862ms step_avg:39.65ms
step:729/2330 train_time:28885ms step_avg:39.62ms
step:730/2330 train_time:28941ms step_avg:39.65ms
step:731/2330 train_time:28963ms step_avg:39.62ms
step:732/2330 train_time:29019ms step_avg:39.64ms
step:733/2330 train_time:29041ms step_avg:39.62ms
step:734/2330 train_time:29097ms step_avg:39.64ms
step:735/2330 train_time:29119ms step_avg:39.62ms
step:736/2330 train_time:29176ms step_avg:39.64ms
step:737/2330 train_time:29199ms step_avg:39.62ms
step:738/2330 train_time:29256ms step_avg:39.64ms
step:739/2330 train_time:29278ms step_avg:39.62ms
step:740/2330 train_time:29335ms step_avg:39.64ms
step:741/2330 train_time:29357ms step_avg:39.62ms
step:742/2330 train_time:29414ms step_avg:39.64ms
step:743/2330 train_time:29437ms step_avg:39.62ms
step:744/2330 train_time:29494ms step_avg:39.64ms
step:745/2330 train_time:29517ms step_avg:39.62ms
step:746/2330 train_time:29573ms step_avg:39.64ms
step:747/2330 train_time:29596ms step_avg:39.62ms
step:748/2330 train_time:29652ms step_avg:39.64ms
step:749/2330 train_time:29675ms step_avg:39.62ms
step:750/2330 train_time:29731ms step_avg:39.64ms
step:750/2330 val_loss:5.3037 train_time:29828ms step_avg:39.77ms
step:751/2330 train_time:29840ms step_avg:39.73ms
step:752/2330 train_time:29852ms step_avg:39.70ms
step:753/2330 train_time:29862ms step_avg:39.66ms
step:754/2330 train_time:29891ms step_avg:39.64ms
step:755/2330 train_time:29913ms step_avg:39.62ms
step:756/2330 train_time:29968ms step_avg:39.64ms
step:757/2330 train_time:29990ms step_avg:39.62ms
step:758/2330 train_time:30045ms step_avg:39.64ms
step:759/2330 train_time:30067ms step_avg:39.61ms
step:760/2330 train_time:30124ms step_avg:39.64ms
step:761/2330 train_time:30151ms step_avg:39.62ms
step:762/2330 train_time:30211ms step_avg:39.65ms
step:763/2330 train_time:30236ms step_avg:39.63ms
step:764/2330 train_time:30295ms step_avg:39.65ms
step:765/2330 train_time:30319ms step_avg:39.63ms
step:766/2330 train_time:30376ms step_avg:39.65ms
step:767/2330 train_time:30396ms step_avg:39.63ms
step:768/2330 train_time:30453ms step_avg:39.65ms
step:769/2330 train_time:30475ms step_avg:39.63ms
step:770/2330 train_time:30531ms step_avg:39.65ms
step:771/2330 train_time:30553ms step_avg:39.63ms
step:772/2330 train_time:30609ms step_avg:39.65ms
step:773/2330 train_time:30631ms step_avg:39.63ms
step:774/2330 train_time:30687ms step_avg:39.65ms
step:775/2330 train_time:30709ms step_avg:39.62ms
step:776/2330 train_time:30765ms step_avg:39.65ms
step:777/2330 train_time:30788ms step_avg:39.62ms
step:778/2330 train_time:30845ms step_avg:39.65ms
step:779/2330 train_time:30867ms step_avg:39.62ms
step:780/2330 train_time:30923ms step_avg:39.65ms
step:781/2330 train_time:30946ms step_avg:39.62ms
step:782/2330 train_time:31002ms step_avg:39.64ms
step:783/2330 train_time:31024ms step_avg:39.62ms
step:784/2330 train_time:31081ms step_avg:39.64ms
step:785/2330 train_time:31104ms step_avg:39.62ms
step:786/2330 train_time:31162ms step_avg:39.65ms
step:787/2330 train_time:31186ms step_avg:39.63ms
step:788/2330 train_time:31244ms step_avg:39.65ms
step:789/2330 train_time:31269ms step_avg:39.63ms
step:790/2330 train_time:31326ms step_avg:39.65ms
step:791/2330 train_time:31350ms step_avg:39.63ms
step:792/2330 train_time:31406ms step_avg:39.65ms
step:793/2330 train_time:31429ms step_avg:39.63ms
step:794/2330 train_time:31485ms step_avg:39.65ms
step:795/2330 train_time:31508ms step_avg:39.63ms
step:796/2330 train_time:31564ms step_avg:39.65ms
step:797/2330 train_time:31587ms step_avg:39.63ms
step:798/2330 train_time:31643ms step_avg:39.65ms
step:799/2330 train_time:31665ms step_avg:39.63ms
step:800/2330 train_time:31721ms step_avg:39.65ms
step:801/2330 train_time:31744ms step_avg:39.63ms
step:802/2330 train_time:31799ms step_avg:39.65ms
step:803/2330 train_time:31821ms step_avg:39.63ms
step:804/2330 train_time:31877ms step_avg:39.65ms
step:805/2330 train_time:31899ms step_avg:39.63ms
step:806/2330 train_time:31955ms step_avg:39.65ms
step:807/2330 train_time:31977ms step_avg:39.62ms
step:808/2330 train_time:32034ms step_avg:39.65ms
step:809/2330 train_time:32056ms step_avg:39.62ms
step:810/2330 train_time:32113ms step_avg:39.65ms
step:811/2330 train_time:32136ms step_avg:39.62ms
step:812/2330 train_time:32194ms step_avg:39.65ms
step:813/2330 train_time:32216ms step_avg:39.63ms
step:814/2330 train_time:32273ms step_avg:39.65ms
step:815/2330 train_time:32296ms step_avg:39.63ms
step:816/2330 train_time:32354ms step_avg:39.65ms
step:817/2330 train_time:32376ms step_avg:39.63ms
step:818/2330 train_time:32432ms step_avg:39.65ms
step:819/2330 train_time:32455ms step_avg:39.63ms
step:820/2330 train_time:32512ms step_avg:39.65ms
step:821/2330 train_time:32535ms step_avg:39.63ms
step:822/2330 train_time:32592ms step_avg:39.65ms
step:823/2330 train_time:32613ms step_avg:39.63ms
step:824/2330 train_time:32670ms step_avg:39.65ms
step:825/2330 train_time:32692ms step_avg:39.63ms
step:826/2330 train_time:32748ms step_avg:39.65ms
step:827/2330 train_time:32771ms step_avg:39.63ms
step:828/2330 train_time:32827ms step_avg:39.65ms
step:829/2330 train_time:32850ms step_avg:39.63ms
step:830/2330 train_time:32905ms step_avg:39.65ms
step:831/2330 train_time:32928ms step_avg:39.62ms
step:832/2330 train_time:32984ms step_avg:39.64ms
step:833/2330 train_time:33007ms step_avg:39.62ms
step:834/2330 train_time:33064ms step_avg:39.65ms
step:835/2330 train_time:33087ms step_avg:39.63ms
step:836/2330 train_time:33144ms step_avg:39.65ms
step:837/2330 train_time:33168ms step_avg:39.63ms
step:838/2330 train_time:33225ms step_avg:39.65ms
step:839/2330 train_time:33249ms step_avg:39.63ms
step:840/2330 train_time:33306ms step_avg:39.65ms
step:841/2330 train_time:33330ms step_avg:39.63ms
step:842/2330 train_time:33386ms step_avg:39.65ms
step:843/2330 train_time:33410ms step_avg:39.63ms
step:844/2330 train_time:33466ms step_avg:39.65ms
step:845/2330 train_time:33489ms step_avg:39.63ms
step:846/2330 train_time:33545ms step_avg:39.65ms
step:847/2330 train_time:33568ms step_avg:39.63ms
step:848/2330 train_time:33625ms step_avg:39.65ms
step:849/2330 train_time:33648ms step_avg:39.63ms
step:850/2330 train_time:33704ms step_avg:39.65ms
step:851/2330 train_time:33727ms step_avg:39.63ms
step:852/2330 train_time:33783ms step_avg:39.65ms
step:853/2330 train_time:33806ms step_avg:39.63ms
step:854/2330 train_time:33863ms step_avg:39.65ms
step:855/2330 train_time:33885ms step_avg:39.63ms
step:856/2330 train_time:33941ms step_avg:39.65ms
step:857/2330 train_time:33963ms step_avg:39.63ms
step:858/2330 train_time:34021ms step_avg:39.65ms
step:859/2330 train_time:34043ms step_avg:39.63ms
step:860/2330 train_time:34101ms step_avg:39.65ms
step:861/2330 train_time:34123ms step_avg:39.63ms
step:862/2330 train_time:34180ms step_avg:39.65ms
step:863/2330 train_time:34203ms step_avg:39.63ms
step:864/2330 train_time:34261ms step_avg:39.65ms
step:865/2330 train_time:34284ms step_avg:39.64ms
step:866/2330 train_time:34341ms step_avg:39.65ms
step:867/2330 train_time:34363ms step_avg:39.63ms
step:868/2330 train_time:34421ms step_avg:39.66ms
step:869/2330 train_time:34443ms step_avg:39.64ms
step:870/2330 train_time:34500ms step_avg:39.66ms
step:871/2330 train_time:34522ms step_avg:39.63ms
step:872/2330 train_time:34578ms step_avg:39.65ms
step:873/2330 train_time:34601ms step_avg:39.63ms
step:874/2330 train_time:34657ms step_avg:39.65ms
step:875/2330 train_time:34679ms step_avg:39.63ms
step:876/2330 train_time:34735ms step_avg:39.65ms
step:877/2330 train_time:34757ms step_avg:39.63ms
step:878/2330 train_time:34814ms step_avg:39.65ms
step:879/2330 train_time:34836ms step_avg:39.63ms
step:880/2330 train_time:34892ms step_avg:39.65ms
step:881/2330 train_time:34914ms step_avg:39.63ms
step:882/2330 train_time:34971ms step_avg:39.65ms
step:883/2330 train_time:34994ms step_avg:39.63ms
step:884/2330 train_time:35050ms step_avg:39.65ms
step:885/2330 train_time:35073ms step_avg:39.63ms
step:886/2330 train_time:35129ms step_avg:39.65ms
step:887/2330 train_time:35152ms step_avg:39.63ms
step:888/2330 train_time:35209ms step_avg:39.65ms
step:889/2330 train_time:35232ms step_avg:39.63ms
step:890/2330 train_time:35289ms step_avg:39.65ms
step:891/2330 train_time:35312ms step_avg:39.63ms
step:892/2330 train_time:35368ms step_avg:39.65ms
step:893/2330 train_time:35392ms step_avg:39.63ms
step:894/2330 train_time:35448ms step_avg:39.65ms
step:895/2330 train_time:35472ms step_avg:39.63ms
step:896/2330 train_time:35528ms step_avg:39.65ms
step:897/2330 train_time:35551ms step_avg:39.63ms
step:898/2330 train_time:35607ms step_avg:39.65ms
step:899/2330 train_time:35630ms step_avg:39.63ms
step:900/2330 train_time:35686ms step_avg:39.65ms
step:901/2330 train_time:35710ms step_avg:39.63ms
step:902/2330 train_time:35766ms step_avg:39.65ms
step:903/2330 train_time:35789ms step_avg:39.63ms
step:904/2330 train_time:35845ms step_avg:39.65ms
step:905/2330 train_time:35868ms step_avg:39.63ms
step:906/2330 train_time:35924ms step_avg:39.65ms
step:907/2330 train_time:35947ms step_avg:39.63ms
step:908/2330 train_time:36003ms step_avg:39.65ms
step:909/2330 train_time:36026ms step_avg:39.63ms
step:910/2330 train_time:36083ms step_avg:39.65ms
step:911/2330 train_time:36105ms step_avg:39.63ms
step:912/2330 train_time:36163ms step_avg:39.65ms
step:913/2330 train_time:36186ms step_avg:39.63ms
step:914/2330 train_time:36242ms step_avg:39.65ms
step:915/2330 train_time:36265ms step_avg:39.63ms
step:916/2330 train_time:36322ms step_avg:39.65ms
step:917/2330 train_time:36345ms step_avg:39.63ms
step:918/2330 train_time:36401ms step_avg:39.65ms
step:919/2330 train_time:36424ms step_avg:39.63ms
step:920/2330 train_time:36481ms step_avg:39.65ms
step:921/2330 train_time:36504ms step_avg:39.63ms
step:922/2330 train_time:36560ms step_avg:39.65ms
step:923/2330 train_time:36583ms step_avg:39.63ms
step:924/2330 train_time:36640ms step_avg:39.65ms
step:925/2330 train_time:36662ms step_avg:39.63ms
step:926/2330 train_time:36718ms step_avg:39.65ms
step:927/2330 train_time:36741ms step_avg:39.63ms
step:928/2330 train_time:36798ms step_avg:39.65ms
step:929/2330 train_time:36820ms step_avg:39.63ms
step:930/2330 train_time:36876ms step_avg:39.65ms
step:931/2330 train_time:36898ms step_avg:39.63ms
step:932/2330 train_time:36954ms step_avg:39.65ms
step:933/2330 train_time:36977ms step_avg:39.63ms
step:934/2330 train_time:37033ms step_avg:39.65ms
step:935/2330 train_time:37055ms step_avg:39.63ms
step:936/2330 train_time:37113ms step_avg:39.65ms
step:937/2330 train_time:37134ms step_avg:39.63ms
step:938/2330 train_time:37191ms step_avg:39.65ms
step:939/2330 train_time:37213ms step_avg:39.63ms
step:940/2330 train_time:37271ms step_avg:39.65ms
step:941/2330 train_time:37294ms step_avg:39.63ms
step:942/2330 train_time:37352ms step_avg:39.65ms
step:943/2330 train_time:37374ms step_avg:39.63ms
step:944/2330 train_time:37432ms step_avg:39.65ms
step:945/2330 train_time:37454ms step_avg:39.63ms
step:946/2330 train_time:37512ms step_avg:39.65ms
step:947/2330 train_time:37534ms step_avg:39.63ms
step:948/2330 train_time:37591ms step_avg:39.65ms
step:949/2330 train_time:37613ms step_avg:39.63ms
step:950/2330 train_time:37670ms step_avg:39.65ms
step:951/2330 train_time:37692ms step_avg:39.63ms
step:952/2330 train_time:37748ms step_avg:39.65ms
step:953/2330 train_time:37771ms step_avg:39.63ms
step:954/2330 train_time:37827ms step_avg:39.65ms
step:955/2330 train_time:37850ms step_avg:39.63ms
step:956/2330 train_time:37906ms step_avg:39.65ms
step:957/2330 train_time:37929ms step_avg:39.63ms
step:958/2330 train_time:37985ms step_avg:39.65ms
step:959/2330 train_time:38008ms step_avg:39.63ms
step:960/2330 train_time:38065ms step_avg:39.65ms
step:961/2330 train_time:38088ms step_avg:39.63ms
step:962/2330 train_time:38144ms step_avg:39.65ms
step:963/2330 train_time:38168ms step_avg:39.63ms
step:964/2330 train_time:38225ms step_avg:39.65ms
step:965/2330 train_time:38248ms step_avg:39.64ms
step:966/2330 train_time:38305ms step_avg:39.65ms
step:967/2330 train_time:38328ms step_avg:39.64ms
step:968/2330 train_time:38384ms step_avg:39.65ms
step:969/2330 train_time:38408ms step_avg:39.64ms
step:970/2330 train_time:38465ms step_avg:39.65ms
step:971/2330 train_time:38488ms step_avg:39.64ms
step:972/2330 train_time:38544ms step_avg:39.65ms
step:973/2330 train_time:38567ms step_avg:39.64ms
step:974/2330 train_time:38624ms step_avg:39.66ms
step:975/2330 train_time:38648ms step_avg:39.64ms
step:976/2330 train_time:38704ms step_avg:39.66ms
step:977/2330 train_time:38728ms step_avg:39.64ms
step:978/2330 train_time:38784ms step_avg:39.66ms
step:979/2330 train_time:38808ms step_avg:39.64ms
step:980/2330 train_time:38864ms step_avg:39.66ms
step:981/2330 train_time:38887ms step_avg:39.64ms
step:982/2330 train_time:38944ms step_avg:39.66ms
step:983/2330 train_time:38967ms step_avg:39.64ms
step:984/2330 train_time:39023ms step_avg:39.66ms
step:985/2330 train_time:39046ms step_avg:39.64ms
step:986/2330 train_time:39102ms step_avg:39.66ms
step:987/2330 train_time:39125ms step_avg:39.64ms
step:988/2330 train_time:39181ms step_avg:39.66ms
step:989/2330 train_time:39205ms step_avg:39.64ms
step:990/2330 train_time:39262ms step_avg:39.66ms
step:991/2330 train_time:39285ms step_avg:39.64ms
step:992/2330 train_time:39342ms step_avg:39.66ms
step:993/2330 train_time:39365ms step_avg:39.64ms
step:994/2330 train_time:39421ms step_avg:39.66ms
step:995/2330 train_time:39444ms step_avg:39.64ms
step:996/2330 train_time:39501ms step_avg:39.66ms
step:997/2330 train_time:39524ms step_avg:39.64ms
step:998/2330 train_time:39580ms step_avg:39.66ms
step:999/2330 train_time:39603ms step_avg:39.64ms
step:1000/2330 train_time:39660ms step_avg:39.66ms
step:1000/2330 val_loss:5.2544 train_time:39757ms step_avg:39.76ms
step:1001/2330 train_time:39768ms step_avg:39.73ms
step:1002/2330 train_time:39780ms step_avg:39.70ms
step:1003/2330 train_time:39790ms step_avg:39.67ms
step:1004/2330 train_time:39820ms step_avg:39.66ms
step:1005/2330 train_time:39841ms step_avg:39.64ms
step:1006/2330 train_time:39896ms step_avg:39.66ms
step:1007/2330 train_time:39919ms step_avg:39.64ms
step:1008/2330 train_time:39974ms step_avg:39.66ms
step:1009/2330 train_time:39997ms step_avg:39.64ms
step:1010/2330 train_time:40053ms step_avg:39.66ms
step:1011/2330 train_time:40079ms step_avg:39.64ms
step:1012/2330 train_time:40140ms step_avg:39.66ms
step:1013/2330 train_time:40165ms step_avg:39.65ms
step:1014/2330 train_time:40222ms step_avg:39.67ms
step:1015/2330 train_time:40246ms step_avg:39.65ms
step:1016/2330 train_time:40302ms step_avg:39.67ms
step:1017/2330 train_time:40324ms step_avg:39.65ms
step:1018/2330 train_time:40380ms step_avg:39.67ms
step:1019/2330 train_time:40402ms step_avg:39.65ms
step:1020/2330 train_time:40458ms step_avg:39.66ms
step:1021/2330 train_time:40480ms step_avg:39.65ms
step:1022/2330 train_time:40536ms step_avg:39.66ms
step:1023/2330 train_time:40559ms step_avg:39.65ms
step:1024/2330 train_time:40615ms step_avg:39.66ms
step:1025/2330 train_time:40638ms step_avg:39.65ms
step:1026/2330 train_time:40694ms step_avg:39.66ms
step:1027/2330 train_time:40718ms step_avg:39.65ms
step:1028/2330 train_time:40774ms step_avg:39.66ms
step:1029/2330 train_time:40797ms step_avg:39.65ms
step:1030/2330 train_time:40852ms step_avg:39.66ms
step:1031/2330 train_time:40875ms step_avg:39.65ms
step:1032/2330 train_time:40931ms step_avg:39.66ms
step:1033/2330 train_time:40953ms step_avg:39.64ms
step:1034/2330 train_time:41011ms step_avg:39.66ms
step:1035/2330 train_time:41034ms step_avg:39.65ms
step:1036/2330 train_time:41092ms step_avg:39.66ms
step:1037/2330 train_time:41115ms step_avg:39.65ms
step:1038/2330 train_time:41173ms step_avg:39.67ms
step:1039/2330 train_time:41197ms step_avg:39.65ms
step:1040/2330 train_time:41254ms step_avg:39.67ms
step:1041/2330 train_time:41278ms step_avg:39.65ms
step:1042/2330 train_time:41335ms step_avg:39.67ms
step:1043/2330 train_time:41358ms step_avg:39.65ms
step:1044/2330 train_time:41414ms step_avg:39.67ms
step:1045/2330 train_time:41437ms step_avg:39.65ms
step:1046/2330 train_time:41493ms step_avg:39.67ms
step:1047/2330 train_time:41516ms step_avg:39.65ms
step:1048/2330 train_time:41572ms step_avg:39.67ms
step:1049/2330 train_time:41595ms step_avg:39.65ms
step:1050/2330 train_time:41651ms step_avg:39.67ms
step:1051/2330 train_time:41673ms step_avg:39.65ms
step:1052/2330 train_time:41729ms step_avg:39.67ms
step:1053/2330 train_time:41752ms step_avg:39.65ms
step:1054/2330 train_time:41808ms step_avg:39.67ms
step:1055/2330 train_time:41829ms step_avg:39.65ms
step:1056/2330 train_time:41885ms step_avg:39.66ms
step:1057/2330 train_time:41907ms step_avg:39.65ms
step:1058/2330 train_time:41964ms step_avg:39.66ms
step:1059/2330 train_time:41986ms step_avg:39.65ms
step:1060/2330 train_time:42043ms step_avg:39.66ms
step:1061/2330 train_time:42066ms step_avg:39.65ms
step:1062/2330 train_time:42124ms step_avg:39.66ms
step:1063/2330 train_time:42147ms step_avg:39.65ms
step:1064/2330 train_time:42204ms step_avg:39.67ms
step:1065/2330 train_time:42227ms step_avg:39.65ms
step:1066/2330 train_time:42284ms step_avg:39.67ms
step:1067/2330 train_time:42307ms step_avg:39.65ms
step:1068/2330 train_time:42364ms step_avg:39.67ms
step:1069/2330 train_time:42386ms step_avg:39.65ms
step:1070/2330 train_time:42442ms step_avg:39.67ms
step:1071/2330 train_time:42465ms step_avg:39.65ms
step:1072/2330 train_time:42521ms step_avg:39.67ms
step:1073/2330 train_time:42544ms step_avg:39.65ms
step:1074/2330 train_time:42600ms step_avg:39.67ms
step:1075/2330 train_time:42623ms step_avg:39.65ms
step:1076/2330 train_time:42679ms step_avg:39.66ms
step:1077/2330 train_time:42701ms step_avg:39.65ms
step:1078/2330 train_time:42757ms step_avg:39.66ms
step:1079/2330 train_time:42780ms step_avg:39.65ms
step:1080/2330 train_time:42836ms step_avg:39.66ms
step:1081/2330 train_time:42859ms step_avg:39.65ms
step:1082/2330 train_time:42916ms step_avg:39.66ms
step:1083/2330 train_time:42938ms step_avg:39.65ms
step:1084/2330 train_time:42995ms step_avg:39.66ms
step:1085/2330 train_time:43019ms step_avg:39.65ms
step:1086/2330 train_time:43077ms step_avg:39.67ms
step:1087/2330 train_time:43101ms step_avg:39.65ms
step:1088/2330 train_time:43157ms step_avg:39.67ms
step:1089/2330 train_time:43181ms step_avg:39.65ms
step:1090/2330 train_time:43238ms step_avg:39.67ms
step:1091/2330 train_time:43261ms step_avg:39.65ms
step:1092/2330 train_time:43317ms step_avg:39.67ms
step:1093/2330 train_time:43340ms step_avg:39.65ms
step:1094/2330 train_time:43396ms step_avg:39.67ms
step:1095/2330 train_time:43419ms step_avg:39.65ms
step:1096/2330 train_time:43475ms step_avg:39.67ms
step:1097/2330 train_time:43499ms step_avg:39.65ms
step:1098/2330 train_time:43555ms step_avg:39.67ms
step:1099/2330 train_time:43578ms step_avg:39.65ms
step:1100/2330 train_time:43635ms step_avg:39.67ms
step:1101/2330 train_time:43658ms step_avg:39.65ms
step:1102/2330 train_time:43714ms step_avg:39.67ms
step:1103/2330 train_time:43736ms step_avg:39.65ms
step:1104/2330 train_time:43793ms step_avg:39.67ms
step:1105/2330 train_time:43815ms step_avg:39.65ms
step:1106/2330 train_time:43872ms step_avg:39.67ms
step:1107/2330 train_time:43894ms step_avg:39.65ms
step:1108/2330 train_time:43951ms step_avg:39.67ms
step:1109/2330 train_time:43973ms step_avg:39.65ms
step:1110/2330 train_time:44030ms step_avg:39.67ms
step:1111/2330 train_time:44054ms step_avg:39.65ms
step:1112/2330 train_time:44111ms step_avg:39.67ms
step:1113/2330 train_time:44133ms step_avg:39.65ms
step:1114/2330 train_time:44190ms step_avg:39.67ms
step:1115/2330 train_time:44212ms step_avg:39.65ms
step:1116/2330 train_time:44269ms step_avg:39.67ms
step:1117/2330 train_time:44291ms step_avg:39.65ms
step:1118/2330 train_time:44348ms step_avg:39.67ms
step:1119/2330 train_time:44371ms step_avg:39.65ms
step:1120/2330 train_time:44427ms step_avg:39.67ms
step:1121/2330 train_time:44450ms step_avg:39.65ms
step:1122/2330 train_time:44507ms step_avg:39.67ms
step:1123/2330 train_time:44529ms step_avg:39.65ms
step:1124/2330 train_time:44586ms step_avg:39.67ms
step:1125/2330 train_time:44608ms step_avg:39.65ms
step:1126/2330 train_time:44665ms step_avg:39.67ms
step:1127/2330 train_time:44687ms step_avg:39.65ms
step:1128/2330 train_time:44743ms step_avg:39.67ms
step:1129/2330 train_time:44765ms step_avg:39.65ms
step:1130/2330 train_time:44822ms step_avg:39.67ms
step:1131/2330 train_time:44844ms step_avg:39.65ms
step:1132/2330 train_time:44901ms step_avg:39.66ms
step:1133/2330 train_time:44923ms step_avg:39.65ms
step:1134/2330 train_time:44980ms step_avg:39.66ms
step:1135/2330 train_time:45003ms step_avg:39.65ms
step:1136/2330 train_time:45060ms step_avg:39.67ms
step:1137/2330 train_time:45083ms step_avg:39.65ms
step:1138/2330 train_time:45139ms step_avg:39.67ms
step:1139/2330 train_time:45162ms step_avg:39.65ms
step:1140/2330 train_time:45218ms step_avg:39.67ms
step:1141/2330 train_time:45242ms step_avg:39.65ms
step:1142/2330 train_time:45298ms step_avg:39.67ms
step:1143/2330 train_time:45322ms step_avg:39.65ms
step:1144/2330 train_time:45378ms step_avg:39.67ms
step:1145/2330 train_time:45402ms step_avg:39.65ms
step:1146/2330 train_time:45458ms step_avg:39.67ms
step:1147/2330 train_time:45482ms step_avg:39.65ms
step:1148/2330 train_time:45538ms step_avg:39.67ms
step:1149/2330 train_time:45561ms step_avg:39.65ms
step:1150/2330 train_time:45617ms step_avg:39.67ms
step:1151/2330 train_time:45640ms step_avg:39.65ms
step:1152/2330 train_time:45696ms step_avg:39.67ms
step:1153/2330 train_time:45719ms step_avg:39.65ms
step:1154/2330 train_time:45775ms step_avg:39.67ms
step:1155/2330 train_time:45798ms step_avg:39.65ms
step:1156/2330 train_time:45854ms step_avg:39.67ms
step:1157/2330 train_time:45877ms step_avg:39.65ms
step:1158/2330 train_time:45933ms step_avg:39.67ms
step:1159/2330 train_time:45957ms step_avg:39.65ms
step:1160/2330 train_time:46013ms step_avg:39.67ms
step:1161/2330 train_time:46037ms step_avg:39.65ms
step:1162/2330 train_time:46094ms step_avg:39.67ms
step:1163/2330 train_time:46117ms step_avg:39.65ms
step:1164/2330 train_time:46174ms step_avg:39.67ms
step:1165/2330 train_time:46196ms step_avg:39.65ms
step:1166/2330 train_time:46254ms step_avg:39.67ms
step:1167/2330 train_time:46276ms step_avg:39.65ms
step:1168/2330 train_time:46333ms step_avg:39.67ms
step:1169/2330 train_time:46356ms step_avg:39.65ms
step:1170/2330 train_time:46414ms step_avg:39.67ms
step:1171/2330 train_time:46437ms step_avg:39.66ms
step:1172/2330 train_time:46494ms step_avg:39.67ms
step:1173/2330 train_time:46518ms step_avg:39.66ms
step:1174/2330 train_time:46575ms step_avg:39.67ms
step:1175/2330 train_time:46598ms step_avg:39.66ms
step:1176/2330 train_time:46654ms step_avg:39.67ms
step:1177/2330 train_time:46677ms step_avg:39.66ms
step:1178/2330 train_time:46733ms step_avg:39.67ms
step:1179/2330 train_time:46755ms step_avg:39.66ms
step:1180/2330 train_time:46812ms step_avg:39.67ms
step:1181/2330 train_time:46835ms step_avg:39.66ms
step:1182/2330 train_time:46891ms step_avg:39.67ms
step:1183/2330 train_time:46914ms step_avg:39.66ms
step:1184/2330 train_time:46971ms step_avg:39.67ms
step:1185/2330 train_time:46994ms step_avg:39.66ms
step:1186/2330 train_time:47051ms step_avg:39.67ms
step:1187/2330 train_time:47073ms step_avg:39.66ms
step:1188/2330 train_time:47129ms step_avg:39.67ms
step:1189/2330 train_time:47152ms step_avg:39.66ms
step:1190/2330 train_time:47209ms step_avg:39.67ms
step:1191/2330 train_time:47231ms step_avg:39.66ms
step:1192/2330 train_time:47287ms step_avg:39.67ms
step:1193/2330 train_time:47309ms step_avg:39.66ms
step:1194/2330 train_time:47366ms step_avg:39.67ms
step:1195/2330 train_time:47389ms step_avg:39.66ms
step:1196/2330 train_time:47446ms step_avg:39.67ms
step:1197/2330 train_time:47468ms step_avg:39.66ms
step:1198/2330 train_time:47526ms step_avg:39.67ms
step:1199/2330 train_time:47548ms step_avg:39.66ms
step:1200/2330 train_time:47604ms step_avg:39.67ms
step:1201/2330 train_time:47626ms step_avg:39.66ms
step:1202/2330 train_time:47683ms step_avg:39.67ms
step:1203/2330 train_time:47705ms step_avg:39.66ms
step:1204/2330 train_time:47763ms step_avg:39.67ms
step:1205/2330 train_time:47784ms step_avg:39.66ms
step:1206/2330 train_time:47841ms step_avg:39.67ms
step:1207/2330 train_time:47864ms step_avg:39.66ms
step:1208/2330 train_time:47921ms step_avg:39.67ms
step:1209/2330 train_time:47944ms step_avg:39.66ms
step:1210/2330 train_time:48000ms step_avg:39.67ms
step:1211/2330 train_time:48023ms step_avg:39.66ms
step:1212/2330 train_time:48079ms step_avg:39.67ms
step:1213/2330 train_time:48101ms step_avg:39.65ms
step:1214/2330 train_time:48157ms step_avg:39.67ms
step:1215/2330 train_time:48181ms step_avg:39.65ms
step:1216/2330 train_time:48237ms step_avg:39.67ms
step:1217/2330 train_time:48261ms step_avg:39.66ms
step:1218/2330 train_time:48318ms step_avg:39.67ms
step:1219/2330 train_time:48341ms step_avg:39.66ms
step:1220/2330 train_time:48397ms step_avg:39.67ms
step:1221/2330 train_time:48421ms step_avg:39.66ms
step:1222/2330 train_time:48477ms step_avg:39.67ms
step:1223/2330 train_time:48501ms step_avg:39.66ms
step:1224/2330 train_time:48557ms step_avg:39.67ms
step:1225/2330 train_time:48580ms step_avg:39.66ms
step:1226/2330 train_time:48636ms step_avg:39.67ms
step:1227/2330 train_time:48660ms step_avg:39.66ms
step:1228/2330 train_time:48716ms step_avg:39.67ms
step:1229/2330 train_time:48739ms step_avg:39.66ms
step:1230/2330 train_time:48795ms step_avg:39.67ms
step:1231/2330 train_time:48819ms step_avg:39.66ms
step:1232/2330 train_time:48875ms step_avg:39.67ms
step:1233/2330 train_time:48899ms step_avg:39.66ms
step:1234/2330 train_time:48955ms step_avg:39.67ms
step:1235/2330 train_time:48978ms step_avg:39.66ms
step:1236/2330 train_time:49034ms step_avg:39.67ms
step:1237/2330 train_time:49057ms step_avg:39.66ms
step:1238/2330 train_time:49113ms step_avg:39.67ms
step:1239/2330 train_time:49136ms step_avg:39.66ms
step:1240/2330 train_time:49193ms step_avg:39.67ms
step:1241/2330 train_time:49217ms step_avg:39.66ms
step:1242/2330 train_time:49274ms step_avg:39.67ms
step:1243/2330 train_time:49297ms step_avg:39.66ms
step:1244/2330 train_time:49354ms step_avg:39.67ms
step:1245/2330 train_time:49377ms step_avg:39.66ms
step:1246/2330 train_time:49434ms step_avg:39.67ms
step:1247/2330 train_time:49457ms step_avg:39.66ms
step:1248/2330 train_time:49515ms step_avg:39.68ms
step:1249/2330 train_time:49537ms step_avg:39.66ms
step:1250/2330 train_time:49594ms step_avg:39.68ms
step:1250/2330 val_loss:5.2019 train_time:49690ms step_avg:39.75ms
step:1251/2330 train_time:49703ms step_avg:39.73ms
step:1252/2330 train_time:49714ms step_avg:39.71ms
step:1253/2330 train_time:49723ms step_avg:39.68ms
step:1254/2330 train_time:49754ms step_avg:39.68ms
step:1255/2330 train_time:49776ms step_avg:39.66ms
step:1256/2330 train_time:49832ms step_avg:39.67ms
step:1257/2330 train_time:49853ms step_avg:39.66ms
step:1258/2330 train_time:49909ms step_avg:39.67ms
step:1259/2330 train_time:49931ms step_avg:39.66ms
step:1260/2330 train_time:49988ms step_avg:39.67ms
step:1261/2330 train_time:50013ms step_avg:39.66ms
step:1262/2330 train_time:50073ms step_avg:39.68ms
step:1263/2330 train_time:50096ms step_avg:39.66ms
step:1264/2330 train_time:50152ms step_avg:39.68ms
step:1265/2330 train_time:50174ms step_avg:39.66ms
step:1266/2330 train_time:50231ms step_avg:39.68ms
step:1267/2330 train_time:50253ms step_avg:39.66ms
step:1268/2330 train_time:50309ms step_avg:39.68ms
step:1269/2330 train_time:50331ms step_avg:39.66ms
step:1270/2330 train_time:50387ms step_avg:39.67ms
step:1271/2330 train_time:50409ms step_avg:39.66ms
step:1272/2330 train_time:50465ms step_avg:39.67ms
step:1273/2330 train_time:50487ms step_avg:39.66ms
step:1274/2330 train_time:50543ms step_avg:39.67ms
step:1275/2330 train_time:50565ms step_avg:39.66ms
step:1276/2330 train_time:50622ms step_avg:39.67ms
step:1277/2330 train_time:50646ms step_avg:39.66ms
step:1278/2330 train_time:50702ms step_avg:39.67ms
step:1279/2330 train_time:50725ms step_avg:39.66ms
step:1280/2330 train_time:50781ms step_avg:39.67ms
step:1281/2330 train_time:50803ms step_avg:39.66ms
step:1282/2330 train_time:50859ms step_avg:39.67ms
step:1283/2330 train_time:50882ms step_avg:39.66ms
step:1284/2330 train_time:50940ms step_avg:39.67ms
step:1285/2330 train_time:50965ms step_avg:39.66ms
step:1286/2330 train_time:51023ms step_avg:39.68ms
step:1287/2330 train_time:51047ms step_avg:39.66ms
step:1288/2330 train_time:51104ms step_avg:39.68ms
step:1289/2330 train_time:51128ms step_avg:39.66ms
step:1290/2330 train_time:51185ms step_avg:39.68ms
step:1291/2330 train_time:51209ms step_avg:39.67ms
step:1292/2330 train_time:51265ms step_avg:39.68ms
step:1293/2330 train_time:51288ms step_avg:39.67ms
step:1294/2330 train_time:51343ms step_avg:39.68ms
step:1295/2330 train_time:51366ms step_avg:39.66ms
step:1296/2330 train_time:51421ms step_avg:39.68ms
step:1297/2330 train_time:51444ms step_avg:39.66ms
step:1298/2330 train_time:51500ms step_avg:39.68ms
step:1299/2330 train_time:51523ms step_avg:39.66ms
step:1300/2330 train_time:51579ms step_avg:39.68ms
step:1301/2330 train_time:51601ms step_avg:39.66ms
step:1302/2330 train_time:51657ms step_avg:39.68ms
step:1303/2330 train_time:51680ms step_avg:39.66ms
step:1304/2330 train_time:51736ms step_avg:39.68ms
step:1305/2330 train_time:51758ms step_avg:39.66ms
step:1306/2330 train_time:51815ms step_avg:39.67ms
step:1307/2330 train_time:51837ms step_avg:39.66ms
step:1308/2330 train_time:51893ms step_avg:39.67ms
step:1309/2330 train_time:51916ms step_avg:39.66ms
step:1310/2330 train_time:51973ms step_avg:39.67ms
step:1311/2330 train_time:51996ms step_avg:39.66ms
step:1312/2330 train_time:52053ms step_avg:39.67ms
step:1313/2330 train_time:52076ms step_avg:39.66ms
step:1314/2330 train_time:52133ms step_avg:39.67ms
step:1315/2330 train_time:52155ms step_avg:39.66ms
step:1316/2330 train_time:52212ms step_avg:39.67ms
step:1317/2330 train_time:52235ms step_avg:39.66ms
step:1318/2330 train_time:52291ms step_avg:39.67ms
step:1319/2330 train_time:52314ms step_avg:39.66ms
step:1320/2330 train_time:52371ms step_avg:39.67ms
step:1321/2330 train_time:52392ms step_avg:39.66ms
step:1322/2330 train_time:52449ms step_avg:39.67ms
step:1323/2330 train_time:52470ms step_avg:39.66ms
step:1324/2330 train_time:52527ms step_avg:39.67ms
step:1325/2330 train_time:52550ms step_avg:39.66ms
step:1326/2330 train_time:52606ms step_avg:39.67ms
step:1327/2330 train_time:52628ms step_avg:39.66ms
step:1328/2330 train_time:52684ms step_avg:39.67ms
step:1329/2330 train_time:52707ms step_avg:39.66ms
step:1330/2330 train_time:52763ms step_avg:39.67ms
step:1331/2330 train_time:52786ms step_avg:39.66ms
step:1332/2330 train_time:52842ms step_avg:39.67ms
step:1333/2330 train_time:52866ms step_avg:39.66ms
step:1334/2330 train_time:52922ms step_avg:39.67ms
step:1335/2330 train_time:52946ms step_avg:39.66ms
step:1336/2330 train_time:53003ms step_avg:39.67ms
step:1337/2330 train_time:53028ms step_avg:39.66ms
step:1338/2330 train_time:53084ms step_avg:39.67ms
step:1339/2330 train_time:53108ms step_avg:39.66ms
step:1340/2330 train_time:53164ms step_avg:39.67ms
step:1341/2330 train_time:53187ms step_avg:39.66ms
step:1342/2330 train_time:53244ms step_avg:39.68ms
step:1343/2330 train_time:53267ms step_avg:39.66ms
step:1344/2330 train_time:53323ms step_avg:39.68ms
step:1345/2330 train_time:53347ms step_avg:39.66ms
step:1346/2330 train_time:53402ms step_avg:39.67ms
step:1347/2330 train_time:53425ms step_avg:39.66ms
step:1348/2330 train_time:53481ms step_avg:39.67ms
step:1349/2330 train_time:53503ms step_avg:39.66ms
step:1350/2330 train_time:53559ms step_avg:39.67ms
step:1351/2330 train_time:53582ms step_avg:39.66ms
step:1352/2330 train_time:53639ms step_avg:39.67ms
step:1353/2330 train_time:53661ms step_avg:39.66ms
step:1354/2330 train_time:53717ms step_avg:39.67ms
step:1355/2330 train_time:53740ms step_avg:39.66ms
step:1356/2330 train_time:53796ms step_avg:39.67ms
step:1357/2330 train_time:53818ms step_avg:39.66ms
step:1358/2330 train_time:53875ms step_avg:39.67ms
step:1359/2330 train_time:53897ms step_avg:39.66ms
step:1360/2330 train_time:53954ms step_avg:39.67ms
step:1361/2330 train_time:53977ms step_avg:39.66ms
step:1362/2330 train_time:54033ms step_avg:39.67ms
step:1363/2330 train_time:54055ms step_avg:39.66ms
step:1364/2330 train_time:54112ms step_avg:39.67ms
step:1365/2330 train_time:54134ms step_avg:39.66ms
step:1366/2330 train_time:54191ms step_avg:39.67ms
step:1367/2330 train_time:54213ms step_avg:39.66ms
step:1368/2330 train_time:54269ms step_avg:39.67ms
step:1369/2330 train_time:54291ms step_avg:39.66ms
step:1370/2330 train_time:54349ms step_avg:39.67ms
step:1371/2330 train_time:54371ms step_avg:39.66ms
step:1372/2330 train_time:54427ms step_avg:39.67ms
step:1373/2330 train_time:54449ms step_avg:39.66ms
step:1374/2330 train_time:54506ms step_avg:39.67ms
step:1375/2330 train_time:54528ms step_avg:39.66ms
step:1376/2330 train_time:54585ms step_avg:39.67ms
step:1377/2330 train_time:54607ms step_avg:39.66ms
step:1378/2330 train_time:54663ms step_avg:39.67ms
step:1379/2330 train_time:54687ms step_avg:39.66ms
step:1380/2330 train_time:54743ms step_avg:39.67ms
step:1381/2330 train_time:54766ms step_avg:39.66ms
step:1382/2330 train_time:54822ms step_avg:39.67ms
step:1383/2330 train_time:54845ms step_avg:39.66ms
step:1384/2330 train_time:54901ms step_avg:39.67ms
step:1385/2330 train_time:54925ms step_avg:39.66ms
step:1386/2330 train_time:54981ms step_avg:39.67ms
step:1387/2330 train_time:55005ms step_avg:39.66ms
step:1388/2330 train_time:55061ms step_avg:39.67ms
step:1389/2330 train_time:55084ms step_avg:39.66ms
step:1390/2330 train_time:55141ms step_avg:39.67ms
step:1391/2330 train_time:55164ms step_avg:39.66ms
step:1392/2330 train_time:55221ms step_avg:39.67ms
step:1393/2330 train_time:55245ms step_avg:39.66ms
step:1394/2330 train_time:55301ms step_avg:39.67ms
step:1395/2330 train_time:55325ms step_avg:39.66ms
step:1396/2330 train_time:55381ms step_avg:39.67ms
step:1397/2330 train_time:55404ms step_avg:39.66ms
step:1398/2330 train_time:55461ms step_avg:39.67ms
step:1399/2330 train_time:55484ms step_avg:39.66ms
step:1400/2330 train_time:55540ms step_avg:39.67ms
step:1401/2330 train_time:55564ms step_avg:39.66ms
step:1402/2330 train_time:55620ms step_avg:39.67ms
step:1403/2330 train_time:55643ms step_avg:39.66ms
step:1404/2330 train_time:55700ms step_avg:39.67ms
step:1405/2330 train_time:55722ms step_avg:39.66ms
step:1406/2330 train_time:55779ms step_avg:39.67ms
step:1407/2330 train_time:55801ms step_avg:39.66ms
step:1408/2330 train_time:55858ms step_avg:39.67ms
step:1409/2330 train_time:55880ms step_avg:39.66ms
step:1410/2330 train_time:55937ms step_avg:39.67ms
step:1411/2330 train_time:55960ms step_avg:39.66ms
step:1412/2330 train_time:56018ms step_avg:39.67ms
step:1413/2330 train_time:56040ms step_avg:39.66ms
step:1414/2330 train_time:56096ms step_avg:39.67ms
step:1415/2330 train_time:56119ms step_avg:39.66ms
step:1416/2330 train_time:56176ms step_avg:39.67ms
step:1417/2330 train_time:56199ms step_avg:39.66ms
step:1418/2330 train_time:56256ms step_avg:39.67ms
step:1419/2330 train_time:56278ms step_avg:39.66ms
step:1420/2330 train_time:56335ms step_avg:39.67ms
step:1421/2330 train_time:56357ms step_avg:39.66ms
step:1422/2330 train_time:56413ms step_avg:39.67ms
step:1423/2330 train_time:56435ms step_avg:39.66ms
step:1424/2330 train_time:56492ms step_avg:39.67ms
step:1425/2330 train_time:56514ms step_avg:39.66ms
step:1426/2330 train_time:56571ms step_avg:39.67ms
step:1427/2330 train_time:56593ms step_avg:39.66ms
step:1428/2330 train_time:56650ms step_avg:39.67ms
step:1429/2330 train_time:56672ms step_avg:39.66ms
step:1430/2330 train_time:56728ms step_avg:39.67ms
step:1431/2330 train_time:56751ms step_avg:39.66ms
step:1432/2330 train_time:56808ms step_avg:39.67ms
step:1433/2330 train_time:56831ms step_avg:39.66ms
step:1434/2330 train_time:56888ms step_avg:39.67ms
step:1435/2330 train_time:56911ms step_avg:39.66ms
step:1436/2330 train_time:56967ms step_avg:39.67ms
step:1437/2330 train_time:56990ms step_avg:39.66ms
step:1438/2330 train_time:57047ms step_avg:39.67ms
step:1439/2330 train_time:57070ms step_avg:39.66ms
step:1440/2330 train_time:57126ms step_avg:39.67ms
step:1441/2330 train_time:57149ms step_avg:39.66ms
step:1442/2330 train_time:57206ms step_avg:39.67ms
step:1443/2330 train_time:57228ms step_avg:39.66ms
step:1444/2330 train_time:57285ms step_avg:39.67ms
step:1445/2330 train_time:57307ms step_avg:39.66ms
step:1446/2330 train_time:57363ms step_avg:39.67ms
step:1447/2330 train_time:57386ms step_avg:39.66ms
step:1448/2330 train_time:57442ms step_avg:39.67ms
step:1449/2330 train_time:57466ms step_avg:39.66ms
step:1450/2330 train_time:57522ms step_avg:39.67ms
step:1451/2330 train_time:57545ms step_avg:39.66ms
step:1452/2330 train_time:57602ms step_avg:39.67ms
step:1453/2330 train_time:57625ms step_avg:39.66ms
step:1454/2330 train_time:57682ms step_avg:39.67ms
step:1455/2330 train_time:57705ms step_avg:39.66ms
step:1456/2330 train_time:57761ms step_avg:39.67ms
step:1457/2330 train_time:57784ms step_avg:39.66ms
step:1458/2330 train_time:57840ms step_avg:39.67ms
step:1459/2330 train_time:57864ms step_avg:39.66ms
step:1460/2330 train_time:57920ms step_avg:39.67ms
step:1461/2330 train_time:57943ms step_avg:39.66ms
step:1462/2330 train_time:57999ms step_avg:39.67ms
step:1463/2330 train_time:58023ms step_avg:39.66ms
step:1464/2330 train_time:58079ms step_avg:39.67ms
step:1465/2330 train_time:58102ms step_avg:39.66ms
step:1466/2330 train_time:58159ms step_avg:39.67ms
step:1467/2330 train_time:58182ms step_avg:39.66ms
step:1468/2330 train_time:58238ms step_avg:39.67ms
step:1469/2330 train_time:58261ms step_avg:39.66ms
step:1470/2330 train_time:58318ms step_avg:39.67ms
step:1471/2330 train_time:58341ms step_avg:39.66ms
step:1472/2330 train_time:58398ms step_avg:39.67ms
step:1473/2330 train_time:58420ms step_avg:39.66ms
step:1474/2330 train_time:58477ms step_avg:39.67ms
step:1475/2330 train_time:58499ms step_avg:39.66ms
step:1476/2330 train_time:58556ms step_avg:39.67ms
step:1477/2330 train_time:58578ms step_avg:39.66ms
step:1478/2330 train_time:58635ms step_avg:39.67ms
step:1479/2330 train_time:58657ms step_avg:39.66ms
step:1480/2330 train_time:58713ms step_avg:39.67ms
step:1481/2330 train_time:58735ms step_avg:39.66ms
step:1482/2330 train_time:58792ms step_avg:39.67ms
step:1483/2330 train_time:58814ms step_avg:39.66ms
step:1484/2330 train_time:58870ms step_avg:39.67ms
step:1485/2330 train_time:58892ms step_avg:39.66ms
step:1486/2330 train_time:58950ms step_avg:39.67ms
step:1487/2330 train_time:58972ms step_avg:39.66ms
step:1488/2330 train_time:59028ms step_avg:39.67ms
step:1489/2330 train_time:59050ms step_avg:39.66ms
step:1490/2330 train_time:59107ms step_avg:39.67ms
step:1491/2330 train_time:59130ms step_avg:39.66ms
step:1492/2330 train_time:59187ms step_avg:39.67ms
step:1493/2330 train_time:59210ms step_avg:39.66ms
step:1494/2330 train_time:59267ms step_avg:39.67ms
step:1495/2330 train_time:59289ms step_avg:39.66ms
step:1496/2330 train_time:59345ms step_avg:39.67ms
step:1497/2330 train_time:59368ms step_avg:39.66ms
step:1498/2330 train_time:59424ms step_avg:39.67ms
step:1499/2330 train_time:59447ms step_avg:39.66ms
step:1500/2330 train_time:59503ms step_avg:39.67ms
step:1500/2330 val_loss:5.1623 train_time:59600ms step_avg:39.73ms
step:1501/2330 train_time:59612ms step_avg:39.71ms
step:1502/2330 train_time:59624ms step_avg:39.70ms
step:1503/2330 train_time:59634ms step_avg:39.68ms
step:1504/2330 train_time:59663ms step_avg:39.67ms
step:1505/2330 train_time:59684ms step_avg:39.66ms
step:1506/2330 train_time:59740ms step_avg:39.67ms
step:1507/2330 train_time:59762ms step_avg:39.66ms
step:1508/2330 train_time:59817ms step_avg:39.67ms
step:1509/2330 train_time:59840ms step_avg:39.66ms
step:1510/2330 train_time:59897ms step_avg:39.67ms
step:1511/2330 train_time:59925ms step_avg:39.66ms
step:1512/2330 train_time:59986ms step_avg:39.67ms
step:1513/2330 train_time:60011ms step_avg:39.66ms
step:1514/2330 train_time:60068ms step_avg:39.67ms
step:1515/2330 train_time:60090ms step_avg:39.66ms
step:1516/2330 train_time:60147ms step_avg:39.67ms
step:1517/2330 train_time:60169ms step_avg:39.66ms
step:1518/2330 train_time:60225ms step_avg:39.67ms
step:1519/2330 train_time:60247ms step_avg:39.66ms
step:1520/2330 train_time:60303ms step_avg:39.67ms
step:1521/2330 train_time:60326ms step_avg:39.66ms
step:1522/2330 train_time:60381ms step_avg:39.67ms
step:1523/2330 train_time:60404ms step_avg:39.66ms
step:1524/2330 train_time:60459ms step_avg:39.67ms
step:1525/2330 train_time:60482ms step_avg:39.66ms
step:1526/2330 train_time:60539ms step_avg:39.67ms
step:1527/2330 train_time:60562ms step_avg:39.66ms
step:1528/2330 train_time:60618ms step_avg:39.67ms
step:1529/2330 train_time:60642ms step_avg:39.66ms
step:1530/2330 train_time:60699ms step_avg:39.67ms
step:1531/2330 train_time:60720ms step_avg:39.66ms
step:1532/2330 train_time:60776ms step_avg:39.67ms
step:1533/2330 train_time:60798ms step_avg:39.66ms
step:1534/2330 train_time:60855ms step_avg:39.67ms
step:1535/2330 train_time:60878ms step_avg:39.66ms
step:1536/2330 train_time:60937ms step_avg:39.67ms
step:1537/2330 train_time:60962ms step_avg:39.66ms
step:1538/2330 train_time:61019ms step_avg:39.67ms
step:1539/2330 train_time:61043ms step_avg:39.66ms
step:1540/2330 train_time:61099ms step_avg:39.67ms
step:1541/2330 train_time:61123ms step_avg:39.66ms
step:1542/2330 train_time:61179ms step_avg:39.68ms
step:1543/2330 train_time:61202ms step_avg:39.66ms
step:1544/2330 train_time:61259ms step_avg:39.68ms
step:1545/2330 train_time:61282ms step_avg:39.66ms
step:1546/2330 train_time:61339ms step_avg:39.68ms
step:1547/2330 train_time:61361ms step_avg:39.66ms
step:1548/2330 train_time:61417ms step_avg:39.68ms
step:1549/2330 train_time:61440ms step_avg:39.66ms
step:1550/2330 train_time:61496ms step_avg:39.67ms
step:1551/2330 train_time:61519ms step_avg:39.66ms
step:1552/2330 train_time:61575ms step_avg:39.67ms
step:1553/2330 train_time:61598ms step_avg:39.66ms
step:1554/2330 train_time:61653ms step_avg:39.67ms
step:1555/2330 train_time:61676ms step_avg:39.66ms
step:1556/2330 train_time:61733ms step_avg:39.67ms
step:1557/2330 train_time:61756ms step_avg:39.66ms
step:1558/2330 train_time:61812ms step_avg:39.67ms
step:1559/2330 train_time:61834ms step_avg:39.66ms
step:1560/2330 train_time:61892ms step_avg:39.67ms
step:1561/2330 train_time:61914ms step_avg:39.66ms
step:1562/2330 train_time:61971ms step_avg:39.67ms
step:1563/2330 train_time:61994ms step_avg:39.66ms
step:1564/2330 train_time:62051ms step_avg:39.67ms
step:1565/2330 train_time:62074ms step_avg:39.66ms
step:1566/2330 train_time:62131ms step_avg:39.68ms
step:1567/2330 train_time:62153ms step_avg:39.66ms
step:1568/2330 train_time:62211ms step_avg:39.68ms
step:1569/2330 train_time:62233ms step_avg:39.66ms
step:1570/2330 train_time:62291ms step_avg:39.68ms
step:1571/2330 train_time:62313ms step_avg:39.66ms
step:1572/2330 train_time:62369ms step_avg:39.68ms
step:1573/2330 train_time:62391ms step_avg:39.66ms
step:1574/2330 train_time:62447ms step_avg:39.67ms
step:1575/2330 train_time:62469ms step_avg:39.66ms
step:1576/2330 train_time:62525ms step_avg:39.67ms
step:1577/2330 train_time:62547ms step_avg:39.66ms
step:1578/2330 train_time:62604ms step_avg:39.67ms
step:1579/2330 train_time:62627ms step_avg:39.66ms
step:1580/2330 train_time:62683ms step_avg:39.67ms
step:1581/2330 train_time:62706ms step_avg:39.66ms
step:1582/2330 train_time:62762ms step_avg:39.67ms
step:1583/2330 train_time:62785ms step_avg:39.66ms
step:1584/2330 train_time:62841ms step_avg:39.67ms
step:1585/2330 train_time:62865ms step_avg:39.66ms
step:1586/2330 train_time:62921ms step_avg:39.67ms
step:1587/2330 train_time:62944ms step_avg:39.66ms
step:1588/2330 train_time:63001ms step_avg:39.67ms
step:1589/2330 train_time:63024ms step_avg:39.66ms
step:1590/2330 train_time:63081ms step_avg:39.67ms
step:1591/2330 train_time:63104ms step_avg:39.66ms
step:1592/2330 train_time:63160ms step_avg:39.67ms
step:1593/2330 train_time:63184ms step_avg:39.66ms
step:1594/2330 train_time:63241ms step_avg:39.67ms
step:1595/2330 train_time:63265ms step_avg:39.66ms
step:1596/2330 train_time:63321ms step_avg:39.67ms
step:1597/2330 train_time:63344ms step_avg:39.66ms
step:1598/2330 train_time:63400ms step_avg:39.67ms
step:1599/2330 train_time:63423ms step_avg:39.66ms
step:1600/2330 train_time:63479ms step_avg:39.67ms
step:1601/2330 train_time:63501ms step_avg:39.66ms
step:1602/2330 train_time:63557ms step_avg:39.67ms
step:1603/2330 train_time:63581ms step_avg:39.66ms
step:1604/2330 train_time:63637ms step_avg:39.67ms
step:1605/2330 train_time:63660ms step_avg:39.66ms
step:1606/2330 train_time:63717ms step_avg:39.67ms
step:1607/2330 train_time:63740ms step_avg:39.66ms
step:1608/2330 train_time:63796ms step_avg:39.67ms
step:1609/2330 train_time:63819ms step_avg:39.66ms
step:1610/2330 train_time:63876ms step_avg:39.67ms
step:1611/2330 train_time:63900ms step_avg:39.66ms
step:1612/2330 train_time:63956ms step_avg:39.67ms
step:1613/2330 train_time:63980ms step_avg:39.67ms
step:1614/2330 train_time:64037ms step_avg:39.68ms
step:1615/2330 train_time:64060ms step_avg:39.67ms
step:1616/2330 train_time:64117ms step_avg:39.68ms
step:1617/2330 train_time:64139ms step_avg:39.67ms
step:1618/2330 train_time:64197ms step_avg:39.68ms
step:1619/2330 train_time:64220ms step_avg:39.67ms
step:1620/2330 train_time:64276ms step_avg:39.68ms
step:1621/2330 train_time:64299ms step_avg:39.67ms
step:1622/2330 train_time:64356ms step_avg:39.68ms
step:1623/2330 train_time:64378ms step_avg:39.67ms
step:1624/2330 train_time:64434ms step_avg:39.68ms
step:1625/2330 train_time:64456ms step_avg:39.67ms
step:1626/2330 train_time:64512ms step_avg:39.68ms
step:1627/2330 train_time:64535ms step_avg:39.66ms
step:1628/2330 train_time:64591ms step_avg:39.68ms
step:1629/2330 train_time:64613ms step_avg:39.66ms
step:1630/2330 train_time:64669ms step_avg:39.67ms
step:1631/2330 train_time:64692ms step_avg:39.66ms
step:1632/2330 train_time:64749ms step_avg:39.67ms
step:1633/2330 train_time:64771ms step_avg:39.66ms
step:1634/2330 train_time:64827ms step_avg:39.67ms
step:1635/2330 train_time:64850ms step_avg:39.66ms
step:1636/2330 train_time:64907ms step_avg:39.67ms
step:1637/2330 train_time:64929ms step_avg:39.66ms
step:1638/2330 train_time:64986ms step_avg:39.67ms
step:1639/2330 train_time:65008ms step_avg:39.66ms
step:1640/2330 train_time:65065ms step_avg:39.67ms
step:1641/2330 train_time:65088ms step_avg:39.66ms
step:1642/2330 train_time:65146ms step_avg:39.67ms
step:1643/2330 train_time:65169ms step_avg:39.66ms
step:1644/2330 train_time:65226ms step_avg:39.67ms
step:1645/2330 train_time:65248ms step_avg:39.66ms
step:1646/2330 train_time:65305ms step_avg:39.67ms
step:1647/2330 train_time:65328ms step_avg:39.66ms
step:1648/2330 train_time:65384ms step_avg:39.67ms
step:1649/2330 train_time:65406ms step_avg:39.66ms
step:1650/2330 train_time:65463ms step_avg:39.67ms
step:1651/2330 train_time:65485ms step_avg:39.66ms
step:1652/2330 train_time:65542ms step_avg:39.67ms
step:1653/2330 train_time:65565ms step_avg:39.66ms
step:1654/2330 train_time:65620ms step_avg:39.67ms
step:1655/2330 train_time:65644ms step_avg:39.66ms
step:1656/2330 train_time:65700ms step_avg:39.67ms
step:1657/2330 train_time:65723ms step_avg:39.66ms
step:1658/2330 train_time:65780ms step_avg:39.67ms
step:1659/2330 train_time:65803ms step_avg:39.66ms
step:1660/2330 train_time:65859ms step_avg:39.67ms
step:1661/2330 train_time:65883ms step_avg:39.66ms
step:1662/2330 train_time:65939ms step_avg:39.67ms
step:1663/2330 train_time:65962ms step_avg:39.66ms
step:1664/2330 train_time:66019ms step_avg:39.67ms
step:1665/2330 train_time:66042ms step_avg:39.66ms
step:1666/2330 train_time:66099ms step_avg:39.68ms
step:1667/2330 train_time:66122ms step_avg:39.67ms
step:1668/2330 train_time:66179ms step_avg:39.68ms
step:1669/2330 train_time:66202ms step_avg:39.67ms
step:1670/2330 train_time:66258ms step_avg:39.68ms
step:1671/2330 train_time:66281ms step_avg:39.67ms
step:1672/2330 train_time:66338ms step_avg:39.68ms
step:1673/2330 train_time:66361ms step_avg:39.67ms
step:1674/2330 train_time:66417ms step_avg:39.68ms
step:1675/2330 train_time:66440ms step_avg:39.67ms
step:1676/2330 train_time:66497ms step_avg:39.68ms
step:1677/2330 train_time:66520ms step_avg:39.67ms
step:1678/2330 train_time:66576ms step_avg:39.68ms
step:1679/2330 train_time:66599ms step_avg:39.67ms
step:1680/2330 train_time:66656ms step_avg:39.68ms
step:1681/2330 train_time:66678ms step_avg:39.67ms
step:1682/2330 train_time:66736ms step_avg:39.68ms
step:1683/2330 train_time:66758ms step_avg:39.67ms
step:1684/2330 train_time:66815ms step_avg:39.68ms
step:1685/2330 train_time:66837ms step_avg:39.67ms
step:1686/2330 train_time:66894ms step_avg:39.68ms
step:1687/2330 train_time:66916ms step_avg:39.67ms
step:1688/2330 train_time:66973ms step_avg:39.68ms
step:1689/2330 train_time:66995ms step_avg:39.67ms
step:1690/2330 train_time:67051ms step_avg:39.68ms
step:1691/2330 train_time:67073ms step_avg:39.66ms
step:1692/2330 train_time:67130ms step_avg:39.68ms
step:1693/2330 train_time:67152ms step_avg:39.66ms
step:1694/2330 train_time:67209ms step_avg:39.67ms
step:1695/2330 train_time:67231ms step_avg:39.66ms
step:1696/2330 train_time:67288ms step_avg:39.67ms
step:1697/2330 train_time:67310ms step_avg:39.66ms
step:1698/2330 train_time:67367ms step_avg:39.67ms
step:1699/2330 train_time:67389ms step_avg:39.66ms
step:1700/2330 train_time:67446ms step_avg:39.67ms
step:1701/2330 train_time:67468ms step_avg:39.66ms
step:1702/2330 train_time:67525ms step_avg:39.67ms
step:1703/2330 train_time:67547ms step_avg:39.66ms
step:1704/2330 train_time:67603ms step_avg:39.67ms
step:1705/2330 train_time:67626ms step_avg:39.66ms
step:1706/2330 train_time:67682ms step_avg:39.67ms
step:1707/2330 train_time:67706ms step_avg:39.66ms
step:1708/2330 train_time:67762ms step_avg:39.67ms
step:1709/2330 train_time:67785ms step_avg:39.66ms
step:1710/2330 train_time:67841ms step_avg:39.67ms
step:1711/2330 train_time:67864ms step_avg:39.66ms
step:1712/2330 train_time:67920ms step_avg:39.67ms
step:1713/2330 train_time:67943ms step_avg:39.66ms
step:1714/2330 train_time:68000ms step_avg:39.67ms
step:1715/2330 train_time:68023ms step_avg:39.66ms
step:1716/2330 train_time:68080ms step_avg:39.67ms
step:1717/2330 train_time:68103ms step_avg:39.66ms
step:1718/2330 train_time:68160ms step_avg:39.67ms
step:1719/2330 train_time:68183ms step_avg:39.66ms
step:1720/2330 train_time:68240ms step_avg:39.67ms
step:1721/2330 train_time:68262ms step_avg:39.66ms
step:1722/2330 train_time:68319ms step_avg:39.67ms
step:1723/2330 train_time:68342ms step_avg:39.66ms
step:1724/2330 train_time:68399ms step_avg:39.67ms
step:1725/2330 train_time:68422ms step_avg:39.66ms
step:1726/2330 train_time:68478ms step_avg:39.67ms
step:1727/2330 train_time:68501ms step_avg:39.66ms
step:1728/2330 train_time:68558ms step_avg:39.67ms
step:1729/2330 train_time:68581ms step_avg:39.67ms
step:1730/2330 train_time:68638ms step_avg:39.68ms
step:1731/2330 train_time:68661ms step_avg:39.67ms
step:1732/2330 train_time:68717ms step_avg:39.68ms
step:1733/2330 train_time:68740ms step_avg:39.67ms
step:1734/2330 train_time:68797ms step_avg:39.68ms
step:1735/2330 train_time:68821ms step_avg:39.67ms
step:1736/2330 train_time:68877ms step_avg:39.68ms
step:1737/2330 train_time:68900ms step_avg:39.67ms
step:1738/2330 train_time:68957ms step_avg:39.68ms
step:1739/2330 train_time:68980ms step_avg:39.67ms
step:1740/2330 train_time:69037ms step_avg:39.68ms
step:1741/2330 train_time:69061ms step_avg:39.67ms
step:1742/2330 train_time:69118ms step_avg:39.68ms
step:1743/2330 train_time:69141ms step_avg:39.67ms
step:1744/2330 train_time:69197ms step_avg:39.68ms
step:1745/2330 train_time:69221ms step_avg:39.67ms
step:1746/2330 train_time:69278ms step_avg:39.68ms
step:1747/2330 train_time:69301ms step_avg:39.67ms
step:1748/2330 train_time:69358ms step_avg:39.68ms
step:1749/2330 train_time:69380ms step_avg:39.67ms
step:1750/2330 train_time:69437ms step_avg:39.68ms
step:1750/2330 val_loss:5.1269 train_time:69534ms step_avg:39.73ms
step:1751/2330 train_time:69546ms step_avg:39.72ms
step:1752/2330 train_time:69557ms step_avg:39.70ms
step:1753/2330 train_time:69567ms step_avg:39.68ms
step:1754/2330 train_time:69596ms step_avg:39.68ms
step:1755/2330 train_time:69618ms step_avg:39.67ms
step:1756/2330 train_time:69673ms step_avg:39.68ms
step:1757/2330 train_time:69695ms step_avg:39.67ms
step:1758/2330 train_time:69751ms step_avg:39.68ms
step:1759/2330 train_time:69772ms step_avg:39.67ms
step:1760/2330 train_time:69829ms step_avg:39.68ms
step:1761/2330 train_time:69853ms step_avg:39.67ms
step:1762/2330 train_time:69914ms step_avg:39.68ms
step:1763/2330 train_time:69938ms step_avg:39.67ms
step:1764/2330 train_time:69996ms step_avg:39.68ms
step:1765/2330 train_time:70020ms step_avg:39.67ms
step:1766/2330 train_time:70076ms step_avg:39.68ms
step:1767/2330 train_time:70100ms step_avg:39.67ms
step:1768/2330 train_time:70157ms step_avg:39.68ms
step:1769/2330 train_time:70179ms step_avg:39.67ms
step:1770/2330 train_time:70236ms step_avg:39.68ms
step:1771/2330 train_time:70258ms step_avg:39.67ms
step:1772/2330 train_time:70314ms step_avg:39.68ms
step:1773/2330 train_time:70336ms step_avg:39.67ms
step:1774/2330 train_time:70393ms step_avg:39.68ms
step:1775/2330 train_time:70415ms step_avg:39.67ms
step:1776/2330 train_time:70473ms step_avg:39.68ms
step:1777/2330 train_time:70496ms step_avg:39.67ms
step:1778/2330 train_time:70553ms step_avg:39.68ms
step:1779/2330 train_time:70575ms step_avg:39.67ms
step:1780/2330 train_time:70631ms step_avg:39.68ms
step:1781/2330 train_time:70653ms step_avg:39.67ms
step:1782/2330 train_time:70709ms step_avg:39.68ms
step:1783/2330 train_time:70731ms step_avg:39.67ms
step:1784/2330 train_time:70788ms step_avg:39.68ms
step:1785/2330 train_time:70810ms step_avg:39.67ms
step:1786/2330 train_time:70868ms step_avg:39.68ms
step:1787/2330 train_time:70890ms step_avg:39.67ms
step:1788/2330 train_time:70948ms step_avg:39.68ms
step:1789/2330 train_time:70970ms step_avg:39.67ms
step:1790/2330 train_time:71027ms step_avg:39.68ms
step:1791/2330 train_time:71050ms step_avg:39.67ms
step:1792/2330 train_time:71107ms step_avg:39.68ms
step:1793/2330 train_time:71130ms step_avg:39.67ms
step:1794/2330 train_time:71186ms step_avg:39.68ms
step:1795/2330 train_time:71209ms step_avg:39.67ms
step:1796/2330 train_time:71266ms step_avg:39.68ms
step:1797/2330 train_time:71287ms step_avg:39.67ms
step:1798/2330 train_time:71344ms step_avg:39.68ms
step:1799/2330 train_time:71366ms step_avg:39.67ms
step:1800/2330 train_time:71422ms step_avg:39.68ms
step:1801/2330 train_time:71445ms step_avg:39.67ms
step:1802/2330 train_time:71501ms step_avg:39.68ms
step:1803/2330 train_time:71524ms step_avg:39.67ms
step:1804/2330 train_time:71580ms step_avg:39.68ms
step:1805/2330 train_time:71603ms step_avg:39.67ms
step:1806/2330 train_time:71659ms step_avg:39.68ms
step:1807/2330 train_time:71682ms step_avg:39.67ms
step:1808/2330 train_time:71738ms step_avg:39.68ms
step:1809/2330 train_time:71762ms step_avg:39.67ms
step:1810/2330 train_time:71819ms step_avg:39.68ms
step:1811/2330 train_time:71843ms step_avg:39.67ms
step:1812/2330 train_time:71900ms step_avg:39.68ms
step:1813/2330 train_time:71923ms step_avg:39.67ms
step:1814/2330 train_time:71979ms step_avg:39.68ms
step:1815/2330 train_time:72004ms step_avg:39.67ms
step:1816/2330 train_time:72060ms step_avg:39.68ms
step:1817/2330 train_time:72084ms step_avg:39.67ms
step:1818/2330 train_time:72140ms step_avg:39.68ms
step:1819/2330 train_time:72163ms step_avg:39.67ms
step:1820/2330 train_time:72219ms step_avg:39.68ms
step:1821/2330 train_time:72242ms step_avg:39.67ms
step:1822/2330 train_time:72299ms step_avg:39.68ms
step:1823/2330 train_time:72322ms step_avg:39.67ms
step:1824/2330 train_time:72378ms step_avg:39.68ms
step:1825/2330 train_time:72401ms step_avg:39.67ms
step:1826/2330 train_time:72456ms step_avg:39.68ms
step:1827/2330 train_time:72479ms step_avg:39.67ms
step:1828/2330 train_time:72536ms step_avg:39.68ms
step:1829/2330 train_time:72558ms step_avg:39.67ms
step:1830/2330 train_time:72614ms step_avg:39.68ms
step:1831/2330 train_time:72637ms step_avg:39.67ms
step:1832/2330 train_time:72694ms step_avg:39.68ms
step:1833/2330 train_time:72716ms step_avg:39.67ms
step:1834/2330 train_time:72773ms step_avg:39.68ms
step:1835/2330 train_time:72795ms step_avg:39.67ms
step:1836/2330 train_time:72854ms step_avg:39.68ms
step:1837/2330 train_time:72876ms step_avg:39.67ms
step:1838/2330 train_time:72934ms step_avg:39.68ms
step:1839/2330 train_time:72957ms step_avg:39.67ms
step:1840/2330 train_time:73014ms step_avg:39.68ms
step:1841/2330 train_time:73036ms step_avg:39.67ms
step:1842/2330 train_time:73094ms step_avg:39.68ms
step:1843/2330 train_time:73116ms step_avg:39.67ms
step:1844/2330 train_time:73173ms step_avg:39.68ms
step:1845/2330 train_time:73195ms step_avg:39.67ms
step:1846/2330 train_time:73252ms step_avg:39.68ms
step:1847/2330 train_time:73274ms step_avg:39.67ms
step:1848/2330 train_time:73331ms step_avg:39.68ms
step:1849/2330 train_time:73353ms step_avg:39.67ms
step:1850/2330 train_time:73409ms step_avg:39.68ms
step:1851/2330 train_time:73431ms step_avg:39.67ms
step:1852/2330 train_time:73488ms step_avg:39.68ms
step:1853/2330 train_time:73510ms step_avg:39.67ms
step:1854/2330 train_time:73566ms step_avg:39.68ms
step:1855/2330 train_time:73588ms step_avg:39.67ms
step:1856/2330 train_time:73645ms step_avg:39.68ms
step:1857/2330 train_time:73667ms step_avg:39.67ms
step:1858/2330 train_time:73723ms step_avg:39.68ms
step:1859/2330 train_time:73746ms step_avg:39.67ms
step:1860/2330 train_time:73802ms step_avg:39.68ms
step:1861/2330 train_time:73826ms step_avg:39.67ms
step:1862/2330 train_time:73882ms step_avg:39.68ms
step:1863/2330 train_time:73905ms step_avg:39.67ms
step:1864/2330 train_time:73961ms step_avg:39.68ms
step:1865/2330 train_time:73984ms step_avg:39.67ms
step:1866/2330 train_time:74041ms step_avg:39.68ms
step:1867/2330 train_time:74064ms step_avg:39.67ms
step:1868/2330 train_time:74120ms step_avg:39.68ms
step:1869/2330 train_time:74143ms step_avg:39.67ms
step:1870/2330 train_time:74200ms step_avg:39.68ms
step:1871/2330 train_time:74223ms step_avg:39.67ms
step:1872/2330 train_time:74280ms step_avg:39.68ms
step:1873/2330 train_time:74304ms step_avg:39.67ms
step:1874/2330 train_time:74360ms step_avg:39.68ms
step:1875/2330 train_time:74383ms step_avg:39.67ms
step:1876/2330 train_time:74439ms step_avg:39.68ms
step:1877/2330 train_time:74462ms step_avg:39.67ms
step:1878/2330 train_time:74518ms step_avg:39.68ms
step:1879/2330 train_time:74541ms step_avg:39.67ms
step:1880/2330 train_time:74597ms step_avg:39.68ms
step:1881/2330 train_time:74620ms step_avg:39.67ms
step:1882/2330 train_time:74676ms step_avg:39.68ms
step:1883/2330 train_time:74699ms step_avg:39.67ms
step:1884/2330 train_time:74756ms step_avg:39.68ms
step:1885/2330 train_time:74780ms step_avg:39.67ms
step:1886/2330 train_time:74836ms step_avg:39.68ms
step:1887/2330 train_time:74859ms step_avg:39.67ms
step:1888/2330 train_time:74915ms step_avg:39.68ms
step:1889/2330 train_time:74939ms step_avg:39.67ms
step:1890/2330 train_time:74997ms step_avg:39.68ms
step:1891/2330 train_time:75020ms step_avg:39.67ms
step:1892/2330 train_time:75076ms step_avg:39.68ms
step:1893/2330 train_time:75099ms step_avg:39.67ms
step:1894/2330 train_time:75156ms step_avg:39.68ms
step:1895/2330 train_time:75178ms step_avg:39.67ms
step:1896/2330 train_time:75235ms step_avg:39.68ms
step:1897/2330 train_time:75258ms step_avg:39.67ms
step:1898/2330 train_time:75315ms step_avg:39.68ms
step:1899/2330 train_time:75337ms step_avg:39.67ms
step:1900/2330 train_time:75394ms step_avg:39.68ms
step:1901/2330 train_time:75417ms step_avg:39.67ms
step:1902/2330 train_time:75473ms step_avg:39.68ms
step:1903/2330 train_time:75495ms step_avg:39.67ms
step:1904/2330 train_time:75551ms step_avg:39.68ms
step:1905/2330 train_time:75574ms step_avg:39.67ms
step:1906/2330 train_time:75630ms step_avg:39.68ms
step:1907/2330 train_time:75652ms step_avg:39.67ms
step:1908/2330 train_time:75709ms step_avg:39.68ms
step:1909/2330 train_time:75731ms step_avg:39.67ms
step:1910/2330 train_time:75787ms step_avg:39.68ms
step:1911/2330 train_time:75809ms step_avg:39.67ms
step:1912/2330 train_time:75866ms step_avg:39.68ms
step:1913/2330 train_time:75888ms step_avg:39.67ms
step:1914/2330 train_time:75946ms step_avg:39.68ms
step:1915/2330 train_time:75968ms step_avg:39.67ms
step:1916/2330 train_time:76024ms step_avg:39.68ms
step:1917/2330 train_time:76046ms step_avg:39.67ms
step:1918/2330 train_time:76103ms step_avg:39.68ms
step:1919/2330 train_time:76126ms step_avg:39.67ms
step:1920/2330 train_time:76183ms step_avg:39.68ms
step:1921/2330 train_time:76206ms step_avg:39.67ms
step:1922/2330 train_time:76263ms step_avg:39.68ms
step:1923/2330 train_time:76286ms step_avg:39.67ms
step:1924/2330 train_time:76342ms step_avg:39.68ms
step:1925/2330 train_time:76365ms step_avg:39.67ms
step:1926/2330 train_time:76422ms step_avg:39.68ms
step:1927/2330 train_time:76445ms step_avg:39.67ms
step:1928/2330 train_time:76501ms step_avg:39.68ms
step:1929/2330 train_time:76524ms step_avg:39.67ms
step:1930/2330 train_time:76580ms step_avg:39.68ms
step:1931/2330 train_time:76603ms step_avg:39.67ms
step:1932/2330 train_time:76660ms step_avg:39.68ms
step:1933/2330 train_time:76683ms step_avg:39.67ms
step:1934/2330 train_time:76739ms step_avg:39.68ms
step:1935/2330 train_time:76763ms step_avg:39.67ms
step:1936/2330 train_time:76819ms step_avg:39.68ms
step:1937/2330 train_time:76842ms step_avg:39.67ms
step:1938/2330 train_time:76898ms step_avg:39.68ms
step:1939/2330 train_time:76922ms step_avg:39.67ms
step:1940/2330 train_time:76978ms step_avg:39.68ms
step:1941/2330 train_time:77001ms step_avg:39.67ms
step:1942/2330 train_time:77058ms step_avg:39.68ms
step:1943/2330 train_time:77081ms step_avg:39.67ms
step:1944/2330 train_time:77137ms step_avg:39.68ms
step:1945/2330 train_time:77160ms step_avg:39.67ms
step:1946/2330 train_time:77217ms step_avg:39.68ms
step:1947/2330 train_time:77241ms step_avg:39.67ms
step:1948/2330 train_time:77298ms step_avg:39.68ms
step:1949/2330 train_time:77321ms step_avg:39.67ms
step:1950/2330 train_time:77377ms step_avg:39.68ms
step:1951/2330 train_time:77400ms step_avg:39.67ms
step:1952/2330 train_time:77457ms step_avg:39.68ms
step:1953/2330 train_time:77480ms step_avg:39.67ms
step:1954/2330 train_time:77537ms step_avg:39.68ms
step:1955/2330 train_time:77560ms step_avg:39.67ms
step:1956/2330 train_time:77617ms step_avg:39.68ms
step:1957/2330 train_time:77640ms step_avg:39.67ms
step:1958/2330 train_time:77698ms step_avg:39.68ms
step:1959/2330 train_time:77721ms step_avg:39.67ms
step:1960/2330 train_time:77777ms step_avg:39.68ms
step:1961/2330 train_time:77800ms step_avg:39.67ms
step:1962/2330 train_time:77857ms step_avg:39.68ms
step:1963/2330 train_time:77880ms step_avg:39.67ms
step:1964/2330 train_time:77937ms step_avg:39.68ms
step:1965/2330 train_time:77960ms step_avg:39.67ms
step:1966/2330 train_time:78016ms step_avg:39.68ms
step:1967/2330 train_time:78040ms step_avg:39.67ms
step:1968/2330 train_time:78096ms step_avg:39.68ms
step:1969/2330 train_time:78120ms step_avg:39.67ms
step:1970/2330 train_time:78176ms step_avg:39.68ms
step:1971/2330 train_time:78199ms step_avg:39.67ms
step:1972/2330 train_time:78255ms step_avg:39.68ms
step:1973/2330 train_time:78278ms step_avg:39.67ms
step:1974/2330 train_time:78335ms step_avg:39.68ms
step:1975/2330 train_time:78357ms step_avg:39.67ms
step:1976/2330 train_time:78414ms step_avg:39.68ms
step:1977/2330 train_time:78437ms step_avg:39.67ms
step:1978/2330 train_time:78494ms step_avg:39.68ms
step:1979/2330 train_time:78517ms step_avg:39.68ms
step:1980/2330 train_time:78573ms step_avg:39.68ms
step:1981/2330 train_time:78596ms step_avg:39.67ms
step:1982/2330 train_time:78653ms step_avg:39.68ms
step:1983/2330 train_time:78676ms step_avg:39.68ms
step:1984/2330 train_time:78733ms step_avg:39.68ms
step:1985/2330 train_time:78755ms step_avg:39.68ms
step:1986/2330 train_time:78812ms step_avg:39.68ms
step:1987/2330 train_time:78834ms step_avg:39.68ms
step:1988/2330 train_time:78891ms step_avg:39.68ms
step:1989/2330 train_time:78913ms step_avg:39.67ms
step:1990/2330 train_time:78970ms step_avg:39.68ms
step:1991/2330 train_time:78992ms step_avg:39.67ms
step:1992/2330 train_time:79049ms step_avg:39.68ms
step:1993/2330 train_time:79071ms step_avg:39.67ms
step:1994/2330 train_time:79127ms step_avg:39.68ms
step:1995/2330 train_time:79149ms step_avg:39.67ms
step:1996/2330 train_time:79206ms step_avg:39.68ms
step:1997/2330 train_time:79228ms step_avg:39.67ms
step:1998/2330 train_time:79284ms step_avg:39.68ms
step:1999/2330 train_time:79306ms step_avg:39.67ms
step:2000/2330 train_time:79363ms step_avg:39.68ms
step:2000/2330 val_loss:5.0980 train_time:79461ms step_avg:39.73ms
step:2001/2330 train_time:79473ms step_avg:39.72ms
step:2002/2330 train_time:79484ms step_avg:39.70ms
step:2003/2330 train_time:79494ms step_avg:39.69ms
step:2004/2330 train_time:79523ms step_avg:39.68ms
step:2005/2330 train_time:79546ms step_avg:39.67ms
step:2006/2330 train_time:79602ms step_avg:39.68ms
step:2007/2330 train_time:79624ms step_avg:39.67ms
step:2008/2330 train_time:79680ms step_avg:39.68ms
step:2009/2330 train_time:79702ms step_avg:39.67ms
step:2010/2330 train_time:79757ms step_avg:39.68ms
step:2011/2330 train_time:79783ms step_avg:39.67ms
step:2012/2330 train_time:79843ms step_avg:39.68ms
step:2013/2330 train_time:79868ms step_avg:39.68ms
step:2014/2330 train_time:79926ms step_avg:39.69ms
step:2015/2330 train_time:79951ms step_avg:39.68ms
step:2016/2330 train_time:80006ms step_avg:39.69ms
step:2017/2330 train_time:80029ms step_avg:39.68ms
step:2018/2330 train_time:80086ms step_avg:39.69ms
step:2019/2330 train_time:80108ms step_avg:39.68ms
step:2020/2330 train_time:80164ms step_avg:39.69ms
step:2021/2330 train_time:80188ms step_avg:39.68ms
step:2022/2330 train_time:80244ms step_avg:39.69ms
step:2023/2330 train_time:80266ms step_avg:39.68ms
step:2024/2330 train_time:80322ms step_avg:39.68ms
step:2025/2330 train_time:80345ms step_avg:39.68ms
step:2026/2330 train_time:80401ms step_avg:39.68ms
step:2027/2330 train_time:80424ms step_avg:39.68ms
step:2028/2330 train_time:80480ms step_avg:39.68ms
step:2029/2330 train_time:80503ms step_avg:39.68ms
step:2030/2330 train_time:80559ms step_avg:39.68ms
step:2031/2330 train_time:80581ms step_avg:39.68ms
step:2032/2330 train_time:80637ms step_avg:39.68ms
step:2033/2330 train_time:80659ms step_avg:39.67ms
step:2034/2330 train_time:80716ms step_avg:39.68ms
step:2035/2330 train_time:80740ms step_avg:39.68ms
step:2036/2330 train_time:80797ms step_avg:39.68ms
step:2037/2330 train_time:80820ms step_avg:39.68ms
step:2038/2330 train_time:80878ms step_avg:39.68ms
step:2039/2330 train_time:80902ms step_avg:39.68ms
step:2040/2330 train_time:80959ms step_avg:39.69ms
step:2041/2330 train_time:80981ms step_avg:39.68ms
step:2042/2330 train_time:81038ms step_avg:39.69ms
step:2043/2330 train_time:81061ms step_avg:39.68ms
step:2044/2330 train_time:81117ms step_avg:39.69ms
step:2045/2330 train_time:81140ms step_avg:39.68ms
step:2046/2330 train_time:81196ms step_avg:39.69ms
step:2047/2330 train_time:81219ms step_avg:39.68ms
step:2048/2330 train_time:81275ms step_avg:39.68ms
step:2049/2330 train_time:81297ms step_avg:39.68ms
step:2050/2330 train_time:81354ms step_avg:39.68ms
step:2051/2330 train_time:81376ms step_avg:39.68ms
step:2052/2330 train_time:81432ms step_avg:39.68ms
step:2053/2330 train_time:81454ms step_avg:39.68ms
step:2054/2330 train_time:81510ms step_avg:39.68ms
step:2055/2330 train_time:81532ms step_avg:39.68ms
step:2056/2330 train_time:81589ms step_avg:39.68ms
step:2057/2330 train_time:81610ms step_avg:39.67ms
step:2058/2330 train_time:81666ms step_avg:39.68ms
step:2059/2330 train_time:81690ms step_avg:39.67ms
step:2060/2330 train_time:81746ms step_avg:39.68ms
step:2061/2330 train_time:81769ms step_avg:39.67ms
step:2062/2330 train_time:81825ms step_avg:39.68ms
step:2063/2330 train_time:81849ms step_avg:39.67ms
step:2064/2330 train_time:81905ms step_avg:39.68ms
step:2065/2330 train_time:81929ms step_avg:39.68ms
step:2066/2330 train_time:81985ms step_avg:39.68ms
step:2067/2330 train_time:82009ms step_avg:39.68ms
step:2068/2330 train_time:82066ms step_avg:39.68ms
step:2069/2330 train_time:82089ms step_avg:39.68ms
step:2070/2330 train_time:82145ms step_avg:39.68ms
step:2071/2330 train_time:82168ms step_avg:39.68ms
step:2072/2330 train_time:82224ms step_avg:39.68ms
step:2073/2330 train_time:82248ms step_avg:39.68ms
step:2074/2330 train_time:82304ms step_avg:39.68ms
step:2075/2330 train_time:82327ms step_avg:39.68ms
step:2076/2330 train_time:82383ms step_avg:39.68ms
step:2077/2330 train_time:82406ms step_avg:39.68ms
step:2078/2330 train_time:82462ms step_avg:39.68ms
step:2079/2330 train_time:82485ms step_avg:39.68ms
step:2080/2330 train_time:82541ms step_avg:39.68ms
step:2081/2330 train_time:82564ms step_avg:39.68ms
step:2082/2330 train_time:82620ms step_avg:39.68ms
step:2083/2330 train_time:82643ms step_avg:39.67ms
step:2084/2330 train_time:82700ms step_avg:39.68ms
step:2085/2330 train_time:82723ms step_avg:39.68ms
step:2086/2330 train_time:82780ms step_avg:39.68ms
step:2087/2330 train_time:82802ms step_avg:39.68ms
step:2088/2330 train_time:82860ms step_avg:39.68ms
step:2089/2330 train_time:82884ms step_avg:39.68ms
step:2090/2330 train_time:82941ms step_avg:39.68ms
step:2091/2330 train_time:82965ms step_avg:39.68ms
step:2092/2330 train_time:83022ms step_avg:39.69ms
step:2093/2330 train_time:83045ms step_avg:39.68ms
step:2094/2330 train_time:83102ms step_avg:39.69ms
step:2095/2330 train_time:83125ms step_avg:39.68ms
step:2096/2330 train_time:83182ms step_avg:39.69ms
step:2097/2330 train_time:83205ms step_avg:39.68ms
step:2098/2330 train_time:83262ms step_avg:39.69ms
step:2099/2330 train_time:83285ms step_avg:39.68ms
step:2100/2330 train_time:83342ms step_avg:39.69ms
step:2101/2330 train_time:83364ms step_avg:39.68ms
step:2102/2330 train_time:83421ms step_avg:39.69ms
step:2103/2330 train_time:83444ms step_avg:39.68ms
step:2104/2330 train_time:83500ms step_avg:39.69ms
step:2105/2330 train_time:83523ms step_avg:39.68ms
step:2106/2330 train_time:83579ms step_avg:39.69ms
step:2107/2330 train_time:83602ms step_avg:39.68ms
step:2108/2330 train_time:83659ms step_avg:39.69ms
step:2109/2330 train_time:83682ms step_avg:39.68ms
step:2110/2330 train_time:83738ms step_avg:39.69ms
step:2111/2330 train_time:83761ms step_avg:39.68ms
step:2112/2330 train_time:83817ms step_avg:39.69ms
step:2113/2330 train_time:83841ms step_avg:39.68ms
step:2114/2330 train_time:83897ms step_avg:39.69ms
step:2115/2330 train_time:83920ms step_avg:39.68ms
step:2116/2330 train_time:83977ms step_avg:39.69ms
step:2117/2330 train_time:83999ms step_avg:39.68ms
step:2118/2330 train_time:84056ms step_avg:39.69ms
step:2119/2330 train_time:84079ms step_avg:39.68ms
step:2120/2330 train_time:84135ms step_avg:39.69ms
step:2121/2330 train_time:84157ms step_avg:39.68ms
step:2122/2330 train_time:84214ms step_avg:39.69ms
step:2123/2330 train_time:84236ms step_avg:39.68ms
step:2124/2330 train_time:84293ms step_avg:39.69ms
step:2125/2330 train_time:84315ms step_avg:39.68ms
step:2126/2330 train_time:84372ms step_avg:39.69ms
step:2127/2330 train_time:84394ms step_avg:39.68ms
step:2128/2330 train_time:84451ms step_avg:39.69ms
step:2129/2330 train_time:84473ms step_avg:39.68ms
step:2130/2330 train_time:84529ms step_avg:39.69ms
step:2131/2330 train_time:84552ms step_avg:39.68ms
step:2132/2330 train_time:84608ms step_avg:39.68ms
step:2133/2330 train_time:84631ms step_avg:39.68ms
step:2134/2330 train_time:84687ms step_avg:39.68ms
step:2135/2330 train_time:84710ms step_avg:39.68ms
step:2136/2330 train_time:84766ms step_avg:39.68ms
step:2137/2330 train_time:84789ms step_avg:39.68ms
step:2138/2330 train_time:84846ms step_avg:39.68ms
step:2139/2330 train_time:84869ms step_avg:39.68ms
step:2140/2330 train_time:84925ms step_avg:39.68ms
step:2141/2330 train_time:84948ms step_avg:39.68ms
step:2142/2330 train_time:85004ms step_avg:39.68ms
step:2143/2330 train_time:85027ms step_avg:39.68ms
step:2144/2330 train_time:85084ms step_avg:39.68ms
step:2145/2330 train_time:85108ms step_avg:39.68ms
step:2146/2330 train_time:85165ms step_avg:39.69ms
step:2147/2330 train_time:85188ms step_avg:39.68ms
step:2148/2330 train_time:85244ms step_avg:39.69ms
step:2149/2330 train_time:85267ms step_avg:39.68ms
step:2150/2330 train_time:85323ms step_avg:39.69ms
step:2151/2330 train_time:85346ms step_avg:39.68ms
step:2152/2330 train_time:85403ms step_avg:39.69ms
step:2153/2330 train_time:85426ms step_avg:39.68ms
step:2154/2330 train_time:85482ms step_avg:39.69ms
step:2155/2330 train_time:85506ms step_avg:39.68ms
step:2156/2330 train_time:85562ms step_avg:39.69ms
step:2157/2330 train_time:85584ms step_avg:39.68ms
step:2158/2330 train_time:85640ms step_avg:39.68ms
step:2159/2330 train_time:85663ms step_avg:39.68ms
step:2160/2330 train_time:85720ms step_avg:39.69ms
step:2161/2330 train_time:85743ms step_avg:39.68ms
step:2162/2330 train_time:85799ms step_avg:39.69ms
step:2163/2330 train_time:85822ms step_avg:39.68ms
step:2164/2330 train_time:85879ms step_avg:39.69ms
step:2165/2330 train_time:85901ms step_avg:39.68ms
step:2166/2330 train_time:85958ms step_avg:39.69ms
step:2167/2330 train_time:85981ms step_avg:39.68ms
step:2168/2330 train_time:86038ms step_avg:39.69ms
step:2169/2330 train_time:86060ms step_avg:39.68ms
step:2170/2330 train_time:86116ms step_avg:39.69ms
step:2171/2330 train_time:86139ms step_avg:39.68ms
step:2172/2330 train_time:86196ms step_avg:39.68ms
step:2173/2330 train_time:86218ms step_avg:39.68ms
step:2174/2330 train_time:86275ms step_avg:39.68ms
step:2175/2330 train_time:86297ms step_avg:39.68ms
step:2176/2330 train_time:86353ms step_avg:39.68ms
step:2177/2330 train_time:86375ms step_avg:39.68ms
step:2178/2330 train_time:86432ms step_avg:39.68ms
step:2179/2330 train_time:86454ms step_avg:39.68ms
step:2180/2330 train_time:86510ms step_avg:39.68ms
step:2181/2330 train_time:86533ms step_avg:39.68ms
step:2182/2330 train_time:86589ms step_avg:39.68ms
step:2183/2330 train_time:86611ms step_avg:39.68ms
step:2184/2330 train_time:86668ms step_avg:39.68ms
step:2185/2330 train_time:86691ms step_avg:39.68ms
step:2186/2330 train_time:86748ms step_avg:39.68ms
step:2187/2330 train_time:86770ms step_avg:39.68ms
step:2188/2330 train_time:86827ms step_avg:39.68ms
step:2189/2330 train_time:86851ms step_avg:39.68ms
step:2190/2330 train_time:86907ms step_avg:39.68ms
step:2191/2330 train_time:86930ms step_avg:39.68ms
step:2192/2330 train_time:86987ms step_avg:39.68ms
step:2193/2330 train_time:87010ms step_avg:39.68ms
step:2194/2330 train_time:87066ms step_avg:39.68ms
step:2195/2330 train_time:87089ms step_avg:39.68ms
step:2196/2330 train_time:87146ms step_avg:39.68ms
step:2197/2330 train_time:87169ms step_avg:39.68ms
step:2198/2330 train_time:87226ms step_avg:39.68ms
step:2199/2330 train_time:87249ms step_avg:39.68ms
step:2200/2330 train_time:87305ms step_avg:39.68ms
step:2201/2330 train_time:87328ms step_avg:39.68ms
step:2202/2330 train_time:87384ms step_avg:39.68ms
step:2203/2330 train_time:87408ms step_avg:39.68ms
step:2204/2330 train_time:87464ms step_avg:39.68ms
step:2205/2330 train_time:87487ms step_avg:39.68ms
step:2206/2330 train_time:87543ms step_avg:39.68ms
step:2207/2330 train_time:87566ms step_avg:39.68ms
step:2208/2330 train_time:87623ms step_avg:39.68ms
step:2209/2330 train_time:87646ms step_avg:39.68ms
step:2210/2330 train_time:87703ms step_avg:39.68ms
step:2211/2330 train_time:87726ms step_avg:39.68ms
step:2212/2330 train_time:87782ms step_avg:39.68ms
step:2213/2330 train_time:87806ms step_avg:39.68ms
step:2214/2330 train_time:87863ms step_avg:39.69ms
step:2215/2330 train_time:87886ms step_avg:39.68ms
step:2216/2330 train_time:87942ms step_avg:39.69ms
step:2217/2330 train_time:87965ms step_avg:39.68ms
step:2218/2330 train_time:88021ms step_avg:39.69ms
step:2219/2330 train_time:88044ms step_avg:39.68ms
step:2220/2330 train_time:88101ms step_avg:39.69ms
step:2221/2330 train_time:88124ms step_avg:39.68ms
step:2222/2330 train_time:88181ms step_avg:39.69ms
step:2223/2330 train_time:88205ms step_avg:39.68ms
step:2224/2330 train_time:88263ms step_avg:39.69ms
step:2225/2330 train_time:88287ms step_avg:39.68ms
step:2226/2330 train_time:88343ms step_avg:39.69ms
step:2227/2330 train_time:88366ms step_avg:39.68ms
step:2228/2330 train_time:88423ms step_avg:39.69ms
step:2229/2330 train_time:88446ms step_avg:39.68ms
step:2230/2330 train_time:88503ms step_avg:39.69ms
step:2231/2330 train_time:88526ms step_avg:39.68ms
step:2232/2330 train_time:88582ms step_avg:39.69ms
step:2233/2330 train_time:88606ms step_avg:39.68ms
step:2234/2330 train_time:88662ms step_avg:39.69ms
step:2235/2330 train_time:88685ms step_avg:39.68ms
step:2236/2330 train_time:88741ms step_avg:39.69ms
step:2237/2330 train_time:88764ms step_avg:39.68ms
step:2238/2330 train_time:88822ms step_avg:39.69ms
step:2239/2330 train_time:88845ms step_avg:39.68ms
step:2240/2330 train_time:88902ms step_avg:39.69ms
step:2241/2330 train_time:88925ms step_avg:39.68ms
step:2242/2330 train_time:88981ms step_avg:39.69ms
step:2243/2330 train_time:89005ms step_avg:39.68ms
step:2244/2330 train_time:89061ms step_avg:39.69ms
step:2245/2330 train_time:89084ms step_avg:39.68ms
step:2246/2330 train_time:89141ms step_avg:39.69ms
step:2247/2330 train_time:89164ms step_avg:39.68ms
step:2248/2330 train_time:89219ms step_avg:39.69ms
step:2249/2330 train_time:89243ms step_avg:39.68ms
step:2250/2330 train_time:89300ms step_avg:39.69ms
step:2250/2330 val_loss:5.0723 train_time:89397ms step_avg:39.73ms
step:2251/2330 train_time:89409ms step_avg:39.72ms
step:2252/2330 train_time:89420ms step_avg:39.71ms
step:2253/2330 train_time:89430ms step_avg:39.69ms
step:2254/2330 train_time:89461ms step_avg:39.69ms
step:2255/2330 train_time:89482ms step_avg:39.68ms
step:2256/2330 train_time:89537ms step_avg:39.69ms
step:2257/2330 train_time:89559ms step_avg:39.68ms
step:2258/2330 train_time:89614ms step_avg:39.69ms
step:2259/2330 train_time:89635ms step_avg:39.68ms
step:2260/2330 train_time:89692ms step_avg:39.69ms
step:2261/2330 train_time:89717ms step_avg:39.68ms
step:2262/2330 train_time:89779ms step_avg:39.69ms
step:2263/2330 train_time:89801ms step_avg:39.68ms
step:2264/2330 train_time:89859ms step_avg:39.69ms
step:2265/2330 train_time:89881ms step_avg:39.68ms
step:2266/2330 train_time:89938ms step_avg:39.69ms
step:2267/2330 train_time:89960ms step_avg:39.68ms
step:2268/2330 train_time:90016ms step_avg:39.69ms
step:2269/2330 train_time:90039ms step_avg:39.68ms
step:2270/2330 train_time:90095ms step_avg:39.69ms
step:2271/2330 train_time:90117ms step_avg:39.68ms
step:2272/2330 train_time:90172ms step_avg:39.69ms
step:2273/2330 train_time:90195ms step_avg:39.68ms
step:2274/2330 train_time:90250ms step_avg:39.69ms
step:2275/2330 train_time:90273ms step_avg:39.68ms
step:2276/2330 train_time:90330ms step_avg:39.69ms
step:2277/2330 train_time:90354ms step_avg:39.68ms
step:2278/2330 train_time:90411ms step_avg:39.69ms
step:2279/2330 train_time:90434ms step_avg:39.68ms
step:2280/2330 train_time:90490ms step_avg:39.69ms
step:2281/2330 train_time:90512ms step_avg:39.68ms
step:2282/2330 train_time:90568ms step_avg:39.69ms
step:2283/2330 train_time:90591ms step_avg:39.68ms
step:2284/2330 train_time:90647ms step_avg:39.69ms
step:2285/2330 train_time:90671ms step_avg:39.68ms
step:2286/2330 train_time:90729ms step_avg:39.69ms
step:2287/2330 train_time:90753ms step_avg:39.68ms
step:2288/2330 train_time:90810ms step_avg:39.69ms
step:2289/2330 train_time:90834ms step_avg:39.68ms
step:2290/2330 train_time:90891ms step_avg:39.69ms
step:2291/2330 train_time:90914ms step_avg:39.68ms
step:2292/2330 train_time:90971ms step_avg:39.69ms
step:2293/2330 train_time:90993ms step_avg:39.68ms
step:2294/2330 train_time:91048ms step_avg:39.69ms
step:2295/2330 train_time:91071ms step_avg:39.68ms
step:2296/2330 train_time:91127ms step_avg:39.69ms
step:2297/2330 train_time:91149ms step_avg:39.68ms
step:2298/2330 train_time:91205ms step_avg:39.69ms
step:2299/2330 train_time:91228ms step_avg:39.68ms
step:2300/2330 train_time:91284ms step_avg:39.69ms
step:2301/2330 train_time:91307ms step_avg:39.68ms
step:2302/2330 train_time:91364ms step_avg:39.69ms
step:2303/2330 train_time:91387ms step_avg:39.68ms
step:2304/2330 train_time:91443ms step_avg:39.69ms
step:2305/2330 train_time:91466ms step_avg:39.68ms
step:2306/2330 train_time:91522ms step_avg:39.69ms
step:2307/2330 train_time:91545ms step_avg:39.68ms
step:2308/2330 train_time:91601ms step_avg:39.69ms
step:2309/2330 train_time:91624ms step_avg:39.68ms
step:2310/2330 train_time:91681ms step_avg:39.69ms
step:2311/2330 train_time:91704ms step_avg:39.68ms
step:2312/2330 train_time:91762ms step_avg:39.69ms
step:2313/2330 train_time:91785ms step_avg:39.68ms
step:2314/2330 train_time:91842ms step_avg:39.69ms
step:2315/2330 train_time:91865ms step_avg:39.68ms
step:2316/2330 train_time:91922ms step_avg:39.69ms
step:2317/2330 train_time:91945ms step_avg:39.68ms
step:2318/2330 train_time:92001ms step_avg:39.69ms
step:2319/2330 train_time:92023ms step_avg:39.68ms
step:2320/2330 train_time:92079ms step_avg:39.69ms
step:2321/2330 train_time:92102ms step_avg:39.68ms
step:2322/2330 train_time:92158ms step_avg:39.69ms
step:2323/2330 train_time:92180ms step_avg:39.68ms
step:2324/2330 train_time:92237ms step_avg:39.69ms
step:2325/2330 train_time:92259ms step_avg:39.68ms
step:2326/2330 train_time:92316ms step_avg:39.69ms
step:2327/2330 train_time:92338ms step_avg:39.68ms
step:2328/2330 train_time:92394ms step_avg:39.69ms
step:2329/2330 train_time:92416ms step_avg:39.68ms
step:2330/2330 train_time:92473ms step_avg:39.69ms
step:2330/2330 val_loss:5.0658 train_time:92569ms step_avg:39.73ms
peak memory allocated: 29226 MiB reserved: 38888 MiB
