import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "adam_lr7e-3"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-1, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.Adam(
    hidden_matrix_params + gate_params,
    lr=7e-3,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 00:32:02 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   34C    P0             113W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   32C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   30C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   34C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:81ms step_avg:80.69ms
step:2/2330 train_time:185ms step_avg:92.37ms
step:3/2330 train_time:204ms step_avg:68.02ms
step:4/2330 train_time:226ms step_avg:56.45ms
step:5/2330 train_time:281ms step_avg:56.15ms
step:6/2330 train_time:340ms step_avg:56.59ms
step:7/2330 train_time:395ms step_avg:56.37ms
step:8/2330 train_time:454ms step_avg:56.71ms
step:9/2330 train_time:509ms step_avg:56.53ms
step:10/2330 train_time:568ms step_avg:56.79ms
step:11/2330 train_time:623ms step_avg:56.62ms
step:12/2330 train_time:682ms step_avg:56.85ms
step:13/2330 train_time:737ms step_avg:56.71ms
step:14/2330 train_time:796ms step_avg:56.88ms
step:15/2330 train_time:851ms step_avg:56.75ms
step:16/2330 train_time:911ms step_avg:56.91ms
step:17/2330 train_time:966ms step_avg:56.83ms
step:18/2330 train_time:1025ms step_avg:56.94ms
step:19/2330 train_time:1080ms step_avg:56.84ms
step:20/2330 train_time:1140ms step_avg:56.98ms
step:21/2330 train_time:1195ms step_avg:56.90ms
step:22/2330 train_time:1256ms step_avg:57.11ms
step:23/2330 train_time:1312ms step_avg:57.02ms
step:24/2330 train_time:1372ms step_avg:57.18ms
step:25/2330 train_time:1428ms step_avg:57.10ms
step:26/2330 train_time:1488ms step_avg:57.22ms
step:27/2330 train_time:1543ms step_avg:57.16ms
step:28/2330 train_time:1604ms step_avg:57.28ms
step:29/2330 train_time:1659ms step_avg:57.21ms
step:30/2330 train_time:1719ms step_avg:57.29ms
step:31/2330 train_time:1774ms step_avg:57.23ms
step:32/2330 train_time:1833ms step_avg:57.28ms
step:33/2330 train_time:1888ms step_avg:57.22ms
step:34/2330 train_time:1948ms step_avg:57.29ms
step:35/2330 train_time:2003ms step_avg:57.23ms
step:36/2330 train_time:2064ms step_avg:57.33ms
step:37/2330 train_time:2119ms step_avg:57.27ms
step:38/2330 train_time:2180ms step_avg:57.36ms
step:39/2330 train_time:2235ms step_avg:57.31ms
step:40/2330 train_time:2294ms step_avg:57.36ms
step:41/2330 train_time:2350ms step_avg:57.31ms
step:42/2330 train_time:2409ms step_avg:57.37ms
step:43/2330 train_time:2465ms step_avg:57.33ms
step:44/2330 train_time:2525ms step_avg:57.40ms
step:45/2330 train_time:2580ms step_avg:57.34ms
step:46/2330 train_time:2640ms step_avg:57.40ms
step:47/2330 train_time:2695ms step_avg:57.35ms
step:48/2330 train_time:2756ms step_avg:57.41ms
step:49/2330 train_time:2811ms step_avg:57.37ms
step:50/2330 train_time:2871ms step_avg:57.42ms
step:51/2330 train_time:2926ms step_avg:57.37ms
step:52/2330 train_time:2986ms step_avg:57.42ms
step:53/2330 train_time:3041ms step_avg:57.38ms
step:54/2330 train_time:3102ms step_avg:57.44ms
step:55/2330 train_time:3157ms step_avg:57.40ms
step:56/2330 train_time:3217ms step_avg:57.44ms
step:57/2330 train_time:3272ms step_avg:57.40ms
step:58/2330 train_time:3332ms step_avg:57.45ms
step:59/2330 train_time:3388ms step_avg:57.42ms
step:60/2330 train_time:3448ms step_avg:57.47ms
step:61/2330 train_time:3503ms step_avg:57.43ms
step:62/2330 train_time:3565ms step_avg:57.49ms
step:63/2330 train_time:3620ms step_avg:57.46ms
step:64/2330 train_time:3680ms step_avg:57.50ms
step:65/2330 train_time:3735ms step_avg:57.46ms
step:66/2330 train_time:3795ms step_avg:57.50ms
step:67/2330 train_time:3850ms step_avg:57.46ms
step:68/2330 train_time:3910ms step_avg:57.50ms
step:69/2330 train_time:3966ms step_avg:57.48ms
step:70/2330 train_time:4027ms step_avg:57.52ms
step:71/2330 train_time:4083ms step_avg:57.50ms
step:72/2330 train_time:4143ms step_avg:57.54ms
step:73/2330 train_time:4198ms step_avg:57.50ms
step:74/2330 train_time:4258ms step_avg:57.54ms
step:75/2330 train_time:4314ms step_avg:57.51ms
step:76/2330 train_time:4374ms step_avg:57.56ms
step:77/2330 train_time:4430ms step_avg:57.53ms
step:78/2330 train_time:4490ms step_avg:57.57ms
step:79/2330 train_time:4546ms step_avg:57.54ms
step:80/2330 train_time:4608ms step_avg:57.60ms
step:81/2330 train_time:4663ms step_avg:57.57ms
step:82/2330 train_time:4723ms step_avg:57.60ms
step:83/2330 train_time:4778ms step_avg:57.57ms
step:84/2330 train_time:4838ms step_avg:57.60ms
step:85/2330 train_time:4893ms step_avg:57.57ms
step:86/2330 train_time:4953ms step_avg:57.60ms
step:87/2330 train_time:5009ms step_avg:57.57ms
step:88/2330 train_time:5069ms step_avg:57.61ms
step:89/2330 train_time:5125ms step_avg:57.59ms
step:90/2330 train_time:5184ms step_avg:57.60ms
step:91/2330 train_time:5240ms step_avg:57.58ms
step:92/2330 train_time:5300ms step_avg:57.61ms
step:93/2330 train_time:5355ms step_avg:57.58ms
step:94/2330 train_time:5416ms step_avg:57.61ms
step:95/2330 train_time:5471ms step_avg:57.59ms
step:96/2330 train_time:5532ms step_avg:57.62ms
step:97/2330 train_time:5587ms step_avg:57.60ms
step:98/2330 train_time:5648ms step_avg:57.63ms
step:99/2330 train_time:5704ms step_avg:57.62ms
step:100/2330 train_time:5763ms step_avg:57.63ms
step:101/2330 train_time:5819ms step_avg:57.61ms
step:102/2330 train_time:5878ms step_avg:57.63ms
step:103/2330 train_time:5933ms step_avg:57.61ms
step:104/2330 train_time:5994ms step_avg:57.63ms
step:105/2330 train_time:6049ms step_avg:57.61ms
step:106/2330 train_time:6110ms step_avg:57.64ms
step:107/2330 train_time:6165ms step_avg:57.62ms
step:108/2330 train_time:6225ms step_avg:57.64ms
step:109/2330 train_time:6281ms step_avg:57.62ms
step:110/2330 train_time:6341ms step_avg:57.65ms
step:111/2330 train_time:6397ms step_avg:57.63ms
step:112/2330 train_time:6457ms step_avg:57.65ms
step:113/2330 train_time:6512ms step_avg:57.63ms
step:114/2330 train_time:6572ms step_avg:57.65ms
step:115/2330 train_time:6628ms step_avg:57.63ms
step:116/2330 train_time:6688ms step_avg:57.66ms
step:117/2330 train_time:6744ms step_avg:57.64ms
step:118/2330 train_time:6803ms step_avg:57.66ms
step:119/2330 train_time:6858ms step_avg:57.63ms
step:120/2330 train_time:6919ms step_avg:57.65ms
step:121/2330 train_time:6974ms step_avg:57.64ms
step:122/2330 train_time:7034ms step_avg:57.65ms
step:123/2330 train_time:7089ms step_avg:57.64ms
step:124/2330 train_time:7149ms step_avg:57.66ms
step:125/2330 train_time:7205ms step_avg:57.64ms
step:126/2330 train_time:7265ms step_avg:57.66ms
step:127/2330 train_time:7322ms step_avg:57.65ms
step:128/2330 train_time:7382ms step_avg:57.67ms
step:129/2330 train_time:7437ms step_avg:57.65ms
step:130/2330 train_time:7497ms step_avg:57.67ms
step:131/2330 train_time:7553ms step_avg:57.65ms
step:132/2330 train_time:7613ms step_avg:57.67ms
step:133/2330 train_time:7668ms step_avg:57.66ms
step:134/2330 train_time:7730ms step_avg:57.69ms
step:135/2330 train_time:7785ms step_avg:57.67ms
step:136/2330 train_time:7846ms step_avg:57.69ms
step:137/2330 train_time:7902ms step_avg:57.68ms
step:138/2330 train_time:7962ms step_avg:57.69ms
step:139/2330 train_time:8018ms step_avg:57.68ms
step:140/2330 train_time:8077ms step_avg:57.69ms
step:141/2330 train_time:8133ms step_avg:57.68ms
step:142/2330 train_time:8194ms step_avg:57.70ms
step:143/2330 train_time:8249ms step_avg:57.69ms
step:144/2330 train_time:8309ms step_avg:57.70ms
step:145/2330 train_time:8365ms step_avg:57.69ms
step:146/2330 train_time:8425ms step_avg:57.71ms
step:147/2330 train_time:8481ms step_avg:57.69ms
step:148/2330 train_time:8540ms step_avg:57.70ms
step:149/2330 train_time:8596ms step_avg:57.69ms
step:150/2330 train_time:8656ms step_avg:57.70ms
step:151/2330 train_time:8711ms step_avg:57.69ms
step:152/2330 train_time:8772ms step_avg:57.71ms
step:153/2330 train_time:8827ms step_avg:57.69ms
step:154/2330 train_time:8888ms step_avg:57.72ms
step:155/2330 train_time:8944ms step_avg:57.70ms
step:156/2330 train_time:9004ms step_avg:57.72ms
step:157/2330 train_time:9060ms step_avg:57.71ms
step:158/2330 train_time:9120ms step_avg:57.72ms
step:159/2330 train_time:9175ms step_avg:57.70ms
step:160/2330 train_time:9236ms step_avg:57.72ms
step:161/2330 train_time:9291ms step_avg:57.71ms
step:162/2330 train_time:9352ms step_avg:57.73ms
step:163/2330 train_time:9407ms step_avg:57.71ms
step:164/2330 train_time:9468ms step_avg:57.73ms
step:165/2330 train_time:9524ms step_avg:57.72ms
step:166/2330 train_time:9584ms step_avg:57.74ms
step:167/2330 train_time:9640ms step_avg:57.72ms
step:168/2330 train_time:9699ms step_avg:57.74ms
step:169/2330 train_time:9755ms step_avg:57.72ms
step:170/2330 train_time:9815ms step_avg:57.74ms
step:171/2330 train_time:9871ms step_avg:57.72ms
step:172/2330 train_time:9931ms step_avg:57.74ms
step:173/2330 train_time:9987ms step_avg:57.73ms
step:174/2330 train_time:10048ms step_avg:57.75ms
step:175/2330 train_time:10104ms step_avg:57.74ms
step:176/2330 train_time:10164ms step_avg:57.75ms
step:177/2330 train_time:10220ms step_avg:57.74ms
step:178/2330 train_time:10279ms step_avg:57.75ms
step:179/2330 train_time:10335ms step_avg:57.74ms
step:180/2330 train_time:10396ms step_avg:57.75ms
step:181/2330 train_time:10451ms step_avg:57.74ms
step:182/2330 train_time:10512ms step_avg:57.76ms
step:183/2330 train_time:10568ms step_avg:57.75ms
step:184/2330 train_time:10629ms step_avg:57.77ms
step:185/2330 train_time:10685ms step_avg:57.75ms
step:186/2330 train_time:10745ms step_avg:57.77ms
step:187/2330 train_time:10800ms step_avg:57.75ms
step:188/2330 train_time:10860ms step_avg:57.77ms
step:189/2330 train_time:10916ms step_avg:57.75ms
step:190/2330 train_time:10976ms step_avg:57.77ms
step:191/2330 train_time:11032ms step_avg:57.76ms
step:192/2330 train_time:11092ms step_avg:57.77ms
step:193/2330 train_time:11148ms step_avg:57.76ms
step:194/2330 train_time:11208ms step_avg:57.78ms
step:195/2330 train_time:11264ms step_avg:57.76ms
step:196/2330 train_time:11324ms step_avg:57.78ms
step:197/2330 train_time:11380ms step_avg:57.77ms
step:198/2330 train_time:11440ms step_avg:57.78ms
step:199/2330 train_time:11496ms step_avg:57.77ms
step:200/2330 train_time:11556ms step_avg:57.78ms
step:201/2330 train_time:11612ms step_avg:57.77ms
step:202/2330 train_time:11672ms step_avg:57.78ms
step:203/2330 train_time:11728ms step_avg:57.78ms
step:204/2330 train_time:11789ms step_avg:57.79ms
step:205/2330 train_time:11845ms step_avg:57.78ms
step:206/2330 train_time:11905ms step_avg:57.79ms
step:207/2330 train_time:11961ms step_avg:57.78ms
step:208/2330 train_time:12021ms step_avg:57.79ms
step:209/2330 train_time:12076ms step_avg:57.78ms
step:210/2330 train_time:12136ms step_avg:57.79ms
step:211/2330 train_time:12192ms step_avg:57.78ms
step:212/2330 train_time:12253ms step_avg:57.80ms
step:213/2330 train_time:12308ms step_avg:57.79ms
step:214/2330 train_time:12369ms step_avg:57.80ms
step:215/2330 train_time:12425ms step_avg:57.79ms
step:216/2330 train_time:12485ms step_avg:57.80ms
step:217/2330 train_time:12541ms step_avg:57.79ms
step:218/2330 train_time:12600ms step_avg:57.80ms
step:219/2330 train_time:12656ms step_avg:57.79ms
step:220/2330 train_time:12716ms step_avg:57.80ms
step:221/2330 train_time:12772ms step_avg:57.79ms
step:222/2330 train_time:12832ms step_avg:57.80ms
step:223/2330 train_time:12888ms step_avg:57.79ms
step:224/2330 train_time:12949ms step_avg:57.81ms
step:225/2330 train_time:13004ms step_avg:57.80ms
step:226/2330 train_time:13066ms step_avg:57.81ms
step:227/2330 train_time:13122ms step_avg:57.81ms
step:228/2330 train_time:13182ms step_avg:57.81ms
step:229/2330 train_time:13238ms step_avg:57.81ms
step:230/2330 train_time:13297ms step_avg:57.81ms
step:231/2330 train_time:13353ms step_avg:57.80ms
step:232/2330 train_time:13414ms step_avg:57.82ms
step:233/2330 train_time:13470ms step_avg:57.81ms
step:234/2330 train_time:13530ms step_avg:57.82ms
step:235/2330 train_time:13586ms step_avg:57.81ms
step:236/2330 train_time:13647ms step_avg:57.82ms
step:237/2330 train_time:13703ms step_avg:57.82ms
step:238/2330 train_time:13763ms step_avg:57.83ms
step:239/2330 train_time:13819ms step_avg:57.82ms
step:240/2330 train_time:13878ms step_avg:57.83ms
step:241/2330 train_time:13934ms step_avg:57.82ms
step:242/2330 train_time:13995ms step_avg:57.83ms
step:243/2330 train_time:14050ms step_avg:57.82ms
step:244/2330 train_time:14111ms step_avg:57.83ms
step:245/2330 train_time:14166ms step_avg:57.82ms
step:246/2330 train_time:14228ms step_avg:57.84ms
step:247/2330 train_time:14283ms step_avg:57.83ms
step:248/2330 train_time:14343ms step_avg:57.84ms
step:249/2330 train_time:14399ms step_avg:57.83ms
step:250/2330 train_time:14459ms step_avg:57.84ms
step:250/2330 val_loss:5.5098 train_time:14536ms step_avg:58.14ms
step:251/2330 train_time:14555ms step_avg:57.99ms
step:252/2330 train_time:14577ms step_avg:57.85ms
step:253/2330 train_time:14634ms step_avg:57.84ms
step:254/2330 train_time:14695ms step_avg:57.86ms
step:255/2330 train_time:14752ms step_avg:57.85ms
step:256/2330 train_time:14813ms step_avg:57.86ms
step:257/2330 train_time:14868ms step_avg:57.85ms
step:258/2330 train_time:14930ms step_avg:57.87ms
step:259/2330 train_time:14985ms step_avg:57.86ms
step:260/2330 train_time:15045ms step_avg:57.87ms
step:261/2330 train_time:15101ms step_avg:57.86ms
step:262/2330 train_time:15161ms step_avg:57.87ms
step:263/2330 train_time:15216ms step_avg:57.86ms
step:264/2330 train_time:15277ms step_avg:57.87ms
step:265/2330 train_time:15332ms step_avg:57.86ms
step:266/2330 train_time:15392ms step_avg:57.86ms
step:267/2330 train_time:15447ms step_avg:57.86ms
step:268/2330 train_time:15507ms step_avg:57.86ms
step:269/2330 train_time:15563ms step_avg:57.85ms
step:270/2330 train_time:15624ms step_avg:57.87ms
step:271/2330 train_time:15679ms step_avg:57.86ms
step:272/2330 train_time:15742ms step_avg:57.88ms
step:273/2330 train_time:15798ms step_avg:57.87ms
step:274/2330 train_time:15860ms step_avg:57.88ms
step:275/2330 train_time:15915ms step_avg:57.87ms
step:276/2330 train_time:15976ms step_avg:57.88ms
step:277/2330 train_time:16032ms step_avg:57.88ms
step:278/2330 train_time:16092ms step_avg:57.89ms
step:279/2330 train_time:16148ms step_avg:57.88ms
step:280/2330 train_time:16208ms step_avg:57.88ms
step:281/2330 train_time:16263ms step_avg:57.88ms
step:282/2330 train_time:16324ms step_avg:57.89ms
step:283/2330 train_time:16380ms step_avg:57.88ms
step:284/2330 train_time:16441ms step_avg:57.89ms
step:285/2330 train_time:16496ms step_avg:57.88ms
step:286/2330 train_time:16556ms step_avg:57.89ms
step:287/2330 train_time:16613ms step_avg:57.88ms
step:288/2330 train_time:16672ms step_avg:57.89ms
step:289/2330 train_time:16728ms step_avg:57.88ms
step:290/2330 train_time:16788ms step_avg:57.89ms
step:291/2330 train_time:16843ms step_avg:57.88ms
step:292/2330 train_time:16904ms step_avg:57.89ms
step:293/2330 train_time:16960ms step_avg:57.88ms
step:294/2330 train_time:17022ms step_avg:57.90ms
step:295/2330 train_time:17077ms step_avg:57.89ms
step:296/2330 train_time:17138ms step_avg:57.90ms
step:297/2330 train_time:17194ms step_avg:57.89ms
step:298/2330 train_time:17254ms step_avg:57.90ms
step:299/2330 train_time:17311ms step_avg:57.89ms
step:300/2330 train_time:17369ms step_avg:57.90ms
step:301/2330 train_time:17425ms step_avg:57.89ms
step:302/2330 train_time:17485ms step_avg:57.90ms
step:303/2330 train_time:17540ms step_avg:57.89ms
step:304/2330 train_time:17600ms step_avg:57.90ms
step:305/2330 train_time:17656ms step_avg:57.89ms
step:306/2330 train_time:17717ms step_avg:57.90ms
step:307/2330 train_time:17772ms step_avg:57.89ms
step:308/2330 train_time:17832ms step_avg:57.90ms
step:309/2330 train_time:17888ms step_avg:57.89ms
step:310/2330 train_time:17949ms step_avg:57.90ms
step:311/2330 train_time:18005ms step_avg:57.89ms
step:312/2330 train_time:18065ms step_avg:57.90ms
step:313/2330 train_time:18120ms step_avg:57.89ms
step:314/2330 train_time:18181ms step_avg:57.90ms
step:315/2330 train_time:18237ms step_avg:57.89ms
step:316/2330 train_time:18299ms step_avg:57.91ms
step:317/2330 train_time:18355ms step_avg:57.90ms
step:318/2330 train_time:18414ms step_avg:57.91ms
step:319/2330 train_time:18469ms step_avg:57.90ms
step:320/2330 train_time:18529ms step_avg:57.90ms
step:321/2330 train_time:18584ms step_avg:57.90ms
step:322/2330 train_time:18645ms step_avg:57.90ms
step:323/2330 train_time:18700ms step_avg:57.89ms
step:324/2330 train_time:18762ms step_avg:57.91ms
step:325/2330 train_time:18818ms step_avg:57.90ms
step:326/2330 train_time:18879ms step_avg:57.91ms
step:327/2330 train_time:18934ms step_avg:57.90ms
step:328/2330 train_time:18995ms step_avg:57.91ms
step:329/2330 train_time:19050ms step_avg:57.90ms
step:330/2330 train_time:19110ms step_avg:57.91ms
step:331/2330 train_time:19166ms step_avg:57.90ms
step:332/2330 train_time:19226ms step_avg:57.91ms
step:333/2330 train_time:19281ms step_avg:57.90ms
step:334/2330 train_time:19343ms step_avg:57.91ms
step:335/2330 train_time:19398ms step_avg:57.91ms
step:336/2330 train_time:19459ms step_avg:57.91ms
step:337/2330 train_time:19515ms step_avg:57.91ms
step:338/2330 train_time:19575ms step_avg:57.92ms
step:339/2330 train_time:19631ms step_avg:57.91ms
step:340/2330 train_time:19691ms step_avg:57.92ms
step:341/2330 train_time:19747ms step_avg:57.91ms
step:342/2330 train_time:19807ms step_avg:57.92ms
step:343/2330 train_time:19863ms step_avg:57.91ms
step:344/2330 train_time:19924ms step_avg:57.92ms
step:345/2330 train_time:19979ms step_avg:57.91ms
step:346/2330 train_time:20040ms step_avg:57.92ms
step:347/2330 train_time:20095ms step_avg:57.91ms
step:348/2330 train_time:20157ms step_avg:57.92ms
step:349/2330 train_time:20213ms step_avg:57.92ms
step:350/2330 train_time:20273ms step_avg:57.92ms
step:351/2330 train_time:20329ms step_avg:57.92ms
step:352/2330 train_time:20389ms step_avg:57.92ms
step:353/2330 train_time:20445ms step_avg:57.92ms
step:354/2330 train_time:20505ms step_avg:57.92ms
step:355/2330 train_time:20560ms step_avg:57.92ms
step:356/2330 train_time:20621ms step_avg:57.93ms
step:357/2330 train_time:20677ms step_avg:57.92ms
step:358/2330 train_time:20738ms step_avg:57.93ms
step:359/2330 train_time:20794ms step_avg:57.92ms
step:360/2330 train_time:20854ms step_avg:57.93ms
step:361/2330 train_time:20909ms step_avg:57.92ms
step:362/2330 train_time:20969ms step_avg:57.93ms
step:363/2330 train_time:21025ms step_avg:57.92ms
step:364/2330 train_time:21086ms step_avg:57.93ms
step:365/2330 train_time:21142ms step_avg:57.92ms
step:366/2330 train_time:21203ms step_avg:57.93ms
step:367/2330 train_time:21258ms step_avg:57.92ms
step:368/2330 train_time:21320ms step_avg:57.93ms
step:369/2330 train_time:21376ms step_avg:57.93ms
step:370/2330 train_time:21436ms step_avg:57.93ms
step:371/2330 train_time:21492ms step_avg:57.93ms
step:372/2330 train_time:21552ms step_avg:57.94ms
step:373/2330 train_time:21607ms step_avg:57.93ms
step:374/2330 train_time:21668ms step_avg:57.94ms
step:375/2330 train_time:21723ms step_avg:57.93ms
step:376/2330 train_time:21784ms step_avg:57.94ms
step:377/2330 train_time:21840ms step_avg:57.93ms
step:378/2330 train_time:21903ms step_avg:57.94ms
step:379/2330 train_time:21958ms step_avg:57.94ms
step:380/2330 train_time:22020ms step_avg:57.95ms
step:381/2330 train_time:22075ms step_avg:57.94ms
step:382/2330 train_time:22137ms step_avg:57.95ms
step:383/2330 train_time:22193ms step_avg:57.94ms
step:384/2330 train_time:22253ms step_avg:57.95ms
step:385/2330 train_time:22309ms step_avg:57.95ms
step:386/2330 train_time:22369ms step_avg:57.95ms
step:387/2330 train_time:22424ms step_avg:57.94ms
step:388/2330 train_time:22485ms step_avg:57.95ms
step:389/2330 train_time:22540ms step_avg:57.94ms
step:390/2330 train_time:22602ms step_avg:57.95ms
step:391/2330 train_time:22657ms step_avg:57.95ms
step:392/2330 train_time:22719ms step_avg:57.96ms
step:393/2330 train_time:22775ms step_avg:57.95ms
step:394/2330 train_time:22835ms step_avg:57.96ms
step:395/2330 train_time:22890ms step_avg:57.95ms
step:396/2330 train_time:22950ms step_avg:57.95ms
step:397/2330 train_time:23006ms step_avg:57.95ms
step:398/2330 train_time:23066ms step_avg:57.95ms
step:399/2330 train_time:23121ms step_avg:57.95ms
step:400/2330 train_time:23182ms step_avg:57.96ms
step:401/2330 train_time:23238ms step_avg:57.95ms
step:402/2330 train_time:23299ms step_avg:57.96ms
step:403/2330 train_time:23355ms step_avg:57.95ms
step:404/2330 train_time:23415ms step_avg:57.96ms
step:405/2330 train_time:23471ms step_avg:57.95ms
step:406/2330 train_time:23530ms step_avg:57.96ms
step:407/2330 train_time:23586ms step_avg:57.95ms
step:408/2330 train_time:23645ms step_avg:57.95ms
step:409/2330 train_time:23700ms step_avg:57.95ms
step:410/2330 train_time:23761ms step_avg:57.95ms
step:411/2330 train_time:23817ms step_avg:57.95ms
step:412/2330 train_time:23878ms step_avg:57.96ms
step:413/2330 train_time:23934ms step_avg:57.95ms
step:414/2330 train_time:23993ms step_avg:57.95ms
step:415/2330 train_time:24049ms step_avg:57.95ms
step:416/2330 train_time:24109ms step_avg:57.95ms
step:417/2330 train_time:24165ms step_avg:57.95ms
step:418/2330 train_time:24225ms step_avg:57.96ms
step:419/2330 train_time:24281ms step_avg:57.95ms
step:420/2330 train_time:24342ms step_avg:57.96ms
step:421/2330 train_time:24398ms step_avg:57.95ms
step:422/2330 train_time:24459ms step_avg:57.96ms
step:423/2330 train_time:24515ms step_avg:57.95ms
step:424/2330 train_time:24576ms step_avg:57.96ms
step:425/2330 train_time:24631ms step_avg:57.96ms
step:426/2330 train_time:24692ms step_avg:57.96ms
step:427/2330 train_time:24748ms step_avg:57.96ms
step:428/2330 train_time:24807ms step_avg:57.96ms
step:429/2330 train_time:24863ms step_avg:57.95ms
step:430/2330 train_time:24924ms step_avg:57.96ms
step:431/2330 train_time:24979ms step_avg:57.96ms
step:432/2330 train_time:25040ms step_avg:57.96ms
step:433/2330 train_time:25096ms step_avg:57.96ms
step:434/2330 train_time:25157ms step_avg:57.97ms
step:435/2330 train_time:25213ms step_avg:57.96ms
step:436/2330 train_time:25274ms step_avg:57.97ms
step:437/2330 train_time:25330ms step_avg:57.96ms
step:438/2330 train_time:25390ms step_avg:57.97ms
step:439/2330 train_time:25445ms step_avg:57.96ms
step:440/2330 train_time:25507ms step_avg:57.97ms
step:441/2330 train_time:25562ms step_avg:57.96ms
step:442/2330 train_time:25623ms step_avg:57.97ms
step:443/2330 train_time:25679ms step_avg:57.97ms
step:444/2330 train_time:25741ms step_avg:57.97ms
step:445/2330 train_time:25797ms step_avg:57.97ms
step:446/2330 train_time:25858ms step_avg:57.98ms
step:447/2330 train_time:25914ms step_avg:57.97ms
step:448/2330 train_time:25974ms step_avg:57.98ms
step:449/2330 train_time:26029ms step_avg:57.97ms
step:450/2330 train_time:26089ms step_avg:57.98ms
step:451/2330 train_time:26145ms step_avg:57.97ms
step:452/2330 train_time:26205ms step_avg:57.98ms
step:453/2330 train_time:26261ms step_avg:57.97ms
step:454/2330 train_time:26321ms step_avg:57.98ms
step:455/2330 train_time:26377ms step_avg:57.97ms
step:456/2330 train_time:26439ms step_avg:57.98ms
step:457/2330 train_time:26494ms step_avg:57.97ms
step:458/2330 train_time:26554ms step_avg:57.98ms
step:459/2330 train_time:26610ms step_avg:57.97ms
step:460/2330 train_time:26670ms step_avg:57.98ms
step:461/2330 train_time:26726ms step_avg:57.97ms
step:462/2330 train_time:26786ms step_avg:57.98ms
step:463/2330 train_time:26841ms step_avg:57.97ms
step:464/2330 train_time:26902ms step_avg:57.98ms
step:465/2330 train_time:26958ms step_avg:57.97ms
step:466/2330 train_time:27020ms step_avg:57.98ms
step:467/2330 train_time:27075ms step_avg:57.98ms
step:468/2330 train_time:27136ms step_avg:57.98ms
step:469/2330 train_time:27192ms step_avg:57.98ms
step:470/2330 train_time:27252ms step_avg:57.98ms
step:471/2330 train_time:27307ms step_avg:57.98ms
step:472/2330 train_time:27367ms step_avg:57.98ms
step:473/2330 train_time:27423ms step_avg:57.98ms
step:474/2330 train_time:27483ms step_avg:57.98ms
step:475/2330 train_time:27539ms step_avg:57.98ms
step:476/2330 train_time:27600ms step_avg:57.98ms
step:477/2330 train_time:27656ms step_avg:57.98ms
step:478/2330 train_time:27717ms step_avg:57.98ms
step:479/2330 train_time:27772ms step_avg:57.98ms
step:480/2330 train_time:27832ms step_avg:57.98ms
step:481/2330 train_time:27888ms step_avg:57.98ms
step:482/2330 train_time:27949ms step_avg:57.99ms
step:483/2330 train_time:28005ms step_avg:57.98ms
step:484/2330 train_time:28065ms step_avg:57.99ms
step:485/2330 train_time:28120ms step_avg:57.98ms
step:486/2330 train_time:28182ms step_avg:57.99ms
step:487/2330 train_time:28238ms step_avg:57.98ms
step:488/2330 train_time:28298ms step_avg:57.99ms
step:489/2330 train_time:28355ms step_avg:57.99ms
step:490/2330 train_time:28414ms step_avg:57.99ms
step:491/2330 train_time:28470ms step_avg:57.98ms
step:492/2330 train_time:28530ms step_avg:57.99ms
step:493/2330 train_time:28585ms step_avg:57.98ms
step:494/2330 train_time:28646ms step_avg:57.99ms
step:495/2330 train_time:28701ms step_avg:57.98ms
step:496/2330 train_time:28762ms step_avg:57.99ms
step:497/2330 train_time:28818ms step_avg:57.98ms
step:498/2330 train_time:28879ms step_avg:57.99ms
step:499/2330 train_time:28935ms step_avg:57.99ms
step:500/2330 train_time:28996ms step_avg:57.99ms
step:500/2330 val_loss:4.7908 train_time:29073ms step_avg:58.15ms
step:501/2330 train_time:29093ms step_avg:58.07ms
step:502/2330 train_time:29114ms step_avg:58.00ms
step:503/2330 train_time:29170ms step_avg:57.99ms
step:504/2330 train_time:29234ms step_avg:58.00ms
step:505/2330 train_time:29290ms step_avg:58.00ms
step:506/2330 train_time:29354ms step_avg:58.01ms
step:507/2330 train_time:29409ms step_avg:58.01ms
step:508/2330 train_time:29471ms step_avg:58.01ms
step:509/2330 train_time:29527ms step_avg:58.01ms
step:510/2330 train_time:29588ms step_avg:58.02ms
step:511/2330 train_time:29643ms step_avg:58.01ms
step:512/2330 train_time:29703ms step_avg:58.01ms
step:513/2330 train_time:29759ms step_avg:58.01ms
step:514/2330 train_time:29819ms step_avg:58.01ms
step:515/2330 train_time:29874ms step_avg:58.01ms
step:516/2330 train_time:29934ms step_avg:58.01ms
step:517/2330 train_time:29989ms step_avg:58.01ms
step:518/2330 train_time:30050ms step_avg:58.01ms
step:519/2330 train_time:30105ms step_avg:58.01ms
step:520/2330 train_time:30167ms step_avg:58.01ms
step:521/2330 train_time:30223ms step_avg:58.01ms
step:522/2330 train_time:30286ms step_avg:58.02ms
step:523/2330 train_time:30342ms step_avg:58.02ms
step:524/2330 train_time:30403ms step_avg:58.02ms
step:525/2330 train_time:30459ms step_avg:58.02ms
step:526/2330 train_time:30520ms step_avg:58.02ms
step:527/2330 train_time:30575ms step_avg:58.02ms
step:528/2330 train_time:30636ms step_avg:58.02ms
step:529/2330 train_time:30691ms step_avg:58.02ms
step:530/2330 train_time:30751ms step_avg:58.02ms
step:531/2330 train_time:30807ms step_avg:58.02ms
step:532/2330 train_time:30866ms step_avg:58.02ms
step:533/2330 train_time:30922ms step_avg:58.02ms
step:534/2330 train_time:30982ms step_avg:58.02ms
step:535/2330 train_time:31038ms step_avg:58.02ms
step:536/2330 train_time:31099ms step_avg:58.02ms
step:537/2330 train_time:31154ms step_avg:58.02ms
step:538/2330 train_time:31215ms step_avg:58.02ms
step:539/2330 train_time:31270ms step_avg:58.02ms
step:540/2330 train_time:31331ms step_avg:58.02ms
step:541/2330 train_time:31387ms step_avg:58.02ms
step:542/2330 train_time:31448ms step_avg:58.02ms
step:543/2330 train_time:31504ms step_avg:58.02ms
step:544/2330 train_time:31566ms step_avg:58.03ms
step:545/2330 train_time:31621ms step_avg:58.02ms
step:546/2330 train_time:31683ms step_avg:58.03ms
step:547/2330 train_time:31739ms step_avg:58.02ms
step:548/2330 train_time:31799ms step_avg:58.03ms
step:549/2330 train_time:31854ms step_avg:58.02ms
step:550/2330 train_time:31914ms step_avg:58.03ms
step:551/2330 train_time:31969ms step_avg:58.02ms
step:552/2330 train_time:32029ms step_avg:58.02ms
step:553/2330 train_time:32084ms step_avg:58.02ms
step:554/2330 train_time:32146ms step_avg:58.02ms
step:555/2330 train_time:32202ms step_avg:58.02ms
step:556/2330 train_time:32262ms step_avg:58.03ms
step:557/2330 train_time:32318ms step_avg:58.02ms
step:558/2330 train_time:32379ms step_avg:58.03ms
step:559/2330 train_time:32435ms step_avg:58.02ms
step:560/2330 train_time:32495ms step_avg:58.03ms
step:561/2330 train_time:32551ms step_avg:58.02ms
step:562/2330 train_time:32612ms step_avg:58.03ms
step:563/2330 train_time:32668ms step_avg:58.02ms
step:564/2330 train_time:32729ms step_avg:58.03ms
step:565/2330 train_time:32784ms step_avg:58.03ms
step:566/2330 train_time:32845ms step_avg:58.03ms
step:567/2330 train_time:32901ms step_avg:58.03ms
step:568/2330 train_time:32960ms step_avg:58.03ms
step:569/2330 train_time:33016ms step_avg:58.03ms
step:570/2330 train_time:33077ms step_avg:58.03ms
step:571/2330 train_time:33133ms step_avg:58.03ms
step:572/2330 train_time:33193ms step_avg:58.03ms
step:573/2330 train_time:33249ms step_avg:58.03ms
step:574/2330 train_time:33309ms step_avg:58.03ms
step:575/2330 train_time:33365ms step_avg:58.03ms
step:576/2330 train_time:33427ms step_avg:58.03ms
step:577/2330 train_time:33482ms step_avg:58.03ms
step:578/2330 train_time:33544ms step_avg:58.03ms
step:579/2330 train_time:33600ms step_avg:58.03ms
step:580/2330 train_time:33661ms step_avg:58.04ms
step:581/2330 train_time:33716ms step_avg:58.03ms
step:582/2330 train_time:33777ms step_avg:58.04ms
step:583/2330 train_time:33832ms step_avg:58.03ms
step:584/2330 train_time:33892ms step_avg:58.03ms
step:585/2330 train_time:33948ms step_avg:58.03ms
step:586/2330 train_time:34009ms step_avg:58.04ms
step:587/2330 train_time:34064ms step_avg:58.03ms
step:588/2330 train_time:34126ms step_avg:58.04ms
step:589/2330 train_time:34182ms step_avg:58.03ms
step:590/2330 train_time:34243ms step_avg:58.04ms
step:591/2330 train_time:34298ms step_avg:58.03ms
step:592/2330 train_time:34359ms step_avg:58.04ms
step:593/2330 train_time:34415ms step_avg:58.03ms
step:594/2330 train_time:34475ms step_avg:58.04ms
step:595/2330 train_time:34531ms step_avg:58.03ms
step:596/2330 train_time:34590ms step_avg:58.04ms
step:597/2330 train_time:34646ms step_avg:58.03ms
step:598/2330 train_time:34709ms step_avg:58.04ms
step:599/2330 train_time:34764ms step_avg:58.04ms
step:600/2330 train_time:34825ms step_avg:58.04ms
step:601/2330 train_time:34881ms step_avg:58.04ms
step:602/2330 train_time:34941ms step_avg:58.04ms
step:603/2330 train_time:34997ms step_avg:58.04ms
step:604/2330 train_time:35057ms step_avg:58.04ms
step:605/2330 train_time:35113ms step_avg:58.04ms
step:606/2330 train_time:35173ms step_avg:58.04ms
step:607/2330 train_time:35229ms step_avg:58.04ms
step:608/2330 train_time:35289ms step_avg:58.04ms
step:609/2330 train_time:35345ms step_avg:58.04ms
step:610/2330 train_time:35407ms step_avg:58.04ms
step:611/2330 train_time:35462ms step_avg:58.04ms
step:612/2330 train_time:35524ms step_avg:58.05ms
step:613/2330 train_time:35579ms step_avg:58.04ms
step:614/2330 train_time:35640ms step_avg:58.05ms
step:615/2330 train_time:35696ms step_avg:58.04ms
step:616/2330 train_time:35758ms step_avg:58.05ms
step:617/2330 train_time:35813ms step_avg:58.04ms
step:618/2330 train_time:35873ms step_avg:58.05ms
step:619/2330 train_time:35929ms step_avg:58.04ms
step:620/2330 train_time:35989ms step_avg:58.05ms
step:621/2330 train_time:36045ms step_avg:58.04ms
step:622/2330 train_time:36106ms step_avg:58.05ms
step:623/2330 train_time:36162ms step_avg:58.05ms
step:624/2330 train_time:36222ms step_avg:58.05ms
step:625/2330 train_time:36279ms step_avg:58.05ms
step:626/2330 train_time:36339ms step_avg:58.05ms
step:627/2330 train_time:36394ms step_avg:58.05ms
step:628/2330 train_time:36454ms step_avg:58.05ms
step:629/2330 train_time:36510ms step_avg:58.04ms
step:630/2330 train_time:36571ms step_avg:58.05ms
step:631/2330 train_time:36627ms step_avg:58.05ms
step:632/2330 train_time:36688ms step_avg:58.05ms
step:633/2330 train_time:36744ms step_avg:58.05ms
step:634/2330 train_time:36805ms step_avg:58.05ms
step:635/2330 train_time:36862ms step_avg:58.05ms
step:636/2330 train_time:36922ms step_avg:58.05ms
step:637/2330 train_time:36978ms step_avg:58.05ms
step:638/2330 train_time:37038ms step_avg:58.05ms
step:639/2330 train_time:37093ms step_avg:58.05ms
step:640/2330 train_time:37154ms step_avg:58.05ms
step:641/2330 train_time:37209ms step_avg:58.05ms
step:642/2330 train_time:37270ms step_avg:58.05ms
step:643/2330 train_time:37325ms step_avg:58.05ms
step:644/2330 train_time:37387ms step_avg:58.05ms
step:645/2330 train_time:37443ms step_avg:58.05ms
step:646/2330 train_time:37505ms step_avg:58.06ms
step:647/2330 train_time:37561ms step_avg:58.05ms
step:648/2330 train_time:37621ms step_avg:58.06ms
step:649/2330 train_time:37677ms step_avg:58.05ms
step:650/2330 train_time:37737ms step_avg:58.06ms
step:651/2330 train_time:37793ms step_avg:58.05ms
step:652/2330 train_time:37854ms step_avg:58.06ms
step:653/2330 train_time:37909ms step_avg:58.05ms
step:654/2330 train_time:37971ms step_avg:58.06ms
step:655/2330 train_time:38026ms step_avg:58.05ms
step:656/2330 train_time:38087ms step_avg:58.06ms
step:657/2330 train_time:38143ms step_avg:58.06ms
step:658/2330 train_time:38204ms step_avg:58.06ms
step:659/2330 train_time:38259ms step_avg:58.06ms
step:660/2330 train_time:38321ms step_avg:58.06ms
step:661/2330 train_time:38376ms step_avg:58.06ms
step:662/2330 train_time:38436ms step_avg:58.06ms
step:663/2330 train_time:38492ms step_avg:58.06ms
step:664/2330 train_time:38552ms step_avg:58.06ms
step:665/2330 train_time:38608ms step_avg:58.06ms
step:666/2330 train_time:38669ms step_avg:58.06ms
step:667/2330 train_time:38725ms step_avg:58.06ms
step:668/2330 train_time:38787ms step_avg:58.06ms
step:669/2330 train_time:38843ms step_avg:58.06ms
step:670/2330 train_time:38904ms step_avg:58.07ms
step:671/2330 train_time:38960ms step_avg:58.06ms
step:672/2330 train_time:39020ms step_avg:58.07ms
step:673/2330 train_time:39076ms step_avg:58.06ms
step:674/2330 train_time:39137ms step_avg:58.07ms
step:675/2330 train_time:39193ms step_avg:58.06ms
step:676/2330 train_time:39253ms step_avg:58.07ms
step:677/2330 train_time:39308ms step_avg:58.06ms
step:678/2330 train_time:39369ms step_avg:58.07ms
step:679/2330 train_time:39424ms step_avg:58.06ms
step:680/2330 train_time:39485ms step_avg:58.07ms
step:681/2330 train_time:39542ms step_avg:58.06ms
step:682/2330 train_time:39601ms step_avg:58.07ms
step:683/2330 train_time:39658ms step_avg:58.06ms
step:684/2330 train_time:39718ms step_avg:58.07ms
step:685/2330 train_time:39774ms step_avg:58.06ms
step:686/2330 train_time:39835ms step_avg:58.07ms
step:687/2330 train_time:39891ms step_avg:58.07ms
step:688/2330 train_time:39951ms step_avg:58.07ms
step:689/2330 train_time:40006ms step_avg:58.06ms
step:690/2330 train_time:40068ms step_avg:58.07ms
step:691/2330 train_time:40124ms step_avg:58.07ms
step:692/2330 train_time:40184ms step_avg:58.07ms
step:693/2330 train_time:40240ms step_avg:58.07ms
step:694/2330 train_time:40299ms step_avg:58.07ms
step:695/2330 train_time:40355ms step_avg:58.06ms
step:696/2330 train_time:40415ms step_avg:58.07ms
step:697/2330 train_time:40470ms step_avg:58.06ms
step:698/2330 train_time:40531ms step_avg:58.07ms
step:699/2330 train_time:40587ms step_avg:58.06ms
step:700/2330 train_time:40648ms step_avg:58.07ms
step:701/2330 train_time:40704ms step_avg:58.06ms
step:702/2330 train_time:40766ms step_avg:58.07ms
step:703/2330 train_time:40822ms step_avg:58.07ms
step:704/2330 train_time:40884ms step_avg:58.07ms
step:705/2330 train_time:40940ms step_avg:58.07ms
step:706/2330 train_time:41001ms step_avg:58.07ms
step:707/2330 train_time:41056ms step_avg:58.07ms
step:708/2330 train_time:41117ms step_avg:58.07ms
step:709/2330 train_time:41172ms step_avg:58.07ms
step:710/2330 train_time:41232ms step_avg:58.07ms
step:711/2330 train_time:41288ms step_avg:58.07ms
step:712/2330 train_time:41349ms step_avg:58.07ms
step:713/2330 train_time:41405ms step_avg:58.07ms
step:714/2330 train_time:41467ms step_avg:58.08ms
step:715/2330 train_time:41522ms step_avg:58.07ms
step:716/2330 train_time:41583ms step_avg:58.08ms
step:717/2330 train_time:41639ms step_avg:58.07ms
step:718/2330 train_time:41699ms step_avg:58.08ms
step:719/2330 train_time:41755ms step_avg:58.07ms
step:720/2330 train_time:41816ms step_avg:58.08ms
step:721/2330 train_time:41871ms step_avg:58.07ms
step:722/2330 train_time:41933ms step_avg:58.08ms
step:723/2330 train_time:41988ms step_avg:58.07ms
step:724/2330 train_time:42049ms step_avg:58.08ms
step:725/2330 train_time:42104ms step_avg:58.07ms
step:726/2330 train_time:42167ms step_avg:58.08ms
step:727/2330 train_time:42222ms step_avg:58.08ms
step:728/2330 train_time:42285ms step_avg:58.08ms
step:729/2330 train_time:42340ms step_avg:58.08ms
step:730/2330 train_time:42401ms step_avg:58.08ms
step:731/2330 train_time:42456ms step_avg:58.08ms
step:732/2330 train_time:42517ms step_avg:58.08ms
step:733/2330 train_time:42573ms step_avg:58.08ms
step:734/2330 train_time:42633ms step_avg:58.08ms
step:735/2330 train_time:42688ms step_avg:58.08ms
step:736/2330 train_time:42749ms step_avg:58.08ms
step:737/2330 train_time:42805ms step_avg:58.08ms
step:738/2330 train_time:42867ms step_avg:58.08ms
step:739/2330 train_time:42922ms step_avg:58.08ms
step:740/2330 train_time:42983ms step_avg:58.09ms
step:741/2330 train_time:43040ms step_avg:58.08ms
step:742/2330 train_time:43099ms step_avg:58.09ms
step:743/2330 train_time:43155ms step_avg:58.08ms
step:744/2330 train_time:43215ms step_avg:58.08ms
step:745/2330 train_time:43270ms step_avg:58.08ms
step:746/2330 train_time:43332ms step_avg:58.09ms
step:747/2330 train_time:43387ms step_avg:58.08ms
step:748/2330 train_time:43448ms step_avg:58.09ms
step:749/2330 train_time:43503ms step_avg:58.08ms
step:750/2330 train_time:43565ms step_avg:58.09ms
step:750/2330 val_loss:4.5191 train_time:43642ms step_avg:58.19ms
step:751/2330 train_time:43663ms step_avg:58.14ms
step:752/2330 train_time:43685ms step_avg:58.09ms
step:753/2330 train_time:43739ms step_avg:58.09ms
step:754/2330 train_time:43803ms step_avg:58.09ms
step:755/2330 train_time:43858ms step_avg:58.09ms
step:756/2330 train_time:43921ms step_avg:58.10ms
step:757/2330 train_time:43976ms step_avg:58.09ms
step:758/2330 train_time:44037ms step_avg:58.10ms
step:759/2330 train_time:44093ms step_avg:58.09ms
step:760/2330 train_time:44153ms step_avg:58.10ms
step:761/2330 train_time:44209ms step_avg:58.09ms
step:762/2330 train_time:44269ms step_avg:58.10ms
step:763/2330 train_time:44324ms step_avg:58.09ms
step:764/2330 train_time:44385ms step_avg:58.10ms
step:765/2330 train_time:44441ms step_avg:58.09ms
step:766/2330 train_time:44501ms step_avg:58.10ms
step:767/2330 train_time:44557ms step_avg:58.09ms
step:768/2330 train_time:44618ms step_avg:58.10ms
step:769/2330 train_time:44674ms step_avg:58.09ms
step:770/2330 train_time:44737ms step_avg:58.10ms
step:771/2330 train_time:44793ms step_avg:58.10ms
step:772/2330 train_time:44857ms step_avg:58.10ms
step:773/2330 train_time:44913ms step_avg:58.10ms
step:774/2330 train_time:44975ms step_avg:58.11ms
step:775/2330 train_time:45032ms step_avg:58.11ms
step:776/2330 train_time:45093ms step_avg:58.11ms
step:777/2330 train_time:45150ms step_avg:58.11ms
step:778/2330 train_time:45210ms step_avg:58.11ms
step:779/2330 train_time:45267ms step_avg:58.11ms
step:780/2330 train_time:45327ms step_avg:58.11ms
step:781/2330 train_time:45384ms step_avg:58.11ms
step:782/2330 train_time:45445ms step_avg:58.11ms
step:783/2330 train_time:45501ms step_avg:58.11ms
step:784/2330 train_time:45562ms step_avg:58.11ms
step:785/2330 train_time:45618ms step_avg:58.11ms
step:786/2330 train_time:45680ms step_avg:58.12ms
step:787/2330 train_time:45736ms step_avg:58.11ms
step:788/2330 train_time:45799ms step_avg:58.12ms
step:789/2330 train_time:45855ms step_avg:58.12ms
step:790/2330 train_time:45918ms step_avg:58.12ms
step:791/2330 train_time:45974ms step_avg:58.12ms
step:792/2330 train_time:46037ms step_avg:58.13ms
step:793/2330 train_time:46093ms step_avg:58.12ms
step:794/2330 train_time:46155ms step_avg:58.13ms
step:795/2330 train_time:46211ms step_avg:58.13ms
step:796/2330 train_time:46274ms step_avg:58.13ms
step:797/2330 train_time:46330ms step_avg:58.13ms
step:798/2330 train_time:46391ms step_avg:58.13ms
step:799/2330 train_time:46448ms step_avg:58.13ms
step:800/2330 train_time:46510ms step_avg:58.14ms
step:801/2330 train_time:46567ms step_avg:58.14ms
step:802/2330 train_time:46627ms step_avg:58.14ms
step:803/2330 train_time:46684ms step_avg:58.14ms
step:804/2330 train_time:46745ms step_avg:58.14ms
step:805/2330 train_time:46801ms step_avg:58.14ms
step:806/2330 train_time:46863ms step_avg:58.14ms
step:807/2330 train_time:46919ms step_avg:58.14ms
step:808/2330 train_time:46981ms step_avg:58.15ms
step:809/2330 train_time:47037ms step_avg:58.14ms
step:810/2330 train_time:47100ms step_avg:58.15ms
step:811/2330 train_time:47156ms step_avg:58.15ms
step:812/2330 train_time:47219ms step_avg:58.15ms
step:813/2330 train_time:47275ms step_avg:58.15ms
step:814/2330 train_time:47338ms step_avg:58.15ms
step:815/2330 train_time:47394ms step_avg:58.15ms
step:816/2330 train_time:47456ms step_avg:58.16ms
step:817/2330 train_time:47513ms step_avg:58.15ms
step:818/2330 train_time:47574ms step_avg:58.16ms
step:819/2330 train_time:47631ms step_avg:58.16ms
step:820/2330 train_time:47692ms step_avg:58.16ms
step:821/2330 train_time:47749ms step_avg:58.16ms
step:822/2330 train_time:47810ms step_avg:58.16ms
step:823/2330 train_time:47867ms step_avg:58.16ms
step:824/2330 train_time:47927ms step_avg:58.16ms
step:825/2330 train_time:47983ms step_avg:58.16ms
step:826/2330 train_time:48045ms step_avg:58.17ms
step:827/2330 train_time:48101ms step_avg:58.16ms
step:828/2330 train_time:48163ms step_avg:58.17ms
step:829/2330 train_time:48219ms step_avg:58.16ms
step:830/2330 train_time:48282ms step_avg:58.17ms
step:831/2330 train_time:48338ms step_avg:58.17ms
step:832/2330 train_time:48400ms step_avg:58.17ms
step:833/2330 train_time:48456ms step_avg:58.17ms
step:834/2330 train_time:48518ms step_avg:58.17ms
step:835/2330 train_time:48574ms step_avg:58.17ms
step:836/2330 train_time:48637ms step_avg:58.18ms
step:837/2330 train_time:48693ms step_avg:58.18ms
step:838/2330 train_time:48755ms step_avg:58.18ms
step:839/2330 train_time:48812ms step_avg:58.18ms
step:840/2330 train_time:48874ms step_avg:58.18ms
step:841/2330 train_time:48930ms step_avg:58.18ms
step:842/2330 train_time:48992ms step_avg:58.19ms
step:843/2330 train_time:49050ms step_avg:58.18ms
step:844/2330 train_time:49111ms step_avg:58.19ms
step:845/2330 train_time:49168ms step_avg:58.19ms
step:846/2330 train_time:49228ms step_avg:58.19ms
step:847/2330 train_time:49285ms step_avg:58.19ms
step:848/2330 train_time:49346ms step_avg:58.19ms
step:849/2330 train_time:49402ms step_avg:58.19ms
step:850/2330 train_time:49463ms step_avg:58.19ms
step:851/2330 train_time:49519ms step_avg:58.19ms
step:852/2330 train_time:49581ms step_avg:58.19ms
step:853/2330 train_time:49637ms step_avg:58.19ms
step:854/2330 train_time:49700ms step_avg:58.20ms
step:855/2330 train_time:49756ms step_avg:58.19ms
step:856/2330 train_time:49818ms step_avg:58.20ms
step:857/2330 train_time:49874ms step_avg:58.20ms
step:858/2330 train_time:49937ms step_avg:58.20ms
step:859/2330 train_time:49993ms step_avg:58.20ms
step:860/2330 train_time:50055ms step_avg:58.20ms
step:861/2330 train_time:50112ms step_avg:58.20ms
step:862/2330 train_time:50174ms step_avg:58.21ms
step:863/2330 train_time:50230ms step_avg:58.20ms
step:864/2330 train_time:50293ms step_avg:58.21ms
step:865/2330 train_time:50350ms step_avg:58.21ms
step:866/2330 train_time:50411ms step_avg:58.21ms
step:867/2330 train_time:50467ms step_avg:58.21ms
step:868/2330 train_time:50528ms step_avg:58.21ms
step:869/2330 train_time:50585ms step_avg:58.21ms
step:870/2330 train_time:50646ms step_avg:58.21ms
step:871/2330 train_time:50702ms step_avg:58.21ms
step:872/2330 train_time:50763ms step_avg:58.21ms
step:873/2330 train_time:50819ms step_avg:58.21ms
step:874/2330 train_time:50882ms step_avg:58.22ms
step:875/2330 train_time:50938ms step_avg:58.21ms
step:876/2330 train_time:51000ms step_avg:58.22ms
step:877/2330 train_time:51056ms step_avg:58.22ms
step:878/2330 train_time:51119ms step_avg:58.22ms
step:879/2330 train_time:51175ms step_avg:58.22ms
step:880/2330 train_time:51237ms step_avg:58.22ms
step:881/2330 train_time:51294ms step_avg:58.22ms
step:882/2330 train_time:51356ms step_avg:58.23ms
step:883/2330 train_time:51413ms step_avg:58.22ms
step:884/2330 train_time:51474ms step_avg:58.23ms
step:885/2330 train_time:51531ms step_avg:58.23ms
step:886/2330 train_time:51593ms step_avg:58.23ms
step:887/2330 train_time:51650ms step_avg:58.23ms
step:888/2330 train_time:51711ms step_avg:58.23ms
step:889/2330 train_time:51768ms step_avg:58.23ms
step:890/2330 train_time:51829ms step_avg:58.23ms
step:891/2330 train_time:51885ms step_avg:58.23ms
step:892/2330 train_time:51946ms step_avg:58.24ms
step:893/2330 train_time:52002ms step_avg:58.23ms
step:894/2330 train_time:52064ms step_avg:58.24ms
step:895/2330 train_time:52120ms step_avg:58.23ms
step:896/2330 train_time:52183ms step_avg:58.24ms
step:897/2330 train_time:52239ms step_avg:58.24ms
step:898/2330 train_time:52301ms step_avg:58.24ms
step:899/2330 train_time:52357ms step_avg:58.24ms
step:900/2330 train_time:52420ms step_avg:58.24ms
step:901/2330 train_time:52476ms step_avg:58.24ms
step:902/2330 train_time:52537ms step_avg:58.25ms
step:903/2330 train_time:52593ms step_avg:58.24ms
step:904/2330 train_time:52656ms step_avg:58.25ms
step:905/2330 train_time:52712ms step_avg:58.25ms
step:906/2330 train_time:52774ms step_avg:58.25ms
step:907/2330 train_time:52832ms step_avg:58.25ms
step:908/2330 train_time:52894ms step_avg:58.25ms
step:909/2330 train_time:52951ms step_avg:58.25ms
step:910/2330 train_time:53012ms step_avg:58.25ms
step:911/2330 train_time:53069ms step_avg:58.25ms
step:912/2330 train_time:53129ms step_avg:58.26ms
step:913/2330 train_time:53186ms step_avg:58.25ms
step:914/2330 train_time:53246ms step_avg:58.26ms
step:915/2330 train_time:53303ms step_avg:58.25ms
step:916/2330 train_time:53364ms step_avg:58.26ms
step:917/2330 train_time:53420ms step_avg:58.26ms
step:918/2330 train_time:53483ms step_avg:58.26ms
step:919/2330 train_time:53538ms step_avg:58.26ms
step:920/2330 train_time:53601ms step_avg:58.26ms
step:921/2330 train_time:53657ms step_avg:58.26ms
step:922/2330 train_time:53720ms step_avg:58.26ms
step:923/2330 train_time:53776ms step_avg:58.26ms
step:924/2330 train_time:53839ms step_avg:58.27ms
step:925/2330 train_time:53895ms step_avg:58.27ms
step:926/2330 train_time:53958ms step_avg:58.27ms
step:927/2330 train_time:54015ms step_avg:58.27ms
step:928/2330 train_time:54077ms step_avg:58.27ms
step:929/2330 train_time:54133ms step_avg:58.27ms
step:930/2330 train_time:54196ms step_avg:58.27ms
step:931/2330 train_time:54252ms step_avg:58.27ms
step:932/2330 train_time:54314ms step_avg:58.28ms
step:933/2330 train_time:54371ms step_avg:58.28ms
step:934/2330 train_time:54431ms step_avg:58.28ms
step:935/2330 train_time:54489ms step_avg:58.28ms
step:936/2330 train_time:54549ms step_avg:58.28ms
step:937/2330 train_time:54607ms step_avg:58.28ms
step:938/2330 train_time:54667ms step_avg:58.28ms
step:939/2330 train_time:54723ms step_avg:58.28ms
step:940/2330 train_time:54784ms step_avg:58.28ms
step:941/2330 train_time:54841ms step_avg:58.28ms
step:942/2330 train_time:54902ms step_avg:58.28ms
step:943/2330 train_time:54958ms step_avg:58.28ms
step:944/2330 train_time:55021ms step_avg:58.28ms
step:945/2330 train_time:55077ms step_avg:58.28ms
step:946/2330 train_time:55140ms step_avg:58.29ms
step:947/2330 train_time:55196ms step_avg:58.28ms
step:948/2330 train_time:55258ms step_avg:58.29ms
step:949/2330 train_time:55314ms step_avg:58.29ms
step:950/2330 train_time:55376ms step_avg:58.29ms
step:951/2330 train_time:55432ms step_avg:58.29ms
step:952/2330 train_time:55494ms step_avg:58.29ms
step:953/2330 train_time:55551ms step_avg:58.29ms
step:954/2330 train_time:55613ms step_avg:58.29ms
step:955/2330 train_time:55670ms step_avg:58.29ms
step:956/2330 train_time:55732ms step_avg:58.30ms
step:957/2330 train_time:55789ms step_avg:58.30ms
step:958/2330 train_time:55850ms step_avg:58.30ms
step:959/2330 train_time:55907ms step_avg:58.30ms
step:960/2330 train_time:55968ms step_avg:58.30ms
step:961/2330 train_time:56024ms step_avg:58.30ms
step:962/2330 train_time:56085ms step_avg:58.30ms
step:963/2330 train_time:56141ms step_avg:58.30ms
step:964/2330 train_time:56203ms step_avg:58.30ms
step:965/2330 train_time:56260ms step_avg:58.30ms
step:966/2330 train_time:56321ms step_avg:58.30ms
step:967/2330 train_time:56377ms step_avg:58.30ms
step:968/2330 train_time:56439ms step_avg:58.30ms
step:969/2330 train_time:56495ms step_avg:58.30ms
step:970/2330 train_time:56558ms step_avg:58.31ms
step:971/2330 train_time:56614ms step_avg:58.31ms
step:972/2330 train_time:56676ms step_avg:58.31ms
step:973/2330 train_time:56733ms step_avg:58.31ms
step:974/2330 train_time:56796ms step_avg:58.31ms
step:975/2330 train_time:56853ms step_avg:58.31ms
step:976/2330 train_time:56915ms step_avg:58.31ms
step:977/2330 train_time:56972ms step_avg:58.31ms
step:978/2330 train_time:57034ms step_avg:58.32ms
step:979/2330 train_time:57091ms step_avg:58.32ms
step:980/2330 train_time:57151ms step_avg:58.32ms
step:981/2330 train_time:57209ms step_avg:58.32ms
step:982/2330 train_time:57269ms step_avg:58.32ms
step:983/2330 train_time:57326ms step_avg:58.32ms
step:984/2330 train_time:57386ms step_avg:58.32ms
step:985/2330 train_time:57443ms step_avg:58.32ms
step:986/2330 train_time:57503ms step_avg:58.32ms
step:987/2330 train_time:57560ms step_avg:58.32ms
step:988/2330 train_time:57622ms step_avg:58.32ms
step:989/2330 train_time:57678ms step_avg:58.32ms
step:990/2330 train_time:57741ms step_avg:58.32ms
step:991/2330 train_time:57797ms step_avg:58.32ms
step:992/2330 train_time:57859ms step_avg:58.33ms
step:993/2330 train_time:57915ms step_avg:58.32ms
step:994/2330 train_time:57978ms step_avg:58.33ms
step:995/2330 train_time:58035ms step_avg:58.33ms
step:996/2330 train_time:58096ms step_avg:58.33ms
step:997/2330 train_time:58153ms step_avg:58.33ms
step:998/2330 train_time:58215ms step_avg:58.33ms
step:999/2330 train_time:58272ms step_avg:58.33ms
step:1000/2330 train_time:58333ms step_avg:58.33ms
step:1000/2330 val_loss:4.3539 train_time:58411ms step_avg:58.41ms
step:1001/2330 train_time:58432ms step_avg:58.37ms
step:1002/2330 train_time:58453ms step_avg:58.34ms
step:1003/2330 train_time:58508ms step_avg:58.33ms
step:1004/2330 train_time:58573ms step_avg:58.34ms
step:1005/2330 train_time:58629ms step_avg:58.34ms
step:1006/2330 train_time:58695ms step_avg:58.34ms
step:1007/2330 train_time:58751ms step_avg:58.34ms
step:1008/2330 train_time:58812ms step_avg:58.35ms
step:1009/2330 train_time:58868ms step_avg:58.34ms
step:1010/2330 train_time:58929ms step_avg:58.35ms
step:1011/2330 train_time:58985ms step_avg:58.34ms
step:1012/2330 train_time:59046ms step_avg:58.35ms
step:1013/2330 train_time:59102ms step_avg:58.34ms
step:1014/2330 train_time:59162ms step_avg:58.35ms
step:1015/2330 train_time:59218ms step_avg:58.34ms
step:1016/2330 train_time:59278ms step_avg:58.34ms
step:1017/2330 train_time:59335ms step_avg:58.34ms
step:1018/2330 train_time:59400ms step_avg:58.35ms
step:1019/2330 train_time:59456ms step_avg:58.35ms
step:1020/2330 train_time:59521ms step_avg:58.35ms
step:1021/2330 train_time:59576ms step_avg:58.35ms
step:1022/2330 train_time:59640ms step_avg:58.36ms
step:1023/2330 train_time:59696ms step_avg:58.35ms
step:1024/2330 train_time:59758ms step_avg:58.36ms
step:1025/2330 train_time:59814ms step_avg:58.35ms
step:1026/2330 train_time:59875ms step_avg:58.36ms
step:1027/2330 train_time:59931ms step_avg:58.36ms
step:1028/2330 train_time:59993ms step_avg:58.36ms
step:1029/2330 train_time:60049ms step_avg:58.36ms
step:1030/2330 train_time:60110ms step_avg:58.36ms
step:1031/2330 train_time:60167ms step_avg:58.36ms
step:1032/2330 train_time:60227ms step_avg:58.36ms
step:1033/2330 train_time:60284ms step_avg:58.36ms
step:1034/2330 train_time:60345ms step_avg:58.36ms
step:1035/2330 train_time:60402ms step_avg:58.36ms
step:1036/2330 train_time:60464ms step_avg:58.36ms
step:1037/2330 train_time:60520ms step_avg:58.36ms
step:1038/2330 train_time:60581ms step_avg:58.36ms
step:1039/2330 train_time:60638ms step_avg:58.36ms
step:1040/2330 train_time:60699ms step_avg:58.36ms
step:1041/2330 train_time:60755ms step_avg:58.36ms
step:1042/2330 train_time:60817ms step_avg:58.37ms
step:1043/2330 train_time:60873ms step_avg:58.36ms
step:1044/2330 train_time:60935ms step_avg:58.37ms
step:1045/2330 train_time:60991ms step_avg:58.36ms
step:1046/2330 train_time:61052ms step_avg:58.37ms
step:1047/2330 train_time:61109ms step_avg:58.37ms
step:1048/2330 train_time:61171ms step_avg:58.37ms
step:1049/2330 train_time:61227ms step_avg:58.37ms
step:1050/2330 train_time:61289ms step_avg:58.37ms
step:1051/2330 train_time:61345ms step_avg:58.37ms
step:1052/2330 train_time:61408ms step_avg:58.37ms
step:1053/2330 train_time:61464ms step_avg:58.37ms
step:1054/2330 train_time:61526ms step_avg:58.37ms
step:1055/2330 train_time:61582ms step_avg:58.37ms
step:1056/2330 train_time:61643ms step_avg:58.37ms
step:1057/2330 train_time:61699ms step_avg:58.37ms
step:1058/2330 train_time:61760ms step_avg:58.37ms
step:1059/2330 train_time:61816ms step_avg:58.37ms
step:1060/2330 train_time:61878ms step_avg:58.38ms
step:1061/2330 train_time:61934ms step_avg:58.37ms
step:1062/2330 train_time:61996ms step_avg:58.38ms
step:1063/2330 train_time:62052ms step_avg:58.37ms
step:1064/2330 train_time:62113ms step_avg:58.38ms
step:1065/2330 train_time:62170ms step_avg:58.38ms
step:1066/2330 train_time:62231ms step_avg:58.38ms
step:1067/2330 train_time:62287ms step_avg:58.38ms
step:1068/2330 train_time:62349ms step_avg:58.38ms
step:1069/2330 train_time:62406ms step_avg:58.38ms
step:1070/2330 train_time:62469ms step_avg:58.38ms
step:1071/2330 train_time:62525ms step_avg:58.38ms
step:1072/2330 train_time:62587ms step_avg:58.38ms
step:1073/2330 train_time:62644ms step_avg:58.38ms
step:1074/2330 train_time:62706ms step_avg:58.39ms
step:1075/2330 train_time:62763ms step_avg:58.38ms
step:1076/2330 train_time:62823ms step_avg:58.39ms
step:1077/2330 train_time:62879ms step_avg:58.38ms
step:1078/2330 train_time:62940ms step_avg:58.39ms
step:1079/2330 train_time:62995ms step_avg:58.38ms
step:1080/2330 train_time:63057ms step_avg:58.39ms
step:1081/2330 train_time:63113ms step_avg:58.38ms
step:1082/2330 train_time:63175ms step_avg:58.39ms
step:1083/2330 train_time:63231ms step_avg:58.39ms
step:1084/2330 train_time:63294ms step_avg:58.39ms
step:1085/2330 train_time:63351ms step_avg:58.39ms
step:1086/2330 train_time:63414ms step_avg:58.39ms
step:1087/2330 train_time:63470ms step_avg:58.39ms
step:1088/2330 train_time:63532ms step_avg:58.39ms
step:1089/2330 train_time:63588ms step_avg:58.39ms
step:1090/2330 train_time:63652ms step_avg:58.40ms
step:1091/2330 train_time:63708ms step_avg:58.39ms
step:1092/2330 train_time:63771ms step_avg:58.40ms
step:1093/2330 train_time:63828ms step_avg:58.40ms
step:1094/2330 train_time:63890ms step_avg:58.40ms
step:1095/2330 train_time:63947ms step_avg:58.40ms
step:1096/2330 train_time:64007ms step_avg:58.40ms
step:1097/2330 train_time:64064ms step_avg:58.40ms
step:1098/2330 train_time:64125ms step_avg:58.40ms
step:1099/2330 train_time:64181ms step_avg:58.40ms
step:1100/2330 train_time:64241ms step_avg:58.40ms
step:1101/2330 train_time:64298ms step_avg:58.40ms
step:1102/2330 train_time:64359ms step_avg:58.40ms
step:1103/2330 train_time:64416ms step_avg:58.40ms
step:1104/2330 train_time:64477ms step_avg:58.40ms
step:1105/2330 train_time:64533ms step_avg:58.40ms
step:1106/2330 train_time:64596ms step_avg:58.41ms
step:1107/2330 train_time:64652ms step_avg:58.40ms
step:1108/2330 train_time:64715ms step_avg:58.41ms
step:1109/2330 train_time:64771ms step_avg:58.40ms
step:1110/2330 train_time:64834ms step_avg:58.41ms
step:1111/2330 train_time:64890ms step_avg:58.41ms
step:1112/2330 train_time:64953ms step_avg:58.41ms
step:1113/2330 train_time:65010ms step_avg:58.41ms
step:1114/2330 train_time:65072ms step_avg:58.41ms
step:1115/2330 train_time:65128ms step_avg:58.41ms
step:1116/2330 train_time:65190ms step_avg:58.41ms
step:1117/2330 train_time:65247ms step_avg:58.41ms
step:1118/2330 train_time:65307ms step_avg:58.41ms
step:1119/2330 train_time:65364ms step_avg:58.41ms
step:1120/2330 train_time:65425ms step_avg:58.42ms
step:1121/2330 train_time:65482ms step_avg:58.41ms
step:1122/2330 train_time:65542ms step_avg:58.42ms
step:1123/2330 train_time:65598ms step_avg:58.41ms
step:1124/2330 train_time:65661ms step_avg:58.42ms
step:1125/2330 train_time:65717ms step_avg:58.42ms
step:1126/2330 train_time:65779ms step_avg:58.42ms
step:1127/2330 train_time:65835ms step_avg:58.42ms
step:1128/2330 train_time:65897ms step_avg:58.42ms
step:1129/2330 train_time:65953ms step_avg:58.42ms
step:1130/2330 train_time:66016ms step_avg:58.42ms
step:1131/2330 train_time:66071ms step_avg:58.42ms
step:1132/2330 train_time:66134ms step_avg:58.42ms
step:1133/2330 train_time:66190ms step_avg:58.42ms
step:1134/2330 train_time:66252ms step_avg:58.42ms
step:1135/2330 train_time:66309ms step_avg:58.42ms
step:1136/2330 train_time:66372ms step_avg:58.43ms
step:1137/2330 train_time:66428ms step_avg:58.42ms
step:1138/2330 train_time:66491ms step_avg:58.43ms
step:1139/2330 train_time:66548ms step_avg:58.43ms
step:1140/2330 train_time:66610ms step_avg:58.43ms
step:1141/2330 train_time:66667ms step_avg:58.43ms
step:1142/2330 train_time:66728ms step_avg:58.43ms
step:1143/2330 train_time:66785ms step_avg:58.43ms
step:1144/2330 train_time:66846ms step_avg:58.43ms
step:1145/2330 train_time:66902ms step_avg:58.43ms
step:1146/2330 train_time:66964ms step_avg:58.43ms
step:1147/2330 train_time:67019ms step_avg:58.43ms
step:1148/2330 train_time:67082ms step_avg:58.43ms
step:1149/2330 train_time:67138ms step_avg:58.43ms
step:1150/2330 train_time:67199ms step_avg:58.43ms
step:1151/2330 train_time:67255ms step_avg:58.43ms
step:1152/2330 train_time:67318ms step_avg:58.44ms
step:1153/2330 train_time:67374ms step_avg:58.43ms
step:1154/2330 train_time:67437ms step_avg:58.44ms
step:1155/2330 train_time:67492ms step_avg:58.43ms
step:1156/2330 train_time:67555ms step_avg:58.44ms
step:1157/2330 train_time:67612ms step_avg:58.44ms
step:1158/2330 train_time:67674ms step_avg:58.44ms
step:1159/2330 train_time:67730ms step_avg:58.44ms
step:1160/2330 train_time:67793ms step_avg:58.44ms
step:1161/2330 train_time:67849ms step_avg:58.44ms
step:1162/2330 train_time:67912ms step_avg:58.44ms
step:1163/2330 train_time:67969ms step_avg:58.44ms
step:1164/2330 train_time:68031ms step_avg:58.45ms
step:1165/2330 train_time:68088ms step_avg:58.44ms
step:1166/2330 train_time:68148ms step_avg:58.45ms
step:1167/2330 train_time:68205ms step_avg:58.45ms
step:1168/2330 train_time:68266ms step_avg:58.45ms
step:1169/2330 train_time:68323ms step_avg:58.45ms
step:1170/2330 train_time:68384ms step_avg:58.45ms
step:1171/2330 train_time:68440ms step_avg:58.45ms
step:1172/2330 train_time:68500ms step_avg:58.45ms
step:1173/2330 train_time:68556ms step_avg:58.45ms
step:1174/2330 train_time:68618ms step_avg:58.45ms
step:1175/2330 train_time:68675ms step_avg:58.45ms
step:1176/2330 train_time:68737ms step_avg:58.45ms
step:1177/2330 train_time:68793ms step_avg:58.45ms
step:1178/2330 train_time:68855ms step_avg:58.45ms
step:1179/2330 train_time:68911ms step_avg:58.45ms
step:1180/2330 train_time:68974ms step_avg:58.45ms
step:1181/2330 train_time:69030ms step_avg:58.45ms
step:1182/2330 train_time:69093ms step_avg:58.45ms
step:1183/2330 train_time:69150ms step_avg:58.45ms
step:1184/2330 train_time:69212ms step_avg:58.46ms
step:1185/2330 train_time:69268ms step_avg:58.45ms
step:1186/2330 train_time:69330ms step_avg:58.46ms
step:1187/2330 train_time:69387ms step_avg:58.46ms
step:1188/2330 train_time:69448ms step_avg:58.46ms
step:1189/2330 train_time:69505ms step_avg:58.46ms
step:1190/2330 train_time:69566ms step_avg:58.46ms
step:1191/2330 train_time:69622ms step_avg:58.46ms
step:1192/2330 train_time:69684ms step_avg:58.46ms
step:1193/2330 train_time:69740ms step_avg:58.46ms
step:1194/2330 train_time:69801ms step_avg:58.46ms
step:1195/2330 train_time:69857ms step_avg:58.46ms
step:1196/2330 train_time:69918ms step_avg:58.46ms
step:1197/2330 train_time:69974ms step_avg:58.46ms
step:1198/2330 train_time:70036ms step_avg:58.46ms
step:1199/2330 train_time:70092ms step_avg:58.46ms
step:1200/2330 train_time:70154ms step_avg:58.46ms
step:1201/2330 train_time:70211ms step_avg:58.46ms
step:1202/2330 train_time:70273ms step_avg:58.46ms
step:1203/2330 train_time:70329ms step_avg:58.46ms
step:1204/2330 train_time:70391ms step_avg:58.46ms
step:1205/2330 train_time:70448ms step_avg:58.46ms
step:1206/2330 train_time:70509ms step_avg:58.47ms
step:1207/2330 train_time:70566ms step_avg:58.46ms
step:1208/2330 train_time:70628ms step_avg:58.47ms
step:1209/2330 train_time:70684ms step_avg:58.47ms
step:1210/2330 train_time:70745ms step_avg:58.47ms
step:1211/2330 train_time:70802ms step_avg:58.47ms
step:1212/2330 train_time:70863ms step_avg:58.47ms
step:1213/2330 train_time:70918ms step_avg:58.47ms
step:1214/2330 train_time:70980ms step_avg:58.47ms
step:1215/2330 train_time:71036ms step_avg:58.47ms
step:1216/2330 train_time:71098ms step_avg:58.47ms
step:1217/2330 train_time:71153ms step_avg:58.47ms
step:1218/2330 train_time:71216ms step_avg:58.47ms
step:1219/2330 train_time:71272ms step_avg:58.47ms
step:1220/2330 train_time:71334ms step_avg:58.47ms
step:1221/2330 train_time:71391ms step_avg:58.47ms
step:1222/2330 train_time:71452ms step_avg:58.47ms
step:1223/2330 train_time:71508ms step_avg:58.47ms
step:1224/2330 train_time:71572ms step_avg:58.47ms
step:1225/2330 train_time:71628ms step_avg:58.47ms
step:1226/2330 train_time:71690ms step_avg:58.48ms
step:1227/2330 train_time:71748ms step_avg:58.47ms
step:1228/2330 train_time:71810ms step_avg:58.48ms
step:1229/2330 train_time:71867ms step_avg:58.48ms
step:1230/2330 train_time:71928ms step_avg:58.48ms
step:1231/2330 train_time:71985ms step_avg:58.48ms
step:1232/2330 train_time:72046ms step_avg:58.48ms
step:1233/2330 train_time:72103ms step_avg:58.48ms
step:1234/2330 train_time:72163ms step_avg:58.48ms
step:1235/2330 train_time:72219ms step_avg:58.48ms
step:1236/2330 train_time:72281ms step_avg:58.48ms
step:1237/2330 train_time:72337ms step_avg:58.48ms
step:1238/2330 train_time:72399ms step_avg:58.48ms
step:1239/2330 train_time:72456ms step_avg:58.48ms
step:1240/2330 train_time:72518ms step_avg:58.48ms
step:1241/2330 train_time:72573ms step_avg:58.48ms
step:1242/2330 train_time:72636ms step_avg:58.48ms
step:1243/2330 train_time:72692ms step_avg:58.48ms
step:1244/2330 train_time:72754ms step_avg:58.48ms
step:1245/2330 train_time:72810ms step_avg:58.48ms
step:1246/2330 train_time:72874ms step_avg:58.49ms
step:1247/2330 train_time:72930ms step_avg:58.48ms
step:1248/2330 train_time:72992ms step_avg:58.49ms
step:1249/2330 train_time:73048ms step_avg:58.49ms
step:1250/2330 train_time:73110ms step_avg:58.49ms
step:1250/2330 val_loss:4.2582 train_time:73188ms step_avg:58.55ms
step:1251/2330 train_time:73208ms step_avg:58.52ms
step:1252/2330 train_time:73231ms step_avg:58.49ms
step:1253/2330 train_time:73289ms step_avg:58.49ms
step:1254/2330 train_time:73354ms step_avg:58.50ms
step:1255/2330 train_time:73411ms step_avg:58.49ms
step:1256/2330 train_time:73474ms step_avg:58.50ms
step:1257/2330 train_time:73530ms step_avg:58.50ms
step:1258/2330 train_time:73592ms step_avg:58.50ms
step:1259/2330 train_time:73648ms step_avg:58.50ms
step:1260/2330 train_time:73709ms step_avg:58.50ms
step:1261/2330 train_time:73766ms step_avg:58.50ms
step:1262/2330 train_time:73826ms step_avg:58.50ms
step:1263/2330 train_time:73882ms step_avg:58.50ms
step:1264/2330 train_time:73943ms step_avg:58.50ms
step:1265/2330 train_time:73999ms step_avg:58.50ms
step:1266/2330 train_time:74059ms step_avg:58.50ms
step:1267/2330 train_time:74115ms step_avg:58.50ms
step:1268/2330 train_time:74176ms step_avg:58.50ms
step:1269/2330 train_time:74233ms step_avg:58.50ms
step:1270/2330 train_time:74296ms step_avg:58.50ms
step:1271/2330 train_time:74353ms step_avg:58.50ms
step:1272/2330 train_time:74415ms step_avg:58.50ms
step:1273/2330 train_time:74472ms step_avg:58.50ms
step:1274/2330 train_time:74535ms step_avg:58.50ms
step:1275/2330 train_time:74590ms step_avg:58.50ms
step:1276/2330 train_time:74653ms step_avg:58.51ms
step:1277/2330 train_time:74709ms step_avg:58.50ms
step:1278/2330 train_time:74771ms step_avg:58.51ms
step:1279/2330 train_time:74828ms step_avg:58.51ms
step:1280/2330 train_time:74889ms step_avg:58.51ms
step:1281/2330 train_time:74946ms step_avg:58.51ms
step:1282/2330 train_time:75006ms step_avg:58.51ms
step:1283/2330 train_time:75063ms step_avg:58.51ms
step:1284/2330 train_time:75123ms step_avg:58.51ms
step:1285/2330 train_time:75179ms step_avg:58.51ms
step:1286/2330 train_time:75240ms step_avg:58.51ms
step:1287/2330 train_time:75297ms step_avg:58.51ms
step:1288/2330 train_time:75359ms step_avg:58.51ms
step:1289/2330 train_time:75415ms step_avg:58.51ms
step:1290/2330 train_time:75477ms step_avg:58.51ms
step:1291/2330 train_time:75533ms step_avg:58.51ms
step:1292/2330 train_time:75596ms step_avg:58.51ms
step:1293/2330 train_time:75652ms step_avg:58.51ms
step:1294/2330 train_time:75714ms step_avg:58.51ms
step:1295/2330 train_time:75770ms step_avg:58.51ms
step:1296/2330 train_time:75833ms step_avg:58.51ms
step:1297/2330 train_time:75889ms step_avg:58.51ms
step:1298/2330 train_time:75952ms step_avg:58.51ms
step:1299/2330 train_time:76008ms step_avg:58.51ms
step:1300/2330 train_time:76070ms step_avg:58.52ms
step:1301/2330 train_time:76127ms step_avg:58.51ms
step:1302/2330 train_time:76187ms step_avg:58.52ms
step:1303/2330 train_time:76244ms step_avg:58.51ms
step:1304/2330 train_time:76305ms step_avg:58.52ms
step:1305/2330 train_time:76362ms step_avg:58.51ms
step:1306/2330 train_time:76423ms step_avg:58.52ms
step:1307/2330 train_time:76479ms step_avg:58.52ms
step:1308/2330 train_time:76541ms step_avg:58.52ms
step:1309/2330 train_time:76598ms step_avg:58.52ms
step:1310/2330 train_time:76660ms step_avg:58.52ms
step:1311/2330 train_time:76716ms step_avg:58.52ms
step:1312/2330 train_time:76778ms step_avg:58.52ms
step:1313/2330 train_time:76834ms step_avg:58.52ms
step:1314/2330 train_time:76896ms step_avg:58.52ms
step:1315/2330 train_time:76952ms step_avg:58.52ms
step:1316/2330 train_time:77014ms step_avg:58.52ms
step:1317/2330 train_time:77070ms step_avg:58.52ms
step:1318/2330 train_time:77132ms step_avg:58.52ms
step:1319/2330 train_time:77188ms step_avg:58.52ms
step:1320/2330 train_time:77250ms step_avg:58.52ms
step:1321/2330 train_time:77307ms step_avg:58.52ms
step:1322/2330 train_time:77369ms step_avg:58.52ms
step:1323/2330 train_time:77425ms step_avg:58.52ms
step:1324/2330 train_time:77487ms step_avg:58.52ms
step:1325/2330 train_time:77543ms step_avg:58.52ms
step:1326/2330 train_time:77604ms step_avg:58.53ms
step:1327/2330 train_time:77661ms step_avg:58.52ms
step:1328/2330 train_time:77722ms step_avg:58.53ms
step:1329/2330 train_time:77778ms step_avg:58.52ms
step:1330/2330 train_time:77839ms step_avg:58.53ms
step:1331/2330 train_time:77895ms step_avg:58.52ms
step:1332/2330 train_time:77958ms step_avg:58.53ms
step:1333/2330 train_time:78013ms step_avg:58.52ms
step:1334/2330 train_time:78076ms step_avg:58.53ms
step:1335/2330 train_time:78132ms step_avg:58.53ms
step:1336/2330 train_time:78195ms step_avg:58.53ms
step:1337/2330 train_time:78251ms step_avg:58.53ms
step:1338/2330 train_time:78314ms step_avg:58.53ms
step:1339/2330 train_time:78370ms step_avg:58.53ms
step:1340/2330 train_time:78432ms step_avg:58.53ms
step:1341/2330 train_time:78489ms step_avg:58.53ms
step:1342/2330 train_time:78552ms step_avg:58.53ms
step:1343/2330 train_time:78608ms step_avg:58.53ms
step:1344/2330 train_time:78671ms step_avg:58.53ms
step:1345/2330 train_time:78727ms step_avg:58.53ms
step:1346/2330 train_time:78788ms step_avg:58.53ms
step:1347/2330 train_time:78844ms step_avg:58.53ms
step:1348/2330 train_time:78905ms step_avg:58.54ms
step:1349/2330 train_time:78961ms step_avg:58.53ms
step:1350/2330 train_time:79023ms step_avg:58.54ms
step:1351/2330 train_time:79079ms step_avg:58.53ms
step:1352/2330 train_time:79140ms step_avg:58.54ms
step:1353/2330 train_time:79196ms step_avg:58.53ms
step:1354/2330 train_time:79258ms step_avg:58.54ms
step:1355/2330 train_time:79314ms step_avg:58.53ms
step:1356/2330 train_time:79376ms step_avg:58.54ms
step:1357/2330 train_time:79432ms step_avg:58.54ms
step:1358/2330 train_time:79495ms step_avg:58.54ms
step:1359/2330 train_time:79551ms step_avg:58.54ms
step:1360/2330 train_time:79614ms step_avg:58.54ms
step:1361/2330 train_time:79670ms step_avg:58.54ms
step:1362/2330 train_time:79732ms step_avg:58.54ms
step:1363/2330 train_time:79789ms step_avg:58.54ms
step:1364/2330 train_time:79850ms step_avg:58.54ms
step:1365/2330 train_time:79906ms step_avg:58.54ms
step:1366/2330 train_time:79968ms step_avg:58.54ms
step:1367/2330 train_time:80025ms step_avg:58.54ms
step:1368/2330 train_time:80086ms step_avg:58.54ms
step:1369/2330 train_time:80143ms step_avg:58.54ms
step:1370/2330 train_time:80204ms step_avg:58.54ms
step:1371/2330 train_time:80260ms step_avg:58.54ms
step:1372/2330 train_time:80321ms step_avg:58.54ms
step:1373/2330 train_time:80377ms step_avg:58.54ms
step:1374/2330 train_time:80439ms step_avg:58.54ms
step:1375/2330 train_time:80495ms step_avg:58.54ms
step:1376/2330 train_time:80557ms step_avg:58.54ms
step:1377/2330 train_time:80613ms step_avg:58.54ms
step:1378/2330 train_time:80676ms step_avg:58.55ms
step:1379/2330 train_time:80732ms step_avg:58.54ms
step:1380/2330 train_time:80795ms step_avg:58.55ms
step:1381/2330 train_time:80851ms step_avg:58.55ms
step:1382/2330 train_time:80913ms step_avg:58.55ms
step:1383/2330 train_time:80969ms step_avg:58.55ms
step:1384/2330 train_time:81032ms step_avg:58.55ms
step:1385/2330 train_time:81089ms step_avg:58.55ms
step:1386/2330 train_time:81151ms step_avg:58.55ms
step:1387/2330 train_time:81207ms step_avg:58.55ms
step:1388/2330 train_time:81269ms step_avg:58.55ms
step:1389/2330 train_time:81327ms step_avg:58.55ms
step:1390/2330 train_time:81387ms step_avg:58.55ms
step:1391/2330 train_time:81444ms step_avg:58.55ms
step:1392/2330 train_time:81505ms step_avg:58.55ms
step:1393/2330 train_time:81561ms step_avg:58.55ms
step:1394/2330 train_time:81623ms step_avg:58.55ms
step:1395/2330 train_time:81678ms step_avg:58.55ms
step:1396/2330 train_time:81740ms step_avg:58.55ms
step:1397/2330 train_time:81796ms step_avg:58.55ms
step:1398/2330 train_time:81858ms step_avg:58.55ms
step:1399/2330 train_time:81914ms step_avg:58.55ms
step:1400/2330 train_time:81976ms step_avg:58.55ms
step:1401/2330 train_time:82032ms step_avg:58.55ms
step:1402/2330 train_time:82094ms step_avg:58.56ms
step:1403/2330 train_time:82151ms step_avg:58.55ms
step:1404/2330 train_time:82213ms step_avg:58.56ms
step:1405/2330 train_time:82269ms step_avg:58.55ms
step:1406/2330 train_time:82331ms step_avg:58.56ms
step:1407/2330 train_time:82388ms step_avg:58.56ms
step:1408/2330 train_time:82449ms step_avg:58.56ms
step:1409/2330 train_time:82506ms step_avg:58.56ms
step:1410/2330 train_time:82568ms step_avg:58.56ms
step:1411/2330 train_time:82626ms step_avg:58.56ms
step:1412/2330 train_time:82687ms step_avg:58.56ms
step:1413/2330 train_time:82744ms step_avg:58.56ms
step:1414/2330 train_time:82804ms step_avg:58.56ms
step:1415/2330 train_time:82860ms step_avg:58.56ms
step:1416/2330 train_time:82921ms step_avg:58.56ms
step:1417/2330 train_time:82977ms step_avg:58.56ms
step:1418/2330 train_time:83039ms step_avg:58.56ms
step:1419/2330 train_time:83095ms step_avg:58.56ms
step:1420/2330 train_time:83157ms step_avg:58.56ms
step:1421/2330 train_time:83213ms step_avg:58.56ms
step:1422/2330 train_time:83275ms step_avg:58.56ms
step:1423/2330 train_time:83331ms step_avg:58.56ms
step:1424/2330 train_time:83393ms step_avg:58.56ms
step:1425/2330 train_time:83449ms step_avg:58.56ms
step:1426/2330 train_time:83512ms step_avg:58.56ms
step:1427/2330 train_time:83569ms step_avg:58.56ms
step:1428/2330 train_time:83630ms step_avg:58.56ms
step:1429/2330 train_time:83687ms step_avg:58.56ms
step:1430/2330 train_time:83749ms step_avg:58.57ms
step:1431/2330 train_time:83805ms step_avg:58.56ms
step:1432/2330 train_time:83866ms step_avg:58.57ms
step:1433/2330 train_time:83922ms step_avg:58.56ms
step:1434/2330 train_time:83983ms step_avg:58.57ms
step:1435/2330 train_time:84039ms step_avg:58.56ms
step:1436/2330 train_time:84101ms step_avg:58.57ms
step:1437/2330 train_time:84157ms step_avg:58.56ms
step:1438/2330 train_time:84219ms step_avg:58.57ms
step:1439/2330 train_time:84274ms step_avg:58.56ms
step:1440/2330 train_time:84337ms step_avg:58.57ms
step:1441/2330 train_time:84393ms step_avg:58.57ms
step:1442/2330 train_time:84455ms step_avg:58.57ms
step:1443/2330 train_time:84511ms step_avg:58.57ms
step:1444/2330 train_time:84573ms step_avg:58.57ms
step:1445/2330 train_time:84630ms step_avg:58.57ms
step:1446/2330 train_time:84692ms step_avg:58.57ms
step:1447/2330 train_time:84749ms step_avg:58.57ms
step:1448/2330 train_time:84810ms step_avg:58.57ms
step:1449/2330 train_time:84867ms step_avg:58.57ms
step:1450/2330 train_time:84929ms step_avg:58.57ms
step:1451/2330 train_time:84986ms step_avg:58.57ms
step:1452/2330 train_time:85048ms step_avg:58.57ms
step:1453/2330 train_time:85105ms step_avg:58.57ms
step:1454/2330 train_time:85165ms step_avg:58.57ms
step:1455/2330 train_time:85222ms step_avg:58.57ms
step:1456/2330 train_time:85283ms step_avg:58.57ms
step:1457/2330 train_time:85339ms step_avg:58.57ms
step:1458/2330 train_time:85401ms step_avg:58.57ms
step:1459/2330 train_time:85457ms step_avg:58.57ms
step:1460/2330 train_time:85519ms step_avg:58.57ms
step:1461/2330 train_time:85575ms step_avg:58.57ms
step:1462/2330 train_time:85637ms step_avg:58.57ms
step:1463/2330 train_time:85692ms step_avg:58.57ms
step:1464/2330 train_time:85755ms step_avg:58.58ms
step:1465/2330 train_time:85811ms step_avg:58.57ms
step:1466/2330 train_time:85873ms step_avg:58.58ms
step:1467/2330 train_time:85930ms step_avg:58.58ms
step:1468/2330 train_time:85992ms step_avg:58.58ms
step:1469/2330 train_time:86048ms step_avg:58.58ms
step:1470/2330 train_time:86110ms step_avg:58.58ms
step:1471/2330 train_time:86167ms step_avg:58.58ms
step:1472/2330 train_time:86228ms step_avg:58.58ms
step:1473/2330 train_time:86286ms step_avg:58.58ms
step:1474/2330 train_time:86346ms step_avg:58.58ms
step:1475/2330 train_time:86403ms step_avg:58.58ms
step:1476/2330 train_time:86464ms step_avg:58.58ms
step:1477/2330 train_time:86520ms step_avg:58.58ms
step:1478/2330 train_time:86581ms step_avg:58.58ms
step:1479/2330 train_time:86637ms step_avg:58.58ms
step:1480/2330 train_time:86698ms step_avg:58.58ms
step:1481/2330 train_time:86754ms step_avg:58.58ms
step:1482/2330 train_time:86816ms step_avg:58.58ms
step:1483/2330 train_time:86873ms step_avg:58.58ms
step:1484/2330 train_time:86935ms step_avg:58.58ms
step:1485/2330 train_time:86991ms step_avg:58.58ms
step:1486/2330 train_time:87054ms step_avg:58.58ms
step:1487/2330 train_time:87110ms step_avg:58.58ms
step:1488/2330 train_time:87172ms step_avg:58.58ms
step:1489/2330 train_time:87229ms step_avg:58.58ms
step:1490/2330 train_time:87291ms step_avg:58.58ms
step:1491/2330 train_time:87348ms step_avg:58.58ms
step:1492/2330 train_time:87408ms step_avg:58.58ms
step:1493/2330 train_time:87465ms step_avg:58.58ms
step:1494/2330 train_time:87526ms step_avg:58.59ms
step:1495/2330 train_time:87583ms step_avg:58.58ms
step:1496/2330 train_time:87644ms step_avg:58.59ms
step:1497/2330 train_time:87700ms step_avg:58.58ms
step:1498/2330 train_time:87761ms step_avg:58.59ms
step:1499/2330 train_time:87817ms step_avg:58.58ms
step:1500/2330 train_time:87879ms step_avg:58.59ms
step:1500/2330 val_loss:4.1715 train_time:87957ms step_avg:58.64ms
step:1501/2330 train_time:87977ms step_avg:58.61ms
step:1502/2330 train_time:88002ms step_avg:58.59ms
step:1503/2330 train_time:88058ms step_avg:58.59ms
step:1504/2330 train_time:88122ms step_avg:58.59ms
step:1505/2330 train_time:88179ms step_avg:58.59ms
step:1506/2330 train_time:88241ms step_avg:58.59ms
step:1507/2330 train_time:88297ms step_avg:58.59ms
step:1508/2330 train_time:88358ms step_avg:58.59ms
step:1509/2330 train_time:88414ms step_avg:58.59ms
step:1510/2330 train_time:88475ms step_avg:58.59ms
step:1511/2330 train_time:88531ms step_avg:58.59ms
step:1512/2330 train_time:88592ms step_avg:58.59ms
step:1513/2330 train_time:88648ms step_avg:58.59ms
step:1514/2330 train_time:88708ms step_avg:58.59ms
step:1515/2330 train_time:88765ms step_avg:58.59ms
step:1516/2330 train_time:88825ms step_avg:58.59ms
step:1517/2330 train_time:88881ms step_avg:58.59ms
step:1518/2330 train_time:88942ms step_avg:58.59ms
step:1519/2330 train_time:88999ms step_avg:58.59ms
step:1520/2330 train_time:89060ms step_avg:58.59ms
step:1521/2330 train_time:89117ms step_avg:58.59ms
step:1522/2330 train_time:89179ms step_avg:58.59ms
step:1523/2330 train_time:89236ms step_avg:58.59ms
step:1524/2330 train_time:89297ms step_avg:58.59ms
step:1525/2330 train_time:89353ms step_avg:58.59ms
step:1526/2330 train_time:89415ms step_avg:58.59ms
step:1527/2330 train_time:89471ms step_avg:58.59ms
step:1528/2330 train_time:89532ms step_avg:58.59ms
step:1529/2330 train_time:89589ms step_avg:58.59ms
step:1530/2330 train_time:89650ms step_avg:58.60ms
step:1531/2330 train_time:89707ms step_avg:58.59ms
step:1532/2330 train_time:89769ms step_avg:58.60ms
step:1533/2330 train_time:89825ms step_avg:58.59ms
step:1534/2330 train_time:89888ms step_avg:58.60ms
step:1535/2330 train_time:89945ms step_avg:58.60ms
step:1536/2330 train_time:90008ms step_avg:58.60ms
step:1537/2330 train_time:90065ms step_avg:58.60ms
step:1538/2330 train_time:90129ms step_avg:58.60ms
step:1539/2330 train_time:90187ms step_avg:58.60ms
step:1540/2330 train_time:90251ms step_avg:58.60ms
step:1541/2330 train_time:90308ms step_avg:58.60ms
step:1542/2330 train_time:90371ms step_avg:58.61ms
step:1543/2330 train_time:90428ms step_avg:58.61ms
step:1544/2330 train_time:90490ms step_avg:58.61ms
step:1545/2330 train_time:90546ms step_avg:58.61ms
step:1546/2330 train_time:90608ms step_avg:58.61ms
step:1547/2330 train_time:90665ms step_avg:58.61ms
step:1548/2330 train_time:90726ms step_avg:58.61ms
step:1549/2330 train_time:90784ms step_avg:58.61ms
step:1550/2330 train_time:90845ms step_avg:58.61ms
step:1551/2330 train_time:90902ms step_avg:58.61ms
step:1552/2330 train_time:90964ms step_avg:58.61ms
step:1553/2330 train_time:91021ms step_avg:58.61ms
step:1554/2330 train_time:91083ms step_avg:58.61ms
step:1555/2330 train_time:91140ms step_avg:58.61ms
step:1556/2330 train_time:91202ms step_avg:58.61ms
step:1557/2330 train_time:91260ms step_avg:58.61ms
step:1558/2330 train_time:91320ms step_avg:58.61ms
step:1559/2330 train_time:91378ms step_avg:58.61ms
step:1560/2330 train_time:91439ms step_avg:58.61ms
step:1561/2330 train_time:91496ms step_avg:58.61ms
step:1562/2330 train_time:91558ms step_avg:58.62ms
step:1563/2330 train_time:91615ms step_avg:58.62ms
step:1564/2330 train_time:91677ms step_avg:58.62ms
step:1565/2330 train_time:91733ms step_avg:58.62ms
step:1566/2330 train_time:91796ms step_avg:58.62ms
step:1567/2330 train_time:91853ms step_avg:58.62ms
step:1568/2330 train_time:91915ms step_avg:58.62ms
step:1569/2330 train_time:91971ms step_avg:58.62ms
step:1570/2330 train_time:92034ms step_avg:58.62ms
step:1571/2330 train_time:92090ms step_avg:58.62ms
step:1572/2330 train_time:92154ms step_avg:58.62ms
step:1573/2330 train_time:92211ms step_avg:58.62ms
step:1574/2330 train_time:92274ms step_avg:58.62ms
step:1575/2330 train_time:92330ms step_avg:58.62ms
step:1576/2330 train_time:92393ms step_avg:58.63ms
step:1577/2330 train_time:92450ms step_avg:58.62ms
step:1578/2330 train_time:92512ms step_avg:58.63ms
step:1579/2330 train_time:92570ms step_avg:58.63ms
step:1580/2330 train_time:92632ms step_avg:58.63ms
step:1581/2330 train_time:92689ms step_avg:58.63ms
step:1582/2330 train_time:92751ms step_avg:58.63ms
step:1583/2330 train_time:92808ms step_avg:58.63ms
step:1584/2330 train_time:92870ms step_avg:58.63ms
step:1585/2330 train_time:92927ms step_avg:58.63ms
step:1586/2330 train_time:92989ms step_avg:58.63ms
step:1587/2330 train_time:93046ms step_avg:58.63ms
step:1588/2330 train_time:93109ms step_avg:58.63ms
step:1589/2330 train_time:93166ms step_avg:58.63ms
step:1590/2330 train_time:93229ms step_avg:58.63ms
step:1591/2330 train_time:93286ms step_avg:58.63ms
step:1592/2330 train_time:93347ms step_avg:58.64ms
step:1593/2330 train_time:93404ms step_avg:58.63ms
step:1594/2330 train_time:93466ms step_avg:58.64ms
step:1595/2330 train_time:93523ms step_avg:58.64ms
step:1596/2330 train_time:93584ms step_avg:58.64ms
step:1597/2330 train_time:93642ms step_avg:58.64ms
step:1598/2330 train_time:93702ms step_avg:58.64ms
step:1599/2330 train_time:93759ms step_avg:58.64ms
step:1600/2330 train_time:93821ms step_avg:58.64ms
step:1601/2330 train_time:93879ms step_avg:58.64ms
step:1602/2330 train_time:93940ms step_avg:58.64ms
step:1603/2330 train_time:93996ms step_avg:58.64ms
step:1604/2330 train_time:94059ms step_avg:58.64ms
step:1605/2330 train_time:94116ms step_avg:58.64ms
step:1606/2330 train_time:94177ms step_avg:58.64ms
step:1607/2330 train_time:94234ms step_avg:58.64ms
step:1608/2330 train_time:94296ms step_avg:58.64ms
step:1609/2330 train_time:94352ms step_avg:58.64ms
step:1610/2330 train_time:94415ms step_avg:58.64ms
step:1611/2330 train_time:94472ms step_avg:58.64ms
step:1612/2330 train_time:94534ms step_avg:58.64ms
step:1613/2330 train_time:94591ms step_avg:58.64ms
step:1614/2330 train_time:94654ms step_avg:58.65ms
step:1615/2330 train_time:94710ms step_avg:58.64ms
step:1616/2330 train_time:94773ms step_avg:58.65ms
step:1617/2330 train_time:94829ms step_avg:58.65ms
step:1618/2330 train_time:94893ms step_avg:58.65ms
step:1619/2330 train_time:94949ms step_avg:58.65ms
step:1620/2330 train_time:95012ms step_avg:58.65ms
step:1621/2330 train_time:95069ms step_avg:58.65ms
step:1622/2330 train_time:95131ms step_avg:58.65ms
step:1623/2330 train_time:95188ms step_avg:58.65ms
step:1624/2330 train_time:95250ms step_avg:58.65ms
step:1625/2330 train_time:95307ms step_avg:58.65ms
step:1626/2330 train_time:95368ms step_avg:58.65ms
step:1627/2330 train_time:95426ms step_avg:58.65ms
step:1628/2330 train_time:95488ms step_avg:58.65ms
step:1629/2330 train_time:95545ms step_avg:58.65ms
step:1630/2330 train_time:95608ms step_avg:58.66ms
step:1631/2330 train_time:95664ms step_avg:58.65ms
step:1632/2330 train_time:95726ms step_avg:58.66ms
step:1633/2330 train_time:95784ms step_avg:58.65ms
step:1634/2330 train_time:95847ms step_avg:58.66ms
step:1635/2330 train_time:95904ms step_avg:58.66ms
step:1636/2330 train_time:95966ms step_avg:58.66ms
step:1637/2330 train_time:96023ms step_avg:58.66ms
step:1638/2330 train_time:96084ms step_avg:58.66ms
step:1639/2330 train_time:96141ms step_avg:58.66ms
step:1640/2330 train_time:96203ms step_avg:58.66ms
step:1641/2330 train_time:96260ms step_avg:58.66ms
step:1642/2330 train_time:96322ms step_avg:58.66ms
step:1643/2330 train_time:96378ms step_avg:58.66ms
step:1644/2330 train_time:96440ms step_avg:58.66ms
step:1645/2330 train_time:96497ms step_avg:58.66ms
step:1646/2330 train_time:96558ms step_avg:58.66ms
step:1647/2330 train_time:96615ms step_avg:58.66ms
step:1648/2330 train_time:96677ms step_avg:58.66ms
step:1649/2330 train_time:96733ms step_avg:58.66ms
step:1650/2330 train_time:96795ms step_avg:58.66ms
step:1651/2330 train_time:96851ms step_avg:58.66ms
step:1652/2330 train_time:96914ms step_avg:58.66ms
step:1653/2330 train_time:96971ms step_avg:58.66ms
step:1654/2330 train_time:97034ms step_avg:58.67ms
step:1655/2330 train_time:97090ms step_avg:58.66ms
step:1656/2330 train_time:97153ms step_avg:58.67ms
step:1657/2330 train_time:97209ms step_avg:58.67ms
step:1658/2330 train_time:97272ms step_avg:58.67ms
step:1659/2330 train_time:97328ms step_avg:58.67ms
step:1660/2330 train_time:97391ms step_avg:58.67ms
step:1661/2330 train_time:97447ms step_avg:58.67ms
step:1662/2330 train_time:97510ms step_avg:58.67ms
step:1663/2330 train_time:97567ms step_avg:58.67ms
step:1664/2330 train_time:97629ms step_avg:58.67ms
step:1665/2330 train_time:97686ms step_avg:58.67ms
step:1666/2330 train_time:97748ms step_avg:58.67ms
step:1667/2330 train_time:97806ms step_avg:58.67ms
step:1668/2330 train_time:97867ms step_avg:58.67ms
step:1669/2330 train_time:97925ms step_avg:58.67ms
step:1670/2330 train_time:97987ms step_avg:58.67ms
step:1671/2330 train_time:98044ms step_avg:58.67ms
step:1672/2330 train_time:98107ms step_avg:58.68ms
step:1673/2330 train_time:98165ms step_avg:58.68ms
step:1674/2330 train_time:98226ms step_avg:58.68ms
step:1675/2330 train_time:98284ms step_avg:58.68ms
step:1676/2330 train_time:98345ms step_avg:58.68ms
step:1677/2330 train_time:98402ms step_avg:58.68ms
step:1678/2330 train_time:98463ms step_avg:58.68ms
step:1679/2330 train_time:98520ms step_avg:58.68ms
step:1680/2330 train_time:98582ms step_avg:58.68ms
step:1681/2330 train_time:98639ms step_avg:58.68ms
step:1682/2330 train_time:98700ms step_avg:58.68ms
step:1683/2330 train_time:98757ms step_avg:58.68ms
step:1684/2330 train_time:98818ms step_avg:58.68ms
step:1685/2330 train_time:98875ms step_avg:58.68ms
step:1686/2330 train_time:98936ms step_avg:58.68ms
step:1687/2330 train_time:98993ms step_avg:58.68ms
step:1688/2330 train_time:99056ms step_avg:58.68ms
step:1689/2330 train_time:99112ms step_avg:58.68ms
step:1690/2330 train_time:99175ms step_avg:58.68ms
step:1691/2330 train_time:99231ms step_avg:58.68ms
step:1692/2330 train_time:99294ms step_avg:58.68ms
step:1693/2330 train_time:99351ms step_avg:58.68ms
step:1694/2330 train_time:99414ms step_avg:58.69ms
step:1695/2330 train_time:99471ms step_avg:58.68ms
step:1696/2330 train_time:99534ms step_avg:58.69ms
step:1697/2330 train_time:99590ms step_avg:58.69ms
step:1698/2330 train_time:99652ms step_avg:58.69ms
step:1699/2330 train_time:99709ms step_avg:58.69ms
step:1700/2330 train_time:99771ms step_avg:58.69ms
step:1701/2330 train_time:99828ms step_avg:58.69ms
step:1702/2330 train_time:99890ms step_avg:58.69ms
step:1703/2330 train_time:99948ms step_avg:58.69ms
step:1704/2330 train_time:100009ms step_avg:58.69ms
step:1705/2330 train_time:100067ms step_avg:58.69ms
step:1706/2330 train_time:100129ms step_avg:58.69ms
step:1707/2330 train_time:100186ms step_avg:58.69ms
step:1708/2330 train_time:100248ms step_avg:58.69ms
step:1709/2330 train_time:100305ms step_avg:58.69ms
step:1710/2330 train_time:100369ms step_avg:58.70ms
step:1711/2330 train_time:100426ms step_avg:58.69ms
step:1712/2330 train_time:100490ms step_avg:58.70ms
step:1713/2330 train_time:100547ms step_avg:58.70ms
step:1714/2330 train_time:100609ms step_avg:58.70ms
step:1715/2330 train_time:100666ms step_avg:58.70ms
step:1716/2330 train_time:100728ms step_avg:58.70ms
step:1717/2330 train_time:100785ms step_avg:58.70ms
step:1718/2330 train_time:100846ms step_avg:58.70ms
step:1719/2330 train_time:100903ms step_avg:58.70ms
step:1720/2330 train_time:100964ms step_avg:58.70ms
step:1721/2330 train_time:101022ms step_avg:58.70ms
step:1722/2330 train_time:101083ms step_avg:58.70ms
step:1723/2330 train_time:101141ms step_avg:58.70ms
step:1724/2330 train_time:101202ms step_avg:58.70ms
step:1725/2330 train_time:101258ms step_avg:58.70ms
step:1726/2330 train_time:101320ms step_avg:58.70ms
step:1727/2330 train_time:101378ms step_avg:58.70ms
step:1728/2330 train_time:101438ms step_avg:58.70ms
step:1729/2330 train_time:101495ms step_avg:58.70ms
step:1730/2330 train_time:101558ms step_avg:58.70ms
step:1731/2330 train_time:101614ms step_avg:58.70ms
step:1732/2330 train_time:101676ms step_avg:58.70ms
step:1733/2330 train_time:101733ms step_avg:58.70ms
step:1734/2330 train_time:101796ms step_avg:58.71ms
step:1735/2330 train_time:101852ms step_avg:58.70ms
step:1736/2330 train_time:101914ms step_avg:58.71ms
step:1737/2330 train_time:101971ms step_avg:58.71ms
step:1738/2330 train_time:102033ms step_avg:58.71ms
step:1739/2330 train_time:102090ms step_avg:58.71ms
step:1740/2330 train_time:102152ms step_avg:58.71ms
step:1741/2330 train_time:102209ms step_avg:58.71ms
step:1742/2330 train_time:102272ms step_avg:58.71ms
step:1743/2330 train_time:102329ms step_avg:58.71ms
step:1744/2330 train_time:102391ms step_avg:58.71ms
step:1745/2330 train_time:102448ms step_avg:58.71ms
step:1746/2330 train_time:102511ms step_avg:58.71ms
step:1747/2330 train_time:102568ms step_avg:58.71ms
step:1748/2330 train_time:102630ms step_avg:58.71ms
step:1749/2330 train_time:102687ms step_avg:58.71ms
step:1750/2330 train_time:102750ms step_avg:58.71ms
step:1750/2330 val_loss:4.0826 train_time:102829ms step_avg:58.76ms
step:1751/2330 train_time:102849ms step_avg:58.74ms
step:1752/2330 train_time:102871ms step_avg:58.72ms
step:1753/2330 train_time:102928ms step_avg:58.72ms
step:1754/2330 train_time:102994ms step_avg:58.72ms
step:1755/2330 train_time:103051ms step_avg:58.72ms
step:1756/2330 train_time:103112ms step_avg:58.72ms
step:1757/2330 train_time:103170ms step_avg:58.72ms
step:1758/2330 train_time:103230ms step_avg:58.72ms
step:1759/2330 train_time:103287ms step_avg:58.72ms
step:1760/2330 train_time:103348ms step_avg:58.72ms
step:1761/2330 train_time:103405ms step_avg:58.72ms
step:1762/2330 train_time:103466ms step_avg:58.72ms
step:1763/2330 train_time:103522ms step_avg:58.72ms
step:1764/2330 train_time:103583ms step_avg:58.72ms
step:1765/2330 train_time:103640ms step_avg:58.72ms
step:1766/2330 train_time:103700ms step_avg:58.72ms
step:1767/2330 train_time:103757ms step_avg:58.72ms
step:1768/2330 train_time:103820ms step_avg:58.72ms
step:1769/2330 train_time:103877ms step_avg:58.72ms
step:1770/2330 train_time:103941ms step_avg:58.72ms
step:1771/2330 train_time:103997ms step_avg:58.72ms
step:1772/2330 train_time:104060ms step_avg:58.72ms
step:1773/2330 train_time:104117ms step_avg:58.72ms
step:1774/2330 train_time:104179ms step_avg:58.73ms
step:1775/2330 train_time:104236ms step_avg:58.72ms
step:1776/2330 train_time:104299ms step_avg:58.73ms
step:1777/2330 train_time:104356ms step_avg:58.73ms
step:1778/2330 train_time:104418ms step_avg:58.73ms
step:1779/2330 train_time:104475ms step_avg:58.73ms
step:1780/2330 train_time:104536ms step_avg:58.73ms
step:1781/2330 train_time:104592ms step_avg:58.73ms
step:1782/2330 train_time:104655ms step_avg:58.73ms
step:1783/2330 train_time:104711ms step_avg:58.73ms
step:1784/2330 train_time:104776ms step_avg:58.73ms
step:1785/2330 train_time:104832ms step_avg:58.73ms
step:1786/2330 train_time:104898ms step_avg:58.73ms
step:1787/2330 train_time:104954ms step_avg:58.73ms
step:1788/2330 train_time:105018ms step_avg:58.74ms
step:1789/2330 train_time:105075ms step_avg:58.73ms
step:1790/2330 train_time:105139ms step_avg:58.74ms
step:1791/2330 train_time:105195ms step_avg:58.74ms
step:1792/2330 train_time:105257ms step_avg:58.74ms
step:1793/2330 train_time:105314ms step_avg:58.74ms
step:1794/2330 train_time:105376ms step_avg:58.74ms
step:1795/2330 train_time:105432ms step_avg:58.74ms
step:1796/2330 train_time:105495ms step_avg:58.74ms
step:1797/2330 train_time:105551ms step_avg:58.74ms
step:1798/2330 train_time:105613ms step_avg:58.74ms
step:1799/2330 train_time:105670ms step_avg:58.74ms
step:1800/2330 train_time:105732ms step_avg:58.74ms
step:1801/2330 train_time:105789ms step_avg:58.74ms
step:1802/2330 train_time:105851ms step_avg:58.74ms
step:1803/2330 train_time:105908ms step_avg:58.74ms
step:1804/2330 train_time:105972ms step_avg:58.74ms
step:1805/2330 train_time:106029ms step_avg:58.74ms
step:1806/2330 train_time:106093ms step_avg:58.74ms
step:1807/2330 train_time:106150ms step_avg:58.74ms
step:1808/2330 train_time:106213ms step_avg:58.75ms
step:1809/2330 train_time:106270ms step_avg:58.75ms
step:1810/2330 train_time:106333ms step_avg:58.75ms
step:1811/2330 train_time:106390ms step_avg:58.75ms
step:1812/2330 train_time:106452ms step_avg:58.75ms
step:1813/2330 train_time:106509ms step_avg:58.75ms
step:1814/2330 train_time:106572ms step_avg:58.75ms
step:1815/2330 train_time:106628ms step_avg:58.75ms
step:1816/2330 train_time:106691ms step_avg:58.75ms
step:1817/2330 train_time:106747ms step_avg:58.75ms
step:1818/2330 train_time:106811ms step_avg:58.75ms
step:1819/2330 train_time:106868ms step_avg:58.75ms
step:1820/2330 train_time:106931ms step_avg:58.75ms
step:1821/2330 train_time:106988ms step_avg:58.75ms
step:1822/2330 train_time:107050ms step_avg:58.75ms
step:1823/2330 train_time:107108ms step_avg:58.75ms
step:1824/2330 train_time:107171ms step_avg:58.76ms
step:1825/2330 train_time:107228ms step_avg:58.75ms
step:1826/2330 train_time:107290ms step_avg:58.76ms
step:1827/2330 train_time:107347ms step_avg:58.76ms
step:1828/2330 train_time:107409ms step_avg:58.76ms
step:1829/2330 train_time:107466ms step_avg:58.76ms
step:1830/2330 train_time:107528ms step_avg:58.76ms
step:1831/2330 train_time:107585ms step_avg:58.76ms
step:1832/2330 train_time:107647ms step_avg:58.76ms
step:1833/2330 train_time:107704ms step_avg:58.76ms
step:1834/2330 train_time:107765ms step_avg:58.76ms
step:1835/2330 train_time:107822ms step_avg:58.76ms
step:1836/2330 train_time:107884ms step_avg:58.76ms
step:1837/2330 train_time:107941ms step_avg:58.76ms
step:1838/2330 train_time:108003ms step_avg:58.76ms
step:1839/2330 train_time:108059ms step_avg:58.76ms
step:1840/2330 train_time:108121ms step_avg:58.76ms
step:1841/2330 train_time:108178ms step_avg:58.76ms
step:1842/2330 train_time:108241ms step_avg:58.76ms
step:1843/2330 train_time:108297ms step_avg:58.76ms
step:1844/2330 train_time:108359ms step_avg:58.76ms
step:1845/2330 train_time:108415ms step_avg:58.76ms
step:1846/2330 train_time:108477ms step_avg:58.76ms
step:1847/2330 train_time:108534ms step_avg:58.76ms
step:1848/2330 train_time:108597ms step_avg:58.76ms
step:1849/2330 train_time:108653ms step_avg:58.76ms
step:1850/2330 train_time:108717ms step_avg:58.77ms
step:1851/2330 train_time:108774ms step_avg:58.76ms
step:1852/2330 train_time:108836ms step_avg:58.77ms
step:1853/2330 train_time:108892ms step_avg:58.77ms
step:1854/2330 train_time:108956ms step_avg:58.77ms
step:1855/2330 train_time:109012ms step_avg:58.77ms
step:1856/2330 train_time:109076ms step_avg:58.77ms
step:1857/2330 train_time:109133ms step_avg:58.77ms
step:1858/2330 train_time:109196ms step_avg:58.77ms
step:1859/2330 train_time:109252ms step_avg:58.77ms
step:1860/2330 train_time:109315ms step_avg:58.77ms
step:1861/2330 train_time:109372ms step_avg:58.77ms
step:1862/2330 train_time:109435ms step_avg:58.77ms
step:1863/2330 train_time:109492ms step_avg:58.77ms
step:1864/2330 train_time:109555ms step_avg:58.77ms
step:1865/2330 train_time:109611ms step_avg:58.77ms
step:1866/2330 train_time:109674ms step_avg:58.78ms
step:1867/2330 train_time:109731ms step_avg:58.77ms
step:1868/2330 train_time:109794ms step_avg:58.78ms
step:1869/2330 train_time:109851ms step_avg:58.78ms
step:1870/2330 train_time:109914ms step_avg:58.78ms
step:1871/2330 train_time:109971ms step_avg:58.78ms
step:1872/2330 train_time:110034ms step_avg:58.78ms
step:1873/2330 train_time:110091ms step_avg:58.78ms
step:1874/2330 train_time:110153ms step_avg:58.78ms
step:1875/2330 train_time:110209ms step_avg:58.78ms
step:1876/2330 train_time:110273ms step_avg:58.78ms
step:1877/2330 train_time:110329ms step_avg:58.78ms
step:1878/2330 train_time:110392ms step_avg:58.78ms
step:1879/2330 train_time:110449ms step_avg:58.78ms
step:1880/2330 train_time:110512ms step_avg:58.78ms
step:1881/2330 train_time:110569ms step_avg:58.78ms
step:1882/2330 train_time:110632ms step_avg:58.78ms
step:1883/2330 train_time:110689ms step_avg:58.78ms
step:1884/2330 train_time:110751ms step_avg:58.78ms
step:1885/2330 train_time:110807ms step_avg:58.78ms
step:1886/2330 train_time:110870ms step_avg:58.79ms
step:1887/2330 train_time:110927ms step_avg:58.78ms
step:1888/2330 train_time:110989ms step_avg:58.79ms
step:1889/2330 train_time:111046ms step_avg:58.79ms
step:1890/2330 train_time:111109ms step_avg:58.79ms
step:1891/2330 train_time:111167ms step_avg:58.79ms
step:1892/2330 train_time:111228ms step_avg:58.79ms
step:1893/2330 train_time:111285ms step_avg:58.79ms
step:1894/2330 train_time:111347ms step_avg:58.79ms
step:1895/2330 train_time:111404ms step_avg:58.79ms
step:1896/2330 train_time:111466ms step_avg:58.79ms
step:1897/2330 train_time:111523ms step_avg:58.79ms
step:1898/2330 train_time:111585ms step_avg:58.79ms
step:1899/2330 train_time:111642ms step_avg:58.79ms
step:1900/2330 train_time:111704ms step_avg:58.79ms
step:1901/2330 train_time:111761ms step_avg:58.79ms
step:1902/2330 train_time:111822ms step_avg:58.79ms
step:1903/2330 train_time:111879ms step_avg:58.79ms
step:1904/2330 train_time:111941ms step_avg:58.79ms
step:1905/2330 train_time:111997ms step_avg:58.79ms
step:1906/2330 train_time:112059ms step_avg:58.79ms
step:1907/2330 train_time:112116ms step_avg:58.79ms
step:1908/2330 train_time:112178ms step_avg:58.79ms
step:1909/2330 train_time:112234ms step_avg:58.79ms
step:1910/2330 train_time:112298ms step_avg:58.79ms
step:1911/2330 train_time:112355ms step_avg:58.79ms
step:1912/2330 train_time:112417ms step_avg:58.80ms
step:1913/2330 train_time:112474ms step_avg:58.79ms
step:1914/2330 train_time:112537ms step_avg:58.80ms
step:1915/2330 train_time:112593ms step_avg:58.80ms
step:1916/2330 train_time:112656ms step_avg:58.80ms
step:1917/2330 train_time:112713ms step_avg:58.80ms
step:1918/2330 train_time:112774ms step_avg:58.80ms
step:1919/2330 train_time:112831ms step_avg:58.80ms
step:1920/2330 train_time:112894ms step_avg:58.80ms
step:1921/2330 train_time:112951ms step_avg:58.80ms
step:1922/2330 train_time:113014ms step_avg:58.80ms
step:1923/2330 train_time:113071ms step_avg:58.80ms
step:1924/2330 train_time:113133ms step_avg:58.80ms
step:1925/2330 train_time:113190ms step_avg:58.80ms
step:1926/2330 train_time:113252ms step_avg:58.80ms
step:1927/2330 train_time:113310ms step_avg:58.80ms
step:1928/2330 train_time:113372ms step_avg:58.80ms
step:1929/2330 train_time:113430ms step_avg:58.80ms
step:1930/2330 train_time:113492ms step_avg:58.80ms
step:1931/2330 train_time:113549ms step_avg:58.80ms
step:1932/2330 train_time:113612ms step_avg:58.81ms
step:1933/2330 train_time:113669ms step_avg:58.80ms
step:1934/2330 train_time:113733ms step_avg:58.81ms
step:1935/2330 train_time:113791ms step_avg:58.81ms
step:1936/2330 train_time:113854ms step_avg:58.81ms
step:1937/2330 train_time:113910ms step_avg:58.81ms
step:1938/2330 train_time:113974ms step_avg:58.81ms
step:1939/2330 train_time:114031ms step_avg:58.81ms
step:1940/2330 train_time:114093ms step_avg:58.81ms
step:1941/2330 train_time:114150ms step_avg:58.81ms
step:1942/2330 train_time:114213ms step_avg:58.81ms
step:1943/2330 train_time:114270ms step_avg:58.81ms
step:1944/2330 train_time:114333ms step_avg:58.81ms
step:1945/2330 train_time:114389ms step_avg:58.81ms
step:1946/2330 train_time:114453ms step_avg:58.81ms
step:1947/2330 train_time:114510ms step_avg:58.81ms
step:1948/2330 train_time:114572ms step_avg:58.82ms
step:1949/2330 train_time:114629ms step_avg:58.81ms
step:1950/2330 train_time:114691ms step_avg:58.82ms
step:1951/2330 train_time:114748ms step_avg:58.81ms
step:1952/2330 train_time:114810ms step_avg:58.82ms
step:1953/2330 train_time:114867ms step_avg:58.82ms
step:1954/2330 train_time:114930ms step_avg:58.82ms
step:1955/2330 train_time:114987ms step_avg:58.82ms
step:1956/2330 train_time:115049ms step_avg:58.82ms
step:1957/2330 train_time:115106ms step_avg:58.82ms
step:1958/2330 train_time:115168ms step_avg:58.82ms
step:1959/2330 train_time:115225ms step_avg:58.82ms
step:1960/2330 train_time:115287ms step_avg:58.82ms
step:1961/2330 train_time:115345ms step_avg:58.82ms
step:1962/2330 train_time:115405ms step_avg:58.82ms
step:1963/2330 train_time:115463ms step_avg:58.82ms
step:1964/2330 train_time:115523ms step_avg:58.82ms
step:1965/2330 train_time:115581ms step_avg:58.82ms
step:1966/2330 train_time:115642ms step_avg:58.82ms
step:1967/2330 train_time:115698ms step_avg:58.82ms
step:1968/2330 train_time:115761ms step_avg:58.82ms
step:1969/2330 train_time:115818ms step_avg:58.82ms
step:1970/2330 train_time:115880ms step_avg:58.82ms
step:1971/2330 train_time:115937ms step_avg:58.82ms
step:1972/2330 train_time:115999ms step_avg:58.82ms
step:1973/2330 train_time:116056ms step_avg:58.82ms
step:1974/2330 train_time:116119ms step_avg:58.82ms
step:1975/2330 train_time:116176ms step_avg:58.82ms
step:1976/2330 train_time:116239ms step_avg:58.83ms
step:1977/2330 train_time:116295ms step_avg:58.82ms
step:1978/2330 train_time:116358ms step_avg:58.83ms
step:1979/2330 train_time:116415ms step_avg:58.83ms
step:1980/2330 train_time:116477ms step_avg:58.83ms
step:1981/2330 train_time:116533ms step_avg:58.83ms
step:1982/2330 train_time:116598ms step_avg:58.83ms
step:1983/2330 train_time:116654ms step_avg:58.83ms
step:1984/2330 train_time:116717ms step_avg:58.83ms
step:1985/2330 train_time:116774ms step_avg:58.83ms
step:1986/2330 train_time:116837ms step_avg:58.83ms
step:1987/2330 train_time:116893ms step_avg:58.83ms
step:1988/2330 train_time:116956ms step_avg:58.83ms
step:1989/2330 train_time:117012ms step_avg:58.83ms
step:1990/2330 train_time:117075ms step_avg:58.83ms
step:1991/2330 train_time:117132ms step_avg:58.83ms
step:1992/2330 train_time:117195ms step_avg:58.83ms
step:1993/2330 train_time:117252ms step_avg:58.83ms
step:1994/2330 train_time:117315ms step_avg:58.83ms
step:1995/2330 train_time:117372ms step_avg:58.83ms
step:1996/2330 train_time:117436ms step_avg:58.84ms
step:1997/2330 train_time:117493ms step_avg:58.83ms
step:1998/2330 train_time:117557ms step_avg:58.84ms
step:1999/2330 train_time:117613ms step_avg:58.84ms
step:2000/2330 train_time:117676ms step_avg:58.84ms
step:2000/2330 val_loss:4.0224 train_time:117756ms step_avg:58.88ms
step:2001/2330 train_time:117775ms step_avg:58.86ms
step:2002/2330 train_time:117798ms step_avg:58.84ms
step:2003/2330 train_time:117857ms step_avg:58.84ms
step:2004/2330 train_time:117924ms step_avg:58.84ms
step:2005/2330 train_time:117981ms step_avg:58.84ms
step:2006/2330 train_time:118046ms step_avg:58.85ms
step:2007/2330 train_time:118103ms step_avg:58.85ms
step:2008/2330 train_time:118165ms step_avg:58.85ms
step:2009/2330 train_time:118221ms step_avg:58.85ms
step:2010/2330 train_time:118285ms step_avg:58.85ms
step:2011/2330 train_time:118341ms step_avg:58.85ms
step:2012/2330 train_time:118402ms step_avg:58.85ms
step:2013/2330 train_time:118459ms step_avg:58.85ms
step:2014/2330 train_time:118520ms step_avg:58.85ms
step:2015/2330 train_time:118576ms step_avg:58.85ms
step:2016/2330 train_time:118637ms step_avg:58.85ms
step:2017/2330 train_time:118694ms step_avg:58.85ms
step:2018/2330 train_time:118755ms step_avg:58.85ms
step:2019/2330 train_time:118813ms step_avg:58.85ms
step:2020/2330 train_time:118875ms step_avg:58.85ms
step:2021/2330 train_time:118933ms step_avg:58.85ms
step:2022/2330 train_time:118996ms step_avg:58.85ms
step:2023/2330 train_time:119053ms step_avg:58.85ms
step:2024/2330 train_time:119115ms step_avg:58.85ms
step:2025/2330 train_time:119172ms step_avg:58.85ms
step:2026/2330 train_time:119233ms step_avg:58.85ms
step:2027/2330 train_time:119290ms step_avg:58.85ms
step:2028/2330 train_time:119352ms step_avg:58.85ms
step:2029/2330 train_time:119409ms step_avg:58.85ms
step:2030/2330 train_time:119470ms step_avg:58.85ms
step:2031/2330 train_time:119527ms step_avg:58.85ms
step:2032/2330 train_time:119588ms step_avg:58.85ms
step:2033/2330 train_time:119644ms step_avg:58.85ms
step:2034/2330 train_time:119706ms step_avg:58.85ms
step:2035/2330 train_time:119763ms step_avg:58.85ms
step:2036/2330 train_time:119825ms step_avg:58.85ms
step:2037/2330 train_time:119881ms step_avg:58.85ms
step:2038/2330 train_time:119946ms step_avg:58.85ms
step:2039/2330 train_time:120003ms step_avg:58.85ms
step:2040/2330 train_time:120067ms step_avg:58.86ms
step:2041/2330 train_time:120123ms step_avg:58.86ms
step:2042/2330 train_time:120187ms step_avg:58.86ms
step:2043/2330 train_time:120244ms step_avg:58.86ms
step:2044/2330 train_time:120306ms step_avg:58.86ms
step:2045/2330 train_time:120363ms step_avg:58.86ms
step:2046/2330 train_time:120425ms step_avg:58.86ms
step:2047/2330 train_time:120482ms step_avg:58.86ms
step:2048/2330 train_time:120543ms step_avg:58.86ms
step:2049/2330 train_time:120600ms step_avg:58.86ms
step:2050/2330 train_time:120661ms step_avg:58.86ms
step:2051/2330 train_time:120719ms step_avg:58.86ms
step:2052/2330 train_time:120781ms step_avg:58.86ms
step:2053/2330 train_time:120837ms step_avg:58.86ms
step:2054/2330 train_time:120901ms step_avg:58.86ms
step:2055/2330 train_time:120958ms step_avg:58.86ms
step:2056/2330 train_time:121022ms step_avg:58.86ms
step:2057/2330 train_time:121079ms step_avg:58.86ms
step:2058/2330 train_time:121142ms step_avg:58.86ms
step:2059/2330 train_time:121200ms step_avg:58.86ms
step:2060/2330 train_time:121262ms step_avg:58.87ms
step:2061/2330 train_time:121319ms step_avg:58.86ms
step:2062/2330 train_time:121381ms step_avg:58.87ms
step:2063/2330 train_time:121438ms step_avg:58.86ms
step:2064/2330 train_time:121500ms step_avg:58.87ms
step:2065/2330 train_time:121557ms step_avg:58.87ms
step:2066/2330 train_time:121618ms step_avg:58.87ms
step:2067/2330 train_time:121675ms step_avg:58.87ms
step:2068/2330 train_time:121737ms step_avg:58.87ms
step:2069/2330 train_time:121794ms step_avg:58.87ms
step:2070/2330 train_time:121855ms step_avg:58.87ms
step:2071/2330 train_time:121913ms step_avg:58.87ms
step:2072/2330 train_time:121974ms step_avg:58.87ms
step:2073/2330 train_time:122031ms step_avg:58.87ms
step:2074/2330 train_time:122092ms step_avg:58.87ms
step:2075/2330 train_time:122149ms step_avg:58.87ms
step:2076/2330 train_time:122211ms step_avg:58.87ms
step:2077/2330 train_time:122268ms step_avg:58.87ms
step:2078/2330 train_time:122330ms step_avg:58.87ms
step:2079/2330 train_time:122387ms step_avg:58.87ms
step:2080/2330 train_time:122450ms step_avg:58.87ms
step:2081/2330 train_time:122507ms step_avg:58.87ms
step:2082/2330 train_time:122570ms step_avg:58.87ms
step:2083/2330 train_time:122626ms step_avg:58.87ms
step:2084/2330 train_time:122688ms step_avg:58.87ms
step:2085/2330 train_time:122744ms step_avg:58.87ms
step:2086/2330 train_time:122808ms step_avg:58.87ms
step:2087/2330 train_time:122864ms step_avg:58.87ms
step:2088/2330 train_time:122927ms step_avg:58.87ms
step:2089/2330 train_time:122983ms step_avg:58.87ms
step:2090/2330 train_time:123046ms step_avg:58.87ms
step:2091/2330 train_time:123103ms step_avg:58.87ms
step:2092/2330 train_time:123167ms step_avg:58.88ms
step:2093/2330 train_time:123223ms step_avg:58.87ms
step:2094/2330 train_time:123286ms step_avg:58.88ms
step:2095/2330 train_time:123343ms step_avg:58.88ms
step:2096/2330 train_time:123405ms step_avg:58.88ms
step:2097/2330 train_time:123462ms step_avg:58.88ms
step:2098/2330 train_time:123524ms step_avg:58.88ms
step:2099/2330 train_time:123580ms step_avg:58.88ms
step:2100/2330 train_time:123643ms step_avg:58.88ms
step:2101/2330 train_time:123700ms step_avg:58.88ms
step:2102/2330 train_time:123762ms step_avg:58.88ms
step:2103/2330 train_time:123820ms step_avg:58.88ms
step:2104/2330 train_time:123881ms step_avg:58.88ms
step:2105/2330 train_time:123938ms step_avg:58.88ms
step:2106/2330 train_time:124000ms step_avg:58.88ms
step:2107/2330 train_time:124058ms step_avg:58.88ms
step:2108/2330 train_time:124120ms step_avg:58.88ms
step:2109/2330 train_time:124178ms step_avg:58.88ms
step:2110/2330 train_time:124239ms step_avg:58.88ms
step:2111/2330 train_time:124297ms step_avg:58.88ms
step:2112/2330 train_time:124357ms step_avg:58.88ms
step:2113/2330 train_time:124415ms step_avg:58.88ms
step:2114/2330 train_time:124475ms step_avg:58.88ms
step:2115/2330 train_time:124532ms step_avg:58.88ms
step:2116/2330 train_time:124594ms step_avg:58.88ms
step:2117/2330 train_time:124652ms step_avg:58.88ms
step:2118/2330 train_time:124713ms step_avg:58.88ms
step:2119/2330 train_time:124770ms step_avg:58.88ms
step:2120/2330 train_time:124831ms step_avg:58.88ms
step:2121/2330 train_time:124888ms step_avg:58.88ms
step:2122/2330 train_time:124950ms step_avg:58.88ms
step:2123/2330 train_time:125006ms step_avg:58.88ms
step:2124/2330 train_time:125069ms step_avg:58.88ms
step:2125/2330 train_time:125125ms step_avg:58.88ms
step:2126/2330 train_time:125187ms step_avg:58.88ms
step:2127/2330 train_time:125243ms step_avg:58.88ms
step:2128/2330 train_time:125306ms step_avg:58.88ms
step:2129/2330 train_time:125363ms step_avg:58.88ms
step:2130/2330 train_time:125426ms step_avg:58.89ms
step:2131/2330 train_time:125483ms step_avg:58.88ms
step:2132/2330 train_time:125545ms step_avg:58.89ms
step:2133/2330 train_time:125602ms step_avg:58.89ms
step:2134/2330 train_time:125664ms step_avg:58.89ms
step:2135/2330 train_time:125720ms step_avg:58.89ms
step:2136/2330 train_time:125784ms step_avg:58.89ms
step:2137/2330 train_time:125841ms step_avg:58.89ms
step:2138/2330 train_time:125903ms step_avg:58.89ms
step:2139/2330 train_time:125960ms step_avg:58.89ms
step:2140/2330 train_time:126022ms step_avg:58.89ms
step:2141/2330 train_time:126078ms step_avg:58.89ms
step:2142/2330 train_time:126142ms step_avg:58.89ms
step:2143/2330 train_time:126199ms step_avg:58.89ms
step:2144/2330 train_time:126261ms step_avg:58.89ms
step:2145/2330 train_time:126317ms step_avg:58.89ms
step:2146/2330 train_time:126380ms step_avg:58.89ms
step:2147/2330 train_time:126437ms step_avg:58.89ms
step:2148/2330 train_time:126499ms step_avg:58.89ms
step:2149/2330 train_time:126557ms step_avg:58.89ms
step:2150/2330 train_time:126618ms step_avg:58.89ms
step:2151/2330 train_time:126675ms step_avg:58.89ms
step:2152/2330 train_time:126737ms step_avg:58.89ms
step:2153/2330 train_time:126794ms step_avg:58.89ms
step:2154/2330 train_time:126855ms step_avg:58.89ms
step:2155/2330 train_time:126913ms step_avg:58.89ms
step:2156/2330 train_time:126975ms step_avg:58.89ms
step:2157/2330 train_time:127031ms step_avg:58.89ms
step:2158/2330 train_time:127094ms step_avg:58.89ms
step:2159/2330 train_time:127151ms step_avg:58.89ms
step:2160/2330 train_time:127213ms step_avg:58.89ms
step:2161/2330 train_time:127270ms step_avg:58.89ms
step:2162/2330 train_time:127332ms step_avg:58.90ms
step:2163/2330 train_time:127388ms step_avg:58.89ms
step:2164/2330 train_time:127450ms step_avg:58.90ms
step:2165/2330 train_time:127507ms step_avg:58.89ms
step:2166/2330 train_time:127570ms step_avg:58.90ms
step:2167/2330 train_time:127626ms step_avg:58.90ms
step:2168/2330 train_time:127687ms step_avg:58.90ms
step:2169/2330 train_time:127744ms step_avg:58.90ms
step:2170/2330 train_time:127808ms step_avg:58.90ms
step:2171/2330 train_time:127864ms step_avg:58.90ms
step:2172/2330 train_time:127927ms step_avg:58.90ms
step:2173/2330 train_time:127984ms step_avg:58.90ms
step:2174/2330 train_time:128047ms step_avg:58.90ms
step:2175/2330 train_time:128104ms step_avg:58.90ms
step:2176/2330 train_time:128167ms step_avg:58.90ms
step:2177/2330 train_time:128223ms step_avg:58.90ms
step:2178/2330 train_time:128287ms step_avg:58.90ms
step:2179/2330 train_time:128343ms step_avg:58.90ms
step:2180/2330 train_time:128406ms step_avg:58.90ms
step:2181/2330 train_time:128463ms step_avg:58.90ms
step:2182/2330 train_time:128526ms step_avg:58.90ms
step:2183/2330 train_time:128582ms step_avg:58.90ms
step:2184/2330 train_time:128645ms step_avg:58.90ms
step:2185/2330 train_time:128702ms step_avg:58.90ms
step:2186/2330 train_time:128764ms step_avg:58.90ms
step:2187/2330 train_time:128821ms step_avg:58.90ms
step:2188/2330 train_time:128884ms step_avg:58.90ms
step:2189/2330 train_time:128941ms step_avg:58.90ms
step:2190/2330 train_time:129003ms step_avg:58.91ms
step:2191/2330 train_time:129060ms step_avg:58.90ms
step:2192/2330 train_time:129123ms step_avg:58.91ms
step:2193/2330 train_time:129180ms step_avg:58.91ms
step:2194/2330 train_time:129242ms step_avg:58.91ms
step:2195/2330 train_time:129300ms step_avg:58.91ms
step:2196/2330 train_time:129361ms step_avg:58.91ms
step:2197/2330 train_time:129418ms step_avg:58.91ms
step:2198/2330 train_time:129480ms step_avg:58.91ms
step:2199/2330 train_time:129537ms step_avg:58.91ms
step:2200/2330 train_time:129599ms step_avg:58.91ms
step:2201/2330 train_time:129656ms step_avg:58.91ms
step:2202/2330 train_time:129717ms step_avg:58.91ms
step:2203/2330 train_time:129775ms step_avg:58.91ms
step:2204/2330 train_time:129837ms step_avg:58.91ms
step:2205/2330 train_time:129895ms step_avg:58.91ms
step:2206/2330 train_time:129955ms step_avg:58.91ms
step:2207/2330 train_time:130012ms step_avg:58.91ms
step:2208/2330 train_time:130075ms step_avg:58.91ms
step:2209/2330 train_time:130131ms step_avg:58.91ms
step:2210/2330 train_time:130193ms step_avg:58.91ms
step:2211/2330 train_time:130250ms step_avg:58.91ms
step:2212/2330 train_time:130312ms step_avg:58.91ms
step:2213/2330 train_time:130369ms step_avg:58.91ms
step:2214/2330 train_time:130432ms step_avg:58.91ms
step:2215/2330 train_time:130489ms step_avg:58.91ms
step:2216/2330 train_time:130551ms step_avg:58.91ms
step:2217/2330 train_time:130607ms step_avg:58.91ms
step:2218/2330 train_time:130670ms step_avg:58.91ms
step:2219/2330 train_time:130726ms step_avg:58.91ms
step:2220/2330 train_time:130788ms step_avg:58.91ms
step:2221/2330 train_time:130845ms step_avg:58.91ms
step:2222/2330 train_time:130908ms step_avg:58.91ms
step:2223/2330 train_time:130964ms step_avg:58.91ms
step:2224/2330 train_time:131027ms step_avg:58.91ms
step:2225/2330 train_time:131083ms step_avg:58.91ms
step:2226/2330 train_time:131146ms step_avg:58.92ms
step:2227/2330 train_time:131202ms step_avg:58.91ms
step:2228/2330 train_time:131265ms step_avg:58.92ms
step:2229/2330 train_time:131321ms step_avg:58.91ms
step:2230/2330 train_time:131384ms step_avg:58.92ms
step:2231/2330 train_time:131441ms step_avg:58.92ms
step:2232/2330 train_time:131504ms step_avg:58.92ms
step:2233/2330 train_time:131560ms step_avg:58.92ms
step:2234/2330 train_time:131623ms step_avg:58.92ms
step:2235/2330 train_time:131679ms step_avg:58.92ms
step:2236/2330 train_time:131741ms step_avg:58.92ms
step:2237/2330 train_time:131799ms step_avg:58.92ms
step:2238/2330 train_time:131861ms step_avg:58.92ms
step:2239/2330 train_time:131918ms step_avg:58.92ms
step:2240/2330 train_time:131980ms step_avg:58.92ms
step:2241/2330 train_time:132037ms step_avg:58.92ms
step:2242/2330 train_time:132100ms step_avg:58.92ms
step:2243/2330 train_time:132157ms step_avg:58.92ms
step:2244/2330 train_time:132219ms step_avg:58.92ms
step:2245/2330 train_time:132276ms step_avg:58.92ms
step:2246/2330 train_time:132338ms step_avg:58.92ms
step:2247/2330 train_time:132396ms step_avg:58.92ms
step:2248/2330 train_time:132458ms step_avg:58.92ms
step:2249/2330 train_time:132515ms step_avg:58.92ms
step:2250/2330 train_time:132576ms step_avg:58.92ms
step:2250/2330 val_loss:3.9767 train_time:132654ms step_avg:58.96ms
step:2251/2330 train_time:132674ms step_avg:58.94ms
step:2252/2330 train_time:132698ms step_avg:58.92ms
step:2253/2330 train_time:132755ms step_avg:58.92ms
step:2254/2330 train_time:132820ms step_avg:58.93ms
step:2255/2330 train_time:132877ms step_avg:58.93ms
step:2256/2330 train_time:132939ms step_avg:58.93ms
step:2257/2330 train_time:132996ms step_avg:58.93ms
step:2258/2330 train_time:133057ms step_avg:58.93ms
step:2259/2330 train_time:133115ms step_avg:58.93ms
step:2260/2330 train_time:133175ms step_avg:58.93ms
step:2261/2330 train_time:133232ms step_avg:58.93ms
step:2262/2330 train_time:133292ms step_avg:58.93ms
step:2263/2330 train_time:133349ms step_avg:58.93ms
step:2264/2330 train_time:133411ms step_avg:58.93ms
step:2265/2330 train_time:133467ms step_avg:58.93ms
step:2266/2330 train_time:133529ms step_avg:58.93ms
step:2267/2330 train_time:133585ms step_avg:58.93ms
step:2268/2330 train_time:133649ms step_avg:58.93ms
step:2269/2330 train_time:133706ms step_avg:58.93ms
step:2270/2330 train_time:133770ms step_avg:58.93ms
step:2271/2330 train_time:133827ms step_avg:58.93ms
step:2272/2330 train_time:133892ms step_avg:58.93ms
step:2273/2330 train_time:133948ms step_avg:58.93ms
step:2274/2330 train_time:134012ms step_avg:58.93ms
step:2275/2330 train_time:134068ms step_avg:58.93ms
step:2276/2330 train_time:134131ms step_avg:58.93ms
step:2277/2330 train_time:134187ms step_avg:58.93ms
step:2278/2330 train_time:134249ms step_avg:58.93ms
step:2279/2330 train_time:134305ms step_avg:58.93ms
step:2280/2330 train_time:134367ms step_avg:58.93ms
step:2281/2330 train_time:134423ms step_avg:58.93ms
step:2282/2330 train_time:134485ms step_avg:58.93ms
step:2283/2330 train_time:134541ms step_avg:58.93ms
step:2284/2330 train_time:134603ms step_avg:58.93ms
step:2285/2330 train_time:134660ms step_avg:58.93ms
step:2286/2330 train_time:134723ms step_avg:58.93ms
step:2287/2330 train_time:134780ms step_avg:58.93ms
step:2288/2330 train_time:134844ms step_avg:58.94ms
step:2289/2330 train_time:134901ms step_avg:58.93ms
step:2290/2330 train_time:134965ms step_avg:58.94ms
step:2291/2330 train_time:135022ms step_avg:58.94ms
step:2292/2330 train_time:135084ms step_avg:58.94ms
step:2293/2330 train_time:135142ms step_avg:58.94ms
step:2294/2330 train_time:135204ms step_avg:58.94ms
step:2295/2330 train_time:135262ms step_avg:58.94ms
step:2296/2330 train_time:135323ms step_avg:58.94ms
step:2297/2330 train_time:135380ms step_avg:58.94ms
step:2298/2330 train_time:135442ms step_avg:58.94ms
step:2299/2330 train_time:135499ms step_avg:58.94ms
step:2300/2330 train_time:135560ms step_avg:58.94ms
step:2301/2330 train_time:135617ms step_avg:58.94ms
step:2302/2330 train_time:135679ms step_avg:58.94ms
step:2303/2330 train_time:135736ms step_avg:58.94ms
step:2304/2330 train_time:135797ms step_avg:58.94ms
step:2305/2330 train_time:135854ms step_avg:58.94ms
step:2306/2330 train_time:135916ms step_avg:58.94ms
step:2307/2330 train_time:135974ms step_avg:58.94ms
step:2308/2330 train_time:136035ms step_avg:58.94ms
step:2309/2330 train_time:136093ms step_avg:58.94ms
step:2310/2330 train_time:136154ms step_avg:58.94ms
step:2311/2330 train_time:136211ms step_avg:58.94ms
step:2312/2330 train_time:136273ms step_avg:58.94ms
step:2313/2330 train_time:136329ms step_avg:58.94ms
step:2314/2330 train_time:136393ms step_avg:58.94ms
step:2315/2330 train_time:136449ms step_avg:58.94ms
step:2316/2330 train_time:136512ms step_avg:58.94ms
step:2317/2330 train_time:136568ms step_avg:58.94ms
step:2318/2330 train_time:136630ms step_avg:58.94ms
step:2319/2330 train_time:136687ms step_avg:58.94ms
step:2320/2330 train_time:136750ms step_avg:58.94ms
step:2321/2330 train_time:136807ms step_avg:58.94ms
step:2322/2330 train_time:136869ms step_avg:58.94ms
step:2323/2330 train_time:136926ms step_avg:58.94ms
step:2324/2330 train_time:136990ms step_avg:58.95ms
step:2325/2330 train_time:137046ms step_avg:58.94ms
step:2326/2330 train_time:137109ms step_avg:58.95ms
step:2327/2330 train_time:137165ms step_avg:58.95ms
step:2328/2330 train_time:137228ms step_avg:58.95ms
step:2329/2330 train_time:137285ms step_avg:58.95ms
step:2330/2330 train_time:137347ms step_avg:58.95ms
step:2330/2330 val_loss:3.9626 train_time:137427ms step_avg:58.98ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
