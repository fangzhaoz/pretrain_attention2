import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_muon_lr3e-3"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=3e-3, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 04:02:38 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   34C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   32C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:71ms step_avg:71.35ms
step:2/2330 train_time:140ms step_avg:69.93ms
step:3/2330 train_time:153ms step_avg:51.05ms
step:4/2330 train_time:166ms step_avg:41.59ms
step:5/2330 train_time:179ms step_avg:35.74ms
step:6/2330 train_time:211ms step_avg:35.14ms
step:7/2330 train_time:244ms step_avg:34.89ms
step:8/2330 train_time:289ms step_avg:36.09ms
step:9/2330 train_time:323ms step_avg:35.85ms
step:10/2330 train_time:367ms step_avg:36.71ms
step:11/2330 train_time:401ms step_avg:36.49ms
step:12/2330 train_time:445ms step_avg:37.11ms
step:13/2330 train_time:480ms step_avg:36.92ms
step:14/2330 train_time:525ms step_avg:37.47ms
step:15/2330 train_time:559ms step_avg:37.28ms
step:16/2330 train_time:603ms step_avg:37.70ms
step:17/2330 train_time:638ms step_avg:37.55ms
step:18/2330 train_time:682ms step_avg:37.90ms
step:19/2330 train_time:717ms step_avg:37.75ms
step:20/2330 train_time:762ms step_avg:38.08ms
step:21/2330 train_time:796ms step_avg:37.92ms
step:22/2330 train_time:841ms step_avg:38.22ms
step:23/2330 train_time:876ms step_avg:38.07ms
step:24/2330 train_time:920ms step_avg:38.33ms
step:25/2330 train_time:955ms step_avg:38.20ms
step:26/2330 train_time:1003ms step_avg:38.57ms
step:27/2330 train_time:1042ms step_avg:38.59ms
step:28/2330 train_time:1090ms step_avg:38.94ms
step:29/2330 train_time:1130ms step_avg:38.96ms
step:30/2330 train_time:1176ms step_avg:39.19ms
step:31/2330 train_time:1211ms step_avg:39.07ms
step:32/2330 train_time:1256ms step_avg:39.25ms
step:33/2330 train_time:1292ms step_avg:39.16ms
step:34/2330 train_time:1337ms step_avg:39.33ms
step:35/2330 train_time:1372ms step_avg:39.20ms
step:36/2330 train_time:1417ms step_avg:39.36ms
step:37/2330 train_time:1452ms step_avg:39.24ms
step:38/2330 train_time:1497ms step_avg:39.40ms
step:39/2330 train_time:1532ms step_avg:39.29ms
step:40/2330 train_time:1577ms step_avg:39.43ms
step:41/2330 train_time:1612ms step_avg:39.31ms
step:42/2330 train_time:1656ms step_avg:39.43ms
step:43/2330 train_time:1691ms step_avg:39.32ms
step:44/2330 train_time:1736ms step_avg:39.45ms
step:45/2330 train_time:1771ms step_avg:39.36ms
step:46/2330 train_time:1815ms step_avg:39.47ms
step:47/2330 train_time:1851ms step_avg:39.37ms
step:48/2330 train_time:1896ms step_avg:39.50ms
step:49/2330 train_time:1932ms step_avg:39.44ms
step:50/2330 train_time:1978ms step_avg:39.56ms
step:51/2330 train_time:2013ms step_avg:39.48ms
step:52/2330 train_time:2059ms step_avg:39.60ms
step:53/2330 train_time:2095ms step_avg:39.53ms
step:54/2330 train_time:2141ms step_avg:39.65ms
step:55/2330 train_time:2177ms step_avg:39.59ms
step:56/2330 train_time:2223ms step_avg:39.70ms
step:57/2330 train_time:2259ms step_avg:39.63ms
step:58/2330 train_time:2304ms step_avg:39.72ms
step:59/2330 train_time:2339ms step_avg:39.65ms
step:60/2330 train_time:2384ms step_avg:39.74ms
step:61/2330 train_time:2420ms step_avg:39.67ms
step:62/2330 train_time:2465ms step_avg:39.76ms
step:63/2330 train_time:2501ms step_avg:39.69ms
step:64/2330 train_time:2546ms step_avg:39.78ms
step:65/2330 train_time:2581ms step_avg:39.71ms
step:66/2330 train_time:2626ms step_avg:39.79ms
step:67/2330 train_time:2662ms step_avg:39.73ms
step:68/2330 train_time:2706ms step_avg:39.80ms
step:69/2330 train_time:2741ms step_avg:39.73ms
step:70/2330 train_time:2786ms step_avg:39.81ms
step:71/2330 train_time:2822ms step_avg:39.75ms
step:72/2330 train_time:2867ms step_avg:39.82ms
step:73/2330 train_time:2902ms step_avg:39.76ms
step:74/2330 train_time:2949ms step_avg:39.85ms
step:75/2330 train_time:2986ms step_avg:39.81ms
step:76/2330 train_time:3032ms step_avg:39.89ms
step:77/2330 train_time:3069ms step_avg:39.86ms
step:78/2330 train_time:3116ms step_avg:39.95ms
step:79/2330 train_time:3151ms step_avg:39.89ms
step:80/2330 train_time:3197ms step_avg:39.97ms
step:81/2330 train_time:3233ms step_avg:39.91ms
step:82/2330 train_time:3277ms step_avg:39.97ms
step:83/2330 train_time:3313ms step_avg:39.91ms
step:84/2330 train_time:3357ms step_avg:39.96ms
step:85/2330 train_time:3392ms step_avg:39.91ms
step:86/2330 train_time:3438ms step_avg:39.98ms
step:87/2330 train_time:3473ms step_avg:39.92ms
step:88/2330 train_time:3519ms step_avg:39.99ms
step:89/2330 train_time:3554ms step_avg:39.93ms
step:90/2330 train_time:3599ms step_avg:39.99ms
step:91/2330 train_time:3634ms step_avg:39.93ms
step:92/2330 train_time:3680ms step_avg:40.00ms
step:93/2330 train_time:3715ms step_avg:39.94ms
step:94/2330 train_time:3760ms step_avg:39.99ms
step:95/2330 train_time:3794ms step_avg:39.94ms
step:96/2330 train_time:3839ms step_avg:39.99ms
step:97/2330 train_time:3874ms step_avg:39.94ms
step:98/2330 train_time:3921ms step_avg:40.01ms
step:99/2330 train_time:3956ms step_avg:39.96ms
step:100/2330 train_time:4001ms step_avg:40.01ms
step:101/2330 train_time:4037ms step_avg:39.97ms
step:102/2330 train_time:4083ms step_avg:40.03ms
step:103/2330 train_time:4119ms step_avg:39.99ms
step:104/2330 train_time:4164ms step_avg:40.04ms
step:105/2330 train_time:4200ms step_avg:40.00ms
step:106/2330 train_time:4245ms step_avg:40.04ms
step:107/2330 train_time:4281ms step_avg:40.01ms
step:108/2330 train_time:4326ms step_avg:40.06ms
step:109/2330 train_time:4362ms step_avg:40.02ms
step:110/2330 train_time:4407ms step_avg:40.07ms
step:111/2330 train_time:4443ms step_avg:40.03ms
step:112/2330 train_time:4489ms step_avg:40.08ms
step:113/2330 train_time:4524ms step_avg:40.04ms
step:114/2330 train_time:4569ms step_avg:40.08ms
step:115/2330 train_time:4604ms step_avg:40.04ms
step:116/2330 train_time:4650ms step_avg:40.09ms
step:117/2330 train_time:4686ms step_avg:40.05ms
step:118/2330 train_time:4732ms step_avg:40.10ms
step:119/2330 train_time:4768ms step_avg:40.07ms
step:120/2330 train_time:4813ms step_avg:40.10ms
step:121/2330 train_time:4848ms step_avg:40.06ms
step:122/2330 train_time:4893ms step_avg:40.11ms
step:123/2330 train_time:4929ms step_avg:40.07ms
step:124/2330 train_time:4974ms step_avg:40.11ms
step:125/2330 train_time:5011ms step_avg:40.09ms
step:126/2330 train_time:5056ms step_avg:40.13ms
step:127/2330 train_time:5092ms step_avg:40.10ms
step:128/2330 train_time:5138ms step_avg:40.14ms
step:129/2330 train_time:5173ms step_avg:40.10ms
step:130/2330 train_time:5218ms step_avg:40.14ms
step:131/2330 train_time:5254ms step_avg:40.11ms
step:132/2330 train_time:5299ms step_avg:40.14ms
step:133/2330 train_time:5334ms step_avg:40.10ms
step:134/2330 train_time:5379ms step_avg:40.14ms
step:135/2330 train_time:5415ms step_avg:40.11ms
step:136/2330 train_time:5460ms step_avg:40.15ms
step:137/2330 train_time:5496ms step_avg:40.12ms
step:138/2330 train_time:5541ms step_avg:40.15ms
step:139/2330 train_time:5577ms step_avg:40.12ms
step:140/2330 train_time:5623ms step_avg:40.16ms
step:141/2330 train_time:5659ms step_avg:40.13ms
step:142/2330 train_time:5704ms step_avg:40.17ms
step:143/2330 train_time:5739ms step_avg:40.14ms
step:144/2330 train_time:5784ms step_avg:40.17ms
step:145/2330 train_time:5820ms step_avg:40.14ms
step:146/2330 train_time:5866ms step_avg:40.18ms
step:147/2330 train_time:5901ms step_avg:40.14ms
step:148/2330 train_time:5947ms step_avg:40.18ms
step:149/2330 train_time:5984ms step_avg:40.16ms
step:150/2330 train_time:6031ms step_avg:40.21ms
step:151/2330 train_time:6068ms step_avg:40.19ms
step:152/2330 train_time:6112ms step_avg:40.21ms
step:153/2330 train_time:6148ms step_avg:40.18ms
step:154/2330 train_time:6194ms step_avg:40.22ms
step:155/2330 train_time:6229ms step_avg:40.19ms
step:156/2330 train_time:6274ms step_avg:40.22ms
step:157/2330 train_time:6309ms step_avg:40.18ms
step:158/2330 train_time:6355ms step_avg:40.22ms
step:159/2330 train_time:6390ms step_avg:40.19ms
step:160/2330 train_time:6436ms step_avg:40.23ms
step:161/2330 train_time:6471ms step_avg:40.19ms
step:162/2330 train_time:6517ms step_avg:40.23ms
step:163/2330 train_time:6552ms step_avg:40.20ms
step:164/2330 train_time:6597ms step_avg:40.23ms
step:165/2330 train_time:6632ms step_avg:40.20ms
step:166/2330 train_time:6678ms step_avg:40.23ms
step:167/2330 train_time:6713ms step_avg:40.20ms
step:168/2330 train_time:6758ms step_avg:40.23ms
step:169/2330 train_time:6794ms step_avg:40.20ms
step:170/2330 train_time:6839ms step_avg:40.23ms
step:171/2330 train_time:6874ms step_avg:40.20ms
step:172/2330 train_time:6921ms step_avg:40.24ms
step:173/2330 train_time:6956ms step_avg:40.21ms
step:174/2330 train_time:7001ms step_avg:40.24ms
step:175/2330 train_time:7037ms step_avg:40.21ms
step:176/2330 train_time:7082ms step_avg:40.24ms
step:177/2330 train_time:7119ms step_avg:40.22ms
step:178/2330 train_time:7165ms step_avg:40.25ms
step:179/2330 train_time:7200ms step_avg:40.22ms
step:180/2330 train_time:7245ms step_avg:40.25ms
step:181/2330 train_time:7281ms step_avg:40.23ms
step:182/2330 train_time:7327ms step_avg:40.26ms
step:183/2330 train_time:7362ms step_avg:40.23ms
step:184/2330 train_time:7407ms step_avg:40.26ms
step:185/2330 train_time:7443ms step_avg:40.23ms
step:186/2330 train_time:7489ms step_avg:40.26ms
step:187/2330 train_time:7525ms step_avg:40.24ms
step:188/2330 train_time:7571ms step_avg:40.27ms
step:189/2330 train_time:7607ms step_avg:40.25ms
step:190/2330 train_time:7652ms step_avg:40.27ms
step:191/2330 train_time:7688ms step_avg:40.25ms
step:192/2330 train_time:7734ms step_avg:40.28ms
step:193/2330 train_time:7769ms step_avg:40.26ms
step:194/2330 train_time:7815ms step_avg:40.28ms
step:195/2330 train_time:7850ms step_avg:40.26ms
step:196/2330 train_time:7896ms step_avg:40.28ms
step:197/2330 train_time:7932ms step_avg:40.26ms
step:198/2330 train_time:7977ms step_avg:40.29ms
step:199/2330 train_time:8013ms step_avg:40.27ms
step:200/2330 train_time:8058ms step_avg:40.29ms
step:201/2330 train_time:8094ms step_avg:40.27ms
step:202/2330 train_time:8139ms step_avg:40.29ms
step:203/2330 train_time:8175ms step_avg:40.27ms
step:204/2330 train_time:8221ms step_avg:40.30ms
step:205/2330 train_time:8257ms step_avg:40.28ms
step:206/2330 train_time:8302ms step_avg:40.30ms
step:207/2330 train_time:8337ms step_avg:40.28ms
step:208/2330 train_time:8381ms step_avg:40.29ms
step:209/2330 train_time:8417ms step_avg:40.27ms
step:210/2330 train_time:8463ms step_avg:40.30ms
step:211/2330 train_time:8499ms step_avg:40.28ms
step:212/2330 train_time:8544ms step_avg:40.30ms
step:213/2330 train_time:8580ms step_avg:40.28ms
step:214/2330 train_time:8624ms step_avg:40.30ms
step:215/2330 train_time:8660ms step_avg:40.28ms
step:216/2330 train_time:8705ms step_avg:40.30ms
step:217/2330 train_time:8741ms step_avg:40.28ms
step:218/2330 train_time:8786ms step_avg:40.30ms
step:219/2330 train_time:8822ms step_avg:40.28ms
step:220/2330 train_time:8867ms step_avg:40.31ms
step:221/2330 train_time:8904ms step_avg:40.29ms
step:222/2330 train_time:8949ms step_avg:40.31ms
step:223/2330 train_time:8986ms step_avg:40.30ms
step:224/2330 train_time:9033ms step_avg:40.33ms
step:225/2330 train_time:9070ms step_avg:40.31ms
step:226/2330 train_time:9115ms step_avg:40.33ms
step:227/2330 train_time:9150ms step_avg:40.31ms
step:228/2330 train_time:9196ms step_avg:40.34ms
step:229/2330 train_time:9232ms step_avg:40.31ms
step:230/2330 train_time:9276ms step_avg:40.33ms
step:231/2330 train_time:9312ms step_avg:40.31ms
step:232/2330 train_time:9358ms step_avg:40.34ms
step:233/2330 train_time:9393ms step_avg:40.31ms
step:234/2330 train_time:9438ms step_avg:40.33ms
step:235/2330 train_time:9474ms step_avg:40.31ms
step:236/2330 train_time:9520ms step_avg:40.34ms
step:237/2330 train_time:9555ms step_avg:40.31ms
step:238/2330 train_time:9600ms step_avg:40.33ms
step:239/2330 train_time:9635ms step_avg:40.31ms
step:240/2330 train_time:9680ms step_avg:40.34ms
step:241/2330 train_time:9716ms step_avg:40.32ms
step:242/2330 train_time:9761ms step_avg:40.34ms
step:243/2330 train_time:9796ms step_avg:40.31ms
step:244/2330 train_time:9841ms step_avg:40.33ms
step:245/2330 train_time:9877ms step_avg:40.32ms
step:246/2330 train_time:9923ms step_avg:40.34ms
step:247/2330 train_time:9959ms step_avg:40.32ms
step:248/2330 train_time:10005ms step_avg:40.34ms
step:249/2330 train_time:10040ms step_avg:40.32ms
step:250/2330 train_time:10085ms step_avg:40.34ms
step:250/2330 val_loss:5.4280 train_time:10175ms step_avg:40.70ms
step:251/2330 train_time:10189ms step_avg:40.59ms
step:252/2330 train_time:10201ms step_avg:40.48ms
step:253/2330 train_time:10212ms step_avg:40.36ms
step:254/2330 train_time:10249ms step_avg:40.35ms
step:255/2330 train_time:10284ms step_avg:40.33ms
step:256/2330 train_time:10328ms step_avg:40.34ms
step:257/2330 train_time:10363ms step_avg:40.32ms
step:258/2330 train_time:10407ms step_avg:40.34ms
step:259/2330 train_time:10442ms step_avg:40.32ms
step:260/2330 train_time:10489ms step_avg:40.34ms
step:261/2330 train_time:10530ms step_avg:40.35ms
step:262/2330 train_time:10579ms step_avg:40.38ms
step:263/2330 train_time:10616ms step_avg:40.36ms
step:264/2330 train_time:10660ms step_avg:40.38ms
step:265/2330 train_time:10696ms step_avg:40.36ms
step:266/2330 train_time:10741ms step_avg:40.38ms
step:267/2330 train_time:10777ms step_avg:40.36ms
step:268/2330 train_time:10821ms step_avg:40.38ms
step:269/2330 train_time:10857ms step_avg:40.36ms
step:270/2330 train_time:10902ms step_avg:40.38ms
step:271/2330 train_time:10937ms step_avg:40.36ms
step:272/2330 train_time:10982ms step_avg:40.38ms
step:273/2330 train_time:11017ms step_avg:40.36ms
step:274/2330 train_time:11063ms step_avg:40.37ms
step:275/2330 train_time:11100ms step_avg:40.36ms
step:276/2330 train_time:11147ms step_avg:40.39ms
step:277/2330 train_time:11184ms step_avg:40.38ms
step:278/2330 train_time:11231ms step_avg:40.40ms
step:279/2330 train_time:11265ms step_avg:40.38ms
step:280/2330 train_time:11310ms step_avg:40.39ms
step:281/2330 train_time:11345ms step_avg:40.37ms
step:282/2330 train_time:11390ms step_avg:40.39ms
step:283/2330 train_time:11427ms step_avg:40.38ms
step:284/2330 train_time:11473ms step_avg:40.40ms
step:285/2330 train_time:11510ms step_avg:40.39ms
step:286/2330 train_time:11557ms step_avg:40.41ms
step:287/2330 train_time:11594ms step_avg:40.40ms
step:288/2330 train_time:11639ms step_avg:40.41ms
step:289/2330 train_time:11674ms step_avg:40.40ms
step:290/2330 train_time:11720ms step_avg:40.41ms
step:291/2330 train_time:11755ms step_avg:40.40ms
step:292/2330 train_time:11799ms step_avg:40.41ms
step:293/2330 train_time:11835ms step_avg:40.39ms
step:294/2330 train_time:11879ms step_avg:40.40ms
step:295/2330 train_time:11914ms step_avg:40.39ms
step:296/2330 train_time:11959ms step_avg:40.40ms
step:297/2330 train_time:11995ms step_avg:40.39ms
step:298/2330 train_time:12040ms step_avg:40.40ms
step:299/2330 train_time:12076ms step_avg:40.39ms
step:300/2330 train_time:12122ms step_avg:40.41ms
step:301/2330 train_time:12158ms step_avg:40.39ms
step:302/2330 train_time:12203ms step_avg:40.41ms
step:303/2330 train_time:12239ms step_avg:40.39ms
step:304/2330 train_time:12284ms step_avg:40.41ms
step:305/2330 train_time:12321ms step_avg:40.40ms
step:306/2330 train_time:12366ms step_avg:40.41ms
step:307/2330 train_time:12402ms step_avg:40.40ms
step:308/2330 train_time:12448ms step_avg:40.42ms
step:309/2330 train_time:12486ms step_avg:40.41ms
step:310/2330 train_time:12532ms step_avg:40.43ms
step:311/2330 train_time:12567ms step_avg:40.41ms
step:312/2330 train_time:12614ms step_avg:40.43ms
step:313/2330 train_time:12650ms step_avg:40.41ms
step:314/2330 train_time:12695ms step_avg:40.43ms
step:315/2330 train_time:12731ms step_avg:40.42ms
step:316/2330 train_time:12776ms step_avg:40.43ms
step:317/2330 train_time:12812ms step_avg:40.42ms
step:318/2330 train_time:12857ms step_avg:40.43ms
step:319/2330 train_time:12892ms step_avg:40.41ms
step:320/2330 train_time:12937ms step_avg:40.43ms
step:321/2330 train_time:12973ms step_avg:40.41ms
step:322/2330 train_time:13018ms step_avg:40.43ms
step:323/2330 train_time:13054ms step_avg:40.41ms
step:324/2330 train_time:13099ms step_avg:40.43ms
step:325/2330 train_time:13135ms step_avg:40.41ms
step:326/2330 train_time:13180ms step_avg:40.43ms
step:327/2330 train_time:13216ms step_avg:40.42ms
step:328/2330 train_time:13261ms step_avg:40.43ms
step:329/2330 train_time:13297ms step_avg:40.42ms
step:330/2330 train_time:13343ms step_avg:40.43ms
step:331/2330 train_time:13379ms step_avg:40.42ms
step:332/2330 train_time:13425ms step_avg:40.44ms
step:333/2330 train_time:13461ms step_avg:40.42ms
step:334/2330 train_time:13506ms step_avg:40.44ms
step:335/2330 train_time:13542ms step_avg:40.42ms
step:336/2330 train_time:13587ms step_avg:40.44ms
step:337/2330 train_time:13623ms step_avg:40.43ms
step:338/2330 train_time:13669ms step_avg:40.44ms
step:339/2330 train_time:13705ms step_avg:40.43ms
step:340/2330 train_time:13751ms step_avg:40.45ms
step:341/2330 train_time:13788ms step_avg:40.43ms
step:342/2330 train_time:13833ms step_avg:40.45ms
step:343/2330 train_time:13869ms step_avg:40.43ms
step:344/2330 train_time:13914ms step_avg:40.45ms
step:345/2330 train_time:13949ms step_avg:40.43ms
step:346/2330 train_time:13994ms step_avg:40.45ms
step:347/2330 train_time:14030ms step_avg:40.43ms
step:348/2330 train_time:14075ms step_avg:40.44ms
step:349/2330 train_time:14111ms step_avg:40.43ms
step:350/2330 train_time:14157ms step_avg:40.45ms
step:351/2330 train_time:14193ms step_avg:40.44ms
step:352/2330 train_time:14238ms step_avg:40.45ms
step:353/2330 train_time:14274ms step_avg:40.44ms
step:354/2330 train_time:14320ms step_avg:40.45ms
step:355/2330 train_time:14355ms step_avg:40.44ms
step:356/2330 train_time:14401ms step_avg:40.45ms
step:357/2330 train_time:14437ms step_avg:40.44ms
step:358/2330 train_time:14483ms step_avg:40.45ms
step:359/2330 train_time:14519ms step_avg:40.44ms
step:360/2330 train_time:14565ms step_avg:40.46ms
step:361/2330 train_time:14600ms step_avg:40.44ms
step:362/2330 train_time:14645ms step_avg:40.46ms
step:363/2330 train_time:14681ms step_avg:40.44ms
step:364/2330 train_time:14727ms step_avg:40.46ms
step:365/2330 train_time:14763ms step_avg:40.45ms
step:366/2330 train_time:14809ms step_avg:40.46ms
step:367/2330 train_time:14845ms step_avg:40.45ms
step:368/2330 train_time:14891ms step_avg:40.46ms
step:369/2330 train_time:14928ms step_avg:40.45ms
step:370/2330 train_time:14973ms step_avg:40.47ms
step:371/2330 train_time:15009ms step_avg:40.46ms
step:372/2330 train_time:15054ms step_avg:40.47ms
step:373/2330 train_time:15091ms step_avg:40.46ms
step:374/2330 train_time:15136ms step_avg:40.47ms
step:375/2330 train_time:15172ms step_avg:40.46ms
step:376/2330 train_time:15217ms step_avg:40.47ms
step:377/2330 train_time:15252ms step_avg:40.46ms
step:378/2330 train_time:15298ms step_avg:40.47ms
step:379/2330 train_time:15333ms step_avg:40.46ms
step:380/2330 train_time:15379ms step_avg:40.47ms
step:381/2330 train_time:15415ms step_avg:40.46ms
step:382/2330 train_time:15461ms step_avg:40.47ms
step:383/2330 train_time:15496ms step_avg:40.46ms
step:384/2330 train_time:15542ms step_avg:40.47ms
step:385/2330 train_time:15577ms step_avg:40.46ms
step:386/2330 train_time:15623ms step_avg:40.47ms
step:387/2330 train_time:15659ms step_avg:40.46ms
step:388/2330 train_time:15704ms step_avg:40.47ms
step:389/2330 train_time:15740ms step_avg:40.46ms
step:390/2330 train_time:15786ms step_avg:40.48ms
step:391/2330 train_time:15822ms step_avg:40.47ms
step:392/2330 train_time:15868ms step_avg:40.48ms
step:393/2330 train_time:15904ms step_avg:40.47ms
step:394/2330 train_time:15950ms step_avg:40.48ms
step:395/2330 train_time:15986ms step_avg:40.47ms
step:396/2330 train_time:16032ms step_avg:40.48ms
step:397/2330 train_time:16068ms step_avg:40.47ms
step:398/2330 train_time:16113ms step_avg:40.49ms
step:399/2330 train_time:16149ms step_avg:40.47ms
step:400/2330 train_time:16195ms step_avg:40.49ms
step:401/2330 train_time:16230ms step_avg:40.47ms
step:402/2330 train_time:16275ms step_avg:40.49ms
step:403/2330 train_time:16311ms step_avg:40.47ms
step:404/2330 train_time:16357ms step_avg:40.49ms
step:405/2330 train_time:16392ms step_avg:40.48ms
step:406/2330 train_time:16437ms step_avg:40.49ms
step:407/2330 train_time:16473ms step_avg:40.47ms
step:408/2330 train_time:16518ms step_avg:40.49ms
step:409/2330 train_time:16554ms step_avg:40.47ms
step:410/2330 train_time:16599ms step_avg:40.49ms
step:411/2330 train_time:16636ms step_avg:40.48ms
step:412/2330 train_time:16681ms step_avg:40.49ms
step:413/2330 train_time:16718ms step_avg:40.48ms
step:414/2330 train_time:16763ms step_avg:40.49ms
step:415/2330 train_time:16799ms step_avg:40.48ms
step:416/2330 train_time:16845ms step_avg:40.49ms
step:417/2330 train_time:16881ms step_avg:40.48ms
step:418/2330 train_time:16927ms step_avg:40.49ms
step:419/2330 train_time:16963ms step_avg:40.48ms
step:420/2330 train_time:17010ms step_avg:40.50ms
step:421/2330 train_time:17046ms step_avg:40.49ms
step:422/2330 train_time:17091ms step_avg:40.50ms
step:423/2330 train_time:17127ms step_avg:40.49ms
step:424/2330 train_time:17172ms step_avg:40.50ms
step:425/2330 train_time:17208ms step_avg:40.49ms
step:426/2330 train_time:17253ms step_avg:40.50ms
step:427/2330 train_time:17290ms step_avg:40.49ms
step:428/2330 train_time:17335ms step_avg:40.50ms
step:429/2330 train_time:17371ms step_avg:40.49ms
step:430/2330 train_time:17416ms step_avg:40.50ms
step:431/2330 train_time:17452ms step_avg:40.49ms
step:432/2330 train_time:17497ms step_avg:40.50ms
step:433/2330 train_time:17533ms step_avg:40.49ms
step:434/2330 train_time:17578ms step_avg:40.50ms
step:435/2330 train_time:17614ms step_avg:40.49ms
step:436/2330 train_time:17659ms step_avg:40.50ms
step:437/2330 train_time:17695ms step_avg:40.49ms
step:438/2330 train_time:17741ms step_avg:40.51ms
step:439/2330 train_time:17776ms step_avg:40.49ms
step:440/2330 train_time:17822ms step_avg:40.50ms
step:441/2330 train_time:17857ms step_avg:40.49ms
step:442/2330 train_time:17903ms step_avg:40.51ms
step:443/2330 train_time:17940ms step_avg:40.50ms
step:444/2330 train_time:17984ms step_avg:40.51ms
step:445/2330 train_time:18020ms step_avg:40.49ms
step:446/2330 train_time:18065ms step_avg:40.51ms
step:447/2330 train_time:18101ms step_avg:40.50ms
step:448/2330 train_time:18146ms step_avg:40.50ms
step:449/2330 train_time:18182ms step_avg:40.50ms
step:450/2330 train_time:18230ms step_avg:40.51ms
step:451/2330 train_time:18265ms step_avg:40.50ms
step:452/2330 train_time:18310ms step_avg:40.51ms
step:453/2330 train_time:18346ms step_avg:40.50ms
step:454/2330 train_time:18392ms step_avg:40.51ms
step:455/2330 train_time:18429ms step_avg:40.50ms
step:456/2330 train_time:18474ms step_avg:40.51ms
step:457/2330 train_time:18510ms step_avg:40.50ms
step:458/2330 train_time:18556ms step_avg:40.51ms
step:459/2330 train_time:18592ms step_avg:40.51ms
step:460/2330 train_time:18637ms step_avg:40.52ms
step:461/2330 train_time:18673ms step_avg:40.51ms
step:462/2330 train_time:18718ms step_avg:40.52ms
step:463/2330 train_time:18754ms step_avg:40.51ms
step:464/2330 train_time:18799ms step_avg:40.51ms
step:465/2330 train_time:18834ms step_avg:40.50ms
step:466/2330 train_time:18880ms step_avg:40.52ms
step:467/2330 train_time:18915ms step_avg:40.50ms
step:468/2330 train_time:18962ms step_avg:40.52ms
step:469/2330 train_time:18997ms step_avg:40.51ms
step:470/2330 train_time:19043ms step_avg:40.52ms
step:471/2330 train_time:19078ms step_avg:40.51ms
step:472/2330 train_time:19123ms step_avg:40.52ms
step:473/2330 train_time:19160ms step_avg:40.51ms
step:474/2330 train_time:19205ms step_avg:40.52ms
step:475/2330 train_time:19241ms step_avg:40.51ms
step:476/2330 train_time:19286ms step_avg:40.52ms
step:477/2330 train_time:19322ms step_avg:40.51ms
step:478/2330 train_time:19368ms step_avg:40.52ms
step:479/2330 train_time:19404ms step_avg:40.51ms
step:480/2330 train_time:19451ms step_avg:40.52ms
step:481/2330 train_time:19487ms step_avg:40.51ms
step:482/2330 train_time:19532ms step_avg:40.52ms
step:483/2330 train_time:19569ms step_avg:40.52ms
step:484/2330 train_time:19614ms step_avg:40.52ms
step:485/2330 train_time:19650ms step_avg:40.52ms
step:486/2330 train_time:19695ms step_avg:40.53ms
step:487/2330 train_time:19732ms step_avg:40.52ms
step:488/2330 train_time:19777ms step_avg:40.53ms
step:489/2330 train_time:19812ms step_avg:40.52ms
step:490/2330 train_time:19857ms step_avg:40.53ms
step:491/2330 train_time:19893ms step_avg:40.52ms
step:492/2330 train_time:19939ms step_avg:40.53ms
step:493/2330 train_time:19974ms step_avg:40.52ms
step:494/2330 train_time:20019ms step_avg:40.53ms
step:495/2330 train_time:20055ms step_avg:40.52ms
step:496/2330 train_time:20101ms step_avg:40.53ms
step:497/2330 train_time:20136ms step_avg:40.52ms
step:498/2330 train_time:20182ms step_avg:40.53ms
step:499/2330 train_time:20218ms step_avg:40.52ms
step:500/2330 train_time:20264ms step_avg:40.53ms
step:500/2330 val_loss:5.2905 train_time:20354ms step_avg:40.71ms
step:501/2330 train_time:20367ms step_avg:40.65ms
step:502/2330 train_time:20379ms step_avg:40.59ms
step:503/2330 train_time:20390ms step_avg:40.54ms
step:504/2330 train_time:20427ms step_avg:40.53ms
step:505/2330 train_time:20462ms step_avg:40.52ms
step:506/2330 train_time:20506ms step_avg:40.53ms
step:507/2330 train_time:20542ms step_avg:40.52ms
step:508/2330 train_time:20587ms step_avg:40.53ms
step:509/2330 train_time:20622ms step_avg:40.52ms
step:510/2330 train_time:20672ms step_avg:40.53ms
step:511/2330 train_time:20713ms step_avg:40.54ms
step:512/2330 train_time:20761ms step_avg:40.55ms
step:513/2330 train_time:20797ms step_avg:40.54ms
step:514/2330 train_time:20842ms step_avg:40.55ms
step:515/2330 train_time:20877ms step_avg:40.54ms
step:516/2330 train_time:20922ms step_avg:40.55ms
step:517/2330 train_time:20957ms step_avg:40.54ms
step:518/2330 train_time:21002ms step_avg:40.54ms
step:519/2330 train_time:21038ms step_avg:40.54ms
step:520/2330 train_time:21083ms step_avg:40.54ms
step:521/2330 train_time:21118ms step_avg:40.53ms
step:522/2330 train_time:21163ms step_avg:40.54ms
step:523/2330 train_time:21198ms step_avg:40.53ms
step:524/2330 train_time:21243ms step_avg:40.54ms
step:525/2330 train_time:21278ms step_avg:40.53ms
step:526/2330 train_time:21324ms step_avg:40.54ms
step:527/2330 train_time:21360ms step_avg:40.53ms
step:528/2330 train_time:21405ms step_avg:40.54ms
step:529/2330 train_time:21441ms step_avg:40.53ms
step:530/2330 train_time:21485ms step_avg:40.54ms
step:531/2330 train_time:21521ms step_avg:40.53ms
step:532/2330 train_time:21566ms step_avg:40.54ms
step:533/2330 train_time:21602ms step_avg:40.53ms
step:534/2330 train_time:21650ms step_avg:40.54ms
step:535/2330 train_time:21689ms step_avg:40.54ms
step:536/2330 train_time:21735ms step_avg:40.55ms
step:537/2330 train_time:21772ms step_avg:40.54ms
step:538/2330 train_time:21817ms step_avg:40.55ms
step:539/2330 train_time:21853ms step_avg:40.54ms
step:540/2330 train_time:21898ms step_avg:40.55ms
step:541/2330 train_time:21935ms step_avg:40.54ms
step:542/2330 train_time:21980ms step_avg:40.55ms
step:543/2330 train_time:22015ms step_avg:40.54ms
step:544/2330 train_time:22061ms step_avg:40.55ms
step:545/2330 train_time:22097ms step_avg:40.55ms
step:546/2330 train_time:22142ms step_avg:40.55ms
step:547/2330 train_time:22177ms step_avg:40.54ms
step:548/2330 train_time:22222ms step_avg:40.55ms
step:549/2330 train_time:22258ms step_avg:40.54ms
step:550/2330 train_time:22303ms step_avg:40.55ms
step:551/2330 train_time:22338ms step_avg:40.54ms
step:552/2330 train_time:22383ms step_avg:40.55ms
step:553/2330 train_time:22419ms step_avg:40.54ms
step:554/2330 train_time:22463ms step_avg:40.55ms
step:555/2330 train_time:22499ms step_avg:40.54ms
step:556/2330 train_time:22544ms step_avg:40.55ms
step:557/2330 train_time:22581ms step_avg:40.54ms
step:558/2330 train_time:22628ms step_avg:40.55ms
step:559/2330 train_time:22666ms step_avg:40.55ms
step:560/2330 train_time:22711ms step_avg:40.56ms
step:561/2330 train_time:22747ms step_avg:40.55ms
step:562/2330 train_time:22793ms step_avg:40.56ms
step:563/2330 train_time:22829ms step_avg:40.55ms
step:564/2330 train_time:22874ms step_avg:40.56ms
step:565/2330 train_time:22910ms step_avg:40.55ms
step:566/2330 train_time:22955ms step_avg:40.56ms
step:567/2330 train_time:22991ms step_avg:40.55ms
step:568/2330 train_time:23036ms step_avg:40.56ms
step:569/2330 train_time:23072ms step_avg:40.55ms
step:570/2330 train_time:23117ms step_avg:40.56ms
step:571/2330 train_time:23153ms step_avg:40.55ms
step:572/2330 train_time:23198ms step_avg:40.56ms
step:573/2330 train_time:23233ms step_avg:40.55ms
step:574/2330 train_time:23279ms step_avg:40.56ms
step:575/2330 train_time:23315ms step_avg:40.55ms
step:576/2330 train_time:23360ms step_avg:40.56ms
step:577/2330 train_time:23397ms step_avg:40.55ms
step:578/2330 train_time:23442ms step_avg:40.56ms
step:579/2330 train_time:23478ms step_avg:40.55ms
step:580/2330 train_time:23523ms step_avg:40.56ms
step:581/2330 train_time:23559ms step_avg:40.55ms
step:582/2330 train_time:23604ms step_avg:40.56ms
step:583/2330 train_time:23640ms step_avg:40.55ms
step:584/2330 train_time:23687ms step_avg:40.56ms
step:585/2330 train_time:23723ms step_avg:40.55ms
step:586/2330 train_time:23769ms step_avg:40.56ms
step:587/2330 train_time:23806ms step_avg:40.56ms
step:588/2330 train_time:23851ms step_avg:40.56ms
step:589/2330 train_time:23888ms step_avg:40.56ms
step:590/2330 train_time:23933ms step_avg:40.56ms
step:591/2330 train_time:23969ms step_avg:40.56ms
step:592/2330 train_time:24014ms step_avg:40.56ms
step:593/2330 train_time:24050ms step_avg:40.56ms
step:594/2330 train_time:24095ms step_avg:40.56ms
step:595/2330 train_time:24131ms step_avg:40.56ms
step:596/2330 train_time:24176ms step_avg:40.56ms
step:597/2330 train_time:24212ms step_avg:40.56ms
step:598/2330 train_time:24257ms step_avg:40.56ms
step:599/2330 train_time:24293ms step_avg:40.56ms
step:600/2330 train_time:24338ms step_avg:40.56ms
step:601/2330 train_time:24373ms step_avg:40.55ms
step:602/2330 train_time:24419ms step_avg:40.56ms
step:603/2330 train_time:24455ms step_avg:40.55ms
step:604/2330 train_time:24500ms step_avg:40.56ms
step:605/2330 train_time:24536ms step_avg:40.56ms
step:606/2330 train_time:24582ms step_avg:40.56ms
step:607/2330 train_time:24618ms step_avg:40.56ms
step:608/2330 train_time:24664ms step_avg:40.57ms
step:609/2330 train_time:24700ms step_avg:40.56ms
step:610/2330 train_time:24745ms step_avg:40.57ms
step:611/2330 train_time:24782ms step_avg:40.56ms
step:612/2330 train_time:24828ms step_avg:40.57ms
step:613/2330 train_time:24864ms step_avg:40.56ms
step:614/2330 train_time:24910ms step_avg:40.57ms
step:615/2330 train_time:24946ms step_avg:40.56ms
step:616/2330 train_time:24992ms step_avg:40.57ms
step:617/2330 train_time:25027ms step_avg:40.56ms
step:618/2330 train_time:25073ms step_avg:40.57ms
step:619/2330 train_time:25109ms step_avg:40.56ms
step:620/2330 train_time:25154ms step_avg:40.57ms
step:621/2330 train_time:25190ms step_avg:40.56ms
step:622/2330 train_time:25236ms step_avg:40.57ms
step:623/2330 train_time:25271ms step_avg:40.56ms
step:624/2330 train_time:25317ms step_avg:40.57ms
step:625/2330 train_time:25353ms step_avg:40.56ms
step:626/2330 train_time:25398ms step_avg:40.57ms
step:627/2330 train_time:25433ms step_avg:40.56ms
step:628/2330 train_time:25478ms step_avg:40.57ms
step:629/2330 train_time:25514ms step_avg:40.56ms
step:630/2330 train_time:25561ms step_avg:40.57ms
step:631/2330 train_time:25596ms step_avg:40.56ms
step:632/2330 train_time:25641ms step_avg:40.57ms
step:633/2330 train_time:25677ms step_avg:40.56ms
step:634/2330 train_time:25722ms step_avg:40.57ms
step:635/2330 train_time:25758ms step_avg:40.56ms
step:636/2330 train_time:25803ms step_avg:40.57ms
step:637/2330 train_time:25840ms step_avg:40.56ms
step:638/2330 train_time:25886ms step_avg:40.57ms
step:639/2330 train_time:25923ms step_avg:40.57ms
step:640/2330 train_time:25969ms step_avg:40.58ms
step:641/2330 train_time:26005ms step_avg:40.57ms
step:642/2330 train_time:26050ms step_avg:40.58ms
step:643/2330 train_time:26086ms step_avg:40.57ms
step:644/2330 train_time:26131ms step_avg:40.58ms
step:645/2330 train_time:26167ms step_avg:40.57ms
step:646/2330 train_time:26213ms step_avg:40.58ms
step:647/2330 train_time:26249ms step_avg:40.57ms
step:648/2330 train_time:26295ms step_avg:40.58ms
step:649/2330 train_time:26331ms step_avg:40.57ms
step:650/2330 train_time:26376ms step_avg:40.58ms
step:651/2330 train_time:26411ms step_avg:40.57ms
step:652/2330 train_time:26457ms step_avg:40.58ms
step:653/2330 train_time:26492ms step_avg:40.57ms
step:654/2330 train_time:26538ms step_avg:40.58ms
step:655/2330 train_time:26574ms step_avg:40.57ms
step:656/2330 train_time:26619ms step_avg:40.58ms
step:657/2330 train_time:26655ms step_avg:40.57ms
step:658/2330 train_time:26700ms step_avg:40.58ms
step:659/2330 train_time:26736ms step_avg:40.57ms
step:660/2330 train_time:26782ms step_avg:40.58ms
step:661/2330 train_time:26818ms step_avg:40.57ms
step:662/2330 train_time:26863ms step_avg:40.58ms
step:663/2330 train_time:26899ms step_avg:40.57ms
step:664/2330 train_time:26944ms step_avg:40.58ms
step:665/2330 train_time:26980ms step_avg:40.57ms
step:666/2330 train_time:27025ms step_avg:40.58ms
step:667/2330 train_time:27061ms step_avg:40.57ms
step:668/2330 train_time:27107ms step_avg:40.58ms
step:669/2330 train_time:27143ms step_avg:40.57ms
step:670/2330 train_time:27190ms step_avg:40.58ms
step:671/2330 train_time:27226ms step_avg:40.57ms
step:672/2330 train_time:27272ms step_avg:40.58ms
step:673/2330 train_time:27308ms step_avg:40.58ms
step:674/2330 train_time:27353ms step_avg:40.58ms
step:675/2330 train_time:27389ms step_avg:40.58ms
step:676/2330 train_time:27435ms step_avg:40.58ms
step:677/2330 train_time:27470ms step_avg:40.58ms
step:678/2330 train_time:27515ms step_avg:40.58ms
step:679/2330 train_time:27551ms step_avg:40.58ms
step:680/2330 train_time:27597ms step_avg:40.58ms
step:681/2330 train_time:27632ms step_avg:40.57ms
step:682/2330 train_time:27678ms step_avg:40.58ms
step:683/2330 train_time:27713ms step_avg:40.58ms
step:684/2330 train_time:27758ms step_avg:40.58ms
step:685/2330 train_time:27794ms step_avg:40.57ms
step:686/2330 train_time:27839ms step_avg:40.58ms
step:687/2330 train_time:27876ms step_avg:40.58ms
step:688/2330 train_time:27922ms step_avg:40.58ms
step:689/2330 train_time:27958ms step_avg:40.58ms
step:690/2330 train_time:28004ms step_avg:40.58ms
step:691/2330 train_time:28040ms step_avg:40.58ms
step:692/2330 train_time:28086ms step_avg:40.59ms
step:693/2330 train_time:28122ms step_avg:40.58ms
step:694/2330 train_time:28168ms step_avg:40.59ms
step:695/2330 train_time:28203ms step_avg:40.58ms
step:696/2330 train_time:28250ms step_avg:40.59ms
step:697/2330 train_time:28286ms step_avg:40.58ms
step:698/2330 train_time:28331ms step_avg:40.59ms
step:699/2330 train_time:28368ms step_avg:40.58ms
step:700/2330 train_time:28413ms step_avg:40.59ms
step:701/2330 train_time:28449ms step_avg:40.58ms
step:702/2330 train_time:28495ms step_avg:40.59ms
step:703/2330 train_time:28530ms step_avg:40.58ms
step:704/2330 train_time:28575ms step_avg:40.59ms
step:705/2330 train_time:28611ms step_avg:40.58ms
step:706/2330 train_time:28657ms step_avg:40.59ms
step:707/2330 train_time:28692ms step_avg:40.58ms
step:708/2330 train_time:28738ms step_avg:40.59ms
step:709/2330 train_time:28774ms step_avg:40.58ms
step:710/2330 train_time:28819ms step_avg:40.59ms
step:711/2330 train_time:28855ms step_avg:40.58ms
step:712/2330 train_time:28900ms step_avg:40.59ms
step:713/2330 train_time:28935ms step_avg:40.58ms
step:714/2330 train_time:28981ms step_avg:40.59ms
step:715/2330 train_time:29017ms step_avg:40.58ms
step:716/2330 train_time:29063ms step_avg:40.59ms
step:717/2330 train_time:29099ms step_avg:40.58ms
step:718/2330 train_time:29144ms step_avg:40.59ms
step:719/2330 train_time:29180ms step_avg:40.58ms
step:720/2330 train_time:29227ms step_avg:40.59ms
step:721/2330 train_time:29264ms step_avg:40.59ms
step:722/2330 train_time:29309ms step_avg:40.59ms
step:723/2330 train_time:29345ms step_avg:40.59ms
step:724/2330 train_time:29390ms step_avg:40.59ms
step:725/2330 train_time:29426ms step_avg:40.59ms
step:726/2330 train_time:29470ms step_avg:40.59ms
step:727/2330 train_time:29507ms step_avg:40.59ms
step:728/2330 train_time:29553ms step_avg:40.59ms
step:729/2330 train_time:29589ms step_avg:40.59ms
step:730/2330 train_time:29634ms step_avg:40.59ms
step:731/2330 train_time:29669ms step_avg:40.59ms
step:732/2330 train_time:29715ms step_avg:40.59ms
step:733/2330 train_time:29751ms step_avg:40.59ms
step:734/2330 train_time:29796ms step_avg:40.59ms
step:735/2330 train_time:29833ms step_avg:40.59ms
step:736/2330 train_time:29878ms step_avg:40.60ms
step:737/2330 train_time:29914ms step_avg:40.59ms
step:738/2330 train_time:29960ms step_avg:40.60ms
step:739/2330 train_time:29996ms step_avg:40.59ms
step:740/2330 train_time:30041ms step_avg:40.60ms
step:741/2330 train_time:30077ms step_avg:40.59ms
step:742/2330 train_time:30122ms step_avg:40.60ms
step:743/2330 train_time:30158ms step_avg:40.59ms
step:744/2330 train_time:30204ms step_avg:40.60ms
step:745/2330 train_time:30240ms step_avg:40.59ms
step:746/2330 train_time:30287ms step_avg:40.60ms
step:747/2330 train_time:30324ms step_avg:40.59ms
step:748/2330 train_time:30369ms step_avg:40.60ms
step:749/2330 train_time:30404ms step_avg:40.59ms
step:750/2330 train_time:30450ms step_avg:40.60ms
step:750/2330 val_loss:5.2276 train_time:30539ms step_avg:40.72ms
step:751/2330 train_time:30553ms step_avg:40.68ms
step:752/2330 train_time:30566ms step_avg:40.65ms
step:753/2330 train_time:30577ms step_avg:40.61ms
step:754/2330 train_time:30612ms step_avg:40.60ms
step:755/2330 train_time:30647ms step_avg:40.59ms
step:756/2330 train_time:30692ms step_avg:40.60ms
step:757/2330 train_time:30727ms step_avg:40.59ms
step:758/2330 train_time:30771ms step_avg:40.60ms
step:759/2330 train_time:30806ms step_avg:40.59ms
step:760/2330 train_time:30856ms step_avg:40.60ms
step:761/2330 train_time:30895ms step_avg:40.60ms
step:762/2330 train_time:30942ms step_avg:40.61ms
step:763/2330 train_time:30979ms step_avg:40.60ms
step:764/2330 train_time:31027ms step_avg:40.61ms
step:765/2330 train_time:31064ms step_avg:40.61ms
step:766/2330 train_time:31108ms step_avg:40.61ms
step:767/2330 train_time:31142ms step_avg:40.60ms
step:768/2330 train_time:31187ms step_avg:40.61ms
step:769/2330 train_time:31222ms step_avg:40.60ms
step:770/2330 train_time:31268ms step_avg:40.61ms
step:771/2330 train_time:31304ms step_avg:40.60ms
step:772/2330 train_time:31348ms step_avg:40.61ms
step:773/2330 train_time:31383ms step_avg:40.60ms
step:774/2330 train_time:31428ms step_avg:40.60ms
step:775/2330 train_time:31464ms step_avg:40.60ms
step:776/2330 train_time:31510ms step_avg:40.61ms
step:777/2330 train_time:31545ms step_avg:40.60ms
step:778/2330 train_time:31590ms step_avg:40.60ms
step:779/2330 train_time:31625ms step_avg:40.60ms
step:780/2330 train_time:31670ms step_avg:40.60ms
step:781/2330 train_time:31705ms step_avg:40.60ms
step:782/2330 train_time:31750ms step_avg:40.60ms
step:783/2330 train_time:31787ms step_avg:40.60ms
step:784/2330 train_time:31833ms step_avg:40.60ms
step:785/2330 train_time:31869ms step_avg:40.60ms
step:786/2330 train_time:31915ms step_avg:40.60ms
step:787/2330 train_time:31951ms step_avg:40.60ms
step:788/2330 train_time:31998ms step_avg:40.61ms
step:789/2330 train_time:32035ms step_avg:40.60ms
step:790/2330 train_time:32081ms step_avg:40.61ms
step:791/2330 train_time:32117ms step_avg:40.60ms
step:792/2330 train_time:32164ms step_avg:40.61ms
step:793/2330 train_time:32199ms step_avg:40.60ms
step:794/2330 train_time:32245ms step_avg:40.61ms
step:795/2330 train_time:32281ms step_avg:40.60ms
step:796/2330 train_time:32326ms step_avg:40.61ms
step:797/2330 train_time:32362ms step_avg:40.60ms
step:798/2330 train_time:32406ms step_avg:40.61ms
step:799/2330 train_time:32442ms step_avg:40.60ms
step:800/2330 train_time:32487ms step_avg:40.61ms
step:801/2330 train_time:32522ms step_avg:40.60ms
step:802/2330 train_time:32566ms step_avg:40.61ms
step:803/2330 train_time:32602ms step_avg:40.60ms
step:804/2330 train_time:32647ms step_avg:40.61ms
step:805/2330 train_time:32683ms step_avg:40.60ms
step:806/2330 train_time:32728ms step_avg:40.61ms
step:807/2330 train_time:32764ms step_avg:40.60ms
step:808/2330 train_time:32810ms step_avg:40.61ms
step:809/2330 train_time:32846ms step_avg:40.60ms
step:810/2330 train_time:32892ms step_avg:40.61ms
step:811/2330 train_time:32928ms step_avg:40.60ms
step:812/2330 train_time:32975ms step_avg:40.61ms
step:813/2330 train_time:33010ms step_avg:40.60ms
step:814/2330 train_time:33055ms step_avg:40.61ms
step:815/2330 train_time:33091ms step_avg:40.60ms
step:816/2330 train_time:33136ms step_avg:40.61ms
step:817/2330 train_time:33173ms step_avg:40.60ms
step:818/2330 train_time:33219ms step_avg:40.61ms
step:819/2330 train_time:33255ms step_avg:40.60ms
step:820/2330 train_time:33299ms step_avg:40.61ms
step:821/2330 train_time:33335ms step_avg:40.60ms
step:822/2330 train_time:33380ms step_avg:40.61ms
step:823/2330 train_time:33416ms step_avg:40.60ms
step:824/2330 train_time:33461ms step_avg:40.61ms
step:825/2330 train_time:33496ms step_avg:40.60ms
step:826/2330 train_time:33542ms step_avg:40.61ms
step:827/2330 train_time:33578ms step_avg:40.60ms
step:828/2330 train_time:33624ms step_avg:40.61ms
step:829/2330 train_time:33660ms step_avg:40.60ms
step:830/2330 train_time:33707ms step_avg:40.61ms
step:831/2330 train_time:33742ms step_avg:40.60ms
step:832/2330 train_time:33788ms step_avg:40.61ms
step:833/2330 train_time:33825ms step_avg:40.61ms
step:834/2330 train_time:33870ms step_avg:40.61ms
step:835/2330 train_time:33907ms step_avg:40.61ms
step:836/2330 train_time:33952ms step_avg:40.61ms
step:837/2330 train_time:33988ms step_avg:40.61ms
step:838/2330 train_time:34033ms step_avg:40.61ms
step:839/2330 train_time:34069ms step_avg:40.61ms
step:840/2330 train_time:34114ms step_avg:40.61ms
step:841/2330 train_time:34150ms step_avg:40.61ms
step:842/2330 train_time:34196ms step_avg:40.61ms
step:843/2330 train_time:34231ms step_avg:40.61ms
step:844/2330 train_time:34277ms step_avg:40.61ms
step:845/2330 train_time:34313ms step_avg:40.61ms
step:846/2330 train_time:34358ms step_avg:40.61ms
step:847/2330 train_time:34394ms step_avg:40.61ms
step:848/2330 train_time:34439ms step_avg:40.61ms
step:849/2330 train_time:34475ms step_avg:40.61ms
step:850/2330 train_time:34521ms step_avg:40.61ms
step:851/2330 train_time:34557ms step_avg:40.61ms
step:852/2330 train_time:34602ms step_avg:40.61ms
step:853/2330 train_time:34638ms step_avg:40.61ms
step:854/2330 train_time:34684ms step_avg:40.61ms
step:855/2330 train_time:34720ms step_avg:40.61ms
step:856/2330 train_time:34765ms step_avg:40.61ms
step:857/2330 train_time:34802ms step_avg:40.61ms
step:858/2330 train_time:34848ms step_avg:40.61ms
step:859/2330 train_time:34884ms step_avg:40.61ms
step:860/2330 train_time:34930ms step_avg:40.62ms
step:861/2330 train_time:34965ms step_avg:40.61ms
step:862/2330 train_time:35011ms step_avg:40.62ms
step:863/2330 train_time:35046ms step_avg:40.61ms
step:864/2330 train_time:35091ms step_avg:40.62ms
step:865/2330 train_time:35128ms step_avg:40.61ms
step:866/2330 train_time:35173ms step_avg:40.62ms
step:867/2330 train_time:35209ms step_avg:40.61ms
step:868/2330 train_time:35255ms step_avg:40.62ms
step:869/2330 train_time:35290ms step_avg:40.61ms
step:870/2330 train_time:35335ms step_avg:40.62ms
step:871/2330 train_time:35371ms step_avg:40.61ms
step:872/2330 train_time:35417ms step_avg:40.62ms
step:873/2330 train_time:35453ms step_avg:40.61ms
step:874/2330 train_time:35498ms step_avg:40.62ms
step:875/2330 train_time:35534ms step_avg:40.61ms
step:876/2330 train_time:35578ms step_avg:40.61ms
step:877/2330 train_time:35614ms step_avg:40.61ms
step:878/2330 train_time:35660ms step_avg:40.62ms
step:879/2330 train_time:35696ms step_avg:40.61ms
step:880/2330 train_time:35741ms step_avg:40.62ms
step:881/2330 train_time:35777ms step_avg:40.61ms
step:882/2330 train_time:35823ms step_avg:40.62ms
step:883/2330 train_time:35860ms step_avg:40.61ms
step:884/2330 train_time:35906ms step_avg:40.62ms
step:885/2330 train_time:35942ms step_avg:40.61ms
step:886/2330 train_time:35988ms step_avg:40.62ms
step:887/2330 train_time:36025ms step_avg:40.61ms
step:888/2330 train_time:36070ms step_avg:40.62ms
step:889/2330 train_time:36106ms step_avg:40.61ms
step:890/2330 train_time:36151ms step_avg:40.62ms
step:891/2330 train_time:36187ms step_avg:40.61ms
step:892/2330 train_time:36232ms step_avg:40.62ms
step:893/2330 train_time:36268ms step_avg:40.61ms
step:894/2330 train_time:36314ms step_avg:40.62ms
step:895/2330 train_time:36350ms step_avg:40.61ms
step:896/2330 train_time:36396ms step_avg:40.62ms
step:897/2330 train_time:36432ms step_avg:40.62ms
step:898/2330 train_time:36476ms step_avg:40.62ms
step:899/2330 train_time:36512ms step_avg:40.61ms
step:900/2330 train_time:36557ms step_avg:40.62ms
step:901/2330 train_time:36594ms step_avg:40.61ms
step:902/2330 train_time:36639ms step_avg:40.62ms
step:903/2330 train_time:36674ms step_avg:40.61ms
step:904/2330 train_time:36719ms step_avg:40.62ms
step:905/2330 train_time:36756ms step_avg:40.61ms
step:906/2330 train_time:36801ms step_avg:40.62ms
step:907/2330 train_time:36837ms step_avg:40.61ms
step:908/2330 train_time:36882ms step_avg:40.62ms
step:909/2330 train_time:36918ms step_avg:40.61ms
step:910/2330 train_time:36965ms step_avg:40.62ms
step:911/2330 train_time:37000ms step_avg:40.61ms
step:912/2330 train_time:37046ms step_avg:40.62ms
step:913/2330 train_time:37082ms step_avg:40.62ms
step:914/2330 train_time:37129ms step_avg:40.62ms
step:915/2330 train_time:37164ms step_avg:40.62ms
step:916/2330 train_time:37211ms step_avg:40.62ms
step:917/2330 train_time:37247ms step_avg:40.62ms
step:918/2330 train_time:37292ms step_avg:40.62ms
step:919/2330 train_time:37328ms step_avg:40.62ms
step:920/2330 train_time:37373ms step_avg:40.62ms
step:921/2330 train_time:37409ms step_avg:40.62ms
step:922/2330 train_time:37454ms step_avg:40.62ms
step:923/2330 train_time:37489ms step_avg:40.62ms
step:924/2330 train_time:37536ms step_avg:40.62ms
step:925/2330 train_time:37571ms step_avg:40.62ms
step:926/2330 train_time:37617ms step_avg:40.62ms
step:927/2330 train_time:37653ms step_avg:40.62ms
step:928/2330 train_time:37698ms step_avg:40.62ms
step:929/2330 train_time:37734ms step_avg:40.62ms
step:930/2330 train_time:37779ms step_avg:40.62ms
step:931/2330 train_time:37816ms step_avg:40.62ms
step:932/2330 train_time:37861ms step_avg:40.62ms
step:933/2330 train_time:37897ms step_avg:40.62ms
step:934/2330 train_time:37943ms step_avg:40.62ms
step:935/2330 train_time:37979ms step_avg:40.62ms
step:936/2330 train_time:38025ms step_avg:40.62ms
step:937/2330 train_time:38060ms step_avg:40.62ms
step:938/2330 train_time:38106ms step_avg:40.62ms
step:939/2330 train_time:38142ms step_avg:40.62ms
step:940/2330 train_time:38188ms step_avg:40.63ms
step:941/2330 train_time:38225ms step_avg:40.62ms
step:942/2330 train_time:38270ms step_avg:40.63ms
step:943/2330 train_time:38307ms step_avg:40.62ms
step:944/2330 train_time:38352ms step_avg:40.63ms
step:945/2330 train_time:38388ms step_avg:40.62ms
step:946/2330 train_time:38434ms step_avg:40.63ms
step:947/2330 train_time:38470ms step_avg:40.62ms
step:948/2330 train_time:38515ms step_avg:40.63ms
step:949/2330 train_time:38550ms step_avg:40.62ms
step:950/2330 train_time:38595ms step_avg:40.63ms
step:951/2330 train_time:38631ms step_avg:40.62ms
step:952/2330 train_time:38677ms step_avg:40.63ms
step:953/2330 train_time:38713ms step_avg:40.62ms
step:954/2330 train_time:38758ms step_avg:40.63ms
step:955/2330 train_time:38793ms step_avg:40.62ms
step:956/2330 train_time:38839ms step_avg:40.63ms
step:957/2330 train_time:38875ms step_avg:40.62ms
step:958/2330 train_time:38920ms step_avg:40.63ms
step:959/2330 train_time:38956ms step_avg:40.62ms
step:960/2330 train_time:39002ms step_avg:40.63ms
step:961/2330 train_time:39038ms step_avg:40.62ms
step:962/2330 train_time:39084ms step_avg:40.63ms
step:963/2330 train_time:39121ms step_avg:40.62ms
step:964/2330 train_time:39166ms step_avg:40.63ms
step:965/2330 train_time:39202ms step_avg:40.62ms
step:966/2330 train_time:39247ms step_avg:40.63ms
step:967/2330 train_time:39282ms step_avg:40.62ms
step:968/2330 train_time:39329ms step_avg:40.63ms
step:969/2330 train_time:39365ms step_avg:40.62ms
step:970/2330 train_time:39410ms step_avg:40.63ms
step:971/2330 train_time:39446ms step_avg:40.62ms
step:972/2330 train_time:39493ms step_avg:40.63ms
step:973/2330 train_time:39528ms step_avg:40.63ms
step:974/2330 train_time:39574ms step_avg:40.63ms
step:975/2330 train_time:39609ms step_avg:40.62ms
step:976/2330 train_time:39654ms step_avg:40.63ms
step:977/2330 train_time:39691ms step_avg:40.62ms
step:978/2330 train_time:39736ms step_avg:40.63ms
step:979/2330 train_time:39771ms step_avg:40.62ms
step:980/2330 train_time:39817ms step_avg:40.63ms
step:981/2330 train_time:39854ms step_avg:40.63ms
step:982/2330 train_time:39899ms step_avg:40.63ms
step:983/2330 train_time:39935ms step_avg:40.63ms
step:984/2330 train_time:39980ms step_avg:40.63ms
step:985/2330 train_time:40016ms step_avg:40.63ms
step:986/2330 train_time:40062ms step_avg:40.63ms
step:987/2330 train_time:40098ms step_avg:40.63ms
step:988/2330 train_time:40144ms step_avg:40.63ms
step:989/2330 train_time:40180ms step_avg:40.63ms
step:990/2330 train_time:40226ms step_avg:40.63ms
step:991/2330 train_time:40262ms step_avg:40.63ms
step:992/2330 train_time:40307ms step_avg:40.63ms
step:993/2330 train_time:40343ms step_avg:40.63ms
step:994/2330 train_time:40389ms step_avg:40.63ms
step:995/2330 train_time:40425ms step_avg:40.63ms
step:996/2330 train_time:40471ms step_avg:40.63ms
step:997/2330 train_time:40507ms step_avg:40.63ms
step:998/2330 train_time:40553ms step_avg:40.63ms
step:999/2330 train_time:40590ms step_avg:40.63ms
step:1000/2330 train_time:40636ms step_avg:40.64ms
step:1000/2330 val_loss:5.1924 train_time:40723ms step_avg:40.72ms
step:1001/2330 train_time:40736ms step_avg:40.70ms
step:1002/2330 train_time:40749ms step_avg:40.67ms
step:1003/2330 train_time:40760ms step_avg:40.64ms
step:1004/2330 train_time:40796ms step_avg:40.63ms
step:1005/2330 train_time:40830ms step_avg:40.63ms
step:1006/2330 train_time:40874ms step_avg:40.63ms
step:1007/2330 train_time:40909ms step_avg:40.62ms
step:1008/2330 train_time:40953ms step_avg:40.63ms
step:1009/2330 train_time:40989ms step_avg:40.62ms
step:1010/2330 train_time:41034ms step_avg:40.63ms
step:1011/2330 train_time:41075ms step_avg:40.63ms
step:1012/2330 train_time:41124ms step_avg:40.64ms
step:1013/2330 train_time:41163ms step_avg:40.63ms
step:1014/2330 train_time:41208ms step_avg:40.64ms
step:1015/2330 train_time:41243ms step_avg:40.63ms
step:1016/2330 train_time:41288ms step_avg:40.64ms
step:1017/2330 train_time:41323ms step_avg:40.63ms
step:1018/2330 train_time:41367ms step_avg:40.64ms
step:1019/2330 train_time:41402ms step_avg:40.63ms
step:1020/2330 train_time:41448ms step_avg:40.63ms
step:1021/2330 train_time:41482ms step_avg:40.63ms
step:1022/2330 train_time:41527ms step_avg:40.63ms
step:1023/2330 train_time:41562ms step_avg:40.63ms
step:1024/2330 train_time:41607ms step_avg:40.63ms
step:1025/2330 train_time:41644ms step_avg:40.63ms
step:1026/2330 train_time:41691ms step_avg:40.63ms
step:1027/2330 train_time:41727ms step_avg:40.63ms
step:1028/2330 train_time:41772ms step_avg:40.63ms
step:1029/2330 train_time:41807ms step_avg:40.63ms
step:1030/2330 train_time:41852ms step_avg:40.63ms
step:1031/2330 train_time:41887ms step_avg:40.63ms
step:1032/2330 train_time:41931ms step_avg:40.63ms
step:1033/2330 train_time:41967ms step_avg:40.63ms
step:1034/2330 train_time:42014ms step_avg:40.63ms
step:1035/2330 train_time:42052ms step_avg:40.63ms
step:1036/2330 train_time:42099ms step_avg:40.64ms
step:1037/2330 train_time:42137ms step_avg:40.63ms
step:1038/2330 train_time:42184ms step_avg:40.64ms
step:1039/2330 train_time:42220ms step_avg:40.64ms
step:1040/2330 train_time:42265ms step_avg:40.64ms
step:1041/2330 train_time:42300ms step_avg:40.63ms
step:1042/2330 train_time:42346ms step_avg:40.64ms
step:1043/2330 train_time:42380ms step_avg:40.63ms
step:1044/2330 train_time:42425ms step_avg:40.64ms
step:1045/2330 train_time:42461ms step_avg:40.63ms
step:1046/2330 train_time:42505ms step_avg:40.64ms
step:1047/2330 train_time:42540ms step_avg:40.63ms
step:1048/2330 train_time:42586ms step_avg:40.64ms
step:1049/2330 train_time:42622ms step_avg:40.63ms
step:1050/2330 train_time:42667ms step_avg:40.63ms
step:1051/2330 train_time:42702ms step_avg:40.63ms
step:1052/2330 train_time:42748ms step_avg:40.63ms
step:1053/2330 train_time:42783ms step_avg:40.63ms
step:1054/2330 train_time:42828ms step_avg:40.63ms
step:1055/2330 train_time:42863ms step_avg:40.63ms
step:1056/2330 train_time:42908ms step_avg:40.63ms
step:1057/2330 train_time:42945ms step_avg:40.63ms
step:1058/2330 train_time:42991ms step_avg:40.63ms
step:1059/2330 train_time:43027ms step_avg:40.63ms
step:1060/2330 train_time:43074ms step_avg:40.64ms
step:1061/2330 train_time:43109ms step_avg:40.63ms
step:1062/2330 train_time:43156ms step_avg:40.64ms
step:1063/2330 train_time:43193ms step_avg:40.63ms
step:1064/2330 train_time:43238ms step_avg:40.64ms
step:1065/2330 train_time:43275ms step_avg:40.63ms
step:1066/2330 train_time:43320ms step_avg:40.64ms
step:1067/2330 train_time:43357ms step_avg:40.63ms
step:1068/2330 train_time:43402ms step_avg:40.64ms
step:1069/2330 train_time:43437ms step_avg:40.63ms
step:1070/2330 train_time:43482ms step_avg:40.64ms
step:1071/2330 train_time:43518ms step_avg:40.63ms
step:1072/2330 train_time:43564ms step_avg:40.64ms
step:1073/2330 train_time:43600ms step_avg:40.63ms
step:1074/2330 train_time:43646ms step_avg:40.64ms
step:1075/2330 train_time:43681ms step_avg:40.63ms
step:1076/2330 train_time:43726ms step_avg:40.64ms
step:1077/2330 train_time:43762ms step_avg:40.63ms
step:1078/2330 train_time:43807ms step_avg:40.64ms
step:1079/2330 train_time:43843ms step_avg:40.63ms
step:1080/2330 train_time:43888ms step_avg:40.64ms
step:1081/2330 train_time:43923ms step_avg:40.63ms
step:1082/2330 train_time:43970ms step_avg:40.64ms
step:1083/2330 train_time:44005ms step_avg:40.63ms
step:1084/2330 train_time:44051ms step_avg:40.64ms
step:1085/2330 train_time:44087ms step_avg:40.63ms
step:1086/2330 train_time:44133ms step_avg:40.64ms
step:1087/2330 train_time:44168ms step_avg:40.63ms
step:1088/2330 train_time:44213ms step_avg:40.64ms
step:1089/2330 train_time:44249ms step_avg:40.63ms
step:1090/2330 train_time:44294ms step_avg:40.64ms
step:1091/2330 train_time:44329ms step_avg:40.63ms
step:1092/2330 train_time:44376ms step_avg:40.64ms
step:1093/2330 train_time:44412ms step_avg:40.63ms
step:1094/2330 train_time:44458ms step_avg:40.64ms
step:1095/2330 train_time:44494ms step_avg:40.63ms
step:1096/2330 train_time:44539ms step_avg:40.64ms
step:1097/2330 train_time:44575ms step_avg:40.63ms
step:1098/2330 train_time:44621ms step_avg:40.64ms
step:1099/2330 train_time:44657ms step_avg:40.63ms
step:1100/2330 train_time:44703ms step_avg:40.64ms
step:1101/2330 train_time:44739ms step_avg:40.63ms
step:1102/2330 train_time:44784ms step_avg:40.64ms
step:1103/2330 train_time:44820ms step_avg:40.63ms
step:1104/2330 train_time:44865ms step_avg:40.64ms
step:1105/2330 train_time:44901ms step_avg:40.63ms
step:1106/2330 train_time:44947ms step_avg:40.64ms
step:1107/2330 train_time:44983ms step_avg:40.64ms
step:1108/2330 train_time:45029ms step_avg:40.64ms
step:1109/2330 train_time:45065ms step_avg:40.64ms
step:1110/2330 train_time:45110ms step_avg:40.64ms
step:1111/2330 train_time:45146ms step_avg:40.64ms
step:1112/2330 train_time:45191ms step_avg:40.64ms
step:1113/2330 train_time:45227ms step_avg:40.64ms
step:1114/2330 train_time:45273ms step_avg:40.64ms
step:1115/2330 train_time:45309ms step_avg:40.64ms
step:1116/2330 train_time:45353ms step_avg:40.64ms
step:1117/2330 train_time:45390ms step_avg:40.64ms
step:1118/2330 train_time:45436ms step_avg:40.64ms
step:1119/2330 train_time:45472ms step_avg:40.64ms
step:1120/2330 train_time:45517ms step_avg:40.64ms
step:1121/2330 train_time:45553ms step_avg:40.64ms
step:1122/2330 train_time:45598ms step_avg:40.64ms
step:1123/2330 train_time:45634ms step_avg:40.64ms
step:1124/2330 train_time:45679ms step_avg:40.64ms
step:1125/2330 train_time:45716ms step_avg:40.64ms
step:1126/2330 train_time:45762ms step_avg:40.64ms
step:1127/2330 train_time:45798ms step_avg:40.64ms
step:1128/2330 train_time:45843ms step_avg:40.64ms
step:1129/2330 train_time:45878ms step_avg:40.64ms
step:1130/2330 train_time:45924ms step_avg:40.64ms
step:1131/2330 train_time:45959ms step_avg:40.64ms
step:1132/2330 train_time:46006ms step_avg:40.64ms
step:1133/2330 train_time:46041ms step_avg:40.64ms
step:1134/2330 train_time:46087ms step_avg:40.64ms
step:1135/2330 train_time:46123ms step_avg:40.64ms
step:1136/2330 train_time:46168ms step_avg:40.64ms
step:1137/2330 train_time:46206ms step_avg:40.64ms
step:1138/2330 train_time:46251ms step_avg:40.64ms
step:1139/2330 train_time:46287ms step_avg:40.64ms
step:1140/2330 train_time:46333ms step_avg:40.64ms
step:1141/2330 train_time:46368ms step_avg:40.64ms
step:1142/2330 train_time:46414ms step_avg:40.64ms
step:1143/2330 train_time:46449ms step_avg:40.64ms
step:1144/2330 train_time:46495ms step_avg:40.64ms
step:1145/2330 train_time:46531ms step_avg:40.64ms
step:1146/2330 train_time:46576ms step_avg:40.64ms
step:1147/2330 train_time:46612ms step_avg:40.64ms
step:1148/2330 train_time:46657ms step_avg:40.64ms
step:1149/2330 train_time:46693ms step_avg:40.64ms
step:1150/2330 train_time:46739ms step_avg:40.64ms
step:1151/2330 train_time:46775ms step_avg:40.64ms
step:1152/2330 train_time:46820ms step_avg:40.64ms
step:1153/2330 train_time:46856ms step_avg:40.64ms
step:1154/2330 train_time:46901ms step_avg:40.64ms
step:1155/2330 train_time:46937ms step_avg:40.64ms
step:1156/2330 train_time:46982ms step_avg:40.64ms
step:1157/2330 train_time:47019ms step_avg:40.64ms
step:1158/2330 train_time:47064ms step_avg:40.64ms
step:1159/2330 train_time:47100ms step_avg:40.64ms
step:1160/2330 train_time:47146ms step_avg:40.64ms
step:1161/2330 train_time:47182ms step_avg:40.64ms
step:1162/2330 train_time:47229ms step_avg:40.64ms
step:1163/2330 train_time:47265ms step_avg:40.64ms
step:1164/2330 train_time:47310ms step_avg:40.64ms
step:1165/2330 train_time:47346ms step_avg:40.64ms
step:1166/2330 train_time:47392ms step_avg:40.64ms
step:1167/2330 train_time:47427ms step_avg:40.64ms
step:1168/2330 train_time:47473ms step_avg:40.64ms
step:1169/2330 train_time:47508ms step_avg:40.64ms
step:1170/2330 train_time:47553ms step_avg:40.64ms
step:1171/2330 train_time:47588ms step_avg:40.64ms
step:1172/2330 train_time:47633ms step_avg:40.64ms
step:1173/2330 train_time:47669ms step_avg:40.64ms
step:1174/2330 train_time:47715ms step_avg:40.64ms
step:1175/2330 train_time:47752ms step_avg:40.64ms
step:1176/2330 train_time:47797ms step_avg:40.64ms
step:1177/2330 train_time:47832ms step_avg:40.64ms
step:1178/2330 train_time:47878ms step_avg:40.64ms
step:1179/2330 train_time:47914ms step_avg:40.64ms
step:1180/2330 train_time:47959ms step_avg:40.64ms
step:1181/2330 train_time:47995ms step_avg:40.64ms
step:1182/2330 train_time:48040ms step_avg:40.64ms
step:1183/2330 train_time:48076ms step_avg:40.64ms
step:1184/2330 train_time:48121ms step_avg:40.64ms
step:1185/2330 train_time:48158ms step_avg:40.64ms
step:1186/2330 train_time:48204ms step_avg:40.64ms
step:1187/2330 train_time:48239ms step_avg:40.64ms
step:1188/2330 train_time:48285ms step_avg:40.64ms
step:1189/2330 train_time:48322ms step_avg:40.64ms
step:1190/2330 train_time:48367ms step_avg:40.64ms
step:1191/2330 train_time:48404ms step_avg:40.64ms
step:1192/2330 train_time:48449ms step_avg:40.65ms
step:1193/2330 train_time:48484ms step_avg:40.64ms
step:1194/2330 train_time:48530ms step_avg:40.64ms
step:1195/2330 train_time:48565ms step_avg:40.64ms
step:1196/2330 train_time:48610ms step_avg:40.64ms
step:1197/2330 train_time:48646ms step_avg:40.64ms
step:1198/2330 train_time:48692ms step_avg:40.64ms
step:1199/2330 train_time:48728ms step_avg:40.64ms
step:1200/2330 train_time:48774ms step_avg:40.64ms
step:1201/2330 train_time:48809ms step_avg:40.64ms
step:1202/2330 train_time:48855ms step_avg:40.64ms
step:1203/2330 train_time:48891ms step_avg:40.64ms
step:1204/2330 train_time:48936ms step_avg:40.64ms
step:1205/2330 train_time:48972ms step_avg:40.64ms
step:1206/2330 train_time:49016ms step_avg:40.64ms
step:1207/2330 train_time:49052ms step_avg:40.64ms
step:1208/2330 train_time:49098ms step_avg:40.64ms
step:1209/2330 train_time:49135ms step_avg:40.64ms
step:1210/2330 train_time:49180ms step_avg:40.64ms
step:1211/2330 train_time:49216ms step_avg:40.64ms
step:1212/2330 train_time:49262ms step_avg:40.65ms
step:1213/2330 train_time:49297ms step_avg:40.64ms
step:1214/2330 train_time:49343ms step_avg:40.65ms
step:1215/2330 train_time:49379ms step_avg:40.64ms
step:1216/2330 train_time:49425ms step_avg:40.65ms
step:1217/2330 train_time:49461ms step_avg:40.64ms
step:1218/2330 train_time:49506ms step_avg:40.65ms
step:1219/2330 train_time:49541ms step_avg:40.64ms
step:1220/2330 train_time:49587ms step_avg:40.64ms
step:1221/2330 train_time:49622ms step_avg:40.64ms
step:1222/2330 train_time:49668ms step_avg:40.65ms
step:1223/2330 train_time:49703ms step_avg:40.64ms
step:1224/2330 train_time:49749ms step_avg:40.64ms
step:1225/2330 train_time:49784ms step_avg:40.64ms
step:1226/2330 train_time:49830ms step_avg:40.64ms
step:1227/2330 train_time:49866ms step_avg:40.64ms
step:1228/2330 train_time:49911ms step_avg:40.64ms
step:1229/2330 train_time:49946ms step_avg:40.64ms
step:1230/2330 train_time:49991ms step_avg:40.64ms
step:1231/2330 train_time:50027ms step_avg:40.64ms
step:1232/2330 train_time:50073ms step_avg:40.64ms
step:1233/2330 train_time:50109ms step_avg:40.64ms
step:1234/2330 train_time:50154ms step_avg:40.64ms
step:1235/2330 train_time:50190ms step_avg:40.64ms
step:1236/2330 train_time:50235ms step_avg:40.64ms
step:1237/2330 train_time:50271ms step_avg:40.64ms
step:1238/2330 train_time:50317ms step_avg:40.64ms
step:1239/2330 train_time:50354ms step_avg:40.64ms
step:1240/2330 train_time:50399ms step_avg:40.64ms
step:1241/2330 train_time:50436ms step_avg:40.64ms
step:1242/2330 train_time:50481ms step_avg:40.64ms
step:1243/2330 train_time:50517ms step_avg:40.64ms
step:1244/2330 train_time:50564ms step_avg:40.65ms
step:1245/2330 train_time:50600ms step_avg:40.64ms
step:1246/2330 train_time:50645ms step_avg:40.65ms
step:1247/2330 train_time:50681ms step_avg:40.64ms
step:1248/2330 train_time:50726ms step_avg:40.65ms
step:1249/2330 train_time:50763ms step_avg:40.64ms
step:1250/2330 train_time:50808ms step_avg:40.65ms
step:1250/2330 val_loss:5.1656 train_time:50895ms step_avg:40.72ms
step:1251/2330 train_time:50908ms step_avg:40.69ms
step:1252/2330 train_time:50921ms step_avg:40.67ms
step:1253/2330 train_time:50934ms step_avg:40.65ms
step:1254/2330 train_time:50969ms step_avg:40.65ms
step:1255/2330 train_time:51004ms step_avg:40.64ms
step:1256/2330 train_time:51048ms step_avg:40.64ms
step:1257/2330 train_time:51083ms step_avg:40.64ms
step:1258/2330 train_time:51130ms step_avg:40.64ms
step:1259/2330 train_time:51166ms step_avg:40.64ms
step:1260/2330 train_time:51214ms step_avg:40.65ms
step:1261/2330 train_time:51253ms step_avg:40.64ms
step:1262/2330 train_time:51301ms step_avg:40.65ms
step:1263/2330 train_time:51337ms step_avg:40.65ms
step:1264/2330 train_time:51383ms step_avg:40.65ms
step:1265/2330 train_time:51419ms step_avg:40.65ms
step:1266/2330 train_time:51463ms step_avg:40.65ms
step:1267/2330 train_time:51499ms step_avg:40.65ms
step:1268/2330 train_time:51544ms step_avg:40.65ms
step:1269/2330 train_time:51579ms step_avg:40.65ms
step:1270/2330 train_time:51623ms step_avg:40.65ms
step:1271/2330 train_time:51658ms step_avg:40.64ms
step:1272/2330 train_time:51703ms step_avg:40.65ms
step:1273/2330 train_time:51739ms step_avg:40.64ms
step:1274/2330 train_time:51783ms step_avg:40.65ms
step:1275/2330 train_time:51818ms step_avg:40.64ms
step:1276/2330 train_time:51864ms step_avg:40.65ms
step:1277/2330 train_time:51900ms step_avg:40.64ms
step:1278/2330 train_time:51945ms step_avg:40.65ms
step:1279/2330 train_time:51980ms step_avg:40.64ms
step:1280/2330 train_time:52025ms step_avg:40.64ms
step:1281/2330 train_time:52061ms step_avg:40.64ms
step:1282/2330 train_time:52106ms step_avg:40.64ms
step:1283/2330 train_time:52142ms step_avg:40.64ms
step:1284/2330 train_time:52189ms step_avg:40.65ms
step:1285/2330 train_time:52228ms step_avg:40.64ms
step:1286/2330 train_time:52275ms step_avg:40.65ms
step:1287/2330 train_time:52312ms step_avg:40.65ms
step:1288/2330 train_time:52357ms step_avg:40.65ms
step:1289/2330 train_time:52393ms step_avg:40.65ms
step:1290/2330 train_time:52438ms step_avg:40.65ms
step:1291/2330 train_time:52473ms step_avg:40.65ms
step:1292/2330 train_time:52518ms step_avg:40.65ms
step:1293/2330 train_time:52554ms step_avg:40.65ms
step:1294/2330 train_time:52599ms step_avg:40.65ms
step:1295/2330 train_time:52634ms step_avg:40.64ms
step:1296/2330 train_time:52679ms step_avg:40.65ms
step:1297/2330 train_time:52713ms step_avg:40.64ms
step:1298/2330 train_time:52758ms step_avg:40.65ms
step:1299/2330 train_time:52795ms step_avg:40.64ms
step:1300/2330 train_time:52840ms step_avg:40.65ms
step:1301/2330 train_time:52876ms step_avg:40.64ms
step:1302/2330 train_time:52921ms step_avg:40.65ms
step:1303/2330 train_time:52957ms step_avg:40.64ms
step:1304/2330 train_time:53001ms step_avg:40.65ms
step:1305/2330 train_time:53037ms step_avg:40.64ms
step:1306/2330 train_time:53083ms step_avg:40.65ms
step:1307/2330 train_time:53120ms step_avg:40.64ms
step:1308/2330 train_time:53166ms step_avg:40.65ms
step:1309/2330 train_time:53202ms step_avg:40.64ms
step:1310/2330 train_time:53248ms step_avg:40.65ms
step:1311/2330 train_time:53284ms step_avg:40.64ms
step:1312/2330 train_time:53330ms step_avg:40.65ms
step:1313/2330 train_time:53366ms step_avg:40.64ms
step:1314/2330 train_time:53412ms step_avg:40.65ms
step:1315/2330 train_time:53448ms step_avg:40.64ms
step:1316/2330 train_time:53494ms step_avg:40.65ms
step:1317/2330 train_time:53530ms step_avg:40.65ms
step:1318/2330 train_time:53575ms step_avg:40.65ms
step:1319/2330 train_time:53610ms step_avg:40.64ms
step:1320/2330 train_time:53655ms step_avg:40.65ms
step:1321/2330 train_time:53690ms step_avg:40.64ms
step:1322/2330 train_time:53736ms step_avg:40.65ms
step:1323/2330 train_time:53771ms step_avg:40.64ms
step:1324/2330 train_time:53816ms step_avg:40.65ms
step:1325/2330 train_time:53851ms step_avg:40.64ms
step:1326/2330 train_time:53897ms step_avg:40.65ms
step:1327/2330 train_time:53932ms step_avg:40.64ms
step:1328/2330 train_time:53977ms step_avg:40.65ms
step:1329/2330 train_time:54013ms step_avg:40.64ms
step:1330/2330 train_time:54059ms step_avg:40.65ms
step:1331/2330 train_time:54095ms step_avg:40.64ms
step:1332/2330 train_time:54140ms step_avg:40.65ms
step:1333/2330 train_time:54176ms step_avg:40.64ms
step:1334/2330 train_time:54223ms step_avg:40.65ms
step:1335/2330 train_time:54259ms step_avg:40.64ms
step:1336/2330 train_time:54304ms step_avg:40.65ms
step:1337/2330 train_time:54340ms step_avg:40.64ms
step:1338/2330 train_time:54385ms step_avg:40.65ms
step:1339/2330 train_time:54422ms step_avg:40.64ms
step:1340/2330 train_time:54467ms step_avg:40.65ms
step:1341/2330 train_time:54504ms step_avg:40.64ms
step:1342/2330 train_time:54549ms step_avg:40.65ms
step:1343/2330 train_time:54585ms step_avg:40.64ms
step:1344/2330 train_time:54631ms step_avg:40.65ms
step:1345/2330 train_time:54667ms step_avg:40.64ms
step:1346/2330 train_time:54712ms step_avg:40.65ms
step:1347/2330 train_time:54747ms step_avg:40.64ms
step:1348/2330 train_time:54793ms step_avg:40.65ms
step:1349/2330 train_time:54829ms step_avg:40.64ms
step:1350/2330 train_time:54874ms step_avg:40.65ms
step:1351/2330 train_time:54910ms step_avg:40.64ms
step:1352/2330 train_time:54956ms step_avg:40.65ms
step:1353/2330 train_time:54991ms step_avg:40.64ms
step:1354/2330 train_time:55037ms step_avg:40.65ms
step:1355/2330 train_time:55072ms step_avg:40.64ms
step:1356/2330 train_time:55119ms step_avg:40.65ms
step:1357/2330 train_time:55155ms step_avg:40.64ms
step:1358/2330 train_time:55200ms step_avg:40.65ms
step:1359/2330 train_time:55236ms step_avg:40.64ms
step:1360/2330 train_time:55281ms step_avg:40.65ms
step:1361/2330 train_time:55317ms step_avg:40.64ms
step:1362/2330 train_time:55362ms step_avg:40.65ms
step:1363/2330 train_time:55397ms step_avg:40.64ms
step:1364/2330 train_time:55443ms step_avg:40.65ms
step:1365/2330 train_time:55479ms step_avg:40.64ms
step:1366/2330 train_time:55524ms step_avg:40.65ms
step:1367/2330 train_time:55560ms step_avg:40.64ms
step:1368/2330 train_time:55605ms step_avg:40.65ms
step:1369/2330 train_time:55641ms step_avg:40.64ms
step:1370/2330 train_time:55687ms step_avg:40.65ms
step:1371/2330 train_time:55723ms step_avg:40.64ms
step:1372/2330 train_time:55769ms step_avg:40.65ms
step:1373/2330 train_time:55805ms step_avg:40.64ms
step:1374/2330 train_time:55851ms step_avg:40.65ms
step:1375/2330 train_time:55887ms step_avg:40.65ms
step:1376/2330 train_time:55932ms step_avg:40.65ms
step:1377/2330 train_time:55967ms step_avg:40.64ms
step:1378/2330 train_time:56012ms step_avg:40.65ms
step:1379/2330 train_time:56048ms step_avg:40.64ms
step:1380/2330 train_time:56094ms step_avg:40.65ms
step:1381/2330 train_time:56131ms step_avg:40.64ms
step:1382/2330 train_time:56176ms step_avg:40.65ms
step:1383/2330 train_time:56211ms step_avg:40.64ms
step:1384/2330 train_time:56257ms step_avg:40.65ms
step:1385/2330 train_time:56292ms step_avg:40.64ms
step:1386/2330 train_time:56338ms step_avg:40.65ms
step:1387/2330 train_time:56373ms step_avg:40.64ms
step:1388/2330 train_time:56418ms step_avg:40.65ms
step:1389/2330 train_time:56454ms step_avg:40.64ms
step:1390/2330 train_time:56500ms step_avg:40.65ms
step:1391/2330 train_time:56536ms step_avg:40.64ms
step:1392/2330 train_time:56581ms step_avg:40.65ms
step:1393/2330 train_time:56616ms step_avg:40.64ms
step:1394/2330 train_time:56662ms step_avg:40.65ms
step:1395/2330 train_time:56698ms step_avg:40.64ms
step:1396/2330 train_time:56743ms step_avg:40.65ms
step:1397/2330 train_time:56779ms step_avg:40.64ms
step:1398/2330 train_time:56824ms step_avg:40.65ms
step:1399/2330 train_time:56860ms step_avg:40.64ms
step:1400/2330 train_time:56905ms step_avg:40.65ms
step:1401/2330 train_time:56940ms step_avg:40.64ms
step:1402/2330 train_time:56985ms step_avg:40.65ms
step:1403/2330 train_time:57022ms step_avg:40.64ms
step:1404/2330 train_time:57068ms step_avg:40.65ms
step:1405/2330 train_time:57105ms step_avg:40.64ms
step:1406/2330 train_time:57150ms step_avg:40.65ms
step:1407/2330 train_time:57186ms step_avg:40.64ms
step:1408/2330 train_time:57232ms step_avg:40.65ms
step:1409/2330 train_time:57268ms step_avg:40.64ms
step:1410/2330 train_time:57313ms step_avg:40.65ms
step:1411/2330 train_time:57349ms step_avg:40.64ms
step:1412/2330 train_time:57394ms step_avg:40.65ms
step:1413/2330 train_time:57430ms step_avg:40.64ms
step:1414/2330 train_time:57476ms step_avg:40.65ms
step:1415/2330 train_time:57512ms step_avg:40.64ms
step:1416/2330 train_time:57557ms step_avg:40.65ms
step:1417/2330 train_time:57592ms step_avg:40.64ms
step:1418/2330 train_time:57637ms step_avg:40.65ms
step:1419/2330 train_time:57673ms step_avg:40.64ms
step:1420/2330 train_time:57718ms step_avg:40.65ms
step:1421/2330 train_time:57754ms step_avg:40.64ms
step:1422/2330 train_time:57800ms step_avg:40.65ms
step:1423/2330 train_time:57835ms step_avg:40.64ms
step:1424/2330 train_time:57881ms step_avg:40.65ms
step:1425/2330 train_time:57916ms step_avg:40.64ms
step:1426/2330 train_time:57961ms step_avg:40.65ms
step:1427/2330 train_time:57997ms step_avg:40.64ms
step:1428/2330 train_time:58043ms step_avg:40.65ms
step:1429/2330 train_time:58079ms step_avg:40.64ms
step:1430/2330 train_time:58124ms step_avg:40.65ms
step:1431/2330 train_time:58161ms step_avg:40.64ms
step:1432/2330 train_time:58206ms step_avg:40.65ms
step:1433/2330 train_time:58242ms step_avg:40.64ms
step:1434/2330 train_time:58287ms step_avg:40.65ms
step:1435/2330 train_time:58323ms step_avg:40.64ms
step:1436/2330 train_time:58369ms step_avg:40.65ms
step:1437/2330 train_time:58405ms step_avg:40.64ms
step:1438/2330 train_time:58451ms step_avg:40.65ms
step:1439/2330 train_time:58488ms step_avg:40.64ms
step:1440/2330 train_time:58533ms step_avg:40.65ms
step:1441/2330 train_time:58569ms step_avg:40.64ms
step:1442/2330 train_time:58613ms step_avg:40.65ms
step:1443/2330 train_time:58650ms step_avg:40.64ms
step:1444/2330 train_time:58695ms step_avg:40.65ms
step:1445/2330 train_time:58731ms step_avg:40.64ms
step:1446/2330 train_time:58776ms step_avg:40.65ms
step:1447/2330 train_time:58811ms step_avg:40.64ms
step:1448/2330 train_time:58856ms step_avg:40.65ms
step:1449/2330 train_time:58892ms step_avg:40.64ms
step:1450/2330 train_time:58938ms step_avg:40.65ms
step:1451/2330 train_time:58973ms step_avg:40.64ms
step:1452/2330 train_time:59019ms step_avg:40.65ms
step:1453/2330 train_time:59055ms step_avg:40.64ms
step:1454/2330 train_time:59100ms step_avg:40.65ms
step:1455/2330 train_time:59137ms step_avg:40.64ms
step:1456/2330 train_time:59182ms step_avg:40.65ms
step:1457/2330 train_time:59218ms step_avg:40.64ms
step:1458/2330 train_time:59264ms step_avg:40.65ms
step:1459/2330 train_time:59300ms step_avg:40.64ms
step:1460/2330 train_time:59345ms step_avg:40.65ms
step:1461/2330 train_time:59382ms step_avg:40.64ms
step:1462/2330 train_time:59427ms step_avg:40.65ms
step:1463/2330 train_time:59463ms step_avg:40.64ms
step:1464/2330 train_time:59508ms step_avg:40.65ms
step:1465/2330 train_time:59544ms step_avg:40.64ms
step:1466/2330 train_time:59590ms step_avg:40.65ms
step:1467/2330 train_time:59626ms step_avg:40.64ms
step:1468/2330 train_time:59672ms step_avg:40.65ms
step:1469/2330 train_time:59707ms step_avg:40.64ms
step:1470/2330 train_time:59752ms step_avg:40.65ms
step:1471/2330 train_time:59788ms step_avg:40.64ms
step:1472/2330 train_time:59833ms step_avg:40.65ms
step:1473/2330 train_time:59870ms step_avg:40.64ms
step:1474/2330 train_time:59915ms step_avg:40.65ms
step:1475/2330 train_time:59950ms step_avg:40.64ms
step:1476/2330 train_time:59996ms step_avg:40.65ms
step:1477/2330 train_time:60032ms step_avg:40.64ms
step:1478/2330 train_time:60077ms step_avg:40.65ms
step:1479/2330 train_time:60113ms step_avg:40.64ms
step:1480/2330 train_time:60158ms step_avg:40.65ms
step:1481/2330 train_time:60193ms step_avg:40.64ms
step:1482/2330 train_time:60240ms step_avg:40.65ms
step:1483/2330 train_time:60275ms step_avg:40.64ms
step:1484/2330 train_time:60322ms step_avg:40.65ms
step:1485/2330 train_time:60358ms step_avg:40.64ms
step:1486/2330 train_time:60402ms step_avg:40.65ms
step:1487/2330 train_time:60439ms step_avg:40.64ms
step:1488/2330 train_time:60484ms step_avg:40.65ms
step:1489/2330 train_time:60520ms step_avg:40.64ms
step:1490/2330 train_time:60566ms step_avg:40.65ms
step:1491/2330 train_time:60601ms step_avg:40.64ms
step:1492/2330 train_time:60646ms step_avg:40.65ms
step:1493/2330 train_time:60682ms step_avg:40.64ms
step:1494/2330 train_time:60728ms step_avg:40.65ms
step:1495/2330 train_time:60764ms step_avg:40.64ms
step:1496/2330 train_time:60810ms step_avg:40.65ms
step:1497/2330 train_time:60845ms step_avg:40.64ms
step:1498/2330 train_time:60890ms step_avg:40.65ms
step:1499/2330 train_time:60927ms step_avg:40.64ms
step:1500/2330 train_time:60972ms step_avg:40.65ms
step:1500/2330 val_loss:5.1337 train_time:61062ms step_avg:40.71ms
step:1501/2330 train_time:61076ms step_avg:40.69ms
step:1502/2330 train_time:61089ms step_avg:40.67ms
step:1503/2330 train_time:61101ms step_avg:40.65ms
step:1504/2330 train_time:61137ms step_avg:40.65ms
step:1505/2330 train_time:61171ms step_avg:40.65ms
step:1506/2330 train_time:61215ms step_avg:40.65ms
step:1507/2330 train_time:61250ms step_avg:40.64ms
step:1508/2330 train_time:61294ms step_avg:40.65ms
step:1509/2330 train_time:61329ms step_avg:40.64ms
step:1510/2330 train_time:61379ms step_avg:40.65ms
step:1511/2330 train_time:61420ms step_avg:40.65ms
step:1512/2330 train_time:61468ms step_avg:40.65ms
step:1513/2330 train_time:61503ms step_avg:40.65ms
step:1514/2330 train_time:61547ms step_avg:40.65ms
step:1515/2330 train_time:61582ms step_avg:40.65ms
step:1516/2330 train_time:61627ms step_avg:40.65ms
step:1517/2330 train_time:61662ms step_avg:40.65ms
step:1518/2330 train_time:61707ms step_avg:40.65ms
step:1519/2330 train_time:61743ms step_avg:40.65ms
step:1520/2330 train_time:61789ms step_avg:40.65ms
step:1521/2330 train_time:61824ms step_avg:40.65ms
step:1522/2330 train_time:61868ms step_avg:40.65ms
step:1523/2330 train_time:62131ms step_avg:40.80ms
step:1524/2330 train_time:62146ms step_avg:40.78ms
step:1525/2330 train_time:62158ms step_avg:40.76ms
step:1526/2330 train_time:62180ms step_avg:40.75ms
step:1527/2330 train_time:62214ms step_avg:40.74ms
step:1528/2330 train_time:62297ms step_avg:40.77ms
step:1529/2330 train_time:62332ms step_avg:40.77ms
step:1530/2330 train_time:62375ms step_avg:40.77ms
step:1531/2330 train_time:62481ms step_avg:40.81ms
step:1532/2330 train_time:62595ms step_avg:40.86ms
step:1533/2330 train_time:62630ms step_avg:40.85ms
step:1534/2330 train_time:62674ms step_avg:40.86ms
step:1535/2330 train_time:62709ms step_avg:40.85ms
step:1536/2330 train_time:62753ms step_avg:40.85ms
step:1537/2330 train_time:62788ms step_avg:40.85ms
step:1538/2330 train_time:62832ms step_avg:40.85ms
step:1539/2330 train_time:62866ms step_avg:40.85ms
step:1540/2330 train_time:62911ms step_avg:40.85ms
step:1541/2330 train_time:62946ms step_avg:40.85ms
step:1542/2330 train_time:62990ms step_avg:40.85ms
step:1543/2330 train_time:63024ms step_avg:40.85ms
step:1544/2330 train_time:63068ms step_avg:40.85ms
step:1545/2330 train_time:63103ms step_avg:40.84ms
step:1546/2330 train_time:63147ms step_avg:40.85ms
step:1547/2330 train_time:63182ms step_avg:40.84ms
step:1548/2330 train_time:63227ms step_avg:40.84ms
step:1549/2330 train_time:63261ms step_avg:40.84ms
step:1550/2330 train_time:63305ms step_avg:40.84ms
step:1551/2330 train_time:63340ms step_avg:40.84ms
step:1552/2330 train_time:63385ms step_avg:40.84ms
step:1553/2330 train_time:63422ms step_avg:40.84ms
step:1554/2330 train_time:63473ms step_avg:40.84ms
step:1555/2330 train_time:63513ms step_avg:40.84ms
step:1556/2330 train_time:63561ms step_avg:40.85ms
step:1557/2330 train_time:63597ms step_avg:40.85ms
step:1558/2330 train_time:63643ms step_avg:40.85ms
step:1559/2330 train_time:63680ms step_avg:40.85ms
step:1560/2330 train_time:63726ms step_avg:40.85ms
step:1561/2330 train_time:63762ms step_avg:40.85ms
step:1562/2330 train_time:63807ms step_avg:40.85ms
step:1563/2330 train_time:63843ms step_avg:40.85ms
step:1564/2330 train_time:63888ms step_avg:40.85ms
step:1565/2330 train_time:63923ms step_avg:40.85ms
step:1566/2330 train_time:63967ms step_avg:40.85ms
step:1567/2330 train_time:64002ms step_avg:40.84ms
step:1568/2330 train_time:64046ms step_avg:40.85ms
step:1569/2330 train_time:64081ms step_avg:40.84ms
step:1570/2330 train_time:64126ms step_avg:40.84ms
step:1571/2330 train_time:64161ms step_avg:40.84ms
step:1572/2330 train_time:64205ms step_avg:40.84ms
step:1573/2330 train_time:64240ms step_avg:40.84ms
step:1574/2330 train_time:64285ms step_avg:40.84ms
step:1575/2330 train_time:64320ms step_avg:40.84ms
step:1576/2330 train_time:64365ms step_avg:40.84ms
step:1577/2330 train_time:64402ms step_avg:40.84ms
step:1578/2330 train_time:64449ms step_avg:40.84ms
step:1579/2330 train_time:64487ms step_avg:40.84ms
step:1580/2330 train_time:64534ms step_avg:40.84ms
step:1581/2330 train_time:64572ms step_avg:40.84ms
step:1582/2330 train_time:64618ms step_avg:40.85ms
step:1583/2330 train_time:64654ms step_avg:40.84ms
step:1584/2330 train_time:64700ms step_avg:40.85ms
step:1585/2330 train_time:64736ms step_avg:40.84ms
step:1586/2330 train_time:64781ms step_avg:40.85ms
step:1587/2330 train_time:64816ms step_avg:40.84ms
step:1588/2330 train_time:64861ms step_avg:40.84ms
step:1589/2330 train_time:64897ms step_avg:40.84ms
step:1590/2330 train_time:64942ms step_avg:40.84ms
step:1591/2330 train_time:64977ms step_avg:40.84ms
step:1592/2330 train_time:65022ms step_avg:40.84ms
step:1593/2330 train_time:65057ms step_avg:40.84ms
step:1594/2330 train_time:65101ms step_avg:40.84ms
step:1595/2330 train_time:65136ms step_avg:40.84ms
step:1596/2330 train_time:65181ms step_avg:40.84ms
step:1597/2330 train_time:65215ms step_avg:40.84ms
step:1598/2330 train_time:65260ms step_avg:40.84ms
step:1599/2330 train_time:65295ms step_avg:40.83ms
step:1600/2330 train_time:65340ms step_avg:40.84ms
step:1601/2330 train_time:65375ms step_avg:40.83ms
step:1602/2330 train_time:65420ms step_avg:40.84ms
step:1603/2330 train_time:65457ms step_avg:40.83ms
step:1604/2330 train_time:65503ms step_avg:40.84ms
step:1605/2330 train_time:65541ms step_avg:40.84ms
step:1606/2330 train_time:65588ms step_avg:40.84ms
step:1607/2330 train_time:65625ms step_avg:40.84ms
step:1608/2330 train_time:65670ms step_avg:40.84ms
step:1609/2330 train_time:65706ms step_avg:40.84ms
step:1610/2330 train_time:65753ms step_avg:40.84ms
step:1611/2330 train_time:65789ms step_avg:40.84ms
step:1612/2330 train_time:65834ms step_avg:40.84ms
step:1613/2330 train_time:65871ms step_avg:40.84ms
step:1614/2330 train_time:65915ms step_avg:40.84ms
step:1615/2330 train_time:65951ms step_avg:40.84ms
step:1616/2330 train_time:65996ms step_avg:40.84ms
step:1617/2330 train_time:66032ms step_avg:40.84ms
step:1618/2330 train_time:66077ms step_avg:40.84ms
step:1619/2330 train_time:66111ms step_avg:40.83ms
step:1620/2330 train_time:66156ms step_avg:40.84ms
step:1621/2330 train_time:66192ms step_avg:40.83ms
step:1622/2330 train_time:66236ms step_avg:40.84ms
step:1623/2330 train_time:66272ms step_avg:40.83ms
step:1624/2330 train_time:66317ms step_avg:40.84ms
step:1625/2330 train_time:66353ms step_avg:40.83ms
step:1626/2330 train_time:66398ms step_avg:40.84ms
step:1627/2330 train_time:66434ms step_avg:40.83ms
step:1628/2330 train_time:66480ms step_avg:40.84ms
step:1629/2330 train_time:66516ms step_avg:40.83ms
step:1630/2330 train_time:66563ms step_avg:40.84ms
step:1631/2330 train_time:66598ms step_avg:40.83ms
step:1632/2330 train_time:66644ms step_avg:40.84ms
step:1633/2330 train_time:66680ms step_avg:40.83ms
step:1634/2330 train_time:66726ms step_avg:40.84ms
step:1635/2330 train_time:66762ms step_avg:40.83ms
step:1636/2330 train_time:66808ms step_avg:40.84ms
step:1637/2330 train_time:66844ms step_avg:40.83ms
step:1638/2330 train_time:66889ms step_avg:40.84ms
step:1639/2330 train_time:66925ms step_avg:40.83ms
step:1640/2330 train_time:66970ms step_avg:40.84ms
step:1641/2330 train_time:67006ms step_avg:40.83ms
step:1642/2330 train_time:67051ms step_avg:40.84ms
step:1643/2330 train_time:67086ms step_avg:40.83ms
step:1644/2330 train_time:67131ms step_avg:40.83ms
step:1645/2330 train_time:67166ms step_avg:40.83ms
step:1646/2330 train_time:67211ms step_avg:40.83ms
step:1647/2330 train_time:67247ms step_avg:40.83ms
step:1648/2330 train_time:67294ms step_avg:40.83ms
step:1649/2330 train_time:67330ms step_avg:40.83ms
step:1650/2330 train_time:67375ms step_avg:40.83ms
step:1651/2330 train_time:67411ms step_avg:40.83ms
step:1652/2330 train_time:67456ms step_avg:40.83ms
step:1653/2330 train_time:67492ms step_avg:40.83ms
step:1654/2330 train_time:67539ms step_avg:40.83ms
step:1655/2330 train_time:67575ms step_avg:40.83ms
step:1656/2330 train_time:67621ms step_avg:40.83ms
step:1657/2330 train_time:67657ms step_avg:40.83ms
step:1658/2330 train_time:67703ms step_avg:40.83ms
step:1659/2330 train_time:67738ms step_avg:40.83ms
step:1660/2330 train_time:67785ms step_avg:40.83ms
step:1661/2330 train_time:67820ms step_avg:40.83ms
step:1662/2330 train_time:67866ms step_avg:40.83ms
step:1663/2330 train_time:67902ms step_avg:40.83ms
step:1664/2330 train_time:67948ms step_avg:40.83ms
step:1665/2330 train_time:67983ms step_avg:40.83ms
step:1666/2330 train_time:68028ms step_avg:40.83ms
step:1667/2330 train_time:68064ms step_avg:40.83ms
step:1668/2330 train_time:68109ms step_avg:40.83ms
step:1669/2330 train_time:68144ms step_avg:40.83ms
step:1670/2330 train_time:68189ms step_avg:40.83ms
step:1671/2330 train_time:68225ms step_avg:40.83ms
step:1672/2330 train_time:68271ms step_avg:40.83ms
step:1673/2330 train_time:68307ms step_avg:40.83ms
step:1674/2330 train_time:68353ms step_avg:40.83ms
step:1675/2330 train_time:68389ms step_avg:40.83ms
step:1676/2330 train_time:68435ms step_avg:40.83ms
step:1677/2330 train_time:68471ms step_avg:40.83ms
step:1678/2330 train_time:68516ms step_avg:40.83ms
step:1679/2330 train_time:68552ms step_avg:40.83ms
step:1680/2330 train_time:68598ms step_avg:40.83ms
step:1681/2330 train_time:68634ms step_avg:40.83ms
step:1682/2330 train_time:68679ms step_avg:40.83ms
step:1683/2330 train_time:68715ms step_avg:40.83ms
step:1684/2330 train_time:68762ms step_avg:40.83ms
step:1685/2330 train_time:68797ms step_avg:40.83ms
step:1686/2330 train_time:68843ms step_avg:40.83ms
step:1687/2330 train_time:68879ms step_avg:40.83ms
step:1688/2330 train_time:68924ms step_avg:40.83ms
step:1689/2330 train_time:68960ms step_avg:40.83ms
step:1690/2330 train_time:69005ms step_avg:40.83ms
step:1691/2330 train_time:69040ms step_avg:40.83ms
step:1692/2330 train_time:69086ms step_avg:40.83ms
step:1693/2330 train_time:69121ms step_avg:40.83ms
step:1694/2330 train_time:69166ms step_avg:40.83ms
step:1695/2330 train_time:69202ms step_avg:40.83ms
step:1696/2330 train_time:69246ms step_avg:40.83ms
step:1697/2330 train_time:69283ms step_avg:40.83ms
step:1698/2330 train_time:69328ms step_avg:40.83ms
step:1699/2330 train_time:69364ms step_avg:40.83ms
step:1700/2330 train_time:69409ms step_avg:40.83ms
step:1701/2330 train_time:69445ms step_avg:40.83ms
step:1702/2330 train_time:69491ms step_avg:40.83ms
step:1703/2330 train_time:69527ms step_avg:40.83ms
step:1704/2330 train_time:69573ms step_avg:40.83ms
step:1705/2330 train_time:69609ms step_avg:40.83ms
step:1706/2330 train_time:69655ms step_avg:40.83ms
step:1707/2330 train_time:69693ms step_avg:40.83ms
step:1708/2330 train_time:69738ms step_avg:40.83ms
step:1709/2330 train_time:69774ms step_avg:40.83ms
step:1710/2330 train_time:69819ms step_avg:40.83ms
step:1711/2330 train_time:69856ms step_avg:40.83ms
step:1712/2330 train_time:69901ms step_avg:40.83ms
step:1713/2330 train_time:69936ms step_avg:40.83ms
step:1714/2330 train_time:69982ms step_avg:40.83ms
step:1715/2330 train_time:70017ms step_avg:40.83ms
step:1716/2330 train_time:70062ms step_avg:40.83ms
step:1717/2330 train_time:70097ms step_avg:40.83ms
step:1718/2330 train_time:70142ms step_avg:40.83ms
step:1719/2330 train_time:70177ms step_avg:40.82ms
step:1720/2330 train_time:70223ms step_avg:40.83ms
step:1721/2330 train_time:70259ms step_avg:40.82ms
step:1722/2330 train_time:70304ms step_avg:40.83ms
step:1723/2330 train_time:70339ms step_avg:40.82ms
step:1724/2330 train_time:70385ms step_avg:40.83ms
step:1725/2330 train_time:70421ms step_avg:40.82ms
step:1726/2330 train_time:70465ms step_avg:40.83ms
step:1727/2330 train_time:70501ms step_avg:40.82ms
step:1728/2330 train_time:70546ms step_avg:40.83ms
step:1729/2330 train_time:70583ms step_avg:40.82ms
step:1730/2330 train_time:70629ms step_avg:40.83ms
step:1731/2330 train_time:70665ms step_avg:40.82ms
step:1732/2330 train_time:70710ms step_avg:40.83ms
step:1733/2330 train_time:70746ms step_avg:40.82ms
step:1734/2330 train_time:70792ms step_avg:40.83ms
step:1735/2330 train_time:70828ms step_avg:40.82ms
step:1736/2330 train_time:70873ms step_avg:40.83ms
step:1737/2330 train_time:70909ms step_avg:40.82ms
step:1738/2330 train_time:70955ms step_avg:40.83ms
step:1739/2330 train_time:70991ms step_avg:40.82ms
step:1740/2330 train_time:71036ms step_avg:40.83ms
step:1741/2330 train_time:71073ms step_avg:40.82ms
step:1742/2330 train_time:71119ms step_avg:40.83ms
step:1743/2330 train_time:71155ms step_avg:40.82ms
step:1744/2330 train_time:71200ms step_avg:40.83ms
step:1745/2330 train_time:71235ms step_avg:40.82ms
step:1746/2330 train_time:71280ms step_avg:40.82ms
step:1747/2330 train_time:71316ms step_avg:40.82ms
step:1748/2330 train_time:71361ms step_avg:40.82ms
step:1749/2330 train_time:71397ms step_avg:40.82ms
step:1750/2330 train_time:71441ms step_avg:40.82ms
step:1750/2330 val_loss:5.0993 train_time:71532ms step_avg:40.88ms
step:1751/2330 train_time:71547ms step_avg:40.86ms
step:1752/2330 train_time:71560ms step_avg:40.84ms
step:1753/2330 train_time:71571ms step_avg:40.83ms
step:1754/2330 train_time:71607ms step_avg:40.82ms
step:1755/2330 train_time:71641ms step_avg:40.82ms
step:1756/2330 train_time:71685ms step_avg:40.82ms
step:1757/2330 train_time:71720ms step_avg:40.82ms
step:1758/2330 train_time:71764ms step_avg:40.82ms
step:1759/2330 train_time:71799ms step_avg:40.82ms
step:1760/2330 train_time:71843ms step_avg:40.82ms
step:1761/2330 train_time:71881ms step_avg:40.82ms
step:1762/2330 train_time:71928ms step_avg:40.82ms
step:1763/2330 train_time:71965ms step_avg:40.82ms
step:1764/2330 train_time:72011ms step_avg:40.82ms
step:1765/2330 train_time:72046ms step_avg:40.82ms
step:1766/2330 train_time:72090ms step_avg:40.82ms
step:1767/2330 train_time:72126ms step_avg:40.82ms
step:1768/2330 train_time:72170ms step_avg:40.82ms
step:1769/2330 train_time:72205ms step_avg:40.82ms
step:1770/2330 train_time:72250ms step_avg:40.82ms
step:1771/2330 train_time:72285ms step_avg:40.82ms
step:1772/2330 train_time:72329ms step_avg:40.82ms
step:1773/2330 train_time:72364ms step_avg:40.81ms
step:1774/2330 train_time:72409ms step_avg:40.82ms
step:1775/2330 train_time:72450ms step_avg:40.82ms
step:1776/2330 train_time:72501ms step_avg:40.82ms
step:1777/2330 train_time:72539ms step_avg:40.82ms
step:1778/2330 train_time:72585ms step_avg:40.82ms
step:1779/2330 train_time:72622ms step_avg:40.82ms
step:1780/2330 train_time:72668ms step_avg:40.82ms
step:1781/2330 train_time:72704ms step_avg:40.82ms
step:1782/2330 train_time:72749ms step_avg:40.82ms
step:1783/2330 train_time:72784ms step_avg:40.82ms
step:1784/2330 train_time:72829ms step_avg:40.82ms
step:1785/2330 train_time:72865ms step_avg:40.82ms
step:1786/2330 train_time:72911ms step_avg:40.82ms
step:1787/2330 train_time:72946ms step_avg:40.82ms
step:1788/2330 train_time:72991ms step_avg:40.82ms
step:1789/2330 train_time:73026ms step_avg:40.82ms
step:1790/2330 train_time:73071ms step_avg:40.82ms
step:1791/2330 train_time:73106ms step_avg:40.82ms
step:1792/2330 train_time:73150ms step_avg:40.82ms
step:1793/2330 train_time:73185ms step_avg:40.82ms
step:1794/2330 train_time:73230ms step_avg:40.82ms
step:1795/2330 train_time:73265ms step_avg:40.82ms
step:1796/2330 train_time:73309ms step_avg:40.82ms
step:1797/2330 train_time:73346ms step_avg:40.82ms
step:1798/2330 train_time:73391ms step_avg:40.82ms
step:1799/2330 train_time:73429ms step_avg:40.82ms
step:1800/2330 train_time:73479ms step_avg:40.82ms
step:1801/2330 train_time:73516ms step_avg:40.82ms
step:1802/2330 train_time:73562ms step_avg:40.82ms
step:1803/2330 train_time:73598ms step_avg:40.82ms
step:1804/2330 train_time:73644ms step_avg:40.82ms
step:1805/2330 train_time:73680ms step_avg:40.82ms
step:1806/2330 train_time:73725ms step_avg:40.82ms
step:1807/2330 train_time:73761ms step_avg:40.82ms
step:1808/2330 train_time:73806ms step_avg:40.82ms
step:1809/2330 train_time:73841ms step_avg:40.82ms
step:1810/2330 train_time:73886ms step_avg:40.82ms
step:1811/2330 train_time:73922ms step_avg:40.82ms
step:1812/2330 train_time:73966ms step_avg:40.82ms
step:1813/2330 train_time:74002ms step_avg:40.82ms
step:1814/2330 train_time:74046ms step_avg:40.82ms
step:1815/2330 train_time:74081ms step_avg:40.82ms
step:1816/2330 train_time:74126ms step_avg:40.82ms
step:1817/2330 train_time:74162ms step_avg:40.82ms
step:1818/2330 train_time:74206ms step_avg:40.82ms
step:1819/2330 train_time:74241ms step_avg:40.81ms
step:1820/2330 train_time:74286ms step_avg:40.82ms
step:1821/2330 train_time:74322ms step_avg:40.81ms
step:1822/2330 train_time:74368ms step_avg:40.82ms
step:1823/2330 train_time:74406ms step_avg:40.82ms
step:1824/2330 train_time:74452ms step_avg:40.82ms
step:1825/2330 train_time:74488ms step_avg:40.82ms
step:1826/2330 train_time:74535ms step_avg:40.82ms
step:1827/2330 train_time:74573ms step_avg:40.82ms
step:1828/2330 train_time:74618ms step_avg:40.82ms
step:1829/2330 train_time:74653ms step_avg:40.82ms
step:1830/2330 train_time:74699ms step_avg:40.82ms
step:1831/2330 train_time:74734ms step_avg:40.82ms
step:1832/2330 train_time:74780ms step_avg:40.82ms
step:1833/2330 train_time:74815ms step_avg:40.82ms
step:1834/2330 train_time:74861ms step_avg:40.82ms
step:1835/2330 train_time:74896ms step_avg:40.82ms
step:1836/2330 train_time:74942ms step_avg:40.82ms
step:1837/2330 train_time:74977ms step_avg:40.81ms
step:1838/2330 train_time:75022ms step_avg:40.82ms
step:1839/2330 train_time:75056ms step_avg:40.81ms
step:1840/2330 train_time:75101ms step_avg:40.82ms
step:1841/2330 train_time:75137ms step_avg:40.81ms
step:1842/2330 train_time:75181ms step_avg:40.82ms
step:1843/2330 train_time:75217ms step_avg:40.81ms
step:1844/2330 train_time:75263ms step_avg:40.81ms
step:1845/2330 train_time:75298ms step_avg:40.81ms
step:1846/2330 train_time:75344ms step_avg:40.81ms
step:1847/2330 train_time:75379ms step_avg:40.81ms
step:1848/2330 train_time:75425ms step_avg:40.81ms
step:1849/2330 train_time:75461ms step_avg:40.81ms
step:1850/2330 train_time:75507ms step_avg:40.81ms
step:1851/2330 train_time:75544ms step_avg:40.81ms
step:1852/2330 train_time:75589ms step_avg:40.82ms
step:1853/2330 train_time:75626ms step_avg:40.81ms
step:1854/2330 train_time:75671ms step_avg:40.81ms
step:1855/2330 train_time:75706ms step_avg:40.81ms
step:1856/2330 train_time:75751ms step_avg:40.81ms
step:1857/2330 train_time:75787ms step_avg:40.81ms
step:1858/2330 train_time:75834ms step_avg:40.81ms
step:1859/2330 train_time:75870ms step_avg:40.81ms
step:1860/2330 train_time:75916ms step_avg:40.82ms
step:1861/2330 train_time:75952ms step_avg:40.81ms
step:1862/2330 train_time:75998ms step_avg:40.82ms
step:1863/2330 train_time:76034ms step_avg:40.81ms
step:1864/2330 train_time:76079ms step_avg:40.81ms
step:1865/2330 train_time:76115ms step_avg:40.81ms
step:1866/2330 train_time:76160ms step_avg:40.81ms
step:1867/2330 train_time:76196ms step_avg:40.81ms
step:1868/2330 train_time:76241ms step_avg:40.81ms
step:1869/2330 train_time:76277ms step_avg:40.81ms
step:1870/2330 train_time:76322ms step_avg:40.81ms
step:1871/2330 train_time:76357ms step_avg:40.81ms
step:1872/2330 train_time:76403ms step_avg:40.81ms
step:1873/2330 train_time:76438ms step_avg:40.81ms
step:1874/2330 train_time:76483ms step_avg:40.81ms
step:1875/2330 train_time:76519ms step_avg:40.81ms
step:1876/2330 train_time:76565ms step_avg:40.81ms
step:1877/2330 train_time:76601ms step_avg:40.81ms
step:1878/2330 train_time:76647ms step_avg:40.81ms
step:1879/2330 train_time:76682ms step_avg:40.81ms
step:1880/2330 train_time:76727ms step_avg:40.81ms
step:1881/2330 train_time:76763ms step_avg:40.81ms
step:1882/2330 train_time:76808ms step_avg:40.81ms
step:1883/2330 train_time:76844ms step_avg:40.81ms
step:1884/2330 train_time:76889ms step_avg:40.81ms
step:1885/2330 train_time:76926ms step_avg:40.81ms
step:1886/2330 train_time:76971ms step_avg:40.81ms
step:1887/2330 train_time:77007ms step_avg:40.81ms
step:1888/2330 train_time:77053ms step_avg:40.81ms
step:1889/2330 train_time:77088ms step_avg:40.81ms
step:1890/2330 train_time:77134ms step_avg:40.81ms
step:1891/2330 train_time:77170ms step_avg:40.81ms
step:1892/2330 train_time:77216ms step_avg:40.81ms
step:1893/2330 train_time:77253ms step_avg:40.81ms
step:1894/2330 train_time:77299ms step_avg:40.81ms
step:1895/2330 train_time:77334ms step_avg:40.81ms
step:1896/2330 train_time:77381ms step_avg:40.81ms
step:1897/2330 train_time:77417ms step_avg:40.81ms
step:1898/2330 train_time:77462ms step_avg:40.81ms
step:1899/2330 train_time:77498ms step_avg:40.81ms
step:1900/2330 train_time:77543ms step_avg:40.81ms
step:1901/2330 train_time:77578ms step_avg:40.81ms
step:1902/2330 train_time:77624ms step_avg:40.81ms
step:1903/2330 train_time:77659ms step_avg:40.81ms
step:1904/2330 train_time:77705ms step_avg:40.81ms
step:1905/2330 train_time:77740ms step_avg:40.81ms
step:1906/2330 train_time:77785ms step_avg:40.81ms
step:1907/2330 train_time:77821ms step_avg:40.81ms
step:1908/2330 train_time:77867ms step_avg:40.81ms
step:1909/2330 train_time:77902ms step_avg:40.81ms
step:1910/2330 train_time:77948ms step_avg:40.81ms
step:1911/2330 train_time:77984ms step_avg:40.81ms
step:1912/2330 train_time:78029ms step_avg:40.81ms
step:1913/2330 train_time:78065ms step_avg:40.81ms
step:1914/2330 train_time:78111ms step_avg:40.81ms
step:1915/2330 train_time:78146ms step_avg:40.81ms
step:1916/2330 train_time:78191ms step_avg:40.81ms
step:1917/2330 train_time:78227ms step_avg:40.81ms
step:1918/2330 train_time:78272ms step_avg:40.81ms
step:1919/2330 train_time:78309ms step_avg:40.81ms
step:1920/2330 train_time:78355ms step_avg:40.81ms
step:1921/2330 train_time:78391ms step_avg:40.81ms
step:1922/2330 train_time:78437ms step_avg:40.81ms
step:1923/2330 train_time:78473ms step_avg:40.81ms
step:1924/2330 train_time:78519ms step_avg:40.81ms
step:1925/2330 train_time:78555ms step_avg:40.81ms
step:1926/2330 train_time:78600ms step_avg:40.81ms
step:1927/2330 train_time:78637ms step_avg:40.81ms
step:1928/2330 train_time:78682ms step_avg:40.81ms
step:1929/2330 train_time:78718ms step_avg:40.81ms
step:1930/2330 train_time:78763ms step_avg:40.81ms
step:1931/2330 train_time:78798ms step_avg:40.81ms
step:1932/2330 train_time:78845ms step_avg:40.81ms
step:1933/2330 train_time:78881ms step_avg:40.81ms
step:1934/2330 train_time:78927ms step_avg:40.81ms
step:1935/2330 train_time:78962ms step_avg:40.81ms
step:1936/2330 train_time:79008ms step_avg:40.81ms
step:1937/2330 train_time:79043ms step_avg:40.81ms
step:1938/2330 train_time:79088ms step_avg:40.81ms
step:1939/2330 train_time:79124ms step_avg:40.81ms
step:1940/2330 train_time:79170ms step_avg:40.81ms
step:1941/2330 train_time:79206ms step_avg:40.81ms
step:1942/2330 train_time:79251ms step_avg:40.81ms
step:1943/2330 train_time:79287ms step_avg:40.81ms
step:1944/2330 train_time:79332ms step_avg:40.81ms
step:1945/2330 train_time:79369ms step_avg:40.81ms
step:1946/2330 train_time:79414ms step_avg:40.81ms
step:1947/2330 train_time:79450ms step_avg:40.81ms
step:1948/2330 train_time:79494ms step_avg:40.81ms
step:1949/2330 train_time:79530ms step_avg:40.81ms
step:1950/2330 train_time:79577ms step_avg:40.81ms
step:1951/2330 train_time:79613ms step_avg:40.81ms
step:1952/2330 train_time:79659ms step_avg:40.81ms
step:1953/2330 train_time:79695ms step_avg:40.81ms
step:1954/2330 train_time:79739ms step_avg:40.81ms
step:1955/2330 train_time:79775ms step_avg:40.81ms
step:1956/2330 train_time:79821ms step_avg:40.81ms
step:1957/2330 train_time:79857ms step_avg:40.81ms
step:1958/2330 train_time:79902ms step_avg:40.81ms
step:1959/2330 train_time:79939ms step_avg:40.81ms
step:1960/2330 train_time:79983ms step_avg:40.81ms
step:1961/2330 train_time:80019ms step_avg:40.81ms
step:1962/2330 train_time:80065ms step_avg:40.81ms
step:1963/2330 train_time:80100ms step_avg:40.80ms
step:1964/2330 train_time:80146ms step_avg:40.81ms
step:1965/2330 train_time:80182ms step_avg:40.81ms
step:1966/2330 train_time:80227ms step_avg:40.81ms
step:1967/2330 train_time:80263ms step_avg:40.80ms
step:1968/2330 train_time:80309ms step_avg:40.81ms
step:1969/2330 train_time:80345ms step_avg:40.80ms
step:1970/2330 train_time:80390ms step_avg:40.81ms
step:1971/2330 train_time:80427ms step_avg:40.81ms
step:1972/2330 train_time:80472ms step_avg:40.81ms
step:1973/2330 train_time:80508ms step_avg:40.80ms
step:1974/2330 train_time:80553ms step_avg:40.81ms
step:1975/2330 train_time:80589ms step_avg:40.80ms
step:1976/2330 train_time:80634ms step_avg:40.81ms
step:1977/2330 train_time:80669ms step_avg:40.80ms
step:1978/2330 train_time:80715ms step_avg:40.81ms
step:1979/2330 train_time:80752ms step_avg:40.80ms
step:1980/2330 train_time:80798ms step_avg:40.81ms
step:1981/2330 train_time:80833ms step_avg:40.80ms
step:1982/2330 train_time:80880ms step_avg:40.81ms
step:1983/2330 train_time:80915ms step_avg:40.80ms
step:1984/2330 train_time:80961ms step_avg:40.81ms
step:1985/2330 train_time:80997ms step_avg:40.80ms
step:1986/2330 train_time:81042ms step_avg:40.81ms
step:1987/2330 train_time:81078ms step_avg:40.80ms
step:1988/2330 train_time:81124ms step_avg:40.81ms
step:1989/2330 train_time:81159ms step_avg:40.80ms
step:1990/2330 train_time:81205ms step_avg:40.81ms
step:1991/2330 train_time:81240ms step_avg:40.80ms
step:1992/2330 train_time:81286ms step_avg:40.81ms
step:1993/2330 train_time:81321ms step_avg:40.80ms
step:1994/2330 train_time:81367ms step_avg:40.81ms
step:1995/2330 train_time:81403ms step_avg:40.80ms
step:1996/2330 train_time:81448ms step_avg:40.81ms
step:1997/2330 train_time:81483ms step_avg:40.80ms
step:1998/2330 train_time:81528ms step_avg:40.80ms
step:1999/2330 train_time:81563ms step_avg:40.80ms
step:2000/2330 train_time:81609ms step_avg:40.80ms
step:2000/2330 val_loss:5.0696 train_time:81699ms step_avg:40.85ms
step:2001/2330 train_time:81714ms step_avg:40.84ms
step:2002/2330 train_time:81728ms step_avg:40.82ms
step:2003/2330 train_time:81741ms step_avg:40.81ms
step:2004/2330 train_time:81773ms step_avg:40.80ms
step:2005/2330 train_time:81807ms step_avg:40.80ms
step:2006/2330 train_time:81852ms step_avg:40.80ms
step:2007/2330 train_time:81887ms step_avg:40.80ms
step:2008/2330 train_time:81931ms step_avg:40.80ms
step:2009/2330 train_time:81966ms step_avg:40.80ms
step:2010/2330 train_time:82014ms step_avg:40.80ms
step:2011/2330 train_time:82052ms step_avg:40.80ms
step:2012/2330 train_time:82100ms step_avg:40.80ms
step:2013/2330 train_time:82137ms step_avg:40.80ms
step:2014/2330 train_time:82184ms step_avg:40.81ms
step:2015/2330 train_time:82221ms step_avg:40.80ms
step:2016/2330 train_time:82267ms step_avg:40.81ms
step:2017/2330 train_time:82302ms step_avg:40.80ms
step:2018/2330 train_time:82346ms step_avg:40.81ms
step:2019/2330 train_time:82381ms step_avg:40.80ms
step:2020/2330 train_time:82426ms step_avg:40.81ms
step:2021/2330 train_time:82462ms step_avg:40.80ms
step:2022/2330 train_time:82506ms step_avg:40.80ms
step:2023/2330 train_time:82542ms step_avg:40.80ms
step:2024/2330 train_time:82587ms step_avg:40.80ms
step:2025/2330 train_time:82623ms step_avg:40.80ms
step:2026/2330 train_time:82669ms step_avg:40.80ms
step:2027/2330 train_time:82705ms step_avg:40.80ms
step:2028/2330 train_time:82750ms step_avg:40.80ms
step:2029/2330 train_time:82785ms step_avg:40.80ms
step:2030/2330 train_time:82829ms step_avg:40.80ms
step:2031/2330 train_time:82864ms step_avg:40.80ms
step:2032/2330 train_time:82910ms step_avg:40.80ms
step:2033/2330 train_time:82945ms step_avg:40.80ms
step:2034/2330 train_time:82992ms step_avg:40.80ms
step:2035/2330 train_time:83028ms step_avg:40.80ms
step:2036/2330 train_time:83074ms step_avg:40.80ms
step:2037/2330 train_time:83110ms step_avg:40.80ms
step:2038/2330 train_time:83156ms step_avg:40.80ms
step:2039/2330 train_time:83193ms step_avg:40.80ms
step:2040/2330 train_time:83239ms step_avg:40.80ms
step:2041/2330 train_time:83276ms step_avg:40.80ms
step:2042/2330 train_time:83321ms step_avg:40.80ms
step:2043/2330 train_time:83357ms step_avg:40.80ms
step:2044/2330 train_time:83403ms step_avg:40.80ms
step:2045/2330 train_time:83438ms step_avg:40.80ms
step:2046/2330 train_time:83483ms step_avg:40.80ms
step:2047/2330 train_time:83518ms step_avg:40.80ms
step:2048/2330 train_time:83564ms step_avg:40.80ms
step:2049/2330 train_time:83600ms step_avg:40.80ms
step:2050/2330 train_time:83646ms step_avg:40.80ms
step:2051/2330 train_time:83683ms step_avg:40.80ms
step:2052/2330 train_time:83728ms step_avg:40.80ms
step:2053/2330 train_time:83762ms step_avg:40.80ms
step:2054/2330 train_time:83807ms step_avg:40.80ms
step:2055/2330 train_time:83843ms step_avg:40.80ms
step:2056/2330 train_time:83888ms step_avg:40.80ms
step:2057/2330 train_time:83924ms step_avg:40.80ms
step:2058/2330 train_time:83969ms step_avg:40.80ms
step:2059/2330 train_time:84005ms step_avg:40.80ms
step:2060/2330 train_time:84052ms step_avg:40.80ms
step:2061/2330 train_time:84088ms step_avg:40.80ms
step:2062/2330 train_time:84133ms step_avg:40.80ms
step:2063/2330 train_time:84169ms step_avg:40.80ms
step:2064/2330 train_time:84215ms step_avg:40.80ms
step:2065/2330 train_time:84251ms step_avg:40.80ms
step:2066/2330 train_time:84296ms step_avg:40.80ms
step:2067/2330 train_time:84331ms step_avg:40.80ms
step:2068/2330 train_time:84376ms step_avg:40.80ms
step:2069/2330 train_time:84412ms step_avg:40.80ms
step:2070/2330 train_time:84457ms step_avg:40.80ms
step:2071/2330 train_time:84493ms step_avg:40.80ms
step:2072/2330 train_time:84538ms step_avg:40.80ms
step:2073/2330 train_time:84574ms step_avg:40.80ms
step:2074/2330 train_time:84620ms step_avg:40.80ms
step:2075/2330 train_time:84656ms step_avg:40.80ms
step:2076/2330 train_time:84701ms step_avg:40.80ms
step:2077/2330 train_time:84737ms step_avg:40.80ms
step:2078/2330 train_time:84783ms step_avg:40.80ms
step:2079/2330 train_time:84819ms step_avg:40.80ms
step:2080/2330 train_time:84864ms step_avg:40.80ms
step:2081/2330 train_time:84899ms step_avg:40.80ms
step:2082/2330 train_time:84947ms step_avg:40.80ms
step:2083/2330 train_time:84983ms step_avg:40.80ms
step:2084/2330 train_time:85029ms step_avg:40.80ms
step:2085/2330 train_time:85065ms step_avg:40.80ms
step:2086/2330 train_time:85111ms step_avg:40.80ms
step:2087/2330 train_time:85147ms step_avg:40.80ms
step:2088/2330 train_time:85193ms step_avg:40.80ms
step:2089/2330 train_time:85227ms step_avg:40.80ms
step:2090/2330 train_time:85272ms step_avg:40.80ms
step:2091/2330 train_time:85308ms step_avg:40.80ms
step:2092/2330 train_time:85354ms step_avg:40.80ms
step:2093/2330 train_time:85389ms step_avg:40.80ms
step:2094/2330 train_time:85434ms step_avg:40.80ms
step:2095/2330 train_time:85471ms step_avg:40.80ms
step:2096/2330 train_time:85516ms step_avg:40.80ms
step:2097/2330 train_time:85552ms step_avg:40.80ms
step:2098/2330 train_time:85598ms step_avg:40.80ms
step:2099/2330 train_time:85634ms step_avg:40.80ms
step:2100/2330 train_time:85679ms step_avg:40.80ms
step:2101/2330 train_time:85714ms step_avg:40.80ms
step:2102/2330 train_time:85759ms step_avg:40.80ms
step:2103/2330 train_time:85794ms step_avg:40.80ms
step:2104/2330 train_time:85840ms step_avg:40.80ms
step:2105/2330 train_time:85876ms step_avg:40.80ms
step:2106/2330 train_time:85921ms step_avg:40.80ms
step:2107/2330 train_time:85957ms step_avg:40.80ms
step:2108/2330 train_time:86003ms step_avg:40.80ms
step:2109/2330 train_time:86039ms step_avg:40.80ms
step:2110/2330 train_time:86086ms step_avg:40.80ms
step:2111/2330 train_time:86122ms step_avg:40.80ms
step:2112/2330 train_time:86167ms step_avg:40.80ms
step:2113/2330 train_time:86203ms step_avg:40.80ms
step:2114/2330 train_time:86250ms step_avg:40.80ms
step:2115/2330 train_time:86286ms step_avg:40.80ms
step:2116/2330 train_time:86331ms step_avg:40.80ms
step:2117/2330 train_time:86367ms step_avg:40.80ms
step:2118/2330 train_time:86412ms step_avg:40.80ms
step:2119/2330 train_time:86447ms step_avg:40.80ms
step:2120/2330 train_time:86492ms step_avg:40.80ms
step:2121/2330 train_time:86529ms step_avg:40.80ms
step:2122/2330 train_time:86574ms step_avg:40.80ms
step:2123/2330 train_time:86610ms step_avg:40.80ms
step:2124/2330 train_time:86655ms step_avg:40.80ms
step:2125/2330 train_time:86691ms step_avg:40.80ms
step:2126/2330 train_time:86737ms step_avg:40.80ms
step:2127/2330 train_time:86773ms step_avg:40.80ms
step:2128/2330 train_time:86818ms step_avg:40.80ms
step:2129/2330 train_time:86854ms step_avg:40.80ms
step:2130/2330 train_time:86899ms step_avg:40.80ms
step:2131/2330 train_time:86935ms step_avg:40.80ms
step:2132/2330 train_time:86980ms step_avg:40.80ms
step:2133/2330 train_time:87016ms step_avg:40.79ms
step:2134/2330 train_time:87061ms step_avg:40.80ms
step:2135/2330 train_time:87097ms step_avg:40.80ms
step:2136/2330 train_time:87143ms step_avg:40.80ms
step:2137/2330 train_time:87180ms step_avg:40.80ms
step:2138/2330 train_time:87226ms step_avg:40.80ms
step:2139/2330 train_time:87262ms step_avg:40.80ms
step:2140/2330 train_time:87308ms step_avg:40.80ms
step:2141/2330 train_time:87344ms step_avg:40.80ms
step:2142/2330 train_time:87389ms step_avg:40.80ms
step:2143/2330 train_time:87425ms step_avg:40.80ms
step:2144/2330 train_time:87471ms step_avg:40.80ms
step:2145/2330 train_time:87506ms step_avg:40.80ms
step:2146/2330 train_time:87552ms step_avg:40.80ms
step:2147/2330 train_time:87587ms step_avg:40.80ms
step:2148/2330 train_time:87632ms step_avg:40.80ms
step:2149/2330 train_time:87667ms step_avg:40.79ms
step:2150/2330 train_time:87713ms step_avg:40.80ms
step:2151/2330 train_time:87749ms step_avg:40.79ms
step:2152/2330 train_time:87793ms step_avg:40.80ms
step:2153/2330 train_time:87829ms step_avg:40.79ms
step:2154/2330 train_time:87875ms step_avg:40.80ms
step:2155/2330 train_time:87911ms step_avg:40.79ms
step:2156/2330 train_time:87956ms step_avg:40.80ms
step:2157/2330 train_time:87993ms step_avg:40.79ms
step:2158/2330 train_time:88038ms step_avg:40.80ms
step:2159/2330 train_time:88074ms step_avg:40.79ms
step:2160/2330 train_time:88118ms step_avg:40.80ms
step:2161/2330 train_time:88154ms step_avg:40.79ms
step:2162/2330 train_time:88199ms step_avg:40.80ms
step:2163/2330 train_time:88235ms step_avg:40.79ms
step:2164/2330 train_time:88281ms step_avg:40.80ms
step:2165/2330 train_time:88317ms step_avg:40.79ms
step:2166/2330 train_time:88362ms step_avg:40.80ms
step:2167/2330 train_time:88398ms step_avg:40.79ms
step:2168/2330 train_time:88445ms step_avg:40.80ms
step:2169/2330 train_time:88482ms step_avg:40.79ms
step:2170/2330 train_time:88527ms step_avg:40.80ms
step:2171/2330 train_time:88562ms step_avg:40.79ms
step:2172/2330 train_time:88607ms step_avg:40.80ms
step:2173/2330 train_time:88643ms step_avg:40.79ms
step:2174/2330 train_time:88689ms step_avg:40.80ms
step:2175/2330 train_time:88724ms step_avg:40.79ms
step:2176/2330 train_time:88769ms step_avg:40.79ms
step:2177/2330 train_time:88805ms step_avg:40.79ms
step:2178/2330 train_time:88851ms step_avg:40.79ms
step:2179/2330 train_time:88887ms step_avg:40.79ms
step:2180/2330 train_time:88931ms step_avg:40.79ms
step:2181/2330 train_time:88967ms step_avg:40.79ms
step:2182/2330 train_time:89011ms step_avg:40.79ms
step:2183/2330 train_time:89047ms step_avg:40.79ms
step:2184/2330 train_time:89093ms step_avg:40.79ms
step:2185/2330 train_time:89128ms step_avg:40.79ms
step:2186/2330 train_time:89174ms step_avg:40.79ms
step:2187/2330 train_time:89210ms step_avg:40.79ms
step:2188/2330 train_time:89256ms step_avg:40.79ms
step:2189/2330 train_time:89292ms step_avg:40.79ms
step:2190/2330 train_time:89337ms step_avg:40.79ms
step:2191/2330 train_time:89374ms step_avg:40.79ms
step:2192/2330 train_time:89419ms step_avg:40.79ms
step:2193/2330 train_time:89454ms step_avg:40.79ms
step:2194/2330 train_time:89499ms step_avg:40.79ms
step:2195/2330 train_time:89535ms step_avg:40.79ms
step:2196/2330 train_time:89580ms step_avg:40.79ms
step:2197/2330 train_time:89615ms step_avg:40.79ms
step:2198/2330 train_time:89660ms step_avg:40.79ms
step:2199/2330 train_time:89696ms step_avg:40.79ms
step:2200/2330 train_time:89742ms step_avg:40.79ms
step:2201/2330 train_time:89778ms step_avg:40.79ms
step:2202/2330 train_time:89825ms step_avg:40.79ms
step:2203/2330 train_time:89862ms step_avg:40.79ms
step:2204/2330 train_time:89907ms step_avg:40.79ms
step:2205/2330 train_time:89942ms step_avg:40.79ms
step:2206/2330 train_time:89989ms step_avg:40.79ms
step:2207/2330 train_time:90024ms step_avg:40.79ms
step:2208/2330 train_time:90069ms step_avg:40.79ms
step:2209/2330 train_time:90105ms step_avg:40.79ms
step:2210/2330 train_time:90150ms step_avg:40.79ms
step:2211/2330 train_time:90186ms step_avg:40.79ms
step:2212/2330 train_time:90231ms step_avg:40.79ms
step:2213/2330 train_time:90267ms step_avg:40.79ms
step:2214/2330 train_time:90312ms step_avg:40.79ms
step:2215/2330 train_time:90348ms step_avg:40.79ms
step:2216/2330 train_time:90393ms step_avg:40.79ms
step:2217/2330 train_time:90429ms step_avg:40.79ms
step:2218/2330 train_time:90474ms step_avg:40.79ms
step:2219/2330 train_time:90511ms step_avg:40.79ms
step:2220/2330 train_time:90555ms step_avg:40.79ms
step:2221/2330 train_time:90591ms step_avg:40.79ms
step:2222/2330 train_time:90636ms step_avg:40.79ms
step:2223/2330 train_time:90673ms step_avg:40.79ms
step:2224/2330 train_time:90718ms step_avg:40.79ms
step:2225/2330 train_time:90754ms step_avg:40.79ms
step:2226/2330 train_time:90800ms step_avg:40.79ms
step:2227/2330 train_time:90835ms step_avg:40.79ms
step:2228/2330 train_time:90880ms step_avg:40.79ms
step:2229/2330 train_time:90916ms step_avg:40.79ms
step:2230/2330 train_time:90962ms step_avg:40.79ms
step:2231/2330 train_time:90999ms step_avg:40.79ms
step:2232/2330 train_time:91045ms step_avg:40.79ms
step:2233/2330 train_time:91082ms step_avg:40.79ms
step:2234/2330 train_time:91127ms step_avg:40.79ms
step:2235/2330 train_time:91163ms step_avg:40.79ms
step:2236/2330 train_time:91208ms step_avg:40.79ms
step:2237/2330 train_time:91245ms step_avg:40.79ms
step:2238/2330 train_time:91290ms step_avg:40.79ms
step:2239/2330 train_time:91326ms step_avg:40.79ms
step:2240/2330 train_time:91371ms step_avg:40.79ms
step:2241/2330 train_time:91407ms step_avg:40.79ms
step:2242/2330 train_time:91453ms step_avg:40.79ms
step:2243/2330 train_time:91489ms step_avg:40.79ms
step:2244/2330 train_time:91533ms step_avg:40.79ms
step:2245/2330 train_time:91568ms step_avg:40.79ms
step:2246/2330 train_time:91614ms step_avg:40.79ms
step:2247/2330 train_time:91649ms step_avg:40.79ms
step:2248/2330 train_time:91695ms step_avg:40.79ms
step:2249/2330 train_time:91730ms step_avg:40.79ms
step:2250/2330 train_time:91776ms step_avg:40.79ms
step:2250/2330 val_loss:5.0450 train_time:91866ms step_avg:40.83ms
step:2251/2330 train_time:91880ms step_avg:40.82ms
step:2252/2330 train_time:91893ms step_avg:40.80ms
step:2253/2330 train_time:91904ms step_avg:40.79ms
step:2254/2330 train_time:91939ms step_avg:40.79ms
step:2255/2330 train_time:91973ms step_avg:40.79ms
step:2256/2330 train_time:92018ms step_avg:40.79ms
step:2257/2330 train_time:92053ms step_avg:40.79ms
step:2258/2330 train_time:92098ms step_avg:40.79ms
step:2259/2330 train_time:92132ms step_avg:40.78ms
step:2260/2330 train_time:92178ms step_avg:40.79ms
step:2261/2330 train_time:92219ms step_avg:40.79ms
step:2262/2330 train_time:92268ms step_avg:40.79ms
step:2263/2330 train_time:92306ms step_avg:40.79ms
step:2264/2330 train_time:92353ms step_avg:40.79ms
step:2265/2330 train_time:92389ms step_avg:40.79ms
step:2266/2330 train_time:92434ms step_avg:40.79ms
step:2267/2330 train_time:92470ms step_avg:40.79ms
step:2268/2330 train_time:92514ms step_avg:40.79ms
step:2269/2330 train_time:92550ms step_avg:40.79ms
step:2270/2330 train_time:92595ms step_avg:40.79ms
step:2271/2330 train_time:92630ms step_avg:40.79ms
step:2272/2330 train_time:92674ms step_avg:40.79ms
step:2273/2330 train_time:92709ms step_avg:40.79ms
step:2274/2330 train_time:92754ms step_avg:40.79ms
step:2275/2330 train_time:92789ms step_avg:40.79ms
step:2276/2330 train_time:92834ms step_avg:40.79ms
step:2277/2330 train_time:92870ms step_avg:40.79ms
step:2278/2330 train_time:92914ms step_avg:40.79ms
step:2279/2330 train_time:92949ms step_avg:40.79ms
step:2280/2330 train_time:92994ms step_avg:40.79ms
step:2281/2330 train_time:93028ms step_avg:40.78ms
step:2282/2330 train_time:93073ms step_avg:40.79ms
step:2283/2330 train_time:93110ms step_avg:40.78ms
step:2284/2330 train_time:93157ms step_avg:40.79ms
step:2285/2330 train_time:93193ms step_avg:40.78ms
step:2286/2330 train_time:93239ms step_avg:40.79ms
step:2287/2330 train_time:93276ms step_avg:40.79ms
step:2288/2330 train_time:93322ms step_avg:40.79ms
step:2289/2330 train_time:93359ms step_avg:40.79ms
step:2290/2330 train_time:93406ms step_avg:40.79ms
step:2291/2330 train_time:93443ms step_avg:40.79ms
step:2292/2330 train_time:93488ms step_avg:40.79ms
step:2293/2330 train_time:93524ms step_avg:40.79ms
step:2294/2330 train_time:93568ms step_avg:40.79ms
step:2295/2330 train_time:93603ms step_avg:40.79ms
step:2296/2330 train_time:93648ms step_avg:40.79ms
step:2297/2330 train_time:93684ms step_avg:40.79ms
step:2298/2330 train_time:93729ms step_avg:40.79ms
step:2299/2330 train_time:93764ms step_avg:40.78ms
step:2300/2330 train_time:93809ms step_avg:40.79ms
step:2301/2330 train_time:93844ms step_avg:40.78ms
step:2302/2330 train_time:93890ms step_avg:40.79ms
step:2303/2330 train_time:93925ms step_avg:40.78ms
step:2304/2330 train_time:93969ms step_avg:40.79ms
step:2305/2330 train_time:94004ms step_avg:40.78ms
step:2306/2330 train_time:94050ms step_avg:40.78ms
step:2307/2330 train_time:94086ms step_avg:40.78ms
step:2308/2330 train_time:94132ms step_avg:40.79ms
step:2309/2330 train_time:94169ms step_avg:40.78ms
step:2310/2330 train_time:94215ms step_avg:40.79ms
step:2311/2330 train_time:94251ms step_avg:40.78ms
step:2312/2330 train_time:94296ms step_avg:40.79ms
step:2313/2330 train_time:94332ms step_avg:40.78ms
step:2314/2330 train_time:94378ms step_avg:40.79ms
step:2315/2330 train_time:94415ms step_avg:40.78ms
step:2316/2330 train_time:94460ms step_avg:40.79ms
step:2317/2330 train_time:94495ms step_avg:40.78ms
step:2318/2330 train_time:94540ms step_avg:40.79ms
step:2319/2330 train_time:94576ms step_avg:40.78ms
step:2320/2330 train_time:94621ms step_avg:40.78ms
step:2321/2330 train_time:94656ms step_avg:40.78ms
step:2322/2330 train_time:94701ms step_avg:40.78ms
step:2323/2330 train_time:94737ms step_avg:40.78ms
step:2324/2330 train_time:94781ms step_avg:40.78ms
step:2325/2330 train_time:94817ms step_avg:40.78ms
step:2326/2330 train_time:94862ms step_avg:40.78ms
step:2327/2330 train_time:94898ms step_avg:40.78ms
step:2328/2330 train_time:94943ms step_avg:40.78ms
step:2329/2330 train_time:94978ms step_avg:40.78ms
step:2330/2330 train_time:95025ms step_avg:40.78ms
step:2330/2330 val_loss:5.0387 train_time:95116ms step_avg:40.82ms
peak memory allocated: 29494 MiB reserved: 38888 MiB
