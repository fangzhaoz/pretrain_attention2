import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = divide_by_norm(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = divide_by_norm(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "gd_1e-2"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-2, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 03:17:08 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   31C    P0             111W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   29C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   28C    P0             108W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:91ms step_avg:90.72ms
step:2/2330 train_time:212ms step_avg:105.97ms
step:3/2330 train_time:234ms step_avg:77.86ms
step:4/2330 train_time:261ms step_avg:65.37ms
step:5/2330 train_time:318ms step_avg:63.56ms
step:6/2330 train_time:378ms step_avg:62.95ms
step:7/2330 train_time:436ms step_avg:62.28ms
step:8/2330 train_time:496ms step_avg:62.02ms
step:9/2330 train_time:554ms step_avg:61.50ms
step:10/2330 train_time:614ms step_avg:61.45ms
step:11/2330 train_time:672ms step_avg:61.11ms
step:12/2330 train_time:734ms step_avg:61.17ms
step:13/2330 train_time:792ms step_avg:60.90ms
step:14/2330 train_time:852ms step_avg:60.88ms
step:15/2330 train_time:911ms step_avg:60.70ms
step:16/2330 train_time:971ms step_avg:60.72ms
step:17/2330 train_time:1031ms step_avg:60.65ms
step:18/2330 train_time:1094ms step_avg:60.78ms
step:19/2330 train_time:1157ms step_avg:60.89ms
step:20/2330 train_time:1220ms step_avg:60.98ms
step:21/2330 train_time:1279ms step_avg:60.89ms
step:22/2330 train_time:1342ms step_avg:60.98ms
step:23/2330 train_time:1400ms step_avg:60.88ms
step:24/2330 train_time:1463ms step_avg:60.94ms
step:25/2330 train_time:1521ms step_avg:60.82ms
step:26/2330 train_time:1582ms step_avg:60.84ms
step:27/2330 train_time:1641ms step_avg:60.77ms
step:28/2330 train_time:1704ms step_avg:60.85ms
step:29/2330 train_time:1762ms step_avg:60.75ms
step:30/2330 train_time:1824ms step_avg:60.79ms
step:31/2330 train_time:1882ms step_avg:60.72ms
step:32/2330 train_time:1944ms step_avg:60.75ms
step:33/2330 train_time:2003ms step_avg:60.71ms
step:34/2330 train_time:2066ms step_avg:60.77ms
step:35/2330 train_time:2125ms step_avg:60.71ms
step:36/2330 train_time:2187ms step_avg:60.75ms
step:37/2330 train_time:2246ms step_avg:60.71ms
step:38/2330 train_time:2309ms step_avg:60.75ms
step:39/2330 train_time:2369ms step_avg:60.74ms
step:40/2330 train_time:2432ms step_avg:60.79ms
step:41/2330 train_time:2490ms step_avg:60.74ms
step:42/2330 train_time:2553ms step_avg:60.78ms
step:43/2330 train_time:2611ms step_avg:60.73ms
step:44/2330 train_time:2673ms step_avg:60.75ms
step:45/2330 train_time:2733ms step_avg:60.73ms
step:46/2330 train_time:2795ms step_avg:60.76ms
step:47/2330 train_time:2854ms step_avg:60.72ms
step:48/2330 train_time:2915ms step_avg:60.73ms
step:49/2330 train_time:2974ms step_avg:60.69ms
step:50/2330 train_time:3036ms step_avg:60.72ms
step:51/2330 train_time:3094ms step_avg:60.67ms
step:52/2330 train_time:3156ms step_avg:60.68ms
step:53/2330 train_time:3215ms step_avg:60.65ms
step:54/2330 train_time:3277ms step_avg:60.68ms
step:55/2330 train_time:3337ms step_avg:60.67ms
step:56/2330 train_time:3398ms step_avg:60.68ms
step:57/2330 train_time:3458ms step_avg:60.66ms
step:58/2330 train_time:3520ms step_avg:60.69ms
step:59/2330 train_time:3579ms step_avg:60.66ms
step:60/2330 train_time:3641ms step_avg:60.69ms
step:61/2330 train_time:3700ms step_avg:60.66ms
step:62/2330 train_time:3763ms step_avg:60.69ms
step:63/2330 train_time:3822ms step_avg:60.66ms
step:64/2330 train_time:3883ms step_avg:60.68ms
step:65/2330 train_time:3942ms step_avg:60.65ms
step:66/2330 train_time:4005ms step_avg:60.68ms
step:67/2330 train_time:4063ms step_avg:60.65ms
step:68/2330 train_time:4125ms step_avg:60.67ms
step:69/2330 train_time:4184ms step_avg:60.64ms
step:70/2330 train_time:4246ms step_avg:60.66ms
step:71/2330 train_time:4305ms step_avg:60.64ms
step:72/2330 train_time:4368ms step_avg:60.66ms
step:73/2330 train_time:4427ms step_avg:60.64ms
step:74/2330 train_time:4490ms step_avg:60.67ms
step:75/2330 train_time:4549ms step_avg:60.66ms
step:76/2330 train_time:4612ms step_avg:60.68ms
step:77/2330 train_time:4671ms step_avg:60.66ms
step:78/2330 train_time:4734ms step_avg:60.70ms
step:79/2330 train_time:4793ms step_avg:60.67ms
step:80/2330 train_time:4855ms step_avg:60.69ms
step:81/2330 train_time:4914ms step_avg:60.67ms
step:82/2330 train_time:4976ms step_avg:60.69ms
step:83/2330 train_time:5035ms step_avg:60.66ms
step:84/2330 train_time:5096ms step_avg:60.67ms
step:85/2330 train_time:5155ms step_avg:60.64ms
step:86/2330 train_time:5216ms step_avg:60.65ms
step:87/2330 train_time:5275ms step_avg:60.63ms
step:88/2330 train_time:5337ms step_avg:60.65ms
step:89/2330 train_time:5396ms step_avg:60.63ms
step:90/2330 train_time:5458ms step_avg:60.64ms
step:91/2330 train_time:5516ms step_avg:60.62ms
step:92/2330 train_time:5579ms step_avg:60.64ms
step:93/2330 train_time:5638ms step_avg:60.62ms
step:94/2330 train_time:5701ms step_avg:60.64ms
step:95/2330 train_time:5759ms step_avg:60.62ms
step:96/2330 train_time:5822ms step_avg:60.64ms
step:97/2330 train_time:5880ms step_avg:60.62ms
step:98/2330 train_time:5944ms step_avg:60.65ms
step:99/2330 train_time:6002ms step_avg:60.63ms
step:100/2330 train_time:6065ms step_avg:60.65ms
step:101/2330 train_time:6123ms step_avg:60.63ms
step:102/2330 train_time:6185ms step_avg:60.64ms
step:103/2330 train_time:6244ms step_avg:60.62ms
step:104/2330 train_time:6305ms step_avg:60.63ms
step:105/2330 train_time:6365ms step_avg:60.62ms
step:106/2330 train_time:6427ms step_avg:60.63ms
step:107/2330 train_time:6485ms step_avg:60.61ms
step:108/2330 train_time:6548ms step_avg:60.63ms
step:109/2330 train_time:6606ms step_avg:60.60ms
step:110/2330 train_time:6668ms step_avg:60.62ms
step:111/2330 train_time:6728ms step_avg:60.62ms
step:112/2330 train_time:6790ms step_avg:60.63ms
step:113/2330 train_time:6850ms step_avg:60.62ms
step:114/2330 train_time:6913ms step_avg:60.64ms
step:115/2330 train_time:6973ms step_avg:60.64ms
step:116/2330 train_time:7036ms step_avg:60.65ms
step:117/2330 train_time:7095ms step_avg:60.64ms
step:118/2330 train_time:7157ms step_avg:60.65ms
step:119/2330 train_time:7215ms step_avg:60.63ms
step:120/2330 train_time:7276ms step_avg:60.64ms
step:121/2330 train_time:7335ms step_avg:60.62ms
step:122/2330 train_time:7396ms step_avg:60.63ms
step:123/2330 train_time:7455ms step_avg:60.61ms
step:124/2330 train_time:7517ms step_avg:60.62ms
step:125/2330 train_time:7576ms step_avg:60.61ms
step:126/2330 train_time:7639ms step_avg:60.63ms
step:127/2330 train_time:7698ms step_avg:60.62ms
step:128/2330 train_time:7761ms step_avg:60.63ms
step:129/2330 train_time:7821ms step_avg:60.63ms
step:130/2330 train_time:7884ms step_avg:60.64ms
step:131/2330 train_time:7942ms step_avg:60.63ms
step:132/2330 train_time:8005ms step_avg:60.64ms
step:133/2330 train_time:8064ms step_avg:60.63ms
step:134/2330 train_time:8126ms step_avg:60.64ms
step:135/2330 train_time:8185ms step_avg:60.63ms
step:136/2330 train_time:8247ms step_avg:60.64ms
step:137/2330 train_time:8306ms step_avg:60.63ms
step:138/2330 train_time:8368ms step_avg:60.64ms
step:139/2330 train_time:8427ms step_avg:60.63ms
step:140/2330 train_time:8489ms step_avg:60.64ms
step:141/2330 train_time:8549ms step_avg:60.63ms
step:142/2330 train_time:8610ms step_avg:60.64ms
step:143/2330 train_time:8669ms step_avg:60.62ms
step:144/2330 train_time:8732ms step_avg:60.64ms
step:145/2330 train_time:8792ms step_avg:60.63ms
step:146/2330 train_time:8854ms step_avg:60.64ms
step:147/2330 train_time:8913ms step_avg:60.63ms
step:148/2330 train_time:8976ms step_avg:60.65ms
step:149/2330 train_time:9035ms step_avg:60.64ms
step:150/2330 train_time:9096ms step_avg:60.64ms
step:151/2330 train_time:9155ms step_avg:60.63ms
step:152/2330 train_time:9216ms step_avg:60.63ms
step:153/2330 train_time:9275ms step_avg:60.62ms
step:154/2330 train_time:9336ms step_avg:60.63ms
step:155/2330 train_time:9395ms step_avg:60.61ms
step:156/2330 train_time:9457ms step_avg:60.62ms
step:157/2330 train_time:9516ms step_avg:60.61ms
step:158/2330 train_time:9578ms step_avg:60.62ms
step:159/2330 train_time:9638ms step_avg:60.62ms
step:160/2330 train_time:9700ms step_avg:60.63ms
step:161/2330 train_time:9760ms step_avg:60.62ms
step:162/2330 train_time:9821ms step_avg:60.63ms
step:163/2330 train_time:9880ms step_avg:60.62ms
step:164/2330 train_time:9944ms step_avg:60.63ms
step:165/2330 train_time:10003ms step_avg:60.62ms
step:166/2330 train_time:10065ms step_avg:60.64ms
step:167/2330 train_time:10124ms step_avg:60.62ms
step:168/2330 train_time:10186ms step_avg:60.63ms
step:169/2330 train_time:10245ms step_avg:60.62ms
step:170/2330 train_time:10306ms step_avg:60.63ms
step:171/2330 train_time:10365ms step_avg:60.62ms
step:172/2330 train_time:10427ms step_avg:60.62ms
step:173/2330 train_time:10487ms step_avg:60.62ms
step:174/2330 train_time:10549ms step_avg:60.63ms
step:175/2330 train_time:10608ms step_avg:60.62ms
step:176/2330 train_time:10670ms step_avg:60.63ms
step:177/2330 train_time:10730ms step_avg:60.62ms
step:178/2330 train_time:10792ms step_avg:60.63ms
step:179/2330 train_time:10851ms step_avg:60.62ms
step:180/2330 train_time:10913ms step_avg:60.63ms
step:181/2330 train_time:10972ms step_avg:60.62ms
step:182/2330 train_time:11035ms step_avg:60.63ms
step:183/2330 train_time:11094ms step_avg:60.62ms
step:184/2330 train_time:11155ms step_avg:60.62ms
step:185/2330 train_time:11214ms step_avg:60.61ms
step:186/2330 train_time:11276ms step_avg:60.62ms
step:187/2330 train_time:11335ms step_avg:60.61ms
step:188/2330 train_time:11396ms step_avg:60.62ms
step:189/2330 train_time:11455ms step_avg:60.61ms
step:190/2330 train_time:11517ms step_avg:60.61ms
step:191/2330 train_time:11575ms step_avg:60.60ms
step:192/2330 train_time:11638ms step_avg:60.61ms
step:193/2330 train_time:11697ms step_avg:60.60ms
step:194/2330 train_time:11758ms step_avg:60.61ms
step:195/2330 train_time:11817ms step_avg:60.60ms
step:196/2330 train_time:11879ms step_avg:60.61ms
step:197/2330 train_time:11939ms step_avg:60.61ms
step:198/2330 train_time:12002ms step_avg:60.62ms
step:199/2330 train_time:12061ms step_avg:60.61ms
step:200/2330 train_time:12124ms step_avg:60.62ms
step:201/2330 train_time:12182ms step_avg:60.61ms
step:202/2330 train_time:12245ms step_avg:60.62ms
step:203/2330 train_time:12303ms step_avg:60.61ms
step:204/2330 train_time:12366ms step_avg:60.62ms
step:205/2330 train_time:12424ms step_avg:60.61ms
step:206/2330 train_time:12486ms step_avg:60.61ms
step:207/2330 train_time:12545ms step_avg:60.60ms
step:208/2330 train_time:12608ms step_avg:60.61ms
step:209/2330 train_time:12667ms step_avg:60.61ms
step:210/2330 train_time:12730ms step_avg:60.62ms
step:211/2330 train_time:12789ms step_avg:60.61ms
step:212/2330 train_time:12852ms step_avg:60.62ms
step:213/2330 train_time:12911ms step_avg:60.61ms
step:214/2330 train_time:12973ms step_avg:60.62ms
step:215/2330 train_time:13035ms step_avg:60.63ms
step:216/2330 train_time:13096ms step_avg:60.63ms
step:217/2330 train_time:13155ms step_avg:60.62ms
step:218/2330 train_time:13216ms step_avg:60.63ms
step:219/2330 train_time:13275ms step_avg:60.62ms
step:220/2330 train_time:13337ms step_avg:60.62ms
step:221/2330 train_time:13395ms step_avg:60.61ms
step:222/2330 train_time:13457ms step_avg:60.62ms
step:223/2330 train_time:13516ms step_avg:60.61ms
step:224/2330 train_time:13578ms step_avg:60.62ms
step:225/2330 train_time:13638ms step_avg:60.61ms
step:226/2330 train_time:13700ms step_avg:60.62ms
step:227/2330 train_time:13760ms step_avg:60.62ms
step:228/2330 train_time:13822ms step_avg:60.62ms
step:229/2330 train_time:13881ms step_avg:60.62ms
step:230/2330 train_time:13945ms step_avg:60.63ms
step:231/2330 train_time:14004ms step_avg:60.62ms
step:232/2330 train_time:14066ms step_avg:60.63ms
step:233/2330 train_time:14125ms step_avg:60.62ms
step:234/2330 train_time:14187ms step_avg:60.63ms
step:235/2330 train_time:14246ms step_avg:60.62ms
step:236/2330 train_time:14308ms step_avg:60.63ms
step:237/2330 train_time:14367ms step_avg:60.62ms
step:238/2330 train_time:14429ms step_avg:60.63ms
step:239/2330 train_time:14488ms step_avg:60.62ms
step:240/2330 train_time:14551ms step_avg:60.63ms
step:241/2330 train_time:14610ms step_avg:60.62ms
step:242/2330 train_time:14673ms step_avg:60.63ms
step:243/2330 train_time:14733ms step_avg:60.63ms
step:244/2330 train_time:14795ms step_avg:60.64ms
step:245/2330 train_time:14854ms step_avg:60.63ms
step:246/2330 train_time:14916ms step_avg:60.63ms
step:247/2330 train_time:14975ms step_avg:60.63ms
step:248/2330 train_time:15037ms step_avg:60.63ms
step:249/2330 train_time:15095ms step_avg:60.62ms
step:250/2330 train_time:15157ms step_avg:60.63ms
step:250/2330 val_loss:5.0980 train_time:15229ms step_avg:60.92ms
step:251/2330 train_time:15251ms step_avg:60.76ms
step:252/2330 train_time:15280ms step_avg:60.63ms
step:253/2330 train_time:15339ms step_avg:60.63ms
step:254/2330 train_time:15406ms step_avg:60.65ms
step:255/2330 train_time:15473ms step_avg:60.68ms
step:256/2330 train_time:15535ms step_avg:60.68ms
step:257/2330 train_time:15594ms step_avg:60.68ms
step:258/2330 train_time:15655ms step_avg:60.68ms
step:259/2330 train_time:15714ms step_avg:60.67ms
step:260/2330 train_time:15776ms step_avg:60.68ms
step:261/2330 train_time:15835ms step_avg:60.67ms
step:262/2330 train_time:15896ms step_avg:60.67ms
step:263/2330 train_time:15954ms step_avg:60.66ms
step:264/2330 train_time:16016ms step_avg:60.67ms
step:265/2330 train_time:16074ms step_avg:60.66ms
step:266/2330 train_time:16135ms step_avg:60.66ms
step:267/2330 train_time:16194ms step_avg:60.65ms
step:268/2330 train_time:16256ms step_avg:60.66ms
step:269/2330 train_time:16316ms step_avg:60.65ms
step:270/2330 train_time:16380ms step_avg:60.67ms
step:271/2330 train_time:16440ms step_avg:60.66ms
step:272/2330 train_time:16503ms step_avg:60.67ms
step:273/2330 train_time:16563ms step_avg:60.67ms
step:274/2330 train_time:16626ms step_avg:60.68ms
step:275/2330 train_time:16685ms step_avg:60.67ms
step:276/2330 train_time:16746ms step_avg:60.68ms
step:277/2330 train_time:16805ms step_avg:60.67ms
step:278/2330 train_time:16867ms step_avg:60.67ms
step:279/2330 train_time:16926ms step_avg:60.67ms
step:280/2330 train_time:16988ms step_avg:60.67ms
step:281/2330 train_time:17047ms step_avg:60.67ms
step:282/2330 train_time:17109ms step_avg:60.67ms
step:283/2330 train_time:17168ms step_avg:60.66ms
step:284/2330 train_time:17229ms step_avg:60.67ms
step:285/2330 train_time:17289ms step_avg:60.66ms
step:286/2330 train_time:17351ms step_avg:60.67ms
step:287/2330 train_time:17410ms step_avg:60.66ms
step:288/2330 train_time:17473ms step_avg:60.67ms
step:289/2330 train_time:17533ms step_avg:60.67ms
step:290/2330 train_time:17595ms step_avg:60.67ms
step:291/2330 train_time:17654ms step_avg:60.67ms
step:292/2330 train_time:17717ms step_avg:60.67ms
step:293/2330 train_time:17776ms step_avg:60.67ms
step:294/2330 train_time:17838ms step_avg:60.67ms
step:295/2330 train_time:17897ms step_avg:60.67ms
step:296/2330 train_time:17959ms step_avg:60.67ms
step:297/2330 train_time:18017ms step_avg:60.66ms
step:298/2330 train_time:18079ms step_avg:60.67ms
step:299/2330 train_time:18138ms step_avg:60.66ms
step:300/2330 train_time:18201ms step_avg:60.67ms
step:301/2330 train_time:18260ms step_avg:60.67ms
step:302/2330 train_time:18323ms step_avg:60.67ms
step:303/2330 train_time:18384ms step_avg:60.67ms
step:304/2330 train_time:18446ms step_avg:60.68ms
step:305/2330 train_time:18506ms step_avg:60.68ms
step:306/2330 train_time:18569ms step_avg:60.68ms
step:307/2330 train_time:18628ms step_avg:60.68ms
step:308/2330 train_time:18689ms step_avg:60.68ms
step:309/2330 train_time:18748ms step_avg:60.67ms
step:310/2330 train_time:18810ms step_avg:60.68ms
step:311/2330 train_time:18869ms step_avg:60.67ms
step:312/2330 train_time:18931ms step_avg:60.68ms
step:313/2330 train_time:18991ms step_avg:60.67ms
step:314/2330 train_time:19053ms step_avg:60.68ms
step:315/2330 train_time:19112ms step_avg:60.67ms
step:316/2330 train_time:19175ms step_avg:60.68ms
step:317/2330 train_time:19234ms step_avg:60.68ms
step:318/2330 train_time:19297ms step_avg:60.68ms
step:319/2330 train_time:19356ms step_avg:60.68ms
step:320/2330 train_time:19419ms step_avg:60.68ms
step:321/2330 train_time:19477ms step_avg:60.68ms
step:322/2330 train_time:19540ms step_avg:60.68ms
step:323/2330 train_time:19599ms step_avg:60.68ms
step:324/2330 train_time:19661ms step_avg:60.68ms
step:325/2330 train_time:19721ms step_avg:60.68ms
step:326/2330 train_time:19784ms step_avg:60.69ms
step:327/2330 train_time:19843ms step_avg:60.68ms
step:328/2330 train_time:19907ms step_avg:60.69ms
step:329/2330 train_time:19966ms step_avg:60.69ms
step:330/2330 train_time:20028ms step_avg:60.69ms
step:331/2330 train_time:20087ms step_avg:60.69ms
step:332/2330 train_time:20149ms step_avg:60.69ms
step:333/2330 train_time:20208ms step_avg:60.68ms
step:334/2330 train_time:20271ms step_avg:60.69ms
step:335/2330 train_time:20329ms step_avg:60.68ms
step:336/2330 train_time:20390ms step_avg:60.69ms
step:337/2330 train_time:20450ms step_avg:60.68ms
step:338/2330 train_time:20513ms step_avg:60.69ms
step:339/2330 train_time:20572ms step_avg:60.68ms
step:340/2330 train_time:20635ms step_avg:60.69ms
step:341/2330 train_time:20694ms step_avg:60.69ms
step:342/2330 train_time:20756ms step_avg:60.69ms
step:343/2330 train_time:20816ms step_avg:60.69ms
step:344/2330 train_time:20879ms step_avg:60.69ms
step:345/2330 train_time:20938ms step_avg:60.69ms
step:346/2330 train_time:21000ms step_avg:60.69ms
step:347/2330 train_time:21058ms step_avg:60.69ms
step:348/2330 train_time:21120ms step_avg:60.69ms
step:349/2330 train_time:21180ms step_avg:60.69ms
step:350/2330 train_time:21241ms step_avg:60.69ms
step:351/2330 train_time:21301ms step_avg:60.69ms
step:352/2330 train_time:21364ms step_avg:60.69ms
step:353/2330 train_time:21424ms step_avg:60.69ms
step:354/2330 train_time:21486ms step_avg:60.70ms
step:355/2330 train_time:21545ms step_avg:60.69ms
step:356/2330 train_time:21608ms step_avg:60.70ms
step:357/2330 train_time:21668ms step_avg:60.69ms
step:358/2330 train_time:21730ms step_avg:60.70ms
step:359/2330 train_time:21789ms step_avg:60.69ms
step:360/2330 train_time:21851ms step_avg:60.70ms
step:361/2330 train_time:21910ms step_avg:60.69ms
step:362/2330 train_time:21971ms step_avg:60.69ms
step:363/2330 train_time:22030ms step_avg:60.69ms
step:364/2330 train_time:22093ms step_avg:60.70ms
step:365/2330 train_time:22153ms step_avg:60.69ms
step:366/2330 train_time:22215ms step_avg:60.70ms
step:367/2330 train_time:22274ms step_avg:60.69ms
step:368/2330 train_time:22337ms step_avg:60.70ms
step:369/2330 train_time:22396ms step_avg:60.69ms
step:370/2330 train_time:22459ms step_avg:60.70ms
step:371/2330 train_time:22518ms step_avg:60.69ms
step:372/2330 train_time:22580ms step_avg:60.70ms
step:373/2330 train_time:22639ms step_avg:60.69ms
step:374/2330 train_time:22701ms step_avg:60.70ms
step:375/2330 train_time:22762ms step_avg:60.70ms
step:376/2330 train_time:22824ms step_avg:60.70ms
step:377/2330 train_time:22884ms step_avg:60.70ms
step:378/2330 train_time:22946ms step_avg:60.70ms
step:379/2330 train_time:23007ms step_avg:60.70ms
step:380/2330 train_time:23069ms step_avg:60.71ms
step:381/2330 train_time:23128ms step_avg:60.70ms
step:382/2330 train_time:23190ms step_avg:60.71ms
step:383/2330 train_time:23248ms step_avg:60.70ms
step:384/2330 train_time:23310ms step_avg:60.70ms
step:385/2330 train_time:23369ms step_avg:60.70ms
step:386/2330 train_time:23431ms step_avg:60.70ms
step:387/2330 train_time:23491ms step_avg:60.70ms
step:388/2330 train_time:23554ms step_avg:60.70ms
step:389/2330 train_time:23613ms step_avg:60.70ms
step:390/2330 train_time:23676ms step_avg:60.71ms
step:391/2330 train_time:23735ms step_avg:60.70ms
step:392/2330 train_time:23798ms step_avg:60.71ms
step:393/2330 train_time:23857ms step_avg:60.71ms
step:394/2330 train_time:23919ms step_avg:60.71ms
step:395/2330 train_time:23978ms step_avg:60.70ms
step:396/2330 train_time:24041ms step_avg:60.71ms
step:397/2330 train_time:24101ms step_avg:60.71ms
step:398/2330 train_time:24163ms step_avg:60.71ms
step:399/2330 train_time:24223ms step_avg:60.71ms
step:400/2330 train_time:24285ms step_avg:60.71ms
step:401/2330 train_time:24345ms step_avg:60.71ms
step:402/2330 train_time:24407ms step_avg:60.71ms
step:403/2330 train_time:24467ms step_avg:60.71ms
step:404/2330 train_time:24528ms step_avg:60.71ms
step:405/2330 train_time:24588ms step_avg:60.71ms
step:406/2330 train_time:24649ms step_avg:60.71ms
step:407/2330 train_time:24709ms step_avg:60.71ms
step:408/2330 train_time:24772ms step_avg:60.72ms
step:409/2330 train_time:24831ms step_avg:60.71ms
step:410/2330 train_time:24893ms step_avg:60.71ms
step:411/2330 train_time:24952ms step_avg:60.71ms
step:412/2330 train_time:25015ms step_avg:60.72ms
step:413/2330 train_time:25074ms step_avg:60.71ms
step:414/2330 train_time:25137ms step_avg:60.72ms
step:415/2330 train_time:25196ms step_avg:60.71ms
step:416/2330 train_time:25259ms step_avg:60.72ms
step:417/2330 train_time:25318ms step_avg:60.71ms
step:418/2330 train_time:25380ms step_avg:60.72ms
step:419/2330 train_time:25440ms step_avg:60.72ms
step:420/2330 train_time:25502ms step_avg:60.72ms
step:421/2330 train_time:25562ms step_avg:60.72ms
step:422/2330 train_time:25626ms step_avg:60.72ms
step:423/2330 train_time:25685ms step_avg:60.72ms
step:424/2330 train_time:25747ms step_avg:60.72ms
step:425/2330 train_time:25806ms step_avg:60.72ms
step:426/2330 train_time:25870ms step_avg:60.73ms
step:427/2330 train_time:25929ms step_avg:60.72ms
step:428/2330 train_time:25990ms step_avg:60.73ms
step:429/2330 train_time:26050ms step_avg:60.72ms
step:430/2330 train_time:26111ms step_avg:60.72ms
step:431/2330 train_time:26171ms step_avg:60.72ms
step:432/2330 train_time:26232ms step_avg:60.72ms
step:433/2330 train_time:26292ms step_avg:60.72ms
step:434/2330 train_time:26355ms step_avg:60.73ms
step:435/2330 train_time:26414ms step_avg:60.72ms
step:436/2330 train_time:26478ms step_avg:60.73ms
step:437/2330 train_time:26536ms step_avg:60.72ms
step:438/2330 train_time:26599ms step_avg:60.73ms
step:439/2330 train_time:26658ms step_avg:60.72ms
step:440/2330 train_time:26720ms step_avg:60.73ms
step:441/2330 train_time:26779ms step_avg:60.72ms
step:442/2330 train_time:26842ms step_avg:60.73ms
step:443/2330 train_time:26902ms step_avg:60.73ms
step:444/2330 train_time:26965ms step_avg:60.73ms
step:445/2330 train_time:27025ms step_avg:60.73ms
step:446/2330 train_time:27087ms step_avg:60.73ms
step:447/2330 train_time:27148ms step_avg:60.73ms
step:448/2330 train_time:27210ms step_avg:60.74ms
step:449/2330 train_time:27270ms step_avg:60.73ms
step:450/2330 train_time:27331ms step_avg:60.74ms
step:451/2330 train_time:27391ms step_avg:60.73ms
step:452/2330 train_time:27452ms step_avg:60.74ms
step:453/2330 train_time:27511ms step_avg:60.73ms
step:454/2330 train_time:27572ms step_avg:60.73ms
step:455/2330 train_time:27632ms step_avg:60.73ms
step:456/2330 train_time:27694ms step_avg:60.73ms
step:457/2330 train_time:27754ms step_avg:60.73ms
step:458/2330 train_time:27818ms step_avg:60.74ms
step:459/2330 train_time:27877ms step_avg:60.73ms
step:460/2330 train_time:27939ms step_avg:60.74ms
step:461/2330 train_time:27999ms step_avg:60.73ms
step:462/2330 train_time:28061ms step_avg:60.74ms
step:463/2330 train_time:28120ms step_avg:60.73ms
step:464/2330 train_time:28183ms step_avg:60.74ms
step:465/2330 train_time:28242ms step_avg:60.74ms
step:466/2330 train_time:28306ms step_avg:60.74ms
step:467/2330 train_time:28366ms step_avg:60.74ms
step:468/2330 train_time:28429ms step_avg:60.74ms
step:469/2330 train_time:28488ms step_avg:60.74ms
step:470/2330 train_time:28550ms step_avg:60.74ms
step:471/2330 train_time:28609ms step_avg:60.74ms
step:472/2330 train_time:28671ms step_avg:60.74ms
step:473/2330 train_time:28730ms step_avg:60.74ms
step:474/2330 train_time:28791ms step_avg:60.74ms
step:475/2330 train_time:28850ms step_avg:60.74ms
step:476/2330 train_time:28913ms step_avg:60.74ms
step:477/2330 train_time:28973ms step_avg:60.74ms
step:478/2330 train_time:29036ms step_avg:60.74ms
step:479/2330 train_time:29096ms step_avg:60.74ms
step:480/2330 train_time:29159ms step_avg:60.75ms
step:481/2330 train_time:29219ms step_avg:60.75ms
step:482/2330 train_time:29281ms step_avg:60.75ms
step:483/2330 train_time:29341ms step_avg:60.75ms
step:484/2330 train_time:29403ms step_avg:60.75ms
step:485/2330 train_time:29463ms step_avg:60.75ms
step:486/2330 train_time:29526ms step_avg:60.75ms
step:487/2330 train_time:29586ms step_avg:60.75ms
step:488/2330 train_time:29649ms step_avg:60.76ms
step:489/2330 train_time:29708ms step_avg:60.75ms
step:490/2330 train_time:29770ms step_avg:60.75ms
step:491/2330 train_time:29830ms step_avg:60.75ms
step:492/2330 train_time:29891ms step_avg:60.75ms
step:493/2330 train_time:29950ms step_avg:60.75ms
step:494/2330 train_time:30012ms step_avg:60.75ms
step:495/2330 train_time:30072ms step_avg:60.75ms
step:496/2330 train_time:30134ms step_avg:60.75ms
step:497/2330 train_time:30193ms step_avg:60.75ms
step:498/2330 train_time:30257ms step_avg:60.76ms
step:499/2330 train_time:30317ms step_avg:60.75ms
step:500/2330 train_time:30379ms step_avg:60.76ms
step:500/2330 val_loss:4.5340 train_time:30453ms step_avg:60.91ms
step:501/2330 train_time:30474ms step_avg:60.83ms
step:502/2330 train_time:30504ms step_avg:60.76ms
step:503/2330 train_time:30565ms step_avg:60.77ms
step:504/2330 train_time:30630ms step_avg:60.77ms
step:505/2330 train_time:30692ms step_avg:60.78ms
step:506/2330 train_time:30755ms step_avg:60.78ms
step:507/2330 train_time:30815ms step_avg:60.78ms
step:508/2330 train_time:30876ms step_avg:60.78ms
step:509/2330 train_time:30935ms step_avg:60.78ms
step:510/2330 train_time:30997ms step_avg:60.78ms
step:511/2330 train_time:31055ms step_avg:60.77ms
step:512/2330 train_time:31117ms step_avg:60.77ms
step:513/2330 train_time:31175ms step_avg:60.77ms
step:514/2330 train_time:31237ms step_avg:60.77ms
step:515/2330 train_time:31296ms step_avg:60.77ms
step:516/2330 train_time:31358ms step_avg:60.77ms
step:517/2330 train_time:31419ms step_avg:60.77ms
step:518/2330 train_time:31482ms step_avg:60.78ms
step:519/2330 train_time:31542ms step_avg:60.77ms
step:520/2330 train_time:31605ms step_avg:60.78ms
step:521/2330 train_time:31664ms step_avg:60.78ms
step:522/2330 train_time:31727ms step_avg:60.78ms
step:523/2330 train_time:31787ms step_avg:60.78ms
step:524/2330 train_time:31849ms step_avg:60.78ms
step:525/2330 train_time:31907ms step_avg:60.78ms
step:526/2330 train_time:31969ms step_avg:60.78ms
step:527/2330 train_time:32028ms step_avg:60.77ms
step:528/2330 train_time:32090ms step_avg:60.78ms
step:529/2330 train_time:32149ms step_avg:60.77ms
step:530/2330 train_time:32211ms step_avg:60.78ms
step:531/2330 train_time:32271ms step_avg:60.77ms
step:532/2330 train_time:32333ms step_avg:60.78ms
step:533/2330 train_time:32393ms step_avg:60.78ms
step:534/2330 train_time:32458ms step_avg:60.78ms
step:535/2330 train_time:32518ms step_avg:60.78ms
step:536/2330 train_time:32581ms step_avg:60.78ms
step:537/2330 train_time:32640ms step_avg:60.78ms
step:538/2330 train_time:32703ms step_avg:60.79ms
step:539/2330 train_time:32761ms step_avg:60.78ms
step:540/2330 train_time:32823ms step_avg:60.78ms
step:541/2330 train_time:32882ms step_avg:60.78ms
step:542/2330 train_time:32944ms step_avg:60.78ms
step:543/2330 train_time:33003ms step_avg:60.78ms
step:544/2330 train_time:33065ms step_avg:60.78ms
step:545/2330 train_time:33124ms step_avg:60.78ms
step:546/2330 train_time:33187ms step_avg:60.78ms
step:547/2330 train_time:33246ms step_avg:60.78ms
step:548/2330 train_time:33310ms step_avg:60.78ms
step:549/2330 train_time:33369ms step_avg:60.78ms
step:550/2330 train_time:33432ms step_avg:60.79ms
step:551/2330 train_time:33491ms step_avg:60.78ms
step:552/2330 train_time:33554ms step_avg:60.79ms
step:553/2330 train_time:33615ms step_avg:60.79ms
step:554/2330 train_time:33678ms step_avg:60.79ms
step:555/2330 train_time:33738ms step_avg:60.79ms
step:556/2330 train_time:33801ms step_avg:60.79ms
step:557/2330 train_time:33860ms step_avg:60.79ms
step:558/2330 train_time:33922ms step_avg:60.79ms
step:559/2330 train_time:33980ms step_avg:60.79ms
step:560/2330 train_time:34043ms step_avg:60.79ms
step:561/2330 train_time:34104ms step_avg:60.79ms
step:562/2330 train_time:34165ms step_avg:60.79ms
step:563/2330 train_time:34224ms step_avg:60.79ms
step:564/2330 train_time:34287ms step_avg:60.79ms
step:565/2330 train_time:34346ms step_avg:60.79ms
step:566/2330 train_time:34409ms step_avg:60.79ms
step:567/2330 train_time:34468ms step_avg:60.79ms
step:568/2330 train_time:34531ms step_avg:60.79ms
step:569/2330 train_time:34590ms step_avg:60.79ms
step:570/2330 train_time:34653ms step_avg:60.79ms
step:571/2330 train_time:34713ms step_avg:60.79ms
step:572/2330 train_time:34776ms step_avg:60.80ms
step:573/2330 train_time:34836ms step_avg:60.80ms
step:574/2330 train_time:34899ms step_avg:60.80ms
step:575/2330 train_time:34958ms step_avg:60.80ms
step:576/2330 train_time:35020ms step_avg:60.80ms
step:577/2330 train_time:35080ms step_avg:60.80ms
step:578/2330 train_time:35141ms step_avg:60.80ms
step:579/2330 train_time:35201ms step_avg:60.80ms
step:580/2330 train_time:35262ms step_avg:60.80ms
step:581/2330 train_time:35321ms step_avg:60.79ms
step:582/2330 train_time:35384ms step_avg:60.80ms
step:583/2330 train_time:35443ms step_avg:60.79ms
step:584/2330 train_time:35506ms step_avg:60.80ms
step:585/2330 train_time:35566ms step_avg:60.80ms
step:586/2330 train_time:35629ms step_avg:60.80ms
step:587/2330 train_time:35688ms step_avg:60.80ms
step:588/2330 train_time:35750ms step_avg:60.80ms
step:589/2330 train_time:35809ms step_avg:60.80ms
step:590/2330 train_time:35872ms step_avg:60.80ms
step:591/2330 train_time:35932ms step_avg:60.80ms
step:592/2330 train_time:35995ms step_avg:60.80ms
step:593/2330 train_time:36055ms step_avg:60.80ms
step:594/2330 train_time:36118ms step_avg:60.80ms
step:595/2330 train_time:36178ms step_avg:60.80ms
step:596/2330 train_time:36241ms step_avg:60.81ms
step:597/2330 train_time:36300ms step_avg:60.80ms
step:598/2330 train_time:36361ms step_avg:60.80ms
step:599/2330 train_time:36420ms step_avg:60.80ms
step:600/2330 train_time:36483ms step_avg:60.81ms
step:601/2330 train_time:36542ms step_avg:60.80ms
step:602/2330 train_time:36605ms step_avg:60.81ms
step:603/2330 train_time:36665ms step_avg:60.80ms
step:604/2330 train_time:36728ms step_avg:60.81ms
step:605/2330 train_time:36787ms step_avg:60.80ms
step:606/2330 train_time:36849ms step_avg:60.81ms
step:607/2330 train_time:36907ms step_avg:60.80ms
step:608/2330 train_time:36970ms step_avg:60.81ms
step:609/2330 train_time:37029ms step_avg:60.80ms
step:610/2330 train_time:37093ms step_avg:60.81ms
step:611/2330 train_time:37152ms step_avg:60.81ms
step:612/2330 train_time:37215ms step_avg:60.81ms
step:613/2330 train_time:37275ms step_avg:60.81ms
step:614/2330 train_time:37337ms step_avg:60.81ms
step:615/2330 train_time:37397ms step_avg:60.81ms
step:616/2330 train_time:37459ms step_avg:60.81ms
step:617/2330 train_time:37519ms step_avg:60.81ms
step:618/2330 train_time:37581ms step_avg:60.81ms
step:619/2330 train_time:37640ms step_avg:60.81ms
step:620/2330 train_time:37702ms step_avg:60.81ms
step:621/2330 train_time:37760ms step_avg:60.81ms
step:622/2330 train_time:37822ms step_avg:60.81ms
step:623/2330 train_time:37882ms step_avg:60.81ms
step:624/2330 train_time:37945ms step_avg:60.81ms
step:625/2330 train_time:38005ms step_avg:60.81ms
step:626/2330 train_time:38068ms step_avg:60.81ms
step:627/2330 train_time:38127ms step_avg:60.81ms
step:628/2330 train_time:38190ms step_avg:60.81ms
step:629/2330 train_time:38249ms step_avg:60.81ms
step:630/2330 train_time:38311ms step_avg:60.81ms
step:631/2330 train_time:38370ms step_avg:60.81ms
step:632/2330 train_time:38434ms step_avg:60.81ms
step:633/2330 train_time:38495ms step_avg:60.81ms
step:634/2330 train_time:38558ms step_avg:60.82ms
step:635/2330 train_time:38617ms step_avg:60.81ms
step:636/2330 train_time:38679ms step_avg:60.82ms
step:637/2330 train_time:38738ms step_avg:60.81ms
step:638/2330 train_time:38800ms step_avg:60.82ms
step:639/2330 train_time:38860ms step_avg:60.81ms
step:640/2330 train_time:38922ms step_avg:60.81ms
step:641/2330 train_time:38982ms step_avg:60.81ms
step:642/2330 train_time:39044ms step_avg:60.82ms
step:643/2330 train_time:39103ms step_avg:60.81ms
step:644/2330 train_time:39165ms step_avg:60.82ms
step:645/2330 train_time:39224ms step_avg:60.81ms
step:646/2330 train_time:39288ms step_avg:60.82ms
step:647/2330 train_time:39347ms step_avg:60.82ms
step:648/2330 train_time:39410ms step_avg:60.82ms
step:649/2330 train_time:39470ms step_avg:60.82ms
step:650/2330 train_time:39532ms step_avg:60.82ms
step:651/2330 train_time:39591ms step_avg:60.82ms
step:652/2330 train_time:39654ms step_avg:60.82ms
step:653/2330 train_time:39714ms step_avg:60.82ms
step:654/2330 train_time:39777ms step_avg:60.82ms
step:655/2330 train_time:39837ms step_avg:60.82ms
step:656/2330 train_time:39901ms step_avg:60.82ms
step:657/2330 train_time:39959ms step_avg:60.82ms
step:658/2330 train_time:40021ms step_avg:60.82ms
step:659/2330 train_time:40080ms step_avg:60.82ms
step:660/2330 train_time:40142ms step_avg:60.82ms
step:661/2330 train_time:40202ms step_avg:60.82ms
step:662/2330 train_time:40264ms step_avg:60.82ms
step:663/2330 train_time:40324ms step_avg:60.82ms
step:664/2330 train_time:40387ms step_avg:60.82ms
step:665/2330 train_time:40446ms step_avg:60.82ms
step:666/2330 train_time:40509ms step_avg:60.82ms
step:667/2330 train_time:40568ms step_avg:60.82ms
step:668/2330 train_time:40630ms step_avg:60.82ms
step:669/2330 train_time:40689ms step_avg:60.82ms
step:670/2330 train_time:40753ms step_avg:60.82ms
step:671/2330 train_time:40812ms step_avg:60.82ms
step:672/2330 train_time:40875ms step_avg:60.83ms
step:673/2330 train_time:40936ms step_avg:60.83ms
step:674/2330 train_time:41000ms step_avg:60.83ms
step:675/2330 train_time:41059ms step_avg:60.83ms
step:676/2330 train_time:41121ms step_avg:60.83ms
step:677/2330 train_time:41180ms step_avg:60.83ms
step:678/2330 train_time:41242ms step_avg:60.83ms
step:679/2330 train_time:41301ms step_avg:60.83ms
step:680/2330 train_time:41363ms step_avg:60.83ms
step:681/2330 train_time:41422ms step_avg:60.83ms
step:682/2330 train_time:41485ms step_avg:60.83ms
step:683/2330 train_time:41544ms step_avg:60.83ms
step:684/2330 train_time:41607ms step_avg:60.83ms
step:685/2330 train_time:41666ms step_avg:60.83ms
step:686/2330 train_time:41729ms step_avg:60.83ms
step:687/2330 train_time:41788ms step_avg:60.83ms
step:688/2330 train_time:41851ms step_avg:60.83ms
step:689/2330 train_time:41910ms step_avg:60.83ms
step:690/2330 train_time:41973ms step_avg:60.83ms
step:691/2330 train_time:42032ms step_avg:60.83ms
step:692/2330 train_time:42096ms step_avg:60.83ms
step:693/2330 train_time:42156ms step_avg:60.83ms
step:694/2330 train_time:42219ms step_avg:60.83ms
step:695/2330 train_time:42278ms step_avg:60.83ms
step:696/2330 train_time:42341ms step_avg:60.84ms
step:697/2330 train_time:42401ms step_avg:60.83ms
step:698/2330 train_time:42464ms step_avg:60.84ms
step:699/2330 train_time:42522ms step_avg:60.83ms
step:700/2330 train_time:42584ms step_avg:60.83ms
step:701/2330 train_time:42643ms step_avg:60.83ms
step:702/2330 train_time:42706ms step_avg:60.84ms
step:703/2330 train_time:42766ms step_avg:60.83ms
step:704/2330 train_time:42829ms step_avg:60.84ms
step:705/2330 train_time:42889ms step_avg:60.84ms
step:706/2330 train_time:42952ms step_avg:60.84ms
step:707/2330 train_time:43011ms step_avg:60.84ms
step:708/2330 train_time:43073ms step_avg:60.84ms
step:709/2330 train_time:43132ms step_avg:60.83ms
step:710/2330 train_time:43196ms step_avg:60.84ms
step:711/2330 train_time:43255ms step_avg:60.84ms
step:712/2330 train_time:43318ms step_avg:60.84ms
step:713/2330 train_time:43378ms step_avg:60.84ms
step:714/2330 train_time:43441ms step_avg:60.84ms
step:715/2330 train_time:43500ms step_avg:60.84ms
step:716/2330 train_time:43562ms step_avg:60.84ms
step:717/2330 train_time:43620ms step_avg:60.84ms
step:718/2330 train_time:43683ms step_avg:60.84ms
step:719/2330 train_time:43742ms step_avg:60.84ms
step:720/2330 train_time:43805ms step_avg:60.84ms
step:721/2330 train_time:43865ms step_avg:60.84ms
step:722/2330 train_time:43928ms step_avg:60.84ms
step:723/2330 train_time:43988ms step_avg:60.84ms
step:724/2330 train_time:44050ms step_avg:60.84ms
step:725/2330 train_time:44109ms step_avg:60.84ms
step:726/2330 train_time:44171ms step_avg:60.84ms
step:727/2330 train_time:44231ms step_avg:60.84ms
step:728/2330 train_time:44294ms step_avg:60.84ms
step:729/2330 train_time:44355ms step_avg:60.84ms
step:730/2330 train_time:44417ms step_avg:60.85ms
step:731/2330 train_time:44478ms step_avg:60.84ms
step:732/2330 train_time:44540ms step_avg:60.85ms
step:733/2330 train_time:44599ms step_avg:60.85ms
step:734/2330 train_time:44661ms step_avg:60.85ms
step:735/2330 train_time:44720ms step_avg:60.84ms
step:736/2330 train_time:44783ms step_avg:60.85ms
step:737/2330 train_time:44842ms step_avg:60.84ms
step:738/2330 train_time:44906ms step_avg:60.85ms
step:739/2330 train_time:44965ms step_avg:60.85ms
step:740/2330 train_time:45028ms step_avg:60.85ms
step:741/2330 train_time:45087ms step_avg:60.85ms
step:742/2330 train_time:45150ms step_avg:60.85ms
step:743/2330 train_time:45208ms step_avg:60.85ms
step:744/2330 train_time:45271ms step_avg:60.85ms
step:745/2330 train_time:45329ms step_avg:60.84ms
step:746/2330 train_time:45392ms step_avg:60.85ms
step:747/2330 train_time:45452ms step_avg:60.85ms
step:748/2330 train_time:45515ms step_avg:60.85ms
step:749/2330 train_time:45575ms step_avg:60.85ms
step:750/2330 train_time:45637ms step_avg:60.85ms
step:750/2330 val_loss:4.1942 train_time:45710ms step_avg:60.95ms
step:751/2330 train_time:45732ms step_avg:60.90ms
step:752/2330 train_time:45761ms step_avg:60.85ms
step:753/2330 train_time:45825ms step_avg:60.86ms
step:754/2330 train_time:45890ms step_avg:60.86ms
step:755/2330 train_time:45951ms step_avg:60.86ms
step:756/2330 train_time:46013ms step_avg:60.86ms
step:757/2330 train_time:46071ms step_avg:60.86ms
step:758/2330 train_time:46133ms step_avg:60.86ms
step:759/2330 train_time:46193ms step_avg:60.86ms
step:760/2330 train_time:46254ms step_avg:60.86ms
step:761/2330 train_time:46313ms step_avg:60.86ms
step:762/2330 train_time:46375ms step_avg:60.86ms
step:763/2330 train_time:46434ms step_avg:60.86ms
step:764/2330 train_time:46495ms step_avg:60.86ms
step:765/2330 train_time:46555ms step_avg:60.86ms
step:766/2330 train_time:46617ms step_avg:60.86ms
step:767/2330 train_time:46677ms step_avg:60.86ms
step:768/2330 train_time:46742ms step_avg:60.86ms
step:769/2330 train_time:46803ms step_avg:60.86ms
step:770/2330 train_time:46868ms step_avg:60.87ms
step:771/2330 train_time:46929ms step_avg:60.87ms
step:772/2330 train_time:46992ms step_avg:60.87ms
step:773/2330 train_time:47052ms step_avg:60.87ms
step:774/2330 train_time:47114ms step_avg:60.87ms
step:775/2330 train_time:47174ms step_avg:60.87ms
step:776/2330 train_time:47237ms step_avg:60.87ms
step:777/2330 train_time:47296ms step_avg:60.87ms
step:778/2330 train_time:47358ms step_avg:60.87ms
step:779/2330 train_time:47419ms step_avg:60.87ms
step:780/2330 train_time:47482ms step_avg:60.87ms
step:781/2330 train_time:47541ms step_avg:60.87ms
step:782/2330 train_time:47605ms step_avg:60.88ms
step:783/2330 train_time:47664ms step_avg:60.87ms
step:784/2330 train_time:47727ms step_avg:60.88ms
step:785/2330 train_time:47788ms step_avg:60.88ms
step:786/2330 train_time:47852ms step_avg:60.88ms
step:787/2330 train_time:47913ms step_avg:60.88ms
step:788/2330 train_time:47975ms step_avg:60.88ms
step:789/2330 train_time:48035ms step_avg:60.88ms
step:790/2330 train_time:48098ms step_avg:60.88ms
step:791/2330 train_time:48158ms step_avg:60.88ms
step:792/2330 train_time:48220ms step_avg:60.88ms
step:793/2330 train_time:48279ms step_avg:60.88ms
step:794/2330 train_time:48343ms step_avg:60.88ms
step:795/2330 train_time:48403ms step_avg:60.88ms
step:796/2330 train_time:48465ms step_avg:60.89ms
step:797/2330 train_time:48524ms step_avg:60.88ms
step:798/2330 train_time:48587ms step_avg:60.89ms
step:799/2330 train_time:48647ms step_avg:60.88ms
step:800/2330 train_time:48710ms step_avg:60.89ms
step:801/2330 train_time:48770ms step_avg:60.89ms
step:802/2330 train_time:48834ms step_avg:60.89ms
step:803/2330 train_time:48894ms step_avg:60.89ms
step:804/2330 train_time:48956ms step_avg:60.89ms
step:805/2330 train_time:49016ms step_avg:60.89ms
step:806/2330 train_time:49079ms step_avg:60.89ms
step:807/2330 train_time:49139ms step_avg:60.89ms
step:808/2330 train_time:49201ms step_avg:60.89ms
step:809/2330 train_time:49261ms step_avg:60.89ms
step:810/2330 train_time:49324ms step_avg:60.89ms
step:811/2330 train_time:49383ms step_avg:60.89ms
step:812/2330 train_time:49446ms step_avg:60.89ms
step:813/2330 train_time:49506ms step_avg:60.89ms
step:814/2330 train_time:49569ms step_avg:60.90ms
step:815/2330 train_time:49628ms step_avg:60.89ms
step:816/2330 train_time:49691ms step_avg:60.90ms
step:817/2330 train_time:49751ms step_avg:60.89ms
step:818/2330 train_time:49814ms step_avg:60.90ms
step:819/2330 train_time:49874ms step_avg:60.90ms
step:820/2330 train_time:49938ms step_avg:60.90ms
step:821/2330 train_time:49998ms step_avg:60.90ms
step:822/2330 train_time:50061ms step_avg:60.90ms
step:823/2330 train_time:50121ms step_avg:60.90ms
step:824/2330 train_time:50183ms step_avg:60.90ms
step:825/2330 train_time:50244ms step_avg:60.90ms
step:826/2330 train_time:50305ms step_avg:60.90ms
step:827/2330 train_time:50365ms step_avg:60.90ms
step:828/2330 train_time:50428ms step_avg:60.90ms
step:829/2330 train_time:50489ms step_avg:60.90ms
step:830/2330 train_time:50552ms step_avg:60.91ms
step:831/2330 train_time:50611ms step_avg:60.90ms
step:832/2330 train_time:50674ms step_avg:60.91ms
step:833/2330 train_time:50734ms step_avg:60.91ms
step:834/2330 train_time:50797ms step_avg:60.91ms
step:835/2330 train_time:50856ms step_avg:60.91ms
step:836/2330 train_time:50920ms step_avg:60.91ms
step:837/2330 train_time:50981ms step_avg:60.91ms
step:838/2330 train_time:51044ms step_avg:60.91ms
step:839/2330 train_time:51104ms step_avg:60.91ms
step:840/2330 train_time:51168ms step_avg:60.91ms
step:841/2330 train_time:51228ms step_avg:60.91ms
step:842/2330 train_time:51291ms step_avg:60.92ms
step:843/2330 train_time:51351ms step_avg:60.91ms
step:844/2330 train_time:51413ms step_avg:60.92ms
step:845/2330 train_time:51473ms step_avg:60.91ms
step:846/2330 train_time:51536ms step_avg:60.92ms
step:847/2330 train_time:51596ms step_avg:60.92ms
step:848/2330 train_time:51658ms step_avg:60.92ms
step:849/2330 train_time:51718ms step_avg:60.92ms
step:850/2330 train_time:51781ms step_avg:60.92ms
step:851/2330 train_time:51841ms step_avg:60.92ms
step:852/2330 train_time:51904ms step_avg:60.92ms
step:853/2330 train_time:51964ms step_avg:60.92ms
step:854/2330 train_time:52027ms step_avg:60.92ms
step:855/2330 train_time:52088ms step_avg:60.92ms
step:856/2330 train_time:52151ms step_avg:60.92ms
step:857/2330 train_time:52211ms step_avg:60.92ms
step:858/2330 train_time:52274ms step_avg:60.93ms
step:859/2330 train_time:52334ms step_avg:60.92ms
step:860/2330 train_time:52397ms step_avg:60.93ms
step:861/2330 train_time:52456ms step_avg:60.92ms
step:862/2330 train_time:52519ms step_avg:60.93ms
step:863/2330 train_time:52579ms step_avg:60.93ms
step:864/2330 train_time:52643ms step_avg:60.93ms
step:865/2330 train_time:52702ms step_avg:60.93ms
step:866/2330 train_time:52765ms step_avg:60.93ms
step:867/2330 train_time:52825ms step_avg:60.93ms
step:868/2330 train_time:52887ms step_avg:60.93ms
step:869/2330 train_time:52948ms step_avg:60.93ms
step:870/2330 train_time:53011ms step_avg:60.93ms
step:871/2330 train_time:53070ms step_avg:60.93ms
step:872/2330 train_time:53134ms step_avg:60.93ms
step:873/2330 train_time:53194ms step_avg:60.93ms
step:874/2330 train_time:53257ms step_avg:60.93ms
step:875/2330 train_time:53317ms step_avg:60.93ms
step:876/2330 train_time:53379ms step_avg:60.94ms
step:877/2330 train_time:53440ms step_avg:60.94ms
step:878/2330 train_time:53503ms step_avg:60.94ms
step:879/2330 train_time:53563ms step_avg:60.94ms
step:880/2330 train_time:53626ms step_avg:60.94ms
step:881/2330 train_time:53686ms step_avg:60.94ms
step:882/2330 train_time:53748ms step_avg:60.94ms
step:883/2330 train_time:53807ms step_avg:60.94ms
step:884/2330 train_time:53870ms step_avg:60.94ms
step:885/2330 train_time:53931ms step_avg:60.94ms
step:886/2330 train_time:53995ms step_avg:60.94ms
step:887/2330 train_time:54054ms step_avg:60.94ms
step:888/2330 train_time:54117ms step_avg:60.94ms
step:889/2330 train_time:54177ms step_avg:60.94ms
step:890/2330 train_time:54241ms step_avg:60.94ms
step:891/2330 train_time:54300ms step_avg:60.94ms
step:892/2330 train_time:54364ms step_avg:60.95ms
step:893/2330 train_time:54423ms step_avg:60.94ms
step:894/2330 train_time:54486ms step_avg:60.95ms
step:895/2330 train_time:54546ms step_avg:60.95ms
step:896/2330 train_time:54609ms step_avg:60.95ms
step:897/2330 train_time:54669ms step_avg:60.95ms
step:898/2330 train_time:54732ms step_avg:60.95ms
step:899/2330 train_time:54791ms step_avg:60.95ms
step:900/2330 train_time:54854ms step_avg:60.95ms
step:901/2330 train_time:54915ms step_avg:60.95ms
step:902/2330 train_time:54977ms step_avg:60.95ms
step:903/2330 train_time:55037ms step_avg:60.95ms
step:904/2330 train_time:55100ms step_avg:60.95ms
step:905/2330 train_time:55160ms step_avg:60.95ms
step:906/2330 train_time:55223ms step_avg:60.95ms
step:907/2330 train_time:55283ms step_avg:60.95ms
step:908/2330 train_time:55347ms step_avg:60.95ms
step:909/2330 train_time:55406ms step_avg:60.95ms
step:910/2330 train_time:55469ms step_avg:60.96ms
step:911/2330 train_time:55530ms step_avg:60.95ms
step:912/2330 train_time:55593ms step_avg:60.96ms
step:913/2330 train_time:55653ms step_avg:60.96ms
step:914/2330 train_time:55716ms step_avg:60.96ms
step:915/2330 train_time:55775ms step_avg:60.96ms
step:916/2330 train_time:55838ms step_avg:60.96ms
step:917/2330 train_time:55898ms step_avg:60.96ms
step:918/2330 train_time:55961ms step_avg:60.96ms
step:919/2330 train_time:56021ms step_avg:60.96ms
step:920/2330 train_time:56083ms step_avg:60.96ms
step:921/2330 train_time:56143ms step_avg:60.96ms
step:922/2330 train_time:56205ms step_avg:60.96ms
step:923/2330 train_time:56265ms step_avg:60.96ms
step:924/2330 train_time:56328ms step_avg:60.96ms
step:925/2330 train_time:56389ms step_avg:60.96ms
step:926/2330 train_time:56452ms step_avg:60.96ms
step:927/2330 train_time:56512ms step_avg:60.96ms
step:928/2330 train_time:56575ms step_avg:60.96ms
step:929/2330 train_time:56635ms step_avg:60.96ms
step:930/2330 train_time:56697ms step_avg:60.96ms
step:931/2330 train_time:56757ms step_avg:60.96ms
step:932/2330 train_time:56820ms step_avg:60.97ms
step:933/2330 train_time:56880ms step_avg:60.96ms
step:934/2330 train_time:56943ms step_avg:60.97ms
step:935/2330 train_time:57002ms step_avg:60.97ms
step:936/2330 train_time:57066ms step_avg:60.97ms
step:937/2330 train_time:57126ms step_avg:60.97ms
step:938/2330 train_time:57188ms step_avg:60.97ms
step:939/2330 train_time:57248ms step_avg:60.97ms
step:940/2330 train_time:57311ms step_avg:60.97ms
step:941/2330 train_time:57370ms step_avg:60.97ms
step:942/2330 train_time:57433ms step_avg:60.97ms
step:943/2330 train_time:57493ms step_avg:60.97ms
step:944/2330 train_time:57556ms step_avg:60.97ms
step:945/2330 train_time:57615ms step_avg:60.97ms
step:946/2330 train_time:57678ms step_avg:60.97ms
step:947/2330 train_time:57738ms step_avg:60.97ms
step:948/2330 train_time:57800ms step_avg:60.97ms
step:949/2330 train_time:57860ms step_avg:60.97ms
step:950/2330 train_time:57923ms step_avg:60.97ms
step:951/2330 train_time:57983ms step_avg:60.97ms
step:952/2330 train_time:58046ms step_avg:60.97ms
step:953/2330 train_time:58105ms step_avg:60.97ms
step:954/2330 train_time:58168ms step_avg:60.97ms
step:955/2330 train_time:58228ms step_avg:60.97ms
step:956/2330 train_time:58292ms step_avg:60.98ms
step:957/2330 train_time:58352ms step_avg:60.97ms
step:958/2330 train_time:58415ms step_avg:60.98ms
step:959/2330 train_time:58475ms step_avg:60.97ms
step:960/2330 train_time:58538ms step_avg:60.98ms
step:961/2330 train_time:58597ms step_avg:60.98ms
step:962/2330 train_time:58659ms step_avg:60.98ms
step:963/2330 train_time:58719ms step_avg:60.97ms
step:964/2330 train_time:58782ms step_avg:60.98ms
step:965/2330 train_time:58842ms step_avg:60.98ms
step:966/2330 train_time:58905ms step_avg:60.98ms
step:967/2330 train_time:58965ms step_avg:60.98ms
step:968/2330 train_time:59027ms step_avg:60.98ms
step:969/2330 train_time:59087ms step_avg:60.98ms
step:970/2330 train_time:59150ms step_avg:60.98ms
step:971/2330 train_time:59209ms step_avg:60.98ms
step:972/2330 train_time:59273ms step_avg:60.98ms
step:973/2330 train_time:59333ms step_avg:60.98ms
step:974/2330 train_time:59396ms step_avg:60.98ms
step:975/2330 train_time:59456ms step_avg:60.98ms
step:976/2330 train_time:59518ms step_avg:60.98ms
step:977/2330 train_time:59578ms step_avg:60.98ms
step:978/2330 train_time:59640ms step_avg:60.98ms
step:979/2330 train_time:59699ms step_avg:60.98ms
step:980/2330 train_time:59762ms step_avg:60.98ms
step:981/2330 train_time:59822ms step_avg:60.98ms
step:982/2330 train_time:59885ms step_avg:60.98ms
step:983/2330 train_time:59945ms step_avg:60.98ms
step:984/2330 train_time:60008ms step_avg:60.98ms
step:985/2330 train_time:60068ms step_avg:60.98ms
step:986/2330 train_time:60131ms step_avg:60.98ms
step:987/2330 train_time:60191ms step_avg:60.98ms
step:988/2330 train_time:60254ms step_avg:60.99ms
step:989/2330 train_time:60314ms step_avg:60.98ms
step:990/2330 train_time:60377ms step_avg:60.99ms
step:991/2330 train_time:60437ms step_avg:60.99ms
step:992/2330 train_time:60499ms step_avg:60.99ms
step:993/2330 train_time:60558ms step_avg:60.99ms
step:994/2330 train_time:60621ms step_avg:60.99ms
step:995/2330 train_time:60681ms step_avg:60.99ms
step:996/2330 train_time:60744ms step_avg:60.99ms
step:997/2330 train_time:60804ms step_avg:60.99ms
step:998/2330 train_time:60867ms step_avg:60.99ms
step:999/2330 train_time:60926ms step_avg:60.99ms
step:1000/2330 train_time:60989ms step_avg:60.99ms
step:1000/2330 val_loss:4.0286 train_time:61063ms step_avg:61.06ms
step:1001/2330 train_time:61085ms step_avg:61.02ms
step:1002/2330 train_time:61114ms step_avg:60.99ms
step:1003/2330 train_time:61175ms step_avg:60.99ms
step:1004/2330 train_time:61242ms step_avg:61.00ms
step:1005/2330 train_time:61303ms step_avg:61.00ms
step:1006/2330 train_time:61366ms step_avg:61.00ms
step:1007/2330 train_time:61425ms step_avg:61.00ms
step:1008/2330 train_time:61488ms step_avg:61.00ms
step:1009/2330 train_time:61548ms step_avg:61.00ms
step:1010/2330 train_time:61610ms step_avg:61.00ms
step:1011/2330 train_time:61669ms step_avg:61.00ms
step:1012/2330 train_time:61731ms step_avg:61.00ms
step:1013/2330 train_time:61791ms step_avg:61.00ms
step:1014/2330 train_time:61853ms step_avg:61.00ms
step:1015/2330 train_time:61912ms step_avg:61.00ms
step:1016/2330 train_time:61977ms step_avg:61.00ms
step:1017/2330 train_time:62038ms step_avg:61.00ms
step:1018/2330 train_time:62100ms step_avg:61.00ms
step:1019/2330 train_time:62162ms step_avg:61.00ms
step:1020/2330 train_time:62227ms step_avg:61.01ms
step:1021/2330 train_time:62287ms step_avg:61.01ms
step:1022/2330 train_time:62350ms step_avg:61.01ms
step:1023/2330 train_time:62410ms step_avg:61.01ms
step:1024/2330 train_time:62473ms step_avg:61.01ms
step:1025/2330 train_time:62532ms step_avg:61.01ms
step:1026/2330 train_time:62594ms step_avg:61.01ms
step:1027/2330 train_time:62653ms step_avg:61.01ms
step:1028/2330 train_time:62715ms step_avg:61.01ms
step:1029/2330 train_time:62774ms step_avg:61.00ms
step:1030/2330 train_time:62835ms step_avg:61.01ms
step:1031/2330 train_time:62895ms step_avg:61.00ms
step:1032/2330 train_time:62959ms step_avg:61.01ms
step:1033/2330 train_time:63019ms step_avg:61.01ms
step:1034/2330 train_time:63082ms step_avg:61.01ms
step:1035/2330 train_time:63143ms step_avg:61.01ms
step:1036/2330 train_time:63207ms step_avg:61.01ms
step:1037/2330 train_time:63268ms step_avg:61.01ms
step:1038/2330 train_time:63332ms step_avg:61.01ms
step:1039/2330 train_time:63392ms step_avg:61.01ms
step:1040/2330 train_time:63455ms step_avg:61.01ms
step:1041/2330 train_time:63517ms step_avg:61.01ms
step:1042/2330 train_time:63578ms step_avg:61.02ms
step:1043/2330 train_time:63637ms step_avg:61.01ms
step:1044/2330 train_time:63700ms step_avg:61.02ms
step:1045/2330 train_time:63760ms step_avg:61.01ms
step:1046/2330 train_time:63823ms step_avg:61.02ms
step:1047/2330 train_time:63883ms step_avg:61.02ms
step:1048/2330 train_time:63945ms step_avg:61.02ms
step:1049/2330 train_time:64005ms step_avg:61.02ms
step:1050/2330 train_time:64067ms step_avg:61.02ms
step:1051/2330 train_time:64127ms step_avg:61.02ms
step:1052/2330 train_time:64191ms step_avg:61.02ms
step:1053/2330 train_time:64252ms step_avg:61.02ms
step:1054/2330 train_time:64316ms step_avg:61.02ms
step:1055/2330 train_time:64375ms step_avg:61.02ms
step:1056/2330 train_time:64438ms step_avg:61.02ms
step:1057/2330 train_time:64498ms step_avg:61.02ms
step:1058/2330 train_time:64561ms step_avg:61.02ms
step:1059/2330 train_time:64621ms step_avg:61.02ms
step:1060/2330 train_time:64684ms step_avg:61.02ms
step:1061/2330 train_time:64743ms step_avg:61.02ms
step:1062/2330 train_time:64807ms step_avg:61.02ms
step:1063/2330 train_time:64865ms step_avg:61.02ms
step:1064/2330 train_time:64928ms step_avg:61.02ms
step:1065/2330 train_time:64988ms step_avg:61.02ms
step:1066/2330 train_time:65051ms step_avg:61.02ms
step:1067/2330 train_time:65111ms step_avg:61.02ms
step:1068/2330 train_time:65175ms step_avg:61.03ms
step:1069/2330 train_time:65235ms step_avg:61.02ms
step:1070/2330 train_time:65298ms step_avg:61.03ms
step:1071/2330 train_time:65358ms step_avg:61.02ms
step:1072/2330 train_time:65420ms step_avg:61.03ms
step:1073/2330 train_time:65480ms step_avg:61.03ms
step:1074/2330 train_time:65544ms step_avg:61.03ms
step:1075/2330 train_time:65603ms step_avg:61.03ms
step:1076/2330 train_time:65666ms step_avg:61.03ms
step:1077/2330 train_time:65726ms step_avg:61.03ms
step:1078/2330 train_time:65788ms step_avg:61.03ms
step:1079/2330 train_time:65847ms step_avg:61.03ms
step:1080/2330 train_time:65910ms step_avg:61.03ms
step:1081/2330 train_time:65970ms step_avg:61.03ms
step:1082/2330 train_time:66033ms step_avg:61.03ms
step:1083/2330 train_time:66093ms step_avg:61.03ms
step:1084/2330 train_time:66157ms step_avg:61.03ms
step:1085/2330 train_time:66217ms step_avg:61.03ms
step:1086/2330 train_time:66280ms step_avg:61.03ms
step:1087/2330 train_time:66339ms step_avg:61.03ms
step:1088/2330 train_time:66401ms step_avg:61.03ms
step:1089/2330 train_time:66461ms step_avg:61.03ms
step:1090/2330 train_time:66525ms step_avg:61.03ms
step:1091/2330 train_time:66584ms step_avg:61.03ms
step:1092/2330 train_time:66648ms step_avg:61.03ms
step:1093/2330 train_time:66708ms step_avg:61.03ms
step:1094/2330 train_time:66771ms step_avg:61.03ms
step:1095/2330 train_time:66830ms step_avg:61.03ms
step:1096/2330 train_time:66893ms step_avg:61.03ms
step:1097/2330 train_time:66953ms step_avg:61.03ms
step:1098/2330 train_time:67016ms step_avg:61.03ms
step:1099/2330 train_time:67075ms step_avg:61.03ms
step:1100/2330 train_time:67138ms step_avg:61.03ms
step:1101/2330 train_time:67198ms step_avg:61.03ms
step:1102/2330 train_time:67262ms step_avg:61.04ms
step:1103/2330 train_time:67322ms step_avg:61.04ms
step:1104/2330 train_time:67384ms step_avg:61.04ms
step:1105/2330 train_time:67443ms step_avg:61.03ms
step:1106/2330 train_time:67506ms step_avg:61.04ms
step:1107/2330 train_time:67566ms step_avg:61.04ms
step:1108/2330 train_time:67629ms step_avg:61.04ms
step:1109/2330 train_time:67689ms step_avg:61.04ms
step:1110/2330 train_time:67752ms step_avg:61.04ms
step:1111/2330 train_time:67813ms step_avg:61.04ms
step:1112/2330 train_time:67874ms step_avg:61.04ms
step:1113/2330 train_time:67934ms step_avg:61.04ms
step:1114/2330 train_time:67996ms step_avg:61.04ms
step:1115/2330 train_time:68057ms step_avg:61.04ms
step:1116/2330 train_time:68119ms step_avg:61.04ms
step:1117/2330 train_time:68178ms step_avg:61.04ms
step:1118/2330 train_time:68242ms step_avg:61.04ms
step:1119/2330 train_time:68302ms step_avg:61.04ms
step:1120/2330 train_time:68365ms step_avg:61.04ms
step:1121/2330 train_time:68425ms step_avg:61.04ms
step:1122/2330 train_time:68488ms step_avg:61.04ms
step:1123/2330 train_time:68548ms step_avg:61.04ms
step:1124/2330 train_time:68611ms step_avg:61.04ms
step:1125/2330 train_time:68671ms step_avg:61.04ms
step:1126/2330 train_time:68734ms step_avg:61.04ms
step:1127/2330 train_time:68794ms step_avg:61.04ms
step:1128/2330 train_time:68857ms step_avg:61.04ms
step:1129/2330 train_time:68917ms step_avg:61.04ms
step:1130/2330 train_time:68979ms step_avg:61.04ms
step:1131/2330 train_time:69039ms step_avg:61.04ms
step:1132/2330 train_time:69102ms step_avg:61.04ms
step:1133/2330 train_time:69162ms step_avg:61.04ms
step:1134/2330 train_time:69226ms step_avg:61.05ms
step:1135/2330 train_time:69286ms step_avg:61.04ms
step:1136/2330 train_time:69349ms step_avg:61.05ms
step:1137/2330 train_time:69409ms step_avg:61.05ms
step:1138/2330 train_time:69471ms step_avg:61.05ms
step:1139/2330 train_time:69531ms step_avg:61.05ms
step:1140/2330 train_time:69594ms step_avg:61.05ms
step:1141/2330 train_time:69653ms step_avg:61.05ms
step:1142/2330 train_time:69716ms step_avg:61.05ms
step:1143/2330 train_time:69775ms step_avg:61.05ms
step:1144/2330 train_time:69838ms step_avg:61.05ms
step:1145/2330 train_time:69898ms step_avg:61.05ms
step:1146/2330 train_time:69960ms step_avg:61.05ms
step:1147/2330 train_time:70020ms step_avg:61.05ms
step:1148/2330 train_time:70083ms step_avg:61.05ms
step:1149/2330 train_time:70143ms step_avg:61.05ms
step:1150/2330 train_time:70206ms step_avg:61.05ms
step:1151/2330 train_time:70266ms step_avg:61.05ms
step:1152/2330 train_time:70329ms step_avg:61.05ms
step:1153/2330 train_time:70390ms step_avg:61.05ms
step:1154/2330 train_time:70452ms step_avg:61.05ms
step:1155/2330 train_time:70513ms step_avg:61.05ms
step:1156/2330 train_time:70575ms step_avg:61.05ms
step:1157/2330 train_time:70635ms step_avg:61.05ms
step:1158/2330 train_time:70698ms step_avg:61.05ms
step:1159/2330 train_time:70758ms step_avg:61.05ms
step:1160/2330 train_time:70821ms step_avg:61.05ms
step:1161/2330 train_time:70880ms step_avg:61.05ms
step:1162/2330 train_time:70944ms step_avg:61.05ms
step:1163/2330 train_time:71003ms step_avg:61.05ms
step:1164/2330 train_time:71067ms step_avg:61.05ms
step:1165/2330 train_time:71127ms step_avg:61.05ms
step:1166/2330 train_time:71189ms step_avg:61.05ms
step:1167/2330 train_time:71250ms step_avg:61.05ms
step:1168/2330 train_time:71313ms step_avg:61.06ms
step:1169/2330 train_time:71373ms step_avg:61.05ms
step:1170/2330 train_time:71436ms step_avg:61.06ms
step:1171/2330 train_time:71496ms step_avg:61.06ms
step:1172/2330 train_time:71559ms step_avg:61.06ms
step:1173/2330 train_time:71619ms step_avg:61.06ms
step:1174/2330 train_time:71681ms step_avg:61.06ms
step:1175/2330 train_time:71741ms step_avg:61.06ms
step:1176/2330 train_time:71805ms step_avg:61.06ms
step:1177/2330 train_time:71864ms step_avg:61.06ms
step:1178/2330 train_time:71927ms step_avg:61.06ms
step:1179/2330 train_time:71987ms step_avg:61.06ms
step:1180/2330 train_time:72049ms step_avg:61.06ms
step:1181/2330 train_time:72109ms step_avg:61.06ms
step:1182/2330 train_time:72173ms step_avg:61.06ms
step:1183/2330 train_time:72233ms step_avg:61.06ms
step:1184/2330 train_time:72296ms step_avg:61.06ms
step:1185/2330 train_time:72356ms step_avg:61.06ms
step:1186/2330 train_time:72419ms step_avg:61.06ms
step:1187/2330 train_time:72478ms step_avg:61.06ms
step:1188/2330 train_time:72542ms step_avg:61.06ms
step:1189/2330 train_time:72601ms step_avg:61.06ms
step:1190/2330 train_time:72664ms step_avg:61.06ms
step:1191/2330 train_time:72725ms step_avg:61.06ms
step:1192/2330 train_time:72788ms step_avg:61.06ms
step:1193/2330 train_time:72848ms step_avg:61.06ms
step:1194/2330 train_time:72911ms step_avg:61.06ms
step:1195/2330 train_time:72970ms step_avg:61.06ms
step:1196/2330 train_time:73033ms step_avg:61.06ms
step:1197/2330 train_time:73092ms step_avg:61.06ms
step:1198/2330 train_time:73156ms step_avg:61.07ms
step:1199/2330 train_time:73217ms step_avg:61.06ms
step:1200/2330 train_time:73279ms step_avg:61.07ms
step:1201/2330 train_time:73339ms step_avg:61.06ms
step:1202/2330 train_time:73402ms step_avg:61.07ms
step:1203/2330 train_time:73462ms step_avg:61.07ms
step:1204/2330 train_time:73524ms step_avg:61.07ms
step:1205/2330 train_time:73584ms step_avg:61.07ms
step:1206/2330 train_time:73647ms step_avg:61.07ms
step:1207/2330 train_time:73706ms step_avg:61.07ms
step:1208/2330 train_time:73769ms step_avg:61.07ms
step:1209/2330 train_time:73830ms step_avg:61.07ms
step:1210/2330 train_time:73893ms step_avg:61.07ms
step:1211/2330 train_time:73952ms step_avg:61.07ms
step:1212/2330 train_time:74015ms step_avg:61.07ms
step:1213/2330 train_time:74075ms step_avg:61.07ms
step:1214/2330 train_time:74138ms step_avg:61.07ms
step:1215/2330 train_time:74197ms step_avg:61.07ms
step:1216/2330 train_time:74260ms step_avg:61.07ms
step:1217/2330 train_time:74321ms step_avg:61.07ms
step:1218/2330 train_time:74383ms step_avg:61.07ms
step:1219/2330 train_time:74443ms step_avg:61.07ms
step:1220/2330 train_time:74506ms step_avg:61.07ms
step:1221/2330 train_time:74565ms step_avg:61.07ms
step:1222/2330 train_time:74628ms step_avg:61.07ms
step:1223/2330 train_time:74688ms step_avg:61.07ms
step:1224/2330 train_time:74752ms step_avg:61.07ms
step:1225/2330 train_time:74811ms step_avg:61.07ms
step:1226/2330 train_time:74874ms step_avg:61.07ms
step:1227/2330 train_time:74934ms step_avg:61.07ms
step:1228/2330 train_time:74997ms step_avg:61.07ms
step:1229/2330 train_time:75058ms step_avg:61.07ms
step:1230/2330 train_time:75121ms step_avg:61.07ms
step:1231/2330 train_time:75180ms step_avg:61.07ms
step:1232/2330 train_time:75243ms step_avg:61.07ms
step:1233/2330 train_time:75302ms step_avg:61.07ms
step:1234/2330 train_time:75366ms step_avg:61.07ms
step:1235/2330 train_time:75425ms step_avg:61.07ms
step:1236/2330 train_time:75489ms step_avg:61.07ms
step:1237/2330 train_time:75548ms step_avg:61.07ms
step:1238/2330 train_time:75611ms step_avg:61.08ms
step:1239/2330 train_time:75671ms step_avg:61.07ms
step:1240/2330 train_time:75734ms step_avg:61.08ms
step:1241/2330 train_time:75794ms step_avg:61.07ms
step:1242/2330 train_time:75857ms step_avg:61.08ms
step:1243/2330 train_time:75917ms step_avg:61.08ms
step:1244/2330 train_time:75979ms step_avg:61.08ms
step:1245/2330 train_time:76039ms step_avg:61.08ms
step:1246/2330 train_time:76101ms step_avg:61.08ms
step:1247/2330 train_time:76162ms step_avg:61.08ms
step:1248/2330 train_time:76225ms step_avg:61.08ms
step:1249/2330 train_time:76284ms step_avg:61.08ms
step:1250/2330 train_time:76348ms step_avg:61.08ms
step:1250/2330 val_loss:4.0381 train_time:76421ms step_avg:61.14ms
step:1251/2330 train_time:76442ms step_avg:61.10ms
step:1252/2330 train_time:76474ms step_avg:61.08ms
step:1253/2330 train_time:76541ms step_avg:61.09ms
step:1254/2330 train_time:76604ms step_avg:61.09ms
step:1255/2330 train_time:76667ms step_avg:61.09ms
step:1256/2330 train_time:76729ms step_avg:61.09ms
step:1257/2330 train_time:76789ms step_avg:61.09ms
step:1258/2330 train_time:76852ms step_avg:61.09ms
step:1259/2330 train_time:76911ms step_avg:61.09ms
step:1260/2330 train_time:76973ms step_avg:61.09ms
step:1261/2330 train_time:77033ms step_avg:61.09ms
step:1262/2330 train_time:77095ms step_avg:61.09ms
step:1263/2330 train_time:77154ms step_avg:61.09ms
step:1264/2330 train_time:77216ms step_avg:61.09ms
step:1265/2330 train_time:77275ms step_avg:61.09ms
step:1266/2330 train_time:77339ms step_avg:61.09ms
step:1267/2330 train_time:77400ms step_avg:61.09ms
step:1268/2330 train_time:77464ms step_avg:61.09ms
step:1269/2330 train_time:77525ms step_avg:61.09ms
step:1270/2330 train_time:77591ms step_avg:61.10ms
step:1271/2330 train_time:77653ms step_avg:61.10ms
step:1272/2330 train_time:77716ms step_avg:61.10ms
step:1273/2330 train_time:77776ms step_avg:61.10ms
step:1274/2330 train_time:77838ms step_avg:61.10ms
step:1275/2330 train_time:77897ms step_avg:61.10ms
step:1276/2330 train_time:77959ms step_avg:61.10ms
step:1277/2330 train_time:78019ms step_avg:61.10ms
step:1278/2330 train_time:78082ms step_avg:61.10ms
step:1279/2330 train_time:78141ms step_avg:61.10ms
step:1280/2330 train_time:78203ms step_avg:61.10ms
step:1281/2330 train_time:78263ms step_avg:61.10ms
step:1282/2330 train_time:78326ms step_avg:61.10ms
step:1283/2330 train_time:78386ms step_avg:61.10ms
step:1284/2330 train_time:78449ms step_avg:61.10ms
step:1285/2330 train_time:78509ms step_avg:61.10ms
step:1286/2330 train_time:78573ms step_avg:61.10ms
step:1287/2330 train_time:78634ms step_avg:61.10ms
step:1288/2330 train_time:78698ms step_avg:61.10ms
step:1289/2330 train_time:78758ms step_avg:61.10ms
step:1290/2330 train_time:78820ms step_avg:61.10ms
step:1291/2330 train_time:78880ms step_avg:61.10ms
step:1292/2330 train_time:78943ms step_avg:61.10ms
step:1293/2330 train_time:79001ms step_avg:61.10ms
step:1294/2330 train_time:79064ms step_avg:61.10ms
step:1295/2330 train_time:79124ms step_avg:61.10ms
step:1296/2330 train_time:79186ms step_avg:61.10ms
step:1297/2330 train_time:79246ms step_avg:61.10ms
step:1298/2330 train_time:79308ms step_avg:61.10ms
step:1299/2330 train_time:79368ms step_avg:61.10ms
step:1300/2330 train_time:79431ms step_avg:61.10ms
step:1301/2330 train_time:79492ms step_avg:61.10ms
step:1302/2330 train_time:79555ms step_avg:61.10ms
step:1303/2330 train_time:79616ms step_avg:61.10ms
step:1304/2330 train_time:79678ms step_avg:61.10ms
step:1305/2330 train_time:79739ms step_avg:61.10ms
step:1306/2330 train_time:79802ms step_avg:61.10ms
step:1307/2330 train_time:79862ms step_avg:61.10ms
step:1308/2330 train_time:79924ms step_avg:61.10ms
step:1309/2330 train_time:79984ms step_avg:61.10ms
step:1310/2330 train_time:80048ms step_avg:61.10ms
step:1311/2330 train_time:80106ms step_avg:61.10ms
step:1312/2330 train_time:80170ms step_avg:61.11ms
step:1313/2330 train_time:80229ms step_avg:61.10ms
step:1314/2330 train_time:80292ms step_avg:61.10ms
step:1315/2330 train_time:80351ms step_avg:61.10ms
step:1316/2330 train_time:80413ms step_avg:61.10ms
step:1317/2330 train_time:80473ms step_avg:61.10ms
step:1318/2330 train_time:80536ms step_avg:61.10ms
step:1319/2330 train_time:80597ms step_avg:61.10ms
step:1320/2330 train_time:80659ms step_avg:61.11ms
step:1321/2330 train_time:80719ms step_avg:61.10ms
step:1322/2330 train_time:80782ms step_avg:61.11ms
step:1323/2330 train_time:80842ms step_avg:61.11ms
step:1324/2330 train_time:80905ms step_avg:61.11ms
step:1325/2330 train_time:80965ms step_avg:61.11ms
step:1326/2330 train_time:81027ms step_avg:61.11ms
step:1327/2330 train_time:81087ms step_avg:61.11ms
step:1328/2330 train_time:81151ms step_avg:61.11ms
step:1329/2330 train_time:81209ms step_avg:61.11ms
step:1330/2330 train_time:81271ms step_avg:61.11ms
step:1331/2330 train_time:81331ms step_avg:61.11ms
step:1332/2330 train_time:81394ms step_avg:61.11ms
step:1333/2330 train_time:81454ms step_avg:61.11ms
step:1334/2330 train_time:81517ms step_avg:61.11ms
step:1335/2330 train_time:81578ms step_avg:61.11ms
step:1336/2330 train_time:81641ms step_avg:61.11ms
step:1337/2330 train_time:81700ms step_avg:61.11ms
step:1338/2330 train_time:81763ms step_avg:61.11ms
step:1339/2330 train_time:81824ms step_avg:61.11ms
step:1340/2330 train_time:81886ms step_avg:61.11ms
step:1341/2330 train_time:81947ms step_avg:61.11ms
step:1342/2330 train_time:82009ms step_avg:61.11ms
step:1343/2330 train_time:82069ms step_avg:61.11ms
step:1344/2330 train_time:82132ms step_avg:61.11ms
step:1345/2330 train_time:82191ms step_avg:61.11ms
step:1346/2330 train_time:82255ms step_avg:61.11ms
step:1347/2330 train_time:82315ms step_avg:61.11ms
step:1348/2330 train_time:82378ms step_avg:61.11ms
step:1349/2330 train_time:82439ms step_avg:61.11ms
step:1350/2330 train_time:82501ms step_avg:61.11ms
step:1351/2330 train_time:82561ms step_avg:61.11ms
step:1352/2330 train_time:82623ms step_avg:61.11ms
step:1353/2330 train_time:82683ms step_avg:61.11ms
step:1354/2330 train_time:82747ms step_avg:61.11ms
step:1355/2330 train_time:82807ms step_avg:61.11ms
step:1356/2330 train_time:82870ms step_avg:61.11ms
step:1357/2330 train_time:82930ms step_avg:61.11ms
step:1358/2330 train_time:82993ms step_avg:61.11ms
step:1359/2330 train_time:83054ms step_avg:61.11ms
step:1360/2330 train_time:83116ms step_avg:61.11ms
step:1361/2330 train_time:83175ms step_avg:61.11ms
step:1362/2330 train_time:83239ms step_avg:61.11ms
step:1363/2330 train_time:83297ms step_avg:61.11ms
step:1364/2330 train_time:83361ms step_avg:61.11ms
step:1365/2330 train_time:83421ms step_avg:61.11ms
step:1366/2330 train_time:83483ms step_avg:61.12ms
step:1367/2330 train_time:83543ms step_avg:61.11ms
step:1368/2330 train_time:83606ms step_avg:61.12ms
step:1369/2330 train_time:83666ms step_avg:61.11ms
step:1370/2330 train_time:83729ms step_avg:61.12ms
step:1371/2330 train_time:83788ms step_avg:61.11ms
step:1372/2330 train_time:83852ms step_avg:61.12ms
step:1373/2330 train_time:83912ms step_avg:61.12ms
step:1374/2330 train_time:83976ms step_avg:61.12ms
step:1375/2330 train_time:84036ms step_avg:61.12ms
step:1376/2330 train_time:84099ms step_avg:61.12ms
step:1377/2330 train_time:84159ms step_avg:61.12ms
step:1378/2330 train_time:84222ms step_avg:61.12ms
step:1379/2330 train_time:84281ms step_avg:61.12ms
step:1380/2330 train_time:84344ms step_avg:61.12ms
step:1381/2330 train_time:84403ms step_avg:61.12ms
step:1382/2330 train_time:84467ms step_avg:61.12ms
step:1383/2330 train_time:84526ms step_avg:61.12ms
step:1384/2330 train_time:84589ms step_avg:61.12ms
step:1385/2330 train_time:84649ms step_avg:61.12ms
step:1386/2330 train_time:84712ms step_avg:61.12ms
step:1387/2330 train_time:84772ms step_avg:61.12ms
step:1388/2330 train_time:84835ms step_avg:61.12ms
step:1389/2330 train_time:84895ms step_avg:61.12ms
step:1390/2330 train_time:84958ms step_avg:61.12ms
step:1391/2330 train_time:85019ms step_avg:61.12ms
step:1392/2330 train_time:85081ms step_avg:61.12ms
step:1393/2330 train_time:85142ms step_avg:61.12ms
step:1394/2330 train_time:85204ms step_avg:61.12ms
step:1395/2330 train_time:85263ms step_avg:61.12ms
step:1396/2330 train_time:85326ms step_avg:61.12ms
step:1397/2330 train_time:85386ms step_avg:61.12ms
step:1398/2330 train_time:85449ms step_avg:61.12ms
step:1399/2330 train_time:85509ms step_avg:61.12ms
step:1400/2330 train_time:85572ms step_avg:61.12ms
step:1401/2330 train_time:85632ms step_avg:61.12ms
step:1402/2330 train_time:85695ms step_avg:61.12ms
step:1403/2330 train_time:85756ms step_avg:61.12ms
step:1404/2330 train_time:85819ms step_avg:61.12ms
step:1405/2330 train_time:85878ms step_avg:61.12ms
step:1406/2330 train_time:85942ms step_avg:61.13ms
step:1407/2330 train_time:86001ms step_avg:61.12ms
step:1408/2330 train_time:86064ms step_avg:61.12ms
step:1409/2330 train_time:86124ms step_avg:61.12ms
step:1410/2330 train_time:86187ms step_avg:61.13ms
step:1411/2330 train_time:86247ms step_avg:61.13ms
step:1412/2330 train_time:86310ms step_avg:61.13ms
step:1413/2330 train_time:86370ms step_avg:61.13ms
step:1414/2330 train_time:86432ms step_avg:61.13ms
step:1415/2330 train_time:86493ms step_avg:61.13ms
step:1416/2330 train_time:86556ms step_avg:61.13ms
step:1417/2330 train_time:86616ms step_avg:61.13ms
step:1418/2330 train_time:86680ms step_avg:61.13ms
step:1419/2330 train_time:86739ms step_avg:61.13ms
step:1420/2330 train_time:86802ms step_avg:61.13ms
step:1421/2330 train_time:86862ms step_avg:61.13ms
step:1422/2330 train_time:86926ms step_avg:61.13ms
step:1423/2330 train_time:86986ms step_avg:61.13ms
step:1424/2330 train_time:87049ms step_avg:61.13ms
step:1425/2330 train_time:87108ms step_avg:61.13ms
step:1426/2330 train_time:87170ms step_avg:61.13ms
step:1427/2330 train_time:87230ms step_avg:61.13ms
step:1428/2330 train_time:87293ms step_avg:61.13ms
step:1429/2330 train_time:87353ms step_avg:61.13ms
step:1430/2330 train_time:87415ms step_avg:61.13ms
step:1431/2330 train_time:87474ms step_avg:61.13ms
step:1432/2330 train_time:87538ms step_avg:61.13ms
step:1433/2330 train_time:87598ms step_avg:61.13ms
step:1434/2330 train_time:87661ms step_avg:61.13ms
step:1435/2330 train_time:87721ms step_avg:61.13ms
step:1436/2330 train_time:87783ms step_avg:61.13ms
step:1437/2330 train_time:87844ms step_avg:61.13ms
step:1438/2330 train_time:87906ms step_avg:61.13ms
step:1439/2330 train_time:87966ms step_avg:61.13ms
step:1440/2330 train_time:88030ms step_avg:61.13ms
step:1441/2330 train_time:88089ms step_avg:61.13ms
step:1442/2330 train_time:88153ms step_avg:61.13ms
step:1443/2330 train_time:88213ms step_avg:61.13ms
step:1444/2330 train_time:88276ms step_avg:61.13ms
step:1445/2330 train_time:88336ms step_avg:61.13ms
step:1446/2330 train_time:88398ms step_avg:61.13ms
step:1447/2330 train_time:88458ms step_avg:61.13ms
step:1448/2330 train_time:88521ms step_avg:61.13ms
step:1449/2330 train_time:88581ms step_avg:61.13ms
step:1450/2330 train_time:88644ms step_avg:61.13ms
step:1451/2330 train_time:88702ms step_avg:61.13ms
step:1452/2330 train_time:88767ms step_avg:61.13ms
step:1453/2330 train_time:88827ms step_avg:61.13ms
step:1454/2330 train_time:88890ms step_avg:61.13ms
step:1455/2330 train_time:88950ms step_avg:61.13ms
step:1456/2330 train_time:89012ms step_avg:61.13ms
step:1457/2330 train_time:89072ms step_avg:61.13ms
step:1458/2330 train_time:89135ms step_avg:61.14ms
step:1459/2330 train_time:89196ms step_avg:61.13ms
step:1460/2330 train_time:89259ms step_avg:61.14ms
step:1461/2330 train_time:89318ms step_avg:61.14ms
step:1462/2330 train_time:89381ms step_avg:61.14ms
step:1463/2330 train_time:89440ms step_avg:61.13ms
step:1464/2330 train_time:89502ms step_avg:61.14ms
step:1465/2330 train_time:89562ms step_avg:61.13ms
step:1466/2330 train_time:89626ms step_avg:61.14ms
step:1467/2330 train_time:89685ms step_avg:61.14ms
step:1468/2330 train_time:89749ms step_avg:61.14ms
step:1469/2330 train_time:89808ms step_avg:61.14ms
step:1470/2330 train_time:89871ms step_avg:61.14ms
step:1471/2330 train_time:89931ms step_avg:61.14ms
step:1472/2330 train_time:89994ms step_avg:61.14ms
step:1473/2330 train_time:90054ms step_avg:61.14ms
step:1474/2330 train_time:90117ms step_avg:61.14ms
step:1475/2330 train_time:90179ms step_avg:61.14ms
step:1476/2330 train_time:90241ms step_avg:61.14ms
step:1477/2330 train_time:90300ms step_avg:61.14ms
step:1478/2330 train_time:90363ms step_avg:61.14ms
step:1479/2330 train_time:90422ms step_avg:61.14ms
step:1480/2330 train_time:90485ms step_avg:61.14ms
step:1481/2330 train_time:90545ms step_avg:61.14ms
step:1482/2330 train_time:90608ms step_avg:61.14ms
step:1483/2330 train_time:90668ms step_avg:61.14ms
step:1484/2330 train_time:90731ms step_avg:61.14ms
step:1485/2330 train_time:90791ms step_avg:61.14ms
step:1486/2330 train_time:90854ms step_avg:61.14ms
step:1487/2330 train_time:90914ms step_avg:61.14ms
step:1488/2330 train_time:90977ms step_avg:61.14ms
step:1489/2330 train_time:91038ms step_avg:61.14ms
step:1490/2330 train_time:91100ms step_avg:61.14ms
step:1491/2330 train_time:91160ms step_avg:61.14ms
step:1492/2330 train_time:91222ms step_avg:61.14ms
step:1493/2330 train_time:91282ms step_avg:61.14ms
step:1494/2330 train_time:91346ms step_avg:61.14ms
step:1495/2330 train_time:91405ms step_avg:61.14ms
step:1496/2330 train_time:91468ms step_avg:61.14ms
step:1497/2330 train_time:91528ms step_avg:61.14ms
step:1498/2330 train_time:91591ms step_avg:61.14ms
step:1499/2330 train_time:91651ms step_avg:61.14ms
step:1500/2330 train_time:91714ms step_avg:61.14ms
step:1500/2330 val_loss:3.8277 train_time:91786ms step_avg:61.19ms
step:1501/2330 train_time:91808ms step_avg:61.16ms
step:1502/2330 train_time:91838ms step_avg:61.14ms
step:1503/2330 train_time:91904ms step_avg:61.15ms
step:1504/2330 train_time:91971ms step_avg:61.15ms
step:1505/2330 train_time:92034ms step_avg:61.15ms
step:1506/2330 train_time:92098ms step_avg:61.15ms
step:1507/2330 train_time:92158ms step_avg:61.15ms
step:1508/2330 train_time:92220ms step_avg:61.15ms
step:1509/2330 train_time:92278ms step_avg:61.15ms
step:1510/2330 train_time:92341ms step_avg:61.15ms
step:1511/2330 train_time:92400ms step_avg:61.15ms
step:1512/2330 train_time:92463ms step_avg:61.15ms
step:1513/2330 train_time:92521ms step_avg:61.15ms
step:1514/2330 train_time:92582ms step_avg:61.15ms
step:1515/2330 train_time:92641ms step_avg:61.15ms
step:1516/2330 train_time:92704ms step_avg:61.15ms
step:1517/2330 train_time:92765ms step_avg:61.15ms
step:1518/2330 train_time:92831ms step_avg:61.15ms
step:1519/2330 train_time:92894ms step_avg:61.15ms
step:1520/2330 train_time:92959ms step_avg:61.16ms
step:1521/2330 train_time:93020ms step_avg:61.16ms
step:1522/2330 train_time:93083ms step_avg:61.16ms
step:1523/2330 train_time:93143ms step_avg:61.16ms
step:1524/2330 train_time:93206ms step_avg:61.16ms
step:1525/2330 train_time:93267ms step_avg:61.16ms
step:1526/2330 train_time:93330ms step_avg:61.16ms
step:1527/2330 train_time:93391ms step_avg:61.16ms
step:1528/2330 train_time:93454ms step_avg:61.16ms
step:1529/2330 train_time:93514ms step_avg:61.16ms
step:1530/2330 train_time:93576ms step_avg:61.16ms
step:1531/2330 train_time:93636ms step_avg:61.16ms
step:1532/2330 train_time:93699ms step_avg:61.16ms
step:1533/2330 train_time:93761ms step_avg:61.16ms
step:1534/2330 train_time:93824ms step_avg:61.16ms
step:1535/2330 train_time:93884ms step_avg:61.16ms
step:1536/2330 train_time:93948ms step_avg:61.16ms
step:1537/2330 train_time:94009ms step_avg:61.16ms
step:1538/2330 train_time:94074ms step_avg:61.17ms
step:1539/2330 train_time:94134ms step_avg:61.17ms
step:1540/2330 train_time:94198ms step_avg:61.17ms
step:1541/2330 train_time:94261ms step_avg:61.17ms
step:1542/2330 train_time:94323ms step_avg:61.17ms
step:1543/2330 train_time:94382ms step_avg:61.17ms
step:1544/2330 train_time:94445ms step_avg:61.17ms
step:1545/2330 train_time:94505ms step_avg:61.17ms
step:1546/2330 train_time:94570ms step_avg:61.17ms
step:1547/2330 train_time:94630ms step_avg:61.17ms
step:1548/2330 train_time:94693ms step_avg:61.17ms
step:1549/2330 train_time:94754ms step_avg:61.17ms
step:1550/2330 train_time:94817ms step_avg:61.17ms
step:1551/2330 train_time:94879ms step_avg:61.17ms
step:1552/2330 train_time:94942ms step_avg:61.17ms
step:1553/2330 train_time:95003ms step_avg:61.17ms
step:1554/2330 train_time:95067ms step_avg:61.18ms
step:1555/2330 train_time:95127ms step_avg:61.17ms
step:1556/2330 train_time:95191ms step_avg:61.18ms
step:1557/2330 train_time:95251ms step_avg:61.18ms
step:1558/2330 train_time:95315ms step_avg:61.18ms
step:1559/2330 train_time:95376ms step_avg:61.18ms
step:1560/2330 train_time:95440ms step_avg:61.18ms
step:1561/2330 train_time:95501ms step_avg:61.18ms
step:1562/2330 train_time:95565ms step_avg:61.18ms
step:1563/2330 train_time:95623ms step_avg:61.18ms
step:1564/2330 train_time:95687ms step_avg:61.18ms
step:1565/2330 train_time:95748ms step_avg:61.18ms
step:1566/2330 train_time:95812ms step_avg:61.18ms
step:1567/2330 train_time:95873ms step_avg:61.18ms
step:1568/2330 train_time:95937ms step_avg:61.18ms
step:1569/2330 train_time:95998ms step_avg:61.18ms
step:1570/2330 train_time:96063ms step_avg:61.19ms
step:1571/2330 train_time:96122ms step_avg:61.19ms
step:1572/2330 train_time:96185ms step_avg:61.19ms
step:1573/2330 train_time:96245ms step_avg:61.19ms
step:1574/2330 train_time:96310ms step_avg:61.19ms
step:1575/2330 train_time:96371ms step_avg:61.19ms
step:1576/2330 train_time:96434ms step_avg:61.19ms
step:1577/2330 train_time:96495ms step_avg:61.19ms
step:1578/2330 train_time:96560ms step_avg:61.19ms
step:1579/2330 train_time:96620ms step_avg:61.19ms
step:1580/2330 train_time:96684ms step_avg:61.19ms
step:1581/2330 train_time:96744ms step_avg:61.19ms
step:1582/2330 train_time:96807ms step_avg:61.19ms
step:1583/2330 train_time:96869ms step_avg:61.19ms
step:1584/2330 train_time:96933ms step_avg:61.20ms
step:1585/2330 train_time:96994ms step_avg:61.19ms
step:1586/2330 train_time:97057ms step_avg:61.20ms
step:1587/2330 train_time:97119ms step_avg:61.20ms
step:1588/2330 train_time:97182ms step_avg:61.20ms
step:1589/2330 train_time:97243ms step_avg:61.20ms
step:1590/2330 train_time:97307ms step_avg:61.20ms
step:1591/2330 train_time:97367ms step_avg:61.20ms
step:1592/2330 train_time:97429ms step_avg:61.20ms
step:1593/2330 train_time:97490ms step_avg:61.20ms
step:1594/2330 train_time:97554ms step_avg:61.20ms
step:1595/2330 train_time:97615ms step_avg:61.20ms
step:1596/2330 train_time:97679ms step_avg:61.20ms
step:1597/2330 train_time:97740ms step_avg:61.20ms
step:1598/2330 train_time:97804ms step_avg:61.20ms
step:1599/2330 train_time:97864ms step_avg:61.20ms
step:1600/2330 train_time:97927ms step_avg:61.20ms
step:1601/2330 train_time:97989ms step_avg:61.20ms
step:1602/2330 train_time:98052ms step_avg:61.21ms
step:1603/2330 train_time:98113ms step_avg:61.21ms
step:1604/2330 train_time:98177ms step_avg:61.21ms
step:1605/2330 train_time:98238ms step_avg:61.21ms
step:1606/2330 train_time:98301ms step_avg:61.21ms
step:1607/2330 train_time:98363ms step_avg:61.21ms
step:1608/2330 train_time:98425ms step_avg:61.21ms
step:1609/2330 train_time:98484ms step_avg:61.21ms
step:1610/2330 train_time:98548ms step_avg:61.21ms
step:1611/2330 train_time:98609ms step_avg:61.21ms
step:1612/2330 train_time:98673ms step_avg:61.21ms
step:1613/2330 train_time:98734ms step_avg:61.21ms
step:1614/2330 train_time:98798ms step_avg:61.21ms
step:1615/2330 train_time:98859ms step_avg:61.21ms
step:1616/2330 train_time:98922ms step_avg:61.21ms
step:1617/2330 train_time:98981ms step_avg:61.21ms
step:1618/2330 train_time:99045ms step_avg:61.21ms
step:1619/2330 train_time:99104ms step_avg:61.21ms
step:1620/2330 train_time:99169ms step_avg:61.22ms
step:1621/2330 train_time:99229ms step_avg:61.21ms
step:1622/2330 train_time:99293ms step_avg:61.22ms
step:1623/2330 train_time:99353ms step_avg:61.22ms
step:1624/2330 train_time:99418ms step_avg:61.22ms
step:1625/2330 train_time:99479ms step_avg:61.22ms
step:1626/2330 train_time:99543ms step_avg:61.22ms
step:1627/2330 train_time:99603ms step_avg:61.22ms
step:1628/2330 train_time:99667ms step_avg:61.22ms
step:1629/2330 train_time:99727ms step_avg:61.22ms
step:1630/2330 train_time:99791ms step_avg:61.22ms
step:1631/2330 train_time:99851ms step_avg:61.22ms
step:1632/2330 train_time:99914ms step_avg:61.22ms
step:1633/2330 train_time:99975ms step_avg:61.22ms
step:1634/2330 train_time:100039ms step_avg:61.22ms
step:1635/2330 train_time:100099ms step_avg:61.22ms
step:1636/2330 train_time:100163ms step_avg:61.22ms
step:1637/2330 train_time:100223ms step_avg:61.22ms
step:1638/2330 train_time:100285ms step_avg:61.22ms
step:1639/2330 train_time:100347ms step_avg:61.22ms
step:1640/2330 train_time:100411ms step_avg:61.23ms
step:1641/2330 train_time:100472ms step_avg:61.23ms
step:1642/2330 train_time:100536ms step_avg:61.23ms
step:1643/2330 train_time:100597ms step_avg:61.23ms
step:1644/2330 train_time:100662ms step_avg:61.23ms
step:1645/2330 train_time:100721ms step_avg:61.23ms
step:1646/2330 train_time:100784ms step_avg:61.23ms
step:1647/2330 train_time:100844ms step_avg:61.23ms
step:1648/2330 train_time:100908ms step_avg:61.23ms
step:1649/2330 train_time:100969ms step_avg:61.23ms
step:1650/2330 train_time:101033ms step_avg:61.23ms
step:1651/2330 train_time:101094ms step_avg:61.23ms
step:1652/2330 train_time:101157ms step_avg:61.23ms
step:1653/2330 train_time:101217ms step_avg:61.23ms
step:1654/2330 train_time:101281ms step_avg:61.23ms
step:1655/2330 train_time:101341ms step_avg:61.23ms
step:1656/2330 train_time:101404ms step_avg:61.23ms
step:1657/2330 train_time:101464ms step_avg:61.23ms
step:1658/2330 train_time:101528ms step_avg:61.24ms
step:1659/2330 train_time:101589ms step_avg:61.24ms
step:1660/2330 train_time:101653ms step_avg:61.24ms
step:1661/2330 train_time:101713ms step_avg:61.24ms
step:1662/2330 train_time:101777ms step_avg:61.24ms
step:1663/2330 train_time:101838ms step_avg:61.24ms
step:1664/2330 train_time:101902ms step_avg:61.24ms
step:1665/2330 train_time:101963ms step_avg:61.24ms
step:1666/2330 train_time:102025ms step_avg:61.24ms
step:1667/2330 train_time:102085ms step_avg:61.24ms
step:1668/2330 train_time:102149ms step_avg:61.24ms
step:1669/2330 train_time:102210ms step_avg:61.24ms
step:1670/2330 train_time:102274ms step_avg:61.24ms
step:1671/2330 train_time:102334ms step_avg:61.24ms
step:1672/2330 train_time:102399ms step_avg:61.24ms
step:1673/2330 train_time:102461ms step_avg:61.24ms
step:1674/2330 train_time:102523ms step_avg:61.24ms
step:1675/2330 train_time:102583ms step_avg:61.24ms
step:1676/2330 train_time:102646ms step_avg:61.24ms
step:1677/2330 train_time:102706ms step_avg:61.24ms
step:1678/2330 train_time:102771ms step_avg:61.25ms
step:1679/2330 train_time:102832ms step_avg:61.25ms
step:1680/2330 train_time:102896ms step_avg:61.25ms
step:1681/2330 train_time:102957ms step_avg:61.25ms
step:1682/2330 train_time:103020ms step_avg:61.25ms
step:1683/2330 train_time:103080ms step_avg:61.25ms
step:1684/2330 train_time:103144ms step_avg:61.25ms
step:1685/2330 train_time:103203ms step_avg:61.25ms
step:1686/2330 train_time:103267ms step_avg:61.25ms
step:1687/2330 train_time:103327ms step_avg:61.25ms
step:1688/2330 train_time:103391ms step_avg:61.25ms
step:1689/2330 train_time:103451ms step_avg:61.25ms
step:1690/2330 train_time:103514ms step_avg:61.25ms
step:1691/2330 train_time:103575ms step_avg:61.25ms
step:1692/2330 train_time:103638ms step_avg:61.25ms
step:1693/2330 train_time:103699ms step_avg:61.25ms
step:1694/2330 train_time:103762ms step_avg:61.25ms
step:1695/2330 train_time:103822ms step_avg:61.25ms
step:1696/2330 train_time:103886ms step_avg:61.25ms
step:1697/2330 train_time:103946ms step_avg:61.25ms
step:1698/2330 train_time:104009ms step_avg:61.25ms
step:1699/2330 train_time:104070ms step_avg:61.25ms
step:1700/2330 train_time:104132ms step_avg:61.25ms
step:1701/2330 train_time:104194ms step_avg:61.25ms
step:1702/2330 train_time:104258ms step_avg:61.26ms
step:1703/2330 train_time:104318ms step_avg:61.26ms
step:1704/2330 train_time:104380ms step_avg:61.26ms
step:1705/2330 train_time:104440ms step_avg:61.26ms
step:1706/2330 train_time:104503ms step_avg:61.26ms
step:1707/2330 train_time:104564ms step_avg:61.26ms
step:1708/2330 train_time:104628ms step_avg:61.26ms
step:1709/2330 train_time:104689ms step_avg:61.26ms
step:1710/2330 train_time:104753ms step_avg:61.26ms
step:1711/2330 train_time:104813ms step_avg:61.26ms
step:1712/2330 train_time:104878ms step_avg:61.26ms
step:1713/2330 train_time:104940ms step_avg:61.26ms
step:1714/2330 train_time:105003ms step_avg:61.26ms
step:1715/2330 train_time:105064ms step_avg:61.26ms
step:1716/2330 train_time:105126ms step_avg:61.26ms
step:1717/2330 train_time:105186ms step_avg:61.26ms
step:1718/2330 train_time:105250ms step_avg:61.26ms
step:1719/2330 train_time:105310ms step_avg:61.26ms
step:1720/2330 train_time:105373ms step_avg:61.26ms
step:1721/2330 train_time:105434ms step_avg:61.26ms
step:1722/2330 train_time:105499ms step_avg:61.27ms
step:1723/2330 train_time:105559ms step_avg:61.26ms
step:1724/2330 train_time:105622ms step_avg:61.27ms
step:1725/2330 train_time:105681ms step_avg:61.26ms
step:1726/2330 train_time:105744ms step_avg:61.27ms
step:1727/2330 train_time:105805ms step_avg:61.27ms
step:1728/2330 train_time:105870ms step_avg:61.27ms
step:1729/2330 train_time:105931ms step_avg:61.27ms
step:1730/2330 train_time:105994ms step_avg:61.27ms
step:1731/2330 train_time:106056ms step_avg:61.27ms
step:1732/2330 train_time:106120ms step_avg:61.27ms
step:1733/2330 train_time:106180ms step_avg:61.27ms
step:1734/2330 train_time:106243ms step_avg:61.27ms
step:1735/2330 train_time:106302ms step_avg:61.27ms
step:1736/2330 train_time:106366ms step_avg:61.27ms
step:1737/2330 train_time:106426ms step_avg:61.27ms
step:1738/2330 train_time:106490ms step_avg:61.27ms
step:1739/2330 train_time:106551ms step_avg:61.27ms
step:1740/2330 train_time:106614ms step_avg:61.27ms
step:1741/2330 train_time:106674ms step_avg:61.27ms
step:1742/2330 train_time:106739ms step_avg:61.27ms
step:1743/2330 train_time:106799ms step_avg:61.27ms
step:1744/2330 train_time:106863ms step_avg:61.27ms
step:1745/2330 train_time:106923ms step_avg:61.27ms
step:1746/2330 train_time:106986ms step_avg:61.27ms
step:1747/2330 train_time:107047ms step_avg:61.27ms
step:1748/2330 train_time:107111ms step_avg:61.28ms
step:1749/2330 train_time:107171ms step_avg:61.28ms
step:1750/2330 train_time:107234ms step_avg:61.28ms
step:1750/2330 val_loss:3.7434 train_time:107307ms step_avg:61.32ms
step:1751/2330 train_time:107329ms step_avg:61.30ms
step:1752/2330 train_time:107359ms step_avg:61.28ms
step:1753/2330 train_time:107418ms step_avg:61.28ms
step:1754/2330 train_time:107486ms step_avg:61.28ms
step:1755/2330 train_time:107551ms step_avg:61.28ms
step:1756/2330 train_time:107614ms step_avg:61.28ms
step:1757/2330 train_time:107674ms step_avg:61.28ms
step:1758/2330 train_time:107736ms step_avg:61.28ms
step:1759/2330 train_time:107796ms step_avg:61.28ms
step:1760/2330 train_time:107859ms step_avg:61.28ms
step:1761/2330 train_time:107918ms step_avg:61.28ms
step:1762/2330 train_time:107981ms step_avg:61.28ms
step:1763/2330 train_time:108040ms step_avg:61.28ms
step:1764/2330 train_time:108103ms step_avg:61.28ms
step:1765/2330 train_time:108162ms step_avg:61.28ms
step:1766/2330 train_time:108225ms step_avg:61.28ms
step:1767/2330 train_time:108290ms step_avg:61.28ms
step:1768/2330 train_time:108356ms step_avg:61.29ms
step:1769/2330 train_time:108417ms step_avg:61.29ms
step:1770/2330 train_time:108482ms step_avg:61.29ms
step:1771/2330 train_time:108544ms step_avg:61.29ms
step:1772/2330 train_time:108607ms step_avg:61.29ms
step:1773/2330 train_time:108668ms step_avg:61.29ms
step:1774/2330 train_time:108731ms step_avg:61.29ms
step:1775/2330 train_time:108790ms step_avg:61.29ms
step:1776/2330 train_time:108853ms step_avg:61.29ms
step:1777/2330 train_time:108913ms step_avg:61.29ms
step:1778/2330 train_time:108976ms step_avg:61.29ms
step:1779/2330 train_time:109036ms step_avg:61.29ms
step:1780/2330 train_time:109098ms step_avg:61.29ms
step:1781/2330 train_time:109159ms step_avg:61.29ms
step:1782/2330 train_time:109223ms step_avg:61.29ms
step:1783/2330 train_time:109285ms step_avg:61.29ms
step:1784/2330 train_time:109350ms step_avg:61.29ms
step:1785/2330 train_time:109410ms step_avg:61.29ms
step:1786/2330 train_time:109475ms step_avg:61.30ms
step:1787/2330 train_time:109537ms step_avg:61.30ms
step:1788/2330 train_time:109601ms step_avg:61.30ms
step:1789/2330 train_time:109661ms step_avg:61.30ms
step:1790/2330 train_time:109724ms step_avg:61.30ms
step:1791/2330 train_time:109785ms step_avg:61.30ms
step:1792/2330 train_time:109849ms step_avg:61.30ms
step:1793/2330 train_time:109910ms step_avg:61.30ms
step:1794/2330 train_time:109972ms step_avg:61.30ms
step:1795/2330 train_time:110031ms step_avg:61.30ms
step:1796/2330 train_time:110095ms step_avg:61.30ms
step:1797/2330 train_time:110156ms step_avg:61.30ms
step:1798/2330 train_time:110220ms step_avg:61.30ms
step:1799/2330 train_time:110280ms step_avg:61.30ms
step:1800/2330 train_time:110344ms step_avg:61.30ms
step:1801/2330 train_time:110405ms step_avg:61.30ms
step:1802/2330 train_time:110468ms step_avg:61.30ms
step:1803/2330 train_time:110530ms step_avg:61.30ms
step:1804/2330 train_time:110593ms step_avg:61.30ms
step:1805/2330 train_time:110654ms step_avg:61.30ms
step:1806/2330 train_time:110718ms step_avg:61.31ms
step:1807/2330 train_time:110778ms step_avg:61.30ms
step:1808/2330 train_time:110841ms step_avg:61.31ms
step:1809/2330 train_time:110902ms step_avg:61.31ms
step:1810/2330 train_time:110966ms step_avg:61.31ms
step:1811/2330 train_time:111027ms step_avg:61.31ms
step:1812/2330 train_time:111091ms step_avg:61.31ms
step:1813/2330 train_time:111152ms step_avg:61.31ms
step:1814/2330 train_time:111214ms step_avg:61.31ms
step:1815/2330 train_time:111274ms step_avg:61.31ms
step:1816/2330 train_time:111338ms step_avg:61.31ms
step:1817/2330 train_time:111399ms step_avg:61.31ms
step:1818/2330 train_time:111463ms step_avg:61.31ms
step:1819/2330 train_time:111524ms step_avg:61.31ms
step:1820/2330 train_time:111587ms step_avg:61.31ms
step:1821/2330 train_time:111648ms step_avg:61.31ms
step:1822/2330 train_time:111710ms step_avg:61.31ms
step:1823/2330 train_time:111770ms step_avg:61.31ms
step:1824/2330 train_time:111834ms step_avg:61.31ms
step:1825/2330 train_time:111894ms step_avg:61.31ms
step:1826/2330 train_time:111960ms step_avg:61.31ms
step:1827/2330 train_time:112020ms step_avg:61.31ms
step:1828/2330 train_time:112084ms step_avg:61.31ms
step:1829/2330 train_time:112145ms step_avg:61.32ms
step:1830/2330 train_time:112209ms step_avg:61.32ms
step:1831/2330 train_time:112269ms step_avg:61.32ms
step:1832/2330 train_time:112332ms step_avg:61.32ms
step:1833/2330 train_time:112392ms step_avg:61.32ms
step:1834/2330 train_time:112456ms step_avg:61.32ms
step:1835/2330 train_time:112517ms step_avg:61.32ms
step:1836/2330 train_time:112582ms step_avg:61.32ms
step:1837/2330 train_time:112642ms step_avg:61.32ms
step:1838/2330 train_time:112705ms step_avg:61.32ms
step:1839/2330 train_time:112765ms step_avg:61.32ms
step:1840/2330 train_time:112829ms step_avg:61.32ms
step:1841/2330 train_time:112889ms step_avg:61.32ms
step:1842/2330 train_time:112952ms step_avg:61.32ms
step:1843/2330 train_time:113012ms step_avg:61.32ms
step:1844/2330 train_time:113076ms step_avg:61.32ms
step:1845/2330 train_time:113136ms step_avg:61.32ms
step:1846/2330 train_time:113200ms step_avg:61.32ms
step:1847/2330 train_time:113260ms step_avg:61.32ms
step:1848/2330 train_time:113324ms step_avg:61.32ms
step:1849/2330 train_time:113386ms step_avg:61.32ms
step:1850/2330 train_time:113451ms step_avg:61.32ms
step:1851/2330 train_time:113510ms step_avg:61.32ms
step:1852/2330 train_time:113574ms step_avg:61.33ms
step:1853/2330 train_time:113634ms step_avg:61.32ms
step:1854/2330 train_time:113697ms step_avg:61.33ms
step:1855/2330 train_time:113758ms step_avg:61.33ms
step:1856/2330 train_time:113822ms step_avg:61.33ms
step:1857/2330 train_time:113884ms step_avg:61.33ms
step:1858/2330 train_time:113947ms step_avg:61.33ms
step:1859/2330 train_time:114007ms step_avg:61.33ms
step:1860/2330 train_time:114071ms step_avg:61.33ms
step:1861/2330 train_time:114131ms step_avg:61.33ms
step:1862/2330 train_time:114194ms step_avg:61.33ms
step:1863/2330 train_time:114254ms step_avg:61.33ms
step:1864/2330 train_time:114319ms step_avg:61.33ms
step:1865/2330 train_time:114379ms step_avg:61.33ms
step:1866/2330 train_time:114443ms step_avg:61.33ms
step:1867/2330 train_time:114503ms step_avg:61.33ms
step:1868/2330 train_time:114567ms step_avg:61.33ms
step:1869/2330 train_time:114628ms step_avg:61.33ms
step:1870/2330 train_time:114691ms step_avg:61.33ms
step:1871/2330 train_time:114752ms step_avg:61.33ms
step:1872/2330 train_time:114815ms step_avg:61.33ms
step:1873/2330 train_time:114875ms step_avg:61.33ms
step:1874/2330 train_time:114939ms step_avg:61.33ms
step:1875/2330 train_time:114999ms step_avg:61.33ms
step:1876/2330 train_time:115062ms step_avg:61.33ms
step:1877/2330 train_time:115123ms step_avg:61.33ms
step:1878/2330 train_time:115187ms step_avg:61.33ms
step:1879/2330 train_time:115248ms step_avg:61.33ms
step:1880/2330 train_time:115311ms step_avg:61.34ms
step:1881/2330 train_time:115370ms step_avg:61.33ms
step:1882/2330 train_time:115434ms step_avg:61.34ms
step:1883/2330 train_time:115494ms step_avg:61.34ms
step:1884/2330 train_time:115559ms step_avg:61.34ms
step:1885/2330 train_time:115620ms step_avg:61.34ms
step:1886/2330 train_time:115684ms step_avg:61.34ms
step:1887/2330 train_time:115744ms step_avg:61.34ms
step:1888/2330 train_time:115808ms step_avg:61.34ms
step:1889/2330 train_time:115868ms step_avg:61.34ms
step:1890/2330 train_time:115931ms step_avg:61.34ms
step:1891/2330 train_time:115991ms step_avg:61.34ms
step:1892/2330 train_time:116054ms step_avg:61.34ms
step:1893/2330 train_time:116115ms step_avg:61.34ms
step:1894/2330 train_time:116179ms step_avg:61.34ms
step:1895/2330 train_time:116238ms step_avg:61.34ms
step:1896/2330 train_time:116301ms step_avg:61.34ms
step:1897/2330 train_time:116363ms step_avg:61.34ms
step:1898/2330 train_time:116427ms step_avg:61.34ms
step:1899/2330 train_time:116488ms step_avg:61.34ms
step:1900/2330 train_time:116550ms step_avg:61.34ms
step:1901/2330 train_time:116610ms step_avg:61.34ms
step:1902/2330 train_time:116674ms step_avg:61.34ms
step:1903/2330 train_time:116734ms step_avg:61.34ms
step:1904/2330 train_time:116798ms step_avg:61.34ms
step:1905/2330 train_time:116858ms step_avg:61.34ms
step:1906/2330 train_time:116921ms step_avg:61.34ms
step:1907/2330 train_time:116982ms step_avg:61.34ms
step:1908/2330 train_time:117046ms step_avg:61.34ms
step:1909/2330 train_time:117106ms step_avg:61.34ms
step:1910/2330 train_time:117169ms step_avg:61.35ms
step:1911/2330 train_time:117229ms step_avg:61.34ms
step:1912/2330 train_time:117292ms step_avg:61.35ms
step:1913/2330 train_time:117353ms step_avg:61.34ms
step:1914/2330 train_time:117416ms step_avg:61.35ms
step:1915/2330 train_time:117477ms step_avg:61.35ms
step:1916/2330 train_time:117540ms step_avg:61.35ms
step:1917/2330 train_time:117600ms step_avg:61.35ms
step:1918/2330 train_time:117664ms step_avg:61.35ms
step:1919/2330 train_time:117726ms step_avg:61.35ms
step:1920/2330 train_time:117789ms step_avg:61.35ms
step:1921/2330 train_time:117849ms step_avg:61.35ms
step:1922/2330 train_time:117911ms step_avg:61.35ms
step:1923/2330 train_time:117971ms step_avg:61.35ms
step:1924/2330 train_time:118036ms step_avg:61.35ms
step:1925/2330 train_time:118096ms step_avg:61.35ms
step:1926/2330 train_time:118160ms step_avg:61.35ms
step:1927/2330 train_time:118220ms step_avg:61.35ms
step:1928/2330 train_time:118284ms step_avg:61.35ms
step:1929/2330 train_time:118345ms step_avg:61.35ms
step:1930/2330 train_time:118408ms step_avg:61.35ms
step:1931/2330 train_time:118469ms step_avg:61.35ms
step:1932/2330 train_time:118532ms step_avg:61.35ms
step:1933/2330 train_time:118593ms step_avg:61.35ms
step:1934/2330 train_time:118656ms step_avg:61.35ms
step:1935/2330 train_time:118716ms step_avg:61.35ms
step:1936/2330 train_time:118781ms step_avg:61.35ms
step:1937/2330 train_time:118840ms step_avg:61.35ms
step:1938/2330 train_time:118904ms step_avg:61.35ms
step:1939/2330 train_time:118965ms step_avg:61.35ms
step:1940/2330 train_time:119029ms step_avg:61.36ms
step:1941/2330 train_time:119090ms step_avg:61.35ms
step:1942/2330 train_time:119152ms step_avg:61.36ms
step:1943/2330 train_time:119212ms step_avg:61.35ms
step:1944/2330 train_time:119276ms step_avg:61.36ms
step:1945/2330 train_time:119337ms step_avg:61.36ms
step:1946/2330 train_time:119402ms step_avg:61.36ms
step:1947/2330 train_time:119461ms step_avg:61.36ms
step:1948/2330 train_time:119526ms step_avg:61.36ms
step:1949/2330 train_time:119586ms step_avg:61.36ms
step:1950/2330 train_time:119650ms step_avg:61.36ms
step:1951/2330 train_time:119710ms step_avg:61.36ms
step:1952/2330 train_time:119773ms step_avg:61.36ms
step:1953/2330 train_time:119833ms step_avg:61.36ms
step:1954/2330 train_time:119897ms step_avg:61.36ms
step:1955/2330 train_time:119957ms step_avg:61.36ms
step:1956/2330 train_time:120021ms step_avg:61.36ms
step:1957/2330 train_time:120082ms step_avg:61.36ms
step:1958/2330 train_time:120145ms step_avg:61.36ms
step:1959/2330 train_time:120206ms step_avg:61.36ms
step:1960/2330 train_time:120269ms step_avg:61.36ms
step:1961/2330 train_time:120329ms step_avg:61.36ms
step:1962/2330 train_time:120392ms step_avg:61.36ms
step:1963/2330 train_time:120453ms step_avg:61.36ms
step:1964/2330 train_time:120516ms step_avg:61.36ms
step:1965/2330 train_time:120577ms step_avg:61.36ms
step:1966/2330 train_time:120640ms step_avg:61.36ms
step:1967/2330 train_time:120701ms step_avg:61.36ms
step:1968/2330 train_time:120765ms step_avg:61.36ms
step:1969/2330 train_time:120825ms step_avg:61.36ms
step:1970/2330 train_time:120890ms step_avg:61.37ms
step:1971/2330 train_time:120949ms step_avg:61.36ms
step:1972/2330 train_time:121012ms step_avg:61.37ms
step:1973/2330 train_time:121072ms step_avg:61.36ms
step:1974/2330 train_time:121136ms step_avg:61.37ms
step:1975/2330 train_time:121196ms step_avg:61.37ms
step:1976/2330 train_time:121260ms step_avg:61.37ms
step:1977/2330 train_time:121320ms step_avg:61.37ms
step:1978/2330 train_time:121383ms step_avg:61.37ms
step:1979/2330 train_time:121446ms step_avg:61.37ms
step:1980/2330 train_time:121508ms step_avg:61.37ms
step:1981/2330 train_time:121568ms step_avg:61.37ms
step:1982/2330 train_time:121631ms step_avg:61.37ms
step:1983/2330 train_time:121691ms step_avg:61.37ms
step:1984/2330 train_time:121755ms step_avg:61.37ms
step:1985/2330 train_time:121816ms step_avg:61.37ms
step:1986/2330 train_time:121880ms step_avg:61.37ms
step:1987/2330 train_time:121941ms step_avg:61.37ms
step:1988/2330 train_time:122005ms step_avg:61.37ms
step:1989/2330 train_time:122066ms step_avg:61.37ms
step:1990/2330 train_time:122131ms step_avg:61.37ms
step:1991/2330 train_time:122190ms step_avg:61.37ms
step:1992/2330 train_time:122253ms step_avg:61.37ms
step:1993/2330 train_time:122314ms step_avg:61.37ms
step:1994/2330 train_time:122378ms step_avg:61.37ms
step:1995/2330 train_time:122438ms step_avg:61.37ms
step:1996/2330 train_time:122502ms step_avg:61.37ms
step:1997/2330 train_time:122563ms step_avg:61.37ms
step:1998/2330 train_time:122627ms step_avg:61.37ms
step:1999/2330 train_time:122687ms step_avg:61.37ms
step:2000/2330 train_time:122751ms step_avg:61.38ms
step:2000/2330 val_loss:3.6881 train_time:122824ms step_avg:61.41ms
step:2001/2330 train_time:122847ms step_avg:61.39ms
step:2002/2330 train_time:122877ms step_avg:61.38ms
step:2003/2330 train_time:122942ms step_avg:61.38ms
step:2004/2330 train_time:123008ms step_avg:61.38ms
step:2005/2330 train_time:123070ms step_avg:61.38ms
step:2006/2330 train_time:123133ms step_avg:61.38ms
step:2007/2330 train_time:123193ms step_avg:61.38ms
step:2008/2330 train_time:123256ms step_avg:61.38ms
step:2009/2330 train_time:123315ms step_avg:61.38ms
step:2010/2330 train_time:123378ms step_avg:61.38ms
step:2011/2330 train_time:123437ms step_avg:61.38ms
step:2012/2330 train_time:123500ms step_avg:61.38ms
step:2013/2330 train_time:123559ms step_avg:61.38ms
step:2014/2330 train_time:123622ms step_avg:61.38ms
step:2015/2330 train_time:123681ms step_avg:61.38ms
step:2016/2330 train_time:123744ms step_avg:61.38ms
step:2017/2330 train_time:123805ms step_avg:61.38ms
step:2018/2330 train_time:123871ms step_avg:61.38ms
step:2019/2330 train_time:123932ms step_avg:61.38ms
step:2020/2330 train_time:123996ms step_avg:61.38ms
step:2021/2330 train_time:124058ms step_avg:61.38ms
step:2022/2330 train_time:124121ms step_avg:61.39ms
step:2023/2330 train_time:124181ms step_avg:61.38ms
step:2024/2330 train_time:124243ms step_avg:61.39ms
step:2025/2330 train_time:124304ms step_avg:61.38ms
step:2026/2330 train_time:124368ms step_avg:61.39ms
step:2027/2330 train_time:124428ms step_avg:61.39ms
step:2028/2330 train_time:124491ms step_avg:61.39ms
step:2029/2330 train_time:124551ms step_avg:61.39ms
step:2030/2330 train_time:124614ms step_avg:61.39ms
step:2031/2330 train_time:124674ms step_avg:61.39ms
step:2032/2330 train_time:124737ms step_avg:61.39ms
step:2033/2330 train_time:124798ms step_avg:61.39ms
step:2034/2330 train_time:124862ms step_avg:61.39ms
step:2035/2330 train_time:124923ms step_avg:61.39ms
step:2036/2330 train_time:124986ms step_avg:61.39ms
step:2037/2330 train_time:125047ms step_avg:61.39ms
step:2038/2330 train_time:125111ms step_avg:61.39ms
step:2039/2330 train_time:125172ms step_avg:61.39ms
step:2040/2330 train_time:125235ms step_avg:61.39ms
step:2041/2330 train_time:125296ms step_avg:61.39ms
step:2042/2330 train_time:125362ms step_avg:61.39ms
step:2043/2330 train_time:125422ms step_avg:61.39ms
step:2044/2330 train_time:125485ms step_avg:61.39ms
step:2045/2330 train_time:125545ms step_avg:61.39ms
step:2046/2330 train_time:125608ms step_avg:61.39ms
step:2047/2330 train_time:125667ms step_avg:61.39ms
step:2048/2330 train_time:125731ms step_avg:61.39ms
step:2049/2330 train_time:125791ms step_avg:61.39ms
step:2050/2330 train_time:125854ms step_avg:61.39ms
step:2051/2330 train_time:125916ms step_avg:61.39ms
step:2052/2330 train_time:125980ms step_avg:61.39ms
step:2053/2330 train_time:126041ms step_avg:61.39ms
step:2054/2330 train_time:126105ms step_avg:61.39ms
step:2055/2330 train_time:126165ms step_avg:61.39ms
step:2056/2330 train_time:126229ms step_avg:61.40ms
step:2057/2330 train_time:126290ms step_avg:61.40ms
step:2058/2330 train_time:126353ms step_avg:61.40ms
step:2059/2330 train_time:126414ms step_avg:61.40ms
step:2060/2330 train_time:126477ms step_avg:61.40ms
step:2061/2330 train_time:126539ms step_avg:61.40ms
step:2062/2330 train_time:126603ms step_avg:61.40ms
step:2063/2330 train_time:126663ms step_avg:61.40ms
step:2064/2330 train_time:126725ms step_avg:61.40ms
step:2065/2330 train_time:126785ms step_avg:61.40ms
step:2066/2330 train_time:126848ms step_avg:61.40ms
step:2067/2330 train_time:126909ms step_avg:61.40ms
step:2068/2330 train_time:126974ms step_avg:61.40ms
step:2069/2330 train_time:127035ms step_avg:61.40ms
step:2070/2330 train_time:127099ms step_avg:61.40ms
step:2071/2330 train_time:127161ms step_avg:61.40ms
step:2072/2330 train_time:127224ms step_avg:61.40ms
step:2073/2330 train_time:127284ms step_avg:61.40ms
step:2074/2330 train_time:127347ms step_avg:61.40ms
step:2075/2330 train_time:127408ms step_avg:61.40ms
step:2076/2330 train_time:127472ms step_avg:61.40ms
step:2077/2330 train_time:127531ms step_avg:61.40ms
step:2078/2330 train_time:127595ms step_avg:61.40ms
step:2079/2330 train_time:127655ms step_avg:61.40ms
step:2080/2330 train_time:127718ms step_avg:61.40ms
step:2081/2330 train_time:127779ms step_avg:61.40ms
step:2082/2330 train_time:127842ms step_avg:61.40ms
step:2083/2330 train_time:127903ms step_avg:61.40ms
step:2084/2330 train_time:127966ms step_avg:61.40ms
step:2085/2330 train_time:128026ms step_avg:61.40ms
step:2086/2330 train_time:128091ms step_avg:61.41ms
step:2087/2330 train_time:128152ms step_avg:61.40ms
step:2088/2330 train_time:128215ms step_avg:61.41ms
step:2089/2330 train_time:128275ms step_avg:61.41ms
step:2090/2330 train_time:128339ms step_avg:61.41ms
step:2091/2330 train_time:128400ms step_avg:61.41ms
step:2092/2330 train_time:128464ms step_avg:61.41ms
step:2093/2330 train_time:128523ms step_avg:61.41ms
step:2094/2330 train_time:128586ms step_avg:61.41ms
step:2095/2330 train_time:128646ms step_avg:61.41ms
step:2096/2330 train_time:128709ms step_avg:61.41ms
step:2097/2330 train_time:128769ms step_avg:61.41ms
step:2098/2330 train_time:128832ms step_avg:61.41ms
step:2099/2330 train_time:128893ms step_avg:61.41ms
step:2100/2330 train_time:128957ms step_avg:61.41ms
step:2101/2330 train_time:129019ms step_avg:61.41ms
step:2102/2330 train_time:129082ms step_avg:61.41ms
step:2103/2330 train_time:129143ms step_avg:61.41ms
step:2104/2330 train_time:129206ms step_avg:61.41ms
step:2105/2330 train_time:129265ms step_avg:61.41ms
step:2106/2330 train_time:129328ms step_avg:61.41ms
step:2107/2330 train_time:129389ms step_avg:61.41ms
step:2108/2330 train_time:129453ms step_avg:61.41ms
step:2109/2330 train_time:129513ms step_avg:61.41ms
step:2110/2330 train_time:129577ms step_avg:61.41ms
step:2111/2330 train_time:129639ms step_avg:61.41ms
step:2112/2330 train_time:129701ms step_avg:61.41ms
step:2113/2330 train_time:129763ms step_avg:61.41ms
step:2114/2330 train_time:129825ms step_avg:61.41ms
step:2115/2330 train_time:129884ms step_avg:61.41ms
step:2116/2330 train_time:129949ms step_avg:61.41ms
step:2117/2330 train_time:130008ms step_avg:61.41ms
step:2118/2330 train_time:130073ms step_avg:61.41ms
step:2119/2330 train_time:130133ms step_avg:61.41ms
step:2120/2330 train_time:130197ms step_avg:61.41ms
step:2121/2330 train_time:130258ms step_avg:61.41ms
step:2122/2330 train_time:130322ms step_avg:61.41ms
step:2123/2330 train_time:130382ms step_avg:61.41ms
step:2124/2330 train_time:130446ms step_avg:61.42ms
step:2125/2330 train_time:130505ms step_avg:61.41ms
step:2126/2330 train_time:130570ms step_avg:61.42ms
step:2127/2330 train_time:130631ms step_avg:61.42ms
step:2128/2330 train_time:130695ms step_avg:61.42ms
step:2129/2330 train_time:130756ms step_avg:61.42ms
step:2130/2330 train_time:130819ms step_avg:61.42ms
step:2131/2330 train_time:130880ms step_avg:61.42ms
step:2132/2330 train_time:130944ms step_avg:61.42ms
step:2133/2330 train_time:131004ms step_avg:61.42ms
step:2134/2330 train_time:131068ms step_avg:61.42ms
step:2135/2330 train_time:131128ms step_avg:61.42ms
step:2136/2330 train_time:131192ms step_avg:61.42ms
step:2137/2330 train_time:131252ms step_avg:61.42ms
step:2138/2330 train_time:131315ms step_avg:61.42ms
step:2139/2330 train_time:131377ms step_avg:61.42ms
step:2140/2330 train_time:131442ms step_avg:61.42ms
step:2141/2330 train_time:131502ms step_avg:61.42ms
step:2142/2330 train_time:131566ms step_avg:61.42ms
step:2143/2330 train_time:131625ms step_avg:61.42ms
step:2144/2330 train_time:131688ms step_avg:61.42ms
step:2145/2330 train_time:131748ms step_avg:61.42ms
step:2146/2330 train_time:131812ms step_avg:61.42ms
step:2147/2330 train_time:131872ms step_avg:61.42ms
step:2148/2330 train_time:131935ms step_avg:61.42ms
step:2149/2330 train_time:131997ms step_avg:61.42ms
step:2150/2330 train_time:132061ms step_avg:61.42ms
step:2151/2330 train_time:132122ms step_avg:61.42ms
step:2152/2330 train_time:132184ms step_avg:61.42ms
step:2153/2330 train_time:132244ms step_avg:61.42ms
step:2154/2330 train_time:132306ms step_avg:61.42ms
step:2155/2330 train_time:132368ms step_avg:61.42ms
step:2156/2330 train_time:132431ms step_avg:61.42ms
step:2157/2330 train_time:132492ms step_avg:61.42ms
step:2158/2330 train_time:132555ms step_avg:61.43ms
step:2159/2330 train_time:132616ms step_avg:61.42ms
step:2160/2330 train_time:132679ms step_avg:61.43ms
step:2161/2330 train_time:132739ms step_avg:61.42ms
step:2162/2330 train_time:132803ms step_avg:61.43ms
step:2163/2330 train_time:132863ms step_avg:61.43ms
step:2164/2330 train_time:132927ms step_avg:61.43ms
step:2165/2330 train_time:132987ms step_avg:61.43ms
step:2166/2330 train_time:133051ms step_avg:61.43ms
step:2167/2330 train_time:133111ms step_avg:61.43ms
step:2168/2330 train_time:133174ms step_avg:61.43ms
step:2169/2330 train_time:133235ms step_avg:61.43ms
step:2170/2330 train_time:133299ms step_avg:61.43ms
step:2171/2330 train_time:133362ms step_avg:61.43ms
step:2172/2330 train_time:133425ms step_avg:61.43ms
step:2173/2330 train_time:133485ms step_avg:61.43ms
step:2174/2330 train_time:133548ms step_avg:61.43ms
step:2175/2330 train_time:133608ms step_avg:61.43ms
step:2176/2330 train_time:133673ms step_avg:61.43ms
step:2177/2330 train_time:133733ms step_avg:61.43ms
step:2178/2330 train_time:133796ms step_avg:61.43ms
step:2179/2330 train_time:133858ms step_avg:61.43ms
step:2180/2330 train_time:133922ms step_avg:61.43ms
step:2181/2330 train_time:133983ms step_avg:61.43ms
step:2182/2330 train_time:134047ms step_avg:61.43ms
step:2183/2330 train_time:134107ms step_avg:61.43ms
step:2184/2330 train_time:134170ms step_avg:61.43ms
step:2185/2330 train_time:134230ms step_avg:61.43ms
step:2186/2330 train_time:134294ms step_avg:61.43ms
step:2187/2330 train_time:134354ms step_avg:61.43ms
step:2188/2330 train_time:134419ms step_avg:61.43ms
step:2189/2330 train_time:134480ms step_avg:61.43ms
step:2190/2330 train_time:134543ms step_avg:61.44ms
step:2191/2330 train_time:134602ms step_avg:61.43ms
step:2192/2330 train_time:134666ms step_avg:61.44ms
step:2193/2330 train_time:134725ms step_avg:61.43ms
step:2194/2330 train_time:134790ms step_avg:61.44ms
step:2195/2330 train_time:134849ms step_avg:61.43ms
step:2196/2330 train_time:134913ms step_avg:61.44ms
step:2197/2330 train_time:134974ms step_avg:61.44ms
step:2198/2330 train_time:135037ms step_avg:61.44ms
step:2199/2330 train_time:135098ms step_avg:61.44ms
step:2200/2330 train_time:135163ms step_avg:61.44ms
step:2201/2330 train_time:135223ms step_avg:61.44ms
step:2202/2330 train_time:135284ms step_avg:61.44ms
step:2203/2330 train_time:135345ms step_avg:61.44ms
step:2204/2330 train_time:135409ms step_avg:61.44ms
step:2205/2330 train_time:135469ms step_avg:61.44ms
step:2206/2330 train_time:135533ms step_avg:61.44ms
step:2207/2330 train_time:135594ms step_avg:61.44ms
step:2208/2330 train_time:135659ms step_avg:61.44ms
step:2209/2330 train_time:135720ms step_avg:61.44ms
step:2210/2330 train_time:135783ms step_avg:61.44ms
step:2211/2330 train_time:135843ms step_avg:61.44ms
step:2212/2330 train_time:135906ms step_avg:61.44ms
step:2213/2330 train_time:135966ms step_avg:61.44ms
step:2214/2330 train_time:136030ms step_avg:61.44ms
step:2215/2330 train_time:136091ms step_avg:61.44ms
step:2216/2330 train_time:136154ms step_avg:61.44ms
step:2217/2330 train_time:136215ms step_avg:61.44ms
step:2218/2330 train_time:136278ms step_avg:61.44ms
step:2219/2330 train_time:136339ms step_avg:61.44ms
step:2220/2330 train_time:136403ms step_avg:61.44ms
step:2221/2330 train_time:136463ms step_avg:61.44ms
step:2222/2330 train_time:136525ms step_avg:61.44ms
step:2223/2330 train_time:136585ms step_avg:61.44ms
step:2224/2330 train_time:136649ms step_avg:61.44ms
step:2225/2330 train_time:136709ms step_avg:61.44ms
step:2226/2330 train_time:136773ms step_avg:61.44ms
step:2227/2330 train_time:136834ms step_avg:61.44ms
step:2228/2330 train_time:136898ms step_avg:61.44ms
step:2229/2330 train_time:136959ms step_avg:61.44ms
step:2230/2330 train_time:137023ms step_avg:61.45ms
step:2231/2330 train_time:137082ms step_avg:61.44ms
step:2232/2330 train_time:137146ms step_avg:61.45ms
step:2233/2330 train_time:137207ms step_avg:61.45ms
step:2234/2330 train_time:137270ms step_avg:61.45ms
step:2235/2330 train_time:137331ms step_avg:61.45ms
step:2236/2330 train_time:137394ms step_avg:61.45ms
step:2237/2330 train_time:137455ms step_avg:61.45ms
step:2238/2330 train_time:137519ms step_avg:61.45ms
step:2239/2330 train_time:137579ms step_avg:61.45ms
step:2240/2330 train_time:137643ms step_avg:61.45ms
step:2241/2330 train_time:137703ms step_avg:61.45ms
step:2242/2330 train_time:137766ms step_avg:61.45ms
step:2243/2330 train_time:137825ms step_avg:61.45ms
step:2244/2330 train_time:137889ms step_avg:61.45ms
step:2245/2330 train_time:137951ms step_avg:61.45ms
step:2246/2330 train_time:138014ms step_avg:61.45ms
step:2247/2330 train_time:138075ms step_avg:61.45ms
step:2248/2330 train_time:138139ms step_avg:61.45ms
step:2249/2330 train_time:138200ms step_avg:61.45ms
step:2250/2330 train_time:138263ms step_avg:61.45ms
step:2250/2330 val_loss:3.6439 train_time:138335ms step_avg:61.48ms
step:2251/2330 train_time:138356ms step_avg:61.46ms
step:2252/2330 train_time:138388ms step_avg:61.45ms
step:2253/2330 train_time:138451ms step_avg:61.45ms
step:2254/2330 train_time:138518ms step_avg:61.45ms
step:2255/2330 train_time:138579ms step_avg:61.45ms
step:2256/2330 train_time:138643ms step_avg:61.46ms
step:2257/2330 train_time:138704ms step_avg:61.45ms
step:2258/2330 train_time:138766ms step_avg:61.46ms
step:2259/2330 train_time:138825ms step_avg:61.45ms
step:2260/2330 train_time:138888ms step_avg:61.45ms
step:2261/2330 train_time:138948ms step_avg:61.45ms
step:2262/2330 train_time:139010ms step_avg:61.45ms
step:2263/2330 train_time:139070ms step_avg:61.45ms
step:2264/2330 train_time:139132ms step_avg:61.45ms
step:2265/2330 train_time:139192ms step_avg:61.45ms
step:2266/2330 train_time:139254ms step_avg:61.45ms
step:2267/2330 train_time:139317ms step_avg:61.45ms
step:2268/2330 train_time:139381ms step_avg:61.46ms
step:2269/2330 train_time:139443ms step_avg:61.46ms
step:2270/2330 train_time:139507ms step_avg:61.46ms
step:2271/2330 train_time:139569ms step_avg:61.46ms
step:2272/2330 train_time:139634ms step_avg:61.46ms
step:2273/2330 train_time:139694ms step_avg:61.46ms
step:2274/2330 train_time:139757ms step_avg:61.46ms
step:2275/2330 train_time:139816ms step_avg:61.46ms
step:2276/2330 train_time:139879ms step_avg:61.46ms
step:2277/2330 train_time:139938ms step_avg:61.46ms
step:2278/2330 train_time:140001ms step_avg:61.46ms
step:2279/2330 train_time:140062ms step_avg:61.46ms
step:2280/2330 train_time:140125ms step_avg:61.46ms
step:2281/2330 train_time:140185ms step_avg:61.46ms
step:2282/2330 train_time:140248ms step_avg:61.46ms
step:2283/2330 train_time:140309ms step_avg:61.46ms
step:2284/2330 train_time:140374ms step_avg:61.46ms
step:2285/2330 train_time:140434ms step_avg:61.46ms
step:2286/2330 train_time:140499ms step_avg:61.46ms
step:2287/2330 train_time:140559ms step_avg:61.46ms
step:2288/2330 train_time:140623ms step_avg:61.46ms
step:2289/2330 train_time:140684ms step_avg:61.46ms
step:2290/2330 train_time:140747ms step_avg:61.46ms
step:2291/2330 train_time:140808ms step_avg:61.46ms
step:2292/2330 train_time:140871ms step_avg:61.46ms
step:2293/2330 train_time:140932ms step_avg:61.46ms
step:2294/2330 train_time:140996ms step_avg:61.46ms
step:2295/2330 train_time:141057ms step_avg:61.46ms
step:2296/2330 train_time:141119ms step_avg:61.46ms
step:2297/2330 train_time:141179ms step_avg:61.46ms
step:2298/2330 train_time:141242ms step_avg:61.46ms
step:2299/2330 train_time:141302ms step_avg:61.46ms
step:2300/2330 train_time:141366ms step_avg:61.46ms
step:2301/2330 train_time:141427ms step_avg:61.46ms
step:2302/2330 train_time:141491ms step_avg:61.46ms
step:2303/2330 train_time:141553ms step_avg:61.46ms
step:2304/2330 train_time:141616ms step_avg:61.47ms
step:2305/2330 train_time:141677ms step_avg:61.47ms
step:2306/2330 train_time:141740ms step_avg:61.47ms
step:2307/2330 train_time:141800ms step_avg:61.46ms
step:2308/2330 train_time:141864ms step_avg:61.47ms
step:2309/2330 train_time:141924ms step_avg:61.47ms
step:2310/2330 train_time:141988ms step_avg:61.47ms
step:2311/2330 train_time:142048ms step_avg:61.47ms
step:2312/2330 train_time:142111ms step_avg:61.47ms
step:2313/2330 train_time:142171ms step_avg:61.47ms
step:2314/2330 train_time:142234ms step_avg:61.47ms
step:2315/2330 train_time:142294ms step_avg:61.47ms
step:2316/2330 train_time:142358ms step_avg:61.47ms
step:2317/2330 train_time:142418ms step_avg:61.47ms
step:2318/2330 train_time:142481ms step_avg:61.47ms
step:2319/2330 train_time:142540ms step_avg:61.47ms
step:2320/2330 train_time:142604ms step_avg:61.47ms
step:2321/2330 train_time:142665ms step_avg:61.47ms
step:2322/2330 train_time:142729ms step_avg:61.47ms
step:2323/2330 train_time:142790ms step_avg:61.47ms
step:2324/2330 train_time:142854ms step_avg:61.47ms
step:2325/2330 train_time:142915ms step_avg:61.47ms
step:2326/2330 train_time:142978ms step_avg:61.47ms
step:2327/2330 train_time:143038ms step_avg:61.47ms
step:2328/2330 train_time:143100ms step_avg:61.47ms
step:2329/2330 train_time:143160ms step_avg:61.47ms
step:2330/2330 train_time:143223ms step_avg:61.47ms
step:2330/2330 val_loss:3.6301 train_time:143298ms step_avg:61.50ms
peak memory allocated: 29226 MiB reserved: 44076 MiB
