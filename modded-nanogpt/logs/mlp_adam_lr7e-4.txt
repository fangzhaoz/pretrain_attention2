import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_adam_lr7e-4"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-1, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizer2 = torch.optim.Adam(
    hidden_matrix_params + gate_params,
    lr=7e-4,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 04:33:52 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   34C    P0             113W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   33C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   30C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   33C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   30C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   34C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.06ms
step:1/2330 train_time:71ms step_avg:71.03ms
step:2/2330 train_time:160ms step_avg:79.90ms
step:3/2330 train_time:175ms step_avg:58.43ms
step:4/2330 train_time:187ms step_avg:46.84ms
step:5/2330 train_time:197ms step_avg:39.46ms
step:6/2330 train_time:208ms step_avg:34.68ms
step:7/2330 train_time:235ms step_avg:33.57ms
step:8/2330 train_time:276ms step_avg:34.44ms
step:9/2330 train_time:310ms step_avg:34.40ms
step:10/2330 train_time:350ms step_avg:35.02ms
step:11/2330 train_time:384ms step_avg:34.94ms
step:12/2330 train_time:424ms step_avg:35.37ms
step:13/2330 train_time:459ms step_avg:35.31ms
step:14/2330 train_time:500ms step_avg:35.68ms
step:15/2330 train_time:534ms step_avg:35.60ms
step:16/2330 train_time:574ms step_avg:35.90ms
step:17/2330 train_time:609ms step_avg:35.82ms
step:18/2330 train_time:649ms step_avg:36.07ms
step:19/2330 train_time:684ms step_avg:36.01ms
step:20/2330 train_time:724ms step_avg:36.22ms
step:21/2330 train_time:759ms step_avg:36.14ms
step:22/2330 train_time:800ms step_avg:36.34ms
step:23/2330 train_time:834ms step_avg:36.26ms
step:24/2330 train_time:875ms step_avg:36.44ms
step:25/2330 train_time:909ms step_avg:36.35ms
step:26/2330 train_time:950ms step_avg:36.52ms
step:27/2330 train_time:985ms step_avg:36.49ms
step:28/2330 train_time:1025ms step_avg:36.62ms
step:29/2330 train_time:1067ms step_avg:36.79ms
step:30/2330 train_time:1109ms step_avg:36.98ms
step:31/2330 train_time:1151ms step_avg:37.13ms
step:32/2330 train_time:1192ms step_avg:37.26ms
step:33/2330 train_time:1229ms step_avg:37.23ms
step:34/2330 train_time:1270ms step_avg:37.34ms
step:35/2330 train_time:1305ms step_avg:37.29ms
step:36/2330 train_time:1346ms step_avg:37.39ms
step:37/2330 train_time:1382ms step_avg:37.34ms
step:38/2330 train_time:1422ms step_avg:37.43ms
step:39/2330 train_time:1457ms step_avg:37.36ms
step:40/2330 train_time:1498ms step_avg:37.44ms
step:41/2330 train_time:1532ms step_avg:37.38ms
step:42/2330 train_time:1573ms step_avg:37.46ms
step:43/2330 train_time:1608ms step_avg:37.39ms
step:44/2330 train_time:1648ms step_avg:37.47ms
step:45/2330 train_time:1683ms step_avg:37.40ms
step:46/2330 train_time:1724ms step_avg:37.47ms
step:47/2330 train_time:1758ms step_avg:37.41ms
step:48/2330 train_time:1799ms step_avg:37.48ms
step:49/2330 train_time:1833ms step_avg:37.41ms
step:50/2330 train_time:1874ms step_avg:37.49ms
step:51/2330 train_time:1909ms step_avg:37.43ms
step:52/2330 train_time:1950ms step_avg:37.50ms
step:53/2330 train_time:1987ms step_avg:37.49ms
step:54/2330 train_time:2028ms step_avg:37.55ms
step:55/2330 train_time:2065ms step_avg:37.55ms
step:56/2330 train_time:2106ms step_avg:37.61ms
step:57/2330 train_time:2144ms step_avg:37.62ms
step:58/2330 train_time:2186ms step_avg:37.68ms
step:59/2330 train_time:2222ms step_avg:37.66ms
step:60/2330 train_time:2263ms step_avg:37.71ms
step:61/2330 train_time:2299ms step_avg:37.69ms
step:62/2330 train_time:2340ms step_avg:37.74ms
step:63/2330 train_time:2375ms step_avg:37.70ms
step:64/2330 train_time:2416ms step_avg:37.74ms
step:65/2330 train_time:2452ms step_avg:37.72ms
step:66/2330 train_time:2492ms step_avg:37.76ms
step:67/2330 train_time:2528ms step_avg:37.73ms
step:68/2330 train_time:2569ms step_avg:37.77ms
step:69/2330 train_time:2604ms step_avg:37.74ms
step:70/2330 train_time:2645ms step_avg:37.78ms
step:71/2330 train_time:2679ms step_avg:37.74ms
step:72/2330 train_time:2720ms step_avg:37.78ms
step:73/2330 train_time:2754ms step_avg:37.73ms
step:74/2330 train_time:2795ms step_avg:37.77ms
step:75/2330 train_time:2830ms step_avg:37.73ms
step:76/2330 train_time:2871ms step_avg:37.78ms
step:77/2330 train_time:2905ms step_avg:37.73ms
step:78/2330 train_time:2947ms step_avg:37.78ms
step:79/2330 train_time:2983ms step_avg:37.75ms
step:80/2330 train_time:3024ms step_avg:37.80ms
step:81/2330 train_time:3061ms step_avg:37.79ms
step:82/2330 train_time:3103ms step_avg:37.84ms
step:83/2330 train_time:3138ms step_avg:37.81ms
step:84/2330 train_time:3180ms step_avg:37.86ms
step:85/2330 train_time:3216ms step_avg:37.83ms
step:86/2330 train_time:3257ms step_avg:37.87ms
step:87/2330 train_time:3293ms step_avg:37.85ms
step:88/2330 train_time:3335ms step_avg:37.90ms
step:89/2330 train_time:3370ms step_avg:37.87ms
step:90/2330 train_time:3411ms step_avg:37.90ms
step:91/2330 train_time:3446ms step_avg:37.87ms
step:92/2330 train_time:3487ms step_avg:37.90ms
step:93/2330 train_time:3523ms step_avg:37.88ms
step:94/2330 train_time:3563ms step_avg:37.90ms
step:95/2330 train_time:3599ms step_avg:37.88ms
step:96/2330 train_time:3639ms step_avg:37.91ms
step:97/2330 train_time:3674ms step_avg:37.87ms
step:98/2330 train_time:3715ms step_avg:37.91ms
step:99/2330 train_time:3749ms step_avg:37.87ms
step:100/2330 train_time:3790ms step_avg:37.90ms
step:101/2330 train_time:3825ms step_avg:37.87ms
step:102/2330 train_time:3865ms step_avg:37.90ms
step:103/2330 train_time:3901ms step_avg:37.87ms
step:104/2330 train_time:3942ms step_avg:37.90ms
step:105/2330 train_time:3978ms step_avg:37.89ms
step:106/2330 train_time:4020ms step_avg:37.92ms
step:107/2330 train_time:4056ms step_avg:37.91ms
step:108/2330 train_time:4097ms step_avg:37.94ms
step:109/2330 train_time:4134ms step_avg:37.93ms
step:110/2330 train_time:4176ms step_avg:37.96ms
step:111/2330 train_time:4212ms step_avg:37.94ms
step:112/2330 train_time:4253ms step_avg:37.97ms
step:113/2330 train_time:4288ms step_avg:37.95ms
step:114/2330 train_time:4330ms step_avg:37.98ms
step:115/2330 train_time:4365ms step_avg:37.95ms
step:116/2330 train_time:4406ms step_avg:37.98ms
step:117/2330 train_time:4440ms step_avg:37.95ms
step:118/2330 train_time:4482ms step_avg:37.98ms
step:119/2330 train_time:4517ms step_avg:37.96ms
step:120/2330 train_time:4558ms step_avg:37.99ms
step:121/2330 train_time:4593ms step_avg:37.96ms
step:122/2330 train_time:4634ms step_avg:37.98ms
step:123/2330 train_time:4669ms step_avg:37.96ms
step:124/2330 train_time:4710ms step_avg:37.99ms
step:125/2330 train_time:4745ms step_avg:37.96ms
step:126/2330 train_time:4786ms step_avg:37.98ms
step:127/2330 train_time:4821ms step_avg:37.96ms
step:128/2330 train_time:4861ms step_avg:37.98ms
step:129/2330 train_time:4897ms step_avg:37.96ms
step:130/2330 train_time:4938ms step_avg:37.98ms
step:131/2330 train_time:4974ms step_avg:37.97ms
step:132/2330 train_time:5015ms step_avg:37.99ms
step:133/2330 train_time:5051ms step_avg:37.98ms
step:134/2330 train_time:5093ms step_avg:38.01ms
step:135/2330 train_time:5129ms step_avg:37.99ms
step:136/2330 train_time:5170ms step_avg:38.02ms
step:137/2330 train_time:5205ms step_avg:37.99ms
step:138/2330 train_time:5246ms step_avg:38.01ms
step:139/2330 train_time:5283ms step_avg:38.00ms
step:140/2330 train_time:5324ms step_avg:38.03ms
step:141/2330 train_time:5359ms step_avg:38.01ms
step:142/2330 train_time:5400ms step_avg:38.03ms
step:143/2330 train_time:5436ms step_avg:38.01ms
step:144/2330 train_time:5477ms step_avg:38.04ms
step:145/2330 train_time:5512ms step_avg:38.01ms
step:146/2330 train_time:5553ms step_avg:38.04ms
step:147/2330 train_time:5588ms step_avg:38.02ms
step:148/2330 train_time:5629ms step_avg:38.04ms
step:149/2330 train_time:5664ms step_avg:38.01ms
step:150/2330 train_time:5705ms step_avg:38.03ms
step:151/2330 train_time:5740ms step_avg:38.01ms
step:152/2330 train_time:5781ms step_avg:38.04ms
step:153/2330 train_time:5816ms step_avg:38.01ms
step:154/2330 train_time:5857ms step_avg:38.03ms
step:155/2330 train_time:5892ms step_avg:38.01ms
step:156/2330 train_time:5933ms step_avg:38.03ms
step:157/2330 train_time:5969ms step_avg:38.02ms
step:158/2330 train_time:6010ms step_avg:38.04ms
step:159/2330 train_time:6046ms step_avg:38.02ms
step:160/2330 train_time:6087ms step_avg:38.04ms
step:161/2330 train_time:6123ms step_avg:38.03ms
step:162/2330 train_time:6164ms step_avg:38.05ms
step:163/2330 train_time:6199ms step_avg:38.03ms
step:164/2330 train_time:6240ms step_avg:38.05ms
step:165/2330 train_time:6275ms step_avg:38.03ms
step:166/2330 train_time:6317ms step_avg:38.05ms
step:167/2330 train_time:6352ms step_avg:38.04ms
step:168/2330 train_time:6393ms step_avg:38.05ms
step:169/2330 train_time:6428ms step_avg:38.04ms
step:170/2330 train_time:6470ms step_avg:38.06ms
step:171/2330 train_time:6505ms step_avg:38.04ms
step:172/2330 train_time:6546ms step_avg:38.06ms
step:173/2330 train_time:6582ms step_avg:38.05ms
step:174/2330 train_time:6623ms step_avg:38.06ms
step:175/2330 train_time:6659ms step_avg:38.05ms
step:176/2330 train_time:6700ms step_avg:38.07ms
step:177/2330 train_time:6734ms step_avg:38.05ms
step:178/2330 train_time:6775ms step_avg:38.06ms
step:179/2330 train_time:6810ms step_avg:38.05ms
step:180/2330 train_time:6851ms step_avg:38.06ms
step:181/2330 train_time:6887ms step_avg:38.05ms
step:182/2330 train_time:6928ms step_avg:38.07ms
step:183/2330 train_time:6964ms step_avg:38.05ms
step:184/2330 train_time:7004ms step_avg:38.07ms
step:185/2330 train_time:7041ms step_avg:38.06ms
step:186/2330 train_time:7082ms step_avg:38.07ms
step:187/2330 train_time:7117ms step_avg:38.06ms
step:188/2330 train_time:7158ms step_avg:38.07ms
step:189/2330 train_time:7194ms step_avg:38.06ms
step:190/2330 train_time:7235ms step_avg:38.08ms
step:191/2330 train_time:7271ms step_avg:38.07ms
step:192/2330 train_time:7312ms step_avg:38.08ms
step:193/2330 train_time:7347ms step_avg:38.07ms
step:194/2330 train_time:7388ms step_avg:38.08ms
step:195/2330 train_time:7424ms step_avg:38.07ms
step:196/2330 train_time:7465ms step_avg:38.09ms
step:197/2330 train_time:7500ms step_avg:38.07ms
step:198/2330 train_time:7541ms step_avg:38.09ms
step:199/2330 train_time:7577ms step_avg:38.07ms
step:200/2330 train_time:7618ms step_avg:38.09ms
step:201/2330 train_time:7653ms step_avg:38.07ms
step:202/2330 train_time:7694ms step_avg:38.09ms
step:203/2330 train_time:7729ms step_avg:38.07ms
step:204/2330 train_time:7770ms step_avg:38.09ms
step:205/2330 train_time:7805ms step_avg:38.07ms
step:206/2330 train_time:7845ms step_avg:38.08ms
step:207/2330 train_time:7880ms step_avg:38.07ms
step:208/2330 train_time:7921ms step_avg:38.08ms
step:209/2330 train_time:7956ms step_avg:38.07ms
step:210/2330 train_time:7997ms step_avg:38.08ms
step:211/2330 train_time:8034ms step_avg:38.08ms
step:212/2330 train_time:8075ms step_avg:38.09ms
step:213/2330 train_time:8110ms step_avg:38.08ms
step:214/2330 train_time:8151ms step_avg:38.09ms
step:215/2330 train_time:8188ms step_avg:38.08ms
step:216/2330 train_time:8228ms step_avg:38.09ms
step:217/2330 train_time:8265ms step_avg:38.09ms
step:218/2330 train_time:8305ms step_avg:38.10ms
step:219/2330 train_time:8341ms step_avg:38.09ms
step:220/2330 train_time:8382ms step_avg:38.10ms
step:221/2330 train_time:8417ms step_avg:38.09ms
step:222/2330 train_time:8458ms step_avg:38.10ms
step:223/2330 train_time:8494ms step_avg:38.09ms
step:224/2330 train_time:8535ms step_avg:38.10ms
step:225/2330 train_time:8570ms step_avg:38.09ms
step:226/2330 train_time:8611ms step_avg:38.10ms
step:227/2330 train_time:8647ms step_avg:38.09ms
step:228/2330 train_time:8687ms step_avg:38.10ms
step:229/2330 train_time:8724ms step_avg:38.10ms
step:230/2330 train_time:8764ms step_avg:38.11ms
step:231/2330 train_time:8800ms step_avg:38.09ms
step:232/2330 train_time:8840ms step_avg:38.11ms
step:233/2330 train_time:8876ms step_avg:38.09ms
step:234/2330 train_time:8916ms step_avg:38.10ms
step:235/2330 train_time:8951ms step_avg:38.09ms
step:236/2330 train_time:8992ms step_avg:38.10ms
step:237/2330 train_time:9027ms step_avg:38.09ms
step:238/2330 train_time:9068ms step_avg:38.10ms
step:239/2330 train_time:9104ms step_avg:38.09ms
step:240/2330 train_time:9144ms step_avg:38.10ms
step:241/2330 train_time:9181ms step_avg:38.09ms
step:242/2330 train_time:9222ms step_avg:38.11ms
step:243/2330 train_time:9257ms step_avg:38.10ms
step:244/2330 train_time:9299ms step_avg:38.11ms
step:245/2330 train_time:9334ms step_avg:38.10ms
step:246/2330 train_time:9375ms step_avg:38.11ms
step:247/2330 train_time:9410ms step_avg:38.10ms
step:248/2330 train_time:9451ms step_avg:38.11ms
step:249/2330 train_time:9486ms step_avg:38.10ms
step:250/2330 train_time:9527ms step_avg:38.11ms
step:250/2330 val_loss:5.5504 train_time:9639ms step_avg:38.55ms
step:251/2330 train_time:9650ms step_avg:38.45ms
step:252/2330 train_time:9662ms step_avg:38.34ms
step:253/2330 train_time:9672ms step_avg:38.23ms
step:254/2330 train_time:9683ms step_avg:38.12ms
step:255/2330 train_time:9716ms step_avg:38.10ms
step:256/2330 train_time:9756ms step_avg:38.11ms
step:257/2330 train_time:9791ms step_avg:38.10ms
step:258/2330 train_time:9831ms step_avg:38.11ms
step:259/2330 train_time:9866ms step_avg:38.09ms
step:260/2330 train_time:9906ms step_avg:38.10ms
step:261/2330 train_time:9942ms step_avg:38.09ms
step:262/2330 train_time:9984ms step_avg:38.11ms
step:263/2330 train_time:10024ms step_avg:38.11ms
step:264/2330 train_time:10065ms step_avg:38.12ms
step:265/2330 train_time:10103ms step_avg:38.12ms
step:266/2330 train_time:10143ms step_avg:38.13ms
step:267/2330 train_time:10178ms step_avg:38.12ms
step:268/2330 train_time:10219ms step_avg:38.13ms
step:269/2330 train_time:10256ms step_avg:38.13ms
step:270/2330 train_time:10296ms step_avg:38.13ms
step:271/2330 train_time:10332ms step_avg:38.12ms
step:272/2330 train_time:10372ms step_avg:38.13ms
step:273/2330 train_time:10408ms step_avg:38.12ms
step:274/2330 train_time:10448ms step_avg:38.13ms
step:275/2330 train_time:10484ms step_avg:38.12ms
step:276/2330 train_time:10525ms step_avg:38.14ms
step:277/2330 train_time:10561ms step_avg:38.13ms
step:278/2330 train_time:10603ms step_avg:38.14ms
step:279/2330 train_time:10637ms step_avg:38.13ms
step:280/2330 train_time:10678ms step_avg:38.14ms
step:281/2330 train_time:10713ms step_avg:38.13ms
step:282/2330 train_time:10755ms step_avg:38.14ms
step:283/2330 train_time:10789ms step_avg:38.12ms
step:284/2330 train_time:10830ms step_avg:38.13ms
step:285/2330 train_time:10865ms step_avg:38.12ms
step:286/2330 train_time:10906ms step_avg:38.13ms
step:287/2330 train_time:10942ms step_avg:38.13ms
step:288/2330 train_time:10984ms step_avg:38.14ms
step:289/2330 train_time:11020ms step_avg:38.13ms
step:290/2330 train_time:11060ms step_avg:38.14ms
step:291/2330 train_time:11097ms step_avg:38.13ms
step:292/2330 train_time:11138ms step_avg:38.15ms
step:293/2330 train_time:11175ms step_avg:38.14ms
step:294/2330 train_time:11215ms step_avg:38.15ms
step:295/2330 train_time:11252ms step_avg:38.14ms
step:296/2330 train_time:11292ms step_avg:38.15ms
step:297/2330 train_time:11327ms step_avg:38.14ms
step:298/2330 train_time:11368ms step_avg:38.15ms
step:299/2330 train_time:11403ms step_avg:38.14ms
step:300/2330 train_time:11444ms step_avg:38.15ms
step:301/2330 train_time:11479ms step_avg:38.14ms
step:302/2330 train_time:11521ms step_avg:38.15ms
step:303/2330 train_time:11556ms step_avg:38.14ms
step:304/2330 train_time:11598ms step_avg:38.15ms
step:305/2330 train_time:11633ms step_avg:38.14ms
step:306/2330 train_time:11674ms step_avg:38.15ms
step:307/2330 train_time:11710ms step_avg:38.14ms
step:308/2330 train_time:11751ms step_avg:38.15ms
step:309/2330 train_time:11786ms step_avg:38.14ms
step:310/2330 train_time:11827ms step_avg:38.15ms
step:311/2330 train_time:11863ms step_avg:38.15ms
step:312/2330 train_time:11904ms step_avg:38.16ms
step:313/2330 train_time:11941ms step_avg:38.15ms
step:314/2330 train_time:11981ms step_avg:38.16ms
step:315/2330 train_time:12018ms step_avg:38.15ms
step:316/2330 train_time:12058ms step_avg:38.16ms
step:317/2330 train_time:12095ms step_avg:38.16ms
step:318/2330 train_time:12136ms step_avg:38.16ms
step:319/2330 train_time:12172ms step_avg:38.16ms
step:320/2330 train_time:12213ms step_avg:38.17ms
step:321/2330 train_time:12248ms step_avg:38.16ms
step:322/2330 train_time:12289ms step_avg:38.16ms
step:323/2330 train_time:12325ms step_avg:38.16ms
step:324/2330 train_time:12365ms step_avg:38.16ms
step:325/2330 train_time:12401ms step_avg:38.16ms
step:326/2330 train_time:12442ms step_avg:38.16ms
step:327/2330 train_time:12477ms step_avg:38.16ms
step:328/2330 train_time:12518ms step_avg:38.17ms
step:329/2330 train_time:12554ms step_avg:38.16ms
step:330/2330 train_time:12595ms step_avg:38.17ms
step:331/2330 train_time:12630ms step_avg:38.16ms
step:332/2330 train_time:12672ms step_avg:38.17ms
step:333/2330 train_time:12707ms step_avg:38.16ms
step:334/2330 train_time:12748ms step_avg:38.17ms
step:335/2330 train_time:12783ms step_avg:38.16ms
step:336/2330 train_time:12825ms step_avg:38.17ms
step:337/2330 train_time:12860ms step_avg:38.16ms
step:338/2330 train_time:12901ms step_avg:38.17ms
step:339/2330 train_time:12937ms step_avg:38.16ms
step:340/2330 train_time:12979ms step_avg:38.17ms
step:341/2330 train_time:13015ms step_avg:38.17ms
step:342/2330 train_time:13056ms step_avg:38.17ms
step:343/2330 train_time:13092ms step_avg:38.17ms
step:344/2330 train_time:13133ms step_avg:38.18ms
step:345/2330 train_time:13170ms step_avg:38.17ms
step:346/2330 train_time:13210ms step_avg:38.18ms
step:347/2330 train_time:13247ms step_avg:38.18ms
step:348/2330 train_time:13288ms step_avg:38.18ms
step:349/2330 train_time:13324ms step_avg:38.18ms
step:350/2330 train_time:13364ms step_avg:38.18ms
step:351/2330 train_time:13400ms step_avg:38.18ms
step:352/2330 train_time:13440ms step_avg:38.18ms
step:353/2330 train_time:13476ms step_avg:38.18ms
step:354/2330 train_time:13516ms step_avg:38.18ms
step:355/2330 train_time:13552ms step_avg:38.17ms
step:356/2330 train_time:13593ms step_avg:38.18ms
step:357/2330 train_time:13628ms step_avg:38.17ms
step:358/2330 train_time:13669ms step_avg:38.18ms
step:359/2330 train_time:13705ms step_avg:38.18ms
step:360/2330 train_time:13746ms step_avg:38.18ms
step:361/2330 train_time:13781ms step_avg:38.17ms
step:362/2330 train_time:13823ms step_avg:38.18ms
step:363/2330 train_time:13858ms step_avg:38.18ms
step:364/2330 train_time:13899ms step_avg:38.18ms
step:365/2330 train_time:13935ms step_avg:38.18ms
step:366/2330 train_time:13976ms step_avg:38.19ms
step:367/2330 train_time:14011ms step_avg:38.18ms
step:368/2330 train_time:14052ms step_avg:38.19ms
step:369/2330 train_time:14088ms step_avg:38.18ms
step:370/2330 train_time:14129ms step_avg:38.19ms
step:371/2330 train_time:14165ms step_avg:38.18ms
step:372/2330 train_time:14206ms step_avg:38.19ms
step:373/2330 train_time:14241ms step_avg:38.18ms
step:374/2330 train_time:14283ms step_avg:38.19ms
step:375/2330 train_time:14318ms step_avg:38.18ms
step:376/2330 train_time:14359ms step_avg:38.19ms
step:377/2330 train_time:14395ms step_avg:38.18ms
step:378/2330 train_time:14436ms step_avg:38.19ms
step:379/2330 train_time:14472ms step_avg:38.18ms
step:380/2330 train_time:14512ms step_avg:38.19ms
step:381/2330 train_time:14548ms step_avg:38.18ms
step:382/2330 train_time:14588ms step_avg:38.19ms
step:383/2330 train_time:14623ms step_avg:38.18ms
step:384/2330 train_time:14664ms step_avg:38.19ms
step:385/2330 train_time:14699ms step_avg:38.18ms
step:386/2330 train_time:14740ms step_avg:38.19ms
step:387/2330 train_time:14776ms step_avg:38.18ms
step:388/2330 train_time:14817ms step_avg:38.19ms
step:389/2330 train_time:14853ms step_avg:38.18ms
step:390/2330 train_time:14894ms step_avg:38.19ms
step:391/2330 train_time:14929ms step_avg:38.18ms
step:392/2330 train_time:14970ms step_avg:38.19ms
step:393/2330 train_time:15006ms step_avg:38.18ms
step:394/2330 train_time:15047ms step_avg:38.19ms
step:395/2330 train_time:15083ms step_avg:38.18ms
step:396/2330 train_time:15124ms step_avg:38.19ms
step:397/2330 train_time:15160ms step_avg:38.19ms
step:398/2330 train_time:15201ms step_avg:38.19ms
step:399/2330 train_time:15236ms step_avg:38.19ms
step:400/2330 train_time:15278ms step_avg:38.19ms
step:401/2330 train_time:15313ms step_avg:38.19ms
step:402/2330 train_time:15354ms step_avg:38.19ms
step:403/2330 train_time:15390ms step_avg:38.19ms
step:404/2330 train_time:15431ms step_avg:38.19ms
step:405/2330 train_time:15467ms step_avg:38.19ms
step:406/2330 train_time:15508ms step_avg:38.20ms
step:407/2330 train_time:15543ms step_avg:38.19ms
step:408/2330 train_time:15585ms step_avg:38.20ms
step:409/2330 train_time:15619ms step_avg:38.19ms
step:410/2330 train_time:15660ms step_avg:38.19ms
step:411/2330 train_time:15696ms step_avg:38.19ms
step:412/2330 train_time:15737ms step_avg:38.20ms
step:413/2330 train_time:15772ms step_avg:38.19ms
step:414/2330 train_time:15813ms step_avg:38.20ms
step:415/2330 train_time:15850ms step_avg:38.19ms
step:416/2330 train_time:15890ms step_avg:38.20ms
step:417/2330 train_time:15927ms step_avg:38.19ms
step:418/2330 train_time:15967ms step_avg:38.20ms
step:419/2330 train_time:16004ms step_avg:38.19ms
step:420/2330 train_time:16045ms step_avg:38.20ms
step:421/2330 train_time:16080ms step_avg:38.19ms
step:422/2330 train_time:16121ms step_avg:38.20ms
step:423/2330 train_time:16157ms step_avg:38.20ms
step:424/2330 train_time:16198ms step_avg:38.20ms
step:425/2330 train_time:16234ms step_avg:38.20ms
step:426/2330 train_time:16275ms step_avg:38.20ms
step:427/2330 train_time:16310ms step_avg:38.20ms
step:428/2330 train_time:16351ms step_avg:38.20ms
step:429/2330 train_time:16387ms step_avg:38.20ms
step:430/2330 train_time:16428ms step_avg:38.20ms
step:431/2330 train_time:16463ms step_avg:38.20ms
step:432/2330 train_time:16504ms step_avg:38.20ms
step:433/2330 train_time:16539ms step_avg:38.20ms
step:434/2330 train_time:16580ms step_avg:38.20ms
step:435/2330 train_time:16615ms step_avg:38.20ms
step:436/2330 train_time:16656ms step_avg:38.20ms
step:437/2330 train_time:16693ms step_avg:38.20ms
step:438/2330 train_time:16734ms step_avg:38.20ms
step:439/2330 train_time:16770ms step_avg:38.20ms
step:440/2330 train_time:16811ms step_avg:38.21ms
step:441/2330 train_time:16846ms step_avg:38.20ms
step:442/2330 train_time:16888ms step_avg:38.21ms
step:443/2330 train_time:16923ms step_avg:38.20ms
step:444/2330 train_time:16964ms step_avg:38.21ms
step:445/2330 train_time:16999ms step_avg:38.20ms
step:446/2330 train_time:17040ms step_avg:38.21ms
step:447/2330 train_time:17076ms step_avg:38.20ms
step:448/2330 train_time:17117ms step_avg:38.21ms
step:449/2330 train_time:17153ms step_avg:38.20ms
step:450/2330 train_time:17194ms step_avg:38.21ms
step:451/2330 train_time:17229ms step_avg:38.20ms
step:452/2330 train_time:17270ms step_avg:38.21ms
step:453/2330 train_time:17306ms step_avg:38.20ms
step:454/2330 train_time:17347ms step_avg:38.21ms
step:455/2330 train_time:17383ms step_avg:38.20ms
step:456/2330 train_time:17424ms step_avg:38.21ms
step:457/2330 train_time:17459ms step_avg:38.20ms
step:458/2330 train_time:17500ms step_avg:38.21ms
step:459/2330 train_time:17536ms step_avg:38.21ms
step:460/2330 train_time:17577ms step_avg:38.21ms
step:461/2330 train_time:17613ms step_avg:38.21ms
step:462/2330 train_time:17654ms step_avg:38.21ms
step:463/2330 train_time:17690ms step_avg:38.21ms
step:464/2330 train_time:17730ms step_avg:38.21ms
step:465/2330 train_time:17766ms step_avg:38.21ms
step:466/2330 train_time:17807ms step_avg:38.21ms
step:467/2330 train_time:17843ms step_avg:38.21ms
step:468/2330 train_time:17885ms step_avg:38.22ms
step:469/2330 train_time:17920ms step_avg:38.21ms
step:470/2330 train_time:17961ms step_avg:38.22ms
step:471/2330 train_time:17996ms step_avg:38.21ms
step:472/2330 train_time:18038ms step_avg:38.22ms
step:473/2330 train_time:18074ms step_avg:38.21ms
step:474/2330 train_time:18115ms step_avg:38.22ms
step:475/2330 train_time:18151ms step_avg:38.21ms
step:476/2330 train_time:18192ms step_avg:38.22ms
step:477/2330 train_time:18228ms step_avg:38.21ms
step:478/2330 train_time:18268ms step_avg:38.22ms
step:479/2330 train_time:18304ms step_avg:38.21ms
step:480/2330 train_time:18345ms step_avg:38.22ms
step:481/2330 train_time:18380ms step_avg:38.21ms
step:482/2330 train_time:18421ms step_avg:38.22ms
step:483/2330 train_time:18457ms step_avg:38.21ms
step:484/2330 train_time:18499ms step_avg:38.22ms
step:485/2330 train_time:18534ms step_avg:38.21ms
step:486/2330 train_time:18575ms step_avg:38.22ms
step:487/2330 train_time:18611ms step_avg:38.22ms
step:488/2330 train_time:18652ms step_avg:38.22ms
step:489/2330 train_time:18689ms step_avg:38.22ms
step:490/2330 train_time:18729ms step_avg:38.22ms
step:491/2330 train_time:18765ms step_avg:38.22ms
step:492/2330 train_time:18806ms step_avg:38.22ms
step:493/2330 train_time:18841ms step_avg:38.22ms
step:494/2330 train_time:18883ms step_avg:38.22ms
step:495/2330 train_time:18918ms step_avg:38.22ms
step:496/2330 train_time:18959ms step_avg:38.22ms
step:497/2330 train_time:18995ms step_avg:38.22ms
step:498/2330 train_time:19036ms step_avg:38.22ms
step:499/2330 train_time:19071ms step_avg:38.22ms
step:500/2330 train_time:19112ms step_avg:38.22ms
step:500/2330 val_loss:5.4144 train_time:19226ms step_avg:38.45ms
step:501/2330 train_time:19238ms step_avg:38.40ms
step:502/2330 train_time:19249ms step_avg:38.34ms
step:503/2330 train_time:19260ms step_avg:38.29ms
step:504/2330 train_time:19271ms step_avg:38.24ms
step:505/2330 train_time:19303ms step_avg:38.22ms
step:506/2330 train_time:19344ms step_avg:38.23ms
step:507/2330 train_time:19379ms step_avg:38.22ms
step:508/2330 train_time:19419ms step_avg:38.23ms
step:509/2330 train_time:19454ms step_avg:38.22ms
step:510/2330 train_time:19495ms step_avg:38.23ms
step:511/2330 train_time:19533ms step_avg:38.22ms
step:512/2330 train_time:19576ms step_avg:38.23ms
step:513/2330 train_time:19613ms step_avg:38.23ms
step:514/2330 train_time:19654ms step_avg:38.24ms
step:515/2330 train_time:19691ms step_avg:38.23ms
step:516/2330 train_time:19732ms step_avg:38.24ms
step:517/2330 train_time:19767ms step_avg:38.23ms
step:518/2330 train_time:19808ms step_avg:38.24ms
step:519/2330 train_time:19844ms step_avg:38.23ms
step:520/2330 train_time:19885ms step_avg:38.24ms
step:521/2330 train_time:19919ms step_avg:38.23ms
step:522/2330 train_time:19960ms step_avg:38.24ms
step:523/2330 train_time:19995ms step_avg:38.23ms
step:524/2330 train_time:20036ms step_avg:38.24ms
step:525/2330 train_time:20072ms step_avg:38.23ms
step:526/2330 train_time:20112ms step_avg:38.24ms
step:527/2330 train_time:20149ms step_avg:38.23ms
step:528/2330 train_time:20191ms step_avg:38.24ms
step:529/2330 train_time:20227ms step_avg:38.24ms
step:530/2330 train_time:20268ms step_avg:38.24ms
step:531/2330 train_time:20303ms step_avg:38.24ms
step:532/2330 train_time:20344ms step_avg:38.24ms
step:533/2330 train_time:20380ms step_avg:38.24ms
step:534/2330 train_time:20421ms step_avg:38.24ms
step:535/2330 train_time:20457ms step_avg:38.24ms
step:536/2330 train_time:20499ms step_avg:38.24ms
step:537/2330 train_time:20535ms step_avg:38.24ms
step:538/2330 train_time:20577ms step_avg:38.25ms
step:539/2330 train_time:20614ms step_avg:38.24ms
step:540/2330 train_time:20655ms step_avg:38.25ms
step:541/2330 train_time:20691ms step_avg:38.25ms
step:542/2330 train_time:20732ms step_avg:38.25ms
step:543/2330 train_time:20768ms step_avg:38.25ms
step:544/2330 train_time:20809ms step_avg:38.25ms
step:545/2330 train_time:20844ms step_avg:38.25ms
step:546/2330 train_time:20884ms step_avg:38.25ms
step:547/2330 train_time:20920ms step_avg:38.25ms
step:548/2330 train_time:20961ms step_avg:38.25ms
step:549/2330 train_time:20997ms step_avg:38.25ms
step:550/2330 train_time:21038ms step_avg:38.25ms
step:551/2330 train_time:21073ms step_avg:38.25ms
step:552/2330 train_time:21114ms step_avg:38.25ms
step:553/2330 train_time:21151ms step_avg:38.25ms
step:554/2330 train_time:21192ms step_avg:38.25ms
step:555/2330 train_time:21230ms step_avg:38.25ms
step:556/2330 train_time:21270ms step_avg:38.26ms
step:557/2330 train_time:21306ms step_avg:38.25ms
step:558/2330 train_time:21347ms step_avg:38.26ms
step:559/2330 train_time:21383ms step_avg:38.25ms
step:560/2330 train_time:21423ms step_avg:38.26ms
step:561/2330 train_time:21459ms step_avg:38.25ms
step:562/2330 train_time:21500ms step_avg:38.26ms
step:563/2330 train_time:21536ms step_avg:38.25ms
step:564/2330 train_time:21578ms step_avg:38.26ms
step:565/2330 train_time:21613ms step_avg:38.25ms
step:566/2330 train_time:21654ms step_avg:38.26ms
step:567/2330 train_time:21690ms step_avg:38.25ms
step:568/2330 train_time:21731ms step_avg:38.26ms
step:569/2330 train_time:21767ms step_avg:38.25ms
step:570/2330 train_time:21808ms step_avg:38.26ms
step:571/2330 train_time:21843ms step_avg:38.25ms
step:572/2330 train_time:21884ms step_avg:38.26ms
step:573/2330 train_time:21920ms step_avg:38.25ms
step:574/2330 train_time:21961ms step_avg:38.26ms
step:575/2330 train_time:21996ms step_avg:38.25ms
step:576/2330 train_time:22037ms step_avg:38.26ms
step:577/2330 train_time:22073ms step_avg:38.26ms
step:578/2330 train_time:22114ms step_avg:38.26ms
step:579/2330 train_time:22151ms step_avg:38.26ms
step:580/2330 train_time:22192ms step_avg:38.26ms
step:581/2330 train_time:22228ms step_avg:38.26ms
step:582/2330 train_time:22269ms step_avg:38.26ms
step:583/2330 train_time:22304ms step_avg:38.26ms
step:584/2330 train_time:22345ms step_avg:38.26ms
step:585/2330 train_time:22381ms step_avg:38.26ms
step:586/2330 train_time:22422ms step_avg:38.26ms
step:587/2330 train_time:22458ms step_avg:38.26ms
step:588/2330 train_time:22500ms step_avg:38.26ms
step:589/2330 train_time:22535ms step_avg:38.26ms
step:590/2330 train_time:22576ms step_avg:38.27ms
step:591/2330 train_time:22613ms step_avg:38.26ms
step:592/2330 train_time:22654ms step_avg:38.27ms
step:593/2330 train_time:22691ms step_avg:38.26ms
step:594/2330 train_time:22732ms step_avg:38.27ms
step:595/2330 train_time:22767ms step_avg:38.26ms
step:596/2330 train_time:22808ms step_avg:38.27ms
step:597/2330 train_time:22844ms step_avg:38.26ms
step:598/2330 train_time:22885ms step_avg:38.27ms
step:599/2330 train_time:22920ms step_avg:38.26ms
step:600/2330 train_time:22961ms step_avg:38.27ms
step:601/2330 train_time:22996ms step_avg:38.26ms
step:602/2330 train_time:23038ms step_avg:38.27ms
step:603/2330 train_time:23073ms step_avg:38.26ms
step:604/2330 train_time:23114ms step_avg:38.27ms
step:605/2330 train_time:23150ms step_avg:38.26ms
step:606/2330 train_time:23191ms step_avg:38.27ms
step:607/2330 train_time:23227ms step_avg:38.27ms
step:608/2330 train_time:23268ms step_avg:38.27ms
step:609/2330 train_time:23303ms step_avg:38.26ms
step:610/2330 train_time:23344ms step_avg:38.27ms
step:611/2330 train_time:23380ms step_avg:38.26ms
step:612/2330 train_time:23421ms step_avg:38.27ms
step:613/2330 train_time:23457ms step_avg:38.27ms
step:614/2330 train_time:23499ms step_avg:38.27ms
step:615/2330 train_time:23534ms step_avg:38.27ms
step:616/2330 train_time:23575ms step_avg:38.27ms
step:617/2330 train_time:23612ms step_avg:38.27ms
step:618/2330 train_time:23653ms step_avg:38.27ms
step:619/2330 train_time:23690ms step_avg:38.27ms
step:620/2330 train_time:23730ms step_avg:38.27ms
step:621/2330 train_time:23766ms step_avg:38.27ms
step:622/2330 train_time:23807ms step_avg:38.27ms
step:623/2330 train_time:23842ms step_avg:38.27ms
step:624/2330 train_time:23883ms step_avg:38.27ms
step:625/2330 train_time:23918ms step_avg:38.27ms
step:626/2330 train_time:23959ms step_avg:38.27ms
step:627/2330 train_time:23995ms step_avg:38.27ms
step:628/2330 train_time:24036ms step_avg:38.27ms
step:629/2330 train_time:24071ms step_avg:38.27ms
step:630/2330 train_time:24112ms step_avg:38.27ms
step:631/2330 train_time:24148ms step_avg:38.27ms
step:632/2330 train_time:24188ms step_avg:38.27ms
step:633/2330 train_time:24224ms step_avg:38.27ms
step:634/2330 train_time:24265ms step_avg:38.27ms
step:635/2330 train_time:24305ms step_avg:38.28ms
step:636/2330 train_time:24341ms step_avg:38.27ms
step:637/2330 train_time:24376ms step_avg:38.27ms
step:638/2330 train_time:24418ms step_avg:38.27ms
step:639/2330 train_time:24454ms step_avg:38.27ms
step:640/2330 train_time:24494ms step_avg:38.27ms
step:641/2330 train_time:24530ms step_avg:38.27ms
step:642/2330 train_time:24571ms step_avg:38.27ms
step:643/2330 train_time:24607ms step_avg:38.27ms
step:644/2330 train_time:24649ms step_avg:38.28ms
step:645/2330 train_time:24685ms step_avg:38.27ms
step:646/2330 train_time:24725ms step_avg:38.27ms
step:647/2330 train_time:24762ms step_avg:38.27ms
step:648/2330 train_time:24802ms step_avg:38.28ms
step:649/2330 train_time:24839ms step_avg:38.27ms
step:650/2330 train_time:24880ms step_avg:38.28ms
step:651/2330 train_time:24916ms step_avg:38.27ms
step:652/2330 train_time:24958ms step_avg:38.28ms
step:653/2330 train_time:24993ms step_avg:38.27ms
step:654/2330 train_time:25034ms step_avg:38.28ms
step:655/2330 train_time:25070ms step_avg:38.28ms
step:656/2330 train_time:25111ms step_avg:38.28ms
step:657/2330 train_time:25147ms step_avg:38.28ms
step:658/2330 train_time:25188ms step_avg:38.28ms
step:659/2330 train_time:25223ms step_avg:38.27ms
step:660/2330 train_time:25264ms step_avg:38.28ms
step:661/2330 train_time:25300ms step_avg:38.28ms
step:662/2330 train_time:25341ms step_avg:38.28ms
step:663/2330 train_time:25377ms step_avg:38.28ms
step:664/2330 train_time:25419ms step_avg:38.28ms
step:665/2330 train_time:25455ms step_avg:38.28ms
step:666/2330 train_time:25496ms step_avg:38.28ms
step:667/2330 train_time:25533ms step_avg:38.28ms
step:668/2330 train_time:25574ms step_avg:38.28ms
step:669/2330 train_time:25610ms step_avg:38.28ms
step:670/2330 train_time:25652ms step_avg:38.29ms
step:671/2330 train_time:25687ms step_avg:38.28ms
step:672/2330 train_time:25728ms step_avg:38.29ms
step:673/2330 train_time:25763ms step_avg:38.28ms
step:674/2330 train_time:25804ms step_avg:38.29ms
step:675/2330 train_time:25840ms step_avg:38.28ms
step:676/2330 train_time:25882ms step_avg:38.29ms
step:677/2330 train_time:25918ms step_avg:38.28ms
step:678/2330 train_time:25960ms step_avg:38.29ms
step:679/2330 train_time:25995ms step_avg:38.28ms
step:680/2330 train_time:26037ms step_avg:38.29ms
step:681/2330 train_time:26073ms step_avg:38.29ms
step:682/2330 train_time:26113ms step_avg:38.29ms
step:683/2330 train_time:26150ms step_avg:38.29ms
step:684/2330 train_time:26191ms step_avg:38.29ms
step:685/2330 train_time:26226ms step_avg:38.29ms
step:686/2330 train_time:26267ms step_avg:38.29ms
step:687/2330 train_time:26303ms step_avg:38.29ms
step:688/2330 train_time:26343ms step_avg:38.29ms
step:689/2330 train_time:26380ms step_avg:38.29ms
step:690/2330 train_time:26421ms step_avg:38.29ms
step:691/2330 train_time:26457ms step_avg:38.29ms
step:692/2330 train_time:26499ms step_avg:38.29ms
step:693/2330 train_time:26534ms step_avg:38.29ms
step:694/2330 train_time:26576ms step_avg:38.29ms
step:695/2330 train_time:26611ms step_avg:38.29ms
step:696/2330 train_time:26653ms step_avg:38.29ms
step:697/2330 train_time:26689ms step_avg:38.29ms
step:698/2330 train_time:26730ms step_avg:38.29ms
step:699/2330 train_time:26765ms step_avg:38.29ms
step:700/2330 train_time:26807ms step_avg:38.30ms
step:701/2330 train_time:26842ms step_avg:38.29ms
step:702/2330 train_time:26884ms step_avg:38.30ms
step:703/2330 train_time:26919ms step_avg:38.29ms
step:704/2330 train_time:26961ms step_avg:38.30ms
step:705/2330 train_time:26997ms step_avg:38.29ms
step:706/2330 train_time:27038ms step_avg:38.30ms
step:707/2330 train_time:27075ms step_avg:38.29ms
step:708/2330 train_time:27115ms step_avg:38.30ms
step:709/2330 train_time:27152ms step_avg:38.30ms
step:710/2330 train_time:27192ms step_avg:38.30ms
step:711/2330 train_time:27228ms step_avg:38.30ms
step:712/2330 train_time:27269ms step_avg:38.30ms
step:713/2330 train_time:27304ms step_avg:38.30ms
step:714/2330 train_time:27346ms step_avg:38.30ms
step:715/2330 train_time:27381ms step_avg:38.29ms
step:716/2330 train_time:27423ms step_avg:38.30ms
step:717/2330 train_time:27458ms step_avg:38.30ms
step:718/2330 train_time:27499ms step_avg:38.30ms
step:719/2330 train_time:27535ms step_avg:38.30ms
step:720/2330 train_time:27577ms step_avg:38.30ms
step:721/2330 train_time:27612ms step_avg:38.30ms
step:722/2330 train_time:27653ms step_avg:38.30ms
step:723/2330 train_time:27689ms step_avg:38.30ms
step:724/2330 train_time:27730ms step_avg:38.30ms
step:725/2330 train_time:27765ms step_avg:38.30ms
step:726/2330 train_time:27806ms step_avg:38.30ms
step:727/2330 train_time:27841ms step_avg:38.30ms
step:728/2330 train_time:27883ms step_avg:38.30ms
step:729/2330 train_time:27918ms step_avg:38.30ms
step:730/2330 train_time:27960ms step_avg:38.30ms
step:731/2330 train_time:27996ms step_avg:38.30ms
step:732/2330 train_time:28037ms step_avg:38.30ms
step:733/2330 train_time:28073ms step_avg:38.30ms
step:734/2330 train_time:28114ms step_avg:38.30ms
step:735/2330 train_time:28151ms step_avg:38.30ms
step:736/2330 train_time:28192ms step_avg:38.30ms
step:737/2330 train_time:28227ms step_avg:38.30ms
step:738/2330 train_time:28268ms step_avg:38.30ms
step:739/2330 train_time:28304ms step_avg:38.30ms
step:740/2330 train_time:28345ms step_avg:38.30ms
step:741/2330 train_time:28381ms step_avg:38.30ms
step:742/2330 train_time:28422ms step_avg:38.30ms
step:743/2330 train_time:28457ms step_avg:38.30ms
step:744/2330 train_time:28499ms step_avg:38.31ms
step:745/2330 train_time:28534ms step_avg:38.30ms
step:746/2330 train_time:28575ms step_avg:38.30ms
step:747/2330 train_time:28611ms step_avg:38.30ms
step:748/2330 train_time:28652ms step_avg:38.30ms
step:749/2330 train_time:28688ms step_avg:38.30ms
step:750/2330 train_time:28729ms step_avg:38.30ms
step:750/2330 val_loss:5.3534 train_time:28841ms step_avg:38.45ms
step:751/2330 train_time:28853ms step_avg:38.42ms
step:752/2330 train_time:28864ms step_avg:38.38ms
step:753/2330 train_time:28873ms step_avg:38.34ms
step:754/2330 train_time:28883ms step_avg:38.31ms
step:755/2330 train_time:28918ms step_avg:38.30ms
step:756/2330 train_time:28958ms step_avg:38.30ms
step:757/2330 train_time:28993ms step_avg:38.30ms
step:758/2330 train_time:29034ms step_avg:38.30ms
step:759/2330 train_time:29069ms step_avg:38.30ms
step:760/2330 train_time:29110ms step_avg:38.30ms
step:761/2330 train_time:29147ms step_avg:38.30ms
step:762/2330 train_time:29189ms step_avg:38.31ms
step:763/2330 train_time:29231ms step_avg:38.31ms
step:764/2330 train_time:29272ms step_avg:38.31ms
step:765/2330 train_time:29311ms step_avg:38.32ms
step:766/2330 train_time:29353ms step_avg:38.32ms
step:767/2330 train_time:29388ms step_avg:38.32ms
step:768/2330 train_time:29430ms step_avg:38.32ms
step:769/2330 train_time:29467ms step_avg:38.32ms
step:770/2330 train_time:29508ms step_avg:38.32ms
step:771/2330 train_time:29543ms step_avg:38.32ms
step:772/2330 train_time:29584ms step_avg:38.32ms
step:773/2330 train_time:29619ms step_avg:38.32ms
step:774/2330 train_time:29659ms step_avg:38.32ms
step:775/2330 train_time:29694ms step_avg:38.31ms
step:776/2330 train_time:29735ms step_avg:38.32ms
step:777/2330 train_time:29770ms step_avg:38.31ms
step:778/2330 train_time:29812ms step_avg:38.32ms
step:779/2330 train_time:29848ms step_avg:38.32ms
step:780/2330 train_time:29889ms step_avg:38.32ms
step:781/2330 train_time:29924ms step_avg:38.32ms
step:782/2330 train_time:29965ms step_avg:38.32ms
step:783/2330 train_time:30000ms step_avg:38.31ms
step:784/2330 train_time:30040ms step_avg:38.32ms
step:785/2330 train_time:30075ms step_avg:38.31ms
step:786/2330 train_time:30117ms step_avg:38.32ms
step:787/2330 train_time:30154ms step_avg:38.31ms
step:788/2330 train_time:30195ms step_avg:38.32ms
step:789/2330 train_time:30232ms step_avg:38.32ms
step:790/2330 train_time:30273ms step_avg:38.32ms
step:791/2330 train_time:30309ms step_avg:38.32ms
step:792/2330 train_time:30350ms step_avg:38.32ms
step:793/2330 train_time:30387ms step_avg:38.32ms
step:794/2330 train_time:30428ms step_avg:38.32ms
step:795/2330 train_time:30465ms step_avg:38.32ms
step:796/2330 train_time:30506ms step_avg:38.32ms
step:797/2330 train_time:30541ms step_avg:38.32ms
step:798/2330 train_time:30582ms step_avg:38.32ms
step:799/2330 train_time:30618ms step_avg:38.32ms
step:800/2330 train_time:30659ms step_avg:38.32ms
step:801/2330 train_time:30695ms step_avg:38.32ms
step:802/2330 train_time:30735ms step_avg:38.32ms
step:803/2330 train_time:30771ms step_avg:38.32ms
step:804/2330 train_time:30812ms step_avg:38.32ms
step:805/2330 train_time:30847ms step_avg:38.32ms
step:806/2330 train_time:30889ms step_avg:38.32ms
step:807/2330 train_time:30923ms step_avg:38.32ms
step:808/2330 train_time:30965ms step_avg:38.32ms
step:809/2330 train_time:31000ms step_avg:38.32ms
step:810/2330 train_time:31041ms step_avg:38.32ms
step:811/2330 train_time:31077ms step_avg:38.32ms
step:812/2330 train_time:31118ms step_avg:38.32ms
step:813/2330 train_time:31154ms step_avg:38.32ms
step:814/2330 train_time:31195ms step_avg:38.32ms
step:815/2330 train_time:31232ms step_avg:38.32ms
step:816/2330 train_time:31273ms step_avg:38.32ms
step:817/2330 train_time:31310ms step_avg:38.32ms
step:818/2330 train_time:31351ms step_avg:38.33ms
step:819/2330 train_time:31387ms step_avg:38.32ms
step:820/2330 train_time:31428ms step_avg:38.33ms
step:821/2330 train_time:31464ms step_avg:38.32ms
step:822/2330 train_time:31506ms step_avg:38.33ms
step:823/2330 train_time:31541ms step_avg:38.32ms
step:824/2330 train_time:31582ms step_avg:38.33ms
step:825/2330 train_time:31619ms step_avg:38.33ms
step:826/2330 train_time:31659ms step_avg:38.33ms
step:827/2330 train_time:31696ms step_avg:38.33ms
step:828/2330 train_time:31737ms step_avg:38.33ms
step:829/2330 train_time:31772ms step_avg:38.33ms
step:830/2330 train_time:31813ms step_avg:38.33ms
step:831/2330 train_time:31848ms step_avg:38.32ms
step:832/2330 train_time:31889ms step_avg:38.33ms
step:833/2330 train_time:31925ms step_avg:38.33ms
step:834/2330 train_time:31966ms step_avg:38.33ms
step:835/2330 train_time:32001ms step_avg:38.32ms
step:836/2330 train_time:32042ms step_avg:38.33ms
step:837/2330 train_time:32077ms step_avg:38.32ms
step:838/2330 train_time:32119ms step_avg:38.33ms
step:839/2330 train_time:32154ms step_avg:38.32ms
step:840/2330 train_time:32195ms step_avg:38.33ms
step:841/2330 train_time:32231ms step_avg:38.32ms
step:842/2330 train_time:32272ms step_avg:38.33ms
step:843/2330 train_time:32309ms step_avg:38.33ms
step:844/2330 train_time:32350ms step_avg:38.33ms
step:845/2330 train_time:32386ms step_avg:38.33ms
step:846/2330 train_time:32427ms step_avg:38.33ms
step:847/2330 train_time:32463ms step_avg:38.33ms
step:848/2330 train_time:32505ms step_avg:38.33ms
step:849/2330 train_time:32541ms step_avg:38.33ms
step:850/2330 train_time:32581ms step_avg:38.33ms
step:851/2330 train_time:32618ms step_avg:38.33ms
step:852/2330 train_time:32659ms step_avg:38.33ms
step:853/2330 train_time:32694ms step_avg:38.33ms
step:854/2330 train_time:32735ms step_avg:38.33ms
step:855/2330 train_time:32771ms step_avg:38.33ms
step:856/2330 train_time:32811ms step_avg:38.33ms
step:857/2330 train_time:32847ms step_avg:38.33ms
step:858/2330 train_time:32888ms step_avg:38.33ms
step:859/2330 train_time:32924ms step_avg:38.33ms
step:860/2330 train_time:32965ms step_avg:38.33ms
step:861/2330 train_time:33001ms step_avg:38.33ms
step:862/2330 train_time:33042ms step_avg:38.33ms
step:863/2330 train_time:33078ms step_avg:38.33ms
step:864/2330 train_time:33119ms step_avg:38.33ms
step:865/2330 train_time:33154ms step_avg:38.33ms
step:866/2330 train_time:33195ms step_avg:38.33ms
step:867/2330 train_time:33230ms step_avg:38.33ms
step:868/2330 train_time:33271ms step_avg:38.33ms
step:869/2330 train_time:33308ms step_avg:38.33ms
step:870/2330 train_time:33349ms step_avg:38.33ms
step:871/2330 train_time:33385ms step_avg:38.33ms
step:872/2330 train_time:33427ms step_avg:38.33ms
step:873/2330 train_time:33462ms step_avg:38.33ms
step:874/2330 train_time:33504ms step_avg:38.33ms
step:875/2330 train_time:33540ms step_avg:38.33ms
step:876/2330 train_time:33580ms step_avg:38.33ms
step:877/2330 train_time:33617ms step_avg:38.33ms
step:878/2330 train_time:33658ms step_avg:38.34ms
step:879/2330 train_time:33694ms step_avg:38.33ms
step:880/2330 train_time:33734ms step_avg:38.33ms
step:881/2330 train_time:33770ms step_avg:38.33ms
step:882/2330 train_time:33810ms step_avg:38.33ms
step:883/2330 train_time:33846ms step_avg:38.33ms
step:884/2330 train_time:33887ms step_avg:38.33ms
step:885/2330 train_time:33923ms step_avg:38.33ms
step:886/2330 train_time:33964ms step_avg:38.33ms
step:887/2330 train_time:34000ms step_avg:38.33ms
step:888/2330 train_time:34040ms step_avg:38.33ms
step:889/2330 train_time:34077ms step_avg:38.33ms
step:890/2330 train_time:34118ms step_avg:38.33ms
step:891/2330 train_time:34154ms step_avg:38.33ms
step:892/2330 train_time:34195ms step_avg:38.34ms
step:893/2330 train_time:34230ms step_avg:38.33ms
step:894/2330 train_time:34272ms step_avg:38.34ms
step:895/2330 train_time:34308ms step_avg:38.33ms
step:896/2330 train_time:34349ms step_avg:38.34ms
step:897/2330 train_time:34384ms step_avg:38.33ms
step:898/2330 train_time:34425ms step_avg:38.34ms
step:899/2330 train_time:34461ms step_avg:38.33ms
step:900/2330 train_time:34502ms step_avg:38.34ms
step:901/2330 train_time:34539ms step_avg:38.33ms
step:902/2330 train_time:34580ms step_avg:38.34ms
step:903/2330 train_time:34617ms step_avg:38.34ms
step:904/2330 train_time:34658ms step_avg:38.34ms
step:905/2330 train_time:34693ms step_avg:38.33ms
step:906/2330 train_time:34734ms step_avg:38.34ms
step:907/2330 train_time:34770ms step_avg:38.33ms
step:908/2330 train_time:34810ms step_avg:38.34ms
step:909/2330 train_time:34847ms step_avg:38.34ms
step:910/2330 train_time:34888ms step_avg:38.34ms
step:911/2330 train_time:34923ms step_avg:38.33ms
step:912/2330 train_time:34964ms step_avg:38.34ms
step:913/2330 train_time:35000ms step_avg:38.33ms
step:914/2330 train_time:35040ms step_avg:38.34ms
step:915/2330 train_time:35076ms step_avg:38.33ms
step:916/2330 train_time:35118ms step_avg:38.34ms
step:917/2330 train_time:35153ms step_avg:38.33ms
step:918/2330 train_time:35194ms step_avg:38.34ms
step:919/2330 train_time:35230ms step_avg:38.34ms
step:920/2330 train_time:35271ms step_avg:38.34ms
step:921/2330 train_time:35307ms step_avg:38.34ms
step:922/2330 train_time:35348ms step_avg:38.34ms
step:923/2330 train_time:35384ms step_avg:38.34ms
step:924/2330 train_time:35425ms step_avg:38.34ms
step:925/2330 train_time:35461ms step_avg:38.34ms
step:926/2330 train_time:35501ms step_avg:38.34ms
step:927/2330 train_time:35537ms step_avg:38.34ms
step:928/2330 train_time:35579ms step_avg:38.34ms
step:929/2330 train_time:35614ms step_avg:38.34ms
step:930/2330 train_time:35656ms step_avg:38.34ms
step:931/2330 train_time:35691ms step_avg:38.34ms
step:932/2330 train_time:35732ms step_avg:38.34ms
step:933/2330 train_time:35768ms step_avg:38.34ms
step:934/2330 train_time:35809ms step_avg:38.34ms
step:935/2330 train_time:35845ms step_avg:38.34ms
step:936/2330 train_time:35886ms step_avg:38.34ms
step:937/2330 train_time:35921ms step_avg:38.34ms
step:938/2330 train_time:35962ms step_avg:38.34ms
step:939/2330 train_time:35998ms step_avg:38.34ms
step:940/2330 train_time:36039ms step_avg:38.34ms
step:941/2330 train_time:36074ms step_avg:38.34ms
step:942/2330 train_time:36115ms step_avg:38.34ms
step:943/2330 train_time:36151ms step_avg:38.34ms
step:944/2330 train_time:36191ms step_avg:38.34ms
step:945/2330 train_time:36228ms step_avg:38.34ms
step:946/2330 train_time:36270ms step_avg:38.34ms
step:947/2330 train_time:36305ms step_avg:38.34ms
step:948/2330 train_time:36347ms step_avg:38.34ms
step:949/2330 train_time:36382ms step_avg:38.34ms
step:950/2330 train_time:36424ms step_avg:38.34ms
step:951/2330 train_time:36459ms step_avg:38.34ms
step:952/2330 train_time:36500ms step_avg:38.34ms
step:953/2330 train_time:36538ms step_avg:38.34ms
step:954/2330 train_time:36579ms step_avg:38.34ms
step:955/2330 train_time:36614ms step_avg:38.34ms
step:956/2330 train_time:36655ms step_avg:38.34ms
step:957/2330 train_time:36690ms step_avg:38.34ms
step:958/2330 train_time:36731ms step_avg:38.34ms
step:959/2330 train_time:36766ms step_avg:38.34ms
step:960/2330 train_time:36808ms step_avg:38.34ms
step:961/2330 train_time:36843ms step_avg:38.34ms
step:962/2330 train_time:36884ms step_avg:38.34ms
step:963/2330 train_time:36920ms step_avg:38.34ms
step:964/2330 train_time:36961ms step_avg:38.34ms
step:965/2330 train_time:36997ms step_avg:38.34ms
step:966/2330 train_time:37038ms step_avg:38.34ms
step:967/2330 train_time:37073ms step_avg:38.34ms
step:968/2330 train_time:37114ms step_avg:38.34ms
step:969/2330 train_time:37150ms step_avg:38.34ms
step:970/2330 train_time:37190ms step_avg:38.34ms
step:971/2330 train_time:37227ms step_avg:38.34ms
step:972/2330 train_time:37268ms step_avg:38.34ms
step:973/2330 train_time:37304ms step_avg:38.34ms
step:974/2330 train_time:37345ms step_avg:38.34ms
step:975/2330 train_time:37381ms step_avg:38.34ms
step:976/2330 train_time:37422ms step_avg:38.34ms
step:977/2330 train_time:37458ms step_avg:38.34ms
step:978/2330 train_time:37499ms step_avg:38.34ms
step:979/2330 train_time:37535ms step_avg:38.34ms
step:980/2330 train_time:37577ms step_avg:38.34ms
step:981/2330 train_time:37612ms step_avg:38.34ms
step:982/2330 train_time:37653ms step_avg:38.34ms
step:983/2330 train_time:37689ms step_avg:38.34ms
step:984/2330 train_time:37730ms step_avg:38.34ms
step:985/2330 train_time:37765ms step_avg:38.34ms
step:986/2330 train_time:37807ms step_avg:38.34ms
step:987/2330 train_time:37842ms step_avg:38.34ms
step:988/2330 train_time:37884ms step_avg:38.34ms
step:989/2330 train_time:37920ms step_avg:38.34ms
step:990/2330 train_time:37961ms step_avg:38.34ms
step:991/2330 train_time:37996ms step_avg:38.34ms
step:992/2330 train_time:38038ms step_avg:38.34ms
step:993/2330 train_time:38073ms step_avg:38.34ms
step:994/2330 train_time:38114ms step_avg:38.34ms
step:995/2330 train_time:38150ms step_avg:38.34ms
step:996/2330 train_time:38190ms step_avg:38.34ms
step:997/2330 train_time:38227ms step_avg:38.34ms
step:998/2330 train_time:38268ms step_avg:38.34ms
step:999/2330 train_time:38303ms step_avg:38.34ms
step:1000/2330 train_time:38345ms step_avg:38.34ms
step:1000/2330 val_loss:5.3130 train_time:38458ms step_avg:38.46ms
step:1001/2330 train_time:38470ms step_avg:38.43ms
step:1002/2330 train_time:38483ms step_avg:38.41ms
step:1003/2330 train_time:38493ms step_avg:38.38ms
step:1004/2330 train_time:38505ms step_avg:38.35ms
step:1005/2330 train_time:38535ms step_avg:38.34ms
step:1006/2330 train_time:38576ms step_avg:38.35ms
step:1007/2330 train_time:38610ms step_avg:38.34ms
step:1008/2330 train_time:38651ms step_avg:38.34ms
step:1009/2330 train_time:38685ms step_avg:38.34ms
step:1010/2330 train_time:38726ms step_avg:38.34ms
step:1011/2330 train_time:38764ms step_avg:38.34ms
step:1012/2330 train_time:38806ms step_avg:38.35ms
step:1013/2330 train_time:38846ms step_avg:38.35ms
step:1014/2330 train_time:38888ms step_avg:38.35ms
step:1015/2330 train_time:38924ms step_avg:38.35ms
step:1016/2330 train_time:38965ms step_avg:38.35ms
step:1017/2330 train_time:39000ms step_avg:38.35ms
step:1018/2330 train_time:39041ms step_avg:38.35ms
step:1019/2330 train_time:39077ms step_avg:38.35ms
step:1020/2330 train_time:39117ms step_avg:38.35ms
step:1021/2330 train_time:39153ms step_avg:38.35ms
step:1022/2330 train_time:39194ms step_avg:38.35ms
step:1023/2330 train_time:39229ms step_avg:38.35ms
step:1024/2330 train_time:39269ms step_avg:38.35ms
step:1025/2330 train_time:39304ms step_avg:38.35ms
step:1026/2330 train_time:39345ms step_avg:38.35ms
step:1027/2330 train_time:39383ms step_avg:38.35ms
step:1028/2330 train_time:39424ms step_avg:38.35ms
step:1029/2330 train_time:39462ms step_avg:38.35ms
step:1030/2330 train_time:39502ms step_avg:38.35ms
step:1031/2330 train_time:39539ms step_avg:38.35ms
step:1032/2330 train_time:39579ms step_avg:38.35ms
step:1033/2330 train_time:39615ms step_avg:38.35ms
step:1034/2330 train_time:39656ms step_avg:38.35ms
step:1035/2330 train_time:39691ms step_avg:38.35ms
step:1036/2330 train_time:39732ms step_avg:38.35ms
step:1037/2330 train_time:39770ms step_avg:38.35ms
step:1038/2330 train_time:39811ms step_avg:38.35ms
step:1039/2330 train_time:39847ms step_avg:38.35ms
step:1040/2330 train_time:39889ms step_avg:38.35ms
step:1041/2330 train_time:39925ms step_avg:38.35ms
step:1042/2330 train_time:39967ms step_avg:38.36ms
step:1043/2330 train_time:40002ms step_avg:38.35ms
step:1044/2330 train_time:40043ms step_avg:38.36ms
step:1045/2330 train_time:40079ms step_avg:38.35ms
step:1046/2330 train_time:40120ms step_avg:38.36ms
step:1047/2330 train_time:40156ms step_avg:38.35ms
step:1048/2330 train_time:40197ms step_avg:38.36ms
step:1049/2330 train_time:40231ms step_avg:38.35ms
step:1050/2330 train_time:40272ms step_avg:38.35ms
step:1051/2330 train_time:40307ms step_avg:38.35ms
step:1052/2330 train_time:40349ms step_avg:38.35ms
step:1053/2330 train_time:40385ms step_avg:38.35ms
step:1054/2330 train_time:40427ms step_avg:38.36ms
step:1055/2330 train_time:40463ms step_avg:38.35ms
step:1056/2330 train_time:40504ms step_avg:38.36ms
step:1057/2330 train_time:40540ms step_avg:38.35ms
step:1058/2330 train_time:40581ms step_avg:38.36ms
step:1059/2330 train_time:40617ms step_avg:38.35ms
step:1060/2330 train_time:40658ms step_avg:38.36ms
step:1061/2330 train_time:40694ms step_avg:38.35ms
step:1062/2330 train_time:40735ms step_avg:38.36ms
step:1063/2330 train_time:40771ms step_avg:38.35ms
step:1064/2330 train_time:40812ms step_avg:38.36ms
step:1065/2330 train_time:40849ms step_avg:38.36ms
step:1066/2330 train_time:40890ms step_avg:38.36ms
step:1067/2330 train_time:40927ms step_avg:38.36ms
step:1068/2330 train_time:40968ms step_avg:38.36ms
step:1069/2330 train_time:41004ms step_avg:38.36ms
step:1070/2330 train_time:41045ms step_avg:38.36ms
step:1071/2330 train_time:41081ms step_avg:38.36ms
step:1072/2330 train_time:41122ms step_avg:38.36ms
step:1073/2330 train_time:41158ms step_avg:38.36ms
step:1074/2330 train_time:41199ms step_avg:38.36ms
step:1075/2330 train_time:41235ms step_avg:38.36ms
step:1076/2330 train_time:41276ms step_avg:38.36ms
step:1077/2330 train_time:41310ms step_avg:38.36ms
step:1078/2330 train_time:41351ms step_avg:38.36ms
step:1079/2330 train_time:41387ms step_avg:38.36ms
step:1080/2330 train_time:41429ms step_avg:38.36ms
step:1081/2330 train_time:41465ms step_avg:38.36ms
step:1082/2330 train_time:41506ms step_avg:38.36ms
step:1083/2330 train_time:41542ms step_avg:38.36ms
step:1084/2330 train_time:41583ms step_avg:38.36ms
step:1085/2330 train_time:41620ms step_avg:38.36ms
step:1086/2330 train_time:41661ms step_avg:38.36ms
step:1087/2330 train_time:41697ms step_avg:38.36ms
step:1088/2330 train_time:41738ms step_avg:38.36ms
step:1089/2330 train_time:41774ms step_avg:38.36ms
step:1090/2330 train_time:41815ms step_avg:38.36ms
step:1091/2330 train_time:41850ms step_avg:38.36ms
step:1092/2330 train_time:41891ms step_avg:38.36ms
step:1093/2330 train_time:41928ms step_avg:38.36ms
step:1094/2330 train_time:41969ms step_avg:38.36ms
step:1095/2330 train_time:42005ms step_avg:38.36ms
step:1096/2330 train_time:42046ms step_avg:38.36ms
step:1097/2330 train_time:42082ms step_avg:38.36ms
step:1098/2330 train_time:42123ms step_avg:38.36ms
step:1099/2330 train_time:42160ms step_avg:38.36ms
step:1100/2330 train_time:42201ms step_avg:38.36ms
step:1101/2330 train_time:42237ms step_avg:38.36ms
step:1102/2330 train_time:42278ms step_avg:38.37ms
step:1103/2330 train_time:42313ms step_avg:38.36ms
step:1104/2330 train_time:42354ms step_avg:38.36ms
step:1105/2330 train_time:42390ms step_avg:38.36ms
step:1106/2330 train_time:42431ms step_avg:38.36ms
step:1107/2330 train_time:42467ms step_avg:38.36ms
step:1108/2330 train_time:42509ms step_avg:38.37ms
step:1109/2330 train_time:42544ms step_avg:38.36ms
step:1110/2330 train_time:42586ms step_avg:38.37ms
step:1111/2330 train_time:42622ms step_avg:38.36ms
step:1112/2330 train_time:42663ms step_avg:38.37ms
step:1113/2330 train_time:42700ms step_avg:38.36ms
step:1114/2330 train_time:42740ms step_avg:38.37ms
step:1115/2330 train_time:42777ms step_avg:38.37ms
step:1116/2330 train_time:42818ms step_avg:38.37ms
step:1117/2330 train_time:42855ms step_avg:38.37ms
step:1118/2330 train_time:42896ms step_avg:38.37ms
step:1119/2330 train_time:42932ms step_avg:38.37ms
step:1120/2330 train_time:42973ms step_avg:38.37ms
step:1121/2330 train_time:43009ms step_avg:38.37ms
step:1122/2330 train_time:43050ms step_avg:38.37ms
step:1123/2330 train_time:43086ms step_avg:38.37ms
step:1124/2330 train_time:43128ms step_avg:38.37ms
step:1125/2330 train_time:43164ms step_avg:38.37ms
step:1126/2330 train_time:43205ms step_avg:38.37ms
step:1127/2330 train_time:43241ms step_avg:38.37ms
step:1128/2330 train_time:43282ms step_avg:38.37ms
step:1129/2330 train_time:43319ms step_avg:38.37ms
step:1130/2330 train_time:43360ms step_avg:38.37ms
step:1131/2330 train_time:43396ms step_avg:38.37ms
step:1132/2330 train_time:43437ms step_avg:38.37ms
step:1133/2330 train_time:43472ms step_avg:38.37ms
step:1134/2330 train_time:43513ms step_avg:38.37ms
step:1135/2330 train_time:43549ms step_avg:38.37ms
step:1136/2330 train_time:43590ms step_avg:38.37ms
step:1137/2330 train_time:43627ms step_avg:38.37ms
step:1138/2330 train_time:43669ms step_avg:38.37ms
step:1139/2330 train_time:43705ms step_avg:38.37ms
step:1140/2330 train_time:43746ms step_avg:38.37ms
step:1141/2330 train_time:43782ms step_avg:38.37ms
step:1142/2330 train_time:43823ms step_avg:38.37ms
step:1143/2330 train_time:43859ms step_avg:38.37ms
step:1144/2330 train_time:43900ms step_avg:38.37ms
step:1145/2330 train_time:43937ms step_avg:38.37ms
step:1146/2330 train_time:43978ms step_avg:38.37ms
step:1147/2330 train_time:44014ms step_avg:38.37ms
step:1148/2330 train_time:44054ms step_avg:38.37ms
step:1149/2330 train_time:44090ms step_avg:38.37ms
step:1150/2330 train_time:44131ms step_avg:38.37ms
step:1151/2330 train_time:44168ms step_avg:38.37ms
step:1152/2330 train_time:44209ms step_avg:38.38ms
step:1153/2330 train_time:44245ms step_avg:38.37ms
step:1154/2330 train_time:44287ms step_avg:38.38ms
step:1155/2330 train_time:44323ms step_avg:38.37ms
step:1156/2330 train_time:44364ms step_avg:38.38ms
step:1157/2330 train_time:44400ms step_avg:38.38ms
step:1158/2330 train_time:44441ms step_avg:38.38ms
step:1159/2330 train_time:44477ms step_avg:38.38ms
step:1160/2330 train_time:44518ms step_avg:38.38ms
step:1161/2330 train_time:44554ms step_avg:38.38ms
step:1162/2330 train_time:44595ms step_avg:38.38ms
step:1163/2330 train_time:44632ms step_avg:38.38ms
step:1164/2330 train_time:44673ms step_avg:38.38ms
step:1165/2330 train_time:44708ms step_avg:38.38ms
step:1166/2330 train_time:44750ms step_avg:38.38ms
step:1167/2330 train_time:44787ms step_avg:38.38ms
step:1168/2330 train_time:44828ms step_avg:38.38ms
step:1169/2330 train_time:44864ms step_avg:38.38ms
step:1170/2330 train_time:44906ms step_avg:38.38ms
step:1171/2330 train_time:44942ms step_avg:38.38ms
step:1172/2330 train_time:44983ms step_avg:38.38ms
step:1173/2330 train_time:45020ms step_avg:38.38ms
step:1174/2330 train_time:45061ms step_avg:38.38ms
step:1175/2330 train_time:45097ms step_avg:38.38ms
step:1176/2330 train_time:45138ms step_avg:38.38ms
step:1177/2330 train_time:45174ms step_avg:38.38ms
step:1178/2330 train_time:45214ms step_avg:38.38ms
step:1179/2330 train_time:45250ms step_avg:38.38ms
step:1180/2330 train_time:45291ms step_avg:38.38ms
step:1181/2330 train_time:45328ms step_avg:38.38ms
step:1182/2330 train_time:45369ms step_avg:38.38ms
step:1183/2330 train_time:45406ms step_avg:38.38ms
step:1184/2330 train_time:45447ms step_avg:38.38ms
step:1185/2330 train_time:45483ms step_avg:38.38ms
step:1186/2330 train_time:45524ms step_avg:38.38ms
step:1187/2330 train_time:45560ms step_avg:38.38ms
step:1188/2330 train_time:45601ms step_avg:38.38ms
step:1189/2330 train_time:45637ms step_avg:38.38ms
step:1190/2330 train_time:45678ms step_avg:38.39ms
step:1191/2330 train_time:45714ms step_avg:38.38ms
step:1192/2330 train_time:45755ms step_avg:38.38ms
step:1193/2330 train_time:45791ms step_avg:38.38ms
step:1194/2330 train_time:45831ms step_avg:38.38ms
step:1195/2330 train_time:45869ms step_avg:38.38ms
step:1196/2330 train_time:45910ms step_avg:38.39ms
step:1197/2330 train_time:45946ms step_avg:38.38ms
step:1198/2330 train_time:45988ms step_avg:38.39ms
step:1199/2330 train_time:46023ms step_avg:38.38ms
step:1200/2330 train_time:46065ms step_avg:38.39ms
step:1201/2330 train_time:46100ms step_avg:38.38ms
step:1202/2330 train_time:46141ms step_avg:38.39ms
step:1203/2330 train_time:46177ms step_avg:38.39ms
step:1204/2330 train_time:46218ms step_avg:38.39ms
step:1205/2330 train_time:46254ms step_avg:38.38ms
step:1206/2330 train_time:46295ms step_avg:38.39ms
step:1207/2330 train_time:46330ms step_avg:38.38ms
step:1208/2330 train_time:46372ms step_avg:38.39ms
step:1209/2330 train_time:46408ms step_avg:38.39ms
step:1210/2330 train_time:46449ms step_avg:38.39ms
step:1211/2330 train_time:46485ms step_avg:38.39ms
step:1212/2330 train_time:46527ms step_avg:38.39ms
step:1213/2330 train_time:46563ms step_avg:38.39ms
step:1214/2330 train_time:46604ms step_avg:38.39ms
step:1215/2330 train_time:46641ms step_avg:38.39ms
step:1216/2330 train_time:46682ms step_avg:38.39ms
step:1217/2330 train_time:46718ms step_avg:38.39ms
step:1218/2330 train_time:46759ms step_avg:38.39ms
step:1219/2330 train_time:46796ms step_avg:38.39ms
step:1220/2330 train_time:46837ms step_avg:38.39ms
step:1221/2330 train_time:46872ms step_avg:38.39ms
step:1222/2330 train_time:46913ms step_avg:38.39ms
step:1223/2330 train_time:46949ms step_avg:38.39ms
step:1224/2330 train_time:46990ms step_avg:38.39ms
step:1225/2330 train_time:47027ms step_avg:38.39ms
step:1226/2330 train_time:47068ms step_avg:38.39ms
step:1227/2330 train_time:47104ms step_avg:38.39ms
step:1228/2330 train_time:47145ms step_avg:38.39ms
step:1229/2330 train_time:47182ms step_avg:38.39ms
step:1230/2330 train_time:47222ms step_avg:38.39ms
step:1231/2330 train_time:47259ms step_avg:38.39ms
step:1232/2330 train_time:47300ms step_avg:38.39ms
step:1233/2330 train_time:47335ms step_avg:38.39ms
step:1234/2330 train_time:47376ms step_avg:38.39ms
step:1235/2330 train_time:47412ms step_avg:38.39ms
step:1236/2330 train_time:47453ms step_avg:38.39ms
step:1237/2330 train_time:47489ms step_avg:38.39ms
step:1238/2330 train_time:47531ms step_avg:38.39ms
step:1239/2330 train_time:47567ms step_avg:38.39ms
step:1240/2330 train_time:47609ms step_avg:38.39ms
step:1241/2330 train_time:47644ms step_avg:38.39ms
step:1242/2330 train_time:47686ms step_avg:38.39ms
step:1243/2330 train_time:47722ms step_avg:38.39ms
step:1244/2330 train_time:47763ms step_avg:38.39ms
step:1245/2330 train_time:47800ms step_avg:38.39ms
step:1246/2330 train_time:47841ms step_avg:38.40ms
step:1247/2330 train_time:47878ms step_avg:38.39ms
step:1248/2330 train_time:47919ms step_avg:38.40ms
step:1249/2330 train_time:47954ms step_avg:38.39ms
step:1250/2330 train_time:47995ms step_avg:38.40ms
step:1250/2330 val_loss:5.2842 train_time:48108ms step_avg:38.49ms
step:1251/2330 train_time:48121ms step_avg:38.47ms
step:1252/2330 train_time:48133ms step_avg:38.44ms
step:1253/2330 train_time:48143ms step_avg:38.42ms
step:1254/2330 train_time:48155ms step_avg:38.40ms
step:1255/2330 train_time:48186ms step_avg:38.39ms
step:1256/2330 train_time:48226ms step_avg:38.40ms
step:1257/2330 train_time:48261ms step_avg:38.39ms
step:1258/2330 train_time:48302ms step_avg:38.40ms
step:1259/2330 train_time:48336ms step_avg:38.39ms
step:1260/2330 train_time:48377ms step_avg:38.39ms
step:1261/2330 train_time:48414ms step_avg:38.39ms
step:1262/2330 train_time:48457ms step_avg:38.40ms
step:1263/2330 train_time:48496ms step_avg:38.40ms
step:1264/2330 train_time:48538ms step_avg:38.40ms
step:1265/2330 train_time:48575ms step_avg:38.40ms
step:1266/2330 train_time:48616ms step_avg:38.40ms
step:1267/2330 train_time:48652ms step_avg:38.40ms
step:1268/2330 train_time:48694ms step_avg:38.40ms
step:1269/2330 train_time:48730ms step_avg:38.40ms
step:1270/2330 train_time:48773ms step_avg:38.40ms
step:1271/2330 train_time:48808ms step_avg:38.40ms
step:1272/2330 train_time:48848ms step_avg:38.40ms
step:1273/2330 train_time:48885ms step_avg:38.40ms
step:1274/2330 train_time:48925ms step_avg:38.40ms
step:1275/2330 train_time:48961ms step_avg:38.40ms
step:1276/2330 train_time:49002ms step_avg:38.40ms
step:1277/2330 train_time:49037ms step_avg:38.40ms
step:1278/2330 train_time:49078ms step_avg:38.40ms
step:1279/2330 train_time:49114ms step_avg:38.40ms
step:1280/2330 train_time:49155ms step_avg:38.40ms
step:1281/2330 train_time:49191ms step_avg:38.40ms
step:1282/2330 train_time:49232ms step_avg:38.40ms
step:1283/2330 train_time:49267ms step_avg:38.40ms
step:1284/2330 train_time:49307ms step_avg:38.40ms
step:1285/2330 train_time:49344ms step_avg:38.40ms
step:1286/2330 train_time:49385ms step_avg:38.40ms
step:1287/2330 train_time:49421ms step_avg:38.40ms
step:1288/2330 train_time:49463ms step_avg:38.40ms
step:1289/2330 train_time:49499ms step_avg:38.40ms
step:1290/2330 train_time:49541ms step_avg:38.40ms
step:1291/2330 train_time:49576ms step_avg:38.40ms
step:1292/2330 train_time:49617ms step_avg:38.40ms
step:1293/2330 train_time:49653ms step_avg:38.40ms
step:1294/2330 train_time:49695ms step_avg:38.40ms
step:1295/2330 train_time:49730ms step_avg:38.40ms
step:1296/2330 train_time:49772ms step_avg:38.40ms
step:1297/2330 train_time:49808ms step_avg:38.40ms
step:1298/2330 train_time:49849ms step_avg:38.40ms
step:1299/2330 train_time:49886ms step_avg:38.40ms
step:1300/2330 train_time:49927ms step_avg:38.41ms
step:1301/2330 train_time:49963ms step_avg:38.40ms
step:1302/2330 train_time:50004ms step_avg:38.41ms
step:1303/2330 train_time:50039ms step_avg:38.40ms
step:1304/2330 train_time:50080ms step_avg:38.40ms
step:1305/2330 train_time:50116ms step_avg:38.40ms
step:1306/2330 train_time:50157ms step_avg:38.41ms
step:1307/2330 train_time:50193ms step_avg:38.40ms
step:1308/2330 train_time:50235ms step_avg:38.41ms
step:1309/2330 train_time:50271ms step_avg:38.40ms
step:1310/2330 train_time:50312ms step_avg:38.41ms
step:1311/2330 train_time:50348ms step_avg:38.40ms
step:1312/2330 train_time:50388ms step_avg:38.41ms
step:1313/2330 train_time:50426ms step_avg:38.40ms
step:1314/2330 train_time:50466ms step_avg:38.41ms
step:1315/2330 train_time:50503ms step_avg:38.41ms
step:1316/2330 train_time:50544ms step_avg:38.41ms
step:1317/2330 train_time:50580ms step_avg:38.41ms
step:1318/2330 train_time:50621ms step_avg:38.41ms
step:1319/2330 train_time:50657ms step_avg:38.41ms
step:1320/2330 train_time:50698ms step_avg:38.41ms
step:1321/2330 train_time:50735ms step_avg:38.41ms
step:1322/2330 train_time:50776ms step_avg:38.41ms
step:1323/2330 train_time:50813ms step_avg:38.41ms
step:1324/2330 train_time:50854ms step_avg:38.41ms
step:1325/2330 train_time:50890ms step_avg:38.41ms
step:1326/2330 train_time:50932ms step_avg:38.41ms
step:1327/2330 train_time:50968ms step_avg:38.41ms
step:1328/2330 train_time:51009ms step_avg:38.41ms
step:1329/2330 train_time:51046ms step_avg:38.41ms
step:1330/2330 train_time:51086ms step_avg:38.41ms
step:1331/2330 train_time:51123ms step_avg:38.41ms
step:1332/2330 train_time:51164ms step_avg:38.41ms
step:1333/2330 train_time:51199ms step_avg:38.41ms
step:1334/2330 train_time:51239ms step_avg:38.41ms
step:1335/2330 train_time:51275ms step_avg:38.41ms
step:1336/2330 train_time:51316ms step_avg:38.41ms
step:1337/2330 train_time:51352ms step_avg:38.41ms
step:1338/2330 train_time:51393ms step_avg:38.41ms
step:1339/2330 train_time:51429ms step_avg:38.41ms
step:1340/2330 train_time:51470ms step_avg:38.41ms
step:1341/2330 train_time:51506ms step_avg:38.41ms
step:1342/2330 train_time:51547ms step_avg:38.41ms
step:1343/2330 train_time:51583ms step_avg:38.41ms
step:1344/2330 train_time:51624ms step_avg:38.41ms
step:1345/2330 train_time:51660ms step_avg:38.41ms
step:1346/2330 train_time:51701ms step_avg:38.41ms
step:1347/2330 train_time:51738ms step_avg:38.41ms
step:1348/2330 train_time:51779ms step_avg:38.41ms
step:1349/2330 train_time:51816ms step_avg:38.41ms
step:1350/2330 train_time:51857ms step_avg:38.41ms
step:1351/2330 train_time:51893ms step_avg:38.41ms
step:1352/2330 train_time:51935ms step_avg:38.41ms
step:1353/2330 train_time:51971ms step_avg:38.41ms
step:1354/2330 train_time:52012ms step_avg:38.41ms
step:1355/2330 train_time:52048ms step_avg:38.41ms
step:1356/2330 train_time:52089ms step_avg:38.41ms
step:1357/2330 train_time:52126ms step_avg:38.41ms
step:1358/2330 train_time:52166ms step_avg:38.41ms
step:1359/2330 train_time:52203ms step_avg:38.41ms
step:1360/2330 train_time:52244ms step_avg:38.41ms
step:1361/2330 train_time:52280ms step_avg:38.41ms
step:1362/2330 train_time:52320ms step_avg:38.41ms
step:1363/2330 train_time:52356ms step_avg:38.41ms
step:1364/2330 train_time:52397ms step_avg:38.41ms
step:1365/2330 train_time:52433ms step_avg:38.41ms
step:1366/2330 train_time:52475ms step_avg:38.41ms
step:1367/2330 train_time:52511ms step_avg:38.41ms
step:1368/2330 train_time:52552ms step_avg:38.42ms
step:1369/2330 train_time:52589ms step_avg:38.41ms
step:1370/2330 train_time:52629ms step_avg:38.42ms
step:1371/2330 train_time:52667ms step_avg:38.42ms
step:1372/2330 train_time:52708ms step_avg:38.42ms
step:1373/2330 train_time:52744ms step_avg:38.42ms
step:1374/2330 train_time:52785ms step_avg:38.42ms
step:1375/2330 train_time:52821ms step_avg:38.42ms
step:1376/2330 train_time:52862ms step_avg:38.42ms
step:1377/2330 train_time:52899ms step_avg:38.42ms
step:1378/2330 train_time:52939ms step_avg:38.42ms
step:1379/2330 train_time:52976ms step_avg:38.42ms
step:1380/2330 train_time:53017ms step_avg:38.42ms
step:1381/2330 train_time:53053ms step_avg:38.42ms
step:1382/2330 train_time:53094ms step_avg:38.42ms
step:1383/2330 train_time:53130ms step_avg:38.42ms
step:1384/2330 train_time:53171ms step_avg:38.42ms
step:1385/2330 train_time:53208ms step_avg:38.42ms
step:1386/2330 train_time:53248ms step_avg:38.42ms
step:1387/2330 train_time:53285ms step_avg:38.42ms
step:1388/2330 train_time:53325ms step_avg:38.42ms
step:1389/2330 train_time:53361ms step_avg:38.42ms
step:1390/2330 train_time:53401ms step_avg:38.42ms
step:1391/2330 train_time:53437ms step_avg:38.42ms
step:1392/2330 train_time:53478ms step_avg:38.42ms
step:1393/2330 train_time:53515ms step_avg:38.42ms
step:1394/2330 train_time:53556ms step_avg:38.42ms
step:1395/2330 train_time:53592ms step_avg:38.42ms
step:1396/2330 train_time:53634ms step_avg:38.42ms
step:1397/2330 train_time:53670ms step_avg:38.42ms
step:1398/2330 train_time:53711ms step_avg:38.42ms
step:1399/2330 train_time:53748ms step_avg:38.42ms
step:1400/2330 train_time:53788ms step_avg:38.42ms
step:1401/2330 train_time:53825ms step_avg:38.42ms
step:1402/2330 train_time:53866ms step_avg:38.42ms
step:1403/2330 train_time:53902ms step_avg:38.42ms
step:1404/2330 train_time:53943ms step_avg:38.42ms
step:1405/2330 train_time:53979ms step_avg:38.42ms
step:1406/2330 train_time:54019ms step_avg:38.42ms
step:1407/2330 train_time:54056ms step_avg:38.42ms
step:1408/2330 train_time:54097ms step_avg:38.42ms
step:1409/2330 train_time:54134ms step_avg:38.42ms
step:1410/2330 train_time:54175ms step_avg:38.42ms
step:1411/2330 train_time:54211ms step_avg:38.42ms
step:1412/2330 train_time:54252ms step_avg:38.42ms
step:1413/2330 train_time:54289ms step_avg:38.42ms
step:1414/2330 train_time:54329ms step_avg:38.42ms
step:1415/2330 train_time:54366ms step_avg:38.42ms
step:1416/2330 train_time:54406ms step_avg:38.42ms
step:1417/2330 train_time:54442ms step_avg:38.42ms
step:1418/2330 train_time:54483ms step_avg:38.42ms
step:1419/2330 train_time:54518ms step_avg:38.42ms
step:1420/2330 train_time:54559ms step_avg:38.42ms
step:1421/2330 train_time:54595ms step_avg:38.42ms
step:1422/2330 train_time:54637ms step_avg:38.42ms
step:1423/2330 train_time:54674ms step_avg:38.42ms
step:1424/2330 train_time:54714ms step_avg:38.42ms
step:1425/2330 train_time:54751ms step_avg:38.42ms
step:1426/2330 train_time:54792ms step_avg:38.42ms
step:1427/2330 train_time:54828ms step_avg:38.42ms
step:1428/2330 train_time:54869ms step_avg:38.42ms
step:1429/2330 train_time:54904ms step_avg:38.42ms
step:1430/2330 train_time:54945ms step_avg:38.42ms
step:1431/2330 train_time:54981ms step_avg:38.42ms
step:1432/2330 train_time:55022ms step_avg:38.42ms
step:1433/2330 train_time:55059ms step_avg:38.42ms
step:1434/2330 train_time:55099ms step_avg:38.42ms
step:1435/2330 train_time:55136ms step_avg:38.42ms
step:1436/2330 train_time:55177ms step_avg:38.42ms
step:1437/2330 train_time:55214ms step_avg:38.42ms
step:1438/2330 train_time:55255ms step_avg:38.42ms
step:1439/2330 train_time:55291ms step_avg:38.42ms
step:1440/2330 train_time:55333ms step_avg:38.43ms
step:1441/2330 train_time:55369ms step_avg:38.42ms
step:1442/2330 train_time:55410ms step_avg:38.43ms
step:1443/2330 train_time:55446ms step_avg:38.42ms
step:1444/2330 train_time:55487ms step_avg:38.43ms
step:1445/2330 train_time:55523ms step_avg:38.42ms
step:1446/2330 train_time:55564ms step_avg:38.43ms
step:1447/2330 train_time:55600ms step_avg:38.42ms
step:1448/2330 train_time:55641ms step_avg:38.43ms
step:1449/2330 train_time:55676ms step_avg:38.42ms
step:1450/2330 train_time:55718ms step_avg:38.43ms
step:1451/2330 train_time:55754ms step_avg:38.42ms
step:1452/2330 train_time:55795ms step_avg:38.43ms
step:1453/2330 train_time:55832ms step_avg:38.43ms
step:1454/2330 train_time:55873ms step_avg:38.43ms
step:1455/2330 train_time:55908ms step_avg:38.43ms
step:1456/2330 train_time:55949ms step_avg:38.43ms
step:1457/2330 train_time:55985ms step_avg:38.42ms
step:1458/2330 train_time:56026ms step_avg:38.43ms
step:1459/2330 train_time:56061ms step_avg:38.42ms
step:1460/2330 train_time:56102ms step_avg:38.43ms
step:1461/2330 train_time:56138ms step_avg:38.42ms
step:1462/2330 train_time:56180ms step_avg:38.43ms
step:1463/2330 train_time:56216ms step_avg:38.42ms
step:1464/2330 train_time:56257ms step_avg:38.43ms
step:1465/2330 train_time:56294ms step_avg:38.43ms
step:1466/2330 train_time:56335ms step_avg:38.43ms
step:1467/2330 train_time:56370ms step_avg:38.43ms
step:1468/2330 train_time:56412ms step_avg:38.43ms
step:1469/2330 train_time:56448ms step_avg:38.43ms
step:1470/2330 train_time:56488ms step_avg:38.43ms
step:1471/2330 train_time:56527ms step_avg:38.43ms
step:1472/2330 train_time:56567ms step_avg:38.43ms
step:1473/2330 train_time:56604ms step_avg:38.43ms
step:1474/2330 train_time:56645ms step_avg:38.43ms
step:1475/2330 train_time:56680ms step_avg:38.43ms
step:1476/2330 train_time:56720ms step_avg:38.43ms
step:1477/2330 train_time:56757ms step_avg:38.43ms
step:1478/2330 train_time:56798ms step_avg:38.43ms
step:1479/2330 train_time:56835ms step_avg:38.43ms
step:1480/2330 train_time:56876ms step_avg:38.43ms
step:1481/2330 train_time:56912ms step_avg:38.43ms
step:1482/2330 train_time:56954ms step_avg:38.43ms
step:1483/2330 train_time:56990ms step_avg:38.43ms
step:1484/2330 train_time:57031ms step_avg:38.43ms
step:1485/2330 train_time:57068ms step_avg:38.43ms
step:1486/2330 train_time:57108ms step_avg:38.43ms
step:1487/2330 train_time:57145ms step_avg:38.43ms
step:1488/2330 train_time:57186ms step_avg:38.43ms
step:1489/2330 train_time:57221ms step_avg:38.43ms
step:1490/2330 train_time:57262ms step_avg:38.43ms
step:1491/2330 train_time:57299ms step_avg:38.43ms
step:1492/2330 train_time:57340ms step_avg:38.43ms
step:1493/2330 train_time:57376ms step_avg:38.43ms
step:1494/2330 train_time:57418ms step_avg:38.43ms
step:1495/2330 train_time:57454ms step_avg:38.43ms
step:1496/2330 train_time:57496ms step_avg:38.43ms
step:1497/2330 train_time:57532ms step_avg:38.43ms
step:1498/2330 train_time:57573ms step_avg:38.43ms
step:1499/2330 train_time:57608ms step_avg:38.43ms
step:1500/2330 train_time:57649ms step_avg:38.43ms
step:1500/2330 val_loss:5.2474 train_time:57763ms step_avg:38.51ms
step:1501/2330 train_time:57775ms step_avg:38.49ms
step:1502/2330 train_time:57786ms step_avg:38.47ms
step:1503/2330 train_time:57795ms step_avg:38.45ms
step:1504/2330 train_time:57806ms step_avg:38.43ms
step:1505/2330 train_time:57840ms step_avg:38.43ms
step:1506/2330 train_time:57881ms step_avg:38.43ms
step:1507/2330 train_time:57916ms step_avg:38.43ms
step:1508/2330 train_time:57957ms step_avg:38.43ms
step:1509/2330 train_time:57993ms step_avg:38.43ms
step:1510/2330 train_time:58034ms step_avg:38.43ms
step:1511/2330 train_time:58071ms step_avg:38.43ms
step:1512/2330 train_time:58113ms step_avg:38.43ms
step:1513/2330 train_time:58152ms step_avg:38.43ms
step:1514/2330 train_time:58193ms step_avg:38.44ms
step:1515/2330 train_time:58230ms step_avg:38.44ms
step:1516/2330 train_time:58271ms step_avg:38.44ms
step:1517/2330 train_time:58306ms step_avg:38.43ms
step:1518/2330 train_time:58348ms step_avg:38.44ms
step:1519/2330 train_time:58383ms step_avg:38.44ms
step:1520/2330 train_time:58424ms step_avg:38.44ms
step:1521/2330 train_time:58459ms step_avg:38.43ms
step:1522/2330 train_time:58500ms step_avg:38.44ms
step:1523/2330 train_time:58536ms step_avg:38.43ms
step:1524/2330 train_time:58577ms step_avg:38.44ms
step:1525/2330 train_time:58612ms step_avg:38.43ms
step:1526/2330 train_time:58654ms step_avg:38.44ms
step:1527/2330 train_time:58690ms step_avg:38.43ms
step:1528/2330 train_time:58731ms step_avg:38.44ms
step:1529/2330 train_time:58768ms step_avg:38.44ms
step:1530/2330 train_time:58809ms step_avg:38.44ms
step:1531/2330 train_time:58844ms step_avg:38.44ms
step:1532/2330 train_time:58885ms step_avg:38.44ms
step:1533/2330 train_time:58920ms step_avg:38.43ms
step:1534/2330 train_time:58961ms step_avg:38.44ms
step:1535/2330 train_time:58998ms step_avg:38.44ms
step:1536/2330 train_time:59039ms step_avg:38.44ms
step:1537/2330 train_time:59077ms step_avg:38.44ms
step:1538/2330 train_time:59118ms step_avg:38.44ms
step:1539/2330 train_time:59156ms step_avg:38.44ms
step:1540/2330 train_time:59197ms step_avg:38.44ms
step:1541/2330 train_time:59234ms step_avg:38.44ms
step:1542/2330 train_time:59275ms step_avg:38.44ms
step:1543/2330 train_time:59311ms step_avg:38.44ms
step:1544/2330 train_time:59352ms step_avg:38.44ms
step:1545/2330 train_time:59387ms step_avg:38.44ms
step:1546/2330 train_time:59428ms step_avg:38.44ms
step:1547/2330 train_time:59463ms step_avg:38.44ms
step:1548/2330 train_time:59504ms step_avg:38.44ms
step:1549/2330 train_time:59540ms step_avg:38.44ms
step:1550/2330 train_time:59581ms step_avg:38.44ms
step:1551/2330 train_time:59618ms step_avg:38.44ms
step:1552/2330 train_time:59658ms step_avg:38.44ms
step:1553/2330 train_time:59695ms step_avg:38.44ms
step:1554/2330 train_time:59735ms step_avg:38.44ms
step:1555/2330 train_time:59772ms step_avg:38.44ms
step:1556/2330 train_time:59813ms step_avg:38.44ms
step:1557/2330 train_time:59849ms step_avg:38.44ms
step:1558/2330 train_time:59889ms step_avg:38.44ms
step:1559/2330 train_time:59925ms step_avg:38.44ms
step:1560/2330 train_time:59967ms step_avg:38.44ms
step:1561/2330 train_time:60003ms step_avg:38.44ms
step:1562/2330 train_time:60045ms step_avg:38.44ms
step:1563/2330 train_time:60082ms step_avg:38.44ms
step:1564/2330 train_time:60123ms step_avg:38.44ms
step:1565/2330 train_time:60160ms step_avg:38.44ms
step:1566/2330 train_time:60202ms step_avg:38.44ms
step:1567/2330 train_time:60240ms step_avg:38.44ms
step:1568/2330 train_time:60280ms step_avg:38.44ms
step:1569/2330 train_time:60318ms step_avg:38.44ms
step:1570/2330 train_time:60359ms step_avg:38.45ms
step:1571/2330 train_time:60396ms step_avg:38.44ms
step:1572/2330 train_time:60437ms step_avg:38.45ms
step:1573/2330 train_time:60473ms step_avg:38.44ms
step:1574/2330 train_time:60514ms step_avg:38.45ms
step:1575/2330 train_time:60549ms step_avg:38.44ms
step:1576/2330 train_time:60589ms step_avg:38.44ms
step:1577/2330 train_time:60625ms step_avg:38.44ms
step:1578/2330 train_time:60666ms step_avg:38.45ms
step:1579/2330 train_time:60702ms step_avg:38.44ms
step:1580/2330 train_time:60743ms step_avg:38.44ms
step:1581/2330 train_time:60779ms step_avg:38.44ms
step:1582/2330 train_time:60820ms step_avg:38.44ms
step:1583/2330 train_time:60857ms step_avg:38.44ms
step:1584/2330 train_time:60897ms step_avg:38.45ms
step:1585/2330 train_time:60934ms step_avg:38.44ms
step:1586/2330 train_time:60975ms step_avg:38.45ms
step:1587/2330 train_time:61011ms step_avg:38.44ms
step:1588/2330 train_time:61052ms step_avg:38.45ms
step:1589/2330 train_time:61088ms step_avg:38.44ms
step:1590/2330 train_time:61130ms step_avg:38.45ms
step:1591/2330 train_time:61166ms step_avg:38.44ms
step:1592/2330 train_time:61207ms step_avg:38.45ms
step:1593/2330 train_time:61244ms step_avg:38.45ms
step:1594/2330 train_time:61286ms step_avg:38.45ms
step:1595/2330 train_time:61322ms step_avg:38.45ms
step:1596/2330 train_time:61363ms step_avg:38.45ms
step:1597/2330 train_time:61399ms step_avg:38.45ms
step:1598/2330 train_time:61441ms step_avg:38.45ms
step:1599/2330 train_time:61477ms step_avg:38.45ms
step:1600/2330 train_time:61517ms step_avg:38.45ms
step:1601/2330 train_time:61554ms step_avg:38.45ms
step:1602/2330 train_time:61595ms step_avg:38.45ms
step:1603/2330 train_time:61630ms step_avg:38.45ms
step:1604/2330 train_time:61671ms step_avg:38.45ms
step:1605/2330 train_time:61706ms step_avg:38.45ms
step:1606/2330 train_time:61747ms step_avg:38.45ms
step:1607/2330 train_time:61783ms step_avg:38.45ms
step:1608/2330 train_time:61825ms step_avg:38.45ms
step:1609/2330 train_time:61860ms step_avg:38.45ms
step:1610/2330 train_time:61902ms step_avg:38.45ms
step:1611/2330 train_time:61938ms step_avg:38.45ms
step:1612/2330 train_time:61979ms step_avg:38.45ms
step:1613/2330 train_time:62015ms step_avg:38.45ms
step:1614/2330 train_time:62056ms step_avg:38.45ms
step:1615/2330 train_time:62092ms step_avg:38.45ms
step:1616/2330 train_time:62133ms step_avg:38.45ms
step:1617/2330 train_time:62169ms step_avg:38.45ms
step:1618/2330 train_time:62211ms step_avg:38.45ms
step:1619/2330 train_time:62247ms step_avg:38.45ms
step:1620/2330 train_time:62287ms step_avg:38.45ms
step:1621/2330 train_time:62324ms step_avg:38.45ms
step:1622/2330 train_time:62366ms step_avg:38.45ms
step:1623/2330 train_time:62402ms step_avg:38.45ms
step:1624/2330 train_time:62444ms step_avg:38.45ms
step:1625/2330 train_time:62480ms step_avg:38.45ms
step:1626/2330 train_time:62521ms step_avg:38.45ms
step:1627/2330 train_time:62558ms step_avg:38.45ms
step:1628/2330 train_time:62599ms step_avg:38.45ms
step:1629/2330 train_time:62635ms step_avg:38.45ms
step:1630/2330 train_time:62676ms step_avg:38.45ms
step:1631/2330 train_time:62713ms step_avg:38.45ms
step:1632/2330 train_time:62753ms step_avg:38.45ms
step:1633/2330 train_time:62788ms step_avg:38.45ms
step:1634/2330 train_time:62829ms step_avg:38.45ms
step:1635/2330 train_time:62865ms step_avg:38.45ms
step:1636/2330 train_time:62906ms step_avg:38.45ms
step:1637/2330 train_time:62942ms step_avg:38.45ms
step:1638/2330 train_time:62983ms step_avg:38.45ms
step:1639/2330 train_time:63019ms step_avg:38.45ms
step:1640/2330 train_time:63060ms step_avg:38.45ms
step:1641/2330 train_time:63096ms step_avg:38.45ms
step:1642/2330 train_time:63137ms step_avg:38.45ms
step:1643/2330 train_time:63174ms step_avg:38.45ms
step:1644/2330 train_time:63215ms step_avg:38.45ms
step:1645/2330 train_time:63250ms step_avg:38.45ms
step:1646/2330 train_time:63291ms step_avg:38.45ms
step:1647/2330 train_time:63327ms step_avg:38.45ms
step:1648/2330 train_time:63368ms step_avg:38.45ms
step:1649/2330 train_time:63404ms step_avg:38.45ms
step:1650/2330 train_time:63446ms step_avg:38.45ms
step:1651/2330 train_time:63482ms step_avg:38.45ms
step:1652/2330 train_time:63524ms step_avg:38.45ms
step:1653/2330 train_time:63560ms step_avg:38.45ms
step:1654/2330 train_time:63601ms step_avg:38.45ms
step:1655/2330 train_time:63638ms step_avg:38.45ms
step:1656/2330 train_time:63679ms step_avg:38.45ms
step:1657/2330 train_time:63716ms step_avg:38.45ms
step:1658/2330 train_time:63756ms step_avg:38.45ms
step:1659/2330 train_time:63792ms step_avg:38.45ms
step:1660/2330 train_time:63833ms step_avg:38.45ms
step:1661/2330 train_time:63868ms step_avg:38.45ms
step:1662/2330 train_time:63909ms step_avg:38.45ms
step:1663/2330 train_time:63945ms step_avg:38.45ms
step:1664/2330 train_time:63986ms step_avg:38.45ms
step:1665/2330 train_time:64022ms step_avg:38.45ms
step:1666/2330 train_time:64063ms step_avg:38.45ms
step:1667/2330 train_time:64100ms step_avg:38.45ms
step:1668/2330 train_time:64141ms step_avg:38.45ms
step:1669/2330 train_time:64177ms step_avg:38.45ms
step:1670/2330 train_time:64218ms step_avg:38.45ms
step:1671/2330 train_time:64255ms step_avg:38.45ms
step:1672/2330 train_time:64296ms step_avg:38.45ms
step:1673/2330 train_time:64333ms step_avg:38.45ms
step:1674/2330 train_time:64373ms step_avg:38.45ms
step:1675/2330 train_time:64410ms step_avg:38.45ms
step:1676/2330 train_time:64450ms step_avg:38.45ms
step:1677/2330 train_time:64487ms step_avg:38.45ms
step:1678/2330 train_time:64528ms step_avg:38.46ms
step:1679/2330 train_time:64564ms step_avg:38.45ms
step:1680/2330 train_time:64606ms step_avg:38.46ms
step:1681/2330 train_time:64642ms step_avg:38.45ms
step:1682/2330 train_time:64682ms step_avg:38.46ms
step:1683/2330 train_time:64719ms step_avg:38.45ms
step:1684/2330 train_time:64760ms step_avg:38.46ms
step:1685/2330 train_time:64797ms step_avg:38.45ms
step:1686/2330 train_time:64837ms step_avg:38.46ms
step:1687/2330 train_time:64872ms step_avg:38.45ms
step:1688/2330 train_time:64913ms step_avg:38.46ms
step:1689/2330 train_time:64948ms step_avg:38.45ms
step:1690/2330 train_time:64989ms step_avg:38.46ms
step:1691/2330 train_time:65025ms step_avg:38.45ms
step:1692/2330 train_time:65067ms step_avg:38.46ms
step:1693/2330 train_time:65103ms step_avg:38.45ms
step:1694/2330 train_time:65145ms step_avg:38.46ms
step:1695/2330 train_time:65181ms step_avg:38.45ms
step:1696/2330 train_time:65222ms step_avg:38.46ms
step:1697/2330 train_time:65259ms step_avg:38.46ms
step:1698/2330 train_time:65300ms step_avg:38.46ms
step:1699/2330 train_time:65338ms step_avg:38.46ms
step:1700/2330 train_time:65378ms step_avg:38.46ms
step:1701/2330 train_time:65415ms step_avg:38.46ms
step:1702/2330 train_time:65456ms step_avg:38.46ms
step:1703/2330 train_time:65492ms step_avg:38.46ms
step:1704/2330 train_time:65533ms step_avg:38.46ms
step:1705/2330 train_time:65568ms step_avg:38.46ms
step:1706/2330 train_time:65609ms step_avg:38.46ms
step:1707/2330 train_time:65644ms step_avg:38.46ms
step:1708/2330 train_time:65686ms step_avg:38.46ms
step:1709/2330 train_time:65722ms step_avg:38.46ms
step:1710/2330 train_time:65764ms step_avg:38.46ms
step:1711/2330 train_time:65800ms step_avg:38.46ms
step:1712/2330 train_time:65841ms step_avg:38.46ms
step:1713/2330 train_time:65877ms step_avg:38.46ms
step:1714/2330 train_time:65918ms step_avg:38.46ms
step:1715/2330 train_time:65954ms step_avg:38.46ms
step:1716/2330 train_time:65994ms step_avg:38.46ms
step:1717/2330 train_time:66030ms step_avg:38.46ms
step:1718/2330 train_time:66071ms step_avg:38.46ms
step:1719/2330 train_time:66108ms step_avg:38.46ms
step:1720/2330 train_time:66148ms step_avg:38.46ms
step:1721/2330 train_time:66184ms step_avg:38.46ms
step:1722/2330 train_time:66226ms step_avg:38.46ms
step:1723/2330 train_time:66262ms step_avg:38.46ms
step:1724/2330 train_time:66303ms step_avg:38.46ms
step:1725/2330 train_time:66339ms step_avg:38.46ms
step:1726/2330 train_time:66379ms step_avg:38.46ms
step:1727/2330 train_time:66416ms step_avg:38.46ms
step:1728/2330 train_time:66457ms step_avg:38.46ms
step:1729/2330 train_time:66494ms step_avg:38.46ms
step:1730/2330 train_time:66535ms step_avg:38.46ms
step:1731/2330 train_time:66570ms step_avg:38.46ms
step:1732/2330 train_time:66612ms step_avg:38.46ms
step:1733/2330 train_time:66647ms step_avg:38.46ms
step:1734/2330 train_time:66689ms step_avg:38.46ms
step:1735/2330 train_time:66725ms step_avg:38.46ms
step:1736/2330 train_time:66767ms step_avg:38.46ms
step:1737/2330 train_time:66803ms step_avg:38.46ms
step:1738/2330 train_time:66844ms step_avg:38.46ms
step:1739/2330 train_time:66879ms step_avg:38.46ms
step:1740/2330 train_time:66921ms step_avg:38.46ms
step:1741/2330 train_time:66957ms step_avg:38.46ms
step:1742/2330 train_time:66997ms step_avg:38.46ms
step:1743/2330 train_time:67033ms step_avg:38.46ms
step:1744/2330 train_time:67074ms step_avg:38.46ms
step:1745/2330 train_time:67110ms step_avg:38.46ms
step:1746/2330 train_time:67150ms step_avg:38.46ms
step:1747/2330 train_time:67187ms step_avg:38.46ms
step:1748/2330 train_time:67227ms step_avg:38.46ms
step:1749/2330 train_time:67264ms step_avg:38.46ms
step:1750/2330 train_time:67305ms step_avg:38.46ms
step:1750/2330 val_loss:5.2090 train_time:67420ms step_avg:38.53ms
step:1751/2330 train_time:67432ms step_avg:38.51ms
step:1752/2330 train_time:67445ms step_avg:38.50ms
step:1753/2330 train_time:67455ms step_avg:38.48ms
step:1754/2330 train_time:67467ms step_avg:38.46ms
step:1755/2330 train_time:67497ms step_avg:38.46ms
step:1756/2330 train_time:67538ms step_avg:38.46ms
step:1757/2330 train_time:67573ms step_avg:38.46ms
step:1758/2330 train_time:67613ms step_avg:38.46ms
step:1759/2330 train_time:67648ms step_avg:38.46ms
step:1760/2330 train_time:67689ms step_avg:38.46ms
step:1761/2330 train_time:67726ms step_avg:38.46ms
step:1762/2330 train_time:67769ms step_avg:38.46ms
step:1763/2330 train_time:67809ms step_avg:38.46ms
step:1764/2330 train_time:67851ms step_avg:38.46ms
step:1765/2330 train_time:67885ms step_avg:38.46ms
step:1766/2330 train_time:67926ms step_avg:38.46ms
step:1767/2330 train_time:67962ms step_avg:38.46ms
step:1768/2330 train_time:68003ms step_avg:38.46ms
step:1769/2330 train_time:68039ms step_avg:38.46ms
step:1770/2330 train_time:68080ms step_avg:38.46ms
step:1771/2330 train_time:68115ms step_avg:38.46ms
step:1772/2330 train_time:68156ms step_avg:38.46ms
step:1773/2330 train_time:68192ms step_avg:38.46ms
step:1774/2330 train_time:68233ms step_avg:38.46ms
step:1775/2330 train_time:68269ms step_avg:38.46ms
step:1776/2330 train_time:68310ms step_avg:38.46ms
step:1777/2330 train_time:68345ms step_avg:38.46ms
step:1778/2330 train_time:68387ms step_avg:38.46ms
step:1779/2330 train_time:68423ms step_avg:38.46ms
step:1780/2330 train_time:68464ms step_avg:38.46ms
step:1781/2330 train_time:68500ms step_avg:38.46ms
step:1782/2330 train_time:68541ms step_avg:38.46ms
step:1783/2330 train_time:68576ms step_avg:38.46ms
step:1784/2330 train_time:68617ms step_avg:38.46ms
step:1785/2330 train_time:68653ms step_avg:38.46ms
step:1786/2330 train_time:68694ms step_avg:38.46ms
step:1787/2330 train_time:68733ms step_avg:38.46ms
step:1788/2330 train_time:68774ms step_avg:38.46ms
step:1789/2330 train_time:68813ms step_avg:38.46ms
step:1790/2330 train_time:68854ms step_avg:38.47ms
step:1791/2330 train_time:68891ms step_avg:38.47ms
step:1792/2330 train_time:68932ms step_avg:38.47ms
step:1793/2330 train_time:68968ms step_avg:38.47ms
step:1794/2330 train_time:69009ms step_avg:38.47ms
step:1795/2330 train_time:69044ms step_avg:38.46ms
step:1796/2330 train_time:69085ms step_avg:38.47ms
step:1797/2330 train_time:69121ms step_avg:38.46ms
step:1798/2330 train_time:69162ms step_avg:38.47ms
step:1799/2330 train_time:69198ms step_avg:38.46ms
step:1800/2330 train_time:69239ms step_avg:38.47ms
step:1801/2330 train_time:69275ms step_avg:38.46ms
step:1802/2330 train_time:69315ms step_avg:38.47ms
step:1803/2330 train_time:69352ms step_avg:38.46ms
step:1804/2330 train_time:69392ms step_avg:38.47ms
step:1805/2330 train_time:69429ms step_avg:38.46ms
step:1806/2330 train_time:69470ms step_avg:38.47ms
step:1807/2330 train_time:69505ms step_avg:38.46ms
step:1808/2330 train_time:69546ms step_avg:38.47ms
step:1809/2330 train_time:69581ms step_avg:38.46ms
step:1810/2330 train_time:69623ms step_avg:38.47ms
step:1811/2330 train_time:69659ms step_avg:38.46ms
step:1812/2330 train_time:69701ms step_avg:38.47ms
step:1813/2330 train_time:69737ms step_avg:38.46ms
step:1814/2330 train_time:69779ms step_avg:38.47ms
step:1815/2330 train_time:69816ms step_avg:38.47ms
step:1816/2330 train_time:69858ms step_avg:38.47ms
step:1817/2330 train_time:69893ms step_avg:38.47ms
step:1818/2330 train_time:69934ms step_avg:38.47ms
step:1819/2330 train_time:69972ms step_avg:38.47ms
step:1820/2330 train_time:70013ms step_avg:38.47ms
step:1821/2330 train_time:70050ms step_avg:38.47ms
step:1822/2330 train_time:70090ms step_avg:38.47ms
step:1823/2330 train_time:70127ms step_avg:38.47ms
step:1824/2330 train_time:70168ms step_avg:38.47ms
step:1825/2330 train_time:70203ms step_avg:38.47ms
step:1826/2330 train_time:70244ms step_avg:38.47ms
step:1827/2330 train_time:70280ms step_avg:38.47ms
step:1828/2330 train_time:70321ms step_avg:38.47ms
step:1829/2330 train_time:70356ms step_avg:38.47ms
step:1830/2330 train_time:70397ms step_avg:38.47ms
step:1831/2330 train_time:70433ms step_avg:38.47ms
step:1832/2330 train_time:70474ms step_avg:38.47ms
step:1833/2330 train_time:70510ms step_avg:38.47ms
step:1834/2330 train_time:70551ms step_avg:38.47ms
step:1835/2330 train_time:70587ms step_avg:38.47ms
step:1836/2330 train_time:70628ms step_avg:38.47ms
step:1837/2330 train_time:70664ms step_avg:38.47ms
step:1838/2330 train_time:70705ms step_avg:38.47ms
step:1839/2330 train_time:70742ms step_avg:38.47ms
step:1840/2330 train_time:70784ms step_avg:38.47ms
step:1841/2330 train_time:70820ms step_avg:38.47ms
step:1842/2330 train_time:70863ms step_avg:38.47ms
step:1843/2330 train_time:70898ms step_avg:38.47ms
step:1844/2330 train_time:70940ms step_avg:38.47ms
step:1845/2330 train_time:70975ms step_avg:38.47ms
step:1846/2330 train_time:71017ms step_avg:38.47ms
step:1847/2330 train_time:71054ms step_avg:38.47ms
step:1848/2330 train_time:71095ms step_avg:38.47ms
step:1849/2330 train_time:71132ms step_avg:38.47ms
step:1850/2330 train_time:71172ms step_avg:38.47ms
step:1851/2330 train_time:71209ms step_avg:38.47ms
step:1852/2330 train_time:71250ms step_avg:38.47ms
step:1853/2330 train_time:71286ms step_avg:38.47ms
step:1854/2330 train_time:71326ms step_avg:38.47ms
step:1855/2330 train_time:71362ms step_avg:38.47ms
step:1856/2330 train_time:71403ms step_avg:38.47ms
step:1857/2330 train_time:71439ms step_avg:38.47ms
step:1858/2330 train_time:71480ms step_avg:38.47ms
step:1859/2330 train_time:71516ms step_avg:38.47ms
step:1860/2330 train_time:71558ms step_avg:38.47ms
step:1861/2330 train_time:71594ms step_avg:38.47ms
step:1862/2330 train_time:71635ms step_avg:38.47ms
step:1863/2330 train_time:71672ms step_avg:38.47ms
step:1864/2330 train_time:71712ms step_avg:38.47ms
step:1865/2330 train_time:71749ms step_avg:38.47ms
step:1866/2330 train_time:71790ms step_avg:38.47ms
step:1867/2330 train_time:71828ms step_avg:38.47ms
step:1868/2330 train_time:71869ms step_avg:38.47ms
step:1869/2330 train_time:71905ms step_avg:38.47ms
step:1870/2330 train_time:71947ms step_avg:38.47ms
step:1871/2330 train_time:71982ms step_avg:38.47ms
step:1872/2330 train_time:72023ms step_avg:38.47ms
step:1873/2330 train_time:72059ms step_avg:38.47ms
step:1874/2330 train_time:72102ms step_avg:38.47ms
step:1875/2330 train_time:72137ms step_avg:38.47ms
step:1876/2330 train_time:72178ms step_avg:38.47ms
step:1877/2330 train_time:72213ms step_avg:38.47ms
step:1878/2330 train_time:72254ms step_avg:38.47ms
step:1879/2330 train_time:72290ms step_avg:38.47ms
step:1880/2330 train_time:72331ms step_avg:38.47ms
step:1881/2330 train_time:72366ms step_avg:38.47ms
step:1882/2330 train_time:72407ms step_avg:38.47ms
step:1883/2330 train_time:72442ms step_avg:38.47ms
step:1884/2330 train_time:72483ms step_avg:38.47ms
step:1885/2330 train_time:72519ms step_avg:38.47ms
step:1886/2330 train_time:72561ms step_avg:38.47ms
step:1887/2330 train_time:72596ms step_avg:38.47ms
step:1888/2330 train_time:72637ms step_avg:38.47ms
step:1889/2330 train_time:72674ms step_avg:38.47ms
step:1890/2330 train_time:72715ms step_avg:38.47ms
step:1891/2330 train_time:72753ms step_avg:38.47ms
step:1892/2330 train_time:72793ms step_avg:38.47ms
step:1893/2330 train_time:72831ms step_avg:38.47ms
step:1894/2330 train_time:72872ms step_avg:38.47ms
step:1895/2330 train_time:72908ms step_avg:38.47ms
step:1896/2330 train_time:72949ms step_avg:38.48ms
step:1897/2330 train_time:72985ms step_avg:38.47ms
step:1898/2330 train_time:73026ms step_avg:38.48ms
step:1899/2330 train_time:73062ms step_avg:38.47ms
step:1900/2330 train_time:73103ms step_avg:38.48ms
step:1901/2330 train_time:73139ms step_avg:38.47ms
step:1902/2330 train_time:73181ms step_avg:38.48ms
step:1903/2330 train_time:73217ms step_avg:38.47ms
step:1904/2330 train_time:73258ms step_avg:38.48ms
step:1905/2330 train_time:73294ms step_avg:38.47ms
step:1906/2330 train_time:73335ms step_avg:38.48ms
step:1907/2330 train_time:73371ms step_avg:38.47ms
step:1908/2330 train_time:73412ms step_avg:38.48ms
step:1909/2330 train_time:73448ms step_avg:38.47ms
step:1910/2330 train_time:73489ms step_avg:38.48ms
step:1911/2330 train_time:73525ms step_avg:38.47ms
step:1912/2330 train_time:73566ms step_avg:38.48ms
step:1913/2330 train_time:73602ms step_avg:38.47ms
step:1914/2330 train_time:73643ms step_avg:38.48ms
step:1915/2330 train_time:73679ms step_avg:38.47ms
step:1916/2330 train_time:73721ms step_avg:38.48ms
step:1917/2330 train_time:73757ms step_avg:38.48ms
step:1918/2330 train_time:73798ms step_avg:38.48ms
step:1919/2330 train_time:73835ms step_avg:38.48ms
step:1920/2330 train_time:73876ms step_avg:38.48ms
step:1921/2330 train_time:73914ms step_avg:38.48ms
step:1922/2330 train_time:73954ms step_avg:38.48ms
step:1923/2330 train_time:73992ms step_avg:38.48ms
step:1924/2330 train_time:74032ms step_avg:38.48ms
step:1925/2330 train_time:74069ms step_avg:38.48ms
step:1926/2330 train_time:74110ms step_avg:38.48ms
step:1927/2330 train_time:74145ms step_avg:38.48ms
step:1928/2330 train_time:74186ms step_avg:38.48ms
step:1929/2330 train_time:74222ms step_avg:38.48ms
step:1930/2330 train_time:74263ms step_avg:38.48ms
step:1931/2330 train_time:74299ms step_avg:38.48ms
step:1932/2330 train_time:74340ms step_avg:38.48ms
step:1933/2330 train_time:74376ms step_avg:38.48ms
step:1934/2330 train_time:74417ms step_avg:38.48ms
step:1935/2330 train_time:74453ms step_avg:38.48ms
step:1936/2330 train_time:74494ms step_avg:38.48ms
step:1937/2330 train_time:74531ms step_avg:38.48ms
step:1938/2330 train_time:74572ms step_avg:38.48ms
step:1939/2330 train_time:74608ms step_avg:38.48ms
step:1940/2330 train_time:74649ms step_avg:38.48ms
step:1941/2330 train_time:74685ms step_avg:38.48ms
step:1942/2330 train_time:74727ms step_avg:38.48ms
step:1943/2330 train_time:74762ms step_avg:38.48ms
step:1944/2330 train_time:74804ms step_avg:38.48ms
step:1945/2330 train_time:74841ms step_avg:38.48ms
step:1946/2330 train_time:74883ms step_avg:38.48ms
step:1947/2330 train_time:74919ms step_avg:38.48ms
step:1948/2330 train_time:74960ms step_avg:38.48ms
step:1949/2330 train_time:74995ms step_avg:38.48ms
step:1950/2330 train_time:75037ms step_avg:38.48ms
step:1951/2330 train_time:75073ms step_avg:38.48ms
step:1952/2330 train_time:75114ms step_avg:38.48ms
step:1953/2330 train_time:75151ms step_avg:38.48ms
step:1954/2330 train_time:75192ms step_avg:38.48ms
step:1955/2330 train_time:75228ms step_avg:38.48ms
step:1956/2330 train_time:75269ms step_avg:38.48ms
step:1957/2330 train_time:75304ms step_avg:38.48ms
step:1958/2330 train_time:75345ms step_avg:38.48ms
step:1959/2330 train_time:75381ms step_avg:38.48ms
step:1960/2330 train_time:75422ms step_avg:38.48ms
step:1961/2330 train_time:75458ms step_avg:38.48ms
step:1962/2330 train_time:75499ms step_avg:38.48ms
step:1963/2330 train_time:75535ms step_avg:38.48ms
step:1964/2330 train_time:75576ms step_avg:38.48ms
step:1965/2330 train_time:75612ms step_avg:38.48ms
step:1966/2330 train_time:75653ms step_avg:38.48ms
step:1967/2330 train_time:75690ms step_avg:38.48ms
step:1968/2330 train_time:75730ms step_avg:38.48ms
step:1969/2330 train_time:75767ms step_avg:38.48ms
step:1970/2330 train_time:75807ms step_avg:38.48ms
step:1971/2330 train_time:75843ms step_avg:38.48ms
step:1972/2330 train_time:75884ms step_avg:38.48ms
step:1973/2330 train_time:75921ms step_avg:38.48ms
step:1974/2330 train_time:75962ms step_avg:38.48ms
step:1975/2330 train_time:75999ms step_avg:38.48ms
step:1976/2330 train_time:76041ms step_avg:38.48ms
step:1977/2330 train_time:76077ms step_avg:38.48ms
step:1978/2330 train_time:76118ms step_avg:38.48ms
step:1979/2330 train_time:76154ms step_avg:38.48ms
step:1980/2330 train_time:76195ms step_avg:38.48ms
step:1981/2330 train_time:76231ms step_avg:38.48ms
step:1982/2330 train_time:76272ms step_avg:38.48ms
step:1983/2330 train_time:76307ms step_avg:38.48ms
step:1984/2330 train_time:76348ms step_avg:38.48ms
step:1985/2330 train_time:76383ms step_avg:38.48ms
step:1986/2330 train_time:76424ms step_avg:38.48ms
step:1987/2330 train_time:76461ms step_avg:38.48ms
step:1988/2330 train_time:76502ms step_avg:38.48ms
step:1989/2330 train_time:76538ms step_avg:38.48ms
step:1990/2330 train_time:76580ms step_avg:38.48ms
step:1991/2330 train_time:76616ms step_avg:38.48ms
step:1992/2330 train_time:76657ms step_avg:38.48ms
step:1993/2330 train_time:76692ms step_avg:38.48ms
step:1994/2330 train_time:76733ms step_avg:38.48ms
step:1995/2330 train_time:76770ms step_avg:38.48ms
step:1996/2330 train_time:76811ms step_avg:38.48ms
step:1997/2330 train_time:76846ms step_avg:38.48ms
step:1998/2330 train_time:76887ms step_avg:38.48ms
step:1999/2330 train_time:76923ms step_avg:38.48ms
step:2000/2330 train_time:76964ms step_avg:38.48ms
step:2000/2330 val_loss:5.1774 train_time:77079ms step_avg:38.54ms
step:2001/2330 train_time:77091ms step_avg:38.53ms
step:2002/2330 train_time:77104ms step_avg:38.51ms
step:2003/2330 train_time:77114ms step_avg:38.50ms
step:2004/2330 train_time:77125ms step_avg:38.49ms
step:2005/2330 train_time:77156ms step_avg:38.48ms
step:2006/2330 train_time:77196ms step_avg:38.48ms
step:2007/2330 train_time:77231ms step_avg:38.48ms
step:2008/2330 train_time:77272ms step_avg:38.48ms
step:2009/2330 train_time:77307ms step_avg:38.48ms
step:2010/2330 train_time:77348ms step_avg:38.48ms
step:2011/2330 train_time:77384ms step_avg:38.48ms
step:2012/2330 train_time:77426ms step_avg:38.48ms
step:2013/2330 train_time:77470ms step_avg:38.48ms
step:2014/2330 train_time:77512ms step_avg:38.49ms
step:2015/2330 train_time:77549ms step_avg:38.49ms
step:2016/2330 train_time:77590ms step_avg:38.49ms
step:2017/2330 train_time:77627ms step_avg:38.49ms
step:2018/2330 train_time:77668ms step_avg:38.49ms
step:2019/2330 train_time:77704ms step_avg:38.49ms
step:2020/2330 train_time:77745ms step_avg:38.49ms
step:2021/2330 train_time:77781ms step_avg:38.49ms
step:2022/2330 train_time:77822ms step_avg:38.49ms
step:2023/2330 train_time:77857ms step_avg:38.49ms
step:2024/2330 train_time:77898ms step_avg:38.49ms
step:2025/2330 train_time:77932ms step_avg:38.49ms
step:2026/2330 train_time:77974ms step_avg:38.49ms
step:2027/2330 train_time:78009ms step_avg:38.48ms
step:2028/2330 train_time:78052ms step_avg:38.49ms
step:2029/2330 train_time:78087ms step_avg:38.49ms
step:2030/2330 train_time:78129ms step_avg:38.49ms
step:2031/2330 train_time:78164ms step_avg:38.49ms
step:2032/2330 train_time:78205ms step_avg:38.49ms
step:2033/2330 train_time:78240ms step_avg:38.48ms
step:2034/2330 train_time:78281ms step_avg:38.49ms
step:2035/2330 train_time:78317ms step_avg:38.48ms
step:2036/2330 train_time:78358ms step_avg:38.49ms
step:2037/2330 train_time:78395ms step_avg:38.49ms
step:2038/2330 train_time:78437ms step_avg:38.49ms
step:2039/2330 train_time:78474ms step_avg:38.49ms
step:2040/2330 train_time:78516ms step_avg:38.49ms
step:2041/2330 train_time:78551ms step_avg:38.49ms
step:2042/2330 train_time:78593ms step_avg:38.49ms
step:2043/2330 train_time:78628ms step_avg:38.49ms
step:2044/2330 train_time:78669ms step_avg:38.49ms
step:2045/2330 train_time:78705ms step_avg:38.49ms
step:2046/2330 train_time:78746ms step_avg:38.49ms
step:2047/2330 train_time:78782ms step_avg:38.49ms
step:2048/2330 train_time:78823ms step_avg:38.49ms
step:2049/2330 train_time:78860ms step_avg:38.49ms
step:2050/2330 train_time:78900ms step_avg:38.49ms
step:2051/2330 train_time:78936ms step_avg:38.49ms
step:2052/2330 train_time:78978ms step_avg:38.49ms
step:2053/2330 train_time:79013ms step_avg:38.49ms
step:2054/2330 train_time:79054ms step_avg:38.49ms
step:2055/2330 train_time:79090ms step_avg:38.49ms
step:2056/2330 train_time:79131ms step_avg:38.49ms
step:2057/2330 train_time:79166ms step_avg:38.49ms
step:2058/2330 train_time:79208ms step_avg:38.49ms
step:2059/2330 train_time:79243ms step_avg:38.49ms
step:2060/2330 train_time:79284ms step_avg:38.49ms
step:2061/2330 train_time:79322ms step_avg:38.49ms
step:2062/2330 train_time:79362ms step_avg:38.49ms
step:2063/2330 train_time:79400ms step_avg:38.49ms
step:2064/2330 train_time:79441ms step_avg:38.49ms
step:2065/2330 train_time:79479ms step_avg:38.49ms
step:2066/2330 train_time:79520ms step_avg:38.49ms
step:2067/2330 train_time:79557ms step_avg:38.49ms
step:2068/2330 train_time:79598ms step_avg:38.49ms
step:2069/2330 train_time:79635ms step_avg:38.49ms
step:2070/2330 train_time:79675ms step_avg:38.49ms
step:2071/2330 train_time:79711ms step_avg:38.49ms
step:2072/2330 train_time:79752ms step_avg:38.49ms
step:2073/2330 train_time:79788ms step_avg:38.49ms
step:2074/2330 train_time:79829ms step_avg:38.49ms
step:2075/2330 train_time:79866ms step_avg:38.49ms
step:2076/2330 train_time:79907ms step_avg:38.49ms
step:2077/2330 train_time:79943ms step_avg:38.49ms
step:2078/2330 train_time:79984ms step_avg:38.49ms
step:2079/2330 train_time:80020ms step_avg:38.49ms
step:2080/2330 train_time:80060ms step_avg:38.49ms
step:2081/2330 train_time:80096ms step_avg:38.49ms
step:2082/2330 train_time:80137ms step_avg:38.49ms
step:2083/2330 train_time:80173ms step_avg:38.49ms
step:2084/2330 train_time:80214ms step_avg:38.49ms
step:2085/2330 train_time:80250ms step_avg:38.49ms
step:2086/2330 train_time:80291ms step_avg:38.49ms
step:2087/2330 train_time:80328ms step_avg:38.49ms
step:2088/2330 train_time:80370ms step_avg:38.49ms
step:2089/2330 train_time:80406ms step_avg:38.49ms
step:2090/2330 train_time:80448ms step_avg:38.49ms
step:2091/2330 train_time:80485ms step_avg:38.49ms
step:2092/2330 train_time:80527ms step_avg:38.49ms
step:2093/2330 train_time:80564ms step_avg:38.49ms
step:2094/2330 train_time:80606ms step_avg:38.49ms
step:2095/2330 train_time:80642ms step_avg:38.49ms
step:2096/2330 train_time:80682ms step_avg:38.49ms
step:2097/2330 train_time:80719ms step_avg:38.49ms
step:2098/2330 train_time:80760ms step_avg:38.49ms
step:2099/2330 train_time:80796ms step_avg:38.49ms
step:2100/2330 train_time:80837ms step_avg:38.49ms
step:2101/2330 train_time:80873ms step_avg:38.49ms
step:2102/2330 train_time:80913ms step_avg:38.49ms
step:2103/2330 train_time:80949ms step_avg:38.49ms
step:2104/2330 train_time:80990ms step_avg:38.49ms
step:2105/2330 train_time:81026ms step_avg:38.49ms
step:2106/2330 train_time:81068ms step_avg:38.49ms
step:2107/2330 train_time:81103ms step_avg:38.49ms
step:2108/2330 train_time:81144ms step_avg:38.49ms
step:2109/2330 train_time:81180ms step_avg:38.49ms
step:2110/2330 train_time:81221ms step_avg:38.49ms
step:2111/2330 train_time:81257ms step_avg:38.49ms
step:2112/2330 train_time:81298ms step_avg:38.49ms
step:2113/2330 train_time:81334ms step_avg:38.49ms
step:2114/2330 train_time:81375ms step_avg:38.49ms
step:2115/2330 train_time:81411ms step_avg:38.49ms
step:2116/2330 train_time:81452ms step_avg:38.49ms
step:2117/2330 train_time:81489ms step_avg:38.49ms
step:2118/2330 train_time:81531ms step_avg:38.49ms
step:2119/2330 train_time:81567ms step_avg:38.49ms
step:2120/2330 train_time:81608ms step_avg:38.49ms
step:2121/2330 train_time:81644ms step_avg:38.49ms
step:2122/2330 train_time:81686ms step_avg:38.49ms
step:2123/2330 train_time:81722ms step_avg:38.49ms
step:2124/2330 train_time:81763ms step_avg:38.49ms
step:2125/2330 train_time:81801ms step_avg:38.49ms
step:2126/2330 train_time:81841ms step_avg:38.50ms
step:2127/2330 train_time:81878ms step_avg:38.49ms
step:2128/2330 train_time:81919ms step_avg:38.50ms
step:2129/2330 train_time:81954ms step_avg:38.49ms
step:2130/2330 train_time:81995ms step_avg:38.50ms
step:2131/2330 train_time:82031ms step_avg:38.49ms
step:2132/2330 train_time:82072ms step_avg:38.50ms
step:2133/2330 train_time:82108ms step_avg:38.49ms
step:2134/2330 train_time:82149ms step_avg:38.50ms
step:2135/2330 train_time:82186ms step_avg:38.49ms
step:2136/2330 train_time:82227ms step_avg:38.50ms
step:2137/2330 train_time:82263ms step_avg:38.49ms
step:2138/2330 train_time:82304ms step_avg:38.50ms
step:2139/2330 train_time:82341ms step_avg:38.50ms
step:2140/2330 train_time:82381ms step_avg:38.50ms
step:2141/2330 train_time:82419ms step_avg:38.50ms
step:2142/2330 train_time:82459ms step_avg:38.50ms
step:2143/2330 train_time:82497ms step_avg:38.50ms
step:2144/2330 train_time:82538ms step_avg:38.50ms
step:2145/2330 train_time:82574ms step_avg:38.50ms
step:2146/2330 train_time:82615ms step_avg:38.50ms
step:2147/2330 train_time:82651ms step_avg:38.50ms
step:2148/2330 train_time:82692ms step_avg:38.50ms
step:2149/2330 train_time:82729ms step_avg:38.50ms
step:2150/2330 train_time:82771ms step_avg:38.50ms
step:2151/2330 train_time:82807ms step_avg:38.50ms
step:2152/2330 train_time:82849ms step_avg:38.50ms
step:2153/2330 train_time:82884ms step_avg:38.50ms
step:2154/2330 train_time:82925ms step_avg:38.50ms
step:2155/2330 train_time:82961ms step_avg:38.50ms
step:2156/2330 train_time:83002ms step_avg:38.50ms
step:2157/2330 train_time:83038ms step_avg:38.50ms
step:2158/2330 train_time:83079ms step_avg:38.50ms
step:2159/2330 train_time:83115ms step_avg:38.50ms
step:2160/2330 train_time:83156ms step_avg:38.50ms
step:2161/2330 train_time:83191ms step_avg:38.50ms
step:2162/2330 train_time:83233ms step_avg:38.50ms
step:2163/2330 train_time:83269ms step_avg:38.50ms
step:2164/2330 train_time:83310ms step_avg:38.50ms
step:2165/2330 train_time:83346ms step_avg:38.50ms
step:2166/2330 train_time:83388ms step_avg:38.50ms
step:2167/2330 train_time:83423ms step_avg:38.50ms
step:2168/2330 train_time:83465ms step_avg:38.50ms
step:2169/2330 train_time:83501ms step_avg:38.50ms
step:2170/2330 train_time:83542ms step_avg:38.50ms
step:2171/2330 train_time:83579ms step_avg:38.50ms
step:2172/2330 train_time:83620ms step_avg:38.50ms
step:2173/2330 train_time:83656ms step_avg:38.50ms
step:2174/2330 train_time:83697ms step_avg:38.50ms
step:2175/2330 train_time:83733ms step_avg:38.50ms
step:2176/2330 train_time:83774ms step_avg:38.50ms
step:2177/2330 train_time:83810ms step_avg:38.50ms
step:2178/2330 train_time:83851ms step_avg:38.50ms
step:2179/2330 train_time:83887ms step_avg:38.50ms
step:2180/2330 train_time:83929ms step_avg:38.50ms
step:2181/2330 train_time:83965ms step_avg:38.50ms
step:2182/2330 train_time:84007ms step_avg:38.50ms
step:2183/2330 train_time:84043ms step_avg:38.50ms
step:2184/2330 train_time:84084ms step_avg:38.50ms
step:2185/2330 train_time:84121ms step_avg:38.50ms
step:2186/2330 train_time:84162ms step_avg:38.50ms
step:2187/2330 train_time:84199ms step_avg:38.50ms
step:2188/2330 train_time:84240ms step_avg:38.50ms
step:2189/2330 train_time:84276ms step_avg:38.50ms
step:2190/2330 train_time:84317ms step_avg:38.50ms
step:2191/2330 train_time:84352ms step_avg:38.50ms
step:2192/2330 train_time:84393ms step_avg:38.50ms
step:2193/2330 train_time:84430ms step_avg:38.50ms
step:2194/2330 train_time:84471ms step_avg:38.50ms
step:2195/2330 train_time:84508ms step_avg:38.50ms
step:2196/2330 train_time:84549ms step_avg:38.50ms
step:2197/2330 train_time:84585ms step_avg:38.50ms
step:2198/2330 train_time:84626ms step_avg:38.50ms
step:2199/2330 train_time:84662ms step_avg:38.50ms
step:2200/2330 train_time:84702ms step_avg:38.50ms
step:2201/2330 train_time:84739ms step_avg:38.50ms
step:2202/2330 train_time:84780ms step_avg:38.50ms
step:2203/2330 train_time:84816ms step_avg:38.50ms
step:2204/2330 train_time:84856ms step_avg:38.50ms
step:2205/2330 train_time:84892ms step_avg:38.50ms
step:2206/2330 train_time:84933ms step_avg:38.50ms
step:2207/2330 train_time:84969ms step_avg:38.50ms
step:2208/2330 train_time:85010ms step_avg:38.50ms
step:2209/2330 train_time:85047ms step_avg:38.50ms
step:2210/2330 train_time:85088ms step_avg:38.50ms
step:2211/2330 train_time:85125ms step_avg:38.50ms
step:2212/2330 train_time:85166ms step_avg:38.50ms
step:2213/2330 train_time:85203ms step_avg:38.50ms
step:2214/2330 train_time:85245ms step_avg:38.50ms
step:2215/2330 train_time:85281ms step_avg:38.50ms
step:2216/2330 train_time:85322ms step_avg:38.50ms
step:2217/2330 train_time:85359ms step_avg:38.50ms
step:2218/2330 train_time:85399ms step_avg:38.50ms
step:2219/2330 train_time:85435ms step_avg:38.50ms
step:2220/2330 train_time:85477ms step_avg:38.50ms
step:2221/2330 train_time:85512ms step_avg:38.50ms
step:2222/2330 train_time:85553ms step_avg:38.50ms
step:2223/2330 train_time:85589ms step_avg:38.50ms
step:2224/2330 train_time:85630ms step_avg:38.50ms
step:2225/2330 train_time:85667ms step_avg:38.50ms
step:2226/2330 train_time:85709ms step_avg:38.50ms
step:2227/2330 train_time:85743ms step_avg:38.50ms
step:2228/2330 train_time:85785ms step_avg:38.50ms
step:2229/2330 train_time:85821ms step_avg:38.50ms
step:2230/2330 train_time:85861ms step_avg:38.50ms
step:2231/2330 train_time:85898ms step_avg:38.50ms
step:2232/2330 train_time:85939ms step_avg:38.50ms
step:2233/2330 train_time:85976ms step_avg:38.50ms
step:2234/2330 train_time:86016ms step_avg:38.50ms
step:2235/2330 train_time:86052ms step_avg:38.50ms
step:2236/2330 train_time:86092ms step_avg:38.50ms
step:2237/2330 train_time:86129ms step_avg:38.50ms
step:2238/2330 train_time:86170ms step_avg:38.50ms
step:2239/2330 train_time:86206ms step_avg:38.50ms
step:2240/2330 train_time:86248ms step_avg:38.50ms
step:2241/2330 train_time:86283ms step_avg:38.50ms
step:2242/2330 train_time:86324ms step_avg:38.50ms
step:2243/2330 train_time:86361ms step_avg:38.50ms
step:2244/2330 train_time:86402ms step_avg:38.50ms
step:2245/2330 train_time:86438ms step_avg:38.50ms
step:2246/2330 train_time:86479ms step_avg:38.50ms
step:2247/2330 train_time:86515ms step_avg:38.50ms
step:2248/2330 train_time:86556ms step_avg:38.50ms
step:2249/2330 train_time:86592ms step_avg:38.50ms
step:2250/2330 train_time:86633ms step_avg:38.50ms
step:2250/2330 val_loss:5.1509 train_time:86747ms step_avg:38.55ms
step:2251/2330 train_time:86759ms step_avg:38.54ms
step:2252/2330 train_time:86770ms step_avg:38.53ms
step:2253/2330 train_time:86779ms step_avg:38.52ms
step:2254/2330 train_time:86790ms step_avg:38.50ms
step:2255/2330 train_time:86825ms step_avg:38.50ms
step:2256/2330 train_time:86866ms step_avg:38.50ms
step:2257/2330 train_time:86900ms step_avg:38.50ms
step:2258/2330 train_time:86941ms step_avg:38.50ms
step:2259/2330 train_time:86976ms step_avg:38.50ms
step:2260/2330 train_time:87017ms step_avg:38.50ms
step:2261/2330 train_time:87054ms step_avg:38.50ms
step:2262/2330 train_time:87097ms step_avg:38.50ms
step:2263/2330 train_time:87139ms step_avg:38.51ms
step:2264/2330 train_time:87181ms step_avg:38.51ms
step:2265/2330 train_time:87216ms step_avg:38.51ms
step:2266/2330 train_time:87258ms step_avg:38.51ms
step:2267/2330 train_time:87292ms step_avg:38.51ms
step:2268/2330 train_time:87334ms step_avg:38.51ms
step:2269/2330 train_time:87370ms step_avg:38.51ms
step:2270/2330 train_time:87411ms step_avg:38.51ms
step:2271/2330 train_time:87447ms step_avg:38.51ms
step:2272/2330 train_time:87488ms step_avg:38.51ms
step:2273/2330 train_time:87523ms step_avg:38.51ms
step:2274/2330 train_time:87563ms step_avg:38.51ms
step:2275/2330 train_time:87598ms step_avg:38.50ms
step:2276/2330 train_time:87639ms step_avg:38.51ms
step:2277/2330 train_time:87676ms step_avg:38.50ms
step:2278/2330 train_time:87717ms step_avg:38.51ms
step:2279/2330 train_time:87754ms step_avg:38.51ms
step:2280/2330 train_time:87795ms step_avg:38.51ms
step:2281/2330 train_time:87831ms step_avg:38.51ms
step:2282/2330 train_time:87873ms step_avg:38.51ms
step:2283/2330 train_time:87908ms step_avg:38.51ms
step:2284/2330 train_time:87949ms step_avg:38.51ms
step:2285/2330 train_time:87985ms step_avg:38.51ms
step:2286/2330 train_time:88026ms step_avg:38.51ms
step:2287/2330 train_time:88063ms step_avg:38.51ms
step:2288/2330 train_time:88104ms step_avg:38.51ms
step:2289/2330 train_time:88141ms step_avg:38.51ms
step:2290/2330 train_time:88182ms step_avg:38.51ms
step:2291/2330 train_time:88220ms step_avg:38.51ms
step:2292/2330 train_time:88261ms step_avg:38.51ms
step:2293/2330 train_time:88297ms step_avg:38.51ms
step:2294/2330 train_time:88338ms step_avg:38.51ms
step:2295/2330 train_time:88374ms step_avg:38.51ms
step:2296/2330 train_time:88414ms step_avg:38.51ms
step:2297/2330 train_time:88451ms step_avg:38.51ms
step:2298/2330 train_time:88492ms step_avg:38.51ms
step:2299/2330 train_time:88527ms step_avg:38.51ms
step:2300/2330 train_time:88569ms step_avg:38.51ms
step:2301/2330 train_time:88604ms step_avg:38.51ms
step:2302/2330 train_time:88645ms step_avg:38.51ms
step:2303/2330 train_time:88680ms step_avg:38.51ms
step:2304/2330 train_time:88721ms step_avg:38.51ms
step:2305/2330 train_time:88756ms step_avg:38.51ms
step:2306/2330 train_time:88798ms step_avg:38.51ms
step:2307/2330 train_time:88833ms step_avg:38.51ms
step:2308/2330 train_time:88875ms step_avg:38.51ms
step:2309/2330 train_time:88910ms step_avg:38.51ms
step:2310/2330 train_time:88952ms step_avg:38.51ms
step:2311/2330 train_time:88988ms step_avg:38.51ms
step:2312/2330 train_time:89029ms step_avg:38.51ms
step:2313/2330 train_time:89066ms step_avg:38.51ms
step:2314/2330 train_time:89106ms step_avg:38.51ms
step:2315/2330 train_time:89143ms step_avg:38.51ms
step:2316/2330 train_time:89184ms step_avg:38.51ms
step:2317/2330 train_time:89222ms step_avg:38.51ms
step:2318/2330 train_time:89263ms step_avg:38.51ms
step:2319/2330 train_time:89299ms step_avg:38.51ms
step:2320/2330 train_time:89340ms step_avg:38.51ms
step:2321/2330 train_time:89376ms step_avg:38.51ms
step:2322/2330 train_time:89416ms step_avg:38.51ms
step:2323/2330 train_time:89452ms step_avg:38.51ms
step:2324/2330 train_time:89493ms step_avg:38.51ms
step:2325/2330 train_time:89529ms step_avg:38.51ms
step:2326/2330 train_time:89570ms step_avg:38.51ms
step:2327/2330 train_time:89606ms step_avg:38.51ms
step:2328/2330 train_time:89647ms step_avg:38.51ms
step:2329/2330 train_time:89683ms step_avg:38.51ms
step:2330/2330 train_time:89723ms step_avg:38.51ms
step:2330/2330 val_loss:5.1441 train_time:89836ms step_avg:38.56ms
peak memory allocated: 27165 MiB reserved: 38888 MiB
