import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "adam_lr7e-2"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-1, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.Adam(
    hidden_matrix_params + gate_params,
    lr=7e-2,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 00:47:18 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   34C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   32C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:87ms step_avg:87.28ms
step:2/2330 train_time:181ms step_avg:90.57ms
step:3/2330 train_time:200ms step_avg:66.72ms
step:4/2330 train_time:222ms step_avg:55.50ms
step:5/2330 train_time:277ms step_avg:55.38ms
step:6/2330 train_time:336ms step_avg:56.01ms
step:7/2330 train_time:391ms step_avg:55.85ms
step:8/2330 train_time:450ms step_avg:56.22ms
step:9/2330 train_time:505ms step_avg:56.09ms
step:10/2330 train_time:564ms step_avg:56.37ms
step:11/2330 train_time:619ms step_avg:56.23ms
step:12/2330 train_time:677ms step_avg:56.43ms
step:13/2330 train_time:732ms step_avg:56.31ms
step:14/2330 train_time:791ms step_avg:56.48ms
step:15/2330 train_time:846ms step_avg:56.37ms
step:16/2330 train_time:905ms step_avg:56.54ms
step:17/2330 train_time:959ms step_avg:56.43ms
step:18/2330 train_time:1018ms step_avg:56.58ms
step:19/2330 train_time:1073ms step_avg:56.48ms
step:20/2330 train_time:1132ms step_avg:56.62ms
step:21/2330 train_time:1187ms step_avg:56.53ms
step:22/2330 train_time:1246ms step_avg:56.66ms
step:23/2330 train_time:1301ms step_avg:56.58ms
step:24/2330 train_time:1361ms step_avg:56.72ms
step:25/2330 train_time:1416ms step_avg:56.65ms
step:26/2330 train_time:1475ms step_avg:56.73ms
step:27/2330 train_time:1530ms step_avg:56.66ms
step:28/2330 train_time:1589ms step_avg:56.75ms
step:29/2330 train_time:1644ms step_avg:56.68ms
step:30/2330 train_time:1704ms step_avg:56.79ms
step:31/2330 train_time:1759ms step_avg:56.73ms
step:32/2330 train_time:1818ms step_avg:56.82ms
step:33/2330 train_time:1873ms step_avg:56.76ms
step:34/2330 train_time:1932ms step_avg:56.83ms
step:35/2330 train_time:1987ms step_avg:56.78ms
step:36/2330 train_time:2047ms step_avg:56.85ms
step:37/2330 train_time:2102ms step_avg:56.80ms
step:38/2330 train_time:2161ms step_avg:56.87ms
step:39/2330 train_time:2216ms step_avg:56.82ms
step:40/2330 train_time:2275ms step_avg:56.87ms
step:41/2330 train_time:2330ms step_avg:56.83ms
step:42/2330 train_time:2389ms step_avg:56.89ms
step:43/2330 train_time:2444ms step_avg:56.85ms
step:44/2330 train_time:2503ms step_avg:56.90ms
step:45/2330 train_time:2558ms step_avg:56.85ms
step:46/2330 train_time:2617ms step_avg:56.90ms
step:47/2330 train_time:2672ms step_avg:56.85ms
step:48/2330 train_time:2732ms step_avg:56.91ms
step:49/2330 train_time:2786ms step_avg:56.87ms
step:50/2330 train_time:2846ms step_avg:56.92ms
step:51/2330 train_time:2901ms step_avg:56.88ms
step:52/2330 train_time:2961ms step_avg:56.94ms
step:53/2330 train_time:3016ms step_avg:56.90ms
step:54/2330 train_time:3075ms step_avg:56.95ms
step:55/2330 train_time:3130ms step_avg:56.92ms
step:56/2330 train_time:3191ms step_avg:56.98ms
step:57/2330 train_time:3246ms step_avg:56.94ms
step:58/2330 train_time:3306ms step_avg:57.01ms
step:59/2330 train_time:3361ms step_avg:56.97ms
step:60/2330 train_time:3421ms step_avg:57.02ms
step:61/2330 train_time:3476ms step_avg:56.99ms
step:62/2330 train_time:3535ms step_avg:57.02ms
step:63/2330 train_time:3590ms step_avg:56.99ms
step:64/2330 train_time:3650ms step_avg:57.04ms
step:65/2330 train_time:3705ms step_avg:57.00ms
step:66/2330 train_time:3765ms step_avg:57.04ms
step:67/2330 train_time:3820ms step_avg:57.01ms
step:68/2330 train_time:3879ms step_avg:57.05ms
step:69/2330 train_time:3934ms step_avg:57.02ms
step:70/2330 train_time:3994ms step_avg:57.06ms
step:71/2330 train_time:4049ms step_avg:57.03ms
step:72/2330 train_time:4109ms step_avg:57.07ms
step:73/2330 train_time:4164ms step_avg:57.05ms
step:74/2330 train_time:4224ms step_avg:57.08ms
step:75/2330 train_time:4279ms step_avg:57.06ms
step:76/2330 train_time:4339ms step_avg:57.09ms
step:77/2330 train_time:4394ms step_avg:57.07ms
step:78/2330 train_time:4453ms step_avg:57.09ms
step:79/2330 train_time:4508ms step_avg:57.06ms
step:80/2330 train_time:4569ms step_avg:57.11ms
step:81/2330 train_time:4624ms step_avg:57.08ms
step:82/2330 train_time:4684ms step_avg:57.12ms
step:83/2330 train_time:4739ms step_avg:57.10ms
step:84/2330 train_time:4799ms step_avg:57.13ms
step:85/2330 train_time:4854ms step_avg:57.11ms
step:86/2330 train_time:4914ms step_avg:57.14ms
step:87/2330 train_time:4969ms step_avg:57.12ms
step:88/2330 train_time:5030ms step_avg:57.16ms
step:89/2330 train_time:5085ms step_avg:57.14ms
step:90/2330 train_time:5145ms step_avg:57.16ms
step:91/2330 train_time:5200ms step_avg:57.14ms
step:92/2330 train_time:5260ms step_avg:57.17ms
step:93/2330 train_time:5315ms step_avg:57.15ms
step:94/2330 train_time:5376ms step_avg:57.19ms
step:95/2330 train_time:5431ms step_avg:57.17ms
step:96/2330 train_time:5491ms step_avg:57.19ms
step:97/2330 train_time:5546ms step_avg:57.17ms
step:98/2330 train_time:5606ms step_avg:57.21ms
step:99/2330 train_time:5661ms step_avg:57.19ms
step:100/2330 train_time:5721ms step_avg:57.21ms
step:101/2330 train_time:5777ms step_avg:57.19ms
step:102/2330 train_time:5835ms step_avg:57.21ms
step:103/2330 train_time:5891ms step_avg:57.19ms
step:104/2330 train_time:5950ms step_avg:57.22ms
step:105/2330 train_time:6005ms step_avg:57.19ms
step:106/2330 train_time:6067ms step_avg:57.23ms
step:107/2330 train_time:6122ms step_avg:57.21ms
step:108/2330 train_time:6182ms step_avg:57.24ms
step:109/2330 train_time:6237ms step_avg:57.22ms
step:110/2330 train_time:6297ms step_avg:57.25ms
step:111/2330 train_time:6353ms step_avg:57.23ms
step:112/2330 train_time:6412ms step_avg:57.25ms
step:113/2330 train_time:6467ms step_avg:57.23ms
step:114/2330 train_time:6527ms step_avg:57.25ms
step:115/2330 train_time:6583ms step_avg:57.24ms
step:116/2330 train_time:6642ms step_avg:57.26ms
step:117/2330 train_time:6698ms step_avg:57.24ms
step:118/2330 train_time:6758ms step_avg:57.27ms
step:119/2330 train_time:6813ms step_avg:57.25ms
step:120/2330 train_time:6874ms step_avg:57.28ms
step:121/2330 train_time:6929ms step_avg:57.26ms
step:122/2330 train_time:6989ms step_avg:57.29ms
step:123/2330 train_time:7045ms step_avg:57.28ms
step:124/2330 train_time:7105ms step_avg:57.30ms
step:125/2330 train_time:7161ms step_avg:57.28ms
step:126/2330 train_time:7221ms step_avg:57.31ms
step:127/2330 train_time:7276ms step_avg:57.29ms
step:128/2330 train_time:7336ms step_avg:57.31ms
step:129/2330 train_time:7391ms step_avg:57.30ms
step:130/2330 train_time:7452ms step_avg:57.32ms
step:131/2330 train_time:7507ms step_avg:57.30ms
step:132/2330 train_time:7567ms step_avg:57.32ms
step:133/2330 train_time:7623ms step_avg:57.31ms
step:134/2330 train_time:7682ms step_avg:57.33ms
step:135/2330 train_time:7738ms step_avg:57.32ms
step:136/2330 train_time:7799ms step_avg:57.34ms
step:137/2330 train_time:7855ms step_avg:57.33ms
step:138/2330 train_time:7916ms step_avg:57.36ms
step:139/2330 train_time:7971ms step_avg:57.34ms
step:140/2330 train_time:8031ms step_avg:57.37ms
step:141/2330 train_time:8087ms step_avg:57.35ms
step:142/2330 train_time:8147ms step_avg:57.38ms
step:143/2330 train_time:8203ms step_avg:57.36ms
step:144/2330 train_time:8263ms step_avg:57.38ms
step:145/2330 train_time:8318ms step_avg:57.37ms
step:146/2330 train_time:8378ms step_avg:57.38ms
step:147/2330 train_time:8434ms step_avg:57.38ms
step:148/2330 train_time:8494ms step_avg:57.39ms
step:149/2330 train_time:8549ms step_avg:57.38ms
step:150/2330 train_time:8609ms step_avg:57.39ms
step:151/2330 train_time:8664ms step_avg:57.38ms
step:152/2330 train_time:8724ms step_avg:57.40ms
step:153/2330 train_time:8780ms step_avg:57.38ms
step:154/2330 train_time:8840ms step_avg:57.40ms
step:155/2330 train_time:8896ms step_avg:57.39ms
step:156/2330 train_time:8956ms step_avg:57.41ms
step:157/2330 train_time:9011ms step_avg:57.40ms
step:158/2330 train_time:9071ms step_avg:57.41ms
step:159/2330 train_time:9127ms step_avg:57.40ms
step:160/2330 train_time:9187ms step_avg:57.42ms
step:161/2330 train_time:9242ms step_avg:57.40ms
step:162/2330 train_time:9303ms step_avg:57.43ms
step:163/2330 train_time:9359ms step_avg:57.42ms
step:164/2330 train_time:9419ms step_avg:57.43ms
step:165/2330 train_time:9475ms step_avg:57.42ms
step:166/2330 train_time:9534ms step_avg:57.43ms
step:167/2330 train_time:9591ms step_avg:57.43ms
step:168/2330 train_time:9650ms step_avg:57.44ms
step:169/2330 train_time:9706ms step_avg:57.43ms
step:170/2330 train_time:9766ms step_avg:57.45ms
step:171/2330 train_time:9822ms step_avg:57.44ms
step:172/2330 train_time:9882ms step_avg:57.45ms
step:173/2330 train_time:9938ms step_avg:57.44ms
step:174/2330 train_time:9998ms step_avg:57.46ms
step:175/2330 train_time:10054ms step_avg:57.45ms
step:176/2330 train_time:10114ms step_avg:57.46ms
step:177/2330 train_time:10169ms step_avg:57.45ms
step:178/2330 train_time:10229ms step_avg:57.47ms
step:179/2330 train_time:10284ms step_avg:57.45ms
step:180/2330 train_time:10345ms step_avg:57.47ms
step:181/2330 train_time:10400ms step_avg:57.46ms
step:182/2330 train_time:10460ms step_avg:57.48ms
step:183/2330 train_time:10516ms step_avg:57.47ms
step:184/2330 train_time:10576ms step_avg:57.48ms
step:185/2330 train_time:10632ms step_avg:57.47ms
step:186/2330 train_time:10691ms step_avg:57.48ms
step:187/2330 train_time:10747ms step_avg:57.47ms
step:188/2330 train_time:10807ms step_avg:57.48ms
step:189/2330 train_time:10862ms step_avg:57.47ms
step:190/2330 train_time:10923ms step_avg:57.49ms
step:191/2330 train_time:10979ms step_avg:57.48ms
step:192/2330 train_time:11040ms step_avg:57.50ms
step:193/2330 train_time:11096ms step_avg:57.49ms
step:194/2330 train_time:11155ms step_avg:57.50ms
step:195/2330 train_time:11211ms step_avg:57.49ms
step:196/2330 train_time:11271ms step_avg:57.50ms
step:197/2330 train_time:11326ms step_avg:57.49ms
step:198/2330 train_time:11387ms step_avg:57.51ms
step:199/2330 train_time:11442ms step_avg:57.50ms
step:200/2330 train_time:11503ms step_avg:57.52ms
step:201/2330 train_time:11559ms step_avg:57.51ms
step:202/2330 train_time:11619ms step_avg:57.52ms
step:203/2330 train_time:11674ms step_avg:57.51ms
step:204/2330 train_time:11734ms step_avg:57.52ms
step:205/2330 train_time:11789ms step_avg:57.51ms
step:206/2330 train_time:11850ms step_avg:57.52ms
step:207/2330 train_time:11905ms step_avg:57.51ms
step:208/2330 train_time:11965ms step_avg:57.52ms
step:209/2330 train_time:12021ms step_avg:57.51ms
step:210/2330 train_time:12082ms step_avg:57.53ms
step:211/2330 train_time:12138ms step_avg:57.52ms
step:212/2330 train_time:12198ms step_avg:57.54ms
step:213/2330 train_time:12253ms step_avg:57.53ms
step:214/2330 train_time:12313ms step_avg:57.54ms
step:215/2330 train_time:12369ms step_avg:57.53ms
step:216/2330 train_time:12429ms step_avg:57.54ms
step:217/2330 train_time:12484ms step_avg:57.53ms
step:218/2330 train_time:12545ms step_avg:57.54ms
step:219/2330 train_time:12600ms step_avg:57.54ms
step:220/2330 train_time:12661ms step_avg:57.55ms
step:221/2330 train_time:12717ms step_avg:57.54ms
step:222/2330 train_time:12777ms step_avg:57.55ms
step:223/2330 train_time:12833ms step_avg:57.55ms
step:224/2330 train_time:12893ms step_avg:57.56ms
step:225/2330 train_time:12948ms step_avg:57.55ms
step:226/2330 train_time:13010ms step_avg:57.57ms
step:227/2330 train_time:13066ms step_avg:57.56ms
step:228/2330 train_time:13127ms step_avg:57.57ms
step:229/2330 train_time:13183ms step_avg:57.57ms
step:230/2330 train_time:13244ms step_avg:57.58ms
step:231/2330 train_time:13299ms step_avg:57.57ms
step:232/2330 train_time:13361ms step_avg:57.59ms
step:233/2330 train_time:13416ms step_avg:57.58ms
step:234/2330 train_time:13477ms step_avg:57.59ms
step:235/2330 train_time:13533ms step_avg:57.59ms
step:236/2330 train_time:13593ms step_avg:57.60ms
step:237/2330 train_time:13648ms step_avg:57.59ms
step:238/2330 train_time:13709ms step_avg:57.60ms
step:239/2330 train_time:13764ms step_avg:57.59ms
step:240/2330 train_time:13825ms step_avg:57.60ms
step:241/2330 train_time:13881ms step_avg:57.60ms
step:242/2330 train_time:13942ms step_avg:57.61ms
step:243/2330 train_time:13998ms step_avg:57.60ms
step:244/2330 train_time:14058ms step_avg:57.62ms
step:245/2330 train_time:14114ms step_avg:57.61ms
step:246/2330 train_time:14174ms step_avg:57.62ms
step:247/2330 train_time:14230ms step_avg:57.61ms
step:248/2330 train_time:14290ms step_avg:57.62ms
step:249/2330 train_time:14345ms step_avg:57.61ms
step:250/2330 train_time:14407ms step_avg:57.63ms
step:250/2330 val_loss:6.4580 train_time:14484ms step_avg:57.94ms
step:251/2330 train_time:14503ms step_avg:57.78ms
step:252/2330 train_time:14525ms step_avg:57.64ms
step:253/2330 train_time:14581ms step_avg:57.63ms
step:254/2330 train_time:14643ms step_avg:57.65ms
step:255/2330 train_time:14698ms step_avg:57.64ms
step:256/2330 train_time:14760ms step_avg:57.66ms
step:257/2330 train_time:14816ms step_avg:57.65ms
step:258/2330 train_time:14877ms step_avg:57.66ms
step:259/2330 train_time:14932ms step_avg:57.65ms
step:260/2330 train_time:14995ms step_avg:57.67ms
step:261/2330 train_time:15050ms step_avg:57.66ms
step:262/2330 train_time:15110ms step_avg:57.67ms
step:263/2330 train_time:15165ms step_avg:57.66ms
step:264/2330 train_time:15226ms step_avg:57.67ms
step:265/2330 train_time:15281ms step_avg:57.67ms
step:266/2330 train_time:15342ms step_avg:57.68ms
step:267/2330 train_time:15397ms step_avg:57.67ms
step:268/2330 train_time:15457ms step_avg:57.68ms
step:269/2330 train_time:15513ms step_avg:57.67ms
step:270/2330 train_time:15574ms step_avg:57.68ms
step:271/2330 train_time:15630ms step_avg:57.67ms
step:272/2330 train_time:15693ms step_avg:57.69ms
step:273/2330 train_time:15748ms step_avg:57.69ms
step:274/2330 train_time:15810ms step_avg:57.70ms
step:275/2330 train_time:15866ms step_avg:57.69ms
step:276/2330 train_time:15927ms step_avg:57.71ms
step:277/2330 train_time:15983ms step_avg:57.70ms
step:278/2330 train_time:16043ms step_avg:57.71ms
step:279/2330 train_time:16099ms step_avg:57.70ms
step:280/2330 train_time:16159ms step_avg:57.71ms
step:281/2330 train_time:16215ms step_avg:57.70ms
step:282/2330 train_time:16274ms step_avg:57.71ms
step:283/2330 train_time:16330ms step_avg:57.70ms
step:284/2330 train_time:16390ms step_avg:57.71ms
step:285/2330 train_time:16447ms step_avg:57.71ms
step:286/2330 train_time:16506ms step_avg:57.71ms
step:287/2330 train_time:16562ms step_avg:57.71ms
step:288/2330 train_time:16623ms step_avg:57.72ms
step:289/2330 train_time:16679ms step_avg:57.71ms
step:290/2330 train_time:16740ms step_avg:57.72ms
step:291/2330 train_time:16796ms step_avg:57.72ms
step:292/2330 train_time:16857ms step_avg:57.73ms
step:293/2330 train_time:16913ms step_avg:57.72ms
step:294/2330 train_time:16975ms step_avg:57.74ms
step:295/2330 train_time:17031ms step_avg:57.73ms
step:296/2330 train_time:17092ms step_avg:57.74ms
step:297/2330 train_time:17147ms step_avg:57.74ms
step:298/2330 train_time:17209ms step_avg:57.75ms
step:299/2330 train_time:17264ms step_avg:57.74ms
step:300/2330 train_time:17324ms step_avg:57.75ms
step:301/2330 train_time:17380ms step_avg:57.74ms
step:302/2330 train_time:17440ms step_avg:57.75ms
step:303/2330 train_time:17495ms step_avg:57.74ms
step:304/2330 train_time:17556ms step_avg:57.75ms
step:305/2330 train_time:17611ms step_avg:57.74ms
step:306/2330 train_time:17672ms step_avg:57.75ms
step:307/2330 train_time:17728ms step_avg:57.75ms
step:308/2330 train_time:17790ms step_avg:57.76ms
step:309/2330 train_time:17846ms step_avg:57.75ms
step:310/2330 train_time:17907ms step_avg:57.76ms
step:311/2330 train_time:17963ms step_avg:57.76ms
step:312/2330 train_time:18023ms step_avg:57.77ms
step:313/2330 train_time:18079ms step_avg:57.76ms
step:314/2330 train_time:18140ms step_avg:57.77ms
step:315/2330 train_time:18196ms step_avg:57.76ms
step:316/2330 train_time:18256ms step_avg:57.77ms
step:317/2330 train_time:18312ms step_avg:57.77ms
step:318/2330 train_time:18373ms step_avg:57.78ms
step:319/2330 train_time:18429ms step_avg:57.77ms
step:320/2330 train_time:18490ms step_avg:57.78ms
step:321/2330 train_time:18546ms step_avg:57.77ms
step:322/2330 train_time:18606ms step_avg:57.78ms
step:323/2330 train_time:18662ms step_avg:57.78ms
step:324/2330 train_time:18722ms step_avg:57.78ms
step:325/2330 train_time:18778ms step_avg:57.78ms
step:326/2330 train_time:18839ms step_avg:57.79ms
step:327/2330 train_time:18895ms step_avg:57.78ms
step:328/2330 train_time:18956ms step_avg:57.79ms
step:329/2330 train_time:19012ms step_avg:57.79ms
step:330/2330 train_time:19073ms step_avg:57.80ms
step:331/2330 train_time:19129ms step_avg:57.79ms
step:332/2330 train_time:19190ms step_avg:57.80ms
step:333/2330 train_time:19246ms step_avg:57.80ms
step:334/2330 train_time:19306ms step_avg:57.80ms
step:335/2330 train_time:19361ms step_avg:57.79ms
step:336/2330 train_time:19422ms step_avg:57.80ms
step:337/2330 train_time:19477ms step_avg:57.80ms
step:338/2330 train_time:19537ms step_avg:57.80ms
step:339/2330 train_time:19593ms step_avg:57.80ms
step:340/2330 train_time:19654ms step_avg:57.81ms
step:341/2330 train_time:19710ms step_avg:57.80ms
step:342/2330 train_time:19771ms step_avg:57.81ms
step:343/2330 train_time:19827ms step_avg:57.80ms
step:344/2330 train_time:19888ms step_avg:57.81ms
step:345/2330 train_time:19945ms step_avg:57.81ms
step:346/2330 train_time:20005ms step_avg:57.82ms
step:347/2330 train_time:20062ms step_avg:57.82ms
step:348/2330 train_time:20122ms step_avg:57.82ms
step:349/2330 train_time:20178ms step_avg:57.82ms
step:350/2330 train_time:20238ms step_avg:57.82ms
step:351/2330 train_time:20294ms step_avg:57.82ms
step:352/2330 train_time:20355ms step_avg:57.83ms
step:353/2330 train_time:20411ms step_avg:57.82ms
step:354/2330 train_time:20472ms step_avg:57.83ms
step:355/2330 train_time:20528ms step_avg:57.82ms
step:356/2330 train_time:20589ms step_avg:57.83ms
step:357/2330 train_time:20645ms step_avg:57.83ms
step:358/2330 train_time:20704ms step_avg:57.83ms
step:359/2330 train_time:20760ms step_avg:57.83ms
step:360/2330 train_time:20821ms step_avg:57.84ms
step:361/2330 train_time:20877ms step_avg:57.83ms
step:362/2330 train_time:20937ms step_avg:57.84ms
step:363/2330 train_time:20993ms step_avg:57.83ms
step:364/2330 train_time:21054ms step_avg:57.84ms
step:365/2330 train_time:21110ms step_avg:57.84ms
step:366/2330 train_time:21171ms step_avg:57.84ms
step:367/2330 train_time:21227ms step_avg:57.84ms
step:368/2330 train_time:21287ms step_avg:57.85ms
step:369/2330 train_time:21343ms step_avg:57.84ms
step:370/2330 train_time:21403ms step_avg:57.85ms
step:371/2330 train_time:21459ms step_avg:57.84ms
step:372/2330 train_time:21519ms step_avg:57.85ms
step:373/2330 train_time:21575ms step_avg:57.84ms
step:374/2330 train_time:21636ms step_avg:57.85ms
step:375/2330 train_time:21692ms step_avg:57.84ms
step:376/2330 train_time:21753ms step_avg:57.85ms
step:377/2330 train_time:21810ms step_avg:57.85ms
step:378/2330 train_time:21870ms step_avg:57.86ms
step:379/2330 train_time:21927ms step_avg:57.85ms
step:380/2330 train_time:21987ms step_avg:57.86ms
step:381/2330 train_time:22043ms step_avg:57.86ms
step:382/2330 train_time:22102ms step_avg:57.86ms
step:383/2330 train_time:22159ms step_avg:57.86ms
step:384/2330 train_time:22219ms step_avg:57.86ms
step:385/2330 train_time:22275ms step_avg:57.86ms
step:386/2330 train_time:22335ms step_avg:57.86ms
step:387/2330 train_time:22391ms step_avg:57.86ms
step:388/2330 train_time:22452ms step_avg:57.87ms
step:389/2330 train_time:22508ms step_avg:57.86ms
step:390/2330 train_time:22568ms step_avg:57.87ms
step:391/2330 train_time:22624ms step_avg:57.86ms
step:392/2330 train_time:22685ms step_avg:57.87ms
step:393/2330 train_time:22741ms step_avg:57.87ms
step:394/2330 train_time:22801ms step_avg:57.87ms
step:395/2330 train_time:22857ms step_avg:57.87ms
step:396/2330 train_time:22917ms step_avg:57.87ms
step:397/2330 train_time:22973ms step_avg:57.87ms
step:398/2330 train_time:23034ms step_avg:57.87ms
step:399/2330 train_time:23089ms step_avg:57.87ms
step:400/2330 train_time:23150ms step_avg:57.88ms
step:401/2330 train_time:23206ms step_avg:57.87ms
step:402/2330 train_time:23267ms step_avg:57.88ms
step:403/2330 train_time:23322ms step_avg:57.87ms
step:404/2330 train_time:23383ms step_avg:57.88ms
step:405/2330 train_time:23438ms step_avg:57.87ms
step:406/2330 train_time:23499ms step_avg:57.88ms
step:407/2330 train_time:23555ms step_avg:57.87ms
step:408/2330 train_time:23616ms step_avg:57.88ms
step:409/2330 train_time:23671ms step_avg:57.88ms
step:410/2330 train_time:23734ms step_avg:57.89ms
step:411/2330 train_time:23789ms step_avg:57.88ms
step:412/2330 train_time:23851ms step_avg:57.89ms
step:413/2330 train_time:23907ms step_avg:57.89ms
step:414/2330 train_time:23967ms step_avg:57.89ms
step:415/2330 train_time:24023ms step_avg:57.89ms
step:416/2330 train_time:24084ms step_avg:57.89ms
step:417/2330 train_time:24140ms step_avg:57.89ms
step:418/2330 train_time:24201ms step_avg:57.90ms
step:419/2330 train_time:24257ms step_avg:57.89ms
step:420/2330 train_time:24317ms step_avg:57.90ms
step:421/2330 train_time:24372ms step_avg:57.89ms
step:422/2330 train_time:24434ms step_avg:57.90ms
step:423/2330 train_time:24490ms step_avg:57.89ms
step:424/2330 train_time:24550ms step_avg:57.90ms
step:425/2330 train_time:24606ms step_avg:57.90ms
step:426/2330 train_time:24667ms step_avg:57.90ms
step:427/2330 train_time:24723ms step_avg:57.90ms
step:428/2330 train_time:24784ms step_avg:57.91ms
step:429/2330 train_time:24840ms step_avg:57.90ms
step:430/2330 train_time:24901ms step_avg:57.91ms
step:431/2330 train_time:24956ms step_avg:57.90ms
step:432/2330 train_time:25017ms step_avg:57.91ms
step:433/2330 train_time:25072ms step_avg:57.90ms
step:434/2330 train_time:25134ms step_avg:57.91ms
step:435/2330 train_time:25189ms step_avg:57.91ms
step:436/2330 train_time:25252ms step_avg:57.92ms
step:437/2330 train_time:25308ms step_avg:57.91ms
step:438/2330 train_time:25369ms step_avg:57.92ms
step:439/2330 train_time:25424ms step_avg:57.91ms
step:440/2330 train_time:25484ms step_avg:57.92ms
step:441/2330 train_time:25540ms step_avg:57.91ms
step:442/2330 train_time:25601ms step_avg:57.92ms
step:443/2330 train_time:25656ms step_avg:57.91ms
step:444/2330 train_time:25718ms step_avg:57.92ms
step:445/2330 train_time:25773ms step_avg:57.92ms
step:446/2330 train_time:25835ms step_avg:57.93ms
step:447/2330 train_time:25890ms step_avg:57.92ms
step:448/2330 train_time:25951ms step_avg:57.93ms
step:449/2330 train_time:26007ms step_avg:57.92ms
step:450/2330 train_time:26070ms step_avg:57.93ms
step:451/2330 train_time:26126ms step_avg:57.93ms
step:452/2330 train_time:26186ms step_avg:57.93ms
step:453/2330 train_time:26242ms step_avg:57.93ms
step:454/2330 train_time:26302ms step_avg:57.93ms
step:455/2330 train_time:26358ms step_avg:57.93ms
step:456/2330 train_time:26418ms step_avg:57.93ms
step:457/2330 train_time:26474ms step_avg:57.93ms
step:458/2330 train_time:26535ms step_avg:57.94ms
step:459/2330 train_time:26591ms step_avg:57.93ms
step:460/2330 train_time:26652ms step_avg:57.94ms
step:461/2330 train_time:26709ms step_avg:57.94ms
step:462/2330 train_time:26770ms step_avg:57.94ms
step:463/2330 train_time:26826ms step_avg:57.94ms
step:464/2330 train_time:26886ms step_avg:57.94ms
step:465/2330 train_time:26942ms step_avg:57.94ms
step:466/2330 train_time:27002ms step_avg:57.94ms
step:467/2330 train_time:27058ms step_avg:57.94ms
step:468/2330 train_time:27119ms step_avg:57.95ms
step:469/2330 train_time:27175ms step_avg:57.94ms
step:470/2330 train_time:27237ms step_avg:57.95ms
step:471/2330 train_time:27292ms step_avg:57.95ms
step:472/2330 train_time:27353ms step_avg:57.95ms
step:473/2330 train_time:27409ms step_avg:57.95ms
step:474/2330 train_time:27470ms step_avg:57.95ms
step:475/2330 train_time:27526ms step_avg:57.95ms
step:476/2330 train_time:27588ms step_avg:57.96ms
step:477/2330 train_time:27643ms step_avg:57.95ms
step:478/2330 train_time:27704ms step_avg:57.96ms
step:479/2330 train_time:27761ms step_avg:57.96ms
step:480/2330 train_time:27821ms step_avg:57.96ms
step:481/2330 train_time:27876ms step_avg:57.95ms
step:482/2330 train_time:27937ms step_avg:57.96ms
step:483/2330 train_time:27993ms step_avg:57.96ms
step:484/2330 train_time:28055ms step_avg:57.96ms
step:485/2330 train_time:28110ms step_avg:57.96ms
step:486/2330 train_time:28172ms step_avg:57.97ms
step:487/2330 train_time:28228ms step_avg:57.96ms
step:488/2330 train_time:28289ms step_avg:57.97ms
step:489/2330 train_time:28345ms step_avg:57.96ms
step:490/2330 train_time:28405ms step_avg:57.97ms
step:491/2330 train_time:28461ms step_avg:57.97ms
step:492/2330 train_time:28521ms step_avg:57.97ms
step:493/2330 train_time:28576ms step_avg:57.96ms
step:494/2330 train_time:28638ms step_avg:57.97ms
step:495/2330 train_time:28694ms step_avg:57.97ms
step:496/2330 train_time:28755ms step_avg:57.97ms
step:497/2330 train_time:28811ms step_avg:57.97ms
step:498/2330 train_time:28872ms step_avg:57.98ms
step:499/2330 train_time:28928ms step_avg:57.97ms
step:500/2330 train_time:28990ms step_avg:57.98ms
step:500/2330 val_loss:5.6304 train_time:29066ms step_avg:58.13ms
step:501/2330 train_time:29084ms step_avg:58.05ms
step:502/2330 train_time:29109ms step_avg:57.99ms
step:503/2330 train_time:29165ms step_avg:57.98ms
step:504/2330 train_time:29228ms step_avg:57.99ms
step:505/2330 train_time:29284ms step_avg:57.99ms
step:506/2330 train_time:29349ms step_avg:58.00ms
step:507/2330 train_time:29405ms step_avg:58.00ms
step:508/2330 train_time:29465ms step_avg:58.00ms
step:509/2330 train_time:29520ms step_avg:58.00ms
step:510/2330 train_time:29581ms step_avg:58.00ms
step:511/2330 train_time:29637ms step_avg:58.00ms
step:512/2330 train_time:29698ms step_avg:58.00ms
step:513/2330 train_time:29753ms step_avg:58.00ms
step:514/2330 train_time:29813ms step_avg:58.00ms
step:515/2330 train_time:29869ms step_avg:58.00ms
step:516/2330 train_time:29928ms step_avg:58.00ms
step:517/2330 train_time:29984ms step_avg:58.00ms
step:518/2330 train_time:30045ms step_avg:58.00ms
step:519/2330 train_time:30101ms step_avg:58.00ms
step:520/2330 train_time:30163ms step_avg:58.01ms
step:521/2330 train_time:30219ms step_avg:58.00ms
step:522/2330 train_time:30283ms step_avg:58.01ms
step:523/2330 train_time:30339ms step_avg:58.01ms
step:524/2330 train_time:30401ms step_avg:58.02ms
step:525/2330 train_time:30457ms step_avg:58.01ms
step:526/2330 train_time:30518ms step_avg:58.02ms
step:527/2330 train_time:30574ms step_avg:58.01ms
step:528/2330 train_time:30633ms step_avg:58.02ms
step:529/2330 train_time:30689ms step_avg:58.01ms
step:530/2330 train_time:30749ms step_avg:58.02ms
step:531/2330 train_time:30805ms step_avg:58.01ms
step:532/2330 train_time:30865ms step_avg:58.02ms
step:533/2330 train_time:30920ms step_avg:58.01ms
step:534/2330 train_time:30981ms step_avg:58.02ms
step:535/2330 train_time:31037ms step_avg:58.01ms
step:536/2330 train_time:31099ms step_avg:58.02ms
step:537/2330 train_time:31155ms step_avg:58.02ms
step:538/2330 train_time:31217ms step_avg:58.02ms
step:539/2330 train_time:31274ms step_avg:58.02ms
step:540/2330 train_time:31335ms step_avg:58.03ms
step:541/2330 train_time:31392ms step_avg:58.03ms
step:542/2330 train_time:31452ms step_avg:58.03ms
step:543/2330 train_time:31507ms step_avg:58.02ms
step:544/2330 train_time:31568ms step_avg:58.03ms
step:545/2330 train_time:31624ms step_avg:58.03ms
step:546/2330 train_time:31685ms step_avg:58.03ms
step:547/2330 train_time:31741ms step_avg:58.03ms
step:548/2330 train_time:31802ms step_avg:58.03ms
step:549/2330 train_time:31857ms step_avg:58.03ms
step:550/2330 train_time:31919ms step_avg:58.03ms
step:551/2330 train_time:31975ms step_avg:58.03ms
step:552/2330 train_time:32035ms step_avg:58.03ms
step:553/2330 train_time:32091ms step_avg:58.03ms
step:554/2330 train_time:32153ms step_avg:58.04ms
step:555/2330 train_time:32210ms step_avg:58.04ms
step:556/2330 train_time:32270ms step_avg:58.04ms
step:557/2330 train_time:32327ms step_avg:58.04ms
step:558/2330 train_time:32388ms step_avg:58.04ms
step:559/2330 train_time:32444ms step_avg:58.04ms
step:560/2330 train_time:32504ms step_avg:58.04ms
step:561/2330 train_time:32560ms step_avg:58.04ms
step:562/2330 train_time:32622ms step_avg:58.05ms
step:563/2330 train_time:32678ms step_avg:58.04ms
step:564/2330 train_time:32739ms step_avg:58.05ms
step:565/2330 train_time:32794ms step_avg:58.04ms
step:566/2330 train_time:32854ms step_avg:58.05ms
step:567/2330 train_time:32910ms step_avg:58.04ms
step:568/2330 train_time:32970ms step_avg:58.05ms
step:569/2330 train_time:33025ms step_avg:58.04ms
step:570/2330 train_time:33087ms step_avg:58.05ms
step:571/2330 train_time:33143ms step_avg:58.04ms
step:572/2330 train_time:33205ms step_avg:58.05ms
step:573/2330 train_time:33261ms step_avg:58.05ms
step:574/2330 train_time:33323ms step_avg:58.05ms
step:575/2330 train_time:33379ms step_avg:58.05ms
step:576/2330 train_time:33441ms step_avg:58.06ms
step:577/2330 train_time:33497ms step_avg:58.05ms
step:578/2330 train_time:33559ms step_avg:58.06ms
step:579/2330 train_time:33616ms step_avg:58.06ms
step:580/2330 train_time:33677ms step_avg:58.06ms
step:581/2330 train_time:33733ms step_avg:58.06ms
step:582/2330 train_time:33794ms step_avg:58.07ms
step:583/2330 train_time:33850ms step_avg:58.06ms
step:584/2330 train_time:33910ms step_avg:58.07ms
step:585/2330 train_time:33966ms step_avg:58.06ms
step:586/2330 train_time:34026ms step_avg:58.06ms
step:587/2330 train_time:34081ms step_avg:58.06ms
step:588/2330 train_time:34144ms step_avg:58.07ms
step:589/2330 train_time:34200ms step_avg:58.06ms
step:590/2330 train_time:34260ms step_avg:58.07ms
step:591/2330 train_time:34316ms step_avg:58.07ms
step:592/2330 train_time:34377ms step_avg:58.07ms
step:593/2330 train_time:34433ms step_avg:58.07ms
step:594/2330 train_time:34493ms step_avg:58.07ms
step:595/2330 train_time:34549ms step_avg:58.06ms
step:596/2330 train_time:34609ms step_avg:58.07ms
step:597/2330 train_time:34665ms step_avg:58.07ms
step:598/2330 train_time:34727ms step_avg:58.07ms
step:599/2330 train_time:34782ms step_avg:58.07ms
step:600/2330 train_time:34844ms step_avg:58.07ms
step:601/2330 train_time:34899ms step_avg:58.07ms
step:602/2330 train_time:34961ms step_avg:58.08ms
step:603/2330 train_time:35017ms step_avg:58.07ms
step:604/2330 train_time:35079ms step_avg:58.08ms
step:605/2330 train_time:35136ms step_avg:58.08ms
step:606/2330 train_time:35197ms step_avg:58.08ms
step:607/2330 train_time:35253ms step_avg:58.08ms
step:608/2330 train_time:35313ms step_avg:58.08ms
step:609/2330 train_time:35370ms step_avg:58.08ms
step:610/2330 train_time:35430ms step_avg:58.08ms
step:611/2330 train_time:35486ms step_avg:58.08ms
step:612/2330 train_time:35546ms step_avg:58.08ms
step:613/2330 train_time:35603ms step_avg:58.08ms
step:614/2330 train_time:35664ms step_avg:58.08ms
step:615/2330 train_time:35720ms step_avg:58.08ms
step:616/2330 train_time:35781ms step_avg:58.09ms
step:617/2330 train_time:35837ms step_avg:58.08ms
step:618/2330 train_time:35898ms step_avg:58.09ms
step:619/2330 train_time:35954ms step_avg:58.08ms
step:620/2330 train_time:36014ms step_avg:58.09ms
step:621/2330 train_time:36071ms step_avg:58.08ms
step:622/2330 train_time:36130ms step_avg:58.09ms
step:623/2330 train_time:36186ms step_avg:58.08ms
step:624/2330 train_time:36247ms step_avg:58.09ms
step:625/2330 train_time:36303ms step_avg:58.08ms
step:626/2330 train_time:36364ms step_avg:58.09ms
step:627/2330 train_time:36420ms step_avg:58.09ms
step:628/2330 train_time:36482ms step_avg:58.09ms
step:629/2330 train_time:36537ms step_avg:58.09ms
step:630/2330 train_time:36599ms step_avg:58.09ms
step:631/2330 train_time:36655ms step_avg:58.09ms
step:632/2330 train_time:36715ms step_avg:58.09ms
step:633/2330 train_time:36771ms step_avg:58.09ms
step:634/2330 train_time:36831ms step_avg:58.09ms
step:635/2330 train_time:36887ms step_avg:58.09ms
step:636/2330 train_time:36947ms step_avg:58.09ms
step:637/2330 train_time:37003ms step_avg:58.09ms
step:638/2330 train_time:37064ms step_avg:58.09ms
step:639/2330 train_time:37120ms step_avg:58.09ms
step:640/2330 train_time:37181ms step_avg:58.10ms
step:641/2330 train_time:37237ms step_avg:58.09ms
step:642/2330 train_time:37298ms step_avg:58.10ms
step:643/2330 train_time:37354ms step_avg:58.09ms
step:644/2330 train_time:37415ms step_avg:58.10ms
step:645/2330 train_time:37471ms step_avg:58.10ms
step:646/2330 train_time:37532ms step_avg:58.10ms
step:647/2330 train_time:37588ms step_avg:58.10ms
step:648/2330 train_time:37649ms step_avg:58.10ms
step:649/2330 train_time:37704ms step_avg:58.10ms
step:650/2330 train_time:37766ms step_avg:58.10ms
step:651/2330 train_time:37821ms step_avg:58.10ms
step:652/2330 train_time:37883ms step_avg:58.10ms
step:653/2330 train_time:37939ms step_avg:58.10ms
step:654/2330 train_time:38001ms step_avg:58.11ms
step:655/2330 train_time:38056ms step_avg:58.10ms
step:656/2330 train_time:38118ms step_avg:58.11ms
step:657/2330 train_time:38173ms step_avg:58.10ms
step:658/2330 train_time:38234ms step_avg:58.11ms
step:659/2330 train_time:38290ms step_avg:58.10ms
step:660/2330 train_time:38351ms step_avg:58.11ms
step:661/2330 train_time:38406ms step_avg:58.10ms
step:662/2330 train_time:38467ms step_avg:58.11ms
step:663/2330 train_time:38523ms step_avg:58.10ms
step:664/2330 train_time:38584ms step_avg:58.11ms
step:665/2330 train_time:38640ms step_avg:58.11ms
step:666/2330 train_time:38703ms step_avg:58.11ms
step:667/2330 train_time:38759ms step_avg:58.11ms
step:668/2330 train_time:38820ms step_avg:58.11ms
step:669/2330 train_time:38876ms step_avg:58.11ms
step:670/2330 train_time:38937ms step_avg:58.11ms
step:671/2330 train_time:38993ms step_avg:58.11ms
step:672/2330 train_time:39053ms step_avg:58.12ms
step:673/2330 train_time:39109ms step_avg:58.11ms
step:674/2330 train_time:39170ms step_avg:58.12ms
step:675/2330 train_time:39226ms step_avg:58.11ms
step:676/2330 train_time:39287ms step_avg:58.12ms
step:677/2330 train_time:39342ms step_avg:58.11ms
step:678/2330 train_time:39403ms step_avg:58.12ms
step:679/2330 train_time:39459ms step_avg:58.11ms
step:680/2330 train_time:39520ms step_avg:58.12ms
step:681/2330 train_time:39577ms step_avg:58.12ms
step:682/2330 train_time:39639ms step_avg:58.12ms
step:683/2330 train_time:39695ms step_avg:58.12ms
step:684/2330 train_time:39756ms step_avg:58.12ms
step:685/2330 train_time:39813ms step_avg:58.12ms
step:686/2330 train_time:39873ms step_avg:58.12ms
step:687/2330 train_time:39930ms step_avg:58.12ms
step:688/2330 train_time:39990ms step_avg:58.12ms
step:689/2330 train_time:40045ms step_avg:58.12ms
step:690/2330 train_time:40107ms step_avg:58.13ms
step:691/2330 train_time:40162ms step_avg:58.12ms
step:692/2330 train_time:40224ms step_avg:58.13ms
step:693/2330 train_time:40280ms step_avg:58.12ms
step:694/2330 train_time:40342ms step_avg:58.13ms
step:695/2330 train_time:40397ms step_avg:58.13ms
step:696/2330 train_time:40458ms step_avg:58.13ms
step:697/2330 train_time:40515ms step_avg:58.13ms
step:698/2330 train_time:40575ms step_avg:58.13ms
step:699/2330 train_time:40631ms step_avg:58.13ms
step:700/2330 train_time:40692ms step_avg:58.13ms
step:701/2330 train_time:40747ms step_avg:58.13ms
step:702/2330 train_time:40808ms step_avg:58.13ms
step:703/2330 train_time:40865ms step_avg:58.13ms
step:704/2330 train_time:40925ms step_avg:58.13ms
step:705/2330 train_time:40981ms step_avg:58.13ms
step:706/2330 train_time:41043ms step_avg:58.13ms
step:707/2330 train_time:41099ms step_avg:58.13ms
step:708/2330 train_time:41160ms step_avg:58.14ms
step:709/2330 train_time:41216ms step_avg:58.13ms
step:710/2330 train_time:41278ms step_avg:58.14ms
step:711/2330 train_time:41334ms step_avg:58.14ms
step:712/2330 train_time:41395ms step_avg:58.14ms
step:713/2330 train_time:41451ms step_avg:58.14ms
step:714/2330 train_time:41510ms step_avg:58.14ms
step:715/2330 train_time:41566ms step_avg:58.13ms
step:716/2330 train_time:41628ms step_avg:58.14ms
step:717/2330 train_time:41683ms step_avg:58.14ms
step:718/2330 train_time:41745ms step_avg:58.14ms
step:719/2330 train_time:41801ms step_avg:58.14ms
step:720/2330 train_time:41862ms step_avg:58.14ms
step:721/2330 train_time:41918ms step_avg:58.14ms
step:722/2330 train_time:41980ms step_avg:58.14ms
step:723/2330 train_time:42037ms step_avg:58.14ms
step:724/2330 train_time:42097ms step_avg:58.15ms
step:725/2330 train_time:42154ms step_avg:58.14ms
step:726/2330 train_time:42213ms step_avg:58.15ms
step:727/2330 train_time:42270ms step_avg:58.14ms
step:728/2330 train_time:42330ms step_avg:58.15ms
step:729/2330 train_time:42386ms step_avg:58.14ms
step:730/2330 train_time:42446ms step_avg:58.15ms
step:731/2330 train_time:42502ms step_avg:58.14ms
step:732/2330 train_time:42563ms step_avg:58.15ms
step:733/2330 train_time:42619ms step_avg:58.14ms
step:734/2330 train_time:42681ms step_avg:58.15ms
step:735/2330 train_time:42737ms step_avg:58.14ms
step:736/2330 train_time:42798ms step_avg:58.15ms
step:737/2330 train_time:42854ms step_avg:58.15ms
step:738/2330 train_time:42915ms step_avg:58.15ms
step:739/2330 train_time:42972ms step_avg:58.15ms
step:740/2330 train_time:43032ms step_avg:58.15ms
step:741/2330 train_time:43088ms step_avg:58.15ms
step:742/2330 train_time:43148ms step_avg:58.15ms
step:743/2330 train_time:43204ms step_avg:58.15ms
step:744/2330 train_time:43265ms step_avg:58.15ms
step:745/2330 train_time:43321ms step_avg:58.15ms
step:746/2330 train_time:43382ms step_avg:58.15ms
step:747/2330 train_time:43438ms step_avg:58.15ms
step:748/2330 train_time:43500ms step_avg:58.15ms
step:749/2330 train_time:43556ms step_avg:58.15ms
step:750/2330 train_time:43618ms step_avg:58.16ms
step:750/2330 val_loss:5.1602 train_time:43694ms step_avg:58.26ms
step:751/2330 train_time:43713ms step_avg:58.21ms
step:752/2330 train_time:43736ms step_avg:58.16ms
step:753/2330 train_time:43792ms step_avg:58.16ms
step:754/2330 train_time:43858ms step_avg:58.17ms
step:755/2330 train_time:43915ms step_avg:58.17ms
step:756/2330 train_time:43976ms step_avg:58.17ms
step:757/2330 train_time:44032ms step_avg:58.17ms
step:758/2330 train_time:44093ms step_avg:58.17ms
step:759/2330 train_time:44148ms step_avg:58.17ms
step:760/2330 train_time:44209ms step_avg:58.17ms
step:761/2330 train_time:44264ms step_avg:58.17ms
step:762/2330 train_time:44326ms step_avg:58.17ms
step:763/2330 train_time:44381ms step_avg:58.17ms
step:764/2330 train_time:44441ms step_avg:58.17ms
step:765/2330 train_time:44498ms step_avg:58.17ms
step:766/2330 train_time:44557ms step_avg:58.17ms
step:767/2330 train_time:44613ms step_avg:58.17ms
step:768/2330 train_time:44674ms step_avg:58.17ms
step:769/2330 train_time:44731ms step_avg:58.17ms
step:770/2330 train_time:44793ms step_avg:58.17ms
step:771/2330 train_time:44850ms step_avg:58.17ms
step:772/2330 train_time:44914ms step_avg:58.18ms
step:773/2330 train_time:44971ms step_avg:58.18ms
step:774/2330 train_time:45032ms step_avg:58.18ms
step:775/2330 train_time:45089ms step_avg:58.18ms
step:776/2330 train_time:45151ms step_avg:58.18ms
step:777/2330 train_time:45207ms step_avg:58.18ms
step:778/2330 train_time:45269ms step_avg:58.19ms
step:779/2330 train_time:45326ms step_avg:58.18ms
step:780/2330 train_time:45387ms step_avg:58.19ms
step:781/2330 train_time:45444ms step_avg:58.19ms
step:782/2330 train_time:45504ms step_avg:58.19ms
step:783/2330 train_time:45562ms step_avg:58.19ms
step:784/2330 train_time:45622ms step_avg:58.19ms
step:785/2330 train_time:45680ms step_avg:58.19ms
step:786/2330 train_time:45740ms step_avg:58.19ms
step:787/2330 train_time:45798ms step_avg:58.19ms
step:788/2330 train_time:45858ms step_avg:58.20ms
step:789/2330 train_time:45915ms step_avg:58.19ms
step:790/2330 train_time:45977ms step_avg:58.20ms
step:791/2330 train_time:46034ms step_avg:58.20ms
step:792/2330 train_time:46096ms step_avg:58.20ms
step:793/2330 train_time:46153ms step_avg:58.20ms
step:794/2330 train_time:46215ms step_avg:58.20ms
step:795/2330 train_time:46271ms step_avg:58.20ms
step:796/2330 train_time:46334ms step_avg:58.21ms
step:797/2330 train_time:46390ms step_avg:58.21ms
step:798/2330 train_time:46452ms step_avg:58.21ms
step:799/2330 train_time:46508ms step_avg:58.21ms
step:800/2330 train_time:46571ms step_avg:58.21ms
step:801/2330 train_time:46628ms step_avg:58.21ms
step:802/2330 train_time:46690ms step_avg:58.22ms
step:803/2330 train_time:46746ms step_avg:58.21ms
step:804/2330 train_time:46808ms step_avg:58.22ms
step:805/2330 train_time:46865ms step_avg:58.22ms
step:806/2330 train_time:46928ms step_avg:58.22ms
step:807/2330 train_time:46985ms step_avg:58.22ms
step:808/2330 train_time:47047ms step_avg:58.23ms
step:809/2330 train_time:47104ms step_avg:58.22ms
step:810/2330 train_time:47166ms step_avg:58.23ms
step:811/2330 train_time:47224ms step_avg:58.23ms
step:812/2330 train_time:47285ms step_avg:58.23ms
step:813/2330 train_time:47342ms step_avg:58.23ms
step:814/2330 train_time:47403ms step_avg:58.23ms
step:815/2330 train_time:47460ms step_avg:58.23ms
step:816/2330 train_time:47521ms step_avg:58.24ms
step:817/2330 train_time:47578ms step_avg:58.24ms
step:818/2330 train_time:47638ms step_avg:58.24ms
step:819/2330 train_time:47695ms step_avg:58.24ms
step:820/2330 train_time:47757ms step_avg:58.24ms
step:821/2330 train_time:47814ms step_avg:58.24ms
step:822/2330 train_time:47875ms step_avg:58.24ms
step:823/2330 train_time:47932ms step_avg:58.24ms
step:824/2330 train_time:47994ms step_avg:58.25ms
step:825/2330 train_time:48050ms step_avg:58.24ms
step:826/2330 train_time:48113ms step_avg:58.25ms
step:827/2330 train_time:48169ms step_avg:58.25ms
step:828/2330 train_time:48232ms step_avg:58.25ms
step:829/2330 train_time:48288ms step_avg:58.25ms
step:830/2330 train_time:48351ms step_avg:58.25ms
step:831/2330 train_time:48407ms step_avg:58.25ms
step:832/2330 train_time:48469ms step_avg:58.26ms
step:833/2330 train_time:48525ms step_avg:58.25ms
step:834/2330 train_time:48588ms step_avg:58.26ms
step:835/2330 train_time:48644ms step_avg:58.26ms
step:836/2330 train_time:48707ms step_avg:58.26ms
step:837/2330 train_time:48764ms step_avg:58.26ms
step:838/2330 train_time:48825ms step_avg:58.26ms
step:839/2330 train_time:48883ms step_avg:58.26ms
step:840/2330 train_time:48944ms step_avg:58.27ms
step:841/2330 train_time:49001ms step_avg:58.27ms
step:842/2330 train_time:49062ms step_avg:58.27ms
step:843/2330 train_time:49120ms step_avg:58.27ms
step:844/2330 train_time:49180ms step_avg:58.27ms
step:845/2330 train_time:49236ms step_avg:58.27ms
step:846/2330 train_time:49298ms step_avg:58.27ms
step:847/2330 train_time:49355ms step_avg:58.27ms
step:848/2330 train_time:49416ms step_avg:58.27ms
step:849/2330 train_time:49473ms step_avg:58.27ms
step:850/2330 train_time:49534ms step_avg:58.28ms
step:851/2330 train_time:49590ms step_avg:58.27ms
step:852/2330 train_time:49652ms step_avg:58.28ms
step:853/2330 train_time:49708ms step_avg:58.27ms
step:854/2330 train_time:49770ms step_avg:58.28ms
step:855/2330 train_time:49827ms step_avg:58.28ms
step:856/2330 train_time:49890ms step_avg:58.28ms
step:857/2330 train_time:49946ms step_avg:58.28ms
step:858/2330 train_time:50009ms step_avg:58.29ms
step:859/2330 train_time:50066ms step_avg:58.28ms
step:860/2330 train_time:50129ms step_avg:58.29ms
step:861/2330 train_time:50186ms step_avg:58.29ms
step:862/2330 train_time:50249ms step_avg:58.29ms
step:863/2330 train_time:50306ms step_avg:58.29ms
step:864/2330 train_time:50368ms step_avg:58.30ms
step:865/2330 train_time:50425ms step_avg:58.30ms
step:866/2330 train_time:50487ms step_avg:58.30ms
step:867/2330 train_time:50544ms step_avg:58.30ms
step:868/2330 train_time:50605ms step_avg:58.30ms
step:869/2330 train_time:50662ms step_avg:58.30ms
step:870/2330 train_time:50723ms step_avg:58.30ms
step:871/2330 train_time:50780ms step_avg:58.30ms
step:872/2330 train_time:50841ms step_avg:58.30ms
step:873/2330 train_time:50898ms step_avg:58.30ms
step:874/2330 train_time:50959ms step_avg:58.31ms
step:875/2330 train_time:51016ms step_avg:58.30ms
step:876/2330 train_time:51077ms step_avg:58.31ms
step:877/2330 train_time:51134ms step_avg:58.31ms
step:878/2330 train_time:51195ms step_avg:58.31ms
step:879/2330 train_time:51252ms step_avg:58.31ms
step:880/2330 train_time:51314ms step_avg:58.31ms
step:881/2330 train_time:51371ms step_avg:58.31ms
step:882/2330 train_time:51432ms step_avg:58.31ms
step:883/2330 train_time:51489ms step_avg:58.31ms
step:884/2330 train_time:51550ms step_avg:58.31ms
step:885/2330 train_time:51607ms step_avg:58.31ms
step:886/2330 train_time:51669ms step_avg:58.32ms
step:887/2330 train_time:51726ms step_avg:58.32ms
step:888/2330 train_time:51789ms step_avg:58.32ms
step:889/2330 train_time:51845ms step_avg:58.32ms
step:890/2330 train_time:51909ms step_avg:58.32ms
step:891/2330 train_time:51965ms step_avg:58.32ms
step:892/2330 train_time:52028ms step_avg:58.33ms
step:893/2330 train_time:52085ms step_avg:58.33ms
step:894/2330 train_time:52147ms step_avg:58.33ms
step:895/2330 train_time:52204ms step_avg:58.33ms
step:896/2330 train_time:52266ms step_avg:58.33ms
step:897/2330 train_time:52323ms step_avg:58.33ms
step:898/2330 train_time:52384ms step_avg:58.33ms
step:899/2330 train_time:52442ms step_avg:58.33ms
step:900/2330 train_time:52502ms step_avg:58.34ms
step:901/2330 train_time:52560ms step_avg:58.34ms
step:902/2330 train_time:52620ms step_avg:58.34ms
step:903/2330 train_time:52677ms step_avg:58.34ms
step:904/2330 train_time:52739ms step_avg:58.34ms
step:905/2330 train_time:52795ms step_avg:58.34ms
step:906/2330 train_time:52858ms step_avg:58.34ms
step:907/2330 train_time:52915ms step_avg:58.34ms
step:908/2330 train_time:52976ms step_avg:58.34ms
step:909/2330 train_time:53033ms step_avg:58.34ms
step:910/2330 train_time:53094ms step_avg:58.35ms
step:911/2330 train_time:53151ms step_avg:58.34ms
step:912/2330 train_time:53213ms step_avg:58.35ms
step:913/2330 train_time:53270ms step_avg:58.35ms
step:914/2330 train_time:53332ms step_avg:58.35ms
step:915/2330 train_time:53388ms step_avg:58.35ms
step:916/2330 train_time:53450ms step_avg:58.35ms
step:917/2330 train_time:53506ms step_avg:58.35ms
step:918/2330 train_time:53569ms step_avg:58.35ms
step:919/2330 train_time:53625ms step_avg:58.35ms
step:920/2330 train_time:53688ms step_avg:58.36ms
step:921/2330 train_time:53744ms step_avg:58.35ms
step:922/2330 train_time:53807ms step_avg:58.36ms
step:923/2330 train_time:53865ms step_avg:58.36ms
step:924/2330 train_time:53926ms step_avg:58.36ms
step:925/2330 train_time:53983ms step_avg:58.36ms
step:926/2330 train_time:54044ms step_avg:58.36ms
step:927/2330 train_time:54102ms step_avg:58.36ms
step:928/2330 train_time:54163ms step_avg:58.37ms
step:929/2330 train_time:54221ms step_avg:58.37ms
step:930/2330 train_time:54281ms step_avg:58.37ms
step:931/2330 train_time:54339ms step_avg:58.37ms
step:932/2330 train_time:54399ms step_avg:58.37ms
step:933/2330 train_time:54456ms step_avg:58.37ms
step:934/2330 train_time:54517ms step_avg:58.37ms
step:935/2330 train_time:54574ms step_avg:58.37ms
step:936/2330 train_time:54635ms step_avg:58.37ms
step:937/2330 train_time:54692ms step_avg:58.37ms
step:938/2330 train_time:54753ms step_avg:58.37ms
step:939/2330 train_time:54810ms step_avg:58.37ms
step:940/2330 train_time:54872ms step_avg:58.37ms
step:941/2330 train_time:54928ms step_avg:58.37ms
step:942/2330 train_time:54990ms step_avg:58.38ms
step:943/2330 train_time:55046ms step_avg:58.37ms
step:944/2330 train_time:55109ms step_avg:58.38ms
step:945/2330 train_time:55166ms step_avg:58.38ms
step:946/2330 train_time:55228ms step_avg:58.38ms
step:947/2330 train_time:55285ms step_avg:58.38ms
step:948/2330 train_time:55348ms step_avg:58.38ms
step:949/2330 train_time:55404ms step_avg:58.38ms
step:950/2330 train_time:55466ms step_avg:58.39ms
step:951/2330 train_time:55524ms step_avg:58.38ms
step:952/2330 train_time:55584ms step_avg:58.39ms
step:953/2330 train_time:55642ms step_avg:58.39ms
step:954/2330 train_time:55703ms step_avg:58.39ms
step:955/2330 train_time:55760ms step_avg:58.39ms
step:956/2330 train_time:55820ms step_avg:58.39ms
step:957/2330 train_time:55878ms step_avg:58.39ms
step:958/2330 train_time:55939ms step_avg:58.39ms
step:959/2330 train_time:55996ms step_avg:58.39ms
step:960/2330 train_time:56058ms step_avg:58.39ms
step:961/2330 train_time:56115ms step_avg:58.39ms
step:962/2330 train_time:56176ms step_avg:58.39ms
step:963/2330 train_time:56232ms step_avg:58.39ms
step:964/2330 train_time:56294ms step_avg:58.40ms
step:965/2330 train_time:56351ms step_avg:58.39ms
step:966/2330 train_time:56412ms step_avg:58.40ms
step:967/2330 train_time:56468ms step_avg:58.40ms
step:968/2330 train_time:56531ms step_avg:58.40ms
step:969/2330 train_time:56587ms step_avg:58.40ms
step:970/2330 train_time:56650ms step_avg:58.40ms
step:971/2330 train_time:56706ms step_avg:58.40ms
step:972/2330 train_time:56769ms step_avg:58.40ms
step:973/2330 train_time:56825ms step_avg:58.40ms
step:974/2330 train_time:56888ms step_avg:58.41ms
step:975/2330 train_time:56945ms step_avg:58.41ms
step:976/2330 train_time:57008ms step_avg:58.41ms
step:977/2330 train_time:57066ms step_avg:58.41ms
step:978/2330 train_time:57126ms step_avg:58.41ms
step:979/2330 train_time:57184ms step_avg:58.41ms
step:980/2330 train_time:57245ms step_avg:58.41ms
step:981/2330 train_time:57302ms step_avg:58.41ms
step:982/2330 train_time:57364ms step_avg:58.41ms
step:983/2330 train_time:57421ms step_avg:58.41ms
step:984/2330 train_time:57482ms step_avg:58.42ms
step:985/2330 train_time:57540ms step_avg:58.42ms
step:986/2330 train_time:57600ms step_avg:58.42ms
step:987/2330 train_time:57657ms step_avg:58.42ms
step:988/2330 train_time:57718ms step_avg:58.42ms
step:989/2330 train_time:57776ms step_avg:58.42ms
step:990/2330 train_time:57837ms step_avg:58.42ms
step:991/2330 train_time:57893ms step_avg:58.42ms
step:992/2330 train_time:57955ms step_avg:58.42ms
step:993/2330 train_time:58012ms step_avg:58.42ms
step:994/2330 train_time:58073ms step_avg:58.42ms
step:995/2330 train_time:58130ms step_avg:58.42ms
step:996/2330 train_time:58192ms step_avg:58.43ms
step:997/2330 train_time:58248ms step_avg:58.42ms
step:998/2330 train_time:58311ms step_avg:58.43ms
step:999/2330 train_time:58368ms step_avg:58.43ms
step:1000/2330 train_time:58430ms step_avg:58.43ms
step:1000/2330 val_loss:4.7710 train_time:58510ms step_avg:58.51ms
step:1001/2330 train_time:58529ms step_avg:58.47ms
step:1002/2330 train_time:58551ms step_avg:58.43ms
step:1003/2330 train_time:58608ms step_avg:58.43ms
step:1004/2330 train_time:58672ms step_avg:58.44ms
step:1005/2330 train_time:58728ms step_avg:58.44ms
step:1006/2330 train_time:58793ms step_avg:58.44ms
step:1007/2330 train_time:58849ms step_avg:58.44ms
step:1008/2330 train_time:58910ms step_avg:58.44ms
step:1009/2330 train_time:58967ms step_avg:58.44ms
step:1010/2330 train_time:59028ms step_avg:58.44ms
step:1011/2330 train_time:59084ms step_avg:58.44ms
step:1012/2330 train_time:59145ms step_avg:58.44ms
step:1013/2330 train_time:59201ms step_avg:58.44ms
step:1014/2330 train_time:59262ms step_avg:58.44ms
step:1015/2330 train_time:59318ms step_avg:58.44ms
step:1016/2330 train_time:59378ms step_avg:58.44ms
step:1017/2330 train_time:59435ms step_avg:58.44ms
step:1018/2330 train_time:59496ms step_avg:58.44ms
step:1019/2330 train_time:59554ms step_avg:58.44ms
step:1020/2330 train_time:59615ms step_avg:58.45ms
step:1021/2330 train_time:59673ms step_avg:58.45ms
step:1022/2330 train_time:59735ms step_avg:58.45ms
step:1023/2330 train_time:59791ms step_avg:58.45ms
step:1024/2330 train_time:59854ms step_avg:58.45ms
step:1025/2330 train_time:59911ms step_avg:58.45ms
step:1026/2330 train_time:59972ms step_avg:58.45ms
step:1027/2330 train_time:60029ms step_avg:58.45ms
step:1028/2330 train_time:60089ms step_avg:58.45ms
step:1029/2330 train_time:60146ms step_avg:58.45ms
step:1030/2330 train_time:60207ms step_avg:58.45ms
step:1031/2330 train_time:60264ms step_avg:58.45ms
step:1032/2330 train_time:60324ms step_avg:58.45ms
step:1033/2330 train_time:60380ms step_avg:58.45ms
step:1034/2330 train_time:60441ms step_avg:58.45ms
step:1035/2330 train_time:60498ms step_avg:58.45ms
step:1036/2330 train_time:60559ms step_avg:58.45ms
step:1037/2330 train_time:60616ms step_avg:58.45ms
step:1038/2330 train_time:60678ms step_avg:58.46ms
step:1039/2330 train_time:60734ms step_avg:58.45ms
step:1040/2330 train_time:60797ms step_avg:58.46ms
step:1041/2330 train_time:60853ms step_avg:58.46ms
step:1042/2330 train_time:60915ms step_avg:58.46ms
step:1043/2330 train_time:60972ms step_avg:58.46ms
step:1044/2330 train_time:61034ms step_avg:58.46ms
step:1045/2330 train_time:61091ms step_avg:58.46ms
step:1046/2330 train_time:61151ms step_avg:58.46ms
step:1047/2330 train_time:61208ms step_avg:58.46ms
step:1048/2330 train_time:61268ms step_avg:58.46ms
step:1049/2330 train_time:61326ms step_avg:58.46ms
step:1050/2330 train_time:61386ms step_avg:58.46ms
step:1051/2330 train_time:61443ms step_avg:58.46ms
step:1052/2330 train_time:61503ms step_avg:58.46ms
step:1053/2330 train_time:61559ms step_avg:58.46ms
step:1054/2330 train_time:61621ms step_avg:58.46ms
step:1055/2330 train_time:61678ms step_avg:58.46ms
step:1056/2330 train_time:61740ms step_avg:58.47ms
step:1057/2330 train_time:61796ms step_avg:58.46ms
step:1058/2330 train_time:61858ms step_avg:58.47ms
step:1059/2330 train_time:61914ms step_avg:58.47ms
step:1060/2330 train_time:61977ms step_avg:58.47ms
step:1061/2330 train_time:62033ms step_avg:58.47ms
step:1062/2330 train_time:62096ms step_avg:58.47ms
step:1063/2330 train_time:62152ms step_avg:58.47ms
step:1064/2330 train_time:62214ms step_avg:58.47ms
step:1065/2330 train_time:62272ms step_avg:58.47ms
step:1066/2330 train_time:62332ms step_avg:58.47ms
step:1067/2330 train_time:62390ms step_avg:58.47ms
step:1068/2330 train_time:62450ms step_avg:58.47ms
step:1069/2330 train_time:62507ms step_avg:58.47ms
step:1070/2330 train_time:62568ms step_avg:58.48ms
step:1071/2330 train_time:62625ms step_avg:58.47ms
step:1072/2330 train_time:62686ms step_avg:58.48ms
step:1073/2330 train_time:62742ms step_avg:58.47ms
step:1074/2330 train_time:62804ms step_avg:58.48ms
step:1075/2330 train_time:62861ms step_avg:58.47ms
step:1076/2330 train_time:62922ms step_avg:58.48ms
step:1077/2330 train_time:62979ms step_avg:58.48ms
step:1078/2330 train_time:63041ms step_avg:58.48ms
step:1079/2330 train_time:63097ms step_avg:58.48ms
step:1080/2330 train_time:63160ms step_avg:58.48ms
step:1081/2330 train_time:63216ms step_avg:58.48ms
step:1082/2330 train_time:63278ms step_avg:58.48ms
step:1083/2330 train_time:63335ms step_avg:58.48ms
step:1084/2330 train_time:63397ms step_avg:58.48ms
step:1085/2330 train_time:63453ms step_avg:58.48ms
step:1086/2330 train_time:63515ms step_avg:58.49ms
step:1087/2330 train_time:63572ms step_avg:58.48ms
step:1088/2330 train_time:63633ms step_avg:58.49ms
step:1089/2330 train_time:63690ms step_avg:58.48ms
step:1090/2330 train_time:63751ms step_avg:58.49ms
step:1091/2330 train_time:63808ms step_avg:58.49ms
step:1092/2330 train_time:63869ms step_avg:58.49ms
step:1093/2330 train_time:63926ms step_avg:58.49ms
step:1094/2330 train_time:63987ms step_avg:58.49ms
step:1095/2330 train_time:64044ms step_avg:58.49ms
step:1096/2330 train_time:64105ms step_avg:58.49ms
step:1097/2330 train_time:64162ms step_avg:58.49ms
step:1098/2330 train_time:64223ms step_avg:58.49ms
step:1099/2330 train_time:64279ms step_avg:58.49ms
step:1100/2330 train_time:64342ms step_avg:58.49ms
step:1101/2330 train_time:64398ms step_avg:58.49ms
step:1102/2330 train_time:64460ms step_avg:58.49ms
step:1103/2330 train_time:64516ms step_avg:58.49ms
step:1104/2330 train_time:64578ms step_avg:58.49ms
step:1105/2330 train_time:64634ms step_avg:58.49ms
step:1106/2330 train_time:64697ms step_avg:58.50ms
step:1107/2330 train_time:64753ms step_avg:58.49ms
step:1108/2330 train_time:64815ms step_avg:58.50ms
step:1109/2330 train_time:64872ms step_avg:58.50ms
step:1110/2330 train_time:64933ms step_avg:58.50ms
step:1111/2330 train_time:64990ms step_avg:58.50ms
step:1112/2330 train_time:65051ms step_avg:58.50ms
step:1113/2330 train_time:65108ms step_avg:58.50ms
step:1114/2330 train_time:65169ms step_avg:58.50ms
step:1115/2330 train_time:65226ms step_avg:58.50ms
step:1116/2330 train_time:65288ms step_avg:58.50ms
step:1117/2330 train_time:65344ms step_avg:58.50ms
step:1118/2330 train_time:65406ms step_avg:58.50ms
step:1119/2330 train_time:65462ms step_avg:58.50ms
step:1120/2330 train_time:65525ms step_avg:58.50ms
step:1121/2330 train_time:65581ms step_avg:58.50ms
step:1122/2330 train_time:65643ms step_avg:58.51ms
step:1123/2330 train_time:65699ms step_avg:58.50ms
step:1124/2330 train_time:65761ms step_avg:58.51ms
step:1125/2330 train_time:65817ms step_avg:58.50ms
step:1126/2330 train_time:65880ms step_avg:58.51ms
step:1127/2330 train_time:65936ms step_avg:58.51ms
step:1128/2330 train_time:65999ms step_avg:58.51ms
step:1129/2330 train_time:66055ms step_avg:58.51ms
step:1130/2330 train_time:66118ms step_avg:58.51ms
step:1131/2330 train_time:66174ms step_avg:58.51ms
step:1132/2330 train_time:66237ms step_avg:58.51ms
step:1133/2330 train_time:66293ms step_avg:58.51ms
step:1134/2330 train_time:66355ms step_avg:58.51ms
step:1135/2330 train_time:66412ms step_avg:58.51ms
step:1136/2330 train_time:66472ms step_avg:58.51ms
step:1137/2330 train_time:66529ms step_avg:58.51ms
step:1138/2330 train_time:66590ms step_avg:58.51ms
step:1139/2330 train_time:66647ms step_avg:58.51ms
step:1140/2330 train_time:66707ms step_avg:58.52ms
step:1141/2330 train_time:66765ms step_avg:58.51ms
step:1142/2330 train_time:66825ms step_avg:58.52ms
step:1143/2330 train_time:66883ms step_avg:58.51ms
step:1144/2330 train_time:66944ms step_avg:58.52ms
step:1145/2330 train_time:67000ms step_avg:58.52ms
step:1146/2330 train_time:67061ms step_avg:58.52ms
step:1147/2330 train_time:67117ms step_avg:58.52ms
step:1148/2330 train_time:67180ms step_avg:58.52ms
step:1149/2330 train_time:67236ms step_avg:58.52ms
step:1150/2330 train_time:67298ms step_avg:58.52ms
step:1151/2330 train_time:67354ms step_avg:58.52ms
step:1152/2330 train_time:67417ms step_avg:58.52ms
step:1153/2330 train_time:67473ms step_avg:58.52ms
step:1154/2330 train_time:67536ms step_avg:58.52ms
step:1155/2330 train_time:67593ms step_avg:58.52ms
step:1156/2330 train_time:67654ms step_avg:58.52ms
step:1157/2330 train_time:67712ms step_avg:58.52ms
step:1158/2330 train_time:67772ms step_avg:58.53ms
step:1159/2330 train_time:67830ms step_avg:58.52ms
step:1160/2330 train_time:67890ms step_avg:58.53ms
step:1161/2330 train_time:67946ms step_avg:58.52ms
step:1162/2330 train_time:68008ms step_avg:58.53ms
step:1163/2330 train_time:68064ms step_avg:58.52ms
step:1164/2330 train_time:68126ms step_avg:58.53ms
step:1165/2330 train_time:68183ms step_avg:58.53ms
step:1166/2330 train_time:68245ms step_avg:58.53ms
step:1167/2330 train_time:68301ms step_avg:58.53ms
step:1168/2330 train_time:68364ms step_avg:58.53ms
step:1169/2330 train_time:68420ms step_avg:58.53ms
step:1170/2330 train_time:68482ms step_avg:58.53ms
step:1171/2330 train_time:68538ms step_avg:58.53ms
step:1172/2330 train_time:68600ms step_avg:58.53ms
step:1173/2330 train_time:68657ms step_avg:58.53ms
step:1174/2330 train_time:68720ms step_avg:58.53ms
step:1175/2330 train_time:68776ms step_avg:58.53ms
step:1176/2330 train_time:68838ms step_avg:58.54ms
step:1177/2330 train_time:68895ms step_avg:58.53ms
step:1178/2330 train_time:68957ms step_avg:58.54ms
step:1179/2330 train_time:69014ms step_avg:58.54ms
step:1180/2330 train_time:69075ms step_avg:58.54ms
step:1181/2330 train_time:69132ms step_avg:58.54ms
step:1182/2330 train_time:69192ms step_avg:58.54ms
step:1183/2330 train_time:69249ms step_avg:58.54ms
step:1184/2330 train_time:69310ms step_avg:58.54ms
step:1185/2330 train_time:69367ms step_avg:58.54ms
step:1186/2330 train_time:69428ms step_avg:58.54ms
step:1187/2330 train_time:69485ms step_avg:58.54ms
step:1188/2330 train_time:69547ms step_avg:58.54ms
step:1189/2330 train_time:69603ms step_avg:58.54ms
step:1190/2330 train_time:69665ms step_avg:58.54ms
step:1191/2330 train_time:69721ms step_avg:58.54ms
step:1192/2330 train_time:69784ms step_avg:58.54ms
step:1193/2330 train_time:69840ms step_avg:58.54ms
step:1194/2330 train_time:69903ms step_avg:58.55ms
step:1195/2330 train_time:69959ms step_avg:58.54ms
step:1196/2330 train_time:70022ms step_avg:58.55ms
step:1197/2330 train_time:70078ms step_avg:58.54ms
step:1198/2330 train_time:70140ms step_avg:58.55ms
step:1199/2330 train_time:70196ms step_avg:58.55ms
step:1200/2330 train_time:70258ms step_avg:58.55ms
step:1201/2330 train_time:70315ms step_avg:58.55ms
step:1202/2330 train_time:70377ms step_avg:58.55ms
step:1203/2330 train_time:70433ms step_avg:58.55ms
step:1204/2330 train_time:70496ms step_avg:58.55ms
step:1205/2330 train_time:70553ms step_avg:58.55ms
step:1206/2330 train_time:70614ms step_avg:58.55ms
step:1207/2330 train_time:70672ms step_avg:58.55ms
step:1208/2330 train_time:70732ms step_avg:58.55ms
step:1209/2330 train_time:70790ms step_avg:58.55ms
step:1210/2330 train_time:70850ms step_avg:58.55ms
step:1211/2330 train_time:70907ms step_avg:58.55ms
step:1212/2330 train_time:70968ms step_avg:58.55ms
step:1213/2330 train_time:71025ms step_avg:58.55ms
step:1214/2330 train_time:71087ms step_avg:58.56ms
step:1215/2330 train_time:71143ms step_avg:58.55ms
step:1216/2330 train_time:71205ms step_avg:58.56ms
step:1217/2330 train_time:71262ms step_avg:58.56ms
step:1218/2330 train_time:71323ms step_avg:58.56ms
step:1219/2330 train_time:71380ms step_avg:58.56ms
step:1220/2330 train_time:71442ms step_avg:58.56ms
step:1221/2330 train_time:71498ms step_avg:58.56ms
step:1222/2330 train_time:71560ms step_avg:58.56ms
step:1223/2330 train_time:71616ms step_avg:58.56ms
step:1224/2330 train_time:71678ms step_avg:58.56ms
step:1225/2330 train_time:71735ms step_avg:58.56ms
step:1226/2330 train_time:71798ms step_avg:58.56ms
step:1227/2330 train_time:71854ms step_avg:58.56ms
step:1228/2330 train_time:71916ms step_avg:58.56ms
step:1229/2330 train_time:71973ms step_avg:58.56ms
step:1230/2330 train_time:72035ms step_avg:58.57ms
step:1231/2330 train_time:72092ms step_avg:58.56ms
step:1232/2330 train_time:72152ms step_avg:58.57ms
step:1233/2330 train_time:72210ms step_avg:58.56ms
step:1234/2330 train_time:72270ms step_avg:58.57ms
step:1235/2330 train_time:72327ms step_avg:58.56ms
step:1236/2330 train_time:72387ms step_avg:58.57ms
step:1237/2330 train_time:72444ms step_avg:58.56ms
step:1238/2330 train_time:72505ms step_avg:58.57ms
step:1239/2330 train_time:72562ms step_avg:58.56ms
step:1240/2330 train_time:72623ms step_avg:58.57ms
step:1241/2330 train_time:72679ms step_avg:58.56ms
step:1242/2330 train_time:72741ms step_avg:58.57ms
step:1243/2330 train_time:72798ms step_avg:58.57ms
step:1244/2330 train_time:72860ms step_avg:58.57ms
step:1245/2330 train_time:72916ms step_avg:58.57ms
step:1246/2330 train_time:72979ms step_avg:58.57ms
step:1247/2330 train_time:73035ms step_avg:58.57ms
step:1248/2330 train_time:73097ms step_avg:58.57ms
step:1249/2330 train_time:73153ms step_avg:58.57ms
step:1250/2330 train_time:73215ms step_avg:58.57ms
step:1250/2330 val_loss:4.5670 train_time:73293ms step_avg:58.63ms
step:1251/2330 train_time:73312ms step_avg:58.60ms
step:1252/2330 train_time:73335ms step_avg:58.57ms
step:1253/2330 train_time:73393ms step_avg:58.57ms
step:1254/2330 train_time:73458ms step_avg:58.58ms
step:1255/2330 train_time:73515ms step_avg:58.58ms
step:1256/2330 train_time:73578ms step_avg:58.58ms
step:1257/2330 train_time:73633ms step_avg:58.58ms
step:1258/2330 train_time:73696ms step_avg:58.58ms
step:1259/2330 train_time:73752ms step_avg:58.58ms
step:1260/2330 train_time:73814ms step_avg:58.58ms
step:1261/2330 train_time:73870ms step_avg:58.58ms
step:1262/2330 train_time:73930ms step_avg:58.58ms
step:1263/2330 train_time:73987ms step_avg:58.58ms
step:1264/2330 train_time:74047ms step_avg:58.58ms
step:1265/2330 train_time:74103ms step_avg:58.58ms
step:1266/2330 train_time:74164ms step_avg:58.58ms
step:1267/2330 train_time:74221ms step_avg:58.58ms
step:1268/2330 train_time:74282ms step_avg:58.58ms
step:1269/2330 train_time:74340ms step_avg:58.58ms
step:1270/2330 train_time:74403ms step_avg:58.58ms
step:1271/2330 train_time:74460ms step_avg:58.58ms
step:1272/2330 train_time:74523ms step_avg:58.59ms
step:1273/2330 train_time:74579ms step_avg:58.59ms
step:1274/2330 train_time:74641ms step_avg:58.59ms
step:1275/2330 train_time:74697ms step_avg:58.59ms
step:1276/2330 train_time:74760ms step_avg:58.59ms
step:1277/2330 train_time:74816ms step_avg:58.59ms
step:1278/2330 train_time:74878ms step_avg:58.59ms
step:1279/2330 train_time:74934ms step_avg:58.59ms
step:1280/2330 train_time:74996ms step_avg:58.59ms
step:1281/2330 train_time:75053ms step_avg:58.59ms
step:1282/2330 train_time:75114ms step_avg:58.59ms
step:1283/2330 train_time:75171ms step_avg:58.59ms
step:1284/2330 train_time:75233ms step_avg:58.59ms
step:1285/2330 train_time:75289ms step_avg:58.59ms
step:1286/2330 train_time:75351ms step_avg:58.59ms
step:1287/2330 train_time:75408ms step_avg:58.59ms
step:1288/2330 train_time:75470ms step_avg:58.59ms
step:1289/2330 train_time:75527ms step_avg:58.59ms
step:1290/2330 train_time:75588ms step_avg:58.60ms
step:1291/2330 train_time:75645ms step_avg:58.59ms
step:1292/2330 train_time:75706ms step_avg:58.60ms
step:1293/2330 train_time:75763ms step_avg:58.59ms
step:1294/2330 train_time:75824ms step_avg:58.60ms
step:1295/2330 train_time:75880ms step_avg:58.59ms
step:1296/2330 train_time:75942ms step_avg:58.60ms
step:1297/2330 train_time:75998ms step_avg:58.60ms
step:1298/2330 train_time:76060ms step_avg:58.60ms
step:1299/2330 train_time:76116ms step_avg:58.60ms
step:1300/2330 train_time:76178ms step_avg:58.60ms
step:1301/2330 train_time:76234ms step_avg:58.60ms
step:1302/2330 train_time:76296ms step_avg:58.60ms
step:1303/2330 train_time:76353ms step_avg:58.60ms
step:1304/2330 train_time:76416ms step_avg:58.60ms
step:1305/2330 train_time:76473ms step_avg:58.60ms
step:1306/2330 train_time:76536ms step_avg:58.60ms
step:1307/2330 train_time:76593ms step_avg:58.60ms
step:1308/2330 train_time:76655ms step_avg:58.60ms
step:1309/2330 train_time:76712ms step_avg:58.60ms
step:1310/2330 train_time:76774ms step_avg:58.61ms
step:1311/2330 train_time:76831ms step_avg:58.60ms
step:1312/2330 train_time:76891ms step_avg:58.61ms
step:1313/2330 train_time:76948ms step_avg:58.60ms
step:1314/2330 train_time:77009ms step_avg:58.61ms
step:1315/2330 train_time:77066ms step_avg:58.61ms
step:1316/2330 train_time:77126ms step_avg:58.61ms
step:1317/2330 train_time:77183ms step_avg:58.61ms
step:1318/2330 train_time:77244ms step_avg:58.61ms
step:1319/2330 train_time:77300ms step_avg:58.61ms
step:1320/2330 train_time:77362ms step_avg:58.61ms
step:1321/2330 train_time:77418ms step_avg:58.61ms
step:1322/2330 train_time:77481ms step_avg:58.61ms
step:1323/2330 train_time:77537ms step_avg:58.61ms
step:1324/2330 train_time:77601ms step_avg:58.61ms
step:1325/2330 train_time:77657ms step_avg:58.61ms
step:1326/2330 train_time:77719ms step_avg:58.61ms
step:1327/2330 train_time:77775ms step_avg:58.61ms
step:1328/2330 train_time:77838ms step_avg:58.61ms
step:1329/2330 train_time:77894ms step_avg:58.61ms
step:1330/2330 train_time:77956ms step_avg:58.61ms
step:1331/2330 train_time:78012ms step_avg:58.61ms
step:1332/2330 train_time:78074ms step_avg:58.61ms
step:1333/2330 train_time:78130ms step_avg:58.61ms
step:1334/2330 train_time:78192ms step_avg:58.61ms
step:1335/2330 train_time:78248ms step_avg:58.61ms
step:1336/2330 train_time:78309ms step_avg:58.61ms
step:1337/2330 train_time:78367ms step_avg:58.61ms
step:1338/2330 train_time:78427ms step_avg:58.62ms
step:1339/2330 train_time:78484ms step_avg:58.61ms
step:1340/2330 train_time:78545ms step_avg:58.62ms
step:1341/2330 train_time:78602ms step_avg:58.61ms
step:1342/2330 train_time:78663ms step_avg:58.62ms
step:1343/2330 train_time:78720ms step_avg:58.61ms
step:1344/2330 train_time:78781ms step_avg:58.62ms
step:1345/2330 train_time:78837ms step_avg:58.61ms
step:1346/2330 train_time:78899ms step_avg:58.62ms
step:1347/2330 train_time:78955ms step_avg:58.62ms
step:1348/2330 train_time:79016ms step_avg:58.62ms
step:1349/2330 train_time:79073ms step_avg:58.62ms
step:1350/2330 train_time:79135ms step_avg:58.62ms
step:1351/2330 train_time:79191ms step_avg:58.62ms
step:1352/2330 train_time:79254ms step_avg:58.62ms
step:1353/2330 train_time:79310ms step_avg:58.62ms
step:1354/2330 train_time:79372ms step_avg:58.62ms
step:1355/2330 train_time:79429ms step_avg:58.62ms
step:1356/2330 train_time:79490ms step_avg:58.62ms
step:1357/2330 train_time:79548ms step_avg:58.62ms
step:1358/2330 train_time:79608ms step_avg:58.62ms
step:1359/2330 train_time:79666ms step_avg:58.62ms
step:1360/2330 train_time:79727ms step_avg:58.62ms
step:1361/2330 train_time:79784ms step_avg:58.62ms
step:1362/2330 train_time:79845ms step_avg:58.62ms
step:1363/2330 train_time:79902ms step_avg:58.62ms
step:1364/2330 train_time:79964ms step_avg:58.62ms
step:1365/2330 train_time:80021ms step_avg:58.62ms
step:1366/2330 train_time:80082ms step_avg:58.62ms
step:1367/2330 train_time:80138ms step_avg:58.62ms
step:1368/2330 train_time:80200ms step_avg:58.63ms
step:1369/2330 train_time:80256ms step_avg:58.62ms
step:1370/2330 train_time:80318ms step_avg:58.63ms
step:1371/2330 train_time:80374ms step_avg:58.62ms
step:1372/2330 train_time:80437ms step_avg:58.63ms
step:1373/2330 train_time:80493ms step_avg:58.63ms
step:1374/2330 train_time:80556ms step_avg:58.63ms
step:1375/2330 train_time:80613ms step_avg:58.63ms
step:1376/2330 train_time:80675ms step_avg:58.63ms
step:1377/2330 train_time:80732ms step_avg:58.63ms
step:1378/2330 train_time:80795ms step_avg:58.63ms
step:1379/2330 train_time:80852ms step_avg:58.63ms
step:1380/2330 train_time:80914ms step_avg:58.63ms
step:1381/2330 train_time:80971ms step_avg:58.63ms
step:1382/2330 train_time:81032ms step_avg:58.63ms
step:1383/2330 train_time:81089ms step_avg:58.63ms
step:1384/2330 train_time:81150ms step_avg:58.63ms
step:1385/2330 train_time:81208ms step_avg:58.63ms
step:1386/2330 train_time:81268ms step_avg:58.63ms
step:1387/2330 train_time:81325ms step_avg:58.63ms
step:1388/2330 train_time:81386ms step_avg:58.64ms
step:1389/2330 train_time:81443ms step_avg:58.63ms
step:1390/2330 train_time:81504ms step_avg:58.64ms
step:1391/2330 train_time:81561ms step_avg:58.64ms
step:1392/2330 train_time:81622ms step_avg:58.64ms
step:1393/2330 train_time:81678ms step_avg:58.63ms
step:1394/2330 train_time:81741ms step_avg:58.64ms
step:1395/2330 train_time:81797ms step_avg:58.64ms
step:1396/2330 train_time:81860ms step_avg:58.64ms
step:1397/2330 train_time:81916ms step_avg:58.64ms
step:1398/2330 train_time:81979ms step_avg:58.64ms
step:1399/2330 train_time:82035ms step_avg:58.64ms
step:1400/2330 train_time:82097ms step_avg:58.64ms
step:1401/2330 train_time:82153ms step_avg:58.64ms
step:1402/2330 train_time:82217ms step_avg:58.64ms
step:1403/2330 train_time:82273ms step_avg:58.64ms
step:1404/2330 train_time:82335ms step_avg:58.64ms
step:1405/2330 train_time:82391ms step_avg:58.64ms
step:1406/2330 train_time:82454ms step_avg:58.64ms
step:1407/2330 train_time:82511ms step_avg:58.64ms
step:1408/2330 train_time:82572ms step_avg:58.64ms
step:1409/2330 train_time:82629ms step_avg:58.64ms
step:1410/2330 train_time:82691ms step_avg:58.65ms
step:1411/2330 train_time:82748ms step_avg:58.65ms
step:1412/2330 train_time:82809ms step_avg:58.65ms
step:1413/2330 train_time:82866ms step_avg:58.65ms
step:1414/2330 train_time:82927ms step_avg:58.65ms
step:1415/2330 train_time:82984ms step_avg:58.65ms
step:1416/2330 train_time:83045ms step_avg:58.65ms
step:1417/2330 train_time:83101ms step_avg:58.65ms
step:1418/2330 train_time:83163ms step_avg:58.65ms
step:1419/2330 train_time:83219ms step_avg:58.65ms
step:1420/2330 train_time:83281ms step_avg:58.65ms
step:1421/2330 train_time:83337ms step_avg:58.65ms
step:1422/2330 train_time:83399ms step_avg:58.65ms
step:1423/2330 train_time:83456ms step_avg:58.65ms
step:1424/2330 train_time:83517ms step_avg:58.65ms
step:1425/2330 train_time:83573ms step_avg:58.65ms
step:1426/2330 train_time:83636ms step_avg:58.65ms
step:1427/2330 train_time:83692ms step_avg:58.65ms
step:1428/2330 train_time:83755ms step_avg:58.65ms
step:1429/2330 train_time:83811ms step_avg:58.65ms
step:1430/2330 train_time:83875ms step_avg:58.65ms
step:1431/2330 train_time:83932ms step_avg:58.65ms
step:1432/2330 train_time:83993ms step_avg:58.65ms
step:1433/2330 train_time:84051ms step_avg:58.65ms
step:1434/2330 train_time:84111ms step_avg:58.66ms
step:1435/2330 train_time:84169ms step_avg:58.65ms
step:1436/2330 train_time:84230ms step_avg:58.66ms
step:1437/2330 train_time:84287ms step_avg:58.65ms
step:1438/2330 train_time:84347ms step_avg:58.66ms
step:1439/2330 train_time:84405ms step_avg:58.66ms
step:1440/2330 train_time:84465ms step_avg:58.66ms
step:1441/2330 train_time:84523ms step_avg:58.66ms
step:1442/2330 train_time:84584ms step_avg:58.66ms
step:1443/2330 train_time:84640ms step_avg:58.66ms
step:1444/2330 train_time:84701ms step_avg:58.66ms
step:1445/2330 train_time:84758ms step_avg:58.66ms
step:1446/2330 train_time:84820ms step_avg:58.66ms
step:1447/2330 train_time:84876ms step_avg:58.66ms
step:1448/2330 train_time:84939ms step_avg:58.66ms
step:1449/2330 train_time:84995ms step_avg:58.66ms
step:1450/2330 train_time:85057ms step_avg:58.66ms
step:1451/2330 train_time:85113ms step_avg:58.66ms
step:1452/2330 train_time:85176ms step_avg:58.66ms
step:1453/2330 train_time:85233ms step_avg:58.66ms
step:1454/2330 train_time:85295ms step_avg:58.66ms
step:1455/2330 train_time:85352ms step_avg:58.66ms
step:1456/2330 train_time:85413ms step_avg:58.66ms
step:1457/2330 train_time:85470ms step_avg:58.66ms
step:1458/2330 train_time:85531ms step_avg:58.66ms
step:1459/2330 train_time:85589ms step_avg:58.66ms
step:1460/2330 train_time:85650ms step_avg:58.66ms
step:1461/2330 train_time:85707ms step_avg:58.66ms
step:1462/2330 train_time:85767ms step_avg:58.66ms
step:1463/2330 train_time:85825ms step_avg:58.66ms
step:1464/2330 train_time:85886ms step_avg:58.67ms
step:1465/2330 train_time:85943ms step_avg:58.66ms
step:1466/2330 train_time:86004ms step_avg:58.67ms
step:1467/2330 train_time:86061ms step_avg:58.66ms
step:1468/2330 train_time:86122ms step_avg:58.67ms
step:1469/2330 train_time:86178ms step_avg:58.66ms
step:1470/2330 train_time:86242ms step_avg:58.67ms
step:1471/2330 train_time:86298ms step_avg:58.67ms
step:1472/2330 train_time:86360ms step_avg:58.67ms
step:1473/2330 train_time:86416ms step_avg:58.67ms
step:1474/2330 train_time:86479ms step_avg:58.67ms
step:1475/2330 train_time:86535ms step_avg:58.67ms
step:1476/2330 train_time:86598ms step_avg:58.67ms
step:1477/2330 train_time:86655ms step_avg:58.67ms
step:1478/2330 train_time:86717ms step_avg:58.67ms
step:1479/2330 train_time:86773ms step_avg:58.67ms
step:1480/2330 train_time:86836ms step_avg:58.67ms
step:1481/2330 train_time:86892ms step_avg:58.67ms
step:1482/2330 train_time:86955ms step_avg:58.67ms
step:1483/2330 train_time:87011ms step_avg:58.67ms
step:1484/2330 train_time:87072ms step_avg:58.67ms
step:1485/2330 train_time:87130ms step_avg:58.67ms
step:1486/2330 train_time:87190ms step_avg:58.67ms
step:1487/2330 train_time:87248ms step_avg:58.67ms
step:1488/2330 train_time:87308ms step_avg:58.67ms
step:1489/2330 train_time:87365ms step_avg:58.67ms
step:1490/2330 train_time:87425ms step_avg:58.67ms
step:1491/2330 train_time:87482ms step_avg:58.67ms
step:1492/2330 train_time:87544ms step_avg:58.68ms
step:1493/2330 train_time:87601ms step_avg:58.67ms
step:1494/2330 train_time:87662ms step_avg:58.68ms
step:1495/2330 train_time:87718ms step_avg:58.67ms
step:1496/2330 train_time:87781ms step_avg:58.68ms
step:1497/2330 train_time:87837ms step_avg:58.68ms
step:1498/2330 train_time:87900ms step_avg:58.68ms
step:1499/2330 train_time:87956ms step_avg:58.68ms
step:1500/2330 train_time:88018ms step_avg:58.68ms
step:1500/2330 val_loss:4.4155 train_time:88097ms step_avg:58.73ms
step:1501/2330 train_time:88115ms step_avg:58.70ms
step:1502/2330 train_time:88140ms step_avg:58.68ms
step:1503/2330 train_time:88199ms step_avg:58.68ms
step:1504/2330 train_time:88262ms step_avg:58.68ms
step:1505/2330 train_time:88320ms step_avg:58.68ms
step:1506/2330 train_time:88380ms step_avg:58.69ms
step:1507/2330 train_time:88437ms step_avg:58.68ms
step:1508/2330 train_time:88498ms step_avg:58.69ms
step:1509/2330 train_time:88555ms step_avg:58.68ms
step:1510/2330 train_time:88615ms step_avg:58.69ms
step:1511/2330 train_time:88672ms step_avg:58.68ms
step:1512/2330 train_time:88733ms step_avg:58.69ms
step:1513/2330 train_time:88789ms step_avg:58.68ms
step:1514/2330 train_time:88849ms step_avg:58.69ms
step:1515/2330 train_time:88905ms step_avg:58.68ms
step:1516/2330 train_time:88966ms step_avg:58.68ms
step:1517/2330 train_time:89023ms step_avg:58.68ms
step:1518/2330 train_time:89084ms step_avg:58.69ms
step:1519/2330 train_time:89141ms step_avg:58.68ms
step:1520/2330 train_time:89204ms step_avg:58.69ms
step:1521/2330 train_time:89261ms step_avg:58.69ms
step:1522/2330 train_time:89326ms step_avg:58.69ms
step:1523/2330 train_time:89383ms step_avg:58.69ms
step:1524/2330 train_time:89444ms step_avg:58.69ms
step:1525/2330 train_time:89501ms step_avg:58.69ms
step:1526/2330 train_time:89561ms step_avg:58.69ms
step:1527/2330 train_time:89618ms step_avg:58.69ms
step:1528/2330 train_time:89679ms step_avg:58.69ms
step:1529/2330 train_time:89738ms step_avg:58.69ms
step:1530/2330 train_time:89798ms step_avg:58.69ms
step:1531/2330 train_time:89854ms step_avg:58.69ms
step:1532/2330 train_time:89915ms step_avg:58.69ms
step:1533/2330 train_time:89972ms step_avg:58.69ms
step:1534/2330 train_time:90034ms step_avg:58.69ms
step:1535/2330 train_time:90090ms step_avg:58.69ms
step:1536/2330 train_time:90153ms step_avg:58.69ms
step:1537/2330 train_time:90209ms step_avg:58.69ms
step:1538/2330 train_time:90274ms step_avg:58.70ms
step:1539/2330 train_time:90330ms step_avg:58.69ms
step:1540/2330 train_time:90395ms step_avg:58.70ms
step:1541/2330 train_time:90451ms step_avg:58.70ms
step:1542/2330 train_time:90515ms step_avg:58.70ms
step:1543/2330 train_time:90572ms step_avg:58.70ms
step:1544/2330 train_time:90635ms step_avg:58.70ms
step:1545/2330 train_time:90691ms step_avg:58.70ms
step:1546/2330 train_time:90754ms step_avg:58.70ms
step:1547/2330 train_time:90810ms step_avg:58.70ms
step:1548/2330 train_time:90872ms step_avg:58.70ms
step:1549/2330 train_time:90928ms step_avg:58.70ms
step:1550/2330 train_time:90990ms step_avg:58.70ms
step:1551/2330 train_time:91047ms step_avg:58.70ms
step:1552/2330 train_time:91110ms step_avg:58.70ms
step:1553/2330 train_time:91166ms step_avg:58.70ms
step:1554/2330 train_time:91229ms step_avg:58.71ms
step:1555/2330 train_time:91286ms step_avg:58.70ms
step:1556/2330 train_time:91350ms step_avg:58.71ms
step:1557/2330 train_time:91406ms step_avg:58.71ms
step:1558/2330 train_time:91470ms step_avg:58.71ms
step:1559/2330 train_time:91527ms step_avg:58.71ms
step:1560/2330 train_time:91590ms step_avg:58.71ms
step:1561/2330 train_time:91647ms step_avg:58.71ms
step:1562/2330 train_time:91709ms step_avg:58.71ms
step:1563/2330 train_time:91766ms step_avg:58.71ms
step:1564/2330 train_time:91828ms step_avg:58.71ms
step:1565/2330 train_time:91885ms step_avg:58.71ms
step:1566/2330 train_time:91946ms step_avg:58.71ms
step:1567/2330 train_time:92003ms step_avg:58.71ms
step:1568/2330 train_time:92065ms step_avg:58.71ms
step:1569/2330 train_time:92122ms step_avg:58.71ms
step:1570/2330 train_time:92183ms step_avg:58.72ms
step:1571/2330 train_time:92241ms step_avg:58.71ms
step:1572/2330 train_time:92302ms step_avg:58.72ms
step:1573/2330 train_time:92361ms step_avg:58.72ms
step:1574/2330 train_time:92421ms step_avg:58.72ms
step:1575/2330 train_time:92479ms step_avg:58.72ms
step:1576/2330 train_time:92542ms step_avg:58.72ms
step:1577/2330 train_time:92600ms step_avg:58.72ms
step:1578/2330 train_time:92661ms step_avg:58.72ms
step:1579/2330 train_time:92719ms step_avg:58.72ms
step:1580/2330 train_time:92779ms step_avg:58.72ms
step:1581/2330 train_time:92837ms step_avg:58.72ms
step:1582/2330 train_time:92898ms step_avg:58.72ms
step:1583/2330 train_time:92955ms step_avg:58.72ms
step:1584/2330 train_time:93016ms step_avg:58.72ms
step:1585/2330 train_time:93073ms step_avg:58.72ms
step:1586/2330 train_time:93134ms step_avg:58.72ms
step:1587/2330 train_time:93190ms step_avg:58.72ms
step:1588/2330 train_time:93255ms step_avg:58.72ms
step:1589/2330 train_time:93311ms step_avg:58.72ms
step:1590/2330 train_time:93374ms step_avg:58.73ms
step:1591/2330 train_time:93431ms step_avg:58.72ms
step:1592/2330 train_time:93495ms step_avg:58.73ms
step:1593/2330 train_time:93551ms step_avg:58.73ms
step:1594/2330 train_time:93615ms step_avg:58.73ms
step:1595/2330 train_time:93672ms step_avg:58.73ms
step:1596/2330 train_time:93735ms step_avg:58.73ms
step:1597/2330 train_time:93791ms step_avg:58.73ms
step:1598/2330 train_time:93854ms step_avg:58.73ms
step:1599/2330 train_time:93910ms step_avg:58.73ms
step:1600/2330 train_time:93973ms step_avg:58.73ms
step:1601/2330 train_time:94030ms step_avg:58.73ms
step:1602/2330 train_time:94092ms step_avg:58.73ms
step:1603/2330 train_time:94148ms step_avg:58.73ms
step:1604/2330 train_time:94211ms step_avg:58.74ms
step:1605/2330 train_time:94268ms step_avg:58.73ms
step:1606/2330 train_time:94330ms step_avg:58.74ms
step:1607/2330 train_time:94387ms step_avg:58.74ms
step:1608/2330 train_time:94450ms step_avg:58.74ms
step:1609/2330 train_time:94507ms step_avg:58.74ms
step:1610/2330 train_time:94572ms step_avg:58.74ms
step:1611/2330 train_time:94629ms step_avg:58.74ms
step:1612/2330 train_time:94691ms step_avg:58.74ms
step:1613/2330 train_time:94748ms step_avg:58.74ms
step:1614/2330 train_time:94810ms step_avg:58.74ms
step:1615/2330 train_time:94868ms step_avg:58.74ms
step:1616/2330 train_time:94930ms step_avg:58.74ms
step:1617/2330 train_time:94987ms step_avg:58.74ms
step:1618/2330 train_time:95049ms step_avg:58.75ms
step:1619/2330 train_time:95106ms step_avg:58.74ms
step:1620/2330 train_time:95169ms step_avg:58.75ms
step:1621/2330 train_time:95225ms step_avg:58.74ms
step:1622/2330 train_time:95288ms step_avg:58.75ms
step:1623/2330 train_time:95345ms step_avg:58.75ms
step:1624/2330 train_time:95409ms step_avg:58.75ms
step:1625/2330 train_time:95466ms step_avg:58.75ms
step:1626/2330 train_time:95529ms step_avg:58.75ms
step:1627/2330 train_time:95586ms step_avg:58.75ms
step:1628/2330 train_time:95649ms step_avg:58.75ms
step:1629/2330 train_time:95706ms step_avg:58.75ms
step:1630/2330 train_time:95769ms step_avg:58.75ms
step:1631/2330 train_time:95826ms step_avg:58.75ms
step:1632/2330 train_time:95888ms step_avg:58.76ms
step:1633/2330 train_time:95945ms step_avg:58.75ms
step:1634/2330 train_time:96007ms step_avg:58.76ms
step:1635/2330 train_time:96064ms step_avg:58.75ms
step:1636/2330 train_time:96126ms step_avg:58.76ms
step:1637/2330 train_time:96183ms step_avg:58.76ms
step:1638/2330 train_time:96244ms step_avg:58.76ms
step:1639/2330 train_time:96301ms step_avg:58.76ms
step:1640/2330 train_time:96362ms step_avg:58.76ms
step:1641/2330 train_time:96419ms step_avg:58.76ms
step:1642/2330 train_time:96481ms step_avg:58.76ms
step:1643/2330 train_time:96539ms step_avg:58.76ms
step:1644/2330 train_time:96601ms step_avg:58.76ms
step:1645/2330 train_time:96658ms step_avg:58.76ms
step:1646/2330 train_time:96720ms step_avg:58.76ms
step:1647/2330 train_time:96777ms step_avg:58.76ms
step:1648/2330 train_time:96838ms step_avg:58.76ms
step:1649/2330 train_time:96896ms step_avg:58.76ms
step:1650/2330 train_time:96957ms step_avg:58.76ms
step:1651/2330 train_time:97014ms step_avg:58.76ms
step:1652/2330 train_time:97077ms step_avg:58.76ms
step:1653/2330 train_time:97133ms step_avg:58.76ms
step:1654/2330 train_time:97195ms step_avg:58.76ms
step:1655/2330 train_time:97252ms step_avg:58.76ms
step:1656/2330 train_time:97314ms step_avg:58.76ms
step:1657/2330 train_time:97370ms step_avg:58.76ms
step:1658/2330 train_time:97434ms step_avg:58.77ms
step:1659/2330 train_time:97491ms step_avg:58.76ms
step:1660/2330 train_time:97552ms step_avg:58.77ms
step:1661/2330 train_time:97609ms step_avg:58.77ms
step:1662/2330 train_time:97673ms step_avg:58.77ms
step:1663/2330 train_time:97729ms step_avg:58.77ms
step:1664/2330 train_time:97791ms step_avg:58.77ms
step:1665/2330 train_time:97848ms step_avg:58.77ms
step:1666/2330 train_time:97910ms step_avg:58.77ms
step:1667/2330 train_time:97967ms step_avg:58.77ms
step:1668/2330 train_time:98029ms step_avg:58.77ms
step:1669/2330 train_time:98087ms step_avg:58.77ms
step:1670/2330 train_time:98148ms step_avg:58.77ms
step:1671/2330 train_time:98205ms step_avg:58.77ms
step:1672/2330 train_time:98268ms step_avg:58.77ms
step:1673/2330 train_time:98325ms step_avg:58.77ms
step:1674/2330 train_time:98389ms step_avg:58.77ms
step:1675/2330 train_time:98446ms step_avg:58.77ms
step:1676/2330 train_time:98508ms step_avg:58.78ms
step:1677/2330 train_time:98565ms step_avg:58.77ms
step:1678/2330 train_time:98627ms step_avg:58.78ms
step:1679/2330 train_time:98685ms step_avg:58.78ms
step:1680/2330 train_time:98747ms step_avg:58.78ms
step:1681/2330 train_time:98805ms step_avg:58.78ms
step:1682/2330 train_time:98865ms step_avg:58.78ms
step:1683/2330 train_time:98923ms step_avg:58.78ms
step:1684/2330 train_time:98985ms step_avg:58.78ms
step:1685/2330 train_time:99042ms step_avg:58.78ms
step:1686/2330 train_time:99104ms step_avg:58.78ms
step:1687/2330 train_time:99161ms step_avg:58.78ms
step:1688/2330 train_time:99223ms step_avg:58.78ms
step:1689/2330 train_time:99280ms step_avg:58.78ms
step:1690/2330 train_time:99342ms step_avg:58.78ms
step:1691/2330 train_time:99400ms step_avg:58.78ms
step:1692/2330 train_time:99461ms step_avg:58.78ms
step:1693/2330 train_time:99518ms step_avg:58.78ms
step:1694/2330 train_time:99580ms step_avg:58.78ms
step:1695/2330 train_time:99638ms step_avg:58.78ms
step:1696/2330 train_time:99700ms step_avg:58.79ms
step:1697/2330 train_time:99757ms step_avg:58.78ms
step:1698/2330 train_time:99819ms step_avg:58.79ms
step:1699/2330 train_time:99877ms step_avg:58.79ms
step:1700/2330 train_time:99938ms step_avg:58.79ms
step:1701/2330 train_time:99994ms step_avg:58.79ms
step:1702/2330 train_time:100056ms step_avg:58.79ms
step:1703/2330 train_time:100113ms step_avg:58.79ms
step:1704/2330 train_time:100175ms step_avg:58.79ms
step:1705/2330 train_time:100231ms step_avg:58.79ms
step:1706/2330 train_time:100294ms step_avg:58.79ms
step:1707/2330 train_time:100351ms step_avg:58.79ms
step:1708/2330 train_time:100414ms step_avg:58.79ms
step:1709/2330 train_time:100471ms step_avg:58.79ms
step:1710/2330 train_time:100534ms step_avg:58.79ms
step:1711/2330 train_time:100590ms step_avg:58.79ms
step:1712/2330 train_time:100653ms step_avg:58.79ms
step:1713/2330 train_time:100709ms step_avg:58.79ms
step:1714/2330 train_time:100772ms step_avg:58.79ms
step:1715/2330 train_time:100829ms step_avg:58.79ms
step:1716/2330 train_time:100892ms step_avg:58.80ms
step:1717/2330 train_time:100949ms step_avg:58.79ms
step:1718/2330 train_time:101012ms step_avg:58.80ms
step:1719/2330 train_time:101069ms step_avg:58.80ms
step:1720/2330 train_time:101131ms step_avg:58.80ms
step:1721/2330 train_time:101188ms step_avg:58.80ms
step:1722/2330 train_time:101251ms step_avg:58.80ms
step:1723/2330 train_time:101307ms step_avg:58.80ms
step:1724/2330 train_time:101371ms step_avg:58.80ms
step:1725/2330 train_time:101427ms step_avg:58.80ms
step:1726/2330 train_time:101491ms step_avg:58.80ms
step:1727/2330 train_time:101548ms step_avg:58.80ms
step:1728/2330 train_time:101610ms step_avg:58.80ms
step:1729/2330 train_time:101667ms step_avg:58.80ms
step:1730/2330 train_time:101729ms step_avg:58.80ms
step:1731/2330 train_time:101787ms step_avg:58.80ms
step:1732/2330 train_time:101849ms step_avg:58.80ms
step:1733/2330 train_time:101905ms step_avg:58.80ms
step:1734/2330 train_time:101968ms step_avg:58.81ms
step:1735/2330 train_time:102025ms step_avg:58.80ms
step:1736/2330 train_time:102088ms step_avg:58.81ms
step:1737/2330 train_time:102145ms step_avg:58.81ms
step:1738/2330 train_time:102208ms step_avg:58.81ms
step:1739/2330 train_time:102264ms step_avg:58.81ms
step:1740/2330 train_time:102327ms step_avg:58.81ms
step:1741/2330 train_time:102385ms step_avg:58.81ms
step:1742/2330 train_time:102447ms step_avg:58.81ms
step:1743/2330 train_time:102504ms step_avg:58.81ms
step:1744/2330 train_time:102567ms step_avg:58.81ms
step:1745/2330 train_time:102625ms step_avg:58.81ms
step:1746/2330 train_time:102687ms step_avg:58.81ms
step:1747/2330 train_time:102744ms step_avg:58.81ms
step:1748/2330 train_time:102806ms step_avg:58.81ms
step:1749/2330 train_time:102863ms step_avg:58.81ms
step:1750/2330 train_time:102924ms step_avg:58.81ms
step:1750/2330 val_loss:4.3013 train_time:103003ms step_avg:58.86ms
step:1751/2330 train_time:103022ms step_avg:58.84ms
step:1752/2330 train_time:103045ms step_avg:58.82ms
step:1753/2330 train_time:103102ms step_avg:58.81ms
step:1754/2330 train_time:103169ms step_avg:58.82ms
step:1755/2330 train_time:103225ms step_avg:58.82ms
step:1756/2330 train_time:103289ms step_avg:58.82ms
step:1757/2330 train_time:103346ms step_avg:58.82ms
step:1758/2330 train_time:103407ms step_avg:58.82ms
step:1759/2330 train_time:103464ms step_avg:58.82ms
step:1760/2330 train_time:103525ms step_avg:58.82ms
step:1761/2330 train_time:103581ms step_avg:58.82ms
step:1762/2330 train_time:103642ms step_avg:58.82ms
step:1763/2330 train_time:103699ms step_avg:58.82ms
step:1764/2330 train_time:103761ms step_avg:58.82ms
step:1765/2330 train_time:103817ms step_avg:58.82ms
step:1766/2330 train_time:103878ms step_avg:58.82ms
step:1767/2330 train_time:103936ms step_avg:58.82ms
step:1768/2330 train_time:103999ms step_avg:58.82ms
step:1769/2330 train_time:104057ms step_avg:58.82ms
step:1770/2330 train_time:104123ms step_avg:58.83ms
step:1771/2330 train_time:104179ms step_avg:58.83ms
step:1772/2330 train_time:104243ms step_avg:58.83ms
step:1773/2330 train_time:104299ms step_avg:58.83ms
step:1774/2330 train_time:104362ms step_avg:58.83ms
step:1775/2330 train_time:104418ms step_avg:58.83ms
step:1776/2330 train_time:104481ms step_avg:58.83ms
step:1777/2330 train_time:104538ms step_avg:58.83ms
step:1778/2330 train_time:104600ms step_avg:58.83ms
step:1779/2330 train_time:104657ms step_avg:58.83ms
step:1780/2330 train_time:104718ms step_avg:58.83ms
step:1781/2330 train_time:104774ms step_avg:58.83ms
step:1782/2330 train_time:104836ms step_avg:58.83ms
step:1783/2330 train_time:104893ms step_avg:58.83ms
step:1784/2330 train_time:104955ms step_avg:58.83ms
step:1785/2330 train_time:105011ms step_avg:58.83ms
step:1786/2330 train_time:105075ms step_avg:58.83ms
step:1787/2330 train_time:105132ms step_avg:58.83ms
step:1788/2330 train_time:105196ms step_avg:58.83ms
step:1789/2330 train_time:105253ms step_avg:58.83ms
step:1790/2330 train_time:105317ms step_avg:58.84ms
step:1791/2330 train_time:105374ms step_avg:58.84ms
step:1792/2330 train_time:105437ms step_avg:58.84ms
step:1793/2330 train_time:105494ms step_avg:58.84ms
step:1794/2330 train_time:105556ms step_avg:58.84ms
step:1795/2330 train_time:105612ms step_avg:58.84ms
step:1796/2330 train_time:105673ms step_avg:58.84ms
step:1797/2330 train_time:105730ms step_avg:58.84ms
step:1798/2330 train_time:105792ms step_avg:58.84ms
step:1799/2330 train_time:105849ms step_avg:58.84ms
step:1800/2330 train_time:105911ms step_avg:58.84ms
step:1801/2330 train_time:105969ms step_avg:58.84ms
step:1802/2330 train_time:106030ms step_avg:58.84ms
step:1803/2330 train_time:106087ms step_avg:58.84ms
step:1804/2330 train_time:106149ms step_avg:58.84ms
step:1805/2330 train_time:106206ms step_avg:58.84ms
step:1806/2330 train_time:106269ms step_avg:58.84ms
step:1807/2330 train_time:106326ms step_avg:58.84ms
step:1808/2330 train_time:106389ms step_avg:58.84ms
step:1809/2330 train_time:106446ms step_avg:58.84ms
step:1810/2330 train_time:106507ms step_avg:58.84ms
step:1811/2330 train_time:106564ms step_avg:58.84ms
step:1812/2330 train_time:106625ms step_avg:58.84ms
step:1813/2330 train_time:106682ms step_avg:58.84ms
step:1814/2330 train_time:106744ms step_avg:58.84ms
step:1815/2330 train_time:106800ms step_avg:58.84ms
step:1816/2330 train_time:106863ms step_avg:58.85ms
step:1817/2330 train_time:106919ms step_avg:58.84ms
step:1818/2330 train_time:106982ms step_avg:58.85ms
step:1819/2330 train_time:107039ms step_avg:58.84ms
step:1820/2330 train_time:107100ms step_avg:58.85ms
step:1821/2330 train_time:107157ms step_avg:58.85ms
step:1822/2330 train_time:107220ms step_avg:58.85ms
step:1823/2330 train_time:107278ms step_avg:58.85ms
step:1824/2330 train_time:107340ms step_avg:58.85ms
step:1825/2330 train_time:107396ms step_avg:58.85ms
step:1826/2330 train_time:107458ms step_avg:58.85ms
step:1827/2330 train_time:107515ms step_avg:58.85ms
step:1828/2330 train_time:107578ms step_avg:58.85ms
step:1829/2330 train_time:107634ms step_avg:58.85ms
step:1830/2330 train_time:107697ms step_avg:58.85ms
step:1831/2330 train_time:107754ms step_avg:58.85ms
step:1832/2330 train_time:107816ms step_avg:58.85ms
step:1833/2330 train_time:107873ms step_avg:58.85ms
step:1834/2330 train_time:107935ms step_avg:58.85ms
step:1835/2330 train_time:107991ms step_avg:58.85ms
step:1836/2330 train_time:108055ms step_avg:58.85ms
step:1837/2330 train_time:108111ms step_avg:58.85ms
step:1838/2330 train_time:108174ms step_avg:58.85ms
step:1839/2330 train_time:108231ms step_avg:58.85ms
step:1840/2330 train_time:108294ms step_avg:58.86ms
step:1841/2330 train_time:108352ms step_avg:58.85ms
step:1842/2330 train_time:108414ms step_avg:58.86ms
step:1843/2330 train_time:108471ms step_avg:58.86ms
step:1844/2330 train_time:108533ms step_avg:58.86ms
step:1845/2330 train_time:108591ms step_avg:58.86ms
step:1846/2330 train_time:108653ms step_avg:58.86ms
step:1847/2330 train_time:108710ms step_avg:58.86ms
step:1848/2330 train_time:108772ms step_avg:58.86ms
step:1849/2330 train_time:108829ms step_avg:58.86ms
step:1850/2330 train_time:108892ms step_avg:58.86ms
step:1851/2330 train_time:108949ms step_avg:58.86ms
step:1852/2330 train_time:109011ms step_avg:58.86ms
step:1853/2330 train_time:109068ms step_avg:58.86ms
step:1854/2330 train_time:109131ms step_avg:58.86ms
step:1855/2330 train_time:109188ms step_avg:58.86ms
step:1856/2330 train_time:109250ms step_avg:58.86ms
step:1857/2330 train_time:109308ms step_avg:58.86ms
step:1858/2330 train_time:109369ms step_avg:58.86ms
step:1859/2330 train_time:109426ms step_avg:58.86ms
step:1860/2330 train_time:109488ms step_avg:58.86ms
step:1861/2330 train_time:109546ms step_avg:58.86ms
step:1862/2330 train_time:109607ms step_avg:58.87ms
step:1863/2330 train_time:109665ms step_avg:58.86ms
step:1864/2330 train_time:109727ms step_avg:58.87ms
step:1865/2330 train_time:109784ms step_avg:58.87ms
step:1866/2330 train_time:109845ms step_avg:58.87ms
step:1867/2330 train_time:109903ms step_avg:58.87ms
step:1868/2330 train_time:109964ms step_avg:58.87ms
step:1869/2330 train_time:110022ms step_avg:58.87ms
step:1870/2330 train_time:110082ms step_avg:58.87ms
step:1871/2330 train_time:110139ms step_avg:58.87ms
step:1872/2330 train_time:110201ms step_avg:58.87ms
step:1873/2330 train_time:110259ms step_avg:58.87ms
step:1874/2330 train_time:110321ms step_avg:58.87ms
step:1875/2330 train_time:110377ms step_avg:58.87ms
step:1876/2330 train_time:110440ms step_avg:58.87ms
step:1877/2330 train_time:110496ms step_avg:58.87ms
step:1878/2330 train_time:110559ms step_avg:58.87ms
step:1879/2330 train_time:110616ms step_avg:58.87ms
step:1880/2330 train_time:110679ms step_avg:58.87ms
step:1881/2330 train_time:110735ms step_avg:58.87ms
step:1882/2330 train_time:110799ms step_avg:58.87ms
step:1883/2330 train_time:110855ms step_avg:58.87ms
step:1884/2330 train_time:110919ms step_avg:58.87ms
step:1885/2330 train_time:110975ms step_avg:58.87ms
step:1886/2330 train_time:111038ms step_avg:58.87ms
step:1887/2330 train_time:111095ms step_avg:58.87ms
step:1888/2330 train_time:111157ms step_avg:58.88ms
step:1889/2330 train_time:111213ms step_avg:58.87ms
step:1890/2330 train_time:111276ms step_avg:58.88ms
step:1891/2330 train_time:111333ms step_avg:58.88ms
step:1892/2330 train_time:111395ms step_avg:58.88ms
step:1893/2330 train_time:111452ms step_avg:58.88ms
step:1894/2330 train_time:111515ms step_avg:58.88ms
step:1895/2330 train_time:111571ms step_avg:58.88ms
step:1896/2330 train_time:111634ms step_avg:58.88ms
step:1897/2330 train_time:111692ms step_avg:58.88ms
step:1898/2330 train_time:111753ms step_avg:58.88ms
step:1899/2330 train_time:111811ms step_avg:58.88ms
step:1900/2330 train_time:111872ms step_avg:58.88ms
step:1901/2330 train_time:111929ms step_avg:58.88ms
step:1902/2330 train_time:111992ms step_avg:58.88ms
step:1903/2330 train_time:112050ms step_avg:58.88ms
step:1904/2330 train_time:112111ms step_avg:58.88ms
step:1905/2330 train_time:112168ms step_avg:58.88ms
step:1906/2330 train_time:112230ms step_avg:58.88ms
step:1907/2330 train_time:112287ms step_avg:58.88ms
step:1908/2330 train_time:112348ms step_avg:58.88ms
step:1909/2330 train_time:112405ms step_avg:58.88ms
step:1910/2330 train_time:112466ms step_avg:58.88ms
step:1911/2330 train_time:112524ms step_avg:58.88ms
step:1912/2330 train_time:112585ms step_avg:58.88ms
step:1913/2330 train_time:112643ms step_avg:58.88ms
step:1914/2330 train_time:112704ms step_avg:58.88ms
step:1915/2330 train_time:112762ms step_avg:58.88ms
step:1916/2330 train_time:112823ms step_avg:58.88ms
step:1917/2330 train_time:112881ms step_avg:58.88ms
step:1918/2330 train_time:112943ms step_avg:58.89ms
step:1919/2330 train_time:113000ms step_avg:58.88ms
step:1920/2330 train_time:113062ms step_avg:58.89ms
step:1921/2330 train_time:113119ms step_avg:58.89ms
step:1922/2330 train_time:113181ms step_avg:58.89ms
step:1923/2330 train_time:113238ms step_avg:58.89ms
step:1924/2330 train_time:113301ms step_avg:58.89ms
step:1925/2330 train_time:113358ms step_avg:58.89ms
step:1926/2330 train_time:113420ms step_avg:58.89ms
step:1927/2330 train_time:113477ms step_avg:58.89ms
step:1928/2330 train_time:113540ms step_avg:58.89ms
step:1929/2330 train_time:113596ms step_avg:58.89ms
step:1930/2330 train_time:113659ms step_avg:58.89ms
step:1931/2330 train_time:113716ms step_avg:58.89ms
step:1932/2330 train_time:113778ms step_avg:58.89ms
step:1933/2330 train_time:113835ms step_avg:58.89ms
step:1934/2330 train_time:113898ms step_avg:58.89ms
step:1935/2330 train_time:113955ms step_avg:58.89ms
step:1936/2330 train_time:114018ms step_avg:58.89ms
step:1937/2330 train_time:114074ms step_avg:58.89ms
step:1938/2330 train_time:114137ms step_avg:58.89ms
step:1939/2330 train_time:114193ms step_avg:58.89ms
step:1940/2330 train_time:114257ms step_avg:58.90ms
step:1941/2330 train_time:114313ms step_avg:58.89ms
step:1942/2330 train_time:114376ms step_avg:58.90ms
step:1943/2330 train_time:114433ms step_avg:58.89ms
step:1944/2330 train_time:114496ms step_avg:58.90ms
step:1945/2330 train_time:114553ms step_avg:58.90ms
step:1946/2330 train_time:114616ms step_avg:58.90ms
step:1947/2330 train_time:114673ms step_avg:58.90ms
step:1948/2330 train_time:114735ms step_avg:58.90ms
step:1949/2330 train_time:114792ms step_avg:58.90ms
step:1950/2330 train_time:114853ms step_avg:58.90ms
step:1951/2330 train_time:114910ms step_avg:58.90ms
step:1952/2330 train_time:114973ms step_avg:58.90ms
step:1953/2330 train_time:115029ms step_avg:58.90ms
step:1954/2330 train_time:115093ms step_avg:58.90ms
step:1955/2330 train_time:115150ms step_avg:58.90ms
step:1956/2330 train_time:115213ms step_avg:58.90ms
step:1957/2330 train_time:115270ms step_avg:58.90ms
step:1958/2330 train_time:115333ms step_avg:58.90ms
step:1959/2330 train_time:115390ms step_avg:58.90ms
step:1960/2330 train_time:115452ms step_avg:58.90ms
step:1961/2330 train_time:115509ms step_avg:58.90ms
step:1962/2330 train_time:115573ms step_avg:58.91ms
step:1963/2330 train_time:115630ms step_avg:58.90ms
step:1964/2330 train_time:115692ms step_avg:58.91ms
step:1965/2330 train_time:115749ms step_avg:58.91ms
step:1966/2330 train_time:115811ms step_avg:58.91ms
step:1967/2330 train_time:115869ms step_avg:58.91ms
step:1968/2330 train_time:115931ms step_avg:58.91ms
step:1969/2330 train_time:115988ms step_avg:58.91ms
step:1970/2330 train_time:116050ms step_avg:58.91ms
step:1971/2330 train_time:116107ms step_avg:58.91ms
step:1972/2330 train_time:116168ms step_avg:58.91ms
step:1973/2330 train_time:116226ms step_avg:58.91ms
step:1974/2330 train_time:116287ms step_avg:58.91ms
step:1975/2330 train_time:116344ms step_avg:58.91ms
step:1976/2330 train_time:116405ms step_avg:58.91ms
step:1977/2330 train_time:116463ms step_avg:58.91ms
step:1978/2330 train_time:116524ms step_avg:58.91ms
step:1979/2330 train_time:116582ms step_avg:58.91ms
step:1980/2330 train_time:116643ms step_avg:58.91ms
step:1981/2330 train_time:116700ms step_avg:58.91ms
step:1982/2330 train_time:116762ms step_avg:58.91ms
step:1983/2330 train_time:116819ms step_avg:58.91ms
step:1984/2330 train_time:116880ms step_avg:58.91ms
step:1985/2330 train_time:116937ms step_avg:58.91ms
step:1986/2330 train_time:117000ms step_avg:58.91ms
step:1987/2330 train_time:117057ms step_avg:58.91ms
step:1988/2330 train_time:117120ms step_avg:58.91ms
step:1989/2330 train_time:117177ms step_avg:58.91ms
step:1990/2330 train_time:117239ms step_avg:58.91ms
step:1991/2330 train_time:117295ms step_avg:58.91ms
step:1992/2330 train_time:117359ms step_avg:58.92ms
step:1993/2330 train_time:117416ms step_avg:58.91ms
step:1994/2330 train_time:117478ms step_avg:58.92ms
step:1995/2330 train_time:117534ms step_avg:58.91ms
step:1996/2330 train_time:117597ms step_avg:58.92ms
step:1997/2330 train_time:117653ms step_avg:58.91ms
step:1998/2330 train_time:117717ms step_avg:58.92ms
step:1999/2330 train_time:117773ms step_avg:58.92ms
step:2000/2330 train_time:117836ms step_avg:58.92ms
step:2000/2330 val_loss:4.2137 train_time:117916ms step_avg:58.96ms
step:2001/2330 train_time:117934ms step_avg:58.94ms
step:2002/2330 train_time:117959ms step_avg:58.92ms
step:2003/2330 train_time:118019ms step_avg:58.92ms
step:2004/2330 train_time:118082ms step_avg:58.92ms
step:2005/2330 train_time:118140ms step_avg:58.92ms
step:2006/2330 train_time:118201ms step_avg:58.92ms
step:2007/2330 train_time:118258ms step_avg:58.92ms
step:2008/2330 train_time:118319ms step_avg:58.92ms
step:2009/2330 train_time:118376ms step_avg:58.92ms
step:2010/2330 train_time:118438ms step_avg:58.92ms
step:2011/2330 train_time:118494ms step_avg:58.92ms
step:2012/2330 train_time:118555ms step_avg:58.92ms
step:2013/2330 train_time:118612ms step_avg:58.92ms
step:2014/2330 train_time:118673ms step_avg:58.92ms
step:2015/2330 train_time:118730ms step_avg:58.92ms
step:2016/2330 train_time:118790ms step_avg:58.92ms
step:2017/2330 train_time:118847ms step_avg:58.92ms
step:2018/2330 train_time:118909ms step_avg:58.92ms
step:2019/2330 train_time:118967ms step_avg:58.92ms
step:2020/2330 train_time:119029ms step_avg:58.93ms
step:2021/2330 train_time:119087ms step_avg:58.92ms
step:2022/2330 train_time:119151ms step_avg:58.93ms
step:2023/2330 train_time:119208ms step_avg:58.93ms
step:2024/2330 train_time:119271ms step_avg:58.93ms
step:2025/2330 train_time:119328ms step_avg:58.93ms
step:2026/2330 train_time:119391ms step_avg:58.93ms
step:2027/2330 train_time:119448ms step_avg:58.93ms
step:2028/2330 train_time:119510ms step_avg:58.93ms
step:2029/2330 train_time:119567ms step_avg:58.93ms
step:2030/2330 train_time:119628ms step_avg:58.93ms
step:2031/2330 train_time:119685ms step_avg:58.93ms
step:2032/2330 train_time:119746ms step_avg:58.93ms
step:2033/2330 train_time:119802ms step_avg:58.93ms
step:2034/2330 train_time:119864ms step_avg:58.93ms
step:2035/2330 train_time:119921ms step_avg:58.93ms
step:2036/2330 train_time:119984ms step_avg:58.93ms
step:2037/2330 train_time:120041ms step_avg:58.93ms
step:2038/2330 train_time:120104ms step_avg:58.93ms
step:2039/2330 train_time:120162ms step_avg:58.93ms
step:2040/2330 train_time:120224ms step_avg:58.93ms
step:2041/2330 train_time:120282ms step_avg:58.93ms
step:2042/2330 train_time:120344ms step_avg:58.93ms
step:2043/2330 train_time:120402ms step_avg:58.93ms
step:2044/2330 train_time:120465ms step_avg:58.94ms
step:2045/2330 train_time:120522ms step_avg:58.93ms
step:2046/2330 train_time:120584ms step_avg:58.94ms
step:2047/2330 train_time:120641ms step_avg:58.94ms
step:2048/2330 train_time:120703ms step_avg:58.94ms
step:2049/2330 train_time:120760ms step_avg:58.94ms
step:2050/2330 train_time:120822ms step_avg:58.94ms
step:2051/2330 train_time:120879ms step_avg:58.94ms
step:2052/2330 train_time:120941ms step_avg:58.94ms
step:2053/2330 train_time:120999ms step_avg:58.94ms
step:2054/2330 train_time:121060ms step_avg:58.94ms
step:2055/2330 train_time:121118ms step_avg:58.94ms
step:2056/2330 train_time:121180ms step_avg:58.94ms
step:2057/2330 train_time:121237ms step_avg:58.94ms
step:2058/2330 train_time:121298ms step_avg:58.94ms
step:2059/2330 train_time:121355ms step_avg:58.94ms
step:2060/2330 train_time:121417ms step_avg:58.94ms
step:2061/2330 train_time:121475ms step_avg:58.94ms
step:2062/2330 train_time:121536ms step_avg:58.94ms
step:2063/2330 train_time:121594ms step_avg:58.94ms
step:2064/2330 train_time:121655ms step_avg:58.94ms
step:2065/2330 train_time:121712ms step_avg:58.94ms
step:2066/2330 train_time:121773ms step_avg:58.94ms
step:2067/2330 train_time:121830ms step_avg:58.94ms
step:2068/2330 train_time:121892ms step_avg:58.94ms
step:2069/2330 train_time:121949ms step_avg:58.94ms
step:2070/2330 train_time:122010ms step_avg:58.94ms
step:2071/2330 train_time:122066ms step_avg:58.94ms
step:2072/2330 train_time:122131ms step_avg:58.94ms
step:2073/2330 train_time:122187ms step_avg:58.94ms
step:2074/2330 train_time:122250ms step_avg:58.94ms
step:2075/2330 train_time:122307ms step_avg:58.94ms
step:2076/2330 train_time:122369ms step_avg:58.94ms
step:2077/2330 train_time:122426ms step_avg:58.94ms
step:2078/2330 train_time:122489ms step_avg:58.95ms
step:2079/2330 train_time:122545ms step_avg:58.94ms
step:2080/2330 train_time:122607ms step_avg:58.95ms
step:2081/2330 train_time:122664ms step_avg:58.94ms
step:2082/2330 train_time:122726ms step_avg:58.95ms
step:2083/2330 train_time:122783ms step_avg:58.95ms
step:2084/2330 train_time:122845ms step_avg:58.95ms
step:2085/2330 train_time:122901ms step_avg:58.95ms
step:2086/2330 train_time:122964ms step_avg:58.95ms
step:2087/2330 train_time:123021ms step_avg:58.95ms
step:2088/2330 train_time:123083ms step_avg:58.95ms
step:2089/2330 train_time:123141ms step_avg:58.95ms
step:2090/2330 train_time:123204ms step_avg:58.95ms
step:2091/2330 train_time:123261ms step_avg:58.95ms
step:2092/2330 train_time:123323ms step_avg:58.95ms
step:2093/2330 train_time:123380ms step_avg:58.95ms
step:2094/2330 train_time:123443ms step_avg:58.95ms
step:2095/2330 train_time:123500ms step_avg:58.95ms
step:2096/2330 train_time:123563ms step_avg:58.95ms
step:2097/2330 train_time:123621ms step_avg:58.95ms
step:2098/2330 train_time:123681ms step_avg:58.95ms
step:2099/2330 train_time:123739ms step_avg:58.95ms
step:2100/2330 train_time:123800ms step_avg:58.95ms
step:2101/2330 train_time:123857ms step_avg:58.95ms
step:2102/2330 train_time:123920ms step_avg:58.95ms
step:2103/2330 train_time:123977ms step_avg:58.95ms
step:2104/2330 train_time:124038ms step_avg:58.95ms
step:2105/2330 train_time:124096ms step_avg:58.95ms
step:2106/2330 train_time:124156ms step_avg:58.95ms
step:2107/2330 train_time:124214ms step_avg:58.95ms
step:2108/2330 train_time:124276ms step_avg:58.95ms
step:2109/2330 train_time:124334ms step_avg:58.95ms
step:2110/2330 train_time:124395ms step_avg:58.95ms
step:2111/2330 train_time:124452ms step_avg:58.95ms
step:2112/2330 train_time:124514ms step_avg:58.96ms
step:2113/2330 train_time:124571ms step_avg:58.95ms
step:2114/2330 train_time:124633ms step_avg:58.96ms
step:2115/2330 train_time:124690ms step_avg:58.96ms
step:2116/2330 train_time:124751ms step_avg:58.96ms
step:2117/2330 train_time:124807ms step_avg:58.95ms
step:2118/2330 train_time:124869ms step_avg:58.96ms
step:2119/2330 train_time:124926ms step_avg:58.96ms
step:2120/2330 train_time:124989ms step_avg:58.96ms
step:2121/2330 train_time:125045ms step_avg:58.96ms
step:2122/2330 train_time:125107ms step_avg:58.96ms
step:2123/2330 train_time:125164ms step_avg:58.96ms
step:2124/2330 train_time:125227ms step_avg:58.96ms
step:2125/2330 train_time:125283ms step_avg:58.96ms
step:2126/2330 train_time:125346ms step_avg:58.96ms
step:2127/2330 train_time:125403ms step_avg:58.96ms
step:2128/2330 train_time:125466ms step_avg:58.96ms
step:2129/2330 train_time:125523ms step_avg:58.96ms
step:2130/2330 train_time:125585ms step_avg:58.96ms
step:2131/2330 train_time:125642ms step_avg:58.96ms
step:2132/2330 train_time:125705ms step_avg:58.96ms
step:2133/2330 train_time:125763ms step_avg:58.96ms
step:2134/2330 train_time:125826ms step_avg:58.96ms
step:2135/2330 train_time:125883ms step_avg:58.96ms
step:2136/2330 train_time:125945ms step_avg:58.96ms
step:2137/2330 train_time:126001ms step_avg:58.96ms
step:2138/2330 train_time:126064ms step_avg:58.96ms
step:2139/2330 train_time:126121ms step_avg:58.96ms
step:2140/2330 train_time:126183ms step_avg:58.96ms
step:2141/2330 train_time:126241ms step_avg:58.96ms
step:2142/2330 train_time:126304ms step_avg:58.97ms
step:2143/2330 train_time:126362ms step_avg:58.96ms
step:2144/2330 train_time:126423ms step_avg:58.97ms
step:2145/2330 train_time:126481ms step_avg:58.97ms
step:2146/2330 train_time:126543ms step_avg:58.97ms
step:2147/2330 train_time:126600ms step_avg:58.97ms
step:2148/2330 train_time:126663ms step_avg:58.97ms
step:2149/2330 train_time:126721ms step_avg:58.97ms
step:2150/2330 train_time:126782ms step_avg:58.97ms
step:2151/2330 train_time:126839ms step_avg:58.97ms
step:2152/2330 train_time:126902ms step_avg:58.97ms
step:2153/2330 train_time:126959ms step_avg:58.97ms
step:2154/2330 train_time:127021ms step_avg:58.97ms
step:2155/2330 train_time:127078ms step_avg:58.97ms
step:2156/2330 train_time:127139ms step_avg:58.97ms
step:2157/2330 train_time:127196ms step_avg:58.97ms
step:2158/2330 train_time:127257ms step_avg:58.97ms
step:2159/2330 train_time:127314ms step_avg:58.97ms
step:2160/2330 train_time:127376ms step_avg:58.97ms
step:2161/2330 train_time:127434ms step_avg:58.97ms
step:2162/2330 train_time:127495ms step_avg:58.97ms
step:2163/2330 train_time:127552ms step_avg:58.97ms
step:2164/2330 train_time:127615ms step_avg:58.97ms
step:2165/2330 train_time:127673ms step_avg:58.97ms
step:2166/2330 train_time:127734ms step_avg:58.97ms
step:2167/2330 train_time:127792ms step_avg:58.97ms
step:2168/2330 train_time:127853ms step_avg:58.97ms
step:2169/2330 train_time:127909ms step_avg:58.97ms
step:2170/2330 train_time:127971ms step_avg:58.97ms
step:2171/2330 train_time:128028ms step_avg:58.97ms
step:2172/2330 train_time:128091ms step_avg:58.97ms
step:2173/2330 train_time:128147ms step_avg:58.97ms
step:2174/2330 train_time:128209ms step_avg:58.97ms
step:2175/2330 train_time:128266ms step_avg:58.97ms
step:2176/2330 train_time:128329ms step_avg:58.97ms
step:2177/2330 train_time:128385ms step_avg:58.97ms
step:2178/2330 train_time:128448ms step_avg:58.98ms
step:2179/2330 train_time:128504ms step_avg:58.97ms
step:2180/2330 train_time:128568ms step_avg:58.98ms
step:2181/2330 train_time:128624ms step_avg:58.97ms
step:2182/2330 train_time:128688ms step_avg:58.98ms
step:2183/2330 train_time:128745ms step_avg:58.98ms
step:2184/2330 train_time:128808ms step_avg:58.98ms
step:2185/2330 train_time:128865ms step_avg:58.98ms
step:2186/2330 train_time:128927ms step_avg:58.98ms
step:2187/2330 train_time:128984ms step_avg:58.98ms
step:2188/2330 train_time:129047ms step_avg:58.98ms
step:2189/2330 train_time:129104ms step_avg:58.98ms
step:2190/2330 train_time:129166ms step_avg:58.98ms
step:2191/2330 train_time:129223ms step_avg:58.98ms
step:2192/2330 train_time:129287ms step_avg:58.98ms
step:2193/2330 train_time:129344ms step_avg:58.98ms
step:2194/2330 train_time:129407ms step_avg:58.98ms
step:2195/2330 train_time:129464ms step_avg:58.98ms
step:2196/2330 train_time:129526ms step_avg:58.98ms
step:2197/2330 train_time:129583ms step_avg:58.98ms
step:2198/2330 train_time:129646ms step_avg:58.98ms
step:2199/2330 train_time:129703ms step_avg:58.98ms
step:2200/2330 train_time:129766ms step_avg:58.98ms
step:2201/2330 train_time:129822ms step_avg:58.98ms
step:2202/2330 train_time:129885ms step_avg:58.98ms
step:2203/2330 train_time:129941ms step_avg:58.98ms
step:2204/2330 train_time:130004ms step_avg:58.99ms
step:2205/2330 train_time:130061ms step_avg:58.98ms
step:2206/2330 train_time:130123ms step_avg:58.99ms
step:2207/2330 train_time:130180ms step_avg:58.98ms
step:2208/2330 train_time:130243ms step_avg:58.99ms
step:2209/2330 train_time:130300ms step_avg:58.99ms
step:2210/2330 train_time:130364ms step_avg:58.99ms
step:2211/2330 train_time:130421ms step_avg:58.99ms
step:2212/2330 train_time:130483ms step_avg:58.99ms
step:2213/2330 train_time:130540ms step_avg:58.99ms
step:2214/2330 train_time:130604ms step_avg:58.99ms
step:2215/2330 train_time:130661ms step_avg:58.99ms
step:2216/2330 train_time:130724ms step_avg:58.99ms
step:2217/2330 train_time:130781ms step_avg:58.99ms
step:2218/2330 train_time:130844ms step_avg:58.99ms
step:2219/2330 train_time:130901ms step_avg:58.99ms
step:2220/2330 train_time:130964ms step_avg:58.99ms
step:2221/2330 train_time:131021ms step_avg:58.99ms
step:2222/2330 train_time:131083ms step_avg:58.99ms
step:2223/2330 train_time:131141ms step_avg:58.99ms
step:2224/2330 train_time:131203ms step_avg:58.99ms
step:2225/2330 train_time:131260ms step_avg:58.99ms
step:2226/2330 train_time:131322ms step_avg:58.99ms
step:2227/2330 train_time:131380ms step_avg:58.99ms
step:2228/2330 train_time:131441ms step_avg:59.00ms
step:2229/2330 train_time:131499ms step_avg:58.99ms
step:2230/2330 train_time:131560ms step_avg:59.00ms
step:2231/2330 train_time:131617ms step_avg:58.99ms
step:2232/2330 train_time:131679ms step_avg:59.00ms
step:2233/2330 train_time:131737ms step_avg:59.00ms
step:2234/2330 train_time:131798ms step_avg:59.00ms
step:2235/2330 train_time:131855ms step_avg:59.00ms
step:2236/2330 train_time:131916ms step_avg:59.00ms
step:2237/2330 train_time:131974ms step_avg:59.00ms
step:2238/2330 train_time:132036ms step_avg:59.00ms
step:2239/2330 train_time:132094ms step_avg:59.00ms
step:2240/2330 train_time:132154ms step_avg:59.00ms
step:2241/2330 train_time:132212ms step_avg:59.00ms
step:2242/2330 train_time:132273ms step_avg:59.00ms
step:2243/2330 train_time:132330ms step_avg:59.00ms
step:2244/2330 train_time:132392ms step_avg:59.00ms
step:2245/2330 train_time:132448ms step_avg:59.00ms
step:2246/2330 train_time:132511ms step_avg:59.00ms
step:2247/2330 train_time:132568ms step_avg:59.00ms
step:2248/2330 train_time:132631ms step_avg:59.00ms
step:2249/2330 train_time:132687ms step_avg:59.00ms
step:2250/2330 train_time:132750ms step_avg:59.00ms
step:2250/2330 val_loss:4.1557 train_time:132831ms step_avg:59.04ms
step:2251/2330 train_time:132850ms step_avg:59.02ms
step:2252/2330 train_time:132874ms step_avg:59.00ms
step:2253/2330 train_time:132933ms step_avg:59.00ms
step:2254/2330 train_time:133001ms step_avg:59.01ms
step:2255/2330 train_time:133058ms step_avg:59.01ms
step:2256/2330 train_time:133121ms step_avg:59.01ms
step:2257/2330 train_time:133177ms step_avg:59.01ms
step:2258/2330 train_time:133239ms step_avg:59.01ms
step:2259/2330 train_time:133296ms step_avg:59.01ms
step:2260/2330 train_time:133357ms step_avg:59.01ms
step:2261/2330 train_time:133414ms step_avg:59.01ms
step:2262/2330 train_time:133474ms step_avg:59.01ms
step:2263/2330 train_time:133531ms step_avg:59.01ms
step:2264/2330 train_time:133592ms step_avg:59.01ms
step:2265/2330 train_time:133649ms step_avg:59.01ms
step:2266/2330 train_time:133710ms step_avg:59.01ms
step:2267/2330 train_time:133766ms step_avg:59.01ms
step:2268/2330 train_time:133830ms step_avg:59.01ms
step:2269/2330 train_time:133887ms step_avg:59.01ms
step:2270/2330 train_time:133952ms step_avg:59.01ms
step:2271/2330 train_time:134009ms step_avg:59.01ms
step:2272/2330 train_time:134073ms step_avg:59.01ms
step:2273/2330 train_time:134130ms step_avg:59.01ms
step:2274/2330 train_time:134194ms step_avg:59.01ms
step:2275/2330 train_time:134251ms step_avg:59.01ms
step:2276/2330 train_time:134312ms step_avg:59.01ms
step:2277/2330 train_time:134369ms step_avg:59.01ms
step:2278/2330 train_time:134430ms step_avg:59.01ms
step:2279/2330 train_time:134487ms step_avg:59.01ms
step:2280/2330 train_time:134548ms step_avg:59.01ms
step:2281/2330 train_time:134605ms step_avg:59.01ms
step:2282/2330 train_time:134666ms step_avg:59.01ms
step:2283/2330 train_time:134723ms step_avg:59.01ms
step:2284/2330 train_time:134784ms step_avg:59.01ms
step:2285/2330 train_time:134842ms step_avg:59.01ms
step:2286/2330 train_time:134903ms step_avg:59.01ms
step:2287/2330 train_time:134961ms step_avg:59.01ms
step:2288/2330 train_time:135023ms step_avg:59.01ms
step:2289/2330 train_time:135081ms step_avg:59.01ms
step:2290/2330 train_time:135144ms step_avg:59.01ms
step:2291/2330 train_time:135202ms step_avg:59.01ms
step:2292/2330 train_time:135264ms step_avg:59.02ms
step:2293/2330 train_time:135321ms step_avg:59.01ms
step:2294/2330 train_time:135383ms step_avg:59.02ms
step:2295/2330 train_time:135440ms step_avg:59.02ms
step:2296/2330 train_time:135501ms step_avg:59.02ms
step:2297/2330 train_time:135559ms step_avg:59.02ms
step:2298/2330 train_time:135619ms step_avg:59.02ms
step:2299/2330 train_time:135676ms step_avg:59.02ms
step:2300/2330 train_time:135737ms step_avg:59.02ms
step:2301/2330 train_time:135795ms step_avg:59.02ms
step:2302/2330 train_time:135857ms step_avg:59.02ms
step:2303/2330 train_time:135913ms step_avg:59.02ms
step:2304/2330 train_time:135976ms step_avg:59.02ms
step:2305/2330 train_time:136033ms step_avg:59.02ms
step:2306/2330 train_time:136096ms step_avg:59.02ms
step:2307/2330 train_time:136153ms step_avg:59.02ms
step:2308/2330 train_time:136216ms step_avg:59.02ms
step:2309/2330 train_time:136273ms step_avg:59.02ms
step:2310/2330 train_time:136336ms step_avg:59.02ms
step:2311/2330 train_time:136392ms step_avg:59.02ms
step:2312/2330 train_time:136455ms step_avg:59.02ms
step:2313/2330 train_time:136511ms step_avg:59.02ms
step:2314/2330 train_time:136573ms step_avg:59.02ms
step:2315/2330 train_time:136629ms step_avg:59.02ms
step:2316/2330 train_time:136692ms step_avg:59.02ms
step:2317/2330 train_time:136748ms step_avg:59.02ms
step:2318/2330 train_time:136811ms step_avg:59.02ms
step:2319/2330 train_time:136868ms step_avg:59.02ms
step:2320/2330 train_time:136931ms step_avg:59.02ms
step:2321/2330 train_time:136988ms step_avg:59.02ms
step:2322/2330 train_time:137051ms step_avg:59.02ms
step:2323/2330 train_time:137108ms step_avg:59.02ms
step:2324/2330 train_time:137171ms step_avg:59.02ms
step:2325/2330 train_time:137228ms step_avg:59.02ms
step:2326/2330 train_time:137292ms step_avg:59.02ms
step:2327/2330 train_time:137349ms step_avg:59.02ms
step:2328/2330 train_time:137411ms step_avg:59.03ms
step:2329/2330 train_time:137468ms step_avg:59.02ms
step:2330/2330 train_time:137530ms step_avg:59.03ms
step:2330/2330 val_loss:4.1383 train_time:137609ms step_avg:59.06ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
