import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = divide_by_norm(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = divide_by_norm(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "gd_1e-3"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-3, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 03:00:31 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   30C    P0             111W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   28C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   27C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:90ms step_avg:89.86ms
step:2/2330 train_time:190ms step_avg:95.04ms
step:3/2330 train_time:212ms step_avg:70.71ms
step:4/2330 train_time:239ms step_avg:59.86ms
step:5/2330 train_time:296ms step_avg:59.28ms
step:6/2330 train_time:357ms step_avg:59.42ms
step:7/2330 train_time:414ms step_avg:59.15ms
step:8/2330 train_time:475ms step_avg:59.36ms
step:9/2330 train_time:533ms step_avg:59.23ms
step:10/2330 train_time:594ms step_avg:59.37ms
step:11/2330 train_time:652ms step_avg:59.23ms
step:12/2330 train_time:712ms step_avg:59.34ms
step:13/2330 train_time:770ms step_avg:59.26ms
step:14/2330 train_time:832ms step_avg:59.41ms
step:15/2330 train_time:890ms step_avg:59.33ms
step:16/2330 train_time:951ms step_avg:59.43ms
step:17/2330 train_time:1009ms step_avg:59.33ms
step:18/2330 train_time:1071ms step_avg:59.52ms
step:19/2330 train_time:1133ms step_avg:59.61ms
step:20/2330 train_time:1197ms step_avg:59.83ms
step:21/2330 train_time:1258ms step_avg:59.90ms
step:22/2330 train_time:1320ms step_avg:60.01ms
step:23/2330 train_time:1378ms step_avg:59.91ms
step:24/2330 train_time:1440ms step_avg:59.99ms
step:25/2330 train_time:1499ms step_avg:59.95ms
step:26/2330 train_time:1560ms step_avg:60.01ms
step:27/2330 train_time:1618ms step_avg:59.94ms
step:28/2330 train_time:1680ms step_avg:59.99ms
step:29/2330 train_time:1739ms step_avg:59.96ms
step:30/2330 train_time:1801ms step_avg:60.03ms
step:31/2330 train_time:1860ms step_avg:60.00ms
step:32/2330 train_time:1922ms step_avg:60.06ms
step:33/2330 train_time:1981ms step_avg:60.02ms
step:34/2330 train_time:2043ms step_avg:60.08ms
step:35/2330 train_time:2103ms step_avg:60.07ms
step:36/2330 train_time:2165ms step_avg:60.14ms
step:37/2330 train_time:2226ms step_avg:60.15ms
step:38/2330 train_time:2289ms step_avg:60.23ms
step:39/2330 train_time:2347ms step_avg:60.18ms
step:40/2330 train_time:2409ms step_avg:60.23ms
step:41/2330 train_time:2468ms step_avg:60.20ms
step:42/2330 train_time:2530ms step_avg:60.24ms
step:43/2330 train_time:2587ms step_avg:60.17ms
step:44/2330 train_time:2649ms step_avg:60.21ms
step:45/2330 train_time:2708ms step_avg:60.18ms
step:46/2330 train_time:2771ms step_avg:60.24ms
step:47/2330 train_time:2829ms step_avg:60.20ms
step:48/2330 train_time:2891ms step_avg:60.24ms
step:49/2330 train_time:2951ms step_avg:60.22ms
step:50/2330 train_time:3014ms step_avg:60.27ms
step:51/2330 train_time:3073ms step_avg:60.26ms
step:52/2330 train_time:3135ms step_avg:60.30ms
step:53/2330 train_time:3195ms step_avg:60.28ms
step:54/2330 train_time:3258ms step_avg:60.33ms
step:55/2330 train_time:3319ms step_avg:60.34ms
step:56/2330 train_time:3380ms step_avg:60.36ms
step:57/2330 train_time:3440ms step_avg:60.34ms
step:58/2330 train_time:3501ms step_avg:60.36ms
step:59/2330 train_time:3559ms step_avg:60.32ms
step:60/2330 train_time:3622ms step_avg:60.36ms
step:61/2330 train_time:3681ms step_avg:60.34ms
step:62/2330 train_time:3743ms step_avg:60.37ms
step:63/2330 train_time:3802ms step_avg:60.34ms
step:64/2330 train_time:3864ms step_avg:60.37ms
step:65/2330 train_time:3923ms step_avg:60.36ms
step:66/2330 train_time:3985ms step_avg:60.38ms
step:67/2330 train_time:4045ms step_avg:60.37ms
step:68/2330 train_time:4108ms step_avg:60.41ms
step:69/2330 train_time:4167ms step_avg:60.39ms
step:70/2330 train_time:4231ms step_avg:60.44ms
step:71/2330 train_time:4290ms step_avg:60.42ms
step:72/2330 train_time:4351ms step_avg:60.43ms
step:73/2330 train_time:4410ms step_avg:60.41ms
step:74/2330 train_time:4472ms step_avg:60.44ms
step:75/2330 train_time:4531ms step_avg:60.41ms
step:76/2330 train_time:4592ms step_avg:60.43ms
step:77/2330 train_time:4651ms step_avg:60.41ms
step:78/2330 train_time:4714ms step_avg:60.43ms
step:79/2330 train_time:4773ms step_avg:60.41ms
step:80/2330 train_time:4835ms step_avg:60.43ms
step:81/2330 train_time:4894ms step_avg:60.42ms
step:82/2330 train_time:4957ms step_avg:60.45ms
step:83/2330 train_time:5016ms step_avg:60.44ms
step:84/2330 train_time:5078ms step_avg:60.45ms
step:85/2330 train_time:5138ms step_avg:60.45ms
step:86/2330 train_time:5199ms step_avg:60.46ms
step:87/2330 train_time:5260ms step_avg:60.46ms
step:88/2330 train_time:5322ms step_avg:60.48ms
step:89/2330 train_time:5380ms step_avg:60.45ms
step:90/2330 train_time:5443ms step_avg:60.48ms
step:91/2330 train_time:5501ms step_avg:60.45ms
step:92/2330 train_time:5563ms step_avg:60.47ms
step:93/2330 train_time:5622ms step_avg:60.45ms
step:94/2330 train_time:5685ms step_avg:60.47ms
step:95/2330 train_time:5744ms step_avg:60.46ms
step:96/2330 train_time:5805ms step_avg:60.47ms
step:97/2330 train_time:5865ms step_avg:60.46ms
step:98/2330 train_time:5928ms step_avg:60.49ms
step:99/2330 train_time:5987ms step_avg:60.48ms
step:100/2330 train_time:6050ms step_avg:60.50ms
step:101/2330 train_time:6109ms step_avg:60.48ms
step:102/2330 train_time:6172ms step_avg:60.51ms
step:103/2330 train_time:6231ms step_avg:60.49ms
step:104/2330 train_time:6293ms step_avg:60.51ms
step:105/2330 train_time:6352ms step_avg:60.50ms
step:106/2330 train_time:6414ms step_avg:60.51ms
step:107/2330 train_time:6474ms step_avg:60.50ms
step:108/2330 train_time:6536ms step_avg:60.52ms
step:109/2330 train_time:6595ms step_avg:60.51ms
step:110/2330 train_time:6659ms step_avg:60.54ms
step:111/2330 train_time:6718ms step_avg:60.52ms
step:112/2330 train_time:6780ms step_avg:60.54ms
step:113/2330 train_time:6839ms step_avg:60.52ms
step:114/2330 train_time:6901ms step_avg:60.53ms
step:115/2330 train_time:6961ms step_avg:60.53ms
step:116/2330 train_time:7023ms step_avg:60.54ms
step:117/2330 train_time:7082ms step_avg:60.53ms
step:118/2330 train_time:7144ms step_avg:60.54ms
step:119/2330 train_time:7203ms step_avg:60.53ms
step:120/2330 train_time:7266ms step_avg:60.55ms
step:121/2330 train_time:7325ms step_avg:60.54ms
step:122/2330 train_time:7388ms step_avg:60.56ms
step:123/2330 train_time:7447ms step_avg:60.55ms
step:124/2330 train_time:7509ms step_avg:60.56ms
step:125/2330 train_time:7568ms step_avg:60.55ms
step:126/2330 train_time:7630ms step_avg:60.56ms
step:127/2330 train_time:7689ms step_avg:60.54ms
step:128/2330 train_time:7752ms step_avg:60.56ms
step:129/2330 train_time:7812ms step_avg:60.56ms
step:130/2330 train_time:7874ms step_avg:60.57ms
step:131/2330 train_time:7934ms step_avg:60.56ms
step:132/2330 train_time:7996ms step_avg:60.58ms
step:133/2330 train_time:8056ms step_avg:60.57ms
step:134/2330 train_time:8118ms step_avg:60.58ms
step:135/2330 train_time:8177ms step_avg:60.57ms
step:136/2330 train_time:8239ms step_avg:60.58ms
step:137/2330 train_time:8299ms step_avg:60.57ms
step:138/2330 train_time:8361ms step_avg:60.59ms
step:139/2330 train_time:8421ms step_avg:60.58ms
step:140/2330 train_time:8483ms step_avg:60.59ms
step:141/2330 train_time:8543ms step_avg:60.59ms
step:142/2330 train_time:8605ms step_avg:60.60ms
step:143/2330 train_time:8664ms step_avg:60.59ms
step:144/2330 train_time:8727ms step_avg:60.60ms
step:145/2330 train_time:8786ms step_avg:60.59ms
step:146/2330 train_time:8848ms step_avg:60.61ms
step:147/2330 train_time:8907ms step_avg:60.59ms
step:148/2330 train_time:8969ms step_avg:60.60ms
step:149/2330 train_time:9028ms step_avg:60.59ms
step:150/2330 train_time:9091ms step_avg:60.61ms
step:151/2330 train_time:9150ms step_avg:60.59ms
step:152/2330 train_time:9213ms step_avg:60.61ms
step:153/2330 train_time:9272ms step_avg:60.60ms
step:154/2330 train_time:9334ms step_avg:60.61ms
step:155/2330 train_time:9393ms step_avg:60.60ms
step:156/2330 train_time:9456ms step_avg:60.61ms
step:157/2330 train_time:9516ms step_avg:60.61ms
step:158/2330 train_time:9579ms step_avg:60.63ms
step:159/2330 train_time:9637ms step_avg:60.61ms
step:160/2330 train_time:9700ms step_avg:60.63ms
step:161/2330 train_time:9760ms step_avg:60.62ms
step:162/2330 train_time:9823ms step_avg:60.63ms
step:163/2330 train_time:9881ms step_avg:60.62ms
step:164/2330 train_time:9943ms step_avg:60.63ms
step:165/2330 train_time:10002ms step_avg:60.62ms
step:166/2330 train_time:10064ms step_avg:60.63ms
step:167/2330 train_time:10124ms step_avg:60.62ms
step:168/2330 train_time:10185ms step_avg:60.63ms
step:169/2330 train_time:10245ms step_avg:60.62ms
step:170/2330 train_time:10308ms step_avg:60.63ms
step:171/2330 train_time:10367ms step_avg:60.63ms
step:172/2330 train_time:10429ms step_avg:60.64ms
step:173/2330 train_time:10488ms step_avg:60.63ms
step:174/2330 train_time:10551ms step_avg:60.64ms
step:175/2330 train_time:10610ms step_avg:60.63ms
step:176/2330 train_time:10672ms step_avg:60.64ms
step:177/2330 train_time:10732ms step_avg:60.63ms
step:178/2330 train_time:10794ms step_avg:60.64ms
step:179/2330 train_time:10854ms step_avg:60.64ms
step:180/2330 train_time:10917ms step_avg:60.65ms
step:181/2330 train_time:10976ms step_avg:60.64ms
step:182/2330 train_time:11039ms step_avg:60.65ms
step:183/2330 train_time:11098ms step_avg:60.64ms
step:184/2330 train_time:11160ms step_avg:60.65ms
step:185/2330 train_time:11220ms step_avg:60.65ms
step:186/2330 train_time:11282ms step_avg:60.66ms
step:187/2330 train_time:11342ms step_avg:60.65ms
step:188/2330 train_time:11403ms step_avg:60.65ms
step:189/2330 train_time:11463ms step_avg:60.65ms
step:190/2330 train_time:11525ms step_avg:60.66ms
step:191/2330 train_time:11586ms step_avg:60.66ms
step:192/2330 train_time:11649ms step_avg:60.67ms
step:193/2330 train_time:11708ms step_avg:60.66ms
step:194/2330 train_time:11770ms step_avg:60.67ms
step:195/2330 train_time:11828ms step_avg:60.66ms
step:196/2330 train_time:11890ms step_avg:60.66ms
step:197/2330 train_time:11949ms step_avg:60.66ms
step:198/2330 train_time:12011ms step_avg:60.66ms
step:199/2330 train_time:12070ms step_avg:60.65ms
step:200/2330 train_time:12133ms step_avg:60.66ms
step:201/2330 train_time:12192ms step_avg:60.66ms
step:202/2330 train_time:12255ms step_avg:60.67ms
step:203/2330 train_time:12315ms step_avg:60.67ms
step:204/2330 train_time:12379ms step_avg:60.68ms
step:205/2330 train_time:12438ms step_avg:60.67ms
step:206/2330 train_time:12500ms step_avg:60.68ms
step:207/2330 train_time:12559ms step_avg:60.67ms
step:208/2330 train_time:12622ms step_avg:60.68ms
step:209/2330 train_time:12681ms step_avg:60.67ms
step:210/2330 train_time:12744ms step_avg:60.68ms
step:211/2330 train_time:12803ms step_avg:60.68ms
step:212/2330 train_time:12865ms step_avg:60.68ms
step:213/2330 train_time:12925ms step_avg:60.68ms
step:214/2330 train_time:12986ms step_avg:60.68ms
step:215/2330 train_time:13046ms step_avg:60.68ms
step:216/2330 train_time:13108ms step_avg:60.69ms
step:217/2330 train_time:13168ms step_avg:60.68ms
step:218/2330 train_time:13231ms step_avg:60.69ms
step:219/2330 train_time:13289ms step_avg:60.68ms
step:220/2330 train_time:13352ms step_avg:60.69ms
step:221/2330 train_time:13411ms step_avg:60.68ms
step:222/2330 train_time:13473ms step_avg:60.69ms
step:223/2330 train_time:13533ms step_avg:60.69ms
step:224/2330 train_time:13597ms step_avg:60.70ms
step:225/2330 train_time:13656ms step_avg:60.69ms
step:226/2330 train_time:13718ms step_avg:60.70ms
step:227/2330 train_time:13777ms step_avg:60.69ms
step:228/2330 train_time:13839ms step_avg:60.70ms
step:229/2330 train_time:13899ms step_avg:60.69ms
step:230/2330 train_time:13961ms step_avg:60.70ms
step:231/2330 train_time:14020ms step_avg:60.69ms
step:232/2330 train_time:14083ms step_avg:60.70ms
step:233/2330 train_time:14142ms step_avg:60.70ms
step:234/2330 train_time:14204ms step_avg:60.70ms
step:235/2330 train_time:14263ms step_avg:60.69ms
step:236/2330 train_time:14327ms step_avg:60.71ms
step:237/2330 train_time:14386ms step_avg:60.70ms
step:238/2330 train_time:14448ms step_avg:60.71ms
step:239/2330 train_time:14507ms step_avg:60.70ms
step:240/2330 train_time:14569ms step_avg:60.70ms
step:241/2330 train_time:14627ms step_avg:60.69ms
step:242/2330 train_time:14690ms step_avg:60.70ms
step:243/2330 train_time:14748ms step_avg:60.69ms
step:244/2330 train_time:14810ms step_avg:60.70ms
step:245/2330 train_time:14870ms step_avg:60.69ms
step:246/2330 train_time:14932ms step_avg:60.70ms
step:247/2330 train_time:14991ms step_avg:60.69ms
step:248/2330 train_time:15054ms step_avg:60.70ms
step:249/2330 train_time:15114ms step_avg:60.70ms
step:250/2330 train_time:15177ms step_avg:60.71ms
step:250/2330 val_loss:5.3452 train_time:15249ms step_avg:61.00ms
step:251/2330 train_time:15272ms step_avg:60.84ms
step:252/2330 train_time:15300ms step_avg:60.72ms
step:253/2330 train_time:15360ms step_avg:60.71ms
step:254/2330 train_time:15429ms step_avg:60.74ms
step:255/2330 train_time:15492ms step_avg:60.75ms
step:256/2330 train_time:15554ms step_avg:60.76ms
step:257/2330 train_time:15614ms step_avg:60.75ms
step:258/2330 train_time:15676ms step_avg:60.76ms
step:259/2330 train_time:15735ms step_avg:60.75ms
step:260/2330 train_time:15797ms step_avg:60.76ms
step:261/2330 train_time:15855ms step_avg:60.75ms
step:262/2330 train_time:15916ms step_avg:60.75ms
step:263/2330 train_time:15974ms step_avg:60.74ms
step:264/2330 train_time:16036ms step_avg:60.74ms
step:265/2330 train_time:16094ms step_avg:60.73ms
step:266/2330 train_time:16156ms step_avg:60.74ms
step:267/2330 train_time:16217ms step_avg:60.74ms
step:268/2330 train_time:16281ms step_avg:60.75ms
step:269/2330 train_time:16342ms step_avg:60.75ms
step:270/2330 train_time:16405ms step_avg:60.76ms
step:271/2330 train_time:16465ms step_avg:60.76ms
step:272/2330 train_time:16527ms step_avg:60.76ms
step:273/2330 train_time:16587ms step_avg:60.76ms
step:274/2330 train_time:16649ms step_avg:60.76ms
step:275/2330 train_time:16708ms step_avg:60.76ms
step:276/2330 train_time:16770ms step_avg:60.76ms
step:277/2330 train_time:16829ms step_avg:60.75ms
step:278/2330 train_time:16891ms step_avg:60.76ms
step:279/2330 train_time:16950ms step_avg:60.75ms
step:280/2330 train_time:17011ms step_avg:60.75ms
step:281/2330 train_time:17070ms step_avg:60.75ms
step:282/2330 train_time:17132ms step_avg:60.75ms
step:283/2330 train_time:17190ms step_avg:60.74ms
step:284/2330 train_time:17252ms step_avg:60.75ms
step:285/2330 train_time:17312ms step_avg:60.74ms
step:286/2330 train_time:17375ms step_avg:60.75ms
step:287/2330 train_time:17436ms step_avg:60.75ms
step:288/2330 train_time:17501ms step_avg:60.77ms
step:289/2330 train_time:17560ms step_avg:60.76ms
step:290/2330 train_time:17623ms step_avg:60.77ms
step:291/2330 train_time:17683ms step_avg:60.77ms
step:292/2330 train_time:17744ms step_avg:60.77ms
step:293/2330 train_time:17804ms step_avg:60.76ms
step:294/2330 train_time:17866ms step_avg:60.77ms
step:295/2330 train_time:17925ms step_avg:60.76ms
step:296/2330 train_time:17987ms step_avg:60.77ms
step:297/2330 train_time:18046ms step_avg:60.76ms
step:298/2330 train_time:18108ms step_avg:60.76ms
step:299/2330 train_time:18167ms step_avg:60.76ms
step:300/2330 train_time:18229ms step_avg:60.76ms
step:301/2330 train_time:18288ms step_avg:60.76ms
step:302/2330 train_time:18352ms step_avg:60.77ms
step:303/2330 train_time:18411ms step_avg:60.76ms
step:304/2330 train_time:18474ms step_avg:60.77ms
step:305/2330 train_time:18533ms step_avg:60.76ms
step:306/2330 train_time:18597ms step_avg:60.77ms
step:307/2330 train_time:18657ms step_avg:60.77ms
step:308/2330 train_time:18719ms step_avg:60.78ms
step:309/2330 train_time:18778ms step_avg:60.77ms
step:310/2330 train_time:18840ms step_avg:60.77ms
step:311/2330 train_time:18899ms step_avg:60.77ms
step:312/2330 train_time:18962ms step_avg:60.78ms
step:313/2330 train_time:19021ms step_avg:60.77ms
step:314/2330 train_time:19083ms step_avg:60.78ms
step:315/2330 train_time:19143ms step_avg:60.77ms
step:316/2330 train_time:19204ms step_avg:60.77ms
step:317/2330 train_time:19264ms step_avg:60.77ms
step:318/2330 train_time:19326ms step_avg:60.77ms
step:319/2330 train_time:19387ms step_avg:60.78ms
step:320/2330 train_time:19449ms step_avg:60.78ms
step:321/2330 train_time:19508ms step_avg:60.77ms
step:322/2330 train_time:19571ms step_avg:60.78ms
step:323/2330 train_time:19631ms step_avg:60.78ms
step:324/2330 train_time:19694ms step_avg:60.78ms
step:325/2330 train_time:19753ms step_avg:60.78ms
step:326/2330 train_time:19814ms step_avg:60.78ms
step:327/2330 train_time:19874ms step_avg:60.78ms
step:328/2330 train_time:19937ms step_avg:60.78ms
step:329/2330 train_time:19996ms step_avg:60.78ms
step:330/2330 train_time:20059ms step_avg:60.78ms
step:331/2330 train_time:20119ms step_avg:60.78ms
step:332/2330 train_time:20182ms step_avg:60.79ms
step:333/2330 train_time:20240ms step_avg:60.78ms
step:334/2330 train_time:20303ms step_avg:60.79ms
step:335/2330 train_time:20362ms step_avg:60.78ms
step:336/2330 train_time:20425ms step_avg:60.79ms
step:337/2330 train_time:20484ms step_avg:60.78ms
step:338/2330 train_time:20547ms step_avg:60.79ms
step:339/2330 train_time:20606ms step_avg:60.79ms
step:340/2330 train_time:20669ms step_avg:60.79ms
step:341/2330 train_time:20728ms step_avg:60.79ms
step:342/2330 train_time:20790ms step_avg:60.79ms
step:343/2330 train_time:20849ms step_avg:60.78ms
step:344/2330 train_time:20911ms step_avg:60.79ms
step:345/2330 train_time:20970ms step_avg:60.78ms
step:346/2330 train_time:21032ms step_avg:60.79ms
step:347/2330 train_time:21092ms step_avg:60.78ms
step:348/2330 train_time:21155ms step_avg:60.79ms
step:349/2330 train_time:21215ms step_avg:60.79ms
step:350/2330 train_time:21279ms step_avg:60.80ms
step:351/2330 train_time:21338ms step_avg:60.79ms
step:352/2330 train_time:21400ms step_avg:60.80ms
step:353/2330 train_time:21459ms step_avg:60.79ms
step:354/2330 train_time:21521ms step_avg:60.79ms
step:355/2330 train_time:21580ms step_avg:60.79ms
step:356/2330 train_time:21644ms step_avg:60.80ms
step:357/2330 train_time:21703ms step_avg:60.79ms
step:358/2330 train_time:21765ms step_avg:60.80ms
step:359/2330 train_time:21825ms step_avg:60.79ms
step:360/2330 train_time:21888ms step_avg:60.80ms
step:361/2330 train_time:21946ms step_avg:60.79ms
step:362/2330 train_time:22008ms step_avg:60.80ms
step:363/2330 train_time:22067ms step_avg:60.79ms
step:364/2330 train_time:22129ms step_avg:60.80ms
step:365/2330 train_time:22189ms step_avg:60.79ms
step:366/2330 train_time:22251ms step_avg:60.79ms
step:367/2330 train_time:22309ms step_avg:60.79ms
step:368/2330 train_time:22372ms step_avg:60.79ms
step:369/2330 train_time:22431ms step_avg:60.79ms
step:370/2330 train_time:22494ms step_avg:60.79ms
step:371/2330 train_time:22553ms step_avg:60.79ms
step:372/2330 train_time:22616ms step_avg:60.80ms
step:373/2330 train_time:22676ms step_avg:60.79ms
step:374/2330 train_time:22739ms step_avg:60.80ms
step:375/2330 train_time:22799ms step_avg:60.80ms
step:376/2330 train_time:22860ms step_avg:60.80ms
step:377/2330 train_time:22919ms step_avg:60.79ms
step:378/2330 train_time:22982ms step_avg:60.80ms
step:379/2330 train_time:23041ms step_avg:60.79ms
step:380/2330 train_time:23103ms step_avg:60.80ms
step:381/2330 train_time:23163ms step_avg:60.80ms
step:382/2330 train_time:23226ms step_avg:60.80ms
step:383/2330 train_time:23285ms step_avg:60.80ms
step:384/2330 train_time:23347ms step_avg:60.80ms
step:385/2330 train_time:23407ms step_avg:60.80ms
step:386/2330 train_time:23470ms step_avg:60.80ms
step:387/2330 train_time:23529ms step_avg:60.80ms
step:388/2330 train_time:23592ms step_avg:60.80ms
step:389/2330 train_time:23651ms step_avg:60.80ms
step:390/2330 train_time:23713ms step_avg:60.80ms
step:391/2330 train_time:23772ms step_avg:60.80ms
step:392/2330 train_time:23835ms step_avg:60.80ms
step:393/2330 train_time:23895ms step_avg:60.80ms
step:394/2330 train_time:23958ms step_avg:60.81ms
step:395/2330 train_time:24018ms step_avg:60.80ms
step:396/2330 train_time:24081ms step_avg:60.81ms
step:397/2330 train_time:24140ms step_avg:60.81ms
step:398/2330 train_time:24203ms step_avg:60.81ms
step:399/2330 train_time:24261ms step_avg:60.81ms
step:400/2330 train_time:24324ms step_avg:60.81ms
step:401/2330 train_time:24383ms step_avg:60.81ms
step:402/2330 train_time:24446ms step_avg:60.81ms
step:403/2330 train_time:24506ms step_avg:60.81ms
step:404/2330 train_time:24567ms step_avg:60.81ms
step:405/2330 train_time:24627ms step_avg:60.81ms
step:406/2330 train_time:24690ms step_avg:60.81ms
step:407/2330 train_time:24749ms step_avg:60.81ms
step:408/2330 train_time:24812ms step_avg:60.81ms
step:409/2330 train_time:24871ms step_avg:60.81ms
step:410/2330 train_time:24934ms step_avg:60.82ms
step:411/2330 train_time:24993ms step_avg:60.81ms
step:412/2330 train_time:25056ms step_avg:60.82ms
step:413/2330 train_time:25116ms step_avg:60.81ms
step:414/2330 train_time:25179ms step_avg:60.82ms
step:415/2330 train_time:25239ms step_avg:60.82ms
step:416/2330 train_time:25300ms step_avg:60.82ms
step:417/2330 train_time:25360ms step_avg:60.81ms
step:418/2330 train_time:25422ms step_avg:60.82ms
step:419/2330 train_time:25481ms step_avg:60.81ms
step:420/2330 train_time:25543ms step_avg:60.82ms
step:421/2330 train_time:25603ms step_avg:60.82ms
step:422/2330 train_time:25665ms step_avg:60.82ms
step:423/2330 train_time:25725ms step_avg:60.82ms
step:424/2330 train_time:25786ms step_avg:60.82ms
step:425/2330 train_time:25846ms step_avg:60.81ms
step:426/2330 train_time:25909ms step_avg:60.82ms
step:427/2330 train_time:25969ms step_avg:60.82ms
step:428/2330 train_time:26032ms step_avg:60.82ms
step:429/2330 train_time:26090ms step_avg:60.82ms
step:430/2330 train_time:26153ms step_avg:60.82ms
step:431/2330 train_time:26211ms step_avg:60.82ms
step:432/2330 train_time:26274ms step_avg:60.82ms
step:433/2330 train_time:26333ms step_avg:60.82ms
step:434/2330 train_time:26397ms step_avg:60.82ms
step:435/2330 train_time:26457ms step_avg:60.82ms
step:436/2330 train_time:26520ms step_avg:60.83ms
step:437/2330 train_time:26580ms step_avg:60.82ms
step:438/2330 train_time:26642ms step_avg:60.83ms
step:439/2330 train_time:26702ms step_avg:60.83ms
step:440/2330 train_time:26765ms step_avg:60.83ms
step:441/2330 train_time:26824ms step_avg:60.83ms
step:442/2330 train_time:26887ms step_avg:60.83ms
step:443/2330 train_time:26946ms step_avg:60.83ms
step:444/2330 train_time:27009ms step_avg:60.83ms
step:445/2330 train_time:27069ms step_avg:60.83ms
step:446/2330 train_time:27131ms step_avg:60.83ms
step:447/2330 train_time:27190ms step_avg:60.83ms
step:448/2330 train_time:27252ms step_avg:60.83ms
step:449/2330 train_time:27311ms step_avg:60.83ms
step:450/2330 train_time:27373ms step_avg:60.83ms
step:451/2330 train_time:27433ms step_avg:60.83ms
step:452/2330 train_time:27495ms step_avg:60.83ms
step:453/2330 train_time:27555ms step_avg:60.83ms
step:454/2330 train_time:27619ms step_avg:60.83ms
step:455/2330 train_time:27678ms step_avg:60.83ms
step:456/2330 train_time:27741ms step_avg:60.84ms
step:457/2330 train_time:27800ms step_avg:60.83ms
step:458/2330 train_time:27862ms step_avg:60.83ms
step:459/2330 train_time:27922ms step_avg:60.83ms
step:460/2330 train_time:27985ms step_avg:60.84ms
step:461/2330 train_time:28044ms step_avg:60.83ms
step:462/2330 train_time:28107ms step_avg:60.84ms
step:463/2330 train_time:28166ms step_avg:60.83ms
step:464/2330 train_time:28229ms step_avg:60.84ms
step:465/2330 train_time:28289ms step_avg:60.84ms
step:466/2330 train_time:28351ms step_avg:60.84ms
step:467/2330 train_time:28410ms step_avg:60.83ms
step:468/2330 train_time:28473ms step_avg:60.84ms
step:469/2330 train_time:28532ms step_avg:60.84ms
step:470/2330 train_time:28595ms step_avg:60.84ms
step:471/2330 train_time:28654ms step_avg:60.84ms
step:472/2330 train_time:28717ms step_avg:60.84ms
step:473/2330 train_time:28777ms step_avg:60.84ms
step:474/2330 train_time:28840ms step_avg:60.84ms
step:475/2330 train_time:28900ms step_avg:60.84ms
step:476/2330 train_time:28962ms step_avg:60.84ms
step:477/2330 train_time:29023ms step_avg:60.84ms
step:478/2330 train_time:29085ms step_avg:60.85ms
step:479/2330 train_time:29144ms step_avg:60.84ms
step:480/2330 train_time:29207ms step_avg:60.85ms
step:481/2330 train_time:29267ms step_avg:60.85ms
step:482/2330 train_time:29329ms step_avg:60.85ms
step:483/2330 train_time:29388ms step_avg:60.85ms
step:484/2330 train_time:29450ms step_avg:60.85ms
step:485/2330 train_time:29509ms step_avg:60.84ms
step:486/2330 train_time:29572ms step_avg:60.85ms
step:487/2330 train_time:29631ms step_avg:60.84ms
step:488/2330 train_time:29694ms step_avg:60.85ms
step:489/2330 train_time:29753ms step_avg:60.84ms
step:490/2330 train_time:29816ms step_avg:60.85ms
step:491/2330 train_time:29876ms step_avg:60.85ms
step:492/2330 train_time:29940ms step_avg:60.85ms
step:493/2330 train_time:29999ms step_avg:60.85ms
step:494/2330 train_time:30063ms step_avg:60.86ms
step:495/2330 train_time:30122ms step_avg:60.85ms
step:496/2330 train_time:30185ms step_avg:60.86ms
step:497/2330 train_time:30244ms step_avg:60.85ms
step:498/2330 train_time:30306ms step_avg:60.86ms
step:499/2330 train_time:30365ms step_avg:60.85ms
step:500/2330 train_time:30428ms step_avg:60.86ms
step:500/2330 val_loss:4.9931 train_time:30501ms step_avg:61.00ms
step:501/2330 train_time:30523ms step_avg:60.92ms
step:502/2330 train_time:30552ms step_avg:60.86ms
step:503/2330 train_time:30615ms step_avg:60.86ms
step:504/2330 train_time:30679ms step_avg:60.87ms
step:505/2330 train_time:30739ms step_avg:60.87ms
step:506/2330 train_time:30801ms step_avg:60.87ms
step:507/2330 train_time:30861ms step_avg:60.87ms
step:508/2330 train_time:30923ms step_avg:60.87ms
step:509/2330 train_time:30982ms step_avg:60.87ms
step:510/2330 train_time:31044ms step_avg:60.87ms
step:511/2330 train_time:31103ms step_avg:60.87ms
step:512/2330 train_time:31166ms step_avg:60.87ms
step:513/2330 train_time:31225ms step_avg:60.87ms
step:514/2330 train_time:31286ms step_avg:60.87ms
step:515/2330 train_time:31346ms step_avg:60.87ms
step:516/2330 train_time:31407ms step_avg:60.87ms
step:517/2330 train_time:31467ms step_avg:60.86ms
step:518/2330 train_time:31531ms step_avg:60.87ms
step:519/2330 train_time:31592ms step_avg:60.87ms
step:520/2330 train_time:31656ms step_avg:60.88ms
step:521/2330 train_time:31717ms step_avg:60.88ms
step:522/2330 train_time:31779ms step_avg:60.88ms
step:523/2330 train_time:31840ms step_avg:60.88ms
step:524/2330 train_time:31902ms step_avg:60.88ms
step:525/2330 train_time:31960ms step_avg:60.88ms
step:526/2330 train_time:32023ms step_avg:60.88ms
step:527/2330 train_time:32081ms step_avg:60.88ms
step:528/2330 train_time:32144ms step_avg:60.88ms
step:529/2330 train_time:32203ms step_avg:60.87ms
step:530/2330 train_time:32264ms step_avg:60.88ms
step:531/2330 train_time:32323ms step_avg:60.87ms
step:532/2330 train_time:32386ms step_avg:60.88ms
step:533/2330 train_time:32445ms step_avg:60.87ms
step:534/2330 train_time:32508ms step_avg:60.88ms
step:535/2330 train_time:32570ms step_avg:60.88ms
step:536/2330 train_time:32634ms step_avg:60.88ms
step:537/2330 train_time:32694ms step_avg:60.88ms
step:538/2330 train_time:32757ms step_avg:60.89ms
step:539/2330 train_time:32817ms step_avg:60.88ms
step:540/2330 train_time:32878ms step_avg:60.89ms
step:541/2330 train_time:32937ms step_avg:60.88ms
step:542/2330 train_time:33000ms step_avg:60.89ms
step:543/2330 train_time:33059ms step_avg:60.88ms
step:544/2330 train_time:33121ms step_avg:60.88ms
step:545/2330 train_time:33180ms step_avg:60.88ms
step:546/2330 train_time:33243ms step_avg:60.88ms
step:547/2330 train_time:33302ms step_avg:60.88ms
step:548/2330 train_time:33364ms step_avg:60.88ms
step:549/2330 train_time:33423ms step_avg:60.88ms
step:550/2330 train_time:33485ms step_avg:60.88ms
step:551/2330 train_time:33545ms step_avg:60.88ms
step:552/2330 train_time:33610ms step_avg:60.89ms
step:553/2330 train_time:33670ms step_avg:60.89ms
step:554/2330 train_time:33733ms step_avg:60.89ms
step:555/2330 train_time:33793ms step_avg:60.89ms
step:556/2330 train_time:33856ms step_avg:60.89ms
step:557/2330 train_time:33916ms step_avg:60.89ms
step:558/2330 train_time:33978ms step_avg:60.89ms
step:559/2330 train_time:34037ms step_avg:60.89ms
step:560/2330 train_time:34099ms step_avg:60.89ms
step:561/2330 train_time:34159ms step_avg:60.89ms
step:562/2330 train_time:34221ms step_avg:60.89ms
step:563/2330 train_time:34280ms step_avg:60.89ms
step:564/2330 train_time:34342ms step_avg:60.89ms
step:565/2330 train_time:34401ms step_avg:60.89ms
step:566/2330 train_time:34463ms step_avg:60.89ms
step:567/2330 train_time:34524ms step_avg:60.89ms
step:568/2330 train_time:34586ms step_avg:60.89ms
step:569/2330 train_time:34646ms step_avg:60.89ms
step:570/2330 train_time:34709ms step_avg:60.89ms
step:571/2330 train_time:34769ms step_avg:60.89ms
step:572/2330 train_time:34832ms step_avg:60.89ms
step:573/2330 train_time:34891ms step_avg:60.89ms
step:574/2330 train_time:34954ms step_avg:60.90ms
step:575/2330 train_time:35014ms step_avg:60.89ms
step:576/2330 train_time:35076ms step_avg:60.90ms
step:577/2330 train_time:35136ms step_avg:60.89ms
step:578/2330 train_time:35198ms step_avg:60.90ms
step:579/2330 train_time:35257ms step_avg:60.89ms
step:580/2330 train_time:35320ms step_avg:60.90ms
step:581/2330 train_time:35380ms step_avg:60.89ms
step:582/2330 train_time:35441ms step_avg:60.90ms
step:583/2330 train_time:35501ms step_avg:60.89ms
step:584/2330 train_time:35564ms step_avg:60.90ms
step:585/2330 train_time:35623ms step_avg:60.89ms
step:586/2330 train_time:35686ms step_avg:60.90ms
step:587/2330 train_time:35746ms step_avg:60.90ms
step:588/2330 train_time:35809ms step_avg:60.90ms
step:589/2330 train_time:35869ms step_avg:60.90ms
step:590/2330 train_time:35931ms step_avg:60.90ms
step:591/2330 train_time:35991ms step_avg:60.90ms
step:592/2330 train_time:36054ms step_avg:60.90ms
step:593/2330 train_time:36116ms step_avg:60.90ms
step:594/2330 train_time:36177ms step_avg:60.90ms
step:595/2330 train_time:36236ms step_avg:60.90ms
step:596/2330 train_time:36299ms step_avg:60.90ms
step:597/2330 train_time:36357ms step_avg:60.90ms
step:598/2330 train_time:36420ms step_avg:60.90ms
step:599/2330 train_time:36479ms step_avg:60.90ms
step:600/2330 train_time:36542ms step_avg:60.90ms
step:601/2330 train_time:36602ms step_avg:60.90ms
step:602/2330 train_time:36665ms step_avg:60.91ms
step:603/2330 train_time:36724ms step_avg:60.90ms
step:604/2330 train_time:36787ms step_avg:60.91ms
step:605/2330 train_time:36847ms step_avg:60.90ms
step:606/2330 train_time:36910ms step_avg:60.91ms
step:607/2330 train_time:36970ms step_avg:60.91ms
step:608/2330 train_time:37032ms step_avg:60.91ms
step:609/2330 train_time:37092ms step_avg:60.91ms
step:610/2330 train_time:37155ms step_avg:60.91ms
step:611/2330 train_time:37215ms step_avg:60.91ms
step:612/2330 train_time:37276ms step_avg:60.91ms
step:613/2330 train_time:37336ms step_avg:60.91ms
step:614/2330 train_time:37398ms step_avg:60.91ms
step:615/2330 train_time:37458ms step_avg:60.91ms
step:616/2330 train_time:37521ms step_avg:60.91ms
step:617/2330 train_time:37581ms step_avg:60.91ms
step:618/2330 train_time:37644ms step_avg:60.91ms
step:619/2330 train_time:37703ms step_avg:60.91ms
step:620/2330 train_time:37766ms step_avg:60.91ms
step:621/2330 train_time:37826ms step_avg:60.91ms
step:622/2330 train_time:37888ms step_avg:60.91ms
step:623/2330 train_time:37948ms step_avg:60.91ms
step:624/2330 train_time:38012ms step_avg:60.92ms
step:625/2330 train_time:38071ms step_avg:60.91ms
step:626/2330 train_time:38134ms step_avg:60.92ms
step:627/2330 train_time:38193ms step_avg:60.91ms
step:628/2330 train_time:38256ms step_avg:60.92ms
step:629/2330 train_time:38317ms step_avg:60.92ms
step:630/2330 train_time:38378ms step_avg:60.92ms
step:631/2330 train_time:38437ms step_avg:60.91ms
step:632/2330 train_time:38499ms step_avg:60.92ms
step:633/2330 train_time:38559ms step_avg:60.91ms
step:634/2330 train_time:38622ms step_avg:60.92ms
step:635/2330 train_time:38681ms step_avg:60.92ms
step:636/2330 train_time:38744ms step_avg:60.92ms
step:637/2330 train_time:38803ms step_avg:60.92ms
step:638/2330 train_time:38866ms step_avg:60.92ms
step:639/2330 train_time:38925ms step_avg:60.92ms
step:640/2330 train_time:38988ms step_avg:60.92ms
step:641/2330 train_time:39048ms step_avg:60.92ms
step:642/2330 train_time:39111ms step_avg:60.92ms
step:643/2330 train_time:39171ms step_avg:60.92ms
step:644/2330 train_time:39234ms step_avg:60.92ms
step:645/2330 train_time:39293ms step_avg:60.92ms
step:646/2330 train_time:39356ms step_avg:60.92ms
step:647/2330 train_time:39416ms step_avg:60.92ms
step:648/2330 train_time:39479ms step_avg:60.92ms
step:649/2330 train_time:39537ms step_avg:60.92ms
step:650/2330 train_time:39600ms step_avg:60.92ms
step:651/2330 train_time:39660ms step_avg:60.92ms
step:652/2330 train_time:39723ms step_avg:60.93ms
step:653/2330 train_time:39783ms step_avg:60.92ms
step:654/2330 train_time:39845ms step_avg:60.93ms
step:655/2330 train_time:39904ms step_avg:60.92ms
step:656/2330 train_time:39967ms step_avg:60.92ms
step:657/2330 train_time:40026ms step_avg:60.92ms
step:658/2330 train_time:40089ms step_avg:60.93ms
step:659/2330 train_time:40148ms step_avg:60.92ms
step:660/2330 train_time:40211ms step_avg:60.93ms
step:661/2330 train_time:40271ms step_avg:60.92ms
step:662/2330 train_time:40334ms step_avg:60.93ms
step:663/2330 train_time:40394ms step_avg:60.93ms
step:664/2330 train_time:40456ms step_avg:60.93ms
step:665/2330 train_time:40516ms step_avg:60.93ms
step:666/2330 train_time:40578ms step_avg:60.93ms
step:667/2330 train_time:40638ms step_avg:60.93ms
step:668/2330 train_time:40700ms step_avg:60.93ms
step:669/2330 train_time:40759ms step_avg:60.93ms
step:670/2330 train_time:40823ms step_avg:60.93ms
step:671/2330 train_time:40883ms step_avg:60.93ms
step:672/2330 train_time:40946ms step_avg:60.93ms
step:673/2330 train_time:41005ms step_avg:60.93ms
step:674/2330 train_time:41066ms step_avg:60.93ms
step:675/2330 train_time:41125ms step_avg:60.93ms
step:676/2330 train_time:41187ms step_avg:60.93ms
step:677/2330 train_time:41247ms step_avg:60.93ms
step:678/2330 train_time:41311ms step_avg:60.93ms
step:679/2330 train_time:41371ms step_avg:60.93ms
step:680/2330 train_time:41433ms step_avg:60.93ms
step:681/2330 train_time:41493ms step_avg:60.93ms
step:682/2330 train_time:41556ms step_avg:60.93ms
step:683/2330 train_time:41615ms step_avg:60.93ms
step:684/2330 train_time:41677ms step_avg:60.93ms
step:685/2330 train_time:41737ms step_avg:60.93ms
step:686/2330 train_time:41799ms step_avg:60.93ms
step:687/2330 train_time:41860ms step_avg:60.93ms
step:688/2330 train_time:41924ms step_avg:60.94ms
step:689/2330 train_time:41982ms step_avg:60.93ms
step:690/2330 train_time:42044ms step_avg:60.93ms
step:691/2330 train_time:42103ms step_avg:60.93ms
step:692/2330 train_time:42166ms step_avg:60.93ms
step:693/2330 train_time:42225ms step_avg:60.93ms
step:694/2330 train_time:42288ms step_avg:60.93ms
step:695/2330 train_time:42348ms step_avg:60.93ms
step:696/2330 train_time:42413ms step_avg:60.94ms
step:697/2330 train_time:42472ms step_avg:60.94ms
step:698/2330 train_time:42535ms step_avg:60.94ms
step:699/2330 train_time:42595ms step_avg:60.94ms
step:700/2330 train_time:42657ms step_avg:60.94ms
step:701/2330 train_time:42716ms step_avg:60.94ms
step:702/2330 train_time:42777ms step_avg:60.94ms
step:703/2330 train_time:42837ms step_avg:60.93ms
step:704/2330 train_time:42899ms step_avg:60.94ms
step:705/2330 train_time:42959ms step_avg:60.94ms
step:706/2330 train_time:43022ms step_avg:60.94ms
step:707/2330 train_time:43081ms step_avg:60.94ms
step:708/2330 train_time:43144ms step_avg:60.94ms
step:709/2330 train_time:43203ms step_avg:60.93ms
step:710/2330 train_time:43265ms step_avg:60.94ms
step:711/2330 train_time:43324ms step_avg:60.93ms
step:712/2330 train_time:43388ms step_avg:60.94ms
step:713/2330 train_time:43449ms step_avg:60.94ms
step:714/2330 train_time:43514ms step_avg:60.94ms
step:715/2330 train_time:43572ms step_avg:60.94ms
step:716/2330 train_time:43635ms step_avg:60.94ms
step:717/2330 train_time:43694ms step_avg:60.94ms
step:718/2330 train_time:43757ms step_avg:60.94ms
step:719/2330 train_time:43816ms step_avg:60.94ms
step:720/2330 train_time:43878ms step_avg:60.94ms
step:721/2330 train_time:43938ms step_avg:60.94ms
step:722/2330 train_time:44001ms step_avg:60.94ms
step:723/2330 train_time:44060ms step_avg:60.94ms
step:724/2330 train_time:44122ms step_avg:60.94ms
step:725/2330 train_time:44182ms step_avg:60.94ms
step:726/2330 train_time:44245ms step_avg:60.94ms
step:727/2330 train_time:44304ms step_avg:60.94ms
step:728/2330 train_time:44367ms step_avg:60.94ms
step:729/2330 train_time:44427ms step_avg:60.94ms
step:730/2330 train_time:44490ms step_avg:60.94ms
step:731/2330 train_time:44550ms step_avg:60.94ms
step:732/2330 train_time:44614ms step_avg:60.95ms
step:733/2330 train_time:44672ms step_avg:60.94ms
step:734/2330 train_time:44735ms step_avg:60.95ms
step:735/2330 train_time:44794ms step_avg:60.94ms
step:736/2330 train_time:44856ms step_avg:60.95ms
step:737/2330 train_time:44917ms step_avg:60.95ms
step:738/2330 train_time:44979ms step_avg:60.95ms
step:739/2330 train_time:45038ms step_avg:60.95ms
step:740/2330 train_time:45101ms step_avg:60.95ms
step:741/2330 train_time:45160ms step_avg:60.94ms
step:742/2330 train_time:45224ms step_avg:60.95ms
step:743/2330 train_time:45283ms step_avg:60.95ms
step:744/2330 train_time:45345ms step_avg:60.95ms
step:745/2330 train_time:45404ms step_avg:60.95ms
step:746/2330 train_time:45467ms step_avg:60.95ms
step:747/2330 train_time:45526ms step_avg:60.95ms
step:748/2330 train_time:45589ms step_avg:60.95ms
step:749/2330 train_time:45649ms step_avg:60.95ms
step:750/2330 train_time:45713ms step_avg:60.95ms
step:750/2330 val_loss:4.7031 train_time:45785ms step_avg:61.05ms
step:751/2330 train_time:45807ms step_avg:60.99ms
step:752/2330 train_time:45837ms step_avg:60.95ms
step:753/2330 train_time:45898ms step_avg:60.95ms
step:754/2330 train_time:45964ms step_avg:60.96ms
step:755/2330 train_time:46025ms step_avg:60.96ms
step:756/2330 train_time:46087ms step_avg:60.96ms
step:757/2330 train_time:46146ms step_avg:60.96ms
step:758/2330 train_time:46208ms step_avg:60.96ms
step:759/2330 train_time:46266ms step_avg:60.96ms
step:760/2330 train_time:46327ms step_avg:60.96ms
step:761/2330 train_time:46385ms step_avg:60.95ms
step:762/2330 train_time:46447ms step_avg:60.95ms
step:763/2330 train_time:46505ms step_avg:60.95ms
step:764/2330 train_time:46567ms step_avg:60.95ms
step:765/2330 train_time:46627ms step_avg:60.95ms
step:766/2330 train_time:46689ms step_avg:60.95ms
step:767/2330 train_time:46749ms step_avg:60.95ms
step:768/2330 train_time:46813ms step_avg:60.95ms
step:769/2330 train_time:46874ms step_avg:60.95ms
step:770/2330 train_time:46939ms step_avg:60.96ms
step:771/2330 train_time:47000ms step_avg:60.96ms
step:772/2330 train_time:47064ms step_avg:60.96ms
step:773/2330 train_time:47124ms step_avg:60.96ms
step:774/2330 train_time:47187ms step_avg:60.97ms
step:775/2330 train_time:47247ms step_avg:60.96ms
step:776/2330 train_time:47310ms step_avg:60.97ms
step:777/2330 train_time:47369ms step_avg:60.96ms
step:778/2330 train_time:47432ms step_avg:60.97ms
step:779/2330 train_time:47491ms step_avg:60.96ms
step:780/2330 train_time:47555ms step_avg:60.97ms
step:781/2330 train_time:47614ms step_avg:60.97ms
step:782/2330 train_time:47677ms step_avg:60.97ms
step:783/2330 train_time:47738ms step_avg:60.97ms
step:784/2330 train_time:47801ms step_avg:60.97ms
step:785/2330 train_time:47862ms step_avg:60.97ms
step:786/2330 train_time:47925ms step_avg:60.97ms
step:787/2330 train_time:47985ms step_avg:60.97ms
step:788/2330 train_time:48048ms step_avg:60.98ms
step:789/2330 train_time:48108ms step_avg:60.97ms
step:790/2330 train_time:48172ms step_avg:60.98ms
step:791/2330 train_time:48232ms step_avg:60.98ms
step:792/2330 train_time:48295ms step_avg:60.98ms
step:793/2330 train_time:48355ms step_avg:60.98ms
step:794/2330 train_time:48419ms step_avg:60.98ms
step:795/2330 train_time:48478ms step_avg:60.98ms
step:796/2330 train_time:48542ms step_avg:60.98ms
step:797/2330 train_time:48601ms step_avg:60.98ms
step:798/2330 train_time:48664ms step_avg:60.98ms
step:799/2330 train_time:48723ms step_avg:60.98ms
step:800/2330 train_time:48787ms step_avg:60.98ms
step:801/2330 train_time:48848ms step_avg:60.98ms
step:802/2330 train_time:48911ms step_avg:60.99ms
step:803/2330 train_time:48972ms step_avg:60.99ms
step:804/2330 train_time:49035ms step_avg:60.99ms
step:805/2330 train_time:49097ms step_avg:60.99ms
step:806/2330 train_time:49161ms step_avg:60.99ms
step:807/2330 train_time:49221ms step_avg:60.99ms
step:808/2330 train_time:49283ms step_avg:60.99ms
step:809/2330 train_time:49343ms step_avg:60.99ms
step:810/2330 train_time:49406ms step_avg:60.99ms
step:811/2330 train_time:49466ms step_avg:60.99ms
step:812/2330 train_time:49528ms step_avg:61.00ms
step:813/2330 train_time:49588ms step_avg:60.99ms
step:814/2330 train_time:49651ms step_avg:61.00ms
step:815/2330 train_time:49710ms step_avg:60.99ms
step:816/2330 train_time:49774ms step_avg:61.00ms
step:817/2330 train_time:49836ms step_avg:61.00ms
step:818/2330 train_time:49899ms step_avg:61.00ms
step:819/2330 train_time:49959ms step_avg:61.00ms
step:820/2330 train_time:50022ms step_avg:61.00ms
step:821/2330 train_time:50082ms step_avg:61.00ms
step:822/2330 train_time:50146ms step_avg:61.01ms
step:823/2330 train_time:50205ms step_avg:61.00ms
step:824/2330 train_time:50268ms step_avg:61.01ms
step:825/2330 train_time:50329ms step_avg:61.00ms
step:826/2330 train_time:50393ms step_avg:61.01ms
step:827/2330 train_time:50452ms step_avg:61.01ms
step:828/2330 train_time:50515ms step_avg:61.01ms
step:829/2330 train_time:50575ms step_avg:61.01ms
step:830/2330 train_time:50639ms step_avg:61.01ms
step:831/2330 train_time:50698ms step_avg:61.01ms
step:832/2330 train_time:50762ms step_avg:61.01ms
step:833/2330 train_time:50822ms step_avg:61.01ms
step:834/2330 train_time:50884ms step_avg:61.01ms
step:835/2330 train_time:50945ms step_avg:61.01ms
step:836/2330 train_time:51007ms step_avg:61.01ms
step:837/2330 train_time:51068ms step_avg:61.01ms
step:838/2330 train_time:51131ms step_avg:61.02ms
step:839/2330 train_time:51191ms step_avg:61.01ms
step:840/2330 train_time:51255ms step_avg:61.02ms
step:841/2330 train_time:51316ms step_avg:61.02ms
step:842/2330 train_time:51380ms step_avg:61.02ms
step:843/2330 train_time:51441ms step_avg:61.02ms
step:844/2330 train_time:51503ms step_avg:61.02ms
step:845/2330 train_time:51562ms step_avg:61.02ms
step:846/2330 train_time:51625ms step_avg:61.02ms
step:847/2330 train_time:51686ms step_avg:61.02ms
step:848/2330 train_time:51749ms step_avg:61.03ms
step:849/2330 train_time:51808ms step_avg:61.02ms
step:850/2330 train_time:51871ms step_avg:61.02ms
step:851/2330 train_time:51931ms step_avg:61.02ms
step:852/2330 train_time:51995ms step_avg:61.03ms
step:853/2330 train_time:52057ms step_avg:61.03ms
step:854/2330 train_time:52120ms step_avg:61.03ms
step:855/2330 train_time:52179ms step_avg:61.03ms
step:856/2330 train_time:52243ms step_avg:61.03ms
step:857/2330 train_time:52302ms step_avg:61.03ms
step:858/2330 train_time:52365ms step_avg:61.03ms
step:859/2330 train_time:52426ms step_avg:61.03ms
step:860/2330 train_time:52488ms step_avg:61.03ms
step:861/2330 train_time:52549ms step_avg:61.03ms
step:862/2330 train_time:52611ms step_avg:61.03ms
step:863/2330 train_time:52672ms step_avg:61.03ms
step:864/2330 train_time:52735ms step_avg:61.04ms
step:865/2330 train_time:52796ms step_avg:61.04ms
step:866/2330 train_time:52860ms step_avg:61.04ms
step:867/2330 train_time:52920ms step_avg:61.04ms
step:868/2330 train_time:52983ms step_avg:61.04ms
step:869/2330 train_time:53043ms step_avg:61.04ms
step:870/2330 train_time:53105ms step_avg:61.04ms
step:871/2330 train_time:53165ms step_avg:61.04ms
step:872/2330 train_time:53228ms step_avg:61.04ms
step:873/2330 train_time:53288ms step_avg:61.04ms
step:874/2330 train_time:53352ms step_avg:61.04ms
step:875/2330 train_time:53412ms step_avg:61.04ms
step:876/2330 train_time:53475ms step_avg:61.04ms
step:877/2330 train_time:53537ms step_avg:61.05ms
step:878/2330 train_time:53600ms step_avg:61.05ms
step:879/2330 train_time:53660ms step_avg:61.05ms
step:880/2330 train_time:53723ms step_avg:61.05ms
step:881/2330 train_time:53783ms step_avg:61.05ms
step:882/2330 train_time:53846ms step_avg:61.05ms
step:883/2330 train_time:53906ms step_avg:61.05ms
step:884/2330 train_time:53969ms step_avg:61.05ms
step:885/2330 train_time:54029ms step_avg:61.05ms
step:886/2330 train_time:54091ms step_avg:61.05ms
step:887/2330 train_time:54151ms step_avg:61.05ms
step:888/2330 train_time:54215ms step_avg:61.05ms
step:889/2330 train_time:54276ms step_avg:61.05ms
step:890/2330 train_time:54341ms step_avg:61.06ms
step:891/2330 train_time:54400ms step_avg:61.05ms
step:892/2330 train_time:54463ms step_avg:61.06ms
step:893/2330 train_time:54522ms step_avg:61.06ms
step:894/2330 train_time:54585ms step_avg:61.06ms
step:895/2330 train_time:54645ms step_avg:61.06ms
step:896/2330 train_time:54708ms step_avg:61.06ms
step:897/2330 train_time:54768ms step_avg:61.06ms
step:898/2330 train_time:54831ms step_avg:61.06ms
step:899/2330 train_time:54890ms step_avg:61.06ms
step:900/2330 train_time:54953ms step_avg:61.06ms
step:901/2330 train_time:55013ms step_avg:61.06ms
step:902/2330 train_time:55076ms step_avg:61.06ms
step:903/2330 train_time:55136ms step_avg:61.06ms
step:904/2330 train_time:55199ms step_avg:61.06ms
step:905/2330 train_time:55259ms step_avg:61.06ms
step:906/2330 train_time:55322ms step_avg:61.06ms
step:907/2330 train_time:55381ms step_avg:61.06ms
step:908/2330 train_time:55445ms step_avg:61.06ms
step:909/2330 train_time:55504ms step_avg:61.06ms
step:910/2330 train_time:55567ms step_avg:61.06ms
step:911/2330 train_time:55626ms step_avg:61.06ms
step:912/2330 train_time:55690ms step_avg:61.06ms
step:913/2330 train_time:55750ms step_avg:61.06ms
step:914/2330 train_time:55813ms step_avg:61.06ms
step:915/2330 train_time:55874ms step_avg:61.06ms
step:916/2330 train_time:55937ms step_avg:61.07ms
step:917/2330 train_time:55997ms step_avg:61.07ms
step:918/2330 train_time:56060ms step_avg:61.07ms
step:919/2330 train_time:56121ms step_avg:61.07ms
step:920/2330 train_time:56182ms step_avg:61.07ms
step:921/2330 train_time:56242ms step_avg:61.07ms
step:922/2330 train_time:56305ms step_avg:61.07ms
step:923/2330 train_time:56365ms step_avg:61.07ms
step:924/2330 train_time:56427ms step_avg:61.07ms
step:925/2330 train_time:56487ms step_avg:61.07ms
step:926/2330 train_time:56550ms step_avg:61.07ms
step:927/2330 train_time:56609ms step_avg:61.07ms
step:928/2330 train_time:56672ms step_avg:61.07ms
step:929/2330 train_time:56732ms step_avg:61.07ms
step:930/2330 train_time:56795ms step_avg:61.07ms
step:931/2330 train_time:56856ms step_avg:61.07ms
step:932/2330 train_time:56918ms step_avg:61.07ms
step:933/2330 train_time:56979ms step_avg:61.07ms
step:934/2330 train_time:57042ms step_avg:61.07ms
step:935/2330 train_time:57101ms step_avg:61.07ms
step:936/2330 train_time:57164ms step_avg:61.07ms
step:937/2330 train_time:57223ms step_avg:61.07ms
step:938/2330 train_time:57286ms step_avg:61.07ms
step:939/2330 train_time:57347ms step_avg:61.07ms
step:940/2330 train_time:57410ms step_avg:61.07ms
step:941/2330 train_time:57469ms step_avg:61.07ms
step:942/2330 train_time:57532ms step_avg:61.07ms
step:943/2330 train_time:57591ms step_avg:61.07ms
step:944/2330 train_time:57654ms step_avg:61.07ms
step:945/2330 train_time:57714ms step_avg:61.07ms
step:946/2330 train_time:57777ms step_avg:61.07ms
step:947/2330 train_time:57838ms step_avg:61.07ms
step:948/2330 train_time:57900ms step_avg:61.08ms
step:949/2330 train_time:57960ms step_avg:61.07ms
step:950/2330 train_time:58023ms step_avg:61.08ms
step:951/2330 train_time:58083ms step_avg:61.08ms
step:952/2330 train_time:58145ms step_avg:61.08ms
step:953/2330 train_time:58205ms step_avg:61.08ms
step:954/2330 train_time:58269ms step_avg:61.08ms
step:955/2330 train_time:58328ms step_avg:61.08ms
step:956/2330 train_time:58392ms step_avg:61.08ms
step:957/2330 train_time:58451ms step_avg:61.08ms
step:958/2330 train_time:58514ms step_avg:61.08ms
step:959/2330 train_time:58574ms step_avg:61.08ms
step:960/2330 train_time:58637ms step_avg:61.08ms
step:961/2330 train_time:58697ms step_avg:61.08ms
step:962/2330 train_time:58760ms step_avg:61.08ms
step:963/2330 train_time:58820ms step_avg:61.08ms
step:964/2330 train_time:58883ms step_avg:61.08ms
step:965/2330 train_time:58943ms step_avg:61.08ms
step:966/2330 train_time:59006ms step_avg:61.08ms
step:967/2330 train_time:59065ms step_avg:61.08ms
step:968/2330 train_time:59128ms step_avg:61.08ms
step:969/2330 train_time:59188ms step_avg:61.08ms
step:970/2330 train_time:59252ms step_avg:61.08ms
step:971/2330 train_time:59311ms step_avg:61.08ms
step:972/2330 train_time:59374ms step_avg:61.08ms
step:973/2330 train_time:59435ms step_avg:61.08ms
step:974/2330 train_time:59498ms step_avg:61.09ms
step:975/2330 train_time:59558ms step_avg:61.09ms
step:976/2330 train_time:59621ms step_avg:61.09ms
step:977/2330 train_time:59681ms step_avg:61.09ms
step:978/2330 train_time:59744ms step_avg:61.09ms
step:979/2330 train_time:59803ms step_avg:61.09ms
step:980/2330 train_time:59865ms step_avg:61.09ms
step:981/2330 train_time:59925ms step_avg:61.09ms
step:982/2330 train_time:59989ms step_avg:61.09ms
step:983/2330 train_time:60049ms step_avg:61.09ms
step:984/2330 train_time:60112ms step_avg:61.09ms
step:985/2330 train_time:60171ms step_avg:61.09ms
step:986/2330 train_time:60234ms step_avg:61.09ms
step:987/2330 train_time:60294ms step_avg:61.09ms
step:988/2330 train_time:60357ms step_avg:61.09ms
step:989/2330 train_time:60417ms step_avg:61.09ms
step:990/2330 train_time:60480ms step_avg:61.09ms
step:991/2330 train_time:60541ms step_avg:61.09ms
step:992/2330 train_time:60603ms step_avg:61.09ms
step:993/2330 train_time:60663ms step_avg:61.09ms
step:994/2330 train_time:60726ms step_avg:61.09ms
step:995/2330 train_time:60786ms step_avg:61.09ms
step:996/2330 train_time:60849ms step_avg:61.09ms
step:997/2330 train_time:60909ms step_avg:61.09ms
step:998/2330 train_time:60972ms step_avg:61.09ms
step:999/2330 train_time:61033ms step_avg:61.09ms
step:1000/2330 train_time:61096ms step_avg:61.10ms
step:1000/2330 val_loss:4.4326 train_time:61169ms step_avg:61.17ms
step:1001/2330 train_time:61192ms step_avg:61.13ms
step:1002/2330 train_time:61221ms step_avg:61.10ms
step:1003/2330 train_time:61283ms step_avg:61.10ms
step:1004/2330 train_time:61352ms step_avg:61.11ms
step:1005/2330 train_time:61414ms step_avg:61.11ms
step:1006/2330 train_time:61478ms step_avg:61.11ms
step:1007/2330 train_time:61539ms step_avg:61.11ms
step:1008/2330 train_time:61601ms step_avg:61.11ms
step:1009/2330 train_time:61661ms step_avg:61.11ms
step:1010/2330 train_time:61723ms step_avg:61.11ms
step:1011/2330 train_time:61782ms step_avg:61.11ms
step:1012/2330 train_time:61845ms step_avg:61.11ms
step:1013/2330 train_time:61904ms step_avg:61.11ms
step:1014/2330 train_time:61966ms step_avg:61.11ms
step:1015/2330 train_time:62025ms step_avg:61.11ms
step:1016/2330 train_time:62088ms step_avg:61.11ms
step:1017/2330 train_time:62149ms step_avg:61.11ms
step:1018/2330 train_time:62214ms step_avg:61.11ms
step:1019/2330 train_time:62275ms step_avg:61.11ms
step:1020/2330 train_time:62339ms step_avg:61.12ms
step:1021/2330 train_time:62400ms step_avg:61.12ms
step:1022/2330 train_time:62464ms step_avg:61.12ms
step:1023/2330 train_time:62524ms step_avg:61.12ms
step:1024/2330 train_time:62588ms step_avg:61.12ms
step:1025/2330 train_time:62648ms step_avg:61.12ms
step:1026/2330 train_time:62710ms step_avg:61.12ms
step:1027/2330 train_time:62770ms step_avg:61.12ms
step:1028/2330 train_time:62833ms step_avg:61.12ms
step:1029/2330 train_time:62893ms step_avg:61.12ms
step:1030/2330 train_time:62955ms step_avg:61.12ms
step:1031/2330 train_time:63016ms step_avg:61.12ms
step:1032/2330 train_time:63078ms step_avg:61.12ms
step:1033/2330 train_time:63138ms step_avg:61.12ms
step:1034/2330 train_time:63201ms step_avg:61.12ms
step:1035/2330 train_time:63263ms step_avg:61.12ms
step:1036/2330 train_time:63328ms step_avg:61.13ms
step:1037/2330 train_time:63388ms step_avg:61.13ms
step:1038/2330 train_time:63452ms step_avg:61.13ms
step:1039/2330 train_time:63513ms step_avg:61.13ms
step:1040/2330 train_time:63576ms step_avg:61.13ms
step:1041/2330 train_time:63636ms step_avg:61.13ms
step:1042/2330 train_time:63699ms step_avg:61.13ms
step:1043/2330 train_time:63759ms step_avg:61.13ms
step:1044/2330 train_time:63822ms step_avg:61.13ms
step:1045/2330 train_time:63883ms step_avg:61.13ms
step:1046/2330 train_time:63946ms step_avg:61.13ms
step:1047/2330 train_time:64006ms step_avg:61.13ms
step:1048/2330 train_time:64068ms step_avg:61.13ms
step:1049/2330 train_time:64128ms step_avg:61.13ms
step:1050/2330 train_time:64193ms step_avg:61.14ms
step:1051/2330 train_time:64252ms step_avg:61.13ms
step:1052/2330 train_time:64316ms step_avg:61.14ms
step:1053/2330 train_time:64376ms step_avg:61.14ms
step:1054/2330 train_time:64440ms step_avg:61.14ms
step:1055/2330 train_time:64500ms step_avg:61.14ms
step:1056/2330 train_time:64563ms step_avg:61.14ms
step:1057/2330 train_time:64623ms step_avg:61.14ms
step:1058/2330 train_time:64686ms step_avg:61.14ms
step:1059/2330 train_time:64747ms step_avg:61.14ms
step:1060/2330 train_time:64810ms step_avg:61.14ms
step:1061/2330 train_time:64870ms step_avg:61.14ms
step:1062/2330 train_time:64932ms step_avg:61.14ms
step:1063/2330 train_time:64992ms step_avg:61.14ms
step:1064/2330 train_time:65054ms step_avg:61.14ms
step:1065/2330 train_time:65114ms step_avg:61.14ms
step:1066/2330 train_time:65177ms step_avg:61.14ms
step:1067/2330 train_time:65237ms step_avg:61.14ms
step:1068/2330 train_time:65301ms step_avg:61.14ms
step:1069/2330 train_time:65361ms step_avg:61.14ms
step:1070/2330 train_time:65425ms step_avg:61.14ms
step:1071/2330 train_time:65485ms step_avg:61.14ms
step:1072/2330 train_time:65547ms step_avg:61.15ms
step:1073/2330 train_time:65608ms step_avg:61.14ms
step:1074/2330 train_time:65672ms step_avg:61.15ms
step:1075/2330 train_time:65732ms step_avg:61.15ms
step:1076/2330 train_time:65794ms step_avg:61.15ms
step:1077/2330 train_time:65855ms step_avg:61.15ms
step:1078/2330 train_time:65919ms step_avg:61.15ms
step:1079/2330 train_time:65978ms step_avg:61.15ms
step:1080/2330 train_time:66040ms step_avg:61.15ms
step:1081/2330 train_time:66099ms step_avg:61.15ms
step:1082/2330 train_time:66163ms step_avg:61.15ms
step:1083/2330 train_time:66223ms step_avg:61.15ms
step:1084/2330 train_time:66286ms step_avg:61.15ms
step:1085/2330 train_time:66345ms step_avg:61.15ms
step:1086/2330 train_time:66409ms step_avg:61.15ms
step:1087/2330 train_time:66469ms step_avg:61.15ms
step:1088/2330 train_time:66532ms step_avg:61.15ms
step:1089/2330 train_time:66593ms step_avg:61.15ms
step:1090/2330 train_time:66655ms step_avg:61.15ms
step:1091/2330 train_time:66716ms step_avg:61.15ms
step:1092/2330 train_time:66779ms step_avg:61.15ms
step:1093/2330 train_time:66840ms step_avg:61.15ms
step:1094/2330 train_time:66903ms step_avg:61.15ms
step:1095/2330 train_time:66963ms step_avg:61.15ms
step:1096/2330 train_time:67027ms step_avg:61.16ms
step:1097/2330 train_time:67086ms step_avg:61.15ms
step:1098/2330 train_time:67149ms step_avg:61.16ms
step:1099/2330 train_time:67211ms step_avg:61.16ms
step:1100/2330 train_time:67274ms step_avg:61.16ms
step:1101/2330 train_time:67334ms step_avg:61.16ms
step:1102/2330 train_time:67396ms step_avg:61.16ms
step:1103/2330 train_time:67456ms step_avg:61.16ms
step:1104/2330 train_time:67519ms step_avg:61.16ms
step:1105/2330 train_time:67579ms step_avg:61.16ms
step:1106/2330 train_time:67642ms step_avg:61.16ms
step:1107/2330 train_time:67703ms step_avg:61.16ms
step:1108/2330 train_time:67766ms step_avg:61.16ms
step:1109/2330 train_time:67826ms step_avg:61.16ms
step:1110/2330 train_time:67890ms step_avg:61.16ms
step:1111/2330 train_time:67951ms step_avg:61.16ms
step:1112/2330 train_time:68014ms step_avg:61.16ms
step:1113/2330 train_time:68074ms step_avg:61.16ms
step:1114/2330 train_time:68137ms step_avg:61.16ms
step:1115/2330 train_time:68196ms step_avg:61.16ms
step:1116/2330 train_time:68259ms step_avg:61.16ms
step:1117/2330 train_time:68319ms step_avg:61.16ms
step:1118/2330 train_time:68382ms step_avg:61.16ms
step:1119/2330 train_time:68443ms step_avg:61.16ms
step:1120/2330 train_time:68506ms step_avg:61.17ms
step:1121/2330 train_time:68566ms step_avg:61.17ms
step:1122/2330 train_time:68630ms step_avg:61.17ms
step:1123/2330 train_time:68690ms step_avg:61.17ms
step:1124/2330 train_time:68754ms step_avg:61.17ms
step:1125/2330 train_time:68815ms step_avg:61.17ms
step:1126/2330 train_time:68877ms step_avg:61.17ms
step:1127/2330 train_time:68937ms step_avg:61.17ms
step:1128/2330 train_time:69000ms step_avg:61.17ms
step:1129/2330 train_time:69059ms step_avg:61.17ms
step:1130/2330 train_time:69122ms step_avg:61.17ms
step:1131/2330 train_time:69183ms step_avg:61.17ms
step:1132/2330 train_time:69246ms step_avg:61.17ms
step:1133/2330 train_time:69307ms step_avg:61.17ms
step:1134/2330 train_time:69370ms step_avg:61.17ms
step:1135/2330 train_time:69431ms step_avg:61.17ms
step:1136/2330 train_time:69494ms step_avg:61.17ms
step:1137/2330 train_time:69554ms step_avg:61.17ms
step:1138/2330 train_time:69617ms step_avg:61.17ms
step:1139/2330 train_time:69677ms step_avg:61.17ms
step:1140/2330 train_time:69740ms step_avg:61.18ms
step:1141/2330 train_time:69800ms step_avg:61.17ms
step:1142/2330 train_time:69863ms step_avg:61.18ms
step:1143/2330 train_time:69923ms step_avg:61.18ms
step:1144/2330 train_time:69987ms step_avg:61.18ms
step:1145/2330 train_time:70046ms step_avg:61.18ms
step:1146/2330 train_time:70109ms step_avg:61.18ms
step:1147/2330 train_time:70170ms step_avg:61.18ms
step:1148/2330 train_time:70233ms step_avg:61.18ms
step:1149/2330 train_time:70293ms step_avg:61.18ms
step:1150/2330 train_time:70356ms step_avg:61.18ms
step:1151/2330 train_time:70416ms step_avg:61.18ms
step:1152/2330 train_time:70479ms step_avg:61.18ms
step:1153/2330 train_time:70539ms step_avg:61.18ms
step:1154/2330 train_time:70603ms step_avg:61.18ms
step:1155/2330 train_time:70663ms step_avg:61.18ms
step:1156/2330 train_time:70726ms step_avg:61.18ms
step:1157/2330 train_time:70787ms step_avg:61.18ms
step:1158/2330 train_time:70850ms step_avg:61.18ms
step:1159/2330 train_time:70912ms step_avg:61.18ms
step:1160/2330 train_time:70974ms step_avg:61.18ms
step:1161/2330 train_time:71034ms step_avg:61.18ms
step:1162/2330 train_time:71097ms step_avg:61.18ms
step:1163/2330 train_time:71156ms step_avg:61.18ms
step:1164/2330 train_time:71220ms step_avg:61.19ms
step:1165/2330 train_time:71281ms step_avg:61.19ms
step:1166/2330 train_time:71344ms step_avg:61.19ms
step:1167/2330 train_time:71404ms step_avg:61.19ms
step:1168/2330 train_time:71467ms step_avg:61.19ms
step:1169/2330 train_time:71527ms step_avg:61.19ms
step:1170/2330 train_time:71591ms step_avg:61.19ms
step:1171/2330 train_time:71652ms step_avg:61.19ms
step:1172/2330 train_time:71715ms step_avg:61.19ms
step:1173/2330 train_time:71775ms step_avg:61.19ms
step:1174/2330 train_time:71838ms step_avg:61.19ms
step:1175/2330 train_time:71898ms step_avg:61.19ms
step:1176/2330 train_time:71961ms step_avg:61.19ms
step:1177/2330 train_time:72021ms step_avg:61.19ms
step:1178/2330 train_time:72084ms step_avg:61.19ms
step:1179/2330 train_time:72144ms step_avg:61.19ms
step:1180/2330 train_time:72207ms step_avg:61.19ms
step:1181/2330 train_time:72268ms step_avg:61.19ms
step:1182/2330 train_time:72331ms step_avg:61.19ms
step:1183/2330 train_time:72392ms step_avg:61.19ms
step:1184/2330 train_time:72455ms step_avg:61.19ms
step:1185/2330 train_time:72514ms step_avg:61.19ms
step:1186/2330 train_time:72577ms step_avg:61.19ms
step:1187/2330 train_time:72637ms step_avg:61.19ms
step:1188/2330 train_time:72701ms step_avg:61.20ms
step:1189/2330 train_time:72760ms step_avg:61.19ms
step:1190/2330 train_time:72824ms step_avg:61.20ms
step:1191/2330 train_time:72883ms step_avg:61.20ms
step:1192/2330 train_time:72947ms step_avg:61.20ms
step:1193/2330 train_time:73007ms step_avg:61.20ms
step:1194/2330 train_time:73071ms step_avg:61.20ms
step:1195/2330 train_time:73131ms step_avg:61.20ms
step:1196/2330 train_time:73193ms step_avg:61.20ms
step:1197/2330 train_time:73253ms step_avg:61.20ms
step:1198/2330 train_time:73317ms step_avg:61.20ms
step:1199/2330 train_time:73377ms step_avg:61.20ms
step:1200/2330 train_time:73439ms step_avg:61.20ms
step:1201/2330 train_time:73499ms step_avg:61.20ms
step:1202/2330 train_time:73563ms step_avg:61.20ms
step:1203/2330 train_time:73623ms step_avg:61.20ms
step:1204/2330 train_time:73685ms step_avg:61.20ms
step:1205/2330 train_time:73746ms step_avg:61.20ms
step:1206/2330 train_time:73809ms step_avg:61.20ms
step:1207/2330 train_time:73869ms step_avg:61.20ms
step:1208/2330 train_time:73933ms step_avg:61.20ms
step:1209/2330 train_time:73993ms step_avg:61.20ms
step:1210/2330 train_time:74056ms step_avg:61.20ms
step:1211/2330 train_time:74117ms step_avg:61.20ms
step:1212/2330 train_time:74179ms step_avg:61.20ms
step:1213/2330 train_time:74239ms step_avg:61.20ms
step:1214/2330 train_time:74302ms step_avg:61.20ms
step:1215/2330 train_time:74362ms step_avg:61.20ms
step:1216/2330 train_time:74425ms step_avg:61.21ms
step:1217/2330 train_time:74484ms step_avg:61.20ms
step:1218/2330 train_time:74548ms step_avg:61.20ms
step:1219/2330 train_time:74608ms step_avg:61.20ms
step:1220/2330 train_time:74672ms step_avg:61.21ms
step:1221/2330 train_time:74732ms step_avg:61.21ms
step:1222/2330 train_time:74795ms step_avg:61.21ms
step:1223/2330 train_time:74854ms step_avg:61.21ms
step:1224/2330 train_time:74917ms step_avg:61.21ms
step:1225/2330 train_time:74977ms step_avg:61.21ms
step:1226/2330 train_time:75040ms step_avg:61.21ms
step:1227/2330 train_time:75101ms step_avg:61.21ms
step:1228/2330 train_time:75163ms step_avg:61.21ms
step:1229/2330 train_time:75223ms step_avg:61.21ms
step:1230/2330 train_time:75286ms step_avg:61.21ms
step:1231/2330 train_time:75346ms step_avg:61.21ms
step:1232/2330 train_time:75409ms step_avg:61.21ms
step:1233/2330 train_time:75469ms step_avg:61.21ms
step:1234/2330 train_time:75533ms step_avg:61.21ms
step:1235/2330 train_time:75593ms step_avg:61.21ms
step:1236/2330 train_time:75655ms step_avg:61.21ms
step:1237/2330 train_time:75715ms step_avg:61.21ms
step:1238/2330 train_time:75778ms step_avg:61.21ms
step:1239/2330 train_time:75838ms step_avg:61.21ms
step:1240/2330 train_time:75901ms step_avg:61.21ms
step:1241/2330 train_time:75961ms step_avg:61.21ms
step:1242/2330 train_time:76025ms step_avg:61.21ms
step:1243/2330 train_time:76085ms step_avg:61.21ms
step:1244/2330 train_time:76147ms step_avg:61.21ms
step:1245/2330 train_time:76208ms step_avg:61.21ms
step:1246/2330 train_time:76271ms step_avg:61.21ms
step:1247/2330 train_time:76331ms step_avg:61.21ms
step:1248/2330 train_time:76394ms step_avg:61.21ms
step:1249/2330 train_time:76454ms step_avg:61.21ms
step:1250/2330 train_time:76517ms step_avg:61.21ms
step:1250/2330 val_loss:4.3607 train_time:76590ms step_avg:61.27ms
step:1251/2330 train_time:76612ms step_avg:61.24ms
step:1252/2330 train_time:76642ms step_avg:61.22ms
step:1253/2330 train_time:76706ms step_avg:61.22ms
step:1254/2330 train_time:76774ms step_avg:61.22ms
step:1255/2330 train_time:76835ms step_avg:61.22ms
step:1256/2330 train_time:76898ms step_avg:61.22ms
step:1257/2330 train_time:76958ms step_avg:61.22ms
step:1258/2330 train_time:77021ms step_avg:61.22ms
step:1259/2330 train_time:77081ms step_avg:61.22ms
step:1260/2330 train_time:77144ms step_avg:61.23ms
step:1261/2330 train_time:77203ms step_avg:61.22ms
step:1262/2330 train_time:77265ms step_avg:61.22ms
step:1263/2330 train_time:77324ms step_avg:61.22ms
step:1264/2330 train_time:77386ms step_avg:61.22ms
step:1265/2330 train_time:77445ms step_avg:61.22ms
step:1266/2330 train_time:77507ms step_avg:61.22ms
step:1267/2330 train_time:77567ms step_avg:61.22ms
step:1268/2330 train_time:77631ms step_avg:61.22ms
step:1269/2330 train_time:77694ms step_avg:61.22ms
step:1270/2330 train_time:77758ms step_avg:61.23ms
step:1271/2330 train_time:77819ms step_avg:61.23ms
step:1272/2330 train_time:77883ms step_avg:61.23ms
step:1273/2330 train_time:77944ms step_avg:61.23ms
step:1274/2330 train_time:78007ms step_avg:61.23ms
step:1275/2330 train_time:78068ms step_avg:61.23ms
step:1276/2330 train_time:78130ms step_avg:61.23ms
step:1277/2330 train_time:78191ms step_avg:61.23ms
step:1278/2330 train_time:78254ms step_avg:61.23ms
step:1279/2330 train_time:78313ms step_avg:61.23ms
step:1280/2330 train_time:78376ms step_avg:61.23ms
step:1281/2330 train_time:78436ms step_avg:61.23ms
step:1282/2330 train_time:78498ms step_avg:61.23ms
step:1283/2330 train_time:78558ms step_avg:61.23ms
step:1284/2330 train_time:78623ms step_avg:61.23ms
step:1285/2330 train_time:78683ms step_avg:61.23ms
step:1286/2330 train_time:78748ms step_avg:61.23ms
step:1287/2330 train_time:78808ms step_avg:61.23ms
step:1288/2330 train_time:78873ms step_avg:61.24ms
step:1289/2330 train_time:78933ms step_avg:61.24ms
step:1290/2330 train_time:78996ms step_avg:61.24ms
step:1291/2330 train_time:79056ms step_avg:61.24ms
step:1292/2330 train_time:79119ms step_avg:61.24ms
step:1293/2330 train_time:79180ms step_avg:61.24ms
step:1294/2330 train_time:79244ms step_avg:61.24ms
step:1295/2330 train_time:79303ms step_avg:61.24ms
step:1296/2330 train_time:79366ms step_avg:61.24ms
step:1297/2330 train_time:79425ms step_avg:61.24ms
step:1298/2330 train_time:79488ms step_avg:61.24ms
step:1299/2330 train_time:79548ms step_avg:61.24ms
step:1300/2330 train_time:79612ms step_avg:61.24ms
step:1301/2330 train_time:79673ms step_avg:61.24ms
step:1302/2330 train_time:79737ms step_avg:61.24ms
step:1303/2330 train_time:79796ms step_avg:61.24ms
step:1304/2330 train_time:79859ms step_avg:61.24ms
step:1305/2330 train_time:79920ms step_avg:61.24ms
step:1306/2330 train_time:79983ms step_avg:61.24ms
step:1307/2330 train_time:80043ms step_avg:61.24ms
step:1308/2330 train_time:80107ms step_avg:61.24ms
step:1309/2330 train_time:80168ms step_avg:61.24ms
step:1310/2330 train_time:80231ms step_avg:61.25ms
step:1311/2330 train_time:80291ms step_avg:61.24ms
step:1312/2330 train_time:80354ms step_avg:61.25ms
step:1313/2330 train_time:80414ms step_avg:61.24ms
step:1314/2330 train_time:80477ms step_avg:61.25ms
step:1315/2330 train_time:80537ms step_avg:61.24ms
step:1316/2330 train_time:80600ms step_avg:61.25ms
step:1317/2330 train_time:80660ms step_avg:61.25ms
step:1318/2330 train_time:80724ms step_avg:61.25ms
step:1319/2330 train_time:80784ms step_avg:61.25ms
step:1320/2330 train_time:80847ms step_avg:61.25ms
step:1321/2330 train_time:80907ms step_avg:61.25ms
step:1322/2330 train_time:80970ms step_avg:61.25ms
step:1323/2330 train_time:81031ms step_avg:61.25ms
step:1324/2330 train_time:81094ms step_avg:61.25ms
step:1325/2330 train_time:81154ms step_avg:61.25ms
step:1326/2330 train_time:81217ms step_avg:61.25ms
step:1327/2330 train_time:81276ms step_avg:61.25ms
step:1328/2330 train_time:81340ms step_avg:61.25ms
step:1329/2330 train_time:81399ms step_avg:61.25ms
step:1330/2330 train_time:81462ms step_avg:61.25ms
step:1331/2330 train_time:81522ms step_avg:61.25ms
step:1332/2330 train_time:81585ms step_avg:61.25ms
step:1333/2330 train_time:81645ms step_avg:61.25ms
step:1334/2330 train_time:81708ms step_avg:61.25ms
step:1335/2330 train_time:81769ms step_avg:61.25ms
step:1336/2330 train_time:81833ms step_avg:61.25ms
step:1337/2330 train_time:81893ms step_avg:61.25ms
step:1338/2330 train_time:81956ms step_avg:61.25ms
step:1339/2330 train_time:82016ms step_avg:61.25ms
step:1340/2330 train_time:82079ms step_avg:61.25ms
step:1341/2330 train_time:82139ms step_avg:61.25ms
step:1342/2330 train_time:82201ms step_avg:61.25ms
step:1343/2330 train_time:82262ms step_avg:61.25ms
step:1344/2330 train_time:82325ms step_avg:61.25ms
step:1345/2330 train_time:82384ms step_avg:61.25ms
step:1346/2330 train_time:82448ms step_avg:61.25ms
step:1347/2330 train_time:82508ms step_avg:61.25ms
step:1348/2330 train_time:82571ms step_avg:61.25ms
step:1349/2330 train_time:82632ms step_avg:61.25ms
step:1350/2330 train_time:82695ms step_avg:61.26ms
step:1351/2330 train_time:82755ms step_avg:61.25ms
step:1352/2330 train_time:82817ms step_avg:61.26ms
step:1353/2330 train_time:82877ms step_avg:61.25ms
step:1354/2330 train_time:82941ms step_avg:61.26ms
step:1355/2330 train_time:83001ms step_avg:61.26ms
step:1356/2330 train_time:83064ms step_avg:61.26ms
step:1357/2330 train_time:83124ms step_avg:61.26ms
step:1358/2330 train_time:83187ms step_avg:61.26ms
step:1359/2330 train_time:83247ms step_avg:61.26ms
step:1360/2330 train_time:83310ms step_avg:61.26ms
step:1361/2330 train_time:83370ms step_avg:61.26ms
step:1362/2330 train_time:83434ms step_avg:61.26ms
step:1363/2330 train_time:83494ms step_avg:61.26ms
step:1364/2330 train_time:83557ms step_avg:61.26ms
step:1365/2330 train_time:83616ms step_avg:61.26ms
step:1366/2330 train_time:83679ms step_avg:61.26ms
step:1367/2330 train_time:83739ms step_avg:61.26ms
step:1368/2330 train_time:83802ms step_avg:61.26ms
step:1369/2330 train_time:83862ms step_avg:61.26ms
step:1370/2330 train_time:83926ms step_avg:61.26ms
step:1371/2330 train_time:83985ms step_avg:61.26ms
step:1372/2330 train_time:84049ms step_avg:61.26ms
step:1373/2330 train_time:84109ms step_avg:61.26ms
step:1374/2330 train_time:84172ms step_avg:61.26ms
step:1375/2330 train_time:84232ms step_avg:61.26ms
step:1376/2330 train_time:84295ms step_avg:61.26ms
step:1377/2330 train_time:84355ms step_avg:61.26ms
step:1378/2330 train_time:84419ms step_avg:61.26ms
step:1379/2330 train_time:84479ms step_avg:61.26ms
step:1380/2330 train_time:84543ms step_avg:61.26ms
step:1381/2330 train_time:84602ms step_avg:61.26ms
step:1382/2330 train_time:84666ms step_avg:61.26ms
step:1383/2330 train_time:84725ms step_avg:61.26ms
step:1384/2330 train_time:84789ms step_avg:61.26ms
step:1385/2330 train_time:84850ms step_avg:61.26ms
step:1386/2330 train_time:84913ms step_avg:61.26ms
step:1387/2330 train_time:84972ms step_avg:61.26ms
step:1388/2330 train_time:85036ms step_avg:61.26ms
step:1389/2330 train_time:85095ms step_avg:61.26ms
step:1390/2330 train_time:85158ms step_avg:61.26ms
step:1391/2330 train_time:85218ms step_avg:61.26ms
step:1392/2330 train_time:85281ms step_avg:61.27ms
step:1393/2330 train_time:85343ms step_avg:61.27ms
step:1394/2330 train_time:85406ms step_avg:61.27ms
step:1395/2330 train_time:85466ms step_avg:61.27ms
step:1396/2330 train_time:85530ms step_avg:61.27ms
step:1397/2330 train_time:85591ms step_avg:61.27ms
step:1398/2330 train_time:85653ms step_avg:61.27ms
step:1399/2330 train_time:85714ms step_avg:61.27ms
step:1400/2330 train_time:85777ms step_avg:61.27ms
step:1401/2330 train_time:85837ms step_avg:61.27ms
step:1402/2330 train_time:85900ms step_avg:61.27ms
step:1403/2330 train_time:85960ms step_avg:61.27ms
step:1404/2330 train_time:86024ms step_avg:61.27ms
step:1405/2330 train_time:86084ms step_avg:61.27ms
step:1406/2330 train_time:86147ms step_avg:61.27ms
step:1407/2330 train_time:86207ms step_avg:61.27ms
step:1408/2330 train_time:86272ms step_avg:61.27ms
step:1409/2330 train_time:86333ms step_avg:61.27ms
step:1410/2330 train_time:86396ms step_avg:61.27ms
step:1411/2330 train_time:86456ms step_avg:61.27ms
step:1412/2330 train_time:86519ms step_avg:61.27ms
step:1413/2330 train_time:86580ms step_avg:61.27ms
step:1414/2330 train_time:86643ms step_avg:61.28ms
step:1415/2330 train_time:86703ms step_avg:61.27ms
step:1416/2330 train_time:86766ms step_avg:61.28ms
step:1417/2330 train_time:86826ms step_avg:61.27ms
step:1418/2330 train_time:86890ms step_avg:61.28ms
step:1419/2330 train_time:86950ms step_avg:61.28ms
step:1420/2330 train_time:87014ms step_avg:61.28ms
step:1421/2330 train_time:87074ms step_avg:61.28ms
step:1422/2330 train_time:87137ms step_avg:61.28ms
step:1423/2330 train_time:87197ms step_avg:61.28ms
step:1424/2330 train_time:87260ms step_avg:61.28ms
step:1425/2330 train_time:87319ms step_avg:61.28ms
step:1426/2330 train_time:87382ms step_avg:61.28ms
step:1427/2330 train_time:87442ms step_avg:61.28ms
step:1428/2330 train_time:87506ms step_avg:61.28ms
step:1429/2330 train_time:87566ms step_avg:61.28ms
step:1430/2330 train_time:87629ms step_avg:61.28ms
step:1431/2330 train_time:87689ms step_avg:61.28ms
step:1432/2330 train_time:87752ms step_avg:61.28ms
step:1433/2330 train_time:87813ms step_avg:61.28ms
step:1434/2330 train_time:87876ms step_avg:61.28ms
step:1435/2330 train_time:87936ms step_avg:61.28ms
step:1436/2330 train_time:87998ms step_avg:61.28ms
step:1437/2330 train_time:88058ms step_avg:61.28ms
step:1438/2330 train_time:88122ms step_avg:61.28ms
step:1439/2330 train_time:88182ms step_avg:61.28ms
step:1440/2330 train_time:88245ms step_avg:61.28ms
step:1441/2330 train_time:88305ms step_avg:61.28ms
step:1442/2330 train_time:88368ms step_avg:61.28ms
step:1443/2330 train_time:88429ms step_avg:61.28ms
step:1444/2330 train_time:88493ms step_avg:61.28ms
step:1445/2330 train_time:88553ms step_avg:61.28ms
step:1446/2330 train_time:88615ms step_avg:61.28ms
step:1447/2330 train_time:88675ms step_avg:61.28ms
step:1448/2330 train_time:88738ms step_avg:61.28ms
step:1449/2330 train_time:88798ms step_avg:61.28ms
step:1450/2330 train_time:88861ms step_avg:61.28ms
step:1451/2330 train_time:88920ms step_avg:61.28ms
step:1452/2330 train_time:88984ms step_avg:61.28ms
step:1453/2330 train_time:89044ms step_avg:61.28ms
step:1454/2330 train_time:89107ms step_avg:61.28ms
step:1455/2330 train_time:89168ms step_avg:61.28ms
step:1456/2330 train_time:89231ms step_avg:61.28ms
step:1457/2330 train_time:89291ms step_avg:61.28ms
step:1458/2330 train_time:89355ms step_avg:61.29ms
step:1459/2330 train_time:89414ms step_avg:61.28ms
step:1460/2330 train_time:89477ms step_avg:61.29ms
step:1461/2330 train_time:89537ms step_avg:61.28ms
step:1462/2330 train_time:89600ms step_avg:61.29ms
step:1463/2330 train_time:89660ms step_avg:61.29ms
step:1464/2330 train_time:89724ms step_avg:61.29ms
step:1465/2330 train_time:89784ms step_avg:61.29ms
step:1466/2330 train_time:89848ms step_avg:61.29ms
step:1467/2330 train_time:89908ms step_avg:61.29ms
step:1468/2330 train_time:89971ms step_avg:61.29ms
step:1469/2330 train_time:90031ms step_avg:61.29ms
step:1470/2330 train_time:90094ms step_avg:61.29ms
step:1471/2330 train_time:90154ms step_avg:61.29ms
step:1472/2330 train_time:90217ms step_avg:61.29ms
step:1473/2330 train_time:90278ms step_avg:61.29ms
step:1474/2330 train_time:90341ms step_avg:61.29ms
step:1475/2330 train_time:90400ms step_avg:61.29ms
step:1476/2330 train_time:90463ms step_avg:61.29ms
step:1477/2330 train_time:90523ms step_avg:61.29ms
step:1478/2330 train_time:90586ms step_avg:61.29ms
step:1479/2330 train_time:90647ms step_avg:61.29ms
step:1480/2330 train_time:90711ms step_avg:61.29ms
step:1481/2330 train_time:90771ms step_avg:61.29ms
step:1482/2330 train_time:90835ms step_avg:61.29ms
step:1483/2330 train_time:90895ms step_avg:61.29ms
step:1484/2330 train_time:90958ms step_avg:61.29ms
step:1485/2330 train_time:91018ms step_avg:61.29ms
step:1486/2330 train_time:91081ms step_avg:61.29ms
step:1487/2330 train_time:91142ms step_avg:61.29ms
step:1488/2330 train_time:91205ms step_avg:61.29ms
step:1489/2330 train_time:91265ms step_avg:61.29ms
step:1490/2330 train_time:91328ms step_avg:61.29ms
step:1491/2330 train_time:91389ms step_avg:61.29ms
step:1492/2330 train_time:91452ms step_avg:61.30ms
step:1493/2330 train_time:91512ms step_avg:61.29ms
step:1494/2330 train_time:91575ms step_avg:61.30ms
step:1495/2330 train_time:91635ms step_avg:61.29ms
step:1496/2330 train_time:91697ms step_avg:61.30ms
step:1497/2330 train_time:91757ms step_avg:61.29ms
step:1498/2330 train_time:91820ms step_avg:61.30ms
step:1499/2330 train_time:91880ms step_avg:61.29ms
step:1500/2330 train_time:91943ms step_avg:61.30ms
step:1500/2330 val_loss:4.2224 train_time:92016ms step_avg:61.34ms
step:1501/2330 train_time:92038ms step_avg:61.32ms
step:1502/2330 train_time:92068ms step_avg:61.30ms
step:1503/2330 train_time:92134ms step_avg:61.30ms
step:1504/2330 train_time:92201ms step_avg:61.30ms
step:1505/2330 train_time:92261ms step_avg:61.30ms
step:1506/2330 train_time:92325ms step_avg:61.30ms
step:1507/2330 train_time:92384ms step_avg:61.30ms
step:1508/2330 train_time:92447ms step_avg:61.30ms
step:1509/2330 train_time:92507ms step_avg:61.30ms
step:1510/2330 train_time:92569ms step_avg:61.30ms
step:1511/2330 train_time:92628ms step_avg:61.30ms
step:1512/2330 train_time:92691ms step_avg:61.30ms
step:1513/2330 train_time:92751ms step_avg:61.30ms
step:1514/2330 train_time:92814ms step_avg:61.30ms
step:1515/2330 train_time:92874ms step_avg:61.30ms
step:1516/2330 train_time:92936ms step_avg:61.30ms
step:1517/2330 train_time:92997ms step_avg:61.30ms
step:1518/2330 train_time:93062ms step_avg:61.31ms
step:1519/2330 train_time:93121ms step_avg:61.30ms
step:1520/2330 train_time:93185ms step_avg:61.31ms
step:1521/2330 train_time:93247ms step_avg:61.31ms
step:1522/2330 train_time:93311ms step_avg:61.31ms
step:1523/2330 train_time:93372ms step_avg:61.31ms
step:1524/2330 train_time:93436ms step_avg:61.31ms
step:1525/2330 train_time:93496ms step_avg:61.31ms
step:1526/2330 train_time:93559ms step_avg:61.31ms
step:1527/2330 train_time:93619ms step_avg:61.31ms
step:1528/2330 train_time:93682ms step_avg:61.31ms
step:1529/2330 train_time:93742ms step_avg:61.31ms
step:1530/2330 train_time:93805ms step_avg:61.31ms
step:1531/2330 train_time:93865ms step_avg:61.31ms
step:1532/2330 train_time:93929ms step_avg:61.31ms
step:1533/2330 train_time:93989ms step_avg:61.31ms
step:1534/2330 train_time:94054ms step_avg:61.31ms
step:1535/2330 train_time:94116ms step_avg:61.31ms
step:1536/2330 train_time:94180ms step_avg:61.31ms
step:1537/2330 train_time:94241ms step_avg:61.31ms
step:1538/2330 train_time:94305ms step_avg:61.32ms
step:1539/2330 train_time:94367ms step_avg:61.32ms
step:1540/2330 train_time:94430ms step_avg:61.32ms
step:1541/2330 train_time:94492ms step_avg:61.32ms
step:1542/2330 train_time:94556ms step_avg:61.32ms
step:1543/2330 train_time:94616ms step_avg:61.32ms
step:1544/2330 train_time:94680ms step_avg:61.32ms
step:1545/2330 train_time:94740ms step_avg:61.32ms
step:1546/2330 train_time:94804ms step_avg:61.32ms
step:1547/2330 train_time:94865ms step_avg:61.32ms
step:1548/2330 train_time:94930ms step_avg:61.32ms
step:1549/2330 train_time:94989ms step_avg:61.32ms
step:1550/2330 train_time:95054ms step_avg:61.32ms
step:1551/2330 train_time:95115ms step_avg:61.32ms
step:1552/2330 train_time:95179ms step_avg:61.33ms
step:1553/2330 train_time:95240ms step_avg:61.33ms
step:1554/2330 train_time:95304ms step_avg:61.33ms
step:1555/2330 train_time:95365ms step_avg:61.33ms
step:1556/2330 train_time:95428ms step_avg:61.33ms
step:1557/2330 train_time:95488ms step_avg:61.33ms
step:1558/2330 train_time:95552ms step_avg:61.33ms
step:1559/2330 train_time:95613ms step_avg:61.33ms
step:1560/2330 train_time:95677ms step_avg:61.33ms
step:1561/2330 train_time:95738ms step_avg:61.33ms
step:1562/2330 train_time:95801ms step_avg:61.33ms
step:1563/2330 train_time:95863ms step_avg:61.33ms
step:1564/2330 train_time:95926ms step_avg:61.33ms
step:1565/2330 train_time:95987ms step_avg:61.33ms
step:1566/2330 train_time:96050ms step_avg:61.33ms
step:1567/2330 train_time:96111ms step_avg:61.33ms
step:1568/2330 train_time:96176ms step_avg:61.34ms
step:1569/2330 train_time:96236ms step_avg:61.34ms
step:1570/2330 train_time:96300ms step_avg:61.34ms
step:1571/2330 train_time:96362ms step_avg:61.34ms
step:1572/2330 train_time:96426ms step_avg:61.34ms
step:1573/2330 train_time:96486ms step_avg:61.34ms
step:1574/2330 train_time:96550ms step_avg:61.34ms
step:1575/2330 train_time:96611ms step_avg:61.34ms
step:1576/2330 train_time:96675ms step_avg:61.34ms
step:1577/2330 train_time:96737ms step_avg:61.34ms
step:1578/2330 train_time:96801ms step_avg:61.34ms
step:1579/2330 train_time:96861ms step_avg:61.34ms
step:1580/2330 train_time:96925ms step_avg:61.34ms
step:1581/2330 train_time:96986ms step_avg:61.34ms
step:1582/2330 train_time:97049ms step_avg:61.35ms
step:1583/2330 train_time:97109ms step_avg:61.35ms
step:1584/2330 train_time:97173ms step_avg:61.35ms
step:1585/2330 train_time:97235ms step_avg:61.35ms
step:1586/2330 train_time:97300ms step_avg:61.35ms
step:1587/2330 train_time:97361ms step_avg:61.35ms
step:1588/2330 train_time:97424ms step_avg:61.35ms
step:1589/2330 train_time:97485ms step_avg:61.35ms
step:1590/2330 train_time:97549ms step_avg:61.35ms
step:1591/2330 train_time:97610ms step_avg:61.35ms
step:1592/2330 train_time:97674ms step_avg:61.35ms
step:1593/2330 train_time:97735ms step_avg:61.35ms
step:1594/2330 train_time:97799ms step_avg:61.35ms
step:1595/2330 train_time:97860ms step_avg:61.35ms
step:1596/2330 train_time:97923ms step_avg:61.36ms
step:1597/2330 train_time:97984ms step_avg:61.35ms
step:1598/2330 train_time:98047ms step_avg:61.36ms
step:1599/2330 train_time:98108ms step_avg:61.36ms
step:1600/2330 train_time:98172ms step_avg:61.36ms
step:1601/2330 train_time:98233ms step_avg:61.36ms
step:1602/2330 train_time:98297ms step_avg:61.36ms
step:1603/2330 train_time:98358ms step_avg:61.36ms
step:1604/2330 train_time:98422ms step_avg:61.36ms
step:1605/2330 train_time:98483ms step_avg:61.36ms
step:1606/2330 train_time:98546ms step_avg:61.36ms
step:1607/2330 train_time:98607ms step_avg:61.36ms
step:1608/2330 train_time:98672ms step_avg:61.36ms
step:1609/2330 train_time:98733ms step_avg:61.36ms
step:1610/2330 train_time:98797ms step_avg:61.36ms
step:1611/2330 train_time:98857ms step_avg:61.36ms
step:1612/2330 train_time:98920ms step_avg:61.36ms
step:1613/2330 train_time:98981ms step_avg:61.36ms
step:1614/2330 train_time:99047ms step_avg:61.37ms
step:1615/2330 train_time:99107ms step_avg:61.37ms
step:1616/2330 train_time:99171ms step_avg:61.37ms
step:1617/2330 train_time:99232ms step_avg:61.37ms
step:1618/2330 train_time:99296ms step_avg:61.37ms
step:1619/2330 train_time:99358ms step_avg:61.37ms
step:1620/2330 train_time:99421ms step_avg:61.37ms
step:1621/2330 train_time:99481ms step_avg:61.37ms
step:1622/2330 train_time:99545ms step_avg:61.37ms
step:1623/2330 train_time:99606ms step_avg:61.37ms
step:1624/2330 train_time:99670ms step_avg:61.37ms
step:1625/2330 train_time:99730ms step_avg:61.37ms
step:1626/2330 train_time:99794ms step_avg:61.37ms
step:1627/2330 train_time:99856ms step_avg:61.37ms
step:1628/2330 train_time:99919ms step_avg:61.38ms
step:1629/2330 train_time:99981ms step_avg:61.38ms
step:1630/2330 train_time:100044ms step_avg:61.38ms
step:1631/2330 train_time:100105ms step_avg:61.38ms
step:1632/2330 train_time:100170ms step_avg:61.38ms
step:1633/2330 train_time:100231ms step_avg:61.38ms
step:1634/2330 train_time:100295ms step_avg:61.38ms
step:1635/2330 train_time:100356ms step_avg:61.38ms
step:1636/2330 train_time:100420ms step_avg:61.38ms
step:1637/2330 train_time:100480ms step_avg:61.38ms
step:1638/2330 train_time:100544ms step_avg:61.38ms
step:1639/2330 train_time:100604ms step_avg:61.38ms
step:1640/2330 train_time:100669ms step_avg:61.38ms
step:1641/2330 train_time:100728ms step_avg:61.38ms
step:1642/2330 train_time:100793ms step_avg:61.38ms
step:1643/2330 train_time:100855ms step_avg:61.38ms
step:1644/2330 train_time:100919ms step_avg:61.39ms
step:1645/2330 train_time:100980ms step_avg:61.39ms
step:1646/2330 train_time:101043ms step_avg:61.39ms
step:1647/2330 train_time:101104ms step_avg:61.39ms
step:1648/2330 train_time:101169ms step_avg:61.39ms
step:1649/2330 train_time:101229ms step_avg:61.39ms
step:1650/2330 train_time:101292ms step_avg:61.39ms
step:1651/2330 train_time:101354ms step_avg:61.39ms
step:1652/2330 train_time:101417ms step_avg:61.39ms
step:1653/2330 train_time:101478ms step_avg:61.39ms
step:1654/2330 train_time:101542ms step_avg:61.39ms
step:1655/2330 train_time:101603ms step_avg:61.39ms
step:1656/2330 train_time:101667ms step_avg:61.39ms
step:1657/2330 train_time:101727ms step_avg:61.39ms
step:1658/2330 train_time:101791ms step_avg:61.39ms
step:1659/2330 train_time:101853ms step_avg:61.39ms
step:1660/2330 train_time:101916ms step_avg:61.39ms
step:1661/2330 train_time:101977ms step_avg:61.39ms
step:1662/2330 train_time:102041ms step_avg:61.40ms
step:1663/2330 train_time:102101ms step_avg:61.40ms
step:1664/2330 train_time:102164ms step_avg:61.40ms
step:1665/2330 train_time:102226ms step_avg:61.40ms
step:1666/2330 train_time:102290ms step_avg:61.40ms
step:1667/2330 train_time:102351ms step_avg:61.40ms
step:1668/2330 train_time:102414ms step_avg:61.40ms
step:1669/2330 train_time:102475ms step_avg:61.40ms
step:1670/2330 train_time:102540ms step_avg:61.40ms
step:1671/2330 train_time:102601ms step_avg:61.40ms
step:1672/2330 train_time:102665ms step_avg:61.40ms
step:1673/2330 train_time:102725ms step_avg:61.40ms
step:1674/2330 train_time:102789ms step_avg:61.40ms
step:1675/2330 train_time:102849ms step_avg:61.40ms
step:1676/2330 train_time:102912ms step_avg:61.40ms
step:1677/2330 train_time:102973ms step_avg:61.40ms
step:1678/2330 train_time:103038ms step_avg:61.40ms
step:1679/2330 train_time:103099ms step_avg:61.41ms
step:1680/2330 train_time:103162ms step_avg:61.41ms
step:1681/2330 train_time:103222ms step_avg:61.41ms
step:1682/2330 train_time:103286ms step_avg:61.41ms
step:1683/2330 train_time:103346ms step_avg:61.41ms
step:1684/2330 train_time:103411ms step_avg:61.41ms
step:1685/2330 train_time:103471ms step_avg:61.41ms
step:1686/2330 train_time:103536ms step_avg:61.41ms
step:1687/2330 train_time:103598ms step_avg:61.41ms
step:1688/2330 train_time:103663ms step_avg:61.41ms
step:1689/2330 train_time:103722ms step_avg:61.41ms
step:1690/2330 train_time:103787ms step_avg:61.41ms
step:1691/2330 train_time:103847ms step_avg:61.41ms
step:1692/2330 train_time:103910ms step_avg:61.41ms
step:1693/2330 train_time:103970ms step_avg:61.41ms
step:1694/2330 train_time:104035ms step_avg:61.41ms
step:1695/2330 train_time:104097ms step_avg:61.41ms
step:1696/2330 train_time:104161ms step_avg:61.42ms
step:1697/2330 train_time:104221ms step_avg:61.41ms
step:1698/2330 train_time:104284ms step_avg:61.42ms
step:1699/2330 train_time:104345ms step_avg:61.42ms
step:1700/2330 train_time:104407ms step_avg:61.42ms
step:1701/2330 train_time:104468ms step_avg:61.42ms
step:1702/2330 train_time:104531ms step_avg:61.42ms
step:1703/2330 train_time:104593ms step_avg:61.42ms
step:1704/2330 train_time:104658ms step_avg:61.42ms
step:1705/2330 train_time:104718ms step_avg:61.42ms
step:1706/2330 train_time:104782ms step_avg:61.42ms
step:1707/2330 train_time:104842ms step_avg:61.42ms
step:1708/2330 train_time:104906ms step_avg:61.42ms
step:1709/2330 train_time:104967ms step_avg:61.42ms
step:1710/2330 train_time:105030ms step_avg:61.42ms
step:1711/2330 train_time:105091ms step_avg:61.42ms
step:1712/2330 train_time:105156ms step_avg:61.42ms
step:1713/2330 train_time:105216ms step_avg:61.42ms
step:1714/2330 train_time:105280ms step_avg:61.42ms
step:1715/2330 train_time:105341ms step_avg:61.42ms
step:1716/2330 train_time:105404ms step_avg:61.42ms
step:1717/2330 train_time:105465ms step_avg:61.42ms
step:1718/2330 train_time:105529ms step_avg:61.43ms
step:1719/2330 train_time:105589ms step_avg:61.42ms
step:1720/2330 train_time:105653ms step_avg:61.43ms
step:1721/2330 train_time:105714ms step_avg:61.43ms
step:1722/2330 train_time:105778ms step_avg:61.43ms
step:1723/2330 train_time:105839ms step_avg:61.43ms
step:1724/2330 train_time:105902ms step_avg:61.43ms
step:1725/2330 train_time:105963ms step_avg:61.43ms
step:1726/2330 train_time:106028ms step_avg:61.43ms
step:1727/2330 train_time:106088ms step_avg:61.43ms
step:1728/2330 train_time:106152ms step_avg:61.43ms
step:1729/2330 train_time:106213ms step_avg:61.43ms
step:1730/2330 train_time:106277ms step_avg:61.43ms
step:1731/2330 train_time:106338ms step_avg:61.43ms
step:1732/2330 train_time:106402ms step_avg:61.43ms
step:1733/2330 train_time:106463ms step_avg:61.43ms
step:1734/2330 train_time:106526ms step_avg:61.43ms
step:1735/2330 train_time:106586ms step_avg:61.43ms
step:1736/2330 train_time:106650ms step_avg:61.43ms
step:1737/2330 train_time:106710ms step_avg:61.43ms
step:1738/2330 train_time:106774ms step_avg:61.43ms
step:1739/2330 train_time:106836ms step_avg:61.44ms
step:1740/2330 train_time:106900ms step_avg:61.44ms
step:1741/2330 train_time:106961ms step_avg:61.44ms
step:1742/2330 train_time:107024ms step_avg:61.44ms
step:1743/2330 train_time:107085ms step_avg:61.44ms
step:1744/2330 train_time:107149ms step_avg:61.44ms
step:1745/2330 train_time:107210ms step_avg:61.44ms
step:1746/2330 train_time:107273ms step_avg:61.44ms
step:1747/2330 train_time:107334ms step_avg:61.44ms
step:1748/2330 train_time:107399ms step_avg:61.44ms
step:1749/2330 train_time:107461ms step_avg:61.44ms
step:1750/2330 train_time:107524ms step_avg:61.44ms
step:1750/2330 val_loss:4.1254 train_time:107597ms step_avg:61.48ms
step:1751/2330 train_time:107621ms step_avg:61.46ms
step:1752/2330 train_time:107650ms step_avg:61.44ms
step:1753/2330 train_time:107716ms step_avg:61.45ms
step:1754/2330 train_time:107782ms step_avg:61.45ms
step:1755/2330 train_time:107844ms step_avg:61.45ms
step:1756/2330 train_time:107909ms step_avg:61.45ms
step:1757/2330 train_time:107969ms step_avg:61.45ms
step:1758/2330 train_time:108031ms step_avg:61.45ms
step:1759/2330 train_time:108091ms step_avg:61.45ms
step:1760/2330 train_time:108153ms step_avg:61.45ms
step:1761/2330 train_time:108213ms step_avg:61.45ms
step:1762/2330 train_time:108275ms step_avg:61.45ms
step:1763/2330 train_time:108334ms step_avg:61.45ms
step:1764/2330 train_time:108397ms step_avg:61.45ms
step:1765/2330 train_time:108456ms step_avg:61.45ms
step:1766/2330 train_time:108520ms step_avg:61.45ms
step:1767/2330 train_time:108584ms step_avg:61.45ms
step:1768/2330 train_time:108651ms step_avg:61.45ms
step:1769/2330 train_time:108713ms step_avg:61.45ms
step:1770/2330 train_time:108777ms step_avg:61.46ms
step:1771/2330 train_time:108837ms step_avg:61.46ms
step:1772/2330 train_time:108899ms step_avg:61.46ms
step:1773/2330 train_time:108961ms step_avg:61.46ms
step:1774/2330 train_time:109024ms step_avg:61.46ms
step:1775/2330 train_time:109084ms step_avg:61.46ms
step:1776/2330 train_time:109148ms step_avg:61.46ms
step:1777/2330 train_time:109210ms step_avg:61.46ms
step:1778/2330 train_time:109273ms step_avg:61.46ms
step:1779/2330 train_time:109332ms step_avg:61.46ms
step:1780/2330 train_time:109394ms step_avg:61.46ms
step:1781/2330 train_time:109454ms step_avg:61.46ms
step:1782/2330 train_time:109518ms step_avg:61.46ms
step:1783/2330 train_time:109579ms step_avg:61.46ms
step:1784/2330 train_time:109644ms step_avg:61.46ms
step:1785/2330 train_time:109704ms step_avg:61.46ms
step:1786/2330 train_time:109769ms step_avg:61.46ms
step:1787/2330 train_time:109831ms step_avg:61.46ms
step:1788/2330 train_time:109896ms step_avg:61.46ms
step:1789/2330 train_time:109955ms step_avg:61.46ms
step:1790/2330 train_time:110019ms step_avg:61.46ms
step:1791/2330 train_time:110080ms step_avg:61.46ms
step:1792/2330 train_time:110143ms step_avg:61.46ms
step:1793/2330 train_time:110203ms step_avg:61.46ms
step:1794/2330 train_time:110267ms step_avg:61.46ms
step:1795/2330 train_time:110327ms step_avg:61.46ms
step:1796/2330 train_time:110391ms step_avg:61.46ms
step:1797/2330 train_time:110452ms step_avg:61.46ms
step:1798/2330 train_time:110517ms step_avg:61.47ms
step:1799/2330 train_time:110576ms step_avg:61.47ms
step:1800/2330 train_time:110640ms step_avg:61.47ms
step:1801/2330 train_time:110701ms step_avg:61.47ms
step:1802/2330 train_time:110765ms step_avg:61.47ms
step:1803/2330 train_time:110826ms step_avg:61.47ms
step:1804/2330 train_time:110891ms step_avg:61.47ms
step:1805/2330 train_time:110953ms step_avg:61.47ms
step:1806/2330 train_time:111017ms step_avg:61.47ms
step:1807/2330 train_time:111076ms step_avg:61.47ms
step:1808/2330 train_time:111139ms step_avg:61.47ms
step:1809/2330 train_time:111199ms step_avg:61.47ms
step:1810/2330 train_time:111263ms step_avg:61.47ms
step:1811/2330 train_time:111323ms step_avg:61.47ms
step:1812/2330 train_time:111386ms step_avg:61.47ms
step:1813/2330 train_time:111447ms step_avg:61.47ms
step:1814/2330 train_time:111513ms step_avg:61.47ms
step:1815/2330 train_time:111573ms step_avg:61.47ms
step:1816/2330 train_time:111636ms step_avg:61.47ms
step:1817/2330 train_time:111696ms step_avg:61.47ms
step:1818/2330 train_time:111760ms step_avg:61.47ms
step:1819/2330 train_time:111820ms step_avg:61.47ms
step:1820/2330 train_time:111884ms step_avg:61.47ms
step:1821/2330 train_time:111946ms step_avg:61.47ms
step:1822/2330 train_time:112010ms step_avg:61.48ms
step:1823/2330 train_time:112070ms step_avg:61.48ms
step:1824/2330 train_time:112133ms step_avg:61.48ms
step:1825/2330 train_time:112194ms step_avg:61.48ms
step:1826/2330 train_time:112257ms step_avg:61.48ms
step:1827/2330 train_time:112318ms step_avg:61.48ms
step:1828/2330 train_time:112382ms step_avg:61.48ms
step:1829/2330 train_time:112442ms step_avg:61.48ms
step:1830/2330 train_time:112504ms step_avg:61.48ms
step:1831/2330 train_time:112566ms step_avg:61.48ms
step:1832/2330 train_time:112630ms step_avg:61.48ms
step:1833/2330 train_time:112691ms step_avg:61.48ms
step:1834/2330 train_time:112754ms step_avg:61.48ms
step:1835/2330 train_time:112815ms step_avg:61.48ms
step:1836/2330 train_time:112877ms step_avg:61.48ms
step:1837/2330 train_time:112938ms step_avg:61.48ms
step:1838/2330 train_time:113001ms step_avg:61.48ms
step:1839/2330 train_time:113062ms step_avg:61.48ms
step:1840/2330 train_time:113126ms step_avg:61.48ms
step:1841/2330 train_time:113187ms step_avg:61.48ms
step:1842/2330 train_time:113251ms step_avg:61.48ms
step:1843/2330 train_time:113312ms step_avg:61.48ms
step:1844/2330 train_time:113376ms step_avg:61.48ms
step:1845/2330 train_time:113436ms step_avg:61.48ms
step:1846/2330 train_time:113499ms step_avg:61.48ms
step:1847/2330 train_time:113559ms step_avg:61.48ms
step:1848/2330 train_time:113623ms step_avg:61.48ms
step:1849/2330 train_time:113684ms step_avg:61.48ms
step:1850/2330 train_time:113748ms step_avg:61.49ms
step:1851/2330 train_time:113810ms step_avg:61.49ms
step:1852/2330 train_time:113873ms step_avg:61.49ms
step:1853/2330 train_time:113934ms step_avg:61.49ms
step:1854/2330 train_time:113997ms step_avg:61.49ms
step:1855/2330 train_time:114057ms step_avg:61.49ms
step:1856/2330 train_time:114122ms step_avg:61.49ms
step:1857/2330 train_time:114182ms step_avg:61.49ms
step:1858/2330 train_time:114246ms step_avg:61.49ms
step:1859/2330 train_time:114308ms step_avg:61.49ms
step:1860/2330 train_time:114372ms step_avg:61.49ms
step:1861/2330 train_time:114432ms step_avg:61.49ms
step:1862/2330 train_time:114496ms step_avg:61.49ms
step:1863/2330 train_time:114556ms step_avg:61.49ms
step:1864/2330 train_time:114620ms step_avg:61.49ms
step:1865/2330 train_time:114680ms step_avg:61.49ms
step:1866/2330 train_time:114744ms step_avg:61.49ms
step:1867/2330 train_time:114804ms step_avg:61.49ms
step:1868/2330 train_time:114867ms step_avg:61.49ms
step:1869/2330 train_time:114930ms step_avg:61.49ms
step:1870/2330 train_time:114994ms step_avg:61.49ms
step:1871/2330 train_time:115054ms step_avg:61.49ms
step:1872/2330 train_time:115117ms step_avg:61.49ms
step:1873/2330 train_time:115178ms step_avg:61.49ms
step:1874/2330 train_time:115241ms step_avg:61.49ms
step:1875/2330 train_time:115302ms step_avg:61.49ms
step:1876/2330 train_time:115366ms step_avg:61.50ms
step:1877/2330 train_time:115427ms step_avg:61.50ms
step:1878/2330 train_time:115492ms step_avg:61.50ms
step:1879/2330 train_time:115553ms step_avg:61.50ms
step:1880/2330 train_time:115617ms step_avg:61.50ms
step:1881/2330 train_time:115677ms step_avg:61.50ms
step:1882/2330 train_time:115740ms step_avg:61.50ms
step:1883/2330 train_time:115800ms step_avg:61.50ms
step:1884/2330 train_time:115864ms step_avg:61.50ms
step:1885/2330 train_time:115925ms step_avg:61.50ms
step:1886/2330 train_time:115988ms step_avg:61.50ms
step:1887/2330 train_time:116050ms step_avg:61.50ms
step:1888/2330 train_time:116115ms step_avg:61.50ms
step:1889/2330 train_time:116175ms step_avg:61.50ms
step:1890/2330 train_time:116237ms step_avg:61.50ms
step:1891/2330 train_time:116298ms step_avg:61.50ms
step:1892/2330 train_time:116362ms step_avg:61.50ms
step:1893/2330 train_time:116422ms step_avg:61.50ms
step:1894/2330 train_time:116486ms step_avg:61.50ms
step:1895/2330 train_time:116547ms step_avg:61.50ms
step:1896/2330 train_time:116611ms step_avg:61.50ms
step:1897/2330 train_time:116671ms step_avg:61.50ms
step:1898/2330 train_time:116734ms step_avg:61.50ms
step:1899/2330 train_time:116795ms step_avg:61.50ms
step:1900/2330 train_time:116858ms step_avg:61.50ms
step:1901/2330 train_time:116920ms step_avg:61.50ms
step:1902/2330 train_time:116983ms step_avg:61.51ms
step:1903/2330 train_time:117043ms step_avg:61.50ms
step:1904/2330 train_time:117107ms step_avg:61.51ms
step:1905/2330 train_time:117168ms step_avg:61.51ms
step:1906/2330 train_time:117232ms step_avg:61.51ms
step:1907/2330 train_time:117293ms step_avg:61.51ms
step:1908/2330 train_time:117356ms step_avg:61.51ms
step:1909/2330 train_time:117415ms step_avg:61.51ms
step:1910/2330 train_time:117479ms step_avg:61.51ms
step:1911/2330 train_time:117540ms step_avg:61.51ms
step:1912/2330 train_time:117603ms step_avg:61.51ms
step:1913/2330 train_time:117664ms step_avg:61.51ms
step:1914/2330 train_time:117728ms step_avg:61.51ms
step:1915/2330 train_time:117789ms step_avg:61.51ms
step:1916/2330 train_time:117853ms step_avg:61.51ms
step:1917/2330 train_time:117915ms step_avg:61.51ms
step:1918/2330 train_time:117978ms step_avg:61.51ms
step:1919/2330 train_time:118038ms step_avg:61.51ms
step:1920/2330 train_time:118102ms step_avg:61.51ms
step:1921/2330 train_time:118161ms step_avg:61.51ms
step:1922/2330 train_time:118225ms step_avg:61.51ms
step:1923/2330 train_time:118286ms step_avg:61.51ms
step:1924/2330 train_time:118351ms step_avg:61.51ms
step:1925/2330 train_time:118413ms step_avg:61.51ms
step:1926/2330 train_time:118475ms step_avg:61.51ms
step:1927/2330 train_time:118536ms step_avg:61.51ms
step:1928/2330 train_time:118600ms step_avg:61.51ms
step:1929/2330 train_time:118660ms step_avg:61.51ms
step:1930/2330 train_time:118725ms step_avg:61.52ms
step:1931/2330 train_time:118786ms step_avg:61.52ms
step:1932/2330 train_time:118851ms step_avg:61.52ms
step:1933/2330 train_time:118913ms step_avg:61.52ms
step:1934/2330 train_time:118976ms step_avg:61.52ms
step:1935/2330 train_time:119036ms step_avg:61.52ms
step:1936/2330 train_time:119100ms step_avg:61.52ms
step:1937/2330 train_time:119160ms step_avg:61.52ms
step:1938/2330 train_time:119223ms step_avg:61.52ms
step:1939/2330 train_time:119284ms step_avg:61.52ms
step:1940/2330 train_time:119348ms step_avg:61.52ms
step:1941/2330 train_time:119410ms step_avg:61.52ms
step:1942/2330 train_time:119473ms step_avg:61.52ms
step:1943/2330 train_time:119534ms step_avg:61.52ms
step:1944/2330 train_time:119597ms step_avg:61.52ms
step:1945/2330 train_time:119658ms step_avg:61.52ms
step:1946/2330 train_time:119723ms step_avg:61.52ms
step:1947/2330 train_time:119782ms step_avg:61.52ms
step:1948/2330 train_time:119846ms step_avg:61.52ms
step:1949/2330 train_time:119906ms step_avg:61.52ms
step:1950/2330 train_time:119970ms step_avg:61.52ms
step:1951/2330 train_time:120031ms step_avg:61.52ms
step:1952/2330 train_time:120095ms step_avg:61.52ms
step:1953/2330 train_time:120155ms step_avg:61.52ms
step:1954/2330 train_time:120218ms step_avg:61.52ms
step:1955/2330 train_time:120279ms step_avg:61.52ms
step:1956/2330 train_time:120343ms step_avg:61.52ms
step:1957/2330 train_time:120403ms step_avg:61.52ms
step:1958/2330 train_time:120466ms step_avg:61.53ms
step:1959/2330 train_time:120527ms step_avg:61.52ms
step:1960/2330 train_time:120592ms step_avg:61.53ms
step:1961/2330 train_time:120653ms step_avg:61.53ms
step:1962/2330 train_time:120717ms step_avg:61.53ms
step:1963/2330 train_time:120776ms step_avg:61.53ms
step:1964/2330 train_time:120839ms step_avg:61.53ms
step:1965/2330 train_time:120900ms step_avg:61.53ms
step:1966/2330 train_time:120964ms step_avg:61.53ms
step:1967/2330 train_time:121024ms step_avg:61.53ms
step:1968/2330 train_time:121088ms step_avg:61.53ms
step:1969/2330 train_time:121149ms step_avg:61.53ms
step:1970/2330 train_time:121214ms step_avg:61.53ms
step:1971/2330 train_time:121274ms step_avg:61.53ms
step:1972/2330 train_time:121336ms step_avg:61.53ms
step:1973/2330 train_time:121397ms step_avg:61.53ms
step:1974/2330 train_time:121461ms step_avg:61.53ms
step:1975/2330 train_time:121521ms step_avg:61.53ms
step:1976/2330 train_time:121584ms step_avg:61.53ms
step:1977/2330 train_time:121646ms step_avg:61.53ms
step:1978/2330 train_time:121711ms step_avg:61.53ms
step:1979/2330 train_time:121772ms step_avg:61.53ms
step:1980/2330 train_time:121835ms step_avg:61.53ms
step:1981/2330 train_time:121895ms step_avg:61.53ms
step:1982/2330 train_time:121958ms step_avg:61.53ms
step:1983/2330 train_time:122018ms step_avg:61.53ms
step:1984/2330 train_time:122082ms step_avg:61.53ms
step:1985/2330 train_time:122142ms step_avg:61.53ms
step:1986/2330 train_time:122206ms step_avg:61.53ms
step:1987/2330 train_time:122267ms step_avg:61.53ms
step:1988/2330 train_time:122332ms step_avg:61.53ms
step:1989/2330 train_time:122392ms step_avg:61.53ms
step:1990/2330 train_time:122455ms step_avg:61.54ms
step:1991/2330 train_time:122516ms step_avg:61.54ms
step:1992/2330 train_time:122580ms step_avg:61.54ms
step:1993/2330 train_time:122640ms step_avg:61.54ms
step:1994/2330 train_time:122705ms step_avg:61.54ms
step:1995/2330 train_time:122765ms step_avg:61.54ms
step:1996/2330 train_time:122829ms step_avg:61.54ms
step:1997/2330 train_time:122891ms step_avg:61.54ms
step:1998/2330 train_time:122954ms step_avg:61.54ms
step:1999/2330 train_time:123016ms step_avg:61.54ms
step:2000/2330 train_time:123078ms step_avg:61.54ms
step:2000/2330 val_loss:4.0675 train_time:123153ms step_avg:61.58ms
step:2001/2330 train_time:123175ms step_avg:61.56ms
step:2002/2330 train_time:123205ms step_avg:61.54ms
step:2003/2330 train_time:123269ms step_avg:61.54ms
step:2004/2330 train_time:123338ms step_avg:61.55ms
step:2005/2330 train_time:123400ms step_avg:61.55ms
step:2006/2330 train_time:123464ms step_avg:61.55ms
step:2007/2330 train_time:123524ms step_avg:61.55ms
step:2008/2330 train_time:123587ms step_avg:61.55ms
step:2009/2330 train_time:123646ms step_avg:61.55ms
step:2010/2330 train_time:123709ms step_avg:61.55ms
step:2011/2330 train_time:123768ms step_avg:61.55ms
step:2012/2330 train_time:123831ms step_avg:61.55ms
step:2013/2330 train_time:123890ms step_avg:61.54ms
step:2014/2330 train_time:123953ms step_avg:61.55ms
step:2015/2330 train_time:124013ms step_avg:61.54ms
step:2016/2330 train_time:124075ms step_avg:61.55ms
step:2017/2330 train_time:124136ms step_avg:61.55ms
step:2018/2330 train_time:124203ms step_avg:61.55ms
step:2019/2330 train_time:124265ms step_avg:61.55ms
step:2020/2330 train_time:124330ms step_avg:61.55ms
step:2021/2330 train_time:124391ms step_avg:61.55ms
step:2022/2330 train_time:124456ms step_avg:61.55ms
step:2023/2330 train_time:124518ms step_avg:61.55ms
step:2024/2330 train_time:124581ms step_avg:61.55ms
step:2025/2330 train_time:124642ms step_avg:61.55ms
step:2026/2330 train_time:124705ms step_avg:61.55ms
step:2027/2330 train_time:124765ms step_avg:61.55ms
step:2028/2330 train_time:124828ms step_avg:61.55ms
step:2029/2330 train_time:124887ms step_avg:61.55ms
step:2030/2330 train_time:124950ms step_avg:61.55ms
step:2031/2330 train_time:125010ms step_avg:61.55ms
step:2032/2330 train_time:125073ms step_avg:61.55ms
step:2033/2330 train_time:125134ms step_avg:61.55ms
step:2034/2330 train_time:125198ms step_avg:61.55ms
step:2035/2330 train_time:125260ms step_avg:61.55ms
step:2036/2330 train_time:125323ms step_avg:61.55ms
step:2037/2330 train_time:125384ms step_avg:61.55ms
step:2038/2330 train_time:125448ms step_avg:61.55ms
step:2039/2330 train_time:125509ms step_avg:61.55ms
step:2040/2330 train_time:125572ms step_avg:61.56ms
step:2041/2330 train_time:125633ms step_avg:61.55ms
step:2042/2330 train_time:125699ms step_avg:61.56ms
step:2043/2330 train_time:125759ms step_avg:61.56ms
step:2044/2330 train_time:125823ms step_avg:61.56ms
step:2045/2330 train_time:125883ms step_avg:61.56ms
step:2046/2330 train_time:125947ms step_avg:61.56ms
step:2047/2330 train_time:126006ms step_avg:61.56ms
step:2048/2330 train_time:126069ms step_avg:61.56ms
step:2049/2330 train_time:126130ms step_avg:61.56ms
step:2050/2330 train_time:126193ms step_avg:61.56ms
step:2051/2330 train_time:126255ms step_avg:61.56ms
step:2052/2330 train_time:126321ms step_avg:61.56ms
step:2053/2330 train_time:126381ms step_avg:61.56ms
step:2054/2330 train_time:126444ms step_avg:61.56ms
step:2055/2330 train_time:126505ms step_avg:61.56ms
step:2056/2330 train_time:126569ms step_avg:61.56ms
step:2057/2330 train_time:126630ms step_avg:61.56ms
step:2058/2330 train_time:126693ms step_avg:61.56ms
step:2059/2330 train_time:126756ms step_avg:61.56ms
step:2060/2330 train_time:126821ms step_avg:61.56ms
step:2061/2330 train_time:126880ms step_avg:61.56ms
step:2062/2330 train_time:126943ms step_avg:61.56ms
step:2063/2330 train_time:127004ms step_avg:61.56ms
step:2064/2330 train_time:127067ms step_avg:61.56ms
step:2065/2330 train_time:127127ms step_avg:61.56ms
step:2066/2330 train_time:127191ms step_avg:61.56ms
step:2067/2330 train_time:127251ms step_avg:61.56ms
step:2068/2330 train_time:127315ms step_avg:61.56ms
step:2069/2330 train_time:127377ms step_avg:61.56ms
step:2070/2330 train_time:127442ms step_avg:61.57ms
step:2071/2330 train_time:127503ms step_avg:61.57ms
step:2072/2330 train_time:127566ms step_avg:61.57ms
step:2073/2330 train_time:127627ms step_avg:61.57ms
step:2074/2330 train_time:127691ms step_avg:61.57ms
step:2075/2330 train_time:127751ms step_avg:61.57ms
step:2076/2330 train_time:127814ms step_avg:61.57ms
step:2077/2330 train_time:127875ms step_avg:61.57ms
step:2078/2330 train_time:127940ms step_avg:61.57ms
step:2079/2330 train_time:128001ms step_avg:61.57ms
step:2080/2330 train_time:128064ms step_avg:61.57ms
step:2081/2330 train_time:128124ms step_avg:61.57ms
step:2082/2330 train_time:128187ms step_avg:61.57ms
step:2083/2330 train_time:128248ms step_avg:61.57ms
step:2084/2330 train_time:128311ms step_avg:61.57ms
step:2085/2330 train_time:128371ms step_avg:61.57ms
step:2086/2330 train_time:128435ms step_avg:61.57ms
step:2087/2330 train_time:128497ms step_avg:61.57ms
step:2088/2330 train_time:128561ms step_avg:61.57ms
step:2089/2330 train_time:128623ms step_avg:61.57ms
step:2090/2330 train_time:128685ms step_avg:61.57ms
step:2091/2330 train_time:128745ms step_avg:61.57ms
step:2092/2330 train_time:128810ms step_avg:61.57ms
step:2093/2330 train_time:128870ms step_avg:61.57ms
step:2094/2330 train_time:128934ms step_avg:61.57ms
step:2095/2330 train_time:128994ms step_avg:61.57ms
step:2096/2330 train_time:129058ms step_avg:61.57ms
step:2097/2330 train_time:129119ms step_avg:61.57ms
step:2098/2330 train_time:129182ms step_avg:61.57ms
step:2099/2330 train_time:129243ms step_avg:61.57ms
step:2100/2330 train_time:129306ms step_avg:61.57ms
step:2101/2330 train_time:129367ms step_avg:61.57ms
step:2102/2330 train_time:129431ms step_avg:61.58ms
step:2103/2330 train_time:129491ms step_avg:61.57ms
step:2104/2330 train_time:129555ms step_avg:61.58ms
step:2105/2330 train_time:129618ms step_avg:61.58ms
step:2106/2330 train_time:129681ms step_avg:61.58ms
step:2107/2330 train_time:129742ms step_avg:61.58ms
step:2108/2330 train_time:129804ms step_avg:61.58ms
step:2109/2330 train_time:129864ms step_avg:61.58ms
step:2110/2330 train_time:129928ms step_avg:61.58ms
step:2111/2330 train_time:129989ms step_avg:61.58ms
step:2112/2330 train_time:130053ms step_avg:61.58ms
step:2113/2330 train_time:130113ms step_avg:61.58ms
step:2114/2330 train_time:130178ms step_avg:61.58ms
step:2115/2330 train_time:130238ms step_avg:61.58ms
step:2116/2330 train_time:130301ms step_avg:61.58ms
step:2117/2330 train_time:130362ms step_avg:61.58ms
step:2118/2330 train_time:130426ms step_avg:61.58ms
step:2119/2330 train_time:130484ms step_avg:61.58ms
step:2120/2330 train_time:130549ms step_avg:61.58ms
step:2121/2330 train_time:130608ms step_avg:61.58ms
step:2122/2330 train_time:130673ms step_avg:61.58ms
step:2123/2330 train_time:130734ms step_avg:61.58ms
step:2124/2330 train_time:130799ms step_avg:61.58ms
step:2125/2330 train_time:130860ms step_avg:61.58ms
step:2126/2330 train_time:130923ms step_avg:61.58ms
step:2127/2330 train_time:130983ms step_avg:61.58ms
step:2128/2330 train_time:131046ms step_avg:61.58ms
step:2129/2330 train_time:131107ms step_avg:61.58ms
step:2130/2330 train_time:131171ms step_avg:61.58ms
step:2131/2330 train_time:131231ms step_avg:61.58ms
step:2132/2330 train_time:131295ms step_avg:61.58ms
step:2133/2330 train_time:131356ms step_avg:61.58ms
step:2134/2330 train_time:131421ms step_avg:61.58ms
step:2135/2330 train_time:131482ms step_avg:61.58ms
step:2136/2330 train_time:131545ms step_avg:61.58ms
step:2137/2330 train_time:131604ms step_avg:61.58ms
step:2138/2330 train_time:131667ms step_avg:61.58ms
step:2139/2330 train_time:131729ms step_avg:61.58ms
step:2140/2330 train_time:131792ms step_avg:61.59ms
step:2141/2330 train_time:131854ms step_avg:61.59ms
step:2142/2330 train_time:131919ms step_avg:61.59ms
step:2143/2330 train_time:131980ms step_avg:61.59ms
step:2144/2330 train_time:132043ms step_avg:61.59ms
step:2145/2330 train_time:132104ms step_avg:61.59ms
step:2146/2330 train_time:132166ms step_avg:61.59ms
step:2147/2330 train_time:132228ms step_avg:61.59ms
step:2148/2330 train_time:132291ms step_avg:61.59ms
step:2149/2330 train_time:132351ms step_avg:61.59ms
step:2150/2330 train_time:132415ms step_avg:61.59ms
step:2151/2330 train_time:132476ms step_avg:61.59ms
step:2152/2330 train_time:132541ms step_avg:61.59ms
step:2153/2330 train_time:132602ms step_avg:61.59ms
step:2154/2330 train_time:132664ms step_avg:61.59ms
step:2155/2330 train_time:132725ms step_avg:61.59ms
step:2156/2330 train_time:132789ms step_avg:61.59ms
step:2157/2330 train_time:132849ms step_avg:61.59ms
step:2158/2330 train_time:132913ms step_avg:61.59ms
step:2159/2330 train_time:132974ms step_avg:61.59ms
step:2160/2330 train_time:133038ms step_avg:61.59ms
step:2161/2330 train_time:133101ms step_avg:61.59ms
step:2162/2330 train_time:133163ms step_avg:61.59ms
step:2163/2330 train_time:133224ms step_avg:61.59ms
step:2164/2330 train_time:133286ms step_avg:61.59ms
step:2165/2330 train_time:133347ms step_avg:61.59ms
step:2166/2330 train_time:133411ms step_avg:61.59ms
step:2167/2330 train_time:133471ms step_avg:61.59ms
step:2168/2330 train_time:133535ms step_avg:61.59ms
step:2169/2330 train_time:133597ms step_avg:61.59ms
step:2170/2330 train_time:133661ms step_avg:61.59ms
step:2171/2330 train_time:133723ms step_avg:61.60ms
step:2172/2330 train_time:133785ms step_avg:61.60ms
step:2173/2330 train_time:133845ms step_avg:61.59ms
step:2174/2330 train_time:133908ms step_avg:61.60ms
step:2175/2330 train_time:133968ms step_avg:61.59ms
step:2176/2330 train_time:134032ms step_avg:61.60ms
step:2177/2330 train_time:134093ms step_avg:61.60ms
step:2178/2330 train_time:134157ms step_avg:61.60ms
step:2179/2330 train_time:134219ms step_avg:61.60ms
step:2180/2330 train_time:134282ms step_avg:61.60ms
step:2181/2330 train_time:134342ms step_avg:61.60ms
step:2182/2330 train_time:134405ms step_avg:61.60ms
step:2183/2330 train_time:134465ms step_avg:61.60ms
step:2184/2330 train_time:134529ms step_avg:61.60ms
step:2185/2330 train_time:134589ms step_avg:61.60ms
step:2186/2330 train_time:134653ms step_avg:61.60ms
step:2187/2330 train_time:134715ms step_avg:61.60ms
step:2188/2330 train_time:134780ms step_avg:61.60ms
step:2189/2330 train_time:134840ms step_avg:61.60ms
step:2190/2330 train_time:134904ms step_avg:61.60ms
step:2191/2330 train_time:134964ms step_avg:61.60ms
step:2192/2330 train_time:135028ms step_avg:61.60ms
step:2193/2330 train_time:135088ms step_avg:61.60ms
step:2194/2330 train_time:135153ms step_avg:61.60ms
step:2195/2330 train_time:135213ms step_avg:61.60ms
step:2196/2330 train_time:135277ms step_avg:61.60ms
step:2197/2330 train_time:135339ms step_avg:61.60ms
step:2198/2330 train_time:135403ms step_avg:61.60ms
step:2199/2330 train_time:135463ms step_avg:61.60ms
step:2200/2330 train_time:135527ms step_avg:61.60ms
step:2201/2330 train_time:135587ms step_avg:61.60ms
step:2202/2330 train_time:135651ms step_avg:61.60ms
step:2203/2330 train_time:135711ms step_avg:61.60ms
step:2204/2330 train_time:135776ms step_avg:61.60ms
step:2205/2330 train_time:135838ms step_avg:61.60ms
step:2206/2330 train_time:135901ms step_avg:61.61ms
step:2207/2330 train_time:135962ms step_avg:61.60ms
step:2208/2330 train_time:136025ms step_avg:61.61ms
step:2209/2330 train_time:136085ms step_avg:61.60ms
step:2210/2330 train_time:136149ms step_avg:61.61ms
step:2211/2330 train_time:136209ms step_avg:61.61ms
step:2212/2330 train_time:136273ms step_avg:61.61ms
step:2213/2330 train_time:136334ms step_avg:61.61ms
step:2214/2330 train_time:136398ms step_avg:61.61ms
step:2215/2330 train_time:136460ms step_avg:61.61ms
step:2216/2330 train_time:136524ms step_avg:61.61ms
step:2217/2330 train_time:136583ms step_avg:61.61ms
step:2218/2330 train_time:136647ms step_avg:61.61ms
step:2219/2330 train_time:136706ms step_avg:61.61ms
step:2220/2330 train_time:136770ms step_avg:61.61ms
step:2221/2330 train_time:136831ms step_avg:61.61ms
step:2222/2330 train_time:136894ms step_avg:61.61ms
step:2223/2330 train_time:136956ms step_avg:61.61ms
step:2224/2330 train_time:137022ms step_avg:61.61ms
step:2225/2330 train_time:137082ms step_avg:61.61ms
step:2226/2330 train_time:137145ms step_avg:61.61ms
step:2227/2330 train_time:137205ms step_avg:61.61ms
step:2228/2330 train_time:137269ms step_avg:61.61ms
step:2229/2330 train_time:137329ms step_avg:61.61ms
step:2230/2330 train_time:137392ms step_avg:61.61ms
step:2231/2330 train_time:137453ms step_avg:61.61ms
step:2232/2330 train_time:137518ms step_avg:61.61ms
step:2233/2330 train_time:137579ms step_avg:61.61ms
step:2234/2330 train_time:137642ms step_avg:61.61ms
step:2235/2330 train_time:137703ms step_avg:61.61ms
step:2236/2330 train_time:137766ms step_avg:61.61ms
step:2237/2330 train_time:137827ms step_avg:61.61ms
step:2238/2330 train_time:137891ms step_avg:61.61ms
step:2239/2330 train_time:137951ms step_avg:61.61ms
step:2240/2330 train_time:138015ms step_avg:61.61ms
step:2241/2330 train_time:138077ms step_avg:61.61ms
step:2242/2330 train_time:138141ms step_avg:61.62ms
step:2243/2330 train_time:138202ms step_avg:61.61ms
step:2244/2330 train_time:138265ms step_avg:61.62ms
step:2245/2330 train_time:138326ms step_avg:61.62ms
step:2246/2330 train_time:138388ms step_avg:61.62ms
step:2247/2330 train_time:138449ms step_avg:61.62ms
step:2248/2330 train_time:138512ms step_avg:61.62ms
step:2249/2330 train_time:138572ms step_avg:61.61ms
step:2250/2330 train_time:138636ms step_avg:61.62ms
step:2250/2330 val_loss:4.0259 train_time:138712ms step_avg:61.65ms
step:2251/2330 train_time:138735ms step_avg:61.63ms
step:2252/2330 train_time:138764ms step_avg:61.62ms
step:2253/2330 train_time:138828ms step_avg:61.62ms
step:2254/2330 train_time:138895ms step_avg:61.62ms
step:2255/2330 train_time:138956ms step_avg:61.62ms
step:2256/2330 train_time:139020ms step_avg:61.62ms
step:2257/2330 train_time:139079ms step_avg:61.62ms
step:2258/2330 train_time:139143ms step_avg:61.62ms
step:2259/2330 train_time:139202ms step_avg:61.62ms
step:2260/2330 train_time:139264ms step_avg:61.62ms
step:2261/2330 train_time:139324ms step_avg:61.62ms
step:2262/2330 train_time:139386ms step_avg:61.62ms
step:2263/2330 train_time:139447ms step_avg:61.62ms
step:2264/2330 train_time:139510ms step_avg:61.62ms
step:2265/2330 train_time:139570ms step_avg:61.62ms
step:2266/2330 train_time:139633ms step_avg:61.62ms
step:2267/2330 train_time:139694ms step_avg:61.62ms
step:2268/2330 train_time:139760ms step_avg:61.62ms
step:2269/2330 train_time:139821ms step_avg:61.62ms
step:2270/2330 train_time:139885ms step_avg:61.62ms
step:2271/2330 train_time:139947ms step_avg:61.62ms
step:2272/2330 train_time:140011ms step_avg:61.62ms
step:2273/2330 train_time:140071ms step_avg:61.62ms
step:2274/2330 train_time:140136ms step_avg:61.63ms
step:2275/2330 train_time:140196ms step_avg:61.62ms
step:2276/2330 train_time:140261ms step_avg:61.63ms
step:2277/2330 train_time:140321ms step_avg:61.63ms
step:2278/2330 train_time:140384ms step_avg:61.63ms
step:2279/2330 train_time:140444ms step_avg:61.63ms
step:2280/2330 train_time:140507ms step_avg:61.63ms
step:2281/2330 train_time:140567ms step_avg:61.63ms
step:2282/2330 train_time:140630ms step_avg:61.63ms
step:2283/2330 train_time:140692ms step_avg:61.63ms
step:2284/2330 train_time:140756ms step_avg:61.63ms
step:2285/2330 train_time:140817ms step_avg:61.63ms
step:2286/2330 train_time:140882ms step_avg:61.63ms
step:2287/2330 train_time:140943ms step_avg:61.63ms
step:2288/2330 train_time:141006ms step_avg:61.63ms
step:2289/2330 train_time:141068ms step_avg:61.63ms
step:2290/2330 train_time:141132ms step_avg:61.63ms
step:2291/2330 train_time:141192ms step_avg:61.63ms
step:2292/2330 train_time:141256ms step_avg:61.63ms
step:2293/2330 train_time:141317ms step_avg:61.63ms
step:2294/2330 train_time:141381ms step_avg:61.63ms
step:2295/2330 train_time:141442ms step_avg:61.63ms
step:2296/2330 train_time:141504ms step_avg:61.63ms
step:2297/2330 train_time:141564ms step_avg:61.63ms
step:2298/2330 train_time:141629ms step_avg:61.63ms
step:2299/2330 train_time:141689ms step_avg:61.63ms
step:2300/2330 train_time:141752ms step_avg:61.63ms
step:2301/2330 train_time:141813ms step_avg:61.63ms
step:2302/2330 train_time:141878ms step_avg:61.63ms
step:2303/2330 train_time:141941ms step_avg:61.63ms
step:2304/2330 train_time:142005ms step_avg:61.63ms
step:2305/2330 train_time:142064ms step_avg:61.63ms
step:2306/2330 train_time:142128ms step_avg:61.63ms
step:2307/2330 train_time:142188ms step_avg:61.63ms
step:2308/2330 train_time:142253ms step_avg:61.63ms
step:2309/2330 train_time:142313ms step_avg:61.63ms
step:2310/2330 train_time:142377ms step_avg:61.63ms
step:2311/2330 train_time:142438ms step_avg:61.63ms
step:2312/2330 train_time:142501ms step_avg:61.64ms
step:2313/2330 train_time:142562ms step_avg:61.63ms
step:2314/2330 train_time:142625ms step_avg:61.64ms
step:2315/2330 train_time:142685ms step_avg:61.63ms
step:2316/2330 train_time:142749ms step_avg:61.64ms
step:2317/2330 train_time:142809ms step_avg:61.64ms
step:2318/2330 train_time:142873ms step_avg:61.64ms
step:2319/2330 train_time:142935ms step_avg:61.64ms
step:2320/2330 train_time:142999ms step_avg:61.64ms
step:2321/2330 train_time:143060ms step_avg:61.64ms
step:2322/2330 train_time:143124ms step_avg:61.64ms
step:2323/2330 train_time:143185ms step_avg:61.64ms
step:2324/2330 train_time:143248ms step_avg:61.64ms
step:2325/2330 train_time:143308ms step_avg:61.64ms
step:2326/2330 train_time:143372ms step_avg:61.64ms
step:2327/2330 train_time:143432ms step_avg:61.64ms
step:2328/2330 train_time:143497ms step_avg:61.64ms
step:2329/2330 train_time:143558ms step_avg:61.64ms
step:2330/2330 train_time:143622ms step_avg:61.64ms
step:2330/2330 val_loss:4.0132 train_time:143696ms step_avg:61.67ms
peak memory allocated: 29512 MiB reserved: 44076 MiB
