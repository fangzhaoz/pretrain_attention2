import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "adam_lr5e-4"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-1, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.Adam(
    hidden_matrix_params + gate_params,
    lr=5e-4,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 00:11:04 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   30C    P0             111W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   29C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   28C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   28C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:97ms step_avg:96.75ms
step:2/2330 train_time:188ms step_avg:93.84ms
step:3/2330 train_time:208ms step_avg:69.31ms
step:4/2330 train_time:229ms step_avg:57.17ms
step:5/2330 train_time:284ms step_avg:56.75ms
step:6/2330 train_time:343ms step_avg:57.20ms
step:7/2330 train_time:399ms step_avg:56.97ms
step:8/2330 train_time:459ms step_avg:57.39ms
step:9/2330 train_time:515ms step_avg:57.20ms
step:10/2330 train_time:576ms step_avg:57.55ms
step:11/2330 train_time:631ms step_avg:57.37ms
step:12/2330 train_time:691ms step_avg:57.62ms
step:13/2330 train_time:748ms step_avg:57.51ms
step:14/2330 train_time:807ms step_avg:57.66ms
step:15/2330 train_time:863ms step_avg:57.55ms
step:16/2330 train_time:923ms step_avg:57.70ms
step:17/2330 train_time:979ms step_avg:57.61ms
step:18/2330 train_time:1039ms step_avg:57.75ms
step:19/2330 train_time:1096ms step_avg:57.70ms
step:20/2330 train_time:1162ms step_avg:58.11ms
step:21/2330 train_time:1221ms step_avg:58.13ms
step:22/2330 train_time:1284ms step_avg:58.37ms
step:23/2330 train_time:1341ms step_avg:58.28ms
step:24/2330 train_time:1402ms step_avg:58.43ms
step:25/2330 train_time:1459ms step_avg:58.34ms
step:26/2330 train_time:1520ms step_avg:58.45ms
step:27/2330 train_time:1576ms step_avg:58.35ms
step:28/2330 train_time:1637ms step_avg:58.45ms
step:29/2330 train_time:1693ms step_avg:58.37ms
step:30/2330 train_time:1755ms step_avg:58.49ms
step:31/2330 train_time:1811ms step_avg:58.42ms
step:32/2330 train_time:1871ms step_avg:58.47ms
step:33/2330 train_time:1927ms step_avg:58.39ms
step:34/2330 train_time:1987ms step_avg:58.44ms
step:35/2330 train_time:2042ms step_avg:58.36ms
step:36/2330 train_time:2104ms step_avg:58.46ms
step:37/2330 train_time:2162ms step_avg:58.44ms
step:38/2330 train_time:2225ms step_avg:58.55ms
step:39/2330 train_time:2282ms step_avg:58.51ms
step:40/2330 train_time:2343ms step_avg:58.58ms
step:41/2330 train_time:2400ms step_avg:58.53ms
step:42/2330 train_time:2462ms step_avg:58.62ms
step:43/2330 train_time:2518ms step_avg:58.57ms
step:44/2330 train_time:2580ms step_avg:58.64ms
step:45/2330 train_time:2637ms step_avg:58.60ms
step:46/2330 train_time:2699ms step_avg:58.66ms
step:47/2330 train_time:2755ms step_avg:58.62ms
step:48/2330 train_time:2816ms step_avg:58.66ms
step:49/2330 train_time:2873ms step_avg:58.62ms
step:50/2330 train_time:2932ms step_avg:58.65ms
step:51/2330 train_time:2989ms step_avg:58.60ms
step:52/2330 train_time:3050ms step_avg:58.65ms
step:53/2330 train_time:3106ms step_avg:58.61ms
step:54/2330 train_time:3168ms step_avg:58.68ms
step:55/2330 train_time:3225ms step_avg:58.64ms
step:56/2330 train_time:3286ms step_avg:58.68ms
step:57/2330 train_time:3342ms step_avg:58.63ms
step:58/2330 train_time:3404ms step_avg:58.69ms
step:59/2330 train_time:3461ms step_avg:58.65ms
step:60/2330 train_time:3522ms step_avg:58.70ms
step:61/2330 train_time:3578ms step_avg:58.66ms
step:62/2330 train_time:3641ms step_avg:58.72ms
step:63/2330 train_time:3697ms step_avg:58.69ms
step:64/2330 train_time:3759ms step_avg:58.74ms
step:65/2330 train_time:3816ms step_avg:58.71ms
step:66/2330 train_time:3877ms step_avg:58.75ms
step:67/2330 train_time:3934ms step_avg:58.72ms
step:68/2330 train_time:3995ms step_avg:58.75ms
step:69/2330 train_time:4052ms step_avg:58.73ms
step:70/2330 train_time:4114ms step_avg:58.77ms
step:71/2330 train_time:4171ms step_avg:58.75ms
step:72/2330 train_time:4232ms step_avg:58.78ms
step:73/2330 train_time:4289ms step_avg:58.75ms
step:74/2330 train_time:4351ms step_avg:58.79ms
step:75/2330 train_time:4407ms step_avg:58.76ms
step:76/2330 train_time:4469ms step_avg:58.81ms
step:77/2330 train_time:4525ms step_avg:58.77ms
step:78/2330 train_time:4587ms step_avg:58.80ms
step:79/2330 train_time:4642ms step_avg:58.76ms
step:80/2330 train_time:4705ms step_avg:58.81ms
step:81/2330 train_time:4761ms step_avg:58.78ms
step:82/2330 train_time:4824ms step_avg:58.83ms
step:83/2330 train_time:4880ms step_avg:58.79ms
step:84/2330 train_time:4943ms step_avg:58.84ms
step:85/2330 train_time:4999ms step_avg:58.81ms
step:86/2330 train_time:5061ms step_avg:58.85ms
step:87/2330 train_time:5119ms step_avg:58.83ms
step:88/2330 train_time:5181ms step_avg:58.88ms
step:89/2330 train_time:5238ms step_avg:58.86ms
step:90/2330 train_time:5300ms step_avg:58.89ms
step:91/2330 train_time:5358ms step_avg:58.88ms
step:92/2330 train_time:5418ms step_avg:58.90ms
step:93/2330 train_time:5475ms step_avg:58.87ms
step:94/2330 train_time:5536ms step_avg:58.90ms
step:95/2330 train_time:5593ms step_avg:58.87ms
step:96/2330 train_time:5653ms step_avg:58.89ms
step:97/2330 train_time:5710ms step_avg:58.86ms
step:98/2330 train_time:5771ms step_avg:58.88ms
step:99/2330 train_time:5827ms step_avg:58.86ms
step:100/2330 train_time:5887ms step_avg:58.87ms
step:101/2330 train_time:5944ms step_avg:58.85ms
step:102/2330 train_time:6005ms step_avg:58.87ms
step:103/2330 train_time:6061ms step_avg:58.85ms
step:104/2330 train_time:6124ms step_avg:58.88ms
step:105/2330 train_time:6180ms step_avg:58.86ms
step:106/2330 train_time:6243ms step_avg:58.90ms
step:107/2330 train_time:6300ms step_avg:58.87ms
step:108/2330 train_time:6362ms step_avg:58.91ms
step:109/2330 train_time:6419ms step_avg:58.89ms
step:110/2330 train_time:6480ms step_avg:58.91ms
step:111/2330 train_time:6537ms step_avg:58.89ms
step:112/2330 train_time:6598ms step_avg:58.91ms
step:113/2330 train_time:6656ms step_avg:58.90ms
step:114/2330 train_time:6716ms step_avg:58.91ms
step:115/2330 train_time:6773ms step_avg:58.90ms
step:116/2330 train_time:6834ms step_avg:58.91ms
step:117/2330 train_time:6891ms step_avg:58.89ms
step:118/2330 train_time:6951ms step_avg:58.91ms
step:119/2330 train_time:7007ms step_avg:58.89ms
step:120/2330 train_time:7068ms step_avg:58.90ms
step:121/2330 train_time:7125ms step_avg:58.88ms
step:122/2330 train_time:7185ms step_avg:58.89ms
step:123/2330 train_time:7241ms step_avg:58.87ms
step:124/2330 train_time:7303ms step_avg:58.89ms
step:125/2330 train_time:7360ms step_avg:58.88ms
step:126/2330 train_time:7421ms step_avg:58.90ms
step:127/2330 train_time:7478ms step_avg:58.88ms
step:128/2330 train_time:7539ms step_avg:58.89ms
step:129/2330 train_time:7595ms step_avg:58.88ms
step:130/2330 train_time:7656ms step_avg:58.89ms
step:131/2330 train_time:7713ms step_avg:58.88ms
step:132/2330 train_time:7774ms step_avg:58.89ms
step:133/2330 train_time:7830ms step_avg:58.87ms
step:134/2330 train_time:7890ms step_avg:58.88ms
step:135/2330 train_time:7947ms step_avg:58.87ms
step:136/2330 train_time:8008ms step_avg:58.89ms
step:137/2330 train_time:8065ms step_avg:58.87ms
step:138/2330 train_time:8126ms step_avg:58.88ms
step:139/2330 train_time:8181ms step_avg:58.86ms
step:140/2330 train_time:8242ms step_avg:58.87ms
step:141/2330 train_time:8299ms step_avg:58.86ms
step:142/2330 train_time:8361ms step_avg:58.88ms
step:143/2330 train_time:8417ms step_avg:58.86ms
step:144/2330 train_time:8478ms step_avg:58.88ms
step:145/2330 train_time:8535ms step_avg:58.86ms
step:146/2330 train_time:8596ms step_avg:58.88ms
step:147/2330 train_time:8653ms step_avg:58.86ms
step:148/2330 train_time:8714ms step_avg:58.88ms
step:149/2330 train_time:8771ms step_avg:58.87ms
step:150/2330 train_time:8833ms step_avg:58.88ms
step:151/2330 train_time:8889ms step_avg:58.87ms
step:152/2330 train_time:8950ms step_avg:58.88ms
step:153/2330 train_time:9006ms step_avg:58.87ms
step:154/2330 train_time:9067ms step_avg:58.88ms
step:155/2330 train_time:9123ms step_avg:58.86ms
step:156/2330 train_time:9183ms step_avg:58.87ms
step:157/2330 train_time:9240ms step_avg:58.85ms
step:158/2330 train_time:9301ms step_avg:58.87ms
step:159/2330 train_time:9357ms step_avg:58.85ms
step:160/2330 train_time:9419ms step_avg:58.87ms
step:161/2330 train_time:9476ms step_avg:58.85ms
step:162/2330 train_time:9536ms step_avg:58.87ms
step:163/2330 train_time:9593ms step_avg:58.85ms
step:164/2330 train_time:9655ms step_avg:58.87ms
step:165/2330 train_time:9711ms step_avg:58.86ms
step:166/2330 train_time:9773ms step_avg:58.87ms
step:167/2330 train_time:9830ms step_avg:58.86ms
step:168/2330 train_time:9891ms step_avg:58.87ms
step:169/2330 train_time:9947ms step_avg:58.86ms
step:170/2330 train_time:10008ms step_avg:58.87ms
step:171/2330 train_time:10064ms step_avg:58.86ms
step:172/2330 train_time:10125ms step_avg:58.86ms
step:173/2330 train_time:10181ms step_avg:58.85ms
step:174/2330 train_time:10241ms step_avg:58.86ms
step:175/2330 train_time:10297ms step_avg:58.84ms
step:176/2330 train_time:10359ms step_avg:58.86ms
step:177/2330 train_time:10416ms step_avg:58.85ms
step:178/2330 train_time:10477ms step_avg:58.86ms
step:179/2330 train_time:10533ms step_avg:58.84ms
step:180/2330 train_time:10594ms step_avg:58.86ms
step:181/2330 train_time:10651ms step_avg:58.84ms
step:182/2330 train_time:10712ms step_avg:58.86ms
step:183/2330 train_time:10769ms step_avg:58.85ms
step:184/2330 train_time:10830ms step_avg:58.86ms
step:185/2330 train_time:10886ms step_avg:58.84ms
step:186/2330 train_time:10947ms step_avg:58.86ms
step:187/2330 train_time:11004ms step_avg:58.84ms
step:188/2330 train_time:11064ms step_avg:58.85ms
step:189/2330 train_time:11120ms step_avg:58.84ms
step:190/2330 train_time:11181ms step_avg:58.85ms
step:191/2330 train_time:11236ms step_avg:58.83ms
step:192/2330 train_time:11297ms step_avg:58.84ms
step:193/2330 train_time:11354ms step_avg:58.83ms
step:194/2330 train_time:11415ms step_avg:58.84ms
step:195/2330 train_time:11471ms step_avg:58.83ms
step:196/2330 train_time:11532ms step_avg:58.84ms
step:197/2330 train_time:11589ms step_avg:58.83ms
step:198/2330 train_time:11650ms step_avg:58.84ms
step:199/2330 train_time:11707ms step_avg:58.83ms
step:200/2330 train_time:11767ms step_avg:58.84ms
step:201/2330 train_time:11823ms step_avg:58.82ms
step:202/2330 train_time:11885ms step_avg:58.84ms
step:203/2330 train_time:11942ms step_avg:58.83ms
step:204/2330 train_time:12003ms step_avg:58.84ms
step:205/2330 train_time:12059ms step_avg:58.82ms
step:206/2330 train_time:12120ms step_avg:58.83ms
step:207/2330 train_time:12176ms step_avg:58.82ms
step:208/2330 train_time:12236ms step_avg:58.83ms
step:209/2330 train_time:12292ms step_avg:58.82ms
step:210/2330 train_time:12354ms step_avg:58.83ms
step:211/2330 train_time:12410ms step_avg:58.81ms
step:212/2330 train_time:12471ms step_avg:58.82ms
step:213/2330 train_time:12527ms step_avg:58.81ms
step:214/2330 train_time:12588ms step_avg:58.82ms
step:215/2330 train_time:12644ms step_avg:58.81ms
step:216/2330 train_time:12705ms step_avg:58.82ms
step:217/2330 train_time:12762ms step_avg:58.81ms
step:218/2330 train_time:12823ms step_avg:58.82ms
step:219/2330 train_time:12880ms step_avg:58.81ms
step:220/2330 train_time:12940ms step_avg:58.82ms
step:221/2330 train_time:12997ms step_avg:58.81ms
step:222/2330 train_time:13058ms step_avg:58.82ms
step:223/2330 train_time:13115ms step_avg:58.81ms
step:224/2330 train_time:13176ms step_avg:58.82ms
step:225/2330 train_time:13232ms step_avg:58.81ms
step:226/2330 train_time:13293ms step_avg:58.82ms
step:227/2330 train_time:13349ms step_avg:58.81ms
step:228/2330 train_time:13410ms step_avg:58.82ms
step:229/2330 train_time:13466ms step_avg:58.80ms
step:230/2330 train_time:13528ms step_avg:58.82ms
step:231/2330 train_time:13584ms step_avg:58.81ms
step:232/2330 train_time:13645ms step_avg:58.81ms
step:233/2330 train_time:13701ms step_avg:58.80ms
step:234/2330 train_time:13763ms step_avg:58.82ms
step:235/2330 train_time:13819ms step_avg:58.81ms
step:236/2330 train_time:13880ms step_avg:58.81ms
step:237/2330 train_time:13937ms step_avg:58.81ms
step:238/2330 train_time:13998ms step_avg:58.82ms
step:239/2330 train_time:14056ms step_avg:58.81ms
step:240/2330 train_time:14116ms step_avg:58.82ms
step:241/2330 train_time:14172ms step_avg:58.81ms
step:242/2330 train_time:14232ms step_avg:58.81ms
step:243/2330 train_time:14289ms step_avg:58.80ms
step:244/2330 train_time:14349ms step_avg:58.81ms
step:245/2330 train_time:14406ms step_avg:58.80ms
step:246/2330 train_time:14466ms step_avg:58.81ms
step:247/2330 train_time:14522ms step_avg:58.80ms
step:248/2330 train_time:14584ms step_avg:58.81ms
step:249/2330 train_time:14640ms step_avg:58.80ms
step:250/2330 train_time:14703ms step_avg:58.81ms
step:250/2330 val_loss:4.9442 train_time:14780ms step_avg:59.12ms
step:251/2330 train_time:14801ms step_avg:58.97ms
step:252/2330 train_time:14822ms step_avg:58.82ms
step:253/2330 train_time:14878ms step_avg:58.81ms
step:254/2330 train_time:14945ms step_avg:58.84ms
step:255/2330 train_time:15001ms step_avg:58.83ms
step:256/2330 train_time:15065ms step_avg:58.85ms
step:257/2330 train_time:15121ms step_avg:58.84ms
step:258/2330 train_time:15183ms step_avg:58.85ms
step:259/2330 train_time:15239ms step_avg:58.84ms
step:260/2330 train_time:15300ms step_avg:58.85ms
step:261/2330 train_time:15356ms step_avg:58.84ms
step:262/2330 train_time:15416ms step_avg:58.84ms
step:263/2330 train_time:15473ms step_avg:58.83ms
step:264/2330 train_time:15532ms step_avg:58.83ms
step:265/2330 train_time:15588ms step_avg:58.82ms
step:266/2330 train_time:15648ms step_avg:58.83ms
step:267/2330 train_time:15705ms step_avg:58.82ms
step:268/2330 train_time:15767ms step_avg:58.83ms
step:269/2330 train_time:15824ms step_avg:58.83ms
step:270/2330 train_time:15886ms step_avg:58.84ms
step:271/2330 train_time:15943ms step_avg:58.83ms
step:272/2330 train_time:16006ms step_avg:58.84ms
step:273/2330 train_time:16062ms step_avg:58.83ms
step:274/2330 train_time:16124ms step_avg:58.85ms
step:275/2330 train_time:16180ms step_avg:58.84ms
step:276/2330 train_time:16242ms step_avg:58.85ms
step:277/2330 train_time:16298ms step_avg:58.84ms
step:278/2330 train_time:16359ms step_avg:58.85ms
step:279/2330 train_time:16416ms step_avg:58.84ms
step:280/2330 train_time:16476ms step_avg:58.84ms
step:281/2330 train_time:16533ms step_avg:58.83ms
step:282/2330 train_time:16594ms step_avg:58.84ms
step:283/2330 train_time:16650ms step_avg:58.83ms
step:284/2330 train_time:16712ms step_avg:58.84ms
step:285/2330 train_time:16768ms step_avg:58.84ms
step:286/2330 train_time:16829ms step_avg:58.84ms
step:287/2330 train_time:16886ms step_avg:58.84ms
step:288/2330 train_time:16947ms step_avg:58.84ms
step:289/2330 train_time:17003ms step_avg:58.83ms
step:290/2330 train_time:17065ms step_avg:58.84ms
step:291/2330 train_time:17121ms step_avg:58.83ms
step:292/2330 train_time:17183ms step_avg:58.85ms
step:293/2330 train_time:17240ms step_avg:58.84ms
step:294/2330 train_time:17301ms step_avg:58.85ms
step:295/2330 train_time:17357ms step_avg:58.84ms
step:296/2330 train_time:17418ms step_avg:58.85ms
step:297/2330 train_time:17475ms step_avg:58.84ms
step:298/2330 train_time:17536ms step_avg:58.84ms
step:299/2330 train_time:17593ms step_avg:58.84ms
step:300/2330 train_time:17654ms step_avg:58.85ms
step:301/2330 train_time:17710ms step_avg:58.84ms
step:302/2330 train_time:17771ms step_avg:58.84ms
step:303/2330 train_time:17828ms step_avg:58.84ms
step:304/2330 train_time:17889ms step_avg:58.85ms
step:305/2330 train_time:17946ms step_avg:58.84ms
step:306/2330 train_time:18007ms step_avg:58.85ms
step:307/2330 train_time:18063ms step_avg:58.84ms
step:308/2330 train_time:18124ms step_avg:58.84ms
step:309/2330 train_time:18181ms step_avg:58.84ms
step:310/2330 train_time:18242ms step_avg:58.84ms
step:311/2330 train_time:18298ms step_avg:58.84ms
step:312/2330 train_time:18359ms step_avg:58.84ms
step:313/2330 train_time:18416ms step_avg:58.84ms
step:314/2330 train_time:18477ms step_avg:58.84ms
step:315/2330 train_time:18534ms step_avg:58.84ms
step:316/2330 train_time:18594ms step_avg:58.84ms
step:317/2330 train_time:18651ms step_avg:58.84ms
step:318/2330 train_time:18712ms step_avg:58.84ms
step:319/2330 train_time:18768ms step_avg:58.83ms
step:320/2330 train_time:18829ms step_avg:58.84ms
step:321/2330 train_time:18886ms step_avg:58.84ms
step:322/2330 train_time:18947ms step_avg:58.84ms
step:323/2330 train_time:19004ms step_avg:58.84ms
step:324/2330 train_time:19065ms step_avg:58.84ms
step:325/2330 train_time:19121ms step_avg:58.83ms
step:326/2330 train_time:19182ms step_avg:58.84ms
step:327/2330 train_time:19238ms step_avg:58.83ms
step:328/2330 train_time:19300ms step_avg:58.84ms
step:329/2330 train_time:19356ms step_avg:58.83ms
step:330/2330 train_time:19417ms step_avg:58.84ms
step:331/2330 train_time:19474ms step_avg:58.83ms
step:332/2330 train_time:19534ms step_avg:58.84ms
step:333/2330 train_time:19591ms step_avg:58.83ms
step:334/2330 train_time:19652ms step_avg:58.84ms
step:335/2330 train_time:19709ms step_avg:58.83ms
step:336/2330 train_time:19769ms step_avg:58.84ms
step:337/2330 train_time:19826ms step_avg:58.83ms
step:338/2330 train_time:19887ms step_avg:58.84ms
step:339/2330 train_time:19944ms step_avg:58.83ms
step:340/2330 train_time:20005ms step_avg:58.84ms
step:341/2330 train_time:20061ms step_avg:58.83ms
step:342/2330 train_time:20122ms step_avg:58.84ms
step:343/2330 train_time:20179ms step_avg:58.83ms
step:344/2330 train_time:20240ms step_avg:58.84ms
step:345/2330 train_time:20296ms step_avg:58.83ms
step:346/2330 train_time:20358ms step_avg:58.84ms
step:347/2330 train_time:20415ms step_avg:58.83ms
step:348/2330 train_time:20476ms step_avg:58.84ms
step:349/2330 train_time:20533ms step_avg:58.83ms
step:350/2330 train_time:20594ms step_avg:58.84ms
step:351/2330 train_time:20650ms step_avg:58.83ms
step:352/2330 train_time:20713ms step_avg:58.84ms
step:353/2330 train_time:20769ms step_avg:58.84ms
step:354/2330 train_time:20832ms step_avg:58.85ms
step:355/2330 train_time:20888ms step_avg:58.84ms
step:356/2330 train_time:20951ms step_avg:58.85ms
step:357/2330 train_time:21007ms step_avg:58.84ms
step:358/2330 train_time:21067ms step_avg:58.85ms
step:359/2330 train_time:21123ms step_avg:58.84ms
step:360/2330 train_time:21184ms step_avg:58.85ms
step:361/2330 train_time:21241ms step_avg:58.84ms
step:362/2330 train_time:21301ms step_avg:58.84ms
step:363/2330 train_time:21357ms step_avg:58.84ms
step:364/2330 train_time:21418ms step_avg:58.84ms
step:365/2330 train_time:21474ms step_avg:58.83ms
step:366/2330 train_time:21536ms step_avg:58.84ms
step:367/2330 train_time:21592ms step_avg:58.83ms
step:368/2330 train_time:21654ms step_avg:58.84ms
step:369/2330 train_time:21710ms step_avg:58.83ms
step:370/2330 train_time:21771ms step_avg:58.84ms
step:371/2330 train_time:21828ms step_avg:58.84ms
step:372/2330 train_time:21889ms step_avg:58.84ms
step:373/2330 train_time:21946ms step_avg:58.84ms
step:374/2330 train_time:22006ms step_avg:58.84ms
step:375/2330 train_time:22062ms step_avg:58.83ms
step:376/2330 train_time:22123ms step_avg:58.84ms
step:377/2330 train_time:22179ms step_avg:58.83ms
step:378/2330 train_time:22240ms step_avg:58.84ms
step:379/2330 train_time:22297ms step_avg:58.83ms
step:380/2330 train_time:22357ms step_avg:58.83ms
step:381/2330 train_time:22413ms step_avg:58.83ms
step:382/2330 train_time:22474ms step_avg:58.83ms
step:383/2330 train_time:22531ms step_avg:58.83ms
step:384/2330 train_time:22593ms step_avg:58.83ms
step:385/2330 train_time:22649ms step_avg:58.83ms
step:386/2330 train_time:22710ms step_avg:58.83ms
step:387/2330 train_time:22766ms step_avg:58.83ms
step:388/2330 train_time:22828ms step_avg:58.83ms
step:389/2330 train_time:22884ms step_avg:58.83ms
step:390/2330 train_time:22945ms step_avg:58.83ms
step:391/2330 train_time:23001ms step_avg:58.83ms
step:392/2330 train_time:23062ms step_avg:58.83ms
step:393/2330 train_time:23119ms step_avg:58.83ms
step:394/2330 train_time:23180ms step_avg:58.83ms
step:395/2330 train_time:23237ms step_avg:58.83ms
step:396/2330 train_time:23298ms step_avg:58.83ms
step:397/2330 train_time:23354ms step_avg:58.83ms
step:398/2330 train_time:23415ms step_avg:58.83ms
step:399/2330 train_time:23471ms step_avg:58.82ms
step:400/2330 train_time:23533ms step_avg:58.83ms
step:401/2330 train_time:23589ms step_avg:58.83ms
step:402/2330 train_time:23651ms step_avg:58.83ms
step:403/2330 train_time:23708ms step_avg:58.83ms
step:404/2330 train_time:23769ms step_avg:58.83ms
step:405/2330 train_time:23825ms step_avg:58.83ms
step:406/2330 train_time:23887ms step_avg:58.83ms
step:407/2330 train_time:23943ms step_avg:58.83ms
step:408/2330 train_time:24005ms step_avg:58.84ms
step:409/2330 train_time:24061ms step_avg:58.83ms
step:410/2330 train_time:24122ms step_avg:58.83ms
step:411/2330 train_time:24178ms step_avg:58.83ms
step:412/2330 train_time:24240ms step_avg:58.84ms
step:413/2330 train_time:24297ms step_avg:58.83ms
step:414/2330 train_time:24358ms step_avg:58.84ms
step:415/2330 train_time:24414ms step_avg:58.83ms
step:416/2330 train_time:24476ms step_avg:58.84ms
step:417/2330 train_time:24533ms step_avg:58.83ms
step:418/2330 train_time:24594ms step_avg:58.84ms
step:419/2330 train_time:24651ms step_avg:58.83ms
step:420/2330 train_time:24713ms step_avg:58.84ms
step:421/2330 train_time:24769ms step_avg:58.83ms
step:422/2330 train_time:24831ms step_avg:58.84ms
step:423/2330 train_time:24887ms step_avg:58.83ms
step:424/2330 train_time:24948ms step_avg:58.84ms
step:425/2330 train_time:25004ms step_avg:58.83ms
step:426/2330 train_time:25066ms step_avg:58.84ms
step:427/2330 train_time:25122ms step_avg:58.83ms
step:428/2330 train_time:25183ms step_avg:58.84ms
step:429/2330 train_time:25239ms step_avg:58.83ms
step:430/2330 train_time:25300ms step_avg:58.84ms
step:431/2330 train_time:25356ms step_avg:58.83ms
step:432/2330 train_time:25419ms step_avg:58.84ms
step:433/2330 train_time:25475ms step_avg:58.83ms
step:434/2330 train_time:25537ms step_avg:58.84ms
step:435/2330 train_time:25594ms step_avg:58.84ms
step:436/2330 train_time:25657ms step_avg:58.85ms
step:437/2330 train_time:25714ms step_avg:58.84ms
step:438/2330 train_time:25775ms step_avg:58.85ms
step:439/2330 train_time:25833ms step_avg:58.84ms
step:440/2330 train_time:25894ms step_avg:58.85ms
step:441/2330 train_time:25951ms step_avg:58.85ms
step:442/2330 train_time:26012ms step_avg:58.85ms
step:443/2330 train_time:26068ms step_avg:58.85ms
step:444/2330 train_time:26130ms step_avg:58.85ms
step:445/2330 train_time:26186ms step_avg:58.84ms
step:446/2330 train_time:26247ms step_avg:58.85ms
step:447/2330 train_time:26303ms step_avg:58.84ms
step:448/2330 train_time:26364ms step_avg:58.85ms
step:449/2330 train_time:26420ms step_avg:58.84ms
step:450/2330 train_time:26482ms step_avg:58.85ms
step:451/2330 train_time:26538ms step_avg:58.84ms
step:452/2330 train_time:26600ms step_avg:58.85ms
step:453/2330 train_time:26656ms step_avg:58.84ms
step:454/2330 train_time:26718ms step_avg:58.85ms
step:455/2330 train_time:26775ms step_avg:58.85ms
step:456/2330 train_time:26837ms step_avg:58.85ms
step:457/2330 train_time:26895ms step_avg:58.85ms
step:458/2330 train_time:26956ms step_avg:58.86ms
step:459/2330 train_time:27013ms step_avg:58.85ms
step:460/2330 train_time:27075ms step_avg:58.86ms
step:461/2330 train_time:27131ms step_avg:58.85ms
step:462/2330 train_time:27194ms step_avg:58.86ms
step:463/2330 train_time:27250ms step_avg:58.86ms
step:464/2330 train_time:27312ms step_avg:58.86ms
step:465/2330 train_time:27368ms step_avg:58.86ms
step:466/2330 train_time:27428ms step_avg:58.86ms
step:467/2330 train_time:27484ms step_avg:58.85ms
step:468/2330 train_time:27546ms step_avg:58.86ms
step:469/2330 train_time:27602ms step_avg:58.85ms
step:470/2330 train_time:27663ms step_avg:58.86ms
step:471/2330 train_time:27718ms step_avg:58.85ms
step:472/2330 train_time:27781ms step_avg:58.86ms
step:473/2330 train_time:27837ms step_avg:58.85ms
step:474/2330 train_time:27899ms step_avg:58.86ms
step:475/2330 train_time:27956ms step_avg:58.85ms
step:476/2330 train_time:28018ms step_avg:58.86ms
step:477/2330 train_time:28076ms step_avg:58.86ms
step:478/2330 train_time:28138ms step_avg:58.87ms
step:479/2330 train_time:28195ms step_avg:58.86ms
step:480/2330 train_time:28257ms step_avg:58.87ms
step:481/2330 train_time:28314ms step_avg:58.86ms
step:482/2330 train_time:28375ms step_avg:58.87ms
step:483/2330 train_time:28432ms step_avg:58.87ms
step:484/2330 train_time:28492ms step_avg:58.87ms
step:485/2330 train_time:28549ms step_avg:58.86ms
step:486/2330 train_time:28609ms step_avg:58.87ms
step:487/2330 train_time:28665ms step_avg:58.86ms
step:488/2330 train_time:28727ms step_avg:58.87ms
step:489/2330 train_time:28783ms step_avg:58.86ms
step:490/2330 train_time:28844ms step_avg:58.86ms
step:491/2330 train_time:28900ms step_avg:58.86ms
step:492/2330 train_time:28963ms step_avg:58.87ms
step:493/2330 train_time:29019ms step_avg:58.86ms
step:494/2330 train_time:29082ms step_avg:58.87ms
step:495/2330 train_time:29139ms step_avg:58.87ms
step:496/2330 train_time:29200ms step_avg:58.87ms
step:497/2330 train_time:29256ms step_avg:58.87ms
step:498/2330 train_time:29318ms step_avg:58.87ms
step:499/2330 train_time:29375ms step_avg:58.87ms
step:500/2330 train_time:29437ms step_avg:58.87ms
step:500/2330 val_loss:4.4588 train_time:29514ms step_avg:59.03ms
step:501/2330 train_time:29535ms step_avg:58.95ms
step:502/2330 train_time:29558ms step_avg:58.88ms
step:503/2330 train_time:29614ms step_avg:58.87ms
step:504/2330 train_time:29680ms step_avg:58.89ms
step:505/2330 train_time:29737ms step_avg:58.89ms
step:506/2330 train_time:29803ms step_avg:58.90ms
step:507/2330 train_time:29859ms step_avg:58.89ms
step:508/2330 train_time:29922ms step_avg:58.90ms
step:509/2330 train_time:29978ms step_avg:58.90ms
step:510/2330 train_time:30038ms step_avg:58.90ms
step:511/2330 train_time:30094ms step_avg:58.89ms
step:512/2330 train_time:30155ms step_avg:58.90ms
step:513/2330 train_time:30211ms step_avg:58.89ms
step:514/2330 train_time:30272ms step_avg:58.90ms
step:515/2330 train_time:30329ms step_avg:58.89ms
step:516/2330 train_time:30389ms step_avg:58.89ms
step:517/2330 train_time:30446ms step_avg:58.89ms
step:518/2330 train_time:30506ms step_avg:58.89ms
step:519/2330 train_time:30563ms step_avg:58.89ms
step:520/2330 train_time:30626ms step_avg:58.90ms
step:521/2330 train_time:30683ms step_avg:58.89ms
step:522/2330 train_time:30746ms step_avg:58.90ms
step:523/2330 train_time:30802ms step_avg:58.89ms
step:524/2330 train_time:30864ms step_avg:58.90ms
step:525/2330 train_time:30920ms step_avg:58.90ms
step:526/2330 train_time:30981ms step_avg:58.90ms
step:527/2330 train_time:31036ms step_avg:58.89ms
step:528/2330 train_time:31099ms step_avg:58.90ms
step:529/2330 train_time:31155ms step_avg:58.89ms
step:530/2330 train_time:31216ms step_avg:58.90ms
step:531/2330 train_time:31272ms step_avg:58.89ms
step:532/2330 train_time:31333ms step_avg:58.90ms
step:533/2330 train_time:31389ms step_avg:58.89ms
step:534/2330 train_time:31451ms step_avg:58.90ms
step:535/2330 train_time:31507ms step_avg:58.89ms
step:536/2330 train_time:31569ms step_avg:58.90ms
step:537/2330 train_time:31625ms step_avg:58.89ms
step:538/2330 train_time:31688ms step_avg:58.90ms
step:539/2330 train_time:31746ms step_avg:58.90ms
step:540/2330 train_time:31808ms step_avg:58.90ms
step:541/2330 train_time:31864ms step_avg:58.90ms
step:542/2330 train_time:31925ms step_avg:58.90ms
step:543/2330 train_time:31982ms step_avg:58.90ms
step:544/2330 train_time:32043ms step_avg:58.90ms
step:545/2330 train_time:32099ms step_avg:58.90ms
step:546/2330 train_time:32159ms step_avg:58.90ms
step:547/2330 train_time:32215ms step_avg:58.89ms
step:548/2330 train_time:32277ms step_avg:58.90ms
step:549/2330 train_time:32333ms step_avg:58.89ms
step:550/2330 train_time:32395ms step_avg:58.90ms
step:551/2330 train_time:32452ms step_avg:58.90ms
step:552/2330 train_time:32513ms step_avg:58.90ms
step:553/2330 train_time:32570ms step_avg:58.90ms
step:554/2330 train_time:32632ms step_avg:58.90ms
step:555/2330 train_time:32690ms step_avg:58.90ms
step:556/2330 train_time:32753ms step_avg:58.91ms
step:557/2330 train_time:32809ms step_avg:58.90ms
step:558/2330 train_time:32873ms step_avg:58.91ms
step:559/2330 train_time:32930ms step_avg:58.91ms
step:560/2330 train_time:32991ms step_avg:58.91ms
step:561/2330 train_time:33047ms step_avg:58.91ms
step:562/2330 train_time:33108ms step_avg:58.91ms
step:563/2330 train_time:33164ms step_avg:58.91ms
step:564/2330 train_time:33225ms step_avg:58.91ms
step:565/2330 train_time:33281ms step_avg:58.91ms
step:566/2330 train_time:33342ms step_avg:58.91ms
step:567/2330 train_time:33398ms step_avg:58.90ms
step:568/2330 train_time:33459ms step_avg:58.91ms
step:569/2330 train_time:33515ms step_avg:58.90ms
step:570/2330 train_time:33578ms step_avg:58.91ms
step:571/2330 train_time:33635ms step_avg:58.90ms
step:572/2330 train_time:33698ms step_avg:58.91ms
step:573/2330 train_time:33755ms step_avg:58.91ms
step:574/2330 train_time:33817ms step_avg:58.92ms
step:575/2330 train_time:33875ms step_avg:58.91ms
step:576/2330 train_time:33936ms step_avg:58.92ms
step:577/2330 train_time:33993ms step_avg:58.91ms
step:578/2330 train_time:34055ms step_avg:58.92ms
step:579/2330 train_time:34112ms step_avg:58.92ms
step:580/2330 train_time:34173ms step_avg:58.92ms
step:581/2330 train_time:34229ms step_avg:58.91ms
step:582/2330 train_time:34290ms step_avg:58.92ms
step:583/2330 train_time:34346ms step_avg:58.91ms
step:584/2330 train_time:34407ms step_avg:58.92ms
step:585/2330 train_time:34462ms step_avg:58.91ms
step:586/2330 train_time:34524ms step_avg:58.91ms
step:587/2330 train_time:34580ms step_avg:58.91ms
step:588/2330 train_time:34642ms step_avg:58.91ms
step:589/2330 train_time:34697ms step_avg:58.91ms
step:590/2330 train_time:34759ms step_avg:58.91ms
step:591/2330 train_time:34816ms step_avg:58.91ms
step:592/2330 train_time:34879ms step_avg:58.92ms
step:593/2330 train_time:34936ms step_avg:58.91ms
step:594/2330 train_time:34998ms step_avg:58.92ms
step:595/2330 train_time:35055ms step_avg:58.92ms
step:596/2330 train_time:35117ms step_avg:58.92ms
step:597/2330 train_time:35174ms step_avg:58.92ms
step:598/2330 train_time:35236ms step_avg:58.92ms
step:599/2330 train_time:35293ms step_avg:58.92ms
step:600/2330 train_time:35353ms step_avg:58.92ms
step:601/2330 train_time:35410ms step_avg:58.92ms
step:602/2330 train_time:35470ms step_avg:58.92ms
step:603/2330 train_time:35526ms step_avg:58.92ms
step:604/2330 train_time:35588ms step_avg:58.92ms
step:605/2330 train_time:35644ms step_avg:58.92ms
step:606/2330 train_time:35706ms step_avg:58.92ms
step:607/2330 train_time:35762ms step_avg:58.92ms
step:608/2330 train_time:35823ms step_avg:58.92ms
step:609/2330 train_time:35879ms step_avg:58.92ms
step:610/2330 train_time:35941ms step_avg:58.92ms
step:611/2330 train_time:35997ms step_avg:58.91ms
step:612/2330 train_time:36059ms step_avg:58.92ms
step:613/2330 train_time:36116ms step_avg:58.92ms
step:614/2330 train_time:36178ms step_avg:58.92ms
step:615/2330 train_time:36235ms step_avg:58.92ms
step:616/2330 train_time:36296ms step_avg:58.92ms
step:617/2330 train_time:36353ms step_avg:58.92ms
step:618/2330 train_time:36413ms step_avg:58.92ms
step:619/2330 train_time:36470ms step_avg:58.92ms
step:620/2330 train_time:36532ms step_avg:58.92ms
step:621/2330 train_time:36589ms step_avg:58.92ms
step:622/2330 train_time:36650ms step_avg:58.92ms
step:623/2330 train_time:36707ms step_avg:58.92ms
step:624/2330 train_time:36768ms step_avg:58.92ms
step:625/2330 train_time:36825ms step_avg:58.92ms
step:626/2330 train_time:36886ms step_avg:58.92ms
step:627/2330 train_time:36942ms step_avg:58.92ms
step:628/2330 train_time:37003ms step_avg:58.92ms
step:629/2330 train_time:37059ms step_avg:58.92ms
step:630/2330 train_time:37120ms step_avg:58.92ms
step:631/2330 train_time:37176ms step_avg:58.92ms
step:632/2330 train_time:37239ms step_avg:58.92ms
step:633/2330 train_time:37295ms step_avg:58.92ms
step:634/2330 train_time:37357ms step_avg:58.92ms
step:635/2330 train_time:37413ms step_avg:58.92ms
step:636/2330 train_time:37474ms step_avg:58.92ms
step:637/2330 train_time:37532ms step_avg:58.92ms
step:638/2330 train_time:37594ms step_avg:58.92ms
step:639/2330 train_time:37650ms step_avg:58.92ms
step:640/2330 train_time:37712ms step_avg:58.92ms
step:641/2330 train_time:37769ms step_avg:58.92ms
step:642/2330 train_time:37831ms step_avg:58.93ms
step:643/2330 train_time:37888ms step_avg:58.92ms
step:644/2330 train_time:37950ms step_avg:58.93ms
step:645/2330 train_time:38006ms step_avg:58.92ms
step:646/2330 train_time:38068ms step_avg:58.93ms
step:647/2330 train_time:38124ms step_avg:58.92ms
step:648/2330 train_time:38186ms step_avg:58.93ms
step:649/2330 train_time:38242ms step_avg:58.92ms
step:650/2330 train_time:38303ms step_avg:58.93ms
step:651/2330 train_time:38359ms step_avg:58.92ms
step:652/2330 train_time:38420ms step_avg:58.93ms
step:653/2330 train_time:38476ms step_avg:58.92ms
step:654/2330 train_time:38539ms step_avg:58.93ms
step:655/2330 train_time:38596ms step_avg:58.92ms
step:656/2330 train_time:38658ms step_avg:58.93ms
step:657/2330 train_time:38715ms step_avg:58.93ms
step:658/2330 train_time:38777ms step_avg:58.93ms
step:659/2330 train_time:38835ms step_avg:58.93ms
step:660/2330 train_time:38896ms step_avg:58.93ms
step:661/2330 train_time:38953ms step_avg:58.93ms
step:662/2330 train_time:39014ms step_avg:58.93ms
step:663/2330 train_time:39070ms step_avg:58.93ms
step:664/2330 train_time:39132ms step_avg:58.93ms
step:665/2330 train_time:39189ms step_avg:58.93ms
step:666/2330 train_time:39250ms step_avg:58.93ms
step:667/2330 train_time:39306ms step_avg:58.93ms
step:668/2330 train_time:39367ms step_avg:58.93ms
step:669/2330 train_time:39423ms step_avg:58.93ms
step:670/2330 train_time:39485ms step_avg:58.93ms
step:671/2330 train_time:39542ms step_avg:58.93ms
step:672/2330 train_time:39603ms step_avg:58.93ms
step:673/2330 train_time:39659ms step_avg:58.93ms
step:674/2330 train_time:39720ms step_avg:58.93ms
step:675/2330 train_time:39777ms step_avg:58.93ms
step:676/2330 train_time:39840ms step_avg:58.93ms
step:677/2330 train_time:39897ms step_avg:58.93ms
step:678/2330 train_time:39957ms step_avg:58.93ms
step:679/2330 train_time:40014ms step_avg:58.93ms
step:680/2330 train_time:40076ms step_avg:58.94ms
step:681/2330 train_time:40133ms step_avg:58.93ms
step:682/2330 train_time:40196ms step_avg:58.94ms
step:683/2330 train_time:40252ms step_avg:58.93ms
step:684/2330 train_time:40313ms step_avg:58.94ms
step:685/2330 train_time:40371ms step_avg:58.94ms
step:686/2330 train_time:40432ms step_avg:58.94ms
step:687/2330 train_time:40490ms step_avg:58.94ms
step:688/2330 train_time:40551ms step_avg:58.94ms
step:689/2330 train_time:40607ms step_avg:58.94ms
step:690/2330 train_time:40669ms step_avg:58.94ms
step:691/2330 train_time:40725ms step_avg:58.94ms
step:692/2330 train_time:40787ms step_avg:58.94ms
step:693/2330 train_time:40843ms step_avg:58.94ms
step:694/2330 train_time:40904ms step_avg:58.94ms
step:695/2330 train_time:40960ms step_avg:58.93ms
step:696/2330 train_time:41021ms step_avg:58.94ms
step:697/2330 train_time:41078ms step_avg:58.94ms
step:698/2330 train_time:41140ms step_avg:58.94ms
step:699/2330 train_time:41196ms step_avg:58.94ms
step:700/2330 train_time:41259ms step_avg:58.94ms
step:701/2330 train_time:41316ms step_avg:58.94ms
step:702/2330 train_time:41377ms step_avg:58.94ms
step:703/2330 train_time:41435ms step_avg:58.94ms
step:704/2330 train_time:41495ms step_avg:58.94ms
step:705/2330 train_time:41552ms step_avg:58.94ms
step:706/2330 train_time:41614ms step_avg:58.94ms
step:707/2330 train_time:41671ms step_avg:58.94ms
step:708/2330 train_time:41733ms step_avg:58.95ms
step:709/2330 train_time:41790ms step_avg:58.94ms
step:710/2330 train_time:41852ms step_avg:58.95ms
step:711/2330 train_time:41908ms step_avg:58.94ms
step:712/2330 train_time:41971ms step_avg:58.95ms
step:713/2330 train_time:42027ms step_avg:58.94ms
step:714/2330 train_time:42090ms step_avg:58.95ms
step:715/2330 train_time:42146ms step_avg:58.95ms
step:716/2330 train_time:42207ms step_avg:58.95ms
step:717/2330 train_time:42263ms step_avg:58.94ms
step:718/2330 train_time:42324ms step_avg:58.95ms
step:719/2330 train_time:42381ms step_avg:58.94ms
step:720/2330 train_time:42442ms step_avg:58.95ms
step:721/2330 train_time:42498ms step_avg:58.94ms
step:722/2330 train_time:42560ms step_avg:58.95ms
step:723/2330 train_time:42616ms step_avg:58.94ms
step:724/2330 train_time:42678ms step_avg:58.95ms
step:725/2330 train_time:42734ms step_avg:58.94ms
step:726/2330 train_time:42796ms step_avg:58.95ms
step:727/2330 train_time:42853ms step_avg:58.95ms
step:728/2330 train_time:42914ms step_avg:58.95ms
step:729/2330 train_time:42971ms step_avg:58.95ms
step:730/2330 train_time:43033ms step_avg:58.95ms
step:731/2330 train_time:43090ms step_avg:58.95ms
step:732/2330 train_time:43151ms step_avg:58.95ms
step:733/2330 train_time:43207ms step_avg:58.95ms
step:734/2330 train_time:43269ms step_avg:58.95ms
step:735/2330 train_time:43325ms step_avg:58.95ms
step:736/2330 train_time:43387ms step_avg:58.95ms
step:737/2330 train_time:43443ms step_avg:58.95ms
step:738/2330 train_time:43503ms step_avg:58.95ms
step:739/2330 train_time:43559ms step_avg:58.94ms
step:740/2330 train_time:43620ms step_avg:58.95ms
step:741/2330 train_time:43676ms step_avg:58.94ms
step:742/2330 train_time:43739ms step_avg:58.95ms
step:743/2330 train_time:43795ms step_avg:58.94ms
step:744/2330 train_time:43856ms step_avg:58.95ms
step:745/2330 train_time:43913ms step_avg:58.94ms
step:746/2330 train_time:43974ms step_avg:58.95ms
step:747/2330 train_time:44031ms step_avg:58.94ms
step:748/2330 train_time:44093ms step_avg:58.95ms
step:749/2330 train_time:44149ms step_avg:58.94ms
step:750/2330 train_time:44212ms step_avg:58.95ms
step:750/2330 val_loss:4.2400 train_time:44290ms step_avg:59.05ms
step:751/2330 train_time:44311ms step_avg:59.00ms
step:752/2330 train_time:44333ms step_avg:58.95ms
step:753/2330 train_time:44391ms step_avg:58.95ms
step:754/2330 train_time:44455ms step_avg:58.96ms
step:755/2330 train_time:44512ms step_avg:58.96ms
step:756/2330 train_time:44577ms step_avg:58.96ms
step:757/2330 train_time:44633ms step_avg:58.96ms
step:758/2330 train_time:44695ms step_avg:58.96ms
step:759/2330 train_time:44752ms step_avg:58.96ms
step:760/2330 train_time:44812ms step_avg:58.96ms
step:761/2330 train_time:44868ms step_avg:58.96ms
step:762/2330 train_time:44928ms step_avg:58.96ms
step:763/2330 train_time:44983ms step_avg:58.96ms
step:764/2330 train_time:45043ms step_avg:58.96ms
step:765/2330 train_time:45100ms step_avg:58.95ms
step:766/2330 train_time:45161ms step_avg:58.96ms
step:767/2330 train_time:45218ms step_avg:58.95ms
step:768/2330 train_time:45280ms step_avg:58.96ms
step:769/2330 train_time:45338ms step_avg:58.96ms
step:770/2330 train_time:45401ms step_avg:58.96ms
step:771/2330 train_time:45458ms step_avg:58.96ms
step:772/2330 train_time:45523ms step_avg:58.97ms
step:773/2330 train_time:45580ms step_avg:58.97ms
step:774/2330 train_time:45643ms step_avg:58.97ms
step:775/2330 train_time:45699ms step_avg:58.97ms
step:776/2330 train_time:45763ms step_avg:58.97ms
step:777/2330 train_time:45820ms step_avg:58.97ms
step:778/2330 train_time:45881ms step_avg:58.97ms
step:779/2330 train_time:45939ms step_avg:58.97ms
step:780/2330 train_time:46000ms step_avg:58.97ms
step:781/2330 train_time:46057ms step_avg:58.97ms
step:782/2330 train_time:46118ms step_avg:58.97ms
step:783/2330 train_time:46175ms step_avg:58.97ms
step:784/2330 train_time:46237ms step_avg:58.98ms
step:785/2330 train_time:46295ms step_avg:58.97ms
step:786/2330 train_time:46357ms step_avg:58.98ms
step:787/2330 train_time:46416ms step_avg:58.98ms
step:788/2330 train_time:46479ms step_avg:58.98ms
step:789/2330 train_time:46536ms step_avg:58.98ms
step:790/2330 train_time:46599ms step_avg:58.99ms
step:791/2330 train_time:46656ms step_avg:58.98ms
step:792/2330 train_time:46720ms step_avg:58.99ms
step:793/2330 train_time:46777ms step_avg:58.99ms
step:794/2330 train_time:46839ms step_avg:58.99ms
step:795/2330 train_time:46896ms step_avg:58.99ms
step:796/2330 train_time:46958ms step_avg:58.99ms
step:797/2330 train_time:47016ms step_avg:58.99ms
step:798/2330 train_time:47077ms step_avg:58.99ms
step:799/2330 train_time:47134ms step_avg:58.99ms
step:800/2330 train_time:47196ms step_avg:58.99ms
step:801/2330 train_time:47253ms step_avg:58.99ms
step:802/2330 train_time:47314ms step_avg:59.00ms
step:803/2330 train_time:47372ms step_avg:58.99ms
step:804/2330 train_time:47434ms step_avg:59.00ms
step:805/2330 train_time:47493ms step_avg:59.00ms
step:806/2330 train_time:47555ms step_avg:59.00ms
step:807/2330 train_time:47613ms step_avg:59.00ms
step:808/2330 train_time:47676ms step_avg:59.00ms
step:809/2330 train_time:47733ms step_avg:59.00ms
step:810/2330 train_time:47796ms step_avg:59.01ms
step:811/2330 train_time:47853ms step_avg:59.01ms
step:812/2330 train_time:47915ms step_avg:59.01ms
step:813/2330 train_time:47972ms step_avg:59.01ms
step:814/2330 train_time:48034ms step_avg:59.01ms
step:815/2330 train_time:48091ms step_avg:59.01ms
step:816/2330 train_time:48152ms step_avg:59.01ms
step:817/2330 train_time:48210ms step_avg:59.01ms
step:818/2330 train_time:48271ms step_avg:59.01ms
step:819/2330 train_time:48329ms step_avg:59.01ms
step:820/2330 train_time:48392ms step_avg:59.01ms
step:821/2330 train_time:48450ms step_avg:59.01ms
step:822/2330 train_time:48512ms step_avg:59.02ms
step:823/2330 train_time:48569ms step_avg:59.01ms
step:824/2330 train_time:48632ms step_avg:59.02ms
step:825/2330 train_time:48689ms step_avg:59.02ms
step:826/2330 train_time:48752ms step_avg:59.02ms
step:827/2330 train_time:48809ms step_avg:59.02ms
step:828/2330 train_time:48871ms step_avg:59.02ms
step:829/2330 train_time:48929ms step_avg:59.02ms
step:830/2330 train_time:48990ms step_avg:59.02ms
step:831/2330 train_time:49047ms step_avg:59.02ms
step:832/2330 train_time:49108ms step_avg:59.02ms
step:833/2330 train_time:49165ms step_avg:59.02ms
step:834/2330 train_time:49227ms step_avg:59.03ms
step:835/2330 train_time:49284ms step_avg:59.02ms
step:836/2330 train_time:49346ms step_avg:59.03ms
step:837/2330 train_time:49403ms step_avg:59.02ms
step:838/2330 train_time:49464ms step_avg:59.03ms
step:839/2330 train_time:49522ms step_avg:59.02ms
step:840/2330 train_time:49584ms step_avg:59.03ms
step:841/2330 train_time:49641ms step_avg:59.03ms
step:842/2330 train_time:49703ms step_avg:59.03ms
step:843/2330 train_time:49760ms step_avg:59.03ms
step:844/2330 train_time:49823ms step_avg:59.03ms
step:845/2330 train_time:49879ms step_avg:59.03ms
step:846/2330 train_time:49942ms step_avg:59.03ms
step:847/2330 train_time:49999ms step_avg:59.03ms
step:848/2330 train_time:50061ms step_avg:59.03ms
step:849/2330 train_time:50118ms step_avg:59.03ms
step:850/2330 train_time:50181ms step_avg:59.04ms
step:851/2330 train_time:50238ms step_avg:59.03ms
step:852/2330 train_time:50300ms step_avg:59.04ms
step:853/2330 train_time:50358ms step_avg:59.04ms
step:854/2330 train_time:50420ms step_avg:59.04ms
step:855/2330 train_time:50477ms step_avg:59.04ms
step:856/2330 train_time:50539ms step_avg:59.04ms
step:857/2330 train_time:50597ms step_avg:59.04ms
step:858/2330 train_time:50659ms step_avg:59.04ms
step:859/2330 train_time:50717ms step_avg:59.04ms
step:860/2330 train_time:50778ms step_avg:59.04ms
step:861/2330 train_time:50836ms step_avg:59.04ms
step:862/2330 train_time:50898ms step_avg:59.05ms
step:863/2330 train_time:50955ms step_avg:59.04ms
step:864/2330 train_time:51018ms step_avg:59.05ms
step:865/2330 train_time:51075ms step_avg:59.05ms
step:866/2330 train_time:51139ms step_avg:59.05ms
step:867/2330 train_time:51196ms step_avg:59.05ms
step:868/2330 train_time:51258ms step_avg:59.05ms
step:869/2330 train_time:51316ms step_avg:59.05ms
step:870/2330 train_time:51378ms step_avg:59.05ms
step:871/2330 train_time:51435ms step_avg:59.05ms
step:872/2330 train_time:51498ms step_avg:59.06ms
step:873/2330 train_time:51555ms step_avg:59.05ms
step:874/2330 train_time:51618ms step_avg:59.06ms
step:875/2330 train_time:51676ms step_avg:59.06ms
step:876/2330 train_time:51738ms step_avg:59.06ms
step:877/2330 train_time:51795ms step_avg:59.06ms
step:878/2330 train_time:51857ms step_avg:59.06ms
step:879/2330 train_time:51915ms step_avg:59.06ms
step:880/2330 train_time:51977ms step_avg:59.06ms
step:881/2330 train_time:52034ms step_avg:59.06ms
step:882/2330 train_time:52095ms step_avg:59.07ms
step:883/2330 train_time:52152ms step_avg:59.06ms
step:884/2330 train_time:52215ms step_avg:59.07ms
step:885/2330 train_time:52272ms step_avg:59.06ms
step:886/2330 train_time:52335ms step_avg:59.07ms
step:887/2330 train_time:52393ms step_avg:59.07ms
step:888/2330 train_time:52455ms step_avg:59.07ms
step:889/2330 train_time:52513ms step_avg:59.07ms
step:890/2330 train_time:52575ms step_avg:59.07ms
step:891/2330 train_time:52633ms step_avg:59.07ms
step:892/2330 train_time:52694ms step_avg:59.07ms
step:893/2330 train_time:52752ms step_avg:59.07ms
step:894/2330 train_time:52814ms step_avg:59.08ms
step:895/2330 train_time:52872ms step_avg:59.08ms
step:896/2330 train_time:52933ms step_avg:59.08ms
step:897/2330 train_time:52991ms step_avg:59.08ms
step:898/2330 train_time:53053ms step_avg:59.08ms
step:899/2330 train_time:53111ms step_avg:59.08ms
step:900/2330 train_time:53173ms step_avg:59.08ms
step:901/2330 train_time:53230ms step_avg:59.08ms
step:902/2330 train_time:53292ms step_avg:59.08ms
step:903/2330 train_time:53348ms step_avg:59.08ms
step:904/2330 train_time:53410ms step_avg:59.08ms
step:905/2330 train_time:53468ms step_avg:59.08ms
step:906/2330 train_time:53531ms step_avg:59.09ms
step:907/2330 train_time:53588ms step_avg:59.08ms
step:908/2330 train_time:53650ms step_avg:59.09ms
step:909/2330 train_time:53707ms step_avg:59.08ms
step:910/2330 train_time:53768ms step_avg:59.09ms
step:911/2330 train_time:53826ms step_avg:59.08ms
step:912/2330 train_time:53887ms step_avg:59.09ms
step:913/2330 train_time:53943ms step_avg:59.08ms
step:914/2330 train_time:54005ms step_avg:59.09ms
step:915/2330 train_time:54062ms step_avg:59.08ms
step:916/2330 train_time:54124ms step_avg:59.09ms
step:917/2330 train_time:54180ms step_avg:59.08ms
step:918/2330 train_time:54243ms step_avg:59.09ms
step:919/2330 train_time:54299ms step_avg:59.09ms
step:920/2330 train_time:54363ms step_avg:59.09ms
step:921/2330 train_time:54420ms step_avg:59.09ms
step:922/2330 train_time:54483ms step_avg:59.09ms
step:923/2330 train_time:54540ms step_avg:59.09ms
step:924/2330 train_time:54602ms step_avg:59.09ms
step:925/2330 train_time:54659ms step_avg:59.09ms
step:926/2330 train_time:54721ms step_avg:59.09ms
step:927/2330 train_time:54779ms step_avg:59.09ms
step:928/2330 train_time:54841ms step_avg:59.10ms
step:929/2330 train_time:54899ms step_avg:59.09ms
step:930/2330 train_time:54960ms step_avg:59.10ms
step:931/2330 train_time:55018ms step_avg:59.10ms
step:932/2330 train_time:55079ms step_avg:59.10ms
step:933/2330 train_time:55137ms step_avg:59.10ms
step:934/2330 train_time:55198ms step_avg:59.10ms
step:935/2330 train_time:55255ms step_avg:59.10ms
step:936/2330 train_time:55318ms step_avg:59.10ms
step:937/2330 train_time:55376ms step_avg:59.10ms
step:938/2330 train_time:55438ms step_avg:59.10ms
step:939/2330 train_time:55496ms step_avg:59.10ms
step:940/2330 train_time:55558ms step_avg:59.10ms
step:941/2330 train_time:55615ms step_avg:59.10ms
step:942/2330 train_time:55678ms step_avg:59.11ms
step:943/2330 train_time:55735ms step_avg:59.10ms
step:944/2330 train_time:55798ms step_avg:59.11ms
step:945/2330 train_time:55855ms step_avg:59.11ms
step:946/2330 train_time:55918ms step_avg:59.11ms
step:947/2330 train_time:55976ms step_avg:59.11ms
step:948/2330 train_time:56037ms step_avg:59.11ms
step:949/2330 train_time:56095ms step_avg:59.11ms
step:950/2330 train_time:56156ms step_avg:59.11ms
step:951/2330 train_time:56213ms step_avg:59.11ms
step:952/2330 train_time:56276ms step_avg:59.11ms
step:953/2330 train_time:56334ms step_avg:59.11ms
step:954/2330 train_time:56396ms step_avg:59.12ms
step:955/2330 train_time:56453ms step_avg:59.11ms
step:956/2330 train_time:56515ms step_avg:59.12ms
step:957/2330 train_time:56573ms step_avg:59.12ms
step:958/2330 train_time:56636ms step_avg:59.12ms
step:959/2330 train_time:56694ms step_avg:59.12ms
step:960/2330 train_time:56756ms step_avg:59.12ms
step:961/2330 train_time:56814ms step_avg:59.12ms
step:962/2330 train_time:56875ms step_avg:59.12ms
step:963/2330 train_time:56932ms step_avg:59.12ms
step:964/2330 train_time:56995ms step_avg:59.12ms
step:965/2330 train_time:57052ms step_avg:59.12ms
step:966/2330 train_time:57114ms step_avg:59.12ms
step:967/2330 train_time:57172ms step_avg:59.12ms
step:968/2330 train_time:57234ms step_avg:59.13ms
step:969/2330 train_time:57291ms step_avg:59.12ms
step:970/2330 train_time:57353ms step_avg:59.13ms
step:971/2330 train_time:57410ms step_avg:59.12ms
step:972/2330 train_time:57472ms step_avg:59.13ms
step:973/2330 train_time:57529ms step_avg:59.13ms
step:974/2330 train_time:57591ms step_avg:59.13ms
step:975/2330 train_time:57648ms step_avg:59.13ms
step:976/2330 train_time:57710ms step_avg:59.13ms
step:977/2330 train_time:57767ms step_avg:59.13ms
step:978/2330 train_time:57829ms step_avg:59.13ms
step:979/2330 train_time:57885ms step_avg:59.13ms
step:980/2330 train_time:57947ms step_avg:59.13ms
step:981/2330 train_time:58004ms step_avg:59.13ms
step:982/2330 train_time:58066ms step_avg:59.13ms
step:983/2330 train_time:58122ms step_avg:59.13ms
step:984/2330 train_time:58185ms step_avg:59.13ms
step:985/2330 train_time:58242ms step_avg:59.13ms
step:986/2330 train_time:58305ms step_avg:59.13ms
step:987/2330 train_time:58362ms step_avg:59.13ms
step:988/2330 train_time:58424ms step_avg:59.13ms
step:989/2330 train_time:58481ms step_avg:59.13ms
step:990/2330 train_time:58544ms step_avg:59.14ms
step:991/2330 train_time:58600ms step_avg:59.13ms
step:992/2330 train_time:58663ms step_avg:59.14ms
step:993/2330 train_time:58720ms step_avg:59.13ms
step:994/2330 train_time:58783ms step_avg:59.14ms
step:995/2330 train_time:58840ms step_avg:59.14ms
step:996/2330 train_time:58902ms step_avg:59.14ms
step:997/2330 train_time:58959ms step_avg:59.14ms
step:998/2330 train_time:59022ms step_avg:59.14ms
step:999/2330 train_time:59079ms step_avg:59.14ms
step:1000/2330 train_time:59141ms step_avg:59.14ms
step:1000/2330 val_loss:4.0916 train_time:59220ms step_avg:59.22ms
step:1001/2330 train_time:59239ms step_avg:59.18ms
step:1002/2330 train_time:59262ms step_avg:59.14ms
step:1003/2330 train_time:59320ms step_avg:59.14ms
step:1004/2330 train_time:59386ms step_avg:59.15ms
step:1005/2330 train_time:59443ms step_avg:59.15ms
step:1006/2330 train_time:59506ms step_avg:59.15ms
step:1007/2330 train_time:59562ms step_avg:59.15ms
step:1008/2330 train_time:59625ms step_avg:59.15ms
step:1009/2330 train_time:59681ms step_avg:59.15ms
step:1010/2330 train_time:59742ms step_avg:59.15ms
step:1011/2330 train_time:59798ms step_avg:59.15ms
step:1012/2330 train_time:59860ms step_avg:59.15ms
step:1013/2330 train_time:59917ms step_avg:59.15ms
step:1014/2330 train_time:59978ms step_avg:59.15ms
step:1015/2330 train_time:60034ms step_avg:59.15ms
step:1016/2330 train_time:60097ms step_avg:59.15ms
step:1017/2330 train_time:60156ms step_avg:59.15ms
step:1018/2330 train_time:60220ms step_avg:59.15ms
step:1019/2330 train_time:60279ms step_avg:59.15ms
step:1020/2330 train_time:60342ms step_avg:59.16ms
step:1021/2330 train_time:60401ms step_avg:59.16ms
step:1022/2330 train_time:60462ms step_avg:59.16ms
step:1023/2330 train_time:60519ms step_avg:59.16ms
step:1024/2330 train_time:60582ms step_avg:59.16ms
step:1025/2330 train_time:60638ms step_avg:59.16ms
step:1026/2330 train_time:60701ms step_avg:59.16ms
step:1027/2330 train_time:60757ms step_avg:59.16ms
step:1028/2330 train_time:60818ms step_avg:59.16ms
step:1029/2330 train_time:60875ms step_avg:59.16ms
step:1030/2330 train_time:60936ms step_avg:59.16ms
step:1031/2330 train_time:60993ms step_avg:59.16ms
step:1032/2330 train_time:61054ms step_avg:59.16ms
step:1033/2330 train_time:61111ms step_avg:59.16ms
step:1034/2330 train_time:61173ms step_avg:59.16ms
step:1035/2330 train_time:61230ms step_avg:59.16ms
step:1036/2330 train_time:61293ms step_avg:59.16ms
step:1037/2330 train_time:61351ms step_avg:59.16ms
step:1038/2330 train_time:61412ms step_avg:59.16ms
step:1039/2330 train_time:61469ms step_avg:59.16ms
step:1040/2330 train_time:61531ms step_avg:59.16ms
step:1041/2330 train_time:61587ms step_avg:59.16ms
step:1042/2330 train_time:61651ms step_avg:59.17ms
step:1043/2330 train_time:61708ms step_avg:59.16ms
step:1044/2330 train_time:61770ms step_avg:59.17ms
step:1045/2330 train_time:61826ms step_avg:59.16ms
step:1046/2330 train_time:61888ms step_avg:59.17ms
step:1047/2330 train_time:61945ms step_avg:59.16ms
step:1048/2330 train_time:62006ms step_avg:59.17ms
step:1049/2330 train_time:62063ms step_avg:59.16ms
step:1050/2330 train_time:62126ms step_avg:59.17ms
step:1051/2330 train_time:62183ms step_avg:59.17ms
step:1052/2330 train_time:62246ms step_avg:59.17ms
step:1053/2330 train_time:62303ms step_avg:59.17ms
step:1054/2330 train_time:62366ms step_avg:59.17ms
step:1055/2330 train_time:62423ms step_avg:59.17ms
step:1056/2330 train_time:62488ms step_avg:59.17ms
step:1057/2330 train_time:62545ms step_avg:59.17ms
step:1058/2330 train_time:62607ms step_avg:59.18ms
step:1059/2330 train_time:62664ms step_avg:59.17ms
step:1060/2330 train_time:62726ms step_avg:59.18ms
step:1061/2330 train_time:62783ms step_avg:59.17ms
step:1062/2330 train_time:62845ms step_avg:59.18ms
step:1063/2330 train_time:62902ms step_avg:59.17ms
step:1064/2330 train_time:62964ms step_avg:59.18ms
step:1065/2330 train_time:63021ms step_avg:59.18ms
step:1066/2330 train_time:63084ms step_avg:59.18ms
step:1067/2330 train_time:63141ms step_avg:59.18ms
step:1068/2330 train_time:63203ms step_avg:59.18ms
step:1069/2330 train_time:63261ms step_avg:59.18ms
step:1070/2330 train_time:63324ms step_avg:59.18ms
step:1071/2330 train_time:63381ms step_avg:59.18ms
step:1072/2330 train_time:63444ms step_avg:59.18ms
step:1073/2330 train_time:63501ms step_avg:59.18ms
step:1074/2330 train_time:63565ms step_avg:59.19ms
step:1075/2330 train_time:63622ms step_avg:59.18ms
step:1076/2330 train_time:63684ms step_avg:59.19ms
step:1077/2330 train_time:63741ms step_avg:59.18ms
step:1078/2330 train_time:63804ms step_avg:59.19ms
step:1079/2330 train_time:63861ms step_avg:59.19ms
step:1080/2330 train_time:63922ms step_avg:59.19ms
step:1081/2330 train_time:63979ms step_avg:59.18ms
step:1082/2330 train_time:64041ms step_avg:59.19ms
step:1083/2330 train_time:64099ms step_avg:59.19ms
step:1084/2330 train_time:64161ms step_avg:59.19ms
step:1085/2330 train_time:64218ms step_avg:59.19ms
step:1086/2330 train_time:64281ms step_avg:59.19ms
step:1087/2330 train_time:64339ms step_avg:59.19ms
step:1088/2330 train_time:64401ms step_avg:59.19ms
step:1089/2330 train_time:64459ms step_avg:59.19ms
step:1090/2330 train_time:64523ms step_avg:59.20ms
step:1091/2330 train_time:64580ms step_avg:59.19ms
step:1092/2330 train_time:64643ms step_avg:59.20ms
step:1093/2330 train_time:64700ms step_avg:59.19ms
step:1094/2330 train_time:64762ms step_avg:59.20ms
step:1095/2330 train_time:64819ms step_avg:59.20ms
step:1096/2330 train_time:64882ms step_avg:59.20ms
step:1097/2330 train_time:64939ms step_avg:59.20ms
step:1098/2330 train_time:65001ms step_avg:59.20ms
step:1099/2330 train_time:65059ms step_avg:59.20ms
step:1100/2330 train_time:65121ms step_avg:59.20ms
step:1101/2330 train_time:65178ms step_avg:59.20ms
step:1102/2330 train_time:65241ms step_avg:59.20ms
step:1103/2330 train_time:65299ms step_avg:59.20ms
step:1104/2330 train_time:65361ms step_avg:59.20ms
step:1105/2330 train_time:65418ms step_avg:59.20ms
step:1106/2330 train_time:65482ms step_avg:59.21ms
step:1107/2330 train_time:65540ms step_avg:59.20ms
step:1108/2330 train_time:65601ms step_avg:59.21ms
step:1109/2330 train_time:65659ms step_avg:59.21ms
step:1110/2330 train_time:65722ms step_avg:59.21ms
step:1111/2330 train_time:65778ms step_avg:59.21ms
step:1112/2330 train_time:65841ms step_avg:59.21ms
step:1113/2330 train_time:65899ms step_avg:59.21ms
step:1114/2330 train_time:65960ms step_avg:59.21ms
step:1115/2330 train_time:66017ms step_avg:59.21ms
step:1116/2330 train_time:66079ms step_avg:59.21ms
step:1117/2330 train_time:66137ms step_avg:59.21ms
step:1118/2330 train_time:66199ms step_avg:59.21ms
step:1119/2330 train_time:66257ms step_avg:59.21ms
step:1120/2330 train_time:66319ms step_avg:59.21ms
step:1121/2330 train_time:66377ms step_avg:59.21ms
step:1122/2330 train_time:66440ms step_avg:59.22ms
step:1123/2330 train_time:66497ms step_avg:59.21ms
step:1124/2330 train_time:66560ms step_avg:59.22ms
step:1125/2330 train_time:66618ms step_avg:59.22ms
step:1126/2330 train_time:66680ms step_avg:59.22ms
step:1127/2330 train_time:66737ms step_avg:59.22ms
step:1128/2330 train_time:66800ms step_avg:59.22ms
step:1129/2330 train_time:66858ms step_avg:59.22ms
step:1130/2330 train_time:66919ms step_avg:59.22ms
step:1131/2330 train_time:66976ms step_avg:59.22ms
step:1132/2330 train_time:67038ms step_avg:59.22ms
step:1133/2330 train_time:67096ms step_avg:59.22ms
step:1134/2330 train_time:67157ms step_avg:59.22ms
step:1135/2330 train_time:67215ms step_avg:59.22ms
step:1136/2330 train_time:67276ms step_avg:59.22ms
step:1137/2330 train_time:67333ms step_avg:59.22ms
step:1138/2330 train_time:67394ms step_avg:59.22ms
step:1139/2330 train_time:67452ms step_avg:59.22ms
step:1140/2330 train_time:67513ms step_avg:59.22ms
step:1141/2330 train_time:67570ms step_avg:59.22ms
step:1142/2330 train_time:67632ms step_avg:59.22ms
step:1143/2330 train_time:67689ms step_avg:59.22ms
step:1144/2330 train_time:67751ms step_avg:59.22ms
step:1145/2330 train_time:67808ms step_avg:59.22ms
step:1146/2330 train_time:67870ms step_avg:59.22ms
step:1147/2330 train_time:67926ms step_avg:59.22ms
step:1148/2330 train_time:67989ms step_avg:59.22ms
step:1149/2330 train_time:68046ms step_avg:59.22ms
step:1150/2330 train_time:68107ms step_avg:59.22ms
step:1151/2330 train_time:68164ms step_avg:59.22ms
step:1152/2330 train_time:68227ms step_avg:59.22ms
step:1153/2330 train_time:68284ms step_avg:59.22ms
step:1154/2330 train_time:68347ms step_avg:59.23ms
step:1155/2330 train_time:68404ms step_avg:59.22ms
step:1156/2330 train_time:68467ms step_avg:59.23ms
step:1157/2330 train_time:68524ms step_avg:59.23ms
step:1158/2330 train_time:68586ms step_avg:59.23ms
step:1159/2330 train_time:68644ms step_avg:59.23ms
step:1160/2330 train_time:68706ms step_avg:59.23ms
step:1161/2330 train_time:68763ms step_avg:59.23ms
step:1162/2330 train_time:68825ms step_avg:59.23ms
step:1163/2330 train_time:68882ms step_avg:59.23ms
step:1164/2330 train_time:68944ms step_avg:59.23ms
step:1165/2330 train_time:69002ms step_avg:59.23ms
step:1166/2330 train_time:69063ms step_avg:59.23ms
step:1167/2330 train_time:69121ms step_avg:59.23ms
step:1168/2330 train_time:69182ms step_avg:59.23ms
step:1169/2330 train_time:69240ms step_avg:59.23ms
step:1170/2330 train_time:69302ms step_avg:59.23ms
step:1171/2330 train_time:69360ms step_avg:59.23ms
step:1172/2330 train_time:69422ms step_avg:59.23ms
step:1173/2330 train_time:69479ms step_avg:59.23ms
step:1174/2330 train_time:69542ms step_avg:59.24ms
step:1175/2330 train_time:69600ms step_avg:59.23ms
step:1176/2330 train_time:69662ms step_avg:59.24ms
step:1177/2330 train_time:69720ms step_avg:59.24ms
step:1178/2330 train_time:69781ms step_avg:59.24ms
step:1179/2330 train_time:69839ms step_avg:59.24ms
step:1180/2330 train_time:69900ms step_avg:59.24ms
step:1181/2330 train_time:69958ms step_avg:59.24ms
step:1182/2330 train_time:70020ms step_avg:59.24ms
step:1183/2330 train_time:70077ms step_avg:59.24ms
step:1184/2330 train_time:70140ms step_avg:59.24ms
step:1185/2330 train_time:70198ms step_avg:59.24ms
step:1186/2330 train_time:70260ms step_avg:59.24ms
step:1187/2330 train_time:70318ms step_avg:59.24ms
step:1188/2330 train_time:70380ms step_avg:59.24ms
step:1189/2330 train_time:70438ms step_avg:59.24ms
step:1190/2330 train_time:70500ms step_avg:59.24ms
step:1191/2330 train_time:70557ms step_avg:59.24ms
step:1192/2330 train_time:70620ms step_avg:59.25ms
step:1193/2330 train_time:70678ms step_avg:59.24ms
step:1194/2330 train_time:70741ms step_avg:59.25ms
step:1195/2330 train_time:70798ms step_avg:59.25ms
step:1196/2330 train_time:70860ms step_avg:59.25ms
step:1197/2330 train_time:70918ms step_avg:59.25ms
step:1198/2330 train_time:70979ms step_avg:59.25ms
step:1199/2330 train_time:71037ms step_avg:59.25ms
step:1200/2330 train_time:71098ms step_avg:59.25ms
step:1201/2330 train_time:71156ms step_avg:59.25ms
step:1202/2330 train_time:71218ms step_avg:59.25ms
step:1203/2330 train_time:71276ms step_avg:59.25ms
step:1204/2330 train_time:71338ms step_avg:59.25ms
step:1205/2330 train_time:71396ms step_avg:59.25ms
step:1206/2330 train_time:71459ms step_avg:59.25ms
step:1207/2330 train_time:71516ms step_avg:59.25ms
step:1208/2330 train_time:71578ms step_avg:59.25ms
step:1209/2330 train_time:71636ms step_avg:59.25ms
step:1210/2330 train_time:71698ms step_avg:59.25ms
step:1211/2330 train_time:71755ms step_avg:59.25ms
step:1212/2330 train_time:71818ms step_avg:59.26ms
step:1213/2330 train_time:71875ms step_avg:59.25ms
step:1214/2330 train_time:71938ms step_avg:59.26ms
step:1215/2330 train_time:71996ms step_avg:59.26ms
step:1216/2330 train_time:72056ms step_avg:59.26ms
step:1217/2330 train_time:72113ms step_avg:59.25ms
step:1218/2330 train_time:72175ms step_avg:59.26ms
step:1219/2330 train_time:72233ms step_avg:59.26ms
step:1220/2330 train_time:72294ms step_avg:59.26ms
step:1221/2330 train_time:72351ms step_avg:59.26ms
step:1222/2330 train_time:72413ms step_avg:59.26ms
step:1223/2330 train_time:72470ms step_avg:59.26ms
step:1224/2330 train_time:72532ms step_avg:59.26ms
step:1225/2330 train_time:72589ms step_avg:59.26ms
step:1226/2330 train_time:72651ms step_avg:59.26ms
step:1227/2330 train_time:72708ms step_avg:59.26ms
step:1228/2330 train_time:72771ms step_avg:59.26ms
step:1229/2330 train_time:72827ms step_avg:59.26ms
step:1230/2330 train_time:72890ms step_avg:59.26ms
step:1231/2330 train_time:72947ms step_avg:59.26ms
step:1232/2330 train_time:73009ms step_avg:59.26ms
step:1233/2330 train_time:73066ms step_avg:59.26ms
step:1234/2330 train_time:73128ms step_avg:59.26ms
step:1235/2330 train_time:73185ms step_avg:59.26ms
step:1236/2330 train_time:73248ms step_avg:59.26ms
step:1237/2330 train_time:73305ms step_avg:59.26ms
step:1238/2330 train_time:73367ms step_avg:59.26ms
step:1239/2330 train_time:73424ms step_avg:59.26ms
step:1240/2330 train_time:73487ms step_avg:59.26ms
step:1241/2330 train_time:73544ms step_avg:59.26ms
step:1242/2330 train_time:73606ms step_avg:59.26ms
step:1243/2330 train_time:73663ms step_avg:59.26ms
step:1244/2330 train_time:73726ms step_avg:59.27ms
step:1245/2330 train_time:73783ms step_avg:59.26ms
step:1246/2330 train_time:73846ms step_avg:59.27ms
step:1247/2330 train_time:73904ms step_avg:59.27ms
step:1248/2330 train_time:73965ms step_avg:59.27ms
step:1249/2330 train_time:74023ms step_avg:59.27ms
step:1250/2330 train_time:74085ms step_avg:59.27ms
step:1250/2330 val_loss:4.0102 train_time:74164ms step_avg:59.33ms
step:1251/2330 train_time:74183ms step_avg:59.30ms
step:1252/2330 train_time:74207ms step_avg:59.27ms
step:1253/2330 train_time:74265ms step_avg:59.27ms
step:1254/2330 train_time:74331ms step_avg:59.28ms
step:1255/2330 train_time:74388ms step_avg:59.27ms
step:1256/2330 train_time:74453ms step_avg:59.28ms
step:1257/2330 train_time:74509ms step_avg:59.28ms
step:1258/2330 train_time:74572ms step_avg:59.28ms
step:1259/2330 train_time:74629ms step_avg:59.28ms
step:1260/2330 train_time:74690ms step_avg:59.28ms
step:1261/2330 train_time:74747ms step_avg:59.28ms
step:1262/2330 train_time:74808ms step_avg:59.28ms
step:1263/2330 train_time:74865ms step_avg:59.28ms
step:1264/2330 train_time:74926ms step_avg:59.28ms
step:1265/2330 train_time:74982ms step_avg:59.27ms
step:1266/2330 train_time:75043ms step_avg:59.28ms
step:1267/2330 train_time:75100ms step_avg:59.27ms
step:1268/2330 train_time:75162ms step_avg:59.28ms
step:1269/2330 train_time:75220ms step_avg:59.27ms
step:1270/2330 train_time:75285ms step_avg:59.28ms
step:1271/2330 train_time:75342ms step_avg:59.28ms
step:1272/2330 train_time:75404ms step_avg:59.28ms
step:1273/2330 train_time:75462ms step_avg:59.28ms
step:1274/2330 train_time:75525ms step_avg:59.28ms
step:1275/2330 train_time:75582ms step_avg:59.28ms
step:1276/2330 train_time:75644ms step_avg:59.28ms
step:1277/2330 train_time:75700ms step_avg:59.28ms
step:1278/2330 train_time:75762ms step_avg:59.28ms
step:1279/2330 train_time:75819ms step_avg:59.28ms
step:1280/2330 train_time:75881ms step_avg:59.28ms
step:1281/2330 train_time:75938ms step_avg:59.28ms
step:1282/2330 train_time:76000ms step_avg:59.28ms
step:1283/2330 train_time:76057ms step_avg:59.28ms
step:1284/2330 train_time:76120ms step_avg:59.28ms
step:1285/2330 train_time:76178ms step_avg:59.28ms
step:1286/2330 train_time:76240ms step_avg:59.28ms
step:1287/2330 train_time:76298ms step_avg:59.28ms
step:1288/2330 train_time:76361ms step_avg:59.29ms
step:1289/2330 train_time:76419ms step_avg:59.29ms
step:1290/2330 train_time:76482ms step_avg:59.29ms
step:1291/2330 train_time:76539ms step_avg:59.29ms
step:1292/2330 train_time:76601ms step_avg:59.29ms
step:1293/2330 train_time:76658ms step_avg:59.29ms
step:1294/2330 train_time:76720ms step_avg:59.29ms
step:1295/2330 train_time:76778ms step_avg:59.29ms
step:1296/2330 train_time:76840ms step_avg:59.29ms
step:1297/2330 train_time:76897ms step_avg:59.29ms
step:1298/2330 train_time:76958ms step_avg:59.29ms
step:1299/2330 train_time:77015ms step_avg:59.29ms
step:1300/2330 train_time:77078ms step_avg:59.29ms
step:1301/2330 train_time:77136ms step_avg:59.29ms
step:1302/2330 train_time:77197ms step_avg:59.29ms
step:1303/2330 train_time:77255ms step_avg:59.29ms
step:1304/2330 train_time:77318ms step_avg:59.29ms
step:1305/2330 train_time:77376ms step_avg:59.29ms
step:1306/2330 train_time:77438ms step_avg:59.29ms
step:1307/2330 train_time:77496ms step_avg:59.29ms
step:1308/2330 train_time:77559ms step_avg:59.30ms
step:1309/2330 train_time:77616ms step_avg:59.29ms
step:1310/2330 train_time:77678ms step_avg:59.30ms
step:1311/2330 train_time:77736ms step_avg:59.30ms
step:1312/2330 train_time:77798ms step_avg:59.30ms
step:1313/2330 train_time:77854ms step_avg:59.30ms
step:1314/2330 train_time:77917ms step_avg:59.30ms
step:1315/2330 train_time:77974ms step_avg:59.30ms
step:1316/2330 train_time:78035ms step_avg:59.30ms
step:1317/2330 train_time:78093ms step_avg:59.30ms
step:1318/2330 train_time:78155ms step_avg:59.30ms
step:1319/2330 train_time:78213ms step_avg:59.30ms
step:1320/2330 train_time:78274ms step_avg:59.30ms
step:1321/2330 train_time:78333ms step_avg:59.30ms
step:1322/2330 train_time:78394ms step_avg:59.30ms
step:1323/2330 train_time:78452ms step_avg:59.30ms
step:1324/2330 train_time:78514ms step_avg:59.30ms
step:1325/2330 train_time:78572ms step_avg:59.30ms
step:1326/2330 train_time:78635ms step_avg:59.30ms
step:1327/2330 train_time:78693ms step_avg:59.30ms
step:1328/2330 train_time:78754ms step_avg:59.30ms
step:1329/2330 train_time:78812ms step_avg:59.30ms
step:1330/2330 train_time:78873ms step_avg:59.30ms
step:1331/2330 train_time:78929ms step_avg:59.30ms
step:1332/2330 train_time:78992ms step_avg:59.30ms
step:1333/2330 train_time:79049ms step_avg:59.30ms
step:1334/2330 train_time:79111ms step_avg:59.30ms
step:1335/2330 train_time:79167ms step_avg:59.30ms
step:1336/2330 train_time:79231ms step_avg:59.30ms
step:1337/2330 train_time:79288ms step_avg:59.30ms
step:1338/2330 train_time:79350ms step_avg:59.30ms
step:1339/2330 train_time:79406ms step_avg:59.30ms
step:1340/2330 train_time:79468ms step_avg:59.30ms
step:1341/2330 train_time:79525ms step_avg:59.30ms
step:1342/2330 train_time:79588ms step_avg:59.31ms
step:1343/2330 train_time:79645ms step_avg:59.30ms
step:1344/2330 train_time:79707ms step_avg:59.31ms
step:1345/2330 train_time:79764ms step_avg:59.30ms
step:1346/2330 train_time:79826ms step_avg:59.31ms
step:1347/2330 train_time:79883ms step_avg:59.30ms
step:1348/2330 train_time:79945ms step_avg:59.31ms
step:1349/2330 train_time:80001ms step_avg:59.30ms
step:1350/2330 train_time:80063ms step_avg:59.31ms
step:1351/2330 train_time:80120ms step_avg:59.30ms
step:1352/2330 train_time:80183ms step_avg:59.31ms
step:1353/2330 train_time:80240ms step_avg:59.31ms
step:1354/2330 train_time:80302ms step_avg:59.31ms
step:1355/2330 train_time:80360ms step_avg:59.31ms
step:1356/2330 train_time:80422ms step_avg:59.31ms
step:1357/2330 train_time:80480ms step_avg:59.31ms
step:1358/2330 train_time:80541ms step_avg:59.31ms
step:1359/2330 train_time:80599ms step_avg:59.31ms
step:1360/2330 train_time:80661ms step_avg:59.31ms
step:1361/2330 train_time:80718ms step_avg:59.31ms
step:1362/2330 train_time:80781ms step_avg:59.31ms
step:1363/2330 train_time:80838ms step_avg:59.31ms
step:1364/2330 train_time:80901ms step_avg:59.31ms
step:1365/2330 train_time:80958ms step_avg:59.31ms
step:1366/2330 train_time:81020ms step_avg:59.31ms
step:1367/2330 train_time:81077ms step_avg:59.31ms
step:1368/2330 train_time:81140ms step_avg:59.31ms
step:1369/2330 train_time:81197ms step_avg:59.31ms
step:1370/2330 train_time:81259ms step_avg:59.31ms
step:1371/2330 train_time:81316ms step_avg:59.31ms
step:1372/2330 train_time:81379ms step_avg:59.31ms
step:1373/2330 train_time:81437ms step_avg:59.31ms
step:1374/2330 train_time:81499ms step_avg:59.32ms
step:1375/2330 train_time:81557ms step_avg:59.31ms
step:1376/2330 train_time:81619ms step_avg:59.32ms
step:1377/2330 train_time:81677ms step_avg:59.32ms
step:1378/2330 train_time:81739ms step_avg:59.32ms
step:1379/2330 train_time:81797ms step_avg:59.32ms
step:1380/2330 train_time:81859ms step_avg:59.32ms
step:1381/2330 train_time:81916ms step_avg:59.32ms
step:1382/2330 train_time:81979ms step_avg:59.32ms
step:1383/2330 train_time:82037ms step_avg:59.32ms
step:1384/2330 train_time:82098ms step_avg:59.32ms
step:1385/2330 train_time:82156ms step_avg:59.32ms
step:1386/2330 train_time:82218ms step_avg:59.32ms
step:1387/2330 train_time:82275ms step_avg:59.32ms
step:1388/2330 train_time:82338ms step_avg:59.32ms
step:1389/2330 train_time:82396ms step_avg:59.32ms
step:1390/2330 train_time:82458ms step_avg:59.32ms
step:1391/2330 train_time:82516ms step_avg:59.32ms
step:1392/2330 train_time:82578ms step_avg:59.32ms
step:1393/2330 train_time:82636ms step_avg:59.32ms
step:1394/2330 train_time:82699ms step_avg:59.32ms
step:1395/2330 train_time:82756ms step_avg:59.32ms
step:1396/2330 train_time:82819ms step_avg:59.33ms
step:1397/2330 train_time:82877ms step_avg:59.32ms
step:1398/2330 train_time:82938ms step_avg:59.33ms
step:1399/2330 train_time:82996ms step_avg:59.33ms
step:1400/2330 train_time:83058ms step_avg:59.33ms
step:1401/2330 train_time:83115ms step_avg:59.33ms
step:1402/2330 train_time:83177ms step_avg:59.33ms
step:1403/2330 train_time:83235ms step_avg:59.33ms
step:1404/2330 train_time:83298ms step_avg:59.33ms
step:1405/2330 train_time:83355ms step_avg:59.33ms
step:1406/2330 train_time:83417ms step_avg:59.33ms
step:1407/2330 train_time:83474ms step_avg:59.33ms
step:1408/2330 train_time:83536ms step_avg:59.33ms
step:1409/2330 train_time:83594ms step_avg:59.33ms
step:1410/2330 train_time:83656ms step_avg:59.33ms
step:1411/2330 train_time:83714ms step_avg:59.33ms
step:1412/2330 train_time:83777ms step_avg:59.33ms
step:1413/2330 train_time:83835ms step_avg:59.33ms
step:1414/2330 train_time:83897ms step_avg:59.33ms
step:1415/2330 train_time:83954ms step_avg:59.33ms
step:1416/2330 train_time:84016ms step_avg:59.33ms
step:1417/2330 train_time:84073ms step_avg:59.33ms
step:1418/2330 train_time:84136ms step_avg:59.33ms
step:1419/2330 train_time:84194ms step_avg:59.33ms
step:1420/2330 train_time:84257ms step_avg:59.34ms
step:1421/2330 train_time:84314ms step_avg:59.33ms
step:1422/2330 train_time:84376ms step_avg:59.34ms
step:1423/2330 train_time:84434ms step_avg:59.34ms
step:1424/2330 train_time:84495ms step_avg:59.34ms
step:1425/2330 train_time:84553ms step_avg:59.34ms
step:1426/2330 train_time:84616ms step_avg:59.34ms
step:1427/2330 train_time:84674ms step_avg:59.34ms
step:1428/2330 train_time:84736ms step_avg:59.34ms
step:1429/2330 train_time:84794ms step_avg:59.34ms
step:1430/2330 train_time:84857ms step_avg:59.34ms
step:1431/2330 train_time:84915ms step_avg:59.34ms
step:1432/2330 train_time:84977ms step_avg:59.34ms
step:1433/2330 train_time:85034ms step_avg:59.34ms
step:1434/2330 train_time:85096ms step_avg:59.34ms
step:1435/2330 train_time:85153ms step_avg:59.34ms
step:1436/2330 train_time:85217ms step_avg:59.34ms
step:1437/2330 train_time:85275ms step_avg:59.34ms
step:1438/2330 train_time:85337ms step_avg:59.34ms
step:1439/2330 train_time:85394ms step_avg:59.34ms
step:1440/2330 train_time:85456ms step_avg:59.34ms
step:1441/2330 train_time:85513ms step_avg:59.34ms
step:1442/2330 train_time:85576ms step_avg:59.35ms
step:1443/2330 train_time:85634ms step_avg:59.34ms
step:1444/2330 train_time:85694ms step_avg:59.35ms
step:1445/2330 train_time:85752ms step_avg:59.34ms
step:1446/2330 train_time:85815ms step_avg:59.35ms
step:1447/2330 train_time:85872ms step_avg:59.34ms
step:1448/2330 train_time:85934ms step_avg:59.35ms
step:1449/2330 train_time:85992ms step_avg:59.35ms
step:1450/2330 train_time:86053ms step_avg:59.35ms
step:1451/2330 train_time:86111ms step_avg:59.35ms
step:1452/2330 train_time:86172ms step_avg:59.35ms
step:1453/2330 train_time:86230ms step_avg:59.35ms
step:1454/2330 train_time:86292ms step_avg:59.35ms
step:1455/2330 train_time:86350ms step_avg:59.35ms
step:1456/2330 train_time:86411ms step_avg:59.35ms
step:1457/2330 train_time:86468ms step_avg:59.35ms
step:1458/2330 train_time:86529ms step_avg:59.35ms
step:1459/2330 train_time:86586ms step_avg:59.35ms
step:1460/2330 train_time:86649ms step_avg:59.35ms
step:1461/2330 train_time:86705ms step_avg:59.35ms
step:1462/2330 train_time:86767ms step_avg:59.35ms
step:1463/2330 train_time:86824ms step_avg:59.35ms
step:1464/2330 train_time:86887ms step_avg:59.35ms
step:1465/2330 train_time:86944ms step_avg:59.35ms
step:1466/2330 train_time:87006ms step_avg:59.35ms
step:1467/2330 train_time:87063ms step_avg:59.35ms
step:1468/2330 train_time:87125ms step_avg:59.35ms
step:1469/2330 train_time:87182ms step_avg:59.35ms
step:1470/2330 train_time:87244ms step_avg:59.35ms
step:1471/2330 train_time:87301ms step_avg:59.35ms
step:1472/2330 train_time:87363ms step_avg:59.35ms
step:1473/2330 train_time:87420ms step_avg:59.35ms
step:1474/2330 train_time:87482ms step_avg:59.35ms
step:1475/2330 train_time:87539ms step_avg:59.35ms
step:1476/2330 train_time:87601ms step_avg:59.35ms
step:1477/2330 train_time:87659ms step_avg:59.35ms
step:1478/2330 train_time:87721ms step_avg:59.35ms
step:1479/2330 train_time:87778ms step_avg:59.35ms
step:1480/2330 train_time:87841ms step_avg:59.35ms
step:1481/2330 train_time:87898ms step_avg:59.35ms
step:1482/2330 train_time:87960ms step_avg:59.35ms
step:1483/2330 train_time:88017ms step_avg:59.35ms
step:1484/2330 train_time:88081ms step_avg:59.35ms
step:1485/2330 train_time:88138ms step_avg:59.35ms
step:1486/2330 train_time:88200ms step_avg:59.35ms
step:1487/2330 train_time:88258ms step_avg:59.35ms
step:1488/2330 train_time:88320ms step_avg:59.35ms
step:1489/2330 train_time:88377ms step_avg:59.35ms
step:1490/2330 train_time:88439ms step_avg:59.36ms
step:1491/2330 train_time:88496ms step_avg:59.35ms
step:1492/2330 train_time:88557ms step_avg:59.35ms
step:1493/2330 train_time:88615ms step_avg:59.35ms
step:1494/2330 train_time:88677ms step_avg:59.36ms
step:1495/2330 train_time:88735ms step_avg:59.35ms
step:1496/2330 train_time:88798ms step_avg:59.36ms
step:1497/2330 train_time:88856ms step_avg:59.36ms
step:1498/2330 train_time:88917ms step_avg:59.36ms
step:1499/2330 train_time:88974ms step_avg:59.36ms
step:1500/2330 train_time:89038ms step_avg:59.36ms
step:1500/2330 val_loss:3.9249 train_time:89117ms step_avg:59.41ms
step:1501/2330 train_time:89135ms step_avg:59.38ms
step:1502/2330 train_time:89159ms step_avg:59.36ms
step:1503/2330 train_time:89217ms step_avg:59.36ms
step:1504/2330 train_time:89286ms step_avg:59.37ms
step:1505/2330 train_time:89344ms step_avg:59.36ms
step:1506/2330 train_time:89405ms step_avg:59.37ms
step:1507/2330 train_time:89463ms step_avg:59.36ms
step:1508/2330 train_time:89523ms step_avg:59.37ms
step:1509/2330 train_time:89580ms step_avg:59.36ms
step:1510/2330 train_time:89641ms step_avg:59.37ms
step:1511/2330 train_time:89698ms step_avg:59.36ms
step:1512/2330 train_time:89759ms step_avg:59.36ms
step:1513/2330 train_time:89816ms step_avg:59.36ms
step:1514/2330 train_time:89877ms step_avg:59.36ms
step:1515/2330 train_time:89933ms step_avg:59.36ms
step:1516/2330 train_time:89995ms step_avg:59.36ms
step:1517/2330 train_time:90053ms step_avg:59.36ms
step:1518/2330 train_time:90117ms step_avg:59.37ms
step:1519/2330 train_time:90177ms step_avg:59.37ms
step:1520/2330 train_time:90241ms step_avg:59.37ms
step:1521/2330 train_time:90299ms step_avg:59.37ms
step:1522/2330 train_time:90363ms step_avg:59.37ms
step:1523/2330 train_time:90420ms step_avg:59.37ms
step:1524/2330 train_time:90482ms step_avg:59.37ms
step:1525/2330 train_time:90539ms step_avg:59.37ms
step:1526/2330 train_time:90600ms step_avg:59.37ms
step:1527/2330 train_time:90657ms step_avg:59.37ms
step:1528/2330 train_time:90718ms step_avg:59.37ms
step:1529/2330 train_time:90776ms step_avg:59.37ms
step:1530/2330 train_time:90837ms step_avg:59.37ms
step:1531/2330 train_time:90894ms step_avg:59.37ms
step:1532/2330 train_time:90956ms step_avg:59.37ms
step:1533/2330 train_time:91014ms step_avg:59.37ms
step:1534/2330 train_time:91077ms step_avg:59.37ms
step:1535/2330 train_time:91135ms step_avg:59.37ms
step:1536/2330 train_time:91199ms step_avg:59.37ms
step:1537/2330 train_time:91257ms step_avg:59.37ms
step:1538/2330 train_time:91324ms step_avg:59.38ms
step:1539/2330 train_time:91381ms step_avg:59.38ms
step:1540/2330 train_time:91443ms step_avg:59.38ms
step:1541/2330 train_time:91501ms step_avg:59.38ms
step:1542/2330 train_time:91563ms step_avg:59.38ms
step:1543/2330 train_time:91620ms step_avg:59.38ms
step:1544/2330 train_time:91683ms step_avg:59.38ms
step:1545/2330 train_time:91739ms step_avg:59.38ms
step:1546/2330 train_time:91802ms step_avg:59.38ms
step:1547/2330 train_time:91859ms step_avg:59.38ms
step:1548/2330 train_time:91921ms step_avg:59.38ms
step:1549/2330 train_time:91978ms step_avg:59.38ms
step:1550/2330 train_time:92041ms step_avg:59.38ms
step:1551/2330 train_time:92098ms step_avg:59.38ms
step:1552/2330 train_time:92162ms step_avg:59.38ms
step:1553/2330 train_time:92220ms step_avg:59.38ms
step:1554/2330 train_time:92284ms step_avg:59.38ms
step:1555/2330 train_time:92341ms step_avg:59.38ms
step:1556/2330 train_time:92405ms step_avg:59.39ms
step:1557/2330 train_time:92462ms step_avg:59.38ms
step:1558/2330 train_time:92525ms step_avg:59.39ms
step:1559/2330 train_time:92582ms step_avg:59.39ms
step:1560/2330 train_time:92644ms step_avg:59.39ms
step:1561/2330 train_time:92701ms step_avg:59.39ms
step:1562/2330 train_time:92763ms step_avg:59.39ms
step:1563/2330 train_time:92821ms step_avg:59.39ms
step:1564/2330 train_time:92882ms step_avg:59.39ms
step:1565/2330 train_time:92939ms step_avg:59.39ms
step:1566/2330 train_time:93002ms step_avg:59.39ms
step:1567/2330 train_time:93059ms step_avg:59.39ms
step:1568/2330 train_time:93122ms step_avg:59.39ms
step:1569/2330 train_time:93179ms step_avg:59.39ms
step:1570/2330 train_time:93243ms step_avg:59.39ms
step:1571/2330 train_time:93301ms step_avg:59.39ms
step:1572/2330 train_time:93364ms step_avg:59.39ms
step:1573/2330 train_time:93422ms step_avg:59.39ms
step:1574/2330 train_time:93484ms step_avg:59.39ms
step:1575/2330 train_time:93541ms step_avg:59.39ms
step:1576/2330 train_time:93603ms step_avg:59.39ms
step:1577/2330 train_time:93661ms step_avg:59.39ms
step:1578/2330 train_time:93723ms step_avg:59.39ms
step:1579/2330 train_time:93780ms step_avg:59.39ms
step:1580/2330 train_time:93842ms step_avg:59.39ms
step:1581/2330 train_time:93899ms step_avg:59.39ms
step:1582/2330 train_time:93961ms step_avg:59.39ms
step:1583/2330 train_time:94019ms step_avg:59.39ms
step:1584/2330 train_time:94089ms step_avg:59.40ms
step:1585/2330 train_time:94139ms step_avg:59.39ms
step:1586/2330 train_time:94202ms step_avg:59.40ms
step:1587/2330 train_time:94260ms step_avg:59.39ms
step:1588/2330 train_time:94323ms step_avg:59.40ms
step:1589/2330 train_time:94381ms step_avg:59.40ms
step:1590/2330 train_time:94444ms step_avg:59.40ms
step:1591/2330 train_time:94502ms step_avg:59.40ms
step:1592/2330 train_time:94564ms step_avg:59.40ms
step:1593/2330 train_time:94621ms step_avg:59.40ms
step:1594/2330 train_time:94684ms step_avg:59.40ms
step:1595/2330 train_time:94742ms step_avg:59.40ms
step:1596/2330 train_time:94804ms step_avg:59.40ms
step:1597/2330 train_time:94861ms step_avg:59.40ms
step:1598/2330 train_time:94922ms step_avg:59.40ms
step:1599/2330 train_time:94979ms step_avg:59.40ms
step:1600/2330 train_time:95042ms step_avg:59.40ms
step:1601/2330 train_time:95100ms step_avg:59.40ms
step:1602/2330 train_time:95162ms step_avg:59.40ms
step:1603/2330 train_time:95220ms step_avg:59.40ms
step:1604/2330 train_time:95282ms step_avg:59.40ms
step:1605/2330 train_time:95340ms step_avg:59.40ms
step:1606/2330 train_time:95402ms step_avg:59.40ms
step:1607/2330 train_time:95460ms step_avg:59.40ms
step:1608/2330 train_time:95523ms step_avg:59.40ms
step:1609/2330 train_time:95581ms step_avg:59.40ms
step:1610/2330 train_time:95643ms step_avg:59.41ms
step:1611/2330 train_time:95700ms step_avg:59.40ms
step:1612/2330 train_time:95762ms step_avg:59.41ms
step:1613/2330 train_time:95820ms step_avg:59.40ms
step:1614/2330 train_time:95883ms step_avg:59.41ms
step:1615/2330 train_time:95940ms step_avg:59.41ms
step:1616/2330 train_time:96002ms step_avg:59.41ms
step:1617/2330 train_time:96060ms step_avg:59.41ms
step:1618/2330 train_time:96123ms step_avg:59.41ms
step:1619/2330 train_time:96180ms step_avg:59.41ms
step:1620/2330 train_time:96242ms step_avg:59.41ms
step:1621/2330 train_time:96299ms step_avg:59.41ms
step:1622/2330 train_time:96363ms step_avg:59.41ms
step:1623/2330 train_time:96420ms step_avg:59.41ms
step:1624/2330 train_time:96483ms step_avg:59.41ms
step:1625/2330 train_time:96541ms step_avg:59.41ms
step:1626/2330 train_time:96604ms step_avg:59.41ms
step:1627/2330 train_time:96661ms step_avg:59.41ms
step:1628/2330 train_time:96723ms step_avg:59.41ms
step:1629/2330 train_time:96781ms step_avg:59.41ms
step:1630/2330 train_time:96844ms step_avg:59.41ms
step:1631/2330 train_time:96901ms step_avg:59.41ms
step:1632/2330 train_time:96962ms step_avg:59.41ms
step:1633/2330 train_time:97020ms step_avg:59.41ms
step:1634/2330 train_time:97083ms step_avg:59.41ms
step:1635/2330 train_time:97140ms step_avg:59.41ms
step:1636/2330 train_time:97203ms step_avg:59.42ms
step:1637/2330 train_time:97260ms step_avg:59.41ms
step:1638/2330 train_time:97323ms step_avg:59.42ms
step:1639/2330 train_time:97380ms step_avg:59.41ms
step:1640/2330 train_time:97443ms step_avg:59.42ms
step:1641/2330 train_time:97500ms step_avg:59.41ms
step:1642/2330 train_time:97564ms step_avg:59.42ms
step:1643/2330 train_time:97621ms step_avg:59.42ms
step:1644/2330 train_time:97684ms step_avg:59.42ms
step:1645/2330 train_time:97742ms step_avg:59.42ms
step:1646/2330 train_time:97804ms step_avg:59.42ms
step:1647/2330 train_time:97862ms step_avg:59.42ms
step:1648/2330 train_time:97924ms step_avg:59.42ms
step:1649/2330 train_time:97982ms step_avg:59.42ms
step:1650/2330 train_time:98044ms step_avg:59.42ms
step:1651/2330 train_time:98102ms step_avg:59.42ms
step:1652/2330 train_time:98164ms step_avg:59.42ms
step:1653/2330 train_time:98222ms step_avg:59.42ms
step:1654/2330 train_time:98284ms step_avg:59.42ms
step:1655/2330 train_time:98341ms step_avg:59.42ms
step:1656/2330 train_time:98404ms step_avg:59.42ms
step:1657/2330 train_time:98461ms step_avg:59.42ms
step:1658/2330 train_time:98525ms step_avg:59.42ms
step:1659/2330 train_time:98582ms step_avg:59.42ms
step:1660/2330 train_time:98645ms step_avg:59.42ms
step:1661/2330 train_time:98702ms step_avg:59.42ms
step:1662/2330 train_time:98764ms step_avg:59.42ms
step:1663/2330 train_time:98822ms step_avg:59.42ms
step:1664/2330 train_time:98884ms step_avg:59.43ms
step:1665/2330 train_time:98941ms step_avg:59.42ms
step:1666/2330 train_time:99004ms step_avg:59.43ms
step:1667/2330 train_time:99061ms step_avg:59.42ms
step:1668/2330 train_time:99124ms step_avg:59.43ms
step:1669/2330 train_time:99182ms step_avg:59.43ms
step:1670/2330 train_time:99245ms step_avg:59.43ms
step:1671/2330 train_time:99302ms step_avg:59.43ms
step:1672/2330 train_time:99365ms step_avg:59.43ms
step:1673/2330 train_time:99423ms step_avg:59.43ms
step:1674/2330 train_time:99486ms step_avg:59.43ms
step:1675/2330 train_time:99542ms step_avg:59.43ms
step:1676/2330 train_time:99605ms step_avg:59.43ms
step:1677/2330 train_time:99662ms step_avg:59.43ms
step:1678/2330 train_time:99725ms step_avg:59.43ms
step:1679/2330 train_time:99782ms step_avg:59.43ms
step:1680/2330 train_time:99844ms step_avg:59.43ms
step:1681/2330 train_time:99902ms step_avg:59.43ms
step:1682/2330 train_time:99963ms step_avg:59.43ms
step:1683/2330 train_time:100021ms step_avg:59.43ms
step:1684/2330 train_time:100084ms step_avg:59.43ms
step:1685/2330 train_time:100141ms step_avg:59.43ms
step:1686/2330 train_time:100203ms step_avg:59.43ms
step:1687/2330 train_time:100260ms step_avg:59.43ms
step:1688/2330 train_time:100323ms step_avg:59.43ms
step:1689/2330 train_time:100381ms step_avg:59.43ms
step:1690/2330 train_time:100445ms step_avg:59.43ms
step:1691/2330 train_time:100502ms step_avg:59.43ms
step:1692/2330 train_time:100564ms step_avg:59.43ms
step:1693/2330 train_time:100622ms step_avg:59.43ms
step:1694/2330 train_time:100684ms step_avg:59.44ms
step:1695/2330 train_time:100741ms step_avg:59.43ms
step:1696/2330 train_time:100803ms step_avg:59.44ms
step:1697/2330 train_time:100860ms step_avg:59.43ms
step:1698/2330 train_time:100923ms step_avg:59.44ms
step:1699/2330 train_time:100981ms step_avg:59.44ms
step:1700/2330 train_time:101043ms step_avg:59.44ms
step:1701/2330 train_time:101100ms step_avg:59.44ms
step:1702/2330 train_time:101164ms step_avg:59.44ms
step:1703/2330 train_time:101221ms step_avg:59.44ms
step:1704/2330 train_time:101283ms step_avg:59.44ms
step:1705/2330 train_time:101340ms step_avg:59.44ms
step:1706/2330 train_time:101403ms step_avg:59.44ms
step:1707/2330 train_time:101460ms step_avg:59.44ms
step:1708/2330 train_time:101523ms step_avg:59.44ms
step:1709/2330 train_time:101581ms step_avg:59.44ms
step:1710/2330 train_time:101643ms step_avg:59.44ms
step:1711/2330 train_time:101700ms step_avg:59.44ms
step:1712/2330 train_time:101764ms step_avg:59.44ms
step:1713/2330 train_time:101821ms step_avg:59.44ms
step:1714/2330 train_time:101884ms step_avg:59.44ms
step:1715/2330 train_time:101941ms step_avg:59.44ms
step:1716/2330 train_time:102004ms step_avg:59.44ms
step:1717/2330 train_time:102061ms step_avg:59.44ms
step:1718/2330 train_time:102124ms step_avg:59.44ms
step:1719/2330 train_time:102181ms step_avg:59.44ms
step:1720/2330 train_time:102243ms step_avg:59.44ms
step:1721/2330 train_time:102300ms step_avg:59.44ms
step:1722/2330 train_time:102363ms step_avg:59.44ms
step:1723/2330 train_time:102420ms step_avg:59.44ms
step:1724/2330 train_time:102483ms step_avg:59.44ms
step:1725/2330 train_time:102541ms step_avg:59.44ms
step:1726/2330 train_time:102603ms step_avg:59.45ms
step:1727/2330 train_time:102661ms step_avg:59.44ms
step:1728/2330 train_time:102723ms step_avg:59.45ms
step:1729/2330 train_time:102780ms step_avg:59.44ms
step:1730/2330 train_time:102843ms step_avg:59.45ms
step:1731/2330 train_time:102900ms step_avg:59.45ms
step:1732/2330 train_time:102963ms step_avg:59.45ms
step:1733/2330 train_time:103020ms step_avg:59.45ms
step:1734/2330 train_time:103083ms step_avg:59.45ms
step:1735/2330 train_time:103141ms step_avg:59.45ms
step:1736/2330 train_time:103203ms step_avg:59.45ms
step:1737/2330 train_time:103261ms step_avg:59.45ms
step:1738/2330 train_time:103323ms step_avg:59.45ms
step:1739/2330 train_time:103380ms step_avg:59.45ms
step:1740/2330 train_time:103443ms step_avg:59.45ms
step:1741/2330 train_time:103500ms step_avg:59.45ms
step:1742/2330 train_time:103562ms step_avg:59.45ms
step:1743/2330 train_time:103620ms step_avg:59.45ms
step:1744/2330 train_time:103683ms step_avg:59.45ms
step:1745/2330 train_time:103740ms step_avg:59.45ms
step:1746/2330 train_time:103803ms step_avg:59.45ms
step:1747/2330 train_time:103860ms step_avg:59.45ms
step:1748/2330 train_time:103924ms step_avg:59.45ms
step:1749/2330 train_time:103981ms step_avg:59.45ms
step:1750/2330 train_time:104044ms step_avg:59.45ms
step:1750/2330 val_loss:3.8364 train_time:104124ms step_avg:59.50ms
step:1751/2330 train_time:104144ms step_avg:59.48ms
step:1752/2330 train_time:104167ms step_avg:59.46ms
step:1753/2330 train_time:104227ms step_avg:59.46ms
step:1754/2330 train_time:104293ms step_avg:59.46ms
step:1755/2330 train_time:104351ms step_avg:59.46ms
step:1756/2330 train_time:104419ms step_avg:59.46ms
step:1757/2330 train_time:104477ms step_avg:59.46ms
step:1758/2330 train_time:104540ms step_avg:59.47ms
step:1759/2330 train_time:104597ms step_avg:59.46ms
step:1760/2330 train_time:104659ms step_avg:59.47ms
step:1761/2330 train_time:104716ms step_avg:59.46ms
step:1762/2330 train_time:104778ms step_avg:59.47ms
step:1763/2330 train_time:104835ms step_avg:59.46ms
step:1764/2330 train_time:104896ms step_avg:59.46ms
step:1765/2330 train_time:104953ms step_avg:59.46ms
step:1766/2330 train_time:105014ms step_avg:59.46ms
step:1767/2330 train_time:105071ms step_avg:59.46ms
step:1768/2330 train_time:105135ms step_avg:59.47ms
step:1769/2330 train_time:105192ms step_avg:59.46ms
step:1770/2330 train_time:105257ms step_avg:59.47ms
step:1771/2330 train_time:105315ms step_avg:59.47ms
step:1772/2330 train_time:105379ms step_avg:59.47ms
step:1773/2330 train_time:105436ms step_avg:59.47ms
step:1774/2330 train_time:105500ms step_avg:59.47ms
step:1775/2330 train_time:105557ms step_avg:59.47ms
step:1776/2330 train_time:105620ms step_avg:59.47ms
step:1777/2330 train_time:105677ms step_avg:59.47ms
step:1778/2330 train_time:105739ms step_avg:59.47ms
step:1779/2330 train_time:105797ms step_avg:59.47ms
step:1780/2330 train_time:105858ms step_avg:59.47ms
step:1781/2330 train_time:105915ms step_avg:59.47ms
step:1782/2330 train_time:105976ms step_avg:59.47ms
step:1783/2330 train_time:106033ms step_avg:59.47ms
step:1784/2330 train_time:106097ms step_avg:59.47ms
step:1785/2330 train_time:106154ms step_avg:59.47ms
step:1786/2330 train_time:106219ms step_avg:59.47ms
step:1787/2330 train_time:106277ms step_avg:59.47ms
step:1788/2330 train_time:106340ms step_avg:59.47ms
step:1789/2330 train_time:106397ms step_avg:59.47ms
step:1790/2330 train_time:106461ms step_avg:59.48ms
step:1791/2330 train_time:106518ms step_avg:59.47ms
step:1792/2330 train_time:106582ms step_avg:59.48ms
step:1793/2330 train_time:106639ms step_avg:59.48ms
step:1794/2330 train_time:106701ms step_avg:59.48ms
step:1795/2330 train_time:106758ms step_avg:59.48ms
step:1796/2330 train_time:106820ms step_avg:59.48ms
step:1797/2330 train_time:106877ms step_avg:59.48ms
step:1798/2330 train_time:106940ms step_avg:59.48ms
step:1799/2330 train_time:106997ms step_avg:59.48ms
step:1800/2330 train_time:107060ms step_avg:59.48ms
step:1801/2330 train_time:107117ms step_avg:59.48ms
step:1802/2330 train_time:107181ms step_avg:59.48ms
step:1803/2330 train_time:107238ms step_avg:59.48ms
step:1804/2330 train_time:107301ms step_avg:59.48ms
step:1805/2330 train_time:107359ms step_avg:59.48ms
step:1806/2330 train_time:107422ms step_avg:59.48ms
step:1807/2330 train_time:107479ms step_avg:59.48ms
step:1808/2330 train_time:107542ms step_avg:59.48ms
step:1809/2330 train_time:107599ms step_avg:59.48ms
step:1810/2330 train_time:107663ms step_avg:59.48ms
step:1811/2330 train_time:107720ms step_avg:59.48ms
step:1812/2330 train_time:107785ms step_avg:59.48ms
step:1813/2330 train_time:107843ms step_avg:59.48ms
step:1814/2330 train_time:107906ms step_avg:59.49ms
step:1815/2330 train_time:107964ms step_avg:59.48ms
step:1816/2330 train_time:108027ms step_avg:59.49ms
step:1817/2330 train_time:108086ms step_avg:59.49ms
step:1818/2330 train_time:108149ms step_avg:59.49ms
step:1819/2330 train_time:108206ms step_avg:59.49ms
step:1820/2330 train_time:108269ms step_avg:59.49ms
step:1821/2330 train_time:108328ms step_avg:59.49ms
step:1822/2330 train_time:108391ms step_avg:59.49ms
step:1823/2330 train_time:108449ms step_avg:59.49ms
step:1824/2330 train_time:108511ms step_avg:59.49ms
step:1825/2330 train_time:108569ms step_avg:59.49ms
step:1826/2330 train_time:108631ms step_avg:59.49ms
step:1827/2330 train_time:108689ms step_avg:59.49ms
step:1828/2330 train_time:108752ms step_avg:59.49ms
step:1829/2330 train_time:108809ms step_avg:59.49ms
step:1830/2330 train_time:108872ms step_avg:59.49ms
step:1831/2330 train_time:108929ms step_avg:59.49ms
step:1832/2330 train_time:108993ms step_avg:59.49ms
step:1833/2330 train_time:109050ms step_avg:59.49ms
step:1834/2330 train_time:109113ms step_avg:59.49ms
step:1835/2330 train_time:109170ms step_avg:59.49ms
step:1836/2330 train_time:109233ms step_avg:59.50ms
step:1837/2330 train_time:109292ms step_avg:59.49ms
step:1838/2330 train_time:109353ms step_avg:59.50ms
step:1839/2330 train_time:109410ms step_avg:59.49ms
step:1840/2330 train_time:109474ms step_avg:59.50ms
step:1841/2330 train_time:109532ms step_avg:59.50ms
step:1842/2330 train_time:109594ms step_avg:59.50ms
step:1843/2330 train_time:109652ms step_avg:59.50ms
step:1844/2330 train_time:109714ms step_avg:59.50ms
step:1845/2330 train_time:109772ms step_avg:59.50ms
step:1846/2330 train_time:109835ms step_avg:59.50ms
step:1847/2330 train_time:109892ms step_avg:59.50ms
step:1848/2330 train_time:109955ms step_avg:59.50ms
step:1849/2330 train_time:110012ms step_avg:59.50ms
step:1850/2330 train_time:110075ms step_avg:59.50ms
step:1851/2330 train_time:110132ms step_avg:59.50ms
step:1852/2330 train_time:110195ms step_avg:59.50ms
step:1853/2330 train_time:110252ms step_avg:59.50ms
step:1854/2330 train_time:110315ms step_avg:59.50ms
step:1855/2330 train_time:110372ms step_avg:59.50ms
step:1856/2330 train_time:110435ms step_avg:59.50ms
step:1857/2330 train_time:110493ms step_avg:59.50ms
step:1858/2330 train_time:110555ms step_avg:59.50ms
step:1859/2330 train_time:110612ms step_avg:59.50ms
step:1860/2330 train_time:110674ms step_avg:59.50ms
step:1861/2330 train_time:110732ms step_avg:59.50ms
step:1862/2330 train_time:110795ms step_avg:59.50ms
step:1863/2330 train_time:110852ms step_avg:59.50ms
step:1864/2330 train_time:110915ms step_avg:59.50ms
step:1865/2330 train_time:110972ms step_avg:59.50ms
step:1866/2330 train_time:111035ms step_avg:59.50ms
step:1867/2330 train_time:111093ms step_avg:59.50ms
step:1868/2330 train_time:111155ms step_avg:59.50ms
step:1869/2330 train_time:111212ms step_avg:59.50ms
step:1870/2330 train_time:111275ms step_avg:59.51ms
step:1871/2330 train_time:111332ms step_avg:59.50ms
step:1872/2330 train_time:111394ms step_avg:59.51ms
step:1873/2330 train_time:111452ms step_avg:59.50ms
step:1874/2330 train_time:111514ms step_avg:59.51ms
step:1875/2330 train_time:111572ms step_avg:59.51ms
step:1876/2330 train_time:111634ms step_avg:59.51ms
step:1877/2330 train_time:111692ms step_avg:59.51ms
step:1878/2330 train_time:111755ms step_avg:59.51ms
step:1879/2330 train_time:111812ms step_avg:59.51ms
step:1880/2330 train_time:111874ms step_avg:59.51ms
step:1881/2330 train_time:111931ms step_avg:59.51ms
step:1882/2330 train_time:111995ms step_avg:59.51ms
step:1883/2330 train_time:112052ms step_avg:59.51ms
step:1884/2330 train_time:112115ms step_avg:59.51ms
step:1885/2330 train_time:112172ms step_avg:59.51ms
step:1886/2330 train_time:112234ms step_avg:59.51ms
step:1887/2330 train_time:112292ms step_avg:59.51ms
step:1888/2330 train_time:112355ms step_avg:59.51ms
step:1889/2330 train_time:112412ms step_avg:59.51ms
step:1890/2330 train_time:112475ms step_avg:59.51ms
step:1891/2330 train_time:112532ms step_avg:59.51ms
step:1892/2330 train_time:112595ms step_avg:59.51ms
step:1893/2330 train_time:112653ms step_avg:59.51ms
step:1894/2330 train_time:112715ms step_avg:59.51ms
step:1895/2330 train_time:112772ms step_avg:59.51ms
step:1896/2330 train_time:112836ms step_avg:59.51ms
step:1897/2330 train_time:112893ms step_avg:59.51ms
step:1898/2330 train_time:112956ms step_avg:59.51ms
step:1899/2330 train_time:113013ms step_avg:59.51ms
step:1900/2330 train_time:113075ms step_avg:59.51ms
step:1901/2330 train_time:113132ms step_avg:59.51ms
step:1902/2330 train_time:113195ms step_avg:59.51ms
step:1903/2330 train_time:113252ms step_avg:59.51ms
step:1904/2330 train_time:113315ms step_avg:59.51ms
step:1905/2330 train_time:113373ms step_avg:59.51ms
step:1906/2330 train_time:113435ms step_avg:59.51ms
step:1907/2330 train_time:113492ms step_avg:59.51ms
step:1908/2330 train_time:113555ms step_avg:59.51ms
step:1909/2330 train_time:113611ms step_avg:59.51ms
step:1910/2330 train_time:113675ms step_avg:59.52ms
step:1911/2330 train_time:113732ms step_avg:59.51ms
step:1912/2330 train_time:113796ms step_avg:59.52ms
step:1913/2330 train_time:113853ms step_avg:59.52ms
step:1914/2330 train_time:113917ms step_avg:59.52ms
step:1915/2330 train_time:113974ms step_avg:59.52ms
step:1916/2330 train_time:114036ms step_avg:59.52ms
step:1917/2330 train_time:114094ms step_avg:59.52ms
step:1918/2330 train_time:114155ms step_avg:59.52ms
step:1919/2330 train_time:114213ms step_avg:59.52ms
step:1920/2330 train_time:114275ms step_avg:59.52ms
step:1921/2330 train_time:114333ms step_avg:59.52ms
step:1922/2330 train_time:114395ms step_avg:59.52ms
step:1923/2330 train_time:114453ms step_avg:59.52ms
step:1924/2330 train_time:114516ms step_avg:59.52ms
step:1925/2330 train_time:114574ms step_avg:59.52ms
step:1926/2330 train_time:114636ms step_avg:59.52ms
step:1927/2330 train_time:114693ms step_avg:59.52ms
step:1928/2330 train_time:114756ms step_avg:59.52ms
step:1929/2330 train_time:114813ms step_avg:59.52ms
step:1930/2330 train_time:114877ms step_avg:59.52ms
step:1931/2330 train_time:114934ms step_avg:59.52ms
step:1932/2330 train_time:114997ms step_avg:59.52ms
step:1933/2330 train_time:115054ms step_avg:59.52ms
step:1934/2330 train_time:115117ms step_avg:59.52ms
step:1935/2330 train_time:115175ms step_avg:59.52ms
step:1936/2330 train_time:115237ms step_avg:59.52ms
step:1937/2330 train_time:115294ms step_avg:59.52ms
step:1938/2330 train_time:115356ms step_avg:59.52ms
step:1939/2330 train_time:115413ms step_avg:59.52ms
step:1940/2330 train_time:115476ms step_avg:59.52ms
step:1941/2330 train_time:115534ms step_avg:59.52ms
step:1942/2330 train_time:115596ms step_avg:59.52ms
step:1943/2330 train_time:115653ms step_avg:59.52ms
step:1944/2330 train_time:115716ms step_avg:59.52ms
step:1945/2330 train_time:115773ms step_avg:59.52ms
step:1946/2330 train_time:115837ms step_avg:59.53ms
step:1947/2330 train_time:115893ms step_avg:59.52ms
step:1948/2330 train_time:115958ms step_avg:59.53ms
step:1949/2330 train_time:116015ms step_avg:59.53ms
step:1950/2330 train_time:116077ms step_avg:59.53ms
step:1951/2330 train_time:116134ms step_avg:59.53ms
step:1952/2330 train_time:116197ms step_avg:59.53ms
step:1953/2330 train_time:116253ms step_avg:59.53ms
step:1954/2330 train_time:116316ms step_avg:59.53ms
step:1955/2330 train_time:116373ms step_avg:59.53ms
step:1956/2330 train_time:116436ms step_avg:59.53ms
step:1957/2330 train_time:116493ms step_avg:59.53ms
step:1958/2330 train_time:116556ms step_avg:59.53ms
step:1959/2330 train_time:116613ms step_avg:59.53ms
step:1960/2330 train_time:116676ms step_avg:59.53ms
step:1961/2330 train_time:116734ms step_avg:59.53ms
step:1962/2330 train_time:116796ms step_avg:59.53ms
step:1963/2330 train_time:116854ms step_avg:59.53ms
step:1964/2330 train_time:116917ms step_avg:59.53ms
step:1965/2330 train_time:116975ms step_avg:59.53ms
step:1966/2330 train_time:117037ms step_avg:59.53ms
step:1967/2330 train_time:117095ms step_avg:59.53ms
step:1968/2330 train_time:117156ms step_avg:59.53ms
step:1969/2330 train_time:117213ms step_avg:59.53ms
step:1970/2330 train_time:117276ms step_avg:59.53ms
step:1971/2330 train_time:117334ms step_avg:59.53ms
step:1972/2330 train_time:117396ms step_avg:59.53ms
step:1973/2330 train_time:117453ms step_avg:59.53ms
step:1974/2330 train_time:117516ms step_avg:59.53ms
step:1975/2330 train_time:117573ms step_avg:59.53ms
step:1976/2330 train_time:117635ms step_avg:59.53ms
step:1977/2330 train_time:117692ms step_avg:59.53ms
step:1978/2330 train_time:117755ms step_avg:59.53ms
step:1979/2330 train_time:117813ms step_avg:59.53ms
step:1980/2330 train_time:117876ms step_avg:59.53ms
step:1981/2330 train_time:117933ms step_avg:59.53ms
step:1982/2330 train_time:117996ms step_avg:59.53ms
step:1983/2330 train_time:118053ms step_avg:59.53ms
step:1984/2330 train_time:118116ms step_avg:59.53ms
step:1985/2330 train_time:118173ms step_avg:59.53ms
step:1986/2330 train_time:118236ms step_avg:59.53ms
step:1987/2330 train_time:118293ms step_avg:59.53ms
step:1988/2330 train_time:118356ms step_avg:59.54ms
step:1989/2330 train_time:118413ms step_avg:59.53ms
step:1990/2330 train_time:118475ms step_avg:59.54ms
step:1991/2330 train_time:118532ms step_avg:59.53ms
step:1992/2330 train_time:118596ms step_avg:59.54ms
step:1993/2330 train_time:118653ms step_avg:59.53ms
step:1994/2330 train_time:118717ms step_avg:59.54ms
step:1995/2330 train_time:118774ms step_avg:59.54ms
step:1996/2330 train_time:118837ms step_avg:59.54ms
step:1997/2330 train_time:118894ms step_avg:59.54ms
step:1998/2330 train_time:118958ms step_avg:59.54ms
step:1999/2330 train_time:119014ms step_avg:59.54ms
step:2000/2330 train_time:119077ms step_avg:59.54ms
step:2000/2330 val_loss:3.7725 train_time:119157ms step_avg:59.58ms
step:2001/2330 train_time:119177ms step_avg:59.56ms
step:2002/2330 train_time:119200ms step_avg:59.54ms
step:2003/2330 train_time:119258ms step_avg:59.54ms
step:2004/2330 train_time:119326ms step_avg:59.54ms
step:2005/2330 train_time:119383ms step_avg:59.54ms
step:2006/2330 train_time:119448ms step_avg:59.55ms
step:2007/2330 train_time:119505ms step_avg:59.54ms
step:2008/2330 train_time:119567ms step_avg:59.55ms
step:2009/2330 train_time:119624ms step_avg:59.54ms
step:2010/2330 train_time:119686ms step_avg:59.55ms
step:2011/2330 train_time:119743ms step_avg:59.54ms
step:2012/2330 train_time:119805ms step_avg:59.55ms
step:2013/2330 train_time:119862ms step_avg:59.54ms
step:2014/2330 train_time:119924ms step_avg:59.54ms
step:2015/2330 train_time:119981ms step_avg:59.54ms
step:2016/2330 train_time:120042ms step_avg:59.54ms
step:2017/2330 train_time:120100ms step_avg:59.54ms
step:2018/2330 train_time:120167ms step_avg:59.55ms
step:2019/2330 train_time:120225ms step_avg:59.55ms
step:2020/2330 train_time:120289ms step_avg:59.55ms
step:2021/2330 train_time:120347ms step_avg:59.55ms
step:2022/2330 train_time:120411ms step_avg:59.55ms
step:2023/2330 train_time:120468ms step_avg:59.55ms
step:2024/2330 train_time:120530ms step_avg:59.55ms
step:2025/2330 train_time:120587ms step_avg:59.55ms
step:2026/2330 train_time:120649ms step_avg:59.55ms
step:2027/2330 train_time:120707ms step_avg:59.55ms
step:2028/2330 train_time:120768ms step_avg:59.55ms
step:2029/2330 train_time:120825ms step_avg:59.55ms
step:2030/2330 train_time:120888ms step_avg:59.55ms
step:2031/2330 train_time:120945ms step_avg:59.55ms
step:2032/2330 train_time:121007ms step_avg:59.55ms
step:2033/2330 train_time:121064ms step_avg:59.55ms
step:2034/2330 train_time:121127ms step_avg:59.55ms
step:2035/2330 train_time:121185ms step_avg:59.55ms
step:2036/2330 train_time:121248ms step_avg:59.55ms
step:2037/2330 train_time:121306ms step_avg:59.55ms
step:2038/2330 train_time:121370ms step_avg:59.55ms
step:2039/2330 train_time:121427ms step_avg:59.55ms
step:2040/2330 train_time:121490ms step_avg:59.55ms
step:2041/2330 train_time:121547ms step_avg:59.55ms
step:2042/2330 train_time:121610ms step_avg:59.55ms
step:2043/2330 train_time:121667ms step_avg:59.55ms
step:2044/2330 train_time:121729ms step_avg:59.55ms
step:2045/2330 train_time:121786ms step_avg:59.55ms
step:2046/2330 train_time:121849ms step_avg:59.55ms
step:2047/2330 train_time:121907ms step_avg:59.55ms
step:2048/2330 train_time:121969ms step_avg:59.55ms
step:2049/2330 train_time:122025ms step_avg:59.55ms
step:2050/2330 train_time:122088ms step_avg:59.56ms
step:2051/2330 train_time:122146ms step_avg:59.55ms
step:2052/2330 train_time:122209ms step_avg:59.56ms
step:2053/2330 train_time:122266ms step_avg:59.55ms
step:2054/2330 train_time:122329ms step_avg:59.56ms
step:2055/2330 train_time:122386ms step_avg:59.56ms
step:2056/2330 train_time:122450ms step_avg:59.56ms
step:2057/2330 train_time:122508ms step_avg:59.56ms
step:2058/2330 train_time:122569ms step_avg:59.56ms
step:2059/2330 train_time:122626ms step_avg:59.56ms
step:2060/2330 train_time:122689ms step_avg:59.56ms
step:2061/2330 train_time:122746ms step_avg:59.56ms
step:2062/2330 train_time:122809ms step_avg:59.56ms
step:2063/2330 train_time:122866ms step_avg:59.56ms
step:2064/2330 train_time:122928ms step_avg:59.56ms
step:2065/2330 train_time:122985ms step_avg:59.56ms
step:2066/2330 train_time:123048ms step_avg:59.56ms
step:2067/2330 train_time:123105ms step_avg:59.56ms
step:2068/2330 train_time:123168ms step_avg:59.56ms
step:2069/2330 train_time:123226ms step_avg:59.56ms
step:2070/2330 train_time:123288ms step_avg:59.56ms
step:2071/2330 train_time:123346ms step_avg:59.56ms
step:2072/2330 train_time:123409ms step_avg:59.56ms
step:2073/2330 train_time:123467ms step_avg:59.56ms
step:2074/2330 train_time:123529ms step_avg:59.56ms
step:2075/2330 train_time:123588ms step_avg:59.56ms
step:2076/2330 train_time:123649ms step_avg:59.56ms
step:2077/2330 train_time:123707ms step_avg:59.56ms
step:2078/2330 train_time:123768ms step_avg:59.56ms
step:2079/2330 train_time:123825ms step_avg:59.56ms
step:2080/2330 train_time:123888ms step_avg:59.56ms
step:2081/2330 train_time:123945ms step_avg:59.56ms
step:2082/2330 train_time:124007ms step_avg:59.56ms
step:2083/2330 train_time:124064ms step_avg:59.56ms
step:2084/2330 train_time:124127ms step_avg:59.56ms
step:2085/2330 train_time:124184ms step_avg:59.56ms
step:2086/2330 train_time:124248ms step_avg:59.56ms
step:2087/2330 train_time:124306ms step_avg:59.56ms
step:2088/2330 train_time:124368ms step_avg:59.56ms
step:2089/2330 train_time:124425ms step_avg:59.56ms
step:2090/2330 train_time:124488ms step_avg:59.56ms
step:2091/2330 train_time:124546ms step_avg:59.56ms
step:2092/2330 train_time:124608ms step_avg:59.56ms
step:2093/2330 train_time:124666ms step_avg:59.56ms
step:2094/2330 train_time:124728ms step_avg:59.56ms
step:2095/2330 train_time:124785ms step_avg:59.56ms
step:2096/2330 train_time:124847ms step_avg:59.56ms
step:2097/2330 train_time:124904ms step_avg:59.56ms
step:2098/2330 train_time:124965ms step_avg:59.56ms
step:2099/2330 train_time:125022ms step_avg:59.56ms
step:2100/2330 train_time:125084ms step_avg:59.56ms
step:2101/2330 train_time:125142ms step_avg:59.56ms
step:2102/2330 train_time:125205ms step_avg:59.56ms
step:2103/2330 train_time:125263ms step_avg:59.56ms
step:2104/2330 train_time:125326ms step_avg:59.57ms
step:2105/2330 train_time:125384ms step_avg:59.56ms
step:2106/2330 train_time:125446ms step_avg:59.57ms
step:2107/2330 train_time:125504ms step_avg:59.57ms
step:2108/2330 train_time:125567ms step_avg:59.57ms
step:2109/2330 train_time:125624ms step_avg:59.57ms
step:2110/2330 train_time:125687ms step_avg:59.57ms
step:2111/2330 train_time:125745ms step_avg:59.57ms
step:2112/2330 train_time:125807ms step_avg:59.57ms
step:2113/2330 train_time:125865ms step_avg:59.57ms
step:2114/2330 train_time:125926ms step_avg:59.57ms
step:2115/2330 train_time:125984ms step_avg:59.57ms
step:2116/2330 train_time:126046ms step_avg:59.57ms
step:2117/2330 train_time:126103ms step_avg:59.57ms
step:2118/2330 train_time:126165ms step_avg:59.57ms
step:2119/2330 train_time:126223ms step_avg:59.57ms
step:2120/2330 train_time:126285ms step_avg:59.57ms
step:2121/2330 train_time:126343ms step_avg:59.57ms
step:2122/2330 train_time:126406ms step_avg:59.57ms
step:2123/2330 train_time:126464ms step_avg:59.57ms
step:2124/2330 train_time:126527ms step_avg:59.57ms
step:2125/2330 train_time:126584ms step_avg:59.57ms
step:2126/2330 train_time:126646ms step_avg:59.57ms
step:2127/2330 train_time:126703ms step_avg:59.57ms
step:2128/2330 train_time:126766ms step_avg:59.57ms
step:2129/2330 train_time:126824ms step_avg:59.57ms
step:2130/2330 train_time:126887ms step_avg:59.57ms
step:2131/2330 train_time:126944ms step_avg:59.57ms
step:2132/2330 train_time:127007ms step_avg:59.57ms
step:2133/2330 train_time:127064ms step_avg:59.57ms
step:2134/2330 train_time:127126ms step_avg:59.57ms
step:2135/2330 train_time:127183ms step_avg:59.57ms
step:2136/2330 train_time:127246ms step_avg:59.57ms
step:2137/2330 train_time:127304ms step_avg:59.57ms
step:2138/2330 train_time:127366ms step_avg:59.57ms
step:2139/2330 train_time:127423ms step_avg:59.57ms
step:2140/2330 train_time:127486ms step_avg:59.57ms
step:2141/2330 train_time:127544ms step_avg:59.57ms
step:2142/2330 train_time:127606ms step_avg:59.57ms
step:2143/2330 train_time:127664ms step_avg:59.57ms
step:2144/2330 train_time:127725ms step_avg:59.57ms
step:2145/2330 train_time:127783ms step_avg:59.57ms
step:2146/2330 train_time:127845ms step_avg:59.57ms
step:2147/2330 train_time:127903ms step_avg:59.57ms
step:2148/2330 train_time:127965ms step_avg:59.57ms
step:2149/2330 train_time:128022ms step_avg:59.57ms
step:2150/2330 train_time:128084ms step_avg:59.57ms
step:2151/2330 train_time:128142ms step_avg:59.57ms
step:2152/2330 train_time:128204ms step_avg:59.57ms
step:2153/2330 train_time:128262ms step_avg:59.57ms
step:2154/2330 train_time:128324ms step_avg:59.57ms
step:2155/2330 train_time:128381ms step_avg:59.57ms
step:2156/2330 train_time:128446ms step_avg:59.58ms
step:2157/2330 train_time:128504ms step_avg:59.58ms
step:2158/2330 train_time:128565ms step_avg:59.58ms
step:2159/2330 train_time:128622ms step_avg:59.58ms
step:2160/2330 train_time:128687ms step_avg:59.58ms
step:2161/2330 train_time:128744ms step_avg:59.58ms
step:2162/2330 train_time:128806ms step_avg:59.58ms
step:2163/2330 train_time:128864ms step_avg:59.58ms
step:2164/2330 train_time:128926ms step_avg:59.58ms
step:2165/2330 train_time:128983ms step_avg:59.58ms
step:2166/2330 train_time:129045ms step_avg:59.58ms
step:2167/2330 train_time:129102ms step_avg:59.58ms
step:2168/2330 train_time:129165ms step_avg:59.58ms
step:2169/2330 train_time:129222ms step_avg:59.58ms
step:2170/2330 train_time:129285ms step_avg:59.58ms
step:2171/2330 train_time:129343ms step_avg:59.58ms
step:2172/2330 train_time:129406ms step_avg:59.58ms
step:2173/2330 train_time:129463ms step_avg:59.58ms
step:2174/2330 train_time:129527ms step_avg:59.58ms
step:2175/2330 train_time:129585ms step_avg:59.58ms
step:2176/2330 train_time:129647ms step_avg:59.58ms
step:2177/2330 train_time:129704ms step_avg:59.58ms
step:2178/2330 train_time:129767ms step_avg:59.58ms
step:2179/2330 train_time:129824ms step_avg:59.58ms
step:2180/2330 train_time:129888ms step_avg:59.58ms
step:2181/2330 train_time:129945ms step_avg:59.58ms
step:2182/2330 train_time:130008ms step_avg:59.58ms
step:2183/2330 train_time:130065ms step_avg:59.58ms
step:2184/2330 train_time:130127ms step_avg:59.58ms
step:2185/2330 train_time:130184ms step_avg:59.58ms
step:2186/2330 train_time:130246ms step_avg:59.58ms
step:2187/2330 train_time:130304ms step_avg:59.58ms
step:2188/2330 train_time:130367ms step_avg:59.58ms
step:2189/2330 train_time:130424ms step_avg:59.58ms
step:2190/2330 train_time:130488ms step_avg:59.58ms
step:2191/2330 train_time:130545ms step_avg:59.58ms
step:2192/2330 train_time:130608ms step_avg:59.58ms
step:2193/2330 train_time:130665ms step_avg:59.58ms
step:2194/2330 train_time:130727ms step_avg:59.58ms
step:2195/2330 train_time:130785ms step_avg:59.58ms
step:2196/2330 train_time:130847ms step_avg:59.58ms
step:2197/2330 train_time:130904ms step_avg:59.58ms
step:2198/2330 train_time:130967ms step_avg:59.58ms
step:2199/2330 train_time:131024ms step_avg:59.58ms
step:2200/2330 train_time:131086ms step_avg:59.58ms
step:2201/2330 train_time:131144ms step_avg:59.58ms
step:2202/2330 train_time:131206ms step_avg:59.58ms
step:2203/2330 train_time:131263ms step_avg:59.58ms
step:2204/2330 train_time:131326ms step_avg:59.59ms
step:2205/2330 train_time:131384ms step_avg:59.58ms
step:2206/2330 train_time:131446ms step_avg:59.59ms
step:2207/2330 train_time:131503ms step_avg:59.58ms
step:2208/2330 train_time:131566ms step_avg:59.59ms
step:2209/2330 train_time:131624ms step_avg:59.59ms
step:2210/2330 train_time:131686ms step_avg:59.59ms
step:2211/2330 train_time:131744ms step_avg:59.59ms
step:2212/2330 train_time:131806ms step_avg:59.59ms
step:2213/2330 train_time:131864ms step_avg:59.59ms
step:2214/2330 train_time:131928ms step_avg:59.59ms
step:2215/2330 train_time:131985ms step_avg:59.59ms
step:2216/2330 train_time:132047ms step_avg:59.59ms
step:2217/2330 train_time:132105ms step_avg:59.59ms
step:2218/2330 train_time:132166ms step_avg:59.59ms
step:2219/2330 train_time:132223ms step_avg:59.59ms
step:2220/2330 train_time:132286ms step_avg:59.59ms
step:2221/2330 train_time:132343ms step_avg:59.59ms
step:2222/2330 train_time:132406ms step_avg:59.59ms
step:2223/2330 train_time:132463ms step_avg:59.59ms
step:2224/2330 train_time:132526ms step_avg:59.59ms
step:2225/2330 train_time:132583ms step_avg:59.59ms
step:2226/2330 train_time:132646ms step_avg:59.59ms
step:2227/2330 train_time:132704ms step_avg:59.59ms
step:2228/2330 train_time:132766ms step_avg:59.59ms
step:2229/2330 train_time:132824ms step_avg:59.59ms
step:2230/2330 train_time:132886ms step_avg:59.59ms
step:2231/2330 train_time:132944ms step_avg:59.59ms
step:2232/2330 train_time:133007ms step_avg:59.59ms
step:2233/2330 train_time:133065ms step_avg:59.59ms
step:2234/2330 train_time:133127ms step_avg:59.59ms
step:2235/2330 train_time:133185ms step_avg:59.59ms
step:2236/2330 train_time:133248ms step_avg:59.59ms
step:2237/2330 train_time:133306ms step_avg:59.59ms
step:2238/2330 train_time:133367ms step_avg:59.59ms
step:2239/2330 train_time:133424ms step_avg:59.59ms
step:2240/2330 train_time:133487ms step_avg:59.59ms
step:2241/2330 train_time:133544ms step_avg:59.59ms
step:2242/2330 train_time:133607ms step_avg:59.59ms
step:2243/2330 train_time:133664ms step_avg:59.59ms
step:2244/2330 train_time:133727ms step_avg:59.59ms
step:2245/2330 train_time:133784ms step_avg:59.59ms
step:2246/2330 train_time:133846ms step_avg:59.59ms
step:2247/2330 train_time:133904ms step_avg:59.59ms
step:2248/2330 train_time:133968ms step_avg:59.59ms
step:2249/2330 train_time:134025ms step_avg:59.59ms
step:2250/2330 train_time:134088ms step_avg:59.59ms
step:2250/2330 val_loss:3.7226 train_time:134167ms step_avg:59.63ms
step:2251/2330 train_time:134186ms step_avg:59.61ms
step:2252/2330 train_time:134209ms step_avg:59.60ms
step:2253/2330 train_time:134269ms step_avg:59.60ms
step:2254/2330 train_time:134336ms step_avg:59.60ms
step:2255/2330 train_time:134394ms step_avg:59.60ms
step:2256/2330 train_time:134455ms step_avg:59.60ms
step:2257/2330 train_time:134512ms step_avg:59.60ms
step:2258/2330 train_time:134574ms step_avg:59.60ms
step:2259/2330 train_time:134631ms step_avg:59.60ms
step:2260/2330 train_time:134692ms step_avg:59.60ms
step:2261/2330 train_time:134749ms step_avg:59.60ms
step:2262/2330 train_time:134811ms step_avg:59.60ms
step:2263/2330 train_time:134868ms step_avg:59.60ms
step:2264/2330 train_time:134930ms step_avg:59.60ms
step:2265/2330 train_time:134987ms step_avg:59.60ms
step:2266/2330 train_time:135049ms step_avg:59.60ms
step:2267/2330 train_time:135106ms step_avg:59.60ms
step:2268/2330 train_time:135171ms step_avg:59.60ms
step:2269/2330 train_time:135229ms step_avg:59.60ms
step:2270/2330 train_time:135295ms step_avg:59.60ms
step:2271/2330 train_time:135353ms step_avg:59.60ms
step:2272/2330 train_time:135416ms step_avg:59.60ms
step:2273/2330 train_time:135473ms step_avg:59.60ms
step:2274/2330 train_time:135535ms step_avg:59.60ms
step:2275/2330 train_time:135592ms step_avg:59.60ms
step:2276/2330 train_time:135654ms step_avg:59.60ms
step:2277/2330 train_time:135711ms step_avg:59.60ms
step:2278/2330 train_time:135773ms step_avg:59.60ms
step:2279/2330 train_time:135830ms step_avg:59.60ms
step:2280/2330 train_time:135892ms step_avg:59.60ms
step:2281/2330 train_time:135948ms step_avg:59.60ms
step:2282/2330 train_time:136011ms step_avg:59.60ms
step:2283/2330 train_time:136068ms step_avg:59.60ms
step:2284/2330 train_time:136131ms step_avg:59.60ms
step:2285/2330 train_time:136190ms step_avg:59.60ms
step:2286/2330 train_time:136252ms step_avg:59.60ms
step:2287/2330 train_time:136310ms step_avg:59.60ms
step:2288/2330 train_time:136373ms step_avg:59.60ms
step:2289/2330 train_time:136431ms step_avg:59.60ms
step:2290/2330 train_time:136495ms step_avg:59.60ms
step:2291/2330 train_time:136552ms step_avg:59.60ms
step:2292/2330 train_time:136614ms step_avg:59.60ms
step:2293/2330 train_time:136671ms step_avg:59.60ms
step:2294/2330 train_time:136733ms step_avg:59.60ms
step:2295/2330 train_time:136791ms step_avg:59.60ms
step:2296/2330 train_time:136852ms step_avg:59.60ms
step:2297/2330 train_time:136909ms step_avg:59.60ms
step:2298/2330 train_time:136971ms step_avg:59.60ms
step:2299/2330 train_time:137028ms step_avg:59.60ms
step:2300/2330 train_time:137090ms step_avg:59.60ms
step:2301/2330 train_time:137148ms step_avg:59.60ms
step:2302/2330 train_time:137211ms step_avg:59.61ms
step:2303/2330 train_time:137269ms step_avg:59.60ms
step:2304/2330 train_time:137332ms step_avg:59.61ms
step:2305/2330 train_time:137389ms step_avg:59.60ms
step:2306/2330 train_time:137453ms step_avg:59.61ms
step:2307/2330 train_time:137510ms step_avg:59.61ms
step:2308/2330 train_time:137573ms step_avg:59.61ms
step:2309/2330 train_time:137631ms step_avg:59.61ms
step:2310/2330 train_time:137693ms step_avg:59.61ms
step:2311/2330 train_time:137750ms step_avg:59.61ms
step:2312/2330 train_time:137812ms step_avg:59.61ms
step:2313/2330 train_time:137869ms step_avg:59.61ms
step:2314/2330 train_time:137931ms step_avg:59.61ms
step:2315/2330 train_time:137988ms step_avg:59.61ms
step:2316/2330 train_time:138050ms step_avg:59.61ms
step:2317/2330 train_time:138107ms step_avg:59.61ms
step:2318/2330 train_time:138170ms step_avg:59.61ms
step:2319/2330 train_time:138228ms step_avg:59.61ms
step:2320/2330 train_time:138291ms step_avg:59.61ms
step:2321/2330 train_time:138349ms step_avg:59.61ms
step:2322/2330 train_time:138411ms step_avg:59.61ms
step:2323/2330 train_time:138470ms step_avg:59.61ms
step:2324/2330 train_time:138533ms step_avg:59.61ms
step:2325/2330 train_time:138591ms step_avg:59.61ms
step:2326/2330 train_time:138652ms step_avg:59.61ms
step:2327/2330 train_time:138709ms step_avg:59.61ms
step:2328/2330 train_time:138771ms step_avg:59.61ms
step:2329/2330 train_time:138829ms step_avg:59.61ms
step:2330/2330 train_time:138891ms step_avg:59.61ms
step:2330/2330 val_loss:3.7068 train_time:138970ms step_avg:59.64ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
