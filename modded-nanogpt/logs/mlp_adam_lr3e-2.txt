import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_adam_lr3e-2"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-1, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizer2 = torch.optim.Adam(
    hidden_matrix_params + gate_params,
    lr=3e-2,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 04:49:30 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   33C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   32C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.05ms
step:1/2330 train_time:73ms step_avg:73.34ms
step:2/2330 train_time:156ms step_avg:77.84ms
step:3/2330 train_time:168ms step_avg:55.87ms
step:4/2330 train_time:179ms step_avg:44.66ms
step:5/2330 train_time:188ms step_avg:37.55ms
step:6/2330 train_time:198ms step_avg:32.98ms
step:7/2330 train_time:247ms step_avg:35.32ms
step:8/2330 train_time:287ms step_avg:35.88ms
step:9/2330 train_time:320ms step_avg:35.59ms
step:10/2330 train_time:360ms step_avg:36.01ms
step:11/2330 train_time:394ms step_avg:35.77ms
step:12/2330 train_time:433ms step_avg:36.11ms
step:13/2330 train_time:467ms step_avg:35.89ms
step:14/2330 train_time:506ms step_avg:36.17ms
step:15/2330 train_time:540ms step_avg:36.00ms
step:16/2330 train_time:580ms step_avg:36.25ms
step:17/2330 train_time:613ms step_avg:36.08ms
step:18/2330 train_time:653ms step_avg:36.30ms
step:19/2330 train_time:687ms step_avg:36.14ms
step:20/2330 train_time:727ms step_avg:36.34ms
step:21/2330 train_time:760ms step_avg:36.20ms
step:22/2330 train_time:800ms step_avg:36.37ms
step:23/2330 train_time:834ms step_avg:36.25ms
step:24/2330 train_time:874ms step_avg:36.40ms
step:25/2330 train_time:907ms step_avg:36.28ms
step:26/2330 train_time:947ms step_avg:36.42ms
step:27/2330 train_time:981ms step_avg:36.33ms
step:28/2330 train_time:1021ms step_avg:36.46ms
step:29/2330 train_time:1055ms step_avg:36.38ms
step:30/2330 train_time:1095ms step_avg:36.50ms
step:31/2330 train_time:1130ms step_avg:36.46ms
step:32/2330 train_time:1170ms step_avg:36.57ms
step:33/2330 train_time:1205ms step_avg:36.52ms
step:34/2330 train_time:1245ms step_avg:36.63ms
step:35/2330 train_time:1281ms step_avg:36.59ms
step:36/2330 train_time:1321ms step_avg:36.69ms
step:37/2330 train_time:1356ms step_avg:36.65ms
step:38/2330 train_time:1396ms step_avg:36.74ms
step:39/2330 train_time:1430ms step_avg:36.68ms
step:40/2330 train_time:1470ms step_avg:36.76ms
step:41/2330 train_time:1504ms step_avg:36.69ms
step:42/2330 train_time:1545ms step_avg:36.78ms
step:43/2330 train_time:1579ms step_avg:36.72ms
step:44/2330 train_time:1619ms step_avg:36.80ms
step:45/2330 train_time:1653ms step_avg:36.72ms
step:46/2330 train_time:1693ms step_avg:36.80ms
step:47/2330 train_time:1726ms step_avg:36.73ms
step:48/2330 train_time:1766ms step_avg:36.79ms
step:49/2330 train_time:1800ms step_avg:36.74ms
step:50/2330 train_time:1840ms step_avg:36.81ms
step:51/2330 train_time:1874ms step_avg:36.75ms
step:52/2330 train_time:1915ms step_avg:36.82ms
step:53/2330 train_time:1948ms step_avg:36.75ms
step:54/2330 train_time:1988ms step_avg:36.82ms
step:55/2330 train_time:2022ms step_avg:36.76ms
step:56/2330 train_time:2062ms step_avg:36.83ms
step:57/2330 train_time:2097ms step_avg:36.78ms
step:58/2330 train_time:2137ms step_avg:36.84ms
step:59/2330 train_time:2171ms step_avg:36.80ms
step:60/2330 train_time:2212ms step_avg:36.86ms
step:61/2330 train_time:2246ms step_avg:36.82ms
step:62/2330 train_time:2286ms step_avg:36.87ms
step:63/2330 train_time:2322ms step_avg:36.85ms
step:64/2330 train_time:2362ms step_avg:36.90ms
step:65/2330 train_time:2397ms step_avg:36.87ms
step:66/2330 train_time:2437ms step_avg:36.92ms
step:67/2330 train_time:2471ms step_avg:36.89ms
step:68/2330 train_time:2511ms step_avg:36.93ms
step:69/2330 train_time:2546ms step_avg:36.90ms
step:70/2330 train_time:2586ms step_avg:36.95ms
step:71/2330 train_time:2621ms step_avg:36.91ms
step:72/2330 train_time:2661ms step_avg:36.96ms
step:73/2330 train_time:2695ms step_avg:36.92ms
step:74/2330 train_time:2735ms step_avg:36.97ms
step:75/2330 train_time:2769ms step_avg:36.92ms
step:76/2330 train_time:2809ms step_avg:36.97ms
step:77/2330 train_time:2843ms step_avg:36.93ms
step:78/2330 train_time:2884ms step_avg:36.97ms
step:79/2330 train_time:2918ms step_avg:36.94ms
step:80/2330 train_time:2958ms step_avg:36.98ms
step:81/2330 train_time:2993ms step_avg:36.94ms
step:82/2330 train_time:3033ms step_avg:36.99ms
step:83/2330 train_time:3067ms step_avg:36.95ms
step:84/2330 train_time:3107ms step_avg:36.99ms
step:85/2330 train_time:3142ms step_avg:36.96ms
step:86/2330 train_time:3182ms step_avg:37.01ms
step:87/2330 train_time:3217ms step_avg:36.98ms
step:88/2330 train_time:3257ms step_avg:37.01ms
step:89/2330 train_time:3292ms step_avg:36.99ms
step:90/2330 train_time:3333ms step_avg:37.03ms
step:91/2330 train_time:3367ms step_avg:37.00ms
step:92/2330 train_time:3407ms step_avg:37.04ms
step:93/2330 train_time:3442ms step_avg:37.01ms
step:94/2330 train_time:3482ms step_avg:37.05ms
step:95/2330 train_time:3517ms step_avg:37.03ms
step:96/2330 train_time:3558ms step_avg:37.06ms
step:97/2330 train_time:3593ms step_avg:37.04ms
step:98/2330 train_time:3633ms step_avg:37.08ms
step:99/2330 train_time:3668ms step_avg:37.05ms
step:100/2330 train_time:3708ms step_avg:37.08ms
step:101/2330 train_time:3742ms step_avg:37.05ms
step:102/2330 train_time:3782ms step_avg:37.08ms
step:103/2330 train_time:3816ms step_avg:37.05ms
step:104/2330 train_time:3857ms step_avg:37.08ms
step:105/2330 train_time:3891ms step_avg:37.06ms
step:106/2330 train_time:3932ms step_avg:37.09ms
step:107/2330 train_time:3966ms step_avg:37.06ms
step:108/2330 train_time:4006ms step_avg:37.09ms
step:109/2330 train_time:4040ms step_avg:37.07ms
step:110/2330 train_time:4081ms step_avg:37.10ms
step:111/2330 train_time:4116ms step_avg:37.08ms
step:112/2330 train_time:4156ms step_avg:37.11ms
step:113/2330 train_time:4191ms step_avg:37.09ms
step:114/2330 train_time:4231ms step_avg:37.12ms
step:115/2330 train_time:4266ms step_avg:37.09ms
step:116/2330 train_time:4306ms step_avg:37.12ms
step:117/2330 train_time:4341ms step_avg:37.10ms
step:118/2330 train_time:4381ms step_avg:37.13ms
step:119/2330 train_time:4417ms step_avg:37.11ms
step:120/2330 train_time:4457ms step_avg:37.14ms
step:121/2330 train_time:4492ms step_avg:37.12ms
step:122/2330 train_time:4533ms step_avg:37.15ms
step:123/2330 train_time:4568ms step_avg:37.14ms
step:124/2330 train_time:4608ms step_avg:37.16ms
step:125/2330 train_time:4642ms step_avg:37.14ms
step:126/2330 train_time:4683ms step_avg:37.16ms
step:127/2330 train_time:4718ms step_avg:37.15ms
step:128/2330 train_time:4758ms step_avg:37.17ms
step:129/2330 train_time:4792ms step_avg:37.15ms
step:130/2330 train_time:4833ms step_avg:37.18ms
step:131/2330 train_time:4867ms step_avg:37.15ms
step:132/2330 train_time:4908ms step_avg:37.18ms
step:133/2330 train_time:4942ms step_avg:37.16ms
step:134/2330 train_time:4982ms step_avg:37.18ms
step:135/2330 train_time:5017ms step_avg:37.16ms
step:136/2330 train_time:5057ms step_avg:37.19ms
step:137/2330 train_time:5092ms step_avg:37.17ms
step:138/2330 train_time:5133ms step_avg:37.19ms
step:139/2330 train_time:5167ms step_avg:37.17ms
step:140/2330 train_time:5207ms step_avg:37.20ms
step:141/2330 train_time:5242ms step_avg:37.18ms
step:142/2330 train_time:5283ms step_avg:37.20ms
step:143/2330 train_time:5317ms step_avg:37.18ms
step:144/2330 train_time:5358ms step_avg:37.21ms
step:145/2330 train_time:5393ms step_avg:37.19ms
step:146/2330 train_time:5433ms step_avg:37.21ms
step:147/2330 train_time:5468ms step_avg:37.20ms
step:148/2330 train_time:5509ms step_avg:37.22ms
step:149/2330 train_time:5544ms step_avg:37.21ms
step:150/2330 train_time:5584ms step_avg:37.23ms
step:151/2330 train_time:5619ms step_avg:37.21ms
step:152/2330 train_time:5659ms step_avg:37.23ms
step:153/2330 train_time:5694ms step_avg:37.22ms
step:154/2330 train_time:5734ms step_avg:37.24ms
step:155/2330 train_time:5769ms step_avg:37.22ms
step:156/2330 train_time:5809ms step_avg:37.24ms
step:157/2330 train_time:5844ms step_avg:37.22ms
step:158/2330 train_time:5884ms step_avg:37.24ms
step:159/2330 train_time:5919ms step_avg:37.22ms
step:160/2330 train_time:5959ms step_avg:37.24ms
step:161/2330 train_time:5994ms step_avg:37.23ms
step:162/2330 train_time:6034ms step_avg:37.25ms
step:163/2330 train_time:6069ms step_avg:37.23ms
step:164/2330 train_time:6109ms step_avg:37.25ms
step:165/2330 train_time:6143ms step_avg:37.23ms
step:166/2330 train_time:6184ms step_avg:37.25ms
step:167/2330 train_time:6218ms step_avg:37.23ms
step:168/2330 train_time:6259ms step_avg:37.25ms
step:169/2330 train_time:6293ms step_avg:37.24ms
step:170/2330 train_time:6334ms step_avg:37.26ms
step:171/2330 train_time:6368ms step_avg:37.24ms
step:172/2330 train_time:6408ms step_avg:37.26ms
step:173/2330 train_time:6443ms step_avg:37.24ms
step:174/2330 train_time:6484ms step_avg:37.26ms
step:175/2330 train_time:6518ms step_avg:37.25ms
step:176/2330 train_time:6559ms step_avg:37.27ms
step:177/2330 train_time:6595ms step_avg:37.26ms
step:178/2330 train_time:6635ms step_avg:37.28ms
step:179/2330 train_time:6670ms step_avg:37.26ms
step:180/2330 train_time:6710ms step_avg:37.28ms
step:181/2330 train_time:6745ms step_avg:37.27ms
step:182/2330 train_time:6786ms step_avg:37.28ms
step:183/2330 train_time:6821ms step_avg:37.27ms
step:184/2330 train_time:6861ms step_avg:37.29ms
step:185/2330 train_time:6896ms step_avg:37.28ms
step:186/2330 train_time:6937ms step_avg:37.29ms
step:187/2330 train_time:6971ms step_avg:37.28ms
step:188/2330 train_time:7011ms step_avg:37.30ms
step:189/2330 train_time:7046ms step_avg:37.28ms
step:190/2330 train_time:7086ms step_avg:37.29ms
step:191/2330 train_time:7121ms step_avg:37.28ms
step:192/2330 train_time:7162ms step_avg:37.30ms
step:193/2330 train_time:7197ms step_avg:37.29ms
step:194/2330 train_time:7237ms step_avg:37.30ms
step:195/2330 train_time:7271ms step_avg:37.29ms
step:196/2330 train_time:7312ms step_avg:37.31ms
step:197/2330 train_time:7347ms step_avg:37.29ms
step:198/2330 train_time:7387ms step_avg:37.31ms
step:199/2330 train_time:7422ms step_avg:37.30ms
step:200/2330 train_time:7462ms step_avg:37.31ms
step:201/2330 train_time:7497ms step_avg:37.30ms
step:202/2330 train_time:7538ms step_avg:37.32ms
step:203/2330 train_time:7573ms step_avg:37.30ms
step:204/2330 train_time:7613ms step_avg:37.32ms
step:205/2330 train_time:7648ms step_avg:37.31ms
step:206/2330 train_time:7689ms step_avg:37.32ms
step:207/2330 train_time:7723ms step_avg:37.31ms
step:208/2330 train_time:7764ms step_avg:37.33ms
step:209/2330 train_time:7799ms step_avg:37.31ms
step:210/2330 train_time:7839ms step_avg:37.33ms
step:211/2330 train_time:7874ms step_avg:37.32ms
step:212/2330 train_time:7914ms step_avg:37.33ms
step:213/2330 train_time:7949ms step_avg:37.32ms
step:214/2330 train_time:7990ms step_avg:37.33ms
step:215/2330 train_time:8024ms step_avg:37.32ms
step:216/2330 train_time:8065ms step_avg:37.34ms
step:217/2330 train_time:8099ms step_avg:37.32ms
step:218/2330 train_time:8140ms step_avg:37.34ms
step:219/2330 train_time:8174ms step_avg:37.33ms
step:220/2330 train_time:8215ms step_avg:37.34ms
step:221/2330 train_time:8249ms step_avg:37.33ms
step:222/2330 train_time:8290ms step_avg:37.34ms
step:223/2330 train_time:8324ms step_avg:37.33ms
step:224/2330 train_time:8365ms step_avg:37.34ms
step:225/2330 train_time:8400ms step_avg:37.33ms
step:226/2330 train_time:8441ms step_avg:37.35ms
step:227/2330 train_time:8475ms step_avg:37.33ms
step:228/2330 train_time:8515ms step_avg:37.35ms
step:229/2330 train_time:8550ms step_avg:37.34ms
step:230/2330 train_time:8591ms step_avg:37.35ms
step:231/2330 train_time:8625ms step_avg:37.34ms
step:232/2330 train_time:8666ms step_avg:37.35ms
step:233/2330 train_time:8701ms step_avg:37.34ms
step:234/2330 train_time:8742ms step_avg:37.36ms
step:235/2330 train_time:8777ms step_avg:37.35ms
step:236/2330 train_time:8817ms step_avg:37.36ms
step:237/2330 train_time:8852ms step_avg:37.35ms
step:238/2330 train_time:8892ms step_avg:37.36ms
step:239/2330 train_time:8927ms step_avg:37.35ms
step:240/2330 train_time:8967ms step_avg:37.36ms
step:241/2330 train_time:9002ms step_avg:37.35ms
step:242/2330 train_time:9043ms step_avg:37.37ms
step:243/2330 train_time:9077ms step_avg:37.36ms
step:244/2330 train_time:9117ms step_avg:37.37ms
step:245/2330 train_time:9153ms step_avg:37.36ms
step:246/2330 train_time:9193ms step_avg:37.37ms
step:247/2330 train_time:9228ms step_avg:37.36ms
step:248/2330 train_time:9268ms step_avg:37.37ms
step:249/2330 train_time:9303ms step_avg:37.36ms
step:250/2330 train_time:9344ms step_avg:37.38ms
step:250/2330 val_loss:5.9428 train_time:9454ms step_avg:37.82ms
step:251/2330 train_time:9465ms step_avg:37.71ms
step:252/2330 train_time:9476ms step_avg:37.60ms
step:253/2330 train_time:9485ms step_avg:37.49ms
step:254/2330 train_time:9496ms step_avg:37.38ms
step:255/2330 train_time:9530ms step_avg:37.37ms
step:256/2330 train_time:9570ms step_avg:37.38ms
step:257/2330 train_time:9604ms step_avg:37.37ms
step:258/2330 train_time:9645ms step_avg:37.38ms
step:259/2330 train_time:9678ms step_avg:37.37ms
step:260/2330 train_time:9719ms step_avg:37.38ms
step:261/2330 train_time:9753ms step_avg:37.37ms
step:262/2330 train_time:9793ms step_avg:37.38ms
step:263/2330 train_time:9831ms step_avg:37.38ms
step:264/2330 train_time:9872ms step_avg:37.39ms
step:265/2330 train_time:9907ms step_avg:37.38ms
step:266/2330 train_time:9947ms step_avg:37.39ms
step:267/2330 train_time:9982ms step_avg:37.39ms
step:268/2330 train_time:10023ms step_avg:37.40ms
step:269/2330 train_time:10058ms step_avg:37.39ms
step:270/2330 train_time:10098ms step_avg:37.40ms
step:271/2330 train_time:10133ms step_avg:37.39ms
step:272/2330 train_time:10173ms step_avg:37.40ms
step:273/2330 train_time:10207ms step_avg:37.39ms
step:274/2330 train_time:10247ms step_avg:37.40ms
step:275/2330 train_time:10283ms step_avg:37.39ms
step:276/2330 train_time:10323ms step_avg:37.40ms
step:277/2330 train_time:10359ms step_avg:37.40ms
step:278/2330 train_time:10400ms step_avg:37.41ms
step:279/2330 train_time:10436ms step_avg:37.40ms
step:280/2330 train_time:10477ms step_avg:37.42ms
step:281/2330 train_time:10512ms step_avg:37.41ms
step:282/2330 train_time:10552ms step_avg:37.42ms
step:283/2330 train_time:10586ms step_avg:37.41ms
step:284/2330 train_time:10627ms step_avg:37.42ms
step:285/2330 train_time:10661ms step_avg:37.41ms
step:286/2330 train_time:10702ms step_avg:37.42ms
step:287/2330 train_time:10737ms step_avg:37.41ms
step:288/2330 train_time:10777ms step_avg:37.42ms
step:289/2330 train_time:10813ms step_avg:37.41ms
step:290/2330 train_time:10853ms step_avg:37.42ms
step:291/2330 train_time:10888ms step_avg:37.42ms
step:292/2330 train_time:10929ms step_avg:37.43ms
step:293/2330 train_time:10965ms step_avg:37.42ms
step:294/2330 train_time:11005ms step_avg:37.43ms
step:295/2330 train_time:11040ms step_avg:37.43ms
step:296/2330 train_time:11081ms step_avg:37.43ms
step:297/2330 train_time:11116ms step_avg:37.43ms
step:298/2330 train_time:11156ms step_avg:37.44ms
step:299/2330 train_time:11191ms step_avg:37.43ms
step:300/2330 train_time:11231ms step_avg:37.44ms
step:301/2330 train_time:11266ms step_avg:37.43ms
step:302/2330 train_time:11307ms step_avg:37.44ms
step:303/2330 train_time:11341ms step_avg:37.43ms
step:304/2330 train_time:11382ms step_avg:37.44ms
step:305/2330 train_time:11417ms step_avg:37.43ms
step:306/2330 train_time:11458ms step_avg:37.44ms
step:307/2330 train_time:11492ms step_avg:37.43ms
step:308/2330 train_time:11533ms step_avg:37.44ms
step:309/2330 train_time:11568ms step_avg:37.44ms
step:310/2330 train_time:11608ms step_avg:37.45ms
step:311/2330 train_time:11643ms step_avg:37.44ms
step:312/2330 train_time:11683ms step_avg:37.45ms
step:313/2330 train_time:11719ms step_avg:37.44ms
step:314/2330 train_time:11760ms step_avg:37.45ms
step:315/2330 train_time:11794ms step_avg:37.44ms
step:316/2330 train_time:11835ms step_avg:37.45ms
step:317/2330 train_time:11870ms step_avg:37.44ms
step:318/2330 train_time:11910ms step_avg:37.45ms
step:319/2330 train_time:11945ms step_avg:37.44ms
step:320/2330 train_time:11985ms step_avg:37.45ms
step:321/2330 train_time:12020ms step_avg:37.45ms
step:322/2330 train_time:12061ms step_avg:37.46ms
step:323/2330 train_time:12097ms step_avg:37.45ms
step:324/2330 train_time:12138ms step_avg:37.46ms
step:325/2330 train_time:12173ms step_avg:37.46ms
step:326/2330 train_time:12214ms step_avg:37.47ms
step:327/2330 train_time:12248ms step_avg:37.46ms
step:328/2330 train_time:12289ms step_avg:37.47ms
step:329/2330 train_time:12323ms step_avg:37.46ms
step:330/2330 train_time:12364ms step_avg:37.47ms
step:331/2330 train_time:12400ms step_avg:37.46ms
step:332/2330 train_time:12440ms step_avg:37.47ms
step:333/2330 train_time:12475ms step_avg:37.46ms
step:334/2330 train_time:12516ms step_avg:37.47ms
step:335/2330 train_time:12550ms step_avg:37.46ms
step:336/2330 train_time:12591ms step_avg:37.47ms
step:337/2330 train_time:12626ms step_avg:37.47ms
step:338/2330 train_time:12667ms step_avg:37.48ms
step:339/2330 train_time:12702ms step_avg:37.47ms
step:340/2330 train_time:12743ms step_avg:37.48ms
step:341/2330 train_time:12778ms step_avg:37.47ms
step:342/2330 train_time:12819ms step_avg:37.48ms
step:343/2330 train_time:12854ms step_avg:37.47ms
step:344/2330 train_time:12894ms step_avg:37.48ms
step:345/2330 train_time:12929ms step_avg:37.47ms
step:346/2330 train_time:12970ms step_avg:37.48ms
step:347/2330 train_time:13004ms step_avg:37.48ms
step:348/2330 train_time:13045ms step_avg:37.48ms
step:349/2330 train_time:13080ms step_avg:37.48ms
step:350/2330 train_time:13121ms step_avg:37.49ms
step:351/2330 train_time:13156ms step_avg:37.48ms
step:352/2330 train_time:13197ms step_avg:37.49ms
step:353/2330 train_time:13232ms step_avg:37.48ms
step:354/2330 train_time:13272ms step_avg:37.49ms
step:355/2330 train_time:13307ms step_avg:37.48ms
step:356/2330 train_time:13348ms step_avg:37.49ms
step:357/2330 train_time:13383ms step_avg:37.49ms
step:358/2330 train_time:13423ms step_avg:37.49ms
step:359/2330 train_time:13458ms step_avg:37.49ms
step:360/2330 train_time:13499ms step_avg:37.50ms
step:361/2330 train_time:13534ms step_avg:37.49ms
step:362/2330 train_time:13574ms step_avg:37.50ms
step:363/2330 train_time:13609ms step_avg:37.49ms
step:364/2330 train_time:13650ms step_avg:37.50ms
step:365/2330 train_time:13685ms step_avg:37.49ms
step:366/2330 train_time:13726ms step_avg:37.50ms
step:367/2330 train_time:13761ms step_avg:37.49ms
step:368/2330 train_time:13801ms step_avg:37.50ms
step:369/2330 train_time:13836ms step_avg:37.50ms
step:370/2330 train_time:13877ms step_avg:37.51ms
step:371/2330 train_time:13912ms step_avg:37.50ms
step:372/2330 train_time:13953ms step_avg:37.51ms
step:373/2330 train_time:13987ms step_avg:37.50ms
step:374/2330 train_time:14029ms step_avg:37.51ms
step:375/2330 train_time:14063ms step_avg:37.50ms
step:376/2330 train_time:14104ms step_avg:37.51ms
step:377/2330 train_time:14139ms step_avg:37.50ms
step:378/2330 train_time:14180ms step_avg:37.51ms
step:379/2330 train_time:14215ms step_avg:37.51ms
step:380/2330 train_time:14256ms step_avg:37.52ms
step:381/2330 train_time:14291ms step_avg:37.51ms
step:382/2330 train_time:14332ms step_avg:37.52ms
step:383/2330 train_time:14366ms step_avg:37.51ms
step:384/2330 train_time:14407ms step_avg:37.52ms
step:385/2330 train_time:14442ms step_avg:37.51ms
step:386/2330 train_time:14482ms step_avg:37.52ms
step:387/2330 train_time:14517ms step_avg:37.51ms
step:388/2330 train_time:14558ms step_avg:37.52ms
step:389/2330 train_time:14592ms step_avg:37.51ms
step:390/2330 train_time:14633ms step_avg:37.52ms
step:391/2330 train_time:14668ms step_avg:37.51ms
step:392/2330 train_time:14709ms step_avg:37.52ms
step:393/2330 train_time:14744ms step_avg:37.52ms
step:394/2330 train_time:14785ms step_avg:37.52ms
step:395/2330 train_time:14820ms step_avg:37.52ms
step:396/2330 train_time:14861ms step_avg:37.53ms
step:397/2330 train_time:14896ms step_avg:37.52ms
step:398/2330 train_time:14937ms step_avg:37.53ms
step:399/2330 train_time:14972ms step_avg:37.52ms
step:400/2330 train_time:15012ms step_avg:37.53ms
step:401/2330 train_time:15047ms step_avg:37.52ms
step:402/2330 train_time:15088ms step_avg:37.53ms
step:403/2330 train_time:15123ms step_avg:37.53ms
step:404/2330 train_time:15163ms step_avg:37.53ms
step:405/2330 train_time:15200ms step_avg:37.53ms
step:406/2330 train_time:15241ms step_avg:37.54ms
step:407/2330 train_time:15277ms step_avg:37.53ms
step:408/2330 train_time:15318ms step_avg:37.54ms
step:409/2330 train_time:15353ms step_avg:37.54ms
step:410/2330 train_time:15393ms step_avg:37.54ms
step:411/2330 train_time:15428ms step_avg:37.54ms
step:412/2330 train_time:15469ms step_avg:37.55ms
step:413/2330 train_time:15504ms step_avg:37.54ms
step:414/2330 train_time:15544ms step_avg:37.55ms
step:415/2330 train_time:15579ms step_avg:37.54ms
step:416/2330 train_time:15620ms step_avg:37.55ms
step:417/2330 train_time:15655ms step_avg:37.54ms
step:418/2330 train_time:15696ms step_avg:37.55ms
step:419/2330 train_time:15731ms step_avg:37.54ms
step:420/2330 train_time:15772ms step_avg:37.55ms
step:421/2330 train_time:15806ms step_avg:37.54ms
step:422/2330 train_time:15847ms step_avg:37.55ms
step:423/2330 train_time:15882ms step_avg:37.55ms
step:424/2330 train_time:15922ms step_avg:37.55ms
step:425/2330 train_time:15957ms step_avg:37.55ms
step:426/2330 train_time:15998ms step_avg:37.55ms
step:427/2330 train_time:16033ms step_avg:37.55ms
step:428/2330 train_time:16073ms step_avg:37.55ms
step:429/2330 train_time:16109ms step_avg:37.55ms
step:430/2330 train_time:16149ms step_avg:37.56ms
step:431/2330 train_time:16184ms step_avg:37.55ms
step:432/2330 train_time:16225ms step_avg:37.56ms
step:433/2330 train_time:16260ms step_avg:37.55ms
step:434/2330 train_time:16301ms step_avg:37.56ms
step:435/2330 train_time:16337ms step_avg:37.56ms
step:436/2330 train_time:16378ms step_avg:37.56ms
step:437/2330 train_time:16413ms step_avg:37.56ms
step:438/2330 train_time:16454ms step_avg:37.57ms
step:439/2330 train_time:16490ms step_avg:37.56ms
step:440/2330 train_time:16530ms step_avg:37.57ms
step:441/2330 train_time:16565ms step_avg:37.56ms
step:442/2330 train_time:16606ms step_avg:37.57ms
step:443/2330 train_time:16641ms step_avg:37.56ms
step:444/2330 train_time:16681ms step_avg:37.57ms
step:445/2330 train_time:16717ms step_avg:37.57ms
step:446/2330 train_time:16758ms step_avg:37.57ms
step:447/2330 train_time:16792ms step_avg:37.57ms
step:448/2330 train_time:16832ms step_avg:37.57ms
step:449/2330 train_time:16867ms step_avg:37.57ms
step:450/2330 train_time:16909ms step_avg:37.58ms
step:451/2330 train_time:16943ms step_avg:37.57ms
step:452/2330 train_time:16984ms step_avg:37.58ms
step:453/2330 train_time:17019ms step_avg:37.57ms
step:454/2330 train_time:17060ms step_avg:37.58ms
step:455/2330 train_time:17095ms step_avg:37.57ms
step:456/2330 train_time:17136ms step_avg:37.58ms
step:457/2330 train_time:17171ms step_avg:37.57ms
step:458/2330 train_time:17212ms step_avg:37.58ms
step:459/2330 train_time:17247ms step_avg:37.57ms
step:460/2330 train_time:17288ms step_avg:37.58ms
step:461/2330 train_time:17323ms step_avg:37.58ms
step:462/2330 train_time:17364ms step_avg:37.58ms
step:463/2330 train_time:17399ms step_avg:37.58ms
step:464/2330 train_time:17439ms step_avg:37.58ms
step:465/2330 train_time:17475ms step_avg:37.58ms
step:466/2330 train_time:17515ms step_avg:37.59ms
step:467/2330 train_time:17551ms step_avg:37.58ms
step:468/2330 train_time:17592ms step_avg:37.59ms
step:469/2330 train_time:17626ms step_avg:37.58ms
step:470/2330 train_time:17667ms step_avg:37.59ms
step:471/2330 train_time:17703ms step_avg:37.59ms
step:472/2330 train_time:17743ms step_avg:37.59ms
step:473/2330 train_time:17778ms step_avg:37.59ms
step:474/2330 train_time:17820ms step_avg:37.59ms
step:475/2330 train_time:17855ms step_avg:37.59ms
step:476/2330 train_time:17896ms step_avg:37.60ms
step:477/2330 train_time:17931ms step_avg:37.59ms
step:478/2330 train_time:17971ms step_avg:37.60ms
step:479/2330 train_time:18006ms step_avg:37.59ms
step:480/2330 train_time:18047ms step_avg:37.60ms
step:481/2330 train_time:18082ms step_avg:37.59ms
step:482/2330 train_time:18123ms step_avg:37.60ms
step:483/2330 train_time:18158ms step_avg:37.59ms
step:484/2330 train_time:18199ms step_avg:37.60ms
step:485/2330 train_time:18234ms step_avg:37.59ms
step:486/2330 train_time:18274ms step_avg:37.60ms
step:487/2330 train_time:18309ms step_avg:37.60ms
step:488/2330 train_time:18351ms step_avg:37.60ms
step:489/2330 train_time:18385ms step_avg:37.60ms
step:490/2330 train_time:18426ms step_avg:37.60ms
step:491/2330 train_time:18461ms step_avg:37.60ms
step:492/2330 train_time:18502ms step_avg:37.61ms
step:493/2330 train_time:18538ms step_avg:37.60ms
step:494/2330 train_time:18579ms step_avg:37.61ms
step:495/2330 train_time:18615ms step_avg:37.61ms
step:496/2330 train_time:18655ms step_avg:37.61ms
step:497/2330 train_time:18691ms step_avg:37.61ms
step:498/2330 train_time:18731ms step_avg:37.61ms
step:499/2330 train_time:18766ms step_avg:37.61ms
step:500/2330 train_time:18807ms step_avg:37.61ms
step:500/2330 val_loss:5.6743 train_time:18918ms step_avg:37.84ms
step:501/2330 train_time:18930ms step_avg:37.78ms
step:502/2330 train_time:18941ms step_avg:37.73ms
step:503/2330 train_time:18950ms step_avg:37.67ms
step:504/2330 train_time:18961ms step_avg:37.62ms
step:505/2330 train_time:18995ms step_avg:37.61ms
step:506/2330 train_time:19035ms step_avg:37.62ms
step:507/2330 train_time:19069ms step_avg:37.61ms
step:508/2330 train_time:19110ms step_avg:37.62ms
step:509/2330 train_time:19145ms step_avg:37.61ms
step:510/2330 train_time:19185ms step_avg:37.62ms
step:511/2330 train_time:19220ms step_avg:37.61ms
step:512/2330 train_time:19262ms step_avg:37.62ms
step:513/2330 train_time:19304ms step_avg:37.63ms
step:514/2330 train_time:19344ms step_avg:37.63ms
step:515/2330 train_time:19380ms step_avg:37.63ms
step:516/2330 train_time:19421ms step_avg:37.64ms
step:517/2330 train_time:19456ms step_avg:37.63ms
step:518/2330 train_time:19496ms step_avg:37.64ms
step:519/2330 train_time:19532ms step_avg:37.63ms
step:520/2330 train_time:19572ms step_avg:37.64ms
step:521/2330 train_time:19607ms step_avg:37.63ms
step:522/2330 train_time:19648ms step_avg:37.64ms
step:523/2330 train_time:19683ms step_avg:37.63ms
step:524/2330 train_time:19723ms step_avg:37.64ms
step:525/2330 train_time:19758ms step_avg:37.63ms
step:526/2330 train_time:19799ms step_avg:37.64ms
step:527/2330 train_time:19834ms step_avg:37.64ms
step:528/2330 train_time:19875ms step_avg:37.64ms
step:529/2330 train_time:19910ms step_avg:37.64ms
step:530/2330 train_time:19952ms step_avg:37.64ms
step:531/2330 train_time:19986ms step_avg:37.64ms
step:532/2330 train_time:20027ms step_avg:37.64ms
step:533/2330 train_time:20061ms step_avg:37.64ms
step:534/2330 train_time:20102ms step_avg:37.64ms
step:535/2330 train_time:20137ms step_avg:37.64ms
step:536/2330 train_time:20177ms step_avg:37.64ms
step:537/2330 train_time:20213ms step_avg:37.64ms
step:538/2330 train_time:20253ms step_avg:37.65ms
step:539/2330 train_time:20290ms step_avg:37.64ms
step:540/2330 train_time:20330ms step_avg:37.65ms
step:541/2330 train_time:20367ms step_avg:37.65ms
step:542/2330 train_time:20408ms step_avg:37.65ms
step:543/2330 train_time:20444ms step_avg:37.65ms
step:544/2330 train_time:20485ms step_avg:37.66ms
step:545/2330 train_time:20521ms step_avg:37.65ms
step:546/2330 train_time:20561ms step_avg:37.66ms
step:547/2330 train_time:20596ms step_avg:37.65ms
step:548/2330 train_time:20636ms step_avg:37.66ms
step:549/2330 train_time:20671ms step_avg:37.65ms
step:550/2330 train_time:20712ms step_avg:37.66ms
step:551/2330 train_time:20748ms step_avg:37.65ms
step:552/2330 train_time:20788ms step_avg:37.66ms
step:553/2330 train_time:20824ms step_avg:37.66ms
step:554/2330 train_time:20865ms step_avg:37.66ms
step:555/2330 train_time:20900ms step_avg:37.66ms
step:556/2330 train_time:20941ms step_avg:37.66ms
step:557/2330 train_time:20975ms step_avg:37.66ms
step:558/2330 train_time:21016ms step_avg:37.66ms
step:559/2330 train_time:21050ms step_avg:37.66ms
step:560/2330 train_time:21092ms step_avg:37.66ms
step:561/2330 train_time:21126ms step_avg:37.66ms
step:562/2330 train_time:21167ms step_avg:37.66ms
step:563/2330 train_time:21203ms step_avg:37.66ms
step:564/2330 train_time:21244ms step_avg:37.67ms
step:565/2330 train_time:21279ms step_avg:37.66ms
step:566/2330 train_time:21320ms step_avg:37.67ms
step:567/2330 train_time:21355ms step_avg:37.66ms
step:568/2330 train_time:21396ms step_avg:37.67ms
step:569/2330 train_time:21431ms step_avg:37.66ms
step:570/2330 train_time:21472ms step_avg:37.67ms
step:571/2330 train_time:21507ms step_avg:37.67ms
step:572/2330 train_time:21548ms step_avg:37.67ms
step:573/2330 train_time:21583ms step_avg:37.67ms
step:574/2330 train_time:21624ms step_avg:37.67ms
step:575/2330 train_time:21659ms step_avg:37.67ms
step:576/2330 train_time:21700ms step_avg:37.67ms
step:577/2330 train_time:21735ms step_avg:37.67ms
step:578/2330 train_time:21775ms step_avg:37.67ms
step:579/2330 train_time:21811ms step_avg:37.67ms
step:580/2330 train_time:21852ms step_avg:37.68ms
step:581/2330 train_time:21887ms step_avg:37.67ms
step:582/2330 train_time:21927ms step_avg:37.68ms
step:583/2330 train_time:21963ms step_avg:37.67ms
step:584/2330 train_time:22003ms step_avg:37.68ms
step:585/2330 train_time:22038ms step_avg:37.67ms
step:586/2330 train_time:22079ms step_avg:37.68ms
step:587/2330 train_time:22114ms step_avg:37.67ms
step:588/2330 train_time:22154ms step_avg:37.68ms
step:589/2330 train_time:22190ms step_avg:37.67ms
step:590/2330 train_time:22231ms step_avg:37.68ms
step:591/2330 train_time:22266ms step_avg:37.67ms
step:592/2330 train_time:22307ms step_avg:37.68ms
step:593/2330 train_time:22342ms step_avg:37.68ms
step:594/2330 train_time:22383ms step_avg:37.68ms
step:595/2330 train_time:22419ms step_avg:37.68ms
step:596/2330 train_time:22460ms step_avg:37.68ms
step:597/2330 train_time:22495ms step_avg:37.68ms
step:598/2330 train_time:22535ms step_avg:37.68ms
step:599/2330 train_time:22570ms step_avg:37.68ms
step:600/2330 train_time:22611ms step_avg:37.69ms
step:601/2330 train_time:22646ms step_avg:37.68ms
step:602/2330 train_time:22687ms step_avg:37.69ms
step:603/2330 train_time:22722ms step_avg:37.68ms
step:604/2330 train_time:22763ms step_avg:37.69ms
step:605/2330 train_time:22799ms step_avg:37.68ms
step:606/2330 train_time:22839ms step_avg:37.69ms
step:607/2330 train_time:22874ms step_avg:37.68ms
step:608/2330 train_time:22915ms step_avg:37.69ms
step:609/2330 train_time:22950ms step_avg:37.68ms
step:610/2330 train_time:22991ms step_avg:37.69ms
step:611/2330 train_time:23026ms step_avg:37.69ms
step:612/2330 train_time:23067ms step_avg:37.69ms
step:613/2330 train_time:23102ms step_avg:37.69ms
step:614/2330 train_time:23143ms step_avg:37.69ms
step:615/2330 train_time:23179ms step_avg:37.69ms
step:616/2330 train_time:23220ms step_avg:37.69ms
step:617/2330 train_time:23256ms step_avg:37.69ms
step:618/2330 train_time:23296ms step_avg:37.70ms
step:619/2330 train_time:23331ms step_avg:37.69ms
step:620/2330 train_time:23373ms step_avg:37.70ms
step:621/2330 train_time:23408ms step_avg:37.69ms
step:622/2330 train_time:23449ms step_avg:37.70ms
step:623/2330 train_time:23485ms step_avg:37.70ms
step:624/2330 train_time:23525ms step_avg:37.70ms
step:625/2330 train_time:23561ms step_avg:37.70ms
step:626/2330 train_time:23602ms step_avg:37.70ms
step:627/2330 train_time:23637ms step_avg:37.70ms
step:628/2330 train_time:23678ms step_avg:37.70ms
step:629/2330 train_time:23713ms step_avg:37.70ms
step:630/2330 train_time:23754ms step_avg:37.70ms
step:631/2330 train_time:23789ms step_avg:37.70ms
step:632/2330 train_time:23830ms step_avg:37.71ms
step:633/2330 train_time:23865ms step_avg:37.70ms
step:634/2330 train_time:23906ms step_avg:37.71ms
step:635/2330 train_time:23941ms step_avg:37.70ms
step:636/2330 train_time:23982ms step_avg:37.71ms
step:637/2330 train_time:24017ms step_avg:37.70ms
step:638/2330 train_time:24057ms step_avg:37.71ms
step:639/2330 train_time:24092ms step_avg:37.70ms
step:640/2330 train_time:24133ms step_avg:37.71ms
step:641/2330 train_time:24168ms step_avg:37.70ms
step:642/2330 train_time:24209ms step_avg:37.71ms
step:643/2330 train_time:24244ms step_avg:37.70ms
step:644/2330 train_time:24285ms step_avg:37.71ms
step:645/2330 train_time:24320ms step_avg:37.71ms
step:646/2330 train_time:24361ms step_avg:37.71ms
step:647/2330 train_time:24397ms step_avg:37.71ms
step:648/2330 train_time:24437ms step_avg:37.71ms
step:649/2330 train_time:24473ms step_avg:37.71ms
step:650/2330 train_time:24514ms step_avg:37.71ms
step:651/2330 train_time:24548ms step_avg:37.71ms
step:652/2330 train_time:24589ms step_avg:37.71ms
step:653/2330 train_time:24624ms step_avg:37.71ms
step:654/2330 train_time:24666ms step_avg:37.72ms
step:655/2330 train_time:24701ms step_avg:37.71ms
step:656/2330 train_time:24743ms step_avg:37.72ms
step:657/2330 train_time:24778ms step_avg:37.71ms
step:658/2330 train_time:24819ms step_avg:37.72ms
step:659/2330 train_time:24854ms step_avg:37.71ms
step:660/2330 train_time:24894ms step_avg:37.72ms
step:661/2330 train_time:24930ms step_avg:37.72ms
step:662/2330 train_time:24971ms step_avg:37.72ms
step:663/2330 train_time:25006ms step_avg:37.72ms
step:664/2330 train_time:25047ms step_avg:37.72ms
step:665/2330 train_time:25082ms step_avg:37.72ms
step:666/2330 train_time:25123ms step_avg:37.72ms
step:667/2330 train_time:25159ms step_avg:37.72ms
step:668/2330 train_time:25199ms step_avg:37.72ms
step:669/2330 train_time:25235ms step_avg:37.72ms
step:670/2330 train_time:25275ms step_avg:37.72ms
step:671/2330 train_time:25310ms step_avg:37.72ms
step:672/2330 train_time:25351ms step_avg:37.72ms
step:673/2330 train_time:25387ms step_avg:37.72ms
step:674/2330 train_time:25428ms step_avg:37.73ms
step:675/2330 train_time:25463ms step_avg:37.72ms
step:676/2330 train_time:25504ms step_avg:37.73ms
step:677/2330 train_time:25540ms step_avg:37.72ms
step:678/2330 train_time:25580ms step_avg:37.73ms
step:679/2330 train_time:25616ms step_avg:37.73ms
step:680/2330 train_time:25657ms step_avg:37.73ms
step:681/2330 train_time:25692ms step_avg:37.73ms
step:682/2330 train_time:25733ms step_avg:37.73ms
step:683/2330 train_time:25768ms step_avg:37.73ms
step:684/2330 train_time:25809ms step_avg:37.73ms
step:685/2330 train_time:25844ms step_avg:37.73ms
step:686/2330 train_time:25885ms step_avg:37.73ms
step:687/2330 train_time:25920ms step_avg:37.73ms
step:688/2330 train_time:25961ms step_avg:37.73ms
step:689/2330 train_time:25996ms step_avg:37.73ms
step:690/2330 train_time:26036ms step_avg:37.73ms
step:691/2330 train_time:26072ms step_avg:37.73ms
step:692/2330 train_time:26113ms step_avg:37.74ms
step:693/2330 train_time:26148ms step_avg:37.73ms
step:694/2330 train_time:26189ms step_avg:37.74ms
step:695/2330 train_time:26225ms step_avg:37.73ms
step:696/2330 train_time:26266ms step_avg:37.74ms
step:697/2330 train_time:26302ms step_avg:37.74ms
step:698/2330 train_time:26342ms step_avg:37.74ms
step:699/2330 train_time:26378ms step_avg:37.74ms
step:700/2330 train_time:26419ms step_avg:37.74ms
step:701/2330 train_time:26454ms step_avg:37.74ms
step:702/2330 train_time:26495ms step_avg:37.74ms
step:703/2330 train_time:26530ms step_avg:37.74ms
step:704/2330 train_time:26572ms step_avg:37.74ms
step:705/2330 train_time:26606ms step_avg:37.74ms
step:706/2330 train_time:26647ms step_avg:37.74ms
step:707/2330 train_time:26683ms step_avg:37.74ms
step:708/2330 train_time:26724ms step_avg:37.75ms
step:709/2330 train_time:26759ms step_avg:37.74ms
step:710/2330 train_time:26800ms step_avg:37.75ms
step:711/2330 train_time:26834ms step_avg:37.74ms
step:712/2330 train_time:26875ms step_avg:37.75ms
step:713/2330 train_time:26909ms step_avg:37.74ms
step:714/2330 train_time:26950ms step_avg:37.75ms
step:715/2330 train_time:26986ms step_avg:37.74ms
step:716/2330 train_time:27027ms step_avg:37.75ms
step:717/2330 train_time:27062ms step_avg:37.74ms
step:718/2330 train_time:27103ms step_avg:37.75ms
step:719/2330 train_time:27138ms step_avg:37.74ms
step:720/2330 train_time:27179ms step_avg:37.75ms
step:721/2330 train_time:27214ms step_avg:37.74ms
step:722/2330 train_time:27254ms step_avg:37.75ms
step:723/2330 train_time:27290ms step_avg:37.75ms
step:724/2330 train_time:27330ms step_avg:37.75ms
step:725/2330 train_time:27366ms step_avg:37.75ms
step:726/2330 train_time:27407ms step_avg:37.75ms
step:727/2330 train_time:27443ms step_avg:37.75ms
step:728/2330 train_time:27484ms step_avg:37.75ms
step:729/2330 train_time:27519ms step_avg:37.75ms
step:730/2330 train_time:27560ms step_avg:37.75ms
step:731/2330 train_time:27596ms step_avg:37.75ms
step:732/2330 train_time:27636ms step_avg:37.75ms
step:733/2330 train_time:27672ms step_avg:37.75ms
step:734/2330 train_time:27713ms step_avg:37.76ms
step:735/2330 train_time:27747ms step_avg:37.75ms
step:736/2330 train_time:27788ms step_avg:37.76ms
step:737/2330 train_time:27824ms step_avg:37.75ms
step:738/2330 train_time:27865ms step_avg:37.76ms
step:739/2330 train_time:27900ms step_avg:37.75ms
step:740/2330 train_time:27941ms step_avg:37.76ms
step:741/2330 train_time:27976ms step_avg:37.75ms
step:742/2330 train_time:28016ms step_avg:37.76ms
step:743/2330 train_time:28051ms step_avg:37.75ms
step:744/2330 train_time:28092ms step_avg:37.76ms
step:745/2330 train_time:28127ms step_avg:37.75ms
step:746/2330 train_time:28168ms step_avg:37.76ms
step:747/2330 train_time:28203ms step_avg:37.76ms
step:748/2330 train_time:28244ms step_avg:37.76ms
step:749/2330 train_time:28279ms step_avg:37.76ms
step:750/2330 train_time:28319ms step_avg:37.76ms
step:750/2330 val_loss:5.5614 train_time:28431ms step_avg:37.91ms
step:751/2330 train_time:28443ms step_avg:37.87ms
step:752/2330 train_time:28454ms step_avg:37.84ms
step:753/2330 train_time:28463ms step_avg:37.80ms
step:754/2330 train_time:28473ms step_avg:37.76ms
step:755/2330 train_time:28510ms step_avg:37.76ms
step:756/2330 train_time:28551ms step_avg:37.77ms
step:757/2330 train_time:28585ms step_avg:37.76ms
step:758/2330 train_time:28626ms step_avg:37.76ms
step:759/2330 train_time:28660ms step_avg:37.76ms
step:760/2330 train_time:28700ms step_avg:37.76ms
step:761/2330 train_time:28736ms step_avg:37.76ms
step:762/2330 train_time:28776ms step_avg:37.76ms
step:763/2330 train_time:28814ms step_avg:37.76ms
step:764/2330 train_time:28854ms step_avg:37.77ms
step:765/2330 train_time:28890ms step_avg:37.77ms
step:766/2330 train_time:28931ms step_avg:37.77ms
step:767/2330 train_time:28966ms step_avg:37.77ms
step:768/2330 train_time:29007ms step_avg:37.77ms
step:769/2330 train_time:29041ms step_avg:37.76ms
step:770/2330 train_time:29081ms step_avg:37.77ms
step:771/2330 train_time:29116ms step_avg:37.76ms
step:772/2330 train_time:29157ms step_avg:37.77ms
step:773/2330 train_time:29191ms step_avg:37.76ms
step:774/2330 train_time:29232ms step_avg:37.77ms
step:775/2330 train_time:29267ms step_avg:37.76ms
step:776/2330 train_time:29308ms step_avg:37.77ms
step:777/2330 train_time:29343ms step_avg:37.76ms
step:778/2330 train_time:29385ms step_avg:37.77ms
step:779/2330 train_time:29421ms step_avg:37.77ms
step:780/2330 train_time:29462ms step_avg:37.77ms
step:781/2330 train_time:29497ms step_avg:37.77ms
step:782/2330 train_time:29538ms step_avg:37.77ms
step:783/2330 train_time:29572ms step_avg:37.77ms
step:784/2330 train_time:29612ms step_avg:37.77ms
step:785/2330 train_time:29647ms step_avg:37.77ms
step:786/2330 train_time:29688ms step_avg:37.77ms
step:787/2330 train_time:29724ms step_avg:37.77ms
step:788/2330 train_time:29765ms step_avg:37.77ms
step:789/2330 train_time:29801ms step_avg:37.77ms
step:790/2330 train_time:29842ms step_avg:37.77ms
step:791/2330 train_time:29878ms step_avg:37.77ms
step:792/2330 train_time:29919ms step_avg:37.78ms
step:793/2330 train_time:29954ms step_avg:37.77ms
step:794/2330 train_time:29994ms step_avg:37.78ms
step:795/2330 train_time:30029ms step_avg:37.77ms
step:796/2330 train_time:30070ms step_avg:37.78ms
step:797/2330 train_time:30105ms step_avg:37.77ms
step:798/2330 train_time:30145ms step_avg:37.78ms
step:799/2330 train_time:30180ms step_avg:37.77ms
step:800/2330 train_time:30221ms step_avg:37.78ms
step:801/2330 train_time:30255ms step_avg:37.77ms
step:802/2330 train_time:30296ms step_avg:37.78ms
step:803/2330 train_time:30331ms step_avg:37.77ms
step:804/2330 train_time:30372ms step_avg:37.78ms
step:805/2330 train_time:30408ms step_avg:37.77ms
step:806/2330 train_time:30449ms step_avg:37.78ms
step:807/2330 train_time:30485ms step_avg:37.78ms
step:808/2330 train_time:30525ms step_avg:37.78ms
step:809/2330 train_time:30561ms step_avg:37.78ms
step:810/2330 train_time:30602ms step_avg:37.78ms
step:811/2330 train_time:30637ms step_avg:37.78ms
step:812/2330 train_time:30677ms step_avg:37.78ms
step:813/2330 train_time:30712ms step_avg:37.78ms
step:814/2330 train_time:30753ms step_avg:37.78ms
step:815/2330 train_time:30787ms step_avg:37.78ms
step:816/2330 train_time:30828ms step_avg:37.78ms
step:817/2330 train_time:30864ms step_avg:37.78ms
step:818/2330 train_time:30905ms step_avg:37.78ms
step:819/2330 train_time:30941ms step_avg:37.78ms
step:820/2330 train_time:30981ms step_avg:37.78ms
step:821/2330 train_time:31017ms step_avg:37.78ms
step:822/2330 train_time:31058ms step_avg:37.78ms
step:823/2330 train_time:31092ms step_avg:37.78ms
step:824/2330 train_time:31133ms step_avg:37.78ms
step:825/2330 train_time:31168ms step_avg:37.78ms
step:826/2330 train_time:31209ms step_avg:37.78ms
step:827/2330 train_time:31245ms step_avg:37.78ms
step:828/2330 train_time:31285ms step_avg:37.78ms
step:829/2330 train_time:31321ms step_avg:37.78ms
step:830/2330 train_time:31363ms step_avg:37.79ms
step:831/2330 train_time:31397ms step_avg:37.78ms
step:832/2330 train_time:31438ms step_avg:37.79ms
step:833/2330 train_time:31473ms step_avg:37.78ms
step:834/2330 train_time:31513ms step_avg:37.79ms
step:835/2330 train_time:31549ms step_avg:37.78ms
step:836/2330 train_time:31590ms step_avg:37.79ms
step:837/2330 train_time:31625ms step_avg:37.78ms
step:838/2330 train_time:31665ms step_avg:37.79ms
step:839/2330 train_time:31701ms step_avg:37.78ms
step:840/2330 train_time:31742ms step_avg:37.79ms
step:841/2330 train_time:31778ms step_avg:37.79ms
step:842/2330 train_time:31819ms step_avg:37.79ms
step:843/2330 train_time:31854ms step_avg:37.79ms
step:844/2330 train_time:31895ms step_avg:37.79ms
step:845/2330 train_time:31930ms step_avg:37.79ms
step:846/2330 train_time:31971ms step_avg:37.79ms
step:847/2330 train_time:32005ms step_avg:37.79ms
step:848/2330 train_time:32046ms step_avg:37.79ms
step:849/2330 train_time:32081ms step_avg:37.79ms
step:850/2330 train_time:32122ms step_avg:37.79ms
step:851/2330 train_time:32158ms step_avg:37.79ms
step:852/2330 train_time:32199ms step_avg:37.79ms
step:853/2330 train_time:32234ms step_avg:37.79ms
step:854/2330 train_time:32275ms step_avg:37.79ms
step:855/2330 train_time:32310ms step_avg:37.79ms
step:856/2330 train_time:32350ms step_avg:37.79ms
step:857/2330 train_time:32387ms step_avg:37.79ms
step:858/2330 train_time:32427ms step_avg:37.79ms
step:859/2330 train_time:32463ms step_avg:37.79ms
step:860/2330 train_time:32503ms step_avg:37.79ms
step:861/2330 train_time:32539ms step_avg:37.79ms
step:862/2330 train_time:32579ms step_avg:37.79ms
step:863/2330 train_time:32614ms step_avg:37.79ms
step:864/2330 train_time:32654ms step_avg:37.79ms
step:865/2330 train_time:32690ms step_avg:37.79ms
step:866/2330 train_time:32731ms step_avg:37.80ms
step:867/2330 train_time:32767ms step_avg:37.79ms
step:868/2330 train_time:32808ms step_avg:37.80ms
step:869/2330 train_time:32843ms step_avg:37.79ms
step:870/2330 train_time:32884ms step_avg:37.80ms
step:871/2330 train_time:32920ms step_avg:37.80ms
step:872/2330 train_time:32961ms step_avg:37.80ms
step:873/2330 train_time:32996ms step_avg:37.80ms
step:874/2330 train_time:33037ms step_avg:37.80ms
step:875/2330 train_time:33072ms step_avg:37.80ms
step:876/2330 train_time:33113ms step_avg:37.80ms
step:877/2330 train_time:33149ms step_avg:37.80ms
step:878/2330 train_time:33189ms step_avg:37.80ms
step:879/2330 train_time:33225ms step_avg:37.80ms
step:880/2330 train_time:33265ms step_avg:37.80ms
step:881/2330 train_time:33301ms step_avg:37.80ms
step:882/2330 train_time:33341ms step_avg:37.80ms
step:883/2330 train_time:33378ms step_avg:37.80ms
step:884/2330 train_time:33418ms step_avg:37.80ms
step:885/2330 train_time:33454ms step_avg:37.80ms
step:886/2330 train_time:33495ms step_avg:37.80ms
step:887/2330 train_time:33530ms step_avg:37.80ms
step:888/2330 train_time:33571ms step_avg:37.80ms
step:889/2330 train_time:33605ms step_avg:37.80ms
step:890/2330 train_time:33646ms step_avg:37.80ms
step:891/2330 train_time:33682ms step_avg:37.80ms
step:892/2330 train_time:33723ms step_avg:37.81ms
step:893/2330 train_time:33758ms step_avg:37.80ms
step:894/2330 train_time:33799ms step_avg:37.81ms
step:895/2330 train_time:33833ms step_avg:37.80ms
step:896/2330 train_time:33873ms step_avg:37.81ms
step:897/2330 train_time:33909ms step_avg:37.80ms
step:898/2330 train_time:33950ms step_avg:37.81ms
step:899/2330 train_time:33985ms step_avg:37.80ms
step:900/2330 train_time:34026ms step_avg:37.81ms
step:901/2330 train_time:34061ms step_avg:37.80ms
step:902/2330 train_time:34102ms step_avg:37.81ms
step:903/2330 train_time:34138ms step_avg:37.80ms
step:904/2330 train_time:34179ms step_avg:37.81ms
step:905/2330 train_time:34214ms step_avg:37.81ms
step:906/2330 train_time:34254ms step_avg:37.81ms
step:907/2330 train_time:34290ms step_avg:37.81ms
step:908/2330 train_time:34330ms step_avg:37.81ms
step:909/2330 train_time:34365ms step_avg:37.81ms
step:910/2330 train_time:34406ms step_avg:37.81ms
step:911/2330 train_time:34441ms step_avg:37.81ms
step:912/2330 train_time:34482ms step_avg:37.81ms
step:913/2330 train_time:34518ms step_avg:37.81ms
step:914/2330 train_time:34559ms step_avg:37.81ms
step:915/2330 train_time:34594ms step_avg:37.81ms
step:916/2330 train_time:34634ms step_avg:37.81ms
step:917/2330 train_time:34669ms step_avg:37.81ms
step:918/2330 train_time:34710ms step_avg:37.81ms
step:919/2330 train_time:34746ms step_avg:37.81ms
step:920/2330 train_time:34786ms step_avg:37.81ms
step:921/2330 train_time:34822ms step_avg:37.81ms
step:922/2330 train_time:34863ms step_avg:37.81ms
step:923/2330 train_time:34898ms step_avg:37.81ms
step:924/2330 train_time:34939ms step_avg:37.81ms
step:925/2330 train_time:34974ms step_avg:37.81ms
step:926/2330 train_time:35015ms step_avg:37.81ms
step:927/2330 train_time:35050ms step_avg:37.81ms
step:928/2330 train_time:35090ms step_avg:37.81ms
step:929/2330 train_time:35125ms step_avg:37.81ms
step:930/2330 train_time:35166ms step_avg:37.81ms
step:931/2330 train_time:35202ms step_avg:37.81ms
step:932/2330 train_time:35244ms step_avg:37.81ms
step:933/2330 train_time:35278ms step_avg:37.81ms
step:934/2330 train_time:35319ms step_avg:37.82ms
step:935/2330 train_time:35354ms step_avg:37.81ms
step:936/2330 train_time:35395ms step_avg:37.81ms
step:937/2330 train_time:35430ms step_avg:37.81ms
step:938/2330 train_time:35471ms step_avg:37.82ms
step:939/2330 train_time:35506ms step_avg:37.81ms
step:940/2330 train_time:35547ms step_avg:37.82ms
step:941/2330 train_time:35583ms step_avg:37.81ms
step:942/2330 train_time:35624ms step_avg:37.82ms
step:943/2330 train_time:35660ms step_avg:37.82ms
step:944/2330 train_time:35700ms step_avg:37.82ms
step:945/2330 train_time:35735ms step_avg:37.82ms
step:946/2330 train_time:35776ms step_avg:37.82ms
step:947/2330 train_time:35811ms step_avg:37.81ms
step:948/2330 train_time:35852ms step_avg:37.82ms
step:949/2330 train_time:35887ms step_avg:37.82ms
step:950/2330 train_time:35928ms step_avg:37.82ms
step:951/2330 train_time:35963ms step_avg:37.82ms
step:952/2330 train_time:36004ms step_avg:37.82ms
step:953/2330 train_time:36039ms step_avg:37.82ms
step:954/2330 train_time:36084ms step_avg:37.82ms
step:955/2330 train_time:36115ms step_avg:37.82ms
step:956/2330 train_time:36156ms step_avg:37.82ms
step:957/2330 train_time:36191ms step_avg:37.82ms
step:958/2330 train_time:36232ms step_avg:37.82ms
step:959/2330 train_time:36266ms step_avg:37.82ms
step:960/2330 train_time:36307ms step_avg:37.82ms
step:961/2330 train_time:36342ms step_avg:37.82ms
step:962/2330 train_time:36383ms step_avg:37.82ms
step:963/2330 train_time:36419ms step_avg:37.82ms
step:964/2330 train_time:36460ms step_avg:37.82ms
step:965/2330 train_time:36495ms step_avg:37.82ms
step:966/2330 train_time:36536ms step_avg:37.82ms
step:967/2330 train_time:36571ms step_avg:37.82ms
step:968/2330 train_time:36612ms step_avg:37.82ms
step:969/2330 train_time:36646ms step_avg:37.82ms
step:970/2330 train_time:36687ms step_avg:37.82ms
step:971/2330 train_time:36722ms step_avg:37.82ms
step:972/2330 train_time:36763ms step_avg:37.82ms
step:973/2330 train_time:36798ms step_avg:37.82ms
step:974/2330 train_time:36839ms step_avg:37.82ms
step:975/2330 train_time:36874ms step_avg:37.82ms
step:976/2330 train_time:36915ms step_avg:37.82ms
step:977/2330 train_time:36950ms step_avg:37.82ms
step:978/2330 train_time:36991ms step_avg:37.82ms
step:979/2330 train_time:37026ms step_avg:37.82ms
step:980/2330 train_time:37067ms step_avg:37.82ms
step:981/2330 train_time:37102ms step_avg:37.82ms
step:982/2330 train_time:37142ms step_avg:37.82ms
step:983/2330 train_time:37178ms step_avg:37.82ms
step:984/2330 train_time:37218ms step_avg:37.82ms
step:985/2330 train_time:37253ms step_avg:37.82ms
step:986/2330 train_time:37294ms step_avg:37.82ms
step:987/2330 train_time:37329ms step_avg:37.82ms
step:988/2330 train_time:37370ms step_avg:37.82ms
step:989/2330 train_time:37405ms step_avg:37.82ms
step:990/2330 train_time:37446ms step_avg:37.82ms
step:991/2330 train_time:37481ms step_avg:37.82ms
step:992/2330 train_time:37522ms step_avg:37.82ms
step:993/2330 train_time:37557ms step_avg:37.82ms
step:994/2330 train_time:37597ms step_avg:37.82ms
step:995/2330 train_time:37632ms step_avg:37.82ms
step:996/2330 train_time:37673ms step_avg:37.82ms
step:997/2330 train_time:37709ms step_avg:37.82ms
step:998/2330 train_time:37750ms step_avg:37.83ms
step:999/2330 train_time:37785ms step_avg:37.82ms
step:1000/2330 train_time:37826ms step_avg:37.83ms
step:1000/2330 val_loss:5.4989 train_time:37939ms step_avg:37.94ms
step:1001/2330 train_time:37951ms step_avg:37.91ms
step:1002/2330 train_time:37962ms step_avg:37.89ms
step:1003/2330 train_time:37971ms step_avg:37.86ms
step:1004/2330 train_time:37981ms step_avg:37.83ms
step:1005/2330 train_time:38015ms step_avg:37.83ms
step:1006/2330 train_time:38056ms step_avg:37.83ms
step:1007/2330 train_time:38090ms step_avg:37.83ms
step:1008/2330 train_time:38130ms step_avg:37.83ms
step:1009/2330 train_time:38166ms step_avg:37.83ms
step:1010/2330 train_time:38206ms step_avg:37.83ms
step:1011/2330 train_time:38241ms step_avg:37.83ms
step:1012/2330 train_time:38282ms step_avg:37.83ms
step:1013/2330 train_time:38321ms step_avg:37.83ms
step:1014/2330 train_time:38362ms step_avg:37.83ms
step:1015/2330 train_time:38398ms step_avg:37.83ms
step:1016/2330 train_time:38438ms step_avg:37.83ms
step:1017/2330 train_time:38474ms step_avg:37.83ms
step:1018/2330 train_time:38514ms step_avg:37.83ms
step:1019/2330 train_time:38550ms step_avg:37.83ms
step:1020/2330 train_time:38591ms step_avg:37.83ms
step:1021/2330 train_time:38625ms step_avg:37.83ms
step:1022/2330 train_time:38666ms step_avg:37.83ms
step:1023/2330 train_time:38700ms step_avg:37.83ms
step:1024/2330 train_time:38741ms step_avg:37.83ms
step:1025/2330 train_time:38775ms step_avg:37.83ms
step:1026/2330 train_time:38816ms step_avg:37.83ms
step:1027/2330 train_time:38852ms step_avg:37.83ms
step:1028/2330 train_time:38894ms step_avg:37.84ms
step:1029/2330 train_time:38929ms step_avg:37.83ms
step:1030/2330 train_time:38971ms step_avg:37.84ms
step:1031/2330 train_time:39005ms step_avg:37.83ms
step:1032/2330 train_time:39046ms step_avg:37.84ms
step:1033/2330 train_time:39080ms step_avg:37.83ms
step:1034/2330 train_time:39121ms step_avg:37.83ms
step:1035/2330 train_time:39155ms step_avg:37.83ms
step:1036/2330 train_time:39196ms step_avg:37.83ms
step:1037/2330 train_time:39232ms step_avg:37.83ms
step:1038/2330 train_time:39274ms step_avg:37.84ms
step:1039/2330 train_time:39308ms step_avg:37.83ms
step:1040/2330 train_time:39349ms step_avg:37.84ms
step:1041/2330 train_time:39386ms step_avg:37.83ms
step:1042/2330 train_time:39426ms step_avg:37.84ms
step:1043/2330 train_time:39463ms step_avg:37.84ms
step:1044/2330 train_time:39503ms step_avg:37.84ms
step:1045/2330 train_time:39539ms step_avg:37.84ms
step:1046/2330 train_time:39579ms step_avg:37.84ms
step:1047/2330 train_time:39614ms step_avg:37.84ms
step:1048/2330 train_time:39655ms step_avg:37.84ms
step:1049/2330 train_time:39690ms step_avg:37.84ms
step:1050/2330 train_time:39731ms step_avg:37.84ms
step:1051/2330 train_time:39766ms step_avg:37.84ms
step:1052/2330 train_time:39807ms step_avg:37.84ms
step:1053/2330 train_time:39842ms step_avg:37.84ms
step:1054/2330 train_time:39883ms step_avg:37.84ms
step:1055/2330 train_time:39918ms step_avg:37.84ms
step:1056/2330 train_time:39959ms step_avg:37.84ms
step:1057/2330 train_time:39993ms step_avg:37.84ms
step:1058/2330 train_time:40035ms step_avg:37.84ms
step:1059/2330 train_time:40070ms step_avg:37.84ms
step:1060/2330 train_time:40111ms step_avg:37.84ms
step:1061/2330 train_time:40147ms step_avg:37.84ms
step:1062/2330 train_time:40187ms step_avg:37.84ms
step:1063/2330 train_time:40223ms step_avg:37.84ms
step:1064/2330 train_time:40264ms step_avg:37.84ms
step:1065/2330 train_time:40300ms step_avg:37.84ms
step:1066/2330 train_time:40341ms step_avg:37.84ms
step:1067/2330 train_time:40376ms step_avg:37.84ms
step:1068/2330 train_time:40417ms step_avg:37.84ms
step:1069/2330 train_time:40452ms step_avg:37.84ms
step:1070/2330 train_time:40493ms step_avg:37.84ms
step:1071/2330 train_time:40529ms step_avg:37.84ms
step:1072/2330 train_time:40570ms step_avg:37.85ms
step:1073/2330 train_time:40605ms step_avg:37.84ms
step:1074/2330 train_time:40646ms step_avg:37.85ms
step:1075/2330 train_time:40682ms step_avg:37.84ms
step:1076/2330 train_time:40723ms step_avg:37.85ms
step:1077/2330 train_time:40758ms step_avg:37.84ms
step:1078/2330 train_time:40798ms step_avg:37.85ms
step:1079/2330 train_time:40833ms step_avg:37.84ms
step:1080/2330 train_time:40874ms step_avg:37.85ms
step:1081/2330 train_time:40908ms step_avg:37.84ms
step:1082/2330 train_time:40949ms step_avg:37.85ms
step:1083/2330 train_time:40984ms step_avg:37.84ms
step:1084/2330 train_time:41026ms step_avg:37.85ms
step:1085/2330 train_time:41060ms step_avg:37.84ms
step:1086/2330 train_time:41102ms step_avg:37.85ms
step:1087/2330 train_time:41136ms step_avg:37.84ms
step:1088/2330 train_time:41177ms step_avg:37.85ms
step:1089/2330 train_time:41212ms step_avg:37.84ms
step:1090/2330 train_time:41252ms step_avg:37.85ms
step:1091/2330 train_time:41288ms step_avg:37.84ms
step:1092/2330 train_time:41328ms step_avg:37.85ms
step:1093/2330 train_time:41364ms step_avg:37.84ms
step:1094/2330 train_time:41406ms step_avg:37.85ms
step:1095/2330 train_time:41441ms step_avg:37.85ms
step:1096/2330 train_time:41482ms step_avg:37.85ms
step:1097/2330 train_time:41517ms step_avg:37.85ms
step:1098/2330 train_time:41558ms step_avg:37.85ms
step:1099/2330 train_time:41593ms step_avg:37.85ms
step:1100/2330 train_time:41634ms step_avg:37.85ms
step:1101/2330 train_time:41670ms step_avg:37.85ms
step:1102/2330 train_time:41710ms step_avg:37.85ms
step:1103/2330 train_time:41746ms step_avg:37.85ms
step:1104/2330 train_time:41787ms step_avg:37.85ms
step:1105/2330 train_time:41822ms step_avg:37.85ms
step:1106/2330 train_time:41862ms step_avg:37.85ms
step:1107/2330 train_time:41897ms step_avg:37.85ms
step:1108/2330 train_time:41937ms step_avg:37.85ms
step:1109/2330 train_time:41973ms step_avg:37.85ms
step:1110/2330 train_time:42014ms step_avg:37.85ms
step:1111/2330 train_time:42049ms step_avg:37.85ms
step:1112/2330 train_time:42090ms step_avg:37.85ms
step:1113/2330 train_time:42125ms step_avg:37.85ms
step:1114/2330 train_time:42166ms step_avg:37.85ms
step:1115/2330 train_time:42200ms step_avg:37.85ms
step:1116/2330 train_time:42241ms step_avg:37.85ms
step:1117/2330 train_time:42276ms step_avg:37.85ms
step:1118/2330 train_time:42317ms step_avg:37.85ms
step:1119/2330 train_time:42352ms step_avg:37.85ms
step:1120/2330 train_time:42394ms step_avg:37.85ms
step:1121/2330 train_time:42429ms step_avg:37.85ms
step:1122/2330 train_time:42471ms step_avg:37.85ms
step:1123/2330 train_time:42506ms step_avg:37.85ms
step:1124/2330 train_time:42547ms step_avg:37.85ms
step:1125/2330 train_time:42583ms step_avg:37.85ms
step:1126/2330 train_time:42623ms step_avg:37.85ms
step:1127/2330 train_time:42659ms step_avg:37.85ms
step:1128/2330 train_time:42700ms step_avg:37.85ms
step:1129/2330 train_time:42735ms step_avg:37.85ms
step:1130/2330 train_time:42776ms step_avg:37.85ms
step:1131/2330 train_time:42811ms step_avg:37.85ms
step:1132/2330 train_time:42852ms step_avg:37.85ms
step:1133/2330 train_time:42887ms step_avg:37.85ms
step:1134/2330 train_time:42927ms step_avg:37.85ms
step:1135/2330 train_time:42962ms step_avg:37.85ms
step:1136/2330 train_time:43003ms step_avg:37.85ms
step:1137/2330 train_time:43038ms step_avg:37.85ms
step:1138/2330 train_time:43079ms step_avg:37.85ms
step:1139/2330 train_time:43114ms step_avg:37.85ms
step:1140/2330 train_time:43155ms step_avg:37.86ms
step:1141/2330 train_time:43190ms step_avg:37.85ms
step:1142/2330 train_time:43231ms step_avg:37.86ms
step:1143/2330 train_time:43267ms step_avg:37.85ms
step:1144/2330 train_time:43307ms step_avg:37.86ms
step:1145/2330 train_time:43343ms step_avg:37.85ms
step:1146/2330 train_time:43384ms step_avg:37.86ms
step:1147/2330 train_time:43420ms step_avg:37.86ms
step:1148/2330 train_time:43460ms step_avg:37.86ms
step:1149/2330 train_time:43495ms step_avg:37.85ms
step:1150/2330 train_time:43536ms step_avg:37.86ms
step:1151/2330 train_time:43571ms step_avg:37.85ms
step:1152/2330 train_time:43611ms step_avg:37.86ms
step:1153/2330 train_time:43647ms step_avg:37.85ms
step:1154/2330 train_time:43687ms step_avg:37.86ms
step:1155/2330 train_time:43722ms step_avg:37.85ms
step:1156/2330 train_time:43763ms step_avg:37.86ms
step:1157/2330 train_time:43798ms step_avg:37.86ms
step:1158/2330 train_time:43839ms step_avg:37.86ms
step:1159/2330 train_time:43874ms step_avg:37.85ms
step:1160/2330 train_time:43915ms step_avg:37.86ms
step:1161/2330 train_time:43950ms step_avg:37.85ms
step:1162/2330 train_time:43990ms step_avg:37.86ms
step:1163/2330 train_time:44026ms step_avg:37.86ms
step:1164/2330 train_time:44066ms step_avg:37.86ms
step:1165/2330 train_time:44101ms step_avg:37.86ms
step:1166/2330 train_time:44142ms step_avg:37.86ms
step:1167/2330 train_time:44177ms step_avg:37.85ms
step:1168/2330 train_time:44217ms step_avg:37.86ms
step:1169/2330 train_time:44253ms step_avg:37.86ms
step:1170/2330 train_time:44294ms step_avg:37.86ms
step:1171/2330 train_time:44330ms step_avg:37.86ms
step:1172/2330 train_time:44370ms step_avg:37.86ms
step:1173/2330 train_time:44406ms step_avg:37.86ms
step:1174/2330 train_time:44447ms step_avg:37.86ms
step:1175/2330 train_time:44482ms step_avg:37.86ms
step:1176/2330 train_time:44523ms step_avg:37.86ms
step:1177/2330 train_time:44557ms step_avg:37.86ms
step:1178/2330 train_time:44598ms step_avg:37.86ms
step:1179/2330 train_time:44633ms step_avg:37.86ms
step:1180/2330 train_time:44674ms step_avg:37.86ms
step:1181/2330 train_time:44710ms step_avg:37.86ms
step:1182/2330 train_time:44750ms step_avg:37.86ms
step:1183/2330 train_time:44786ms step_avg:37.86ms
step:1184/2330 train_time:44827ms step_avg:37.86ms
step:1185/2330 train_time:44863ms step_avg:37.86ms
step:1186/2330 train_time:44904ms step_avg:37.86ms
step:1187/2330 train_time:44940ms step_avg:37.86ms
step:1188/2330 train_time:44980ms step_avg:37.86ms
step:1189/2330 train_time:45015ms step_avg:37.86ms
step:1190/2330 train_time:45056ms step_avg:37.86ms
step:1191/2330 train_time:45091ms step_avg:37.86ms
step:1192/2330 train_time:45133ms step_avg:37.86ms
step:1193/2330 train_time:45169ms step_avg:37.86ms
step:1194/2330 train_time:45209ms step_avg:37.86ms
step:1195/2330 train_time:45245ms step_avg:37.86ms
step:1196/2330 train_time:45286ms step_avg:37.86ms
step:1197/2330 train_time:45320ms step_avg:37.86ms
step:1198/2330 train_time:45361ms step_avg:37.86ms
step:1199/2330 train_time:45397ms step_avg:37.86ms
step:1200/2330 train_time:45437ms step_avg:37.86ms
step:1201/2330 train_time:45474ms step_avg:37.86ms
step:1202/2330 train_time:45515ms step_avg:37.87ms
step:1203/2330 train_time:45550ms step_avg:37.86ms
step:1204/2330 train_time:45590ms step_avg:37.87ms
step:1205/2330 train_time:45626ms step_avg:37.86ms
step:1206/2330 train_time:45666ms step_avg:37.87ms
step:1207/2330 train_time:45702ms step_avg:37.86ms
step:1208/2330 train_time:45743ms step_avg:37.87ms
step:1209/2330 train_time:45777ms step_avg:37.86ms
step:1210/2330 train_time:45818ms step_avg:37.87ms
step:1211/2330 train_time:45853ms step_avg:37.86ms
step:1212/2330 train_time:45894ms step_avg:37.87ms
step:1213/2330 train_time:45929ms step_avg:37.86ms
step:1214/2330 train_time:45970ms step_avg:37.87ms
step:1215/2330 train_time:46006ms step_avg:37.87ms
step:1216/2330 train_time:46047ms step_avg:37.87ms
step:1217/2330 train_time:46083ms step_avg:37.87ms
step:1218/2330 train_time:46123ms step_avg:37.87ms
step:1219/2330 train_time:46159ms step_avg:37.87ms
step:1220/2330 train_time:46200ms step_avg:37.87ms
step:1221/2330 train_time:46235ms step_avg:37.87ms
step:1222/2330 train_time:46276ms step_avg:37.87ms
step:1223/2330 train_time:46311ms step_avg:37.87ms
step:1224/2330 train_time:46351ms step_avg:37.87ms
step:1225/2330 train_time:46387ms step_avg:37.87ms
step:1226/2330 train_time:46428ms step_avg:37.87ms
step:1227/2330 train_time:46463ms step_avg:37.87ms
step:1228/2330 train_time:46504ms step_avg:37.87ms
step:1229/2330 train_time:46540ms step_avg:37.87ms
step:1230/2330 train_time:46580ms step_avg:37.87ms
step:1231/2330 train_time:46616ms step_avg:37.87ms
step:1232/2330 train_time:46656ms step_avg:37.87ms
step:1233/2330 train_time:46692ms step_avg:37.87ms
step:1234/2330 train_time:46733ms step_avg:37.87ms
step:1235/2330 train_time:46769ms step_avg:37.87ms
step:1236/2330 train_time:46810ms step_avg:37.87ms
step:1237/2330 train_time:46845ms step_avg:37.87ms
step:1238/2330 train_time:46885ms step_avg:37.87ms
step:1239/2330 train_time:46921ms step_avg:37.87ms
step:1240/2330 train_time:46961ms step_avg:37.87ms
step:1241/2330 train_time:46996ms step_avg:37.87ms
step:1242/2330 train_time:47037ms step_avg:37.87ms
step:1243/2330 train_time:47073ms step_avg:37.87ms
step:1244/2330 train_time:47114ms step_avg:37.87ms
step:1245/2330 train_time:47149ms step_avg:37.87ms
step:1246/2330 train_time:47189ms step_avg:37.87ms
step:1247/2330 train_time:47225ms step_avg:37.87ms
step:1248/2330 train_time:47266ms step_avg:37.87ms
step:1249/2330 train_time:47301ms step_avg:37.87ms
step:1250/2330 train_time:47341ms step_avg:37.87ms
step:1250/2330 val_loss:5.4468 train_time:47453ms step_avg:37.96ms
step:1251/2330 train_time:47465ms step_avg:37.94ms
step:1252/2330 train_time:47477ms step_avg:37.92ms
step:1253/2330 train_time:47486ms step_avg:37.90ms
step:1254/2330 train_time:47497ms step_avg:37.88ms
step:1255/2330 train_time:47529ms step_avg:37.87ms
step:1256/2330 train_time:47570ms step_avg:37.87ms
step:1257/2330 train_time:47604ms step_avg:37.87ms
step:1258/2330 train_time:47645ms step_avg:37.87ms
step:1259/2330 train_time:47679ms step_avg:37.87ms
step:1260/2330 train_time:47720ms step_avg:37.87ms
step:1261/2330 train_time:47754ms step_avg:37.87ms
step:1262/2330 train_time:47795ms step_avg:37.87ms
step:1263/2330 train_time:47833ms step_avg:37.87ms
step:1264/2330 train_time:47874ms step_avg:37.88ms
step:1265/2330 train_time:47910ms step_avg:37.87ms
step:1266/2330 train_time:47951ms step_avg:37.88ms
step:1267/2330 train_time:47986ms step_avg:37.87ms
step:1268/2330 train_time:48027ms step_avg:37.88ms
step:1269/2330 train_time:48063ms step_avg:37.88ms
step:1270/2330 train_time:48105ms step_avg:37.88ms
step:1271/2330 train_time:48138ms step_avg:37.87ms
step:1272/2330 train_time:48179ms step_avg:37.88ms
step:1273/2330 train_time:48213ms step_avg:37.87ms
step:1274/2330 train_time:48254ms step_avg:37.88ms
step:1275/2330 train_time:48288ms step_avg:37.87ms
step:1276/2330 train_time:48329ms step_avg:37.88ms
step:1277/2330 train_time:48364ms step_avg:37.87ms
step:1278/2330 train_time:48406ms step_avg:37.88ms
step:1279/2330 train_time:48441ms step_avg:37.87ms
step:1280/2330 train_time:48482ms step_avg:37.88ms
step:1281/2330 train_time:48517ms step_avg:37.87ms
step:1282/2330 train_time:48557ms step_avg:37.88ms
step:1283/2330 train_time:48592ms step_avg:37.87ms
step:1284/2330 train_time:48632ms step_avg:37.88ms
step:1285/2330 train_time:48667ms step_avg:37.87ms
step:1286/2330 train_time:48708ms step_avg:37.88ms
step:1287/2330 train_time:48744ms step_avg:37.87ms
step:1288/2330 train_time:48785ms step_avg:37.88ms
step:1289/2330 train_time:48820ms step_avg:37.87ms
step:1290/2330 train_time:48861ms step_avg:37.88ms
step:1291/2330 train_time:48895ms step_avg:37.87ms
step:1292/2330 train_time:48936ms step_avg:37.88ms
step:1293/2330 train_time:48971ms step_avg:37.87ms
step:1294/2330 train_time:49012ms step_avg:37.88ms
step:1295/2330 train_time:49048ms step_avg:37.87ms
step:1296/2330 train_time:49088ms step_avg:37.88ms
step:1297/2330 train_time:49124ms step_avg:37.88ms
step:1298/2330 train_time:49165ms step_avg:37.88ms
step:1299/2330 train_time:49200ms step_avg:37.88ms
step:1300/2330 train_time:49240ms step_avg:37.88ms
step:1301/2330 train_time:49275ms step_avg:37.87ms
step:1302/2330 train_time:49315ms step_avg:37.88ms
step:1303/2330 train_time:49351ms step_avg:37.87ms
step:1304/2330 train_time:49392ms step_avg:37.88ms
step:1305/2330 train_time:49428ms step_avg:37.88ms
step:1306/2330 train_time:49468ms step_avg:37.88ms
step:1307/2330 train_time:49503ms step_avg:37.88ms
step:1308/2330 train_time:49544ms step_avg:37.88ms
step:1309/2330 train_time:49578ms step_avg:37.87ms
step:1310/2330 train_time:49619ms step_avg:37.88ms
step:1311/2330 train_time:49654ms step_avg:37.87ms
step:1312/2330 train_time:49694ms step_avg:37.88ms
step:1313/2330 train_time:49729ms step_avg:37.87ms
step:1314/2330 train_time:49769ms step_avg:37.88ms
step:1315/2330 train_time:49804ms step_avg:37.87ms
step:1316/2330 train_time:49845ms step_avg:37.88ms
step:1317/2330 train_time:49880ms step_avg:37.87ms
step:1318/2330 train_time:49921ms step_avg:37.88ms
step:1319/2330 train_time:49956ms step_avg:37.87ms
step:1320/2330 train_time:49997ms step_avg:37.88ms
step:1321/2330 train_time:50032ms step_avg:37.87ms
step:1322/2330 train_time:50073ms step_avg:37.88ms
step:1323/2330 train_time:50107ms step_avg:37.87ms
step:1324/2330 train_time:50148ms step_avg:37.88ms
step:1325/2330 train_time:50183ms step_avg:37.87ms
step:1326/2330 train_time:50224ms step_avg:37.88ms
step:1327/2330 train_time:50259ms step_avg:37.87ms
step:1328/2330 train_time:50300ms step_avg:37.88ms
step:1329/2330 train_time:50335ms step_avg:37.87ms
step:1330/2330 train_time:50375ms step_avg:37.88ms
step:1331/2330 train_time:50411ms step_avg:37.87ms
step:1332/2330 train_time:50452ms step_avg:37.88ms
step:1333/2330 train_time:50487ms step_avg:37.87ms
step:1334/2330 train_time:50527ms step_avg:37.88ms
step:1335/2330 train_time:50563ms step_avg:37.87ms
step:1336/2330 train_time:50603ms step_avg:37.88ms
step:1337/2330 train_time:50638ms step_avg:37.87ms
step:1338/2330 train_time:50679ms step_avg:37.88ms
step:1339/2330 train_time:50714ms step_avg:37.87ms
step:1340/2330 train_time:50755ms step_avg:37.88ms
step:1341/2330 train_time:50789ms step_avg:37.87ms
step:1342/2330 train_time:50829ms step_avg:37.88ms
step:1343/2330 train_time:50865ms step_avg:37.87ms
step:1344/2330 train_time:50906ms step_avg:37.88ms
step:1345/2330 train_time:50941ms step_avg:37.87ms
step:1346/2330 train_time:50982ms step_avg:37.88ms
step:1347/2330 train_time:51017ms step_avg:37.87ms
step:1348/2330 train_time:51057ms step_avg:37.88ms
step:1349/2330 train_time:51092ms step_avg:37.87ms
step:1350/2330 train_time:51133ms step_avg:37.88ms
step:1351/2330 train_time:51168ms step_avg:37.87ms
step:1352/2330 train_time:51209ms step_avg:37.88ms
step:1353/2330 train_time:51245ms step_avg:37.87ms
step:1354/2330 train_time:51286ms step_avg:37.88ms
step:1355/2330 train_time:51321ms step_avg:37.88ms
step:1356/2330 train_time:51362ms step_avg:37.88ms
step:1357/2330 train_time:51396ms step_avg:37.87ms
step:1358/2330 train_time:51436ms step_avg:37.88ms
step:1359/2330 train_time:51471ms step_avg:37.87ms
step:1360/2330 train_time:51512ms step_avg:37.88ms
step:1361/2330 train_time:51548ms step_avg:37.87ms
step:1362/2330 train_time:51588ms step_avg:37.88ms
step:1363/2330 train_time:51624ms step_avg:37.88ms
step:1364/2330 train_time:51665ms step_avg:37.88ms
step:1365/2330 train_time:51700ms step_avg:37.88ms
step:1366/2330 train_time:51741ms step_avg:37.88ms
step:1367/2330 train_time:51775ms step_avg:37.88ms
step:1368/2330 train_time:51816ms step_avg:37.88ms
step:1369/2330 train_time:51851ms step_avg:37.87ms
step:1370/2330 train_time:51892ms step_avg:37.88ms
step:1371/2330 train_time:51927ms step_avg:37.88ms
step:1372/2330 train_time:51967ms step_avg:37.88ms
step:1373/2330 train_time:52003ms step_avg:37.88ms
step:1374/2330 train_time:52044ms step_avg:37.88ms
step:1375/2330 train_time:52078ms step_avg:37.88ms
step:1376/2330 train_time:52119ms step_avg:37.88ms
step:1377/2330 train_time:52154ms step_avg:37.88ms
step:1378/2330 train_time:52195ms step_avg:37.88ms
step:1379/2330 train_time:52230ms step_avg:37.88ms
step:1380/2330 train_time:52271ms step_avg:37.88ms
step:1381/2330 train_time:52306ms step_avg:37.88ms
step:1382/2330 train_time:52347ms step_avg:37.88ms
step:1383/2330 train_time:52382ms step_avg:37.88ms
step:1384/2330 train_time:52423ms step_avg:37.88ms
step:1385/2330 train_time:52457ms step_avg:37.88ms
step:1386/2330 train_time:52498ms step_avg:37.88ms
step:1387/2330 train_time:52533ms step_avg:37.88ms
step:1388/2330 train_time:52574ms step_avg:37.88ms
step:1389/2330 train_time:52608ms step_avg:37.87ms
step:1390/2330 train_time:52650ms step_avg:37.88ms
step:1391/2330 train_time:52684ms step_avg:37.88ms
step:1392/2330 train_time:52725ms step_avg:37.88ms
step:1393/2330 train_time:52760ms step_avg:37.87ms
step:1394/2330 train_time:52800ms step_avg:37.88ms
step:1395/2330 train_time:52836ms step_avg:37.87ms
step:1396/2330 train_time:52876ms step_avg:37.88ms
step:1397/2330 train_time:52911ms step_avg:37.87ms
step:1398/2330 train_time:52952ms step_avg:37.88ms
step:1399/2330 train_time:52988ms step_avg:37.88ms
step:1400/2330 train_time:53029ms step_avg:37.88ms
step:1401/2330 train_time:53065ms step_avg:37.88ms
step:1402/2330 train_time:53106ms step_avg:37.88ms
step:1403/2330 train_time:53141ms step_avg:37.88ms
step:1404/2330 train_time:53182ms step_avg:37.88ms
step:1405/2330 train_time:53217ms step_avg:37.88ms
step:1406/2330 train_time:53257ms step_avg:37.88ms
step:1407/2330 train_time:53292ms step_avg:37.88ms
step:1408/2330 train_time:53333ms step_avg:37.88ms
step:1409/2330 train_time:53368ms step_avg:37.88ms
step:1410/2330 train_time:53409ms step_avg:37.88ms
step:1411/2330 train_time:53445ms step_avg:37.88ms
step:1412/2330 train_time:53485ms step_avg:37.88ms
step:1413/2330 train_time:53521ms step_avg:37.88ms
step:1414/2330 train_time:53561ms step_avg:37.88ms
step:1415/2330 train_time:53597ms step_avg:37.88ms
step:1416/2330 train_time:53637ms step_avg:37.88ms
step:1417/2330 train_time:53672ms step_avg:37.88ms
step:1418/2330 train_time:53713ms step_avg:37.88ms
step:1419/2330 train_time:53748ms step_avg:37.88ms
step:1420/2330 train_time:53788ms step_avg:37.88ms
step:1421/2330 train_time:53824ms step_avg:37.88ms
step:1422/2330 train_time:53864ms step_avg:37.88ms
step:1423/2330 train_time:53900ms step_avg:37.88ms
step:1424/2330 train_time:53941ms step_avg:37.88ms
step:1425/2330 train_time:53977ms step_avg:37.88ms
step:1426/2330 train_time:54017ms step_avg:37.88ms
step:1427/2330 train_time:54053ms step_avg:37.88ms
step:1428/2330 train_time:54094ms step_avg:37.88ms
step:1429/2330 train_time:54128ms step_avg:37.88ms
step:1430/2330 train_time:54168ms step_avg:37.88ms
step:1431/2330 train_time:54204ms step_avg:37.88ms
step:1432/2330 train_time:54245ms step_avg:37.88ms
step:1433/2330 train_time:54280ms step_avg:37.88ms
step:1434/2330 train_time:54321ms step_avg:37.88ms
step:1435/2330 train_time:54355ms step_avg:37.88ms
step:1436/2330 train_time:54396ms step_avg:37.88ms
step:1437/2330 train_time:54431ms step_avg:37.88ms
step:1438/2330 train_time:54472ms step_avg:37.88ms
step:1439/2330 train_time:54507ms step_avg:37.88ms
step:1440/2330 train_time:54548ms step_avg:37.88ms
step:1441/2330 train_time:54583ms step_avg:37.88ms
step:1442/2330 train_time:54623ms step_avg:37.88ms
step:1443/2330 train_time:54658ms step_avg:37.88ms
step:1444/2330 train_time:54699ms step_avg:37.88ms
step:1445/2330 train_time:54734ms step_avg:37.88ms
step:1446/2330 train_time:54774ms step_avg:37.88ms
step:1447/2330 train_time:54810ms step_avg:37.88ms
step:1448/2330 train_time:54851ms step_avg:37.88ms
step:1449/2330 train_time:54885ms step_avg:37.88ms
step:1450/2330 train_time:54927ms step_avg:37.88ms
step:1451/2330 train_time:54961ms step_avg:37.88ms
step:1452/2330 train_time:55002ms step_avg:37.88ms
step:1453/2330 train_time:55036ms step_avg:37.88ms
step:1454/2330 train_time:55077ms step_avg:37.88ms
step:1455/2330 train_time:55111ms step_avg:37.88ms
step:1456/2330 train_time:55152ms step_avg:37.88ms
step:1457/2330 train_time:55187ms step_avg:37.88ms
step:1458/2330 train_time:55228ms step_avg:37.88ms
step:1459/2330 train_time:55263ms step_avg:37.88ms
step:1460/2330 train_time:55303ms step_avg:37.88ms
step:1461/2330 train_time:55338ms step_avg:37.88ms
step:1462/2330 train_time:55379ms step_avg:37.88ms
step:1463/2330 train_time:55414ms step_avg:37.88ms
step:1464/2330 train_time:55455ms step_avg:37.88ms
step:1465/2330 train_time:55490ms step_avg:37.88ms
step:1466/2330 train_time:55531ms step_avg:37.88ms
step:1467/2330 train_time:55566ms step_avg:37.88ms
step:1468/2330 train_time:55606ms step_avg:37.88ms
step:1469/2330 train_time:55641ms step_avg:37.88ms
step:1470/2330 train_time:55682ms step_avg:37.88ms
step:1471/2330 train_time:55717ms step_avg:37.88ms
step:1472/2330 train_time:55758ms step_avg:37.88ms
step:1473/2330 train_time:55793ms step_avg:37.88ms
step:1474/2330 train_time:55834ms step_avg:37.88ms
step:1475/2330 train_time:55869ms step_avg:37.88ms
step:1476/2330 train_time:55909ms step_avg:37.88ms
step:1477/2330 train_time:55944ms step_avg:37.88ms
step:1478/2330 train_time:55985ms step_avg:37.88ms
step:1479/2330 train_time:56020ms step_avg:37.88ms
step:1480/2330 train_time:56061ms step_avg:37.88ms
step:1481/2330 train_time:56096ms step_avg:37.88ms
step:1482/2330 train_time:56137ms step_avg:37.88ms
step:1483/2330 train_time:56171ms step_avg:37.88ms
step:1484/2330 train_time:56212ms step_avg:37.88ms
step:1485/2330 train_time:56247ms step_avg:37.88ms
step:1486/2330 train_time:56287ms step_avg:37.88ms
step:1487/2330 train_time:56323ms step_avg:37.88ms
step:1488/2330 train_time:56364ms step_avg:37.88ms
step:1489/2330 train_time:56399ms step_avg:37.88ms
step:1490/2330 train_time:56440ms step_avg:37.88ms
step:1491/2330 train_time:56474ms step_avg:37.88ms
step:1492/2330 train_time:56515ms step_avg:37.88ms
step:1493/2330 train_time:56551ms step_avg:37.88ms
step:1494/2330 train_time:56591ms step_avg:37.88ms
step:1495/2330 train_time:56626ms step_avg:37.88ms
step:1496/2330 train_time:56667ms step_avg:37.88ms
step:1497/2330 train_time:56702ms step_avg:37.88ms
step:1498/2330 train_time:56744ms step_avg:37.88ms
step:1499/2330 train_time:56779ms step_avg:37.88ms
step:1500/2330 train_time:56819ms step_avg:37.88ms
step:1500/2330 val_loss:5.3965 train_time:56929ms step_avg:37.95ms
step:1501/2330 train_time:56941ms step_avg:37.94ms
step:1502/2330 train_time:56952ms step_avg:37.92ms
step:1503/2330 train_time:56961ms step_avg:37.90ms
step:1504/2330 train_time:56971ms step_avg:37.88ms
step:1505/2330 train_time:57006ms step_avg:37.88ms
step:1506/2330 train_time:57046ms step_avg:37.88ms
step:1507/2330 train_time:57080ms step_avg:37.88ms
step:1508/2330 train_time:57121ms step_avg:37.88ms
step:1509/2330 train_time:57155ms step_avg:37.88ms
step:1510/2330 train_time:57196ms step_avg:37.88ms
step:1511/2330 train_time:57231ms step_avg:37.88ms
step:1512/2330 train_time:57272ms step_avg:37.88ms
step:1513/2330 train_time:57311ms step_avg:37.88ms
step:1514/2330 train_time:57352ms step_avg:37.88ms
step:1515/2330 train_time:57388ms step_avg:37.88ms
step:1516/2330 train_time:57428ms step_avg:37.88ms
step:1517/2330 train_time:57464ms step_avg:37.88ms
step:1518/2330 train_time:57504ms step_avg:37.88ms
step:1519/2330 train_time:57539ms step_avg:37.88ms
step:1520/2330 train_time:57580ms step_avg:37.88ms
step:1521/2330 train_time:57615ms step_avg:37.88ms
step:1522/2330 train_time:57656ms step_avg:37.88ms
step:1523/2330 train_time:57690ms step_avg:37.88ms
step:1524/2330 train_time:57731ms step_avg:37.88ms
step:1525/2330 train_time:57767ms step_avg:37.88ms
step:1526/2330 train_time:57807ms step_avg:37.88ms
step:1527/2330 train_time:57844ms step_avg:37.88ms
step:1528/2330 train_time:57885ms step_avg:37.88ms
step:1529/2330 train_time:57923ms step_avg:37.88ms
step:1530/2330 train_time:57964ms step_avg:37.89ms
step:1531/2330 train_time:57998ms step_avg:37.88ms
step:1532/2330 train_time:58039ms step_avg:37.88ms
step:1533/2330 train_time:58073ms step_avg:37.88ms
step:1534/2330 train_time:58114ms step_avg:37.88ms
step:1535/2330 train_time:58148ms step_avg:37.88ms
step:1536/2330 train_time:58189ms step_avg:37.88ms
step:1537/2330 train_time:58225ms step_avg:37.88ms
step:1538/2330 train_time:58265ms step_avg:37.88ms
step:1539/2330 train_time:58301ms step_avg:37.88ms
step:1540/2330 train_time:58342ms step_avg:37.88ms
step:1541/2330 train_time:58378ms step_avg:37.88ms
step:1542/2330 train_time:58418ms step_avg:37.88ms
step:1543/2330 train_time:58454ms step_avg:37.88ms
step:1544/2330 train_time:58494ms step_avg:37.88ms
step:1545/2330 train_time:58529ms step_avg:37.88ms
step:1546/2330 train_time:58570ms step_avg:37.88ms
step:1547/2330 train_time:58604ms step_avg:37.88ms
step:1548/2330 train_time:58645ms step_avg:37.88ms
step:1549/2330 train_time:58680ms step_avg:37.88ms
step:1550/2330 train_time:58721ms step_avg:37.88ms
step:1551/2330 train_time:58755ms step_avg:37.88ms
step:1552/2330 train_time:58797ms step_avg:37.88ms
step:1553/2330 train_time:58832ms step_avg:37.88ms
step:1554/2330 train_time:58874ms step_avg:37.89ms
step:1555/2330 train_time:58909ms step_avg:37.88ms
step:1556/2330 train_time:58950ms step_avg:37.89ms
step:1557/2330 train_time:58985ms step_avg:37.88ms
step:1558/2330 train_time:59025ms step_avg:37.89ms
step:1559/2330 train_time:59061ms step_avg:37.88ms
step:1560/2330 train_time:59102ms step_avg:37.89ms
step:1561/2330 train_time:59137ms step_avg:37.88ms
step:1562/2330 train_time:59178ms step_avg:37.89ms
step:1563/2330 train_time:59213ms step_avg:37.88ms
step:1564/2330 train_time:59254ms step_avg:37.89ms
step:1565/2330 train_time:59290ms step_avg:37.88ms
step:1566/2330 train_time:59331ms step_avg:37.89ms
step:1567/2330 train_time:59367ms step_avg:37.89ms
step:1568/2330 train_time:59408ms step_avg:37.89ms
step:1569/2330 train_time:59443ms step_avg:37.89ms
step:1570/2330 train_time:59484ms step_avg:37.89ms
step:1571/2330 train_time:59519ms step_avg:37.89ms
step:1572/2330 train_time:59559ms step_avg:37.89ms
step:1573/2330 train_time:59595ms step_avg:37.89ms
step:1574/2330 train_time:59635ms step_avg:37.89ms
step:1575/2330 train_time:59671ms step_avg:37.89ms
step:1576/2330 train_time:59711ms step_avg:37.89ms
step:1577/2330 train_time:59747ms step_avg:37.89ms
step:1578/2330 train_time:59787ms step_avg:37.89ms
step:1579/2330 train_time:59822ms step_avg:37.89ms
step:1580/2330 train_time:59863ms step_avg:37.89ms
step:1581/2330 train_time:59899ms step_avg:37.89ms
step:1582/2330 train_time:59941ms step_avg:37.89ms
step:1583/2330 train_time:59975ms step_avg:37.89ms
step:1584/2330 train_time:60016ms step_avg:37.89ms
step:1585/2330 train_time:60051ms step_avg:37.89ms
step:1586/2330 train_time:60092ms step_avg:37.89ms
step:1587/2330 train_time:60126ms step_avg:37.89ms
step:1588/2330 train_time:60167ms step_avg:37.89ms
step:1589/2330 train_time:60202ms step_avg:37.89ms
step:1590/2330 train_time:60243ms step_avg:37.89ms
step:1591/2330 train_time:60278ms step_avg:37.89ms
step:1592/2330 train_time:60319ms step_avg:37.89ms
step:1593/2330 train_time:60354ms step_avg:37.89ms
step:1594/2330 train_time:60395ms step_avg:37.89ms
step:1595/2330 train_time:60431ms step_avg:37.89ms
step:1596/2330 train_time:60472ms step_avg:37.89ms
step:1597/2330 train_time:60508ms step_avg:37.89ms
step:1598/2330 train_time:60548ms step_avg:37.89ms
step:1599/2330 train_time:60583ms step_avg:37.89ms
step:1600/2330 train_time:60624ms step_avg:37.89ms
step:1601/2330 train_time:60659ms step_avg:37.89ms
step:1602/2330 train_time:60699ms step_avg:37.89ms
step:1603/2330 train_time:60736ms step_avg:37.89ms
step:1604/2330 train_time:60776ms step_avg:37.89ms
step:1605/2330 train_time:60813ms step_avg:37.89ms
step:1606/2330 train_time:60853ms step_avg:37.89ms
step:1607/2330 train_time:60889ms step_avg:37.89ms
step:1608/2330 train_time:60929ms step_avg:37.89ms
step:1609/2330 train_time:60965ms step_avg:37.89ms
step:1610/2330 train_time:61006ms step_avg:37.89ms
step:1611/2330 train_time:61041ms step_avg:37.89ms
step:1612/2330 train_time:61082ms step_avg:37.89ms
step:1613/2330 train_time:61117ms step_avg:37.89ms
step:1614/2330 train_time:61158ms step_avg:37.89ms
step:1615/2330 train_time:61193ms step_avg:37.89ms
step:1616/2330 train_time:61234ms step_avg:37.89ms
step:1617/2330 train_time:61269ms step_avg:37.89ms
step:1618/2330 train_time:61310ms step_avg:37.89ms
step:1619/2330 train_time:61344ms step_avg:37.89ms
step:1620/2330 train_time:61385ms step_avg:37.89ms
step:1621/2330 train_time:61420ms step_avg:37.89ms
step:1622/2330 train_time:61461ms step_avg:37.89ms
step:1623/2330 train_time:61496ms step_avg:37.89ms
step:1624/2330 train_time:61537ms step_avg:37.89ms
step:1625/2330 train_time:61572ms step_avg:37.89ms
step:1626/2330 train_time:61614ms step_avg:37.89ms
step:1627/2330 train_time:61649ms step_avg:37.89ms
step:1628/2330 train_time:61690ms step_avg:37.89ms
step:1629/2330 train_time:61726ms step_avg:37.89ms
step:1630/2330 train_time:61767ms step_avg:37.89ms
step:1631/2330 train_time:61802ms step_avg:37.89ms
step:1632/2330 train_time:61843ms step_avg:37.89ms
step:1633/2330 train_time:61878ms step_avg:37.89ms
step:1634/2330 train_time:61919ms step_avg:37.89ms
step:1635/2330 train_time:61954ms step_avg:37.89ms
step:1636/2330 train_time:61995ms step_avg:37.89ms
step:1637/2330 train_time:62031ms step_avg:37.89ms
step:1638/2330 train_time:62072ms step_avg:37.89ms
step:1639/2330 train_time:62108ms step_avg:37.89ms
step:1640/2330 train_time:62148ms step_avg:37.90ms
step:1641/2330 train_time:62183ms step_avg:37.89ms
step:1642/2330 train_time:62224ms step_avg:37.90ms
step:1643/2330 train_time:62259ms step_avg:37.89ms
step:1644/2330 train_time:62300ms step_avg:37.90ms
step:1645/2330 train_time:62335ms step_avg:37.89ms
step:1646/2330 train_time:62376ms step_avg:37.90ms
step:1647/2330 train_time:62412ms step_avg:37.89ms
step:1648/2330 train_time:62453ms step_avg:37.90ms
step:1649/2330 train_time:62487ms step_avg:37.89ms
step:1650/2330 train_time:62528ms step_avg:37.90ms
step:1651/2330 train_time:62564ms step_avg:37.89ms
step:1652/2330 train_time:62604ms step_avg:37.90ms
step:1653/2330 train_time:62639ms step_avg:37.89ms
step:1654/2330 train_time:62681ms step_avg:37.90ms
step:1655/2330 train_time:62717ms step_avg:37.90ms
step:1656/2330 train_time:62757ms step_avg:37.90ms
step:1657/2330 train_time:62793ms step_avg:37.90ms
step:1658/2330 train_time:62833ms step_avg:37.90ms
step:1659/2330 train_time:62868ms step_avg:37.90ms
step:1660/2330 train_time:62909ms step_avg:37.90ms
step:1661/2330 train_time:62944ms step_avg:37.90ms
step:1662/2330 train_time:62985ms step_avg:37.90ms
step:1663/2330 train_time:63020ms step_avg:37.90ms
step:1664/2330 train_time:63061ms step_avg:37.90ms
step:1665/2330 train_time:63096ms step_avg:37.90ms
step:1666/2330 train_time:63137ms step_avg:37.90ms
step:1667/2330 train_time:63173ms step_avg:37.90ms
step:1668/2330 train_time:63214ms step_avg:37.90ms
step:1669/2330 train_time:63250ms step_avg:37.90ms
step:1670/2330 train_time:63290ms step_avg:37.90ms
step:1671/2330 train_time:63326ms step_avg:37.90ms
step:1672/2330 train_time:63367ms step_avg:37.90ms
step:1673/2330 train_time:63402ms step_avg:37.90ms
step:1674/2330 train_time:63443ms step_avg:37.90ms
step:1675/2330 train_time:63478ms step_avg:37.90ms
step:1676/2330 train_time:63519ms step_avg:37.90ms
step:1677/2330 train_time:63554ms step_avg:37.90ms
step:1678/2330 train_time:63595ms step_avg:37.90ms
step:1679/2330 train_time:63631ms step_avg:37.90ms
step:1680/2330 train_time:63673ms step_avg:37.90ms
step:1681/2330 train_time:63708ms step_avg:37.90ms
step:1682/2330 train_time:63749ms step_avg:37.90ms
step:1683/2330 train_time:63785ms step_avg:37.90ms
step:1684/2330 train_time:63825ms step_avg:37.90ms
step:1685/2330 train_time:63860ms step_avg:37.90ms
step:1686/2330 train_time:63901ms step_avg:37.90ms
step:1687/2330 train_time:63937ms step_avg:37.90ms
step:1688/2330 train_time:63978ms step_avg:37.90ms
step:1689/2330 train_time:64014ms step_avg:37.90ms
step:1690/2330 train_time:64054ms step_avg:37.90ms
step:1691/2330 train_time:64090ms step_avg:37.90ms
step:1692/2330 train_time:64131ms step_avg:37.90ms
step:1693/2330 train_time:64165ms step_avg:37.90ms
step:1694/2330 train_time:64206ms step_avg:37.90ms
step:1695/2330 train_time:64241ms step_avg:37.90ms
step:1696/2330 train_time:64282ms step_avg:37.90ms
step:1697/2330 train_time:64317ms step_avg:37.90ms
step:1698/2330 train_time:64358ms step_avg:37.90ms
step:1699/2330 train_time:64393ms step_avg:37.90ms
step:1700/2330 train_time:64434ms step_avg:37.90ms
step:1701/2330 train_time:64470ms step_avg:37.90ms
step:1702/2330 train_time:64510ms step_avg:37.90ms
step:1703/2330 train_time:64547ms step_avg:37.90ms
step:1704/2330 train_time:64587ms step_avg:37.90ms
step:1705/2330 train_time:64623ms step_avg:37.90ms
step:1706/2330 train_time:64663ms step_avg:37.90ms
step:1707/2330 train_time:64698ms step_avg:37.90ms
step:1708/2330 train_time:64740ms step_avg:37.90ms
step:1709/2330 train_time:64775ms step_avg:37.90ms
step:1710/2330 train_time:64816ms step_avg:37.90ms
step:1711/2330 train_time:64852ms step_avg:37.90ms
step:1712/2330 train_time:64893ms step_avg:37.90ms
step:1713/2330 train_time:64928ms step_avg:37.90ms
step:1714/2330 train_time:64969ms step_avg:37.90ms
step:1715/2330 train_time:65004ms step_avg:37.90ms
step:1716/2330 train_time:65044ms step_avg:37.90ms
step:1717/2330 train_time:65079ms step_avg:37.90ms
step:1718/2330 train_time:65120ms step_avg:37.90ms
step:1719/2330 train_time:65155ms step_avg:37.90ms
step:1720/2330 train_time:65196ms step_avg:37.90ms
step:1721/2330 train_time:65231ms step_avg:37.90ms
step:1722/2330 train_time:65273ms step_avg:37.91ms
step:1723/2330 train_time:65308ms step_avg:37.90ms
step:1724/2330 train_time:65349ms step_avg:37.91ms
step:1725/2330 train_time:65384ms step_avg:37.90ms
step:1726/2330 train_time:65425ms step_avg:37.91ms
step:1727/2330 train_time:65460ms step_avg:37.90ms
step:1728/2330 train_time:65501ms step_avg:37.91ms
step:1729/2330 train_time:65536ms step_avg:37.90ms
step:1730/2330 train_time:65577ms step_avg:37.91ms
step:1731/2330 train_time:65613ms step_avg:37.90ms
step:1732/2330 train_time:65654ms step_avg:37.91ms
step:1733/2330 train_time:65689ms step_avg:37.90ms
step:1734/2330 train_time:65730ms step_avg:37.91ms
step:1735/2330 train_time:65765ms step_avg:37.90ms
step:1736/2330 train_time:65805ms step_avg:37.91ms
step:1737/2330 train_time:65840ms step_avg:37.90ms
step:1738/2330 train_time:65882ms step_avg:37.91ms
step:1739/2330 train_time:65917ms step_avg:37.91ms
step:1740/2330 train_time:65958ms step_avg:37.91ms
step:1741/2330 train_time:65994ms step_avg:37.91ms
step:1742/2330 train_time:66035ms step_avg:37.91ms
step:1743/2330 train_time:66071ms step_avg:37.91ms
step:1744/2330 train_time:66112ms step_avg:37.91ms
step:1745/2330 train_time:66147ms step_avg:37.91ms
step:1746/2330 train_time:66187ms step_avg:37.91ms
step:1747/2330 train_time:66223ms step_avg:37.91ms
step:1748/2330 train_time:66263ms step_avg:37.91ms
step:1749/2330 train_time:66299ms step_avg:37.91ms
step:1750/2330 train_time:66340ms step_avg:37.91ms
step:1750/2330 val_loss:5.3507 train_time:66451ms step_avg:37.97ms
step:1751/2330 train_time:66463ms step_avg:37.96ms
step:1752/2330 train_time:66474ms step_avg:37.94ms
step:1753/2330 train_time:66485ms step_avg:37.93ms
step:1754/2330 train_time:66495ms step_avg:37.91ms
step:1755/2330 train_time:66529ms step_avg:37.91ms
step:1756/2330 train_time:66569ms step_avg:37.91ms
step:1757/2330 train_time:66603ms step_avg:37.91ms
step:1758/2330 train_time:66643ms step_avg:37.91ms
step:1759/2330 train_time:66678ms step_avg:37.91ms
step:1760/2330 train_time:66718ms step_avg:37.91ms
step:1761/2330 train_time:66752ms step_avg:37.91ms
step:1762/2330 train_time:66794ms step_avg:37.91ms
step:1763/2330 train_time:66834ms step_avg:37.91ms
step:1764/2330 train_time:66876ms step_avg:37.91ms
step:1765/2330 train_time:66914ms step_avg:37.91ms
step:1766/2330 train_time:66955ms step_avg:37.91ms
step:1767/2330 train_time:66992ms step_avg:37.91ms
step:1768/2330 train_time:67033ms step_avg:37.91ms
step:1769/2330 train_time:67068ms step_avg:37.91ms
step:1770/2330 train_time:67109ms step_avg:37.91ms
step:1771/2330 train_time:67143ms step_avg:37.91ms
step:1772/2330 train_time:67184ms step_avg:37.91ms
step:1773/2330 train_time:67219ms step_avg:37.91ms
step:1774/2330 train_time:67259ms step_avg:37.91ms
step:1775/2330 train_time:67294ms step_avg:37.91ms
step:1776/2330 train_time:67334ms step_avg:37.91ms
step:1777/2330 train_time:67370ms step_avg:37.91ms
step:1778/2330 train_time:67410ms step_avg:37.91ms
step:1779/2330 train_time:67444ms step_avg:37.91ms
step:1780/2330 train_time:67485ms step_avg:37.91ms
step:1781/2330 train_time:67520ms step_avg:37.91ms
step:1782/2330 train_time:67561ms step_avg:37.91ms
step:1783/2330 train_time:67595ms step_avg:37.91ms
step:1784/2330 train_time:67636ms step_avg:37.91ms
step:1785/2330 train_time:67671ms step_avg:37.91ms
step:1786/2330 train_time:67712ms step_avg:37.91ms
step:1787/2330 train_time:67747ms step_avg:37.91ms
step:1788/2330 train_time:67788ms step_avg:37.91ms
step:1789/2330 train_time:67825ms step_avg:37.91ms
step:1790/2330 train_time:67865ms step_avg:37.91ms
step:1791/2330 train_time:67901ms step_avg:37.91ms
step:1792/2330 train_time:67941ms step_avg:37.91ms
step:1793/2330 train_time:67978ms step_avg:37.91ms
step:1794/2330 train_time:68019ms step_avg:37.91ms
step:1795/2330 train_time:68055ms step_avg:37.91ms
step:1796/2330 train_time:68095ms step_avg:37.92ms
step:1797/2330 train_time:68132ms step_avg:37.91ms
step:1798/2330 train_time:68172ms step_avg:37.92ms
step:1799/2330 train_time:68207ms step_avg:37.91ms
step:1800/2330 train_time:68248ms step_avg:37.92ms
step:1801/2330 train_time:68283ms step_avg:37.91ms
step:1802/2330 train_time:68324ms step_avg:37.92ms
step:1803/2330 train_time:68359ms step_avg:37.91ms
step:1804/2330 train_time:68400ms step_avg:37.92ms
step:1805/2330 train_time:68436ms step_avg:37.91ms
step:1806/2330 train_time:68476ms step_avg:37.92ms
step:1807/2330 train_time:68511ms step_avg:37.91ms
step:1808/2330 train_time:68552ms step_avg:37.92ms
step:1809/2330 train_time:68586ms step_avg:37.91ms
step:1810/2330 train_time:68627ms step_avg:37.92ms
step:1811/2330 train_time:68662ms step_avg:37.91ms
step:1812/2330 train_time:68704ms step_avg:37.92ms
step:1813/2330 train_time:68738ms step_avg:37.91ms
step:1814/2330 train_time:68780ms step_avg:37.92ms
step:1815/2330 train_time:68815ms step_avg:37.91ms
step:1816/2330 train_time:68856ms step_avg:37.92ms
step:1817/2330 train_time:68892ms step_avg:37.92ms
step:1818/2330 train_time:68933ms step_avg:37.92ms
step:1819/2330 train_time:68968ms step_avg:37.92ms
step:1820/2330 train_time:69008ms step_avg:37.92ms
step:1821/2330 train_time:69044ms step_avg:37.92ms
step:1822/2330 train_time:69085ms step_avg:37.92ms
step:1823/2330 train_time:69120ms step_avg:37.92ms
step:1824/2330 train_time:69161ms step_avg:37.92ms
step:1825/2330 train_time:69196ms step_avg:37.92ms
step:1826/2330 train_time:69237ms step_avg:37.92ms
step:1827/2330 train_time:69272ms step_avg:37.92ms
step:1828/2330 train_time:69313ms step_avg:37.92ms
step:1829/2330 train_time:69348ms step_avg:37.92ms
step:1830/2330 train_time:69388ms step_avg:37.92ms
step:1831/2330 train_time:69423ms step_avg:37.92ms
step:1832/2330 train_time:69464ms step_avg:37.92ms
step:1833/2330 train_time:69499ms step_avg:37.92ms
step:1834/2330 train_time:69539ms step_avg:37.92ms
step:1835/2330 train_time:69574ms step_avg:37.91ms
step:1836/2330 train_time:69615ms step_avg:37.92ms
step:1837/2330 train_time:69650ms step_avg:37.91ms
step:1838/2330 train_time:69690ms step_avg:37.92ms
step:1839/2330 train_time:69726ms step_avg:37.92ms
step:1840/2330 train_time:69766ms step_avg:37.92ms
step:1841/2330 train_time:69801ms step_avg:37.91ms
step:1842/2330 train_time:69843ms step_avg:37.92ms
step:1843/2330 train_time:69879ms step_avg:37.92ms
step:1844/2330 train_time:69920ms step_avg:37.92ms
step:1845/2330 train_time:69956ms step_avg:37.92ms
step:1846/2330 train_time:69997ms step_avg:37.92ms
step:1847/2330 train_time:70032ms step_avg:37.92ms
step:1848/2330 train_time:70074ms step_avg:37.92ms
step:1849/2330 train_time:70108ms step_avg:37.92ms
step:1850/2330 train_time:70149ms step_avg:37.92ms
step:1851/2330 train_time:70184ms step_avg:37.92ms
step:1852/2330 train_time:70225ms step_avg:37.92ms
step:1853/2330 train_time:70261ms step_avg:37.92ms
step:1854/2330 train_time:70302ms step_avg:37.92ms
step:1855/2330 train_time:70337ms step_avg:37.92ms
step:1856/2330 train_time:70378ms step_avg:37.92ms
step:1857/2330 train_time:70413ms step_avg:37.92ms
step:1858/2330 train_time:70453ms step_avg:37.92ms
step:1859/2330 train_time:70488ms step_avg:37.92ms
step:1860/2330 train_time:70528ms step_avg:37.92ms
step:1861/2330 train_time:70563ms step_avg:37.92ms
step:1862/2330 train_time:70604ms step_avg:37.92ms
step:1863/2330 train_time:70640ms step_avg:37.92ms
step:1864/2330 train_time:70680ms step_avg:37.92ms
step:1865/2330 train_time:70716ms step_avg:37.92ms
step:1866/2330 train_time:70756ms step_avg:37.92ms
step:1867/2330 train_time:70792ms step_avg:37.92ms
step:1868/2330 train_time:70832ms step_avg:37.92ms
step:1869/2330 train_time:70868ms step_avg:37.92ms
step:1870/2330 train_time:70908ms step_avg:37.92ms
step:1871/2330 train_time:70944ms step_avg:37.92ms
step:1872/2330 train_time:70985ms step_avg:37.92ms
step:1873/2330 train_time:71020ms step_avg:37.92ms
step:1874/2330 train_time:71062ms step_avg:37.92ms
step:1875/2330 train_time:71097ms step_avg:37.92ms
step:1876/2330 train_time:71138ms step_avg:37.92ms
step:1877/2330 train_time:71174ms step_avg:37.92ms
step:1878/2330 train_time:71215ms step_avg:37.92ms
step:1879/2330 train_time:71251ms step_avg:37.92ms
step:1880/2330 train_time:71291ms step_avg:37.92ms
step:1881/2330 train_time:71327ms step_avg:37.92ms
step:1882/2330 train_time:71367ms step_avg:37.92ms
step:1883/2330 train_time:71403ms step_avg:37.92ms
step:1884/2330 train_time:71443ms step_avg:37.92ms
step:1885/2330 train_time:71479ms step_avg:37.92ms
step:1886/2330 train_time:71519ms step_avg:37.92ms
step:1887/2330 train_time:71554ms step_avg:37.92ms
step:1888/2330 train_time:71595ms step_avg:37.92ms
step:1889/2330 train_time:71630ms step_avg:37.92ms
step:1890/2330 train_time:71670ms step_avg:37.92ms
step:1891/2330 train_time:71706ms step_avg:37.92ms
step:1892/2330 train_time:71747ms step_avg:37.92ms
step:1893/2330 train_time:71782ms step_avg:37.92ms
step:1894/2330 train_time:71823ms step_avg:37.92ms
step:1895/2330 train_time:71859ms step_avg:37.92ms
step:1896/2330 train_time:71899ms step_avg:37.92ms
step:1897/2330 train_time:71935ms step_avg:37.92ms
step:1898/2330 train_time:71976ms step_avg:37.92ms
step:1899/2330 train_time:72013ms step_avg:37.92ms
step:1900/2330 train_time:72054ms step_avg:37.92ms
step:1901/2330 train_time:72090ms step_avg:37.92ms
step:1902/2330 train_time:72130ms step_avg:37.92ms
step:1903/2330 train_time:72165ms step_avg:37.92ms
step:1904/2330 train_time:72206ms step_avg:37.92ms
step:1905/2330 train_time:72241ms step_avg:37.92ms
step:1906/2330 train_time:72282ms step_avg:37.92ms
step:1907/2330 train_time:72317ms step_avg:37.92ms
step:1908/2330 train_time:72358ms step_avg:37.92ms
step:1909/2330 train_time:72394ms step_avg:37.92ms
step:1910/2330 train_time:72435ms step_avg:37.92ms
step:1911/2330 train_time:72471ms step_avg:37.92ms
step:1912/2330 train_time:72511ms step_avg:37.92ms
step:1913/2330 train_time:72546ms step_avg:37.92ms
step:1914/2330 train_time:72586ms step_avg:37.92ms
step:1915/2330 train_time:72622ms step_avg:37.92ms
step:1916/2330 train_time:72663ms step_avg:37.92ms
step:1917/2330 train_time:72699ms step_avg:37.92ms
step:1918/2330 train_time:72739ms step_avg:37.92ms
step:1919/2330 train_time:72775ms step_avg:37.92ms
step:1920/2330 train_time:72816ms step_avg:37.93ms
step:1921/2330 train_time:72852ms step_avg:37.92ms
step:1922/2330 train_time:72892ms step_avg:37.93ms
step:1923/2330 train_time:72928ms step_avg:37.92ms
step:1924/2330 train_time:72969ms step_avg:37.93ms
step:1925/2330 train_time:73004ms step_avg:37.92ms
step:1926/2330 train_time:73045ms step_avg:37.93ms
step:1927/2330 train_time:73081ms step_avg:37.92ms
step:1928/2330 train_time:73122ms step_avg:37.93ms
step:1929/2330 train_time:73158ms step_avg:37.93ms
step:1930/2330 train_time:73198ms step_avg:37.93ms
step:1931/2330 train_time:73233ms step_avg:37.93ms
step:1932/2330 train_time:73274ms step_avg:37.93ms
step:1933/2330 train_time:73310ms step_avg:37.93ms
step:1934/2330 train_time:73350ms step_avg:37.93ms
step:1935/2330 train_time:73386ms step_avg:37.93ms
step:1936/2330 train_time:73426ms step_avg:37.93ms
step:1937/2330 train_time:73461ms step_avg:37.93ms
step:1938/2330 train_time:73502ms step_avg:37.93ms
step:1939/2330 train_time:73537ms step_avg:37.93ms
step:1940/2330 train_time:73577ms step_avg:37.93ms
step:1941/2330 train_time:73613ms step_avg:37.93ms
step:1942/2330 train_time:73654ms step_avg:37.93ms
step:1943/2330 train_time:73690ms step_avg:37.93ms
step:1944/2330 train_time:73730ms step_avg:37.93ms
step:1945/2330 train_time:73765ms step_avg:37.93ms
step:1946/2330 train_time:73806ms step_avg:37.93ms
step:1947/2330 train_time:73841ms step_avg:37.93ms
step:1948/2330 train_time:73882ms step_avg:37.93ms
step:1949/2330 train_time:73917ms step_avg:37.93ms
step:1950/2330 train_time:73958ms step_avg:37.93ms
step:1951/2330 train_time:73995ms step_avg:37.93ms
step:1952/2330 train_time:74035ms step_avg:37.93ms
step:1953/2330 train_time:74071ms step_avg:37.93ms
step:1954/2330 train_time:74112ms step_avg:37.93ms
step:1955/2330 train_time:74148ms step_avg:37.93ms
step:1956/2330 train_time:74188ms step_avg:37.93ms
step:1957/2330 train_time:74224ms step_avg:37.93ms
step:1958/2330 train_time:74265ms step_avg:37.93ms
step:1959/2330 train_time:74300ms step_avg:37.93ms
step:1960/2330 train_time:74340ms step_avg:37.93ms
step:1961/2330 train_time:74377ms step_avg:37.93ms
step:1962/2330 train_time:74417ms step_avg:37.93ms
step:1963/2330 train_time:74454ms step_avg:37.93ms
step:1964/2330 train_time:74495ms step_avg:37.93ms
step:1965/2330 train_time:74530ms step_avg:37.93ms
step:1966/2330 train_time:74571ms step_avg:37.93ms
step:1967/2330 train_time:74605ms step_avg:37.93ms
step:1968/2330 train_time:74646ms step_avg:37.93ms
step:1969/2330 train_time:74680ms step_avg:37.93ms
step:1970/2330 train_time:74721ms step_avg:37.93ms
step:1971/2330 train_time:74757ms step_avg:37.93ms
step:1972/2330 train_time:74798ms step_avg:37.93ms
step:1973/2330 train_time:74833ms step_avg:37.93ms
step:1974/2330 train_time:74873ms step_avg:37.93ms
step:1975/2330 train_time:74908ms step_avg:37.93ms
step:1976/2330 train_time:74949ms step_avg:37.93ms
step:1977/2330 train_time:74984ms step_avg:37.93ms
step:1978/2330 train_time:75025ms step_avg:37.93ms
step:1979/2330 train_time:75059ms step_avg:37.93ms
step:1980/2330 train_time:75100ms step_avg:37.93ms
step:1981/2330 train_time:75134ms step_avg:37.93ms
step:1982/2330 train_time:75175ms step_avg:37.93ms
step:1983/2330 train_time:75211ms step_avg:37.93ms
step:1984/2330 train_time:75252ms step_avg:37.93ms
step:1985/2330 train_time:75287ms step_avg:37.93ms
step:1986/2330 train_time:75328ms step_avg:37.93ms
step:1987/2330 train_time:75364ms step_avg:37.93ms
step:1988/2330 train_time:75405ms step_avg:37.93ms
step:1989/2330 train_time:75439ms step_avg:37.93ms
step:1990/2330 train_time:75480ms step_avg:37.93ms
step:1991/2330 train_time:75516ms step_avg:37.93ms
step:1992/2330 train_time:75557ms step_avg:37.93ms
step:1993/2330 train_time:75592ms step_avg:37.93ms
step:1994/2330 train_time:75633ms step_avg:37.93ms
step:1995/2330 train_time:75668ms step_avg:37.93ms
step:1996/2330 train_time:75708ms step_avg:37.93ms
step:1997/2330 train_time:75743ms step_avg:37.93ms
step:1998/2330 train_time:75784ms step_avg:37.93ms
step:1999/2330 train_time:75819ms step_avg:37.93ms
step:2000/2330 train_time:75860ms step_avg:37.93ms
step:2000/2330 val_loss:5.3127 train_time:75973ms step_avg:37.99ms
step:2001/2330 train_time:75984ms step_avg:37.97ms
step:2002/2330 train_time:75996ms step_avg:37.96ms
step:2003/2330 train_time:76005ms step_avg:37.95ms
step:2004/2330 train_time:76016ms step_avg:37.93ms
step:2005/2330 train_time:76050ms step_avg:37.93ms
step:2006/2330 train_time:76090ms step_avg:37.93ms
step:2007/2330 train_time:76125ms step_avg:37.93ms
step:2008/2330 train_time:76166ms step_avg:37.93ms
step:2009/2330 train_time:76200ms step_avg:37.93ms
step:2010/2330 train_time:76240ms step_avg:37.93ms
step:2011/2330 train_time:76276ms step_avg:37.93ms
step:2012/2330 train_time:76316ms step_avg:37.93ms
step:2013/2330 train_time:76355ms step_avg:37.93ms
step:2014/2330 train_time:76395ms step_avg:37.93ms
step:2015/2330 train_time:76432ms step_avg:37.93ms
step:2016/2330 train_time:76473ms step_avg:37.93ms
step:2017/2330 train_time:76507ms step_avg:37.93ms
step:2018/2330 train_time:76548ms step_avg:37.93ms
step:2019/2330 train_time:76584ms step_avg:37.93ms
step:2020/2330 train_time:76624ms step_avg:37.93ms
step:2021/2330 train_time:76660ms step_avg:37.93ms
step:2022/2330 train_time:76700ms step_avg:37.93ms
step:2023/2330 train_time:76735ms step_avg:37.93ms
step:2024/2330 train_time:76776ms step_avg:37.93ms
step:2025/2330 train_time:76811ms step_avg:37.93ms
step:2026/2330 train_time:76852ms step_avg:37.93ms
step:2027/2330 train_time:76887ms step_avg:37.93ms
step:2028/2330 train_time:76930ms step_avg:37.93ms
step:2029/2330 train_time:76965ms step_avg:37.93ms
step:2030/2330 train_time:77007ms step_avg:37.93ms
step:2031/2330 train_time:77041ms step_avg:37.93ms
step:2032/2330 train_time:77082ms step_avg:37.93ms
step:2033/2330 train_time:77117ms step_avg:37.93ms
step:2034/2330 train_time:77158ms step_avg:37.93ms
step:2035/2330 train_time:77192ms step_avg:37.93ms
step:2036/2330 train_time:77233ms step_avg:37.93ms
step:2037/2330 train_time:77269ms step_avg:37.93ms
step:2038/2330 train_time:77310ms step_avg:37.93ms
step:2039/2330 train_time:77346ms step_avg:37.93ms
step:2040/2330 train_time:77387ms step_avg:37.93ms
step:2041/2330 train_time:77423ms step_avg:37.93ms
step:2042/2330 train_time:77464ms step_avg:37.94ms
step:2043/2330 train_time:77499ms step_avg:37.93ms
step:2044/2330 train_time:77539ms step_avg:37.94ms
step:2045/2330 train_time:77574ms step_avg:37.93ms
step:2046/2330 train_time:77615ms step_avg:37.93ms
step:2047/2330 train_time:77650ms step_avg:37.93ms
step:2048/2330 train_time:77691ms step_avg:37.94ms
step:2049/2330 train_time:77727ms step_avg:37.93ms
step:2050/2330 train_time:77767ms step_avg:37.94ms
step:2051/2330 train_time:77803ms step_avg:37.93ms
step:2052/2330 train_time:77844ms step_avg:37.94ms
step:2053/2330 train_time:77878ms step_avg:37.93ms
step:2054/2330 train_time:77919ms step_avg:37.94ms
step:2055/2330 train_time:77954ms step_avg:37.93ms
step:2056/2330 train_time:77995ms step_avg:37.94ms
step:2057/2330 train_time:78030ms step_avg:37.93ms
step:2058/2330 train_time:78071ms step_avg:37.94ms
step:2059/2330 train_time:78106ms step_avg:37.93ms
step:2060/2330 train_time:78147ms step_avg:37.94ms
step:2061/2330 train_time:78183ms step_avg:37.93ms
step:2062/2330 train_time:78224ms step_avg:37.94ms
step:2063/2330 train_time:78259ms step_avg:37.93ms
step:2064/2330 train_time:78300ms step_avg:37.94ms
step:2065/2330 train_time:78335ms step_avg:37.93ms
step:2066/2330 train_time:78375ms step_avg:37.94ms
step:2067/2330 train_time:78411ms step_avg:37.93ms
step:2068/2330 train_time:78452ms step_avg:37.94ms
step:2069/2330 train_time:78487ms step_avg:37.93ms
step:2070/2330 train_time:78528ms step_avg:37.94ms
step:2071/2330 train_time:78563ms step_avg:37.93ms
step:2072/2330 train_time:78604ms step_avg:37.94ms
step:2073/2330 train_time:78638ms step_avg:37.93ms
step:2074/2330 train_time:78679ms step_avg:37.94ms
step:2075/2330 train_time:78714ms step_avg:37.93ms
step:2076/2330 train_time:78754ms step_avg:37.94ms
step:2077/2330 train_time:78789ms step_avg:37.93ms
step:2078/2330 train_time:78831ms step_avg:37.94ms
step:2079/2330 train_time:78866ms step_avg:37.93ms
step:2080/2330 train_time:78907ms step_avg:37.94ms
step:2081/2330 train_time:78942ms step_avg:37.93ms
step:2082/2330 train_time:78983ms step_avg:37.94ms
step:2083/2330 train_time:79020ms step_avg:37.94ms
step:2084/2330 train_time:79060ms step_avg:37.94ms
step:2085/2330 train_time:79096ms step_avg:37.94ms
step:2086/2330 train_time:79136ms step_avg:37.94ms
step:2087/2330 train_time:79172ms step_avg:37.94ms
step:2088/2330 train_time:79213ms step_avg:37.94ms
step:2089/2330 train_time:79248ms step_avg:37.94ms
step:2090/2330 train_time:79289ms step_avg:37.94ms
step:2091/2330 train_time:79325ms step_avg:37.94ms
step:2092/2330 train_time:79365ms step_avg:37.94ms
step:2093/2330 train_time:79401ms step_avg:37.94ms
step:2094/2330 train_time:79442ms step_avg:37.94ms
step:2095/2330 train_time:79478ms step_avg:37.94ms
step:2096/2330 train_time:79519ms step_avg:37.94ms
step:2097/2330 train_time:79554ms step_avg:37.94ms
step:2098/2330 train_time:79594ms step_avg:37.94ms
step:2099/2330 train_time:79630ms step_avg:37.94ms
step:2100/2330 train_time:79670ms step_avg:37.94ms
step:2101/2330 train_time:79706ms step_avg:37.94ms
step:2102/2330 train_time:79747ms step_avg:37.94ms
step:2103/2330 train_time:79782ms step_avg:37.94ms
step:2104/2330 train_time:79823ms step_avg:37.94ms
step:2105/2330 train_time:79857ms step_avg:37.94ms
step:2106/2330 train_time:79898ms step_avg:37.94ms
step:2107/2330 train_time:79933ms step_avg:37.94ms
step:2108/2330 train_time:79974ms step_avg:37.94ms
step:2109/2330 train_time:80010ms step_avg:37.94ms
step:2110/2330 train_time:80050ms step_avg:37.94ms
step:2111/2330 train_time:80086ms step_avg:37.94ms
step:2112/2330 train_time:80127ms step_avg:37.94ms
step:2113/2330 train_time:80163ms step_avg:37.94ms
step:2114/2330 train_time:80204ms step_avg:37.94ms
step:2115/2330 train_time:80240ms step_avg:37.94ms
step:2116/2330 train_time:80281ms step_avg:37.94ms
step:2117/2330 train_time:80316ms step_avg:37.94ms
step:2118/2330 train_time:80357ms step_avg:37.94ms
step:2119/2330 train_time:80391ms step_avg:37.94ms
step:2120/2330 train_time:80433ms step_avg:37.94ms
step:2121/2330 train_time:80467ms step_avg:37.94ms
step:2122/2330 train_time:80508ms step_avg:37.94ms
step:2123/2330 train_time:80543ms step_avg:37.94ms
step:2124/2330 train_time:80584ms step_avg:37.94ms
step:2125/2330 train_time:80619ms step_avg:37.94ms
step:2126/2330 train_time:80659ms step_avg:37.94ms
step:2127/2330 train_time:80695ms step_avg:37.94ms
step:2128/2330 train_time:80735ms step_avg:37.94ms
step:2129/2330 train_time:80770ms step_avg:37.94ms
step:2130/2330 train_time:80811ms step_avg:37.94ms
step:2131/2330 train_time:80846ms step_avg:37.94ms
step:2132/2330 train_time:80887ms step_avg:37.94ms
step:2133/2330 train_time:80922ms step_avg:37.94ms
step:2134/2330 train_time:80963ms step_avg:37.94ms
step:2135/2330 train_time:80998ms step_avg:37.94ms
step:2136/2330 train_time:81039ms step_avg:37.94ms
step:2137/2330 train_time:81074ms step_avg:37.94ms
step:2138/2330 train_time:81115ms step_avg:37.94ms
step:2139/2330 train_time:81150ms step_avg:37.94ms
step:2140/2330 train_time:81191ms step_avg:37.94ms
step:2141/2330 train_time:81227ms step_avg:37.94ms
step:2142/2330 train_time:81268ms step_avg:37.94ms
step:2143/2330 train_time:81303ms step_avg:37.94ms
step:2144/2330 train_time:81343ms step_avg:37.94ms
step:2145/2330 train_time:81378ms step_avg:37.94ms
step:2146/2330 train_time:81419ms step_avg:37.94ms
step:2147/2330 train_time:81455ms step_avg:37.94ms
step:2148/2330 train_time:81495ms step_avg:37.94ms
step:2149/2330 train_time:81530ms step_avg:37.94ms
step:2150/2330 train_time:81571ms step_avg:37.94ms
step:2151/2330 train_time:81606ms step_avg:37.94ms
step:2152/2330 train_time:81646ms step_avg:37.94ms
step:2153/2330 train_time:81682ms step_avg:37.94ms
step:2154/2330 train_time:81722ms step_avg:37.94ms
step:2155/2330 train_time:81758ms step_avg:37.94ms
step:2156/2330 train_time:81798ms step_avg:37.94ms
step:2157/2330 train_time:81834ms step_avg:37.94ms
step:2158/2330 train_time:81874ms step_avg:37.94ms
step:2159/2330 train_time:81909ms step_avg:37.94ms
step:2160/2330 train_time:81951ms step_avg:37.94ms
step:2161/2330 train_time:81986ms step_avg:37.94ms
step:2162/2330 train_time:82027ms step_avg:37.94ms
step:2163/2330 train_time:82062ms step_avg:37.94ms
step:2164/2330 train_time:82104ms step_avg:37.94ms
step:2165/2330 train_time:82139ms step_avg:37.94ms
step:2166/2330 train_time:82181ms step_avg:37.94ms
step:2167/2330 train_time:82216ms step_avg:37.94ms
step:2168/2330 train_time:82257ms step_avg:37.94ms
step:2169/2330 train_time:82292ms step_avg:37.94ms
step:2170/2330 train_time:82333ms step_avg:37.94ms
step:2171/2330 train_time:82368ms step_avg:37.94ms
step:2172/2330 train_time:82409ms step_avg:37.94ms
step:2173/2330 train_time:82445ms step_avg:37.94ms
step:2174/2330 train_time:82485ms step_avg:37.94ms
step:2175/2330 train_time:82521ms step_avg:37.94ms
step:2176/2330 train_time:82562ms step_avg:37.94ms
step:2177/2330 train_time:82597ms step_avg:37.94ms
step:2178/2330 train_time:82638ms step_avg:37.94ms
step:2179/2330 train_time:82672ms step_avg:37.94ms
step:2180/2330 train_time:82713ms step_avg:37.94ms
step:2181/2330 train_time:82748ms step_avg:37.94ms
step:2182/2330 train_time:82789ms step_avg:37.94ms
step:2183/2330 train_time:82825ms step_avg:37.94ms
step:2184/2330 train_time:82866ms step_avg:37.94ms
step:2185/2330 train_time:82902ms step_avg:37.94ms
step:2186/2330 train_time:82942ms step_avg:37.94ms
step:2187/2330 train_time:82979ms step_avg:37.94ms
step:2188/2330 train_time:83019ms step_avg:37.94ms
step:2189/2330 train_time:83055ms step_avg:37.94ms
step:2190/2330 train_time:83095ms step_avg:37.94ms
step:2191/2330 train_time:83130ms step_avg:37.94ms
step:2192/2330 train_time:83171ms step_avg:37.94ms
step:2193/2330 train_time:83206ms step_avg:37.94ms
step:2194/2330 train_time:83247ms step_avg:37.94ms
step:2195/2330 train_time:83283ms step_avg:37.94ms
step:2196/2330 train_time:83323ms step_avg:37.94ms
step:2197/2330 train_time:83358ms step_avg:37.94ms
step:2198/2330 train_time:83399ms step_avg:37.94ms
step:2199/2330 train_time:83434ms step_avg:37.94ms
step:2200/2330 train_time:83475ms step_avg:37.94ms
step:2201/2330 train_time:83510ms step_avg:37.94ms
step:2202/2330 train_time:83551ms step_avg:37.94ms
step:2203/2330 train_time:83586ms step_avg:37.94ms
step:2204/2330 train_time:83627ms step_avg:37.94ms
step:2205/2330 train_time:83662ms step_avg:37.94ms
step:2206/2330 train_time:83703ms step_avg:37.94ms
step:2207/2330 train_time:83739ms step_avg:37.94ms
step:2208/2330 train_time:83780ms step_avg:37.94ms
step:2209/2330 train_time:83816ms step_avg:37.94ms
step:2210/2330 train_time:83856ms step_avg:37.94ms
step:2211/2330 train_time:83892ms step_avg:37.94ms
step:2212/2330 train_time:83933ms step_avg:37.94ms
step:2213/2330 train_time:83968ms step_avg:37.94ms
step:2214/2330 train_time:84009ms step_avg:37.94ms
step:2215/2330 train_time:84044ms step_avg:37.94ms
step:2216/2330 train_time:84085ms step_avg:37.94ms
step:2217/2330 train_time:84122ms step_avg:37.94ms
step:2218/2330 train_time:84163ms step_avg:37.95ms
step:2219/2330 train_time:84198ms step_avg:37.94ms
step:2220/2330 train_time:84239ms step_avg:37.95ms
step:2221/2330 train_time:84274ms step_avg:37.94ms
step:2222/2330 train_time:84314ms step_avg:37.95ms
step:2223/2330 train_time:84350ms step_avg:37.94ms
step:2224/2330 train_time:84391ms step_avg:37.95ms
step:2225/2330 train_time:84427ms step_avg:37.94ms
step:2226/2330 train_time:84468ms step_avg:37.95ms
step:2227/2330 train_time:84504ms step_avg:37.95ms
step:2228/2330 train_time:84544ms step_avg:37.95ms
step:2229/2330 train_time:84581ms step_avg:37.95ms
step:2230/2330 train_time:84622ms step_avg:37.95ms
step:2231/2330 train_time:84657ms step_avg:37.95ms
step:2232/2330 train_time:84698ms step_avg:37.95ms
step:2233/2330 train_time:84733ms step_avg:37.95ms
step:2234/2330 train_time:84774ms step_avg:37.95ms
step:2235/2330 train_time:84808ms step_avg:37.95ms
step:2236/2330 train_time:84850ms step_avg:37.95ms
step:2237/2330 train_time:84885ms step_avg:37.95ms
step:2238/2330 train_time:84926ms step_avg:37.95ms
step:2239/2330 train_time:84962ms step_avg:37.95ms
step:2240/2330 train_time:85002ms step_avg:37.95ms
step:2241/2330 train_time:85038ms step_avg:37.95ms
step:2242/2330 train_time:85079ms step_avg:37.95ms
step:2243/2330 train_time:85114ms step_avg:37.95ms
step:2244/2330 train_time:85154ms step_avg:37.95ms
step:2245/2330 train_time:85190ms step_avg:37.95ms
step:2246/2330 train_time:85232ms step_avg:37.95ms
step:2247/2330 train_time:85266ms step_avg:37.95ms
step:2248/2330 train_time:85308ms step_avg:37.95ms
step:2249/2330 train_time:85343ms step_avg:37.95ms
step:2250/2330 train_time:85384ms step_avg:37.95ms
step:2250/2330 val_loss:5.2811 train_time:85497ms step_avg:38.00ms
step:2251/2330 train_time:85510ms step_avg:37.99ms
step:2252/2330 train_time:85521ms step_avg:37.98ms
step:2253/2330 train_time:85530ms step_avg:37.96ms
step:2254/2330 train_time:85541ms step_avg:37.95ms
step:2255/2330 train_time:85574ms step_avg:37.95ms
step:2256/2330 train_time:85615ms step_avg:37.95ms
step:2257/2330 train_time:85650ms step_avg:37.95ms
step:2258/2330 train_time:85690ms step_avg:37.95ms
step:2259/2330 train_time:85724ms step_avg:37.95ms
step:2260/2330 train_time:85765ms step_avg:37.95ms
step:2261/2330 train_time:85802ms step_avg:37.95ms
step:2262/2330 train_time:85843ms step_avg:37.95ms
step:2263/2330 train_time:85886ms step_avg:37.95ms
step:2264/2330 train_time:85927ms step_avg:37.95ms
step:2265/2330 train_time:85963ms step_avg:37.95ms
step:2266/2330 train_time:86003ms step_avg:37.95ms
step:2267/2330 train_time:86039ms step_avg:37.95ms
step:2268/2330 train_time:86079ms step_avg:37.95ms
step:2269/2330 train_time:86115ms step_avg:37.95ms
step:2270/2330 train_time:86155ms step_avg:37.95ms
step:2271/2330 train_time:86191ms step_avg:37.95ms
step:2272/2330 train_time:86232ms step_avg:37.95ms
step:2273/2330 train_time:86267ms step_avg:37.95ms
step:2274/2330 train_time:86307ms step_avg:37.95ms
step:2275/2330 train_time:86341ms step_avg:37.95ms
step:2276/2330 train_time:86382ms step_avg:37.95ms
step:2277/2330 train_time:86417ms step_avg:37.95ms
step:2278/2330 train_time:86457ms step_avg:37.95ms
step:2279/2330 train_time:86493ms step_avg:37.95ms
step:2280/2330 train_time:86533ms step_avg:37.95ms
step:2281/2330 train_time:86568ms step_avg:37.95ms
step:2282/2330 train_time:86608ms step_avg:37.95ms
step:2283/2330 train_time:86643ms step_avg:37.95ms
step:2284/2330 train_time:86683ms step_avg:37.95ms
step:2285/2330 train_time:86718ms step_avg:37.95ms
step:2286/2330 train_time:86759ms step_avg:37.95ms
step:2287/2330 train_time:86798ms step_avg:37.95ms
step:2288/2330 train_time:86839ms step_avg:37.95ms
step:2289/2330 train_time:86876ms step_avg:37.95ms
step:2290/2330 train_time:86917ms step_avg:37.95ms
step:2291/2330 train_time:86952ms step_avg:37.95ms
step:2292/2330 train_time:86994ms step_avg:37.96ms
step:2293/2330 train_time:87028ms step_avg:37.95ms
step:2294/2330 train_time:87068ms step_avg:37.95ms
step:2295/2330 train_time:87104ms step_avg:37.95ms
step:2296/2330 train_time:87144ms step_avg:37.95ms
step:2297/2330 train_time:87179ms step_avg:37.95ms
step:2298/2330 train_time:87221ms step_avg:37.96ms
step:2299/2330 train_time:87256ms step_avg:37.95ms
step:2300/2330 train_time:87297ms step_avg:37.96ms
step:2301/2330 train_time:87333ms step_avg:37.95ms
step:2302/2330 train_time:87373ms step_avg:37.96ms
step:2303/2330 train_time:87408ms step_avg:37.95ms
step:2304/2330 train_time:87448ms step_avg:37.95ms
step:2305/2330 train_time:87483ms step_avg:37.95ms
step:2306/2330 train_time:87524ms step_avg:37.95ms
step:2307/2330 train_time:87560ms step_avg:37.95ms
step:2308/2330 train_time:87601ms step_avg:37.96ms
step:2309/2330 train_time:87636ms step_avg:37.95ms
step:2310/2330 train_time:87676ms step_avg:37.96ms
step:2311/2330 train_time:87711ms step_avg:37.95ms
step:2312/2330 train_time:87752ms step_avg:37.95ms
step:2313/2330 train_time:87788ms step_avg:37.95ms
step:2314/2330 train_time:87828ms step_avg:37.96ms
step:2315/2330 train_time:87864ms step_avg:37.95ms
step:2316/2330 train_time:87905ms step_avg:37.96ms
step:2317/2330 train_time:87940ms step_avg:37.95ms
step:2318/2330 train_time:87981ms step_avg:37.96ms
step:2319/2330 train_time:88017ms step_avg:37.95ms
step:2320/2330 train_time:88058ms step_avg:37.96ms
step:2321/2330 train_time:88093ms step_avg:37.95ms
step:2322/2330 train_time:88134ms step_avg:37.96ms
step:2323/2330 train_time:88169ms step_avg:37.95ms
step:2324/2330 train_time:88209ms step_avg:37.96ms
step:2325/2330 train_time:88245ms step_avg:37.95ms
step:2326/2330 train_time:88286ms step_avg:37.96ms
step:2327/2330 train_time:88320ms step_avg:37.95ms
step:2328/2330 train_time:88362ms step_avg:37.96ms
step:2329/2330 train_time:88397ms step_avg:37.96ms
step:2330/2330 train_time:88438ms step_avg:37.96ms
step:2330/2330 val_loss:5.2728 train_time:88550ms step_avg:38.00ms
peak memory allocated: 27165 MiB reserved: 38888 MiB
