import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "adam_lr5e-2"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-1, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.Adam(
    hidden_matrix_params + gate_params,
    lr=5e-2,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 00:43:30 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   33C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   32C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:79ms step_avg:79.33ms
step:2/2330 train_time:200ms step_avg:99.99ms
step:3/2330 train_time:218ms step_avg:72.71ms
step:4/2330 train_time:241ms step_avg:60.19ms
step:5/2330 train_time:296ms step_avg:59.18ms
step:6/2330 train_time:355ms step_avg:59.16ms
step:7/2330 train_time:410ms step_avg:58.57ms
step:8/2330 train_time:469ms step_avg:58.59ms
step:9/2330 train_time:524ms step_avg:58.18ms
step:10/2330 train_time:582ms step_avg:58.24ms
step:11/2330 train_time:637ms step_avg:57.94ms
step:12/2330 train_time:696ms step_avg:58.02ms
step:13/2330 train_time:751ms step_avg:57.77ms
step:14/2330 train_time:810ms step_avg:57.84ms
step:15/2330 train_time:865ms step_avg:57.65ms
step:16/2330 train_time:924ms step_avg:57.73ms
step:17/2330 train_time:979ms step_avg:57.57ms
step:18/2330 train_time:1037ms step_avg:57.62ms
step:19/2330 train_time:1092ms step_avg:57.49ms
step:20/2330 train_time:1152ms step_avg:57.58ms
step:21/2330 train_time:1207ms step_avg:57.47ms
step:22/2330 train_time:1267ms step_avg:57.60ms
step:23/2330 train_time:1322ms step_avg:57.50ms
step:24/2330 train_time:1382ms step_avg:57.57ms
step:25/2330 train_time:1437ms step_avg:57.47ms
step:26/2330 train_time:1496ms step_avg:57.54ms
step:27/2330 train_time:1551ms step_avg:57.45ms
step:28/2330 train_time:1610ms step_avg:57.51ms
step:29/2330 train_time:1666ms step_avg:57.44ms
step:30/2330 train_time:1725ms step_avg:57.49ms
step:31/2330 train_time:1780ms step_avg:57.42ms
step:32/2330 train_time:1839ms step_avg:57.46ms
step:33/2330 train_time:1894ms step_avg:57.39ms
step:34/2330 train_time:1953ms step_avg:57.44ms
step:35/2330 train_time:2008ms step_avg:57.37ms
step:36/2330 train_time:2067ms step_avg:57.42ms
step:37/2330 train_time:2122ms step_avg:57.35ms
step:38/2330 train_time:2181ms step_avg:57.40ms
step:39/2330 train_time:2236ms step_avg:57.34ms
step:40/2330 train_time:2296ms step_avg:57.41ms
step:41/2330 train_time:2352ms step_avg:57.36ms
step:42/2330 train_time:2411ms step_avg:57.42ms
step:43/2330 train_time:2467ms step_avg:57.36ms
step:44/2330 train_time:2527ms step_avg:57.42ms
step:45/2330 train_time:2582ms step_avg:57.37ms
step:46/2330 train_time:2642ms step_avg:57.44ms
step:47/2330 train_time:2697ms step_avg:57.39ms
step:48/2330 train_time:2757ms step_avg:57.43ms
step:49/2330 train_time:2812ms step_avg:57.39ms
step:50/2330 train_time:2872ms step_avg:57.44ms
step:51/2330 train_time:2927ms step_avg:57.40ms
step:52/2330 train_time:2986ms step_avg:57.43ms
step:53/2330 train_time:3042ms step_avg:57.39ms
step:54/2330 train_time:3101ms step_avg:57.43ms
step:55/2330 train_time:3156ms step_avg:57.39ms
step:56/2330 train_time:3216ms step_avg:57.42ms
step:57/2330 train_time:3271ms step_avg:57.39ms
step:58/2330 train_time:3331ms step_avg:57.43ms
step:59/2330 train_time:3387ms step_avg:57.40ms
step:60/2330 train_time:3445ms step_avg:57.42ms
step:61/2330 train_time:3502ms step_avg:57.41ms
step:62/2330 train_time:3562ms step_avg:57.44ms
step:63/2330 train_time:3617ms step_avg:57.41ms
step:64/2330 train_time:3676ms step_avg:57.44ms
step:65/2330 train_time:3732ms step_avg:57.41ms
step:66/2330 train_time:3792ms step_avg:57.45ms
step:67/2330 train_time:3847ms step_avg:57.42ms
step:68/2330 train_time:3908ms step_avg:57.48ms
step:69/2330 train_time:3964ms step_avg:57.45ms
step:70/2330 train_time:4023ms step_avg:57.47ms
step:71/2330 train_time:4078ms step_avg:57.44ms
step:72/2330 train_time:4139ms step_avg:57.49ms
step:73/2330 train_time:4195ms step_avg:57.46ms
step:74/2330 train_time:4254ms step_avg:57.49ms
step:75/2330 train_time:4310ms step_avg:57.46ms
step:76/2330 train_time:4370ms step_avg:57.50ms
step:77/2330 train_time:4425ms step_avg:57.47ms
step:78/2330 train_time:4485ms step_avg:57.49ms
step:79/2330 train_time:4541ms step_avg:57.48ms
step:80/2330 train_time:4600ms step_avg:57.50ms
step:81/2330 train_time:4656ms step_avg:57.48ms
step:82/2330 train_time:4715ms step_avg:57.50ms
step:83/2330 train_time:4770ms step_avg:57.47ms
step:84/2330 train_time:4830ms step_avg:57.50ms
step:85/2330 train_time:4886ms step_avg:57.48ms
step:86/2330 train_time:4946ms step_avg:57.51ms
step:87/2330 train_time:5001ms step_avg:57.49ms
step:88/2330 train_time:5061ms step_avg:57.51ms
step:89/2330 train_time:5116ms step_avg:57.48ms
step:90/2330 train_time:5176ms step_avg:57.51ms
step:91/2330 train_time:5232ms step_avg:57.49ms
step:92/2330 train_time:5291ms step_avg:57.51ms
step:93/2330 train_time:5347ms step_avg:57.49ms
step:94/2330 train_time:5407ms step_avg:57.52ms
step:95/2330 train_time:5463ms step_avg:57.50ms
step:96/2330 train_time:5522ms step_avg:57.52ms
step:97/2330 train_time:5578ms step_avg:57.50ms
step:98/2330 train_time:5637ms step_avg:57.52ms
step:99/2330 train_time:5692ms step_avg:57.50ms
step:100/2330 train_time:5752ms step_avg:57.52ms
step:101/2330 train_time:5808ms step_avg:57.51ms
step:102/2330 train_time:5868ms step_avg:57.53ms
step:103/2330 train_time:5924ms step_avg:57.51ms
step:104/2330 train_time:5983ms step_avg:57.53ms
step:105/2330 train_time:6039ms step_avg:57.51ms
step:106/2330 train_time:6098ms step_avg:57.53ms
step:107/2330 train_time:6154ms step_avg:57.51ms
step:108/2330 train_time:6214ms step_avg:57.54ms
step:109/2330 train_time:6270ms step_avg:57.53ms
step:110/2330 train_time:6330ms step_avg:57.54ms
step:111/2330 train_time:6385ms step_avg:57.52ms
step:112/2330 train_time:6445ms step_avg:57.54ms
step:113/2330 train_time:6501ms step_avg:57.53ms
step:114/2330 train_time:6560ms step_avg:57.55ms
step:115/2330 train_time:6616ms step_avg:57.53ms
step:116/2330 train_time:6676ms step_avg:57.55ms
step:117/2330 train_time:6732ms step_avg:57.54ms
step:118/2330 train_time:6792ms step_avg:57.56ms
step:119/2330 train_time:6847ms step_avg:57.54ms
step:120/2330 train_time:6908ms step_avg:57.57ms
step:121/2330 train_time:6964ms step_avg:57.55ms
step:122/2330 train_time:7023ms step_avg:57.57ms
step:123/2330 train_time:7079ms step_avg:57.55ms
step:124/2330 train_time:7139ms step_avg:57.57ms
step:125/2330 train_time:7195ms step_avg:57.56ms
step:126/2330 train_time:7255ms step_avg:57.58ms
step:127/2330 train_time:7310ms step_avg:57.56ms
step:128/2330 train_time:7371ms step_avg:57.59ms
step:129/2330 train_time:7427ms step_avg:57.57ms
step:130/2330 train_time:7486ms step_avg:57.58ms
step:131/2330 train_time:7542ms step_avg:57.57ms
step:132/2330 train_time:7601ms step_avg:57.58ms
step:133/2330 train_time:7657ms step_avg:57.57ms
step:134/2330 train_time:7718ms step_avg:57.60ms
step:135/2330 train_time:7773ms step_avg:57.58ms
step:136/2330 train_time:7834ms step_avg:57.60ms
step:137/2330 train_time:7889ms step_avg:57.59ms
step:138/2330 train_time:7951ms step_avg:57.61ms
step:139/2330 train_time:8006ms step_avg:57.60ms
step:140/2330 train_time:8067ms step_avg:57.62ms
step:141/2330 train_time:8123ms step_avg:57.61ms
step:142/2330 train_time:8182ms step_avg:57.62ms
step:143/2330 train_time:8237ms step_avg:57.60ms
step:144/2330 train_time:8298ms step_avg:57.63ms
step:145/2330 train_time:8353ms step_avg:57.61ms
step:146/2330 train_time:8414ms step_avg:57.63ms
step:147/2330 train_time:8469ms step_avg:57.62ms
step:148/2330 train_time:8530ms step_avg:57.64ms
step:149/2330 train_time:8586ms step_avg:57.62ms
step:150/2330 train_time:8646ms step_avg:57.64ms
step:151/2330 train_time:8703ms step_avg:57.63ms
step:152/2330 train_time:8762ms step_avg:57.65ms
step:153/2330 train_time:8818ms step_avg:57.63ms
step:154/2330 train_time:8878ms step_avg:57.65ms
step:155/2330 train_time:8934ms step_avg:57.64ms
step:156/2330 train_time:8994ms step_avg:57.65ms
step:157/2330 train_time:9050ms step_avg:57.64ms
step:158/2330 train_time:9110ms step_avg:57.66ms
step:159/2330 train_time:9165ms step_avg:57.64ms
step:160/2330 train_time:9225ms step_avg:57.66ms
step:161/2330 train_time:9281ms step_avg:57.64ms
step:162/2330 train_time:9341ms step_avg:57.66ms
step:163/2330 train_time:9397ms step_avg:57.65ms
step:164/2330 train_time:9457ms step_avg:57.67ms
step:165/2330 train_time:9512ms step_avg:57.65ms
step:166/2330 train_time:9573ms step_avg:57.67ms
step:167/2330 train_time:9628ms step_avg:57.65ms
step:168/2330 train_time:9690ms step_avg:57.68ms
step:169/2330 train_time:9746ms step_avg:57.67ms
step:170/2330 train_time:9806ms step_avg:57.68ms
step:171/2330 train_time:9863ms step_avg:57.68ms
step:172/2330 train_time:9923ms step_avg:57.69ms
step:173/2330 train_time:9978ms step_avg:57.68ms
step:174/2330 train_time:10038ms step_avg:57.69ms
step:175/2330 train_time:10094ms step_avg:57.68ms
step:176/2330 train_time:10154ms step_avg:57.69ms
step:177/2330 train_time:10210ms step_avg:57.69ms
step:178/2330 train_time:10271ms step_avg:57.70ms
step:179/2330 train_time:10327ms step_avg:57.69ms
step:180/2330 train_time:10387ms step_avg:57.70ms
step:181/2330 train_time:10442ms step_avg:57.69ms
step:182/2330 train_time:10502ms step_avg:57.70ms
step:183/2330 train_time:10558ms step_avg:57.69ms
step:184/2330 train_time:10618ms step_avg:57.71ms
step:185/2330 train_time:10673ms step_avg:57.69ms
step:186/2330 train_time:10734ms step_avg:57.71ms
step:187/2330 train_time:10790ms step_avg:57.70ms
step:188/2330 train_time:10851ms step_avg:57.72ms
step:189/2330 train_time:10907ms step_avg:57.71ms
step:190/2330 train_time:10968ms step_avg:57.72ms
step:191/2330 train_time:11023ms step_avg:57.71ms
step:192/2330 train_time:11083ms step_avg:57.72ms
step:193/2330 train_time:11138ms step_avg:57.71ms
step:194/2330 train_time:11198ms step_avg:57.72ms
step:195/2330 train_time:11254ms step_avg:57.71ms
step:196/2330 train_time:11314ms step_avg:57.72ms
step:197/2330 train_time:11369ms step_avg:57.71ms
step:198/2330 train_time:11431ms step_avg:57.73ms
step:199/2330 train_time:11487ms step_avg:57.72ms
step:200/2330 train_time:11548ms step_avg:57.74ms
step:201/2330 train_time:11603ms step_avg:57.73ms
step:202/2330 train_time:11663ms step_avg:57.74ms
step:203/2330 train_time:11719ms step_avg:57.73ms
step:204/2330 train_time:11780ms step_avg:57.74ms
step:205/2330 train_time:11835ms step_avg:57.73ms
step:206/2330 train_time:11896ms step_avg:57.75ms
step:207/2330 train_time:11951ms step_avg:57.74ms
step:208/2330 train_time:12012ms step_avg:57.75ms
step:209/2330 train_time:12068ms step_avg:57.74ms
step:210/2330 train_time:12129ms step_avg:57.76ms
step:211/2330 train_time:12185ms step_avg:57.75ms
step:212/2330 train_time:12245ms step_avg:57.76ms
step:213/2330 train_time:12300ms step_avg:57.75ms
step:214/2330 train_time:12361ms step_avg:57.76ms
step:215/2330 train_time:12417ms step_avg:57.75ms
step:216/2330 train_time:12477ms step_avg:57.76ms
step:217/2330 train_time:12533ms step_avg:57.75ms
step:218/2330 train_time:12594ms step_avg:57.77ms
step:219/2330 train_time:12649ms step_avg:57.76ms
step:220/2330 train_time:12710ms step_avg:57.77ms
step:221/2330 train_time:12766ms step_avg:57.77ms
step:222/2330 train_time:12826ms step_avg:57.77ms
step:223/2330 train_time:12883ms step_avg:57.77ms
step:224/2330 train_time:12942ms step_avg:57.78ms
step:225/2330 train_time:12998ms step_avg:57.77ms
step:226/2330 train_time:13059ms step_avg:57.78ms
step:227/2330 train_time:13115ms step_avg:57.77ms
step:228/2330 train_time:13175ms step_avg:57.78ms
step:229/2330 train_time:13230ms step_avg:57.77ms
step:230/2330 train_time:13292ms step_avg:57.79ms
step:231/2330 train_time:13347ms step_avg:57.78ms
step:232/2330 train_time:13408ms step_avg:57.79ms
step:233/2330 train_time:13464ms step_avg:57.79ms
step:234/2330 train_time:13524ms step_avg:57.79ms
step:235/2330 train_time:13579ms step_avg:57.78ms
step:236/2330 train_time:13639ms step_avg:57.79ms
step:237/2330 train_time:13695ms step_avg:57.78ms
step:238/2330 train_time:13756ms step_avg:57.80ms
step:239/2330 train_time:13811ms step_avg:57.79ms
step:240/2330 train_time:13873ms step_avg:57.80ms
step:241/2330 train_time:13928ms step_avg:57.79ms
step:242/2330 train_time:13989ms step_avg:57.81ms
step:243/2330 train_time:14045ms step_avg:57.80ms
step:244/2330 train_time:14105ms step_avg:57.81ms
step:245/2330 train_time:14161ms step_avg:57.80ms
step:246/2330 train_time:14220ms step_avg:57.81ms
step:247/2330 train_time:14276ms step_avg:57.80ms
step:248/2330 train_time:14337ms step_avg:57.81ms
step:249/2330 train_time:14393ms step_avg:57.80ms
step:250/2330 train_time:14454ms step_avg:57.82ms
step:250/2330 val_loss:6.3291 train_time:14531ms step_avg:58.12ms
step:251/2330 train_time:14550ms step_avg:57.97ms
step:252/2330 train_time:14573ms step_avg:57.83ms
step:253/2330 train_time:14628ms step_avg:57.82ms
step:254/2330 train_time:14692ms step_avg:57.84ms
step:255/2330 train_time:14747ms step_avg:57.83ms
step:256/2330 train_time:14809ms step_avg:57.85ms
step:257/2330 train_time:14865ms step_avg:57.84ms
step:258/2330 train_time:14925ms step_avg:57.85ms
step:259/2330 train_time:14981ms step_avg:57.84ms
step:260/2330 train_time:15041ms step_avg:57.85ms
step:261/2330 train_time:15096ms step_avg:57.84ms
step:262/2330 train_time:15156ms step_avg:57.85ms
step:263/2330 train_time:15212ms step_avg:57.84ms
step:264/2330 train_time:15271ms step_avg:57.84ms
step:265/2330 train_time:15326ms step_avg:57.84ms
step:266/2330 train_time:15386ms step_avg:57.84ms
step:267/2330 train_time:15442ms step_avg:57.84ms
step:268/2330 train_time:15502ms step_avg:57.84ms
step:269/2330 train_time:15558ms step_avg:57.84ms
step:270/2330 train_time:15619ms step_avg:57.85ms
step:271/2330 train_time:15675ms step_avg:57.84ms
step:272/2330 train_time:15737ms step_avg:57.86ms
step:273/2330 train_time:15793ms step_avg:57.85ms
step:274/2330 train_time:15854ms step_avg:57.86ms
step:275/2330 train_time:15910ms step_avg:57.85ms
step:276/2330 train_time:15970ms step_avg:57.86ms
step:277/2330 train_time:16026ms step_avg:57.85ms
step:278/2330 train_time:16086ms step_avg:57.86ms
step:279/2330 train_time:16142ms step_avg:57.86ms
step:280/2330 train_time:16201ms step_avg:57.86ms
step:281/2330 train_time:16256ms step_avg:57.85ms
step:282/2330 train_time:16317ms step_avg:57.86ms
step:283/2330 train_time:16373ms step_avg:57.85ms
step:284/2330 train_time:16433ms step_avg:57.86ms
step:285/2330 train_time:16489ms step_avg:57.85ms
step:286/2330 train_time:16549ms step_avg:57.86ms
step:287/2330 train_time:16605ms step_avg:57.86ms
step:288/2330 train_time:16666ms step_avg:57.87ms
step:289/2330 train_time:16722ms step_avg:57.86ms
step:290/2330 train_time:16783ms step_avg:57.87ms
step:291/2330 train_time:16838ms step_avg:57.86ms
step:292/2330 train_time:16901ms step_avg:57.88ms
step:293/2330 train_time:16957ms step_avg:57.87ms
step:294/2330 train_time:17018ms step_avg:57.88ms
step:295/2330 train_time:17074ms step_avg:57.88ms
step:296/2330 train_time:17134ms step_avg:57.89ms
step:297/2330 train_time:17191ms step_avg:57.88ms
step:298/2330 train_time:17250ms step_avg:57.89ms
step:299/2330 train_time:17305ms step_avg:57.88ms
step:300/2330 train_time:17365ms step_avg:57.88ms
step:301/2330 train_time:17421ms step_avg:57.88ms
step:302/2330 train_time:17481ms step_avg:57.88ms
step:303/2330 train_time:17537ms step_avg:57.88ms
step:304/2330 train_time:17598ms step_avg:57.89ms
step:305/2330 train_time:17654ms step_avg:57.88ms
step:306/2330 train_time:17714ms step_avg:57.89ms
step:307/2330 train_time:17770ms step_avg:57.88ms
step:308/2330 train_time:17830ms step_avg:57.89ms
step:309/2330 train_time:17885ms step_avg:57.88ms
step:310/2330 train_time:17946ms step_avg:57.89ms
step:311/2330 train_time:18001ms step_avg:57.88ms
step:312/2330 train_time:18062ms step_avg:57.89ms
step:313/2330 train_time:18117ms step_avg:57.88ms
step:314/2330 train_time:18178ms step_avg:57.89ms
step:315/2330 train_time:18234ms step_avg:57.89ms
step:316/2330 train_time:18295ms step_avg:57.89ms
step:317/2330 train_time:18352ms step_avg:57.89ms
step:318/2330 train_time:18411ms step_avg:57.90ms
step:319/2330 train_time:18468ms step_avg:57.89ms
step:320/2330 train_time:18527ms step_avg:57.90ms
step:321/2330 train_time:18583ms step_avg:57.89ms
step:322/2330 train_time:18643ms step_avg:57.90ms
step:323/2330 train_time:18698ms step_avg:57.89ms
step:324/2330 train_time:18759ms step_avg:57.90ms
step:325/2330 train_time:18814ms step_avg:57.89ms
step:326/2330 train_time:18876ms step_avg:57.90ms
step:327/2330 train_time:18932ms step_avg:57.90ms
step:328/2330 train_time:18993ms step_avg:57.90ms
step:329/2330 train_time:19049ms step_avg:57.90ms
step:330/2330 train_time:19109ms step_avg:57.91ms
step:331/2330 train_time:19165ms step_avg:57.90ms
step:332/2330 train_time:19225ms step_avg:57.91ms
step:333/2330 train_time:19280ms step_avg:57.90ms
step:334/2330 train_time:19342ms step_avg:57.91ms
step:335/2330 train_time:19397ms step_avg:57.90ms
step:336/2330 train_time:19458ms step_avg:57.91ms
step:337/2330 train_time:19514ms step_avg:57.90ms
step:338/2330 train_time:19575ms step_avg:57.91ms
step:339/2330 train_time:19630ms step_avg:57.91ms
step:340/2330 train_time:19690ms step_avg:57.91ms
step:341/2330 train_time:19746ms step_avg:57.91ms
step:342/2330 train_time:19806ms step_avg:57.91ms
step:343/2330 train_time:19862ms step_avg:57.91ms
step:344/2330 train_time:19923ms step_avg:57.91ms
step:345/2330 train_time:19978ms step_avg:57.91ms
step:346/2330 train_time:20039ms step_avg:57.92ms
step:347/2330 train_time:20095ms step_avg:57.91ms
step:348/2330 train_time:20155ms step_avg:57.92ms
step:349/2330 train_time:20211ms step_avg:57.91ms
step:350/2330 train_time:20271ms step_avg:57.92ms
step:351/2330 train_time:20326ms step_avg:57.91ms
step:352/2330 train_time:20387ms step_avg:57.92ms
step:353/2330 train_time:20442ms step_avg:57.91ms
step:354/2330 train_time:20503ms step_avg:57.92ms
step:355/2330 train_time:20559ms step_avg:57.91ms
step:356/2330 train_time:20619ms step_avg:57.92ms
step:357/2330 train_time:20675ms step_avg:57.91ms
step:358/2330 train_time:20736ms step_avg:57.92ms
step:359/2330 train_time:20792ms step_avg:57.92ms
step:360/2330 train_time:20853ms step_avg:57.92ms
step:361/2330 train_time:20909ms step_avg:57.92ms
step:362/2330 train_time:20969ms step_avg:57.93ms
step:363/2330 train_time:21025ms step_avg:57.92ms
step:364/2330 train_time:21085ms step_avg:57.93ms
step:365/2330 train_time:21141ms step_avg:57.92ms
step:366/2330 train_time:21201ms step_avg:57.93ms
step:367/2330 train_time:21257ms step_avg:57.92ms
step:368/2330 train_time:21318ms step_avg:57.93ms
step:369/2330 train_time:21374ms step_avg:57.92ms
step:370/2330 train_time:21435ms step_avg:57.93ms
step:371/2330 train_time:21492ms step_avg:57.93ms
step:372/2330 train_time:21552ms step_avg:57.93ms
step:373/2330 train_time:21608ms step_avg:57.93ms
step:374/2330 train_time:21668ms step_avg:57.94ms
step:375/2330 train_time:21724ms step_avg:57.93ms
step:376/2330 train_time:21783ms step_avg:57.93ms
step:377/2330 train_time:21839ms step_avg:57.93ms
step:378/2330 train_time:21900ms step_avg:57.94ms
step:379/2330 train_time:21956ms step_avg:57.93ms
step:380/2330 train_time:22017ms step_avg:57.94ms
step:381/2330 train_time:22072ms step_avg:57.93ms
step:382/2330 train_time:22133ms step_avg:57.94ms
step:383/2330 train_time:22189ms step_avg:57.93ms
step:384/2330 train_time:22249ms step_avg:57.94ms
step:385/2330 train_time:22305ms step_avg:57.93ms
step:386/2330 train_time:22366ms step_avg:57.94ms
step:387/2330 train_time:22422ms step_avg:57.94ms
step:388/2330 train_time:22483ms step_avg:57.95ms
step:389/2330 train_time:22539ms step_avg:57.94ms
step:390/2330 train_time:22599ms step_avg:57.95ms
step:391/2330 train_time:22655ms step_avg:57.94ms
step:392/2330 train_time:22716ms step_avg:57.95ms
step:393/2330 train_time:22771ms step_avg:57.94ms
step:394/2330 train_time:22832ms step_avg:57.95ms
step:395/2330 train_time:22888ms step_avg:57.95ms
step:396/2330 train_time:22948ms step_avg:57.95ms
step:397/2330 train_time:23005ms step_avg:57.95ms
step:398/2330 train_time:23064ms step_avg:57.95ms
step:399/2330 train_time:23120ms step_avg:57.95ms
step:400/2330 train_time:23181ms step_avg:57.95ms
step:401/2330 train_time:23237ms step_avg:57.95ms
step:402/2330 train_time:23298ms step_avg:57.95ms
step:403/2330 train_time:23353ms step_avg:57.95ms
step:404/2330 train_time:23415ms step_avg:57.96ms
step:405/2330 train_time:23471ms step_avg:57.95ms
step:406/2330 train_time:23532ms step_avg:57.96ms
step:407/2330 train_time:23588ms step_avg:57.96ms
step:408/2330 train_time:23648ms step_avg:57.96ms
step:409/2330 train_time:23704ms step_avg:57.96ms
step:410/2330 train_time:23764ms step_avg:57.96ms
step:411/2330 train_time:23820ms step_avg:57.96ms
step:412/2330 train_time:23881ms step_avg:57.96ms
step:413/2330 train_time:23937ms step_avg:57.96ms
step:414/2330 train_time:23998ms step_avg:57.97ms
step:415/2330 train_time:24054ms step_avg:57.96ms
step:416/2330 train_time:24116ms step_avg:57.97ms
step:417/2330 train_time:24172ms step_avg:57.97ms
step:418/2330 train_time:24232ms step_avg:57.97ms
step:419/2330 train_time:24288ms step_avg:57.97ms
step:420/2330 train_time:24348ms step_avg:57.97ms
step:421/2330 train_time:24404ms step_avg:57.97ms
step:422/2330 train_time:24464ms step_avg:57.97ms
step:423/2330 train_time:24520ms step_avg:57.97ms
step:424/2330 train_time:24581ms step_avg:57.97ms
step:425/2330 train_time:24637ms step_avg:57.97ms
step:426/2330 train_time:24697ms step_avg:57.98ms
step:427/2330 train_time:24753ms step_avg:57.97ms
step:428/2330 train_time:24814ms step_avg:57.98ms
step:429/2330 train_time:24870ms step_avg:57.97ms
step:430/2330 train_time:24931ms step_avg:57.98ms
step:431/2330 train_time:24987ms step_avg:57.97ms
step:432/2330 train_time:25047ms step_avg:57.98ms
step:433/2330 train_time:25104ms step_avg:57.98ms
step:434/2330 train_time:25164ms step_avg:57.98ms
step:435/2330 train_time:25219ms step_avg:57.98ms
step:436/2330 train_time:25280ms step_avg:57.98ms
step:437/2330 train_time:25336ms step_avg:57.98ms
step:438/2330 train_time:25397ms step_avg:57.98ms
step:439/2330 train_time:25453ms step_avg:57.98ms
step:440/2330 train_time:25514ms step_avg:57.99ms
step:441/2330 train_time:25570ms step_avg:57.98ms
step:442/2330 train_time:25630ms step_avg:57.99ms
step:443/2330 train_time:25686ms step_avg:57.98ms
step:444/2330 train_time:25746ms step_avg:57.99ms
step:445/2330 train_time:25802ms step_avg:57.98ms
step:446/2330 train_time:25862ms step_avg:57.99ms
step:447/2330 train_time:25918ms step_avg:57.98ms
step:448/2330 train_time:25980ms step_avg:57.99ms
step:449/2330 train_time:26036ms step_avg:57.99ms
step:450/2330 train_time:26098ms step_avg:58.00ms
step:451/2330 train_time:26154ms step_avg:57.99ms
step:452/2330 train_time:26215ms step_avg:58.00ms
step:453/2330 train_time:26271ms step_avg:57.99ms
step:454/2330 train_time:26331ms step_avg:58.00ms
step:455/2330 train_time:26387ms step_avg:57.99ms
step:456/2330 train_time:26447ms step_avg:58.00ms
step:457/2330 train_time:26503ms step_avg:57.99ms
step:458/2330 train_time:26564ms step_avg:58.00ms
step:459/2330 train_time:26619ms step_avg:57.99ms
step:460/2330 train_time:26680ms step_avg:58.00ms
step:461/2330 train_time:26736ms step_avg:58.00ms
step:462/2330 train_time:26797ms step_avg:58.00ms
step:463/2330 train_time:26853ms step_avg:58.00ms
step:464/2330 train_time:26915ms step_avg:58.01ms
step:465/2330 train_time:26971ms step_avg:58.00ms
step:466/2330 train_time:27033ms step_avg:58.01ms
step:467/2330 train_time:27089ms step_avg:58.01ms
step:468/2330 train_time:27150ms step_avg:58.01ms
step:469/2330 train_time:27206ms step_avg:58.01ms
step:470/2330 train_time:27266ms step_avg:58.01ms
step:471/2330 train_time:27322ms step_avg:58.01ms
step:472/2330 train_time:27381ms step_avg:58.01ms
step:473/2330 train_time:27438ms step_avg:58.01ms
step:474/2330 train_time:27498ms step_avg:58.01ms
step:475/2330 train_time:27554ms step_avg:58.01ms
step:476/2330 train_time:27615ms step_avg:58.02ms
step:477/2330 train_time:27671ms step_avg:58.01ms
step:478/2330 train_time:27733ms step_avg:58.02ms
step:479/2330 train_time:27789ms step_avg:58.02ms
step:480/2330 train_time:27849ms step_avg:58.02ms
step:481/2330 train_time:27905ms step_avg:58.01ms
step:482/2330 train_time:27965ms step_avg:58.02ms
step:483/2330 train_time:28020ms step_avg:58.01ms
step:484/2330 train_time:28082ms step_avg:58.02ms
step:485/2330 train_time:28138ms step_avg:58.02ms
step:486/2330 train_time:28199ms step_avg:58.02ms
step:487/2330 train_time:28255ms step_avg:58.02ms
step:488/2330 train_time:28317ms step_avg:58.03ms
step:489/2330 train_time:28373ms step_avg:58.02ms
step:490/2330 train_time:28433ms step_avg:58.03ms
step:491/2330 train_time:28489ms step_avg:58.02ms
step:492/2330 train_time:28549ms step_avg:58.03ms
step:493/2330 train_time:28605ms step_avg:58.02ms
step:494/2330 train_time:28665ms step_avg:58.03ms
step:495/2330 train_time:28721ms step_avg:58.02ms
step:496/2330 train_time:28782ms step_avg:58.03ms
step:497/2330 train_time:28839ms step_avg:58.03ms
step:498/2330 train_time:28900ms step_avg:58.03ms
step:499/2330 train_time:28956ms step_avg:58.03ms
step:500/2330 train_time:29017ms step_avg:58.03ms
step:500/2330 val_loss:5.3597 train_time:29094ms step_avg:58.19ms
step:501/2330 train_time:29113ms step_avg:58.11ms
step:502/2330 train_time:29136ms step_avg:58.04ms
step:503/2330 train_time:29192ms step_avg:58.04ms
step:504/2330 train_time:29257ms step_avg:58.05ms
step:505/2330 train_time:29312ms step_avg:58.04ms
step:506/2330 train_time:29377ms step_avg:58.06ms
step:507/2330 train_time:29433ms step_avg:58.05ms
step:508/2330 train_time:29494ms step_avg:58.06ms
step:509/2330 train_time:29550ms step_avg:58.05ms
step:510/2330 train_time:29611ms step_avg:58.06ms
step:511/2330 train_time:29667ms step_avg:58.06ms
step:512/2330 train_time:29727ms step_avg:58.06ms
step:513/2330 train_time:29783ms step_avg:58.06ms
step:514/2330 train_time:29842ms step_avg:58.06ms
step:515/2330 train_time:29898ms step_avg:58.05ms
step:516/2330 train_time:29957ms step_avg:58.06ms
step:517/2330 train_time:30013ms step_avg:58.05ms
step:518/2330 train_time:30073ms step_avg:58.06ms
step:519/2330 train_time:30128ms step_avg:58.05ms
step:520/2330 train_time:30190ms step_avg:58.06ms
step:521/2330 train_time:30247ms step_avg:58.06ms
step:522/2330 train_time:30309ms step_avg:58.06ms
step:523/2330 train_time:30365ms step_avg:58.06ms
step:524/2330 train_time:30426ms step_avg:58.06ms
step:525/2330 train_time:30482ms step_avg:58.06ms
step:526/2330 train_time:30543ms step_avg:58.07ms
step:527/2330 train_time:30599ms step_avg:58.06ms
step:528/2330 train_time:30659ms step_avg:58.07ms
step:529/2330 train_time:30715ms step_avg:58.06ms
step:530/2330 train_time:30775ms step_avg:58.07ms
step:531/2330 train_time:30830ms step_avg:58.06ms
step:532/2330 train_time:30891ms step_avg:58.07ms
step:533/2330 train_time:30946ms step_avg:58.06ms
step:534/2330 train_time:31007ms step_avg:58.06ms
step:535/2330 train_time:31063ms step_avg:58.06ms
step:536/2330 train_time:31124ms step_avg:58.07ms
step:537/2330 train_time:31180ms step_avg:58.06ms
step:538/2330 train_time:31241ms step_avg:58.07ms
step:539/2330 train_time:31297ms step_avg:58.06ms
step:540/2330 train_time:31358ms step_avg:58.07ms
step:541/2330 train_time:31414ms step_avg:58.07ms
step:542/2330 train_time:31476ms step_avg:58.07ms
step:543/2330 train_time:31532ms step_avg:58.07ms
step:544/2330 train_time:31592ms step_avg:58.07ms
step:545/2330 train_time:31649ms step_avg:58.07ms
step:546/2330 train_time:31710ms step_avg:58.08ms
step:547/2330 train_time:31766ms step_avg:58.07ms
step:548/2330 train_time:31826ms step_avg:58.08ms
step:549/2330 train_time:31882ms step_avg:58.07ms
step:550/2330 train_time:31941ms step_avg:58.08ms
step:551/2330 train_time:31997ms step_avg:58.07ms
step:552/2330 train_time:32057ms step_avg:58.07ms
step:553/2330 train_time:32113ms step_avg:58.07ms
step:554/2330 train_time:32174ms step_avg:58.08ms
step:555/2330 train_time:32230ms step_avg:58.07ms
step:556/2330 train_time:32290ms step_avg:58.08ms
step:557/2330 train_time:32347ms step_avg:58.07ms
step:558/2330 train_time:32409ms step_avg:58.08ms
step:559/2330 train_time:32465ms step_avg:58.08ms
step:560/2330 train_time:32526ms step_avg:58.08ms
step:561/2330 train_time:32582ms step_avg:58.08ms
step:562/2330 train_time:32643ms step_avg:58.08ms
step:563/2330 train_time:32700ms step_avg:58.08ms
step:564/2330 train_time:32759ms step_avg:58.08ms
step:565/2330 train_time:32815ms step_avg:58.08ms
step:566/2330 train_time:32876ms step_avg:58.08ms
step:567/2330 train_time:32931ms step_avg:58.08ms
step:568/2330 train_time:32993ms step_avg:58.09ms
step:569/2330 train_time:33049ms step_avg:58.08ms
step:570/2330 train_time:33109ms step_avg:58.09ms
step:571/2330 train_time:33165ms step_avg:58.08ms
step:572/2330 train_time:33227ms step_avg:58.09ms
step:573/2330 train_time:33282ms step_avg:58.08ms
step:574/2330 train_time:33343ms step_avg:58.09ms
step:575/2330 train_time:33399ms step_avg:58.08ms
step:576/2330 train_time:33460ms step_avg:58.09ms
step:577/2330 train_time:33515ms step_avg:58.09ms
step:578/2330 train_time:33576ms step_avg:58.09ms
step:579/2330 train_time:33632ms step_avg:58.09ms
step:580/2330 train_time:33692ms step_avg:58.09ms
step:581/2330 train_time:33748ms step_avg:58.09ms
step:582/2330 train_time:33810ms step_avg:58.09ms
step:583/2330 train_time:33865ms step_avg:58.09ms
step:584/2330 train_time:33927ms step_avg:58.09ms
step:585/2330 train_time:33983ms step_avg:58.09ms
step:586/2330 train_time:34043ms step_avg:58.09ms
step:587/2330 train_time:34099ms step_avg:58.09ms
step:588/2330 train_time:34159ms step_avg:58.09ms
step:589/2330 train_time:34215ms step_avg:58.09ms
step:590/2330 train_time:34275ms step_avg:58.09ms
step:591/2330 train_time:34331ms step_avg:58.09ms
step:592/2330 train_time:34392ms step_avg:58.10ms
step:593/2330 train_time:34448ms step_avg:58.09ms
step:594/2330 train_time:34509ms step_avg:58.10ms
step:595/2330 train_time:34565ms step_avg:58.09ms
step:596/2330 train_time:34626ms step_avg:58.10ms
step:597/2330 train_time:34683ms step_avg:58.09ms
step:598/2330 train_time:34743ms step_avg:58.10ms
step:599/2330 train_time:34799ms step_avg:58.10ms
step:600/2330 train_time:34859ms step_avg:58.10ms
step:601/2330 train_time:34915ms step_avg:58.09ms
step:602/2330 train_time:34976ms step_avg:58.10ms
step:603/2330 train_time:35032ms step_avg:58.10ms
step:604/2330 train_time:35093ms step_avg:58.10ms
step:605/2330 train_time:35148ms step_avg:58.10ms
step:606/2330 train_time:35210ms step_avg:58.10ms
step:607/2330 train_time:35266ms step_avg:58.10ms
step:608/2330 train_time:35327ms step_avg:58.10ms
step:609/2330 train_time:35383ms step_avg:58.10ms
step:610/2330 train_time:35444ms step_avg:58.10ms
step:611/2330 train_time:35501ms step_avg:58.10ms
step:612/2330 train_time:35560ms step_avg:58.10ms
step:613/2330 train_time:35616ms step_avg:58.10ms
step:614/2330 train_time:35676ms step_avg:58.10ms
step:615/2330 train_time:35731ms step_avg:58.10ms
step:616/2330 train_time:35793ms step_avg:58.10ms
step:617/2330 train_time:35849ms step_avg:58.10ms
step:618/2330 train_time:35910ms step_avg:58.11ms
step:619/2330 train_time:35966ms step_avg:58.10ms
step:620/2330 train_time:36028ms step_avg:58.11ms
step:621/2330 train_time:36085ms step_avg:58.11ms
step:622/2330 train_time:36145ms step_avg:58.11ms
step:623/2330 train_time:36201ms step_avg:58.11ms
step:624/2330 train_time:36261ms step_avg:58.11ms
step:625/2330 train_time:36317ms step_avg:58.11ms
step:626/2330 train_time:36377ms step_avg:58.11ms
step:627/2330 train_time:36433ms step_avg:58.11ms
step:628/2330 train_time:36494ms step_avg:58.11ms
step:629/2330 train_time:36550ms step_avg:58.11ms
step:630/2330 train_time:36611ms step_avg:58.11ms
step:631/2330 train_time:36667ms step_avg:58.11ms
step:632/2330 train_time:36729ms step_avg:58.12ms
step:633/2330 train_time:36785ms step_avg:58.11ms
step:634/2330 train_time:36846ms step_avg:58.12ms
step:635/2330 train_time:36902ms step_avg:58.11ms
step:636/2330 train_time:36962ms step_avg:58.12ms
step:637/2330 train_time:37018ms step_avg:58.11ms
step:638/2330 train_time:37078ms step_avg:58.12ms
step:639/2330 train_time:37134ms step_avg:58.11ms
step:640/2330 train_time:37194ms step_avg:58.12ms
step:641/2330 train_time:37250ms step_avg:58.11ms
step:642/2330 train_time:37311ms step_avg:58.12ms
step:643/2330 train_time:37367ms step_avg:58.11ms
step:644/2330 train_time:37429ms step_avg:58.12ms
step:645/2330 train_time:37485ms step_avg:58.12ms
step:646/2330 train_time:37545ms step_avg:58.12ms
step:647/2330 train_time:37602ms step_avg:58.12ms
step:648/2330 train_time:37662ms step_avg:58.12ms
step:649/2330 train_time:37718ms step_avg:58.12ms
step:650/2330 train_time:37778ms step_avg:58.12ms
step:651/2330 train_time:37835ms step_avg:58.12ms
step:652/2330 train_time:37895ms step_avg:58.12ms
step:653/2330 train_time:37951ms step_avg:58.12ms
step:654/2330 train_time:38012ms step_avg:58.12ms
step:655/2330 train_time:38068ms step_avg:58.12ms
step:656/2330 train_time:38129ms step_avg:58.12ms
step:657/2330 train_time:38185ms step_avg:58.12ms
step:658/2330 train_time:38246ms step_avg:58.12ms
step:659/2330 train_time:38303ms step_avg:58.12ms
step:660/2330 train_time:38363ms step_avg:58.13ms
step:661/2330 train_time:38420ms step_avg:58.12ms
step:662/2330 train_time:38479ms step_avg:58.13ms
step:663/2330 train_time:38535ms step_avg:58.12ms
step:664/2330 train_time:38596ms step_avg:58.13ms
step:665/2330 train_time:38652ms step_avg:58.12ms
step:666/2330 train_time:38713ms step_avg:58.13ms
step:667/2330 train_time:38769ms step_avg:58.12ms
step:668/2330 train_time:38830ms step_avg:58.13ms
step:669/2330 train_time:38886ms step_avg:58.13ms
step:670/2330 train_time:38947ms step_avg:58.13ms
step:671/2330 train_time:39003ms step_avg:58.13ms
step:672/2330 train_time:39064ms step_avg:58.13ms
step:673/2330 train_time:39121ms step_avg:58.13ms
step:674/2330 train_time:39181ms step_avg:58.13ms
step:675/2330 train_time:39237ms step_avg:58.13ms
step:676/2330 train_time:39297ms step_avg:58.13ms
step:677/2330 train_time:39353ms step_avg:58.13ms
step:678/2330 train_time:39415ms step_avg:58.13ms
step:679/2330 train_time:39470ms step_avg:58.13ms
step:680/2330 train_time:39532ms step_avg:58.14ms
step:681/2330 train_time:39587ms step_avg:58.13ms
step:682/2330 train_time:39648ms step_avg:58.13ms
step:683/2330 train_time:39704ms step_avg:58.13ms
step:684/2330 train_time:39765ms step_avg:58.14ms
step:685/2330 train_time:39821ms step_avg:58.13ms
step:686/2330 train_time:39881ms step_avg:58.14ms
step:687/2330 train_time:39937ms step_avg:58.13ms
step:688/2330 train_time:39998ms step_avg:58.14ms
step:689/2330 train_time:40054ms step_avg:58.13ms
step:690/2330 train_time:40114ms step_avg:58.14ms
step:691/2330 train_time:40170ms step_avg:58.13ms
step:692/2330 train_time:40231ms step_avg:58.14ms
step:693/2330 train_time:40288ms step_avg:58.13ms
step:694/2330 train_time:40349ms step_avg:58.14ms
step:695/2330 train_time:40405ms step_avg:58.14ms
step:696/2330 train_time:40466ms step_avg:58.14ms
step:697/2330 train_time:40522ms step_avg:58.14ms
step:698/2330 train_time:40582ms step_avg:58.14ms
step:699/2330 train_time:40638ms step_avg:58.14ms
step:700/2330 train_time:40699ms step_avg:58.14ms
step:701/2330 train_time:40754ms step_avg:58.14ms
step:702/2330 train_time:40814ms step_avg:58.14ms
step:703/2330 train_time:40870ms step_avg:58.14ms
step:704/2330 train_time:40931ms step_avg:58.14ms
step:705/2330 train_time:40987ms step_avg:58.14ms
step:706/2330 train_time:41048ms step_avg:58.14ms
step:707/2330 train_time:41104ms step_avg:58.14ms
step:708/2330 train_time:41164ms step_avg:58.14ms
step:709/2330 train_time:41220ms step_avg:58.14ms
step:710/2330 train_time:41281ms step_avg:58.14ms
step:711/2330 train_time:41337ms step_avg:58.14ms
step:712/2330 train_time:41397ms step_avg:58.14ms
step:713/2330 train_time:41453ms step_avg:58.14ms
step:714/2330 train_time:41514ms step_avg:58.14ms
step:715/2330 train_time:41570ms step_avg:58.14ms
step:716/2330 train_time:41632ms step_avg:58.14ms
step:717/2330 train_time:41687ms step_avg:58.14ms
step:718/2330 train_time:41750ms step_avg:58.15ms
step:719/2330 train_time:41806ms step_avg:58.14ms
step:720/2330 train_time:41867ms step_avg:58.15ms
step:721/2330 train_time:41923ms step_avg:58.15ms
step:722/2330 train_time:41984ms step_avg:58.15ms
step:723/2330 train_time:42040ms step_avg:58.15ms
step:724/2330 train_time:42100ms step_avg:58.15ms
step:725/2330 train_time:42156ms step_avg:58.15ms
step:726/2330 train_time:42217ms step_avg:58.15ms
step:727/2330 train_time:42273ms step_avg:58.15ms
step:728/2330 train_time:42333ms step_avg:58.15ms
step:729/2330 train_time:42390ms step_avg:58.15ms
step:730/2330 train_time:42450ms step_avg:58.15ms
step:731/2330 train_time:42506ms step_avg:58.15ms
step:732/2330 train_time:42567ms step_avg:58.15ms
step:733/2330 train_time:42624ms step_avg:58.15ms
step:734/2330 train_time:42684ms step_avg:58.15ms
step:735/2330 train_time:42740ms step_avg:58.15ms
step:736/2330 train_time:42800ms step_avg:58.15ms
step:737/2330 train_time:42856ms step_avg:58.15ms
step:738/2330 train_time:42916ms step_avg:58.15ms
step:739/2330 train_time:42972ms step_avg:58.15ms
step:740/2330 train_time:43033ms step_avg:58.15ms
step:741/2330 train_time:43088ms step_avg:58.15ms
step:742/2330 train_time:43150ms step_avg:58.15ms
step:743/2330 train_time:43205ms step_avg:58.15ms
step:744/2330 train_time:43267ms step_avg:58.15ms
step:745/2330 train_time:43323ms step_avg:58.15ms
step:746/2330 train_time:43383ms step_avg:58.15ms
step:747/2330 train_time:43439ms step_avg:58.15ms
step:748/2330 train_time:43499ms step_avg:58.15ms
step:749/2330 train_time:43556ms step_avg:58.15ms
step:750/2330 train_time:43616ms step_avg:58.15ms
step:750/2330 val_loss:4.9489 train_time:43693ms step_avg:58.26ms
step:751/2330 train_time:43712ms step_avg:58.21ms
step:752/2330 train_time:43735ms step_avg:58.16ms
step:753/2330 train_time:43791ms step_avg:58.16ms
step:754/2330 train_time:43854ms step_avg:58.16ms
step:755/2330 train_time:43910ms step_avg:58.16ms
step:756/2330 train_time:43973ms step_avg:58.17ms
step:757/2330 train_time:44028ms step_avg:58.16ms
step:758/2330 train_time:44089ms step_avg:58.17ms
step:759/2330 train_time:44145ms step_avg:58.16ms
step:760/2330 train_time:44205ms step_avg:58.16ms
step:761/2330 train_time:44260ms step_avg:58.16ms
step:762/2330 train_time:44321ms step_avg:58.16ms
step:763/2330 train_time:44377ms step_avg:58.16ms
step:764/2330 train_time:44437ms step_avg:58.16ms
step:765/2330 train_time:44493ms step_avg:58.16ms
step:766/2330 train_time:44553ms step_avg:58.16ms
step:767/2330 train_time:44609ms step_avg:58.16ms
step:768/2330 train_time:44670ms step_avg:58.16ms
step:769/2330 train_time:44727ms step_avg:58.16ms
step:770/2330 train_time:44788ms step_avg:58.17ms
step:771/2330 train_time:44845ms step_avg:58.16ms
step:772/2330 train_time:44908ms step_avg:58.17ms
step:773/2330 train_time:44965ms step_avg:58.17ms
step:774/2330 train_time:45027ms step_avg:58.17ms
step:775/2330 train_time:45083ms step_avg:58.17ms
step:776/2330 train_time:45145ms step_avg:58.18ms
step:777/2330 train_time:45201ms step_avg:58.17ms
step:778/2330 train_time:45264ms step_avg:58.18ms
step:779/2330 train_time:45320ms step_avg:58.18ms
step:780/2330 train_time:45381ms step_avg:58.18ms
step:781/2330 train_time:45438ms step_avg:58.18ms
step:782/2330 train_time:45499ms step_avg:58.18ms
step:783/2330 train_time:45555ms step_avg:58.18ms
step:784/2330 train_time:45616ms step_avg:58.18ms
step:785/2330 train_time:45672ms step_avg:58.18ms
step:786/2330 train_time:45734ms step_avg:58.19ms
step:787/2330 train_time:45791ms step_avg:58.18ms
step:788/2330 train_time:45852ms step_avg:58.19ms
step:789/2330 train_time:45909ms step_avg:58.19ms
step:790/2330 train_time:45970ms step_avg:58.19ms
step:791/2330 train_time:46027ms step_avg:58.19ms
step:792/2330 train_time:46087ms step_avg:58.19ms
step:793/2330 train_time:46143ms step_avg:58.19ms
step:794/2330 train_time:46205ms step_avg:58.19ms
step:795/2330 train_time:46262ms step_avg:58.19ms
step:796/2330 train_time:46323ms step_avg:58.19ms
step:797/2330 train_time:46379ms step_avg:58.19ms
step:798/2330 train_time:46441ms step_avg:58.20ms
step:799/2330 train_time:46497ms step_avg:58.19ms
step:800/2330 train_time:46559ms step_avg:58.20ms
step:801/2330 train_time:46616ms step_avg:58.20ms
step:802/2330 train_time:46677ms step_avg:58.20ms
step:803/2330 train_time:46734ms step_avg:58.20ms
step:804/2330 train_time:46796ms step_avg:58.20ms
step:805/2330 train_time:46853ms step_avg:58.20ms
step:806/2330 train_time:46914ms step_avg:58.21ms
step:807/2330 train_time:46971ms step_avg:58.20ms
step:808/2330 train_time:47032ms step_avg:58.21ms
step:809/2330 train_time:47089ms step_avg:58.21ms
step:810/2330 train_time:47150ms step_avg:58.21ms
step:811/2330 train_time:47207ms step_avg:58.21ms
step:812/2330 train_time:47267ms step_avg:58.21ms
step:813/2330 train_time:47324ms step_avg:58.21ms
step:814/2330 train_time:47386ms step_avg:58.21ms
step:815/2330 train_time:47442ms step_avg:58.21ms
step:816/2330 train_time:47503ms step_avg:58.21ms
step:817/2330 train_time:47559ms step_avg:58.21ms
step:818/2330 train_time:47621ms step_avg:58.22ms
step:819/2330 train_time:47676ms step_avg:58.21ms
step:820/2330 train_time:47739ms step_avg:58.22ms
step:821/2330 train_time:47796ms step_avg:58.22ms
step:822/2330 train_time:47858ms step_avg:58.22ms
step:823/2330 train_time:47915ms step_avg:58.22ms
step:824/2330 train_time:47976ms step_avg:58.22ms
step:825/2330 train_time:48033ms step_avg:58.22ms
step:826/2330 train_time:48094ms step_avg:58.22ms
step:827/2330 train_time:48152ms step_avg:58.23ms
step:828/2330 train_time:48213ms step_avg:58.23ms
step:829/2330 train_time:48271ms step_avg:58.23ms
step:830/2330 train_time:48331ms step_avg:58.23ms
step:831/2330 train_time:48388ms step_avg:58.23ms
step:832/2330 train_time:48450ms step_avg:58.23ms
step:833/2330 train_time:48507ms step_avg:58.23ms
step:834/2330 train_time:48567ms step_avg:58.23ms
step:835/2330 train_time:48623ms step_avg:58.23ms
step:836/2330 train_time:48685ms step_avg:58.24ms
step:837/2330 train_time:48741ms step_avg:58.23ms
step:838/2330 train_time:48804ms step_avg:58.24ms
step:839/2330 train_time:48860ms step_avg:58.24ms
step:840/2330 train_time:48922ms step_avg:58.24ms
step:841/2330 train_time:48978ms step_avg:58.24ms
step:842/2330 train_time:49041ms step_avg:58.24ms
step:843/2330 train_time:49097ms step_avg:58.24ms
step:844/2330 train_time:49160ms step_avg:58.25ms
step:845/2330 train_time:49216ms step_avg:58.24ms
step:846/2330 train_time:49279ms step_avg:58.25ms
step:847/2330 train_time:49336ms step_avg:58.25ms
step:848/2330 train_time:49397ms step_avg:58.25ms
step:849/2330 train_time:49455ms step_avg:58.25ms
step:850/2330 train_time:49515ms step_avg:58.25ms
step:851/2330 train_time:49573ms step_avg:58.25ms
step:852/2330 train_time:49633ms step_avg:58.26ms
step:853/2330 train_time:49690ms step_avg:58.25ms
step:854/2330 train_time:49751ms step_avg:58.26ms
step:855/2330 train_time:49808ms step_avg:58.26ms
step:856/2330 train_time:49869ms step_avg:58.26ms
step:857/2330 train_time:49925ms step_avg:58.26ms
step:858/2330 train_time:49987ms step_avg:58.26ms
step:859/2330 train_time:50043ms step_avg:58.26ms
step:860/2330 train_time:50105ms step_avg:58.26ms
step:861/2330 train_time:50162ms step_avg:58.26ms
step:862/2330 train_time:50224ms step_avg:58.26ms
step:863/2330 train_time:50280ms step_avg:58.26ms
step:864/2330 train_time:50342ms step_avg:58.27ms
step:865/2330 train_time:50398ms step_avg:58.26ms
step:866/2330 train_time:50461ms step_avg:58.27ms
step:867/2330 train_time:50517ms step_avg:58.27ms
step:868/2330 train_time:50579ms step_avg:58.27ms
step:869/2330 train_time:50636ms step_avg:58.27ms
step:870/2330 train_time:50697ms step_avg:58.27ms
step:871/2330 train_time:50753ms step_avg:58.27ms
step:872/2330 train_time:50814ms step_avg:58.27ms
step:873/2330 train_time:50872ms step_avg:58.27ms
step:874/2330 train_time:50932ms step_avg:58.27ms
step:875/2330 train_time:50989ms step_avg:58.27ms
step:876/2330 train_time:51050ms step_avg:58.28ms
step:877/2330 train_time:51107ms step_avg:58.28ms
step:878/2330 train_time:51168ms step_avg:58.28ms
step:879/2330 train_time:51224ms step_avg:58.28ms
step:880/2330 train_time:51286ms step_avg:58.28ms
step:881/2330 train_time:51343ms step_avg:58.28ms
step:882/2330 train_time:51404ms step_avg:58.28ms
step:883/2330 train_time:51460ms step_avg:58.28ms
step:884/2330 train_time:51522ms step_avg:58.28ms
step:885/2330 train_time:51578ms step_avg:58.28ms
step:886/2330 train_time:51641ms step_avg:58.29ms
step:887/2330 train_time:51697ms step_avg:58.28ms
step:888/2330 train_time:51761ms step_avg:58.29ms
step:889/2330 train_time:51817ms step_avg:58.29ms
step:890/2330 train_time:51880ms step_avg:58.29ms
step:891/2330 train_time:51936ms step_avg:58.29ms
step:892/2330 train_time:51998ms step_avg:58.29ms
step:893/2330 train_time:52055ms step_avg:58.29ms
step:894/2330 train_time:52117ms step_avg:58.30ms
step:895/2330 train_time:52174ms step_avg:58.30ms
step:896/2330 train_time:52235ms step_avg:58.30ms
step:897/2330 train_time:52292ms step_avg:58.30ms
step:898/2330 train_time:52353ms step_avg:58.30ms
step:899/2330 train_time:52410ms step_avg:58.30ms
step:900/2330 train_time:52470ms step_avg:58.30ms
step:901/2330 train_time:52527ms step_avg:58.30ms
step:902/2330 train_time:52588ms step_avg:58.30ms
step:903/2330 train_time:52644ms step_avg:58.30ms
step:904/2330 train_time:52706ms step_avg:58.30ms
step:905/2330 train_time:52762ms step_avg:58.30ms
step:906/2330 train_time:52824ms step_avg:58.30ms
step:907/2330 train_time:52881ms step_avg:58.30ms
step:908/2330 train_time:52943ms step_avg:58.31ms
step:909/2330 train_time:52999ms step_avg:58.31ms
step:910/2330 train_time:53062ms step_avg:58.31ms
step:911/2330 train_time:53118ms step_avg:58.31ms
step:912/2330 train_time:53181ms step_avg:58.31ms
step:913/2330 train_time:53237ms step_avg:58.31ms
step:914/2330 train_time:53301ms step_avg:58.32ms
step:915/2330 train_time:53357ms step_avg:58.31ms
step:916/2330 train_time:53419ms step_avg:58.32ms
step:917/2330 train_time:53476ms step_avg:58.32ms
step:918/2330 train_time:53537ms step_avg:58.32ms
step:919/2330 train_time:53594ms step_avg:58.32ms
step:920/2330 train_time:53655ms step_avg:58.32ms
step:921/2330 train_time:53712ms step_avg:58.32ms
step:922/2330 train_time:53772ms step_avg:58.32ms
step:923/2330 train_time:53829ms step_avg:58.32ms
step:924/2330 train_time:53890ms step_avg:58.32ms
step:925/2330 train_time:53947ms step_avg:58.32ms
step:926/2330 train_time:54008ms step_avg:58.32ms
step:927/2330 train_time:54065ms step_avg:58.32ms
step:928/2330 train_time:54127ms step_avg:58.33ms
step:929/2330 train_time:54183ms step_avg:58.32ms
step:930/2330 train_time:54245ms step_avg:58.33ms
step:931/2330 train_time:54301ms step_avg:58.33ms
step:932/2330 train_time:54363ms step_avg:58.33ms
step:933/2330 train_time:54420ms step_avg:58.33ms
step:934/2330 train_time:54481ms step_avg:58.33ms
step:935/2330 train_time:54537ms step_avg:58.33ms
step:936/2330 train_time:54600ms step_avg:58.33ms
step:937/2330 train_time:54656ms step_avg:58.33ms
step:938/2330 train_time:54718ms step_avg:58.33ms
step:939/2330 train_time:54775ms step_avg:58.33ms
step:940/2330 train_time:54837ms step_avg:58.34ms
step:941/2330 train_time:54894ms step_avg:58.34ms
step:942/2330 train_time:54955ms step_avg:58.34ms
step:943/2330 train_time:55012ms step_avg:58.34ms
step:944/2330 train_time:55073ms step_avg:58.34ms
step:945/2330 train_time:55130ms step_avg:58.34ms
step:946/2330 train_time:55190ms step_avg:58.34ms
step:947/2330 train_time:55247ms step_avg:58.34ms
step:948/2330 train_time:55308ms step_avg:58.34ms
step:949/2330 train_time:55365ms step_avg:58.34ms
step:950/2330 train_time:55427ms step_avg:58.34ms
step:951/2330 train_time:55483ms step_avg:58.34ms
step:952/2330 train_time:55545ms step_avg:58.35ms
step:953/2330 train_time:55601ms step_avg:58.34ms
step:954/2330 train_time:55663ms step_avg:58.35ms
step:955/2330 train_time:55719ms step_avg:58.34ms
step:956/2330 train_time:55782ms step_avg:58.35ms
step:957/2330 train_time:55838ms step_avg:58.35ms
step:958/2330 train_time:55900ms step_avg:58.35ms
step:959/2330 train_time:55956ms step_avg:58.35ms
step:960/2330 train_time:56019ms step_avg:58.35ms
step:961/2330 train_time:56075ms step_avg:58.35ms
step:962/2330 train_time:56137ms step_avg:58.35ms
step:963/2330 train_time:56194ms step_avg:58.35ms
step:964/2330 train_time:56255ms step_avg:58.36ms
step:965/2330 train_time:56313ms step_avg:58.36ms
step:966/2330 train_time:56373ms step_avg:58.36ms
step:967/2330 train_time:56430ms step_avg:58.36ms
step:968/2330 train_time:56491ms step_avg:58.36ms
step:969/2330 train_time:56548ms step_avg:58.36ms
step:970/2330 train_time:56608ms step_avg:58.36ms
step:971/2330 train_time:56666ms step_avg:58.36ms
step:972/2330 train_time:56727ms step_avg:58.36ms
step:973/2330 train_time:56784ms step_avg:58.36ms
step:974/2330 train_time:56844ms step_avg:58.36ms
step:975/2330 train_time:56900ms step_avg:58.36ms
step:976/2330 train_time:56962ms step_avg:58.36ms
step:977/2330 train_time:57018ms step_avg:58.36ms
step:978/2330 train_time:57081ms step_avg:58.36ms
step:979/2330 train_time:57136ms step_avg:58.36ms
step:980/2330 train_time:57199ms step_avg:58.37ms
step:981/2330 train_time:57256ms step_avg:58.36ms
step:982/2330 train_time:57319ms step_avg:58.37ms
step:983/2330 train_time:57376ms step_avg:58.37ms
step:984/2330 train_time:57436ms step_avg:58.37ms
step:985/2330 train_time:57493ms step_avg:58.37ms
step:986/2330 train_time:57554ms step_avg:58.37ms
step:987/2330 train_time:57611ms step_avg:58.37ms
step:988/2330 train_time:57672ms step_avg:58.37ms
step:989/2330 train_time:57729ms step_avg:58.37ms
step:990/2330 train_time:57791ms step_avg:58.37ms
step:991/2330 train_time:57847ms step_avg:58.37ms
step:992/2330 train_time:57908ms step_avg:58.37ms
step:993/2330 train_time:57964ms step_avg:58.37ms
step:994/2330 train_time:58025ms step_avg:58.38ms
step:995/2330 train_time:58081ms step_avg:58.37ms
step:996/2330 train_time:58143ms step_avg:58.38ms
step:997/2330 train_time:58199ms step_avg:58.37ms
step:998/2330 train_time:58261ms step_avg:58.38ms
step:999/2330 train_time:58317ms step_avg:58.38ms
step:1000/2330 train_time:58380ms step_avg:58.38ms
step:1000/2330 val_loss:4.6549 train_time:58460ms step_avg:58.46ms
step:1001/2330 train_time:58481ms step_avg:58.42ms
step:1002/2330 train_time:58503ms step_avg:58.39ms
step:1003/2330 train_time:58557ms step_avg:58.38ms
step:1004/2330 train_time:58621ms step_avg:58.39ms
step:1005/2330 train_time:58677ms step_avg:58.39ms
step:1006/2330 train_time:58743ms step_avg:58.39ms
step:1007/2330 train_time:58799ms step_avg:58.39ms
step:1008/2330 train_time:58862ms step_avg:58.40ms
step:1009/2330 train_time:58919ms step_avg:58.39ms
step:1010/2330 train_time:58981ms step_avg:58.40ms
step:1011/2330 train_time:59037ms step_avg:58.40ms
step:1012/2330 train_time:59097ms step_avg:58.40ms
step:1013/2330 train_time:59154ms step_avg:58.39ms
step:1014/2330 train_time:59214ms step_avg:58.40ms
step:1015/2330 train_time:59270ms step_avg:58.39ms
step:1016/2330 train_time:59331ms step_avg:58.40ms
step:1017/2330 train_time:59389ms step_avg:58.40ms
step:1018/2330 train_time:59450ms step_avg:58.40ms
step:1019/2330 train_time:59507ms step_avg:58.40ms
step:1020/2330 train_time:59569ms step_avg:58.40ms
step:1021/2330 train_time:59625ms step_avg:58.40ms
step:1022/2330 train_time:59688ms step_avg:58.40ms
step:1023/2330 train_time:59744ms step_avg:58.40ms
step:1024/2330 train_time:59808ms step_avg:58.41ms
step:1025/2330 train_time:59864ms step_avg:58.40ms
step:1026/2330 train_time:59927ms step_avg:58.41ms
step:1027/2330 train_time:59984ms step_avg:58.41ms
step:1028/2330 train_time:60045ms step_avg:58.41ms
step:1029/2330 train_time:60102ms step_avg:58.41ms
step:1030/2330 train_time:60164ms step_avg:58.41ms
step:1031/2330 train_time:60220ms step_avg:58.41ms
step:1032/2330 train_time:60281ms step_avg:58.41ms
step:1033/2330 train_time:60338ms step_avg:58.41ms
step:1034/2330 train_time:60399ms step_avg:58.41ms
step:1035/2330 train_time:60457ms step_avg:58.41ms
step:1036/2330 train_time:60518ms step_avg:58.42ms
step:1037/2330 train_time:60575ms step_avg:58.41ms
step:1038/2330 train_time:60636ms step_avg:58.42ms
step:1039/2330 train_time:60692ms step_avg:58.41ms
step:1040/2330 train_time:60754ms step_avg:58.42ms
step:1041/2330 train_time:60810ms step_avg:58.42ms
step:1042/2330 train_time:60872ms step_avg:58.42ms
step:1043/2330 train_time:60928ms step_avg:58.42ms
step:1044/2330 train_time:60991ms step_avg:58.42ms
step:1045/2330 train_time:61047ms step_avg:58.42ms
step:1046/2330 train_time:61109ms step_avg:58.42ms
step:1047/2330 train_time:61165ms step_avg:58.42ms
step:1048/2330 train_time:61227ms step_avg:58.42ms
step:1049/2330 train_time:61283ms step_avg:58.42ms
step:1050/2330 train_time:61345ms step_avg:58.42ms
step:1051/2330 train_time:61401ms step_avg:58.42ms
step:1052/2330 train_time:61464ms step_avg:58.43ms
step:1053/2330 train_time:61520ms step_avg:58.42ms
step:1054/2330 train_time:61583ms step_avg:58.43ms
step:1055/2330 train_time:61639ms step_avg:58.43ms
step:1056/2330 train_time:61702ms step_avg:58.43ms
step:1057/2330 train_time:61759ms step_avg:58.43ms
step:1058/2330 train_time:61820ms step_avg:58.43ms
step:1059/2330 train_time:61877ms step_avg:58.43ms
step:1060/2330 train_time:61939ms step_avg:58.43ms
step:1061/2330 train_time:61996ms step_avg:58.43ms
step:1062/2330 train_time:62057ms step_avg:58.43ms
step:1063/2330 train_time:62115ms step_avg:58.43ms
step:1064/2330 train_time:62176ms step_avg:58.44ms
step:1065/2330 train_time:62232ms step_avg:58.43ms
step:1066/2330 train_time:62293ms step_avg:58.44ms
step:1067/2330 train_time:62350ms step_avg:58.43ms
step:1068/2330 train_time:62411ms step_avg:58.44ms
step:1069/2330 train_time:62467ms step_avg:58.43ms
step:1070/2330 train_time:62529ms step_avg:58.44ms
step:1071/2330 train_time:62585ms step_avg:58.44ms
step:1072/2330 train_time:62647ms step_avg:58.44ms
step:1073/2330 train_time:62703ms step_avg:58.44ms
step:1074/2330 train_time:62766ms step_avg:58.44ms
step:1075/2330 train_time:62822ms step_avg:58.44ms
step:1076/2330 train_time:62885ms step_avg:58.44ms
step:1077/2330 train_time:62941ms step_avg:58.44ms
step:1078/2330 train_time:63004ms step_avg:58.45ms
step:1079/2330 train_time:63060ms step_avg:58.44ms
step:1080/2330 train_time:63123ms step_avg:58.45ms
step:1081/2330 train_time:63180ms step_avg:58.45ms
step:1082/2330 train_time:63241ms step_avg:58.45ms
step:1083/2330 train_time:63298ms step_avg:58.45ms
step:1084/2330 train_time:63359ms step_avg:58.45ms
step:1085/2330 train_time:63417ms step_avg:58.45ms
step:1086/2330 train_time:63477ms step_avg:58.45ms
step:1087/2330 train_time:63535ms step_avg:58.45ms
step:1088/2330 train_time:63595ms step_avg:58.45ms
step:1089/2330 train_time:63652ms step_avg:58.45ms
step:1090/2330 train_time:63713ms step_avg:58.45ms
step:1091/2330 train_time:63770ms step_avg:58.45ms
step:1092/2330 train_time:63831ms step_avg:58.45ms
step:1093/2330 train_time:63888ms step_avg:58.45ms
step:1094/2330 train_time:63950ms step_avg:58.45ms
step:1095/2330 train_time:64006ms step_avg:58.45ms
step:1096/2330 train_time:64068ms step_avg:58.46ms
step:1097/2330 train_time:64124ms step_avg:58.45ms
step:1098/2330 train_time:64186ms step_avg:58.46ms
step:1099/2330 train_time:64243ms step_avg:58.46ms
step:1100/2330 train_time:64306ms step_avg:58.46ms
step:1101/2330 train_time:64362ms step_avg:58.46ms
step:1102/2330 train_time:64425ms step_avg:58.46ms
step:1103/2330 train_time:64481ms step_avg:58.46ms
step:1104/2330 train_time:64544ms step_avg:58.46ms
step:1105/2330 train_time:64600ms step_avg:58.46ms
step:1106/2330 train_time:64662ms step_avg:58.46ms
step:1107/2330 train_time:64719ms step_avg:58.46ms
step:1108/2330 train_time:64780ms step_avg:58.47ms
step:1109/2330 train_time:64837ms step_avg:58.46ms
step:1110/2330 train_time:64898ms step_avg:58.47ms
step:1111/2330 train_time:64956ms step_avg:58.47ms
step:1112/2330 train_time:65016ms step_avg:58.47ms
step:1113/2330 train_time:65073ms step_avg:58.47ms
step:1114/2330 train_time:65133ms step_avg:58.47ms
step:1115/2330 train_time:65190ms step_avg:58.47ms
step:1116/2330 train_time:65252ms step_avg:58.47ms
step:1117/2330 train_time:65308ms step_avg:58.47ms
step:1118/2330 train_time:65370ms step_avg:58.47ms
step:1119/2330 train_time:65427ms step_avg:58.47ms
step:1120/2330 train_time:65489ms step_avg:58.47ms
step:1121/2330 train_time:65545ms step_avg:58.47ms
step:1122/2330 train_time:65607ms step_avg:58.47ms
step:1123/2330 train_time:65664ms step_avg:58.47ms
step:1124/2330 train_time:65727ms step_avg:58.48ms
step:1125/2330 train_time:65783ms step_avg:58.47ms
step:1126/2330 train_time:65846ms step_avg:58.48ms
step:1127/2330 train_time:65902ms step_avg:58.48ms
step:1128/2330 train_time:65965ms step_avg:58.48ms
step:1129/2330 train_time:66021ms step_avg:58.48ms
step:1130/2330 train_time:66084ms step_avg:58.48ms
step:1131/2330 train_time:66141ms step_avg:58.48ms
step:1132/2330 train_time:66203ms step_avg:58.48ms
step:1133/2330 train_time:66261ms step_avg:58.48ms
step:1134/2330 train_time:66321ms step_avg:58.48ms
step:1135/2330 train_time:66379ms step_avg:58.48ms
step:1136/2330 train_time:66439ms step_avg:58.48ms
step:1137/2330 train_time:66496ms step_avg:58.48ms
step:1138/2330 train_time:66557ms step_avg:58.49ms
step:1139/2330 train_time:66614ms step_avg:58.48ms
step:1140/2330 train_time:66675ms step_avg:58.49ms
step:1141/2330 train_time:66732ms step_avg:58.49ms
step:1142/2330 train_time:66793ms step_avg:58.49ms
step:1143/2330 train_time:66850ms step_avg:58.49ms
step:1144/2330 train_time:66911ms step_avg:58.49ms
step:1145/2330 train_time:66967ms step_avg:58.49ms
step:1146/2330 train_time:67029ms step_avg:58.49ms
step:1147/2330 train_time:67085ms step_avg:58.49ms
step:1148/2330 train_time:67148ms step_avg:58.49ms
step:1149/2330 train_time:67205ms step_avg:58.49ms
step:1150/2330 train_time:67266ms step_avg:58.49ms
step:1151/2330 train_time:67323ms step_avg:58.49ms
step:1152/2330 train_time:67385ms step_avg:58.49ms
step:1153/2330 train_time:67441ms step_avg:58.49ms
step:1154/2330 train_time:67504ms step_avg:58.50ms
step:1155/2330 train_time:67560ms step_avg:58.49ms
step:1156/2330 train_time:67623ms step_avg:58.50ms
step:1157/2330 train_time:67679ms step_avg:58.50ms
step:1158/2330 train_time:67741ms step_avg:58.50ms
step:1159/2330 train_time:67799ms step_avg:58.50ms
step:1160/2330 train_time:67860ms step_avg:58.50ms
step:1161/2330 train_time:67917ms step_avg:58.50ms
step:1162/2330 train_time:67978ms step_avg:58.50ms
step:1163/2330 train_time:68035ms step_avg:58.50ms
step:1164/2330 train_time:68096ms step_avg:58.50ms
step:1165/2330 train_time:68154ms step_avg:58.50ms
step:1166/2330 train_time:68214ms step_avg:58.50ms
step:1167/2330 train_time:68271ms step_avg:58.50ms
step:1168/2330 train_time:68332ms step_avg:58.50ms
step:1169/2330 train_time:68388ms step_avg:58.50ms
step:1170/2330 train_time:68450ms step_avg:58.50ms
step:1171/2330 train_time:68507ms step_avg:58.50ms
step:1172/2330 train_time:68569ms step_avg:58.51ms
step:1173/2330 train_time:68625ms step_avg:58.50ms
step:1174/2330 train_time:68687ms step_avg:58.51ms
step:1175/2330 train_time:68743ms step_avg:58.50ms
step:1176/2330 train_time:68806ms step_avg:58.51ms
step:1177/2330 train_time:68862ms step_avg:58.51ms
step:1178/2330 train_time:68925ms step_avg:58.51ms
step:1179/2330 train_time:68981ms step_avg:58.51ms
step:1180/2330 train_time:69043ms step_avg:58.51ms
step:1181/2330 train_time:69099ms step_avg:58.51ms
step:1182/2330 train_time:69162ms step_avg:58.51ms
step:1183/2330 train_time:69218ms step_avg:58.51ms
step:1184/2330 train_time:69281ms step_avg:58.51ms
step:1185/2330 train_time:69337ms step_avg:58.51ms
step:1186/2330 train_time:69400ms step_avg:58.52ms
step:1187/2330 train_time:69456ms step_avg:58.51ms
step:1188/2330 train_time:69518ms step_avg:58.52ms
step:1189/2330 train_time:69576ms step_avg:58.52ms
step:1190/2330 train_time:69636ms step_avg:58.52ms
step:1191/2330 train_time:69693ms step_avg:58.52ms
step:1192/2330 train_time:69754ms step_avg:58.52ms
step:1193/2330 train_time:69811ms step_avg:58.52ms
step:1194/2330 train_time:69872ms step_avg:58.52ms
step:1195/2330 train_time:69928ms step_avg:58.52ms
step:1196/2330 train_time:69990ms step_avg:58.52ms
step:1197/2330 train_time:70046ms step_avg:58.52ms
step:1198/2330 train_time:70108ms step_avg:58.52ms
step:1199/2330 train_time:70164ms step_avg:58.52ms
step:1200/2330 train_time:70226ms step_avg:58.52ms
step:1201/2330 train_time:70282ms step_avg:58.52ms
step:1202/2330 train_time:70345ms step_avg:58.52ms
step:1203/2330 train_time:70401ms step_avg:58.52ms
step:1204/2330 train_time:70464ms step_avg:58.52ms
step:1205/2330 train_time:70520ms step_avg:58.52ms
step:1206/2330 train_time:70583ms step_avg:58.53ms
step:1207/2330 train_time:70639ms step_avg:58.52ms
step:1208/2330 train_time:70702ms step_avg:58.53ms
step:1209/2330 train_time:70759ms step_avg:58.53ms
step:1210/2330 train_time:70820ms step_avg:58.53ms
step:1211/2330 train_time:70877ms step_avg:58.53ms
step:1212/2330 train_time:70938ms step_avg:58.53ms
step:1213/2330 train_time:70994ms step_avg:58.53ms
step:1214/2330 train_time:71055ms step_avg:58.53ms
step:1215/2330 train_time:71112ms step_avg:58.53ms
step:1216/2330 train_time:71174ms step_avg:58.53ms
step:1217/2330 train_time:71230ms step_avg:58.53ms
step:1218/2330 train_time:71292ms step_avg:58.53ms
step:1219/2330 train_time:71348ms step_avg:58.53ms
step:1220/2330 train_time:71410ms step_avg:58.53ms
step:1221/2330 train_time:71466ms step_avg:58.53ms
step:1222/2330 train_time:71529ms step_avg:58.53ms
step:1223/2330 train_time:71585ms step_avg:58.53ms
step:1224/2330 train_time:71648ms step_avg:58.54ms
step:1225/2330 train_time:71704ms step_avg:58.53ms
step:1226/2330 train_time:71766ms step_avg:58.54ms
step:1227/2330 train_time:71823ms step_avg:58.54ms
step:1228/2330 train_time:71886ms step_avg:58.54ms
step:1229/2330 train_time:71942ms step_avg:58.54ms
step:1230/2330 train_time:72004ms step_avg:58.54ms
step:1231/2330 train_time:72061ms step_avg:58.54ms
step:1232/2330 train_time:72124ms step_avg:58.54ms
step:1233/2330 train_time:72181ms step_avg:58.54ms
step:1234/2330 train_time:72242ms step_avg:58.54ms
step:1235/2330 train_time:72299ms step_avg:58.54ms
step:1236/2330 train_time:72360ms step_avg:58.54ms
step:1237/2330 train_time:72417ms step_avg:58.54ms
step:1238/2330 train_time:72478ms step_avg:58.54ms
step:1239/2330 train_time:72535ms step_avg:58.54ms
step:1240/2330 train_time:72596ms step_avg:58.54ms
step:1241/2330 train_time:72653ms step_avg:58.54ms
step:1242/2330 train_time:72714ms step_avg:58.55ms
step:1243/2330 train_time:72771ms step_avg:58.54ms
step:1244/2330 train_time:72832ms step_avg:58.55ms
step:1245/2330 train_time:72888ms step_avg:58.54ms
step:1246/2330 train_time:72951ms step_avg:58.55ms
step:1247/2330 train_time:73007ms step_avg:58.55ms
step:1248/2330 train_time:73069ms step_avg:58.55ms
step:1249/2330 train_time:73125ms step_avg:58.55ms
step:1250/2330 train_time:73187ms step_avg:58.55ms
step:1250/2330 val_loss:4.4876 train_time:73267ms step_avg:58.61ms
step:1251/2330 train_time:73287ms step_avg:58.58ms
step:1252/2330 train_time:73311ms step_avg:58.56ms
step:1253/2330 train_time:73370ms step_avg:58.56ms
step:1254/2330 train_time:73435ms step_avg:58.56ms
step:1255/2330 train_time:73494ms step_avg:58.56ms
step:1256/2330 train_time:73555ms step_avg:58.56ms
step:1257/2330 train_time:73612ms step_avg:58.56ms
step:1258/2330 train_time:73672ms step_avg:58.56ms
step:1259/2330 train_time:73729ms step_avg:58.56ms
step:1260/2330 train_time:73789ms step_avg:58.56ms
step:1261/2330 train_time:73845ms step_avg:58.56ms
step:1262/2330 train_time:73906ms step_avg:58.56ms
step:1263/2330 train_time:73962ms step_avg:58.56ms
step:1264/2330 train_time:74022ms step_avg:58.56ms
step:1265/2330 train_time:74079ms step_avg:58.56ms
step:1266/2330 train_time:74139ms step_avg:58.56ms
step:1267/2330 train_time:74195ms step_avg:58.56ms
step:1268/2330 train_time:74256ms step_avg:58.56ms
step:1269/2330 train_time:74313ms step_avg:58.56ms
step:1270/2330 train_time:74375ms step_avg:58.56ms
step:1271/2330 train_time:74432ms step_avg:58.56ms
step:1272/2330 train_time:74494ms step_avg:58.56ms
step:1273/2330 train_time:74552ms step_avg:58.56ms
step:1274/2330 train_time:74613ms step_avg:58.57ms
step:1275/2330 train_time:74670ms step_avg:58.56ms
step:1276/2330 train_time:74730ms step_avg:58.57ms
step:1277/2330 train_time:74787ms step_avg:58.56ms
step:1278/2330 train_time:74847ms step_avg:58.57ms
step:1279/2330 train_time:74904ms step_avg:58.56ms
step:1280/2330 train_time:74964ms step_avg:58.57ms
step:1281/2330 train_time:75021ms step_avg:58.56ms
step:1282/2330 train_time:75081ms step_avg:58.57ms
step:1283/2330 train_time:75138ms step_avg:58.56ms
step:1284/2330 train_time:75198ms step_avg:58.57ms
step:1285/2330 train_time:75255ms step_avg:58.56ms
step:1286/2330 train_time:75317ms step_avg:58.57ms
step:1287/2330 train_time:75374ms step_avg:58.57ms
step:1288/2330 train_time:75437ms step_avg:58.57ms
step:1289/2330 train_time:75493ms step_avg:58.57ms
step:1290/2330 train_time:75556ms step_avg:58.57ms
step:1291/2330 train_time:75613ms step_avg:58.57ms
step:1292/2330 train_time:75676ms step_avg:58.57ms
step:1293/2330 train_time:75733ms step_avg:58.57ms
step:1294/2330 train_time:75793ms step_avg:58.57ms
step:1295/2330 train_time:75851ms step_avg:58.57ms
step:1296/2330 train_time:75911ms step_avg:58.57ms
step:1297/2330 train_time:75968ms step_avg:58.57ms
step:1298/2330 train_time:76029ms step_avg:58.57ms
step:1299/2330 train_time:76085ms step_avg:58.57ms
step:1300/2330 train_time:76146ms step_avg:58.57ms
step:1301/2330 train_time:76202ms step_avg:58.57ms
step:1302/2330 train_time:76264ms step_avg:58.57ms
step:1303/2330 train_time:76320ms step_avg:58.57ms
step:1304/2330 train_time:76383ms step_avg:58.58ms
step:1305/2330 train_time:76439ms step_avg:58.57ms
step:1306/2330 train_time:76502ms step_avg:58.58ms
step:1307/2330 train_time:76559ms step_avg:58.58ms
step:1308/2330 train_time:76622ms step_avg:58.58ms
step:1309/2330 train_time:76678ms step_avg:58.58ms
step:1310/2330 train_time:76741ms step_avg:58.58ms
step:1311/2330 train_time:76798ms step_avg:58.58ms
step:1312/2330 train_time:76860ms step_avg:58.58ms
step:1313/2330 train_time:76916ms step_avg:58.58ms
step:1314/2330 train_time:76979ms step_avg:58.58ms
step:1315/2330 train_time:77035ms step_avg:58.58ms
step:1316/2330 train_time:77096ms step_avg:58.58ms
step:1317/2330 train_time:77153ms step_avg:58.58ms
step:1318/2330 train_time:77213ms step_avg:58.58ms
step:1319/2330 train_time:77270ms step_avg:58.58ms
step:1320/2330 train_time:77331ms step_avg:58.58ms
step:1321/2330 train_time:77388ms step_avg:58.58ms
step:1322/2330 train_time:77449ms step_avg:58.58ms
step:1323/2330 train_time:77506ms step_avg:58.58ms
step:1324/2330 train_time:77567ms step_avg:58.59ms
step:1325/2330 train_time:77623ms step_avg:58.58ms
step:1326/2330 train_time:77685ms step_avg:58.59ms
step:1327/2330 train_time:77742ms step_avg:58.58ms
step:1328/2330 train_time:77804ms step_avg:58.59ms
step:1329/2330 train_time:77860ms step_avg:58.59ms
step:1330/2330 train_time:77922ms step_avg:58.59ms
step:1331/2330 train_time:77978ms step_avg:58.59ms
step:1332/2330 train_time:78040ms step_avg:58.59ms
step:1333/2330 train_time:78097ms step_avg:58.59ms
step:1334/2330 train_time:78158ms step_avg:58.59ms
step:1335/2330 train_time:78215ms step_avg:58.59ms
step:1336/2330 train_time:78276ms step_avg:58.59ms
step:1337/2330 train_time:78333ms step_avg:58.59ms
step:1338/2330 train_time:78394ms step_avg:58.59ms
step:1339/2330 train_time:78451ms step_avg:58.59ms
step:1340/2330 train_time:78511ms step_avg:58.59ms
step:1341/2330 train_time:78568ms step_avg:58.59ms
step:1342/2330 train_time:78629ms step_avg:58.59ms
step:1343/2330 train_time:78687ms step_avg:58.59ms
step:1344/2330 train_time:78747ms step_avg:58.59ms
step:1345/2330 train_time:78804ms step_avg:58.59ms
step:1346/2330 train_time:78864ms step_avg:58.59ms
step:1347/2330 train_time:78921ms step_avg:58.59ms
step:1348/2330 train_time:78983ms step_avg:58.59ms
step:1349/2330 train_time:79039ms step_avg:58.59ms
step:1350/2330 train_time:79101ms step_avg:58.59ms
step:1351/2330 train_time:79158ms step_avg:58.59ms
step:1352/2330 train_time:79221ms step_avg:58.60ms
step:1353/2330 train_time:79277ms step_avg:58.59ms
step:1354/2330 train_time:79340ms step_avg:58.60ms
step:1355/2330 train_time:79396ms step_avg:58.59ms
step:1356/2330 train_time:79459ms step_avg:58.60ms
step:1357/2330 train_time:79515ms step_avg:58.60ms
step:1358/2330 train_time:79577ms step_avg:58.60ms
step:1359/2330 train_time:79633ms step_avg:58.60ms
step:1360/2330 train_time:79696ms step_avg:58.60ms
step:1361/2330 train_time:79753ms step_avg:58.60ms
step:1362/2330 train_time:79813ms step_avg:58.60ms
step:1363/2330 train_time:79871ms step_avg:58.60ms
step:1364/2330 train_time:79931ms step_avg:58.60ms
step:1365/2330 train_time:79988ms step_avg:58.60ms
step:1366/2330 train_time:80050ms step_avg:58.60ms
step:1367/2330 train_time:80107ms step_avg:58.60ms
step:1368/2330 train_time:80168ms step_avg:58.60ms
step:1369/2330 train_time:80224ms step_avg:58.60ms
step:1370/2330 train_time:80286ms step_avg:58.60ms
step:1371/2330 train_time:80342ms step_avg:58.60ms
step:1372/2330 train_time:80405ms step_avg:58.60ms
step:1373/2330 train_time:80461ms step_avg:58.60ms
step:1374/2330 train_time:80523ms step_avg:58.60ms
step:1375/2330 train_time:80579ms step_avg:58.60ms
step:1376/2330 train_time:80641ms step_avg:58.61ms
step:1377/2330 train_time:80697ms step_avg:58.60ms
step:1378/2330 train_time:80760ms step_avg:58.61ms
step:1379/2330 train_time:80816ms step_avg:58.60ms
step:1380/2330 train_time:80878ms step_avg:58.61ms
step:1381/2330 train_time:80935ms step_avg:58.61ms
step:1382/2330 train_time:80997ms step_avg:58.61ms
step:1383/2330 train_time:81054ms step_avg:58.61ms
step:1384/2330 train_time:81115ms step_avg:58.61ms
step:1385/2330 train_time:81172ms step_avg:58.61ms
step:1386/2330 train_time:81233ms step_avg:58.61ms
step:1387/2330 train_time:81290ms step_avg:58.61ms
step:1388/2330 train_time:81351ms step_avg:58.61ms
step:1389/2330 train_time:81409ms step_avg:58.61ms
step:1390/2330 train_time:81469ms step_avg:58.61ms
step:1391/2330 train_time:81526ms step_avg:58.61ms
step:1392/2330 train_time:81588ms step_avg:58.61ms
step:1393/2330 train_time:81644ms step_avg:58.61ms
step:1394/2330 train_time:81706ms step_avg:58.61ms
step:1395/2330 train_time:81762ms step_avg:58.61ms
step:1396/2330 train_time:81824ms step_avg:58.61ms
step:1397/2330 train_time:81880ms step_avg:58.61ms
step:1398/2330 train_time:81943ms step_avg:58.61ms
step:1399/2330 train_time:81999ms step_avg:58.61ms
step:1400/2330 train_time:82062ms step_avg:58.62ms
step:1401/2330 train_time:82118ms step_avg:58.61ms
step:1402/2330 train_time:82180ms step_avg:58.62ms
step:1403/2330 train_time:82236ms step_avg:58.61ms
step:1404/2330 train_time:82299ms step_avg:58.62ms
step:1405/2330 train_time:82356ms step_avg:58.62ms
step:1406/2330 train_time:82417ms step_avg:58.62ms
step:1407/2330 train_time:82474ms step_avg:58.62ms
step:1408/2330 train_time:82535ms step_avg:58.62ms
step:1409/2330 train_time:82591ms step_avg:58.62ms
step:1410/2330 train_time:82652ms step_avg:58.62ms
step:1411/2330 train_time:82709ms step_avg:58.62ms
step:1412/2330 train_time:82770ms step_avg:58.62ms
step:1413/2330 train_time:82827ms step_avg:58.62ms
step:1414/2330 train_time:82888ms step_avg:58.62ms
step:1415/2330 train_time:82945ms step_avg:58.62ms
step:1416/2330 train_time:83006ms step_avg:58.62ms
step:1417/2330 train_time:83063ms step_avg:58.62ms
step:1418/2330 train_time:83124ms step_avg:58.62ms
step:1419/2330 train_time:83181ms step_avg:58.62ms
step:1420/2330 train_time:83243ms step_avg:58.62ms
step:1421/2330 train_time:83299ms step_avg:58.62ms
step:1422/2330 train_time:83361ms step_avg:58.62ms
step:1423/2330 train_time:83417ms step_avg:58.62ms
step:1424/2330 train_time:83479ms step_avg:58.62ms
step:1425/2330 train_time:83535ms step_avg:58.62ms
step:1426/2330 train_time:83598ms step_avg:58.62ms
step:1427/2330 train_time:83655ms step_avg:58.62ms
step:1428/2330 train_time:83716ms step_avg:58.62ms
step:1429/2330 train_time:83773ms step_avg:58.62ms
step:1430/2330 train_time:83835ms step_avg:58.63ms
step:1431/2330 train_time:83892ms step_avg:58.62ms
step:1432/2330 train_time:83952ms step_avg:58.63ms
step:1433/2330 train_time:84010ms step_avg:58.63ms
step:1434/2330 train_time:84071ms step_avg:58.63ms
step:1435/2330 train_time:84128ms step_avg:58.63ms
step:1436/2330 train_time:84189ms step_avg:58.63ms
step:1437/2330 train_time:84245ms step_avg:58.63ms
step:1438/2330 train_time:84307ms step_avg:58.63ms
step:1439/2330 train_time:84363ms step_avg:58.63ms
step:1440/2330 train_time:84425ms step_avg:58.63ms
step:1441/2330 train_time:84481ms step_avg:58.63ms
step:1442/2330 train_time:84544ms step_avg:58.63ms
step:1443/2330 train_time:84600ms step_avg:58.63ms
step:1444/2330 train_time:84662ms step_avg:58.63ms
step:1445/2330 train_time:84719ms step_avg:58.63ms
step:1446/2330 train_time:84781ms step_avg:58.63ms
step:1447/2330 train_time:84838ms step_avg:58.63ms
step:1448/2330 train_time:84900ms step_avg:58.63ms
step:1449/2330 train_time:84956ms step_avg:58.63ms
step:1450/2330 train_time:85019ms step_avg:58.63ms
step:1451/2330 train_time:85075ms step_avg:58.63ms
step:1452/2330 train_time:85136ms step_avg:58.63ms
step:1453/2330 train_time:85193ms step_avg:58.63ms
step:1454/2330 train_time:85254ms step_avg:58.63ms
step:1455/2330 train_time:85312ms step_avg:58.63ms
step:1456/2330 train_time:85372ms step_avg:58.63ms
step:1457/2330 train_time:85429ms step_avg:58.63ms
step:1458/2330 train_time:85490ms step_avg:58.63ms
step:1459/2330 train_time:85547ms step_avg:58.63ms
step:1460/2330 train_time:85607ms step_avg:58.64ms
step:1461/2330 train_time:85664ms step_avg:58.63ms
step:1462/2330 train_time:85726ms step_avg:58.64ms
step:1463/2330 train_time:85782ms step_avg:58.63ms
step:1464/2330 train_time:85843ms step_avg:58.64ms
step:1465/2330 train_time:85900ms step_avg:58.63ms
step:1466/2330 train_time:85962ms step_avg:58.64ms
step:1467/2330 train_time:86018ms step_avg:58.64ms
step:1468/2330 train_time:86080ms step_avg:58.64ms
step:1469/2330 train_time:86137ms step_avg:58.64ms
step:1470/2330 train_time:86199ms step_avg:58.64ms
step:1471/2330 train_time:86256ms step_avg:58.64ms
step:1472/2330 train_time:86318ms step_avg:58.64ms
step:1473/2330 train_time:86375ms step_avg:58.64ms
step:1474/2330 train_time:86436ms step_avg:58.64ms
step:1475/2330 train_time:86492ms step_avg:58.64ms
step:1476/2330 train_time:86554ms step_avg:58.64ms
step:1477/2330 train_time:86612ms step_avg:58.64ms
step:1478/2330 train_time:86672ms step_avg:58.64ms
step:1479/2330 train_time:86729ms step_avg:58.64ms
step:1480/2330 train_time:86790ms step_avg:58.64ms
step:1481/2330 train_time:86846ms step_avg:58.64ms
step:1482/2330 train_time:86908ms step_avg:58.64ms
step:1483/2330 train_time:86964ms step_avg:58.64ms
step:1484/2330 train_time:87027ms step_avg:58.64ms
step:1485/2330 train_time:87083ms step_avg:58.64ms
step:1486/2330 train_time:87144ms step_avg:58.64ms
step:1487/2330 train_time:87201ms step_avg:58.64ms
step:1488/2330 train_time:87263ms step_avg:58.64ms
step:1489/2330 train_time:87319ms step_avg:58.64ms
step:1490/2330 train_time:87380ms step_avg:58.64ms
step:1491/2330 train_time:87437ms step_avg:58.64ms
step:1492/2330 train_time:87499ms step_avg:58.65ms
step:1493/2330 train_time:87556ms step_avg:58.64ms
step:1494/2330 train_time:87619ms step_avg:58.65ms
step:1495/2330 train_time:87675ms step_avg:58.65ms
step:1496/2330 train_time:87737ms step_avg:58.65ms
step:1497/2330 train_time:87794ms step_avg:58.65ms
step:1498/2330 train_time:87855ms step_avg:58.65ms
step:1499/2330 train_time:87913ms step_avg:58.65ms
step:1500/2330 train_time:87973ms step_avg:58.65ms
step:1500/2330 val_loss:4.3608 train_time:88050ms step_avg:58.70ms
step:1501/2330 train_time:88071ms step_avg:58.67ms
step:1502/2330 train_time:88093ms step_avg:58.65ms
step:1503/2330 train_time:88151ms step_avg:58.65ms
step:1504/2330 train_time:88214ms step_avg:58.65ms
step:1505/2330 train_time:88271ms step_avg:58.65ms
step:1506/2330 train_time:88335ms step_avg:58.66ms
step:1507/2330 train_time:88392ms step_avg:58.65ms
step:1508/2330 train_time:88452ms step_avg:58.66ms
step:1509/2330 train_time:88509ms step_avg:58.65ms
step:1510/2330 train_time:88568ms step_avg:58.65ms
step:1511/2330 train_time:88624ms step_avg:58.65ms
step:1512/2330 train_time:88685ms step_avg:58.65ms
step:1513/2330 train_time:88742ms step_avg:58.65ms
step:1514/2330 train_time:88802ms step_avg:58.65ms
step:1515/2330 train_time:88858ms step_avg:58.65ms
step:1516/2330 train_time:88919ms step_avg:58.65ms
step:1517/2330 train_time:88977ms step_avg:58.65ms
step:1518/2330 train_time:89038ms step_avg:58.65ms
step:1519/2330 train_time:89095ms step_avg:58.65ms
step:1520/2330 train_time:89157ms step_avg:58.66ms
step:1521/2330 train_time:89215ms step_avg:58.66ms
step:1522/2330 train_time:89277ms step_avg:58.66ms
step:1523/2330 train_time:89334ms step_avg:58.66ms
step:1524/2330 train_time:89397ms step_avg:58.66ms
step:1525/2330 train_time:89453ms step_avg:58.66ms
step:1526/2330 train_time:89515ms step_avg:58.66ms
step:1527/2330 train_time:89572ms step_avg:58.66ms
step:1528/2330 train_time:89632ms step_avg:58.66ms
step:1529/2330 train_time:89690ms step_avg:58.66ms
step:1530/2330 train_time:89749ms step_avg:58.66ms
step:1531/2330 train_time:89806ms step_avg:58.66ms
step:1532/2330 train_time:89867ms step_avg:58.66ms
step:1533/2330 train_time:89924ms step_avg:58.66ms
step:1534/2330 train_time:89986ms step_avg:58.66ms
step:1535/2330 train_time:90044ms step_avg:58.66ms
step:1536/2330 train_time:90104ms step_avg:58.66ms
step:1537/2330 train_time:90162ms step_avg:58.66ms
step:1538/2330 train_time:90224ms step_avg:58.66ms
step:1539/2330 train_time:90281ms step_avg:58.66ms
step:1540/2330 train_time:90344ms step_avg:58.66ms
step:1541/2330 train_time:90401ms step_avg:58.66ms
step:1542/2330 train_time:90464ms step_avg:58.67ms
step:1543/2330 train_time:90521ms step_avg:58.67ms
step:1544/2330 train_time:90583ms step_avg:58.67ms
step:1545/2330 train_time:90640ms step_avg:58.67ms
step:1546/2330 train_time:90702ms step_avg:58.67ms
step:1547/2330 train_time:90759ms step_avg:58.67ms
step:1548/2330 train_time:90821ms step_avg:58.67ms
step:1549/2330 train_time:90878ms step_avg:58.67ms
step:1550/2330 train_time:90940ms step_avg:58.67ms
step:1551/2330 train_time:90997ms step_avg:58.67ms
step:1552/2330 train_time:91059ms step_avg:58.67ms
step:1553/2330 train_time:91116ms step_avg:58.67ms
step:1554/2330 train_time:91178ms step_avg:58.67ms
step:1555/2330 train_time:91235ms step_avg:58.67ms
step:1556/2330 train_time:91297ms step_avg:58.67ms
step:1557/2330 train_time:91355ms step_avg:58.67ms
step:1558/2330 train_time:91418ms step_avg:58.68ms
step:1559/2330 train_time:91475ms step_avg:58.68ms
step:1560/2330 train_time:91537ms step_avg:58.68ms
step:1561/2330 train_time:91595ms step_avg:58.68ms
step:1562/2330 train_time:91657ms step_avg:58.68ms
step:1563/2330 train_time:91714ms step_avg:58.68ms
step:1564/2330 train_time:91777ms step_avg:58.68ms
step:1565/2330 train_time:91834ms step_avg:58.68ms
step:1566/2330 train_time:91896ms step_avg:58.68ms
step:1567/2330 train_time:91953ms step_avg:58.68ms
step:1568/2330 train_time:92015ms step_avg:58.68ms
step:1569/2330 train_time:92072ms step_avg:58.68ms
step:1570/2330 train_time:92133ms step_avg:58.68ms
step:1571/2330 train_time:92190ms step_avg:58.68ms
step:1572/2330 train_time:92252ms step_avg:58.68ms
step:1573/2330 train_time:92310ms step_avg:58.68ms
step:1574/2330 train_time:92371ms step_avg:58.69ms
step:1575/2330 train_time:92429ms step_avg:58.68ms
step:1576/2330 train_time:92490ms step_avg:58.69ms
step:1577/2330 train_time:92548ms step_avg:58.69ms
step:1578/2330 train_time:92609ms step_avg:58.69ms
step:1579/2330 train_time:92667ms step_avg:58.69ms
step:1580/2330 train_time:92728ms step_avg:58.69ms
step:1581/2330 train_time:92786ms step_avg:58.69ms
step:1582/2330 train_time:92847ms step_avg:58.69ms
step:1583/2330 train_time:92904ms step_avg:58.69ms
step:1584/2330 train_time:92967ms step_avg:58.69ms
step:1585/2330 train_time:93023ms step_avg:58.69ms
step:1586/2330 train_time:93085ms step_avg:58.69ms
step:1587/2330 train_time:93141ms step_avg:58.69ms
step:1588/2330 train_time:93204ms step_avg:58.69ms
step:1589/2330 train_time:93260ms step_avg:58.69ms
step:1590/2330 train_time:93323ms step_avg:58.69ms
step:1591/2330 train_time:93380ms step_avg:58.69ms
step:1592/2330 train_time:93443ms step_avg:58.70ms
step:1593/2330 train_time:93499ms step_avg:58.69ms
step:1594/2330 train_time:93563ms step_avg:58.70ms
step:1595/2330 train_time:93619ms step_avg:58.70ms
step:1596/2330 train_time:93682ms step_avg:58.70ms
step:1597/2330 train_time:93738ms step_avg:58.70ms
step:1598/2330 train_time:93800ms step_avg:58.70ms
step:1599/2330 train_time:93856ms step_avg:58.70ms
step:1600/2330 train_time:93919ms step_avg:58.70ms
step:1601/2330 train_time:93976ms step_avg:58.70ms
step:1602/2330 train_time:94039ms step_avg:58.70ms
step:1603/2330 train_time:94096ms step_avg:58.70ms
step:1604/2330 train_time:94158ms step_avg:58.70ms
step:1605/2330 train_time:94215ms step_avg:58.70ms
step:1606/2330 train_time:94278ms step_avg:58.70ms
step:1607/2330 train_time:94336ms step_avg:58.70ms
step:1608/2330 train_time:94397ms step_avg:58.70ms
step:1609/2330 train_time:94455ms step_avg:58.70ms
step:1610/2330 train_time:94518ms step_avg:58.71ms
step:1611/2330 train_time:94575ms step_avg:58.71ms
step:1612/2330 train_time:94637ms step_avg:58.71ms
step:1613/2330 train_time:94694ms step_avg:58.71ms
step:1614/2330 train_time:94756ms step_avg:58.71ms
step:1615/2330 train_time:94814ms step_avg:58.71ms
step:1616/2330 train_time:94875ms step_avg:58.71ms
step:1617/2330 train_time:94931ms step_avg:58.71ms
step:1618/2330 train_time:94994ms step_avg:58.71ms
step:1619/2330 train_time:95052ms step_avg:58.71ms
step:1620/2330 train_time:95112ms step_avg:58.71ms
step:1621/2330 train_time:95169ms step_avg:58.71ms
step:1622/2330 train_time:95230ms step_avg:58.71ms
step:1623/2330 train_time:95288ms step_avg:58.71ms
step:1624/2330 train_time:95349ms step_avg:58.71ms
step:1625/2330 train_time:95406ms step_avg:58.71ms
step:1626/2330 train_time:95467ms step_avg:58.71ms
step:1627/2330 train_time:95524ms step_avg:58.71ms
step:1628/2330 train_time:95586ms step_avg:58.71ms
step:1629/2330 train_time:95643ms step_avg:58.71ms
step:1630/2330 train_time:95705ms step_avg:58.71ms
step:1631/2330 train_time:95762ms step_avg:58.71ms
step:1632/2330 train_time:95824ms step_avg:58.72ms
step:1633/2330 train_time:95880ms step_avg:58.71ms
step:1634/2330 train_time:95942ms step_avg:58.72ms
step:1635/2330 train_time:95999ms step_avg:58.71ms
step:1636/2330 train_time:96062ms step_avg:58.72ms
step:1637/2330 train_time:96118ms step_avg:58.72ms
step:1638/2330 train_time:96181ms step_avg:58.72ms
step:1639/2330 train_time:96238ms step_avg:58.72ms
step:1640/2330 train_time:96301ms step_avg:58.72ms
step:1641/2330 train_time:96357ms step_avg:58.72ms
step:1642/2330 train_time:96420ms step_avg:58.72ms
step:1643/2330 train_time:96477ms step_avg:58.72ms
step:1644/2330 train_time:96540ms step_avg:58.72ms
step:1645/2330 train_time:96597ms step_avg:58.72ms
step:1646/2330 train_time:96660ms step_avg:58.72ms
step:1647/2330 train_time:96717ms step_avg:58.72ms
step:1648/2330 train_time:96779ms step_avg:58.72ms
step:1649/2330 train_time:96836ms step_avg:58.72ms
step:1650/2330 train_time:96897ms step_avg:58.73ms
step:1651/2330 train_time:96955ms step_avg:58.72ms
step:1652/2330 train_time:97016ms step_avg:58.73ms
step:1653/2330 train_time:97073ms step_avg:58.73ms
step:1654/2330 train_time:97136ms step_avg:58.73ms
step:1655/2330 train_time:97194ms step_avg:58.73ms
step:1656/2330 train_time:97254ms step_avg:58.73ms
step:1657/2330 train_time:97311ms step_avg:58.73ms
step:1658/2330 train_time:97373ms step_avg:58.73ms
step:1659/2330 train_time:97431ms step_avg:58.73ms
step:1660/2330 train_time:97492ms step_avg:58.73ms
step:1661/2330 train_time:97549ms step_avg:58.73ms
step:1662/2330 train_time:97610ms step_avg:58.73ms
step:1663/2330 train_time:97668ms step_avg:58.73ms
step:1664/2330 train_time:97729ms step_avg:58.73ms
step:1665/2330 train_time:97786ms step_avg:58.73ms
step:1666/2330 train_time:97849ms step_avg:58.73ms
step:1667/2330 train_time:97906ms step_avg:58.73ms
step:1668/2330 train_time:97967ms step_avg:58.73ms
step:1669/2330 train_time:98025ms step_avg:58.73ms
step:1670/2330 train_time:98086ms step_avg:58.73ms
step:1671/2330 train_time:98143ms step_avg:58.73ms
step:1672/2330 train_time:98205ms step_avg:58.74ms
step:1673/2330 train_time:98262ms step_avg:58.73ms
step:1674/2330 train_time:98324ms step_avg:58.74ms
step:1675/2330 train_time:98380ms step_avg:58.73ms
step:1676/2330 train_time:98443ms step_avg:58.74ms
step:1677/2330 train_time:98499ms step_avg:58.74ms
step:1678/2330 train_time:98563ms step_avg:58.74ms
step:1679/2330 train_time:98619ms step_avg:58.74ms
step:1680/2330 train_time:98683ms step_avg:58.74ms
step:1681/2330 train_time:98740ms step_avg:58.74ms
step:1682/2330 train_time:98802ms step_avg:58.74ms
step:1683/2330 train_time:98858ms step_avg:58.74ms
step:1684/2330 train_time:98921ms step_avg:58.74ms
step:1685/2330 train_time:98978ms step_avg:58.74ms
step:1686/2330 train_time:99041ms step_avg:58.74ms
step:1687/2330 train_time:99097ms step_avg:58.74ms
step:1688/2330 train_time:99162ms step_avg:58.75ms
step:1689/2330 train_time:99218ms step_avg:58.74ms
step:1690/2330 train_time:99281ms step_avg:58.75ms
step:1691/2330 train_time:99338ms step_avg:58.74ms
step:1692/2330 train_time:99401ms step_avg:58.75ms
step:1693/2330 train_time:99457ms step_avg:58.75ms
step:1694/2330 train_time:99520ms step_avg:58.75ms
step:1695/2330 train_time:99576ms step_avg:58.75ms
step:1696/2330 train_time:99640ms step_avg:58.75ms
step:1697/2330 train_time:99697ms step_avg:58.75ms
step:1698/2330 train_time:99759ms step_avg:58.75ms
step:1699/2330 train_time:99816ms step_avg:58.75ms
step:1700/2330 train_time:99877ms step_avg:58.75ms
step:1701/2330 train_time:99934ms step_avg:58.75ms
step:1702/2330 train_time:99997ms step_avg:58.75ms
step:1703/2330 train_time:100054ms step_avg:58.75ms
step:1704/2330 train_time:100115ms step_avg:58.75ms
step:1705/2330 train_time:100173ms step_avg:58.75ms
step:1706/2330 train_time:100235ms step_avg:58.75ms
step:1707/2330 train_time:100292ms step_avg:58.75ms
step:1708/2330 train_time:100354ms step_avg:58.76ms
step:1709/2330 train_time:100411ms step_avg:58.75ms
step:1710/2330 train_time:100472ms step_avg:58.76ms
step:1711/2330 train_time:100529ms step_avg:58.75ms
step:1712/2330 train_time:100591ms step_avg:58.76ms
step:1713/2330 train_time:100648ms step_avg:58.76ms
step:1714/2330 train_time:100710ms step_avg:58.76ms
step:1715/2330 train_time:100767ms step_avg:58.76ms
step:1716/2330 train_time:100829ms step_avg:58.76ms
step:1717/2330 train_time:100886ms step_avg:58.76ms
step:1718/2330 train_time:100947ms step_avg:58.76ms
step:1719/2330 train_time:101004ms step_avg:58.76ms
step:1720/2330 train_time:101066ms step_avg:58.76ms
step:1721/2330 train_time:101122ms step_avg:58.76ms
step:1722/2330 train_time:101184ms step_avg:58.76ms
step:1723/2330 train_time:101241ms step_avg:58.76ms
step:1724/2330 train_time:101303ms step_avg:58.76ms
step:1725/2330 train_time:101360ms step_avg:58.76ms
step:1726/2330 train_time:101422ms step_avg:58.76ms
step:1727/2330 train_time:101479ms step_avg:58.76ms
step:1728/2330 train_time:101541ms step_avg:58.76ms
step:1729/2330 train_time:101598ms step_avg:58.76ms
step:1730/2330 train_time:101662ms step_avg:58.76ms
step:1731/2330 train_time:101718ms step_avg:58.76ms
step:1732/2330 train_time:101781ms step_avg:58.77ms
step:1733/2330 train_time:101838ms step_avg:58.76ms
step:1734/2330 train_time:101900ms step_avg:58.77ms
step:1735/2330 train_time:101956ms step_avg:58.76ms
step:1736/2330 train_time:102018ms step_avg:58.77ms
step:1737/2330 train_time:102076ms step_avg:58.77ms
step:1738/2330 train_time:102138ms step_avg:58.77ms
step:1739/2330 train_time:102195ms step_avg:58.77ms
step:1740/2330 train_time:102257ms step_avg:58.77ms
step:1741/2330 train_time:102314ms step_avg:58.77ms
step:1742/2330 train_time:102377ms step_avg:58.77ms
step:1743/2330 train_time:102435ms step_avg:58.77ms
step:1744/2330 train_time:102497ms step_avg:58.77ms
step:1745/2330 train_time:102555ms step_avg:58.77ms
step:1746/2330 train_time:102617ms step_avg:58.77ms
step:1747/2330 train_time:102674ms step_avg:58.77ms
step:1748/2330 train_time:102737ms step_avg:58.77ms
step:1749/2330 train_time:102795ms step_avg:58.77ms
step:1750/2330 train_time:102856ms step_avg:58.77ms
step:1750/2330 val_loss:4.2447 train_time:102935ms step_avg:58.82ms
step:1751/2330 train_time:102956ms step_avg:58.80ms
step:1752/2330 train_time:102977ms step_avg:58.78ms
step:1753/2330 train_time:103034ms step_avg:58.78ms
step:1754/2330 train_time:103101ms step_avg:58.78ms
step:1755/2330 train_time:103157ms step_avg:58.78ms
step:1756/2330 train_time:103221ms step_avg:58.78ms
step:1757/2330 train_time:103277ms step_avg:58.78ms
step:1758/2330 train_time:103339ms step_avg:58.78ms
step:1759/2330 train_time:103396ms step_avg:58.78ms
step:1760/2330 train_time:103457ms step_avg:58.78ms
step:1761/2330 train_time:103513ms step_avg:58.78ms
step:1762/2330 train_time:103574ms step_avg:58.78ms
step:1763/2330 train_time:103631ms step_avg:58.78ms
step:1764/2330 train_time:103691ms step_avg:58.78ms
step:1765/2330 train_time:103748ms step_avg:58.78ms
step:1766/2330 train_time:103809ms step_avg:58.78ms
step:1767/2330 train_time:103868ms step_avg:58.78ms
step:1768/2330 train_time:103930ms step_avg:58.78ms
step:1769/2330 train_time:103987ms step_avg:58.78ms
step:1770/2330 train_time:104049ms step_avg:58.78ms
step:1771/2330 train_time:104106ms step_avg:58.78ms
step:1772/2330 train_time:104168ms step_avg:58.79ms
step:1773/2330 train_time:104225ms step_avg:58.78ms
step:1774/2330 train_time:104289ms step_avg:58.79ms
step:1775/2330 train_time:104345ms step_avg:58.79ms
step:1776/2330 train_time:104408ms step_avg:58.79ms
step:1777/2330 train_time:104464ms step_avg:58.79ms
step:1778/2330 train_time:104527ms step_avg:58.79ms
step:1779/2330 train_time:104584ms step_avg:58.79ms
step:1780/2330 train_time:104645ms step_avg:58.79ms
step:1781/2330 train_time:104701ms step_avg:58.79ms
step:1782/2330 train_time:104763ms step_avg:58.79ms
step:1783/2330 train_time:104820ms step_avg:58.79ms
step:1784/2330 train_time:104883ms step_avg:58.79ms
step:1785/2330 train_time:104939ms step_avg:58.79ms
step:1786/2330 train_time:105003ms step_avg:58.79ms
step:1787/2330 train_time:105059ms step_avg:58.79ms
step:1788/2330 train_time:105123ms step_avg:58.79ms
step:1789/2330 train_time:105180ms step_avg:58.79ms
step:1790/2330 train_time:105242ms step_avg:58.79ms
step:1791/2330 train_time:105299ms step_avg:58.79ms
step:1792/2330 train_time:105362ms step_avg:58.80ms
step:1793/2330 train_time:105419ms step_avg:58.79ms
step:1794/2330 train_time:105481ms step_avg:58.80ms
step:1795/2330 train_time:105538ms step_avg:58.80ms
step:1796/2330 train_time:105599ms step_avg:58.80ms
step:1797/2330 train_time:105655ms step_avg:58.80ms
step:1798/2330 train_time:105717ms step_avg:58.80ms
step:1799/2330 train_time:105774ms step_avg:58.80ms
step:1800/2330 train_time:105836ms step_avg:58.80ms
step:1801/2330 train_time:105893ms step_avg:58.80ms
step:1802/2330 train_time:105954ms step_avg:58.80ms
step:1803/2330 train_time:106013ms step_avg:58.80ms
step:1804/2330 train_time:106074ms step_avg:58.80ms
step:1805/2330 train_time:106131ms step_avg:58.80ms
step:1806/2330 train_time:106192ms step_avg:58.80ms
step:1807/2330 train_time:106249ms step_avg:58.80ms
step:1808/2330 train_time:106311ms step_avg:58.80ms
step:1809/2330 train_time:106368ms step_avg:58.80ms
step:1810/2330 train_time:106431ms step_avg:58.80ms
step:1811/2330 train_time:106488ms step_avg:58.80ms
step:1812/2330 train_time:106550ms step_avg:58.80ms
step:1813/2330 train_time:106606ms step_avg:58.80ms
step:1814/2330 train_time:106669ms step_avg:58.80ms
step:1815/2330 train_time:106726ms step_avg:58.80ms
step:1816/2330 train_time:106788ms step_avg:58.80ms
step:1817/2330 train_time:106845ms step_avg:58.80ms
step:1818/2330 train_time:106907ms step_avg:58.80ms
step:1819/2330 train_time:106963ms step_avg:58.80ms
step:1820/2330 train_time:107026ms step_avg:58.81ms
step:1821/2330 train_time:107082ms step_avg:58.80ms
step:1822/2330 train_time:107145ms step_avg:58.81ms
step:1823/2330 train_time:107202ms step_avg:58.81ms
step:1824/2330 train_time:107265ms step_avg:58.81ms
step:1825/2330 train_time:107322ms step_avg:58.81ms
step:1826/2330 train_time:107385ms step_avg:58.81ms
step:1827/2330 train_time:107441ms step_avg:58.81ms
step:1828/2330 train_time:107504ms step_avg:58.81ms
step:1829/2330 train_time:107561ms step_avg:58.81ms
step:1830/2330 train_time:107623ms step_avg:58.81ms
step:1831/2330 train_time:107679ms step_avg:58.81ms
step:1832/2330 train_time:107742ms step_avg:58.81ms
step:1833/2330 train_time:107799ms step_avg:58.81ms
step:1834/2330 train_time:107862ms step_avg:58.81ms
step:1835/2330 train_time:107919ms step_avg:58.81ms
step:1836/2330 train_time:107981ms step_avg:58.81ms
step:1837/2330 train_time:108038ms step_avg:58.81ms
step:1838/2330 train_time:108099ms step_avg:58.81ms
step:1839/2330 train_time:108157ms step_avg:58.81ms
step:1840/2330 train_time:108218ms step_avg:58.81ms
step:1841/2330 train_time:108275ms step_avg:58.81ms
step:1842/2330 train_time:108337ms step_avg:58.82ms
step:1843/2330 train_time:108394ms step_avg:58.81ms
step:1844/2330 train_time:108457ms step_avg:58.82ms
step:1845/2330 train_time:108515ms step_avg:58.82ms
step:1846/2330 train_time:108576ms step_avg:58.82ms
step:1847/2330 train_time:108633ms step_avg:58.82ms
step:1848/2330 train_time:108694ms step_avg:58.82ms
step:1849/2330 train_time:108751ms step_avg:58.82ms
step:1850/2330 train_time:108813ms step_avg:58.82ms
step:1851/2330 train_time:108870ms step_avg:58.82ms
step:1852/2330 train_time:108933ms step_avg:58.82ms
step:1853/2330 train_time:108990ms step_avg:58.82ms
step:1854/2330 train_time:109051ms step_avg:58.82ms
step:1855/2330 train_time:109108ms step_avg:58.82ms
step:1856/2330 train_time:109170ms step_avg:58.82ms
step:1857/2330 train_time:109227ms step_avg:58.82ms
step:1858/2330 train_time:109289ms step_avg:58.82ms
step:1859/2330 train_time:109346ms step_avg:58.82ms
step:1860/2330 train_time:109409ms step_avg:58.82ms
step:1861/2330 train_time:109466ms step_avg:58.82ms
step:1862/2330 train_time:109528ms step_avg:58.82ms
step:1863/2330 train_time:109585ms step_avg:58.82ms
step:1864/2330 train_time:109647ms step_avg:58.82ms
step:1865/2330 train_time:109703ms step_avg:58.82ms
step:1866/2330 train_time:109767ms step_avg:58.82ms
step:1867/2330 train_time:109823ms step_avg:58.82ms
step:1868/2330 train_time:109887ms step_avg:58.83ms
step:1869/2330 train_time:109943ms step_avg:58.82ms
step:1870/2330 train_time:110007ms step_avg:58.83ms
step:1871/2330 train_time:110063ms step_avg:58.83ms
step:1872/2330 train_time:110125ms step_avg:58.83ms
step:1873/2330 train_time:110182ms step_avg:58.83ms
step:1874/2330 train_time:110244ms step_avg:58.83ms
step:1875/2330 train_time:110300ms step_avg:58.83ms
step:1876/2330 train_time:110363ms step_avg:58.83ms
step:1877/2330 train_time:110420ms step_avg:58.83ms
step:1878/2330 train_time:110483ms step_avg:58.83ms
step:1879/2330 train_time:110540ms step_avg:58.83ms
step:1880/2330 train_time:110603ms step_avg:58.83ms
step:1881/2330 train_time:110660ms step_avg:58.83ms
step:1882/2330 train_time:110723ms step_avg:58.83ms
step:1883/2330 train_time:110779ms step_avg:58.83ms
step:1884/2330 train_time:110843ms step_avg:58.83ms
step:1885/2330 train_time:110900ms step_avg:58.83ms
step:1886/2330 train_time:110963ms step_avg:58.84ms
step:1887/2330 train_time:111020ms step_avg:58.83ms
step:1888/2330 train_time:111082ms step_avg:58.84ms
step:1889/2330 train_time:111139ms step_avg:58.83ms
step:1890/2330 train_time:111201ms step_avg:58.84ms
step:1891/2330 train_time:111258ms step_avg:58.84ms
step:1892/2330 train_time:111320ms step_avg:58.84ms
step:1893/2330 train_time:111378ms step_avg:58.84ms
step:1894/2330 train_time:111439ms step_avg:58.84ms
step:1895/2330 train_time:111497ms step_avg:58.84ms
step:1896/2330 train_time:111558ms step_avg:58.84ms
step:1897/2330 train_time:111615ms step_avg:58.84ms
step:1898/2330 train_time:111677ms step_avg:58.84ms
step:1899/2330 train_time:111735ms step_avg:58.84ms
step:1900/2330 train_time:111797ms step_avg:58.84ms
step:1901/2330 train_time:111854ms step_avg:58.84ms
step:1902/2330 train_time:111916ms step_avg:58.84ms
step:1903/2330 train_time:111977ms step_avg:58.84ms
step:1904/2330 train_time:112035ms step_avg:58.84ms
step:1905/2330 train_time:112092ms step_avg:58.84ms
step:1906/2330 train_time:112154ms step_avg:58.84ms
step:1907/2330 train_time:112212ms step_avg:58.84ms
step:1908/2330 train_time:112273ms step_avg:58.84ms
step:1909/2330 train_time:112331ms step_avg:58.84ms
step:1910/2330 train_time:112392ms step_avg:58.84ms
step:1911/2330 train_time:112449ms step_avg:58.84ms
step:1912/2330 train_time:112510ms step_avg:58.84ms
step:1913/2330 train_time:112567ms step_avg:58.84ms
step:1914/2330 train_time:112628ms step_avg:58.84ms
step:1915/2330 train_time:112685ms step_avg:58.84ms
step:1916/2330 train_time:112747ms step_avg:58.85ms
step:1917/2330 train_time:112804ms step_avg:58.84ms
step:1918/2330 train_time:112867ms step_avg:58.85ms
step:1919/2330 train_time:112923ms step_avg:58.84ms
step:1920/2330 train_time:112986ms step_avg:58.85ms
step:1921/2330 train_time:113042ms step_avg:58.85ms
step:1922/2330 train_time:113105ms step_avg:58.85ms
step:1923/2330 train_time:113162ms step_avg:58.85ms
step:1924/2330 train_time:113225ms step_avg:58.85ms
step:1925/2330 train_time:113281ms step_avg:58.85ms
step:1926/2330 train_time:113344ms step_avg:58.85ms
step:1927/2330 train_time:113400ms step_avg:58.85ms
step:1928/2330 train_time:113463ms step_avg:58.85ms
step:1929/2330 train_time:113520ms step_avg:58.85ms
step:1930/2330 train_time:113583ms step_avg:58.85ms
step:1931/2330 train_time:113639ms step_avg:58.85ms
step:1932/2330 train_time:113703ms step_avg:58.85ms
step:1933/2330 train_time:113760ms step_avg:58.85ms
step:1934/2330 train_time:113822ms step_avg:58.85ms
step:1935/2330 train_time:113878ms step_avg:58.85ms
step:1936/2330 train_time:113941ms step_avg:58.85ms
step:1937/2330 train_time:113998ms step_avg:58.85ms
step:1938/2330 train_time:114060ms step_avg:58.85ms
step:1939/2330 train_time:114118ms step_avg:58.85ms
step:1940/2330 train_time:114181ms step_avg:58.86ms
step:1941/2330 train_time:114238ms step_avg:58.86ms
step:1942/2330 train_time:114300ms step_avg:58.86ms
step:1943/2330 train_time:114357ms step_avg:58.86ms
step:1944/2330 train_time:114419ms step_avg:58.86ms
step:1945/2330 train_time:114476ms step_avg:58.86ms
step:1946/2330 train_time:114538ms step_avg:58.86ms
step:1947/2330 train_time:114595ms step_avg:58.86ms
step:1948/2330 train_time:114657ms step_avg:58.86ms
step:1949/2330 train_time:114715ms step_avg:58.86ms
step:1950/2330 train_time:114776ms step_avg:58.86ms
step:1951/2330 train_time:114833ms step_avg:58.86ms
step:1952/2330 train_time:114894ms step_avg:58.86ms
step:1953/2330 train_time:114952ms step_avg:58.86ms
step:1954/2330 train_time:115013ms step_avg:58.86ms
step:1955/2330 train_time:115071ms step_avg:58.86ms
step:1956/2330 train_time:115132ms step_avg:58.86ms
step:1957/2330 train_time:115190ms step_avg:58.86ms
step:1958/2330 train_time:115250ms step_avg:58.86ms
step:1959/2330 train_time:115307ms step_avg:58.86ms
step:1960/2330 train_time:115369ms step_avg:58.86ms
step:1961/2330 train_time:115426ms step_avg:58.86ms
step:1962/2330 train_time:115488ms step_avg:58.86ms
step:1963/2330 train_time:115544ms step_avg:58.86ms
step:1964/2330 train_time:115608ms step_avg:58.86ms
step:1965/2330 train_time:115664ms step_avg:58.86ms
step:1966/2330 train_time:115727ms step_avg:58.86ms
step:1967/2330 train_time:115784ms step_avg:58.86ms
step:1968/2330 train_time:115847ms step_avg:58.87ms
step:1969/2330 train_time:115903ms step_avg:58.86ms
step:1970/2330 train_time:115966ms step_avg:58.87ms
step:1971/2330 train_time:116023ms step_avg:58.87ms
step:1972/2330 train_time:116085ms step_avg:58.87ms
step:1973/2330 train_time:116142ms step_avg:58.87ms
step:1974/2330 train_time:116204ms step_avg:58.87ms
step:1975/2330 train_time:116261ms step_avg:58.87ms
step:1976/2330 train_time:116323ms step_avg:58.87ms
step:1977/2330 train_time:116380ms step_avg:58.87ms
step:1978/2330 train_time:116443ms step_avg:58.87ms
step:1979/2330 train_time:116500ms step_avg:58.87ms
step:1980/2330 train_time:116562ms step_avg:58.87ms
step:1981/2330 train_time:116618ms step_avg:58.87ms
step:1982/2330 train_time:116681ms step_avg:58.87ms
step:1983/2330 train_time:116738ms step_avg:58.87ms
step:1984/2330 train_time:116801ms step_avg:58.87ms
step:1985/2330 train_time:116858ms step_avg:58.87ms
step:1986/2330 train_time:116921ms step_avg:58.87ms
step:1987/2330 train_time:116978ms step_avg:58.87ms
step:1988/2330 train_time:117040ms step_avg:58.87ms
step:1989/2330 train_time:117097ms step_avg:58.87ms
step:1990/2330 train_time:117161ms step_avg:58.87ms
step:1991/2330 train_time:117218ms step_avg:58.87ms
step:1992/2330 train_time:117280ms step_avg:58.88ms
step:1993/2330 train_time:117338ms step_avg:58.87ms
step:1994/2330 train_time:117399ms step_avg:58.88ms
step:1995/2330 train_time:117456ms step_avg:58.88ms
step:1996/2330 train_time:117518ms step_avg:58.88ms
step:1997/2330 train_time:117575ms step_avg:58.88ms
step:1998/2330 train_time:117637ms step_avg:58.88ms
step:1999/2330 train_time:117694ms step_avg:58.88ms
step:2000/2330 train_time:117756ms step_avg:58.88ms
step:2000/2330 val_loss:4.1747 train_time:117835ms step_avg:58.92ms
step:2001/2330 train_time:117855ms step_avg:58.90ms
step:2002/2330 train_time:117878ms step_avg:58.88ms
step:2003/2330 train_time:117936ms step_avg:58.88ms
step:2004/2330 train_time:118003ms step_avg:58.88ms
step:2005/2330 train_time:118060ms step_avg:58.88ms
step:2006/2330 train_time:118124ms step_avg:58.89ms
step:2007/2330 train_time:118181ms step_avg:58.88ms
step:2008/2330 train_time:118242ms step_avg:58.89ms
step:2009/2330 train_time:118299ms step_avg:58.88ms
step:2010/2330 train_time:118360ms step_avg:58.89ms
step:2011/2330 train_time:118416ms step_avg:58.88ms
step:2012/2330 train_time:118477ms step_avg:58.89ms
step:2013/2330 train_time:118533ms step_avg:58.88ms
step:2014/2330 train_time:118595ms step_avg:58.89ms
step:2015/2330 train_time:118651ms step_avg:58.88ms
step:2016/2330 train_time:118713ms step_avg:58.89ms
step:2017/2330 train_time:118769ms step_avg:58.88ms
step:2018/2330 train_time:118832ms step_avg:58.89ms
step:2019/2330 train_time:118890ms step_avg:58.89ms
step:2020/2330 train_time:118955ms step_avg:58.89ms
step:2021/2330 train_time:119012ms step_avg:58.89ms
step:2022/2330 train_time:119077ms step_avg:58.89ms
step:2023/2330 train_time:119134ms step_avg:58.89ms
step:2024/2330 train_time:119198ms step_avg:58.89ms
step:2025/2330 train_time:119254ms step_avg:58.89ms
step:2026/2330 train_time:119318ms step_avg:58.89ms
step:2027/2330 train_time:119374ms step_avg:58.89ms
step:2028/2330 train_time:119436ms step_avg:58.89ms
step:2029/2330 train_time:119492ms step_avg:58.89ms
step:2030/2330 train_time:119553ms step_avg:58.89ms
step:2031/2330 train_time:119610ms step_avg:58.89ms
step:2032/2330 train_time:119671ms step_avg:58.89ms
step:2033/2330 train_time:119728ms step_avg:58.89ms
step:2034/2330 train_time:119789ms step_avg:58.89ms
step:2035/2330 train_time:119846ms step_avg:58.89ms
step:2036/2330 train_time:119908ms step_avg:58.89ms
step:2037/2330 train_time:119966ms step_avg:58.89ms
step:2038/2330 train_time:120029ms step_avg:58.90ms
step:2039/2330 train_time:120088ms step_avg:58.90ms
step:2040/2330 train_time:120149ms step_avg:58.90ms
step:2041/2330 train_time:120207ms step_avg:58.90ms
step:2042/2330 train_time:120269ms step_avg:58.90ms
step:2043/2330 train_time:120326ms step_avg:58.90ms
step:2044/2330 train_time:120387ms step_avg:58.90ms
step:2045/2330 train_time:120445ms step_avg:58.90ms
step:2046/2330 train_time:120506ms step_avg:58.90ms
step:2047/2330 train_time:120564ms step_avg:58.90ms
step:2048/2330 train_time:120625ms step_avg:58.90ms
step:2049/2330 train_time:120682ms step_avg:58.90ms
step:2050/2330 train_time:120742ms step_avg:58.90ms
step:2051/2330 train_time:120799ms step_avg:58.90ms
step:2052/2330 train_time:120861ms step_avg:58.90ms
step:2053/2330 train_time:120918ms step_avg:58.90ms
step:2054/2330 train_time:120981ms step_avg:58.90ms
step:2055/2330 train_time:121037ms step_avg:58.90ms
step:2056/2330 train_time:121101ms step_avg:58.90ms
step:2057/2330 train_time:121158ms step_avg:58.90ms
step:2058/2330 train_time:121221ms step_avg:58.90ms
step:2059/2330 train_time:121278ms step_avg:58.90ms
step:2060/2330 train_time:121341ms step_avg:58.90ms
step:2061/2330 train_time:121398ms step_avg:58.90ms
step:2062/2330 train_time:121460ms step_avg:58.90ms
step:2063/2330 train_time:121517ms step_avg:58.90ms
step:2064/2330 train_time:121579ms step_avg:58.90ms
step:2065/2330 train_time:121635ms step_avg:58.90ms
step:2066/2330 train_time:121697ms step_avg:58.90ms
step:2067/2330 train_time:121753ms step_avg:58.90ms
step:2068/2330 train_time:121816ms step_avg:58.91ms
step:2069/2330 train_time:121873ms step_avg:58.90ms
step:2070/2330 train_time:121936ms step_avg:58.91ms
step:2071/2330 train_time:121993ms step_avg:58.91ms
step:2072/2330 train_time:122056ms step_avg:58.91ms
step:2073/2330 train_time:122113ms step_avg:58.91ms
step:2074/2330 train_time:122175ms step_avg:58.91ms
step:2075/2330 train_time:122232ms step_avg:58.91ms
step:2076/2330 train_time:122295ms step_avg:58.91ms
step:2077/2330 train_time:122352ms step_avg:58.91ms
step:2078/2330 train_time:122415ms step_avg:58.91ms
step:2079/2330 train_time:122472ms step_avg:58.91ms
step:2080/2330 train_time:122533ms step_avg:58.91ms
step:2081/2330 train_time:122591ms step_avg:58.91ms
step:2082/2330 train_time:122653ms step_avg:58.91ms
step:2083/2330 train_time:122710ms step_avg:58.91ms
step:2084/2330 train_time:122771ms step_avg:58.91ms
step:2085/2330 train_time:122829ms step_avg:58.91ms
step:2086/2330 train_time:122891ms step_avg:58.91ms
step:2087/2330 train_time:122949ms step_avg:58.91ms
step:2088/2330 train_time:123011ms step_avg:58.91ms
step:2089/2330 train_time:123069ms step_avg:58.91ms
step:2090/2330 train_time:123129ms step_avg:58.91ms
step:2091/2330 train_time:123187ms step_avg:58.91ms
step:2092/2330 train_time:123249ms step_avg:58.91ms
step:2093/2330 train_time:123307ms step_avg:58.91ms
step:2094/2330 train_time:123368ms step_avg:58.91ms
step:2095/2330 train_time:123426ms step_avg:58.91ms
step:2096/2330 train_time:123487ms step_avg:58.92ms
step:2097/2330 train_time:123544ms step_avg:58.91ms
step:2098/2330 train_time:123605ms step_avg:58.92ms
step:2099/2330 train_time:123662ms step_avg:58.91ms
step:2100/2330 train_time:123722ms step_avg:58.92ms
step:2101/2330 train_time:123780ms step_avg:58.91ms
step:2102/2330 train_time:123841ms step_avg:58.92ms
step:2103/2330 train_time:123899ms step_avg:58.92ms
step:2104/2330 train_time:123961ms step_avg:58.92ms
step:2105/2330 train_time:124018ms step_avg:58.92ms
step:2106/2330 train_time:124080ms step_avg:58.92ms
step:2107/2330 train_time:124137ms step_avg:58.92ms
step:2108/2330 train_time:124200ms step_avg:58.92ms
step:2109/2330 train_time:124257ms step_avg:58.92ms
step:2110/2330 train_time:124320ms step_avg:58.92ms
step:2111/2330 train_time:124377ms step_avg:58.92ms
step:2112/2330 train_time:124439ms step_avg:58.92ms
step:2113/2330 train_time:124496ms step_avg:58.92ms
step:2114/2330 train_time:124558ms step_avg:58.92ms
step:2115/2330 train_time:124614ms step_avg:58.92ms
step:2116/2330 train_time:124676ms step_avg:58.92ms
step:2117/2330 train_time:124733ms step_avg:58.92ms
step:2118/2330 train_time:124795ms step_avg:58.92ms
step:2119/2330 train_time:124852ms step_avg:58.92ms
step:2120/2330 train_time:124915ms step_avg:58.92ms
step:2121/2330 train_time:124972ms step_avg:58.92ms
step:2122/2330 train_time:125034ms step_avg:58.92ms
step:2123/2330 train_time:125092ms step_avg:58.92ms
step:2124/2330 train_time:125154ms step_avg:58.92ms
step:2125/2330 train_time:125212ms step_avg:58.92ms
step:2126/2330 train_time:125273ms step_avg:58.92ms
step:2127/2330 train_time:125330ms step_avg:58.92ms
step:2128/2330 train_time:125394ms step_avg:58.93ms
step:2129/2330 train_time:125451ms step_avg:58.92ms
step:2130/2330 train_time:125513ms step_avg:58.93ms
step:2131/2330 train_time:125570ms step_avg:58.93ms
step:2132/2330 train_time:125632ms step_avg:58.93ms
step:2133/2330 train_time:125690ms step_avg:58.93ms
step:2134/2330 train_time:125751ms step_avg:58.93ms
step:2135/2330 train_time:125808ms step_avg:58.93ms
step:2136/2330 train_time:125869ms step_avg:58.93ms
step:2137/2330 train_time:125926ms step_avg:58.93ms
step:2138/2330 train_time:125987ms step_avg:58.93ms
step:2139/2330 train_time:126045ms step_avg:58.93ms
step:2140/2330 train_time:126106ms step_avg:58.93ms
step:2141/2330 train_time:126164ms step_avg:58.93ms
step:2142/2330 train_time:126225ms step_avg:58.93ms
step:2143/2330 train_time:126282ms step_avg:58.93ms
step:2144/2330 train_time:126344ms step_avg:58.93ms
step:2145/2330 train_time:126401ms step_avg:58.93ms
step:2146/2330 train_time:126462ms step_avg:58.93ms
step:2147/2330 train_time:126519ms step_avg:58.93ms
step:2148/2330 train_time:126582ms step_avg:58.93ms
step:2149/2330 train_time:126638ms step_avg:58.93ms
step:2150/2330 train_time:126701ms step_avg:58.93ms
step:2151/2330 train_time:126757ms step_avg:58.93ms
step:2152/2330 train_time:126820ms step_avg:58.93ms
step:2153/2330 train_time:126876ms step_avg:58.93ms
step:2154/2330 train_time:126940ms step_avg:58.93ms
step:2155/2330 train_time:126997ms step_avg:58.93ms
step:2156/2330 train_time:127059ms step_avg:58.93ms
step:2157/2330 train_time:127115ms step_avg:58.93ms
step:2158/2330 train_time:127178ms step_avg:58.93ms
step:2159/2330 train_time:127234ms step_avg:58.93ms
step:2160/2330 train_time:127298ms step_avg:58.93ms
step:2161/2330 train_time:127355ms step_avg:58.93ms
step:2162/2330 train_time:127418ms step_avg:58.94ms
step:2163/2330 train_time:127475ms step_avg:58.93ms
step:2164/2330 train_time:127536ms step_avg:58.94ms
step:2165/2330 train_time:127593ms step_avg:58.93ms
step:2166/2330 train_time:127655ms step_avg:58.94ms
step:2167/2330 train_time:127712ms step_avg:58.94ms
step:2168/2330 train_time:127774ms step_avg:58.94ms
step:2169/2330 train_time:127831ms step_avg:58.94ms
step:2170/2330 train_time:127896ms step_avg:58.94ms
step:2171/2330 train_time:127953ms step_avg:58.94ms
step:2172/2330 train_time:128016ms step_avg:58.94ms
step:2173/2330 train_time:128072ms step_avg:58.94ms
step:2174/2330 train_time:128136ms step_avg:58.94ms
step:2175/2330 train_time:128193ms step_avg:58.94ms
step:2176/2330 train_time:128255ms step_avg:58.94ms
step:2177/2330 train_time:128312ms step_avg:58.94ms
step:2178/2330 train_time:128374ms step_avg:58.94ms
step:2179/2330 train_time:128431ms step_avg:58.94ms
step:2180/2330 train_time:128494ms step_avg:58.94ms
step:2181/2330 train_time:128551ms step_avg:58.94ms
step:2182/2330 train_time:128614ms step_avg:58.94ms
step:2183/2330 train_time:128671ms step_avg:58.94ms
step:2184/2330 train_time:128733ms step_avg:58.94ms
step:2185/2330 train_time:128791ms step_avg:58.94ms
step:2186/2330 train_time:128853ms step_avg:58.94ms
step:2187/2330 train_time:128911ms step_avg:58.94ms
step:2188/2330 train_time:128973ms step_avg:58.95ms
step:2189/2330 train_time:129030ms step_avg:58.94ms
step:2190/2330 train_time:129093ms step_avg:58.95ms
step:2191/2330 train_time:129151ms step_avg:58.95ms
step:2192/2330 train_time:129213ms step_avg:58.95ms
step:2193/2330 train_time:129270ms step_avg:58.95ms
step:2194/2330 train_time:129332ms step_avg:58.95ms
step:2195/2330 train_time:129389ms step_avg:58.95ms
step:2196/2330 train_time:129452ms step_avg:58.95ms
step:2197/2330 train_time:129508ms step_avg:58.95ms
step:2198/2330 train_time:129571ms step_avg:58.95ms
step:2199/2330 train_time:129628ms step_avg:58.95ms
step:2200/2330 train_time:129690ms step_avg:58.95ms
step:2201/2330 train_time:129747ms step_avg:58.95ms
step:2202/2330 train_time:129808ms step_avg:58.95ms
step:2203/2330 train_time:129866ms step_avg:58.95ms
step:2204/2330 train_time:129927ms step_avg:58.95ms
step:2205/2330 train_time:129985ms step_avg:58.95ms
step:2206/2330 train_time:130047ms step_avg:58.95ms
step:2207/2330 train_time:130104ms step_avg:58.95ms
step:2208/2330 train_time:130165ms step_avg:58.95ms
step:2209/2330 train_time:130222ms step_avg:58.95ms
step:2210/2330 train_time:130284ms step_avg:58.95ms
step:2211/2330 train_time:130341ms step_avg:58.95ms
step:2212/2330 train_time:130402ms step_avg:58.95ms
step:2213/2330 train_time:130459ms step_avg:58.95ms
step:2214/2330 train_time:130521ms step_avg:58.95ms
step:2215/2330 train_time:130577ms step_avg:58.95ms
step:2216/2330 train_time:130641ms step_avg:58.95ms
step:2217/2330 train_time:130697ms step_avg:58.95ms
step:2218/2330 train_time:130761ms step_avg:58.95ms
step:2219/2330 train_time:130817ms step_avg:58.95ms
step:2220/2330 train_time:130880ms step_avg:58.95ms
step:2221/2330 train_time:130936ms step_avg:58.95ms
step:2222/2330 train_time:130999ms step_avg:58.96ms
step:2223/2330 train_time:131056ms step_avg:58.95ms
step:2224/2330 train_time:131120ms step_avg:58.96ms
step:2225/2330 train_time:131177ms step_avg:58.96ms
step:2226/2330 train_time:131240ms step_avg:58.96ms
step:2227/2330 train_time:131297ms step_avg:58.96ms
step:2228/2330 train_time:131360ms step_avg:58.96ms
step:2229/2330 train_time:131416ms step_avg:58.96ms
step:2230/2330 train_time:131478ms step_avg:58.96ms
step:2231/2330 train_time:131534ms step_avg:58.96ms
step:2232/2330 train_time:131597ms step_avg:58.96ms
step:2233/2330 train_time:131654ms step_avg:58.96ms
step:2234/2330 train_time:131717ms step_avg:58.96ms
step:2235/2330 train_time:131774ms step_avg:58.96ms
step:2236/2330 train_time:131837ms step_avg:58.96ms
step:2237/2330 train_time:131893ms step_avg:58.96ms
step:2238/2330 train_time:131956ms step_avg:58.96ms
step:2239/2330 train_time:132013ms step_avg:58.96ms
step:2240/2330 train_time:132075ms step_avg:58.96ms
step:2241/2330 train_time:132132ms step_avg:58.96ms
step:2242/2330 train_time:132195ms step_avg:58.96ms
step:2243/2330 train_time:132252ms step_avg:58.96ms
step:2244/2330 train_time:132315ms step_avg:58.96ms
step:2245/2330 train_time:132372ms step_avg:58.96ms
step:2246/2330 train_time:132434ms step_avg:58.96ms
step:2247/2330 train_time:132492ms step_avg:58.96ms
step:2248/2330 train_time:132554ms step_avg:58.97ms
step:2249/2330 train_time:132611ms step_avg:58.96ms
step:2250/2330 train_time:132672ms step_avg:58.97ms
step:2250/2330 val_loss:4.1146 train_time:132751ms step_avg:59.00ms
step:2251/2330 train_time:132772ms step_avg:58.98ms
step:2252/2330 train_time:132794ms step_avg:58.97ms
step:2253/2330 train_time:132851ms step_avg:58.97ms
step:2254/2330 train_time:132922ms step_avg:58.97ms
step:2255/2330 train_time:132980ms step_avg:58.97ms
step:2256/2330 train_time:133041ms step_avg:58.97ms
step:2257/2330 train_time:133099ms step_avg:58.97ms
step:2258/2330 train_time:133160ms step_avg:58.97ms
step:2259/2330 train_time:133217ms step_avg:58.97ms
step:2260/2330 train_time:133278ms step_avg:58.97ms
step:2261/2330 train_time:133335ms step_avg:58.97ms
step:2262/2330 train_time:133395ms step_avg:58.97ms
step:2263/2330 train_time:133452ms step_avg:58.97ms
step:2264/2330 train_time:133513ms step_avg:58.97ms
step:2265/2330 train_time:133570ms step_avg:58.97ms
step:2266/2330 train_time:133631ms step_avg:58.97ms
step:2267/2330 train_time:133688ms step_avg:58.97ms
step:2268/2330 train_time:133750ms step_avg:58.97ms
step:2269/2330 train_time:133807ms step_avg:58.97ms
step:2270/2330 train_time:133871ms step_avg:58.97ms
step:2271/2330 train_time:133929ms step_avg:58.97ms
step:2272/2330 train_time:133992ms step_avg:58.98ms
step:2273/2330 train_time:134048ms step_avg:58.97ms
step:2274/2330 train_time:134111ms step_avg:58.98ms
step:2275/2330 train_time:134167ms step_avg:58.97ms
step:2276/2330 train_time:134229ms step_avg:58.98ms
step:2277/2330 train_time:134285ms step_avg:58.97ms
step:2278/2330 train_time:134348ms step_avg:58.98ms
step:2279/2330 train_time:134404ms step_avg:58.98ms
step:2280/2330 train_time:134467ms step_avg:58.98ms
step:2281/2330 train_time:134524ms step_avg:58.98ms
step:2282/2330 train_time:134586ms step_avg:58.98ms
step:2283/2330 train_time:134642ms step_avg:58.98ms
step:2284/2330 train_time:134704ms step_avg:58.98ms
step:2285/2330 train_time:134761ms step_avg:58.98ms
step:2286/2330 train_time:134823ms step_avg:58.98ms
step:2287/2330 train_time:134880ms step_avg:58.98ms
step:2288/2330 train_time:134944ms step_avg:58.98ms
step:2289/2330 train_time:135001ms step_avg:58.98ms
step:2290/2330 train_time:135064ms step_avg:58.98ms
step:2291/2330 train_time:135121ms step_avg:58.98ms
step:2292/2330 train_time:135183ms step_avg:58.98ms
step:2293/2330 train_time:135240ms step_avg:58.98ms
step:2294/2330 train_time:135303ms step_avg:58.98ms
step:2295/2330 train_time:135361ms step_avg:58.98ms
step:2296/2330 train_time:135424ms step_avg:58.98ms
step:2297/2330 train_time:135481ms step_avg:58.98ms
step:2298/2330 train_time:135543ms step_avg:58.98ms
step:2299/2330 train_time:135600ms step_avg:58.98ms
step:2300/2330 train_time:135661ms step_avg:58.98ms
step:2301/2330 train_time:135719ms step_avg:58.98ms
step:2302/2330 train_time:135781ms step_avg:58.98ms
step:2303/2330 train_time:135838ms step_avg:58.98ms
step:2304/2330 train_time:135900ms step_avg:58.98ms
step:2305/2330 train_time:135958ms step_avg:58.98ms
step:2306/2330 train_time:136020ms step_avg:58.99ms
step:2307/2330 train_time:136078ms step_avg:58.98ms
step:2308/2330 train_time:136140ms step_avg:58.99ms
step:2309/2330 train_time:136198ms step_avg:58.99ms
step:2310/2330 train_time:136259ms step_avg:58.99ms
step:2311/2330 train_time:136317ms step_avg:58.99ms
step:2312/2330 train_time:136378ms step_avg:58.99ms
step:2313/2330 train_time:136436ms step_avg:58.99ms
step:2314/2330 train_time:136496ms step_avg:58.99ms
step:2315/2330 train_time:136553ms step_avg:58.99ms
step:2316/2330 train_time:136615ms step_avg:58.99ms
step:2317/2330 train_time:136673ms step_avg:58.99ms
step:2318/2330 train_time:136734ms step_avg:58.99ms
step:2319/2330 train_time:136791ms step_avg:58.99ms
step:2320/2330 train_time:136852ms step_avg:58.99ms
step:2321/2330 train_time:136909ms step_avg:58.99ms
step:2322/2330 train_time:136972ms step_avg:58.99ms
step:2323/2330 train_time:137028ms step_avg:58.99ms
step:2324/2330 train_time:137091ms step_avg:58.99ms
step:2325/2330 train_time:137147ms step_avg:58.99ms
step:2326/2330 train_time:137210ms step_avg:58.99ms
step:2327/2330 train_time:137267ms step_avg:58.99ms
step:2328/2330 train_time:137330ms step_avg:58.99ms
step:2329/2330 train_time:137386ms step_avg:58.99ms
step:2330/2330 train_time:137449ms step_avg:58.99ms
step:2330/2330 val_loss:4.0993 train_time:137529ms step_avg:59.03ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
