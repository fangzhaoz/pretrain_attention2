import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = divide_by_norm(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = divide_by_norm(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_gd_lr1e-4"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-4, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 05:01:20 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   29C    P0             110W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   27C    P0             108W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:76ms step_avg:76.18ms
step:2/2330 train_time:159ms step_avg:79.72ms
step:3/2330 train_time:172ms step_avg:57.37ms
step:4/2330 train_time:185ms step_avg:46.32ms
step:5/2330 train_time:196ms step_avg:39.10ms
step:6/2330 train_time:224ms step_avg:37.37ms
step:7/2330 train_time:245ms step_avg:35.05ms
step:8/2330 train_time:300ms step_avg:37.51ms
step:9/2330 train_time:323ms step_avg:35.84ms
step:10/2330 train_time:378ms step_avg:37.77ms
step:11/2330 train_time:400ms step_avg:36.35ms
step:12/2330 train_time:456ms step_avg:37.98ms
step:13/2330 train_time:478ms step_avg:36.73ms
step:14/2330 train_time:533ms step_avg:38.05ms
step:15/2330 train_time:555ms step_avg:36.97ms
step:16/2330 train_time:610ms step_avg:38.10ms
step:17/2330 train_time:632ms step_avg:37.16ms
step:18/2330 train_time:687ms step_avg:38.16ms
step:19/2330 train_time:709ms step_avg:37.33ms
step:20/2330 train_time:765ms step_avg:38.23ms
step:21/2330 train_time:787ms step_avg:37.47ms
step:22/2330 train_time:842ms step_avg:38.28ms
step:23/2330 train_time:865ms step_avg:37.60ms
step:24/2330 train_time:921ms step_avg:38.36ms
step:25/2330 train_time:943ms step_avg:37.72ms
step:26/2330 train_time:1002ms step_avg:38.53ms
step:27/2330 train_time:1028ms step_avg:38.08ms
step:28/2330 train_time:1090ms step_avg:38.93ms
step:29/2330 train_time:1115ms step_avg:38.45ms
step:30/2330 train_time:1174ms step_avg:39.12ms
step:31/2330 train_time:1196ms step_avg:38.58ms
step:32/2330 train_time:1253ms step_avg:39.17ms
step:33/2330 train_time:1276ms step_avg:38.66ms
step:34/2330 train_time:1333ms step_avg:39.19ms
step:35/2330 train_time:1354ms step_avg:38.70ms
step:36/2330 train_time:1411ms step_avg:39.18ms
step:37/2330 train_time:1432ms step_avg:38.71ms
step:38/2330 train_time:1487ms step_avg:39.14ms
step:39/2330 train_time:1510ms step_avg:38.71ms
step:40/2330 train_time:1565ms step_avg:39.13ms
step:41/2330 train_time:1588ms step_avg:38.73ms
step:42/2330 train_time:1643ms step_avg:39.13ms
step:43/2330 train_time:1665ms step_avg:38.73ms
step:44/2330 train_time:1721ms step_avg:39.12ms
step:45/2330 train_time:1743ms step_avg:38.74ms
step:46/2330 train_time:1799ms step_avg:39.12ms
step:47/2330 train_time:1821ms step_avg:38.75ms
step:48/2330 train_time:1877ms step_avg:39.11ms
step:49/2330 train_time:1900ms step_avg:38.78ms
step:50/2330 train_time:1959ms step_avg:39.17ms
step:51/2330 train_time:1982ms step_avg:38.87ms
step:52/2330 train_time:2042ms step_avg:39.27ms
step:53/2330 train_time:2067ms step_avg:39.00ms
step:54/2330 train_time:2126ms step_avg:39.37ms
step:55/2330 train_time:2150ms step_avg:39.10ms
step:56/2330 train_time:2208ms step_avg:39.42ms
step:57/2330 train_time:2231ms step_avg:39.14ms
step:58/2330 train_time:2287ms step_avg:39.43ms
step:59/2330 train_time:2311ms step_avg:39.16ms
step:60/2330 train_time:2367ms step_avg:39.45ms
step:61/2330 train_time:2390ms step_avg:39.19ms
step:62/2330 train_time:2446ms step_avg:39.46ms
step:63/2330 train_time:2469ms step_avg:39.20ms
step:64/2330 train_time:2526ms step_avg:39.46ms
step:65/2330 train_time:2548ms step_avg:39.21ms
step:66/2330 train_time:2604ms step_avg:39.45ms
step:67/2330 train_time:2626ms step_avg:39.19ms
step:68/2330 train_time:2681ms step_avg:39.43ms
step:69/2330 train_time:2704ms step_avg:39.19ms
step:70/2330 train_time:2761ms step_avg:39.44ms
step:71/2330 train_time:2783ms step_avg:39.19ms
step:72/2330 train_time:2839ms step_avg:39.43ms
step:73/2330 train_time:2862ms step_avg:39.20ms
step:74/2330 train_time:2919ms step_avg:39.45ms
step:75/2330 train_time:2943ms step_avg:39.24ms
step:76/2330 train_time:3001ms step_avg:39.49ms
step:77/2330 train_time:3025ms step_avg:39.28ms
step:78/2330 train_time:3083ms step_avg:39.52ms
step:79/2330 train_time:3108ms step_avg:39.34ms
step:80/2330 train_time:3166ms step_avg:39.57ms
step:81/2330 train_time:3190ms step_avg:39.38ms
step:82/2330 train_time:3247ms step_avg:39.60ms
step:83/2330 train_time:3270ms step_avg:39.40ms
step:84/2330 train_time:3327ms step_avg:39.61ms
step:85/2330 train_time:3350ms step_avg:39.41ms
step:86/2330 train_time:3406ms step_avg:39.60ms
step:87/2330 train_time:3429ms step_avg:39.41ms
step:88/2330 train_time:3486ms step_avg:39.61ms
step:89/2330 train_time:3509ms step_avg:39.42ms
step:90/2330 train_time:3565ms step_avg:39.61ms
step:91/2330 train_time:3587ms step_avg:39.42ms
step:92/2330 train_time:3643ms step_avg:39.60ms
step:93/2330 train_time:3666ms step_avg:39.42ms
step:94/2330 train_time:3722ms step_avg:39.60ms
step:95/2330 train_time:3745ms step_avg:39.42ms
step:96/2330 train_time:3802ms step_avg:39.60ms
step:97/2330 train_time:3825ms step_avg:39.43ms
step:98/2330 train_time:3882ms step_avg:39.61ms
step:99/2330 train_time:3905ms step_avg:39.45ms
step:100/2330 train_time:3962ms step_avg:39.62ms
step:101/2330 train_time:3986ms step_avg:39.46ms
step:102/2330 train_time:4044ms step_avg:39.65ms
step:103/2330 train_time:4068ms step_avg:39.49ms
step:104/2330 train_time:4125ms step_avg:39.66ms
step:105/2330 train_time:4149ms step_avg:39.52ms
step:106/2330 train_time:4206ms step_avg:39.68ms
step:107/2330 train_time:4230ms step_avg:39.53ms
step:108/2330 train_time:4287ms step_avg:39.69ms
step:109/2330 train_time:4310ms step_avg:39.54ms
step:110/2330 train_time:4367ms step_avg:39.70ms
step:111/2330 train_time:4390ms step_avg:39.55ms
step:112/2330 train_time:4447ms step_avg:39.70ms
step:113/2330 train_time:4470ms step_avg:39.56ms
step:114/2330 train_time:4527ms step_avg:39.71ms
step:115/2330 train_time:4550ms step_avg:39.56ms
step:116/2330 train_time:4605ms step_avg:39.70ms
step:117/2330 train_time:4629ms step_avg:39.56ms
step:118/2330 train_time:4685ms step_avg:39.70ms
step:119/2330 train_time:4708ms step_avg:39.57ms
step:120/2330 train_time:4765ms step_avg:39.71ms
step:121/2330 train_time:4789ms step_avg:39.58ms
step:122/2330 train_time:4845ms step_avg:39.72ms
step:123/2330 train_time:4869ms step_avg:39.58ms
step:124/2330 train_time:4926ms step_avg:39.73ms
step:125/2330 train_time:4950ms step_avg:39.60ms
step:126/2330 train_time:5006ms step_avg:39.73ms
step:127/2330 train_time:5029ms step_avg:39.60ms
step:128/2330 train_time:5087ms step_avg:39.74ms
step:129/2330 train_time:5111ms step_avg:39.62ms
step:130/2330 train_time:5167ms step_avg:39.75ms
step:131/2330 train_time:5191ms step_avg:39.62ms
step:132/2330 train_time:5247ms step_avg:39.75ms
step:133/2330 train_time:5272ms step_avg:39.64ms
step:134/2330 train_time:5329ms step_avg:39.77ms
step:135/2330 train_time:5352ms step_avg:39.64ms
step:136/2330 train_time:5408ms step_avg:39.77ms
step:137/2330 train_time:5431ms step_avg:39.64ms
step:138/2330 train_time:5487ms step_avg:39.76ms
step:139/2330 train_time:5510ms step_avg:39.64ms
step:140/2330 train_time:5566ms step_avg:39.76ms
step:141/2330 train_time:5589ms step_avg:39.64ms
step:142/2330 train_time:5645ms step_avg:39.75ms
step:143/2330 train_time:5668ms step_avg:39.64ms
step:144/2330 train_time:5725ms step_avg:39.76ms
step:145/2330 train_time:5748ms step_avg:39.64ms
step:146/2330 train_time:5804ms step_avg:39.75ms
step:147/2330 train_time:5828ms step_avg:39.65ms
step:148/2330 train_time:5885ms step_avg:39.76ms
step:149/2330 train_time:5909ms step_avg:39.66ms
step:150/2330 train_time:5965ms step_avg:39.77ms
step:151/2330 train_time:5989ms step_avg:39.66ms
step:152/2330 train_time:6046ms step_avg:39.78ms
step:153/2330 train_time:6070ms step_avg:39.67ms
step:154/2330 train_time:6128ms step_avg:39.79ms
step:155/2330 train_time:6151ms step_avg:39.68ms
step:156/2330 train_time:6207ms step_avg:39.79ms
step:157/2330 train_time:6231ms step_avg:39.69ms
step:158/2330 train_time:6287ms step_avg:39.79ms
step:159/2330 train_time:6311ms step_avg:39.69ms
step:160/2330 train_time:6368ms step_avg:39.80ms
step:161/2330 train_time:6391ms step_avg:39.70ms
step:162/2330 train_time:6447ms step_avg:39.80ms
step:163/2330 train_time:6470ms step_avg:39.69ms
step:164/2330 train_time:6526ms step_avg:39.79ms
step:165/2330 train_time:6549ms step_avg:39.69ms
step:166/2330 train_time:6605ms step_avg:39.79ms
step:167/2330 train_time:6628ms step_avg:39.69ms
step:168/2330 train_time:6685ms step_avg:39.79ms
step:169/2330 train_time:6708ms step_avg:39.69ms
step:170/2330 train_time:6764ms step_avg:39.79ms
step:171/2330 train_time:6788ms step_avg:39.70ms
step:172/2330 train_time:6845ms step_avg:39.80ms
step:173/2330 train_time:6868ms step_avg:39.70ms
step:174/2330 train_time:6925ms step_avg:39.80ms
step:175/2330 train_time:6948ms step_avg:39.70ms
step:176/2330 train_time:7005ms step_avg:39.80ms
step:177/2330 train_time:7029ms step_avg:39.71ms
step:178/2330 train_time:7085ms step_avg:39.81ms
step:179/2330 train_time:7109ms step_avg:39.71ms
step:180/2330 train_time:7165ms step_avg:39.80ms
step:181/2330 train_time:7188ms step_avg:39.71ms
step:182/2330 train_time:7245ms step_avg:39.81ms
step:183/2330 train_time:7269ms step_avg:39.72ms
step:184/2330 train_time:7325ms step_avg:39.81ms
step:185/2330 train_time:7348ms step_avg:39.72ms
step:186/2330 train_time:7405ms step_avg:39.81ms
step:187/2330 train_time:7428ms step_avg:39.72ms
step:188/2330 train_time:7484ms step_avg:39.81ms
step:189/2330 train_time:7507ms step_avg:39.72ms
step:190/2330 train_time:7564ms step_avg:39.81ms
step:191/2330 train_time:7586ms step_avg:39.72ms
step:192/2330 train_time:7643ms step_avg:39.81ms
step:193/2330 train_time:7666ms step_avg:39.72ms
step:194/2330 train_time:7723ms step_avg:39.81ms
step:195/2330 train_time:7745ms step_avg:39.72ms
step:196/2330 train_time:7803ms step_avg:39.81ms
step:197/2330 train_time:7825ms step_avg:39.72ms
step:198/2330 train_time:7883ms step_avg:39.81ms
step:199/2330 train_time:7906ms step_avg:39.73ms
step:200/2330 train_time:7963ms step_avg:39.82ms
step:201/2330 train_time:7986ms step_avg:39.73ms
step:202/2330 train_time:8044ms step_avg:39.82ms
step:203/2330 train_time:8068ms step_avg:39.74ms
step:204/2330 train_time:8125ms step_avg:39.83ms
step:205/2330 train_time:8148ms step_avg:39.75ms
step:206/2330 train_time:8205ms step_avg:39.83ms
step:207/2330 train_time:8229ms step_avg:39.75ms
step:208/2330 train_time:8286ms step_avg:39.84ms
step:209/2330 train_time:8309ms step_avg:39.75ms
step:210/2330 train_time:8365ms step_avg:39.83ms
step:211/2330 train_time:8388ms step_avg:39.75ms
step:212/2330 train_time:8445ms step_avg:39.83ms
step:213/2330 train_time:8467ms step_avg:39.75ms
step:214/2330 train_time:8524ms step_avg:39.83ms
step:215/2330 train_time:8547ms step_avg:39.75ms
step:216/2330 train_time:8604ms step_avg:39.83ms
step:217/2330 train_time:8626ms step_avg:39.75ms
step:218/2330 train_time:8682ms step_avg:39.83ms
step:219/2330 train_time:8706ms step_avg:39.75ms
step:220/2330 train_time:8763ms step_avg:39.83ms
step:221/2330 train_time:8786ms step_avg:39.76ms
step:222/2330 train_time:8843ms step_avg:39.83ms
step:223/2330 train_time:8866ms step_avg:39.76ms
step:224/2330 train_time:8924ms step_avg:39.84ms
step:225/2330 train_time:8946ms step_avg:39.76ms
step:226/2330 train_time:9004ms step_avg:39.84ms
step:227/2330 train_time:9026ms step_avg:39.76ms
step:228/2330 train_time:9084ms step_avg:39.84ms
step:229/2330 train_time:9108ms step_avg:39.77ms
step:230/2330 train_time:9165ms step_avg:39.85ms
step:231/2330 train_time:9189ms step_avg:39.78ms
step:232/2330 train_time:9246ms step_avg:39.85ms
step:233/2330 train_time:9269ms step_avg:39.78ms
step:234/2330 train_time:9325ms step_avg:39.85ms
step:235/2330 train_time:9348ms step_avg:39.78ms
step:236/2330 train_time:9405ms step_avg:39.85ms
step:237/2330 train_time:9427ms step_avg:39.78ms
step:238/2330 train_time:9484ms step_avg:39.85ms
step:239/2330 train_time:9507ms step_avg:39.78ms
step:240/2330 train_time:9563ms step_avg:39.85ms
step:241/2330 train_time:9587ms step_avg:39.78ms
step:242/2330 train_time:9643ms step_avg:39.85ms
step:243/2330 train_time:9666ms step_avg:39.78ms
step:244/2330 train_time:9723ms step_avg:39.85ms
step:245/2330 train_time:9746ms step_avg:39.78ms
step:246/2330 train_time:9803ms step_avg:39.85ms
step:247/2330 train_time:9826ms step_avg:39.78ms
step:248/2330 train_time:9884ms step_avg:39.85ms
step:249/2330 train_time:9907ms step_avg:39.79ms
step:250/2330 train_time:9964ms step_avg:39.86ms
step:250/2330 val_loss:5.7443 train_time:10061ms step_avg:40.24ms
step:251/2330 train_time:10073ms step_avg:40.13ms
step:252/2330 train_time:10085ms step_avg:40.02ms
step:253/2330 train_time:10095ms step_avg:39.90ms
step:254/2330 train_time:10124ms step_avg:39.86ms
step:255/2330 train_time:10146ms step_avg:39.79ms
step:256/2330 train_time:10202ms step_avg:39.85ms
step:257/2330 train_time:10224ms step_avg:39.78ms
step:258/2330 train_time:10280ms step_avg:39.85ms
step:259/2330 train_time:10302ms step_avg:39.78ms
step:260/2330 train_time:10360ms step_avg:39.85ms
step:261/2330 train_time:10383ms step_avg:39.78ms
step:262/2330 train_time:10444ms step_avg:39.86ms
step:263/2330 train_time:10471ms step_avg:39.81ms
step:264/2330 train_time:10529ms step_avg:39.88ms
step:265/2330 train_time:10553ms step_avg:39.82ms
step:266/2330 train_time:10609ms step_avg:39.88ms
step:267/2330 train_time:10633ms step_avg:39.82ms
step:268/2330 train_time:10689ms step_avg:39.89ms
step:269/2330 train_time:10712ms step_avg:39.82ms
step:270/2330 train_time:10768ms step_avg:39.88ms
step:271/2330 train_time:10791ms step_avg:39.82ms
step:272/2330 train_time:10847ms step_avg:39.88ms
step:273/2330 train_time:10870ms step_avg:39.82ms
step:274/2330 train_time:10926ms step_avg:39.88ms
step:275/2330 train_time:10949ms step_avg:39.81ms
step:276/2330 train_time:11007ms step_avg:39.88ms
step:277/2330 train_time:11029ms step_avg:39.82ms
step:278/2330 train_time:11086ms step_avg:39.88ms
step:279/2330 train_time:11108ms step_avg:39.81ms
step:280/2330 train_time:11164ms step_avg:39.87ms
step:281/2330 train_time:11187ms step_avg:39.81ms
step:282/2330 train_time:11243ms step_avg:39.87ms
step:283/2330 train_time:11266ms step_avg:39.81ms
step:284/2330 train_time:11324ms step_avg:39.87ms
step:285/2330 train_time:11347ms step_avg:39.81ms
step:286/2330 train_time:11406ms step_avg:39.88ms
step:287/2330 train_time:11430ms step_avg:39.83ms
step:288/2330 train_time:11488ms step_avg:39.89ms
step:289/2330 train_time:11511ms step_avg:39.83ms
step:290/2330 train_time:11569ms step_avg:39.89ms
step:291/2330 train_time:11592ms step_avg:39.83ms
step:292/2330 train_time:11649ms step_avg:39.89ms
step:293/2330 train_time:11671ms step_avg:39.83ms
step:294/2330 train_time:11729ms step_avg:39.89ms
step:295/2330 train_time:11751ms step_avg:39.83ms
step:296/2330 train_time:11807ms step_avg:39.89ms
step:297/2330 train_time:11830ms step_avg:39.83ms
step:298/2330 train_time:11887ms step_avg:39.89ms
step:299/2330 train_time:11910ms step_avg:39.83ms
step:300/2330 train_time:11966ms step_avg:39.89ms
step:301/2330 train_time:11989ms step_avg:39.83ms
step:302/2330 train_time:12046ms step_avg:39.89ms
step:303/2330 train_time:12069ms step_avg:39.83ms
step:304/2330 train_time:12126ms step_avg:39.89ms
step:305/2330 train_time:12149ms step_avg:39.83ms
step:306/2330 train_time:12206ms step_avg:39.89ms
step:307/2330 train_time:12229ms step_avg:39.83ms
step:308/2330 train_time:12286ms step_avg:39.89ms
step:309/2330 train_time:12310ms step_avg:39.84ms
step:310/2330 train_time:12367ms step_avg:39.90ms
step:311/2330 train_time:12391ms step_avg:39.84ms
step:312/2330 train_time:12448ms step_avg:39.90ms
step:313/2330 train_time:12472ms step_avg:39.85ms
step:314/2330 train_time:12529ms step_avg:39.90ms
step:315/2330 train_time:12552ms step_avg:39.85ms
step:316/2330 train_time:12608ms step_avg:39.90ms
step:317/2330 train_time:12632ms step_avg:39.85ms
step:318/2330 train_time:12689ms step_avg:39.90ms
step:319/2330 train_time:12711ms step_avg:39.85ms
step:320/2330 train_time:12768ms step_avg:39.90ms
step:321/2330 train_time:12790ms step_avg:39.85ms
step:322/2330 train_time:12847ms step_avg:39.90ms
step:323/2330 train_time:12870ms step_avg:39.85ms
step:324/2330 train_time:12927ms step_avg:39.90ms
step:325/2330 train_time:12950ms step_avg:39.85ms
step:326/2330 train_time:13006ms step_avg:39.90ms
step:327/2330 train_time:13029ms step_avg:39.84ms
step:328/2330 train_time:13086ms step_avg:39.90ms
step:329/2330 train_time:13109ms step_avg:39.85ms
step:330/2330 train_time:13166ms step_avg:39.90ms
step:331/2330 train_time:13189ms step_avg:39.85ms
step:332/2330 train_time:13246ms step_avg:39.90ms
step:333/2330 train_time:13270ms step_avg:39.85ms
step:334/2330 train_time:13327ms step_avg:39.90ms
step:335/2330 train_time:13351ms step_avg:39.85ms
step:336/2330 train_time:13408ms step_avg:39.90ms
step:337/2330 train_time:13431ms step_avg:39.85ms
step:338/2330 train_time:13488ms step_avg:39.91ms
step:339/2330 train_time:13512ms step_avg:39.86ms
step:340/2330 train_time:13569ms step_avg:39.91ms
step:341/2330 train_time:13593ms step_avg:39.86ms
step:342/2330 train_time:13649ms step_avg:39.91ms
step:343/2330 train_time:13672ms step_avg:39.86ms
step:344/2330 train_time:13729ms step_avg:39.91ms
step:345/2330 train_time:13752ms step_avg:39.86ms
step:346/2330 train_time:13809ms step_avg:39.91ms
step:347/2330 train_time:13832ms step_avg:39.86ms
step:348/2330 train_time:13888ms step_avg:39.91ms
step:349/2330 train_time:13911ms step_avg:39.86ms
step:350/2330 train_time:13968ms step_avg:39.91ms
step:351/2330 train_time:13991ms step_avg:39.86ms
step:352/2330 train_time:14047ms step_avg:39.91ms
step:353/2330 train_time:14070ms step_avg:39.86ms
step:354/2330 train_time:14127ms step_avg:39.91ms
step:355/2330 train_time:14150ms step_avg:39.86ms
step:356/2330 train_time:14207ms step_avg:39.91ms
step:357/2330 train_time:14230ms step_avg:39.86ms
step:358/2330 train_time:14287ms step_avg:39.91ms
step:359/2330 train_time:14310ms step_avg:39.86ms
step:360/2330 train_time:14367ms step_avg:39.91ms
step:361/2330 train_time:14391ms step_avg:39.86ms
step:362/2330 train_time:14448ms step_avg:39.91ms
step:363/2330 train_time:14471ms step_avg:39.86ms
step:364/2330 train_time:14528ms step_avg:39.91ms
step:365/2330 train_time:14552ms step_avg:39.87ms
step:366/2330 train_time:14609ms step_avg:39.92ms
step:367/2330 train_time:14632ms step_avg:39.87ms
step:368/2330 train_time:14688ms step_avg:39.91ms
step:369/2330 train_time:14712ms step_avg:39.87ms
step:370/2330 train_time:14768ms step_avg:39.91ms
step:371/2330 train_time:14791ms step_avg:39.87ms
step:372/2330 train_time:14848ms step_avg:39.91ms
step:373/2330 train_time:14872ms step_avg:39.87ms
step:374/2330 train_time:14929ms step_avg:39.92ms
step:375/2330 train_time:14951ms step_avg:39.87ms
step:376/2330 train_time:15008ms step_avg:39.92ms
step:377/2330 train_time:15031ms step_avg:39.87ms
step:378/2330 train_time:15088ms step_avg:39.92ms
step:379/2330 train_time:15111ms step_avg:39.87ms
step:380/2330 train_time:15168ms step_avg:39.92ms
step:381/2330 train_time:15192ms step_avg:39.87ms
step:382/2330 train_time:15248ms step_avg:39.92ms
step:383/2330 train_time:15273ms step_avg:39.88ms
step:384/2330 train_time:15330ms step_avg:39.92ms
step:385/2330 train_time:15353ms step_avg:39.88ms
step:386/2330 train_time:15409ms step_avg:39.92ms
step:387/2330 train_time:15432ms step_avg:39.88ms
step:388/2330 train_time:15489ms step_avg:39.92ms
step:389/2330 train_time:15512ms step_avg:39.88ms
step:390/2330 train_time:15568ms step_avg:39.92ms
step:391/2330 train_time:15592ms step_avg:39.88ms
step:392/2330 train_time:15649ms step_avg:39.92ms
step:393/2330 train_time:15673ms step_avg:39.88ms
step:394/2330 train_time:15730ms step_avg:39.92ms
step:395/2330 train_time:15753ms step_avg:39.88ms
step:396/2330 train_time:15810ms step_avg:39.92ms
step:397/2330 train_time:15833ms step_avg:39.88ms
step:398/2330 train_time:15889ms step_avg:39.92ms
step:399/2330 train_time:15912ms step_avg:39.88ms
step:400/2330 train_time:15968ms step_avg:39.92ms
step:401/2330 train_time:15992ms step_avg:39.88ms
step:402/2330 train_time:16048ms step_avg:39.92ms
step:403/2330 train_time:16071ms step_avg:39.88ms
step:404/2330 train_time:16129ms step_avg:39.92ms
step:405/2330 train_time:16152ms step_avg:39.88ms
step:406/2330 train_time:16208ms step_avg:39.92ms
step:407/2330 train_time:16232ms step_avg:39.88ms
step:408/2330 train_time:16289ms step_avg:39.92ms
step:409/2330 train_time:16311ms step_avg:39.88ms
step:410/2330 train_time:16368ms step_avg:39.92ms
step:411/2330 train_time:16391ms step_avg:39.88ms
step:412/2330 train_time:16448ms step_avg:39.92ms
step:413/2330 train_time:16472ms step_avg:39.88ms
step:414/2330 train_time:16529ms step_avg:39.93ms
step:415/2330 train_time:16552ms step_avg:39.88ms
step:416/2330 train_time:16609ms step_avg:39.93ms
step:417/2330 train_time:16632ms step_avg:39.88ms
step:418/2330 train_time:16689ms step_avg:39.93ms
step:419/2330 train_time:16713ms step_avg:39.89ms
step:420/2330 train_time:16769ms step_avg:39.93ms
step:421/2330 train_time:16793ms step_avg:39.89ms
step:422/2330 train_time:16849ms step_avg:39.93ms
step:423/2330 train_time:16872ms step_avg:39.89ms
step:424/2330 train_time:16928ms step_avg:39.93ms
step:425/2330 train_time:16952ms step_avg:39.89ms
step:426/2330 train_time:17008ms step_avg:39.92ms
step:427/2330 train_time:17031ms step_avg:39.89ms
step:428/2330 train_time:17088ms step_avg:39.93ms
step:429/2330 train_time:17112ms step_avg:39.89ms
step:430/2330 train_time:17168ms step_avg:39.93ms
step:431/2330 train_time:17191ms step_avg:39.89ms
step:432/2330 train_time:17248ms step_avg:39.93ms
step:433/2330 train_time:17272ms step_avg:39.89ms
step:434/2330 train_time:17330ms step_avg:39.93ms
step:435/2330 train_time:17352ms step_avg:39.89ms
step:436/2330 train_time:17409ms step_avg:39.93ms
step:437/2330 train_time:17432ms step_avg:39.89ms
step:438/2330 train_time:17489ms step_avg:39.93ms
step:439/2330 train_time:17512ms step_avg:39.89ms
step:440/2330 train_time:17569ms step_avg:39.93ms
step:441/2330 train_time:17591ms step_avg:39.89ms
step:442/2330 train_time:17649ms step_avg:39.93ms
step:443/2330 train_time:17672ms step_avg:39.89ms
step:444/2330 train_time:17729ms step_avg:39.93ms
step:445/2330 train_time:17752ms step_avg:39.89ms
step:446/2330 train_time:17809ms step_avg:39.93ms
step:447/2330 train_time:17832ms step_avg:39.89ms
step:448/2330 train_time:17889ms step_avg:39.93ms
step:449/2330 train_time:17912ms step_avg:39.89ms
step:450/2330 train_time:17969ms step_avg:39.93ms
step:451/2330 train_time:17991ms step_avg:39.89ms
step:452/2330 train_time:18048ms step_avg:39.93ms
step:453/2330 train_time:18071ms step_avg:39.89ms
step:454/2330 train_time:18128ms step_avg:39.93ms
step:455/2330 train_time:18151ms step_avg:39.89ms
step:456/2330 train_time:18208ms step_avg:39.93ms
step:457/2330 train_time:18230ms step_avg:39.89ms
step:458/2330 train_time:18287ms step_avg:39.93ms
step:459/2330 train_time:18310ms step_avg:39.89ms
step:460/2330 train_time:18367ms step_avg:39.93ms
step:461/2330 train_time:18390ms step_avg:39.89ms
step:462/2330 train_time:18447ms step_avg:39.93ms
step:463/2330 train_time:18470ms step_avg:39.89ms
step:464/2330 train_time:18528ms step_avg:39.93ms
step:465/2330 train_time:18551ms step_avg:39.89ms
step:466/2330 train_time:18607ms step_avg:39.93ms
step:467/2330 train_time:18631ms step_avg:39.89ms
step:468/2330 train_time:18688ms step_avg:39.93ms
step:469/2330 train_time:18711ms step_avg:39.90ms
step:470/2330 train_time:18768ms step_avg:39.93ms
step:471/2330 train_time:18792ms step_avg:39.90ms
step:472/2330 train_time:18848ms step_avg:39.93ms
step:473/2330 train_time:18872ms step_avg:39.90ms
step:474/2330 train_time:18929ms step_avg:39.93ms
step:475/2330 train_time:18951ms step_avg:39.90ms
step:476/2330 train_time:19008ms step_avg:39.93ms
step:477/2330 train_time:19032ms step_avg:39.90ms
step:478/2330 train_time:19089ms step_avg:39.93ms
step:479/2330 train_time:19113ms step_avg:39.90ms
step:480/2330 train_time:19169ms step_avg:39.94ms
step:481/2330 train_time:19192ms step_avg:39.90ms
step:482/2330 train_time:19248ms step_avg:39.93ms
step:483/2330 train_time:19271ms step_avg:39.90ms
step:484/2330 train_time:19328ms step_avg:39.93ms
step:485/2330 train_time:19351ms step_avg:39.90ms
step:486/2330 train_time:19408ms step_avg:39.93ms
step:487/2330 train_time:19431ms step_avg:39.90ms
step:488/2330 train_time:19488ms step_avg:39.93ms
step:489/2330 train_time:19511ms step_avg:39.90ms
step:490/2330 train_time:19568ms step_avg:39.93ms
step:491/2330 train_time:19591ms step_avg:39.90ms
step:492/2330 train_time:19648ms step_avg:39.93ms
step:493/2330 train_time:19671ms step_avg:39.90ms
step:494/2330 train_time:19729ms step_avg:39.94ms
step:495/2330 train_time:19752ms step_avg:39.90ms
step:496/2330 train_time:19809ms step_avg:39.94ms
step:497/2330 train_time:19832ms step_avg:39.90ms
step:498/2330 train_time:19889ms step_avg:39.94ms
step:499/2330 train_time:19912ms step_avg:39.90ms
step:500/2330 train_time:19969ms step_avg:39.94ms
step:500/2330 val_loss:5.6875 train_time:20066ms step_avg:40.13ms
step:501/2330 train_time:20078ms step_avg:40.08ms
step:502/2330 train_time:20091ms step_avg:40.02ms
step:503/2330 train_time:20100ms step_avg:39.96ms
step:504/2330 train_time:20129ms step_avg:39.94ms
step:505/2330 train_time:20151ms step_avg:39.90ms
step:506/2330 train_time:20206ms step_avg:39.93ms
step:507/2330 train_time:20229ms step_avg:39.90ms
step:508/2330 train_time:20285ms step_avg:39.93ms
step:509/2330 train_time:20307ms step_avg:39.90ms
step:510/2330 train_time:20366ms step_avg:39.93ms
step:511/2330 train_time:20393ms step_avg:39.91ms
step:512/2330 train_time:20454ms step_avg:39.95ms
step:513/2330 train_time:20478ms step_avg:39.92ms
step:514/2330 train_time:20534ms step_avg:39.95ms
step:515/2330 train_time:20556ms step_avg:39.92ms
step:516/2330 train_time:20612ms step_avg:39.95ms
step:517/2330 train_time:20635ms step_avg:39.91ms
step:518/2330 train_time:20690ms step_avg:39.94ms
step:519/2330 train_time:20713ms step_avg:39.91ms
step:520/2330 train_time:20769ms step_avg:39.94ms
step:521/2330 train_time:20792ms step_avg:39.91ms
step:522/2330 train_time:20848ms step_avg:39.94ms
step:523/2330 train_time:20871ms step_avg:39.91ms
step:524/2330 train_time:20927ms step_avg:39.94ms
step:525/2330 train_time:20950ms step_avg:39.90ms
step:526/2330 train_time:21007ms step_avg:39.94ms
step:527/2330 train_time:21031ms step_avg:39.91ms
step:528/2330 train_time:21087ms step_avg:39.94ms
step:529/2330 train_time:21109ms step_avg:39.90ms
step:530/2330 train_time:21165ms step_avg:39.93ms
step:531/2330 train_time:21188ms step_avg:39.90ms
step:532/2330 train_time:21245ms step_avg:39.93ms
step:533/2330 train_time:21268ms step_avg:39.90ms
step:534/2330 train_time:21326ms step_avg:39.94ms
step:535/2330 train_time:21351ms step_avg:39.91ms
step:536/2330 train_time:21410ms step_avg:39.94ms
step:537/2330 train_time:21435ms step_avg:39.92ms
step:538/2330 train_time:21493ms step_avg:39.95ms
step:539/2330 train_time:21517ms step_avg:39.92ms
step:540/2330 train_time:21573ms step_avg:39.95ms
step:541/2330 train_time:21596ms step_avg:39.92ms
step:542/2330 train_time:21652ms step_avg:39.95ms
step:543/2330 train_time:21674ms step_avg:39.92ms
step:544/2330 train_time:21731ms step_avg:39.95ms
step:545/2330 train_time:21754ms step_avg:39.92ms
step:546/2330 train_time:21809ms step_avg:39.94ms
step:547/2330 train_time:21832ms step_avg:39.91ms
step:548/2330 train_time:21888ms step_avg:39.94ms
step:549/2330 train_time:21911ms step_avg:39.91ms
step:550/2330 train_time:21968ms step_avg:39.94ms
step:551/2330 train_time:21991ms step_avg:39.91ms
step:552/2330 train_time:22047ms step_avg:39.94ms
step:553/2330 train_time:22070ms step_avg:39.91ms
step:554/2330 train_time:22127ms step_avg:39.94ms
step:555/2330 train_time:22149ms step_avg:39.91ms
step:556/2330 train_time:22205ms step_avg:39.94ms
step:557/2330 train_time:22228ms step_avg:39.91ms
step:558/2330 train_time:22287ms step_avg:39.94ms
step:559/2330 train_time:22310ms step_avg:39.91ms
step:560/2330 train_time:22368ms step_avg:39.94ms
step:561/2330 train_time:22392ms step_avg:39.91ms
step:562/2330 train_time:22450ms step_avg:39.95ms
step:563/2330 train_time:22474ms step_avg:39.92ms
step:564/2330 train_time:22531ms step_avg:39.95ms
step:565/2330 train_time:22554ms step_avg:39.92ms
step:566/2330 train_time:22610ms step_avg:39.95ms
step:567/2330 train_time:22633ms step_avg:39.92ms
step:568/2330 train_time:22690ms step_avg:39.95ms
step:569/2330 train_time:22713ms step_avg:39.92ms
step:570/2330 train_time:22768ms step_avg:39.94ms
step:571/2330 train_time:22791ms step_avg:39.91ms
step:572/2330 train_time:22848ms step_avg:39.94ms
step:573/2330 train_time:22871ms step_avg:39.91ms
step:574/2330 train_time:22927ms step_avg:39.94ms
step:575/2330 train_time:22950ms step_avg:39.91ms
step:576/2330 train_time:23006ms step_avg:39.94ms
step:577/2330 train_time:23029ms step_avg:39.91ms
step:578/2330 train_time:23086ms step_avg:39.94ms
step:579/2330 train_time:23108ms step_avg:39.91ms
step:580/2330 train_time:23165ms step_avg:39.94ms
step:581/2330 train_time:23188ms step_avg:39.91ms
step:582/2330 train_time:23246ms step_avg:39.94ms
step:583/2330 train_time:23269ms step_avg:39.91ms
step:584/2330 train_time:23326ms step_avg:39.94ms
step:585/2330 train_time:23350ms step_avg:39.92ms
step:586/2330 train_time:23409ms step_avg:39.95ms
step:587/2330 train_time:23433ms step_avg:39.92ms
step:588/2330 train_time:23490ms step_avg:39.95ms
step:589/2330 train_time:23513ms step_avg:39.92ms
step:590/2330 train_time:23570ms step_avg:39.95ms
step:591/2330 train_time:23593ms step_avg:39.92ms
step:592/2330 train_time:23650ms step_avg:39.95ms
step:593/2330 train_time:23673ms step_avg:39.92ms
step:594/2330 train_time:23729ms step_avg:39.95ms
step:595/2330 train_time:23752ms step_avg:39.92ms
step:596/2330 train_time:23809ms step_avg:39.95ms
step:597/2330 train_time:23832ms step_avg:39.92ms
step:598/2330 train_time:23888ms step_avg:39.95ms
step:599/2330 train_time:23911ms step_avg:39.92ms
step:600/2330 train_time:23967ms step_avg:39.95ms
step:601/2330 train_time:23990ms step_avg:39.92ms
step:602/2330 train_time:24046ms step_avg:39.94ms
step:603/2330 train_time:24070ms step_avg:39.92ms
step:604/2330 train_time:24126ms step_avg:39.94ms
step:605/2330 train_time:24149ms step_avg:39.92ms
step:606/2330 train_time:24206ms step_avg:39.94ms
step:607/2330 train_time:24229ms step_avg:39.92ms
step:608/2330 train_time:24287ms step_avg:39.95ms
step:609/2330 train_time:24310ms step_avg:39.92ms
step:610/2330 train_time:24367ms step_avg:39.95ms
step:611/2330 train_time:24391ms step_avg:39.92ms
step:612/2330 train_time:24448ms step_avg:39.95ms
step:613/2330 train_time:24471ms step_avg:39.92ms
step:614/2330 train_time:24528ms step_avg:39.95ms
step:615/2330 train_time:24552ms step_avg:39.92ms
step:616/2330 train_time:24609ms step_avg:39.95ms
step:617/2330 train_time:24632ms step_avg:39.92ms
step:618/2330 train_time:24689ms step_avg:39.95ms
step:619/2330 train_time:24711ms step_avg:39.92ms
step:620/2330 train_time:24768ms step_avg:39.95ms
step:621/2330 train_time:24790ms step_avg:39.92ms
step:622/2330 train_time:24847ms step_avg:39.95ms
step:623/2330 train_time:24871ms step_avg:39.92ms
step:624/2330 train_time:24928ms step_avg:39.95ms
step:625/2330 train_time:24951ms step_avg:39.92ms
step:626/2330 train_time:25007ms step_avg:39.95ms
step:627/2330 train_time:25031ms step_avg:39.92ms
step:628/2330 train_time:25088ms step_avg:39.95ms
step:629/2330 train_time:25112ms step_avg:39.92ms
step:630/2330 train_time:25169ms step_avg:39.95ms
step:631/2330 train_time:25192ms step_avg:39.92ms
step:632/2330 train_time:25249ms step_avg:39.95ms
step:633/2330 train_time:25273ms step_avg:39.93ms
step:634/2330 train_time:25330ms step_avg:39.95ms
step:635/2330 train_time:25353ms step_avg:39.93ms
step:636/2330 train_time:25410ms step_avg:39.95ms
step:637/2330 train_time:25433ms step_avg:39.93ms
step:638/2330 train_time:25490ms step_avg:39.95ms
step:639/2330 train_time:25513ms step_avg:39.93ms
step:640/2330 train_time:25570ms step_avg:39.95ms
step:641/2330 train_time:25592ms step_avg:39.93ms
step:642/2330 train_time:25648ms step_avg:39.95ms
step:643/2330 train_time:25672ms step_avg:39.93ms
step:644/2330 train_time:25728ms step_avg:39.95ms
step:645/2330 train_time:25751ms step_avg:39.92ms
step:646/2330 train_time:25808ms step_avg:39.95ms
step:647/2330 train_time:25832ms step_avg:39.93ms
step:648/2330 train_time:25888ms step_avg:39.95ms
step:649/2330 train_time:25911ms step_avg:39.92ms
step:650/2330 train_time:25968ms step_avg:39.95ms
step:651/2330 train_time:25990ms step_avg:39.92ms
step:652/2330 train_time:26048ms step_avg:39.95ms
step:653/2330 train_time:26071ms step_avg:39.93ms
step:654/2330 train_time:26128ms step_avg:39.95ms
step:655/2330 train_time:26151ms step_avg:39.93ms
step:656/2330 train_time:26208ms step_avg:39.95ms
step:657/2330 train_time:26231ms step_avg:39.92ms
step:658/2330 train_time:26288ms step_avg:39.95ms
step:659/2330 train_time:26311ms step_avg:39.93ms
step:660/2330 train_time:26368ms step_avg:39.95ms
step:661/2330 train_time:26391ms step_avg:39.93ms
step:662/2330 train_time:26449ms step_avg:39.95ms
step:663/2330 train_time:26472ms step_avg:39.93ms
step:664/2330 train_time:26529ms step_avg:39.95ms
step:665/2330 train_time:26553ms step_avg:39.93ms
step:666/2330 train_time:26609ms step_avg:39.95ms
step:667/2330 train_time:26632ms step_avg:39.93ms
step:668/2330 train_time:26689ms step_avg:39.95ms
step:669/2330 train_time:26712ms step_avg:39.93ms
step:670/2330 train_time:26769ms step_avg:39.95ms
step:671/2330 train_time:26793ms step_avg:39.93ms
step:672/2330 train_time:26849ms step_avg:39.95ms
step:673/2330 train_time:26873ms step_avg:39.93ms
step:674/2330 train_time:26930ms step_avg:39.95ms
step:675/2330 train_time:26953ms step_avg:39.93ms
step:676/2330 train_time:27009ms step_avg:39.95ms
step:677/2330 train_time:27033ms step_avg:39.93ms
step:678/2330 train_time:27089ms step_avg:39.95ms
step:679/2330 train_time:27113ms step_avg:39.93ms
step:680/2330 train_time:27170ms step_avg:39.96ms
step:681/2330 train_time:27193ms step_avg:39.93ms
step:682/2330 train_time:27250ms step_avg:39.96ms
step:683/2330 train_time:27272ms step_avg:39.93ms
step:684/2330 train_time:27330ms step_avg:39.96ms
step:685/2330 train_time:27353ms step_avg:39.93ms
step:686/2330 train_time:27409ms step_avg:39.96ms
step:687/2330 train_time:27432ms step_avg:39.93ms
step:688/2330 train_time:27489ms step_avg:39.96ms
step:689/2330 train_time:27513ms step_avg:39.93ms
step:690/2330 train_time:27569ms step_avg:39.95ms
step:691/2330 train_time:27592ms step_avg:39.93ms
step:692/2330 train_time:27649ms step_avg:39.95ms
step:693/2330 train_time:27673ms step_avg:39.93ms
step:694/2330 train_time:27729ms step_avg:39.96ms
step:695/2330 train_time:27753ms step_avg:39.93ms
step:696/2330 train_time:27809ms step_avg:39.95ms
step:697/2330 train_time:27833ms step_avg:39.93ms
step:698/2330 train_time:27889ms step_avg:39.96ms
step:699/2330 train_time:27913ms step_avg:39.93ms
step:700/2330 train_time:27969ms step_avg:39.96ms
step:701/2330 train_time:27993ms step_avg:39.93ms
step:702/2330 train_time:28049ms step_avg:39.96ms
step:703/2330 train_time:28073ms step_avg:39.93ms
step:704/2330 train_time:28130ms step_avg:39.96ms
step:705/2330 train_time:28153ms step_avg:39.93ms
step:706/2330 train_time:28209ms step_avg:39.96ms
step:707/2330 train_time:28234ms step_avg:39.93ms
step:708/2330 train_time:28290ms step_avg:39.96ms
step:709/2330 train_time:28313ms step_avg:39.93ms
step:710/2330 train_time:28370ms step_avg:39.96ms
step:711/2330 train_time:28393ms step_avg:39.93ms
step:712/2330 train_time:28450ms step_avg:39.96ms
step:713/2330 train_time:28473ms step_avg:39.93ms
step:714/2330 train_time:28529ms step_avg:39.96ms
step:715/2330 train_time:28552ms step_avg:39.93ms
step:716/2330 train_time:28609ms step_avg:39.96ms
step:717/2330 train_time:28632ms step_avg:39.93ms
step:718/2330 train_time:28688ms step_avg:39.96ms
step:719/2330 train_time:28711ms step_avg:39.93ms
step:720/2330 train_time:28768ms step_avg:39.96ms
step:721/2330 train_time:28791ms step_avg:39.93ms
step:722/2330 train_time:28848ms step_avg:39.96ms
step:723/2330 train_time:28872ms step_avg:39.93ms
step:724/2330 train_time:28928ms step_avg:39.96ms
step:725/2330 train_time:28952ms step_avg:39.93ms
step:726/2330 train_time:29008ms step_avg:39.96ms
step:727/2330 train_time:29032ms step_avg:39.93ms
step:728/2330 train_time:29089ms step_avg:39.96ms
step:729/2330 train_time:29111ms step_avg:39.93ms
step:730/2330 train_time:29168ms step_avg:39.96ms
step:731/2330 train_time:29191ms step_avg:39.93ms
step:732/2330 train_time:29248ms step_avg:39.96ms
step:733/2330 train_time:29271ms step_avg:39.93ms
step:734/2330 train_time:29328ms step_avg:39.96ms
step:735/2330 train_time:29352ms step_avg:39.93ms
step:736/2330 train_time:29409ms step_avg:39.96ms
step:737/2330 train_time:29432ms step_avg:39.94ms
step:738/2330 train_time:29489ms step_avg:39.96ms
step:739/2330 train_time:29513ms step_avg:39.94ms
step:740/2330 train_time:29569ms step_avg:39.96ms
step:741/2330 train_time:29593ms step_avg:39.94ms
step:742/2330 train_time:29649ms step_avg:39.96ms
step:743/2330 train_time:29672ms step_avg:39.94ms
step:744/2330 train_time:29729ms step_avg:39.96ms
step:745/2330 train_time:29753ms step_avg:39.94ms
step:746/2330 train_time:29809ms step_avg:39.96ms
step:747/2330 train_time:29833ms step_avg:39.94ms
step:748/2330 train_time:29890ms step_avg:39.96ms
step:749/2330 train_time:29913ms step_avg:39.94ms
step:750/2330 train_time:29970ms step_avg:39.96ms
step:750/2330 val_loss:5.6670 train_time:30067ms step_avg:40.09ms
step:751/2330 train_time:30079ms step_avg:40.05ms
step:752/2330 train_time:30090ms step_avg:40.01ms
step:753/2330 train_time:30100ms step_avg:39.97ms
step:754/2330 train_time:30131ms step_avg:39.96ms
step:755/2330 train_time:30153ms step_avg:39.94ms
step:756/2330 train_time:30210ms step_avg:39.96ms
step:757/2330 train_time:30232ms step_avg:39.94ms
step:758/2330 train_time:30288ms step_avg:39.96ms
step:759/2330 train_time:30310ms step_avg:39.93ms
step:760/2330 train_time:30367ms step_avg:39.96ms
step:761/2330 train_time:30396ms step_avg:39.94ms
step:762/2330 train_time:30458ms step_avg:39.97ms
step:763/2330 train_time:30481ms step_avg:39.95ms
step:764/2330 train_time:30541ms step_avg:39.97ms
step:765/2330 train_time:30565ms step_avg:39.95ms
step:766/2330 train_time:30622ms step_avg:39.98ms
step:767/2330 train_time:30645ms step_avg:39.95ms
step:768/2330 train_time:30701ms step_avg:39.98ms
step:769/2330 train_time:30723ms step_avg:39.95ms
step:770/2330 train_time:30780ms step_avg:39.97ms
step:771/2330 train_time:30802ms step_avg:39.95ms
step:772/2330 train_time:30858ms step_avg:39.97ms
step:773/2330 train_time:30881ms step_avg:39.95ms
step:774/2330 train_time:30938ms step_avg:39.97ms
step:775/2330 train_time:30961ms step_avg:39.95ms
step:776/2330 train_time:31018ms step_avg:39.97ms
step:777/2330 train_time:31042ms step_avg:39.95ms
step:778/2330 train_time:31099ms step_avg:39.97ms
step:779/2330 train_time:31123ms step_avg:39.95ms
step:780/2330 train_time:31179ms step_avg:39.97ms
step:781/2330 train_time:31202ms step_avg:39.95ms
step:782/2330 train_time:31258ms step_avg:39.97ms
step:783/2330 train_time:31283ms step_avg:39.95ms
step:784/2330 train_time:31341ms step_avg:39.98ms
step:785/2330 train_time:31366ms step_avg:39.96ms
step:786/2330 train_time:31424ms step_avg:39.98ms
step:787/2330 train_time:31448ms step_avg:39.96ms
step:788/2330 train_time:31505ms step_avg:39.98ms
step:789/2330 train_time:31528ms step_avg:39.96ms
step:790/2330 train_time:31585ms step_avg:39.98ms
step:791/2330 train_time:31608ms step_avg:39.96ms
step:792/2330 train_time:31664ms step_avg:39.98ms
step:793/2330 train_time:31688ms step_avg:39.96ms
step:794/2330 train_time:31744ms step_avg:39.98ms
step:795/2330 train_time:31767ms step_avg:39.96ms
step:796/2330 train_time:31823ms step_avg:39.98ms
step:797/2330 train_time:31846ms step_avg:39.96ms
step:798/2330 train_time:31902ms step_avg:39.98ms
step:799/2330 train_time:31925ms step_avg:39.96ms
step:800/2330 train_time:31982ms step_avg:39.98ms
step:801/2330 train_time:32005ms step_avg:39.96ms
step:802/2330 train_time:32062ms step_avg:39.98ms
step:803/2330 train_time:32086ms step_avg:39.96ms
step:804/2330 train_time:32142ms step_avg:39.98ms
step:805/2330 train_time:32165ms step_avg:39.96ms
step:806/2330 train_time:32222ms step_avg:39.98ms
step:807/2330 train_time:32245ms step_avg:39.96ms
step:808/2330 train_time:32303ms step_avg:39.98ms
step:809/2330 train_time:32326ms step_avg:39.96ms
step:810/2330 train_time:32383ms step_avg:39.98ms
step:811/2330 train_time:32407ms step_avg:39.96ms
step:812/2330 train_time:32463ms step_avg:39.98ms
step:813/2330 train_time:32487ms step_avg:39.96ms
step:814/2330 train_time:32544ms step_avg:39.98ms
step:815/2330 train_time:32568ms step_avg:39.96ms
step:816/2330 train_time:32625ms step_avg:39.98ms
step:817/2330 train_time:32647ms step_avg:39.96ms
step:818/2330 train_time:32704ms step_avg:39.98ms
step:819/2330 train_time:32727ms step_avg:39.96ms
step:820/2330 train_time:32783ms step_avg:39.98ms
step:821/2330 train_time:32806ms step_avg:39.96ms
step:822/2330 train_time:32862ms step_avg:39.98ms
step:823/2330 train_time:32885ms step_avg:39.96ms
step:824/2330 train_time:32942ms step_avg:39.98ms
step:825/2330 train_time:32964ms step_avg:39.96ms
step:826/2330 train_time:33020ms step_avg:39.98ms
step:827/2330 train_time:33043ms step_avg:39.96ms
step:828/2330 train_time:33100ms step_avg:39.98ms
step:829/2330 train_time:33123ms step_avg:39.96ms
step:830/2330 train_time:33180ms step_avg:39.98ms
step:831/2330 train_time:33203ms step_avg:39.96ms
step:832/2330 train_time:33260ms step_avg:39.98ms
step:833/2330 train_time:33285ms step_avg:39.96ms
step:834/2330 train_time:33343ms step_avg:39.98ms
step:835/2330 train_time:33366ms step_avg:39.96ms
step:836/2330 train_time:33422ms step_avg:39.98ms
step:837/2330 train_time:33446ms step_avg:39.96ms
step:838/2330 train_time:33503ms step_avg:39.98ms
step:839/2330 train_time:33527ms step_avg:39.96ms
step:840/2330 train_time:33583ms step_avg:39.98ms
step:841/2330 train_time:33607ms step_avg:39.96ms
step:842/2330 train_time:33663ms step_avg:39.98ms
step:843/2330 train_time:33687ms step_avg:39.96ms
step:844/2330 train_time:33743ms step_avg:39.98ms
step:845/2330 train_time:33766ms step_avg:39.96ms
step:846/2330 train_time:33822ms step_avg:39.98ms
step:847/2330 train_time:33846ms step_avg:39.96ms
step:848/2330 train_time:33902ms step_avg:39.98ms
step:849/2330 train_time:33925ms step_avg:39.96ms
step:850/2330 train_time:33982ms step_avg:39.98ms
step:851/2330 train_time:34005ms step_avg:39.96ms
step:852/2330 train_time:34062ms step_avg:39.98ms
step:853/2330 train_time:34086ms step_avg:39.96ms
step:854/2330 train_time:34143ms step_avg:39.98ms
step:855/2330 train_time:34166ms step_avg:39.96ms
step:856/2330 train_time:34222ms step_avg:39.98ms
step:857/2330 train_time:34245ms step_avg:39.96ms
step:858/2330 train_time:34302ms step_avg:39.98ms
step:859/2330 train_time:34325ms step_avg:39.96ms
step:860/2330 train_time:34383ms step_avg:39.98ms
step:861/2330 train_time:34407ms step_avg:39.96ms
step:862/2330 train_time:34464ms step_avg:39.98ms
step:863/2330 train_time:34488ms step_avg:39.96ms
step:864/2330 train_time:34546ms step_avg:39.98ms
step:865/2330 train_time:34569ms step_avg:39.96ms
step:866/2330 train_time:34624ms step_avg:39.98ms
step:867/2330 train_time:34647ms step_avg:39.96ms
step:868/2330 train_time:34704ms step_avg:39.98ms
step:869/2330 train_time:34728ms step_avg:39.96ms
step:870/2330 train_time:34784ms step_avg:39.98ms
step:871/2330 train_time:34807ms step_avg:39.96ms
step:872/2330 train_time:34863ms step_avg:39.98ms
step:873/2330 train_time:34886ms step_avg:39.96ms
step:874/2330 train_time:34942ms step_avg:39.98ms
step:875/2330 train_time:34965ms step_avg:39.96ms
step:876/2330 train_time:35021ms step_avg:39.98ms
step:877/2330 train_time:35044ms step_avg:39.96ms
step:878/2330 train_time:35102ms step_avg:39.98ms
step:879/2330 train_time:35126ms step_avg:39.96ms
step:880/2330 train_time:35182ms step_avg:39.98ms
step:881/2330 train_time:35205ms step_avg:39.96ms
step:882/2330 train_time:35262ms step_avg:39.98ms
step:883/2330 train_time:35286ms step_avg:39.96ms
step:884/2330 train_time:35343ms step_avg:39.98ms
step:885/2330 train_time:35366ms step_avg:39.96ms
step:886/2330 train_time:35422ms step_avg:39.98ms
step:887/2330 train_time:35446ms step_avg:39.96ms
step:888/2330 train_time:35503ms step_avg:39.98ms
step:889/2330 train_time:35527ms step_avg:39.96ms
step:890/2330 train_time:35584ms step_avg:39.98ms
step:891/2330 train_time:35607ms step_avg:39.96ms
step:892/2330 train_time:35663ms step_avg:39.98ms
step:893/2330 train_time:35687ms step_avg:39.96ms
step:894/2330 train_time:35743ms step_avg:39.98ms
step:895/2330 train_time:35766ms step_avg:39.96ms
step:896/2330 train_time:35823ms step_avg:39.98ms
step:897/2330 train_time:35846ms step_avg:39.96ms
step:898/2330 train_time:35902ms step_avg:39.98ms
step:899/2330 train_time:35925ms step_avg:39.96ms
step:900/2330 train_time:35982ms step_avg:39.98ms
step:901/2330 train_time:36006ms step_avg:39.96ms
step:902/2330 train_time:36063ms step_avg:39.98ms
step:903/2330 train_time:36086ms step_avg:39.96ms
step:904/2330 train_time:36143ms step_avg:39.98ms
step:905/2330 train_time:36167ms step_avg:39.96ms
step:906/2330 train_time:36223ms step_avg:39.98ms
step:907/2330 train_time:36246ms step_avg:39.96ms
step:908/2330 train_time:36303ms step_avg:39.98ms
step:909/2330 train_time:36327ms step_avg:39.96ms
step:910/2330 train_time:36384ms step_avg:39.98ms
step:911/2330 train_time:36407ms step_avg:39.96ms
step:912/2330 train_time:36463ms step_avg:39.98ms
step:913/2330 train_time:36488ms step_avg:39.96ms
step:914/2330 train_time:36545ms step_avg:39.98ms
step:915/2330 train_time:36568ms step_avg:39.97ms
step:916/2330 train_time:36625ms step_avg:39.98ms
step:917/2330 train_time:36648ms step_avg:39.96ms
step:918/2330 train_time:36704ms step_avg:39.98ms
step:919/2330 train_time:36727ms step_avg:39.96ms
step:920/2330 train_time:36783ms step_avg:39.98ms
step:921/2330 train_time:36806ms step_avg:39.96ms
step:922/2330 train_time:36862ms step_avg:39.98ms
step:923/2330 train_time:36885ms step_avg:39.96ms
step:924/2330 train_time:36943ms step_avg:39.98ms
step:925/2330 train_time:36965ms step_avg:39.96ms
step:926/2330 train_time:37022ms step_avg:39.98ms
step:927/2330 train_time:37046ms step_avg:39.96ms
step:928/2330 train_time:37102ms step_avg:39.98ms
step:929/2330 train_time:37126ms step_avg:39.96ms
step:930/2330 train_time:37182ms step_avg:39.98ms
step:931/2330 train_time:37205ms step_avg:39.96ms
step:932/2330 train_time:37263ms step_avg:39.98ms
step:933/2330 train_time:37287ms step_avg:39.96ms
step:934/2330 train_time:37344ms step_avg:39.98ms
step:935/2330 train_time:37367ms step_avg:39.97ms
step:936/2330 train_time:37424ms step_avg:39.98ms
step:937/2330 train_time:37448ms step_avg:39.97ms
step:938/2330 train_time:37505ms step_avg:39.98ms
step:939/2330 train_time:37529ms step_avg:39.97ms
step:940/2330 train_time:37586ms step_avg:39.98ms
step:941/2330 train_time:37609ms step_avg:39.97ms
step:942/2330 train_time:37665ms step_avg:39.98ms
step:943/2330 train_time:37688ms step_avg:39.97ms
step:944/2330 train_time:37744ms step_avg:39.98ms
step:945/2330 train_time:37767ms step_avg:39.97ms
step:946/2330 train_time:37823ms step_avg:39.98ms
step:947/2330 train_time:37847ms step_avg:39.97ms
step:948/2330 train_time:37903ms step_avg:39.98ms
step:949/2330 train_time:37926ms step_avg:39.96ms
step:950/2330 train_time:37983ms step_avg:39.98ms
step:951/2330 train_time:38006ms step_avg:39.96ms
step:952/2330 train_time:38062ms step_avg:39.98ms
step:953/2330 train_time:38086ms step_avg:39.96ms
step:954/2330 train_time:38143ms step_avg:39.98ms
step:955/2330 train_time:38166ms step_avg:39.96ms
step:956/2330 train_time:38222ms step_avg:39.98ms
step:957/2330 train_time:38245ms step_avg:39.96ms
step:958/2330 train_time:38302ms step_avg:39.98ms
step:959/2330 train_time:38326ms step_avg:39.96ms
step:960/2330 train_time:38383ms step_avg:39.98ms
step:961/2330 train_time:38407ms step_avg:39.97ms
step:962/2330 train_time:38463ms step_avg:39.98ms
step:963/2330 train_time:38487ms step_avg:39.97ms
step:964/2330 train_time:38544ms step_avg:39.98ms
step:965/2330 train_time:38568ms step_avg:39.97ms
step:966/2330 train_time:38625ms step_avg:39.98ms
step:967/2330 train_time:38648ms step_avg:39.97ms
step:968/2330 train_time:38705ms step_avg:39.98ms
step:969/2330 train_time:38728ms step_avg:39.97ms
step:970/2330 train_time:38784ms step_avg:39.98ms
step:971/2330 train_time:38807ms step_avg:39.97ms
step:972/2330 train_time:38863ms step_avg:39.98ms
step:973/2330 train_time:38886ms step_avg:39.97ms
step:974/2330 train_time:38943ms step_avg:39.98ms
step:975/2330 train_time:38965ms step_avg:39.96ms
step:976/2330 train_time:39022ms step_avg:39.98ms
step:977/2330 train_time:39045ms step_avg:39.96ms
step:978/2330 train_time:39101ms step_avg:39.98ms
step:979/2330 train_time:39124ms step_avg:39.96ms
step:980/2330 train_time:39181ms step_avg:39.98ms
step:981/2330 train_time:39204ms step_avg:39.96ms
step:982/2330 train_time:39262ms step_avg:39.98ms
step:983/2330 train_time:39286ms step_avg:39.97ms
step:984/2330 train_time:39343ms step_avg:39.98ms
step:985/2330 train_time:39366ms step_avg:39.97ms
step:986/2330 train_time:39423ms step_avg:39.98ms
step:987/2330 train_time:39447ms step_avg:39.97ms
step:988/2330 train_time:39504ms step_avg:39.98ms
step:989/2330 train_time:39527ms step_avg:39.97ms
step:990/2330 train_time:39584ms step_avg:39.98ms
step:991/2330 train_time:39607ms step_avg:39.97ms
step:992/2330 train_time:39663ms step_avg:39.98ms
step:993/2330 train_time:39686ms step_avg:39.97ms
step:994/2330 train_time:39742ms step_avg:39.98ms
step:995/2330 train_time:39765ms step_avg:39.96ms
step:996/2330 train_time:39822ms step_avg:39.98ms
step:997/2330 train_time:39845ms step_avg:39.97ms
step:998/2330 train_time:39902ms step_avg:39.98ms
step:999/2330 train_time:39925ms step_avg:39.96ms
step:1000/2330 train_time:39981ms step_avg:39.98ms
step:1000/2330 val_loss:5.6482 train_time:40078ms step_avg:40.08ms
step:1001/2330 train_time:40091ms step_avg:40.05ms
step:1002/2330 train_time:40102ms step_avg:40.02ms
step:1003/2330 train_time:40112ms step_avg:39.99ms
step:1004/2330 train_time:40141ms step_avg:39.98ms
step:1005/2330 train_time:40163ms step_avg:39.96ms
step:1006/2330 train_time:40219ms step_avg:39.98ms
step:1007/2330 train_time:40241ms step_avg:39.96ms
step:1008/2330 train_time:40297ms step_avg:39.98ms
step:1009/2330 train_time:40320ms step_avg:39.96ms
step:1010/2330 train_time:40379ms step_avg:39.98ms
step:1011/2330 train_time:40404ms step_avg:39.96ms
step:1012/2330 train_time:40466ms step_avg:39.99ms
step:1013/2330 train_time:40490ms step_avg:39.97ms
step:1014/2330 train_time:40548ms step_avg:39.99ms
step:1015/2330 train_time:40571ms step_avg:39.97ms
step:1016/2330 train_time:40627ms step_avg:39.99ms
step:1017/2330 train_time:40651ms step_avg:39.97ms
step:1018/2330 train_time:40708ms step_avg:39.99ms
step:1019/2330 train_time:40730ms step_avg:39.97ms
step:1020/2330 train_time:40786ms step_avg:39.99ms
step:1021/2330 train_time:40809ms step_avg:39.97ms
step:1022/2330 train_time:40864ms step_avg:39.98ms
step:1023/2330 train_time:40887ms step_avg:39.97ms
step:1024/2330 train_time:40943ms step_avg:39.98ms
step:1025/2330 train_time:40966ms step_avg:39.97ms
step:1026/2330 train_time:41025ms step_avg:39.99ms
step:1027/2330 train_time:41050ms step_avg:39.97ms
step:1028/2330 train_time:41106ms step_avg:39.99ms
step:1029/2330 train_time:41129ms step_avg:39.97ms
step:1030/2330 train_time:41185ms step_avg:39.99ms
step:1031/2330 train_time:41208ms step_avg:39.97ms
step:1032/2330 train_time:41264ms step_avg:39.98ms
step:1033/2330 train_time:41287ms step_avg:39.97ms
step:1034/2330 train_time:41345ms step_avg:39.99ms
step:1035/2330 train_time:41369ms step_avg:39.97ms
step:1036/2330 train_time:41426ms step_avg:39.99ms
step:1037/2330 train_time:41450ms step_avg:39.97ms
step:1038/2330 train_time:41507ms step_avg:39.99ms
step:1039/2330 train_time:41532ms step_avg:39.97ms
step:1040/2330 train_time:41589ms step_avg:39.99ms
step:1041/2330 train_time:41612ms step_avg:39.97ms
step:1042/2330 train_time:41669ms step_avg:39.99ms
step:1043/2330 train_time:41692ms step_avg:39.97ms
step:1044/2330 train_time:41749ms step_avg:39.99ms
step:1045/2330 train_time:41771ms step_avg:39.97ms
step:1046/2330 train_time:41828ms step_avg:39.99ms
step:1047/2330 train_time:41851ms step_avg:39.97ms
step:1048/2330 train_time:41908ms step_avg:39.99ms
step:1049/2330 train_time:41931ms step_avg:39.97ms
step:1050/2330 train_time:41987ms step_avg:39.99ms
step:1051/2330 train_time:42011ms step_avg:39.97ms
step:1052/2330 train_time:42068ms step_avg:39.99ms
step:1053/2330 train_time:42091ms step_avg:39.97ms
step:1054/2330 train_time:42147ms step_avg:39.99ms
step:1055/2330 train_time:42170ms step_avg:39.97ms
step:1056/2330 train_time:42227ms step_avg:39.99ms
step:1057/2330 train_time:42250ms step_avg:39.97ms
step:1058/2330 train_time:42307ms step_avg:39.99ms
step:1059/2330 train_time:42330ms step_avg:39.97ms
step:1060/2330 train_time:42387ms step_avg:39.99ms
step:1061/2330 train_time:42410ms step_avg:39.97ms
step:1062/2330 train_time:42467ms step_avg:39.99ms
step:1063/2330 train_time:42490ms step_avg:39.97ms
step:1064/2330 train_time:42547ms step_avg:39.99ms
step:1065/2330 train_time:42570ms step_avg:39.97ms
step:1066/2330 train_time:42627ms step_avg:39.99ms
step:1067/2330 train_time:42650ms step_avg:39.97ms
step:1068/2330 train_time:42707ms step_avg:39.99ms
step:1069/2330 train_time:42730ms step_avg:39.97ms
step:1070/2330 train_time:42787ms step_avg:39.99ms
step:1071/2330 train_time:42811ms step_avg:39.97ms
step:1072/2330 train_time:42868ms step_avg:39.99ms
step:1073/2330 train_time:42891ms step_avg:39.97ms
step:1074/2330 train_time:42947ms step_avg:39.99ms
step:1075/2330 train_time:42971ms step_avg:39.97ms
step:1076/2330 train_time:43027ms step_avg:39.99ms
step:1077/2330 train_time:43050ms step_avg:39.97ms
step:1078/2330 train_time:43107ms step_avg:39.99ms
step:1079/2330 train_time:43130ms step_avg:39.97ms
step:1080/2330 train_time:43186ms step_avg:39.99ms
step:1081/2330 train_time:43209ms step_avg:39.97ms
step:1082/2330 train_time:43266ms step_avg:39.99ms
step:1083/2330 train_time:43290ms step_avg:39.97ms
step:1084/2330 train_time:43346ms step_avg:39.99ms
step:1085/2330 train_time:43370ms step_avg:39.97ms
step:1086/2330 train_time:43426ms step_avg:39.99ms
step:1087/2330 train_time:43449ms step_avg:39.97ms
step:1088/2330 train_time:43505ms step_avg:39.99ms
step:1089/2330 train_time:43529ms step_avg:39.97ms
step:1090/2330 train_time:43586ms step_avg:39.99ms
step:1091/2330 train_time:43609ms step_avg:39.97ms
step:1092/2330 train_time:43665ms step_avg:39.99ms
step:1093/2330 train_time:43690ms step_avg:39.97ms
step:1094/2330 train_time:43746ms step_avg:39.99ms
step:1095/2330 train_time:43769ms step_avg:39.97ms
step:1096/2330 train_time:43825ms step_avg:39.99ms
step:1097/2330 train_time:43848ms step_avg:39.97ms
step:1098/2330 train_time:43905ms step_avg:39.99ms
step:1099/2330 train_time:43928ms step_avg:39.97ms
step:1100/2330 train_time:43985ms step_avg:39.99ms
step:1101/2330 train_time:44008ms step_avg:39.97ms
step:1102/2330 train_time:44064ms step_avg:39.99ms
step:1103/2330 train_time:44087ms step_avg:39.97ms
step:1104/2330 train_time:44144ms step_avg:39.99ms
step:1105/2330 train_time:44167ms step_avg:39.97ms
step:1106/2330 train_time:44223ms step_avg:39.98ms
step:1107/2330 train_time:44247ms step_avg:39.97ms
step:1108/2330 train_time:44304ms step_avg:39.99ms
step:1109/2330 train_time:44328ms step_avg:39.97ms
step:1110/2330 train_time:44385ms step_avg:39.99ms
step:1111/2330 train_time:44408ms step_avg:39.97ms
step:1112/2330 train_time:44465ms step_avg:39.99ms
step:1113/2330 train_time:44488ms step_avg:39.97ms
step:1114/2330 train_time:44545ms step_avg:39.99ms
step:1115/2330 train_time:44568ms step_avg:39.97ms
step:1116/2330 train_time:44625ms step_avg:39.99ms
step:1117/2330 train_time:44648ms step_avg:39.97ms
step:1118/2330 train_time:44705ms step_avg:39.99ms
step:1119/2330 train_time:44728ms step_avg:39.97ms
step:1120/2330 train_time:44785ms step_avg:39.99ms
step:1121/2330 train_time:44808ms step_avg:39.97ms
step:1122/2330 train_time:44865ms step_avg:39.99ms
step:1123/2330 train_time:44888ms step_avg:39.97ms
step:1124/2330 train_time:44944ms step_avg:39.99ms
step:1125/2330 train_time:44968ms step_avg:39.97ms
step:1126/2330 train_time:45025ms step_avg:39.99ms
step:1127/2330 train_time:45048ms step_avg:39.97ms
step:1128/2330 train_time:45104ms step_avg:39.99ms
step:1129/2330 train_time:45127ms step_avg:39.97ms
step:1130/2330 train_time:45184ms step_avg:39.99ms
step:1131/2330 train_time:45207ms step_avg:39.97ms
step:1132/2330 train_time:45264ms step_avg:39.99ms
step:1133/2330 train_time:45288ms step_avg:39.97ms
step:1134/2330 train_time:45345ms step_avg:39.99ms
step:1135/2330 train_time:45369ms step_avg:39.97ms
step:1136/2330 train_time:45425ms step_avg:39.99ms
step:1137/2330 train_time:45448ms step_avg:39.97ms
step:1138/2330 train_time:45505ms step_avg:39.99ms
step:1139/2330 train_time:45529ms step_avg:39.97ms
step:1140/2330 train_time:45585ms step_avg:39.99ms
step:1141/2330 train_time:45609ms step_avg:39.97ms
step:1142/2330 train_time:45665ms step_avg:39.99ms
step:1143/2330 train_time:45689ms step_avg:39.97ms
step:1144/2330 train_time:45745ms step_avg:39.99ms
step:1145/2330 train_time:45768ms step_avg:39.97ms
step:1146/2330 train_time:45825ms step_avg:39.99ms
step:1147/2330 train_time:45848ms step_avg:39.97ms
step:1148/2330 train_time:45905ms step_avg:39.99ms
step:1149/2330 train_time:45928ms step_avg:39.97ms
step:1150/2330 train_time:45985ms step_avg:39.99ms
step:1151/2330 train_time:46008ms step_avg:39.97ms
step:1152/2330 train_time:46064ms step_avg:39.99ms
step:1153/2330 train_time:46088ms step_avg:39.97ms
step:1154/2330 train_time:46145ms step_avg:39.99ms
step:1155/2330 train_time:46168ms step_avg:39.97ms
step:1156/2330 train_time:46225ms step_avg:39.99ms
step:1157/2330 train_time:46248ms step_avg:39.97ms
step:1158/2330 train_time:46305ms step_avg:39.99ms
step:1159/2330 train_time:46329ms step_avg:39.97ms
step:1160/2330 train_time:46385ms step_avg:39.99ms
step:1161/2330 train_time:46408ms step_avg:39.97ms
step:1162/2330 train_time:46465ms step_avg:39.99ms
step:1163/2330 train_time:46488ms step_avg:39.97ms
step:1164/2330 train_time:46545ms step_avg:39.99ms
step:1165/2330 train_time:46568ms step_avg:39.97ms
step:1166/2330 train_time:46625ms step_avg:39.99ms
step:1167/2330 train_time:46649ms step_avg:39.97ms
step:1168/2330 train_time:46705ms step_avg:39.99ms
step:1169/2330 train_time:46729ms step_avg:39.97ms
step:1170/2330 train_time:46785ms step_avg:39.99ms
step:1171/2330 train_time:46808ms step_avg:39.97ms
step:1172/2330 train_time:46865ms step_avg:39.99ms
step:1173/2330 train_time:46889ms step_avg:39.97ms
step:1174/2330 train_time:46945ms step_avg:39.99ms
step:1175/2330 train_time:46968ms step_avg:39.97ms
step:1176/2330 train_time:47025ms step_avg:39.99ms
step:1177/2330 train_time:47048ms step_avg:39.97ms
step:1178/2330 train_time:47105ms step_avg:39.99ms
step:1179/2330 train_time:47129ms step_avg:39.97ms
step:1180/2330 train_time:47186ms step_avg:39.99ms
step:1181/2330 train_time:47209ms step_avg:39.97ms
step:1182/2330 train_time:47265ms step_avg:39.99ms
step:1183/2330 train_time:47289ms step_avg:39.97ms
step:1184/2330 train_time:47345ms step_avg:39.99ms
step:1185/2330 train_time:47369ms step_avg:39.97ms
step:1186/2330 train_time:47425ms step_avg:39.99ms
step:1187/2330 train_time:47449ms step_avg:39.97ms
step:1188/2330 train_time:47505ms step_avg:39.99ms
step:1189/2330 train_time:47529ms step_avg:39.97ms
step:1190/2330 train_time:47586ms step_avg:39.99ms
step:1191/2330 train_time:47610ms step_avg:39.98ms
step:1192/2330 train_time:47667ms step_avg:39.99ms
step:1193/2330 train_time:47691ms step_avg:39.98ms
step:1194/2330 train_time:47747ms step_avg:39.99ms
step:1195/2330 train_time:47770ms step_avg:39.98ms
step:1196/2330 train_time:47827ms step_avg:39.99ms
step:1197/2330 train_time:47850ms step_avg:39.97ms
step:1198/2330 train_time:47907ms step_avg:39.99ms
step:1199/2330 train_time:47930ms step_avg:39.98ms
step:1200/2330 train_time:47987ms step_avg:39.99ms
step:1201/2330 train_time:48010ms step_avg:39.98ms
step:1202/2330 train_time:48067ms step_avg:39.99ms
step:1203/2330 train_time:48090ms step_avg:39.98ms
step:1204/2330 train_time:48147ms step_avg:39.99ms
step:1205/2330 train_time:48170ms step_avg:39.98ms
step:1206/2330 train_time:48227ms step_avg:39.99ms
step:1207/2330 train_time:48250ms step_avg:39.98ms
step:1208/2330 train_time:48306ms step_avg:39.99ms
step:1209/2330 train_time:48329ms step_avg:39.97ms
step:1210/2330 train_time:48386ms step_avg:39.99ms
step:1211/2330 train_time:48409ms step_avg:39.97ms
step:1212/2330 train_time:48465ms step_avg:39.99ms
step:1213/2330 train_time:48488ms step_avg:39.97ms
step:1214/2330 train_time:48546ms step_avg:39.99ms
step:1215/2330 train_time:48569ms step_avg:39.97ms
step:1216/2330 train_time:48626ms step_avg:39.99ms
step:1217/2330 train_time:48649ms step_avg:39.97ms
step:1218/2330 train_time:48705ms step_avg:39.99ms
step:1219/2330 train_time:48728ms step_avg:39.97ms
step:1220/2330 train_time:48785ms step_avg:39.99ms
step:1221/2330 train_time:48808ms step_avg:39.97ms
step:1222/2330 train_time:48865ms step_avg:39.99ms
step:1223/2330 train_time:48888ms step_avg:39.97ms
step:1224/2330 train_time:48945ms step_avg:39.99ms
step:1225/2330 train_time:48968ms step_avg:39.97ms
step:1226/2330 train_time:49025ms step_avg:39.99ms
step:1227/2330 train_time:49048ms step_avg:39.97ms
step:1228/2330 train_time:49105ms step_avg:39.99ms
step:1229/2330 train_time:49129ms step_avg:39.97ms
step:1230/2330 train_time:49186ms step_avg:39.99ms
step:1231/2330 train_time:49209ms step_avg:39.97ms
step:1232/2330 train_time:49265ms step_avg:39.99ms
step:1233/2330 train_time:49288ms step_avg:39.97ms
step:1234/2330 train_time:49345ms step_avg:39.99ms
step:1235/2330 train_time:49369ms step_avg:39.97ms
step:1236/2330 train_time:49425ms step_avg:39.99ms
step:1237/2330 train_time:49448ms step_avg:39.97ms
step:1238/2330 train_time:49505ms step_avg:39.99ms
step:1239/2330 train_time:49528ms step_avg:39.97ms
step:1240/2330 train_time:49586ms step_avg:39.99ms
step:1241/2330 train_time:49609ms step_avg:39.98ms
step:1242/2330 train_time:49665ms step_avg:39.99ms
step:1243/2330 train_time:49689ms step_avg:39.98ms
step:1244/2330 train_time:49746ms step_avg:39.99ms
step:1245/2330 train_time:49770ms step_avg:39.98ms
step:1246/2330 train_time:49826ms step_avg:39.99ms
step:1247/2330 train_time:49849ms step_avg:39.98ms
step:1248/2330 train_time:49905ms step_avg:39.99ms
step:1249/2330 train_time:49928ms step_avg:39.97ms
step:1250/2330 train_time:49986ms step_avg:39.99ms
step:1250/2330 val_loss:5.6428 train_time:50083ms step_avg:40.07ms
step:1251/2330 train_time:50095ms step_avg:40.04ms
step:1252/2330 train_time:50108ms step_avg:40.02ms
step:1253/2330 train_time:50118ms step_avg:40.00ms
step:1254/2330 train_time:50146ms step_avg:39.99ms
step:1255/2330 train_time:50169ms step_avg:39.97ms
step:1256/2330 train_time:50225ms step_avg:39.99ms
step:1257/2330 train_time:50247ms step_avg:39.97ms
step:1258/2330 train_time:50303ms step_avg:39.99ms
step:1259/2330 train_time:50325ms step_avg:39.97ms
step:1260/2330 train_time:50385ms step_avg:39.99ms
step:1261/2330 train_time:50410ms step_avg:39.98ms
step:1262/2330 train_time:50473ms step_avg:39.99ms
step:1263/2330 train_time:50498ms step_avg:39.98ms
step:1264/2330 train_time:50556ms step_avg:40.00ms
step:1265/2330 train_time:50579ms step_avg:39.98ms
step:1266/2330 train_time:50635ms step_avg:40.00ms
step:1267/2330 train_time:50658ms step_avg:39.98ms
step:1268/2330 train_time:50714ms step_avg:40.00ms
step:1269/2330 train_time:50737ms step_avg:39.98ms
step:1270/2330 train_time:50794ms step_avg:39.99ms
step:1271/2330 train_time:50816ms step_avg:39.98ms
step:1272/2330 train_time:50873ms step_avg:39.99ms
step:1273/2330 train_time:50895ms step_avg:39.98ms
step:1274/2330 train_time:50952ms step_avg:39.99ms
step:1275/2330 train_time:50975ms step_avg:39.98ms
step:1276/2330 train_time:51033ms step_avg:39.99ms
step:1277/2330 train_time:51056ms step_avg:39.98ms
step:1278/2330 train_time:51113ms step_avg:39.99ms
step:1279/2330 train_time:51136ms step_avg:39.98ms
step:1280/2330 train_time:51194ms step_avg:40.00ms
step:1281/2330 train_time:51217ms step_avg:39.98ms
step:1282/2330 train_time:51274ms step_avg:40.00ms
step:1283/2330 train_time:51297ms step_avg:39.98ms
step:1284/2330 train_time:51355ms step_avg:40.00ms
step:1285/2330 train_time:51381ms step_avg:39.98ms
step:1286/2330 train_time:51439ms step_avg:40.00ms
step:1287/2330 train_time:51462ms step_avg:39.99ms
step:1288/2330 train_time:51519ms step_avg:40.00ms
step:1289/2330 train_time:51542ms step_avg:39.99ms
step:1290/2330 train_time:51598ms step_avg:40.00ms
step:1291/2330 train_time:51621ms step_avg:39.99ms
step:1292/2330 train_time:51678ms step_avg:40.00ms
step:1293/2330 train_time:51701ms step_avg:39.99ms
step:1294/2330 train_time:51757ms step_avg:40.00ms
step:1295/2330 train_time:51781ms step_avg:39.98ms
step:1296/2330 train_time:51836ms step_avg:40.00ms
step:1297/2330 train_time:51859ms step_avg:39.98ms
step:1298/2330 train_time:51916ms step_avg:40.00ms
step:1299/2330 train_time:51940ms step_avg:39.98ms
step:1300/2330 train_time:51997ms step_avg:40.00ms
step:1301/2330 train_time:52020ms step_avg:39.98ms
step:1302/2330 train_time:52077ms step_avg:40.00ms
step:1303/2330 train_time:52100ms step_avg:39.99ms
step:1304/2330 train_time:52157ms step_avg:40.00ms
step:1305/2330 train_time:52180ms step_avg:39.98ms
step:1306/2330 train_time:52237ms step_avg:40.00ms
step:1307/2330 train_time:52260ms step_avg:39.98ms
step:1308/2330 train_time:52317ms step_avg:40.00ms
step:1309/2330 train_time:52341ms step_avg:39.99ms
step:1310/2330 train_time:52398ms step_avg:40.00ms
step:1311/2330 train_time:52422ms step_avg:39.99ms
step:1312/2330 train_time:52479ms step_avg:40.00ms
step:1313/2330 train_time:52502ms step_avg:39.99ms
step:1314/2330 train_time:52558ms step_avg:40.00ms
step:1315/2330 train_time:52581ms step_avg:39.99ms
step:1316/2330 train_time:52638ms step_avg:40.00ms
step:1317/2330 train_time:52661ms step_avg:39.99ms
step:1318/2330 train_time:52717ms step_avg:40.00ms
step:1319/2330 train_time:52740ms step_avg:39.98ms
step:1320/2330 train_time:52797ms step_avg:40.00ms
step:1321/2330 train_time:52820ms step_avg:39.98ms
step:1322/2330 train_time:52877ms step_avg:40.00ms
step:1323/2330 train_time:52900ms step_avg:39.99ms
step:1324/2330 train_time:52957ms step_avg:40.00ms
step:1325/2330 train_time:52980ms step_avg:39.99ms
step:1326/2330 train_time:53037ms step_avg:40.00ms
step:1327/2330 train_time:53060ms step_avg:39.98ms
step:1328/2330 train_time:53116ms step_avg:40.00ms
step:1329/2330 train_time:53139ms step_avg:39.98ms
step:1330/2330 train_time:53196ms step_avg:40.00ms
step:1331/2330 train_time:53220ms step_avg:39.98ms
step:1332/2330 train_time:53277ms step_avg:40.00ms
step:1333/2330 train_time:53301ms step_avg:39.99ms
step:1334/2330 train_time:53358ms step_avg:40.00ms
step:1335/2330 train_time:53382ms step_avg:39.99ms
step:1336/2330 train_time:53439ms step_avg:40.00ms
step:1337/2330 train_time:53462ms step_avg:39.99ms
step:1338/2330 train_time:53518ms step_avg:40.00ms
step:1339/2330 train_time:53541ms step_avg:39.99ms
step:1340/2330 train_time:53597ms step_avg:40.00ms
step:1341/2330 train_time:53621ms step_avg:39.99ms
step:1342/2330 train_time:53677ms step_avg:40.00ms
step:1343/2330 train_time:53700ms step_avg:39.99ms
step:1344/2330 train_time:53757ms step_avg:40.00ms
step:1345/2330 train_time:53781ms step_avg:39.99ms
step:1346/2330 train_time:53837ms step_avg:40.00ms
step:1347/2330 train_time:53860ms step_avg:39.99ms
step:1348/2330 train_time:53916ms step_avg:40.00ms
step:1349/2330 train_time:53940ms step_avg:39.99ms
step:1350/2330 train_time:53996ms step_avg:40.00ms
step:1351/2330 train_time:54020ms step_avg:39.99ms
step:1352/2330 train_time:54076ms step_avg:40.00ms
step:1353/2330 train_time:54099ms step_avg:39.98ms
step:1354/2330 train_time:54156ms step_avg:40.00ms
step:1355/2330 train_time:54180ms step_avg:39.98ms
step:1356/2330 train_time:54236ms step_avg:40.00ms
step:1357/2330 train_time:54261ms step_avg:39.99ms
step:1358/2330 train_time:54317ms step_avg:40.00ms
step:1359/2330 train_time:54341ms step_avg:39.99ms
step:1360/2330 train_time:54398ms step_avg:40.00ms
step:1361/2330 train_time:54422ms step_avg:39.99ms
step:1362/2330 train_time:54478ms step_avg:40.00ms
step:1363/2330 train_time:54501ms step_avg:39.99ms
step:1364/2330 train_time:54557ms step_avg:40.00ms
step:1365/2330 train_time:54580ms step_avg:39.99ms
step:1366/2330 train_time:54637ms step_avg:40.00ms
step:1367/2330 train_time:54660ms step_avg:39.99ms
step:1368/2330 train_time:54716ms step_avg:40.00ms
step:1369/2330 train_time:54739ms step_avg:39.98ms
step:1370/2330 train_time:54796ms step_avg:40.00ms
step:1371/2330 train_time:54820ms step_avg:39.99ms
step:1372/2330 train_time:54876ms step_avg:40.00ms
step:1373/2330 train_time:54900ms step_avg:39.99ms
step:1374/2330 train_time:54956ms step_avg:40.00ms
step:1375/2330 train_time:54980ms step_avg:39.99ms
step:1376/2330 train_time:55036ms step_avg:40.00ms
step:1377/2330 train_time:55059ms step_avg:39.99ms
step:1378/2330 train_time:55116ms step_avg:40.00ms
step:1379/2330 train_time:55139ms step_avg:39.98ms
step:1380/2330 train_time:55197ms step_avg:40.00ms
step:1381/2330 train_time:55222ms step_avg:39.99ms
step:1382/2330 train_time:55279ms step_avg:40.00ms
step:1383/2330 train_time:55303ms step_avg:39.99ms
step:1384/2330 train_time:55359ms step_avg:40.00ms
step:1385/2330 train_time:55383ms step_avg:39.99ms
step:1386/2330 train_time:55439ms step_avg:40.00ms
step:1387/2330 train_time:55463ms step_avg:39.99ms
step:1388/2330 train_time:55519ms step_avg:40.00ms
step:1389/2330 train_time:55542ms step_avg:39.99ms
step:1390/2330 train_time:55598ms step_avg:40.00ms
step:1391/2330 train_time:55621ms step_avg:39.99ms
step:1392/2330 train_time:55677ms step_avg:40.00ms
step:1393/2330 train_time:55700ms step_avg:39.99ms
step:1394/2330 train_time:55756ms step_avg:40.00ms
step:1395/2330 train_time:55780ms step_avg:39.99ms
step:1396/2330 train_time:55837ms step_avg:40.00ms
step:1397/2330 train_time:55860ms step_avg:39.99ms
step:1398/2330 train_time:55916ms step_avg:40.00ms
step:1399/2330 train_time:55939ms step_avg:39.98ms
step:1400/2330 train_time:55995ms step_avg:40.00ms
step:1401/2330 train_time:56019ms step_avg:39.99ms
step:1402/2330 train_time:56076ms step_avg:40.00ms
step:1403/2330 train_time:56100ms step_avg:39.99ms
step:1404/2330 train_time:56157ms step_avg:40.00ms
step:1405/2330 train_time:56181ms step_avg:39.99ms
step:1406/2330 train_time:56237ms step_avg:40.00ms
step:1407/2330 train_time:56260ms step_avg:39.99ms
step:1408/2330 train_time:56317ms step_avg:40.00ms
step:1409/2330 train_time:56341ms step_avg:39.99ms
step:1410/2330 train_time:56398ms step_avg:40.00ms
step:1411/2330 train_time:56421ms step_avg:39.99ms
step:1412/2330 train_time:56478ms step_avg:40.00ms
step:1413/2330 train_time:56501ms step_avg:39.99ms
step:1414/2330 train_time:56557ms step_avg:40.00ms
step:1415/2330 train_time:56581ms step_avg:39.99ms
step:1416/2330 train_time:56637ms step_avg:40.00ms
step:1417/2330 train_time:56660ms step_avg:39.99ms
step:1418/2330 train_time:56717ms step_avg:40.00ms
step:1419/2330 train_time:56740ms step_avg:39.99ms
step:1420/2330 train_time:56796ms step_avg:40.00ms
step:1421/2330 train_time:56820ms step_avg:39.99ms
step:1422/2330 train_time:56876ms step_avg:40.00ms
step:1423/2330 train_time:56899ms step_avg:39.99ms
step:1424/2330 train_time:56956ms step_avg:40.00ms
step:1425/2330 train_time:56979ms step_avg:39.99ms
step:1426/2330 train_time:57036ms step_avg:40.00ms
step:1427/2330 train_time:57059ms step_avg:39.99ms
step:1428/2330 train_time:57117ms step_avg:40.00ms
step:1429/2330 train_time:57140ms step_avg:39.99ms
step:1430/2330 train_time:57197ms step_avg:40.00ms
step:1431/2330 train_time:57221ms step_avg:39.99ms
step:1432/2330 train_time:57277ms step_avg:40.00ms
step:1433/2330 train_time:57300ms step_avg:39.99ms
step:1434/2330 train_time:57357ms step_avg:40.00ms
step:1435/2330 train_time:57381ms step_avg:39.99ms
step:1436/2330 train_time:57438ms step_avg:40.00ms
step:1437/2330 train_time:57462ms step_avg:39.99ms
step:1438/2330 train_time:57518ms step_avg:40.00ms
step:1439/2330 train_time:57541ms step_avg:39.99ms
step:1440/2330 train_time:57597ms step_avg:40.00ms
step:1441/2330 train_time:57621ms step_avg:39.99ms
step:1442/2330 train_time:57677ms step_avg:40.00ms
step:1443/2330 train_time:57700ms step_avg:39.99ms
step:1444/2330 train_time:57756ms step_avg:40.00ms
step:1445/2330 train_time:57779ms step_avg:39.99ms
step:1446/2330 train_time:57836ms step_avg:40.00ms
step:1447/2330 train_time:57859ms step_avg:39.99ms
step:1448/2330 train_time:57916ms step_avg:40.00ms
step:1449/2330 train_time:57939ms step_avg:39.99ms
step:1450/2330 train_time:57996ms step_avg:40.00ms
step:1451/2330 train_time:58019ms step_avg:39.99ms
step:1452/2330 train_time:58076ms step_avg:40.00ms
step:1453/2330 train_time:58099ms step_avg:39.99ms
step:1454/2330 train_time:58156ms step_avg:40.00ms
step:1455/2330 train_time:58180ms step_avg:39.99ms
step:1456/2330 train_time:58237ms step_avg:40.00ms
step:1457/2330 train_time:58261ms step_avg:39.99ms
step:1458/2330 train_time:58317ms step_avg:40.00ms
step:1459/2330 train_time:58341ms step_avg:39.99ms
step:1460/2330 train_time:58398ms step_avg:40.00ms
step:1461/2330 train_time:58422ms step_avg:39.99ms
step:1462/2330 train_time:58478ms step_avg:40.00ms
step:1463/2330 train_time:58501ms step_avg:39.99ms
step:1464/2330 train_time:58557ms step_avg:40.00ms
step:1465/2330 train_time:58580ms step_avg:39.99ms
step:1466/2330 train_time:58637ms step_avg:40.00ms
step:1467/2330 train_time:58661ms step_avg:39.99ms
step:1468/2330 train_time:58717ms step_avg:40.00ms
step:1469/2330 train_time:58740ms step_avg:39.99ms
step:1470/2330 train_time:58796ms step_avg:40.00ms
step:1471/2330 train_time:58820ms step_avg:39.99ms
step:1472/2330 train_time:58876ms step_avg:40.00ms
step:1473/2330 train_time:58900ms step_avg:39.99ms
step:1474/2330 train_time:58957ms step_avg:40.00ms
step:1475/2330 train_time:58980ms step_avg:39.99ms
step:1476/2330 train_time:59036ms step_avg:40.00ms
step:1477/2330 train_time:59060ms step_avg:39.99ms
step:1478/2330 train_time:59116ms step_avg:40.00ms
step:1479/2330 train_time:59140ms step_avg:39.99ms
step:1480/2330 train_time:59197ms step_avg:40.00ms
step:1481/2330 train_time:59220ms step_avg:39.99ms
step:1482/2330 train_time:59277ms step_avg:40.00ms
step:1483/2330 train_time:59300ms step_avg:39.99ms
step:1484/2330 train_time:59358ms step_avg:40.00ms
step:1485/2330 train_time:59381ms step_avg:39.99ms
step:1486/2330 train_time:59438ms step_avg:40.00ms
step:1487/2330 train_time:59462ms step_avg:39.99ms
step:1488/2330 train_time:59518ms step_avg:40.00ms
step:1489/2330 train_time:59541ms step_avg:39.99ms
step:1490/2330 train_time:59597ms step_avg:40.00ms
step:1491/2330 train_time:59621ms step_avg:39.99ms
step:1492/2330 train_time:59677ms step_avg:40.00ms
step:1493/2330 train_time:59701ms step_avg:39.99ms
step:1494/2330 train_time:59756ms step_avg:40.00ms
step:1495/2330 train_time:59779ms step_avg:39.99ms
step:1496/2330 train_time:59836ms step_avg:40.00ms
step:1497/2330 train_time:59859ms step_avg:39.99ms
step:1498/2330 train_time:59916ms step_avg:40.00ms
step:1499/2330 train_time:59939ms step_avg:39.99ms
step:1500/2330 train_time:59996ms step_avg:40.00ms
step:1500/2330 val_loss:5.6062 train_time:60094ms step_avg:40.06ms
step:1501/2330 train_time:60106ms step_avg:40.04ms
step:1502/2330 train_time:60117ms step_avg:40.02ms
step:1503/2330 train_time:60127ms step_avg:40.00ms
step:1504/2330 train_time:60158ms step_avg:40.00ms
step:1505/2330 train_time:60179ms step_avg:39.99ms
step:1506/2330 train_time:60236ms step_avg:40.00ms
step:1507/2330 train_time:60259ms step_avg:39.99ms
step:1508/2330 train_time:60315ms step_avg:40.00ms
step:1509/2330 train_time:60337ms step_avg:39.98ms
step:1510/2330 train_time:60394ms step_avg:40.00ms
step:1511/2330 train_time:60420ms step_avg:39.99ms
step:1512/2330 train_time:60482ms step_avg:40.00ms
step:1513/2330 train_time:60506ms step_avg:39.99ms
step:1514/2330 train_time:60564ms step_avg:40.00ms
step:1515/2330 train_time:60588ms step_avg:39.99ms
step:1516/2330 train_time:60645ms step_avg:40.00ms
step:1517/2330 train_time:60668ms step_avg:39.99ms
step:1518/2330 train_time:60725ms step_avg:40.00ms
step:1519/2330 train_time:60747ms step_avg:39.99ms
step:1520/2330 train_time:60804ms step_avg:40.00ms
step:1521/2330 train_time:60827ms step_avg:39.99ms
step:1522/2330 train_time:60884ms step_avg:40.00ms
step:1523/2330 train_time:60907ms step_avg:39.99ms
step:1524/2330 train_time:60963ms step_avg:40.00ms
step:1525/2330 train_time:60987ms step_avg:39.99ms
step:1526/2330 train_time:61046ms step_avg:40.00ms
step:1527/2330 train_time:61070ms step_avg:39.99ms
step:1528/2330 train_time:61128ms step_avg:40.01ms
step:1529/2330 train_time:61152ms step_avg:40.00ms
step:1530/2330 train_time:61209ms step_avg:40.01ms
step:1531/2330 train_time:61231ms step_avg:39.99ms
step:1532/2330 train_time:61288ms step_avg:40.01ms
step:1533/2330 train_time:61311ms step_avg:39.99ms
step:1534/2330 train_time:61368ms step_avg:40.01ms
step:1535/2330 train_time:61393ms step_avg:40.00ms
step:1536/2330 train_time:61450ms step_avg:40.01ms
step:1537/2330 train_time:61475ms step_avg:40.00ms
step:1538/2330 train_time:61531ms step_avg:40.01ms
step:1539/2330 train_time:61555ms step_avg:40.00ms
step:1540/2330 train_time:61611ms step_avg:40.01ms
step:1541/2330 train_time:61634ms step_avg:40.00ms
step:1542/2330 train_time:61691ms step_avg:40.01ms
step:1543/2330 train_time:61715ms step_avg:40.00ms
step:1544/2330 train_time:61770ms step_avg:40.01ms
step:1545/2330 train_time:61793ms step_avg:40.00ms
step:1546/2330 train_time:61849ms step_avg:40.01ms
step:1547/2330 train_time:61872ms step_avg:39.99ms
step:1548/2330 train_time:61928ms step_avg:40.01ms
step:1549/2330 train_time:61952ms step_avg:39.99ms
step:1550/2330 train_time:62008ms step_avg:40.01ms
step:1551/2330 train_time:62033ms step_avg:40.00ms
step:1552/2330 train_time:62090ms step_avg:40.01ms
step:1553/2330 train_time:62113ms step_avg:40.00ms
step:1554/2330 train_time:62170ms step_avg:40.01ms
step:1555/2330 train_time:62194ms step_avg:40.00ms
step:1556/2330 train_time:62250ms step_avg:40.01ms
step:1557/2330 train_time:62273ms step_avg:40.00ms
step:1558/2330 train_time:62330ms step_avg:40.01ms
step:1559/2330 train_time:62354ms step_avg:40.00ms
step:1560/2330 train_time:62411ms step_avg:40.01ms
step:1561/2330 train_time:62436ms step_avg:40.00ms
step:1562/2330 train_time:62492ms step_avg:40.01ms
step:1563/2330 train_time:62516ms step_avg:40.00ms
step:1564/2330 train_time:62573ms step_avg:40.01ms
step:1565/2330 train_time:62596ms step_avg:40.00ms
step:1566/2330 train_time:62652ms step_avg:40.01ms
step:1567/2330 train_time:62675ms step_avg:40.00ms
step:1568/2330 train_time:62731ms step_avg:40.01ms
step:1569/2330 train_time:62754ms step_avg:40.00ms
step:1570/2330 train_time:62811ms step_avg:40.01ms
step:1571/2330 train_time:62835ms step_avg:40.00ms
step:1572/2330 train_time:62891ms step_avg:40.01ms
step:1573/2330 train_time:62915ms step_avg:40.00ms
step:1574/2330 train_time:62971ms step_avg:40.01ms
step:1575/2330 train_time:62995ms step_avg:40.00ms
step:1576/2330 train_time:63052ms step_avg:40.01ms
step:1577/2330 train_time:63075ms step_avg:40.00ms
step:1578/2330 train_time:63132ms step_avg:40.01ms
step:1579/2330 train_time:63155ms step_avg:40.00ms
step:1580/2330 train_time:63211ms step_avg:40.01ms
step:1581/2330 train_time:63234ms step_avg:40.00ms
step:1582/2330 train_time:63290ms step_avg:40.01ms
step:1583/2330 train_time:63312ms step_avg:40.00ms
step:1584/2330 train_time:63370ms step_avg:40.01ms
step:1585/2330 train_time:63393ms step_avg:40.00ms
step:1586/2330 train_time:63450ms step_avg:40.01ms
step:1587/2330 train_time:63475ms step_avg:40.00ms
step:1588/2330 train_time:63531ms step_avg:40.01ms
step:1589/2330 train_time:63554ms step_avg:40.00ms
step:1590/2330 train_time:63610ms step_avg:40.01ms
step:1591/2330 train_time:63634ms step_avg:40.00ms
step:1592/2330 train_time:63690ms step_avg:40.01ms
step:1593/2330 train_time:63713ms step_avg:40.00ms
step:1594/2330 train_time:63770ms step_avg:40.01ms
step:1595/2330 train_time:63793ms step_avg:40.00ms
step:1596/2330 train_time:63850ms step_avg:40.01ms
step:1597/2330 train_time:63873ms step_avg:40.00ms
step:1598/2330 train_time:63929ms step_avg:40.01ms
step:1599/2330 train_time:63952ms step_avg:40.00ms
step:1600/2330 train_time:64009ms step_avg:40.01ms
step:1601/2330 train_time:64033ms step_avg:40.00ms
step:1602/2330 train_time:64089ms step_avg:40.01ms
step:1603/2330 train_time:64112ms step_avg:39.99ms
step:1604/2330 train_time:64168ms step_avg:40.01ms
step:1605/2330 train_time:64192ms step_avg:39.99ms
step:1606/2330 train_time:64249ms step_avg:40.01ms
step:1607/2330 train_time:64273ms step_avg:40.00ms
step:1608/2330 train_time:64329ms step_avg:40.01ms
step:1609/2330 train_time:64353ms step_avg:40.00ms
step:1610/2330 train_time:64410ms step_avg:40.01ms
step:1611/2330 train_time:64434ms step_avg:40.00ms
step:1612/2330 train_time:64490ms step_avg:40.01ms
step:1613/2330 train_time:64513ms step_avg:40.00ms
step:1614/2330 train_time:64569ms step_avg:40.01ms
step:1615/2330 train_time:64593ms step_avg:40.00ms
step:1616/2330 train_time:64650ms step_avg:40.01ms
step:1617/2330 train_time:64673ms step_avg:40.00ms
step:1618/2330 train_time:64730ms step_avg:40.01ms
step:1619/2330 train_time:64754ms step_avg:40.00ms
step:1620/2330 train_time:64810ms step_avg:40.01ms
step:1621/2330 train_time:64834ms step_avg:40.00ms
step:1622/2330 train_time:64890ms step_avg:40.01ms
step:1623/2330 train_time:64913ms step_avg:40.00ms
step:1624/2330 train_time:64969ms step_avg:40.01ms
step:1625/2330 train_time:64993ms step_avg:40.00ms
step:1626/2330 train_time:65050ms step_avg:40.01ms
step:1627/2330 train_time:65073ms step_avg:40.00ms
step:1628/2330 train_time:65130ms step_avg:40.01ms
step:1629/2330 train_time:65153ms step_avg:40.00ms
step:1630/2330 train_time:65209ms step_avg:40.01ms
step:1631/2330 train_time:65233ms step_avg:40.00ms
step:1632/2330 train_time:65290ms step_avg:40.01ms
step:1633/2330 train_time:65314ms step_avg:40.00ms
step:1634/2330 train_time:65371ms step_avg:40.01ms
step:1635/2330 train_time:65394ms step_avg:40.00ms
step:1636/2330 train_time:65451ms step_avg:40.01ms
step:1637/2330 train_time:65475ms step_avg:40.00ms
step:1638/2330 train_time:65531ms step_avg:40.01ms
step:1639/2330 train_time:65555ms step_avg:40.00ms
step:1640/2330 train_time:65611ms step_avg:40.01ms
step:1641/2330 train_time:65635ms step_avg:40.00ms
step:1642/2330 train_time:65692ms step_avg:40.01ms
step:1643/2330 train_time:65716ms step_avg:40.00ms
step:1644/2330 train_time:65772ms step_avg:40.01ms
step:1645/2330 train_time:65795ms step_avg:40.00ms
step:1646/2330 train_time:65852ms step_avg:40.01ms
step:1647/2330 train_time:65875ms step_avg:40.00ms
step:1648/2330 train_time:65931ms step_avg:40.01ms
step:1649/2330 train_time:65954ms step_avg:40.00ms
step:1650/2330 train_time:66011ms step_avg:40.01ms
step:1651/2330 train_time:66034ms step_avg:40.00ms
step:1652/2330 train_time:66091ms step_avg:40.01ms
step:1653/2330 train_time:66114ms step_avg:40.00ms
step:1654/2330 train_time:66170ms step_avg:40.01ms
step:1655/2330 train_time:66194ms step_avg:40.00ms
step:1656/2330 train_time:66250ms step_avg:40.01ms
step:1657/2330 train_time:66274ms step_avg:40.00ms
step:1658/2330 train_time:66331ms step_avg:40.01ms
step:1659/2330 train_time:66354ms step_avg:40.00ms
step:1660/2330 train_time:66411ms step_avg:40.01ms
step:1661/2330 train_time:66435ms step_avg:40.00ms
step:1662/2330 train_time:66492ms step_avg:40.01ms
step:1663/2330 train_time:66516ms step_avg:40.00ms
step:1664/2330 train_time:66572ms step_avg:40.01ms
step:1665/2330 train_time:66597ms step_avg:40.00ms
step:1666/2330 train_time:66653ms step_avg:40.01ms
step:1667/2330 train_time:66676ms step_avg:40.00ms
step:1668/2330 train_time:66734ms step_avg:40.01ms
step:1669/2330 train_time:66756ms step_avg:40.00ms
step:1670/2330 train_time:66813ms step_avg:40.01ms
step:1671/2330 train_time:66836ms step_avg:40.00ms
step:1672/2330 train_time:66892ms step_avg:40.01ms
step:1673/2330 train_time:66915ms step_avg:40.00ms
step:1674/2330 train_time:66972ms step_avg:40.01ms
step:1675/2330 train_time:66995ms step_avg:40.00ms
step:1676/2330 train_time:67053ms step_avg:40.01ms
step:1677/2330 train_time:67076ms step_avg:40.00ms
step:1678/2330 train_time:67133ms step_avg:40.01ms
step:1679/2330 train_time:67156ms step_avg:40.00ms
step:1680/2330 train_time:67213ms step_avg:40.01ms
step:1681/2330 train_time:67235ms step_avg:40.00ms
step:1682/2330 train_time:67293ms step_avg:40.01ms
step:1683/2330 train_time:67316ms step_avg:40.00ms
step:1684/2330 train_time:67373ms step_avg:40.01ms
step:1685/2330 train_time:67396ms step_avg:40.00ms
step:1686/2330 train_time:67453ms step_avg:40.01ms
step:1687/2330 train_time:67475ms step_avg:40.00ms
step:1688/2330 train_time:67532ms step_avg:40.01ms
step:1689/2330 train_time:67555ms step_avg:40.00ms
step:1690/2330 train_time:67611ms step_avg:40.01ms
step:1691/2330 train_time:67635ms step_avg:40.00ms
step:1692/2330 train_time:67692ms step_avg:40.01ms
step:1693/2330 train_time:67716ms step_avg:40.00ms
step:1694/2330 train_time:67773ms step_avg:40.01ms
step:1695/2330 train_time:67797ms step_avg:40.00ms
step:1696/2330 train_time:67853ms step_avg:40.01ms
step:1697/2330 train_time:67875ms step_avg:40.00ms
step:1698/2330 train_time:67932ms step_avg:40.01ms
step:1699/2330 train_time:67955ms step_avg:40.00ms
step:1700/2330 train_time:68011ms step_avg:40.01ms
step:1701/2330 train_time:68034ms step_avg:40.00ms
step:1702/2330 train_time:68091ms step_avg:40.01ms
step:1703/2330 train_time:68114ms step_avg:40.00ms
step:1704/2330 train_time:68171ms step_avg:40.01ms
step:1705/2330 train_time:68194ms step_avg:40.00ms
step:1706/2330 train_time:68250ms step_avg:40.01ms
step:1707/2330 train_time:68273ms step_avg:40.00ms
step:1708/2330 train_time:68330ms step_avg:40.01ms
step:1709/2330 train_time:68353ms step_avg:40.00ms
step:1710/2330 train_time:68409ms step_avg:40.01ms
step:1711/2330 train_time:68433ms step_avg:40.00ms
step:1712/2330 train_time:68489ms step_avg:40.01ms
step:1713/2330 train_time:68512ms step_avg:40.00ms
step:1714/2330 train_time:68569ms step_avg:40.01ms
step:1715/2330 train_time:68592ms step_avg:40.00ms
step:1716/2330 train_time:68649ms step_avg:40.01ms
step:1717/2330 train_time:68672ms step_avg:40.00ms
step:1718/2330 train_time:68728ms step_avg:40.00ms
step:1719/2330 train_time:68752ms step_avg:40.00ms
step:1720/2330 train_time:68809ms step_avg:40.01ms
step:1721/2330 train_time:68832ms step_avg:40.00ms
step:1722/2330 train_time:68889ms step_avg:40.01ms
step:1723/2330 train_time:68912ms step_avg:40.00ms
step:1724/2330 train_time:68969ms step_avg:40.01ms
step:1725/2330 train_time:68992ms step_avg:40.00ms
step:1726/2330 train_time:69050ms step_avg:40.01ms
step:1727/2330 train_time:69073ms step_avg:40.00ms
step:1728/2330 train_time:69129ms step_avg:40.01ms
step:1729/2330 train_time:69152ms step_avg:40.00ms
step:1730/2330 train_time:69209ms step_avg:40.01ms
step:1731/2330 train_time:69233ms step_avg:40.00ms
step:1732/2330 train_time:69290ms step_avg:40.01ms
step:1733/2330 train_time:69313ms step_avg:40.00ms
step:1734/2330 train_time:69369ms step_avg:40.01ms
step:1735/2330 train_time:69392ms step_avg:40.00ms
step:1736/2330 train_time:69449ms step_avg:40.01ms
step:1737/2330 train_time:69472ms step_avg:40.00ms
step:1738/2330 train_time:69529ms step_avg:40.00ms
step:1739/2330 train_time:69552ms step_avg:40.00ms
step:1740/2330 train_time:69609ms step_avg:40.01ms
step:1741/2330 train_time:69633ms step_avg:40.00ms
step:1742/2330 train_time:69690ms step_avg:40.01ms
step:1743/2330 train_time:69713ms step_avg:40.00ms
step:1744/2330 train_time:69770ms step_avg:40.01ms
step:1745/2330 train_time:69793ms step_avg:40.00ms
step:1746/2330 train_time:69849ms step_avg:40.01ms
step:1747/2330 train_time:69872ms step_avg:40.00ms
step:1748/2330 train_time:69929ms step_avg:40.01ms
step:1749/2330 train_time:69952ms step_avg:40.00ms
step:1750/2330 train_time:70009ms step_avg:40.01ms
step:1750/2330 val_loss:5.5571 train_time:70108ms step_avg:40.06ms
step:1751/2330 train_time:70121ms step_avg:40.05ms
step:1752/2330 train_time:70132ms step_avg:40.03ms
step:1753/2330 train_time:70143ms step_avg:40.01ms
step:1754/2330 train_time:70172ms step_avg:40.01ms
step:1755/2330 train_time:70193ms step_avg:40.00ms
step:1756/2330 train_time:70249ms step_avg:40.01ms
step:1757/2330 train_time:70272ms step_avg:40.00ms
step:1758/2330 train_time:70328ms step_avg:40.00ms
step:1759/2330 train_time:70349ms step_avg:39.99ms
step:1760/2330 train_time:70406ms step_avg:40.00ms
step:1761/2330 train_time:70433ms step_avg:40.00ms
step:1762/2330 train_time:70494ms step_avg:40.01ms
step:1763/2330 train_time:70518ms step_avg:40.00ms
step:1764/2330 train_time:70578ms step_avg:40.01ms
step:1765/2330 train_time:70601ms step_avg:40.00ms
step:1766/2330 train_time:70658ms step_avg:40.01ms
step:1767/2330 train_time:70682ms step_avg:40.00ms
step:1768/2330 train_time:70739ms step_avg:40.01ms
step:1769/2330 train_time:70762ms step_avg:40.00ms
step:1770/2330 train_time:70818ms step_avg:40.01ms
step:1771/2330 train_time:70842ms step_avg:40.00ms
step:1772/2330 train_time:70898ms step_avg:40.01ms
step:1773/2330 train_time:70922ms step_avg:40.00ms
step:1774/2330 train_time:70979ms step_avg:40.01ms
step:1775/2330 train_time:71002ms step_avg:40.00ms
step:1776/2330 train_time:71061ms step_avg:40.01ms
step:1777/2330 train_time:71086ms step_avg:40.00ms
step:1778/2330 train_time:71143ms step_avg:40.01ms
step:1779/2330 train_time:71167ms step_avg:40.00ms
step:1780/2330 train_time:71223ms step_avg:40.01ms
step:1781/2330 train_time:71246ms step_avg:40.00ms
step:1782/2330 train_time:71303ms step_avg:40.01ms
step:1783/2330 train_time:71326ms step_avg:40.00ms
step:1784/2330 train_time:71384ms step_avg:40.01ms
step:1785/2330 train_time:71408ms step_avg:40.00ms
step:1786/2330 train_time:71466ms step_avg:40.01ms
step:1787/2330 train_time:71490ms step_avg:40.01ms
step:1788/2330 train_time:71547ms step_avg:40.02ms
step:1789/2330 train_time:71571ms step_avg:40.01ms
step:1790/2330 train_time:71627ms step_avg:40.02ms
step:1791/2330 train_time:71651ms step_avg:40.01ms
step:1792/2330 train_time:71707ms step_avg:40.01ms
step:1793/2330 train_time:71729ms step_avg:40.01ms
step:1794/2330 train_time:71785ms step_avg:40.01ms
step:1795/2330 train_time:71808ms step_avg:40.00ms
step:1796/2330 train_time:71865ms step_avg:40.01ms
step:1797/2330 train_time:71888ms step_avg:40.00ms
step:1798/2330 train_time:71944ms step_avg:40.01ms
step:1799/2330 train_time:71968ms step_avg:40.00ms
step:1800/2330 train_time:72025ms step_avg:40.01ms
step:1801/2330 train_time:72049ms step_avg:40.00ms
step:1802/2330 train_time:72106ms step_avg:40.01ms
step:1803/2330 train_time:72129ms step_avg:40.00ms
step:1804/2330 train_time:72185ms step_avg:40.01ms
step:1805/2330 train_time:72209ms step_avg:40.01ms
step:1806/2330 train_time:72265ms step_avg:40.01ms
step:1807/2330 train_time:72288ms step_avg:40.00ms
step:1808/2330 train_time:72345ms step_avg:40.01ms
step:1809/2330 train_time:72369ms step_avg:40.00ms
step:1810/2330 train_time:72426ms step_avg:40.01ms
step:1811/2330 train_time:72449ms step_avg:40.01ms
step:1812/2330 train_time:72507ms step_avg:40.01ms
step:1813/2330 train_time:72530ms step_avg:40.01ms
step:1814/2330 train_time:72587ms step_avg:40.01ms
step:1815/2330 train_time:72610ms step_avg:40.01ms
step:1816/2330 train_time:72666ms step_avg:40.01ms
step:1817/2330 train_time:72689ms step_avg:40.00ms
step:1818/2330 train_time:72745ms step_avg:40.01ms
step:1819/2330 train_time:72769ms step_avg:40.00ms
step:1820/2330 train_time:72825ms step_avg:40.01ms
step:1821/2330 train_time:72848ms step_avg:40.00ms
step:1822/2330 train_time:72904ms step_avg:40.01ms
step:1823/2330 train_time:72928ms step_avg:40.00ms
step:1824/2330 train_time:72984ms step_avg:40.01ms
step:1825/2330 train_time:73008ms step_avg:40.00ms
step:1826/2330 train_time:73064ms step_avg:40.01ms
step:1827/2330 train_time:73088ms step_avg:40.00ms
step:1828/2330 train_time:73144ms step_avg:40.01ms
step:1829/2330 train_time:73167ms step_avg:40.00ms
step:1830/2330 train_time:73224ms step_avg:40.01ms
step:1831/2330 train_time:73248ms step_avg:40.00ms
step:1832/2330 train_time:73304ms step_avg:40.01ms
step:1833/2330 train_time:73328ms step_avg:40.00ms
step:1834/2330 train_time:73385ms step_avg:40.01ms
step:1835/2330 train_time:73408ms step_avg:40.00ms
step:1836/2330 train_time:73465ms step_avg:40.01ms
step:1837/2330 train_time:73488ms step_avg:40.00ms
step:1838/2330 train_time:73545ms step_avg:40.01ms
step:1839/2330 train_time:73569ms step_avg:40.00ms
step:1840/2330 train_time:73625ms step_avg:40.01ms
step:1841/2330 train_time:73649ms step_avg:40.00ms
step:1842/2330 train_time:73705ms step_avg:40.01ms
step:1843/2330 train_time:73728ms step_avg:40.00ms
step:1844/2330 train_time:73784ms step_avg:40.01ms
step:1845/2330 train_time:73808ms step_avg:40.00ms
step:1846/2330 train_time:73865ms step_avg:40.01ms
step:1847/2330 train_time:73888ms step_avg:40.00ms
step:1848/2330 train_time:73945ms step_avg:40.01ms
step:1849/2330 train_time:73968ms step_avg:40.00ms
step:1850/2330 train_time:74024ms step_avg:40.01ms
step:1851/2330 train_time:74047ms step_avg:40.00ms
step:1852/2330 train_time:74104ms step_avg:40.01ms
step:1853/2330 train_time:74127ms step_avg:40.00ms
step:1854/2330 train_time:74185ms step_avg:40.01ms
step:1855/2330 train_time:74208ms step_avg:40.00ms
step:1856/2330 train_time:74265ms step_avg:40.01ms
step:1857/2330 train_time:74289ms step_avg:40.00ms
step:1858/2330 train_time:74345ms step_avg:40.01ms
step:1859/2330 train_time:74369ms step_avg:40.00ms
step:1860/2330 train_time:74425ms step_avg:40.01ms
step:1861/2330 train_time:74448ms step_avg:40.00ms
step:1862/2330 train_time:74506ms step_avg:40.01ms
step:1863/2330 train_time:74530ms step_avg:40.01ms
step:1864/2330 train_time:74586ms step_avg:40.01ms
step:1865/2330 train_time:74609ms step_avg:40.00ms
step:1866/2330 train_time:74666ms step_avg:40.01ms
step:1867/2330 train_time:74689ms step_avg:40.00ms
step:1868/2330 train_time:74745ms step_avg:40.01ms
step:1869/2330 train_time:74769ms step_avg:40.00ms
step:1870/2330 train_time:74825ms step_avg:40.01ms
step:1871/2330 train_time:74849ms step_avg:40.00ms
step:1872/2330 train_time:74905ms step_avg:40.01ms
step:1873/2330 train_time:74928ms step_avg:40.00ms
step:1874/2330 train_time:74985ms step_avg:40.01ms
step:1875/2330 train_time:75008ms step_avg:40.00ms
step:1876/2330 train_time:75064ms step_avg:40.01ms
step:1877/2330 train_time:75088ms step_avg:40.00ms
step:1878/2330 train_time:75145ms step_avg:40.01ms
step:1879/2330 train_time:75168ms step_avg:40.00ms
step:1880/2330 train_time:75225ms step_avg:40.01ms
step:1881/2330 train_time:75248ms step_avg:40.00ms
step:1882/2330 train_time:75305ms step_avg:40.01ms
step:1883/2330 train_time:75328ms step_avg:40.00ms
step:1884/2330 train_time:75385ms step_avg:40.01ms
step:1885/2330 train_time:75409ms step_avg:40.00ms
step:1886/2330 train_time:75466ms step_avg:40.01ms
step:1887/2330 train_time:75489ms step_avg:40.00ms
step:1888/2330 train_time:75546ms step_avg:40.01ms
step:1889/2330 train_time:75569ms step_avg:40.00ms
step:1890/2330 train_time:75625ms step_avg:40.01ms
step:1891/2330 train_time:75649ms step_avg:40.00ms
step:1892/2330 train_time:75705ms step_avg:40.01ms
step:1893/2330 train_time:75728ms step_avg:40.00ms
step:1894/2330 train_time:75785ms step_avg:40.01ms
step:1895/2330 train_time:75808ms step_avg:40.00ms
step:1896/2330 train_time:75865ms step_avg:40.01ms
step:1897/2330 train_time:75888ms step_avg:40.00ms
step:1898/2330 train_time:75944ms step_avg:40.01ms
step:1899/2330 train_time:75968ms step_avg:40.00ms
step:1900/2330 train_time:76024ms step_avg:40.01ms
step:1901/2330 train_time:76047ms step_avg:40.00ms
step:1902/2330 train_time:76105ms step_avg:40.01ms
step:1903/2330 train_time:76128ms step_avg:40.00ms
step:1904/2330 train_time:76185ms step_avg:40.01ms
step:1905/2330 train_time:76208ms step_avg:40.00ms
step:1906/2330 train_time:76264ms step_avg:40.01ms
step:1907/2330 train_time:76288ms step_avg:40.00ms
step:1908/2330 train_time:76345ms step_avg:40.01ms
step:1909/2330 train_time:76369ms step_avg:40.00ms
step:1910/2330 train_time:76426ms step_avg:40.01ms
step:1911/2330 train_time:76450ms step_avg:40.01ms
step:1912/2330 train_time:76506ms step_avg:40.01ms
step:1913/2330 train_time:76530ms step_avg:40.01ms
step:1914/2330 train_time:76587ms step_avg:40.01ms
step:1915/2330 train_time:76610ms step_avg:40.01ms
step:1916/2330 train_time:76667ms step_avg:40.01ms
step:1917/2330 train_time:76690ms step_avg:40.01ms
step:1918/2330 train_time:76747ms step_avg:40.01ms
step:1919/2330 train_time:76771ms step_avg:40.01ms
step:1920/2330 train_time:76828ms step_avg:40.01ms
step:1921/2330 train_time:76852ms step_avg:40.01ms
step:1922/2330 train_time:76908ms step_avg:40.01ms
step:1923/2330 train_time:76932ms step_avg:40.01ms
step:1924/2330 train_time:76990ms step_avg:40.02ms
step:1925/2330 train_time:77013ms step_avg:40.01ms
step:1926/2330 train_time:77071ms step_avg:40.02ms
step:1927/2330 train_time:77094ms step_avg:40.01ms
step:1928/2330 train_time:77152ms step_avg:40.02ms
step:1929/2330 train_time:77175ms step_avg:40.01ms
step:1930/2330 train_time:77234ms step_avg:40.02ms
step:1931/2330 train_time:77257ms step_avg:40.01ms
step:1932/2330 train_time:77315ms step_avg:40.02ms
step:1933/2330 train_time:77338ms step_avg:40.01ms
step:1934/2330 train_time:77397ms step_avg:40.02ms
step:1935/2330 train_time:77420ms step_avg:40.01ms
step:1936/2330 train_time:77478ms step_avg:40.02ms
step:1937/2330 train_time:77501ms step_avg:40.01ms
step:1938/2330 train_time:77559ms step_avg:40.02ms
step:1939/2330 train_time:77583ms step_avg:40.01ms
step:1940/2330 train_time:77640ms step_avg:40.02ms
step:1941/2330 train_time:77663ms step_avg:40.01ms
step:1942/2330 train_time:77720ms step_avg:40.02ms
step:1943/2330 train_time:77745ms step_avg:40.01ms
step:1944/2330 train_time:77802ms step_avg:40.02ms
step:1945/2330 train_time:77826ms step_avg:40.01ms
step:1946/2330 train_time:77883ms step_avg:40.02ms
step:1947/2330 train_time:77907ms step_avg:40.01ms
step:1948/2330 train_time:77964ms step_avg:40.02ms
step:1949/2330 train_time:77988ms step_avg:40.01ms
step:1950/2330 train_time:78044ms step_avg:40.02ms
step:1951/2330 train_time:78068ms step_avg:40.01ms
step:1952/2330 train_time:78124ms step_avg:40.02ms
step:1953/2330 train_time:78148ms step_avg:40.01ms
step:1954/2330 train_time:78205ms step_avg:40.02ms
step:1955/2330 train_time:78229ms step_avg:40.01ms
step:1956/2330 train_time:78285ms step_avg:40.02ms
step:1957/2330 train_time:78310ms step_avg:40.02ms
step:1958/2330 train_time:78367ms step_avg:40.02ms
step:1959/2330 train_time:78390ms step_avg:40.02ms
step:1960/2330 train_time:78446ms step_avg:40.02ms
step:1961/2330 train_time:78469ms step_avg:40.01ms
step:1962/2330 train_time:78525ms step_avg:40.02ms
step:1963/2330 train_time:78548ms step_avg:40.01ms
step:1964/2330 train_time:78605ms step_avg:40.02ms
step:1965/2330 train_time:78628ms step_avg:40.01ms
step:1966/2330 train_time:78685ms step_avg:40.02ms
step:1967/2330 train_time:78708ms step_avg:40.01ms
step:1968/2330 train_time:78765ms step_avg:40.02ms
step:1969/2330 train_time:78789ms step_avg:40.01ms
step:1970/2330 train_time:78845ms step_avg:40.02ms
step:1971/2330 train_time:78869ms step_avg:40.01ms
step:1972/2330 train_time:78927ms step_avg:40.02ms
step:1973/2330 train_time:78951ms step_avg:40.02ms
step:1974/2330 train_time:79007ms step_avg:40.02ms
step:1975/2330 train_time:79031ms step_avg:40.02ms
step:1976/2330 train_time:79089ms step_avg:40.02ms
step:1977/2330 train_time:79112ms step_avg:40.02ms
step:1978/2330 train_time:79169ms step_avg:40.02ms
step:1979/2330 train_time:79191ms step_avg:40.02ms
step:1980/2330 train_time:79249ms step_avg:40.02ms
step:1981/2330 train_time:79272ms step_avg:40.02ms
step:1982/2330 train_time:79330ms step_avg:40.03ms
step:1983/2330 train_time:79353ms step_avg:40.02ms
step:1984/2330 train_time:79409ms step_avg:40.02ms
step:1985/2330 train_time:79432ms step_avg:40.02ms
step:1986/2330 train_time:79489ms step_avg:40.02ms
step:1987/2330 train_time:79512ms step_avg:40.02ms
step:1988/2330 train_time:79569ms step_avg:40.02ms
step:1989/2330 train_time:79591ms step_avg:40.02ms
step:1990/2330 train_time:79649ms step_avg:40.02ms
step:1991/2330 train_time:79673ms step_avg:40.02ms
step:1992/2330 train_time:79731ms step_avg:40.03ms
step:1993/2330 train_time:79754ms step_avg:40.02ms
step:1994/2330 train_time:79812ms step_avg:40.03ms
step:1995/2330 train_time:79835ms step_avg:40.02ms
step:1996/2330 train_time:79894ms step_avg:40.03ms
step:1997/2330 train_time:79916ms step_avg:40.02ms
step:1998/2330 train_time:79975ms step_avg:40.03ms
step:1999/2330 train_time:79998ms step_avg:40.02ms
step:2000/2330 train_time:80057ms step_avg:40.03ms
step:2000/2330 val_loss:5.4785 train_time:80157ms step_avg:40.08ms
step:2001/2330 train_time:80169ms step_avg:40.06ms
step:2002/2330 train_time:80181ms step_avg:40.05ms
step:2003/2330 train_time:80191ms step_avg:40.04ms
step:2004/2330 train_time:80220ms step_avg:40.03ms
step:2005/2330 train_time:80242ms step_avg:40.02ms
step:2006/2330 train_time:80299ms step_avg:40.03ms
step:2007/2330 train_time:80321ms step_avg:40.02ms
step:2008/2330 train_time:80377ms step_avg:40.03ms
step:2009/2330 train_time:80399ms step_avg:40.02ms
step:2010/2330 train_time:80456ms step_avg:40.03ms
step:2011/2330 train_time:80482ms step_avg:40.02ms
step:2012/2330 train_time:80544ms step_avg:40.03ms
step:2013/2330 train_time:80569ms step_avg:40.02ms
step:2014/2330 train_time:80628ms step_avg:40.03ms
step:2015/2330 train_time:80651ms step_avg:40.03ms
step:2016/2330 train_time:80708ms step_avg:40.03ms
step:2017/2330 train_time:80731ms step_avg:40.03ms
step:2018/2330 train_time:80788ms step_avg:40.03ms
step:2019/2330 train_time:80810ms step_avg:40.02ms
step:2020/2330 train_time:80867ms step_avg:40.03ms
step:2021/2330 train_time:80890ms step_avg:40.02ms
step:2022/2330 train_time:80947ms step_avg:40.03ms
step:2023/2330 train_time:80970ms step_avg:40.02ms
step:2024/2330 train_time:81027ms step_avg:40.03ms
step:2025/2330 train_time:81052ms step_avg:40.03ms
step:2026/2330 train_time:81111ms step_avg:40.04ms
step:2027/2330 train_time:81136ms step_avg:40.03ms
step:2028/2330 train_time:81193ms step_avg:40.04ms
step:2029/2330 train_time:81216ms step_avg:40.03ms
step:2030/2330 train_time:81273ms step_avg:40.04ms
step:2031/2330 train_time:81296ms step_avg:40.03ms
step:2032/2330 train_time:81352ms step_avg:40.04ms
step:2033/2330 train_time:81375ms step_avg:40.03ms
step:2034/2330 train_time:81433ms step_avg:40.04ms
step:2035/2330 train_time:81458ms step_avg:40.03ms
step:2036/2330 train_time:81516ms step_avg:40.04ms
step:2037/2330 train_time:81540ms step_avg:40.03ms
step:2038/2330 train_time:81596ms step_avg:40.04ms
step:2039/2330 train_time:81620ms step_avg:40.03ms
step:2040/2330 train_time:81676ms step_avg:40.04ms
step:2041/2330 train_time:81700ms step_avg:40.03ms
step:2042/2330 train_time:81756ms step_avg:40.04ms
step:2043/2330 train_time:81780ms step_avg:40.03ms
step:2044/2330 train_time:81836ms step_avg:40.04ms
step:2045/2330 train_time:81860ms step_avg:40.03ms
step:2046/2330 train_time:81915ms step_avg:40.04ms
step:2047/2330 train_time:81938ms step_avg:40.03ms
step:2048/2330 train_time:81994ms step_avg:40.04ms
step:2049/2330 train_time:82018ms step_avg:40.03ms
step:2050/2330 train_time:82075ms step_avg:40.04ms
step:2051/2330 train_time:82099ms step_avg:40.03ms
step:2052/2330 train_time:82156ms step_avg:40.04ms
step:2053/2330 train_time:82179ms step_avg:40.03ms
step:2054/2330 train_time:82235ms step_avg:40.04ms
step:2055/2330 train_time:82258ms step_avg:40.03ms
step:2056/2330 train_time:82315ms step_avg:40.04ms
step:2057/2330 train_time:82338ms step_avg:40.03ms
step:2058/2330 train_time:82394ms step_avg:40.04ms
step:2059/2330 train_time:82418ms step_avg:40.03ms
step:2060/2330 train_time:82475ms step_avg:40.04ms
step:2061/2330 train_time:82498ms step_avg:40.03ms
step:2062/2330 train_time:82556ms step_avg:40.04ms
step:2063/2330 train_time:82580ms step_avg:40.03ms
step:2064/2330 train_time:82637ms step_avg:40.04ms
step:2065/2330 train_time:82661ms step_avg:40.03ms
step:2066/2330 train_time:82717ms step_avg:40.04ms
step:2067/2330 train_time:82740ms step_avg:40.03ms
step:2068/2330 train_time:82797ms step_avg:40.04ms
step:2069/2330 train_time:82820ms step_avg:40.03ms
step:2070/2330 train_time:82875ms step_avg:40.04ms
step:2071/2330 train_time:82899ms step_avg:40.03ms
step:2072/2330 train_time:82955ms step_avg:40.04ms
step:2073/2330 train_time:82978ms step_avg:40.03ms
step:2074/2330 train_time:83035ms step_avg:40.04ms
step:2075/2330 train_time:83058ms step_avg:40.03ms
step:2076/2330 train_time:83114ms step_avg:40.04ms
step:2077/2330 train_time:83138ms step_avg:40.03ms
step:2078/2330 train_time:83195ms step_avg:40.04ms
step:2079/2330 train_time:83218ms step_avg:40.03ms
step:2080/2330 train_time:83274ms step_avg:40.04ms
step:2081/2330 train_time:83298ms step_avg:40.03ms
step:2082/2330 train_time:83356ms step_avg:40.04ms
step:2083/2330 train_time:83379ms step_avg:40.03ms
step:2084/2330 train_time:83436ms step_avg:40.04ms
step:2085/2330 train_time:83459ms step_avg:40.03ms
step:2086/2330 train_time:83516ms step_avg:40.04ms
step:2087/2330 train_time:83540ms step_avg:40.03ms
step:2088/2330 train_time:83596ms step_avg:40.04ms
step:2089/2330 train_time:83619ms step_avg:40.03ms
step:2090/2330 train_time:83676ms step_avg:40.04ms
step:2091/2330 train_time:83699ms step_avg:40.03ms
step:2092/2330 train_time:83756ms step_avg:40.04ms
step:2093/2330 train_time:83779ms step_avg:40.03ms
step:2094/2330 train_time:83835ms step_avg:40.04ms
step:2095/2330 train_time:83858ms step_avg:40.03ms
step:2096/2330 train_time:83915ms step_avg:40.04ms
step:2097/2330 train_time:83938ms step_avg:40.03ms
step:2098/2330 train_time:83995ms step_avg:40.04ms
step:2099/2330 train_time:84019ms step_avg:40.03ms
step:2100/2330 train_time:84075ms step_avg:40.04ms
step:2101/2330 train_time:84098ms step_avg:40.03ms
step:2102/2330 train_time:84155ms step_avg:40.04ms
step:2103/2330 train_time:84179ms step_avg:40.03ms
step:2104/2330 train_time:84235ms step_avg:40.04ms
step:2105/2330 train_time:84259ms step_avg:40.03ms
step:2106/2330 train_time:84316ms step_avg:40.04ms
step:2107/2330 train_time:84340ms step_avg:40.03ms
step:2108/2330 train_time:84396ms step_avg:40.04ms
step:2109/2330 train_time:84419ms step_avg:40.03ms
step:2110/2330 train_time:84476ms step_avg:40.04ms
step:2111/2330 train_time:84500ms step_avg:40.03ms
step:2112/2330 train_time:84556ms step_avg:40.04ms
step:2113/2330 train_time:84579ms step_avg:40.03ms
step:2114/2330 train_time:84636ms step_avg:40.04ms
step:2115/2330 train_time:84659ms step_avg:40.03ms
step:2116/2330 train_time:84716ms step_avg:40.04ms
step:2117/2330 train_time:84739ms step_avg:40.03ms
step:2118/2330 train_time:84796ms step_avg:40.04ms
step:2119/2330 train_time:84820ms step_avg:40.03ms
step:2120/2330 train_time:84877ms step_avg:40.04ms
step:2121/2330 train_time:84900ms step_avg:40.03ms
step:2122/2330 train_time:84956ms step_avg:40.04ms
step:2123/2330 train_time:84980ms step_avg:40.03ms
step:2124/2330 train_time:85037ms step_avg:40.04ms
step:2125/2330 train_time:85060ms step_avg:40.03ms
step:2126/2330 train_time:85117ms step_avg:40.04ms
step:2127/2330 train_time:85139ms step_avg:40.03ms
step:2128/2330 train_time:85196ms step_avg:40.04ms
step:2129/2330 train_time:85219ms step_avg:40.03ms
step:2130/2330 train_time:85276ms step_avg:40.04ms
step:2131/2330 train_time:85299ms step_avg:40.03ms
step:2132/2330 train_time:85355ms step_avg:40.04ms
step:2133/2330 train_time:85378ms step_avg:40.03ms
step:2134/2330 train_time:85435ms step_avg:40.04ms
step:2135/2330 train_time:85459ms step_avg:40.03ms
step:2136/2330 train_time:85516ms step_avg:40.04ms
step:2137/2330 train_time:85540ms step_avg:40.03ms
step:2138/2330 train_time:85596ms step_avg:40.04ms
step:2139/2330 train_time:85619ms step_avg:40.03ms
step:2140/2330 train_time:85676ms step_avg:40.04ms
step:2141/2330 train_time:85699ms step_avg:40.03ms
step:2142/2330 train_time:85755ms step_avg:40.04ms
step:2143/2330 train_time:85779ms step_avg:40.03ms
step:2144/2330 train_time:85835ms step_avg:40.04ms
step:2145/2330 train_time:85859ms step_avg:40.03ms
step:2146/2330 train_time:85915ms step_avg:40.03ms
step:2147/2330 train_time:85938ms step_avg:40.03ms
step:2148/2330 train_time:85994ms step_avg:40.03ms
step:2149/2330 train_time:86018ms step_avg:40.03ms
step:2150/2330 train_time:86075ms step_avg:40.03ms
step:2151/2330 train_time:86098ms step_avg:40.03ms
step:2152/2330 train_time:86155ms step_avg:40.03ms
step:2153/2330 train_time:86178ms step_avg:40.03ms
step:2154/2330 train_time:86235ms step_avg:40.03ms
step:2155/2330 train_time:86258ms step_avg:40.03ms
step:2156/2330 train_time:86315ms step_avg:40.03ms
step:2157/2330 train_time:86339ms step_avg:40.03ms
step:2158/2330 train_time:86396ms step_avg:40.04ms
step:2159/2330 train_time:86419ms step_avg:40.03ms
step:2160/2330 train_time:86476ms step_avg:40.04ms
step:2161/2330 train_time:86499ms step_avg:40.03ms
step:2162/2330 train_time:86556ms step_avg:40.03ms
step:2163/2330 train_time:86579ms step_avg:40.03ms
step:2164/2330 train_time:86636ms step_avg:40.04ms
step:2165/2330 train_time:86660ms step_avg:40.03ms
step:2166/2330 train_time:86716ms step_avg:40.04ms
step:2167/2330 train_time:86740ms step_avg:40.03ms
step:2168/2330 train_time:86796ms step_avg:40.03ms
step:2169/2330 train_time:86820ms step_avg:40.03ms
step:2170/2330 train_time:86876ms step_avg:40.03ms
step:2171/2330 train_time:86899ms step_avg:40.03ms
step:2172/2330 train_time:86955ms step_avg:40.03ms
step:2173/2330 train_time:86978ms step_avg:40.03ms
step:2174/2330 train_time:87034ms step_avg:40.03ms
step:2175/2330 train_time:87058ms step_avg:40.03ms
step:2176/2330 train_time:87114ms step_avg:40.03ms
step:2177/2330 train_time:87138ms step_avg:40.03ms
step:2178/2330 train_time:87194ms step_avg:40.03ms
step:2179/2330 train_time:87218ms step_avg:40.03ms
step:2180/2330 train_time:87274ms step_avg:40.03ms
step:2181/2330 train_time:87298ms step_avg:40.03ms
step:2182/2330 train_time:87354ms step_avg:40.03ms
step:2183/2330 train_time:87377ms step_avg:40.03ms
step:2184/2330 train_time:87434ms step_avg:40.03ms
step:2185/2330 train_time:87458ms step_avg:40.03ms
step:2186/2330 train_time:87514ms step_avg:40.03ms
step:2187/2330 train_time:87537ms step_avg:40.03ms
step:2188/2330 train_time:87594ms step_avg:40.03ms
step:2189/2330 train_time:87617ms step_avg:40.03ms
step:2190/2330 train_time:87674ms step_avg:40.03ms
step:2191/2330 train_time:87698ms step_avg:40.03ms
step:2192/2330 train_time:87755ms step_avg:40.03ms
step:2193/2330 train_time:87778ms step_avg:40.03ms
step:2194/2330 train_time:87835ms step_avg:40.03ms
step:2195/2330 train_time:87858ms step_avg:40.03ms
step:2196/2330 train_time:87915ms step_avg:40.03ms
step:2197/2330 train_time:87938ms step_avg:40.03ms
step:2198/2330 train_time:87995ms step_avg:40.03ms
step:2199/2330 train_time:88019ms step_avg:40.03ms
step:2200/2330 train_time:88075ms step_avg:40.03ms
step:2201/2330 train_time:88098ms step_avg:40.03ms
step:2202/2330 train_time:88155ms step_avg:40.03ms
step:2203/2330 train_time:88178ms step_avg:40.03ms
step:2204/2330 train_time:88234ms step_avg:40.03ms
step:2205/2330 train_time:88258ms step_avg:40.03ms
step:2206/2330 train_time:88314ms step_avg:40.03ms
step:2207/2330 train_time:88337ms step_avg:40.03ms
step:2208/2330 train_time:88394ms step_avg:40.03ms
step:2209/2330 train_time:88417ms step_avg:40.03ms
step:2210/2330 train_time:88474ms step_avg:40.03ms
step:2211/2330 train_time:88498ms step_avg:40.03ms
step:2212/2330 train_time:88555ms step_avg:40.03ms
step:2213/2330 train_time:88578ms step_avg:40.03ms
step:2214/2330 train_time:88634ms step_avg:40.03ms
step:2215/2330 train_time:88658ms step_avg:40.03ms
step:2216/2330 train_time:88715ms step_avg:40.03ms
step:2217/2330 train_time:88738ms step_avg:40.03ms
step:2218/2330 train_time:88794ms step_avg:40.03ms
step:2219/2330 train_time:88817ms step_avg:40.03ms
step:2220/2330 train_time:88874ms step_avg:40.03ms
step:2221/2330 train_time:88898ms step_avg:40.03ms
step:2222/2330 train_time:88955ms step_avg:40.03ms
step:2223/2330 train_time:88979ms step_avg:40.03ms
step:2224/2330 train_time:89035ms step_avg:40.03ms
step:2225/2330 train_time:89059ms step_avg:40.03ms
step:2226/2330 train_time:89115ms step_avg:40.03ms
step:2227/2330 train_time:89139ms step_avg:40.03ms
step:2228/2330 train_time:89195ms step_avg:40.03ms
step:2229/2330 train_time:89220ms step_avg:40.03ms
step:2230/2330 train_time:89276ms step_avg:40.03ms
step:2231/2330 train_time:89299ms step_avg:40.03ms
step:2232/2330 train_time:89356ms step_avg:40.03ms
step:2233/2330 train_time:89379ms step_avg:40.03ms
step:2234/2330 train_time:89435ms step_avg:40.03ms
step:2235/2330 train_time:89459ms step_avg:40.03ms
step:2236/2330 train_time:89516ms step_avg:40.03ms
step:2237/2330 train_time:89539ms step_avg:40.03ms
step:2238/2330 train_time:89595ms step_avg:40.03ms
step:2239/2330 train_time:89619ms step_avg:40.03ms
step:2240/2330 train_time:89676ms step_avg:40.03ms
step:2241/2330 train_time:89699ms step_avg:40.03ms
step:2242/2330 train_time:89756ms step_avg:40.03ms
step:2243/2330 train_time:89780ms step_avg:40.03ms
step:2244/2330 train_time:89836ms step_avg:40.03ms
step:2245/2330 train_time:89859ms step_avg:40.03ms
step:2246/2330 train_time:89916ms step_avg:40.03ms
step:2247/2330 train_time:89939ms step_avg:40.03ms
step:2248/2330 train_time:89995ms step_avg:40.03ms
step:2249/2330 train_time:90019ms step_avg:40.03ms
step:2250/2330 train_time:90075ms step_avg:40.03ms
step:2250/2330 val_loss:5.4333 train_time:90173ms step_avg:40.08ms
step:2251/2330 train_time:90185ms step_avg:40.06ms
step:2252/2330 train_time:90197ms step_avg:40.05ms
step:2253/2330 train_time:90207ms step_avg:40.04ms
step:2254/2330 train_time:90237ms step_avg:40.03ms
step:2255/2330 train_time:90259ms step_avg:40.03ms
step:2256/2330 train_time:90315ms step_avg:40.03ms
step:2257/2330 train_time:90337ms step_avg:40.03ms
step:2258/2330 train_time:90394ms step_avg:40.03ms
step:2259/2330 train_time:90417ms step_avg:40.03ms
step:2260/2330 train_time:90476ms step_avg:40.03ms
step:2261/2330 train_time:90503ms step_avg:40.03ms
step:2262/2330 train_time:90564ms step_avg:40.04ms
step:2263/2330 train_time:90589ms step_avg:40.03ms
step:2264/2330 train_time:90647ms step_avg:40.04ms
step:2265/2330 train_time:90670ms step_avg:40.03ms
step:2266/2330 train_time:90726ms step_avg:40.04ms
step:2267/2330 train_time:90749ms step_avg:40.03ms
step:2268/2330 train_time:90805ms step_avg:40.04ms
step:2269/2330 train_time:90828ms step_avg:40.03ms
step:2270/2330 train_time:90884ms step_avg:40.04ms
step:2271/2330 train_time:90907ms step_avg:40.03ms
step:2272/2330 train_time:90963ms step_avg:40.04ms
step:2273/2330 train_time:90986ms step_avg:40.03ms
step:2274/2330 train_time:91043ms step_avg:40.04ms
step:2275/2330 train_time:91066ms step_avg:40.03ms
step:2276/2330 train_time:91125ms step_avg:40.04ms
step:2277/2330 train_time:91150ms step_avg:40.03ms
step:2278/2330 train_time:91208ms step_avg:40.04ms
step:2279/2330 train_time:91231ms step_avg:40.03ms
step:2280/2330 train_time:91287ms step_avg:40.04ms
step:2281/2330 train_time:91310ms step_avg:40.03ms
step:2282/2330 train_time:91366ms step_avg:40.04ms
step:2283/2330 train_time:91390ms step_avg:40.03ms
step:2284/2330 train_time:91447ms step_avg:40.04ms
step:2285/2330 train_time:91471ms step_avg:40.03ms
step:2286/2330 train_time:91529ms step_avg:40.04ms
step:2287/2330 train_time:91553ms step_avg:40.03ms
step:2288/2330 train_time:91610ms step_avg:40.04ms
step:2289/2330 train_time:91633ms step_avg:40.03ms
step:2290/2330 train_time:91691ms step_avg:40.04ms
step:2291/2330 train_time:91714ms step_avg:40.03ms
step:2292/2330 train_time:91771ms step_avg:40.04ms
step:2293/2330 train_time:91794ms step_avg:40.03ms
step:2294/2330 train_time:91851ms step_avg:40.04ms
step:2295/2330 train_time:91873ms step_avg:40.03ms
step:2296/2330 train_time:91931ms step_avg:40.04ms
step:2297/2330 train_time:91954ms step_avg:40.03ms
step:2298/2330 train_time:92011ms step_avg:40.04ms
step:2299/2330 train_time:92034ms step_avg:40.03ms
step:2300/2330 train_time:92092ms step_avg:40.04ms
step:2301/2330 train_time:92115ms step_avg:40.03ms
step:2302/2330 train_time:92173ms step_avg:40.04ms
step:2303/2330 train_time:92195ms step_avg:40.03ms
step:2304/2330 train_time:92252ms step_avg:40.04ms
step:2305/2330 train_time:92275ms step_avg:40.03ms
step:2306/2330 train_time:92333ms step_avg:40.04ms
step:2307/2330 train_time:92356ms step_avg:40.03ms
step:2308/2330 train_time:92415ms step_avg:40.04ms
step:2309/2330 train_time:92438ms step_avg:40.03ms
step:2310/2330 train_time:92496ms step_avg:40.04ms
step:2311/2330 train_time:92519ms step_avg:40.03ms
step:2312/2330 train_time:92577ms step_avg:40.04ms
step:2313/2330 train_time:92601ms step_avg:40.03ms
step:2314/2330 train_time:92659ms step_avg:40.04ms
step:2315/2330 train_time:92683ms step_avg:40.04ms
step:2316/2330 train_time:92740ms step_avg:40.04ms
step:2317/2330 train_time:92764ms step_avg:40.04ms
step:2318/2330 train_time:92821ms step_avg:40.04ms
step:2319/2330 train_time:92844ms step_avg:40.04ms
step:2320/2330 train_time:92901ms step_avg:40.04ms
step:2321/2330 train_time:92924ms step_avg:40.04ms
step:2322/2330 train_time:92981ms step_avg:40.04ms
step:2323/2330 train_time:93005ms step_avg:40.04ms
step:2324/2330 train_time:93062ms step_avg:40.04ms
step:2325/2330 train_time:93085ms step_avg:40.04ms
step:2326/2330 train_time:93143ms step_avg:40.04ms
step:2327/2330 train_time:93166ms step_avg:40.04ms
step:2328/2330 train_time:93223ms step_avg:40.04ms
step:2329/2330 train_time:93247ms step_avg:40.04ms
step:2330/2330 train_time:93305ms step_avg:40.04ms
step:2330/2330 val_loss:5.4233 train_time:93404ms step_avg:40.09ms
peak memory allocated: 29226 MiB reserved: 38888 MiB
