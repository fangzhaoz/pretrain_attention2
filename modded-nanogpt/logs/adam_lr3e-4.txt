import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "adam_lr3e-4"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-1, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.Adam(
    hidden_matrix_params + gate_params,
    lr=3e-4,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 00:06:22 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   29C    P0             110W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   27C    P0             108W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:90ms step_avg:90.39ms
step:2/2330 train_time:182ms step_avg:91.19ms
step:3/2330 train_time:202ms step_avg:67.48ms
step:4/2330 train_time:223ms step_avg:55.87ms
step:5/2330 train_time:278ms step_avg:55.70ms
step:6/2330 train_time:338ms step_avg:56.30ms
step:7/2330 train_time:394ms step_avg:56.22ms
step:8/2330 train_time:453ms step_avg:56.66ms
step:9/2330 train_time:509ms step_avg:56.58ms
step:10/2330 train_time:570ms step_avg:56.96ms
step:11/2330 train_time:625ms step_avg:56.86ms
step:12/2330 train_time:686ms step_avg:57.17ms
step:13/2330 train_time:742ms step_avg:57.10ms
step:14/2330 train_time:803ms step_avg:57.36ms
step:15/2330 train_time:859ms step_avg:57.29ms
step:16/2330 train_time:919ms step_avg:57.45ms
step:17/2330 train_time:975ms step_avg:57.33ms
step:18/2330 train_time:1037ms step_avg:57.59ms
step:19/2330 train_time:1094ms step_avg:57.57ms
step:20/2330 train_time:1159ms step_avg:57.93ms
step:21/2330 train_time:1215ms step_avg:57.86ms
step:22/2330 train_time:1279ms step_avg:58.12ms
step:23/2330 train_time:1335ms step_avg:58.04ms
step:24/2330 train_time:1397ms step_avg:58.21ms
step:25/2330 train_time:1454ms step_avg:58.15ms
step:26/2330 train_time:1515ms step_avg:58.25ms
step:27/2330 train_time:1571ms step_avg:58.17ms
step:28/2330 train_time:1632ms step_avg:58.27ms
step:29/2330 train_time:1688ms step_avg:58.20ms
step:30/2330 train_time:1748ms step_avg:58.28ms
step:31/2330 train_time:1804ms step_avg:58.21ms
step:32/2330 train_time:1865ms step_avg:58.29ms
step:33/2330 train_time:1921ms step_avg:58.22ms
step:34/2330 train_time:1983ms step_avg:58.32ms
step:35/2330 train_time:2040ms step_avg:58.29ms
step:36/2330 train_time:2103ms step_avg:58.42ms
step:37/2330 train_time:2161ms step_avg:58.40ms
step:38/2330 train_time:2224ms step_avg:58.52ms
step:39/2330 train_time:2281ms step_avg:58.49ms
step:40/2330 train_time:2344ms step_avg:58.61ms
step:41/2330 train_time:2401ms step_avg:58.57ms
step:42/2330 train_time:2464ms step_avg:58.67ms
step:43/2330 train_time:2521ms step_avg:58.63ms
step:44/2330 train_time:2583ms step_avg:58.70ms
step:45/2330 train_time:2640ms step_avg:58.66ms
step:46/2330 train_time:2702ms step_avg:58.73ms
step:47/2330 train_time:2758ms step_avg:58.68ms
step:48/2330 train_time:2819ms step_avg:58.74ms
step:49/2330 train_time:2875ms step_avg:58.68ms
step:50/2330 train_time:2936ms step_avg:58.73ms
step:51/2330 train_time:2992ms step_avg:58.68ms
step:52/2330 train_time:3054ms step_avg:58.73ms
step:53/2330 train_time:3110ms step_avg:58.69ms
step:54/2330 train_time:3173ms step_avg:58.76ms
step:55/2330 train_time:3230ms step_avg:58.73ms
step:56/2330 train_time:3291ms step_avg:58.77ms
step:57/2330 train_time:3347ms step_avg:58.73ms
step:58/2330 train_time:3410ms step_avg:58.79ms
step:59/2330 train_time:3466ms step_avg:58.75ms
step:60/2330 train_time:3529ms step_avg:58.82ms
step:61/2330 train_time:3586ms step_avg:58.78ms
step:62/2330 train_time:3648ms step_avg:58.84ms
step:63/2330 train_time:3704ms step_avg:58.80ms
step:64/2330 train_time:3765ms step_avg:58.83ms
step:65/2330 train_time:3822ms step_avg:58.79ms
step:66/2330 train_time:3884ms step_avg:58.86ms
step:67/2330 train_time:3941ms step_avg:58.83ms
step:68/2330 train_time:4003ms step_avg:58.87ms
step:69/2330 train_time:4060ms step_avg:58.84ms
step:70/2330 train_time:4122ms step_avg:58.88ms
step:71/2330 train_time:4179ms step_avg:58.86ms
step:72/2330 train_time:4241ms step_avg:58.90ms
step:73/2330 train_time:4298ms step_avg:58.87ms
step:74/2330 train_time:4361ms step_avg:58.93ms
step:75/2330 train_time:4417ms step_avg:58.90ms
step:76/2330 train_time:4479ms step_avg:58.94ms
step:77/2330 train_time:4536ms step_avg:58.90ms
step:78/2330 train_time:4597ms step_avg:58.94ms
step:79/2330 train_time:4654ms step_avg:58.91ms
step:80/2330 train_time:4715ms step_avg:58.94ms
step:81/2330 train_time:4771ms step_avg:58.90ms
step:82/2330 train_time:4832ms step_avg:58.93ms
step:83/2330 train_time:4888ms step_avg:58.89ms
step:84/2330 train_time:4949ms step_avg:58.92ms
step:85/2330 train_time:5005ms step_avg:58.89ms
step:86/2330 train_time:5066ms step_avg:58.91ms
step:87/2330 train_time:5123ms step_avg:58.89ms
step:88/2330 train_time:5186ms step_avg:58.93ms
step:89/2330 train_time:5243ms step_avg:58.91ms
step:90/2330 train_time:5305ms step_avg:58.95ms
step:91/2330 train_time:5362ms step_avg:58.92ms
step:92/2330 train_time:5424ms step_avg:58.96ms
step:93/2330 train_time:5481ms step_avg:58.93ms
step:94/2330 train_time:5543ms step_avg:58.97ms
step:95/2330 train_time:5601ms step_avg:58.95ms
step:96/2330 train_time:5662ms step_avg:58.98ms
step:97/2330 train_time:5720ms step_avg:58.97ms
step:98/2330 train_time:5780ms step_avg:58.98ms
step:99/2330 train_time:5837ms step_avg:58.96ms
step:100/2330 train_time:5899ms step_avg:58.99ms
step:101/2330 train_time:5955ms step_avg:58.96ms
step:102/2330 train_time:6016ms step_avg:58.98ms
step:103/2330 train_time:6072ms step_avg:58.95ms
step:104/2330 train_time:6133ms step_avg:58.97ms
step:105/2330 train_time:6189ms step_avg:58.94ms
step:106/2330 train_time:6251ms step_avg:58.97ms
step:107/2330 train_time:6307ms step_avg:58.95ms
step:108/2330 train_time:6369ms step_avg:58.97ms
step:109/2330 train_time:6425ms step_avg:58.95ms
step:110/2330 train_time:6488ms step_avg:58.98ms
step:111/2330 train_time:6544ms step_avg:58.96ms
step:112/2330 train_time:6606ms step_avg:58.98ms
step:113/2330 train_time:6662ms step_avg:58.96ms
step:114/2330 train_time:6724ms step_avg:58.98ms
step:115/2330 train_time:6781ms step_avg:58.96ms
step:116/2330 train_time:6843ms step_avg:58.99ms
step:117/2330 train_time:6900ms step_avg:58.97ms
step:118/2330 train_time:6961ms step_avg:59.00ms
step:119/2330 train_time:7018ms step_avg:58.98ms
step:120/2330 train_time:7078ms step_avg:58.99ms
step:121/2330 train_time:7134ms step_avg:58.96ms
step:122/2330 train_time:7196ms step_avg:58.99ms
step:123/2330 train_time:7253ms step_avg:58.97ms
step:124/2330 train_time:7314ms step_avg:58.98ms
step:125/2330 train_time:7369ms step_avg:58.96ms
step:126/2330 train_time:7431ms step_avg:58.98ms
step:127/2330 train_time:7487ms step_avg:58.96ms
step:128/2330 train_time:7549ms step_avg:58.98ms
step:129/2330 train_time:7606ms step_avg:58.96ms
step:130/2330 train_time:7667ms step_avg:58.98ms
step:131/2330 train_time:7724ms step_avg:58.96ms
step:132/2330 train_time:7786ms step_avg:58.98ms
step:133/2330 train_time:7843ms step_avg:58.97ms
step:134/2330 train_time:7904ms step_avg:58.99ms
step:135/2330 train_time:7961ms step_avg:58.97ms
step:136/2330 train_time:8022ms step_avg:58.98ms
step:137/2330 train_time:8078ms step_avg:58.97ms
step:138/2330 train_time:8140ms step_avg:58.98ms
step:139/2330 train_time:8196ms step_avg:58.97ms
step:140/2330 train_time:8258ms step_avg:58.99ms
step:141/2330 train_time:8315ms step_avg:58.97ms
step:142/2330 train_time:8376ms step_avg:58.99ms
step:143/2330 train_time:8433ms step_avg:58.97ms
step:144/2330 train_time:8494ms step_avg:58.98ms
step:145/2330 train_time:8550ms step_avg:58.96ms
step:146/2330 train_time:8612ms step_avg:58.99ms
step:147/2330 train_time:8668ms step_avg:58.96ms
step:148/2330 train_time:8729ms step_avg:58.98ms
step:149/2330 train_time:8785ms step_avg:58.96ms
step:150/2330 train_time:8847ms step_avg:58.98ms
step:151/2330 train_time:8904ms step_avg:58.97ms
step:152/2330 train_time:8965ms step_avg:58.98ms
step:153/2330 train_time:9021ms step_avg:58.96ms
step:154/2330 train_time:9083ms step_avg:58.98ms
step:155/2330 train_time:9140ms step_avg:58.97ms
step:156/2330 train_time:9201ms step_avg:58.98ms
step:157/2330 train_time:9258ms step_avg:58.97ms
step:158/2330 train_time:9319ms step_avg:58.98ms
step:159/2330 train_time:9376ms step_avg:58.97ms
step:160/2330 train_time:9437ms step_avg:58.98ms
step:161/2330 train_time:9494ms step_avg:58.97ms
step:162/2330 train_time:9556ms step_avg:58.99ms
step:163/2330 train_time:9612ms step_avg:58.97ms
step:164/2330 train_time:9673ms step_avg:58.98ms
step:165/2330 train_time:9729ms step_avg:58.96ms
step:166/2330 train_time:9790ms step_avg:58.98ms
step:167/2330 train_time:9846ms step_avg:58.96ms
step:168/2330 train_time:9907ms step_avg:58.97ms
step:169/2330 train_time:9964ms step_avg:58.96ms
step:170/2330 train_time:10025ms step_avg:58.97ms
step:171/2330 train_time:10081ms step_avg:58.96ms
step:172/2330 train_time:10143ms step_avg:58.97ms
step:173/2330 train_time:10200ms step_avg:58.96ms
step:174/2330 train_time:10262ms step_avg:58.98ms
step:175/2330 train_time:10319ms step_avg:58.96ms
step:176/2330 train_time:10380ms step_avg:58.97ms
step:177/2330 train_time:10437ms step_avg:58.97ms
step:178/2330 train_time:10498ms step_avg:58.98ms
step:179/2330 train_time:10555ms step_avg:58.97ms
step:180/2330 train_time:10616ms step_avg:58.98ms
step:181/2330 train_time:10672ms step_avg:58.96ms
step:182/2330 train_time:10734ms step_avg:58.98ms
step:183/2330 train_time:10790ms step_avg:58.96ms
step:184/2330 train_time:10852ms step_avg:58.98ms
step:185/2330 train_time:10908ms step_avg:58.96ms
step:186/2330 train_time:10969ms step_avg:58.97ms
step:187/2330 train_time:11025ms step_avg:58.96ms
step:188/2330 train_time:11086ms step_avg:58.97ms
step:189/2330 train_time:11143ms step_avg:58.96ms
step:190/2330 train_time:11206ms step_avg:58.98ms
step:191/2330 train_time:11263ms step_avg:58.97ms
step:192/2330 train_time:11325ms step_avg:58.99ms
step:193/2330 train_time:11382ms step_avg:58.98ms
step:194/2330 train_time:11445ms step_avg:58.99ms
step:195/2330 train_time:11502ms step_avg:58.98ms
step:196/2330 train_time:11565ms step_avg:59.00ms
step:197/2330 train_time:11622ms step_avg:58.99ms
step:198/2330 train_time:11683ms step_avg:59.00ms
step:199/2330 train_time:11740ms step_avg:58.99ms
step:200/2330 train_time:11801ms step_avg:59.01ms
step:201/2330 train_time:11858ms step_avg:59.00ms
step:202/2330 train_time:11920ms step_avg:59.01ms
step:203/2330 train_time:11976ms step_avg:59.00ms
step:204/2330 train_time:12037ms step_avg:59.00ms
step:205/2330 train_time:12093ms step_avg:58.99ms
step:206/2330 train_time:12154ms step_avg:59.00ms
step:207/2330 train_time:12210ms step_avg:58.99ms
step:208/2330 train_time:12272ms step_avg:59.00ms
step:209/2330 train_time:12328ms step_avg:58.98ms
step:210/2330 train_time:12389ms step_avg:59.00ms
step:211/2330 train_time:12446ms step_avg:58.98ms
step:212/2330 train_time:12508ms step_avg:59.00ms
step:213/2330 train_time:12565ms step_avg:58.99ms
step:214/2330 train_time:12627ms step_avg:59.00ms
step:215/2330 train_time:12683ms step_avg:58.99ms
step:216/2330 train_time:12746ms step_avg:59.01ms
step:217/2330 train_time:12803ms step_avg:59.00ms
step:218/2330 train_time:12866ms step_avg:59.02ms
step:219/2330 train_time:12923ms step_avg:59.01ms
step:220/2330 train_time:12983ms step_avg:59.01ms
step:221/2330 train_time:13039ms step_avg:59.00ms
step:222/2330 train_time:13100ms step_avg:59.01ms
step:223/2330 train_time:13157ms step_avg:59.00ms
step:224/2330 train_time:13219ms step_avg:59.01ms
step:225/2330 train_time:13275ms step_avg:59.00ms
step:226/2330 train_time:13336ms step_avg:59.01ms
step:227/2330 train_time:13393ms step_avg:59.00ms
step:228/2330 train_time:13454ms step_avg:59.01ms
step:229/2330 train_time:13510ms step_avg:59.00ms
step:230/2330 train_time:13572ms step_avg:59.01ms
step:231/2330 train_time:13628ms step_avg:59.00ms
step:232/2330 train_time:13689ms step_avg:59.00ms
step:233/2330 train_time:13745ms step_avg:58.99ms
step:234/2330 train_time:13807ms step_avg:59.00ms
step:235/2330 train_time:13863ms step_avg:58.99ms
step:236/2330 train_time:13925ms step_avg:59.00ms
step:237/2330 train_time:13980ms step_avg:58.99ms
step:238/2330 train_time:14043ms step_avg:59.00ms
step:239/2330 train_time:14101ms step_avg:59.00ms
step:240/2330 train_time:14162ms step_avg:59.01ms
step:241/2330 train_time:14219ms step_avg:59.00ms
step:242/2330 train_time:14280ms step_avg:59.01ms
step:243/2330 train_time:14336ms step_avg:59.00ms
step:244/2330 train_time:14397ms step_avg:59.00ms
step:245/2330 train_time:14453ms step_avg:58.99ms
step:246/2330 train_time:14515ms step_avg:59.00ms
step:247/2330 train_time:14571ms step_avg:58.99ms
step:248/2330 train_time:14632ms step_avg:59.00ms
step:249/2330 train_time:14688ms step_avg:58.99ms
step:250/2330 train_time:14750ms step_avg:59.00ms
step:250/2330 val_loss:4.9944 train_time:14827ms step_avg:59.31ms
step:251/2330 train_time:14847ms step_avg:59.15ms
step:252/2330 train_time:14869ms step_avg:59.00ms
step:253/2330 train_time:14925ms step_avg:58.99ms
step:254/2330 train_time:14991ms step_avg:59.02ms
step:255/2330 train_time:15049ms step_avg:59.01ms
step:256/2330 train_time:15116ms step_avg:59.05ms
step:257/2330 train_time:15172ms step_avg:59.04ms
step:258/2330 train_time:15234ms step_avg:59.05ms
step:259/2330 train_time:15290ms step_avg:59.04ms
step:260/2330 train_time:15351ms step_avg:59.04ms
step:261/2330 train_time:15408ms step_avg:59.03ms
step:262/2330 train_time:15468ms step_avg:59.04ms
step:263/2330 train_time:15524ms step_avg:59.03ms
step:264/2330 train_time:15585ms step_avg:59.04ms
step:265/2330 train_time:15641ms step_avg:59.02ms
step:266/2330 train_time:15702ms step_avg:59.03ms
step:267/2330 train_time:15758ms step_avg:59.02ms
step:268/2330 train_time:15819ms step_avg:59.03ms
step:269/2330 train_time:15876ms step_avg:59.02ms
step:270/2330 train_time:15938ms step_avg:59.03ms
step:271/2330 train_time:15995ms step_avg:59.02ms
step:272/2330 train_time:16058ms step_avg:59.04ms
step:273/2330 train_time:16115ms step_avg:59.03ms
step:274/2330 train_time:16176ms step_avg:59.04ms
step:275/2330 train_time:16232ms step_avg:59.03ms
step:276/2330 train_time:16294ms step_avg:59.04ms
step:277/2330 train_time:16350ms step_avg:59.03ms
step:278/2330 train_time:16413ms step_avg:59.04ms
step:279/2330 train_time:16469ms step_avg:59.03ms
step:280/2330 train_time:16529ms step_avg:59.03ms
step:281/2330 train_time:16586ms step_avg:59.03ms
step:282/2330 train_time:16647ms step_avg:59.03ms
step:283/2330 train_time:16703ms step_avg:59.02ms
step:284/2330 train_time:16765ms step_avg:59.03ms
step:285/2330 train_time:16822ms step_avg:59.02ms
step:286/2330 train_time:16883ms step_avg:59.03ms
step:287/2330 train_time:16939ms step_avg:59.02ms
step:288/2330 train_time:17001ms step_avg:59.03ms
step:289/2330 train_time:17058ms step_avg:59.02ms
step:290/2330 train_time:17120ms step_avg:59.04ms
step:291/2330 train_time:17177ms step_avg:59.03ms
step:292/2330 train_time:17238ms step_avg:59.03ms
step:293/2330 train_time:17294ms step_avg:59.02ms
step:294/2330 train_time:17356ms step_avg:59.03ms
step:295/2330 train_time:17411ms step_avg:59.02ms
step:296/2330 train_time:17472ms step_avg:59.03ms
step:297/2330 train_time:17529ms step_avg:59.02ms
step:298/2330 train_time:17590ms step_avg:59.03ms
step:299/2330 train_time:17646ms step_avg:59.02ms
step:300/2330 train_time:17707ms step_avg:59.02ms
step:301/2330 train_time:17765ms step_avg:59.02ms
step:302/2330 train_time:17826ms step_avg:59.03ms
step:303/2330 train_time:17884ms step_avg:59.02ms
step:304/2330 train_time:17946ms step_avg:59.03ms
step:305/2330 train_time:18003ms step_avg:59.03ms
step:306/2330 train_time:18066ms step_avg:59.04ms
step:307/2330 train_time:18122ms step_avg:59.03ms
step:308/2330 train_time:18186ms step_avg:59.05ms
step:309/2330 train_time:18242ms step_avg:59.04ms
step:310/2330 train_time:18304ms step_avg:59.04ms
step:311/2330 train_time:18360ms step_avg:59.03ms
step:312/2330 train_time:18423ms step_avg:59.05ms
step:313/2330 train_time:18479ms step_avg:59.04ms
step:314/2330 train_time:18540ms step_avg:59.04ms
step:315/2330 train_time:18595ms step_avg:59.03ms
step:316/2330 train_time:18656ms step_avg:59.04ms
step:317/2330 train_time:18712ms step_avg:59.03ms
step:318/2330 train_time:18773ms step_avg:59.03ms
step:319/2330 train_time:18829ms step_avg:59.03ms
step:320/2330 train_time:18891ms step_avg:59.04ms
step:321/2330 train_time:18948ms step_avg:59.03ms
step:322/2330 train_time:19011ms step_avg:59.04ms
step:323/2330 train_time:19068ms step_avg:59.03ms
step:324/2330 train_time:19130ms step_avg:59.04ms
step:325/2330 train_time:19187ms step_avg:59.04ms
step:326/2330 train_time:19248ms step_avg:59.04ms
step:327/2330 train_time:19305ms step_avg:59.04ms
step:328/2330 train_time:19366ms step_avg:59.04ms
step:329/2330 train_time:19423ms step_avg:59.04ms
step:330/2330 train_time:19485ms step_avg:59.04ms
step:331/2330 train_time:19541ms step_avg:59.04ms
step:332/2330 train_time:19602ms step_avg:59.04ms
step:333/2330 train_time:19658ms step_avg:59.03ms
step:334/2330 train_time:19719ms step_avg:59.04ms
step:335/2330 train_time:19775ms step_avg:59.03ms
step:336/2330 train_time:19835ms step_avg:59.03ms
step:337/2330 train_time:19891ms step_avg:59.02ms
step:338/2330 train_time:19953ms step_avg:59.03ms
step:339/2330 train_time:20010ms step_avg:59.03ms
step:340/2330 train_time:20073ms step_avg:59.04ms
step:341/2330 train_time:20131ms step_avg:59.03ms
step:342/2330 train_time:20192ms step_avg:59.04ms
step:343/2330 train_time:20249ms step_avg:59.03ms
step:344/2330 train_time:20310ms step_avg:59.04ms
step:345/2330 train_time:20367ms step_avg:59.03ms
step:346/2330 train_time:20429ms step_avg:59.04ms
step:347/2330 train_time:20486ms step_avg:59.04ms
step:348/2330 train_time:20547ms step_avg:59.04ms
step:349/2330 train_time:20604ms step_avg:59.04ms
step:350/2330 train_time:20665ms step_avg:59.04ms
step:351/2330 train_time:20722ms step_avg:59.04ms
step:352/2330 train_time:20782ms step_avg:59.04ms
step:353/2330 train_time:20838ms step_avg:59.03ms
step:354/2330 train_time:20900ms step_avg:59.04ms
step:355/2330 train_time:20956ms step_avg:59.03ms
step:356/2330 train_time:21018ms step_avg:59.04ms
step:357/2330 train_time:21074ms step_avg:59.03ms
step:358/2330 train_time:21135ms step_avg:59.04ms
step:359/2330 train_time:21192ms step_avg:59.03ms
step:360/2330 train_time:21254ms step_avg:59.04ms
step:361/2330 train_time:21311ms step_avg:59.03ms
step:362/2330 train_time:21373ms step_avg:59.04ms
step:363/2330 train_time:21430ms step_avg:59.03ms
step:364/2330 train_time:21491ms step_avg:59.04ms
step:365/2330 train_time:21548ms step_avg:59.04ms
step:366/2330 train_time:21610ms step_avg:59.04ms
step:367/2330 train_time:21667ms step_avg:59.04ms
step:368/2330 train_time:21728ms step_avg:59.04ms
step:369/2330 train_time:21785ms step_avg:59.04ms
step:370/2330 train_time:21847ms step_avg:59.05ms
step:371/2330 train_time:21903ms step_avg:59.04ms
step:372/2330 train_time:21967ms step_avg:59.05ms
step:373/2330 train_time:22023ms step_avg:59.04ms
step:374/2330 train_time:22086ms step_avg:59.05ms
step:375/2330 train_time:22141ms step_avg:59.04ms
step:376/2330 train_time:22203ms step_avg:59.05ms
step:377/2330 train_time:22259ms step_avg:59.04ms
step:378/2330 train_time:22321ms step_avg:59.05ms
step:379/2330 train_time:22377ms step_avg:59.04ms
step:380/2330 train_time:22438ms step_avg:59.05ms
step:381/2330 train_time:22494ms step_avg:59.04ms
step:382/2330 train_time:22556ms step_avg:59.05ms
step:383/2330 train_time:22612ms step_avg:59.04ms
step:384/2330 train_time:22674ms step_avg:59.05ms
step:385/2330 train_time:22730ms step_avg:59.04ms
step:386/2330 train_time:22792ms step_avg:59.05ms
step:387/2330 train_time:22849ms step_avg:59.04ms
step:388/2330 train_time:22912ms step_avg:59.05ms
step:389/2330 train_time:22969ms step_avg:59.05ms
step:390/2330 train_time:23029ms step_avg:59.05ms
step:391/2330 train_time:23086ms step_avg:59.04ms
step:392/2330 train_time:23148ms step_avg:59.05ms
step:393/2330 train_time:23204ms step_avg:59.04ms
step:394/2330 train_time:23267ms step_avg:59.05ms
step:395/2330 train_time:23324ms step_avg:59.05ms
step:396/2330 train_time:23386ms step_avg:59.06ms
step:397/2330 train_time:23442ms step_avg:59.05ms
step:398/2330 train_time:23505ms step_avg:59.06ms
step:399/2330 train_time:23561ms step_avg:59.05ms
step:400/2330 train_time:23623ms step_avg:59.06ms
step:401/2330 train_time:23679ms step_avg:59.05ms
step:402/2330 train_time:23739ms step_avg:59.05ms
step:403/2330 train_time:23796ms step_avg:59.05ms
step:404/2330 train_time:23858ms step_avg:59.05ms
step:405/2330 train_time:23914ms step_avg:59.05ms
step:406/2330 train_time:23976ms step_avg:59.05ms
step:407/2330 train_time:24032ms step_avg:59.05ms
step:408/2330 train_time:24094ms step_avg:59.05ms
step:409/2330 train_time:24151ms step_avg:59.05ms
step:410/2330 train_time:24213ms step_avg:59.06ms
step:411/2330 train_time:24270ms step_avg:59.05ms
step:412/2330 train_time:24332ms step_avg:59.06ms
step:413/2330 train_time:24390ms step_avg:59.05ms
step:414/2330 train_time:24451ms step_avg:59.06ms
step:415/2330 train_time:24507ms step_avg:59.05ms
step:416/2330 train_time:24569ms step_avg:59.06ms
step:417/2330 train_time:24626ms step_avg:59.06ms
step:418/2330 train_time:24687ms step_avg:59.06ms
step:419/2330 train_time:24743ms step_avg:59.05ms
step:420/2330 train_time:24805ms step_avg:59.06ms
step:421/2330 train_time:24862ms step_avg:59.05ms
step:422/2330 train_time:24923ms step_avg:59.06ms
step:423/2330 train_time:24980ms step_avg:59.05ms
step:424/2330 train_time:25041ms step_avg:59.06ms
step:425/2330 train_time:25097ms step_avg:59.05ms
step:426/2330 train_time:25159ms step_avg:59.06ms
step:427/2330 train_time:25215ms step_avg:59.05ms
step:428/2330 train_time:25276ms step_avg:59.06ms
step:429/2330 train_time:25332ms step_avg:59.05ms
step:430/2330 train_time:25395ms step_avg:59.06ms
step:431/2330 train_time:25452ms step_avg:59.05ms
step:432/2330 train_time:25514ms step_avg:59.06ms
step:433/2330 train_time:25570ms step_avg:59.05ms
step:434/2330 train_time:25632ms step_avg:59.06ms
step:435/2330 train_time:25688ms step_avg:59.05ms
step:436/2330 train_time:25751ms step_avg:59.06ms
step:437/2330 train_time:25808ms step_avg:59.06ms
step:438/2330 train_time:25869ms step_avg:59.06ms
step:439/2330 train_time:25925ms step_avg:59.06ms
step:440/2330 train_time:25987ms step_avg:59.06ms
step:441/2330 train_time:26044ms step_avg:59.06ms
step:442/2330 train_time:26106ms step_avg:59.06ms
step:443/2330 train_time:26162ms step_avg:59.06ms
step:444/2330 train_time:26226ms step_avg:59.07ms
step:445/2330 train_time:26281ms step_avg:59.06ms
step:446/2330 train_time:26344ms step_avg:59.07ms
step:447/2330 train_time:26399ms step_avg:59.06ms
step:448/2330 train_time:26462ms step_avg:59.07ms
step:449/2330 train_time:26518ms step_avg:59.06ms
step:450/2330 train_time:26580ms step_avg:59.07ms
step:451/2330 train_time:26636ms step_avg:59.06ms
step:452/2330 train_time:26698ms step_avg:59.07ms
step:453/2330 train_time:26754ms step_avg:59.06ms
step:454/2330 train_time:26816ms step_avg:59.07ms
step:455/2330 train_time:26872ms step_avg:59.06ms
step:456/2330 train_time:26934ms step_avg:59.07ms
step:457/2330 train_time:26991ms step_avg:59.06ms
step:458/2330 train_time:27054ms step_avg:59.07ms
step:459/2330 train_time:27112ms step_avg:59.07ms
step:460/2330 train_time:27172ms step_avg:59.07ms
step:461/2330 train_time:27229ms step_avg:59.06ms
step:462/2330 train_time:27291ms step_avg:59.07ms
step:463/2330 train_time:27348ms step_avg:59.07ms
step:464/2330 train_time:27410ms step_avg:59.07ms
step:465/2330 train_time:27468ms step_avg:59.07ms
step:466/2330 train_time:27528ms step_avg:59.07ms
step:467/2330 train_time:27585ms step_avg:59.07ms
step:468/2330 train_time:27647ms step_avg:59.07ms
step:469/2330 train_time:27703ms step_avg:59.07ms
step:470/2330 train_time:27766ms step_avg:59.08ms
step:471/2330 train_time:27821ms step_avg:59.07ms
step:472/2330 train_time:27884ms step_avg:59.08ms
step:473/2330 train_time:27939ms step_avg:59.07ms
step:474/2330 train_time:28001ms step_avg:59.07ms
step:475/2330 train_time:28057ms step_avg:59.07ms
step:476/2330 train_time:28119ms step_avg:59.07ms
step:477/2330 train_time:28175ms step_avg:59.07ms
step:478/2330 train_time:28236ms step_avg:59.07ms
step:479/2330 train_time:28292ms step_avg:59.07ms
step:480/2330 train_time:28354ms step_avg:59.07ms
step:481/2330 train_time:28410ms step_avg:59.07ms
step:482/2330 train_time:28472ms step_avg:59.07ms
step:483/2330 train_time:28529ms step_avg:59.07ms
step:484/2330 train_time:28590ms step_avg:59.07ms
step:485/2330 train_time:28647ms step_avg:59.07ms
step:486/2330 train_time:28710ms step_avg:59.07ms
step:487/2330 train_time:28767ms step_avg:59.07ms
step:488/2330 train_time:28829ms step_avg:59.08ms
step:489/2330 train_time:28887ms step_avg:59.07ms
step:490/2330 train_time:28947ms step_avg:59.08ms
step:491/2330 train_time:29004ms step_avg:59.07ms
step:492/2330 train_time:29066ms step_avg:59.08ms
step:493/2330 train_time:29123ms step_avg:59.07ms
step:494/2330 train_time:29185ms step_avg:59.08ms
step:495/2330 train_time:29241ms step_avg:59.07ms
step:496/2330 train_time:29302ms step_avg:59.08ms
step:497/2330 train_time:29358ms step_avg:59.07ms
step:498/2330 train_time:29419ms step_avg:59.08ms
step:499/2330 train_time:29475ms step_avg:59.07ms
step:500/2330 train_time:29536ms step_avg:59.07ms
step:500/2330 val_loss:4.5316 train_time:29615ms step_avg:59.23ms
step:501/2330 train_time:29634ms step_avg:59.15ms
step:502/2330 train_time:29657ms step_avg:59.08ms
step:503/2330 train_time:29715ms step_avg:59.07ms
step:504/2330 train_time:29781ms step_avg:59.09ms
step:505/2330 train_time:29837ms step_avg:59.08ms
step:506/2330 train_time:29900ms step_avg:59.09ms
step:507/2330 train_time:29956ms step_avg:59.09ms
step:508/2330 train_time:30017ms step_avg:59.09ms
step:509/2330 train_time:30073ms step_avg:59.08ms
step:510/2330 train_time:30134ms step_avg:59.09ms
step:511/2330 train_time:30190ms step_avg:59.08ms
step:512/2330 train_time:30251ms step_avg:59.08ms
step:513/2330 train_time:30307ms step_avg:59.08ms
step:514/2330 train_time:30368ms step_avg:59.08ms
step:515/2330 train_time:30424ms step_avg:59.08ms
step:516/2330 train_time:30484ms step_avg:59.08ms
step:517/2330 train_time:30541ms step_avg:59.07ms
step:518/2330 train_time:30604ms step_avg:59.08ms
step:519/2330 train_time:30661ms step_avg:59.08ms
step:520/2330 train_time:30725ms step_avg:59.09ms
step:521/2330 train_time:30782ms step_avg:59.08ms
step:522/2330 train_time:30845ms step_avg:59.09ms
step:523/2330 train_time:30902ms step_avg:59.09ms
step:524/2330 train_time:30965ms step_avg:59.09ms
step:525/2330 train_time:31022ms step_avg:59.09ms
step:526/2330 train_time:31083ms step_avg:59.09ms
step:527/2330 train_time:31139ms step_avg:59.09ms
step:528/2330 train_time:31201ms step_avg:59.09ms
step:529/2330 train_time:31257ms step_avg:59.09ms
step:530/2330 train_time:31318ms step_avg:59.09ms
step:531/2330 train_time:31374ms step_avg:59.08ms
step:532/2330 train_time:31435ms step_avg:59.09ms
step:533/2330 train_time:31490ms step_avg:59.08ms
step:534/2330 train_time:31551ms step_avg:59.08ms
step:535/2330 train_time:31607ms step_avg:59.08ms
step:536/2330 train_time:31670ms step_avg:59.09ms
step:537/2330 train_time:31726ms step_avg:59.08ms
step:538/2330 train_time:31790ms step_avg:59.09ms
step:539/2330 train_time:31846ms step_avg:59.08ms
step:540/2330 train_time:31909ms step_avg:59.09ms
step:541/2330 train_time:31966ms step_avg:59.09ms
step:542/2330 train_time:32028ms step_avg:59.09ms
step:543/2330 train_time:32085ms step_avg:59.09ms
step:544/2330 train_time:32147ms step_avg:59.09ms
step:545/2330 train_time:32204ms step_avg:59.09ms
step:546/2330 train_time:32266ms step_avg:59.09ms
step:547/2330 train_time:32323ms step_avg:59.09ms
step:548/2330 train_time:32383ms step_avg:59.09ms
step:549/2330 train_time:32440ms step_avg:59.09ms
step:550/2330 train_time:32501ms step_avg:59.09ms
step:551/2330 train_time:32557ms step_avg:59.09ms
step:552/2330 train_time:32620ms step_avg:59.09ms
step:553/2330 train_time:32676ms step_avg:59.09ms
step:554/2330 train_time:32738ms step_avg:59.09ms
step:555/2330 train_time:32794ms step_avg:59.09ms
step:556/2330 train_time:32856ms step_avg:59.09ms
step:557/2330 train_time:32912ms step_avg:59.09ms
step:558/2330 train_time:32974ms step_avg:59.09ms
step:559/2330 train_time:33031ms step_avg:59.09ms
step:560/2330 train_time:33093ms step_avg:59.09ms
step:561/2330 train_time:33149ms step_avg:59.09ms
step:562/2330 train_time:33211ms step_avg:59.10ms
step:563/2330 train_time:33268ms step_avg:59.09ms
step:564/2330 train_time:33329ms step_avg:59.09ms
step:565/2330 train_time:33386ms step_avg:59.09ms
step:566/2330 train_time:33448ms step_avg:59.09ms
step:567/2330 train_time:33504ms step_avg:59.09ms
step:568/2330 train_time:33566ms step_avg:59.09ms
step:569/2330 train_time:33623ms step_avg:59.09ms
step:570/2330 train_time:33684ms step_avg:59.09ms
step:571/2330 train_time:33741ms step_avg:59.09ms
step:572/2330 train_time:33803ms step_avg:59.10ms
step:573/2330 train_time:33860ms step_avg:59.09ms
step:574/2330 train_time:33921ms step_avg:59.10ms
step:575/2330 train_time:33977ms step_avg:59.09ms
step:576/2330 train_time:34040ms step_avg:59.10ms
step:577/2330 train_time:34096ms step_avg:59.09ms
step:578/2330 train_time:34159ms step_avg:59.10ms
step:579/2330 train_time:34215ms step_avg:59.09ms
step:580/2330 train_time:34277ms step_avg:59.10ms
step:581/2330 train_time:34333ms step_avg:59.09ms
step:582/2330 train_time:34394ms step_avg:59.10ms
step:583/2330 train_time:34450ms step_avg:59.09ms
step:584/2330 train_time:34512ms step_avg:59.10ms
step:585/2330 train_time:34568ms step_avg:59.09ms
step:586/2330 train_time:34630ms step_avg:59.10ms
step:587/2330 train_time:34687ms step_avg:59.09ms
step:588/2330 train_time:34750ms step_avg:59.10ms
step:589/2330 train_time:34807ms step_avg:59.10ms
step:590/2330 train_time:34868ms step_avg:59.10ms
step:591/2330 train_time:34926ms step_avg:59.10ms
step:592/2330 train_time:34987ms step_avg:59.10ms
step:593/2330 train_time:35044ms step_avg:59.10ms
step:594/2330 train_time:35105ms step_avg:59.10ms
step:595/2330 train_time:35162ms step_avg:59.10ms
step:596/2330 train_time:35223ms step_avg:59.10ms
step:597/2330 train_time:35280ms step_avg:59.09ms
step:598/2330 train_time:35341ms step_avg:59.10ms
step:599/2330 train_time:35397ms step_avg:59.09ms
step:600/2330 train_time:35459ms step_avg:59.10ms
step:601/2330 train_time:35515ms step_avg:59.09ms
step:602/2330 train_time:35576ms step_avg:59.10ms
step:603/2330 train_time:35632ms step_avg:59.09ms
step:604/2330 train_time:35693ms step_avg:59.09ms
step:605/2330 train_time:35750ms step_avg:59.09ms
step:606/2330 train_time:35811ms step_avg:59.09ms
step:607/2330 train_time:35867ms step_avg:59.09ms
step:608/2330 train_time:35929ms step_avg:59.09ms
step:609/2330 train_time:35986ms step_avg:59.09ms
step:610/2330 train_time:36048ms step_avg:59.09ms
step:611/2330 train_time:36105ms step_avg:59.09ms
step:612/2330 train_time:36166ms step_avg:59.09ms
step:613/2330 train_time:36223ms step_avg:59.09ms
step:614/2330 train_time:36286ms step_avg:59.10ms
step:615/2330 train_time:36343ms step_avg:59.09ms
step:616/2330 train_time:36405ms step_avg:59.10ms
step:617/2330 train_time:36462ms step_avg:59.10ms
step:618/2330 train_time:36522ms step_avg:59.10ms
step:619/2330 train_time:36579ms step_avg:59.09ms
step:620/2330 train_time:36641ms step_avg:59.10ms
step:621/2330 train_time:36698ms step_avg:59.09ms
step:622/2330 train_time:36760ms step_avg:59.10ms
step:623/2330 train_time:36816ms step_avg:59.10ms
step:624/2330 train_time:36878ms step_avg:59.10ms
step:625/2330 train_time:36934ms step_avg:59.09ms
step:626/2330 train_time:36995ms step_avg:59.10ms
step:627/2330 train_time:37051ms step_avg:59.09ms
step:628/2330 train_time:37113ms step_avg:59.10ms
step:629/2330 train_time:37169ms step_avg:59.09ms
step:630/2330 train_time:37231ms step_avg:59.10ms
step:631/2330 train_time:37288ms step_avg:59.09ms
step:632/2330 train_time:37350ms step_avg:59.10ms
step:633/2330 train_time:37407ms step_avg:59.10ms
step:634/2330 train_time:37469ms step_avg:59.10ms
step:635/2330 train_time:37526ms step_avg:59.10ms
step:636/2330 train_time:37588ms step_avg:59.10ms
step:637/2330 train_time:37647ms step_avg:59.10ms
step:638/2330 train_time:37707ms step_avg:59.10ms
step:639/2330 train_time:37764ms step_avg:59.10ms
step:640/2330 train_time:37826ms step_avg:59.10ms
step:641/2330 train_time:37883ms step_avg:59.10ms
step:642/2330 train_time:37945ms step_avg:59.10ms
step:643/2330 train_time:38002ms step_avg:59.10ms
step:644/2330 train_time:38063ms step_avg:59.10ms
step:645/2330 train_time:38120ms step_avg:59.10ms
step:646/2330 train_time:38182ms step_avg:59.10ms
step:647/2330 train_time:38238ms step_avg:59.10ms
step:648/2330 train_time:38300ms step_avg:59.10ms
step:649/2330 train_time:38356ms step_avg:59.10ms
step:650/2330 train_time:38417ms step_avg:59.10ms
step:651/2330 train_time:38473ms step_avg:59.10ms
step:652/2330 train_time:38535ms step_avg:59.10ms
step:653/2330 train_time:38591ms step_avg:59.10ms
step:654/2330 train_time:38652ms step_avg:59.10ms
step:655/2330 train_time:38709ms step_avg:59.10ms
step:656/2330 train_time:38770ms step_avg:59.10ms
step:657/2330 train_time:38827ms step_avg:59.10ms
step:658/2330 train_time:38889ms step_avg:59.10ms
step:659/2330 train_time:38946ms step_avg:59.10ms
step:660/2330 train_time:39009ms step_avg:59.10ms
step:661/2330 train_time:39065ms step_avg:59.10ms
step:662/2330 train_time:39127ms step_avg:59.10ms
step:663/2330 train_time:39183ms step_avg:59.10ms
step:664/2330 train_time:39245ms step_avg:59.10ms
step:665/2330 train_time:39303ms step_avg:59.10ms
step:666/2330 train_time:39364ms step_avg:59.11ms
step:667/2330 train_time:39421ms step_avg:59.10ms
step:668/2330 train_time:39483ms step_avg:59.11ms
step:669/2330 train_time:39540ms step_avg:59.10ms
step:670/2330 train_time:39601ms step_avg:59.11ms
step:671/2330 train_time:39658ms step_avg:59.10ms
step:672/2330 train_time:39719ms step_avg:59.11ms
step:673/2330 train_time:39775ms step_avg:59.10ms
step:674/2330 train_time:39836ms step_avg:59.10ms
step:675/2330 train_time:39893ms step_avg:59.10ms
step:676/2330 train_time:39954ms step_avg:59.10ms
step:677/2330 train_time:40010ms step_avg:59.10ms
step:678/2330 train_time:40072ms step_avg:59.10ms
step:679/2330 train_time:40128ms step_avg:59.10ms
step:680/2330 train_time:40191ms step_avg:59.10ms
step:681/2330 train_time:40247ms step_avg:59.10ms
step:682/2330 train_time:40309ms step_avg:59.10ms
step:683/2330 train_time:40367ms step_avg:59.10ms
step:684/2330 train_time:40428ms step_avg:59.10ms
step:685/2330 train_time:40485ms step_avg:59.10ms
step:686/2330 train_time:40547ms step_avg:59.11ms
step:687/2330 train_time:40604ms step_avg:59.10ms
step:688/2330 train_time:40667ms step_avg:59.11ms
step:689/2330 train_time:40724ms step_avg:59.11ms
step:690/2330 train_time:40784ms step_avg:59.11ms
step:691/2330 train_time:40841ms step_avg:59.10ms
step:692/2330 train_time:40904ms step_avg:59.11ms
step:693/2330 train_time:40960ms step_avg:59.11ms
step:694/2330 train_time:41022ms step_avg:59.11ms
step:695/2330 train_time:41078ms step_avg:59.10ms
step:696/2330 train_time:41140ms step_avg:59.11ms
step:697/2330 train_time:41196ms step_avg:59.10ms
step:698/2330 train_time:41258ms step_avg:59.11ms
step:699/2330 train_time:41314ms step_avg:59.10ms
step:700/2330 train_time:41376ms step_avg:59.11ms
step:701/2330 train_time:41432ms step_avg:59.10ms
step:702/2330 train_time:41493ms step_avg:59.11ms
step:703/2330 train_time:41550ms step_avg:59.10ms
step:704/2330 train_time:41612ms step_avg:59.11ms
step:705/2330 train_time:41668ms step_avg:59.10ms
step:706/2330 train_time:41730ms step_avg:59.11ms
step:707/2330 train_time:41786ms step_avg:59.10ms
step:708/2330 train_time:41849ms step_avg:59.11ms
step:709/2330 train_time:41906ms step_avg:59.11ms
step:710/2330 train_time:41967ms step_avg:59.11ms
step:711/2330 train_time:42024ms step_avg:59.11ms
step:712/2330 train_time:42086ms step_avg:59.11ms
step:713/2330 train_time:42144ms step_avg:59.11ms
step:714/2330 train_time:42204ms step_avg:59.11ms
step:715/2330 train_time:42261ms step_avg:59.11ms
step:716/2330 train_time:42323ms step_avg:59.11ms
step:717/2330 train_time:42379ms step_avg:59.11ms
step:718/2330 train_time:42441ms step_avg:59.11ms
step:719/2330 train_time:42498ms step_avg:59.11ms
step:720/2330 train_time:42560ms step_avg:59.11ms
step:721/2330 train_time:42616ms step_avg:59.11ms
step:722/2330 train_time:42678ms step_avg:59.11ms
step:723/2330 train_time:42733ms step_avg:59.11ms
step:724/2330 train_time:42795ms step_avg:59.11ms
step:725/2330 train_time:42851ms step_avg:59.10ms
step:726/2330 train_time:42912ms step_avg:59.11ms
step:727/2330 train_time:42968ms step_avg:59.10ms
step:728/2330 train_time:43029ms step_avg:59.11ms
step:729/2330 train_time:43086ms step_avg:59.10ms
step:730/2330 train_time:43148ms step_avg:59.11ms
step:731/2330 train_time:43204ms step_avg:59.10ms
step:732/2330 train_time:43267ms step_avg:59.11ms
step:733/2330 train_time:43324ms step_avg:59.11ms
step:734/2330 train_time:43386ms step_avg:59.11ms
step:735/2330 train_time:43443ms step_avg:59.11ms
step:736/2330 train_time:43504ms step_avg:59.11ms
step:737/2330 train_time:43562ms step_avg:59.11ms
step:738/2330 train_time:43623ms step_avg:59.11ms
step:739/2330 train_time:43680ms step_avg:59.11ms
step:740/2330 train_time:43742ms step_avg:59.11ms
step:741/2330 train_time:43798ms step_avg:59.11ms
step:742/2330 train_time:43860ms step_avg:59.11ms
step:743/2330 train_time:43916ms step_avg:59.11ms
step:744/2330 train_time:43978ms step_avg:59.11ms
step:745/2330 train_time:44034ms step_avg:59.11ms
step:746/2330 train_time:44095ms step_avg:59.11ms
step:747/2330 train_time:44151ms step_avg:59.10ms
step:748/2330 train_time:44213ms step_avg:59.11ms
step:749/2330 train_time:44269ms step_avg:59.10ms
step:750/2330 train_time:44331ms step_avg:59.11ms
step:750/2330 val_loss:4.2545 train_time:44409ms step_avg:59.21ms
step:751/2330 train_time:44428ms step_avg:59.16ms
step:752/2330 train_time:44451ms step_avg:59.11ms
step:753/2330 train_time:44509ms step_avg:59.11ms
step:754/2330 train_time:44575ms step_avg:59.12ms
step:755/2330 train_time:44632ms step_avg:59.12ms
step:756/2330 train_time:44695ms step_avg:59.12ms
step:757/2330 train_time:44752ms step_avg:59.12ms
step:758/2330 train_time:44812ms step_avg:59.12ms
step:759/2330 train_time:44868ms step_avg:59.11ms
step:760/2330 train_time:44928ms step_avg:59.12ms
step:761/2330 train_time:44984ms step_avg:59.11ms
step:762/2330 train_time:45045ms step_avg:59.11ms
step:763/2330 train_time:45101ms step_avg:59.11ms
step:764/2330 train_time:45162ms step_avg:59.11ms
step:765/2330 train_time:45218ms step_avg:59.11ms
step:766/2330 train_time:45279ms step_avg:59.11ms
step:767/2330 train_time:45336ms step_avg:59.11ms
step:768/2330 train_time:45398ms step_avg:59.11ms
step:769/2330 train_time:45456ms step_avg:59.11ms
step:770/2330 train_time:45521ms step_avg:59.12ms
step:771/2330 train_time:45579ms step_avg:59.12ms
step:772/2330 train_time:45644ms step_avg:59.12ms
step:773/2330 train_time:45701ms step_avg:59.12ms
step:774/2330 train_time:45764ms step_avg:59.13ms
step:775/2330 train_time:45822ms step_avg:59.12ms
step:776/2330 train_time:45884ms step_avg:59.13ms
step:777/2330 train_time:45941ms step_avg:59.13ms
step:778/2330 train_time:46002ms step_avg:59.13ms
step:779/2330 train_time:46059ms step_avg:59.13ms
step:780/2330 train_time:46120ms step_avg:59.13ms
step:781/2330 train_time:46177ms step_avg:59.13ms
step:782/2330 train_time:46239ms step_avg:59.13ms
step:783/2330 train_time:46297ms step_avg:59.13ms
step:784/2330 train_time:46357ms step_avg:59.13ms
step:785/2330 train_time:46415ms step_avg:59.13ms
step:786/2330 train_time:46478ms step_avg:59.13ms
step:787/2330 train_time:46535ms step_avg:59.13ms
step:788/2330 train_time:46599ms step_avg:59.14ms
step:789/2330 train_time:46656ms step_avg:59.13ms
step:790/2330 train_time:46722ms step_avg:59.14ms
step:791/2330 train_time:46779ms step_avg:59.14ms
step:792/2330 train_time:46842ms step_avg:59.14ms
step:793/2330 train_time:46899ms step_avg:59.14ms
step:794/2330 train_time:46961ms step_avg:59.14ms
step:795/2330 train_time:47018ms step_avg:59.14ms
step:796/2330 train_time:47079ms step_avg:59.14ms
step:797/2330 train_time:47135ms step_avg:59.14ms
step:798/2330 train_time:47197ms step_avg:59.14ms
step:799/2330 train_time:47254ms step_avg:59.14ms
step:800/2330 train_time:47315ms step_avg:59.14ms
step:801/2330 train_time:47372ms step_avg:59.14ms
step:802/2330 train_time:47434ms step_avg:59.14ms
step:803/2330 train_time:47491ms step_avg:59.14ms
step:804/2330 train_time:47553ms step_avg:59.15ms
step:805/2330 train_time:47610ms step_avg:59.14ms
step:806/2330 train_time:47673ms step_avg:59.15ms
step:807/2330 train_time:47730ms step_avg:59.14ms
step:808/2330 train_time:47793ms step_avg:59.15ms
step:809/2330 train_time:47849ms step_avg:59.15ms
step:810/2330 train_time:47912ms step_avg:59.15ms
step:811/2330 train_time:47969ms step_avg:59.15ms
step:812/2330 train_time:48030ms step_avg:59.15ms
step:813/2330 train_time:48087ms step_avg:59.15ms
step:814/2330 train_time:48151ms step_avg:59.15ms
step:815/2330 train_time:48207ms step_avg:59.15ms
step:816/2330 train_time:48269ms step_avg:59.15ms
step:817/2330 train_time:48325ms step_avg:59.15ms
step:818/2330 train_time:48388ms step_avg:59.15ms
step:819/2330 train_time:48446ms step_avg:59.15ms
step:820/2330 train_time:48509ms step_avg:59.16ms
step:821/2330 train_time:48566ms step_avg:59.16ms
step:822/2330 train_time:48628ms step_avg:59.16ms
step:823/2330 train_time:48686ms step_avg:59.16ms
step:824/2330 train_time:48748ms step_avg:59.16ms
step:825/2330 train_time:48805ms step_avg:59.16ms
step:826/2330 train_time:48868ms step_avg:59.16ms
step:827/2330 train_time:48925ms step_avg:59.16ms
step:828/2330 train_time:48987ms step_avg:59.16ms
step:829/2330 train_time:49045ms step_avg:59.16ms
step:830/2330 train_time:49106ms step_avg:59.16ms
step:831/2330 train_time:49164ms step_avg:59.16ms
step:832/2330 train_time:49226ms step_avg:59.17ms
step:833/2330 train_time:49282ms step_avg:59.16ms
step:834/2330 train_time:49345ms step_avg:59.17ms
step:835/2330 train_time:49403ms step_avg:59.16ms
step:836/2330 train_time:49465ms step_avg:59.17ms
step:837/2330 train_time:49522ms step_avg:59.17ms
step:838/2330 train_time:49585ms step_avg:59.17ms
step:839/2330 train_time:49643ms step_avg:59.17ms
step:840/2330 train_time:49705ms step_avg:59.17ms
step:841/2330 train_time:49762ms step_avg:59.17ms
step:842/2330 train_time:49826ms step_avg:59.18ms
step:843/2330 train_time:49883ms step_avg:59.17ms
step:844/2330 train_time:49946ms step_avg:59.18ms
step:845/2330 train_time:50003ms step_avg:59.18ms
step:846/2330 train_time:50066ms step_avg:59.18ms
step:847/2330 train_time:50123ms step_avg:59.18ms
step:848/2330 train_time:50186ms step_avg:59.18ms
step:849/2330 train_time:50242ms step_avg:59.18ms
step:850/2330 train_time:50305ms step_avg:59.18ms
step:851/2330 train_time:50362ms step_avg:59.18ms
step:852/2330 train_time:50424ms step_avg:59.18ms
step:853/2330 train_time:50482ms step_avg:59.18ms
step:854/2330 train_time:50544ms step_avg:59.18ms
step:855/2330 train_time:50601ms step_avg:59.18ms
step:856/2330 train_time:50665ms step_avg:59.19ms
step:857/2330 train_time:50722ms step_avg:59.19ms
step:858/2330 train_time:50785ms step_avg:59.19ms
step:859/2330 train_time:50842ms step_avg:59.19ms
step:860/2330 train_time:50904ms step_avg:59.19ms
step:861/2330 train_time:50962ms step_avg:59.19ms
step:862/2330 train_time:51024ms step_avg:59.19ms
step:863/2330 train_time:51081ms step_avg:59.19ms
step:864/2330 train_time:51143ms step_avg:59.19ms
step:865/2330 train_time:51200ms step_avg:59.19ms
step:866/2330 train_time:51262ms step_avg:59.19ms
step:867/2330 train_time:51319ms step_avg:59.19ms
step:868/2330 train_time:51382ms step_avg:59.20ms
step:869/2330 train_time:51440ms step_avg:59.19ms
step:870/2330 train_time:51502ms step_avg:59.20ms
step:871/2330 train_time:51559ms step_avg:59.19ms
step:872/2330 train_time:51621ms step_avg:59.20ms
step:873/2330 train_time:51678ms step_avg:59.20ms
step:874/2330 train_time:51741ms step_avg:59.20ms
step:875/2330 train_time:51798ms step_avg:59.20ms
step:876/2330 train_time:51861ms step_avg:59.20ms
step:877/2330 train_time:51918ms step_avg:59.20ms
step:878/2330 train_time:51980ms step_avg:59.20ms
step:879/2330 train_time:52038ms step_avg:59.20ms
step:880/2330 train_time:52100ms step_avg:59.20ms
step:881/2330 train_time:52157ms step_avg:59.20ms
step:882/2330 train_time:52219ms step_avg:59.21ms
step:883/2330 train_time:52276ms step_avg:59.20ms
step:884/2330 train_time:52338ms step_avg:59.21ms
step:885/2330 train_time:52396ms step_avg:59.20ms
step:886/2330 train_time:52458ms step_avg:59.21ms
step:887/2330 train_time:52515ms step_avg:59.21ms
step:888/2330 train_time:52578ms step_avg:59.21ms
step:889/2330 train_time:52635ms step_avg:59.21ms
step:890/2330 train_time:52697ms step_avg:59.21ms
step:891/2330 train_time:52754ms step_avg:59.21ms
step:892/2330 train_time:52817ms step_avg:59.21ms
step:893/2330 train_time:52874ms step_avg:59.21ms
step:894/2330 train_time:52936ms step_avg:59.21ms
step:895/2330 train_time:52993ms step_avg:59.21ms
step:896/2330 train_time:53056ms step_avg:59.21ms
step:897/2330 train_time:53112ms step_avg:59.21ms
step:898/2330 train_time:53174ms step_avg:59.21ms
step:899/2330 train_time:53231ms step_avg:59.21ms
step:900/2330 train_time:53292ms step_avg:59.21ms
step:901/2330 train_time:53349ms step_avg:59.21ms
step:902/2330 train_time:53411ms step_avg:59.21ms
step:903/2330 train_time:53468ms step_avg:59.21ms
step:904/2330 train_time:53531ms step_avg:59.22ms
step:905/2330 train_time:53588ms step_avg:59.21ms
step:906/2330 train_time:53650ms step_avg:59.22ms
step:907/2330 train_time:53707ms step_avg:59.21ms
step:908/2330 train_time:53769ms step_avg:59.22ms
step:909/2330 train_time:53826ms step_avg:59.21ms
step:910/2330 train_time:53888ms step_avg:59.22ms
step:911/2330 train_time:53946ms step_avg:59.22ms
step:912/2330 train_time:54008ms step_avg:59.22ms
step:913/2330 train_time:54066ms step_avg:59.22ms
step:914/2330 train_time:54128ms step_avg:59.22ms
step:915/2330 train_time:54185ms step_avg:59.22ms
step:916/2330 train_time:54247ms step_avg:59.22ms
step:917/2330 train_time:54304ms step_avg:59.22ms
step:918/2330 train_time:54366ms step_avg:59.22ms
step:919/2330 train_time:54424ms step_avg:59.22ms
step:920/2330 train_time:54486ms step_avg:59.22ms
step:921/2330 train_time:54543ms step_avg:59.22ms
step:922/2330 train_time:54606ms step_avg:59.23ms
step:923/2330 train_time:54664ms step_avg:59.22ms
step:924/2330 train_time:54725ms step_avg:59.23ms
step:925/2330 train_time:54783ms step_avg:59.22ms
step:926/2330 train_time:54845ms step_avg:59.23ms
step:927/2330 train_time:54902ms step_avg:59.23ms
step:928/2330 train_time:54964ms step_avg:59.23ms
step:929/2330 train_time:55021ms step_avg:59.23ms
step:930/2330 train_time:55083ms step_avg:59.23ms
step:931/2330 train_time:55141ms step_avg:59.23ms
step:932/2330 train_time:55204ms step_avg:59.23ms
step:933/2330 train_time:55261ms step_avg:59.23ms
step:934/2330 train_time:55323ms step_avg:59.23ms
step:935/2330 train_time:55381ms step_avg:59.23ms
step:936/2330 train_time:55442ms step_avg:59.23ms
step:937/2330 train_time:55500ms step_avg:59.23ms
step:938/2330 train_time:55562ms step_avg:59.23ms
step:939/2330 train_time:55620ms step_avg:59.23ms
step:940/2330 train_time:55682ms step_avg:59.24ms
step:941/2330 train_time:55739ms step_avg:59.23ms
step:942/2330 train_time:55802ms step_avg:59.24ms
step:943/2330 train_time:55860ms step_avg:59.24ms
step:944/2330 train_time:55922ms step_avg:59.24ms
step:945/2330 train_time:55980ms step_avg:59.24ms
step:946/2330 train_time:56041ms step_avg:59.24ms
step:947/2330 train_time:56099ms step_avg:59.24ms
step:948/2330 train_time:56161ms step_avg:59.24ms
step:949/2330 train_time:56218ms step_avg:59.24ms
step:950/2330 train_time:56280ms step_avg:59.24ms
step:951/2330 train_time:56337ms step_avg:59.24ms
step:952/2330 train_time:56400ms step_avg:59.24ms
step:953/2330 train_time:56457ms step_avg:59.24ms
step:954/2330 train_time:56520ms step_avg:59.25ms
step:955/2330 train_time:56578ms step_avg:59.24ms
step:956/2330 train_time:56640ms step_avg:59.25ms
step:957/2330 train_time:56698ms step_avg:59.25ms
step:958/2330 train_time:56761ms step_avg:59.25ms
step:959/2330 train_time:56817ms step_avg:59.25ms
step:960/2330 train_time:56881ms step_avg:59.25ms
step:961/2330 train_time:56937ms step_avg:59.25ms
step:962/2330 train_time:57000ms step_avg:59.25ms
step:963/2330 train_time:57057ms step_avg:59.25ms
step:964/2330 train_time:57119ms step_avg:59.25ms
step:965/2330 train_time:57176ms step_avg:59.25ms
step:966/2330 train_time:57239ms step_avg:59.25ms
step:967/2330 train_time:57296ms step_avg:59.25ms
step:968/2330 train_time:57360ms step_avg:59.26ms
step:969/2330 train_time:57416ms step_avg:59.25ms
step:970/2330 train_time:57479ms step_avg:59.26ms
step:971/2330 train_time:57535ms step_avg:59.25ms
step:972/2330 train_time:57599ms step_avg:59.26ms
step:973/2330 train_time:57656ms step_avg:59.26ms
step:974/2330 train_time:57720ms step_avg:59.26ms
step:975/2330 train_time:57777ms step_avg:59.26ms
step:976/2330 train_time:57840ms step_avg:59.26ms
step:977/2330 train_time:57897ms step_avg:59.26ms
step:978/2330 train_time:57960ms step_avg:59.26ms
step:979/2330 train_time:58017ms step_avg:59.26ms
step:980/2330 train_time:58080ms step_avg:59.26ms
step:981/2330 train_time:58136ms step_avg:59.26ms
step:982/2330 train_time:58200ms step_avg:59.27ms
step:983/2330 train_time:58256ms step_avg:59.26ms
step:984/2330 train_time:58320ms step_avg:59.27ms
step:985/2330 train_time:58377ms step_avg:59.27ms
step:986/2330 train_time:58440ms step_avg:59.27ms
step:987/2330 train_time:58497ms step_avg:59.27ms
step:988/2330 train_time:58560ms step_avg:59.27ms
step:989/2330 train_time:58617ms step_avg:59.27ms
step:990/2330 train_time:58679ms step_avg:59.27ms
step:991/2330 train_time:58736ms step_avg:59.27ms
step:992/2330 train_time:58798ms step_avg:59.27ms
step:993/2330 train_time:58856ms step_avg:59.27ms
step:994/2330 train_time:58918ms step_avg:59.27ms
step:995/2330 train_time:58975ms step_avg:59.27ms
step:996/2330 train_time:59038ms step_avg:59.27ms
step:997/2330 train_time:59095ms step_avg:59.27ms
step:998/2330 train_time:59157ms step_avg:59.28ms
step:999/2330 train_time:59214ms step_avg:59.27ms
step:1000/2330 train_time:59276ms step_avg:59.28ms
step:1000/2330 val_loss:4.1108 train_time:59356ms step_avg:59.36ms
step:1001/2330 train_time:59375ms step_avg:59.32ms
step:1002/2330 train_time:59398ms step_avg:59.28ms
step:1003/2330 train_time:59454ms step_avg:59.28ms
step:1004/2330 train_time:59521ms step_avg:59.28ms
step:1005/2330 train_time:59578ms step_avg:59.28ms
step:1006/2330 train_time:59643ms step_avg:59.29ms
step:1007/2330 train_time:59700ms step_avg:59.29ms
step:1008/2330 train_time:59764ms step_avg:59.29ms
step:1009/2330 train_time:59820ms step_avg:59.29ms
step:1010/2330 train_time:59883ms step_avg:59.29ms
step:1011/2330 train_time:59940ms step_avg:59.29ms
step:1012/2330 train_time:60003ms step_avg:59.29ms
step:1013/2330 train_time:60060ms step_avg:59.29ms
step:1014/2330 train_time:60121ms step_avg:59.29ms
step:1015/2330 train_time:60178ms step_avg:59.29ms
step:1016/2330 train_time:60239ms step_avg:59.29ms
step:1017/2330 train_time:60297ms step_avg:59.29ms
step:1018/2330 train_time:60360ms step_avg:59.29ms
step:1019/2330 train_time:60418ms step_avg:59.29ms
step:1020/2330 train_time:60481ms step_avg:59.29ms
step:1021/2330 train_time:60539ms step_avg:59.29ms
step:1022/2330 train_time:60603ms step_avg:59.30ms
step:1023/2330 train_time:60660ms step_avg:59.30ms
step:1024/2330 train_time:60723ms step_avg:59.30ms
step:1025/2330 train_time:60779ms step_avg:59.30ms
step:1026/2330 train_time:60843ms step_avg:59.30ms
step:1027/2330 train_time:60899ms step_avg:59.30ms
step:1028/2330 train_time:60962ms step_avg:59.30ms
step:1029/2330 train_time:61019ms step_avg:59.30ms
step:1030/2330 train_time:61080ms step_avg:59.30ms
step:1031/2330 train_time:61137ms step_avg:59.30ms
step:1032/2330 train_time:61198ms step_avg:59.30ms
step:1033/2330 train_time:61256ms step_avg:59.30ms
step:1034/2330 train_time:61317ms step_avg:59.30ms
step:1035/2330 train_time:61375ms step_avg:59.30ms
step:1036/2330 train_time:61438ms step_avg:59.30ms
step:1037/2330 train_time:61495ms step_avg:59.30ms
step:1038/2330 train_time:61557ms step_avg:59.30ms
step:1039/2330 train_time:61614ms step_avg:59.30ms
step:1040/2330 train_time:61676ms step_avg:59.30ms
step:1041/2330 train_time:61734ms step_avg:59.30ms
step:1042/2330 train_time:61796ms step_avg:59.31ms
step:1043/2330 train_time:61852ms step_avg:59.30ms
step:1044/2330 train_time:61915ms step_avg:59.31ms
step:1045/2330 train_time:61972ms step_avg:59.30ms
step:1046/2330 train_time:62034ms step_avg:59.31ms
step:1047/2330 train_time:62091ms step_avg:59.30ms
step:1048/2330 train_time:62153ms step_avg:59.31ms
step:1049/2330 train_time:62210ms step_avg:59.30ms
step:1050/2330 train_time:62272ms step_avg:59.31ms
step:1051/2330 train_time:62330ms step_avg:59.31ms
step:1052/2330 train_time:62393ms step_avg:59.31ms
step:1053/2330 train_time:62451ms step_avg:59.31ms
step:1054/2330 train_time:62513ms step_avg:59.31ms
step:1055/2330 train_time:62570ms step_avg:59.31ms
step:1056/2330 train_time:62633ms step_avg:59.31ms
step:1057/2330 train_time:62690ms step_avg:59.31ms
step:1058/2330 train_time:62753ms step_avg:59.31ms
step:1059/2330 train_time:62811ms step_avg:59.31ms
step:1060/2330 train_time:62874ms step_avg:59.31ms
step:1061/2330 train_time:62931ms step_avg:59.31ms
step:1062/2330 train_time:62993ms step_avg:59.32ms
step:1063/2330 train_time:63050ms step_avg:59.31ms
step:1064/2330 train_time:63112ms step_avg:59.32ms
step:1065/2330 train_time:63170ms step_avg:59.31ms
step:1066/2330 train_time:63232ms step_avg:59.32ms
step:1067/2330 train_time:63290ms step_avg:59.32ms
step:1068/2330 train_time:63352ms step_avg:59.32ms
step:1069/2330 train_time:63410ms step_avg:59.32ms
step:1070/2330 train_time:63471ms step_avg:59.32ms
step:1071/2330 train_time:63529ms step_avg:59.32ms
step:1072/2330 train_time:63592ms step_avg:59.32ms
step:1073/2330 train_time:63650ms step_avg:59.32ms
step:1074/2330 train_time:63712ms step_avg:59.32ms
step:1075/2330 train_time:63770ms step_avg:59.32ms
step:1076/2330 train_time:63832ms step_avg:59.32ms
step:1077/2330 train_time:63889ms step_avg:59.32ms
step:1078/2330 train_time:63951ms step_avg:59.32ms
step:1079/2330 train_time:64008ms step_avg:59.32ms
step:1080/2330 train_time:64071ms step_avg:59.32ms
step:1081/2330 train_time:64127ms step_avg:59.32ms
step:1082/2330 train_time:64190ms step_avg:59.33ms
step:1083/2330 train_time:64248ms step_avg:59.32ms
step:1084/2330 train_time:64310ms step_avg:59.33ms
step:1085/2330 train_time:64368ms step_avg:59.33ms
step:1086/2330 train_time:64430ms step_avg:59.33ms
step:1087/2330 train_time:64489ms step_avg:59.33ms
step:1088/2330 train_time:64551ms step_avg:59.33ms
step:1089/2330 train_time:64609ms step_avg:59.33ms
step:1090/2330 train_time:64671ms step_avg:59.33ms
step:1091/2330 train_time:64728ms step_avg:59.33ms
step:1092/2330 train_time:64791ms step_avg:59.33ms
step:1093/2330 train_time:64849ms step_avg:59.33ms
step:1094/2330 train_time:64911ms step_avg:59.33ms
step:1095/2330 train_time:64968ms step_avg:59.33ms
step:1096/2330 train_time:65031ms step_avg:59.33ms
step:1097/2330 train_time:65088ms step_avg:59.33ms
step:1098/2330 train_time:65150ms step_avg:59.34ms
step:1099/2330 train_time:65208ms step_avg:59.33ms
step:1100/2330 train_time:65270ms step_avg:59.34ms
step:1101/2330 train_time:65328ms step_avg:59.33ms
step:1102/2330 train_time:65391ms step_avg:59.34ms
step:1103/2330 train_time:65449ms step_avg:59.34ms
step:1104/2330 train_time:65511ms step_avg:59.34ms
step:1105/2330 train_time:65568ms step_avg:59.34ms
step:1106/2330 train_time:65631ms step_avg:59.34ms
step:1107/2330 train_time:65689ms step_avg:59.34ms
step:1108/2330 train_time:65752ms step_avg:59.34ms
step:1109/2330 train_time:65809ms step_avg:59.34ms
step:1110/2330 train_time:65871ms step_avg:59.34ms
step:1111/2330 train_time:65929ms step_avg:59.34ms
step:1112/2330 train_time:65992ms step_avg:59.34ms
step:1113/2330 train_time:66048ms step_avg:59.34ms
step:1114/2330 train_time:66112ms step_avg:59.35ms
step:1115/2330 train_time:66170ms step_avg:59.34ms
step:1116/2330 train_time:66231ms step_avg:59.35ms
step:1117/2330 train_time:66289ms step_avg:59.35ms
step:1118/2330 train_time:66351ms step_avg:59.35ms
step:1119/2330 train_time:66409ms step_avg:59.35ms
step:1120/2330 train_time:66471ms step_avg:59.35ms
step:1121/2330 train_time:66528ms step_avg:59.35ms
step:1122/2330 train_time:66592ms step_avg:59.35ms
step:1123/2330 train_time:66649ms step_avg:59.35ms
step:1124/2330 train_time:66712ms step_avg:59.35ms
step:1125/2330 train_time:66769ms step_avg:59.35ms
step:1126/2330 train_time:66832ms step_avg:59.35ms
step:1127/2330 train_time:66889ms step_avg:59.35ms
step:1128/2330 train_time:66952ms step_avg:59.35ms
step:1129/2330 train_time:67009ms step_avg:59.35ms
step:1130/2330 train_time:67071ms step_avg:59.36ms
step:1131/2330 train_time:67129ms step_avg:59.35ms
step:1132/2330 train_time:67192ms step_avg:59.36ms
step:1133/2330 train_time:67250ms step_avg:59.36ms
step:1134/2330 train_time:67312ms step_avg:59.36ms
step:1135/2330 train_time:67369ms step_avg:59.36ms
step:1136/2330 train_time:67431ms step_avg:59.36ms
step:1137/2330 train_time:67489ms step_avg:59.36ms
step:1138/2330 train_time:67551ms step_avg:59.36ms
step:1139/2330 train_time:67608ms step_avg:59.36ms
step:1140/2330 train_time:67671ms step_avg:59.36ms
step:1141/2330 train_time:67729ms step_avg:59.36ms
step:1142/2330 train_time:67792ms step_avg:59.36ms
step:1143/2330 train_time:67850ms step_avg:59.36ms
step:1144/2330 train_time:67911ms step_avg:59.36ms
step:1145/2330 train_time:67969ms step_avg:59.36ms
step:1146/2330 train_time:68031ms step_avg:59.36ms
step:1147/2330 train_time:68089ms step_avg:59.36ms
step:1148/2330 train_time:68151ms step_avg:59.37ms
step:1149/2330 train_time:68209ms step_avg:59.36ms
step:1150/2330 train_time:68271ms step_avg:59.37ms
step:1151/2330 train_time:68329ms step_avg:59.36ms
step:1152/2330 train_time:68392ms step_avg:59.37ms
step:1153/2330 train_time:68449ms step_avg:59.37ms
step:1154/2330 train_time:68512ms step_avg:59.37ms
step:1155/2330 train_time:68568ms step_avg:59.37ms
step:1156/2330 train_time:68632ms step_avg:59.37ms
step:1157/2330 train_time:68690ms step_avg:59.37ms
step:1158/2330 train_time:68751ms step_avg:59.37ms
step:1159/2330 train_time:68809ms step_avg:59.37ms
step:1160/2330 train_time:68871ms step_avg:59.37ms
step:1161/2330 train_time:68929ms step_avg:59.37ms
step:1162/2330 train_time:68992ms step_avg:59.37ms
step:1163/2330 train_time:69050ms step_avg:59.37ms
step:1164/2330 train_time:69112ms step_avg:59.37ms
step:1165/2330 train_time:69169ms step_avg:59.37ms
step:1166/2330 train_time:69232ms step_avg:59.38ms
step:1167/2330 train_time:69290ms step_avg:59.37ms
step:1168/2330 train_time:69352ms step_avg:59.38ms
step:1169/2330 train_time:69409ms step_avg:59.37ms
step:1170/2330 train_time:69472ms step_avg:59.38ms
step:1171/2330 train_time:69529ms step_avg:59.38ms
step:1172/2330 train_time:69592ms step_avg:59.38ms
step:1173/2330 train_time:69649ms step_avg:59.38ms
step:1174/2330 train_time:69712ms step_avg:59.38ms
step:1175/2330 train_time:69770ms step_avg:59.38ms
step:1176/2330 train_time:69831ms step_avg:59.38ms
step:1177/2330 train_time:69889ms step_avg:59.38ms
step:1178/2330 train_time:69952ms step_avg:59.38ms
step:1179/2330 train_time:70009ms step_avg:59.38ms
step:1180/2330 train_time:70071ms step_avg:59.38ms
step:1181/2330 train_time:70128ms step_avg:59.38ms
step:1182/2330 train_time:70192ms step_avg:59.38ms
step:1183/2330 train_time:70249ms step_avg:59.38ms
step:1184/2330 train_time:70312ms step_avg:59.39ms
step:1185/2330 train_time:70369ms step_avg:59.38ms
step:1186/2330 train_time:70431ms step_avg:59.39ms
step:1187/2330 train_time:70489ms step_avg:59.38ms
step:1188/2330 train_time:70551ms step_avg:59.39ms
step:1189/2330 train_time:70609ms step_avg:59.38ms
step:1190/2330 train_time:70671ms step_avg:59.39ms
step:1191/2330 train_time:70728ms step_avg:59.39ms
step:1192/2330 train_time:70791ms step_avg:59.39ms
step:1193/2330 train_time:70848ms step_avg:59.39ms
step:1194/2330 train_time:70911ms step_avg:59.39ms
step:1195/2330 train_time:70969ms step_avg:59.39ms
step:1196/2330 train_time:71030ms step_avg:59.39ms
step:1197/2330 train_time:71088ms step_avg:59.39ms
step:1198/2330 train_time:71150ms step_avg:59.39ms
step:1199/2330 train_time:71208ms step_avg:59.39ms
step:1200/2330 train_time:71270ms step_avg:59.39ms
step:1201/2330 train_time:71328ms step_avg:59.39ms
step:1202/2330 train_time:71390ms step_avg:59.39ms
step:1203/2330 train_time:71448ms step_avg:59.39ms
step:1204/2330 train_time:71510ms step_avg:59.39ms
step:1205/2330 train_time:71567ms step_avg:59.39ms
step:1206/2330 train_time:71630ms step_avg:59.39ms
step:1207/2330 train_time:71688ms step_avg:59.39ms
step:1208/2330 train_time:71750ms step_avg:59.40ms
step:1209/2330 train_time:71808ms step_avg:59.39ms
step:1210/2330 train_time:71870ms step_avg:59.40ms
step:1211/2330 train_time:71927ms step_avg:59.39ms
step:1212/2330 train_time:71991ms step_avg:59.40ms
step:1213/2330 train_time:72048ms step_avg:59.40ms
step:1214/2330 train_time:72112ms step_avg:59.40ms
step:1215/2330 train_time:72169ms step_avg:59.40ms
step:1216/2330 train_time:72232ms step_avg:59.40ms
step:1217/2330 train_time:72290ms step_avg:59.40ms
step:1218/2330 train_time:72351ms step_avg:59.40ms
step:1219/2330 train_time:72409ms step_avg:59.40ms
step:1220/2330 train_time:72471ms step_avg:59.40ms
step:1221/2330 train_time:72528ms step_avg:59.40ms
step:1222/2330 train_time:72591ms step_avg:59.40ms
step:1223/2330 train_time:72649ms step_avg:59.40ms
step:1224/2330 train_time:72712ms step_avg:59.40ms
step:1225/2330 train_time:72769ms step_avg:59.40ms
step:1226/2330 train_time:72831ms step_avg:59.41ms
step:1227/2330 train_time:72889ms step_avg:59.40ms
step:1228/2330 train_time:72951ms step_avg:59.41ms
step:1229/2330 train_time:73008ms step_avg:59.40ms
step:1230/2330 train_time:73071ms step_avg:59.41ms
step:1231/2330 train_time:73128ms step_avg:59.41ms
step:1232/2330 train_time:73191ms step_avg:59.41ms
step:1233/2330 train_time:73249ms step_avg:59.41ms
step:1234/2330 train_time:73311ms step_avg:59.41ms
step:1235/2330 train_time:73369ms step_avg:59.41ms
step:1236/2330 train_time:73431ms step_avg:59.41ms
step:1237/2330 train_time:73488ms step_avg:59.41ms
step:1238/2330 train_time:73551ms step_avg:59.41ms
step:1239/2330 train_time:73609ms step_avg:59.41ms
step:1240/2330 train_time:73671ms step_avg:59.41ms
step:1241/2330 train_time:73728ms step_avg:59.41ms
step:1242/2330 train_time:73792ms step_avg:59.41ms
step:1243/2330 train_time:73849ms step_avg:59.41ms
step:1244/2330 train_time:73910ms step_avg:59.41ms
step:1245/2330 train_time:73967ms step_avg:59.41ms
step:1246/2330 train_time:74031ms step_avg:59.42ms
step:1247/2330 train_time:74089ms step_avg:59.41ms
step:1248/2330 train_time:74151ms step_avg:59.42ms
step:1249/2330 train_time:74208ms step_avg:59.41ms
step:1250/2330 train_time:74270ms step_avg:59.42ms
step:1250/2330 val_loss:4.0415 train_time:74351ms step_avg:59.48ms
step:1251/2330 train_time:74370ms step_avg:59.45ms
step:1252/2330 train_time:74393ms step_avg:59.42ms
step:1253/2330 train_time:74453ms step_avg:59.42ms
step:1254/2330 train_time:74521ms step_avg:59.43ms
step:1255/2330 train_time:74578ms step_avg:59.42ms
step:1256/2330 train_time:74641ms step_avg:59.43ms
step:1257/2330 train_time:74698ms step_avg:59.43ms
step:1258/2330 train_time:74760ms step_avg:59.43ms
step:1259/2330 train_time:74817ms step_avg:59.43ms
step:1260/2330 train_time:74879ms step_avg:59.43ms
step:1261/2330 train_time:74935ms step_avg:59.43ms
step:1262/2330 train_time:74997ms step_avg:59.43ms
step:1263/2330 train_time:75054ms step_avg:59.43ms
step:1264/2330 train_time:75116ms step_avg:59.43ms
step:1265/2330 train_time:75173ms step_avg:59.42ms
step:1266/2330 train_time:75233ms step_avg:59.43ms
step:1267/2330 train_time:75290ms step_avg:59.42ms
step:1268/2330 train_time:75355ms step_avg:59.43ms
step:1269/2330 train_time:75414ms step_avg:59.43ms
step:1270/2330 train_time:75479ms step_avg:59.43ms
step:1271/2330 train_time:75538ms step_avg:59.43ms
step:1272/2330 train_time:75599ms step_avg:59.43ms
step:1273/2330 train_time:75656ms step_avg:59.43ms
step:1274/2330 train_time:75719ms step_avg:59.43ms
step:1275/2330 train_time:75776ms step_avg:59.43ms
step:1276/2330 train_time:75838ms step_avg:59.43ms
step:1277/2330 train_time:75895ms step_avg:59.43ms
step:1278/2330 train_time:75957ms step_avg:59.43ms
step:1279/2330 train_time:76014ms step_avg:59.43ms
step:1280/2330 train_time:76076ms step_avg:59.43ms
step:1281/2330 train_time:76132ms step_avg:59.43ms
step:1282/2330 train_time:76194ms step_avg:59.43ms
step:1283/2330 train_time:76251ms step_avg:59.43ms
step:1284/2330 train_time:76314ms step_avg:59.43ms
step:1285/2330 train_time:76372ms step_avg:59.43ms
step:1286/2330 train_time:76436ms step_avg:59.44ms
step:1287/2330 train_time:76494ms step_avg:59.44ms
step:1288/2330 train_time:76558ms step_avg:59.44ms
step:1289/2330 train_time:76615ms step_avg:59.44ms
step:1290/2330 train_time:76678ms step_avg:59.44ms
step:1291/2330 train_time:76735ms step_avg:59.44ms
step:1292/2330 train_time:76798ms step_avg:59.44ms
step:1293/2330 train_time:76855ms step_avg:59.44ms
step:1294/2330 train_time:76917ms step_avg:59.44ms
step:1295/2330 train_time:76974ms step_avg:59.44ms
step:1296/2330 train_time:77036ms step_avg:59.44ms
step:1297/2330 train_time:77093ms step_avg:59.44ms
step:1298/2330 train_time:77155ms step_avg:59.44ms
step:1299/2330 train_time:77211ms step_avg:59.44ms
step:1300/2330 train_time:77274ms step_avg:59.44ms
step:1301/2330 train_time:77331ms step_avg:59.44ms
step:1302/2330 train_time:77394ms step_avg:59.44ms
step:1303/2330 train_time:77452ms step_avg:59.44ms
step:1304/2330 train_time:77516ms step_avg:59.44ms
step:1305/2330 train_time:77574ms step_avg:59.44ms
step:1306/2330 train_time:77637ms step_avg:59.45ms
step:1307/2330 train_time:77695ms step_avg:59.45ms
step:1308/2330 train_time:77757ms step_avg:59.45ms
step:1309/2330 train_time:77815ms step_avg:59.45ms
step:1310/2330 train_time:77877ms step_avg:59.45ms
step:1311/2330 train_time:77935ms step_avg:59.45ms
step:1312/2330 train_time:77996ms step_avg:59.45ms
step:1313/2330 train_time:78053ms step_avg:59.45ms
step:1314/2330 train_time:78116ms step_avg:59.45ms
step:1315/2330 train_time:78173ms step_avg:59.45ms
step:1316/2330 train_time:78235ms step_avg:59.45ms
step:1317/2330 train_time:78292ms step_avg:59.45ms
step:1318/2330 train_time:78356ms step_avg:59.45ms
step:1319/2330 train_time:78413ms step_avg:59.45ms
step:1320/2330 train_time:78476ms step_avg:59.45ms
step:1321/2330 train_time:78534ms step_avg:59.45ms
step:1322/2330 train_time:78597ms step_avg:59.45ms
step:1323/2330 train_time:78655ms step_avg:59.45ms
step:1324/2330 train_time:78717ms step_avg:59.45ms
step:1325/2330 train_time:78775ms step_avg:59.45ms
step:1326/2330 train_time:78837ms step_avg:59.45ms
step:1327/2330 train_time:78894ms step_avg:59.45ms
step:1328/2330 train_time:78957ms step_avg:59.46ms
step:1329/2330 train_time:79014ms step_avg:59.45ms
step:1330/2330 train_time:79077ms step_avg:59.46ms
step:1331/2330 train_time:79134ms step_avg:59.45ms
step:1332/2330 train_time:79196ms step_avg:59.46ms
step:1333/2330 train_time:79253ms step_avg:59.45ms
step:1334/2330 train_time:79316ms step_avg:59.46ms
step:1335/2330 train_time:79373ms step_avg:59.46ms
step:1336/2330 train_time:79437ms step_avg:59.46ms
step:1337/2330 train_time:79494ms step_avg:59.46ms
step:1338/2330 train_time:79557ms step_avg:59.46ms
step:1339/2330 train_time:79614ms step_avg:59.46ms
step:1340/2330 train_time:79677ms step_avg:59.46ms
step:1341/2330 train_time:79734ms step_avg:59.46ms
step:1342/2330 train_time:79797ms step_avg:59.46ms
step:1343/2330 train_time:79854ms step_avg:59.46ms
step:1344/2330 train_time:79918ms step_avg:59.46ms
step:1345/2330 train_time:79975ms step_avg:59.46ms
step:1346/2330 train_time:80037ms step_avg:59.46ms
step:1347/2330 train_time:80094ms step_avg:59.46ms
step:1348/2330 train_time:80156ms step_avg:59.46ms
step:1349/2330 train_time:80214ms step_avg:59.46ms
step:1350/2330 train_time:80276ms step_avg:59.46ms
step:1351/2330 train_time:80333ms step_avg:59.46ms
step:1352/2330 train_time:80396ms step_avg:59.46ms
step:1353/2330 train_time:80453ms step_avg:59.46ms
step:1354/2330 train_time:80517ms step_avg:59.47ms
step:1355/2330 train_time:80574ms step_avg:59.46ms
step:1356/2330 train_time:80637ms step_avg:59.47ms
step:1357/2330 train_time:80694ms step_avg:59.47ms
step:1358/2330 train_time:80756ms step_avg:59.47ms
step:1359/2330 train_time:80813ms step_avg:59.47ms
step:1360/2330 train_time:80876ms step_avg:59.47ms
step:1361/2330 train_time:80933ms step_avg:59.47ms
step:1362/2330 train_time:80996ms step_avg:59.47ms
step:1363/2330 train_time:81053ms step_avg:59.47ms
step:1364/2330 train_time:81116ms step_avg:59.47ms
step:1365/2330 train_time:81173ms step_avg:59.47ms
step:1366/2330 train_time:81236ms step_avg:59.47ms
step:1367/2330 train_time:81293ms step_avg:59.47ms
step:1368/2330 train_time:81356ms step_avg:59.47ms
step:1369/2330 train_time:81414ms step_avg:59.47ms
step:1370/2330 train_time:81477ms step_avg:59.47ms
step:1371/2330 train_time:81534ms step_avg:59.47ms
step:1372/2330 train_time:81597ms step_avg:59.47ms
step:1373/2330 train_time:81654ms step_avg:59.47ms
step:1374/2330 train_time:81717ms step_avg:59.47ms
step:1375/2330 train_time:81774ms step_avg:59.47ms
step:1376/2330 train_time:81836ms step_avg:59.47ms
step:1377/2330 train_time:81894ms step_avg:59.47ms
step:1378/2330 train_time:81956ms step_avg:59.47ms
step:1379/2330 train_time:82013ms step_avg:59.47ms
step:1380/2330 train_time:82076ms step_avg:59.48ms
step:1381/2330 train_time:82132ms step_avg:59.47ms
step:1382/2330 train_time:82195ms step_avg:59.48ms
step:1383/2330 train_time:82253ms step_avg:59.47ms
step:1384/2330 train_time:82315ms step_avg:59.48ms
step:1385/2330 train_time:82373ms step_avg:59.48ms
step:1386/2330 train_time:82435ms step_avg:59.48ms
step:1387/2330 train_time:82492ms step_avg:59.48ms
step:1388/2330 train_time:82555ms step_avg:59.48ms
step:1389/2330 train_time:82613ms step_avg:59.48ms
step:1390/2330 train_time:82676ms step_avg:59.48ms
step:1391/2330 train_time:82733ms step_avg:59.48ms
step:1392/2330 train_time:82795ms step_avg:59.48ms
step:1393/2330 train_time:82852ms step_avg:59.48ms
step:1394/2330 train_time:82916ms step_avg:59.48ms
step:1395/2330 train_time:82974ms step_avg:59.48ms
step:1396/2330 train_time:83037ms step_avg:59.48ms
step:1397/2330 train_time:83094ms step_avg:59.48ms
step:1398/2330 train_time:83157ms step_avg:59.48ms
step:1399/2330 train_time:83214ms step_avg:59.48ms
step:1400/2330 train_time:83278ms step_avg:59.48ms
step:1401/2330 train_time:83335ms step_avg:59.48ms
step:1402/2330 train_time:83396ms step_avg:59.48ms
step:1403/2330 train_time:83453ms step_avg:59.48ms
step:1404/2330 train_time:83517ms step_avg:59.49ms
step:1405/2330 train_time:83574ms step_avg:59.48ms
step:1406/2330 train_time:83637ms step_avg:59.49ms
step:1407/2330 train_time:83695ms step_avg:59.48ms
step:1408/2330 train_time:83757ms step_avg:59.49ms
step:1409/2330 train_time:83815ms step_avg:59.49ms
step:1410/2330 train_time:83877ms step_avg:59.49ms
step:1411/2330 train_time:83934ms step_avg:59.49ms
step:1412/2330 train_time:83996ms step_avg:59.49ms
step:1413/2330 train_time:84053ms step_avg:59.49ms
step:1414/2330 train_time:84116ms step_avg:59.49ms
step:1415/2330 train_time:84174ms step_avg:59.49ms
step:1416/2330 train_time:84236ms step_avg:59.49ms
step:1417/2330 train_time:84293ms step_avg:59.49ms
step:1418/2330 train_time:84355ms step_avg:59.49ms
step:1419/2330 train_time:84413ms step_avg:59.49ms
step:1420/2330 train_time:84476ms step_avg:59.49ms
step:1421/2330 train_time:84533ms step_avg:59.49ms
step:1422/2330 train_time:84595ms step_avg:59.49ms
step:1423/2330 train_time:84653ms step_avg:59.49ms
step:1424/2330 train_time:84716ms step_avg:59.49ms
step:1425/2330 train_time:84773ms step_avg:59.49ms
step:1426/2330 train_time:84836ms step_avg:59.49ms
step:1427/2330 train_time:84894ms step_avg:59.49ms
step:1428/2330 train_time:84956ms step_avg:59.49ms
step:1429/2330 train_time:85014ms step_avg:59.49ms
step:1430/2330 train_time:85076ms step_avg:59.49ms
step:1431/2330 train_time:85134ms step_avg:59.49ms
step:1432/2330 train_time:85196ms step_avg:59.49ms
step:1433/2330 train_time:85253ms step_avg:59.49ms
step:1434/2330 train_time:85315ms step_avg:59.49ms
step:1435/2330 train_time:85373ms step_avg:59.49ms
step:1436/2330 train_time:85435ms step_avg:59.50ms
step:1437/2330 train_time:85493ms step_avg:59.49ms
step:1438/2330 train_time:85556ms step_avg:59.50ms
step:1439/2330 train_time:85613ms step_avg:59.49ms
step:1440/2330 train_time:85677ms step_avg:59.50ms
step:1441/2330 train_time:85735ms step_avg:59.50ms
step:1442/2330 train_time:85796ms step_avg:59.50ms
step:1443/2330 train_time:85854ms step_avg:59.50ms
step:1444/2330 train_time:85917ms step_avg:59.50ms
step:1445/2330 train_time:85974ms step_avg:59.50ms
step:1446/2330 train_time:86037ms step_avg:59.50ms
step:1447/2330 train_time:86094ms step_avg:59.50ms
step:1448/2330 train_time:86156ms step_avg:59.50ms
step:1449/2330 train_time:86214ms step_avg:59.50ms
step:1450/2330 train_time:86276ms step_avg:59.50ms
step:1451/2330 train_time:86333ms step_avg:59.50ms
step:1452/2330 train_time:86395ms step_avg:59.50ms
step:1453/2330 train_time:86453ms step_avg:59.50ms
step:1454/2330 train_time:86516ms step_avg:59.50ms
step:1455/2330 train_time:86573ms step_avg:59.50ms
step:1456/2330 train_time:86636ms step_avg:59.50ms
step:1457/2330 train_time:86694ms step_avg:59.50ms
step:1458/2330 train_time:86756ms step_avg:59.50ms
step:1459/2330 train_time:86813ms step_avg:59.50ms
step:1460/2330 train_time:86876ms step_avg:59.50ms
step:1461/2330 train_time:86933ms step_avg:59.50ms
step:1462/2330 train_time:86995ms step_avg:59.50ms
step:1463/2330 train_time:87052ms step_avg:59.50ms
step:1464/2330 train_time:87115ms step_avg:59.50ms
step:1465/2330 train_time:87173ms step_avg:59.50ms
step:1466/2330 train_time:87236ms step_avg:59.51ms
step:1467/2330 train_time:87293ms step_avg:59.50ms
step:1468/2330 train_time:87356ms step_avg:59.51ms
step:1469/2330 train_time:87413ms step_avg:59.51ms
step:1470/2330 train_time:87476ms step_avg:59.51ms
step:1471/2330 train_time:87533ms step_avg:59.51ms
step:1472/2330 train_time:87595ms step_avg:59.51ms
step:1473/2330 train_time:87653ms step_avg:59.51ms
step:1474/2330 train_time:87716ms step_avg:59.51ms
step:1475/2330 train_time:87774ms step_avg:59.51ms
step:1476/2330 train_time:87837ms step_avg:59.51ms
step:1477/2330 train_time:87894ms step_avg:59.51ms
step:1478/2330 train_time:87956ms step_avg:59.51ms
step:1479/2330 train_time:88014ms step_avg:59.51ms
step:1480/2330 train_time:88076ms step_avg:59.51ms
step:1481/2330 train_time:88133ms step_avg:59.51ms
step:1482/2330 train_time:88196ms step_avg:59.51ms
step:1483/2330 train_time:88253ms step_avg:59.51ms
step:1484/2330 train_time:88316ms step_avg:59.51ms
step:1485/2330 train_time:88373ms step_avg:59.51ms
step:1486/2330 train_time:88437ms step_avg:59.51ms
step:1487/2330 train_time:88494ms step_avg:59.51ms
step:1488/2330 train_time:88556ms step_avg:59.51ms
step:1489/2330 train_time:88614ms step_avg:59.51ms
step:1490/2330 train_time:88677ms step_avg:59.51ms
step:1491/2330 train_time:88734ms step_avg:59.51ms
step:1492/2330 train_time:88796ms step_avg:59.51ms
step:1493/2330 train_time:88853ms step_avg:59.51ms
step:1494/2330 train_time:88917ms step_avg:59.52ms
step:1495/2330 train_time:88974ms step_avg:59.51ms
step:1496/2330 train_time:89038ms step_avg:59.52ms
step:1497/2330 train_time:89095ms step_avg:59.52ms
step:1498/2330 train_time:89158ms step_avg:59.52ms
step:1499/2330 train_time:89215ms step_avg:59.52ms
step:1500/2330 train_time:89278ms step_avg:59.52ms
step:1500/2330 val_loss:3.9360 train_time:89357ms step_avg:59.57ms
step:1501/2330 train_time:89376ms step_avg:59.54ms
step:1502/2330 train_time:89400ms step_avg:59.52ms
step:1503/2330 train_time:89459ms step_avg:59.52ms
step:1504/2330 train_time:89528ms step_avg:59.53ms
step:1505/2330 train_time:89584ms step_avg:59.52ms
step:1506/2330 train_time:89648ms step_avg:59.53ms
step:1507/2330 train_time:89705ms step_avg:59.53ms
step:1508/2330 train_time:89766ms step_avg:59.53ms
step:1509/2330 train_time:89822ms step_avg:59.52ms
step:1510/2330 train_time:89884ms step_avg:59.53ms
step:1511/2330 train_time:89941ms step_avg:59.52ms
step:1512/2330 train_time:90003ms step_avg:59.53ms
step:1513/2330 train_time:90060ms step_avg:59.52ms
step:1514/2330 train_time:90121ms step_avg:59.52ms
step:1515/2330 train_time:90178ms step_avg:59.52ms
step:1516/2330 train_time:90239ms step_avg:59.52ms
step:1517/2330 train_time:90296ms step_avg:59.52ms
step:1518/2330 train_time:90359ms step_avg:59.53ms
step:1519/2330 train_time:90417ms step_avg:59.52ms
step:1520/2330 train_time:90483ms step_avg:59.53ms
step:1521/2330 train_time:90541ms step_avg:59.53ms
step:1522/2330 train_time:90605ms step_avg:59.53ms
step:1523/2330 train_time:90662ms step_avg:59.53ms
step:1524/2330 train_time:90725ms step_avg:59.53ms
step:1525/2330 train_time:90781ms step_avg:59.53ms
step:1526/2330 train_time:90843ms step_avg:59.53ms
step:1527/2330 train_time:90900ms step_avg:59.53ms
step:1528/2330 train_time:90961ms step_avg:59.53ms
step:1529/2330 train_time:91019ms step_avg:59.53ms
step:1530/2330 train_time:91080ms step_avg:59.53ms
step:1531/2330 train_time:91138ms step_avg:59.53ms
step:1532/2330 train_time:91200ms step_avg:59.53ms
step:1533/2330 train_time:91257ms step_avg:59.53ms
step:1534/2330 train_time:91320ms step_avg:59.53ms
step:1535/2330 train_time:91377ms step_avg:59.53ms
step:1536/2330 train_time:91443ms step_avg:59.53ms
step:1537/2330 train_time:91500ms step_avg:59.53ms
step:1538/2330 train_time:91566ms step_avg:59.54ms
step:1539/2330 train_time:91624ms step_avg:59.53ms
step:1540/2330 train_time:91687ms step_avg:59.54ms
step:1541/2330 train_time:91744ms step_avg:59.54ms
step:1542/2330 train_time:91807ms step_avg:59.54ms
step:1543/2330 train_time:91864ms step_avg:59.54ms
step:1544/2330 train_time:91926ms step_avg:59.54ms
step:1545/2330 train_time:91983ms step_avg:59.54ms
step:1546/2330 train_time:92046ms step_avg:59.54ms
step:1547/2330 train_time:92103ms step_avg:59.54ms
step:1548/2330 train_time:92165ms step_avg:59.54ms
step:1549/2330 train_time:92222ms step_avg:59.54ms
step:1550/2330 train_time:92285ms step_avg:59.54ms
step:1551/2330 train_time:92342ms step_avg:59.54ms
step:1552/2330 train_time:92405ms step_avg:59.54ms
step:1553/2330 train_time:92463ms step_avg:59.54ms
step:1554/2330 train_time:92526ms step_avg:59.54ms
step:1555/2330 train_time:92584ms step_avg:59.54ms
step:1556/2330 train_time:92647ms step_avg:59.54ms
step:1557/2330 train_time:92704ms step_avg:59.54ms
step:1558/2330 train_time:92768ms step_avg:59.54ms
step:1559/2330 train_time:92825ms step_avg:59.54ms
step:1560/2330 train_time:92888ms step_avg:59.54ms
step:1561/2330 train_time:92945ms step_avg:59.54ms
step:1562/2330 train_time:93008ms step_avg:59.54ms
step:1563/2330 train_time:93065ms step_avg:59.54ms
step:1564/2330 train_time:93127ms step_avg:59.54ms
step:1565/2330 train_time:93184ms step_avg:59.54ms
step:1566/2330 train_time:93248ms step_avg:59.55ms
step:1567/2330 train_time:93305ms step_avg:59.54ms
step:1568/2330 train_time:93369ms step_avg:59.55ms
step:1569/2330 train_time:93425ms step_avg:59.54ms
step:1570/2330 train_time:93490ms step_avg:59.55ms
step:1571/2330 train_time:93547ms step_avg:59.55ms
step:1572/2330 train_time:93613ms step_avg:59.55ms
step:1573/2330 train_time:93670ms step_avg:59.55ms
step:1574/2330 train_time:93733ms step_avg:59.55ms
step:1575/2330 train_time:93792ms step_avg:59.55ms
step:1576/2330 train_time:93855ms step_avg:59.55ms
step:1577/2330 train_time:93914ms step_avg:59.55ms
step:1578/2330 train_time:93977ms step_avg:59.55ms
step:1579/2330 train_time:94035ms step_avg:59.55ms
step:1580/2330 train_time:94099ms step_avg:59.56ms
step:1581/2330 train_time:94156ms step_avg:59.55ms
step:1582/2330 train_time:94219ms step_avg:59.56ms
step:1583/2330 train_time:94277ms step_avg:59.56ms
step:1584/2330 train_time:94340ms step_avg:59.56ms
step:1585/2330 train_time:94398ms step_avg:59.56ms
step:1586/2330 train_time:94461ms step_avg:59.56ms
step:1587/2330 train_time:94518ms step_avg:59.56ms
step:1588/2330 train_time:94582ms step_avg:59.56ms
step:1589/2330 train_time:94639ms step_avg:59.56ms
step:1590/2330 train_time:94704ms step_avg:59.56ms
step:1591/2330 train_time:94761ms step_avg:59.56ms
step:1592/2330 train_time:94824ms step_avg:59.56ms
step:1593/2330 train_time:94881ms step_avg:59.56ms
step:1594/2330 train_time:94944ms step_avg:59.56ms
step:1595/2330 train_time:95002ms step_avg:59.56ms
step:1596/2330 train_time:95065ms step_avg:59.56ms
step:1597/2330 train_time:95122ms step_avg:59.56ms
step:1598/2330 train_time:95183ms step_avg:59.56ms
step:1599/2330 train_time:95241ms step_avg:59.56ms
step:1600/2330 train_time:95305ms step_avg:59.57ms
step:1601/2330 train_time:95362ms step_avg:59.56ms
step:1602/2330 train_time:95424ms step_avg:59.57ms
step:1603/2330 train_time:95482ms step_avg:59.56ms
step:1604/2330 train_time:95545ms step_avg:59.57ms
step:1605/2330 train_time:95603ms step_avg:59.57ms
step:1606/2330 train_time:95666ms step_avg:59.57ms
step:1607/2330 train_time:95723ms step_avg:59.57ms
step:1608/2330 train_time:95786ms step_avg:59.57ms
step:1609/2330 train_time:95843ms step_avg:59.57ms
step:1610/2330 train_time:95907ms step_avg:59.57ms
step:1611/2330 train_time:95963ms step_avg:59.57ms
step:1612/2330 train_time:96026ms step_avg:59.57ms
step:1613/2330 train_time:96083ms step_avg:59.57ms
step:1614/2330 train_time:96146ms step_avg:59.57ms
step:1615/2330 train_time:96203ms step_avg:59.57ms
step:1616/2330 train_time:96266ms step_avg:59.57ms
step:1617/2330 train_time:96323ms step_avg:59.57ms
step:1618/2330 train_time:96386ms step_avg:59.57ms
step:1619/2330 train_time:96443ms step_avg:59.57ms
step:1620/2330 train_time:96505ms step_avg:59.57ms
step:1621/2330 train_time:96562ms step_avg:59.57ms
step:1622/2330 train_time:96626ms step_avg:59.57ms
step:1623/2330 train_time:96683ms step_avg:59.57ms
step:1624/2330 train_time:96746ms step_avg:59.57ms
step:1625/2330 train_time:96803ms step_avg:59.57ms
step:1626/2330 train_time:96866ms step_avg:59.57ms
step:1627/2330 train_time:96923ms step_avg:59.57ms
step:1628/2330 train_time:96985ms step_avg:59.57ms
step:1629/2330 train_time:97042ms step_avg:59.57ms
step:1630/2330 train_time:97106ms step_avg:59.57ms
step:1631/2330 train_time:97163ms step_avg:59.57ms
step:1632/2330 train_time:97225ms step_avg:59.57ms
step:1633/2330 train_time:97282ms step_avg:59.57ms
step:1634/2330 train_time:97345ms step_avg:59.57ms
step:1635/2330 train_time:97402ms step_avg:59.57ms
step:1636/2330 train_time:97465ms step_avg:59.58ms
step:1637/2330 train_time:97523ms step_avg:59.57ms
step:1638/2330 train_time:97585ms step_avg:59.58ms
step:1639/2330 train_time:97642ms step_avg:59.57ms
step:1640/2330 train_time:97705ms step_avg:59.58ms
step:1641/2330 train_time:97762ms step_avg:59.57ms
step:1642/2330 train_time:97826ms step_avg:59.58ms
step:1643/2330 train_time:97883ms step_avg:59.58ms
step:1644/2330 train_time:97946ms step_avg:59.58ms
step:1645/2330 train_time:98003ms step_avg:59.58ms
step:1646/2330 train_time:98066ms step_avg:59.58ms
step:1647/2330 train_time:98124ms step_avg:59.58ms
step:1648/2330 train_time:98186ms step_avg:59.58ms
step:1649/2330 train_time:98243ms step_avg:59.58ms
step:1650/2330 train_time:98305ms step_avg:59.58ms
step:1651/2330 train_time:98362ms step_avg:59.58ms
step:1652/2330 train_time:98426ms step_avg:59.58ms
step:1653/2330 train_time:98483ms step_avg:59.58ms
step:1654/2330 train_time:98545ms step_avg:59.58ms
step:1655/2330 train_time:98603ms step_avg:59.58ms
step:1656/2330 train_time:98665ms step_avg:59.58ms
step:1657/2330 train_time:98722ms step_avg:59.58ms
step:1658/2330 train_time:98786ms step_avg:59.58ms
step:1659/2330 train_time:98843ms step_avg:59.58ms
step:1660/2330 train_time:98905ms step_avg:59.58ms
step:1661/2330 train_time:98962ms step_avg:59.58ms
step:1662/2330 train_time:99026ms step_avg:59.58ms
step:1663/2330 train_time:99083ms step_avg:59.58ms
step:1664/2330 train_time:99145ms step_avg:59.58ms
step:1665/2330 train_time:99202ms step_avg:59.58ms
step:1666/2330 train_time:99265ms step_avg:59.58ms
step:1667/2330 train_time:99322ms step_avg:59.58ms
step:1668/2330 train_time:99384ms step_avg:59.58ms
step:1669/2330 train_time:99442ms step_avg:59.58ms
step:1670/2330 train_time:99504ms step_avg:59.58ms
step:1671/2330 train_time:99562ms step_avg:59.58ms
step:1672/2330 train_time:99625ms step_avg:59.58ms
step:1673/2330 train_time:99682ms step_avg:59.58ms
step:1674/2330 train_time:99744ms step_avg:59.58ms
step:1675/2330 train_time:99802ms step_avg:59.58ms
step:1676/2330 train_time:99864ms step_avg:59.58ms
step:1677/2330 train_time:99922ms step_avg:59.58ms
step:1678/2330 train_time:99985ms step_avg:59.59ms
step:1679/2330 train_time:100042ms step_avg:59.58ms
step:1680/2330 train_time:100105ms step_avg:59.59ms
step:1681/2330 train_time:100162ms step_avg:59.58ms
step:1682/2330 train_time:100225ms step_avg:59.59ms
step:1683/2330 train_time:100283ms step_avg:59.59ms
step:1684/2330 train_time:100345ms step_avg:59.59ms
step:1685/2330 train_time:100403ms step_avg:59.59ms
step:1686/2330 train_time:100465ms step_avg:59.59ms
step:1687/2330 train_time:100523ms step_avg:59.59ms
step:1688/2330 train_time:100586ms step_avg:59.59ms
step:1689/2330 train_time:100643ms step_avg:59.59ms
step:1690/2330 train_time:100706ms step_avg:59.59ms
step:1691/2330 train_time:100763ms step_avg:59.59ms
step:1692/2330 train_time:100825ms step_avg:59.59ms
step:1693/2330 train_time:100883ms step_avg:59.59ms
step:1694/2330 train_time:100946ms step_avg:59.59ms
step:1695/2330 train_time:101003ms step_avg:59.59ms
step:1696/2330 train_time:101065ms step_avg:59.59ms
step:1697/2330 train_time:101122ms step_avg:59.59ms
step:1698/2330 train_time:101185ms step_avg:59.59ms
step:1699/2330 train_time:101243ms step_avg:59.59ms
step:1700/2330 train_time:101305ms step_avg:59.59ms
step:1701/2330 train_time:101361ms step_avg:59.59ms
step:1702/2330 train_time:101425ms step_avg:59.59ms
step:1703/2330 train_time:101482ms step_avg:59.59ms
step:1704/2330 train_time:101545ms step_avg:59.59ms
step:1705/2330 train_time:101602ms step_avg:59.59ms
step:1706/2330 train_time:101665ms step_avg:59.59ms
step:1707/2330 train_time:101722ms step_avg:59.59ms
step:1708/2330 train_time:101785ms step_avg:59.59ms
step:1709/2330 train_time:101843ms step_avg:59.59ms
step:1710/2330 train_time:101905ms step_avg:59.59ms
step:1711/2330 train_time:101963ms step_avg:59.59ms
step:1712/2330 train_time:102025ms step_avg:59.59ms
step:1713/2330 train_time:102083ms step_avg:59.59ms
step:1714/2330 train_time:102146ms step_avg:59.60ms
step:1715/2330 train_time:102203ms step_avg:59.59ms
step:1716/2330 train_time:102265ms step_avg:59.60ms
step:1717/2330 train_time:102323ms step_avg:59.59ms
step:1718/2330 train_time:102384ms step_avg:59.60ms
step:1719/2330 train_time:102442ms step_avg:59.59ms
step:1720/2330 train_time:102504ms step_avg:59.60ms
step:1721/2330 train_time:102561ms step_avg:59.59ms
step:1722/2330 train_time:102625ms step_avg:59.60ms
step:1723/2330 train_time:102682ms step_avg:59.59ms
step:1724/2330 train_time:102745ms step_avg:59.60ms
step:1725/2330 train_time:102802ms step_avg:59.60ms
step:1726/2330 train_time:102865ms step_avg:59.60ms
step:1727/2330 train_time:102922ms step_avg:59.60ms
step:1728/2330 train_time:102986ms step_avg:59.60ms
step:1729/2330 train_time:103044ms step_avg:59.60ms
step:1730/2330 train_time:103106ms step_avg:59.60ms
step:1731/2330 train_time:103163ms step_avg:59.60ms
step:1732/2330 train_time:103226ms step_avg:59.60ms
step:1733/2330 train_time:103283ms step_avg:59.60ms
step:1734/2330 train_time:103346ms step_avg:59.60ms
step:1735/2330 train_time:103403ms step_avg:59.60ms
step:1736/2330 train_time:103466ms step_avg:59.60ms
step:1737/2330 train_time:103523ms step_avg:59.60ms
step:1738/2330 train_time:103585ms step_avg:59.60ms
step:1739/2330 train_time:103642ms step_avg:59.60ms
step:1740/2330 train_time:103706ms step_avg:59.60ms
step:1741/2330 train_time:103763ms step_avg:59.60ms
step:1742/2330 train_time:103827ms step_avg:59.60ms
step:1743/2330 train_time:103884ms step_avg:59.60ms
step:1744/2330 train_time:103948ms step_avg:59.60ms
step:1745/2330 train_time:104005ms step_avg:59.60ms
step:1746/2330 train_time:104070ms step_avg:59.60ms
step:1747/2330 train_time:104127ms step_avg:59.60ms
step:1748/2330 train_time:104191ms step_avg:59.61ms
step:1749/2330 train_time:104249ms step_avg:59.61ms
step:1750/2330 train_time:104313ms step_avg:59.61ms
step:1750/2330 val_loss:3.8531 train_time:104393ms step_avg:59.65ms
step:1751/2330 train_time:104411ms step_avg:59.63ms
step:1752/2330 train_time:104435ms step_avg:59.61ms
step:1753/2330 train_time:104492ms step_avg:59.61ms
step:1754/2330 train_time:104562ms step_avg:59.61ms
step:1755/2330 train_time:104620ms step_avg:59.61ms
step:1756/2330 train_time:104685ms step_avg:59.62ms
step:1757/2330 train_time:104742ms step_avg:59.61ms
step:1758/2330 train_time:104805ms step_avg:59.62ms
step:1759/2330 train_time:104862ms step_avg:59.61ms
step:1760/2330 train_time:104924ms step_avg:59.62ms
step:1761/2330 train_time:104981ms step_avg:59.61ms
step:1762/2330 train_time:105042ms step_avg:59.62ms
step:1763/2330 train_time:105099ms step_avg:59.61ms
step:1764/2330 train_time:105161ms step_avg:59.61ms
step:1765/2330 train_time:105217ms step_avg:59.61ms
step:1766/2330 train_time:105279ms step_avg:59.61ms
step:1767/2330 train_time:105337ms step_avg:59.61ms
step:1768/2330 train_time:105405ms step_avg:59.62ms
step:1769/2330 train_time:105462ms step_avg:59.62ms
step:1770/2330 train_time:105527ms step_avg:59.62ms
step:1771/2330 train_time:105584ms step_avg:59.62ms
step:1772/2330 train_time:105648ms step_avg:59.62ms
step:1773/2330 train_time:105704ms step_avg:59.62ms
step:1774/2330 train_time:105769ms step_avg:59.62ms
step:1775/2330 train_time:105826ms step_avg:59.62ms
step:1776/2330 train_time:105890ms step_avg:59.62ms
step:1777/2330 train_time:105947ms step_avg:59.62ms
step:1778/2330 train_time:106011ms step_avg:59.62ms
step:1779/2330 train_time:106068ms step_avg:59.62ms
step:1780/2330 train_time:106130ms step_avg:59.62ms
step:1781/2330 train_time:106187ms step_avg:59.62ms
step:1782/2330 train_time:106251ms step_avg:59.62ms
step:1783/2330 train_time:106310ms step_avg:59.62ms
step:1784/2330 train_time:106372ms step_avg:59.63ms
step:1785/2330 train_time:106431ms step_avg:59.63ms
step:1786/2330 train_time:106494ms step_avg:59.63ms
step:1787/2330 train_time:106553ms step_avg:59.63ms
step:1788/2330 train_time:106617ms step_avg:59.63ms
step:1789/2330 train_time:106674ms step_avg:59.63ms
step:1790/2330 train_time:106739ms step_avg:59.63ms
step:1791/2330 train_time:106797ms step_avg:59.63ms
step:1792/2330 train_time:106859ms step_avg:59.63ms
step:1793/2330 train_time:106917ms step_avg:59.63ms
step:1794/2330 train_time:106979ms step_avg:59.63ms
step:1795/2330 train_time:107036ms step_avg:59.63ms
step:1796/2330 train_time:107098ms step_avg:59.63ms
step:1797/2330 train_time:107156ms step_avg:59.63ms
step:1798/2330 train_time:107218ms step_avg:59.63ms
step:1799/2330 train_time:107276ms step_avg:59.63ms
step:1800/2330 train_time:107339ms step_avg:59.63ms
step:1801/2330 train_time:107397ms step_avg:59.63ms
step:1802/2330 train_time:107461ms step_avg:59.63ms
step:1803/2330 train_time:107519ms step_avg:59.63ms
step:1804/2330 train_time:107582ms step_avg:59.63ms
step:1805/2330 train_time:107639ms step_avg:59.63ms
step:1806/2330 train_time:107703ms step_avg:59.64ms
step:1807/2330 train_time:107760ms step_avg:59.64ms
step:1808/2330 train_time:107823ms step_avg:59.64ms
step:1809/2330 train_time:107881ms step_avg:59.64ms
step:1810/2330 train_time:107943ms step_avg:59.64ms
step:1811/2330 train_time:108000ms step_avg:59.64ms
step:1812/2330 train_time:108062ms step_avg:59.64ms
step:1813/2330 train_time:108119ms step_avg:59.64ms
step:1814/2330 train_time:108182ms step_avg:59.64ms
step:1815/2330 train_time:108239ms step_avg:59.64ms
step:1816/2330 train_time:108302ms step_avg:59.64ms
step:1817/2330 train_time:108359ms step_avg:59.64ms
step:1818/2330 train_time:108422ms step_avg:59.64ms
step:1819/2330 train_time:108480ms step_avg:59.64ms
step:1820/2330 train_time:108542ms step_avg:59.64ms
step:1821/2330 train_time:108600ms step_avg:59.64ms
step:1822/2330 train_time:108663ms step_avg:59.64ms
step:1823/2330 train_time:108721ms step_avg:59.64ms
step:1824/2330 train_time:108783ms step_avg:59.64ms
step:1825/2330 train_time:108840ms step_avg:59.64ms
step:1826/2330 train_time:108903ms step_avg:59.64ms
step:1827/2330 train_time:108959ms step_avg:59.64ms
step:1828/2330 train_time:109022ms step_avg:59.64ms
step:1829/2330 train_time:109080ms step_avg:59.64ms
step:1830/2330 train_time:109142ms step_avg:59.64ms
step:1831/2330 train_time:109198ms step_avg:59.64ms
step:1832/2330 train_time:109262ms step_avg:59.64ms
step:1833/2330 train_time:109319ms step_avg:59.64ms
step:1834/2330 train_time:109382ms step_avg:59.64ms
step:1835/2330 train_time:109439ms step_avg:59.64ms
step:1836/2330 train_time:109502ms step_avg:59.64ms
step:1837/2330 train_time:109560ms step_avg:59.64ms
step:1838/2330 train_time:109623ms step_avg:59.64ms
step:1839/2330 train_time:109681ms step_avg:59.64ms
step:1840/2330 train_time:109744ms step_avg:59.64ms
step:1841/2330 train_time:109801ms step_avg:59.64ms
step:1842/2330 train_time:109863ms step_avg:59.64ms
step:1843/2330 train_time:109921ms step_avg:59.64ms
step:1844/2330 train_time:109983ms step_avg:59.64ms
step:1845/2330 train_time:110041ms step_avg:59.64ms
step:1846/2330 train_time:110103ms step_avg:59.64ms
step:1847/2330 train_time:110160ms step_avg:59.64ms
step:1848/2330 train_time:110223ms step_avg:59.64ms
step:1849/2330 train_time:110280ms step_avg:59.64ms
step:1850/2330 train_time:110343ms step_avg:59.64ms
step:1851/2330 train_time:110401ms step_avg:59.64ms
step:1852/2330 train_time:110462ms step_avg:59.64ms
step:1853/2330 train_time:110519ms step_avg:59.64ms
step:1854/2330 train_time:110583ms step_avg:59.65ms
step:1855/2330 train_time:110640ms step_avg:59.64ms
step:1856/2330 train_time:110704ms step_avg:59.65ms
step:1857/2330 train_time:110762ms step_avg:59.65ms
step:1858/2330 train_time:110823ms step_avg:59.65ms
step:1859/2330 train_time:110881ms step_avg:59.65ms
step:1860/2330 train_time:110944ms step_avg:59.65ms
step:1861/2330 train_time:111001ms step_avg:59.65ms
step:1862/2330 train_time:111064ms step_avg:59.65ms
step:1863/2330 train_time:111121ms step_avg:59.65ms
step:1864/2330 train_time:111183ms step_avg:59.65ms
step:1865/2330 train_time:111240ms step_avg:59.65ms
step:1866/2330 train_time:111303ms step_avg:59.65ms
step:1867/2330 train_time:111360ms step_avg:59.65ms
step:1868/2330 train_time:111423ms step_avg:59.65ms
step:1869/2330 train_time:111480ms step_avg:59.65ms
step:1870/2330 train_time:111542ms step_avg:59.65ms
step:1871/2330 train_time:111600ms step_avg:59.65ms
step:1872/2330 train_time:111662ms step_avg:59.65ms
step:1873/2330 train_time:111720ms step_avg:59.65ms
step:1874/2330 train_time:111782ms step_avg:59.65ms
step:1875/2330 train_time:111840ms step_avg:59.65ms
step:1876/2330 train_time:111903ms step_avg:59.65ms
step:1877/2330 train_time:111960ms step_avg:59.65ms
step:1878/2330 train_time:112023ms step_avg:59.65ms
step:1879/2330 train_time:112081ms step_avg:59.65ms
step:1880/2330 train_time:112143ms step_avg:59.65ms
step:1881/2330 train_time:112200ms step_avg:59.65ms
step:1882/2330 train_time:112263ms step_avg:59.65ms
step:1883/2330 train_time:112320ms step_avg:59.65ms
step:1884/2330 train_time:112384ms step_avg:59.65ms
step:1885/2330 train_time:112441ms step_avg:59.65ms
step:1886/2330 train_time:112504ms step_avg:59.65ms
step:1887/2330 train_time:112561ms step_avg:59.65ms
step:1888/2330 train_time:112623ms step_avg:59.65ms
step:1889/2330 train_time:112681ms step_avg:59.65ms
step:1890/2330 train_time:112744ms step_avg:59.65ms
step:1891/2330 train_time:112801ms step_avg:59.65ms
step:1892/2330 train_time:112864ms step_avg:59.65ms
step:1893/2330 train_time:112921ms step_avg:59.65ms
step:1894/2330 train_time:112984ms step_avg:59.65ms
step:1895/2330 train_time:113041ms step_avg:59.65ms
step:1896/2330 train_time:113103ms step_avg:59.65ms
step:1897/2330 train_time:113161ms step_avg:59.65ms
step:1898/2330 train_time:113223ms step_avg:59.65ms
step:1899/2330 train_time:113280ms step_avg:59.65ms
step:1900/2330 train_time:113344ms step_avg:59.65ms
step:1901/2330 train_time:113401ms step_avg:59.65ms
step:1902/2330 train_time:113464ms step_avg:59.65ms
step:1903/2330 train_time:113521ms step_avg:59.65ms
step:1904/2330 train_time:113584ms step_avg:59.66ms
step:1905/2330 train_time:113642ms step_avg:59.65ms
step:1906/2330 train_time:113704ms step_avg:59.66ms
step:1907/2330 train_time:113761ms step_avg:59.65ms
step:1908/2330 train_time:113823ms step_avg:59.66ms
step:1909/2330 train_time:113880ms step_avg:59.65ms
step:1910/2330 train_time:113943ms step_avg:59.66ms
step:1911/2330 train_time:114000ms step_avg:59.65ms
step:1912/2330 train_time:114063ms step_avg:59.66ms
step:1913/2330 train_time:114120ms step_avg:59.66ms
step:1914/2330 train_time:114183ms step_avg:59.66ms
step:1915/2330 train_time:114241ms step_avg:59.66ms
step:1916/2330 train_time:114304ms step_avg:59.66ms
step:1917/2330 train_time:114361ms step_avg:59.66ms
step:1918/2330 train_time:114423ms step_avg:59.66ms
step:1919/2330 train_time:114481ms step_avg:59.66ms
step:1920/2330 train_time:114542ms step_avg:59.66ms
step:1921/2330 train_time:114599ms step_avg:59.66ms
step:1922/2330 train_time:114662ms step_avg:59.66ms
step:1923/2330 train_time:114720ms step_avg:59.66ms
step:1924/2330 train_time:114783ms step_avg:59.66ms
step:1925/2330 train_time:114842ms step_avg:59.66ms
step:1926/2330 train_time:114904ms step_avg:59.66ms
step:1927/2330 train_time:114962ms step_avg:59.66ms
step:1928/2330 train_time:115024ms step_avg:59.66ms
step:1929/2330 train_time:115082ms step_avg:59.66ms
step:1930/2330 train_time:115144ms step_avg:59.66ms
step:1931/2330 train_time:115202ms step_avg:59.66ms
step:1932/2330 train_time:115264ms step_avg:59.66ms
step:1933/2330 train_time:115321ms step_avg:59.66ms
step:1934/2330 train_time:115385ms step_avg:59.66ms
step:1935/2330 train_time:115443ms step_avg:59.66ms
step:1936/2330 train_time:115506ms step_avg:59.66ms
step:1937/2330 train_time:115562ms step_avg:59.66ms
step:1938/2330 train_time:115625ms step_avg:59.66ms
step:1939/2330 train_time:115682ms step_avg:59.66ms
step:1940/2330 train_time:115745ms step_avg:59.66ms
step:1941/2330 train_time:115802ms step_avg:59.66ms
step:1942/2330 train_time:115865ms step_avg:59.66ms
step:1943/2330 train_time:115922ms step_avg:59.66ms
step:1944/2330 train_time:115985ms step_avg:59.66ms
step:1945/2330 train_time:116042ms step_avg:59.66ms
step:1946/2330 train_time:116106ms step_avg:59.66ms
step:1947/2330 train_time:116162ms step_avg:59.66ms
step:1948/2330 train_time:116225ms step_avg:59.66ms
step:1949/2330 train_time:116283ms step_avg:59.66ms
step:1950/2330 train_time:116344ms step_avg:59.66ms
step:1951/2330 train_time:116401ms step_avg:59.66ms
step:1952/2330 train_time:116464ms step_avg:59.66ms
step:1953/2330 train_time:116521ms step_avg:59.66ms
step:1954/2330 train_time:116584ms step_avg:59.66ms
step:1955/2330 train_time:116641ms step_avg:59.66ms
step:1956/2330 train_time:116704ms step_avg:59.66ms
step:1957/2330 train_time:116762ms step_avg:59.66ms
step:1958/2330 train_time:116825ms step_avg:59.67ms
step:1959/2330 train_time:116882ms step_avg:59.66ms
step:1960/2330 train_time:116944ms step_avg:59.67ms
step:1961/2330 train_time:117001ms step_avg:59.66ms
step:1962/2330 train_time:117065ms step_avg:59.67ms
step:1963/2330 train_time:117122ms step_avg:59.66ms
step:1964/2330 train_time:117184ms step_avg:59.67ms
step:1965/2330 train_time:117242ms step_avg:59.67ms
step:1966/2330 train_time:117305ms step_avg:59.67ms
step:1967/2330 train_time:117361ms step_avg:59.67ms
step:1968/2330 train_time:117424ms step_avg:59.67ms
step:1969/2330 train_time:117482ms step_avg:59.67ms
step:1970/2330 train_time:117545ms step_avg:59.67ms
step:1971/2330 train_time:117603ms step_avg:59.67ms
step:1972/2330 train_time:117665ms step_avg:59.67ms
step:1973/2330 train_time:117722ms step_avg:59.67ms
step:1974/2330 train_time:117785ms step_avg:59.67ms
step:1975/2330 train_time:117842ms step_avg:59.67ms
step:1976/2330 train_time:117905ms step_avg:59.67ms
step:1977/2330 train_time:117962ms step_avg:59.67ms
step:1978/2330 train_time:118025ms step_avg:59.67ms
step:1979/2330 train_time:118082ms step_avg:59.67ms
step:1980/2330 train_time:118145ms step_avg:59.67ms
step:1981/2330 train_time:118202ms step_avg:59.67ms
step:1982/2330 train_time:118265ms step_avg:59.67ms
step:1983/2330 train_time:118322ms step_avg:59.67ms
step:1984/2330 train_time:118385ms step_avg:59.67ms
step:1985/2330 train_time:118442ms step_avg:59.67ms
step:1986/2330 train_time:118505ms step_avg:59.67ms
step:1987/2330 train_time:118561ms step_avg:59.67ms
step:1988/2330 train_time:118625ms step_avg:59.67ms
step:1989/2330 train_time:118682ms step_avg:59.67ms
step:1990/2330 train_time:118744ms step_avg:59.67ms
step:1991/2330 train_time:118801ms step_avg:59.67ms
step:1992/2330 train_time:118865ms step_avg:59.67ms
step:1993/2330 train_time:118922ms step_avg:59.67ms
step:1994/2330 train_time:118985ms step_avg:59.67ms
step:1995/2330 train_time:119041ms step_avg:59.67ms
step:1996/2330 train_time:119105ms step_avg:59.67ms
step:1997/2330 train_time:119162ms step_avg:59.67ms
step:1998/2330 train_time:119225ms step_avg:59.67ms
step:1999/2330 train_time:119283ms step_avg:59.67ms
step:2000/2330 train_time:119345ms step_avg:59.67ms
step:2000/2330 val_loss:3.7897 train_time:119425ms step_avg:59.71ms
step:2001/2330 train_time:119444ms step_avg:59.69ms
step:2002/2330 train_time:119468ms step_avg:59.67ms
step:2003/2330 train_time:119528ms step_avg:59.67ms
step:2004/2330 train_time:119598ms step_avg:59.68ms
step:2005/2330 train_time:119656ms step_avg:59.68ms
step:2006/2330 train_time:119719ms step_avg:59.68ms
step:2007/2330 train_time:119776ms step_avg:59.68ms
step:2008/2330 train_time:119838ms step_avg:59.68ms
step:2009/2330 train_time:119895ms step_avg:59.68ms
step:2010/2330 train_time:119958ms step_avg:59.68ms
step:2011/2330 train_time:120015ms step_avg:59.68ms
step:2012/2330 train_time:120078ms step_avg:59.68ms
step:2013/2330 train_time:120135ms step_avg:59.68ms
step:2014/2330 train_time:120197ms step_avg:59.68ms
step:2015/2330 train_time:120253ms step_avg:59.68ms
step:2016/2330 train_time:120315ms step_avg:59.68ms
step:2017/2330 train_time:120372ms step_avg:59.68ms
step:2018/2330 train_time:120436ms step_avg:59.68ms
step:2019/2330 train_time:120494ms step_avg:59.68ms
step:2020/2330 train_time:120559ms step_avg:59.68ms
step:2021/2330 train_time:120616ms step_avg:59.68ms
step:2022/2330 train_time:120682ms step_avg:59.68ms
step:2023/2330 train_time:120740ms step_avg:59.68ms
step:2024/2330 train_time:120804ms step_avg:59.69ms
step:2025/2330 train_time:120861ms step_avg:59.68ms
step:2026/2330 train_time:120925ms step_avg:59.69ms
step:2027/2330 train_time:120982ms step_avg:59.69ms
step:2028/2330 train_time:121046ms step_avg:59.69ms
step:2029/2330 train_time:121104ms step_avg:59.69ms
step:2030/2330 train_time:121166ms step_avg:59.69ms
step:2031/2330 train_time:121224ms step_avg:59.69ms
step:2032/2330 train_time:121286ms step_avg:59.69ms
step:2033/2330 train_time:121345ms step_avg:59.69ms
step:2034/2330 train_time:121406ms step_avg:59.69ms
step:2035/2330 train_time:121465ms step_avg:59.69ms
step:2036/2330 train_time:121529ms step_avg:59.69ms
step:2037/2330 train_time:121587ms step_avg:59.69ms
step:2038/2330 train_time:121652ms step_avg:59.69ms
step:2039/2330 train_time:121709ms step_avg:59.69ms
step:2040/2330 train_time:121772ms step_avg:59.69ms
step:2041/2330 train_time:121830ms step_avg:59.69ms
step:2042/2330 train_time:121893ms step_avg:59.69ms
step:2043/2330 train_time:121950ms step_avg:59.69ms
step:2044/2330 train_time:122013ms step_avg:59.69ms
step:2045/2330 train_time:122070ms step_avg:59.69ms
step:2046/2330 train_time:122133ms step_avg:59.69ms
step:2047/2330 train_time:122190ms step_avg:59.69ms
step:2048/2330 train_time:122252ms step_avg:59.69ms
step:2049/2330 train_time:122309ms step_avg:59.69ms
step:2050/2330 train_time:122372ms step_avg:59.69ms
step:2051/2330 train_time:122430ms step_avg:59.69ms
step:2052/2330 train_time:122493ms step_avg:59.69ms
step:2053/2330 train_time:122551ms step_avg:59.69ms
step:2054/2330 train_time:122614ms step_avg:59.70ms
step:2055/2330 train_time:122672ms step_avg:59.69ms
step:2056/2330 train_time:122735ms step_avg:59.70ms
step:2057/2330 train_time:122792ms step_avg:59.69ms
step:2058/2330 train_time:122855ms step_avg:59.70ms
step:2059/2330 train_time:122913ms step_avg:59.70ms
step:2060/2330 train_time:122975ms step_avg:59.70ms
step:2061/2330 train_time:123032ms step_avg:59.70ms
step:2062/2330 train_time:123095ms step_avg:59.70ms
step:2063/2330 train_time:123152ms step_avg:59.70ms
step:2064/2330 train_time:123215ms step_avg:59.70ms
step:2065/2330 train_time:123272ms step_avg:59.70ms
step:2066/2330 train_time:123333ms step_avg:59.70ms
step:2067/2330 train_time:123390ms step_avg:59.70ms
step:2068/2330 train_time:123454ms step_avg:59.70ms
step:2069/2330 train_time:123512ms step_avg:59.70ms
step:2070/2330 train_time:123574ms step_avg:59.70ms
step:2071/2330 train_time:123632ms step_avg:59.70ms
step:2072/2330 train_time:123696ms step_avg:59.70ms
step:2073/2330 train_time:123754ms step_avg:59.70ms
step:2074/2330 train_time:123817ms step_avg:59.70ms
step:2075/2330 train_time:123874ms step_avg:59.70ms
step:2076/2330 train_time:123937ms step_avg:59.70ms
step:2077/2330 train_time:123994ms step_avg:59.70ms
step:2078/2330 train_time:124057ms step_avg:59.70ms
step:2079/2330 train_time:124114ms step_avg:59.70ms
step:2080/2330 train_time:124176ms step_avg:59.70ms
step:2081/2330 train_time:124233ms step_avg:59.70ms
step:2082/2330 train_time:124296ms step_avg:59.70ms
step:2083/2330 train_time:124353ms step_avg:59.70ms
step:2084/2330 train_time:124416ms step_avg:59.70ms
step:2085/2330 train_time:124473ms step_avg:59.70ms
step:2086/2330 train_time:124536ms step_avg:59.70ms
step:2087/2330 train_time:124593ms step_avg:59.70ms
step:2088/2330 train_time:124657ms step_avg:59.70ms
step:2089/2330 train_time:124714ms step_avg:59.70ms
step:2090/2330 train_time:124776ms step_avg:59.70ms
step:2091/2330 train_time:124834ms step_avg:59.70ms
step:2092/2330 train_time:124897ms step_avg:59.70ms
step:2093/2330 train_time:124955ms step_avg:59.70ms
step:2094/2330 train_time:125017ms step_avg:59.70ms
step:2095/2330 train_time:125075ms step_avg:59.70ms
step:2096/2330 train_time:125136ms step_avg:59.70ms
step:2097/2330 train_time:125194ms step_avg:59.70ms
step:2098/2330 train_time:125257ms step_avg:59.70ms
step:2099/2330 train_time:125314ms step_avg:59.70ms
step:2100/2330 train_time:125376ms step_avg:59.70ms
step:2101/2330 train_time:125433ms step_avg:59.70ms
step:2102/2330 train_time:125496ms step_avg:59.70ms
step:2103/2330 train_time:125553ms step_avg:59.70ms
step:2104/2330 train_time:125616ms step_avg:59.70ms
step:2105/2330 train_time:125674ms step_avg:59.70ms
step:2106/2330 train_time:125736ms step_avg:59.70ms
step:2107/2330 train_time:125793ms step_avg:59.70ms
step:2108/2330 train_time:125856ms step_avg:59.70ms
step:2109/2330 train_time:125913ms step_avg:59.70ms
step:2110/2330 train_time:125976ms step_avg:59.70ms
step:2111/2330 train_time:126033ms step_avg:59.70ms
step:2112/2330 train_time:126097ms step_avg:59.70ms
step:2113/2330 train_time:126155ms step_avg:59.70ms
step:2114/2330 train_time:126217ms step_avg:59.71ms
step:2115/2330 train_time:126274ms step_avg:59.70ms
step:2116/2330 train_time:126337ms step_avg:59.71ms
step:2117/2330 train_time:126393ms step_avg:59.70ms
step:2118/2330 train_time:126457ms step_avg:59.71ms
step:2119/2330 train_time:126513ms step_avg:59.70ms
step:2120/2330 train_time:126577ms step_avg:59.71ms
step:2121/2330 train_time:126635ms step_avg:59.71ms
step:2122/2330 train_time:126699ms step_avg:59.71ms
step:2123/2330 train_time:126756ms step_avg:59.71ms
step:2124/2330 train_time:126821ms step_avg:59.71ms
step:2125/2330 train_time:126878ms step_avg:59.71ms
step:2126/2330 train_time:126941ms step_avg:59.71ms
step:2127/2330 train_time:126998ms step_avg:59.71ms
step:2128/2330 train_time:127063ms step_avg:59.71ms
step:2129/2330 train_time:127121ms step_avg:59.71ms
step:2130/2330 train_time:127186ms step_avg:59.71ms
step:2131/2330 train_time:127244ms step_avg:59.71ms
step:2132/2330 train_time:127308ms step_avg:59.71ms
step:2133/2330 train_time:127366ms step_avg:59.71ms
step:2134/2330 train_time:127430ms step_avg:59.71ms
step:2135/2330 train_time:127488ms step_avg:59.71ms
step:2136/2330 train_time:127551ms step_avg:59.71ms
step:2137/2330 train_time:127609ms step_avg:59.71ms
step:2138/2330 train_time:127672ms step_avg:59.72ms
step:2139/2330 train_time:127730ms step_avg:59.71ms
step:2140/2330 train_time:127792ms step_avg:59.72ms
step:2141/2330 train_time:127850ms step_avg:59.72ms
step:2142/2330 train_time:127912ms step_avg:59.72ms
step:2143/2330 train_time:127969ms step_avg:59.72ms
step:2144/2330 train_time:128032ms step_avg:59.72ms
step:2145/2330 train_time:128089ms step_avg:59.72ms
step:2146/2330 train_time:128153ms step_avg:59.72ms
step:2147/2330 train_time:128210ms step_avg:59.72ms
step:2148/2330 train_time:128274ms step_avg:59.72ms
step:2149/2330 train_time:128331ms step_avg:59.72ms
step:2150/2330 train_time:128394ms step_avg:59.72ms
step:2151/2330 train_time:128451ms step_avg:59.72ms
step:2152/2330 train_time:128514ms step_avg:59.72ms
step:2153/2330 train_time:128571ms step_avg:59.72ms
step:2154/2330 train_time:128634ms step_avg:59.72ms
step:2155/2330 train_time:128691ms step_avg:59.72ms
step:2156/2330 train_time:128754ms step_avg:59.72ms
step:2157/2330 train_time:128812ms step_avg:59.72ms
step:2158/2330 train_time:128874ms step_avg:59.72ms
step:2159/2330 train_time:128932ms step_avg:59.72ms
step:2160/2330 train_time:128995ms step_avg:59.72ms
step:2161/2330 train_time:129052ms step_avg:59.72ms
step:2162/2330 train_time:129115ms step_avg:59.72ms
step:2163/2330 train_time:129171ms step_avg:59.72ms
step:2164/2330 train_time:129234ms step_avg:59.72ms
step:2165/2330 train_time:129291ms step_avg:59.72ms
step:2166/2330 train_time:129355ms step_avg:59.72ms
step:2167/2330 train_time:129412ms step_avg:59.72ms
step:2168/2330 train_time:129475ms step_avg:59.72ms
step:2169/2330 train_time:129532ms step_avg:59.72ms
step:2170/2330 train_time:129595ms step_avg:59.72ms
step:2171/2330 train_time:129653ms step_avg:59.72ms
step:2172/2330 train_time:129717ms step_avg:59.72ms
step:2173/2330 train_time:129774ms step_avg:59.72ms
step:2174/2330 train_time:129837ms step_avg:59.72ms
step:2175/2330 train_time:129895ms step_avg:59.72ms
step:2176/2330 train_time:129957ms step_avg:59.72ms
step:2177/2330 train_time:130015ms step_avg:59.72ms
step:2178/2330 train_time:130077ms step_avg:59.72ms
step:2179/2330 train_time:130134ms step_avg:59.72ms
step:2180/2330 train_time:130197ms step_avg:59.72ms
step:2181/2330 train_time:130254ms step_avg:59.72ms
step:2182/2330 train_time:130318ms step_avg:59.72ms
step:2183/2330 train_time:130375ms step_avg:59.72ms
step:2184/2330 train_time:130438ms step_avg:59.72ms
step:2185/2330 train_time:130494ms step_avg:59.72ms
step:2186/2330 train_time:130558ms step_avg:59.72ms
step:2187/2330 train_time:130615ms step_avg:59.72ms
step:2188/2330 train_time:130679ms step_avg:59.73ms
step:2189/2330 train_time:130737ms step_avg:59.72ms
step:2190/2330 train_time:130801ms step_avg:59.73ms
step:2191/2330 train_time:130859ms step_avg:59.73ms
step:2192/2330 train_time:130923ms step_avg:59.73ms
step:2193/2330 train_time:130980ms step_avg:59.73ms
step:2194/2330 train_time:131044ms step_avg:59.73ms
step:2195/2330 train_time:131101ms step_avg:59.73ms
step:2196/2330 train_time:131164ms step_avg:59.73ms
step:2197/2330 train_time:131222ms step_avg:59.73ms
step:2198/2330 train_time:131286ms step_avg:59.73ms
step:2199/2330 train_time:131345ms step_avg:59.73ms
step:2200/2330 train_time:131408ms step_avg:59.73ms
step:2201/2330 train_time:131466ms step_avg:59.73ms
step:2202/2330 train_time:131529ms step_avg:59.73ms
step:2203/2330 train_time:131588ms step_avg:59.73ms
step:2204/2330 train_time:131650ms step_avg:59.73ms
step:2205/2330 train_time:131708ms step_avg:59.73ms
step:2206/2330 train_time:131771ms step_avg:59.73ms
step:2207/2330 train_time:131828ms step_avg:59.73ms
step:2208/2330 train_time:131893ms step_avg:59.73ms
step:2209/2330 train_time:131950ms step_avg:59.73ms
step:2210/2330 train_time:132013ms step_avg:59.73ms
step:2211/2330 train_time:132069ms step_avg:59.73ms
step:2212/2330 train_time:132132ms step_avg:59.73ms
step:2213/2330 train_time:132190ms step_avg:59.73ms
step:2214/2330 train_time:132253ms step_avg:59.73ms
step:2215/2330 train_time:132310ms step_avg:59.73ms
step:2216/2330 train_time:132373ms step_avg:59.74ms
step:2217/2330 train_time:132430ms step_avg:59.73ms
step:2218/2330 train_time:132493ms step_avg:59.74ms
step:2219/2330 train_time:132551ms step_avg:59.73ms
step:2220/2330 train_time:132613ms step_avg:59.74ms
step:2221/2330 train_time:132670ms step_avg:59.73ms
step:2222/2330 train_time:132733ms step_avg:59.74ms
step:2223/2330 train_time:132791ms step_avg:59.73ms
step:2224/2330 train_time:132853ms step_avg:59.74ms
step:2225/2330 train_time:132911ms step_avg:59.74ms
step:2226/2330 train_time:132974ms step_avg:59.74ms
step:2227/2330 train_time:133031ms step_avg:59.74ms
step:2228/2330 train_time:133094ms step_avg:59.74ms
step:2229/2330 train_time:133151ms step_avg:59.74ms
step:2230/2330 train_time:133214ms step_avg:59.74ms
step:2231/2330 train_time:133270ms step_avg:59.74ms
step:2232/2330 train_time:133333ms step_avg:59.74ms
step:2233/2330 train_time:133390ms step_avg:59.74ms
step:2234/2330 train_time:133454ms step_avg:59.74ms
step:2235/2330 train_time:133511ms step_avg:59.74ms
step:2236/2330 train_time:133574ms step_avg:59.74ms
step:2237/2330 train_time:133631ms step_avg:59.74ms
step:2238/2330 train_time:133695ms step_avg:59.74ms
step:2239/2330 train_time:133752ms step_avg:59.74ms
step:2240/2330 train_time:133814ms step_avg:59.74ms
step:2241/2330 train_time:133872ms step_avg:59.74ms
step:2242/2330 train_time:133934ms step_avg:59.74ms
step:2243/2330 train_time:133992ms step_avg:59.74ms
step:2244/2330 train_time:134054ms step_avg:59.74ms
step:2245/2330 train_time:134111ms step_avg:59.74ms
step:2246/2330 train_time:134173ms step_avg:59.74ms
step:2247/2330 train_time:134231ms step_avg:59.74ms
step:2248/2330 train_time:134294ms step_avg:59.74ms
step:2249/2330 train_time:134351ms step_avg:59.74ms
step:2250/2330 train_time:134414ms step_avg:59.74ms
step:2250/2330 val_loss:3.7413 train_time:134494ms step_avg:59.78ms
step:2251/2330 train_time:134513ms step_avg:59.76ms
step:2252/2330 train_time:134537ms step_avg:59.74ms
step:2253/2330 train_time:134597ms step_avg:59.74ms
step:2254/2330 train_time:134664ms step_avg:59.74ms
step:2255/2330 train_time:134722ms step_avg:59.74ms
step:2256/2330 train_time:134784ms step_avg:59.74ms
step:2257/2330 train_time:134842ms step_avg:59.74ms
step:2258/2330 train_time:134904ms step_avg:59.74ms
step:2259/2330 train_time:134961ms step_avg:59.74ms
step:2260/2330 train_time:135023ms step_avg:59.74ms
step:2261/2330 train_time:135080ms step_avg:59.74ms
step:2262/2330 train_time:135141ms step_avg:59.74ms
step:2263/2330 train_time:135198ms step_avg:59.74ms
step:2264/2330 train_time:135260ms step_avg:59.74ms
step:2265/2330 train_time:135317ms step_avg:59.74ms
step:2266/2330 train_time:135380ms step_avg:59.74ms
step:2267/2330 train_time:135438ms step_avg:59.74ms
step:2268/2330 train_time:135502ms step_avg:59.75ms
step:2269/2330 train_time:135560ms step_avg:59.74ms
step:2270/2330 train_time:135625ms step_avg:59.75ms
step:2271/2330 train_time:135684ms step_avg:59.75ms
step:2272/2330 train_time:135747ms step_avg:59.75ms
step:2273/2330 train_time:135804ms step_avg:59.75ms
step:2274/2330 train_time:135867ms step_avg:59.75ms
step:2275/2330 train_time:135924ms step_avg:59.75ms
step:2276/2330 train_time:135986ms step_avg:59.75ms
step:2277/2330 train_time:136043ms step_avg:59.75ms
step:2278/2330 train_time:136104ms step_avg:59.75ms
step:2279/2330 train_time:136161ms step_avg:59.75ms
step:2280/2330 train_time:136223ms step_avg:59.75ms
step:2281/2330 train_time:136280ms step_avg:59.75ms
step:2282/2330 train_time:136342ms step_avg:59.75ms
step:2283/2330 train_time:136399ms step_avg:59.75ms
step:2284/2330 train_time:136464ms step_avg:59.75ms
step:2285/2330 train_time:136521ms step_avg:59.75ms
step:2286/2330 train_time:136585ms step_avg:59.75ms
step:2287/2330 train_time:136642ms step_avg:59.75ms
step:2288/2330 train_time:136706ms step_avg:59.75ms
step:2289/2330 train_time:136763ms step_avg:59.75ms
step:2290/2330 train_time:136827ms step_avg:59.75ms
step:2291/2330 train_time:136885ms step_avg:59.75ms
step:2292/2330 train_time:136946ms step_avg:59.75ms
step:2293/2330 train_time:137003ms step_avg:59.75ms
step:2294/2330 train_time:137065ms step_avg:59.75ms
step:2295/2330 train_time:137122ms step_avg:59.75ms
step:2296/2330 train_time:137184ms step_avg:59.75ms
step:2297/2330 train_time:137241ms step_avg:59.75ms
step:2298/2330 train_time:137304ms step_avg:59.75ms
step:2299/2330 train_time:137361ms step_avg:59.75ms
step:2300/2330 train_time:137424ms step_avg:59.75ms
step:2301/2330 train_time:137481ms step_avg:59.75ms
step:2302/2330 train_time:137545ms step_avg:59.75ms
step:2303/2330 train_time:137602ms step_avg:59.75ms
step:2304/2330 train_time:137665ms step_avg:59.75ms
step:2305/2330 train_time:137723ms step_avg:59.75ms
step:2306/2330 train_time:137786ms step_avg:59.75ms
step:2307/2330 train_time:137843ms step_avg:59.75ms
step:2308/2330 train_time:137905ms step_avg:59.75ms
step:2309/2330 train_time:137963ms step_avg:59.75ms
step:2310/2330 train_time:138024ms step_avg:59.75ms
step:2311/2330 train_time:138082ms step_avg:59.75ms
step:2312/2330 train_time:138144ms step_avg:59.75ms
step:2313/2330 train_time:138200ms step_avg:59.75ms
step:2314/2330 train_time:138263ms step_avg:59.75ms
step:2315/2330 train_time:138320ms step_avg:59.75ms
step:2316/2330 train_time:138382ms step_avg:59.75ms
step:2317/2330 train_time:138440ms step_avg:59.75ms
step:2318/2330 train_time:138503ms step_avg:59.75ms
step:2319/2330 train_time:138560ms step_avg:59.75ms
step:2320/2330 train_time:138623ms step_avg:59.75ms
step:2321/2330 train_time:138681ms step_avg:59.75ms
step:2322/2330 train_time:138744ms step_avg:59.75ms
step:2323/2330 train_time:138801ms step_avg:59.75ms
step:2324/2330 train_time:138864ms step_avg:59.75ms
step:2325/2330 train_time:138922ms step_avg:59.75ms
step:2326/2330 train_time:138984ms step_avg:59.75ms
step:2327/2330 train_time:139041ms step_avg:59.75ms
step:2328/2330 train_time:139104ms step_avg:59.75ms
step:2329/2330 train_time:139161ms step_avg:59.75ms
step:2330/2330 train_time:139223ms step_avg:59.75ms
step:2330/2330 val_loss:3.7262 train_time:139303ms step_avg:59.79ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
