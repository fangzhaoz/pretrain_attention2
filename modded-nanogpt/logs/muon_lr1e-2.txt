import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "muon_lr1e-2"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-2, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Oct 28 23:18:01 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   34C    P0             113W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   32C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   30C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:88ms step_avg:87.80ms
step:2/2330 train_time:198ms step_avg:98.80ms
step:3/2330 train_time:220ms step_avg:73.33ms
step:4/2330 train_time:254ms step_avg:63.58ms
step:5/2330 train_time:312ms step_avg:62.35ms
step:6/2330 train_time:372ms step_avg:62.05ms
step:7/2330 train_time:431ms step_avg:61.56ms
step:8/2330 train_time:492ms step_avg:61.49ms
step:9/2330 train_time:550ms step_avg:61.15ms
step:10/2330 train_time:611ms step_avg:61.10ms
step:11/2330 train_time:670ms step_avg:60.88ms
step:12/2330 train_time:730ms step_avg:60.86ms
step:13/2330 train_time:789ms step_avg:60.68ms
step:14/2330 train_time:850ms step_avg:60.70ms
step:15/2330 train_time:908ms step_avg:60.56ms
step:16/2330 train_time:970ms step_avg:60.61ms
step:17/2330 train_time:1032ms step_avg:60.68ms
step:18/2330 train_time:1098ms step_avg:61.01ms
step:19/2330 train_time:1162ms step_avg:61.15ms
step:20/2330 train_time:1225ms step_avg:61.24ms
step:21/2330 train_time:1284ms step_avg:61.15ms
step:22/2330 train_time:1346ms step_avg:61.19ms
step:23/2330 train_time:1405ms step_avg:61.11ms
step:24/2330 train_time:1467ms step_avg:61.12ms
step:25/2330 train_time:1526ms step_avg:61.04ms
step:26/2330 train_time:1587ms step_avg:61.04ms
step:27/2330 train_time:1646ms step_avg:60.98ms
step:28/2330 train_time:1708ms step_avg:60.99ms
step:29/2330 train_time:1766ms step_avg:60.91ms
step:30/2330 train_time:1828ms step_avg:60.92ms
step:31/2330 train_time:1888ms step_avg:60.89ms
step:32/2330 train_time:1949ms step_avg:60.91ms
step:33/2330 train_time:2009ms step_avg:60.89ms
step:34/2330 train_time:2071ms step_avg:60.93ms
step:35/2330 train_time:2132ms step_avg:60.91ms
step:36/2330 train_time:2195ms step_avg:60.96ms
step:37/2330 train_time:2255ms step_avg:60.95ms
step:38/2330 train_time:2318ms step_avg:60.99ms
step:39/2330 train_time:2378ms step_avg:60.97ms
step:40/2330 train_time:2440ms step_avg:61.00ms
step:41/2330 train_time:2500ms step_avg:60.97ms
step:42/2330 train_time:2562ms step_avg:60.99ms
step:43/2330 train_time:2621ms step_avg:60.96ms
step:44/2330 train_time:2684ms step_avg:60.99ms
step:45/2330 train_time:2743ms step_avg:60.96ms
step:46/2330 train_time:2805ms step_avg:60.97ms
step:47/2330 train_time:2864ms step_avg:60.94ms
step:48/2330 train_time:2926ms step_avg:60.96ms
step:49/2330 train_time:2986ms step_avg:60.93ms
step:50/2330 train_time:3048ms step_avg:60.95ms
step:51/2330 train_time:3108ms step_avg:60.93ms
step:52/2330 train_time:3169ms step_avg:60.95ms
step:53/2330 train_time:3229ms step_avg:60.92ms
step:54/2330 train_time:3291ms step_avg:60.95ms
step:55/2330 train_time:3352ms step_avg:60.94ms
step:56/2330 train_time:3413ms step_avg:60.94ms
step:57/2330 train_time:3472ms step_avg:60.92ms
step:58/2330 train_time:3535ms step_avg:60.95ms
step:59/2330 train_time:3594ms step_avg:60.92ms
step:60/2330 train_time:3656ms step_avg:60.94ms
step:61/2330 train_time:3717ms step_avg:60.94ms
step:62/2330 train_time:3779ms step_avg:60.95ms
step:63/2330 train_time:3839ms step_avg:60.94ms
step:64/2330 train_time:3901ms step_avg:60.95ms
step:65/2330 train_time:3961ms step_avg:60.95ms
step:66/2330 train_time:4024ms step_avg:60.97ms
step:67/2330 train_time:4084ms step_avg:60.96ms
step:68/2330 train_time:4146ms step_avg:60.96ms
step:69/2330 train_time:4205ms step_avg:60.95ms
step:70/2330 train_time:4267ms step_avg:60.96ms
step:71/2330 train_time:4326ms step_avg:60.93ms
step:72/2330 train_time:4387ms step_avg:60.93ms
step:73/2330 train_time:4447ms step_avg:60.92ms
step:74/2330 train_time:4509ms step_avg:60.93ms
step:75/2330 train_time:4569ms step_avg:60.92ms
step:76/2330 train_time:4631ms step_avg:60.93ms
step:77/2330 train_time:4690ms step_avg:60.91ms
step:78/2330 train_time:4752ms step_avg:60.92ms
step:79/2330 train_time:4811ms step_avg:60.90ms
step:80/2330 train_time:4874ms step_avg:60.92ms
step:81/2330 train_time:4934ms step_avg:60.92ms
step:82/2330 train_time:4997ms step_avg:60.94ms
step:83/2330 train_time:5057ms step_avg:60.93ms
step:84/2330 train_time:5119ms step_avg:60.94ms
step:85/2330 train_time:5179ms step_avg:60.93ms
step:86/2330 train_time:5241ms step_avg:60.94ms
step:87/2330 train_time:5301ms step_avg:60.93ms
step:88/2330 train_time:5364ms step_avg:60.95ms
step:89/2330 train_time:5424ms step_avg:60.94ms
step:90/2330 train_time:5485ms step_avg:60.95ms
step:91/2330 train_time:5545ms step_avg:60.93ms
step:92/2330 train_time:5606ms step_avg:60.93ms
step:93/2330 train_time:5665ms step_avg:60.91ms
step:94/2330 train_time:5727ms step_avg:60.93ms
step:95/2330 train_time:5786ms step_avg:60.91ms
step:96/2330 train_time:5848ms step_avg:60.92ms
step:97/2330 train_time:5908ms step_avg:60.91ms
step:98/2330 train_time:5971ms step_avg:60.93ms
step:99/2330 train_time:6031ms step_avg:60.92ms
step:100/2330 train_time:6092ms step_avg:60.92ms
step:101/2330 train_time:6152ms step_avg:60.92ms
step:102/2330 train_time:6214ms step_avg:60.93ms
step:103/2330 train_time:6275ms step_avg:60.92ms
step:104/2330 train_time:6337ms step_avg:60.93ms
step:105/2330 train_time:6397ms step_avg:60.92ms
step:106/2330 train_time:6459ms step_avg:60.93ms
step:107/2330 train_time:6519ms step_avg:60.93ms
step:108/2330 train_time:6581ms step_avg:60.93ms
step:109/2330 train_time:6640ms step_avg:60.92ms
step:110/2330 train_time:6702ms step_avg:60.93ms
step:111/2330 train_time:6763ms step_avg:60.93ms
step:112/2330 train_time:6825ms step_avg:60.94ms
step:113/2330 train_time:6885ms step_avg:60.93ms
step:114/2330 train_time:6947ms step_avg:60.94ms
step:115/2330 train_time:7006ms step_avg:60.92ms
step:116/2330 train_time:7068ms step_avg:60.93ms
step:117/2330 train_time:7127ms step_avg:60.92ms
step:118/2330 train_time:7189ms step_avg:60.92ms
step:119/2330 train_time:7248ms step_avg:60.91ms
step:120/2330 train_time:7310ms step_avg:60.91ms
step:121/2330 train_time:7369ms step_avg:60.90ms
step:122/2330 train_time:7431ms step_avg:60.91ms
step:123/2330 train_time:7491ms step_avg:60.90ms
step:124/2330 train_time:7553ms step_avg:60.91ms
step:125/2330 train_time:7612ms step_avg:60.90ms
step:126/2330 train_time:7675ms step_avg:60.91ms
step:127/2330 train_time:7736ms step_avg:60.91ms
step:128/2330 train_time:7798ms step_avg:60.92ms
step:129/2330 train_time:7858ms step_avg:60.92ms
step:130/2330 train_time:7921ms step_avg:60.93ms
step:131/2330 train_time:7980ms step_avg:60.92ms
step:132/2330 train_time:8042ms step_avg:60.92ms
step:133/2330 train_time:8102ms step_avg:60.91ms
step:134/2330 train_time:8164ms step_avg:60.93ms
step:135/2330 train_time:8224ms step_avg:60.92ms
step:136/2330 train_time:8286ms step_avg:60.92ms
step:137/2330 train_time:8346ms step_avg:60.92ms
step:138/2330 train_time:8407ms step_avg:60.92ms
step:139/2330 train_time:8467ms step_avg:60.91ms
step:140/2330 train_time:8529ms step_avg:60.92ms
step:141/2330 train_time:8588ms step_avg:60.91ms
step:142/2330 train_time:8650ms step_avg:60.92ms
step:143/2330 train_time:8711ms step_avg:60.91ms
step:144/2330 train_time:8772ms step_avg:60.92ms
step:145/2330 train_time:8832ms step_avg:60.91ms
step:146/2330 train_time:8894ms step_avg:60.92ms
step:147/2330 train_time:8954ms step_avg:60.91ms
step:148/2330 train_time:9016ms step_avg:60.92ms
step:149/2330 train_time:9076ms step_avg:60.91ms
step:150/2330 train_time:9138ms step_avg:60.92ms
step:151/2330 train_time:9198ms step_avg:60.91ms
step:152/2330 train_time:9259ms step_avg:60.92ms
step:153/2330 train_time:9319ms step_avg:60.91ms
step:154/2330 train_time:9381ms step_avg:60.92ms
step:155/2330 train_time:9442ms step_avg:60.91ms
step:156/2330 train_time:9503ms step_avg:60.92ms
step:157/2330 train_time:9564ms step_avg:60.92ms
step:158/2330 train_time:9626ms step_avg:60.92ms
step:159/2330 train_time:9686ms step_avg:60.92ms
step:160/2330 train_time:9747ms step_avg:60.92ms
step:161/2330 train_time:9807ms step_avg:60.91ms
step:162/2330 train_time:9868ms step_avg:60.92ms
step:163/2330 train_time:9928ms step_avg:60.91ms
step:164/2330 train_time:9989ms step_avg:60.91ms
step:165/2330 train_time:10050ms step_avg:60.91ms
step:166/2330 train_time:10111ms step_avg:60.91ms
step:167/2330 train_time:10172ms step_avg:60.91ms
step:168/2330 train_time:10234ms step_avg:60.92ms
step:169/2330 train_time:10294ms step_avg:60.91ms
step:170/2330 train_time:10356ms step_avg:60.92ms
step:171/2330 train_time:10416ms step_avg:60.91ms
step:172/2330 train_time:10479ms step_avg:60.92ms
step:173/2330 train_time:10540ms step_avg:60.92ms
step:174/2330 train_time:10601ms step_avg:60.92ms
step:175/2330 train_time:10661ms step_avg:60.92ms
step:176/2330 train_time:10724ms step_avg:60.93ms
step:177/2330 train_time:10783ms step_avg:60.92ms
step:178/2330 train_time:10845ms step_avg:60.93ms
step:179/2330 train_time:10905ms step_avg:60.92ms
step:180/2330 train_time:10967ms step_avg:60.93ms
step:181/2330 train_time:11027ms step_avg:60.92ms
step:182/2330 train_time:11089ms step_avg:60.93ms
step:183/2330 train_time:11148ms step_avg:60.92ms
step:184/2330 train_time:11211ms step_avg:60.93ms
step:185/2330 train_time:11271ms step_avg:60.92ms
step:186/2330 train_time:11333ms step_avg:60.93ms
step:187/2330 train_time:11393ms step_avg:60.92ms
step:188/2330 train_time:11454ms step_avg:60.93ms
step:189/2330 train_time:11514ms step_avg:60.92ms
step:190/2330 train_time:11576ms step_avg:60.93ms
step:191/2330 train_time:11636ms step_avg:60.92ms
step:192/2330 train_time:11698ms step_avg:60.93ms
step:193/2330 train_time:11758ms step_avg:60.92ms
step:194/2330 train_time:11820ms step_avg:60.93ms
step:195/2330 train_time:11880ms step_avg:60.92ms
step:196/2330 train_time:11942ms step_avg:60.93ms
step:197/2330 train_time:12003ms step_avg:60.93ms
step:198/2330 train_time:12065ms step_avg:60.93ms
step:199/2330 train_time:12125ms step_avg:60.93ms
step:200/2330 train_time:12187ms step_avg:60.93ms
step:201/2330 train_time:12247ms step_avg:60.93ms
step:202/2330 train_time:12309ms step_avg:60.93ms
step:203/2330 train_time:12368ms step_avg:60.93ms
step:204/2330 train_time:12430ms step_avg:60.93ms
step:205/2330 train_time:12490ms step_avg:60.93ms
step:206/2330 train_time:12551ms step_avg:60.93ms
step:207/2330 train_time:12611ms step_avg:60.92ms
step:208/2330 train_time:12672ms step_avg:60.92ms
step:209/2330 train_time:12732ms step_avg:60.92ms
step:210/2330 train_time:12794ms step_avg:60.92ms
step:211/2330 train_time:12854ms step_avg:60.92ms
step:212/2330 train_time:12917ms step_avg:60.93ms
step:213/2330 train_time:12977ms step_avg:60.92ms
step:214/2330 train_time:13040ms step_avg:60.93ms
step:215/2330 train_time:13100ms step_avg:60.93ms
step:216/2330 train_time:13162ms step_avg:60.93ms
step:217/2330 train_time:13222ms step_avg:60.93ms
step:218/2330 train_time:13284ms step_avg:60.94ms
step:219/2330 train_time:13344ms step_avg:60.93ms
step:220/2330 train_time:13405ms step_avg:60.93ms
step:221/2330 train_time:13465ms step_avg:60.93ms
step:222/2330 train_time:13526ms step_avg:60.93ms
step:223/2330 train_time:13586ms step_avg:60.92ms
step:224/2330 train_time:13648ms step_avg:60.93ms
step:225/2330 train_time:13708ms step_avg:60.92ms
step:226/2330 train_time:13770ms step_avg:60.93ms
step:227/2330 train_time:13829ms step_avg:60.92ms
step:228/2330 train_time:13891ms step_avg:60.93ms
step:229/2330 train_time:13951ms step_avg:60.92ms
step:230/2330 train_time:14013ms step_avg:60.93ms
step:231/2330 train_time:14073ms step_avg:60.92ms
step:232/2330 train_time:14136ms step_avg:60.93ms
step:233/2330 train_time:14196ms step_avg:60.93ms
step:234/2330 train_time:14258ms step_avg:60.93ms
step:235/2330 train_time:14318ms step_avg:60.93ms
step:236/2330 train_time:14380ms step_avg:60.93ms
step:237/2330 train_time:14439ms step_avg:60.93ms
step:238/2330 train_time:14501ms step_avg:60.93ms
step:239/2330 train_time:14562ms step_avg:60.93ms
step:240/2330 train_time:14625ms step_avg:60.94ms
step:241/2330 train_time:14685ms step_avg:60.93ms
step:242/2330 train_time:14746ms step_avg:60.93ms
step:243/2330 train_time:14805ms step_avg:60.93ms
step:244/2330 train_time:14867ms step_avg:60.93ms
step:245/2330 train_time:14927ms step_avg:60.93ms
step:246/2330 train_time:14988ms step_avg:60.93ms
step:247/2330 train_time:15048ms step_avg:60.92ms
step:248/2330 train_time:15110ms step_avg:60.93ms
step:249/2330 train_time:15170ms step_avg:60.92ms
step:250/2330 train_time:15231ms step_avg:60.92ms
step:250/2330 val_loss:4.1416 train_time:15295ms step_avg:61.18ms
step:251/2330 train_time:15319ms step_avg:61.03ms
step:252/2330 train_time:15354ms step_avg:60.93ms
step:253/2330 train_time:15419ms step_avg:60.95ms
step:254/2330 train_time:15485ms step_avg:60.96ms
step:255/2330 train_time:15544ms step_avg:60.96ms
step:256/2330 train_time:15606ms step_avg:60.96ms
step:257/2330 train_time:15666ms step_avg:60.96ms
step:258/2330 train_time:15728ms step_avg:60.96ms
step:259/2330 train_time:15787ms step_avg:60.95ms
step:260/2330 train_time:15848ms step_avg:60.95ms
step:261/2330 train_time:15907ms step_avg:60.95ms
step:262/2330 train_time:15968ms step_avg:60.95ms
step:263/2330 train_time:16027ms step_avg:60.94ms
step:264/2330 train_time:16088ms step_avg:60.94ms
step:265/2330 train_time:16147ms step_avg:60.93ms
step:266/2330 train_time:16208ms step_avg:60.93ms
step:267/2330 train_time:16272ms step_avg:60.94ms
step:268/2330 train_time:16335ms step_avg:60.95ms
step:269/2330 train_time:16395ms step_avg:60.95ms
step:270/2330 train_time:16458ms step_avg:60.96ms
step:271/2330 train_time:16518ms step_avg:60.95ms
step:272/2330 train_time:16581ms step_avg:60.96ms
step:273/2330 train_time:16641ms step_avg:60.95ms
step:274/2330 train_time:16702ms step_avg:60.96ms
step:275/2330 train_time:16761ms step_avg:60.95ms
step:276/2330 train_time:16823ms step_avg:60.95ms
step:277/2330 train_time:16882ms step_avg:60.95ms
step:278/2330 train_time:16943ms step_avg:60.95ms
step:279/2330 train_time:17003ms step_avg:60.94ms
step:280/2330 train_time:17064ms step_avg:60.94ms
step:281/2330 train_time:17123ms step_avg:60.94ms
step:282/2330 train_time:17185ms step_avg:60.94ms
step:283/2330 train_time:17246ms step_avg:60.94ms
step:284/2330 train_time:17309ms step_avg:60.95ms
step:285/2330 train_time:17369ms step_avg:60.94ms
step:286/2330 train_time:17431ms step_avg:60.95ms
step:287/2330 train_time:17491ms step_avg:60.94ms
step:288/2330 train_time:17552ms step_avg:60.95ms
step:289/2330 train_time:17612ms step_avg:60.94ms
step:290/2330 train_time:17673ms step_avg:60.94ms
step:291/2330 train_time:17733ms step_avg:60.94ms
step:292/2330 train_time:17795ms step_avg:60.94ms
step:293/2330 train_time:17855ms step_avg:60.94ms
step:294/2330 train_time:17917ms step_avg:60.94ms
step:295/2330 train_time:17977ms step_avg:60.94ms
step:296/2330 train_time:18039ms step_avg:60.94ms
step:297/2330 train_time:18098ms step_avg:60.94ms
step:298/2330 train_time:18160ms step_avg:60.94ms
step:299/2330 train_time:18220ms step_avg:60.94ms
step:300/2330 train_time:18281ms step_avg:60.94ms
step:301/2330 train_time:18342ms step_avg:60.94ms
step:302/2330 train_time:18404ms step_avg:60.94ms
step:303/2330 train_time:18464ms step_avg:60.94ms
step:304/2330 train_time:18527ms step_avg:60.94ms
step:305/2330 train_time:18588ms step_avg:60.94ms
step:306/2330 train_time:18650ms step_avg:60.95ms
step:307/2330 train_time:18710ms step_avg:60.94ms
step:308/2330 train_time:18771ms step_avg:60.94ms
step:309/2330 train_time:18830ms step_avg:60.94ms
step:310/2330 train_time:18892ms step_avg:60.94ms
step:311/2330 train_time:18950ms step_avg:60.93ms
step:312/2330 train_time:19013ms step_avg:60.94ms
step:313/2330 train_time:19073ms step_avg:60.94ms
step:314/2330 train_time:19135ms step_avg:60.94ms
step:315/2330 train_time:19194ms step_avg:60.93ms
step:316/2330 train_time:19256ms step_avg:60.94ms
step:317/2330 train_time:19316ms step_avg:60.93ms
step:318/2330 train_time:19378ms step_avg:60.94ms
step:319/2330 train_time:19438ms step_avg:60.93ms
step:320/2330 train_time:19501ms step_avg:60.94ms
step:321/2330 train_time:19561ms step_avg:60.94ms
step:322/2330 train_time:19623ms step_avg:60.94ms
step:323/2330 train_time:19682ms step_avg:60.94ms
step:324/2330 train_time:19745ms step_avg:60.94ms
step:325/2330 train_time:19805ms step_avg:60.94ms
step:326/2330 train_time:19866ms step_avg:60.94ms
step:327/2330 train_time:19926ms step_avg:60.94ms
step:328/2330 train_time:19987ms step_avg:60.94ms
step:329/2330 train_time:20047ms step_avg:60.93ms
step:330/2330 train_time:20109ms step_avg:60.94ms
step:331/2330 train_time:20169ms step_avg:60.93ms
step:332/2330 train_time:20231ms step_avg:60.94ms
step:333/2330 train_time:20290ms step_avg:60.93ms
step:334/2330 train_time:20352ms step_avg:60.93ms
step:335/2330 train_time:20411ms step_avg:60.93ms
step:336/2330 train_time:20473ms step_avg:60.93ms
step:337/2330 train_time:20533ms step_avg:60.93ms
step:338/2330 train_time:20595ms step_avg:60.93ms
step:339/2330 train_time:20655ms step_avg:60.93ms
step:340/2330 train_time:20717ms step_avg:60.93ms
step:341/2330 train_time:20777ms step_avg:60.93ms
step:342/2330 train_time:20839ms step_avg:60.93ms
step:343/2330 train_time:20898ms step_avg:60.93ms
step:344/2330 train_time:20960ms step_avg:60.93ms
step:345/2330 train_time:21020ms step_avg:60.93ms
step:346/2330 train_time:21082ms step_avg:60.93ms
step:347/2330 train_time:21142ms step_avg:60.93ms
step:348/2330 train_time:21204ms step_avg:60.93ms
step:349/2330 train_time:21265ms step_avg:60.93ms
step:350/2330 train_time:21327ms step_avg:60.93ms
step:351/2330 train_time:21386ms step_avg:60.93ms
step:352/2330 train_time:21448ms step_avg:60.93ms
step:353/2330 train_time:21508ms step_avg:60.93ms
step:354/2330 train_time:21570ms step_avg:60.93ms
step:355/2330 train_time:21630ms step_avg:60.93ms
step:356/2330 train_time:21691ms step_avg:60.93ms
step:357/2330 train_time:21751ms step_avg:60.93ms
step:358/2330 train_time:21812ms step_avg:60.93ms
step:359/2330 train_time:21872ms step_avg:60.92ms
step:360/2330 train_time:21934ms step_avg:60.93ms
step:361/2330 train_time:21993ms step_avg:60.92ms
step:362/2330 train_time:22055ms step_avg:60.93ms
step:363/2330 train_time:22115ms step_avg:60.92ms
step:364/2330 train_time:22177ms step_avg:60.93ms
step:365/2330 train_time:22238ms step_avg:60.92ms
step:366/2330 train_time:22300ms step_avg:60.93ms
step:367/2330 train_time:22361ms step_avg:60.93ms
step:368/2330 train_time:22423ms step_avg:60.93ms
step:369/2330 train_time:22483ms step_avg:60.93ms
step:370/2330 train_time:22545ms step_avg:60.93ms
step:371/2330 train_time:22605ms step_avg:60.93ms
step:372/2330 train_time:22667ms step_avg:60.93ms
step:373/2330 train_time:22727ms step_avg:60.93ms
step:374/2330 train_time:22789ms step_avg:60.93ms
step:375/2330 train_time:22848ms step_avg:60.93ms
step:376/2330 train_time:22910ms step_avg:60.93ms
step:377/2330 train_time:22969ms step_avg:60.93ms
step:378/2330 train_time:23031ms step_avg:60.93ms
step:379/2330 train_time:23091ms step_avg:60.93ms
step:380/2330 train_time:23153ms step_avg:60.93ms
step:381/2330 train_time:23214ms step_avg:60.93ms
step:382/2330 train_time:23275ms step_avg:60.93ms
step:383/2330 train_time:23335ms step_avg:60.93ms
step:384/2330 train_time:23397ms step_avg:60.93ms
step:385/2330 train_time:23457ms step_avg:60.93ms
step:386/2330 train_time:23520ms step_avg:60.93ms
step:387/2330 train_time:23580ms step_avg:60.93ms
step:388/2330 train_time:23642ms step_avg:60.93ms
step:389/2330 train_time:23702ms step_avg:60.93ms
step:390/2330 train_time:23765ms step_avg:60.94ms
step:391/2330 train_time:23825ms step_avg:60.93ms
step:392/2330 train_time:23887ms step_avg:60.94ms
step:393/2330 train_time:23947ms step_avg:60.93ms
step:394/2330 train_time:24008ms step_avg:60.93ms
step:395/2330 train_time:24068ms step_avg:60.93ms
step:396/2330 train_time:24130ms step_avg:60.93ms
step:397/2330 train_time:24189ms step_avg:60.93ms
step:398/2330 train_time:24251ms step_avg:60.93ms
step:399/2330 train_time:24311ms step_avg:60.93ms
step:400/2330 train_time:24373ms step_avg:60.93ms
step:401/2330 train_time:24433ms step_avg:60.93ms
step:402/2330 train_time:24495ms step_avg:60.93ms
step:403/2330 train_time:24554ms step_avg:60.93ms
step:404/2330 train_time:24617ms step_avg:60.93ms
step:405/2330 train_time:24677ms step_avg:60.93ms
step:406/2330 train_time:24739ms step_avg:60.93ms
step:407/2330 train_time:24798ms step_avg:60.93ms
step:408/2330 train_time:24861ms step_avg:60.93ms
step:409/2330 train_time:24921ms step_avg:60.93ms
step:410/2330 train_time:24983ms step_avg:60.93ms
step:411/2330 train_time:25043ms step_avg:60.93ms
step:412/2330 train_time:25106ms step_avg:60.94ms
step:413/2330 train_time:25166ms step_avg:60.93ms
step:414/2330 train_time:25228ms step_avg:60.94ms
step:415/2330 train_time:25288ms step_avg:60.93ms
step:416/2330 train_time:25349ms step_avg:60.94ms
step:417/2330 train_time:25409ms step_avg:60.93ms
step:418/2330 train_time:25470ms step_avg:60.93ms
step:419/2330 train_time:25530ms step_avg:60.93ms
step:420/2330 train_time:25592ms step_avg:60.93ms
step:421/2330 train_time:25651ms step_avg:60.93ms
step:422/2330 train_time:25713ms step_avg:60.93ms
step:423/2330 train_time:25773ms step_avg:60.93ms
step:424/2330 train_time:25835ms step_avg:60.93ms
step:425/2330 train_time:25895ms step_avg:60.93ms
step:426/2330 train_time:25957ms step_avg:60.93ms
step:427/2330 train_time:26018ms step_avg:60.93ms
step:428/2330 train_time:26080ms step_avg:60.93ms
step:429/2330 train_time:26140ms step_avg:60.93ms
step:430/2330 train_time:26202ms step_avg:60.93ms
step:431/2330 train_time:26261ms step_avg:60.93ms
step:432/2330 train_time:26323ms step_avg:60.93ms
step:433/2330 train_time:26383ms step_avg:60.93ms
step:434/2330 train_time:26445ms step_avg:60.93ms
step:435/2330 train_time:26505ms step_avg:60.93ms
step:436/2330 train_time:26567ms step_avg:60.93ms
step:437/2330 train_time:26628ms step_avg:60.93ms
step:438/2330 train_time:26689ms step_avg:60.93ms
step:439/2330 train_time:26748ms step_avg:60.93ms
step:440/2330 train_time:26809ms step_avg:60.93ms
step:441/2330 train_time:26869ms step_avg:60.93ms
step:442/2330 train_time:26931ms step_avg:60.93ms
step:443/2330 train_time:26991ms step_avg:60.93ms
step:444/2330 train_time:27053ms step_avg:60.93ms
step:445/2330 train_time:27112ms step_avg:60.93ms
step:446/2330 train_time:27174ms step_avg:60.93ms
step:447/2330 train_time:27234ms step_avg:60.93ms
step:448/2330 train_time:27296ms step_avg:60.93ms
step:449/2330 train_time:27356ms step_avg:60.93ms
step:450/2330 train_time:27419ms step_avg:60.93ms
step:451/2330 train_time:27479ms step_avg:60.93ms
step:452/2330 train_time:27541ms step_avg:60.93ms
step:453/2330 train_time:27601ms step_avg:60.93ms
step:454/2330 train_time:27663ms step_avg:60.93ms
step:455/2330 train_time:27722ms step_avg:60.93ms
step:456/2330 train_time:27784ms step_avg:60.93ms
step:457/2330 train_time:27845ms step_avg:60.93ms
step:458/2330 train_time:27907ms step_avg:60.93ms
step:459/2330 train_time:27967ms step_avg:60.93ms
step:460/2330 train_time:28029ms step_avg:60.93ms
step:461/2330 train_time:28088ms step_avg:60.93ms
step:462/2330 train_time:28150ms step_avg:60.93ms
step:463/2330 train_time:28209ms step_avg:60.93ms
step:464/2330 train_time:28271ms step_avg:60.93ms
step:465/2330 train_time:28331ms step_avg:60.93ms
step:466/2330 train_time:28393ms step_avg:60.93ms
step:467/2330 train_time:28453ms step_avg:60.93ms
step:468/2330 train_time:28515ms step_avg:60.93ms
step:469/2330 train_time:28575ms step_avg:60.93ms
step:470/2330 train_time:28637ms step_avg:60.93ms
step:471/2330 train_time:28697ms step_avg:60.93ms
step:472/2330 train_time:28759ms step_avg:60.93ms
step:473/2330 train_time:28819ms step_avg:60.93ms
step:474/2330 train_time:28880ms step_avg:60.93ms
step:475/2330 train_time:28941ms step_avg:60.93ms
step:476/2330 train_time:29003ms step_avg:60.93ms
step:477/2330 train_time:29062ms step_avg:60.93ms
step:478/2330 train_time:29124ms step_avg:60.93ms
step:479/2330 train_time:29184ms step_avg:60.93ms
step:480/2330 train_time:29246ms step_avg:60.93ms
step:481/2330 train_time:29306ms step_avg:60.93ms
step:482/2330 train_time:29368ms step_avg:60.93ms
step:483/2330 train_time:29428ms step_avg:60.93ms
step:484/2330 train_time:29489ms step_avg:60.93ms
step:485/2330 train_time:29549ms step_avg:60.92ms
step:486/2330 train_time:29610ms step_avg:60.93ms
step:487/2330 train_time:29670ms step_avg:60.92ms
step:488/2330 train_time:29732ms step_avg:60.93ms
step:489/2330 train_time:29792ms step_avg:60.92ms
step:490/2330 train_time:29854ms step_avg:60.93ms
step:491/2330 train_time:29914ms step_avg:60.92ms
step:492/2330 train_time:29976ms step_avg:60.93ms
step:493/2330 train_time:30036ms step_avg:60.93ms
step:494/2330 train_time:30098ms step_avg:60.93ms
step:495/2330 train_time:30157ms step_avg:60.92ms
step:496/2330 train_time:30220ms step_avg:60.93ms
step:497/2330 train_time:30280ms step_avg:60.93ms
step:498/2330 train_time:30342ms step_avg:60.93ms
step:499/2330 train_time:30402ms step_avg:60.93ms
step:500/2330 train_time:30464ms step_avg:60.93ms
step:500/2330 val_loss:3.8481 train_time:30528ms step_avg:61.06ms
step:501/2330 train_time:30550ms step_avg:60.98ms
step:502/2330 train_time:30591ms step_avg:60.94ms
step:503/2330 train_time:30654ms step_avg:60.94ms
step:504/2330 train_time:30716ms step_avg:60.95ms
step:505/2330 train_time:30776ms step_avg:60.94ms
step:506/2330 train_time:30839ms step_avg:60.95ms
step:507/2330 train_time:30897ms step_avg:60.94ms
step:508/2330 train_time:30958ms step_avg:60.94ms
step:509/2330 train_time:31017ms step_avg:60.94ms
step:510/2330 train_time:31078ms step_avg:60.94ms
step:511/2330 train_time:31138ms step_avg:60.93ms
step:512/2330 train_time:31199ms step_avg:60.94ms
step:513/2330 train_time:31258ms step_avg:60.93ms
step:514/2330 train_time:31320ms step_avg:60.93ms
step:515/2330 train_time:31379ms step_avg:60.93ms
step:516/2330 train_time:31441ms step_avg:60.93ms
step:517/2330 train_time:31502ms step_avg:60.93ms
step:518/2330 train_time:31566ms step_avg:60.94ms
step:519/2330 train_time:31628ms step_avg:60.94ms
step:520/2330 train_time:31691ms step_avg:60.94ms
step:521/2330 train_time:31751ms step_avg:60.94ms
step:522/2330 train_time:31813ms step_avg:60.94ms
step:523/2330 train_time:31872ms step_avg:60.94ms
step:524/2330 train_time:31933ms step_avg:60.94ms
step:525/2330 train_time:31993ms step_avg:60.94ms
step:526/2330 train_time:32055ms step_avg:60.94ms
step:527/2330 train_time:32114ms step_avg:60.94ms
step:528/2330 train_time:32175ms step_avg:60.94ms
step:529/2330 train_time:32234ms step_avg:60.93ms
step:530/2330 train_time:32295ms step_avg:60.93ms
step:531/2330 train_time:32354ms step_avg:60.93ms
step:532/2330 train_time:32417ms step_avg:60.93ms
step:533/2330 train_time:32477ms step_avg:60.93ms
step:534/2330 train_time:32540ms step_avg:60.94ms
step:535/2330 train_time:32601ms step_avg:60.94ms
step:536/2330 train_time:32663ms step_avg:60.94ms
step:537/2330 train_time:32723ms step_avg:60.94ms
step:538/2330 train_time:32786ms step_avg:60.94ms
step:539/2330 train_time:32846ms step_avg:60.94ms
step:540/2330 train_time:32907ms step_avg:60.94ms
step:541/2330 train_time:32968ms step_avg:60.94ms
step:542/2330 train_time:33029ms step_avg:60.94ms
step:543/2330 train_time:33089ms step_avg:60.94ms
step:544/2330 train_time:33151ms step_avg:60.94ms
step:545/2330 train_time:33211ms step_avg:60.94ms
step:546/2330 train_time:33272ms step_avg:60.94ms
step:547/2330 train_time:33331ms step_avg:60.94ms
step:548/2330 train_time:33393ms step_avg:60.94ms
step:549/2330 train_time:33453ms step_avg:60.93ms
step:550/2330 train_time:33514ms step_avg:60.94ms
step:551/2330 train_time:33574ms step_avg:60.93ms
step:552/2330 train_time:33636ms step_avg:60.94ms
step:553/2330 train_time:33697ms step_avg:60.93ms
step:554/2330 train_time:33758ms step_avg:60.93ms
step:555/2330 train_time:33818ms step_avg:60.93ms
step:556/2330 train_time:33880ms step_avg:60.94ms
step:557/2330 train_time:33940ms step_avg:60.93ms
step:558/2330 train_time:34001ms step_avg:60.93ms
step:559/2330 train_time:34061ms step_avg:60.93ms
step:560/2330 train_time:34123ms step_avg:60.93ms
step:561/2330 train_time:34183ms step_avg:60.93ms
step:562/2330 train_time:34245ms step_avg:60.93ms
step:563/2330 train_time:34304ms step_avg:60.93ms
step:564/2330 train_time:34367ms step_avg:60.93ms
step:565/2330 train_time:34427ms step_avg:60.93ms
step:566/2330 train_time:34488ms step_avg:60.93ms
step:567/2330 train_time:34548ms step_avg:60.93ms
step:568/2330 train_time:34610ms step_avg:60.93ms
step:569/2330 train_time:34669ms step_avg:60.93ms
step:570/2330 train_time:34731ms step_avg:60.93ms
step:571/2330 train_time:34791ms step_avg:60.93ms
step:572/2330 train_time:34852ms step_avg:60.93ms
step:573/2330 train_time:34912ms step_avg:60.93ms
step:574/2330 train_time:34974ms step_avg:60.93ms
step:575/2330 train_time:35033ms step_avg:60.93ms
step:576/2330 train_time:35094ms step_avg:60.93ms
step:577/2330 train_time:35154ms step_avg:60.93ms
step:578/2330 train_time:35215ms step_avg:60.93ms
step:579/2330 train_time:35276ms step_avg:60.93ms
step:580/2330 train_time:35337ms step_avg:60.93ms
step:581/2330 train_time:35397ms step_avg:60.92ms
step:582/2330 train_time:35459ms step_avg:60.93ms
step:583/2330 train_time:35518ms step_avg:60.92ms
step:584/2330 train_time:35579ms step_avg:60.92ms
step:585/2330 train_time:35639ms step_avg:60.92ms
step:586/2330 train_time:35701ms step_avg:60.92ms
step:587/2330 train_time:35762ms step_avg:60.92ms
step:588/2330 train_time:35824ms step_avg:60.93ms
step:589/2330 train_time:35884ms step_avg:60.92ms
step:590/2330 train_time:35946ms step_avg:60.93ms
step:591/2330 train_time:36006ms step_avg:60.92ms
step:592/2330 train_time:36068ms step_avg:60.93ms
step:593/2330 train_time:36128ms step_avg:60.92ms
step:594/2330 train_time:36190ms step_avg:60.93ms
step:595/2330 train_time:36250ms step_avg:60.92ms
step:596/2330 train_time:36312ms step_avg:60.93ms
step:597/2330 train_time:36372ms step_avg:60.92ms
step:598/2330 train_time:36433ms step_avg:60.93ms
step:599/2330 train_time:36493ms step_avg:60.92ms
step:600/2330 train_time:36554ms step_avg:60.92ms
step:601/2330 train_time:36614ms step_avg:60.92ms
step:602/2330 train_time:36676ms step_avg:60.92ms
step:603/2330 train_time:36735ms step_avg:60.92ms
step:604/2330 train_time:36797ms step_avg:60.92ms
step:605/2330 train_time:36856ms step_avg:60.92ms
step:606/2330 train_time:36918ms step_avg:60.92ms
step:607/2330 train_time:36978ms step_avg:60.92ms
step:608/2330 train_time:37039ms step_avg:60.92ms
step:609/2330 train_time:37099ms step_avg:60.92ms
step:610/2330 train_time:37161ms step_avg:60.92ms
step:611/2330 train_time:37221ms step_avg:60.92ms
step:612/2330 train_time:37283ms step_avg:60.92ms
step:613/2330 train_time:37343ms step_avg:60.92ms
step:614/2330 train_time:37405ms step_avg:60.92ms
step:615/2330 train_time:37465ms step_avg:60.92ms
step:616/2330 train_time:37527ms step_avg:60.92ms
step:617/2330 train_time:37588ms step_avg:60.92ms
step:618/2330 train_time:37651ms step_avg:60.92ms
step:619/2330 train_time:37711ms step_avg:60.92ms
step:620/2330 train_time:37772ms step_avg:60.92ms
step:621/2330 train_time:37832ms step_avg:60.92ms
step:622/2330 train_time:37893ms step_avg:60.92ms
step:623/2330 train_time:37953ms step_avg:60.92ms
step:624/2330 train_time:38015ms step_avg:60.92ms
step:625/2330 train_time:38073ms step_avg:60.92ms
step:626/2330 train_time:38135ms step_avg:60.92ms
step:627/2330 train_time:38194ms step_avg:60.92ms
step:628/2330 train_time:38256ms step_avg:60.92ms
step:629/2330 train_time:38316ms step_avg:60.92ms
step:630/2330 train_time:38378ms step_avg:60.92ms
step:631/2330 train_time:38438ms step_avg:60.92ms
step:632/2330 train_time:38500ms step_avg:60.92ms
step:633/2330 train_time:38561ms step_avg:60.92ms
step:634/2330 train_time:38623ms step_avg:60.92ms
step:635/2330 train_time:38683ms step_avg:60.92ms
step:636/2330 train_time:38745ms step_avg:60.92ms
step:637/2330 train_time:38804ms step_avg:60.92ms
step:638/2330 train_time:38867ms step_avg:60.92ms
step:639/2330 train_time:38927ms step_avg:60.92ms
step:640/2330 train_time:38988ms step_avg:60.92ms
step:641/2330 train_time:39048ms step_avg:60.92ms
step:642/2330 train_time:39111ms step_avg:60.92ms
step:643/2330 train_time:39171ms step_avg:60.92ms
step:644/2330 train_time:39233ms step_avg:60.92ms
step:645/2330 train_time:39292ms step_avg:60.92ms
step:646/2330 train_time:39354ms step_avg:60.92ms
step:647/2330 train_time:39413ms step_avg:60.92ms
step:648/2330 train_time:39475ms step_avg:60.92ms
step:649/2330 train_time:39535ms step_avg:60.92ms
step:650/2330 train_time:39596ms step_avg:60.92ms
step:651/2330 train_time:39656ms step_avg:60.92ms
step:652/2330 train_time:39719ms step_avg:60.92ms
step:653/2330 train_time:39779ms step_avg:60.92ms
step:654/2330 train_time:39840ms step_avg:60.92ms
step:655/2330 train_time:39900ms step_avg:60.92ms
step:656/2330 train_time:39962ms step_avg:60.92ms
step:657/2330 train_time:40022ms step_avg:60.92ms
step:658/2330 train_time:40085ms step_avg:60.92ms
step:659/2330 train_time:40144ms step_avg:60.92ms
step:660/2330 train_time:40206ms step_avg:60.92ms
step:661/2330 train_time:40266ms step_avg:60.92ms
step:662/2330 train_time:40328ms step_avg:60.92ms
step:663/2330 train_time:40388ms step_avg:60.92ms
step:664/2330 train_time:40450ms step_avg:60.92ms
step:665/2330 train_time:40510ms step_avg:60.92ms
step:666/2330 train_time:40572ms step_avg:60.92ms
step:667/2330 train_time:40632ms step_avg:60.92ms
step:668/2330 train_time:40693ms step_avg:60.92ms
step:669/2330 train_time:40753ms step_avg:60.92ms
step:670/2330 train_time:40814ms step_avg:60.92ms
step:671/2330 train_time:40874ms step_avg:60.91ms
step:672/2330 train_time:40936ms step_avg:60.92ms
step:673/2330 train_time:40996ms step_avg:60.91ms
step:674/2330 train_time:41058ms step_avg:60.92ms
step:675/2330 train_time:41117ms step_avg:60.91ms
step:676/2330 train_time:41179ms step_avg:60.92ms
step:677/2330 train_time:41239ms step_avg:60.91ms
step:678/2330 train_time:41301ms step_avg:60.92ms
step:679/2330 train_time:41361ms step_avg:60.91ms
step:680/2330 train_time:41423ms step_avg:60.92ms
step:681/2330 train_time:41483ms step_avg:60.92ms
step:682/2330 train_time:41546ms step_avg:60.92ms
step:683/2330 train_time:41606ms step_avg:60.92ms
step:684/2330 train_time:41668ms step_avg:60.92ms
step:685/2330 train_time:41727ms step_avg:60.92ms
step:686/2330 train_time:41789ms step_avg:60.92ms
step:687/2330 train_time:41849ms step_avg:60.92ms
step:688/2330 train_time:41911ms step_avg:60.92ms
step:689/2330 train_time:41972ms step_avg:60.92ms
step:690/2330 train_time:42034ms step_avg:60.92ms
step:691/2330 train_time:42093ms step_avg:60.92ms
step:692/2330 train_time:42155ms step_avg:60.92ms
step:693/2330 train_time:42214ms step_avg:60.91ms
step:694/2330 train_time:42275ms step_avg:60.92ms
step:695/2330 train_time:42334ms step_avg:60.91ms
step:696/2330 train_time:42396ms step_avg:60.91ms
step:697/2330 train_time:42456ms step_avg:60.91ms
step:698/2330 train_time:42518ms step_avg:60.91ms
step:699/2330 train_time:42578ms step_avg:60.91ms
step:700/2330 train_time:42639ms step_avg:60.91ms
step:701/2330 train_time:42698ms step_avg:60.91ms
step:702/2330 train_time:42760ms step_avg:60.91ms
step:703/2330 train_time:42820ms step_avg:60.91ms
step:704/2330 train_time:42883ms step_avg:60.91ms
step:705/2330 train_time:42944ms step_avg:60.91ms
step:706/2330 train_time:43006ms step_avg:60.91ms
step:707/2330 train_time:43066ms step_avg:60.91ms
step:708/2330 train_time:43128ms step_avg:60.92ms
step:709/2330 train_time:43188ms step_avg:60.91ms
step:710/2330 train_time:43250ms step_avg:60.92ms
step:711/2330 train_time:43309ms step_avg:60.91ms
step:712/2330 train_time:43371ms step_avg:60.91ms
step:713/2330 train_time:43431ms step_avg:60.91ms
step:714/2330 train_time:43493ms step_avg:60.91ms
step:715/2330 train_time:43552ms step_avg:60.91ms
step:716/2330 train_time:43614ms step_avg:60.91ms
step:717/2330 train_time:43673ms step_avg:60.91ms
step:718/2330 train_time:43735ms step_avg:60.91ms
step:719/2330 train_time:43794ms step_avg:60.91ms
step:720/2330 train_time:43856ms step_avg:60.91ms
step:721/2330 train_time:43916ms step_avg:60.91ms
step:722/2330 train_time:43978ms step_avg:60.91ms
step:723/2330 train_time:44038ms step_avg:60.91ms
step:724/2330 train_time:44100ms step_avg:60.91ms
step:725/2330 train_time:44160ms step_avg:60.91ms
step:726/2330 train_time:44223ms step_avg:60.91ms
step:727/2330 train_time:44282ms step_avg:60.91ms
step:728/2330 train_time:44344ms step_avg:60.91ms
step:729/2330 train_time:44404ms step_avg:60.91ms
step:730/2330 train_time:44466ms step_avg:60.91ms
step:731/2330 train_time:44526ms step_avg:60.91ms
step:732/2330 train_time:44588ms step_avg:60.91ms
step:733/2330 train_time:44648ms step_avg:60.91ms
step:734/2330 train_time:44710ms step_avg:60.91ms
step:735/2330 train_time:44770ms step_avg:60.91ms
step:736/2330 train_time:44832ms step_avg:60.91ms
step:737/2330 train_time:44892ms step_avg:60.91ms
step:738/2330 train_time:44954ms step_avg:60.91ms
step:739/2330 train_time:45014ms step_avg:60.91ms
step:740/2330 train_time:45076ms step_avg:60.91ms
step:741/2330 train_time:45135ms step_avg:60.91ms
step:742/2330 train_time:45197ms step_avg:60.91ms
step:743/2330 train_time:45257ms step_avg:60.91ms
step:744/2330 train_time:45318ms step_avg:60.91ms
step:745/2330 train_time:45378ms step_avg:60.91ms
step:746/2330 train_time:45439ms step_avg:60.91ms
step:747/2330 train_time:45499ms step_avg:60.91ms
step:748/2330 train_time:45561ms step_avg:60.91ms
step:749/2330 train_time:45621ms step_avg:60.91ms
step:750/2330 train_time:45683ms step_avg:60.91ms
step:750/2330 val_loss:3.7057 train_time:45747ms step_avg:61.00ms
step:751/2330 train_time:45770ms step_avg:60.95ms
step:752/2330 train_time:45808ms step_avg:60.91ms
step:753/2330 train_time:45871ms step_avg:60.92ms
step:754/2330 train_time:45936ms step_avg:60.92ms
step:755/2330 train_time:45995ms step_avg:60.92ms
step:756/2330 train_time:46057ms step_avg:60.92ms
step:757/2330 train_time:46116ms step_avg:60.92ms
step:758/2330 train_time:46176ms step_avg:60.92ms
step:759/2330 train_time:46235ms step_avg:60.92ms
step:760/2330 train_time:46296ms step_avg:60.92ms
step:761/2330 train_time:46355ms step_avg:60.91ms
step:762/2330 train_time:46416ms step_avg:60.91ms
step:763/2330 train_time:46475ms step_avg:60.91ms
step:764/2330 train_time:46536ms step_avg:60.91ms
step:765/2330 train_time:46596ms step_avg:60.91ms
step:766/2330 train_time:46659ms step_avg:60.91ms
step:767/2330 train_time:46722ms step_avg:60.91ms
step:768/2330 train_time:46785ms step_avg:60.92ms
step:769/2330 train_time:46847ms step_avg:60.92ms
step:770/2330 train_time:46911ms step_avg:60.92ms
step:771/2330 train_time:46973ms step_avg:60.92ms
step:772/2330 train_time:47036ms step_avg:60.93ms
step:773/2330 train_time:47097ms step_avg:60.93ms
step:774/2330 train_time:47159ms step_avg:60.93ms
step:775/2330 train_time:47218ms step_avg:60.93ms
step:776/2330 train_time:47280ms step_avg:60.93ms
step:777/2330 train_time:47340ms step_avg:60.93ms
step:778/2330 train_time:47401ms step_avg:60.93ms
step:779/2330 train_time:47461ms step_avg:60.93ms
step:780/2330 train_time:47523ms step_avg:60.93ms
step:781/2330 train_time:47583ms step_avg:60.93ms
step:782/2330 train_time:47645ms step_avg:60.93ms
step:783/2330 train_time:47705ms step_avg:60.93ms
step:784/2330 train_time:47768ms step_avg:60.93ms
step:785/2330 train_time:47830ms step_avg:60.93ms
step:786/2330 train_time:47893ms step_avg:60.93ms
step:787/2330 train_time:47954ms step_avg:60.93ms
step:788/2330 train_time:48017ms step_avg:60.94ms
step:789/2330 train_time:48078ms step_avg:60.94ms
step:790/2330 train_time:48140ms step_avg:60.94ms
step:791/2330 train_time:48200ms step_avg:60.94ms
step:792/2330 train_time:48262ms step_avg:60.94ms
step:793/2330 train_time:48322ms step_avg:60.94ms
step:794/2330 train_time:48383ms step_avg:60.94ms
step:795/2330 train_time:48443ms step_avg:60.93ms
step:796/2330 train_time:48505ms step_avg:60.94ms
step:797/2330 train_time:48565ms step_avg:60.94ms
step:798/2330 train_time:48628ms step_avg:60.94ms
step:799/2330 train_time:48688ms step_avg:60.94ms
step:800/2330 train_time:48751ms step_avg:60.94ms
step:801/2330 train_time:48813ms step_avg:60.94ms
step:802/2330 train_time:48875ms step_avg:60.94ms
step:803/2330 train_time:48936ms step_avg:60.94ms
step:804/2330 train_time:48999ms step_avg:60.94ms
step:805/2330 train_time:49060ms step_avg:60.94ms
step:806/2330 train_time:49123ms step_avg:60.95ms
step:807/2330 train_time:49183ms step_avg:60.95ms
step:808/2330 train_time:49245ms step_avg:60.95ms
step:809/2330 train_time:49306ms step_avg:60.95ms
step:810/2330 train_time:49368ms step_avg:60.95ms
step:811/2330 train_time:49428ms step_avg:60.95ms
step:812/2330 train_time:49490ms step_avg:60.95ms
step:813/2330 train_time:49550ms step_avg:60.95ms
step:814/2330 train_time:49613ms step_avg:60.95ms
step:815/2330 train_time:49673ms step_avg:60.95ms
step:816/2330 train_time:49735ms step_avg:60.95ms
step:817/2330 train_time:49796ms step_avg:60.95ms
step:818/2330 train_time:49859ms step_avg:60.95ms
step:819/2330 train_time:49919ms step_avg:60.95ms
step:820/2330 train_time:49982ms step_avg:60.95ms
step:821/2330 train_time:50042ms step_avg:60.95ms
step:822/2330 train_time:50105ms step_avg:60.96ms
step:823/2330 train_time:50166ms step_avg:60.95ms
step:824/2330 train_time:50229ms step_avg:60.96ms
step:825/2330 train_time:50289ms step_avg:60.96ms
step:826/2330 train_time:50352ms step_avg:60.96ms
step:827/2330 train_time:50412ms step_avg:60.96ms
step:828/2330 train_time:50474ms step_avg:60.96ms
step:829/2330 train_time:50534ms step_avg:60.96ms
step:830/2330 train_time:50597ms step_avg:60.96ms
step:831/2330 train_time:50657ms step_avg:60.96ms
step:832/2330 train_time:50720ms step_avg:60.96ms
step:833/2330 train_time:50780ms step_avg:60.96ms
step:834/2330 train_time:50842ms step_avg:60.96ms
step:835/2330 train_time:50903ms step_avg:60.96ms
step:836/2330 train_time:50966ms step_avg:60.96ms
step:837/2330 train_time:51026ms step_avg:60.96ms
step:838/2330 train_time:51088ms step_avg:60.96ms
step:839/2330 train_time:51149ms step_avg:60.96ms
step:840/2330 train_time:51211ms step_avg:60.97ms
step:841/2330 train_time:51271ms step_avg:60.96ms
step:842/2330 train_time:51334ms step_avg:60.97ms
step:843/2330 train_time:51395ms step_avg:60.97ms
step:844/2330 train_time:51457ms step_avg:60.97ms
step:845/2330 train_time:51518ms step_avg:60.97ms
step:846/2330 train_time:51579ms step_avg:60.97ms
step:847/2330 train_time:51640ms step_avg:60.97ms
step:848/2330 train_time:51702ms step_avg:60.97ms
step:849/2330 train_time:51762ms step_avg:60.97ms
step:850/2330 train_time:51825ms step_avg:60.97ms
step:851/2330 train_time:51885ms step_avg:60.97ms
step:852/2330 train_time:51947ms step_avg:60.97ms
step:853/2330 train_time:52008ms step_avg:60.97ms
step:854/2330 train_time:52071ms step_avg:60.97ms
step:855/2330 train_time:52131ms step_avg:60.97ms
step:856/2330 train_time:52193ms step_avg:60.97ms
step:857/2330 train_time:52254ms step_avg:60.97ms
step:858/2330 train_time:52316ms step_avg:60.97ms
step:859/2330 train_time:52376ms step_avg:60.97ms
step:860/2330 train_time:52439ms step_avg:60.98ms
step:861/2330 train_time:52499ms step_avg:60.97ms
step:862/2330 train_time:52561ms step_avg:60.98ms
step:863/2330 train_time:52622ms step_avg:60.98ms
step:864/2330 train_time:52683ms step_avg:60.98ms
step:865/2330 train_time:52743ms step_avg:60.98ms
step:866/2330 train_time:52806ms step_avg:60.98ms
step:867/2330 train_time:52867ms step_avg:60.98ms
step:868/2330 train_time:52930ms step_avg:60.98ms
step:869/2330 train_time:52990ms step_avg:60.98ms
step:870/2330 train_time:53052ms step_avg:60.98ms
step:871/2330 train_time:53113ms step_avg:60.98ms
step:872/2330 train_time:53175ms step_avg:60.98ms
step:873/2330 train_time:53236ms step_avg:60.98ms
step:874/2330 train_time:53299ms step_avg:60.98ms
step:875/2330 train_time:53359ms step_avg:60.98ms
step:876/2330 train_time:53421ms step_avg:60.98ms
step:877/2330 train_time:53481ms step_avg:60.98ms
step:878/2330 train_time:53543ms step_avg:60.98ms
step:879/2330 train_time:53603ms step_avg:60.98ms
step:880/2330 train_time:53665ms step_avg:60.98ms
step:881/2330 train_time:53726ms step_avg:60.98ms
step:882/2330 train_time:53788ms step_avg:60.98ms
step:883/2330 train_time:53848ms step_avg:60.98ms
step:884/2330 train_time:53911ms step_avg:60.99ms
step:885/2330 train_time:53971ms step_avg:60.98ms
step:886/2330 train_time:54034ms step_avg:60.99ms
step:887/2330 train_time:54094ms step_avg:60.99ms
step:888/2330 train_time:54157ms step_avg:60.99ms
step:889/2330 train_time:54217ms step_avg:60.99ms
step:890/2330 train_time:54279ms step_avg:60.99ms
step:891/2330 train_time:54339ms step_avg:60.99ms
step:892/2330 train_time:54402ms step_avg:60.99ms
step:893/2330 train_time:54462ms step_avg:60.99ms
step:894/2330 train_time:54524ms step_avg:60.99ms
step:895/2330 train_time:54584ms step_avg:60.99ms
step:896/2330 train_time:54646ms step_avg:60.99ms
step:897/2330 train_time:54707ms step_avg:60.99ms
step:898/2330 train_time:54770ms step_avg:60.99ms
step:899/2330 train_time:54830ms step_avg:60.99ms
step:900/2330 train_time:54892ms step_avg:60.99ms
step:901/2330 train_time:54953ms step_avg:60.99ms
step:902/2330 train_time:55016ms step_avg:60.99ms
step:903/2330 train_time:55076ms step_avg:60.99ms
step:904/2330 train_time:55138ms step_avg:60.99ms
step:905/2330 train_time:55199ms step_avg:60.99ms
step:906/2330 train_time:55261ms step_avg:60.99ms
step:907/2330 train_time:55321ms step_avg:60.99ms
step:908/2330 train_time:55383ms step_avg:60.99ms
step:909/2330 train_time:55443ms step_avg:60.99ms
step:910/2330 train_time:55505ms step_avg:60.99ms
step:911/2330 train_time:55565ms step_avg:60.99ms
step:912/2330 train_time:55628ms step_avg:61.00ms
step:913/2330 train_time:55688ms step_avg:60.99ms
step:914/2330 train_time:55751ms step_avg:61.00ms
step:915/2330 train_time:55812ms step_avg:61.00ms
step:916/2330 train_time:55874ms step_avg:61.00ms
step:917/2330 train_time:55935ms step_avg:61.00ms
step:918/2330 train_time:55997ms step_avg:61.00ms
step:919/2330 train_time:56058ms step_avg:61.00ms
step:920/2330 train_time:56121ms step_avg:61.00ms
step:921/2330 train_time:56180ms step_avg:61.00ms
step:922/2330 train_time:56242ms step_avg:61.00ms
step:923/2330 train_time:56303ms step_avg:61.00ms
step:924/2330 train_time:56365ms step_avg:61.00ms
step:925/2330 train_time:56425ms step_avg:61.00ms
step:926/2330 train_time:56487ms step_avg:61.00ms
step:927/2330 train_time:56547ms step_avg:61.00ms
step:928/2330 train_time:56610ms step_avg:61.00ms
step:929/2330 train_time:56671ms step_avg:61.00ms
step:930/2330 train_time:56733ms step_avg:61.00ms
step:931/2330 train_time:56793ms step_avg:61.00ms
step:932/2330 train_time:56856ms step_avg:61.00ms
step:933/2330 train_time:56916ms step_avg:61.00ms
step:934/2330 train_time:56978ms step_avg:61.00ms
step:935/2330 train_time:57039ms step_avg:61.00ms
step:936/2330 train_time:57101ms step_avg:61.01ms
step:937/2330 train_time:57161ms step_avg:61.00ms
step:938/2330 train_time:57223ms step_avg:61.01ms
step:939/2330 train_time:57283ms step_avg:61.00ms
step:940/2330 train_time:57345ms step_avg:61.01ms
step:941/2330 train_time:57406ms step_avg:61.01ms
step:942/2330 train_time:57469ms step_avg:61.01ms
step:943/2330 train_time:57529ms step_avg:61.01ms
step:944/2330 train_time:57592ms step_avg:61.01ms
step:945/2330 train_time:57653ms step_avg:61.01ms
step:946/2330 train_time:57715ms step_avg:61.01ms
step:947/2330 train_time:57775ms step_avg:61.01ms
step:948/2330 train_time:57838ms step_avg:61.01ms
step:949/2330 train_time:57898ms step_avg:61.01ms
step:950/2330 train_time:57960ms step_avg:61.01ms
step:951/2330 train_time:58020ms step_avg:61.01ms
step:952/2330 train_time:58082ms step_avg:61.01ms
step:953/2330 train_time:58143ms step_avg:61.01ms
step:954/2330 train_time:58205ms step_avg:61.01ms
step:955/2330 train_time:58266ms step_avg:61.01ms
step:956/2330 train_time:58329ms step_avg:61.01ms
step:957/2330 train_time:58389ms step_avg:61.01ms
step:958/2330 train_time:58452ms step_avg:61.01ms
step:959/2330 train_time:58512ms step_avg:61.01ms
step:960/2330 train_time:58575ms step_avg:61.02ms
step:961/2330 train_time:58635ms step_avg:61.01ms
step:962/2330 train_time:58698ms step_avg:61.02ms
step:963/2330 train_time:58758ms step_avg:61.02ms
step:964/2330 train_time:58820ms step_avg:61.02ms
step:965/2330 train_time:58880ms step_avg:61.02ms
step:966/2330 train_time:58942ms step_avg:61.02ms
step:967/2330 train_time:59002ms step_avg:61.02ms
step:968/2330 train_time:59064ms step_avg:61.02ms
step:969/2330 train_time:59125ms step_avg:61.02ms
step:970/2330 train_time:59187ms step_avg:61.02ms
step:971/2330 train_time:59247ms step_avg:61.02ms
step:972/2330 train_time:59310ms step_avg:61.02ms
step:973/2330 train_time:59370ms step_avg:61.02ms
step:974/2330 train_time:59433ms step_avg:61.02ms
step:975/2330 train_time:59493ms step_avg:61.02ms
step:976/2330 train_time:59556ms step_avg:61.02ms
step:977/2330 train_time:59617ms step_avg:61.02ms
step:978/2330 train_time:59679ms step_avg:61.02ms
step:979/2330 train_time:59739ms step_avg:61.02ms
step:980/2330 train_time:59801ms step_avg:61.02ms
step:981/2330 train_time:59862ms step_avg:61.02ms
step:982/2330 train_time:59925ms step_avg:61.02ms
step:983/2330 train_time:59986ms step_avg:61.02ms
step:984/2330 train_time:60048ms step_avg:61.02ms
step:985/2330 train_time:60108ms step_avg:61.02ms
step:986/2330 train_time:60171ms step_avg:61.03ms
step:987/2330 train_time:60231ms step_avg:61.02ms
step:988/2330 train_time:60294ms step_avg:61.03ms
step:989/2330 train_time:60354ms step_avg:61.03ms
step:990/2330 train_time:60417ms step_avg:61.03ms
step:991/2330 train_time:60477ms step_avg:61.03ms
step:992/2330 train_time:60539ms step_avg:61.03ms
step:993/2330 train_time:60600ms step_avg:61.03ms
step:994/2330 train_time:60662ms step_avg:61.03ms
step:995/2330 train_time:60723ms step_avg:61.03ms
step:996/2330 train_time:60785ms step_avg:61.03ms
step:997/2330 train_time:60846ms step_avg:61.03ms
step:998/2330 train_time:60908ms step_avg:61.03ms
step:999/2330 train_time:60969ms step_avg:61.03ms
step:1000/2330 train_time:61032ms step_avg:61.03ms
step:1000/2330 val_loss:3.5931 train_time:61097ms step_avg:61.10ms
step:1001/2330 train_time:61121ms step_avg:61.06ms
step:1002/2330 train_time:61157ms step_avg:61.03ms
step:1003/2330 train_time:61224ms step_avg:61.04ms
step:1004/2330 train_time:61289ms step_avg:61.05ms
step:1005/2330 train_time:61351ms step_avg:61.05ms
step:1006/2330 train_time:61415ms step_avg:61.05ms
step:1007/2330 train_time:61475ms step_avg:61.05ms
step:1008/2330 train_time:61538ms step_avg:61.05ms
step:1009/2330 train_time:61598ms step_avg:61.05ms
step:1010/2330 train_time:61659ms step_avg:61.05ms
step:1011/2330 train_time:61719ms step_avg:61.05ms
step:1012/2330 train_time:61780ms step_avg:61.05ms
step:1013/2330 train_time:61839ms step_avg:61.05ms
step:1014/2330 train_time:61900ms step_avg:61.05ms
step:1015/2330 train_time:61960ms step_avg:61.04ms
step:1016/2330 train_time:62022ms step_avg:61.04ms
step:1017/2330 train_time:62082ms step_avg:61.04ms
step:1018/2330 train_time:62145ms step_avg:61.05ms
step:1019/2330 train_time:62206ms step_avg:61.05ms
step:1020/2330 train_time:62269ms step_avg:61.05ms
step:1021/2330 train_time:62331ms step_avg:61.05ms
step:1022/2330 train_time:62395ms step_avg:61.05ms
step:1023/2330 train_time:62456ms step_avg:61.05ms
step:1024/2330 train_time:62518ms step_avg:61.05ms
step:1025/2330 train_time:62578ms step_avg:61.05ms
step:1026/2330 train_time:62640ms step_avg:61.05ms
step:1027/2330 train_time:62700ms step_avg:61.05ms
step:1028/2330 train_time:62761ms step_avg:61.05ms
step:1029/2330 train_time:62820ms step_avg:61.05ms
step:1030/2330 train_time:62881ms step_avg:61.05ms
step:1031/2330 train_time:62940ms step_avg:61.05ms
step:1032/2330 train_time:63002ms step_avg:61.05ms
step:1033/2330 train_time:63063ms step_avg:61.05ms
step:1034/2330 train_time:63126ms step_avg:61.05ms
step:1035/2330 train_time:63186ms step_avg:61.05ms
step:1036/2330 train_time:63249ms step_avg:61.05ms
step:1037/2330 train_time:63309ms step_avg:61.05ms
step:1038/2330 train_time:63372ms step_avg:61.05ms
step:1039/2330 train_time:63433ms step_avg:61.05ms
step:1040/2330 train_time:63496ms step_avg:61.05ms
step:1041/2330 train_time:63557ms step_avg:61.05ms
step:1042/2330 train_time:63619ms step_avg:61.05ms
step:1043/2330 train_time:63679ms step_avg:61.05ms
step:1044/2330 train_time:63741ms step_avg:61.05ms
step:1045/2330 train_time:63800ms step_avg:61.05ms
step:1046/2330 train_time:63862ms step_avg:61.05ms
step:1047/2330 train_time:63922ms step_avg:61.05ms
step:1048/2330 train_time:63983ms step_avg:61.05ms
step:1049/2330 train_time:64043ms step_avg:61.05ms
step:1050/2330 train_time:64105ms step_avg:61.05ms
step:1051/2330 train_time:64165ms step_avg:61.05ms
step:1052/2330 train_time:64228ms step_avg:61.05ms
step:1053/2330 train_time:64288ms step_avg:61.05ms
step:1054/2330 train_time:64350ms step_avg:61.05ms
step:1055/2330 train_time:64411ms step_avg:61.05ms
step:1056/2330 train_time:64473ms step_avg:61.05ms
step:1057/2330 train_time:64535ms step_avg:61.05ms
step:1058/2330 train_time:64597ms step_avg:61.06ms
step:1059/2330 train_time:64658ms step_avg:61.06ms
step:1060/2330 train_time:64720ms step_avg:61.06ms
step:1061/2330 train_time:64780ms step_avg:61.06ms
step:1062/2330 train_time:64842ms step_avg:61.06ms
step:1063/2330 train_time:64902ms step_avg:61.06ms
step:1064/2330 train_time:64964ms step_avg:61.06ms
step:1065/2330 train_time:65024ms step_avg:61.06ms
step:1066/2330 train_time:65086ms step_avg:61.06ms
step:1067/2330 train_time:65146ms step_avg:61.05ms
step:1068/2330 train_time:65208ms step_avg:61.06ms
step:1069/2330 train_time:65268ms step_avg:61.06ms
step:1070/2330 train_time:65331ms step_avg:61.06ms
step:1071/2330 train_time:65391ms step_avg:61.06ms
step:1072/2330 train_time:65454ms step_avg:61.06ms
step:1073/2330 train_time:65515ms step_avg:61.06ms
step:1074/2330 train_time:65577ms step_avg:61.06ms
step:1075/2330 train_time:65639ms step_avg:61.06ms
step:1076/2330 train_time:65701ms step_avg:61.06ms
step:1077/2330 train_time:65761ms step_avg:61.06ms
step:1078/2330 train_time:65823ms step_avg:61.06ms
step:1079/2330 train_time:65883ms step_avg:61.06ms
step:1080/2330 train_time:65945ms step_avg:61.06ms
step:1081/2330 train_time:66005ms step_avg:61.06ms
step:1082/2330 train_time:66067ms step_avg:61.06ms
step:1083/2330 train_time:66127ms step_avg:61.06ms
step:1084/2330 train_time:66189ms step_avg:61.06ms
step:1085/2330 train_time:66249ms step_avg:61.06ms
step:1086/2330 train_time:66313ms step_avg:61.06ms
step:1087/2330 train_time:66373ms step_avg:61.06ms
step:1088/2330 train_time:66436ms step_avg:61.06ms
step:1089/2330 train_time:66496ms step_avg:61.06ms
step:1090/2330 train_time:66559ms step_avg:61.06ms
step:1091/2330 train_time:66619ms step_avg:61.06ms
step:1092/2330 train_time:66681ms step_avg:61.06ms
step:1093/2330 train_time:66741ms step_avg:61.06ms
step:1094/2330 train_time:66803ms step_avg:61.06ms
step:1095/2330 train_time:66862ms step_avg:61.06ms
step:1096/2330 train_time:66925ms step_avg:61.06ms
step:1097/2330 train_time:66984ms step_avg:61.06ms
step:1098/2330 train_time:67047ms step_avg:61.06ms
step:1099/2330 train_time:67107ms step_avg:61.06ms
step:1100/2330 train_time:67169ms step_avg:61.06ms
step:1101/2330 train_time:67229ms step_avg:61.06ms
step:1102/2330 train_time:67291ms step_avg:61.06ms
step:1103/2330 train_time:67352ms step_avg:61.06ms
step:1104/2330 train_time:67416ms step_avg:61.06ms
step:1105/2330 train_time:67476ms step_avg:61.06ms
step:1106/2330 train_time:67538ms step_avg:61.06ms
step:1107/2330 train_time:67598ms step_avg:61.06ms
step:1108/2330 train_time:67660ms step_avg:61.07ms
step:1109/2330 train_time:67720ms step_avg:61.06ms
step:1110/2330 train_time:67783ms step_avg:61.07ms
step:1111/2330 train_time:67843ms step_avg:61.07ms
step:1112/2330 train_time:67905ms step_avg:61.07ms
step:1113/2330 train_time:67965ms step_avg:61.06ms
step:1114/2330 train_time:68027ms step_avg:61.07ms
step:1115/2330 train_time:68087ms step_avg:61.06ms
step:1116/2330 train_time:68149ms step_avg:61.07ms
step:1117/2330 train_time:68209ms step_avg:61.06ms
step:1118/2330 train_time:68272ms step_avg:61.07ms
step:1119/2330 train_time:68333ms step_avg:61.07ms
step:1120/2330 train_time:68395ms step_avg:61.07ms
step:1121/2330 train_time:68456ms step_avg:61.07ms
step:1122/2330 train_time:68518ms step_avg:61.07ms
step:1123/2330 train_time:68578ms step_avg:61.07ms
step:1124/2330 train_time:68640ms step_avg:61.07ms
step:1125/2330 train_time:68700ms step_avg:61.07ms
step:1126/2330 train_time:68762ms step_avg:61.07ms
step:1127/2330 train_time:68823ms step_avg:61.07ms
step:1128/2330 train_time:68884ms step_avg:61.07ms
step:1129/2330 train_time:68944ms step_avg:61.07ms
step:1130/2330 train_time:69006ms step_avg:61.07ms
step:1131/2330 train_time:69066ms step_avg:61.07ms
step:1132/2330 train_time:69128ms step_avg:61.07ms
step:1133/2330 train_time:69189ms step_avg:61.07ms
step:1134/2330 train_time:69251ms step_avg:61.07ms
step:1135/2330 train_time:69311ms step_avg:61.07ms
step:1136/2330 train_time:69374ms step_avg:61.07ms
step:1137/2330 train_time:69435ms step_avg:61.07ms
step:1138/2330 train_time:69498ms step_avg:61.07ms
step:1139/2330 train_time:69558ms step_avg:61.07ms
step:1140/2330 train_time:69620ms step_avg:61.07ms
step:1141/2330 train_time:69680ms step_avg:61.07ms
step:1142/2330 train_time:69742ms step_avg:61.07ms
step:1143/2330 train_time:69802ms step_avg:61.07ms
step:1144/2330 train_time:69863ms step_avg:61.07ms
step:1145/2330 train_time:69924ms step_avg:61.07ms
step:1146/2330 train_time:69986ms step_avg:61.07ms
step:1147/2330 train_time:70046ms step_avg:61.07ms
step:1148/2330 train_time:70108ms step_avg:61.07ms
step:1149/2330 train_time:70168ms step_avg:61.07ms
step:1150/2330 train_time:70231ms step_avg:61.07ms
step:1151/2330 train_time:70291ms step_avg:61.07ms
step:1152/2330 train_time:70354ms step_avg:61.07ms
step:1153/2330 train_time:70415ms step_avg:61.07ms
step:1154/2330 train_time:70477ms step_avg:61.07ms
step:1155/2330 train_time:70538ms step_avg:61.07ms
step:1156/2330 train_time:70601ms step_avg:61.07ms
step:1157/2330 train_time:70660ms step_avg:61.07ms
step:1158/2330 train_time:70723ms step_avg:61.07ms
step:1159/2330 train_time:70784ms step_avg:61.07ms
step:1160/2330 train_time:70846ms step_avg:61.07ms
step:1161/2330 train_time:70905ms step_avg:61.07ms
step:1162/2330 train_time:70967ms step_avg:61.07ms
step:1163/2330 train_time:71027ms step_avg:61.07ms
step:1164/2330 train_time:71090ms step_avg:61.07ms
step:1165/2330 train_time:71150ms step_avg:61.07ms
step:1166/2330 train_time:71213ms step_avg:61.07ms
step:1167/2330 train_time:71273ms step_avg:61.07ms
step:1168/2330 train_time:71335ms step_avg:61.07ms
step:1169/2330 train_time:71397ms step_avg:61.07ms
step:1170/2330 train_time:71459ms step_avg:61.08ms
step:1171/2330 train_time:71519ms step_avg:61.08ms
step:1172/2330 train_time:71581ms step_avg:61.08ms
step:1173/2330 train_time:71640ms step_avg:61.07ms
step:1174/2330 train_time:71703ms step_avg:61.08ms
step:1175/2330 train_time:71763ms step_avg:61.08ms
step:1176/2330 train_time:71826ms step_avg:61.08ms
step:1177/2330 train_time:71886ms step_avg:61.08ms
step:1178/2330 train_time:71948ms step_avg:61.08ms
step:1179/2330 train_time:72008ms step_avg:61.08ms
step:1180/2330 train_time:72070ms step_avg:61.08ms
step:1181/2330 train_time:72130ms step_avg:61.08ms
step:1182/2330 train_time:72192ms step_avg:61.08ms
step:1183/2330 train_time:72253ms step_avg:61.08ms
step:1184/2330 train_time:72316ms step_avg:61.08ms
step:1185/2330 train_time:72376ms step_avg:61.08ms
step:1186/2330 train_time:72439ms step_avg:61.08ms
step:1187/2330 train_time:72500ms step_avg:61.08ms
step:1188/2330 train_time:72562ms step_avg:61.08ms
step:1189/2330 train_time:72622ms step_avg:61.08ms
step:1190/2330 train_time:72685ms step_avg:61.08ms
step:1191/2330 train_time:72745ms step_avg:61.08ms
step:1192/2330 train_time:72808ms step_avg:61.08ms
step:1193/2330 train_time:72869ms step_avg:61.08ms
step:1194/2330 train_time:72931ms step_avg:61.08ms
step:1195/2330 train_time:72991ms step_avg:61.08ms
step:1196/2330 train_time:73054ms step_avg:61.08ms
step:1197/2330 train_time:73113ms step_avg:61.08ms
step:1198/2330 train_time:73176ms step_avg:61.08ms
step:1199/2330 train_time:73236ms step_avg:61.08ms
step:1200/2330 train_time:73299ms step_avg:61.08ms
step:1201/2330 train_time:73358ms step_avg:61.08ms
step:1202/2330 train_time:73420ms step_avg:61.08ms
step:1203/2330 train_time:73480ms step_avg:61.08ms
step:1204/2330 train_time:73543ms step_avg:61.08ms
step:1205/2330 train_time:73603ms step_avg:61.08ms
step:1206/2330 train_time:73665ms step_avg:61.08ms
step:1207/2330 train_time:73726ms step_avg:61.08ms
step:1208/2330 train_time:73788ms step_avg:61.08ms
step:1209/2330 train_time:73848ms step_avg:61.08ms
step:1210/2330 train_time:73911ms step_avg:61.08ms
step:1211/2330 train_time:73972ms step_avg:61.08ms
step:1212/2330 train_time:74034ms step_avg:61.08ms
step:1213/2330 train_time:74094ms step_avg:61.08ms
step:1214/2330 train_time:74156ms step_avg:61.08ms
step:1215/2330 train_time:74217ms step_avg:61.08ms
step:1216/2330 train_time:74279ms step_avg:61.08ms
step:1217/2330 train_time:74340ms step_avg:61.08ms
step:1218/2330 train_time:74402ms step_avg:61.09ms
step:1219/2330 train_time:74462ms step_avg:61.08ms
step:1220/2330 train_time:74524ms step_avg:61.09ms
step:1221/2330 train_time:74584ms step_avg:61.08ms
step:1222/2330 train_time:74647ms step_avg:61.09ms
step:1223/2330 train_time:74707ms step_avg:61.08ms
step:1224/2330 train_time:74769ms step_avg:61.09ms
step:1225/2330 train_time:74829ms step_avg:61.08ms
step:1226/2330 train_time:74891ms step_avg:61.09ms
step:1227/2330 train_time:74951ms step_avg:61.09ms
step:1228/2330 train_time:75014ms step_avg:61.09ms
step:1229/2330 train_time:75074ms step_avg:61.09ms
step:1230/2330 train_time:75137ms step_avg:61.09ms
step:1231/2330 train_time:75197ms step_avg:61.09ms
step:1232/2330 train_time:75259ms step_avg:61.09ms
step:1233/2330 train_time:75320ms step_avg:61.09ms
step:1234/2330 train_time:75381ms step_avg:61.09ms
step:1235/2330 train_time:75441ms step_avg:61.09ms
step:1236/2330 train_time:75504ms step_avg:61.09ms
step:1237/2330 train_time:75564ms step_avg:61.09ms
step:1238/2330 train_time:75626ms step_avg:61.09ms
step:1239/2330 train_time:75686ms step_avg:61.09ms
step:1240/2330 train_time:75748ms step_avg:61.09ms
step:1241/2330 train_time:75809ms step_avg:61.09ms
step:1242/2330 train_time:75872ms step_avg:61.09ms
step:1243/2330 train_time:75932ms step_avg:61.09ms
step:1244/2330 train_time:75995ms step_avg:61.09ms
step:1245/2330 train_time:76055ms step_avg:61.09ms
step:1246/2330 train_time:76117ms step_avg:61.09ms
step:1247/2330 train_time:76177ms step_avg:61.09ms
step:1248/2330 train_time:76239ms step_avg:61.09ms
step:1249/2330 train_time:76299ms step_avg:61.09ms
step:1250/2330 train_time:76361ms step_avg:61.09ms
step:1250/2330 val_loss:3.5331 train_time:76426ms step_avg:61.14ms
step:1251/2330 train_time:76449ms step_avg:61.11ms
step:1252/2330 train_time:76489ms step_avg:61.09ms
step:1253/2330 train_time:76555ms step_avg:61.10ms
step:1254/2330 train_time:76619ms step_avg:61.10ms
step:1255/2330 train_time:76679ms step_avg:61.10ms
step:1256/2330 train_time:76743ms step_avg:61.10ms
step:1257/2330 train_time:76802ms step_avg:61.10ms
step:1258/2330 train_time:76865ms step_avg:61.10ms
step:1259/2330 train_time:76924ms step_avg:61.10ms
step:1260/2330 train_time:76985ms step_avg:61.10ms
step:1261/2330 train_time:77045ms step_avg:61.10ms
step:1262/2330 train_time:77106ms step_avg:61.10ms
step:1263/2330 train_time:77165ms step_avg:61.10ms
step:1264/2330 train_time:77227ms step_avg:61.10ms
step:1265/2330 train_time:77286ms step_avg:61.10ms
step:1266/2330 train_time:77349ms step_avg:61.10ms
step:1267/2330 train_time:77411ms step_avg:61.10ms
step:1268/2330 train_time:77474ms step_avg:61.10ms
step:1269/2330 train_time:77537ms step_avg:61.10ms
step:1270/2330 train_time:77600ms step_avg:61.10ms
step:1271/2330 train_time:77660ms step_avg:61.10ms
step:1272/2330 train_time:77723ms step_avg:61.10ms
step:1273/2330 train_time:77784ms step_avg:61.10ms
step:1274/2330 train_time:77846ms step_avg:61.10ms
step:1275/2330 train_time:77905ms step_avg:61.10ms
step:1276/2330 train_time:77967ms step_avg:61.10ms
step:1277/2330 train_time:78026ms step_avg:61.10ms
step:1278/2330 train_time:78088ms step_avg:61.10ms
step:1279/2330 train_time:78147ms step_avg:61.10ms
step:1280/2330 train_time:78209ms step_avg:61.10ms
step:1281/2330 train_time:78268ms step_avg:61.10ms
step:1282/2330 train_time:78330ms step_avg:61.10ms
step:1283/2330 train_time:78390ms step_avg:61.10ms
step:1284/2330 train_time:78453ms step_avg:61.10ms
step:1285/2330 train_time:78513ms step_avg:61.10ms
step:1286/2330 train_time:78577ms step_avg:61.10ms
step:1287/2330 train_time:78639ms step_avg:61.10ms
step:1288/2330 train_time:78701ms step_avg:61.10ms
step:1289/2330 train_time:78763ms step_avg:61.10ms
step:1290/2330 train_time:78826ms step_avg:61.11ms
step:1291/2330 train_time:78885ms step_avg:61.10ms
step:1292/2330 train_time:78947ms step_avg:61.10ms
step:1293/2330 train_time:79007ms step_avg:61.10ms
step:1294/2330 train_time:79068ms step_avg:61.10ms
step:1295/2330 train_time:79128ms step_avg:61.10ms
step:1296/2330 train_time:79189ms step_avg:61.10ms
step:1297/2330 train_time:79249ms step_avg:61.10ms
step:1298/2330 train_time:79311ms step_avg:61.10ms
step:1299/2330 train_time:79371ms step_avg:61.10ms
step:1300/2330 train_time:79434ms step_avg:61.10ms
step:1301/2330 train_time:79495ms step_avg:61.10ms
step:1302/2330 train_time:79558ms step_avg:61.10ms
step:1303/2330 train_time:79619ms step_avg:61.10ms
step:1304/2330 train_time:79682ms step_avg:61.11ms
step:1305/2330 train_time:79742ms step_avg:61.10ms
step:1306/2330 train_time:79804ms step_avg:61.11ms
step:1307/2330 train_time:79864ms step_avg:61.10ms
step:1308/2330 train_time:79927ms step_avg:61.11ms
step:1309/2330 train_time:79987ms step_avg:61.11ms
step:1310/2330 train_time:80048ms step_avg:61.11ms
step:1311/2330 train_time:80107ms step_avg:61.10ms
step:1312/2330 train_time:80169ms step_avg:61.10ms
step:1313/2330 train_time:80229ms step_avg:61.10ms
step:1314/2330 train_time:80291ms step_avg:61.10ms
step:1315/2330 train_time:80351ms step_avg:61.10ms
step:1316/2330 train_time:80414ms step_avg:61.10ms
step:1317/2330 train_time:80475ms step_avg:61.10ms
step:1318/2330 train_time:80538ms step_avg:61.11ms
step:1319/2330 train_time:80598ms step_avg:61.11ms
step:1320/2330 train_time:80661ms step_avg:61.11ms
step:1321/2330 train_time:80722ms step_avg:61.11ms
step:1322/2330 train_time:80785ms step_avg:61.11ms
step:1323/2330 train_time:80845ms step_avg:61.11ms
step:1324/2330 train_time:80907ms step_avg:61.11ms
step:1325/2330 train_time:80968ms step_avg:61.11ms
step:1326/2330 train_time:81030ms step_avg:61.11ms
step:1327/2330 train_time:81089ms step_avg:61.11ms
step:1328/2330 train_time:81151ms step_avg:61.11ms
step:1329/2330 train_time:81210ms step_avg:61.11ms
step:1330/2330 train_time:81272ms step_avg:61.11ms
step:1331/2330 train_time:81332ms step_avg:61.11ms
step:1332/2330 train_time:81395ms step_avg:61.11ms
step:1333/2330 train_time:81456ms step_avg:61.11ms
step:1334/2330 train_time:81518ms step_avg:61.11ms
step:1335/2330 train_time:81579ms step_avg:61.11ms
step:1336/2330 train_time:81642ms step_avg:61.11ms
step:1337/2330 train_time:81703ms step_avg:61.11ms
step:1338/2330 train_time:81765ms step_avg:61.11ms
step:1339/2330 train_time:81825ms step_avg:61.11ms
step:1340/2330 train_time:81887ms step_avg:61.11ms
step:1341/2330 train_time:81947ms step_avg:61.11ms
step:1342/2330 train_time:82009ms step_avg:61.11ms
step:1343/2330 train_time:82069ms step_avg:61.11ms
step:1344/2330 train_time:82131ms step_avg:61.11ms
step:1345/2330 train_time:82190ms step_avg:61.11ms
step:1346/2330 train_time:82252ms step_avg:61.11ms
step:1347/2330 train_time:82312ms step_avg:61.11ms
step:1348/2330 train_time:82375ms step_avg:61.11ms
step:1349/2330 train_time:82435ms step_avg:61.11ms
step:1350/2330 train_time:82498ms step_avg:61.11ms
step:1351/2330 train_time:82559ms step_avg:61.11ms
step:1352/2330 train_time:82621ms step_avg:61.11ms
step:1353/2330 train_time:82681ms step_avg:61.11ms
step:1354/2330 train_time:82744ms step_avg:61.11ms
step:1355/2330 train_time:82804ms step_avg:61.11ms
step:1356/2330 train_time:82866ms step_avg:61.11ms
step:1357/2330 train_time:82927ms step_avg:61.11ms
step:1358/2330 train_time:82988ms step_avg:61.11ms
step:1359/2330 train_time:83049ms step_avg:61.11ms
step:1360/2330 train_time:83110ms step_avg:61.11ms
step:1361/2330 train_time:83170ms step_avg:61.11ms
step:1362/2330 train_time:83232ms step_avg:61.11ms
step:1363/2330 train_time:83293ms step_avg:61.11ms
step:1364/2330 train_time:83355ms step_avg:61.11ms
step:1365/2330 train_time:83415ms step_avg:61.11ms
step:1366/2330 train_time:83479ms step_avg:61.11ms
step:1367/2330 train_time:83539ms step_avg:61.11ms
step:1368/2330 train_time:83602ms step_avg:61.11ms
step:1369/2330 train_time:83663ms step_avg:61.11ms
step:1370/2330 train_time:83725ms step_avg:61.11ms
step:1371/2330 train_time:83785ms step_avg:61.11ms
step:1372/2330 train_time:83848ms step_avg:61.11ms
step:1373/2330 train_time:83908ms step_avg:61.11ms
step:1374/2330 train_time:83970ms step_avg:61.11ms
step:1375/2330 train_time:84030ms step_avg:61.11ms
step:1376/2330 train_time:84092ms step_avg:61.11ms
step:1377/2330 train_time:84151ms step_avg:61.11ms
step:1378/2330 train_time:84213ms step_avg:61.11ms
step:1379/2330 train_time:84273ms step_avg:61.11ms
step:1380/2330 train_time:84335ms step_avg:61.11ms
step:1381/2330 train_time:84396ms step_avg:61.11ms
step:1382/2330 train_time:84458ms step_avg:61.11ms
step:1383/2330 train_time:84519ms step_avg:61.11ms
step:1384/2330 train_time:84582ms step_avg:61.11ms
step:1385/2330 train_time:84643ms step_avg:61.11ms
step:1386/2330 train_time:84705ms step_avg:61.11ms
step:1387/2330 train_time:84765ms step_avg:61.11ms
step:1388/2330 train_time:84827ms step_avg:61.11ms
step:1389/2330 train_time:84887ms step_avg:61.11ms
step:1390/2330 train_time:84949ms step_avg:61.11ms
step:1391/2330 train_time:85009ms step_avg:61.11ms
step:1392/2330 train_time:85071ms step_avg:61.11ms
step:1393/2330 train_time:85131ms step_avg:61.11ms
step:1394/2330 train_time:85194ms step_avg:61.11ms
step:1395/2330 train_time:85254ms step_avg:61.11ms
step:1396/2330 train_time:85316ms step_avg:61.11ms
step:1397/2330 train_time:85376ms step_avg:61.11ms
step:1398/2330 train_time:85439ms step_avg:61.12ms
step:1399/2330 train_time:85499ms step_avg:61.11ms
step:1400/2330 train_time:85561ms step_avg:61.12ms
step:1401/2330 train_time:85622ms step_avg:61.11ms
step:1402/2330 train_time:85684ms step_avg:61.12ms
step:1403/2330 train_time:85745ms step_avg:61.12ms
step:1404/2330 train_time:85807ms step_avg:61.12ms
step:1405/2330 train_time:85867ms step_avg:61.12ms
step:1406/2330 train_time:85929ms step_avg:61.12ms
step:1407/2330 train_time:85989ms step_avg:61.12ms
step:1408/2330 train_time:86051ms step_avg:61.12ms
step:1409/2330 train_time:86111ms step_avg:61.11ms
step:1410/2330 train_time:86173ms step_avg:61.12ms
step:1411/2330 train_time:86233ms step_avg:61.12ms
step:1412/2330 train_time:86296ms step_avg:61.12ms
step:1413/2330 train_time:86355ms step_avg:61.11ms
step:1414/2330 train_time:86418ms step_avg:61.12ms
step:1415/2330 train_time:86478ms step_avg:61.12ms
step:1416/2330 train_time:86541ms step_avg:61.12ms
step:1417/2330 train_time:86602ms step_avg:61.12ms
step:1418/2330 train_time:86664ms step_avg:61.12ms
step:1419/2330 train_time:86725ms step_avg:61.12ms
step:1420/2330 train_time:86787ms step_avg:61.12ms
step:1421/2330 train_time:86847ms step_avg:61.12ms
step:1422/2330 train_time:86909ms step_avg:61.12ms
step:1423/2330 train_time:86969ms step_avg:61.12ms
step:1424/2330 train_time:87031ms step_avg:61.12ms
step:1425/2330 train_time:87091ms step_avg:61.12ms
step:1426/2330 train_time:87154ms step_avg:61.12ms
step:1427/2330 train_time:87214ms step_avg:61.12ms
step:1428/2330 train_time:87276ms step_avg:61.12ms
step:1429/2330 train_time:87337ms step_avg:61.12ms
step:1430/2330 train_time:87400ms step_avg:61.12ms
step:1431/2330 train_time:87459ms step_avg:61.12ms
step:1432/2330 train_time:87522ms step_avg:61.12ms
step:1433/2330 train_time:87583ms step_avg:61.12ms
step:1434/2330 train_time:87646ms step_avg:61.12ms
step:1435/2330 train_time:87706ms step_avg:61.12ms
step:1436/2330 train_time:87768ms step_avg:61.12ms
step:1437/2330 train_time:87829ms step_avg:61.12ms
step:1438/2330 train_time:87890ms step_avg:61.12ms
step:1439/2330 train_time:87951ms step_avg:61.12ms
step:1440/2330 train_time:88013ms step_avg:61.12ms
step:1441/2330 train_time:88073ms step_avg:61.12ms
step:1442/2330 train_time:88136ms step_avg:61.12ms
step:1443/2330 train_time:88196ms step_avg:61.12ms
step:1444/2330 train_time:88258ms step_avg:61.12ms
step:1445/2330 train_time:88318ms step_avg:61.12ms
step:1446/2330 train_time:88380ms step_avg:61.12ms
step:1447/2330 train_time:88441ms step_avg:61.12ms
step:1448/2330 train_time:88503ms step_avg:61.12ms
step:1449/2330 train_time:88563ms step_avg:61.12ms
step:1450/2330 train_time:88626ms step_avg:61.12ms
step:1451/2330 train_time:88686ms step_avg:61.12ms
step:1452/2330 train_time:88748ms step_avg:61.12ms
step:1453/2330 train_time:88808ms step_avg:61.12ms
step:1454/2330 train_time:88870ms step_avg:61.12ms
step:1455/2330 train_time:88930ms step_avg:61.12ms
step:1456/2330 train_time:88991ms step_avg:61.12ms
step:1457/2330 train_time:89051ms step_avg:61.12ms
step:1458/2330 train_time:89113ms step_avg:61.12ms
step:1459/2330 train_time:89173ms step_avg:61.12ms
step:1460/2330 train_time:89236ms step_avg:61.12ms
step:1461/2330 train_time:89297ms step_avg:61.12ms
step:1462/2330 train_time:89360ms step_avg:61.12ms
step:1463/2330 train_time:89421ms step_avg:61.12ms
step:1464/2330 train_time:89483ms step_avg:61.12ms
step:1465/2330 train_time:89543ms step_avg:61.12ms
step:1466/2330 train_time:89605ms step_avg:61.12ms
step:1467/2330 train_time:89665ms step_avg:61.12ms
step:1468/2330 train_time:89728ms step_avg:61.12ms
step:1469/2330 train_time:89789ms step_avg:61.12ms
step:1470/2330 train_time:89851ms step_avg:61.12ms
step:1471/2330 train_time:89910ms step_avg:61.12ms
step:1472/2330 train_time:89972ms step_avg:61.12ms
step:1473/2330 train_time:90032ms step_avg:61.12ms
step:1474/2330 train_time:90094ms step_avg:61.12ms
step:1475/2330 train_time:90154ms step_avg:61.12ms
step:1476/2330 train_time:90216ms step_avg:61.12ms
step:1477/2330 train_time:90276ms step_avg:61.12ms
step:1478/2330 train_time:90339ms step_avg:61.12ms
step:1479/2330 train_time:90400ms step_avg:61.12ms
step:1480/2330 train_time:90462ms step_avg:61.12ms
step:1481/2330 train_time:90523ms step_avg:61.12ms
step:1482/2330 train_time:90585ms step_avg:61.12ms
step:1483/2330 train_time:90645ms step_avg:61.12ms
step:1484/2330 train_time:90708ms step_avg:61.12ms
step:1485/2330 train_time:90769ms step_avg:61.12ms
step:1486/2330 train_time:90831ms step_avg:61.12ms
step:1487/2330 train_time:90891ms step_avg:61.12ms
step:1488/2330 train_time:90953ms step_avg:61.12ms
step:1489/2330 train_time:91013ms step_avg:61.12ms
step:1490/2330 train_time:91075ms step_avg:61.12ms
step:1491/2330 train_time:91135ms step_avg:61.12ms
step:1492/2330 train_time:91197ms step_avg:61.12ms
step:1493/2330 train_time:91258ms step_avg:61.12ms
step:1494/2330 train_time:91321ms step_avg:61.12ms
step:1495/2330 train_time:91380ms step_avg:61.12ms
step:1496/2330 train_time:91443ms step_avg:61.12ms
step:1497/2330 train_time:91503ms step_avg:61.12ms
step:1498/2330 train_time:91565ms step_avg:61.12ms
step:1499/2330 train_time:91625ms step_avg:61.12ms
step:1500/2330 train_time:91686ms step_avg:61.12ms
step:1500/2330 val_loss:3.4708 train_time:91751ms step_avg:61.17ms
step:1501/2330 train_time:91775ms step_avg:61.14ms
step:1502/2330 train_time:91813ms step_avg:61.13ms
step:1503/2330 train_time:91877ms step_avg:61.13ms
step:1504/2330 train_time:91941ms step_avg:61.13ms
step:1505/2330 train_time:92001ms step_avg:61.13ms
step:1506/2330 train_time:92063ms step_avg:61.13ms
step:1507/2330 train_time:92123ms step_avg:61.13ms
step:1508/2330 train_time:92185ms step_avg:61.13ms
step:1509/2330 train_time:92245ms step_avg:61.13ms
step:1510/2330 train_time:92306ms step_avg:61.13ms
step:1511/2330 train_time:92366ms step_avg:61.13ms
step:1512/2330 train_time:92428ms step_avg:61.13ms
step:1513/2330 train_time:92488ms step_avg:61.13ms
step:1514/2330 train_time:92551ms step_avg:61.13ms
step:1515/2330 train_time:92610ms step_avg:61.13ms
step:1516/2330 train_time:92673ms step_avg:61.13ms
step:1517/2330 train_time:92734ms step_avg:61.13ms
step:1518/2330 train_time:92798ms step_avg:61.13ms
step:1519/2330 train_time:92860ms step_avg:61.13ms
step:1520/2330 train_time:92923ms step_avg:61.13ms
step:1521/2330 train_time:92984ms step_avg:61.13ms
step:1522/2330 train_time:93047ms step_avg:61.13ms
step:1523/2330 train_time:93107ms step_avg:61.13ms
step:1524/2330 train_time:93169ms step_avg:61.13ms
step:1525/2330 train_time:93229ms step_avg:61.13ms
step:1526/2330 train_time:93291ms step_avg:61.13ms
step:1527/2330 train_time:93351ms step_avg:61.13ms
step:1528/2330 train_time:93412ms step_avg:61.13ms
step:1529/2330 train_time:93472ms step_avg:61.13ms
step:1530/2330 train_time:93534ms step_avg:61.13ms
step:1531/2330 train_time:93595ms step_avg:61.13ms
step:1532/2330 train_time:93658ms step_avg:61.13ms
step:1533/2330 train_time:93719ms step_avg:61.13ms
step:1534/2330 train_time:93783ms step_avg:61.14ms
step:1535/2330 train_time:93845ms step_avg:61.14ms
step:1536/2330 train_time:93909ms step_avg:61.14ms
step:1537/2330 train_time:93971ms step_avg:61.14ms
step:1538/2330 train_time:94033ms step_avg:61.14ms
step:1539/2330 train_time:94094ms step_avg:61.14ms
step:1540/2330 train_time:94156ms step_avg:61.14ms
step:1541/2330 train_time:94218ms step_avg:61.14ms
step:1542/2330 train_time:94280ms step_avg:61.14ms
step:1543/2330 train_time:94340ms step_avg:61.14ms
step:1544/2330 train_time:94403ms step_avg:61.14ms
step:1545/2330 train_time:94464ms step_avg:61.14ms
step:1546/2330 train_time:94526ms step_avg:61.14ms
step:1547/2330 train_time:94587ms step_avg:61.14ms
step:1548/2330 train_time:94651ms step_avg:61.14ms
step:1549/2330 train_time:94712ms step_avg:61.14ms
step:1550/2330 train_time:94775ms step_avg:61.15ms
step:1551/2330 train_time:94836ms step_avg:61.14ms
step:1552/2330 train_time:94898ms step_avg:61.15ms
step:1553/2330 train_time:94960ms step_avg:61.15ms
step:1554/2330 train_time:95023ms step_avg:61.15ms
step:1555/2330 train_time:95084ms step_avg:61.15ms
step:1556/2330 train_time:95148ms step_avg:61.15ms
step:1557/2330 train_time:95209ms step_avg:61.15ms
step:1558/2330 train_time:95271ms step_avg:61.15ms
step:1559/2330 train_time:95331ms step_avg:61.15ms
step:1560/2330 train_time:95394ms step_avg:61.15ms
step:1561/2330 train_time:95454ms step_avg:61.15ms
step:1562/2330 train_time:95517ms step_avg:61.15ms
step:1563/2330 train_time:95578ms step_avg:61.15ms
step:1564/2330 train_time:95641ms step_avg:61.15ms
step:1565/2330 train_time:95702ms step_avg:61.15ms
step:1566/2330 train_time:95766ms step_avg:61.15ms
step:1567/2330 train_time:95827ms step_avg:61.15ms
step:1568/2330 train_time:95891ms step_avg:61.16ms
step:1569/2330 train_time:95952ms step_avg:61.16ms
step:1570/2330 train_time:96015ms step_avg:61.16ms
step:1571/2330 train_time:96076ms step_avg:61.16ms
step:1572/2330 train_time:96139ms step_avg:61.16ms
step:1573/2330 train_time:96199ms step_avg:61.16ms
step:1574/2330 train_time:96262ms step_avg:61.16ms
step:1575/2330 train_time:96323ms step_avg:61.16ms
step:1576/2330 train_time:96387ms step_avg:61.16ms
step:1577/2330 train_time:96447ms step_avg:61.16ms
step:1578/2330 train_time:96510ms step_avg:61.16ms
step:1579/2330 train_time:96571ms step_avg:61.16ms
step:1580/2330 train_time:96634ms step_avg:61.16ms
step:1581/2330 train_time:96694ms step_avg:61.16ms
step:1582/2330 train_time:96757ms step_avg:61.16ms
step:1583/2330 train_time:96818ms step_avg:61.16ms
step:1584/2330 train_time:96881ms step_avg:61.16ms
step:1585/2330 train_time:96942ms step_avg:61.16ms
step:1586/2330 train_time:97006ms step_avg:61.16ms
step:1587/2330 train_time:97068ms step_avg:61.16ms
step:1588/2330 train_time:97131ms step_avg:61.17ms
step:1589/2330 train_time:97192ms step_avg:61.17ms
step:1590/2330 train_time:97255ms step_avg:61.17ms
step:1591/2330 train_time:97315ms step_avg:61.17ms
step:1592/2330 train_time:97378ms step_avg:61.17ms
step:1593/2330 train_time:97439ms step_avg:61.17ms
step:1594/2330 train_time:97502ms step_avg:61.17ms
step:1595/2330 train_time:97564ms step_avg:61.17ms
step:1596/2330 train_time:97627ms step_avg:61.17ms
step:1597/2330 train_time:97688ms step_avg:61.17ms
step:1598/2330 train_time:97751ms step_avg:61.17ms
step:1599/2330 train_time:97812ms step_avg:61.17ms
step:1600/2330 train_time:97875ms step_avg:61.17ms
step:1601/2330 train_time:97935ms step_avg:61.17ms
step:1602/2330 train_time:97997ms step_avg:61.17ms
step:1603/2330 train_time:98058ms step_avg:61.17ms
step:1604/2330 train_time:98121ms step_avg:61.17ms
step:1605/2330 train_time:98182ms step_avg:61.17ms
step:1606/2330 train_time:98245ms step_avg:61.17ms
step:1607/2330 train_time:98306ms step_avg:61.17ms
step:1608/2330 train_time:98370ms step_avg:61.18ms
step:1609/2330 train_time:98430ms step_avg:61.17ms
step:1610/2330 train_time:98493ms step_avg:61.18ms
step:1611/2330 train_time:98553ms step_avg:61.18ms
step:1612/2330 train_time:98616ms step_avg:61.18ms
step:1613/2330 train_time:98677ms step_avg:61.18ms
step:1614/2330 train_time:98740ms step_avg:61.18ms
step:1615/2330 train_time:98800ms step_avg:61.18ms
step:1616/2330 train_time:98865ms step_avg:61.18ms
step:1617/2330 train_time:98926ms step_avg:61.18ms
step:1618/2330 train_time:98989ms step_avg:61.18ms
step:1619/2330 train_time:99051ms step_avg:61.18ms
step:1620/2330 train_time:99113ms step_avg:61.18ms
step:1621/2330 train_time:99174ms step_avg:61.18ms
step:1622/2330 train_time:99236ms step_avg:61.18ms
step:1623/2330 train_time:99297ms step_avg:61.18ms
step:1624/2330 train_time:99359ms step_avg:61.18ms
step:1625/2330 train_time:99421ms step_avg:61.18ms
step:1626/2330 train_time:99484ms step_avg:61.18ms
step:1627/2330 train_time:99544ms step_avg:61.18ms
step:1628/2330 train_time:99607ms step_avg:61.18ms
step:1629/2330 train_time:99668ms step_avg:61.18ms
step:1630/2330 train_time:99731ms step_avg:61.18ms
step:1631/2330 train_time:99792ms step_avg:61.18ms
step:1632/2330 train_time:99854ms step_avg:61.19ms
step:1633/2330 train_time:99914ms step_avg:61.18ms
step:1634/2330 train_time:99977ms step_avg:61.19ms
step:1635/2330 train_time:100038ms step_avg:61.19ms
step:1636/2330 train_time:100101ms step_avg:61.19ms
step:1637/2330 train_time:100161ms step_avg:61.19ms
step:1638/2330 train_time:100224ms step_avg:61.19ms
step:1639/2330 train_time:100285ms step_avg:61.19ms
step:1640/2330 train_time:100349ms step_avg:61.19ms
step:1641/2330 train_time:100409ms step_avg:61.19ms
step:1642/2330 train_time:100472ms step_avg:61.19ms
step:1643/2330 train_time:100532ms step_avg:61.19ms
step:1644/2330 train_time:100595ms step_avg:61.19ms
step:1645/2330 train_time:100655ms step_avg:61.19ms
step:1646/2330 train_time:100718ms step_avg:61.19ms
step:1647/2330 train_time:100779ms step_avg:61.19ms
step:1648/2330 train_time:100843ms step_avg:61.19ms
step:1649/2330 train_time:100904ms step_avg:61.19ms
step:1650/2330 train_time:100967ms step_avg:61.19ms
step:1651/2330 train_time:101028ms step_avg:61.19ms
step:1652/2330 train_time:101090ms step_avg:61.19ms
step:1653/2330 train_time:101152ms step_avg:61.19ms
step:1654/2330 train_time:101214ms step_avg:61.19ms
step:1655/2330 train_time:101275ms step_avg:61.19ms
step:1656/2330 train_time:101337ms step_avg:61.19ms
step:1657/2330 train_time:101398ms step_avg:61.19ms
step:1658/2330 train_time:101461ms step_avg:61.19ms
step:1659/2330 train_time:101521ms step_avg:61.19ms
step:1660/2330 train_time:101583ms step_avg:61.19ms
step:1661/2330 train_time:101644ms step_avg:61.19ms
step:1662/2330 train_time:101707ms step_avg:61.20ms
step:1663/2330 train_time:101768ms step_avg:61.20ms
step:1664/2330 train_time:101831ms step_avg:61.20ms
step:1665/2330 train_time:101891ms step_avg:61.20ms
step:1666/2330 train_time:101954ms step_avg:61.20ms
step:1667/2330 train_time:102014ms step_avg:61.20ms
step:1668/2330 train_time:102077ms step_avg:61.20ms
step:1669/2330 train_time:102138ms step_avg:61.20ms
step:1670/2330 train_time:102201ms step_avg:61.20ms
step:1671/2330 train_time:102262ms step_avg:61.20ms
step:1672/2330 train_time:102324ms step_avg:61.20ms
step:1673/2330 train_time:102385ms step_avg:61.20ms
step:1674/2330 train_time:102449ms step_avg:61.20ms
step:1675/2330 train_time:102509ms step_avg:61.20ms
step:1676/2330 train_time:102572ms step_avg:61.20ms
step:1677/2330 train_time:102632ms step_avg:61.20ms
step:1678/2330 train_time:102694ms step_avg:61.20ms
step:1679/2330 train_time:102755ms step_avg:61.20ms
step:1680/2330 train_time:102817ms step_avg:61.20ms
step:1681/2330 train_time:102878ms step_avg:61.20ms
step:1682/2330 train_time:102941ms step_avg:61.20ms
step:1683/2330 train_time:103002ms step_avg:61.20ms
step:1684/2330 train_time:103066ms step_avg:61.20ms
step:1685/2330 train_time:103127ms step_avg:61.20ms
step:1686/2330 train_time:103190ms step_avg:61.20ms
step:1687/2330 train_time:103251ms step_avg:61.20ms
step:1688/2330 train_time:103313ms step_avg:61.20ms
step:1689/2330 train_time:103374ms step_avg:61.20ms
step:1690/2330 train_time:103436ms step_avg:61.20ms
step:1691/2330 train_time:103497ms step_avg:61.20ms
step:1692/2330 train_time:103559ms step_avg:61.21ms
step:1693/2330 train_time:103620ms step_avg:61.20ms
step:1694/2330 train_time:103682ms step_avg:61.21ms
step:1695/2330 train_time:103744ms step_avg:61.21ms
step:1696/2330 train_time:103807ms step_avg:61.21ms
step:1697/2330 train_time:103868ms step_avg:61.21ms
step:1698/2330 train_time:103932ms step_avg:61.21ms
step:1699/2330 train_time:103992ms step_avg:61.21ms
step:1700/2330 train_time:104055ms step_avg:61.21ms
step:1701/2330 train_time:104115ms step_avg:61.21ms
step:1702/2330 train_time:104179ms step_avg:61.21ms
step:1703/2330 train_time:104240ms step_avg:61.21ms
step:1704/2330 train_time:104303ms step_avg:61.21ms
step:1705/2330 train_time:104364ms step_avg:61.21ms
step:1706/2330 train_time:104427ms step_avg:61.21ms
step:1707/2330 train_time:104488ms step_avg:61.21ms
step:1708/2330 train_time:104552ms step_avg:61.21ms
step:1709/2330 train_time:104612ms step_avg:61.21ms
step:1710/2330 train_time:104675ms step_avg:61.21ms
step:1711/2330 train_time:104735ms step_avg:61.21ms
step:1712/2330 train_time:104797ms step_avg:61.21ms
step:1713/2330 train_time:104858ms step_avg:61.21ms
step:1714/2330 train_time:104922ms step_avg:61.21ms
step:1715/2330 train_time:104982ms step_avg:61.21ms
step:1716/2330 train_time:105045ms step_avg:61.22ms
step:1717/2330 train_time:105107ms step_avg:61.22ms
step:1718/2330 train_time:105170ms step_avg:61.22ms
step:1719/2330 train_time:105231ms step_avg:61.22ms
step:1720/2330 train_time:105294ms step_avg:61.22ms
step:1721/2330 train_time:105354ms step_avg:61.22ms
step:1722/2330 train_time:105417ms step_avg:61.22ms
step:1723/2330 train_time:105478ms step_avg:61.22ms
step:1724/2330 train_time:105540ms step_avg:61.22ms
step:1725/2330 train_time:105601ms step_avg:61.22ms
step:1726/2330 train_time:105665ms step_avg:61.22ms
step:1727/2330 train_time:105726ms step_avg:61.22ms
step:1728/2330 train_time:105789ms step_avg:61.22ms
step:1729/2330 train_time:105852ms step_avg:61.22ms
step:1730/2330 train_time:105915ms step_avg:61.22ms
step:1731/2330 train_time:105975ms step_avg:61.22ms
step:1732/2330 train_time:106038ms step_avg:61.22ms
step:1733/2330 train_time:106099ms step_avg:61.22ms
step:1734/2330 train_time:106162ms step_avg:61.22ms
step:1735/2330 train_time:106223ms step_avg:61.22ms
step:1736/2330 train_time:106286ms step_avg:61.22ms
step:1737/2330 train_time:106348ms step_avg:61.23ms
step:1738/2330 train_time:106411ms step_avg:61.23ms
step:1739/2330 train_time:106472ms step_avg:61.23ms
step:1740/2330 train_time:106534ms step_avg:61.23ms
step:1741/2330 train_time:106595ms step_avg:61.23ms
step:1742/2330 train_time:106657ms step_avg:61.23ms
step:1743/2330 train_time:106718ms step_avg:61.23ms
step:1744/2330 train_time:106781ms step_avg:61.23ms
step:1745/2330 train_time:106842ms step_avg:61.23ms
step:1746/2330 train_time:106905ms step_avg:61.23ms
step:1747/2330 train_time:106966ms step_avg:61.23ms
step:1748/2330 train_time:107029ms step_avg:61.23ms
step:1749/2330 train_time:107090ms step_avg:61.23ms
step:1750/2330 train_time:107153ms step_avg:61.23ms
step:1750/2330 val_loss:3.4018 train_time:107217ms step_avg:61.27ms
step:1751/2330 train_time:107241ms step_avg:61.25ms
step:1752/2330 train_time:107278ms step_avg:61.23ms
step:1753/2330 train_time:107346ms step_avg:61.24ms
step:1754/2330 train_time:107413ms step_avg:61.24ms
step:1755/2330 train_time:107475ms step_avg:61.24ms
step:1756/2330 train_time:107539ms step_avg:61.24ms
step:1757/2330 train_time:107599ms step_avg:61.24ms
step:1758/2330 train_time:107661ms step_avg:61.24ms
step:1759/2330 train_time:107720ms step_avg:61.24ms
step:1760/2330 train_time:107782ms step_avg:61.24ms
step:1761/2330 train_time:107841ms step_avg:61.24ms
step:1762/2330 train_time:107903ms step_avg:61.24ms
step:1763/2330 train_time:107963ms step_avg:61.24ms
step:1764/2330 train_time:108024ms step_avg:61.24ms
step:1765/2330 train_time:108084ms step_avg:61.24ms
step:1766/2330 train_time:108149ms step_avg:61.24ms
step:1767/2330 train_time:108210ms step_avg:61.24ms
step:1768/2330 train_time:108273ms step_avg:61.24ms
step:1769/2330 train_time:108337ms step_avg:61.24ms
step:1770/2330 train_time:108401ms step_avg:61.24ms
step:1771/2330 train_time:108462ms step_avg:61.24ms
step:1772/2330 train_time:108525ms step_avg:61.24ms
step:1773/2330 train_time:108586ms step_avg:61.24ms
step:1774/2330 train_time:108649ms step_avg:61.25ms
step:1775/2330 train_time:108710ms step_avg:61.24ms
step:1776/2330 train_time:108772ms step_avg:61.25ms
step:1777/2330 train_time:108833ms step_avg:61.25ms
step:1778/2330 train_time:108896ms step_avg:61.25ms
step:1779/2330 train_time:108957ms step_avg:61.25ms
step:1780/2330 train_time:109019ms step_avg:61.25ms
step:1781/2330 train_time:109080ms step_avg:61.25ms
step:1782/2330 train_time:109143ms step_avg:61.25ms
step:1783/2330 train_time:109203ms step_avg:61.25ms
step:1784/2330 train_time:109266ms step_avg:61.25ms
step:1785/2330 train_time:109328ms step_avg:61.25ms
step:1786/2330 train_time:109391ms step_avg:61.25ms
step:1787/2330 train_time:109453ms step_avg:61.25ms
step:1788/2330 train_time:109516ms step_avg:61.25ms
step:1789/2330 train_time:109577ms step_avg:61.25ms
step:1790/2330 train_time:109640ms step_avg:61.25ms
step:1791/2330 train_time:109701ms step_avg:61.25ms
step:1792/2330 train_time:109764ms step_avg:61.25ms
step:1793/2330 train_time:109824ms step_avg:61.25ms
step:1794/2330 train_time:109887ms step_avg:61.25ms
step:1795/2330 train_time:109947ms step_avg:61.25ms
step:1796/2330 train_time:110010ms step_avg:61.25ms
step:1797/2330 train_time:110070ms step_avg:61.25ms
step:1798/2330 train_time:110132ms step_avg:61.25ms
step:1799/2330 train_time:110194ms step_avg:61.25ms
step:1800/2330 train_time:110257ms step_avg:61.25ms
step:1801/2330 train_time:110317ms step_avg:61.25ms
step:1802/2330 train_time:110381ms step_avg:61.25ms
step:1803/2330 train_time:110442ms step_avg:61.25ms
step:1804/2330 train_time:110506ms step_avg:61.26ms
step:1805/2330 train_time:110567ms step_avg:61.26ms
step:1806/2330 train_time:110631ms step_avg:61.26ms
step:1807/2330 train_time:110692ms step_avg:61.26ms
step:1808/2330 train_time:110755ms step_avg:61.26ms
step:1809/2330 train_time:110815ms step_avg:61.26ms
step:1810/2330 train_time:110878ms step_avg:61.26ms
step:1811/2330 train_time:110939ms step_avg:61.26ms
step:1812/2330 train_time:111002ms step_avg:61.26ms
step:1813/2330 train_time:111062ms step_avg:61.26ms
step:1814/2330 train_time:111125ms step_avg:61.26ms
step:1815/2330 train_time:111186ms step_avg:61.26ms
step:1816/2330 train_time:111248ms step_avg:61.26ms
step:1817/2330 train_time:111309ms step_avg:61.26ms
step:1818/2330 train_time:111372ms step_avg:61.26ms
step:1819/2330 train_time:111433ms step_avg:61.26ms
step:1820/2330 train_time:111496ms step_avg:61.26ms
step:1821/2330 train_time:111558ms step_avg:61.26ms
step:1822/2330 train_time:111620ms step_avg:61.26ms
step:1823/2330 train_time:111681ms step_avg:61.26ms
step:1824/2330 train_time:111744ms step_avg:61.26ms
step:1825/2330 train_time:111805ms step_avg:61.26ms
step:1826/2330 train_time:111867ms step_avg:61.26ms
step:1827/2330 train_time:111928ms step_avg:61.26ms
step:1828/2330 train_time:111991ms step_avg:61.26ms
step:1829/2330 train_time:112051ms step_avg:61.26ms
step:1830/2330 train_time:112114ms step_avg:61.26ms
step:1831/2330 train_time:112175ms step_avg:61.26ms
step:1832/2330 train_time:112238ms step_avg:61.27ms
step:1833/2330 train_time:112298ms step_avg:61.26ms
step:1834/2330 train_time:112360ms step_avg:61.27ms
step:1835/2330 train_time:112421ms step_avg:61.26ms
step:1836/2330 train_time:112484ms step_avg:61.27ms
step:1837/2330 train_time:112544ms step_avg:61.27ms
step:1838/2330 train_time:112607ms step_avg:61.27ms
step:1839/2330 train_time:112668ms step_avg:61.27ms
step:1840/2330 train_time:112731ms step_avg:61.27ms
step:1841/2330 train_time:112792ms step_avg:61.27ms
step:1842/2330 train_time:112855ms step_avg:61.27ms
step:1843/2330 train_time:112915ms step_avg:61.27ms
step:1844/2330 train_time:112978ms step_avg:61.27ms
step:1845/2330 train_time:113039ms step_avg:61.27ms
step:1846/2330 train_time:113102ms step_avg:61.27ms
step:1847/2330 train_time:113162ms step_avg:61.27ms
step:1848/2330 train_time:113224ms step_avg:61.27ms
step:1849/2330 train_time:113284ms step_avg:61.27ms
step:1850/2330 train_time:113346ms step_avg:61.27ms
step:1851/2330 train_time:113407ms step_avg:61.27ms
step:1852/2330 train_time:113470ms step_avg:61.27ms
step:1853/2330 train_time:113532ms step_avg:61.27ms
step:1854/2330 train_time:113595ms step_avg:61.27ms
step:1855/2330 train_time:113656ms step_avg:61.27ms
step:1856/2330 train_time:113719ms step_avg:61.27ms
step:1857/2330 train_time:113780ms step_avg:61.27ms
step:1858/2330 train_time:113843ms step_avg:61.27ms
step:1859/2330 train_time:113903ms step_avg:61.27ms
step:1860/2330 train_time:113965ms step_avg:61.27ms
step:1861/2330 train_time:114026ms step_avg:61.27ms
step:1862/2330 train_time:114089ms step_avg:61.27ms
step:1863/2330 train_time:114150ms step_avg:61.27ms
step:1864/2330 train_time:114212ms step_avg:61.27ms
step:1865/2330 train_time:114273ms step_avg:61.27ms
step:1866/2330 train_time:114336ms step_avg:61.27ms
step:1867/2330 train_time:114397ms step_avg:61.27ms
step:1868/2330 train_time:114459ms step_avg:61.27ms
step:1869/2330 train_time:114520ms step_avg:61.27ms
step:1870/2330 train_time:114582ms step_avg:61.27ms
step:1871/2330 train_time:114643ms step_avg:61.27ms
step:1872/2330 train_time:114706ms step_avg:61.27ms
step:1873/2330 train_time:114766ms step_avg:61.27ms
step:1874/2330 train_time:114829ms step_avg:61.27ms
step:1875/2330 train_time:114890ms step_avg:61.27ms
step:1876/2330 train_time:114952ms step_avg:61.27ms
step:1877/2330 train_time:115013ms step_avg:61.27ms
step:1878/2330 train_time:115076ms step_avg:61.28ms
step:1879/2330 train_time:115136ms step_avg:61.28ms
step:1880/2330 train_time:115199ms step_avg:61.28ms
step:1881/2330 train_time:115260ms step_avg:61.28ms
step:1882/2330 train_time:115323ms step_avg:61.28ms
step:1883/2330 train_time:115383ms step_avg:61.28ms
step:1884/2330 train_time:115445ms step_avg:61.28ms
step:1885/2330 train_time:115506ms step_avg:61.28ms
step:1886/2330 train_time:115569ms step_avg:61.28ms
step:1887/2330 train_time:115629ms step_avg:61.28ms
step:1888/2330 train_time:115693ms step_avg:61.28ms
step:1889/2330 train_time:115754ms step_avg:61.28ms
step:1890/2330 train_time:115817ms step_avg:61.28ms
step:1891/2330 train_time:115878ms step_avg:61.28ms
step:1892/2330 train_time:115940ms step_avg:61.28ms
step:1893/2330 train_time:116001ms step_avg:61.28ms
step:1894/2330 train_time:116063ms step_avg:61.28ms
step:1895/2330 train_time:116123ms step_avg:61.28ms
step:1896/2330 train_time:116186ms step_avg:61.28ms
step:1897/2330 train_time:116247ms step_avg:61.28ms
step:1898/2330 train_time:116310ms step_avg:61.28ms
step:1899/2330 train_time:116371ms step_avg:61.28ms
step:1900/2330 train_time:116434ms step_avg:61.28ms
step:1901/2330 train_time:116495ms step_avg:61.28ms
step:1902/2330 train_time:116558ms step_avg:61.28ms
step:1903/2330 train_time:116619ms step_avg:61.28ms
step:1904/2330 train_time:116682ms step_avg:61.28ms
step:1905/2330 train_time:116743ms step_avg:61.28ms
step:1906/2330 train_time:116806ms step_avg:61.28ms
step:1907/2330 train_time:116866ms step_avg:61.28ms
step:1908/2330 train_time:116929ms step_avg:61.28ms
step:1909/2330 train_time:116991ms step_avg:61.28ms
step:1910/2330 train_time:117053ms step_avg:61.28ms
step:1911/2330 train_time:117114ms step_avg:61.28ms
step:1912/2330 train_time:117176ms step_avg:61.28ms
step:1913/2330 train_time:117237ms step_avg:61.28ms
step:1914/2330 train_time:117300ms step_avg:61.29ms
step:1915/2330 train_time:117361ms step_avg:61.28ms
step:1916/2330 train_time:117423ms step_avg:61.29ms
step:1917/2330 train_time:117483ms step_avg:61.28ms
step:1918/2330 train_time:117546ms step_avg:61.29ms
step:1919/2330 train_time:117606ms step_avg:61.29ms
step:1920/2330 train_time:117669ms step_avg:61.29ms
step:1921/2330 train_time:117730ms step_avg:61.29ms
step:1922/2330 train_time:117794ms step_avg:61.29ms
step:1923/2330 train_time:117855ms step_avg:61.29ms
step:1924/2330 train_time:117918ms step_avg:61.29ms
step:1925/2330 train_time:117978ms step_avg:61.29ms
step:1926/2330 train_time:118041ms step_avg:61.29ms
step:1927/2330 train_time:118102ms step_avg:61.29ms
step:1928/2330 train_time:118164ms step_avg:61.29ms
step:1929/2330 train_time:118224ms step_avg:61.29ms
step:1930/2330 train_time:118287ms step_avg:61.29ms
step:1931/2330 train_time:118348ms step_avg:61.29ms
step:1932/2330 train_time:118411ms step_avg:61.29ms
step:1933/2330 train_time:118471ms step_avg:61.29ms
step:1934/2330 train_time:118534ms step_avg:61.29ms
step:1935/2330 train_time:118595ms step_avg:61.29ms
step:1936/2330 train_time:118658ms step_avg:61.29ms
step:1937/2330 train_time:118718ms step_avg:61.29ms
step:1938/2330 train_time:118781ms step_avg:61.29ms
step:1939/2330 train_time:118842ms step_avg:61.29ms
step:1940/2330 train_time:118905ms step_avg:61.29ms
step:1941/2330 train_time:118965ms step_avg:61.29ms
step:1942/2330 train_time:119029ms step_avg:61.29ms
step:1943/2330 train_time:119090ms step_avg:61.29ms
step:1944/2330 train_time:119153ms step_avg:61.29ms
step:1945/2330 train_time:119213ms step_avg:61.29ms
step:1946/2330 train_time:119277ms step_avg:61.29ms
step:1947/2330 train_time:119338ms step_avg:61.29ms
step:1948/2330 train_time:119401ms step_avg:61.29ms
step:1949/2330 train_time:119461ms step_avg:61.29ms
step:1950/2330 train_time:119524ms step_avg:61.29ms
step:1951/2330 train_time:119584ms step_avg:61.29ms
step:1952/2330 train_time:119646ms step_avg:61.29ms
step:1953/2330 train_time:119707ms step_avg:61.29ms
step:1954/2330 train_time:119770ms step_avg:61.29ms
step:1955/2330 train_time:119831ms step_avg:61.29ms
step:1956/2330 train_time:119894ms step_avg:61.30ms
step:1957/2330 train_time:119955ms step_avg:61.30ms
step:1958/2330 train_time:120018ms step_avg:61.30ms
step:1959/2330 train_time:120079ms step_avg:61.30ms
step:1960/2330 train_time:120142ms step_avg:61.30ms
step:1961/2330 train_time:120202ms step_avg:61.30ms
step:1962/2330 train_time:120265ms step_avg:61.30ms
step:1963/2330 train_time:120325ms step_avg:61.30ms
step:1964/2330 train_time:120388ms step_avg:61.30ms
step:1965/2330 train_time:120449ms step_avg:61.30ms
step:1966/2330 train_time:120513ms step_avg:61.30ms
step:1967/2330 train_time:120573ms step_avg:61.30ms
step:1968/2330 train_time:120636ms step_avg:61.30ms
step:1969/2330 train_time:120696ms step_avg:61.30ms
step:1970/2330 train_time:120760ms step_avg:61.30ms
step:1971/2330 train_time:120821ms step_avg:61.30ms
step:1972/2330 train_time:120883ms step_avg:61.30ms
step:1973/2330 train_time:120944ms step_avg:61.30ms
step:1974/2330 train_time:121007ms step_avg:61.30ms
step:1975/2330 train_time:121067ms step_avg:61.30ms
step:1976/2330 train_time:121131ms step_avg:61.30ms
step:1977/2330 train_time:121192ms step_avg:61.30ms
step:1978/2330 train_time:121255ms step_avg:61.30ms
step:1979/2330 train_time:121315ms step_avg:61.30ms
step:1980/2330 train_time:121379ms step_avg:61.30ms
step:1981/2330 train_time:121439ms step_avg:61.30ms
step:1982/2330 train_time:121503ms step_avg:61.30ms
step:1983/2330 train_time:121563ms step_avg:61.30ms
step:1984/2330 train_time:121626ms step_avg:61.30ms
step:1985/2330 train_time:121686ms step_avg:61.30ms
step:1986/2330 train_time:121748ms step_avg:61.30ms
step:1987/2330 train_time:121809ms step_avg:61.30ms
step:1988/2330 train_time:121872ms step_avg:61.30ms
step:1989/2330 train_time:121932ms step_avg:61.30ms
step:1990/2330 train_time:121995ms step_avg:61.30ms
step:1991/2330 train_time:122057ms step_avg:61.30ms
step:1992/2330 train_time:122119ms step_avg:61.30ms
step:1993/2330 train_time:122180ms step_avg:61.30ms
step:1994/2330 train_time:122242ms step_avg:61.31ms
step:1995/2330 train_time:122303ms step_avg:61.30ms
step:1996/2330 train_time:122366ms step_avg:61.31ms
step:1997/2330 train_time:122427ms step_avg:61.31ms
step:1998/2330 train_time:122490ms step_avg:61.31ms
step:1999/2330 train_time:122551ms step_avg:61.31ms
step:2000/2330 train_time:122614ms step_avg:61.31ms
step:2000/2330 val_loss:3.3530 train_time:122679ms step_avg:61.34ms
step:2001/2330 train_time:122702ms step_avg:61.32ms
step:2002/2330 train_time:122742ms step_avg:61.31ms
step:2003/2330 train_time:122811ms step_avg:61.31ms
step:2004/2330 train_time:122875ms step_avg:61.31ms
step:2005/2330 train_time:122935ms step_avg:61.31ms
step:2006/2330 train_time:122997ms step_avg:61.31ms
step:2007/2330 train_time:123057ms step_avg:61.31ms
step:2008/2330 train_time:123120ms step_avg:61.31ms
step:2009/2330 train_time:123180ms step_avg:61.31ms
step:2010/2330 train_time:123242ms step_avg:61.31ms
step:2011/2330 train_time:123301ms step_avg:61.31ms
step:2012/2330 train_time:123363ms step_avg:61.31ms
step:2013/2330 train_time:123423ms step_avg:61.31ms
step:2014/2330 train_time:123485ms step_avg:61.31ms
step:2015/2330 train_time:123545ms step_avg:61.31ms
step:2016/2330 train_time:123608ms step_avg:61.31ms
step:2017/2330 train_time:123670ms step_avg:61.31ms
step:2018/2330 train_time:123735ms step_avg:61.32ms
step:2019/2330 train_time:123798ms step_avg:61.32ms
step:2020/2330 train_time:123861ms step_avg:61.32ms
step:2021/2330 train_time:123922ms step_avg:61.32ms
step:2022/2330 train_time:123986ms step_avg:61.32ms
step:2023/2330 train_time:124046ms step_avg:61.32ms
step:2024/2330 train_time:124109ms step_avg:61.32ms
step:2025/2330 train_time:124170ms step_avg:61.32ms
step:2026/2330 train_time:124232ms step_avg:61.32ms
step:2027/2330 train_time:124292ms step_avg:61.32ms
step:2028/2330 train_time:124355ms step_avg:61.32ms
step:2029/2330 train_time:124415ms step_avg:61.32ms
step:2030/2330 train_time:124477ms step_avg:61.32ms
step:2031/2330 train_time:124538ms step_avg:61.32ms
step:2032/2330 train_time:124601ms step_avg:61.32ms
step:2033/2330 train_time:124661ms step_avg:61.32ms
step:2034/2330 train_time:124725ms step_avg:61.32ms
step:2035/2330 train_time:124787ms step_avg:61.32ms
step:2036/2330 train_time:124851ms step_avg:61.32ms
step:2037/2330 train_time:124911ms step_avg:61.32ms
step:2038/2330 train_time:124974ms step_avg:61.32ms
step:2039/2330 train_time:125035ms step_avg:61.32ms
step:2040/2330 train_time:125097ms step_avg:61.32ms
step:2041/2330 train_time:125157ms step_avg:61.32ms
step:2042/2330 train_time:125220ms step_avg:61.32ms
step:2043/2330 train_time:125281ms step_avg:61.32ms
step:2044/2330 train_time:125343ms step_avg:61.32ms
step:2045/2330 train_time:125405ms step_avg:61.32ms
step:2046/2330 train_time:125467ms step_avg:61.32ms
step:2047/2330 train_time:125526ms step_avg:61.32ms
step:2048/2330 train_time:125590ms step_avg:61.32ms
step:2049/2330 train_time:125651ms step_avg:61.32ms
step:2050/2330 train_time:125714ms step_avg:61.32ms
step:2051/2330 train_time:125774ms step_avg:61.32ms
step:2052/2330 train_time:125837ms step_avg:61.32ms
step:2053/2330 train_time:125898ms step_avg:61.32ms
step:2054/2330 train_time:125961ms step_avg:61.32ms
step:2055/2330 train_time:126022ms step_avg:61.32ms
step:2056/2330 train_time:126085ms step_avg:61.33ms
step:2057/2330 train_time:126146ms step_avg:61.33ms
step:2058/2330 train_time:126209ms step_avg:61.33ms
step:2059/2330 train_time:126270ms step_avg:61.33ms
step:2060/2330 train_time:126333ms step_avg:61.33ms
step:2061/2330 train_time:126393ms step_avg:61.33ms
step:2062/2330 train_time:126456ms step_avg:61.33ms
step:2063/2330 train_time:126517ms step_avg:61.33ms
step:2064/2330 train_time:126579ms step_avg:61.33ms
step:2065/2330 train_time:126640ms step_avg:61.33ms
step:2066/2330 train_time:126703ms step_avg:61.33ms
step:2067/2330 train_time:126764ms step_avg:61.33ms
step:2068/2330 train_time:126827ms step_avg:61.33ms
step:2069/2330 train_time:126888ms step_avg:61.33ms
step:2070/2330 train_time:126951ms step_avg:61.33ms
step:2071/2330 train_time:127012ms step_avg:61.33ms
step:2072/2330 train_time:127075ms step_avg:61.33ms
step:2073/2330 train_time:127136ms step_avg:61.33ms
step:2074/2330 train_time:127198ms step_avg:61.33ms
step:2075/2330 train_time:127259ms step_avg:61.33ms
step:2076/2330 train_time:127322ms step_avg:61.33ms
step:2077/2330 train_time:127383ms step_avg:61.33ms
step:2078/2330 train_time:127445ms step_avg:61.33ms
step:2079/2330 train_time:127506ms step_avg:61.33ms
step:2080/2330 train_time:127568ms step_avg:61.33ms
step:2081/2330 train_time:127630ms step_avg:61.33ms
step:2082/2330 train_time:127692ms step_avg:61.33ms
step:2083/2330 train_time:127753ms step_avg:61.33ms
step:2084/2330 train_time:127816ms step_avg:61.33ms
step:2085/2330 train_time:127876ms step_avg:61.33ms
step:2086/2330 train_time:127939ms step_avg:61.33ms
step:2087/2330 train_time:127999ms step_avg:61.33ms
step:2088/2330 train_time:128062ms step_avg:61.33ms
step:2089/2330 train_time:128123ms step_avg:61.33ms
step:2090/2330 train_time:128186ms step_avg:61.33ms
step:2091/2330 train_time:128247ms step_avg:61.33ms
step:2092/2330 train_time:128310ms step_avg:61.33ms
step:2093/2330 train_time:128370ms step_avg:61.33ms
step:2094/2330 train_time:128433ms step_avg:61.33ms
step:2095/2330 train_time:128494ms step_avg:61.33ms
step:2096/2330 train_time:128557ms step_avg:61.33ms
step:2097/2330 train_time:128617ms step_avg:61.33ms
step:2098/2330 train_time:128680ms step_avg:61.33ms
step:2099/2330 train_time:128740ms step_avg:61.33ms
step:2100/2330 train_time:128803ms step_avg:61.33ms
step:2101/2330 train_time:128864ms step_avg:61.33ms
step:2102/2330 train_time:128927ms step_avg:61.34ms
step:2103/2330 train_time:128989ms step_avg:61.34ms
step:2104/2330 train_time:129052ms step_avg:61.34ms
step:2105/2330 train_time:129112ms step_avg:61.34ms
step:2106/2330 train_time:129174ms step_avg:61.34ms
step:2107/2330 train_time:129235ms step_avg:61.34ms
step:2108/2330 train_time:129298ms step_avg:61.34ms
step:2109/2330 train_time:129358ms step_avg:61.34ms
step:2110/2330 train_time:129422ms step_avg:61.34ms
step:2111/2330 train_time:129482ms step_avg:61.34ms
step:2112/2330 train_time:129545ms step_avg:61.34ms
step:2113/2330 train_time:129605ms step_avg:61.34ms
step:2114/2330 train_time:129668ms step_avg:61.34ms
step:2115/2330 train_time:129729ms step_avg:61.34ms
step:2116/2330 train_time:129792ms step_avg:61.34ms
step:2117/2330 train_time:129852ms step_avg:61.34ms
step:2118/2330 train_time:129915ms step_avg:61.34ms
step:2119/2330 train_time:129976ms step_avg:61.34ms
step:2120/2330 train_time:130038ms step_avg:61.34ms
step:2121/2330 train_time:130099ms step_avg:61.34ms
step:2122/2330 train_time:130162ms step_avg:61.34ms
step:2123/2330 train_time:130223ms step_avg:61.34ms
step:2124/2330 train_time:130286ms step_avg:61.34ms
step:2125/2330 train_time:130346ms step_avg:61.34ms
step:2126/2330 train_time:130409ms step_avg:61.34ms
step:2127/2330 train_time:130470ms step_avg:61.34ms
step:2128/2330 train_time:130533ms step_avg:61.34ms
step:2129/2330 train_time:130593ms step_avg:61.34ms
step:2130/2330 train_time:130656ms step_avg:61.34ms
step:2131/2330 train_time:130717ms step_avg:61.34ms
step:2132/2330 train_time:130779ms step_avg:61.34ms
step:2133/2330 train_time:130840ms step_avg:61.34ms
step:2134/2330 train_time:130902ms step_avg:61.34ms
step:2135/2330 train_time:130963ms step_avg:61.34ms
step:2136/2330 train_time:131026ms step_avg:61.34ms
step:2137/2330 train_time:131087ms step_avg:61.34ms
step:2138/2330 train_time:131149ms step_avg:61.34ms
step:2139/2330 train_time:131210ms step_avg:61.34ms
step:2140/2330 train_time:131273ms step_avg:61.34ms
step:2141/2330 train_time:131334ms step_avg:61.34ms
step:2142/2330 train_time:131396ms step_avg:61.34ms
step:2143/2330 train_time:131456ms step_avg:61.34ms
step:2144/2330 train_time:131519ms step_avg:61.34ms
step:2145/2330 train_time:131580ms step_avg:61.34ms
step:2146/2330 train_time:131642ms step_avg:61.34ms
step:2147/2330 train_time:131703ms step_avg:61.34ms
step:2148/2330 train_time:131765ms step_avg:61.34ms
step:2149/2330 train_time:131826ms step_avg:61.34ms
step:2150/2330 train_time:131889ms step_avg:61.34ms
step:2151/2330 train_time:131950ms step_avg:61.34ms
step:2152/2330 train_time:132013ms step_avg:61.34ms
step:2153/2330 train_time:132073ms step_avg:61.34ms
step:2154/2330 train_time:132136ms step_avg:61.34ms
step:2155/2330 train_time:132196ms step_avg:61.34ms
step:2156/2330 train_time:132259ms step_avg:61.34ms
step:2157/2330 train_time:132320ms step_avg:61.34ms
step:2158/2330 train_time:132383ms step_avg:61.35ms
step:2159/2330 train_time:132444ms step_avg:61.34ms
step:2160/2330 train_time:132507ms step_avg:61.35ms
step:2161/2330 train_time:132568ms step_avg:61.35ms
step:2162/2330 train_time:132632ms step_avg:61.35ms
step:2163/2330 train_time:132693ms step_avg:61.35ms
step:2164/2330 train_time:132755ms step_avg:61.35ms
step:2165/2330 train_time:132816ms step_avg:61.35ms
step:2166/2330 train_time:132878ms step_avg:61.35ms
step:2167/2330 train_time:132939ms step_avg:61.35ms
step:2168/2330 train_time:133002ms step_avg:61.35ms
step:2169/2330 train_time:133063ms step_avg:61.35ms
step:2170/2330 train_time:133126ms step_avg:61.35ms
step:2171/2330 train_time:133187ms step_avg:61.35ms
step:2172/2330 train_time:133250ms step_avg:61.35ms
step:2173/2330 train_time:133311ms step_avg:61.35ms
step:2174/2330 train_time:133374ms step_avg:61.35ms
step:2175/2330 train_time:133434ms step_avg:61.35ms
step:2176/2330 train_time:133496ms step_avg:61.35ms
step:2177/2330 train_time:133557ms step_avg:61.35ms
step:2178/2330 train_time:133620ms step_avg:61.35ms
step:2179/2330 train_time:133681ms step_avg:61.35ms
step:2180/2330 train_time:133745ms step_avg:61.35ms
step:2181/2330 train_time:133806ms step_avg:61.35ms
step:2182/2330 train_time:133869ms step_avg:61.35ms
step:2183/2330 train_time:133930ms step_avg:61.35ms
step:2184/2330 train_time:133993ms step_avg:61.35ms
step:2185/2330 train_time:134054ms step_avg:61.35ms
step:2186/2330 train_time:134117ms step_avg:61.35ms
step:2187/2330 train_time:134178ms step_avg:61.35ms
step:2188/2330 train_time:134240ms step_avg:61.35ms
step:2189/2330 train_time:134301ms step_avg:61.35ms
step:2190/2330 train_time:134364ms step_avg:61.35ms
step:2191/2330 train_time:134425ms step_avg:61.35ms
step:2192/2330 train_time:134488ms step_avg:61.35ms
step:2193/2330 train_time:134549ms step_avg:61.35ms
step:2194/2330 train_time:134612ms step_avg:61.35ms
step:2195/2330 train_time:134673ms step_avg:61.35ms
step:2196/2330 train_time:134736ms step_avg:61.36ms
step:2197/2330 train_time:134796ms step_avg:61.35ms
step:2198/2330 train_time:134859ms step_avg:61.36ms
step:2199/2330 train_time:134920ms step_avg:61.35ms
step:2200/2330 train_time:134983ms step_avg:61.36ms
step:2201/2330 train_time:135045ms step_avg:61.36ms
step:2202/2330 train_time:135107ms step_avg:61.36ms
step:2203/2330 train_time:135168ms step_avg:61.36ms
step:2204/2330 train_time:135231ms step_avg:61.36ms
step:2205/2330 train_time:135292ms step_avg:61.36ms
step:2206/2330 train_time:135354ms step_avg:61.36ms
step:2207/2330 train_time:135415ms step_avg:61.36ms
step:2208/2330 train_time:135478ms step_avg:61.36ms
step:2209/2330 train_time:135539ms step_avg:61.36ms
step:2210/2330 train_time:135601ms step_avg:61.36ms
step:2211/2330 train_time:135662ms step_avg:61.36ms
step:2212/2330 train_time:135725ms step_avg:61.36ms
step:2213/2330 train_time:135786ms step_avg:61.36ms
step:2214/2330 train_time:135848ms step_avg:61.36ms
step:2215/2330 train_time:135909ms step_avg:61.36ms
step:2216/2330 train_time:135973ms step_avg:61.36ms
step:2217/2330 train_time:136034ms step_avg:61.36ms
step:2218/2330 train_time:136096ms step_avg:61.36ms
step:2219/2330 train_time:136156ms step_avg:61.36ms
step:2220/2330 train_time:136220ms step_avg:61.36ms
step:2221/2330 train_time:136281ms step_avg:61.36ms
step:2222/2330 train_time:136344ms step_avg:61.36ms
step:2223/2330 train_time:136404ms step_avg:61.36ms
step:2224/2330 train_time:136467ms step_avg:61.36ms
step:2225/2330 train_time:136528ms step_avg:61.36ms
step:2226/2330 train_time:136591ms step_avg:61.36ms
step:2227/2330 train_time:136651ms step_avg:61.36ms
step:2228/2330 train_time:136714ms step_avg:61.36ms
step:2229/2330 train_time:136774ms step_avg:61.36ms
step:2230/2330 train_time:136838ms step_avg:61.36ms
step:2231/2330 train_time:136898ms step_avg:61.36ms
step:2232/2330 train_time:136961ms step_avg:61.36ms
step:2233/2330 train_time:137022ms step_avg:61.36ms
step:2234/2330 train_time:137084ms step_avg:61.36ms
step:2235/2330 train_time:137145ms step_avg:61.36ms
step:2236/2330 train_time:137209ms step_avg:61.36ms
step:2237/2330 train_time:137270ms step_avg:61.36ms
step:2238/2330 train_time:137333ms step_avg:61.36ms
step:2239/2330 train_time:137394ms step_avg:61.36ms
step:2240/2330 train_time:137456ms step_avg:61.36ms
step:2241/2330 train_time:137516ms step_avg:61.36ms
step:2242/2330 train_time:137579ms step_avg:61.36ms
step:2243/2330 train_time:137639ms step_avg:61.36ms
step:2244/2330 train_time:137701ms step_avg:61.36ms
step:2245/2330 train_time:137762ms step_avg:61.36ms
step:2246/2330 train_time:137825ms step_avg:61.36ms
step:2247/2330 train_time:137885ms step_avg:61.36ms
step:2248/2330 train_time:137948ms step_avg:61.36ms
step:2249/2330 train_time:138008ms step_avg:61.36ms
step:2250/2330 train_time:138071ms step_avg:61.36ms
step:2250/2330 val_loss:3.3130 train_time:138136ms step_avg:61.39ms
step:2251/2330 train_time:138161ms step_avg:61.38ms
step:2252/2330 train_time:138199ms step_avg:61.37ms
step:2253/2330 train_time:138265ms step_avg:61.37ms
step:2254/2330 train_time:138331ms step_avg:61.37ms
step:2255/2330 train_time:138392ms step_avg:61.37ms
step:2256/2330 train_time:138455ms step_avg:61.37ms
step:2257/2330 train_time:138515ms step_avg:61.37ms
step:2258/2330 train_time:138577ms step_avg:61.37ms
step:2259/2330 train_time:138637ms step_avg:61.37ms
step:2260/2330 train_time:138699ms step_avg:61.37ms
step:2261/2330 train_time:138759ms step_avg:61.37ms
step:2262/2330 train_time:138821ms step_avg:61.37ms
step:2263/2330 train_time:138881ms step_avg:61.37ms
step:2264/2330 train_time:138943ms step_avg:61.37ms
step:2265/2330 train_time:139003ms step_avg:61.37ms
step:2266/2330 train_time:139066ms step_avg:61.37ms
step:2267/2330 train_time:139128ms step_avg:61.37ms
step:2268/2330 train_time:139192ms step_avg:61.37ms
step:2269/2330 train_time:139255ms step_avg:61.37ms
step:2270/2330 train_time:139319ms step_avg:61.37ms
step:2271/2330 train_time:139381ms step_avg:61.37ms
step:2272/2330 train_time:139444ms step_avg:61.38ms
step:2273/2330 train_time:139505ms step_avg:61.37ms
step:2274/2330 train_time:139567ms step_avg:61.38ms
step:2275/2330 train_time:139628ms step_avg:61.37ms
step:2276/2330 train_time:139690ms step_avg:61.38ms
step:2277/2330 train_time:139751ms step_avg:61.38ms
step:2278/2330 train_time:139814ms step_avg:61.38ms
step:2279/2330 train_time:139874ms step_avg:61.37ms
step:2280/2330 train_time:139936ms step_avg:61.38ms
step:2281/2330 train_time:139996ms step_avg:61.37ms
step:2282/2330 train_time:140059ms step_avg:61.38ms
step:2283/2330 train_time:140121ms step_avg:61.38ms
step:2284/2330 train_time:140184ms step_avg:61.38ms
step:2285/2330 train_time:140246ms step_avg:61.38ms
step:2286/2330 train_time:140310ms step_avg:61.38ms
step:2287/2330 train_time:140371ms step_avg:61.38ms
step:2288/2330 train_time:140435ms step_avg:61.38ms
step:2289/2330 train_time:140495ms step_avg:61.38ms
step:2290/2330 train_time:140558ms step_avg:61.38ms
step:2291/2330 train_time:140618ms step_avg:61.38ms
step:2292/2330 train_time:140681ms step_avg:61.38ms
step:2293/2330 train_time:140741ms step_avg:61.38ms
step:2294/2330 train_time:140804ms step_avg:61.38ms
step:2295/2330 train_time:140865ms step_avg:61.38ms
step:2296/2330 train_time:140927ms step_avg:61.38ms
step:2297/2330 train_time:140988ms step_avg:61.38ms
step:2298/2330 train_time:141050ms step_avg:61.38ms
step:2299/2330 train_time:141110ms step_avg:61.38ms
step:2300/2330 train_time:141172ms step_avg:61.38ms
step:2301/2330 train_time:141234ms step_avg:61.38ms
step:2302/2330 train_time:141297ms step_avg:61.38ms
step:2303/2330 train_time:141358ms step_avg:61.38ms
step:2304/2330 train_time:141422ms step_avg:61.38ms
step:2305/2330 train_time:141483ms step_avg:61.38ms
step:2306/2330 train_time:141546ms step_avg:61.38ms
step:2307/2330 train_time:141606ms step_avg:61.38ms
step:2308/2330 train_time:141669ms step_avg:61.38ms
step:2309/2330 train_time:141729ms step_avg:61.38ms
step:2310/2330 train_time:141792ms step_avg:61.38ms
step:2311/2330 train_time:141852ms step_avg:61.38ms
step:2312/2330 train_time:141915ms step_avg:61.38ms
step:2313/2330 train_time:141976ms step_avg:61.38ms
step:2314/2330 train_time:142038ms step_avg:61.38ms
step:2315/2330 train_time:142098ms step_avg:61.38ms
step:2316/2330 train_time:142162ms step_avg:61.38ms
step:2317/2330 train_time:142222ms step_avg:61.38ms
step:2318/2330 train_time:142286ms step_avg:61.38ms
step:2319/2330 train_time:142346ms step_avg:61.38ms
step:2320/2330 train_time:142410ms step_avg:61.38ms
step:2321/2330 train_time:142470ms step_avg:61.38ms
step:2322/2330 train_time:142533ms step_avg:61.38ms
step:2323/2330 train_time:142594ms step_avg:61.38ms
step:2324/2330 train_time:142657ms step_avg:61.38ms
step:2325/2330 train_time:142719ms step_avg:61.38ms
step:2326/2330 train_time:142781ms step_avg:61.38ms
step:2327/2330 train_time:142841ms step_avg:61.38ms
step:2328/2330 train_time:142904ms step_avg:61.39ms
step:2329/2330 train_time:142965ms step_avg:61.38ms
step:2330/2330 train_time:143028ms step_avg:61.39ms
step:2330/2330 val_loss:3.2990 train_time:143093ms step_avg:61.41ms
peak memory allocated: 29226 MiB reserved: 44076 MiB
