import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_muon_lr5e-2"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=5e-2, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 04:16:49 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   30C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   30C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   32C    P0             115W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:80ms step_avg:79.61ms
step:2/2330 train_time:154ms step_avg:77.05ms
step:3/2330 train_time:190ms step_avg:63.42ms
step:4/2330 train_time:203ms step_avg:50.85ms
step:5/2330 train_time:215ms step_avg:42.90ms
step:6/2330 train_time:239ms step_avg:39.76ms
step:7/2330 train_time:272ms step_avg:38.83ms
step:8/2330 train_time:315ms step_avg:39.42ms
step:9/2330 train_time:350ms step_avg:38.86ms
step:10/2330 train_time:394ms step_avg:39.38ms
step:11/2330 train_time:428ms step_avg:38.94ms
step:12/2330 train_time:473ms step_avg:39.41ms
step:13/2330 train_time:508ms step_avg:39.05ms
step:14/2330 train_time:552ms step_avg:39.39ms
step:15/2330 train_time:586ms step_avg:39.07ms
step:16/2330 train_time:629ms step_avg:39.33ms
step:17/2330 train_time:664ms step_avg:39.07ms
step:18/2330 train_time:708ms step_avg:39.35ms
step:19/2330 train_time:743ms step_avg:39.10ms
step:20/2330 train_time:786ms step_avg:39.31ms
step:21/2330 train_time:821ms step_avg:39.08ms
step:22/2330 train_time:865ms step_avg:39.31ms
step:23/2330 train_time:899ms step_avg:39.11ms
step:24/2330 train_time:944ms step_avg:39.33ms
step:25/2330 train_time:978ms step_avg:39.13ms
step:26/2330 train_time:1025ms step_avg:39.42ms
step:27/2330 train_time:1063ms step_avg:39.38ms
step:28/2330 train_time:1114ms step_avg:39.78ms
step:29/2330 train_time:1151ms step_avg:39.70ms
step:30/2330 train_time:1197ms step_avg:39.89ms
step:31/2330 train_time:1232ms step_avg:39.75ms
step:32/2330 train_time:1277ms step_avg:39.92ms
step:33/2330 train_time:1313ms step_avg:39.78ms
step:34/2330 train_time:1357ms step_avg:39.91ms
step:35/2330 train_time:1392ms step_avg:39.77ms
step:36/2330 train_time:1437ms step_avg:39.92ms
step:37/2330 train_time:1473ms step_avg:39.80ms
step:38/2330 train_time:1517ms step_avg:39.92ms
step:39/2330 train_time:1552ms step_avg:39.79ms
step:40/2330 train_time:1596ms step_avg:39.90ms
step:41/2330 train_time:1631ms step_avg:39.79ms
step:42/2330 train_time:1675ms step_avg:39.89ms
step:43/2330 train_time:1710ms step_avg:39.77ms
step:44/2330 train_time:1754ms step_avg:39.86ms
step:45/2330 train_time:1788ms step_avg:39.74ms
step:46/2330 train_time:1833ms step_avg:39.85ms
step:47/2330 train_time:1868ms step_avg:39.74ms
step:48/2330 train_time:1912ms step_avg:39.83ms
step:49/2330 train_time:1947ms step_avg:39.74ms
step:50/2330 train_time:1993ms step_avg:39.85ms
step:51/2330 train_time:2028ms step_avg:39.77ms
step:52/2330 train_time:2074ms step_avg:39.88ms
step:53/2330 train_time:2110ms step_avg:39.81ms
step:54/2330 train_time:2156ms step_avg:39.92ms
step:55/2330 train_time:2192ms step_avg:39.85ms
step:56/2330 train_time:2237ms step_avg:39.94ms
step:57/2330 train_time:2272ms step_avg:39.86ms
step:58/2330 train_time:2317ms step_avg:39.94ms
step:59/2330 train_time:2352ms step_avg:39.86ms
step:60/2330 train_time:2397ms step_avg:39.95ms
step:61/2330 train_time:2433ms step_avg:39.89ms
step:62/2330 train_time:2478ms step_avg:39.96ms
step:63/2330 train_time:2513ms step_avg:39.88ms
step:64/2330 train_time:2557ms step_avg:39.96ms
step:65/2330 train_time:2593ms step_avg:39.88ms
step:66/2330 train_time:2637ms step_avg:39.95ms
step:67/2330 train_time:2672ms step_avg:39.88ms
step:68/2330 train_time:2717ms step_avg:39.95ms
step:69/2330 train_time:2752ms step_avg:39.88ms
step:70/2330 train_time:2796ms step_avg:39.94ms
step:71/2330 train_time:2831ms step_avg:39.87ms
step:72/2330 train_time:2876ms step_avg:39.94ms
step:73/2330 train_time:2911ms step_avg:39.88ms
step:74/2330 train_time:2956ms step_avg:39.95ms
step:75/2330 train_time:2992ms step_avg:39.89ms
step:76/2330 train_time:3037ms step_avg:39.96ms
step:77/2330 train_time:3072ms step_avg:39.90ms
step:78/2330 train_time:3117ms step_avg:39.96ms
step:79/2330 train_time:3152ms step_avg:39.90ms
step:80/2330 train_time:3198ms step_avg:39.97ms
step:81/2330 train_time:3233ms step_avg:39.91ms
step:82/2330 train_time:3277ms step_avg:39.97ms
step:83/2330 train_time:3313ms step_avg:39.91ms
step:84/2330 train_time:3358ms step_avg:39.97ms
step:85/2330 train_time:3393ms step_avg:39.92ms
step:86/2330 train_time:3438ms step_avg:39.98ms
step:87/2330 train_time:3473ms step_avg:39.92ms
step:88/2330 train_time:3517ms step_avg:39.97ms
step:89/2330 train_time:3553ms step_avg:39.92ms
step:90/2330 train_time:3597ms step_avg:39.97ms
step:91/2330 train_time:3632ms step_avg:39.91ms
step:92/2330 train_time:3676ms step_avg:39.96ms
step:93/2330 train_time:3711ms step_avg:39.91ms
step:94/2330 train_time:3756ms step_avg:39.95ms
step:95/2330 train_time:3790ms step_avg:39.90ms
step:96/2330 train_time:3835ms step_avg:39.95ms
step:97/2330 train_time:3870ms step_avg:39.90ms
step:98/2330 train_time:3914ms step_avg:39.94ms
step:99/2330 train_time:3950ms step_avg:39.90ms
step:100/2330 train_time:3994ms step_avg:39.94ms
step:101/2330 train_time:4029ms step_avg:39.89ms
step:102/2330 train_time:4074ms step_avg:39.95ms
step:103/2330 train_time:4110ms step_avg:39.91ms
step:104/2330 train_time:4155ms step_avg:39.96ms
step:105/2330 train_time:4191ms step_avg:39.91ms
step:106/2330 train_time:4236ms step_avg:39.96ms
step:107/2330 train_time:4271ms step_avg:39.91ms
step:108/2330 train_time:4316ms step_avg:39.96ms
step:109/2330 train_time:4351ms step_avg:39.91ms
step:110/2330 train_time:4396ms step_avg:39.96ms
step:111/2330 train_time:4431ms step_avg:39.92ms
step:112/2330 train_time:4476ms step_avg:39.96ms
step:113/2330 train_time:4511ms step_avg:39.92ms
step:114/2330 train_time:4555ms step_avg:39.96ms
step:115/2330 train_time:4590ms step_avg:39.92ms
step:116/2330 train_time:4635ms step_avg:39.95ms
step:117/2330 train_time:4669ms step_avg:39.91ms
step:118/2330 train_time:4713ms step_avg:39.94ms
step:119/2330 train_time:4748ms step_avg:39.90ms
step:120/2330 train_time:4792ms step_avg:39.93ms
step:121/2330 train_time:4827ms step_avg:39.89ms
step:122/2330 train_time:4871ms step_avg:39.93ms
step:123/2330 train_time:4906ms step_avg:39.89ms
step:124/2330 train_time:4950ms step_avg:39.92ms
step:125/2330 train_time:4986ms step_avg:39.88ms
step:126/2330 train_time:5030ms step_avg:39.92ms
step:127/2330 train_time:5065ms step_avg:39.88ms
step:128/2330 train_time:5111ms step_avg:39.93ms
step:129/2330 train_time:5145ms step_avg:39.89ms
step:130/2330 train_time:5191ms step_avg:39.93ms
step:131/2330 train_time:5226ms step_avg:39.89ms
step:132/2330 train_time:5270ms step_avg:39.93ms
step:133/2330 train_time:5306ms step_avg:39.90ms
step:134/2330 train_time:5351ms step_avg:39.93ms
step:135/2330 train_time:5386ms step_avg:39.90ms
step:136/2330 train_time:5430ms step_avg:39.93ms
step:137/2330 train_time:5465ms step_avg:39.89ms
step:138/2330 train_time:5510ms step_avg:39.93ms
step:139/2330 train_time:5545ms step_avg:39.89ms
step:140/2330 train_time:5589ms step_avg:39.92ms
step:141/2330 train_time:5624ms step_avg:39.89ms
step:142/2330 train_time:5669ms step_avg:39.92ms
step:143/2330 train_time:5704ms step_avg:39.89ms
step:144/2330 train_time:5748ms step_avg:39.92ms
step:145/2330 train_time:5783ms step_avg:39.88ms
step:146/2330 train_time:5827ms step_avg:39.91ms
step:147/2330 train_time:5862ms step_avg:39.88ms
step:148/2330 train_time:5907ms step_avg:39.91ms
step:149/2330 train_time:5942ms step_avg:39.88ms
step:150/2330 train_time:5987ms step_avg:39.91ms
step:151/2330 train_time:6022ms step_avg:39.88ms
step:152/2330 train_time:6067ms step_avg:39.91ms
step:153/2330 train_time:6103ms step_avg:39.89ms
step:154/2330 train_time:6148ms step_avg:39.92ms
step:155/2330 train_time:6183ms step_avg:39.89ms
step:156/2330 train_time:6227ms step_avg:39.92ms
step:157/2330 train_time:6263ms step_avg:39.89ms
step:158/2330 train_time:6307ms step_avg:39.92ms
step:159/2330 train_time:6343ms step_avg:39.89ms
step:160/2330 train_time:6388ms step_avg:39.93ms
step:161/2330 train_time:6424ms step_avg:39.90ms
step:162/2330 train_time:6468ms step_avg:39.93ms
step:163/2330 train_time:6503ms step_avg:39.90ms
step:164/2330 train_time:6548ms step_avg:39.93ms
step:165/2330 train_time:6583ms step_avg:39.90ms
step:166/2330 train_time:6628ms step_avg:39.93ms
step:167/2330 train_time:6663ms step_avg:39.90ms
step:168/2330 train_time:6707ms step_avg:39.92ms
step:169/2330 train_time:6742ms step_avg:39.89ms
step:170/2330 train_time:6786ms step_avg:39.92ms
step:171/2330 train_time:6820ms step_avg:39.89ms
step:172/2330 train_time:6865ms step_avg:39.91ms
step:173/2330 train_time:6900ms step_avg:39.89ms
step:174/2330 train_time:6945ms step_avg:39.91ms
step:175/2330 train_time:6980ms step_avg:39.89ms
step:176/2330 train_time:7024ms step_avg:39.91ms
step:177/2330 train_time:7060ms step_avg:39.88ms
step:178/2330 train_time:7105ms step_avg:39.91ms
step:179/2330 train_time:7140ms step_avg:39.89ms
step:180/2330 train_time:7186ms step_avg:39.92ms
step:181/2330 train_time:7221ms step_avg:39.90ms
step:182/2330 train_time:7267ms step_avg:39.93ms
step:183/2330 train_time:7301ms step_avg:39.90ms
step:184/2330 train_time:7346ms step_avg:39.93ms
step:185/2330 train_time:7382ms step_avg:39.90ms
step:186/2330 train_time:7427ms step_avg:39.93ms
step:187/2330 train_time:7462ms step_avg:39.90ms
step:188/2330 train_time:7507ms step_avg:39.93ms
step:189/2330 train_time:7542ms step_avg:39.90ms
step:190/2330 train_time:7586ms step_avg:39.93ms
step:191/2330 train_time:7621ms step_avg:39.90ms
step:192/2330 train_time:7666ms step_avg:39.93ms
step:193/2330 train_time:7701ms step_avg:39.90ms
step:194/2330 train_time:7745ms step_avg:39.92ms
step:195/2330 train_time:7780ms step_avg:39.90ms
step:196/2330 train_time:7825ms step_avg:39.92ms
step:197/2330 train_time:7861ms step_avg:39.90ms
step:198/2330 train_time:7905ms step_avg:39.93ms
step:199/2330 train_time:7940ms step_avg:39.90ms
step:200/2330 train_time:7984ms step_avg:39.92ms
step:201/2330 train_time:8020ms step_avg:39.90ms
step:202/2330 train_time:8064ms step_avg:39.92ms
step:203/2330 train_time:8099ms step_avg:39.90ms
step:204/2330 train_time:8144ms step_avg:39.92ms
step:205/2330 train_time:8180ms step_avg:39.90ms
step:206/2330 train_time:8225ms step_avg:39.93ms
step:207/2330 train_time:8261ms step_avg:39.91ms
step:208/2330 train_time:8306ms step_avg:39.93ms
step:209/2330 train_time:8341ms step_avg:39.91ms
step:210/2330 train_time:8387ms step_avg:39.94ms
step:211/2330 train_time:8421ms step_avg:39.91ms
step:212/2330 train_time:8466ms step_avg:39.94ms
step:213/2330 train_time:8501ms step_avg:39.91ms
step:214/2330 train_time:8546ms step_avg:39.93ms
step:215/2330 train_time:8581ms step_avg:39.91ms
step:216/2330 train_time:8625ms step_avg:39.93ms
step:217/2330 train_time:8661ms step_avg:39.91ms
step:218/2330 train_time:8705ms step_avg:39.93ms
step:219/2330 train_time:8741ms step_avg:39.91ms
step:220/2330 train_time:8786ms step_avg:39.93ms
step:221/2330 train_time:8821ms step_avg:39.91ms
step:222/2330 train_time:8866ms step_avg:39.94ms
step:223/2330 train_time:8901ms step_avg:39.91ms
step:224/2330 train_time:8946ms step_avg:39.94ms
step:225/2330 train_time:8981ms step_avg:39.91ms
step:226/2330 train_time:9025ms step_avg:39.93ms
step:227/2330 train_time:9060ms step_avg:39.91ms
step:228/2330 train_time:9104ms step_avg:39.93ms
step:229/2330 train_time:9140ms step_avg:39.91ms
step:230/2330 train_time:9185ms step_avg:39.94ms
step:231/2330 train_time:9220ms step_avg:39.91ms
step:232/2330 train_time:9265ms step_avg:39.94ms
step:233/2330 train_time:9301ms step_avg:39.92ms
step:234/2330 train_time:9346ms step_avg:39.94ms
step:235/2330 train_time:9381ms step_avg:39.92ms
step:236/2330 train_time:9426ms step_avg:39.94ms
step:237/2330 train_time:9462ms step_avg:39.92ms
step:238/2330 train_time:9507ms step_avg:39.94ms
step:239/2330 train_time:9541ms step_avg:39.92ms
step:240/2330 train_time:9586ms step_avg:39.94ms
step:241/2330 train_time:9621ms step_avg:39.92ms
step:242/2330 train_time:9666ms step_avg:39.94ms
step:243/2330 train_time:9701ms step_avg:39.92ms
step:244/2330 train_time:9746ms step_avg:39.94ms
step:245/2330 train_time:9781ms step_avg:39.92ms
step:246/2330 train_time:9825ms step_avg:39.94ms
step:247/2330 train_time:9861ms step_avg:39.92ms
step:248/2330 train_time:9906ms step_avg:39.94ms
step:249/2330 train_time:9941ms step_avg:39.92ms
step:250/2330 train_time:9985ms step_avg:39.94ms
step:250/2330 val_loss:5.4139 train_time:10072ms step_avg:40.29ms
step:251/2330 train_time:10086ms step_avg:40.18ms
step:252/2330 train_time:10098ms step_avg:40.07ms
step:253/2330 train_time:10109ms step_avg:39.96ms
step:254/2330 train_time:10145ms step_avg:39.94ms
step:255/2330 train_time:10179ms step_avg:39.92ms
step:256/2330 train_time:10223ms step_avg:39.93ms
step:257/2330 train_time:10257ms step_avg:39.91ms
step:258/2330 train_time:10301ms step_avg:39.93ms
step:259/2330 train_time:10335ms step_avg:39.90ms
step:260/2330 train_time:10379ms step_avg:39.92ms
step:261/2330 train_time:10418ms step_avg:39.91ms
step:262/2330 train_time:10466ms step_avg:39.95ms
step:263/2330 train_time:10502ms step_avg:39.93ms
step:264/2330 train_time:10547ms step_avg:39.95ms
step:265/2330 train_time:10583ms step_avg:39.94ms
step:266/2330 train_time:10627ms step_avg:39.95ms
step:267/2330 train_time:10663ms step_avg:39.94ms
step:268/2330 train_time:10708ms step_avg:39.95ms
step:269/2330 train_time:10742ms step_avg:39.93ms
step:270/2330 train_time:10788ms step_avg:39.95ms
step:271/2330 train_time:10823ms step_avg:39.94ms
step:272/2330 train_time:10867ms step_avg:39.95ms
step:273/2330 train_time:10902ms step_avg:39.93ms
step:274/2330 train_time:10946ms step_avg:39.95ms
step:275/2330 train_time:10981ms step_avg:39.93ms
step:276/2330 train_time:11025ms step_avg:39.95ms
step:277/2330 train_time:11061ms step_avg:39.93ms
step:278/2330 train_time:11104ms step_avg:39.94ms
step:279/2330 train_time:11139ms step_avg:39.93ms
step:280/2330 train_time:11184ms step_avg:39.94ms
step:281/2330 train_time:11218ms step_avg:39.92ms
step:282/2330 train_time:11261ms step_avg:39.93ms
step:283/2330 train_time:11296ms step_avg:39.92ms
step:284/2330 train_time:11341ms step_avg:39.93ms
step:285/2330 train_time:11376ms step_avg:39.91ms
step:286/2330 train_time:11420ms step_avg:39.93ms
step:287/2330 train_time:11456ms step_avg:39.92ms
step:288/2330 train_time:11501ms step_avg:39.93ms
step:289/2330 train_time:11537ms step_avg:39.92ms
step:290/2330 train_time:11581ms step_avg:39.94ms
step:291/2330 train_time:11616ms step_avg:39.92ms
step:292/2330 train_time:11661ms step_avg:39.94ms
step:293/2330 train_time:11697ms step_avg:39.92ms
step:294/2330 train_time:11741ms step_avg:39.93ms
step:295/2330 train_time:11776ms step_avg:39.92ms
step:296/2330 train_time:11820ms step_avg:39.93ms
step:297/2330 train_time:11855ms step_avg:39.92ms
step:298/2330 train_time:11900ms step_avg:39.93ms
step:299/2330 train_time:11935ms step_avg:39.92ms
step:300/2330 train_time:11979ms step_avg:39.93ms
step:301/2330 train_time:12014ms step_avg:39.91ms
step:302/2330 train_time:12058ms step_avg:39.93ms
step:303/2330 train_time:12093ms step_avg:39.91ms
step:304/2330 train_time:12137ms step_avg:39.93ms
step:305/2330 train_time:12172ms step_avg:39.91ms
step:306/2330 train_time:12215ms step_avg:39.92ms
step:307/2330 train_time:12250ms step_avg:39.90ms
step:308/2330 train_time:12294ms step_avg:39.92ms
step:309/2330 train_time:12329ms step_avg:39.90ms
step:310/2330 train_time:12374ms step_avg:39.92ms
step:311/2330 train_time:12409ms step_avg:39.90ms
step:312/2330 train_time:12453ms step_avg:39.91ms
step:313/2330 train_time:12489ms step_avg:39.90ms
step:314/2330 train_time:12535ms step_avg:39.92ms
step:315/2330 train_time:12570ms step_avg:39.90ms
step:316/2330 train_time:12614ms step_avg:39.92ms
step:317/2330 train_time:12650ms step_avg:39.90ms
step:318/2330 train_time:12695ms step_avg:39.92ms
step:319/2330 train_time:12730ms step_avg:39.91ms
step:320/2330 train_time:12774ms step_avg:39.92ms
step:321/2330 train_time:12809ms step_avg:39.90ms
step:322/2330 train_time:12855ms step_avg:39.92ms
step:323/2330 train_time:12890ms step_avg:39.91ms
step:324/2330 train_time:12934ms step_avg:39.92ms
step:325/2330 train_time:12970ms step_avg:39.91ms
step:326/2330 train_time:13014ms step_avg:39.92ms
step:327/2330 train_time:13050ms step_avg:39.91ms
step:328/2330 train_time:13094ms step_avg:39.92ms
step:329/2330 train_time:13129ms step_avg:39.90ms
step:330/2330 train_time:13173ms step_avg:39.92ms
step:331/2330 train_time:13207ms step_avg:39.90ms
step:332/2330 train_time:13251ms step_avg:39.91ms
step:333/2330 train_time:13285ms step_avg:39.89ms
step:334/2330 train_time:13328ms step_avg:39.91ms
step:335/2330 train_time:13364ms step_avg:39.89ms
step:336/2330 train_time:13408ms step_avg:39.91ms
step:337/2330 train_time:13444ms step_avg:39.89ms
step:338/2330 train_time:13489ms step_avg:39.91ms
step:339/2330 train_time:13524ms step_avg:39.90ms
step:340/2330 train_time:13569ms step_avg:39.91ms
step:341/2330 train_time:13605ms step_avg:39.90ms
step:342/2330 train_time:13649ms step_avg:39.91ms
step:343/2330 train_time:13685ms step_avg:39.90ms
step:344/2330 train_time:13730ms step_avg:39.91ms
step:345/2330 train_time:13765ms step_avg:39.90ms
step:346/2330 train_time:13809ms step_avg:39.91ms
step:347/2330 train_time:13845ms step_avg:39.90ms
step:348/2330 train_time:13889ms step_avg:39.91ms
step:349/2330 train_time:13925ms step_avg:39.90ms
step:350/2330 train_time:13969ms step_avg:39.91ms
step:351/2330 train_time:14005ms step_avg:39.90ms
step:352/2330 train_time:14049ms step_avg:39.91ms
step:353/2330 train_time:14084ms step_avg:39.90ms
step:354/2330 train_time:14128ms step_avg:39.91ms
step:355/2330 train_time:14163ms step_avg:39.90ms
step:356/2330 train_time:14207ms step_avg:39.91ms
step:357/2330 train_time:14242ms step_avg:39.89ms
step:358/2330 train_time:14286ms step_avg:39.91ms
step:359/2330 train_time:14321ms step_avg:39.89ms
step:360/2330 train_time:14365ms step_avg:39.90ms
step:361/2330 train_time:14401ms step_avg:39.89ms
step:362/2330 train_time:14445ms step_avg:39.90ms
step:363/2330 train_time:14480ms step_avg:39.89ms
step:364/2330 train_time:14524ms step_avg:39.90ms
step:365/2330 train_time:14559ms step_avg:39.89ms
step:366/2330 train_time:14603ms step_avg:39.90ms
step:367/2330 train_time:14638ms step_avg:39.89ms
step:368/2330 train_time:14683ms step_avg:39.90ms
step:369/2330 train_time:14718ms step_avg:39.89ms
step:370/2330 train_time:14762ms step_avg:39.90ms
step:371/2330 train_time:14797ms step_avg:39.89ms
step:372/2330 train_time:14842ms step_avg:39.90ms
step:373/2330 train_time:14877ms step_avg:39.88ms
step:374/2330 train_time:14921ms step_avg:39.90ms
step:375/2330 train_time:14957ms step_avg:39.89ms
step:376/2330 train_time:15002ms step_avg:39.90ms
step:377/2330 train_time:15036ms step_avg:39.88ms
step:378/2330 train_time:15081ms step_avg:39.90ms
step:379/2330 train_time:15116ms step_avg:39.88ms
step:380/2330 train_time:15161ms step_avg:39.90ms
step:381/2330 train_time:15196ms step_avg:39.88ms
step:382/2330 train_time:15240ms step_avg:39.89ms
step:383/2330 train_time:15275ms step_avg:39.88ms
step:384/2330 train_time:15319ms step_avg:39.89ms
step:385/2330 train_time:15355ms step_avg:39.88ms
step:386/2330 train_time:15399ms step_avg:39.89ms
step:387/2330 train_time:15434ms step_avg:39.88ms
step:388/2330 train_time:15479ms step_avg:39.89ms
step:389/2330 train_time:15513ms step_avg:39.88ms
step:390/2330 train_time:15558ms step_avg:39.89ms
step:391/2330 train_time:15593ms step_avg:39.88ms
step:392/2330 train_time:15637ms step_avg:39.89ms
step:393/2330 train_time:15672ms step_avg:39.88ms
step:394/2330 train_time:15717ms step_avg:39.89ms
step:395/2330 train_time:15752ms step_avg:39.88ms
step:396/2330 train_time:15797ms step_avg:39.89ms
step:397/2330 train_time:15832ms step_avg:39.88ms
step:398/2330 train_time:15877ms step_avg:39.89ms
step:399/2330 train_time:15912ms step_avg:39.88ms
step:400/2330 train_time:15957ms step_avg:39.89ms
step:401/2330 train_time:15992ms step_avg:39.88ms
step:402/2330 train_time:16036ms step_avg:39.89ms
step:403/2330 train_time:16071ms step_avg:39.88ms
step:404/2330 train_time:16116ms step_avg:39.89ms
step:405/2330 train_time:16151ms step_avg:39.88ms
step:406/2330 train_time:16196ms step_avg:39.89ms
step:407/2330 train_time:16231ms step_avg:39.88ms
step:408/2330 train_time:16276ms step_avg:39.89ms
step:409/2330 train_time:16311ms step_avg:39.88ms
step:410/2330 train_time:16356ms step_avg:39.89ms
step:411/2330 train_time:16392ms step_avg:39.88ms
step:412/2330 train_time:16437ms step_avg:39.89ms
step:413/2330 train_time:16472ms step_avg:39.88ms
step:414/2330 train_time:16517ms step_avg:39.90ms
step:415/2330 train_time:16552ms step_avg:39.88ms
step:416/2330 train_time:16597ms step_avg:39.90ms
step:417/2330 train_time:16632ms step_avg:39.88ms
step:418/2330 train_time:16675ms step_avg:39.89ms
step:419/2330 train_time:16711ms step_avg:39.88ms
step:420/2330 train_time:16757ms step_avg:39.90ms
step:421/2330 train_time:16792ms step_avg:39.88ms
step:422/2330 train_time:16836ms step_avg:39.89ms
step:423/2330 train_time:16871ms step_avg:39.88ms
step:424/2330 train_time:16916ms step_avg:39.90ms
step:425/2330 train_time:16951ms step_avg:39.88ms
step:426/2330 train_time:16996ms step_avg:39.90ms
step:427/2330 train_time:17032ms step_avg:39.89ms
step:428/2330 train_time:17077ms step_avg:39.90ms
step:429/2330 train_time:17112ms step_avg:39.89ms
step:430/2330 train_time:17157ms step_avg:39.90ms
step:431/2330 train_time:17192ms step_avg:39.89ms
step:432/2330 train_time:17237ms step_avg:39.90ms
step:433/2330 train_time:17272ms step_avg:39.89ms
step:434/2330 train_time:17317ms step_avg:39.90ms
step:435/2330 train_time:17352ms step_avg:39.89ms
step:436/2330 train_time:17398ms step_avg:39.90ms
step:437/2330 train_time:17432ms step_avg:39.89ms
step:438/2330 train_time:17478ms step_avg:39.90ms
step:439/2330 train_time:17512ms step_avg:39.89ms
step:440/2330 train_time:17557ms step_avg:39.90ms
step:441/2330 train_time:17592ms step_avg:39.89ms
step:442/2330 train_time:17637ms step_avg:39.90ms
step:443/2330 train_time:17672ms step_avg:39.89ms
step:444/2330 train_time:17717ms step_avg:39.90ms
step:445/2330 train_time:17752ms step_avg:39.89ms
step:446/2330 train_time:17796ms step_avg:39.90ms
step:447/2330 train_time:17831ms step_avg:39.89ms
step:448/2330 train_time:17876ms step_avg:39.90ms
step:449/2330 train_time:17911ms step_avg:39.89ms
step:450/2330 train_time:17956ms step_avg:39.90ms
step:451/2330 train_time:17991ms step_avg:39.89ms
step:452/2330 train_time:18035ms step_avg:39.90ms
step:453/2330 train_time:18070ms step_avg:39.89ms
step:454/2330 train_time:18115ms step_avg:39.90ms
step:455/2330 train_time:18150ms step_avg:39.89ms
step:456/2330 train_time:18195ms step_avg:39.90ms
step:457/2330 train_time:18231ms step_avg:39.89ms
step:458/2330 train_time:18276ms step_avg:39.90ms
step:459/2330 train_time:18311ms step_avg:39.89ms
step:460/2330 train_time:18355ms step_avg:39.90ms
step:461/2330 train_time:18390ms step_avg:39.89ms
step:462/2330 train_time:18434ms step_avg:39.90ms
step:463/2330 train_time:18470ms step_avg:39.89ms
step:464/2330 train_time:18514ms step_avg:39.90ms
step:465/2330 train_time:18549ms step_avg:39.89ms
step:466/2330 train_time:18595ms step_avg:39.90ms
step:467/2330 train_time:18630ms step_avg:39.89ms
step:468/2330 train_time:18675ms step_avg:39.90ms
step:469/2330 train_time:18709ms step_avg:39.89ms
step:470/2330 train_time:18753ms step_avg:39.90ms
step:471/2330 train_time:18788ms step_avg:39.89ms
step:472/2330 train_time:18833ms step_avg:39.90ms
step:473/2330 train_time:18867ms step_avg:39.89ms
step:474/2330 train_time:18911ms step_avg:39.90ms
step:475/2330 train_time:18946ms step_avg:39.89ms
step:476/2330 train_time:18991ms step_avg:39.90ms
step:477/2330 train_time:19025ms step_avg:39.89ms
step:478/2330 train_time:19069ms step_avg:39.89ms
step:479/2330 train_time:19105ms step_avg:39.88ms
step:480/2330 train_time:19149ms step_avg:39.89ms
step:481/2330 train_time:19185ms step_avg:39.89ms
step:482/2330 train_time:19230ms step_avg:39.90ms
step:483/2330 train_time:19265ms step_avg:39.89ms
step:484/2330 train_time:19309ms step_avg:39.90ms
step:485/2330 train_time:19345ms step_avg:39.89ms
step:486/2330 train_time:19389ms step_avg:39.90ms
step:487/2330 train_time:19425ms step_avg:39.89ms
step:488/2330 train_time:19469ms step_avg:39.90ms
step:489/2330 train_time:19504ms step_avg:39.88ms
step:490/2330 train_time:19548ms step_avg:39.89ms
step:491/2330 train_time:19585ms step_avg:39.89ms
step:492/2330 train_time:19629ms step_avg:39.90ms
step:493/2330 train_time:19664ms step_avg:39.89ms
step:494/2330 train_time:19708ms step_avg:39.90ms
step:495/2330 train_time:19743ms step_avg:39.89ms
step:496/2330 train_time:19787ms step_avg:39.89ms
step:497/2330 train_time:19822ms step_avg:39.88ms
step:498/2330 train_time:19867ms step_avg:39.89ms
step:499/2330 train_time:19903ms step_avg:39.89ms
step:500/2330 train_time:19946ms step_avg:39.89ms
step:500/2330 val_loss:5.2963 train_time:20035ms step_avg:40.07ms
step:501/2330 train_time:20048ms step_avg:40.02ms
step:502/2330 train_time:20060ms step_avg:39.96ms
step:503/2330 train_time:20070ms step_avg:39.90ms
step:504/2330 train_time:20109ms step_avg:39.90ms
step:505/2330 train_time:20143ms step_avg:39.89ms
step:506/2330 train_time:20186ms step_avg:39.89ms
step:507/2330 train_time:20221ms step_avg:39.88ms
step:508/2330 train_time:20265ms step_avg:39.89ms
step:509/2330 train_time:20300ms step_avg:39.88ms
step:510/2330 train_time:20346ms step_avg:39.89ms
step:511/2330 train_time:20385ms step_avg:39.89ms
step:512/2330 train_time:20434ms step_avg:39.91ms
step:513/2330 train_time:20471ms step_avg:39.90ms
step:514/2330 train_time:20517ms step_avg:39.92ms
step:515/2330 train_time:20551ms step_avg:39.91ms
step:516/2330 train_time:20596ms step_avg:39.92ms
step:517/2330 train_time:20631ms step_avg:39.91ms
step:518/2330 train_time:20675ms step_avg:39.91ms
step:519/2330 train_time:20710ms step_avg:39.90ms
step:520/2330 train_time:20754ms step_avg:39.91ms
step:521/2330 train_time:20788ms step_avg:39.90ms
step:522/2330 train_time:20832ms step_avg:39.91ms
step:523/2330 train_time:20866ms step_avg:39.90ms
step:524/2330 train_time:20909ms step_avg:39.90ms
step:525/2330 train_time:20944ms step_avg:39.89ms
step:526/2330 train_time:20988ms step_avg:39.90ms
step:527/2330 train_time:21023ms step_avg:39.89ms
step:528/2330 train_time:21067ms step_avg:39.90ms
step:529/2330 train_time:21100ms step_avg:39.89ms
step:530/2330 train_time:21144ms step_avg:39.89ms
step:531/2330 train_time:21179ms step_avg:39.89ms
step:532/2330 train_time:21223ms step_avg:39.89ms
step:533/2330 train_time:21258ms step_avg:39.88ms
step:534/2330 train_time:21303ms step_avg:39.89ms
step:535/2330 train_time:21340ms step_avg:39.89ms
step:536/2330 train_time:21385ms step_avg:39.90ms
step:537/2330 train_time:21421ms step_avg:39.89ms
step:538/2330 train_time:21467ms step_avg:39.90ms
step:539/2330 train_time:21504ms step_avg:39.90ms
step:540/2330 train_time:21548ms step_avg:39.90ms
step:541/2330 train_time:21584ms step_avg:39.90ms
step:542/2330 train_time:21629ms step_avg:39.91ms
step:543/2330 train_time:21664ms step_avg:39.90ms
step:544/2330 train_time:21708ms step_avg:39.90ms
step:545/2330 train_time:21742ms step_avg:39.89ms
step:546/2330 train_time:21786ms step_avg:39.90ms
step:547/2330 train_time:21822ms step_avg:39.89ms
step:548/2330 train_time:21865ms step_avg:39.90ms
step:549/2330 train_time:21900ms step_avg:39.89ms
step:550/2330 train_time:21944ms step_avg:39.90ms
step:551/2330 train_time:21979ms step_avg:39.89ms
step:552/2330 train_time:22022ms step_avg:39.90ms
step:553/2330 train_time:22057ms step_avg:39.89ms
step:554/2330 train_time:22100ms step_avg:39.89ms
step:555/2330 train_time:22136ms step_avg:39.88ms
step:556/2330 train_time:22180ms step_avg:39.89ms
step:557/2330 train_time:22214ms step_avg:39.88ms
step:558/2330 train_time:22258ms step_avg:39.89ms
step:559/2330 train_time:22293ms step_avg:39.88ms
step:560/2330 train_time:22338ms step_avg:39.89ms
step:561/2330 train_time:22373ms step_avg:39.88ms
step:562/2330 train_time:22417ms step_avg:39.89ms
step:563/2330 train_time:22453ms step_avg:39.88ms
step:564/2330 train_time:22498ms step_avg:39.89ms
step:565/2330 train_time:22534ms step_avg:39.88ms
step:566/2330 train_time:22579ms step_avg:39.89ms
step:567/2330 train_time:22614ms step_avg:39.88ms
step:568/2330 train_time:22658ms step_avg:39.89ms
step:569/2330 train_time:22693ms step_avg:39.88ms
step:570/2330 train_time:22738ms step_avg:39.89ms
step:571/2330 train_time:22773ms step_avg:39.88ms
step:572/2330 train_time:22817ms step_avg:39.89ms
step:573/2330 train_time:22852ms step_avg:39.88ms
step:574/2330 train_time:22897ms step_avg:39.89ms
step:575/2330 train_time:22931ms step_avg:39.88ms
step:576/2330 train_time:22975ms step_avg:39.89ms
step:577/2330 train_time:23010ms step_avg:39.88ms
step:578/2330 train_time:23054ms step_avg:39.89ms
step:579/2330 train_time:23089ms step_avg:39.88ms
step:580/2330 train_time:23133ms step_avg:39.89ms
step:581/2330 train_time:23168ms step_avg:39.88ms
step:582/2330 train_time:23213ms step_avg:39.88ms
step:583/2330 train_time:23247ms step_avg:39.88ms
step:584/2330 train_time:23292ms step_avg:39.88ms
step:585/2330 train_time:23328ms step_avg:39.88ms
step:586/2330 train_time:23372ms step_avg:39.88ms
step:587/2330 train_time:23408ms step_avg:39.88ms
step:588/2330 train_time:23452ms step_avg:39.88ms
step:589/2330 train_time:23487ms step_avg:39.88ms
step:590/2330 train_time:23533ms step_avg:39.89ms
step:591/2330 train_time:23568ms step_avg:39.88ms
step:592/2330 train_time:23613ms step_avg:39.89ms
step:593/2330 train_time:23648ms step_avg:39.88ms
step:594/2330 train_time:23692ms step_avg:39.89ms
step:595/2330 train_time:23728ms step_avg:39.88ms
step:596/2330 train_time:23773ms step_avg:39.89ms
step:597/2330 train_time:23807ms step_avg:39.88ms
step:598/2330 train_time:23851ms step_avg:39.88ms
step:599/2330 train_time:23886ms step_avg:39.88ms
step:600/2330 train_time:23931ms step_avg:39.88ms
step:601/2330 train_time:23967ms step_avg:39.88ms
step:602/2330 train_time:24010ms step_avg:39.88ms
step:603/2330 train_time:24045ms step_avg:39.88ms
step:604/2330 train_time:24090ms step_avg:39.88ms
step:605/2330 train_time:24126ms step_avg:39.88ms
step:606/2330 train_time:24170ms step_avg:39.88ms
step:607/2330 train_time:24205ms step_avg:39.88ms
step:608/2330 train_time:24249ms step_avg:39.88ms
step:609/2330 train_time:24284ms step_avg:39.88ms
step:610/2330 train_time:24329ms step_avg:39.88ms
step:611/2330 train_time:24365ms step_avg:39.88ms
step:612/2330 train_time:24410ms step_avg:39.89ms
step:613/2330 train_time:24445ms step_avg:39.88ms
step:614/2330 train_time:24489ms step_avg:39.88ms
step:615/2330 train_time:24525ms step_avg:39.88ms
step:616/2330 train_time:24570ms step_avg:39.89ms
step:617/2330 train_time:24605ms step_avg:39.88ms
step:618/2330 train_time:24649ms step_avg:39.88ms
step:619/2330 train_time:24684ms step_avg:39.88ms
step:620/2330 train_time:24730ms step_avg:39.89ms
step:621/2330 train_time:24764ms step_avg:39.88ms
step:622/2330 train_time:24809ms step_avg:39.89ms
step:623/2330 train_time:24843ms step_avg:39.88ms
step:624/2330 train_time:24888ms step_avg:39.88ms
step:625/2330 train_time:24923ms step_avg:39.88ms
step:626/2330 train_time:24967ms step_avg:39.88ms
step:627/2330 train_time:25001ms step_avg:39.87ms
step:628/2330 train_time:25045ms step_avg:39.88ms
step:629/2330 train_time:25080ms step_avg:39.87ms
step:630/2330 train_time:25124ms step_avg:39.88ms
step:631/2330 train_time:25159ms step_avg:39.87ms
step:632/2330 train_time:25203ms step_avg:39.88ms
step:633/2330 train_time:25238ms step_avg:39.87ms
step:634/2330 train_time:25282ms step_avg:39.88ms
step:635/2330 train_time:25317ms step_avg:39.87ms
step:636/2330 train_time:25362ms step_avg:39.88ms
step:637/2330 train_time:25397ms step_avg:39.87ms
step:638/2330 train_time:25442ms step_avg:39.88ms
step:639/2330 train_time:25477ms step_avg:39.87ms
step:640/2330 train_time:25521ms step_avg:39.88ms
step:641/2330 train_time:25557ms step_avg:39.87ms
step:642/2330 train_time:25602ms step_avg:39.88ms
step:643/2330 train_time:25637ms step_avg:39.87ms
step:644/2330 train_time:25682ms step_avg:39.88ms
step:645/2330 train_time:25716ms step_avg:39.87ms
step:646/2330 train_time:25760ms step_avg:39.88ms
step:647/2330 train_time:25795ms step_avg:39.87ms
step:648/2330 train_time:25840ms step_avg:39.88ms
step:649/2330 train_time:25875ms step_avg:39.87ms
step:650/2330 train_time:25920ms step_avg:39.88ms
step:651/2330 train_time:25955ms step_avg:39.87ms
step:652/2330 train_time:25999ms step_avg:39.88ms
step:653/2330 train_time:26033ms step_avg:39.87ms
step:654/2330 train_time:26077ms step_avg:39.87ms
step:655/2330 train_time:26112ms step_avg:39.87ms
step:656/2330 train_time:26157ms step_avg:39.87ms
step:657/2330 train_time:26191ms step_avg:39.87ms
step:658/2330 train_time:26235ms step_avg:39.87ms
step:659/2330 train_time:26271ms step_avg:39.86ms
step:660/2330 train_time:26315ms step_avg:39.87ms
step:661/2330 train_time:26351ms step_avg:39.86ms
step:662/2330 train_time:26395ms step_avg:39.87ms
step:663/2330 train_time:26430ms step_avg:39.86ms
step:664/2330 train_time:26475ms step_avg:39.87ms
step:665/2330 train_time:26511ms step_avg:39.87ms
step:666/2330 train_time:26555ms step_avg:39.87ms
step:667/2330 train_time:26591ms step_avg:39.87ms
step:668/2330 train_time:26635ms step_avg:39.87ms
step:669/2330 train_time:26670ms step_avg:39.87ms
step:670/2330 train_time:26714ms step_avg:39.87ms
step:671/2330 train_time:26749ms step_avg:39.86ms
step:672/2330 train_time:26793ms step_avg:39.87ms
step:673/2330 train_time:26828ms step_avg:39.86ms
step:674/2330 train_time:26873ms step_avg:39.87ms
step:675/2330 train_time:26908ms step_avg:39.86ms
step:676/2330 train_time:26953ms step_avg:39.87ms
step:677/2330 train_time:26987ms step_avg:39.86ms
step:678/2330 train_time:27031ms step_avg:39.87ms
step:679/2330 train_time:27066ms step_avg:39.86ms
step:680/2330 train_time:27110ms step_avg:39.87ms
step:681/2330 train_time:27146ms step_avg:39.86ms
step:682/2330 train_time:27190ms step_avg:39.87ms
step:683/2330 train_time:27225ms step_avg:39.86ms
step:684/2330 train_time:27270ms step_avg:39.87ms
step:685/2330 train_time:27304ms step_avg:39.86ms
step:686/2330 train_time:27348ms step_avg:39.87ms
step:687/2330 train_time:27383ms step_avg:39.86ms
step:688/2330 train_time:27427ms step_avg:39.87ms
step:689/2330 train_time:27463ms step_avg:39.86ms
step:690/2330 train_time:27507ms step_avg:39.87ms
step:691/2330 train_time:27542ms step_avg:39.86ms
step:692/2330 train_time:27586ms step_avg:39.86ms
step:693/2330 train_time:27622ms step_avg:39.86ms
step:694/2330 train_time:27667ms step_avg:39.87ms
step:695/2330 train_time:27702ms step_avg:39.86ms
step:696/2330 train_time:27746ms step_avg:39.86ms
step:697/2330 train_time:27781ms step_avg:39.86ms
step:698/2330 train_time:27826ms step_avg:39.86ms
step:699/2330 train_time:27861ms step_avg:39.86ms
step:700/2330 train_time:27905ms step_avg:39.86ms
step:701/2330 train_time:27940ms step_avg:39.86ms
step:702/2330 train_time:27985ms step_avg:39.86ms
step:703/2330 train_time:28021ms step_avg:39.86ms
step:704/2330 train_time:28065ms step_avg:39.87ms
step:705/2330 train_time:28101ms step_avg:39.86ms
step:706/2330 train_time:28145ms step_avg:39.87ms
step:707/2330 train_time:28181ms step_avg:39.86ms
step:708/2330 train_time:28226ms step_avg:39.87ms
step:709/2330 train_time:28262ms step_avg:39.86ms
step:710/2330 train_time:28306ms step_avg:39.87ms
step:711/2330 train_time:28341ms step_avg:39.86ms
step:712/2330 train_time:28386ms step_avg:39.87ms
step:713/2330 train_time:28421ms step_avg:39.86ms
step:714/2330 train_time:28465ms step_avg:39.87ms
step:715/2330 train_time:28501ms step_avg:39.86ms
step:716/2330 train_time:28545ms step_avg:39.87ms
step:717/2330 train_time:28581ms step_avg:39.86ms
step:718/2330 train_time:28626ms step_avg:39.87ms
step:719/2330 train_time:28661ms step_avg:39.86ms
step:720/2330 train_time:28705ms step_avg:39.87ms
step:721/2330 train_time:28741ms step_avg:39.86ms
step:722/2330 train_time:28785ms step_avg:39.87ms
step:723/2330 train_time:28820ms step_avg:39.86ms
step:724/2330 train_time:28865ms step_avg:39.87ms
step:725/2330 train_time:28901ms step_avg:39.86ms
step:726/2330 train_time:28945ms step_avg:39.87ms
step:727/2330 train_time:28980ms step_avg:39.86ms
step:728/2330 train_time:29025ms step_avg:39.87ms
step:729/2330 train_time:29060ms step_avg:39.86ms
step:730/2330 train_time:29105ms step_avg:39.87ms
step:731/2330 train_time:29141ms step_avg:39.86ms
step:732/2330 train_time:29185ms step_avg:39.87ms
step:733/2330 train_time:29221ms step_avg:39.86ms
step:734/2330 train_time:29265ms step_avg:39.87ms
step:735/2330 train_time:29300ms step_avg:39.86ms
step:736/2330 train_time:29344ms step_avg:39.87ms
step:737/2330 train_time:29380ms step_avg:39.86ms
step:738/2330 train_time:29424ms step_avg:39.87ms
step:739/2330 train_time:29459ms step_avg:39.86ms
step:740/2330 train_time:29503ms step_avg:39.87ms
step:741/2330 train_time:29538ms step_avg:39.86ms
step:742/2330 train_time:29583ms step_avg:39.87ms
step:743/2330 train_time:29618ms step_avg:39.86ms
step:744/2330 train_time:29662ms step_avg:39.87ms
step:745/2330 train_time:29697ms step_avg:39.86ms
step:746/2330 train_time:29741ms step_avg:39.87ms
step:747/2330 train_time:29776ms step_avg:39.86ms
step:748/2330 train_time:29821ms step_avg:39.87ms
step:749/2330 train_time:29855ms step_avg:39.86ms
step:750/2330 train_time:29900ms step_avg:39.87ms
step:750/2330 val_loss:5.2333 train_time:29986ms step_avg:39.98ms
step:751/2330 train_time:29999ms step_avg:39.95ms
step:752/2330 train_time:30011ms step_avg:39.91ms
step:753/2330 train_time:30021ms step_avg:39.87ms
step:754/2330 train_time:30059ms step_avg:39.87ms
step:755/2330 train_time:30093ms step_avg:39.86ms
step:756/2330 train_time:30137ms step_avg:39.86ms
step:757/2330 train_time:30172ms step_avg:39.86ms
step:758/2330 train_time:30216ms step_avg:39.86ms
step:759/2330 train_time:30251ms step_avg:39.86ms
step:760/2330 train_time:30296ms step_avg:39.86ms
step:761/2330 train_time:30338ms step_avg:39.87ms
step:762/2330 train_time:30385ms step_avg:39.87ms
step:763/2330 train_time:30420ms step_avg:39.87ms
step:764/2330 train_time:30465ms step_avg:39.88ms
step:765/2330 train_time:30501ms step_avg:39.87ms
step:766/2330 train_time:30545ms step_avg:39.88ms
step:767/2330 train_time:30579ms step_avg:39.87ms
step:768/2330 train_time:30623ms step_avg:39.87ms
step:769/2330 train_time:30658ms step_avg:39.87ms
step:770/2330 train_time:30702ms step_avg:39.87ms
step:771/2330 train_time:30737ms step_avg:39.87ms
step:772/2330 train_time:30781ms step_avg:39.87ms
step:773/2330 train_time:30815ms step_avg:39.86ms
step:774/2330 train_time:30859ms step_avg:39.87ms
step:775/2330 train_time:30893ms step_avg:39.86ms
step:776/2330 train_time:30938ms step_avg:39.87ms
step:777/2330 train_time:30973ms step_avg:39.86ms
step:778/2330 train_time:31017ms step_avg:39.87ms
step:779/2330 train_time:31052ms step_avg:39.86ms
step:780/2330 train_time:31096ms step_avg:39.87ms
step:781/2330 train_time:31131ms step_avg:39.86ms
step:782/2330 train_time:31174ms step_avg:39.86ms
step:783/2330 train_time:31209ms step_avg:39.86ms
step:784/2330 train_time:31253ms step_avg:39.86ms
step:785/2330 train_time:31289ms step_avg:39.86ms
step:786/2330 train_time:31335ms step_avg:39.87ms
step:787/2330 train_time:31371ms step_avg:39.86ms
step:788/2330 train_time:31417ms step_avg:39.87ms
step:789/2330 train_time:31452ms step_avg:39.86ms
step:790/2330 train_time:31497ms step_avg:39.87ms
step:791/2330 train_time:31532ms step_avg:39.86ms
step:792/2330 train_time:31577ms step_avg:39.87ms
step:793/2330 train_time:31611ms step_avg:39.86ms
step:794/2330 train_time:31655ms step_avg:39.87ms
step:795/2330 train_time:31690ms step_avg:39.86ms
step:796/2330 train_time:31734ms step_avg:39.87ms
step:797/2330 train_time:31768ms step_avg:39.86ms
step:798/2330 train_time:31812ms step_avg:39.86ms
step:799/2330 train_time:31847ms step_avg:39.86ms
step:800/2330 train_time:31891ms step_avg:39.86ms
step:801/2330 train_time:31926ms step_avg:39.86ms
step:802/2330 train_time:31971ms step_avg:39.86ms
step:803/2330 train_time:32005ms step_avg:39.86ms
step:804/2330 train_time:32049ms step_avg:39.86ms
step:805/2330 train_time:32083ms step_avg:39.86ms
step:806/2330 train_time:32127ms step_avg:39.86ms
step:807/2330 train_time:32162ms step_avg:39.85ms
step:808/2330 train_time:32206ms step_avg:39.86ms
step:809/2330 train_time:32241ms step_avg:39.85ms
step:810/2330 train_time:32286ms step_avg:39.86ms
step:811/2330 train_time:32320ms step_avg:39.85ms
step:812/2330 train_time:32365ms step_avg:39.86ms
step:813/2330 train_time:32401ms step_avg:39.85ms
step:814/2330 train_time:32445ms step_avg:39.86ms
step:815/2330 train_time:32480ms step_avg:39.85ms
step:816/2330 train_time:32524ms step_avg:39.86ms
step:817/2330 train_time:32560ms step_avg:39.85ms
step:818/2330 train_time:32604ms step_avg:39.86ms
step:819/2330 train_time:32640ms step_avg:39.85ms
step:820/2330 train_time:32685ms step_avg:39.86ms
step:821/2330 train_time:32720ms step_avg:39.85ms
step:822/2330 train_time:32764ms step_avg:39.86ms
step:823/2330 train_time:32799ms step_avg:39.85ms
step:824/2330 train_time:32843ms step_avg:39.86ms
step:825/2330 train_time:32878ms step_avg:39.85ms
step:826/2330 train_time:32923ms step_avg:39.86ms
step:827/2330 train_time:32957ms step_avg:39.85ms
step:828/2330 train_time:33002ms step_avg:39.86ms
step:829/2330 train_time:33037ms step_avg:39.85ms
step:830/2330 train_time:33081ms step_avg:39.86ms
step:831/2330 train_time:33116ms step_avg:39.85ms
step:832/2330 train_time:33160ms step_avg:39.86ms
step:833/2330 train_time:33195ms step_avg:39.85ms
step:834/2330 train_time:33240ms step_avg:39.86ms
step:835/2330 train_time:33275ms step_avg:39.85ms
step:836/2330 train_time:33320ms step_avg:39.86ms
step:837/2330 train_time:33355ms step_avg:39.85ms
step:838/2330 train_time:33400ms step_avg:39.86ms
step:839/2330 train_time:33436ms step_avg:39.85ms
step:840/2330 train_time:33480ms step_avg:39.86ms
step:841/2330 train_time:33516ms step_avg:39.85ms
step:842/2330 train_time:33561ms step_avg:39.86ms
step:843/2330 train_time:33596ms step_avg:39.85ms
step:844/2330 train_time:33641ms step_avg:39.86ms
step:845/2330 train_time:33676ms step_avg:39.85ms
step:846/2330 train_time:33720ms step_avg:39.86ms
step:847/2330 train_time:33755ms step_avg:39.85ms
step:848/2330 train_time:33800ms step_avg:39.86ms
step:849/2330 train_time:33835ms step_avg:39.85ms
step:850/2330 train_time:33880ms step_avg:39.86ms
step:851/2330 train_time:33914ms step_avg:39.85ms
step:852/2330 train_time:33959ms step_avg:39.86ms
step:853/2330 train_time:33994ms step_avg:39.85ms
step:854/2330 train_time:34038ms step_avg:39.86ms
step:855/2330 train_time:34073ms step_avg:39.85ms
step:856/2330 train_time:34117ms step_avg:39.86ms
step:857/2330 train_time:34152ms step_avg:39.85ms
step:858/2330 train_time:34197ms step_avg:39.86ms
step:859/2330 train_time:34232ms step_avg:39.85ms
step:860/2330 train_time:34276ms step_avg:39.86ms
step:861/2330 train_time:34311ms step_avg:39.85ms
step:862/2330 train_time:34355ms step_avg:39.85ms
step:863/2330 train_time:34390ms step_avg:39.85ms
step:864/2330 train_time:34434ms step_avg:39.85ms
step:865/2330 train_time:34469ms step_avg:39.85ms
step:866/2330 train_time:34513ms step_avg:39.85ms
step:867/2330 train_time:34548ms step_avg:39.85ms
step:868/2330 train_time:34592ms step_avg:39.85ms
step:869/2330 train_time:34628ms step_avg:39.85ms
step:870/2330 train_time:34672ms step_avg:39.85ms
step:871/2330 train_time:34707ms step_avg:39.85ms
step:872/2330 train_time:34751ms step_avg:39.85ms
step:873/2330 train_time:34787ms step_avg:39.85ms
step:874/2330 train_time:34831ms step_avg:39.85ms
step:875/2330 train_time:34867ms step_avg:39.85ms
step:876/2330 train_time:34911ms step_avg:39.85ms
step:877/2330 train_time:34946ms step_avg:39.85ms
step:878/2330 train_time:34990ms step_avg:39.85ms
step:879/2330 train_time:35025ms step_avg:39.85ms
step:880/2330 train_time:35069ms step_avg:39.85ms
step:881/2330 train_time:35104ms step_avg:39.85ms
step:882/2330 train_time:35148ms step_avg:39.85ms
step:883/2330 train_time:35183ms step_avg:39.84ms
step:884/2330 train_time:35227ms step_avg:39.85ms
step:885/2330 train_time:35262ms step_avg:39.84ms
step:886/2330 train_time:35306ms step_avg:39.85ms
step:887/2330 train_time:35341ms step_avg:39.84ms
step:888/2330 train_time:35385ms step_avg:39.85ms
step:889/2330 train_time:35420ms step_avg:39.84ms
step:890/2330 train_time:35464ms step_avg:39.85ms
step:891/2330 train_time:35499ms step_avg:39.84ms
step:892/2330 train_time:35544ms step_avg:39.85ms
step:893/2330 train_time:35579ms step_avg:39.84ms
step:894/2330 train_time:35624ms step_avg:39.85ms
step:895/2330 train_time:35659ms step_avg:39.84ms
step:896/2330 train_time:35703ms step_avg:39.85ms
step:897/2330 train_time:35738ms step_avg:39.84ms
step:898/2330 train_time:35783ms step_avg:39.85ms
step:899/2330 train_time:35818ms step_avg:39.84ms
step:900/2330 train_time:35862ms step_avg:39.85ms
step:901/2330 train_time:35897ms step_avg:39.84ms
step:902/2330 train_time:35941ms step_avg:39.85ms
step:903/2330 train_time:35976ms step_avg:39.84ms
step:904/2330 train_time:36020ms step_avg:39.85ms
step:905/2330 train_time:36055ms step_avg:39.84ms
step:906/2330 train_time:36100ms step_avg:39.85ms
step:907/2330 train_time:36135ms step_avg:39.84ms
step:908/2330 train_time:36180ms step_avg:39.85ms
step:909/2330 train_time:36214ms step_avg:39.84ms
step:910/2330 train_time:36258ms step_avg:39.84ms
step:911/2330 train_time:36294ms step_avg:39.84ms
step:912/2330 train_time:36339ms step_avg:39.85ms
step:913/2330 train_time:36374ms step_avg:39.84ms
step:914/2330 train_time:36419ms step_avg:39.85ms
step:915/2330 train_time:36455ms step_avg:39.84ms
step:916/2330 train_time:36500ms step_avg:39.85ms
step:917/2330 train_time:36535ms step_avg:39.84ms
step:918/2330 train_time:36580ms step_avg:39.85ms
step:919/2330 train_time:36615ms step_avg:39.84ms
step:920/2330 train_time:36660ms step_avg:39.85ms
step:921/2330 train_time:36696ms step_avg:39.84ms
step:922/2330 train_time:36741ms step_avg:39.85ms
step:923/2330 train_time:36776ms step_avg:39.84ms
step:924/2330 train_time:36820ms step_avg:39.85ms
step:925/2330 train_time:36855ms step_avg:39.84ms
step:926/2330 train_time:36901ms step_avg:39.85ms
step:927/2330 train_time:36935ms step_avg:39.84ms
step:928/2330 train_time:36980ms step_avg:39.85ms
step:929/2330 train_time:37016ms step_avg:39.84ms
step:930/2330 train_time:37060ms step_avg:39.85ms
step:931/2330 train_time:37095ms step_avg:39.84ms
step:932/2330 train_time:37140ms step_avg:39.85ms
step:933/2330 train_time:37175ms step_avg:39.84ms
step:934/2330 train_time:37219ms step_avg:39.85ms
step:935/2330 train_time:37255ms step_avg:39.84ms
step:936/2330 train_time:37299ms step_avg:39.85ms
step:937/2330 train_time:37335ms step_avg:39.85ms
step:938/2330 train_time:37379ms step_avg:39.85ms
step:939/2330 train_time:37414ms step_avg:39.84ms
step:940/2330 train_time:37459ms step_avg:39.85ms
step:941/2330 train_time:37493ms step_avg:39.84ms
step:942/2330 train_time:37538ms step_avg:39.85ms
step:943/2330 train_time:37573ms step_avg:39.84ms
step:944/2330 train_time:37618ms step_avg:39.85ms
step:945/2330 train_time:37654ms step_avg:39.85ms
step:946/2330 train_time:37698ms step_avg:39.85ms
step:947/2330 train_time:37735ms step_avg:39.85ms
step:948/2330 train_time:37778ms step_avg:39.85ms
step:949/2330 train_time:37814ms step_avg:39.85ms
step:950/2330 train_time:37858ms step_avg:39.85ms
step:951/2330 train_time:37895ms step_avg:39.85ms
step:952/2330 train_time:37939ms step_avg:39.85ms
step:953/2330 train_time:37974ms step_avg:39.85ms
step:954/2330 train_time:38018ms step_avg:39.85ms
step:955/2330 train_time:38053ms step_avg:39.85ms
step:956/2330 train_time:38097ms step_avg:39.85ms
step:957/2330 train_time:38131ms step_avg:39.84ms
step:958/2330 train_time:38177ms step_avg:39.85ms
step:959/2330 train_time:38211ms step_avg:39.84ms
step:960/2330 train_time:38255ms step_avg:39.85ms
step:961/2330 train_time:38290ms step_avg:39.84ms
step:962/2330 train_time:38333ms step_avg:39.85ms
step:963/2330 train_time:38369ms step_avg:39.84ms
step:964/2330 train_time:38413ms step_avg:39.85ms
step:965/2330 train_time:38448ms step_avg:39.84ms
step:966/2330 train_time:38492ms step_avg:39.85ms
step:967/2330 train_time:38527ms step_avg:39.84ms
step:968/2330 train_time:38572ms step_avg:39.85ms
step:969/2330 train_time:38607ms step_avg:39.84ms
step:970/2330 train_time:38651ms step_avg:39.85ms
step:971/2330 train_time:38686ms step_avg:39.84ms
step:972/2330 train_time:38730ms step_avg:39.85ms
step:973/2330 train_time:38766ms step_avg:39.84ms
step:974/2330 train_time:38811ms step_avg:39.85ms
step:975/2330 train_time:38846ms step_avg:39.84ms
step:976/2330 train_time:38890ms step_avg:39.85ms
step:977/2330 train_time:38925ms step_avg:39.84ms
step:978/2330 train_time:38969ms step_avg:39.85ms
step:979/2330 train_time:39004ms step_avg:39.84ms
step:980/2330 train_time:39047ms step_avg:39.84ms
step:981/2330 train_time:39082ms step_avg:39.84ms
step:982/2330 train_time:39126ms step_avg:39.84ms
step:983/2330 train_time:39161ms step_avg:39.84ms
step:984/2330 train_time:39206ms step_avg:39.84ms
step:985/2330 train_time:39240ms step_avg:39.84ms
step:986/2330 train_time:39285ms step_avg:39.84ms
step:987/2330 train_time:39319ms step_avg:39.84ms
step:988/2330 train_time:39364ms step_avg:39.84ms
step:989/2330 train_time:39399ms step_avg:39.84ms
step:990/2330 train_time:39444ms step_avg:39.84ms
step:991/2330 train_time:39479ms step_avg:39.84ms
step:992/2330 train_time:39523ms step_avg:39.84ms
step:993/2330 train_time:39559ms step_avg:39.84ms
step:994/2330 train_time:39604ms step_avg:39.84ms
step:995/2330 train_time:39638ms step_avg:39.84ms
step:996/2330 train_time:39682ms step_avg:39.84ms
step:997/2330 train_time:39717ms step_avg:39.84ms
step:998/2330 train_time:39762ms step_avg:39.84ms
step:999/2330 train_time:39797ms step_avg:39.84ms
step:1000/2330 train_time:39841ms step_avg:39.84ms
step:1000/2330 val_loss:5.1951 train_time:39928ms step_avg:39.93ms
step:1001/2330 train_time:39941ms step_avg:39.90ms
step:1002/2330 train_time:39953ms step_avg:39.87ms
step:1003/2330 train_time:39964ms step_avg:39.84ms
step:1004/2330 train_time:40002ms step_avg:39.84ms
step:1005/2330 train_time:40036ms step_avg:39.84ms
step:1006/2330 train_time:40079ms step_avg:39.84ms
step:1007/2330 train_time:40113ms step_avg:39.83ms
step:1008/2330 train_time:40157ms step_avg:39.84ms
step:1009/2330 train_time:40191ms step_avg:39.83ms
step:1010/2330 train_time:40235ms step_avg:39.84ms
step:1011/2330 train_time:40272ms step_avg:39.83ms
step:1012/2330 train_time:40320ms step_avg:39.84ms
step:1013/2330 train_time:40356ms step_avg:39.84ms
step:1014/2330 train_time:40401ms step_avg:39.84ms
step:1015/2330 train_time:40437ms step_avg:39.84ms
step:1016/2330 train_time:40481ms step_avg:39.84ms
step:1017/2330 train_time:40515ms step_avg:39.84ms
step:1018/2330 train_time:40559ms step_avg:39.84ms
step:1019/2330 train_time:40594ms step_avg:39.84ms
step:1020/2330 train_time:40639ms step_avg:39.84ms
step:1021/2330 train_time:40674ms step_avg:39.84ms
step:1022/2330 train_time:40717ms step_avg:39.84ms
step:1023/2330 train_time:40752ms step_avg:39.84ms
step:1024/2330 train_time:40796ms step_avg:39.84ms
step:1025/2330 train_time:40831ms step_avg:39.83ms
step:1026/2330 train_time:40876ms step_avg:39.84ms
step:1027/2330 train_time:40911ms step_avg:39.84ms
step:1028/2330 train_time:40955ms step_avg:39.84ms
step:1029/2330 train_time:40990ms step_avg:39.84ms
step:1030/2330 train_time:41034ms step_avg:39.84ms
step:1031/2330 train_time:41069ms step_avg:39.83ms
step:1032/2330 train_time:41112ms step_avg:39.84ms
step:1033/2330 train_time:41147ms step_avg:39.83ms
step:1034/2330 train_time:41192ms step_avg:39.84ms
step:1035/2330 train_time:41227ms step_avg:39.83ms
step:1036/2330 train_time:41272ms step_avg:39.84ms
step:1037/2330 train_time:41308ms step_avg:39.83ms
step:1038/2330 train_time:41353ms step_avg:39.84ms
step:1039/2330 train_time:41389ms step_avg:39.84ms
step:1040/2330 train_time:41434ms step_avg:39.84ms
step:1041/2330 train_time:41470ms step_avg:39.84ms
step:1042/2330 train_time:41514ms step_avg:39.84ms
step:1043/2330 train_time:41550ms step_avg:39.84ms
step:1044/2330 train_time:41594ms step_avg:39.84ms
step:1045/2330 train_time:41629ms step_avg:39.84ms
step:1046/2330 train_time:41672ms step_avg:39.84ms
step:1047/2330 train_time:41707ms step_avg:39.83ms
step:1048/2330 train_time:41751ms step_avg:39.84ms
step:1049/2330 train_time:41786ms step_avg:39.83ms
step:1050/2330 train_time:41830ms step_avg:39.84ms
step:1051/2330 train_time:41865ms step_avg:39.83ms
step:1052/2330 train_time:41909ms step_avg:39.84ms
step:1053/2330 train_time:41945ms step_avg:39.83ms
step:1054/2330 train_time:41989ms step_avg:39.84ms
step:1055/2330 train_time:42023ms step_avg:39.83ms
step:1056/2330 train_time:42067ms step_avg:39.84ms
step:1057/2330 train_time:42102ms step_avg:39.83ms
step:1058/2330 train_time:42146ms step_avg:39.84ms
step:1059/2330 train_time:42181ms step_avg:39.83ms
step:1060/2330 train_time:42226ms step_avg:39.84ms
step:1061/2330 train_time:42262ms step_avg:39.83ms
step:1062/2330 train_time:42307ms step_avg:39.84ms
step:1063/2330 train_time:42342ms step_avg:39.83ms
step:1064/2330 train_time:42387ms step_avg:39.84ms
step:1065/2330 train_time:42423ms step_avg:39.83ms
step:1066/2330 train_time:42467ms step_avg:39.84ms
step:1067/2330 train_time:42503ms step_avg:39.83ms
step:1068/2330 train_time:42548ms step_avg:39.84ms
step:1069/2330 train_time:42584ms step_avg:39.84ms
step:1070/2330 train_time:42629ms step_avg:39.84ms
step:1071/2330 train_time:42664ms step_avg:39.84ms
step:1072/2330 train_time:42708ms step_avg:39.84ms
step:1073/2330 train_time:42743ms step_avg:39.83ms
step:1074/2330 train_time:42787ms step_avg:39.84ms
step:1075/2330 train_time:42821ms step_avg:39.83ms
step:1076/2330 train_time:42866ms step_avg:39.84ms
step:1077/2330 train_time:42901ms step_avg:39.83ms
step:1078/2330 train_time:42945ms step_avg:39.84ms
step:1079/2330 train_time:42979ms step_avg:39.83ms
step:1080/2330 train_time:43023ms step_avg:39.84ms
step:1081/2330 train_time:43058ms step_avg:39.83ms
step:1082/2330 train_time:43102ms step_avg:39.84ms
step:1083/2330 train_time:43136ms step_avg:39.83ms
step:1084/2330 train_time:43180ms step_avg:39.83ms
step:1085/2330 train_time:43215ms step_avg:39.83ms
step:1086/2330 train_time:43260ms step_avg:39.83ms
step:1087/2330 train_time:43295ms step_avg:39.83ms
step:1088/2330 train_time:43339ms step_avg:39.83ms
step:1089/2330 train_time:43374ms step_avg:39.83ms
step:1090/2330 train_time:43418ms step_avg:39.83ms
step:1091/2330 train_time:43454ms step_avg:39.83ms
step:1092/2330 train_time:43498ms step_avg:39.83ms
step:1093/2330 train_time:43534ms step_avg:39.83ms
step:1094/2330 train_time:43579ms step_avg:39.83ms
step:1095/2330 train_time:43614ms step_avg:39.83ms
step:1096/2330 train_time:43658ms step_avg:39.83ms
step:1097/2330 train_time:43694ms step_avg:39.83ms
step:1098/2330 train_time:43739ms step_avg:39.83ms
step:1099/2330 train_time:43774ms step_avg:39.83ms
step:1100/2330 train_time:43818ms step_avg:39.83ms
step:1101/2330 train_time:43854ms step_avg:39.83ms
step:1102/2330 train_time:43897ms step_avg:39.83ms
step:1103/2330 train_time:43933ms step_avg:39.83ms
step:1104/2330 train_time:43977ms step_avg:39.83ms
step:1105/2330 train_time:44011ms step_avg:39.83ms
step:1106/2330 train_time:44056ms step_avg:39.83ms
step:1107/2330 train_time:44091ms step_avg:39.83ms
step:1108/2330 train_time:44135ms step_avg:39.83ms
step:1109/2330 train_time:44170ms step_avg:39.83ms
step:1110/2330 train_time:44215ms step_avg:39.83ms
step:1111/2330 train_time:44249ms step_avg:39.83ms
step:1112/2330 train_time:44293ms step_avg:39.83ms
step:1113/2330 train_time:44328ms step_avg:39.83ms
step:1114/2330 train_time:44373ms step_avg:39.83ms
step:1115/2330 train_time:44409ms step_avg:39.83ms
step:1116/2330 train_time:44454ms step_avg:39.83ms
step:1117/2330 train_time:44490ms step_avg:39.83ms
step:1118/2330 train_time:44534ms step_avg:39.83ms
step:1119/2330 train_time:44569ms step_avg:39.83ms
step:1120/2330 train_time:44613ms step_avg:39.83ms
step:1121/2330 train_time:44649ms step_avg:39.83ms
step:1122/2330 train_time:44693ms step_avg:39.83ms
step:1123/2330 train_time:44728ms step_avg:39.83ms
step:1124/2330 train_time:44772ms step_avg:39.83ms
step:1125/2330 train_time:44808ms step_avg:39.83ms
step:1126/2330 train_time:44852ms step_avg:39.83ms
step:1127/2330 train_time:44887ms step_avg:39.83ms
step:1128/2330 train_time:44930ms step_avg:39.83ms
step:1129/2330 train_time:44965ms step_avg:39.83ms
step:1130/2330 train_time:45009ms step_avg:39.83ms
step:1131/2330 train_time:45044ms step_avg:39.83ms
step:1132/2330 train_time:45089ms step_avg:39.83ms
step:1133/2330 train_time:45124ms step_avg:39.83ms
step:1134/2330 train_time:45169ms step_avg:39.83ms
step:1135/2330 train_time:45203ms step_avg:39.83ms
step:1136/2330 train_time:45248ms step_avg:39.83ms
step:1137/2330 train_time:45282ms step_avg:39.83ms
step:1138/2330 train_time:45326ms step_avg:39.83ms
step:1139/2330 train_time:45362ms step_avg:39.83ms
step:1140/2330 train_time:45406ms step_avg:39.83ms
step:1141/2330 train_time:45442ms step_avg:39.83ms
step:1142/2330 train_time:45487ms step_avg:39.83ms
step:1143/2330 train_time:45522ms step_avg:39.83ms
step:1144/2330 train_time:45566ms step_avg:39.83ms
step:1145/2330 train_time:45602ms step_avg:39.83ms
step:1146/2330 train_time:45646ms step_avg:39.83ms
step:1147/2330 train_time:45682ms step_avg:39.83ms
step:1148/2330 train_time:45727ms step_avg:39.83ms
step:1149/2330 train_time:45762ms step_avg:39.83ms
step:1150/2330 train_time:45807ms step_avg:39.83ms
step:1151/2330 train_time:45842ms step_avg:39.83ms
step:1152/2330 train_time:45887ms step_avg:39.83ms
step:1153/2330 train_time:45922ms step_avg:39.83ms
step:1154/2330 train_time:45966ms step_avg:39.83ms
step:1155/2330 train_time:46001ms step_avg:39.83ms
step:1156/2330 train_time:46045ms step_avg:39.83ms
step:1157/2330 train_time:46081ms step_avg:39.83ms
step:1158/2330 train_time:46125ms step_avg:39.83ms
step:1159/2330 train_time:46160ms step_avg:39.83ms
step:1160/2330 train_time:46204ms step_avg:39.83ms
step:1161/2330 train_time:46239ms step_avg:39.83ms
step:1162/2330 train_time:46283ms step_avg:39.83ms
step:1163/2330 train_time:46319ms step_avg:39.83ms
step:1164/2330 train_time:46363ms step_avg:39.83ms
step:1165/2330 train_time:46398ms step_avg:39.83ms
step:1166/2330 train_time:46442ms step_avg:39.83ms
step:1167/2330 train_time:46477ms step_avg:39.83ms
step:1168/2330 train_time:46522ms step_avg:39.83ms
step:1169/2330 train_time:46557ms step_avg:39.83ms
step:1170/2330 train_time:46601ms step_avg:39.83ms
step:1171/2330 train_time:46637ms step_avg:39.83ms
step:1172/2330 train_time:46680ms step_avg:39.83ms
step:1173/2330 train_time:46715ms step_avg:39.83ms
step:1174/2330 train_time:46760ms step_avg:39.83ms
step:1175/2330 train_time:46796ms step_avg:39.83ms
step:1176/2330 train_time:46840ms step_avg:39.83ms
step:1177/2330 train_time:46875ms step_avg:39.83ms
step:1178/2330 train_time:46919ms step_avg:39.83ms
step:1179/2330 train_time:46954ms step_avg:39.83ms
step:1180/2330 train_time:46999ms step_avg:39.83ms
step:1181/2330 train_time:47033ms step_avg:39.82ms
step:1182/2330 train_time:47077ms step_avg:39.83ms
step:1183/2330 train_time:47112ms step_avg:39.82ms
step:1184/2330 train_time:47157ms step_avg:39.83ms
step:1185/2330 train_time:47192ms step_avg:39.82ms
step:1186/2330 train_time:47236ms step_avg:39.83ms
step:1187/2330 train_time:47272ms step_avg:39.82ms
step:1188/2330 train_time:47317ms step_avg:39.83ms
step:1189/2330 train_time:47352ms step_avg:39.82ms
step:1190/2330 train_time:47396ms step_avg:39.83ms
step:1191/2330 train_time:47432ms step_avg:39.82ms
step:1192/2330 train_time:47475ms step_avg:39.83ms
step:1193/2330 train_time:47510ms step_avg:39.82ms
step:1194/2330 train_time:47555ms step_avg:39.83ms
step:1195/2330 train_time:47590ms step_avg:39.82ms
step:1196/2330 train_time:47634ms step_avg:39.83ms
step:1197/2330 train_time:47670ms step_avg:39.82ms
step:1198/2330 train_time:47714ms step_avg:39.83ms
step:1199/2330 train_time:47749ms step_avg:39.82ms
step:1200/2330 train_time:47794ms step_avg:39.83ms
step:1201/2330 train_time:47829ms step_avg:39.82ms
step:1202/2330 train_time:47873ms step_avg:39.83ms
step:1203/2330 train_time:47908ms step_avg:39.82ms
step:1204/2330 train_time:47953ms step_avg:39.83ms
step:1205/2330 train_time:47988ms step_avg:39.82ms
step:1206/2330 train_time:48032ms step_avg:39.83ms
step:1207/2330 train_time:48067ms step_avg:39.82ms
step:1208/2330 train_time:48112ms step_avg:39.83ms
step:1209/2330 train_time:48148ms step_avg:39.82ms
step:1210/2330 train_time:48192ms step_avg:39.83ms
step:1211/2330 train_time:48228ms step_avg:39.82ms
step:1212/2330 train_time:48273ms step_avg:39.83ms
step:1213/2330 train_time:48308ms step_avg:39.83ms
step:1214/2330 train_time:48354ms step_avg:39.83ms
step:1215/2330 train_time:48390ms step_avg:39.83ms
step:1216/2330 train_time:48434ms step_avg:39.83ms
step:1217/2330 train_time:48469ms step_avg:39.83ms
step:1218/2330 train_time:48513ms step_avg:39.83ms
step:1219/2330 train_time:48548ms step_avg:39.83ms
step:1220/2330 train_time:48592ms step_avg:39.83ms
step:1221/2330 train_time:48628ms step_avg:39.83ms
step:1222/2330 train_time:48671ms step_avg:39.83ms
step:1223/2330 train_time:48706ms step_avg:39.83ms
step:1224/2330 train_time:48752ms step_avg:39.83ms
step:1225/2330 train_time:48787ms step_avg:39.83ms
step:1226/2330 train_time:48831ms step_avg:39.83ms
step:1227/2330 train_time:48865ms step_avg:39.82ms
step:1228/2330 train_time:48909ms step_avg:39.83ms
step:1229/2330 train_time:48944ms step_avg:39.82ms
step:1230/2330 train_time:48988ms step_avg:39.83ms
step:1231/2330 train_time:49023ms step_avg:39.82ms
step:1232/2330 train_time:49068ms step_avg:39.83ms
step:1233/2330 train_time:49103ms step_avg:39.82ms
step:1234/2330 train_time:49147ms step_avg:39.83ms
step:1235/2330 train_time:49183ms step_avg:39.82ms
step:1236/2330 train_time:49227ms step_avg:39.83ms
step:1237/2330 train_time:49262ms step_avg:39.82ms
step:1238/2330 train_time:49307ms step_avg:39.83ms
step:1239/2330 train_time:49342ms step_avg:39.82ms
step:1240/2330 train_time:49386ms step_avg:39.83ms
step:1241/2330 train_time:49422ms step_avg:39.82ms
step:1242/2330 train_time:49465ms step_avg:39.83ms
step:1243/2330 train_time:49500ms step_avg:39.82ms
step:1244/2330 train_time:49544ms step_avg:39.83ms
step:1245/2330 train_time:49579ms step_avg:39.82ms
step:1246/2330 train_time:49624ms step_avg:39.83ms
step:1247/2330 train_time:49659ms step_avg:39.82ms
step:1248/2330 train_time:49704ms step_avg:39.83ms
step:1249/2330 train_time:49739ms step_avg:39.82ms
step:1250/2330 train_time:49783ms step_avg:39.83ms
step:1250/2330 val_loss:5.1699 train_time:49868ms step_avg:39.89ms
step:1251/2330 train_time:49881ms step_avg:39.87ms
step:1252/2330 train_time:49893ms step_avg:39.85ms
step:1253/2330 train_time:49904ms step_avg:39.83ms
step:1254/2330 train_time:49941ms step_avg:39.83ms
step:1255/2330 train_time:49975ms step_avg:39.82ms
step:1256/2330 train_time:50019ms step_avg:39.82ms
step:1257/2330 train_time:50053ms step_avg:39.82ms
step:1258/2330 train_time:50097ms step_avg:39.82ms
step:1259/2330 train_time:50132ms step_avg:39.82ms
step:1260/2330 train_time:50177ms step_avg:39.82ms
step:1261/2330 train_time:50218ms step_avg:39.82ms
step:1262/2330 train_time:50266ms step_avg:39.83ms
step:1263/2330 train_time:50301ms step_avg:39.83ms
step:1264/2330 train_time:50345ms step_avg:39.83ms
step:1265/2330 train_time:50381ms step_avg:39.83ms
step:1266/2330 train_time:50425ms step_avg:39.83ms
step:1267/2330 train_time:50460ms step_avg:39.83ms
step:1268/2330 train_time:50504ms step_avg:39.83ms
step:1269/2330 train_time:50538ms step_avg:39.82ms
step:1270/2330 train_time:50581ms step_avg:39.83ms
step:1271/2330 train_time:50616ms step_avg:39.82ms
step:1272/2330 train_time:50793ms step_avg:39.93ms
step:1273/2330 train_time:50827ms step_avg:39.93ms
step:1274/2330 train_time:50870ms step_avg:39.93ms
step:1275/2330 train_time:50904ms step_avg:39.92ms
step:1276/2330 train_time:50947ms step_avg:39.93ms
step:1277/2330 train_time:50981ms step_avg:39.92ms
step:1278/2330 train_time:51025ms step_avg:39.93ms
step:1279/2330 train_time:51059ms step_avg:39.92ms
step:1280/2330 train_time:51103ms step_avg:39.92ms
step:1281/2330 train_time:51137ms step_avg:39.92ms
step:1282/2330 train_time:51180ms step_avg:39.92ms
step:1283/2330 train_time:51214ms step_avg:39.92ms
step:1284/2330 train_time:51258ms step_avg:39.92ms
step:1285/2330 train_time:51292ms step_avg:39.92ms
step:1286/2330 train_time:51335ms step_avg:39.92ms
step:1287/2330 train_time:51369ms step_avg:39.91ms
step:1288/2330 train_time:51412ms step_avg:39.92ms
step:1289/2330 train_time:51446ms step_avg:39.91ms
step:1290/2330 train_time:51490ms step_avg:39.91ms
step:1291/2330 train_time:51524ms step_avg:39.91ms
step:1292/2330 train_time:51567ms step_avg:39.91ms
step:1293/2330 train_time:51601ms step_avg:39.91ms
step:1294/2330 train_time:51648ms step_avg:39.91ms
step:1295/2330 train_time:51687ms step_avg:39.91ms
step:1296/2330 train_time:51735ms step_avg:39.92ms
step:1297/2330 train_time:51773ms step_avg:39.92ms
step:1298/2330 train_time:51819ms step_avg:39.92ms
step:1299/2330 train_time:51854ms step_avg:39.92ms
step:1300/2330 train_time:51899ms step_avg:39.92ms
step:1301/2330 train_time:51934ms step_avg:39.92ms
step:1302/2330 train_time:51978ms step_avg:39.92ms
step:1303/2330 train_time:52013ms step_avg:39.92ms
step:1304/2330 train_time:52057ms step_avg:39.92ms
step:1305/2330 train_time:52092ms step_avg:39.92ms
step:1306/2330 train_time:52136ms step_avg:39.92ms
step:1307/2330 train_time:52170ms step_avg:39.92ms
step:1308/2330 train_time:52214ms step_avg:39.92ms
step:1309/2330 train_time:52248ms step_avg:39.91ms
step:1310/2330 train_time:52291ms step_avg:39.92ms
step:1311/2330 train_time:52325ms step_avg:39.91ms
step:1312/2330 train_time:52369ms step_avg:39.92ms
step:1313/2330 train_time:52404ms step_avg:39.91ms
step:1314/2330 train_time:52447ms step_avg:39.91ms
step:1315/2330 train_time:52482ms step_avg:39.91ms
step:1316/2330 train_time:52525ms step_avg:39.91ms
step:1317/2330 train_time:52560ms step_avg:39.91ms
step:1318/2330 train_time:52605ms step_avg:39.91ms
step:1319/2330 train_time:52640ms step_avg:39.91ms
step:1320/2330 train_time:52686ms step_avg:39.91ms
step:1321/2330 train_time:52721ms step_avg:39.91ms
step:1322/2330 train_time:52767ms step_avg:39.91ms
step:1323/2330 train_time:52803ms step_avg:39.91ms
step:1324/2330 train_time:52848ms step_avg:39.92ms
step:1325/2330 train_time:52883ms step_avg:39.91ms
step:1326/2330 train_time:52927ms step_avg:39.91ms
step:1327/2330 train_time:52962ms step_avg:39.91ms
step:1328/2330 train_time:53006ms step_avg:39.91ms
step:1329/2330 train_time:53040ms step_avg:39.91ms
step:1330/2330 train_time:53085ms step_avg:39.91ms
step:1331/2330 train_time:53119ms step_avg:39.91ms
step:1332/2330 train_time:53163ms step_avg:39.91ms
step:1333/2330 train_time:53198ms step_avg:39.91ms
step:1334/2330 train_time:53242ms step_avg:39.91ms
step:1335/2330 train_time:53276ms step_avg:39.91ms
step:1336/2330 train_time:53321ms step_avg:39.91ms
step:1337/2330 train_time:53355ms step_avg:39.91ms
step:1338/2330 train_time:53399ms step_avg:39.91ms
step:1339/2330 train_time:53433ms step_avg:39.91ms
step:1340/2330 train_time:53477ms step_avg:39.91ms
step:1341/2330 train_time:53512ms step_avg:39.90ms
step:1342/2330 train_time:53558ms step_avg:39.91ms
step:1343/2330 train_time:53593ms step_avg:39.91ms
step:1344/2330 train_time:53638ms step_avg:39.91ms
step:1345/2330 train_time:53674ms step_avg:39.91ms
step:1346/2330 train_time:53721ms step_avg:39.91ms
step:1347/2330 train_time:53756ms step_avg:39.91ms
step:1348/2330 train_time:53801ms step_avg:39.91ms
step:1349/2330 train_time:53837ms step_avg:39.91ms
step:1350/2330 train_time:53881ms step_avg:39.91ms
step:1351/2330 train_time:53916ms step_avg:39.91ms
step:1352/2330 train_time:53961ms step_avg:39.91ms
step:1353/2330 train_time:53996ms step_avg:39.91ms
step:1354/2330 train_time:54040ms step_avg:39.91ms
step:1355/2330 train_time:54075ms step_avg:39.91ms
step:1356/2330 train_time:54119ms step_avg:39.91ms
step:1357/2330 train_time:54154ms step_avg:39.91ms
step:1358/2330 train_time:54199ms step_avg:39.91ms
step:1359/2330 train_time:54233ms step_avg:39.91ms
step:1360/2330 train_time:54277ms step_avg:39.91ms
step:1361/2330 train_time:54312ms step_avg:39.91ms
step:1362/2330 train_time:54356ms step_avg:39.91ms
step:1363/2330 train_time:54390ms step_avg:39.90ms
step:1364/2330 train_time:54434ms step_avg:39.91ms
step:1365/2330 train_time:54468ms step_avg:39.90ms
step:1366/2330 train_time:54512ms step_avg:39.91ms
step:1367/2330 train_time:54547ms step_avg:39.90ms
step:1368/2330 train_time:54591ms step_avg:39.91ms
step:1369/2330 train_time:54626ms step_avg:39.90ms
step:1370/2330 train_time:54671ms step_avg:39.91ms
step:1371/2330 train_time:54707ms step_avg:39.90ms
step:1372/2330 train_time:54753ms step_avg:39.91ms
step:1373/2330 train_time:54789ms step_avg:39.90ms
step:1374/2330 train_time:54833ms step_avg:39.91ms
step:1375/2330 train_time:54869ms step_avg:39.90ms
step:1376/2330 train_time:54914ms step_avg:39.91ms
step:1377/2330 train_time:54949ms step_avg:39.90ms
step:1378/2330 train_time:54993ms step_avg:39.91ms
step:1379/2330 train_time:55028ms step_avg:39.90ms
step:1380/2330 train_time:55072ms step_avg:39.91ms
step:1381/2330 train_time:55107ms step_avg:39.90ms
step:1382/2330 train_time:55151ms step_avg:39.91ms
step:1383/2330 train_time:55186ms step_avg:39.90ms
step:1384/2330 train_time:55230ms step_avg:39.91ms
step:1385/2330 train_time:55265ms step_avg:39.90ms
step:1386/2330 train_time:55309ms step_avg:39.91ms
step:1387/2330 train_time:55344ms step_avg:39.90ms
step:1388/2330 train_time:55388ms step_avg:39.90ms
step:1389/2330 train_time:55422ms step_avg:39.90ms
step:1390/2330 train_time:55466ms step_avg:39.90ms
step:1391/2330 train_time:55501ms step_avg:39.90ms
step:1392/2330 train_time:55545ms step_avg:39.90ms
step:1393/2330 train_time:55579ms step_avg:39.90ms
step:1394/2330 train_time:55624ms step_avg:39.90ms
step:1395/2330 train_time:55659ms step_avg:39.90ms
step:1396/2330 train_time:55703ms step_avg:39.90ms
step:1397/2330 train_time:55738ms step_avg:39.90ms
step:1398/2330 train_time:55783ms step_avg:39.90ms
step:1399/2330 train_time:55819ms step_avg:39.90ms
step:1400/2330 train_time:55863ms step_avg:39.90ms
step:1401/2330 train_time:55898ms step_avg:39.90ms
step:1402/2330 train_time:55943ms step_avg:39.90ms
step:1403/2330 train_time:55979ms step_avg:39.90ms
step:1404/2330 train_time:56023ms step_avg:39.90ms
step:1405/2330 train_time:56058ms step_avg:39.90ms
step:1406/2330 train_time:56103ms step_avg:39.90ms
step:1407/2330 train_time:56138ms step_avg:39.90ms
step:1408/2330 train_time:56182ms step_avg:39.90ms
step:1409/2330 train_time:56217ms step_avg:39.90ms
step:1410/2330 train_time:56262ms step_avg:39.90ms
step:1411/2330 train_time:56296ms step_avg:39.90ms
step:1412/2330 train_time:56339ms step_avg:39.90ms
step:1413/2330 train_time:56374ms step_avg:39.90ms
step:1414/2330 train_time:56418ms step_avg:39.90ms
step:1415/2330 train_time:56453ms step_avg:39.90ms
step:1416/2330 train_time:56497ms step_avg:39.90ms
step:1417/2330 train_time:56531ms step_avg:39.89ms
step:1418/2330 train_time:56576ms step_avg:39.90ms
step:1419/2330 train_time:56612ms step_avg:39.90ms
step:1420/2330 train_time:56657ms step_avg:39.90ms
step:1421/2330 train_time:56693ms step_avg:39.90ms
step:1422/2330 train_time:56738ms step_avg:39.90ms
step:1423/2330 train_time:56774ms step_avg:39.90ms
step:1424/2330 train_time:56820ms step_avg:39.90ms
step:1425/2330 train_time:56855ms step_avg:39.90ms
step:1426/2330 train_time:56899ms step_avg:39.90ms
step:1427/2330 train_time:56935ms step_avg:39.90ms
step:1428/2330 train_time:56979ms step_avg:39.90ms
step:1429/2330 train_time:57015ms step_avg:39.90ms
step:1430/2330 train_time:57060ms step_avg:39.90ms
step:1431/2330 train_time:57094ms step_avg:39.90ms
step:1432/2330 train_time:57139ms step_avg:39.90ms
step:1433/2330 train_time:57173ms step_avg:39.90ms
step:1434/2330 train_time:57218ms step_avg:39.90ms
step:1435/2330 train_time:57252ms step_avg:39.90ms
step:1436/2330 train_time:57297ms step_avg:39.90ms
step:1437/2330 train_time:57331ms step_avg:39.90ms
step:1438/2330 train_time:57376ms step_avg:39.90ms
step:1439/2330 train_time:57411ms step_avg:39.90ms
step:1440/2330 train_time:57455ms step_avg:39.90ms
step:1441/2330 train_time:57490ms step_avg:39.90ms
step:1442/2330 train_time:57534ms step_avg:39.90ms
step:1443/2330 train_time:57568ms step_avg:39.89ms
step:1444/2330 train_time:57612ms step_avg:39.90ms
step:1445/2330 train_time:57646ms step_avg:39.89ms
step:1446/2330 train_time:57691ms step_avg:39.90ms
step:1447/2330 train_time:57726ms step_avg:39.89ms
step:1448/2330 train_time:57770ms step_avg:39.90ms
step:1449/2330 train_time:57806ms step_avg:39.89ms
step:1450/2330 train_time:57850ms step_avg:39.90ms
step:1451/2330 train_time:57886ms step_avg:39.89ms
step:1452/2330 train_time:57931ms step_avg:39.90ms
step:1453/2330 train_time:57966ms step_avg:39.89ms
step:1454/2330 train_time:58011ms step_avg:39.90ms
step:1455/2330 train_time:58046ms step_avg:39.89ms
step:1456/2330 train_time:58091ms step_avg:39.90ms
step:1457/2330 train_time:58126ms step_avg:39.89ms
step:1458/2330 train_time:58171ms step_avg:39.90ms
step:1459/2330 train_time:58207ms step_avg:39.89ms
step:1460/2330 train_time:58251ms step_avg:39.90ms
step:1461/2330 train_time:58286ms step_avg:39.89ms
step:1462/2330 train_time:58330ms step_avg:39.90ms
step:1463/2330 train_time:58365ms step_avg:39.89ms
step:1464/2330 train_time:58409ms step_avg:39.90ms
step:1465/2330 train_time:58443ms step_avg:39.89ms
step:1466/2330 train_time:58487ms step_avg:39.90ms
step:1467/2330 train_time:58522ms step_avg:39.89ms
step:1468/2330 train_time:58566ms step_avg:39.90ms
step:1469/2330 train_time:58601ms step_avg:39.89ms
step:1470/2330 train_time:58645ms step_avg:39.89ms
step:1471/2330 train_time:58680ms step_avg:39.89ms
step:1472/2330 train_time:58725ms step_avg:39.89ms
step:1473/2330 train_time:58760ms step_avg:39.89ms
step:1474/2330 train_time:58804ms step_avg:39.89ms
step:1475/2330 train_time:58839ms step_avg:39.89ms
step:1476/2330 train_time:58883ms step_avg:39.89ms
step:1477/2330 train_time:58918ms step_avg:39.89ms
step:1478/2330 train_time:58963ms step_avg:39.89ms
step:1479/2330 train_time:58998ms step_avg:39.89ms
step:1480/2330 train_time:59043ms step_avg:39.89ms
step:1481/2330 train_time:59078ms step_avg:39.89ms
step:1482/2330 train_time:59122ms step_avg:39.89ms
step:1483/2330 train_time:59157ms step_avg:39.89ms
step:1484/2330 train_time:59202ms step_avg:39.89ms
step:1485/2330 train_time:59236ms step_avg:39.89ms
step:1486/2330 train_time:59281ms step_avg:39.89ms
step:1487/2330 train_time:59316ms step_avg:39.89ms
step:1488/2330 train_time:59360ms step_avg:39.89ms
step:1489/2330 train_time:59395ms step_avg:39.89ms
step:1490/2330 train_time:59440ms step_avg:39.89ms
step:1491/2330 train_time:59474ms step_avg:39.89ms
step:1492/2330 train_time:59519ms step_avg:39.89ms
step:1493/2330 train_time:59555ms step_avg:39.89ms
step:1494/2330 train_time:59599ms step_avg:39.89ms
step:1495/2330 train_time:59634ms step_avg:39.89ms
step:1496/2330 train_time:59679ms step_avg:39.89ms
step:1497/2330 train_time:59714ms step_avg:39.89ms
step:1498/2330 train_time:59759ms step_avg:39.89ms
step:1499/2330 train_time:59795ms step_avg:39.89ms
step:1500/2330 train_time:59840ms step_avg:39.89ms
step:1500/2330 val_loss:5.1362 train_time:59926ms step_avg:39.95ms
step:1501/2330 train_time:59939ms step_avg:39.93ms
step:1502/2330 train_time:59951ms step_avg:39.91ms
step:1503/2330 train_time:59962ms step_avg:39.90ms
step:1504/2330 train_time:60000ms step_avg:39.89ms
step:1505/2330 train_time:60034ms step_avg:39.89ms
step:1506/2330 train_time:60077ms step_avg:39.89ms
step:1507/2330 train_time:60111ms step_avg:39.89ms
step:1508/2330 train_time:60155ms step_avg:39.89ms
step:1509/2330 train_time:60189ms step_avg:39.89ms
step:1510/2330 train_time:60233ms step_avg:39.89ms
step:1511/2330 train_time:60273ms step_avg:39.89ms
step:1512/2330 train_time:60320ms step_avg:39.89ms
step:1513/2330 train_time:60356ms step_avg:39.89ms
step:1514/2330 train_time:60401ms step_avg:39.89ms
step:1515/2330 train_time:60436ms step_avg:39.89ms
step:1516/2330 train_time:60480ms step_avg:39.89ms
step:1517/2330 train_time:60514ms step_avg:39.89ms
step:1518/2330 train_time:60558ms step_avg:39.89ms
step:1519/2330 train_time:60738ms step_avg:39.99ms
step:1520/2330 train_time:60751ms step_avg:39.97ms
step:1521/2330 train_time:60762ms step_avg:39.95ms
step:1522/2330 train_time:60791ms step_avg:39.94ms
step:1523/2330 train_time:60824ms step_avg:39.94ms
step:1524/2330 train_time:60867ms step_avg:39.94ms
step:1525/2330 train_time:60902ms step_avg:39.94ms
step:1526/2330 train_time:60945ms step_avg:39.94ms
step:1527/2330 train_time:60979ms step_avg:39.93ms
step:1528/2330 train_time:61022ms step_avg:39.94ms
step:1529/2330 train_time:61057ms step_avg:39.93ms
step:1530/2330 train_time:61100ms step_avg:39.93ms
step:1531/2330 train_time:61135ms step_avg:39.93ms
step:1532/2330 train_time:61178ms step_avg:39.93ms
step:1533/2330 train_time:61212ms step_avg:39.93ms
step:1534/2330 train_time:61256ms step_avg:39.93ms
step:1535/2330 train_time:61290ms step_avg:39.93ms
step:1536/2330 train_time:61334ms step_avg:39.93ms
step:1537/2330 train_time:61368ms step_avg:39.93ms
step:1538/2330 train_time:61411ms step_avg:39.93ms
step:1539/2330 train_time:61445ms step_avg:39.93ms
step:1540/2330 train_time:61488ms step_avg:39.93ms
step:1541/2330 train_time:61523ms step_avg:39.92ms
step:1542/2330 train_time:61566ms step_avg:39.93ms
step:1543/2330 train_time:61600ms step_avg:39.92ms
step:1544/2330 train_time:61647ms step_avg:39.93ms
step:1545/2330 train_time:61685ms step_avg:39.93ms
step:1546/2330 train_time:61733ms step_avg:39.93ms
step:1547/2330 train_time:61771ms step_avg:39.93ms
step:1548/2330 train_time:61817ms step_avg:39.93ms
step:1549/2330 train_time:61852ms step_avg:39.93ms
step:1550/2330 train_time:61897ms step_avg:39.93ms
step:1551/2330 train_time:61931ms step_avg:39.93ms
step:1552/2330 train_time:61975ms step_avg:39.93ms
step:1553/2330 train_time:62010ms step_avg:39.93ms
step:1554/2330 train_time:62055ms step_avg:39.93ms
step:1555/2330 train_time:62089ms step_avg:39.93ms
step:1556/2330 train_time:62132ms step_avg:39.93ms
step:1557/2330 train_time:62167ms step_avg:39.93ms
step:1558/2330 train_time:62211ms step_avg:39.93ms
step:1559/2330 train_time:62245ms step_avg:39.93ms
step:1560/2330 train_time:62288ms step_avg:39.93ms
step:1561/2330 train_time:62322ms step_avg:39.92ms
step:1562/2330 train_time:62366ms step_avg:39.93ms
step:1563/2330 train_time:62400ms step_avg:39.92ms
step:1564/2330 train_time:62444ms step_avg:39.93ms
step:1565/2330 train_time:62478ms step_avg:39.92ms
step:1566/2330 train_time:62521ms step_avg:39.92ms
step:1567/2330 train_time:62555ms step_avg:39.92ms
step:1568/2330 train_time:62600ms step_avg:39.92ms
step:1569/2330 train_time:62635ms step_avg:39.92ms
step:1570/2330 train_time:62681ms step_avg:39.92ms
step:1571/2330 train_time:62716ms step_avg:39.92ms
step:1572/2330 train_time:62761ms step_avg:39.92ms
step:1573/2330 train_time:62797ms step_avg:39.92ms
step:1574/2330 train_time:62841ms step_avg:39.92ms
step:1575/2330 train_time:62877ms step_avg:39.92ms
step:1576/2330 train_time:62921ms step_avg:39.92ms
step:1577/2330 train_time:62955ms step_avg:39.92ms
step:1578/2330 train_time:62999ms step_avg:39.92ms
step:1579/2330 train_time:63034ms step_avg:39.92ms
step:1580/2330 train_time:63079ms step_avg:39.92ms
step:1581/2330 train_time:63113ms step_avg:39.92ms
step:1582/2330 train_time:63157ms step_avg:39.92ms
step:1583/2330 train_time:63192ms step_avg:39.92ms
step:1584/2330 train_time:63236ms step_avg:39.92ms
step:1585/2330 train_time:63271ms step_avg:39.92ms
step:1586/2330 train_time:63314ms step_avg:39.92ms
step:1587/2330 train_time:63349ms step_avg:39.92ms
step:1588/2330 train_time:63393ms step_avg:39.92ms
step:1589/2330 train_time:63429ms step_avg:39.92ms
step:1590/2330 train_time:63473ms step_avg:39.92ms
step:1591/2330 train_time:63507ms step_avg:39.92ms
step:1592/2330 train_time:63552ms step_avg:39.92ms
step:1593/2330 train_time:63587ms step_avg:39.92ms
step:1594/2330 train_time:63633ms step_avg:39.92ms
step:1595/2330 train_time:63669ms step_avg:39.92ms
step:1596/2330 train_time:63713ms step_avg:39.92ms
step:1597/2330 train_time:63749ms step_avg:39.92ms
step:1598/2330 train_time:63795ms step_avg:39.92ms
step:1599/2330 train_time:63830ms step_avg:39.92ms
step:1600/2330 train_time:63875ms step_avg:39.92ms
step:1601/2330 train_time:63910ms step_avg:39.92ms
step:1602/2330 train_time:63955ms step_avg:39.92ms
step:1603/2330 train_time:63990ms step_avg:39.92ms
step:1604/2330 train_time:64035ms step_avg:39.92ms
step:1605/2330 train_time:64069ms step_avg:39.92ms
step:1606/2330 train_time:64113ms step_avg:39.92ms
step:1607/2330 train_time:64148ms step_avg:39.92ms
step:1608/2330 train_time:64191ms step_avg:39.92ms
step:1609/2330 train_time:64227ms step_avg:39.92ms
step:1610/2330 train_time:64271ms step_avg:39.92ms
step:1611/2330 train_time:64305ms step_avg:39.92ms
step:1612/2330 train_time:64349ms step_avg:39.92ms
step:1613/2330 train_time:64383ms step_avg:39.91ms
step:1614/2330 train_time:64426ms step_avg:39.92ms
step:1615/2330 train_time:64461ms step_avg:39.91ms
step:1616/2330 train_time:64505ms step_avg:39.92ms
step:1617/2330 train_time:64540ms step_avg:39.91ms
step:1618/2330 train_time:64584ms step_avg:39.92ms
step:1619/2330 train_time:64618ms step_avg:39.91ms
step:1620/2330 train_time:64663ms step_avg:39.92ms
step:1621/2330 train_time:64698ms step_avg:39.91ms
step:1622/2330 train_time:64743ms step_avg:39.92ms
step:1623/2330 train_time:64777ms step_avg:39.91ms
step:1624/2330 train_time:64822ms step_avg:39.92ms
step:1625/2330 train_time:64858ms step_avg:39.91ms
step:1626/2330 train_time:64902ms step_avg:39.91ms
step:1627/2330 train_time:64937ms step_avg:39.91ms
step:1628/2330 train_time:64981ms step_avg:39.91ms
step:1629/2330 train_time:65015ms step_avg:39.91ms
step:1630/2330 train_time:65059ms step_avg:39.91ms
step:1631/2330 train_time:65094ms step_avg:39.91ms
step:1632/2330 train_time:65138ms step_avg:39.91ms
step:1633/2330 train_time:65173ms step_avg:39.91ms
step:1634/2330 train_time:65218ms step_avg:39.91ms
step:1635/2330 train_time:65253ms step_avg:39.91ms
step:1636/2330 train_time:65297ms step_avg:39.91ms
step:1637/2330 train_time:65332ms step_avg:39.91ms
step:1638/2330 train_time:65376ms step_avg:39.91ms
step:1639/2330 train_time:65411ms step_avg:39.91ms
step:1640/2330 train_time:65456ms step_avg:39.91ms
step:1641/2330 train_time:65491ms step_avg:39.91ms
step:1642/2330 train_time:65535ms step_avg:39.91ms
step:1643/2330 train_time:65570ms step_avg:39.91ms
step:1644/2330 train_time:65614ms step_avg:39.91ms
step:1645/2330 train_time:65650ms step_avg:39.91ms
step:1646/2330 train_time:65695ms step_avg:39.91ms
step:1647/2330 train_time:65730ms step_avg:39.91ms
step:1648/2330 train_time:65775ms step_avg:39.91ms
step:1649/2330 train_time:65810ms step_avg:39.91ms
step:1650/2330 train_time:65854ms step_avg:39.91ms
step:1651/2330 train_time:65889ms step_avg:39.91ms
step:1652/2330 train_time:65934ms step_avg:39.91ms
step:1653/2330 train_time:65970ms step_avg:39.91ms
step:1654/2330 train_time:66014ms step_avg:39.91ms
step:1655/2330 train_time:66049ms step_avg:39.91ms
step:1656/2330 train_time:66094ms step_avg:39.91ms
step:1657/2330 train_time:66129ms step_avg:39.91ms
step:1658/2330 train_time:66174ms step_avg:39.91ms
step:1659/2330 train_time:66208ms step_avg:39.91ms
step:1660/2330 train_time:66251ms step_avg:39.91ms
step:1661/2330 train_time:66286ms step_avg:39.91ms
step:1662/2330 train_time:66330ms step_avg:39.91ms
step:1663/2330 train_time:66364ms step_avg:39.91ms
step:1664/2330 train_time:66407ms step_avg:39.91ms
step:1665/2330 train_time:66443ms step_avg:39.91ms
step:1666/2330 train_time:66487ms step_avg:39.91ms
step:1667/2330 train_time:66521ms step_avg:39.90ms
step:1668/2330 train_time:66565ms step_avg:39.91ms
step:1669/2330 train_time:66600ms step_avg:39.90ms
step:1670/2330 train_time:66645ms step_avg:39.91ms
step:1671/2330 train_time:66680ms step_avg:39.90ms
step:1672/2330 train_time:66724ms step_avg:39.91ms
step:1673/2330 train_time:66760ms step_avg:39.90ms
step:1674/2330 train_time:66804ms step_avg:39.91ms
step:1675/2330 train_time:66839ms step_avg:39.90ms
step:1676/2330 train_time:66883ms step_avg:39.91ms
step:1677/2330 train_time:66918ms step_avg:39.90ms
step:1678/2330 train_time:66962ms step_avg:39.91ms
step:1679/2330 train_time:66997ms step_avg:39.90ms
step:1680/2330 train_time:67042ms step_avg:39.91ms
step:1681/2330 train_time:67077ms step_avg:39.90ms
step:1682/2330 train_time:67121ms step_avg:39.91ms
step:1683/2330 train_time:67155ms step_avg:39.90ms
step:1684/2330 train_time:67199ms step_avg:39.90ms
step:1685/2330 train_time:67235ms step_avg:39.90ms
step:1686/2330 train_time:67279ms step_avg:39.90ms
step:1687/2330 train_time:67314ms step_avg:39.90ms
step:1688/2330 train_time:67359ms step_avg:39.90ms
step:1689/2330 train_time:67393ms step_avg:39.90ms
step:1690/2330 train_time:67438ms step_avg:39.90ms
step:1691/2330 train_time:67474ms step_avg:39.90ms
step:1692/2330 train_time:67518ms step_avg:39.90ms
step:1693/2330 train_time:67553ms step_avg:39.90ms
step:1694/2330 train_time:67597ms step_avg:39.90ms
step:1695/2330 train_time:67632ms step_avg:39.90ms
step:1696/2330 train_time:67677ms step_avg:39.90ms
step:1697/2330 train_time:67712ms step_avg:39.90ms
step:1698/2330 train_time:67756ms step_avg:39.90ms
step:1699/2330 train_time:67791ms step_avg:39.90ms
step:1700/2330 train_time:67836ms step_avg:39.90ms
step:1701/2330 train_time:67871ms step_avg:39.90ms
step:1702/2330 train_time:67915ms step_avg:39.90ms
step:1703/2330 train_time:67950ms step_avg:39.90ms
step:1704/2330 train_time:67995ms step_avg:39.90ms
step:1705/2330 train_time:68030ms step_avg:39.90ms
step:1706/2330 train_time:68075ms step_avg:39.90ms
step:1707/2330 train_time:68109ms step_avg:39.90ms
step:1708/2330 train_time:68153ms step_avg:39.90ms
step:1709/2330 train_time:68188ms step_avg:39.90ms
step:1710/2330 train_time:68233ms step_avg:39.90ms
step:1711/2330 train_time:68268ms step_avg:39.90ms
step:1712/2330 train_time:68312ms step_avg:39.90ms
step:1713/2330 train_time:68346ms step_avg:39.90ms
step:1714/2330 train_time:68391ms step_avg:39.90ms
step:1715/2330 train_time:68426ms step_avg:39.90ms
step:1716/2330 train_time:68470ms step_avg:39.90ms
step:1717/2330 train_time:68504ms step_avg:39.90ms
step:1718/2330 train_time:68548ms step_avg:39.90ms
step:1719/2330 train_time:68583ms step_avg:39.90ms
step:1720/2330 train_time:68627ms step_avg:39.90ms
step:1721/2330 train_time:68663ms step_avg:39.90ms
step:1722/2330 train_time:68707ms step_avg:39.90ms
step:1723/2330 train_time:68742ms step_avg:39.90ms
step:1724/2330 train_time:68786ms step_avg:39.90ms
step:1725/2330 train_time:68821ms step_avg:39.90ms
step:1726/2330 train_time:68866ms step_avg:39.90ms
step:1727/2330 train_time:68901ms step_avg:39.90ms
step:1728/2330 train_time:68945ms step_avg:39.90ms
step:1729/2330 train_time:68979ms step_avg:39.90ms
step:1730/2330 train_time:69024ms step_avg:39.90ms
step:1731/2330 train_time:69059ms step_avg:39.90ms
step:1732/2330 train_time:69103ms step_avg:39.90ms
step:1733/2330 train_time:69138ms step_avg:39.89ms
step:1734/2330 train_time:69182ms step_avg:39.90ms
step:1735/2330 train_time:69217ms step_avg:39.89ms
step:1736/2330 train_time:69261ms step_avg:39.90ms
step:1737/2330 train_time:69296ms step_avg:39.89ms
step:1738/2330 train_time:69340ms step_avg:39.90ms
step:1739/2330 train_time:69375ms step_avg:39.89ms
step:1740/2330 train_time:69419ms step_avg:39.90ms
step:1741/2330 train_time:69454ms step_avg:39.89ms
step:1742/2330 train_time:69498ms step_avg:39.90ms
step:1743/2330 train_time:69534ms step_avg:39.89ms
step:1744/2330 train_time:69579ms step_avg:39.90ms
step:1745/2330 train_time:69613ms step_avg:39.89ms
step:1746/2330 train_time:69657ms step_avg:39.90ms
step:1747/2330 train_time:69693ms step_avg:39.89ms
step:1748/2330 train_time:69738ms step_avg:39.90ms
step:1749/2330 train_time:69773ms step_avg:39.89ms
step:1750/2330 train_time:69817ms step_avg:39.90ms
step:1750/2330 val_loss:5.1022 train_time:69903ms step_avg:39.94ms
step:1751/2330 train_time:69917ms step_avg:39.93ms
step:1752/2330 train_time:69929ms step_avg:39.91ms
step:1753/2330 train_time:69940ms step_avg:39.90ms
step:1754/2330 train_time:69975ms step_avg:39.89ms
step:1755/2330 train_time:70009ms step_avg:39.89ms
step:1756/2330 train_time:70052ms step_avg:39.89ms
step:1757/2330 train_time:70087ms step_avg:39.89ms
step:1758/2330 train_time:70130ms step_avg:39.89ms
step:1759/2330 train_time:70165ms step_avg:39.89ms
step:1760/2330 train_time:70208ms step_avg:39.89ms
step:1761/2330 train_time:70246ms step_avg:39.89ms
step:1762/2330 train_time:70294ms step_avg:39.89ms
step:1763/2330 train_time:70331ms step_avg:39.89ms
step:1764/2330 train_time:70376ms step_avg:39.90ms
step:1765/2330 train_time:70411ms step_avg:39.89ms
step:1766/2330 train_time:70456ms step_avg:39.90ms
step:1767/2330 train_time:70491ms step_avg:39.89ms
step:1768/2330 train_time:70535ms step_avg:39.90ms
step:1769/2330 train_time:70570ms step_avg:39.89ms
step:1770/2330 train_time:70613ms step_avg:39.89ms
step:1771/2330 train_time:70649ms step_avg:39.89ms
step:1772/2330 train_time:70692ms step_avg:39.89ms
step:1773/2330 train_time:70727ms step_avg:39.89ms
step:1774/2330 train_time:70770ms step_avg:39.89ms
step:1775/2330 train_time:70805ms step_avg:39.89ms
step:1776/2330 train_time:70851ms step_avg:39.89ms
step:1777/2330 train_time:70886ms step_avg:39.89ms
step:1778/2330 train_time:70931ms step_avg:39.89ms
step:1779/2330 train_time:70966ms step_avg:39.89ms
step:1780/2330 train_time:71010ms step_avg:39.89ms
step:1781/2330 train_time:71044ms step_avg:39.89ms
step:1782/2330 train_time:71088ms step_avg:39.89ms
step:1783/2330 train_time:71123ms step_avg:39.89ms
step:1784/2330 train_time:71167ms step_avg:39.89ms
step:1785/2330 train_time:71202ms step_avg:39.89ms
step:1786/2330 train_time:71246ms step_avg:39.89ms
step:1787/2330 train_time:71281ms step_avg:39.89ms
step:1788/2330 train_time:71326ms step_avg:39.89ms
step:1789/2330 train_time:71361ms step_avg:39.89ms
step:1790/2330 train_time:71405ms step_avg:39.89ms
step:1791/2330 train_time:71441ms step_avg:39.89ms
step:1792/2330 train_time:71485ms step_avg:39.89ms
step:1793/2330 train_time:71521ms step_avg:39.89ms
step:1794/2330 train_time:71565ms step_avg:39.89ms
step:1795/2330 train_time:71600ms step_avg:39.89ms
step:1796/2330 train_time:71645ms step_avg:39.89ms
step:1797/2330 train_time:71680ms step_avg:39.89ms
step:1798/2330 train_time:71724ms step_avg:39.89ms
step:1799/2330 train_time:71759ms step_avg:39.89ms
step:1800/2330 train_time:71804ms step_avg:39.89ms
step:1801/2330 train_time:71839ms step_avg:39.89ms
step:1802/2330 train_time:71884ms step_avg:39.89ms
step:1803/2330 train_time:71920ms step_avg:39.89ms
step:1804/2330 train_time:71964ms step_avg:39.89ms
step:1805/2330 train_time:71999ms step_avg:39.89ms
step:1806/2330 train_time:72043ms step_avg:39.89ms
step:1807/2330 train_time:72078ms step_avg:39.89ms
step:1808/2330 train_time:72122ms step_avg:39.89ms
step:1809/2330 train_time:72157ms step_avg:39.89ms
step:1810/2330 train_time:72201ms step_avg:39.89ms
step:1811/2330 train_time:72236ms step_avg:39.89ms
step:1812/2330 train_time:72280ms step_avg:39.89ms
step:1813/2330 train_time:72316ms step_avg:39.89ms
step:1814/2330 train_time:72360ms step_avg:39.89ms
step:1815/2330 train_time:72396ms step_avg:39.89ms
step:1816/2330 train_time:72440ms step_avg:39.89ms
step:1817/2330 train_time:72475ms step_avg:39.89ms
step:1818/2330 train_time:72520ms step_avg:39.89ms
step:1819/2330 train_time:72556ms step_avg:39.89ms
step:1820/2330 train_time:72600ms step_avg:39.89ms
step:1821/2330 train_time:72635ms step_avg:39.89ms
step:1822/2330 train_time:72680ms step_avg:39.89ms
step:1823/2330 train_time:72715ms step_avg:39.89ms
step:1824/2330 train_time:72759ms step_avg:39.89ms
step:1825/2330 train_time:72794ms step_avg:39.89ms
step:1826/2330 train_time:72838ms step_avg:39.89ms
step:1827/2330 train_time:72873ms step_avg:39.89ms
step:1828/2330 train_time:72918ms step_avg:39.89ms
step:1829/2330 train_time:72953ms step_avg:39.89ms
step:1830/2330 train_time:72998ms step_avg:39.89ms
step:1831/2330 train_time:73034ms step_avg:39.89ms
step:1832/2330 train_time:73078ms step_avg:39.89ms
step:1833/2330 train_time:73113ms step_avg:39.89ms
step:1834/2330 train_time:73157ms step_avg:39.89ms
step:1835/2330 train_time:73192ms step_avg:39.89ms
step:1836/2330 train_time:73236ms step_avg:39.89ms
step:1837/2330 train_time:73271ms step_avg:39.89ms
step:1838/2330 train_time:73316ms step_avg:39.89ms
step:1839/2330 train_time:73351ms step_avg:39.89ms
step:1840/2330 train_time:73395ms step_avg:39.89ms
step:1841/2330 train_time:73430ms step_avg:39.89ms
step:1842/2330 train_time:73474ms step_avg:39.89ms
step:1843/2330 train_time:73509ms step_avg:39.89ms
step:1844/2330 train_time:73554ms step_avg:39.89ms
step:1845/2330 train_time:73589ms step_avg:39.89ms
step:1846/2330 train_time:73633ms step_avg:39.89ms
step:1847/2330 train_time:73669ms step_avg:39.89ms
step:1848/2330 train_time:73714ms step_avg:39.89ms
step:1849/2330 train_time:73749ms step_avg:39.89ms
step:1850/2330 train_time:73793ms step_avg:39.89ms
step:1851/2330 train_time:73829ms step_avg:39.89ms
step:1852/2330 train_time:73873ms step_avg:39.89ms
step:1853/2330 train_time:73909ms step_avg:39.89ms
step:1854/2330 train_time:73953ms step_avg:39.89ms
step:1855/2330 train_time:73988ms step_avg:39.89ms
step:1856/2330 train_time:74033ms step_avg:39.89ms
step:1857/2330 train_time:74068ms step_avg:39.89ms
step:1858/2330 train_time:74112ms step_avg:39.89ms
step:1859/2330 train_time:74148ms step_avg:39.89ms
step:1860/2330 train_time:74192ms step_avg:39.89ms
step:1861/2330 train_time:74228ms step_avg:39.89ms
step:1862/2330 train_time:74272ms step_avg:39.89ms
step:1863/2330 train_time:74307ms step_avg:39.89ms
step:1864/2330 train_time:74351ms step_avg:39.89ms
step:1865/2330 train_time:74386ms step_avg:39.89ms
step:1866/2330 train_time:74431ms step_avg:39.89ms
step:1867/2330 train_time:74466ms step_avg:39.89ms
step:1868/2330 train_time:74510ms step_avg:39.89ms
step:1869/2330 train_time:74545ms step_avg:39.88ms
step:1870/2330 train_time:74588ms step_avg:39.89ms
step:1871/2330 train_time:74623ms step_avg:39.88ms
step:1872/2330 train_time:74668ms step_avg:39.89ms
step:1873/2330 train_time:74703ms step_avg:39.88ms
step:1874/2330 train_time:74748ms step_avg:39.89ms
step:1875/2330 train_time:74783ms step_avg:39.88ms
step:1876/2330 train_time:74827ms step_avg:39.89ms
step:1877/2330 train_time:74862ms step_avg:39.88ms
step:1878/2330 train_time:74907ms step_avg:39.89ms
step:1879/2330 train_time:74942ms step_avg:39.88ms
step:1880/2330 train_time:74986ms step_avg:39.89ms
step:1881/2330 train_time:75021ms step_avg:39.88ms
step:1882/2330 train_time:75065ms step_avg:39.89ms
step:1883/2330 train_time:75100ms step_avg:39.88ms
step:1884/2330 train_time:75144ms step_avg:39.89ms
step:1885/2330 train_time:75180ms step_avg:39.88ms
step:1886/2330 train_time:75223ms step_avg:39.89ms
step:1887/2330 train_time:75258ms step_avg:39.88ms
step:1888/2330 train_time:75302ms step_avg:39.88ms
step:1889/2330 train_time:75337ms step_avg:39.88ms
step:1890/2330 train_time:75382ms step_avg:39.88ms
step:1891/2330 train_time:75417ms step_avg:39.88ms
step:1892/2330 train_time:75462ms step_avg:39.88ms
step:1893/2330 train_time:75496ms step_avg:39.88ms
step:1894/2330 train_time:75541ms step_avg:39.88ms
step:1895/2330 train_time:75576ms step_avg:39.88ms
step:1896/2330 train_time:75621ms step_avg:39.88ms
step:1897/2330 train_time:75656ms step_avg:39.88ms
step:1898/2330 train_time:75700ms step_avg:39.88ms
step:1899/2330 train_time:75735ms step_avg:39.88ms
step:1900/2330 train_time:75780ms step_avg:39.88ms
step:1901/2330 train_time:75816ms step_avg:39.88ms
step:1902/2330 train_time:75861ms step_avg:39.88ms
step:1903/2330 train_time:75895ms step_avg:39.88ms
step:1904/2330 train_time:75940ms step_avg:39.88ms
step:1905/2330 train_time:75974ms step_avg:39.88ms
step:1906/2330 train_time:76019ms step_avg:39.88ms
step:1907/2330 train_time:76054ms step_avg:39.88ms
step:1908/2330 train_time:76098ms step_avg:39.88ms
step:1909/2330 train_time:76133ms step_avg:39.88ms
step:1910/2330 train_time:76177ms step_avg:39.88ms
step:1911/2330 train_time:76213ms step_avg:39.88ms
step:1912/2330 train_time:76257ms step_avg:39.88ms
step:1913/2330 train_time:76293ms step_avg:39.88ms
step:1914/2330 train_time:76336ms step_avg:39.88ms
step:1915/2330 train_time:76371ms step_avg:39.88ms
step:1916/2330 train_time:76416ms step_avg:39.88ms
step:1917/2330 train_time:76450ms step_avg:39.88ms
step:1918/2330 train_time:76494ms step_avg:39.88ms
step:1919/2330 train_time:76529ms step_avg:39.88ms
step:1920/2330 train_time:76573ms step_avg:39.88ms
step:1921/2330 train_time:76609ms step_avg:39.88ms
step:1922/2330 train_time:76653ms step_avg:39.88ms
step:1923/2330 train_time:76688ms step_avg:39.88ms
step:1924/2330 train_time:76733ms step_avg:39.88ms
step:1925/2330 train_time:76768ms step_avg:39.88ms
step:1926/2330 train_time:76813ms step_avg:39.88ms
step:1927/2330 train_time:76849ms step_avg:39.88ms
step:1928/2330 train_time:76894ms step_avg:39.88ms
step:1929/2330 train_time:76929ms step_avg:39.88ms
step:1930/2330 train_time:76973ms step_avg:39.88ms
step:1931/2330 train_time:77009ms step_avg:39.88ms
step:1932/2330 train_time:77053ms step_avg:39.88ms
step:1933/2330 train_time:77088ms step_avg:39.88ms
step:1934/2330 train_time:77132ms step_avg:39.88ms
step:1935/2330 train_time:77167ms step_avg:39.88ms
step:1936/2330 train_time:77211ms step_avg:39.88ms
step:1937/2330 train_time:77246ms step_avg:39.88ms
step:1938/2330 train_time:77291ms step_avg:39.88ms
step:1939/2330 train_time:77326ms step_avg:39.88ms
step:1940/2330 train_time:77370ms step_avg:39.88ms
step:1941/2330 train_time:77405ms step_avg:39.88ms
step:1942/2330 train_time:77449ms step_avg:39.88ms
step:1943/2330 train_time:77484ms step_avg:39.88ms
step:1944/2330 train_time:77528ms step_avg:39.88ms
step:1945/2330 train_time:77563ms step_avg:39.88ms
step:1946/2330 train_time:77608ms step_avg:39.88ms
step:1947/2330 train_time:77643ms step_avg:39.88ms
step:1948/2330 train_time:77687ms step_avg:39.88ms
step:1949/2330 train_time:77722ms step_avg:39.88ms
step:1950/2330 train_time:77767ms step_avg:39.88ms
step:1951/2330 train_time:77802ms step_avg:39.88ms
step:1952/2330 train_time:77846ms step_avg:39.88ms
step:1953/2330 train_time:77881ms step_avg:39.88ms
step:1954/2330 train_time:77925ms step_avg:39.88ms
step:1955/2330 train_time:77960ms step_avg:39.88ms
step:1956/2330 train_time:78003ms step_avg:39.88ms
step:1957/2330 train_time:78039ms step_avg:39.88ms
step:1958/2330 train_time:78083ms step_avg:39.88ms
step:1959/2330 train_time:78118ms step_avg:39.88ms
step:1960/2330 train_time:78163ms step_avg:39.88ms
step:1961/2330 train_time:78197ms step_avg:39.88ms
step:1962/2330 train_time:78242ms step_avg:39.88ms
step:1963/2330 train_time:78277ms step_avg:39.88ms
step:1964/2330 train_time:78322ms step_avg:39.88ms
step:1965/2330 train_time:78357ms step_avg:39.88ms
step:1966/2330 train_time:78402ms step_avg:39.88ms
step:1967/2330 train_time:78437ms step_avg:39.88ms
step:1968/2330 train_time:78482ms step_avg:39.88ms
step:1969/2330 train_time:78517ms step_avg:39.88ms
step:1970/2330 train_time:78561ms step_avg:39.88ms
step:1971/2330 train_time:78595ms step_avg:39.88ms
step:1972/2330 train_time:78641ms step_avg:39.88ms
step:1973/2330 train_time:78676ms step_avg:39.88ms
step:1974/2330 train_time:78721ms step_avg:39.88ms
step:1975/2330 train_time:78755ms step_avg:39.88ms
step:1976/2330 train_time:78799ms step_avg:39.88ms
step:1977/2330 train_time:78835ms step_avg:39.88ms
step:1978/2330 train_time:78880ms step_avg:39.88ms
step:1979/2330 train_time:78915ms step_avg:39.88ms
step:1980/2330 train_time:78959ms step_avg:39.88ms
step:1981/2330 train_time:78993ms step_avg:39.88ms
step:1982/2330 train_time:79037ms step_avg:39.88ms
step:1983/2330 train_time:79074ms step_avg:39.88ms
step:1984/2330 train_time:79118ms step_avg:39.88ms
step:1985/2330 train_time:79152ms step_avg:39.88ms
step:1986/2330 train_time:79196ms step_avg:39.88ms
step:1987/2330 train_time:79232ms step_avg:39.88ms
step:1988/2330 train_time:79276ms step_avg:39.88ms
step:1989/2330 train_time:79311ms step_avg:39.87ms
step:1990/2330 train_time:79356ms step_avg:39.88ms
step:1991/2330 train_time:79390ms step_avg:39.87ms
step:1992/2330 train_time:79434ms step_avg:39.88ms
step:1993/2330 train_time:79469ms step_avg:39.87ms
step:1994/2330 train_time:79513ms step_avg:39.88ms
step:1995/2330 train_time:79549ms step_avg:39.87ms
step:1996/2330 train_time:79593ms step_avg:39.88ms
step:1997/2330 train_time:79629ms step_avg:39.87ms
step:1998/2330 train_time:79674ms step_avg:39.88ms
step:1999/2330 train_time:79710ms step_avg:39.87ms
step:2000/2330 train_time:79754ms step_avg:39.88ms
step:2000/2330 val_loss:5.0726 train_time:79843ms step_avg:39.92ms
step:2001/2330 train_time:79856ms step_avg:39.91ms
step:2002/2330 train_time:79868ms step_avg:39.89ms
step:2003/2330 train_time:79879ms step_avg:39.88ms
step:2004/2330 train_time:79915ms step_avg:39.88ms
step:2005/2330 train_time:79949ms step_avg:39.87ms
step:2006/2330 train_time:79992ms step_avg:39.88ms
step:2007/2330 train_time:80027ms step_avg:39.87ms
step:2008/2330 train_time:80070ms step_avg:39.88ms
step:2009/2330 train_time:80104ms step_avg:39.87ms
step:2010/2330 train_time:80148ms step_avg:39.87ms
step:2011/2330 train_time:80187ms step_avg:39.87ms
step:2012/2330 train_time:80234ms step_avg:39.88ms
step:2013/2330 train_time:80270ms step_avg:39.88ms
step:2014/2330 train_time:80316ms step_avg:39.88ms
step:2015/2330 train_time:80352ms step_avg:39.88ms
step:2016/2330 train_time:80396ms step_avg:39.88ms
step:2017/2330 train_time:80432ms step_avg:39.88ms
step:2018/2330 train_time:80476ms step_avg:39.88ms
step:2019/2330 train_time:80510ms step_avg:39.88ms
step:2020/2330 train_time:80554ms step_avg:39.88ms
step:2021/2330 train_time:80589ms step_avg:39.88ms
step:2022/2330 train_time:80632ms step_avg:39.88ms
step:2023/2330 train_time:80667ms step_avg:39.87ms
step:2024/2330 train_time:80710ms step_avg:39.88ms
step:2025/2330 train_time:80745ms step_avg:39.87ms
step:2026/2330 train_time:80788ms step_avg:39.88ms
step:2027/2330 train_time:80824ms step_avg:39.87ms
step:2028/2330 train_time:80868ms step_avg:39.88ms
step:2029/2330 train_time:80903ms step_avg:39.87ms
step:2030/2330 train_time:80946ms step_avg:39.87ms
step:2031/2330 train_time:80981ms step_avg:39.87ms
step:2032/2330 train_time:81024ms step_avg:39.87ms
step:2033/2330 train_time:81059ms step_avg:39.87ms
step:2034/2330 train_time:81104ms step_avg:39.87ms
step:2035/2330 train_time:81140ms step_avg:39.87ms
step:2036/2330 train_time:81186ms step_avg:39.88ms
step:2037/2330 train_time:81222ms step_avg:39.87ms
step:2038/2330 train_time:81268ms step_avg:39.88ms
step:2039/2330 train_time:81303ms step_avg:39.87ms
step:2040/2330 train_time:81348ms step_avg:39.88ms
step:2041/2330 train_time:81383ms step_avg:39.87ms
step:2042/2330 train_time:81427ms step_avg:39.88ms
step:2043/2330 train_time:81463ms step_avg:39.87ms
step:2044/2330 train_time:81507ms step_avg:39.88ms
step:2045/2330 train_time:81542ms step_avg:39.87ms
step:2046/2330 train_time:81586ms step_avg:39.88ms
step:2047/2330 train_time:81620ms step_avg:39.87ms
step:2048/2330 train_time:81664ms step_avg:39.88ms
step:2049/2330 train_time:81699ms step_avg:39.87ms
step:2050/2330 train_time:81743ms step_avg:39.87ms
step:2051/2330 train_time:81778ms step_avg:39.87ms
step:2052/2330 train_time:81822ms step_avg:39.87ms
step:2053/2330 train_time:81857ms step_avg:39.87ms
step:2054/2330 train_time:81901ms step_avg:39.87ms
step:2055/2330 train_time:81935ms step_avg:39.87ms
step:2056/2330 train_time:81980ms step_avg:39.87ms
step:2057/2330 train_time:82014ms step_avg:39.87ms
step:2058/2330 train_time:82058ms step_avg:39.87ms
step:2059/2330 train_time:82093ms step_avg:39.87ms
step:2060/2330 train_time:82137ms step_avg:39.87ms
step:2061/2330 train_time:82173ms step_avg:39.87ms
step:2062/2330 train_time:82217ms step_avg:39.87ms
step:2063/2330 train_time:82253ms step_avg:39.87ms
step:2064/2330 train_time:82297ms step_avg:39.87ms
step:2065/2330 train_time:82333ms step_avg:39.87ms
step:2066/2330 train_time:82378ms step_avg:39.87ms
step:2067/2330 train_time:82414ms step_avg:39.87ms
step:2068/2330 train_time:82458ms step_avg:39.87ms
step:2069/2330 train_time:82493ms step_avg:39.87ms
step:2070/2330 train_time:82537ms step_avg:39.87ms
step:2071/2330 train_time:82572ms step_avg:39.87ms
step:2072/2330 train_time:82616ms step_avg:39.87ms
step:2073/2330 train_time:82651ms step_avg:39.87ms
step:2074/2330 train_time:82695ms step_avg:39.87ms
step:2075/2330 train_time:82731ms step_avg:39.87ms
step:2076/2330 train_time:82775ms step_avg:39.87ms
step:2077/2330 train_time:82809ms step_avg:39.87ms
step:2078/2330 train_time:82853ms step_avg:39.87ms
step:2079/2330 train_time:82887ms step_avg:39.87ms
step:2080/2330 train_time:82932ms step_avg:39.87ms
step:2081/2330 train_time:82966ms step_avg:39.87ms
step:2082/2330 train_time:83011ms step_avg:39.87ms
step:2083/2330 train_time:83046ms step_avg:39.87ms
step:2084/2330 train_time:83089ms step_avg:39.87ms
step:2085/2330 train_time:83125ms step_avg:39.87ms
step:2086/2330 train_time:83169ms step_avg:39.87ms
step:2087/2330 train_time:83204ms step_avg:39.87ms
step:2088/2330 train_time:83249ms step_avg:39.87ms
step:2089/2330 train_time:83284ms step_avg:39.87ms
step:2090/2330 train_time:83329ms step_avg:39.87ms
step:2091/2330 train_time:83364ms step_avg:39.87ms
step:2092/2330 train_time:83408ms step_avg:39.87ms
step:2093/2330 train_time:83443ms step_avg:39.87ms
step:2094/2330 train_time:83489ms step_avg:39.87ms
step:2095/2330 train_time:83524ms step_avg:39.87ms
step:2096/2330 train_time:83568ms step_avg:39.87ms
step:2097/2330 train_time:83602ms step_avg:39.87ms
step:2098/2330 train_time:83648ms step_avg:39.87ms
step:2099/2330 train_time:83683ms step_avg:39.87ms
step:2100/2330 train_time:83726ms step_avg:39.87ms
step:2101/2330 train_time:83761ms step_avg:39.87ms
step:2102/2330 train_time:83805ms step_avg:39.87ms
step:2103/2330 train_time:83840ms step_avg:39.87ms
step:2104/2330 train_time:83884ms step_avg:39.87ms
step:2105/2330 train_time:83919ms step_avg:39.87ms
step:2106/2330 train_time:83962ms step_avg:39.87ms
step:2107/2330 train_time:83997ms step_avg:39.87ms
step:2108/2330 train_time:84041ms step_avg:39.87ms
step:2109/2330 train_time:84076ms step_avg:39.87ms
step:2110/2330 train_time:84121ms step_avg:39.87ms
step:2111/2330 train_time:84156ms step_avg:39.87ms
step:2112/2330 train_time:84201ms step_avg:39.87ms
step:2113/2330 train_time:84236ms step_avg:39.87ms
step:2114/2330 train_time:84281ms step_avg:39.87ms
step:2115/2330 train_time:84316ms step_avg:39.87ms
step:2116/2330 train_time:84360ms step_avg:39.87ms
step:2117/2330 train_time:84395ms step_avg:39.87ms
step:2118/2330 train_time:84440ms step_avg:39.87ms
step:2119/2330 train_time:84475ms step_avg:39.87ms
step:2120/2330 train_time:84519ms step_avg:39.87ms
step:2121/2330 train_time:84555ms step_avg:39.87ms
step:2122/2330 train_time:84599ms step_avg:39.87ms
step:2123/2330 train_time:84634ms step_avg:39.87ms
step:2124/2330 train_time:84678ms step_avg:39.87ms
step:2125/2330 train_time:84712ms step_avg:39.86ms
step:2126/2330 train_time:84756ms step_avg:39.87ms
step:2127/2330 train_time:84791ms step_avg:39.86ms
step:2128/2330 train_time:84835ms step_avg:39.87ms
step:2129/2330 train_time:84870ms step_avg:39.86ms
step:2130/2330 train_time:84913ms step_avg:39.87ms
step:2131/2330 train_time:84948ms step_avg:39.86ms
step:2132/2330 train_time:84991ms step_avg:39.86ms
step:2133/2330 train_time:85026ms step_avg:39.86ms
step:2134/2330 train_time:85071ms step_avg:39.86ms
step:2135/2330 train_time:85105ms step_avg:39.86ms
step:2136/2330 train_time:85149ms step_avg:39.86ms
step:2137/2330 train_time:85184ms step_avg:39.86ms
step:2138/2330 train_time:85228ms step_avg:39.86ms
step:2139/2330 train_time:85263ms step_avg:39.86ms
step:2140/2330 train_time:85307ms step_avg:39.86ms
step:2141/2330 train_time:85343ms step_avg:39.86ms
step:2142/2330 train_time:85387ms step_avg:39.86ms
step:2143/2330 train_time:85423ms step_avg:39.86ms
step:2144/2330 train_time:85467ms step_avg:39.86ms
step:2145/2330 train_time:85502ms step_avg:39.86ms
step:2146/2330 train_time:85547ms step_avg:39.86ms
step:2147/2330 train_time:85583ms step_avg:39.86ms
step:2148/2330 train_time:85628ms step_avg:39.86ms
step:2149/2330 train_time:85663ms step_avg:39.86ms
step:2150/2330 train_time:85707ms step_avg:39.86ms
step:2151/2330 train_time:85742ms step_avg:39.86ms
step:2152/2330 train_time:85787ms step_avg:39.86ms
step:2153/2330 train_time:85822ms step_avg:39.86ms
step:2154/2330 train_time:85866ms step_avg:39.86ms
step:2155/2330 train_time:85900ms step_avg:39.86ms
step:2156/2330 train_time:85944ms step_avg:39.86ms
step:2157/2330 train_time:85979ms step_avg:39.86ms
step:2158/2330 train_time:86023ms step_avg:39.86ms
step:2159/2330 train_time:86058ms step_avg:39.86ms
step:2160/2330 train_time:86102ms step_avg:39.86ms
step:2161/2330 train_time:86137ms step_avg:39.86ms
step:2162/2330 train_time:86181ms step_avg:39.86ms
step:2163/2330 train_time:86216ms step_avg:39.86ms
step:2164/2330 train_time:86260ms step_avg:39.86ms
step:2165/2330 train_time:86295ms step_avg:39.86ms
step:2166/2330 train_time:86341ms step_avg:39.86ms
step:2167/2330 train_time:86375ms step_avg:39.86ms
step:2168/2330 train_time:86420ms step_avg:39.86ms
step:2169/2330 train_time:86456ms step_avg:39.86ms
step:2170/2330 train_time:86500ms step_avg:39.86ms
step:2171/2330 train_time:86535ms step_avg:39.86ms
step:2172/2330 train_time:86580ms step_avg:39.86ms
step:2173/2330 train_time:86615ms step_avg:39.86ms
step:2174/2330 train_time:86659ms step_avg:39.86ms
step:2175/2330 train_time:86693ms step_avg:39.86ms
step:2176/2330 train_time:86738ms step_avg:39.86ms
step:2177/2330 train_time:86772ms step_avg:39.86ms
step:2178/2330 train_time:86816ms step_avg:39.86ms
step:2179/2330 train_time:86851ms step_avg:39.86ms
step:2180/2330 train_time:86895ms step_avg:39.86ms
step:2181/2330 train_time:86930ms step_avg:39.86ms
step:2182/2330 train_time:86974ms step_avg:39.86ms
step:2183/2330 train_time:87009ms step_avg:39.86ms
step:2184/2330 train_time:87053ms step_avg:39.86ms
step:2185/2330 train_time:87089ms step_avg:39.86ms
step:2186/2330 train_time:87132ms step_avg:39.86ms
step:2187/2330 train_time:87167ms step_avg:39.86ms
step:2188/2330 train_time:87212ms step_avg:39.86ms
step:2189/2330 train_time:87247ms step_avg:39.86ms
step:2190/2330 train_time:87292ms step_avg:39.86ms
step:2191/2330 train_time:87327ms step_avg:39.86ms
step:2192/2330 train_time:87372ms step_avg:39.86ms
step:2193/2330 train_time:87407ms step_avg:39.86ms
step:2194/2330 train_time:87452ms step_avg:39.86ms
step:2195/2330 train_time:87487ms step_avg:39.86ms
step:2196/2330 train_time:87531ms step_avg:39.86ms
step:2197/2330 train_time:87566ms step_avg:39.86ms
step:2198/2330 train_time:87610ms step_avg:39.86ms
step:2199/2330 train_time:87645ms step_avg:39.86ms
step:2200/2330 train_time:87689ms step_avg:39.86ms
step:2201/2330 train_time:87724ms step_avg:39.86ms
step:2202/2330 train_time:87768ms step_avg:39.86ms
step:2203/2330 train_time:87803ms step_avg:39.86ms
step:2204/2330 train_time:87849ms step_avg:39.86ms
step:2205/2330 train_time:87883ms step_avg:39.86ms
step:2206/2330 train_time:87928ms step_avg:39.86ms
step:2207/2330 train_time:87962ms step_avg:39.86ms
step:2208/2330 train_time:88007ms step_avg:39.86ms
step:2209/2330 train_time:88042ms step_avg:39.86ms
step:2210/2330 train_time:88086ms step_avg:39.86ms
step:2211/2330 train_time:88121ms step_avg:39.86ms
step:2212/2330 train_time:88166ms step_avg:39.86ms
step:2213/2330 train_time:88200ms step_avg:39.86ms
step:2214/2330 train_time:88245ms step_avg:39.86ms
step:2215/2330 train_time:88280ms step_avg:39.86ms
step:2216/2330 train_time:88325ms step_avg:39.86ms
step:2217/2330 train_time:88360ms step_avg:39.86ms
step:2218/2330 train_time:88405ms step_avg:39.86ms
step:2219/2330 train_time:88439ms step_avg:39.86ms
step:2220/2330 train_time:88484ms step_avg:39.86ms
step:2221/2330 train_time:88519ms step_avg:39.86ms
step:2222/2330 train_time:88563ms step_avg:39.86ms
step:2223/2330 train_time:88599ms step_avg:39.86ms
step:2224/2330 train_time:88643ms step_avg:39.86ms
step:2225/2330 train_time:88678ms step_avg:39.86ms
step:2226/2330 train_time:88722ms step_avg:39.86ms
step:2227/2330 train_time:88757ms step_avg:39.85ms
step:2228/2330 train_time:88801ms step_avg:39.86ms
step:2229/2330 train_time:88836ms step_avg:39.85ms
step:2230/2330 train_time:88880ms step_avg:39.86ms
step:2231/2330 train_time:88916ms step_avg:39.85ms
step:2232/2330 train_time:88960ms step_avg:39.86ms
step:2233/2330 train_time:88995ms step_avg:39.85ms
step:2234/2330 train_time:89039ms step_avg:39.86ms
step:2235/2330 train_time:89073ms step_avg:39.85ms
step:2236/2330 train_time:89116ms step_avg:39.86ms
step:2237/2330 train_time:89151ms step_avg:39.85ms
step:2238/2330 train_time:89195ms step_avg:39.85ms
step:2239/2330 train_time:89230ms step_avg:39.85ms
step:2240/2330 train_time:89275ms step_avg:39.85ms
step:2241/2330 train_time:89310ms step_avg:39.85ms
step:2242/2330 train_time:89354ms step_avg:39.85ms
step:2243/2330 train_time:89389ms step_avg:39.85ms
step:2244/2330 train_time:89433ms step_avg:39.85ms
step:2245/2330 train_time:89468ms step_avg:39.85ms
step:2246/2330 train_time:89513ms step_avg:39.85ms
step:2247/2330 train_time:89548ms step_avg:39.85ms
step:2248/2330 train_time:89592ms step_avg:39.85ms
step:2249/2330 train_time:89627ms step_avg:39.85ms
step:2250/2330 train_time:89671ms step_avg:39.85ms
step:2250/2330 val_loss:5.0474 train_time:89757ms step_avg:39.89ms
step:2251/2330 train_time:89770ms step_avg:39.88ms
step:2252/2330 train_time:89788ms step_avg:39.87ms
step:2253/2330 train_time:89799ms step_avg:39.86ms
step:2254/2330 train_time:89831ms step_avg:39.85ms
step:2255/2330 train_time:89865ms step_avg:39.85ms
step:2256/2330 train_time:89909ms step_avg:39.85ms
step:2257/2330 train_time:89943ms step_avg:39.85ms
step:2258/2330 train_time:89986ms step_avg:39.85ms
step:2259/2330 train_time:90020ms step_avg:39.85ms
step:2260/2330 train_time:90064ms step_avg:39.85ms
step:2261/2330 train_time:90101ms step_avg:39.85ms
step:2262/2330 train_time:90149ms step_avg:39.85ms
step:2263/2330 train_time:90184ms step_avg:39.85ms
step:2264/2330 train_time:90230ms step_avg:39.85ms
step:2265/2330 train_time:90265ms step_avg:39.85ms
step:2266/2330 train_time:90310ms step_avg:39.85ms
step:2267/2330 train_time:90344ms step_avg:39.85ms
step:2268/2330 train_time:90388ms step_avg:39.85ms
step:2269/2330 train_time:90423ms step_avg:39.85ms
step:2270/2330 train_time:90466ms step_avg:39.85ms
step:2271/2330 train_time:90501ms step_avg:39.85ms
step:2272/2330 train_time:90545ms step_avg:39.85ms
step:2273/2330 train_time:90581ms step_avg:39.85ms
step:2274/2330 train_time:90625ms step_avg:39.85ms
step:2275/2330 train_time:90659ms step_avg:39.85ms
step:2276/2330 train_time:90704ms step_avg:39.85ms
step:2277/2330 train_time:90739ms step_avg:39.85ms
step:2278/2330 train_time:90783ms step_avg:39.85ms
step:2279/2330 train_time:90818ms step_avg:39.85ms
step:2280/2330 train_time:90862ms step_avg:39.85ms
step:2281/2330 train_time:90896ms step_avg:39.85ms
step:2282/2330 train_time:90941ms step_avg:39.85ms
step:2283/2330 train_time:90976ms step_avg:39.85ms
step:2284/2330 train_time:91020ms step_avg:39.85ms
step:2285/2330 train_time:91055ms step_avg:39.85ms
step:2286/2330 train_time:91100ms step_avg:39.85ms
step:2287/2330 train_time:91135ms step_avg:39.85ms
step:2288/2330 train_time:91180ms step_avg:39.85ms
step:2289/2330 train_time:91215ms step_avg:39.85ms
step:2290/2330 train_time:91260ms step_avg:39.85ms
step:2291/2330 train_time:91295ms step_avg:39.85ms
step:2292/2330 train_time:91339ms step_avg:39.85ms
step:2293/2330 train_time:91374ms step_avg:39.85ms
step:2294/2330 train_time:91419ms step_avg:39.85ms
step:2295/2330 train_time:91454ms step_avg:39.85ms
step:2296/2330 train_time:91499ms step_avg:39.85ms
step:2297/2330 train_time:91534ms step_avg:39.85ms
step:2298/2330 train_time:91577ms step_avg:39.85ms
step:2299/2330 train_time:91613ms step_avg:39.85ms
step:2300/2330 train_time:91657ms step_avg:39.85ms
step:2301/2330 train_time:91692ms step_avg:39.85ms
step:2302/2330 train_time:91736ms step_avg:39.85ms
step:2303/2330 train_time:91771ms step_avg:39.85ms
step:2304/2330 train_time:91815ms step_avg:39.85ms
step:2305/2330 train_time:91850ms step_avg:39.85ms
step:2306/2330 train_time:91895ms step_avg:39.85ms
step:2307/2330 train_time:91929ms step_avg:39.85ms
step:2308/2330 train_time:91973ms step_avg:39.85ms
step:2309/2330 train_time:92009ms step_avg:39.85ms
step:2310/2330 train_time:92053ms step_avg:39.85ms
step:2311/2330 train_time:92088ms step_avg:39.85ms
step:2312/2330 train_time:92133ms step_avg:39.85ms
step:2313/2330 train_time:92167ms step_avg:39.85ms
step:2314/2330 train_time:92212ms step_avg:39.85ms
step:2315/2330 train_time:92248ms step_avg:39.85ms
step:2316/2330 train_time:92292ms step_avg:39.85ms
step:2317/2330 train_time:92327ms step_avg:39.85ms
step:2318/2330 train_time:92371ms step_avg:39.85ms
step:2319/2330 train_time:92408ms step_avg:39.85ms
step:2320/2330 train_time:92452ms step_avg:39.85ms
step:2321/2330 train_time:92486ms step_avg:39.85ms
step:2322/2330 train_time:92530ms step_avg:39.85ms
step:2323/2330 train_time:92565ms step_avg:39.85ms
step:2324/2330 train_time:92610ms step_avg:39.85ms
step:2325/2330 train_time:92645ms step_avg:39.85ms
step:2326/2330 train_time:92690ms step_avg:39.85ms
step:2327/2330 train_time:92724ms step_avg:39.85ms
step:2328/2330 train_time:92768ms step_avg:39.85ms
step:2329/2330 train_time:92803ms step_avg:39.85ms
step:2330/2330 train_time:92847ms step_avg:39.85ms
step:2330/2330 val_loss:5.0407 train_time:92933ms step_avg:39.89ms
peak memory allocated: 29494 MiB reserved: 38888 MiB
