import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = divide_by_norm(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = divide_by_norm(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "gd_3e-2"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=3e-2, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 03:21:08 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   33C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   31C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   31C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             115W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:80ms step_avg:80.10ms
step:2/2330 train_time:185ms step_avg:92.34ms
step:3/2330 train_time:205ms step_avg:68.40ms
step:4/2330 train_time:234ms step_avg:58.49ms
step:5/2330 train_time:291ms step_avg:58.19ms
step:6/2330 train_time:351ms step_avg:58.45ms
step:7/2330 train_time:409ms step_avg:58.47ms
step:8/2330 train_time:470ms step_avg:58.76ms
step:9/2330 train_time:528ms step_avg:58.65ms
step:10/2330 train_time:589ms step_avg:58.87ms
step:11/2330 train_time:647ms step_avg:58.78ms
step:12/2330 train_time:708ms step_avg:58.99ms
step:13/2330 train_time:766ms step_avg:58.89ms
step:14/2330 train_time:826ms step_avg:59.00ms
step:15/2330 train_time:884ms step_avg:58.97ms
step:16/2330 train_time:945ms step_avg:59.09ms
step:17/2330 train_time:1004ms step_avg:59.04ms
step:18/2330 train_time:1066ms step_avg:59.24ms
step:19/2330 train_time:1129ms step_avg:59.42ms
step:20/2330 train_time:1195ms step_avg:59.76ms
step:21/2330 train_time:1256ms step_avg:59.80ms
step:22/2330 train_time:1318ms step_avg:59.90ms
step:23/2330 train_time:1377ms step_avg:59.86ms
step:24/2330 train_time:1438ms step_avg:59.90ms
step:25/2330 train_time:1497ms step_avg:59.87ms
step:26/2330 train_time:1558ms step_avg:59.94ms
step:27/2330 train_time:1618ms step_avg:59.93ms
step:28/2330 train_time:1681ms step_avg:60.03ms
step:29/2330 train_time:1740ms step_avg:60.00ms
step:30/2330 train_time:1801ms step_avg:60.03ms
step:31/2330 train_time:1859ms step_avg:59.98ms
step:32/2330 train_time:1921ms step_avg:60.04ms
step:33/2330 train_time:1980ms step_avg:59.99ms
step:34/2330 train_time:2042ms step_avg:60.06ms
step:35/2330 train_time:2101ms step_avg:60.04ms
step:36/2330 train_time:2163ms step_avg:60.09ms
step:37/2330 train_time:2222ms step_avg:60.06ms
step:38/2330 train_time:2285ms step_avg:60.12ms
step:39/2330 train_time:2343ms step_avg:60.09ms
step:40/2330 train_time:2404ms step_avg:60.10ms
step:41/2330 train_time:2463ms step_avg:60.08ms
step:42/2330 train_time:2526ms step_avg:60.13ms
step:43/2330 train_time:2585ms step_avg:60.11ms
step:44/2330 train_time:2646ms step_avg:60.15ms
step:45/2330 train_time:2706ms step_avg:60.12ms
step:46/2330 train_time:2768ms step_avg:60.17ms
step:47/2330 train_time:2826ms step_avg:60.12ms
step:48/2330 train_time:2888ms step_avg:60.17ms
step:49/2330 train_time:2947ms step_avg:60.15ms
step:50/2330 train_time:3010ms step_avg:60.19ms
step:51/2330 train_time:3068ms step_avg:60.16ms
step:52/2330 train_time:3130ms step_avg:60.19ms
step:53/2330 train_time:3189ms step_avg:60.17ms
step:54/2330 train_time:3251ms step_avg:60.20ms
step:55/2330 train_time:3310ms step_avg:60.18ms
step:56/2330 train_time:3372ms step_avg:60.21ms
step:57/2330 train_time:3430ms step_avg:60.18ms
step:58/2330 train_time:3493ms step_avg:60.23ms
step:59/2330 train_time:3552ms step_avg:60.20ms
step:60/2330 train_time:3614ms step_avg:60.23ms
step:61/2330 train_time:3672ms step_avg:60.20ms
step:62/2330 train_time:3734ms step_avg:60.22ms
step:63/2330 train_time:3793ms step_avg:60.20ms
step:64/2330 train_time:3854ms step_avg:60.22ms
step:65/2330 train_time:3915ms step_avg:60.23ms
step:66/2330 train_time:3978ms step_avg:60.27ms
step:67/2330 train_time:4038ms step_avg:60.26ms
step:68/2330 train_time:4100ms step_avg:60.29ms
step:69/2330 train_time:4160ms step_avg:60.29ms
step:70/2330 train_time:4222ms step_avg:60.32ms
step:71/2330 train_time:4282ms step_avg:60.31ms
step:72/2330 train_time:4343ms step_avg:60.32ms
step:73/2330 train_time:4401ms step_avg:60.29ms
step:74/2330 train_time:4463ms step_avg:60.31ms
step:75/2330 train_time:4522ms step_avg:60.29ms
step:76/2330 train_time:4583ms step_avg:60.30ms
step:77/2330 train_time:4642ms step_avg:60.29ms
step:78/2330 train_time:4704ms step_avg:60.31ms
step:79/2330 train_time:4763ms step_avg:60.29ms
step:80/2330 train_time:4825ms step_avg:60.31ms
step:81/2330 train_time:4884ms step_avg:60.30ms
step:82/2330 train_time:4947ms step_avg:60.33ms
step:83/2330 train_time:5006ms step_avg:60.32ms
step:84/2330 train_time:5069ms step_avg:60.35ms
step:85/2330 train_time:5128ms step_avg:60.33ms
step:86/2330 train_time:5190ms step_avg:60.35ms
step:87/2330 train_time:5249ms step_avg:60.33ms
step:88/2330 train_time:5312ms step_avg:60.36ms
step:89/2330 train_time:5370ms step_avg:60.34ms
step:90/2330 train_time:5432ms step_avg:60.36ms
step:91/2330 train_time:5491ms step_avg:60.34ms
step:92/2330 train_time:5553ms step_avg:60.36ms
step:93/2330 train_time:5612ms step_avg:60.34ms
step:94/2330 train_time:5674ms step_avg:60.36ms
step:95/2330 train_time:5733ms step_avg:60.34ms
step:96/2330 train_time:5795ms step_avg:60.36ms
step:97/2330 train_time:5854ms step_avg:60.35ms
step:98/2330 train_time:5918ms step_avg:60.38ms
step:99/2330 train_time:5977ms step_avg:60.37ms
step:100/2330 train_time:6040ms step_avg:60.40ms
step:101/2330 train_time:6099ms step_avg:60.39ms
step:102/2330 train_time:6161ms step_avg:60.41ms
step:103/2330 train_time:6222ms step_avg:60.40ms
step:104/2330 train_time:6284ms step_avg:60.42ms
step:105/2330 train_time:6342ms step_avg:60.40ms
step:106/2330 train_time:6404ms step_avg:60.41ms
step:107/2330 train_time:6462ms step_avg:60.40ms
step:108/2330 train_time:6523ms step_avg:60.40ms
step:109/2330 train_time:6582ms step_avg:60.39ms
step:110/2330 train_time:6643ms step_avg:60.39ms
step:111/2330 train_time:6702ms step_avg:60.38ms
step:112/2330 train_time:6764ms step_avg:60.39ms
step:113/2330 train_time:6823ms step_avg:60.38ms
step:114/2330 train_time:6885ms step_avg:60.40ms
step:115/2330 train_time:6944ms step_avg:60.39ms
step:116/2330 train_time:7007ms step_avg:60.41ms
step:117/2330 train_time:7066ms step_avg:60.40ms
step:118/2330 train_time:7129ms step_avg:60.41ms
step:119/2330 train_time:7188ms step_avg:60.40ms
step:120/2330 train_time:7250ms step_avg:60.42ms
step:121/2330 train_time:7309ms step_avg:60.40ms
step:122/2330 train_time:7371ms step_avg:60.41ms
step:123/2330 train_time:7429ms step_avg:60.40ms
step:124/2330 train_time:7491ms step_avg:60.41ms
step:125/2330 train_time:7549ms step_avg:60.40ms
step:126/2330 train_time:7611ms step_avg:60.41ms
step:127/2330 train_time:7670ms step_avg:60.40ms
step:128/2330 train_time:7732ms step_avg:60.41ms
step:129/2330 train_time:7790ms step_avg:60.39ms
step:130/2330 train_time:7854ms step_avg:60.41ms
step:131/2330 train_time:7913ms step_avg:60.40ms
step:132/2330 train_time:7975ms step_avg:60.42ms
step:133/2330 train_time:8035ms step_avg:60.41ms
step:134/2330 train_time:8097ms step_avg:60.43ms
step:135/2330 train_time:8157ms step_avg:60.42ms
step:136/2330 train_time:8218ms step_avg:60.43ms
step:137/2330 train_time:8280ms step_avg:60.44ms
step:138/2330 train_time:8342ms step_avg:60.45ms
step:139/2330 train_time:8400ms step_avg:60.43ms
step:140/2330 train_time:8463ms step_avg:60.45ms
step:141/2330 train_time:8521ms step_avg:60.43ms
step:142/2330 train_time:8584ms step_avg:60.45ms
step:143/2330 train_time:8642ms step_avg:60.43ms
step:144/2330 train_time:8703ms step_avg:60.44ms
step:145/2330 train_time:8761ms step_avg:60.42ms
step:146/2330 train_time:8823ms step_avg:60.43ms
step:147/2330 train_time:8883ms step_avg:60.43ms
step:148/2330 train_time:8944ms step_avg:60.43ms
step:149/2330 train_time:9003ms step_avg:60.42ms
step:150/2330 train_time:9066ms step_avg:60.44ms
step:151/2330 train_time:9125ms step_avg:60.43ms
step:152/2330 train_time:9188ms step_avg:60.45ms
step:153/2330 train_time:9246ms step_avg:60.43ms
step:154/2330 train_time:9309ms step_avg:60.45ms
step:155/2330 train_time:9368ms step_avg:60.44ms
step:156/2330 train_time:9431ms step_avg:60.45ms
step:157/2330 train_time:9489ms step_avg:60.44ms
step:158/2330 train_time:9551ms step_avg:60.45ms
step:159/2330 train_time:9610ms step_avg:60.44ms
step:160/2330 train_time:9672ms step_avg:60.45ms
step:161/2330 train_time:9730ms step_avg:60.43ms
step:162/2330 train_time:9792ms step_avg:60.44ms
step:163/2330 train_time:9850ms step_avg:60.43ms
step:164/2330 train_time:9912ms step_avg:60.44ms
step:165/2330 train_time:9970ms step_avg:60.43ms
step:166/2330 train_time:10033ms step_avg:60.44ms
step:167/2330 train_time:10092ms step_avg:60.43ms
step:168/2330 train_time:10154ms step_avg:60.44ms
step:169/2330 train_time:10214ms step_avg:60.44ms
step:170/2330 train_time:10277ms step_avg:60.45ms
step:171/2330 train_time:10337ms step_avg:60.45ms
step:172/2330 train_time:10399ms step_avg:60.46ms
step:173/2330 train_time:10459ms step_avg:60.46ms
step:174/2330 train_time:10521ms step_avg:60.47ms
step:175/2330 train_time:10581ms step_avg:60.46ms
step:176/2330 train_time:10642ms step_avg:60.47ms
step:177/2330 train_time:10701ms step_avg:60.46ms
step:178/2330 train_time:10763ms step_avg:60.47ms
step:179/2330 train_time:10822ms step_avg:60.46ms
step:180/2330 train_time:10884ms step_avg:60.46ms
step:181/2330 train_time:10943ms step_avg:60.46ms
step:182/2330 train_time:11003ms step_avg:60.46ms
step:183/2330 train_time:11062ms step_avg:60.45ms
step:184/2330 train_time:11124ms step_avg:60.46ms
step:185/2330 train_time:11185ms step_avg:60.46ms
step:186/2330 train_time:11247ms step_avg:60.47ms
step:187/2330 train_time:11306ms step_avg:60.46ms
step:188/2330 train_time:11369ms step_avg:60.47ms
step:189/2330 train_time:11428ms step_avg:60.47ms
step:190/2330 train_time:11491ms step_avg:60.48ms
step:191/2330 train_time:11550ms step_avg:60.47ms
step:192/2330 train_time:11613ms step_avg:60.48ms
step:193/2330 train_time:11671ms step_avg:60.47ms
step:194/2330 train_time:11733ms step_avg:60.48ms
step:195/2330 train_time:11791ms step_avg:60.47ms
step:196/2330 train_time:11853ms step_avg:60.47ms
step:197/2330 train_time:11912ms step_avg:60.47ms
step:198/2330 train_time:11974ms step_avg:60.48ms
step:199/2330 train_time:12033ms step_avg:60.47ms
step:200/2330 train_time:12096ms step_avg:60.48ms
step:201/2330 train_time:12156ms step_avg:60.48ms
step:202/2330 train_time:12218ms step_avg:60.49ms
step:203/2330 train_time:12278ms step_avg:60.48ms
step:204/2330 train_time:12341ms step_avg:60.49ms
step:205/2330 train_time:12400ms step_avg:60.49ms
step:206/2330 train_time:12462ms step_avg:60.50ms
step:207/2330 train_time:12522ms step_avg:60.49ms
step:208/2330 train_time:12584ms step_avg:60.50ms
step:209/2330 train_time:12642ms step_avg:60.49ms
step:210/2330 train_time:12705ms step_avg:60.50ms
step:211/2330 train_time:12763ms step_avg:60.49ms
step:212/2330 train_time:12825ms step_avg:60.49ms
step:213/2330 train_time:12883ms step_avg:60.48ms
step:214/2330 train_time:12945ms step_avg:60.49ms
step:215/2330 train_time:13004ms step_avg:60.48ms
step:216/2330 train_time:13066ms step_avg:60.49ms
step:217/2330 train_time:13125ms step_avg:60.48ms
step:218/2330 train_time:13188ms step_avg:60.49ms
step:219/2330 train_time:13247ms step_avg:60.49ms
step:220/2330 train_time:13309ms step_avg:60.50ms
step:221/2330 train_time:13368ms step_avg:60.49ms
step:222/2330 train_time:13430ms step_avg:60.50ms
step:223/2330 train_time:13490ms step_avg:60.49ms
step:224/2330 train_time:13552ms step_avg:60.50ms
step:225/2330 train_time:13611ms step_avg:60.49ms
step:226/2330 train_time:13673ms step_avg:60.50ms
step:227/2330 train_time:13731ms step_avg:60.49ms
step:228/2330 train_time:13793ms step_avg:60.50ms
step:229/2330 train_time:13852ms step_avg:60.49ms
step:230/2330 train_time:13914ms step_avg:60.49ms
step:231/2330 train_time:13972ms step_avg:60.49ms
step:232/2330 train_time:14035ms step_avg:60.49ms
step:233/2330 train_time:14095ms step_avg:60.50ms
step:234/2330 train_time:14158ms step_avg:60.50ms
step:235/2330 train_time:14218ms step_avg:60.50ms
step:236/2330 train_time:14282ms step_avg:60.52ms
step:237/2330 train_time:14341ms step_avg:60.51ms
step:238/2330 train_time:14403ms step_avg:60.52ms
step:239/2330 train_time:14462ms step_avg:60.51ms
step:240/2330 train_time:14524ms step_avg:60.52ms
step:241/2330 train_time:14583ms step_avg:60.51ms
step:242/2330 train_time:14645ms step_avg:60.51ms
step:243/2330 train_time:14704ms step_avg:60.51ms
step:244/2330 train_time:14765ms step_avg:60.51ms
step:245/2330 train_time:14823ms step_avg:60.50ms
step:246/2330 train_time:14887ms step_avg:60.51ms
step:247/2330 train_time:14945ms step_avg:60.51ms
step:248/2330 train_time:15006ms step_avg:60.51ms
step:249/2330 train_time:15065ms step_avg:60.50ms
step:250/2330 train_time:15127ms step_avg:60.51ms
step:250/2330 val_loss:4.8927 train_time:15201ms step_avg:60.81ms
step:251/2330 train_time:15222ms step_avg:60.65ms
step:252/2330 train_time:15252ms step_avg:60.52ms
step:253/2330 train_time:15312ms step_avg:60.52ms
step:254/2330 train_time:15380ms step_avg:60.55ms
step:255/2330 train_time:15444ms step_avg:60.56ms
step:256/2330 train_time:15507ms step_avg:60.58ms
step:257/2330 train_time:15566ms step_avg:60.57ms
step:258/2330 train_time:15628ms step_avg:60.57ms
step:259/2330 train_time:15687ms step_avg:60.57ms
step:260/2330 train_time:15749ms step_avg:60.57ms
step:261/2330 train_time:15807ms step_avg:60.56ms
step:262/2330 train_time:15869ms step_avg:60.57ms
step:263/2330 train_time:15928ms step_avg:60.56ms
step:264/2330 train_time:15990ms step_avg:60.57ms
step:265/2330 train_time:16049ms step_avg:60.56ms
step:266/2330 train_time:16110ms step_avg:60.56ms
step:267/2330 train_time:16170ms step_avg:60.56ms
step:268/2330 train_time:16232ms step_avg:60.57ms
step:269/2330 train_time:16292ms step_avg:60.57ms
step:270/2330 train_time:16356ms step_avg:60.58ms
step:271/2330 train_time:16415ms step_avg:60.57ms
step:272/2330 train_time:16479ms step_avg:60.58ms
step:273/2330 train_time:16538ms step_avg:60.58ms
step:274/2330 train_time:16600ms step_avg:60.59ms
step:275/2330 train_time:16660ms step_avg:60.58ms
step:276/2330 train_time:16721ms step_avg:60.58ms
step:277/2330 train_time:16780ms step_avg:60.58ms
step:278/2330 train_time:16842ms step_avg:60.58ms
step:279/2330 train_time:16900ms step_avg:60.57ms
step:280/2330 train_time:16962ms step_avg:60.58ms
step:281/2330 train_time:17021ms step_avg:60.57ms
step:282/2330 train_time:17083ms step_avg:60.58ms
step:283/2330 train_time:17142ms step_avg:60.57ms
step:284/2330 train_time:17206ms step_avg:60.58ms
step:285/2330 train_time:17266ms step_avg:60.58ms
step:286/2330 train_time:17329ms step_avg:60.59ms
step:287/2330 train_time:17389ms step_avg:60.59ms
step:288/2330 train_time:17453ms step_avg:60.60ms
step:289/2330 train_time:17512ms step_avg:60.59ms
step:290/2330 train_time:17573ms step_avg:60.60ms
step:291/2330 train_time:17631ms step_avg:60.59ms
step:292/2330 train_time:17693ms step_avg:60.59ms
step:293/2330 train_time:17752ms step_avg:60.59ms
step:294/2330 train_time:17814ms step_avg:60.59ms
step:295/2330 train_time:17873ms step_avg:60.59ms
step:296/2330 train_time:17935ms step_avg:60.59ms
step:297/2330 train_time:17993ms step_avg:60.58ms
step:298/2330 train_time:18056ms step_avg:60.59ms
step:299/2330 train_time:18115ms step_avg:60.58ms
step:300/2330 train_time:18177ms step_avg:60.59ms
step:301/2330 train_time:18236ms step_avg:60.58ms
step:302/2330 train_time:18298ms step_avg:60.59ms
step:303/2330 train_time:18358ms step_avg:60.59ms
step:304/2330 train_time:18421ms step_avg:60.59ms
step:305/2330 train_time:18479ms step_avg:60.59ms
step:306/2330 train_time:18541ms step_avg:60.59ms
step:307/2330 train_time:18599ms step_avg:60.58ms
step:308/2330 train_time:18662ms step_avg:60.59ms
step:309/2330 train_time:18722ms step_avg:60.59ms
step:310/2330 train_time:18784ms step_avg:60.59ms
step:311/2330 train_time:18843ms step_avg:60.59ms
step:312/2330 train_time:18906ms step_avg:60.60ms
step:313/2330 train_time:18965ms step_avg:60.59ms
step:314/2330 train_time:19027ms step_avg:60.60ms
step:315/2330 train_time:19086ms step_avg:60.59ms
step:316/2330 train_time:19149ms step_avg:60.60ms
step:317/2330 train_time:19208ms step_avg:60.59ms
step:318/2330 train_time:19271ms step_avg:60.60ms
step:319/2330 train_time:19330ms step_avg:60.60ms
step:320/2330 train_time:19392ms step_avg:60.60ms
step:321/2330 train_time:19450ms step_avg:60.59ms
step:322/2330 train_time:19511ms step_avg:60.59ms
step:323/2330 train_time:19569ms step_avg:60.59ms
step:324/2330 train_time:19632ms step_avg:60.59ms
step:325/2330 train_time:19690ms step_avg:60.59ms
step:326/2330 train_time:19754ms step_avg:60.60ms
step:327/2330 train_time:19813ms step_avg:60.59ms
step:328/2330 train_time:19876ms step_avg:60.60ms
step:329/2330 train_time:19935ms step_avg:60.59ms
step:330/2330 train_time:19998ms step_avg:60.60ms
step:331/2330 train_time:20057ms step_avg:60.60ms
step:332/2330 train_time:20119ms step_avg:60.60ms
step:333/2330 train_time:20178ms step_avg:60.60ms
step:334/2330 train_time:20240ms step_avg:60.60ms
step:335/2330 train_time:20299ms step_avg:60.59ms
step:336/2330 train_time:20361ms step_avg:60.60ms
step:337/2330 train_time:20420ms step_avg:60.59ms
step:338/2330 train_time:20481ms step_avg:60.59ms
step:339/2330 train_time:20539ms step_avg:60.59ms
step:340/2330 train_time:20602ms step_avg:60.59ms
step:341/2330 train_time:20661ms step_avg:60.59ms
step:342/2330 train_time:20723ms step_avg:60.59ms
step:343/2330 train_time:20783ms step_avg:60.59ms
step:344/2330 train_time:20847ms step_avg:60.60ms
step:345/2330 train_time:20906ms step_avg:60.60ms
step:346/2330 train_time:20968ms step_avg:60.60ms
step:347/2330 train_time:21027ms step_avg:60.60ms
step:348/2330 train_time:21090ms step_avg:60.60ms
step:349/2330 train_time:21150ms step_avg:60.60ms
step:350/2330 train_time:21211ms step_avg:60.60ms
step:351/2330 train_time:21270ms step_avg:60.60ms
step:352/2330 train_time:21332ms step_avg:60.60ms
step:353/2330 train_time:21390ms step_avg:60.60ms
step:354/2330 train_time:21452ms step_avg:60.60ms
step:355/2330 train_time:21510ms step_avg:60.59ms
step:356/2330 train_time:21572ms step_avg:60.59ms
step:357/2330 train_time:21631ms step_avg:60.59ms
step:358/2330 train_time:21694ms step_avg:60.60ms
step:359/2330 train_time:21752ms step_avg:60.59ms
step:360/2330 train_time:21815ms step_avg:60.60ms
step:361/2330 train_time:21875ms step_avg:60.59ms
step:362/2330 train_time:21937ms step_avg:60.60ms
step:363/2330 train_time:21996ms step_avg:60.59ms
step:364/2330 train_time:22058ms step_avg:60.60ms
step:365/2330 train_time:22117ms step_avg:60.59ms
step:366/2330 train_time:22179ms step_avg:60.60ms
step:367/2330 train_time:22238ms step_avg:60.59ms
step:368/2330 train_time:22299ms step_avg:60.60ms
step:369/2330 train_time:22358ms step_avg:60.59ms
step:370/2330 train_time:22419ms step_avg:60.59ms
step:371/2330 train_time:22477ms step_avg:60.59ms
step:372/2330 train_time:22539ms step_avg:60.59ms
step:373/2330 train_time:22598ms step_avg:60.58ms
step:374/2330 train_time:22661ms step_avg:60.59ms
step:375/2330 train_time:22719ms step_avg:60.58ms
step:376/2330 train_time:22782ms step_avg:60.59ms
step:377/2330 train_time:22841ms step_avg:60.59ms
step:378/2330 train_time:22904ms step_avg:60.59ms
step:379/2330 train_time:22964ms step_avg:60.59ms
step:380/2330 train_time:23027ms step_avg:60.60ms
step:381/2330 train_time:23086ms step_avg:60.59ms
step:382/2330 train_time:23148ms step_avg:60.60ms
step:383/2330 train_time:23206ms step_avg:60.59ms
step:384/2330 train_time:23268ms step_avg:60.59ms
step:385/2330 train_time:23327ms step_avg:60.59ms
step:386/2330 train_time:23390ms step_avg:60.60ms
step:387/2330 train_time:23449ms step_avg:60.59ms
step:388/2330 train_time:23511ms step_avg:60.59ms
step:389/2330 train_time:23570ms step_avg:60.59ms
step:390/2330 train_time:23631ms step_avg:60.59ms
step:391/2330 train_time:23690ms step_avg:60.59ms
step:392/2330 train_time:23753ms step_avg:60.59ms
step:393/2330 train_time:23811ms step_avg:60.59ms
step:394/2330 train_time:23874ms step_avg:60.59ms
step:395/2330 train_time:23933ms step_avg:60.59ms
step:396/2330 train_time:23997ms step_avg:60.60ms
step:397/2330 train_time:24056ms step_avg:60.59ms
step:398/2330 train_time:24118ms step_avg:60.60ms
step:399/2330 train_time:24177ms step_avg:60.59ms
step:400/2330 train_time:24240ms step_avg:60.60ms
step:401/2330 train_time:24299ms step_avg:60.60ms
step:402/2330 train_time:24361ms step_avg:60.60ms
step:403/2330 train_time:24420ms step_avg:60.60ms
step:404/2330 train_time:24482ms step_avg:60.60ms
step:405/2330 train_time:24540ms step_avg:60.59ms
step:406/2330 train_time:24602ms step_avg:60.60ms
step:407/2330 train_time:24663ms step_avg:60.60ms
step:408/2330 train_time:24725ms step_avg:60.60ms
step:409/2330 train_time:24785ms step_avg:60.60ms
step:410/2330 train_time:24848ms step_avg:60.61ms
step:411/2330 train_time:24908ms step_avg:60.60ms
step:412/2330 train_time:24971ms step_avg:60.61ms
step:413/2330 train_time:25030ms step_avg:60.61ms
step:414/2330 train_time:25092ms step_avg:60.61ms
step:415/2330 train_time:25151ms step_avg:60.61ms
step:416/2330 train_time:25212ms step_avg:60.61ms
step:417/2330 train_time:25272ms step_avg:60.60ms
step:418/2330 train_time:25334ms step_avg:60.61ms
step:419/2330 train_time:25394ms step_avg:60.61ms
step:420/2330 train_time:25456ms step_avg:60.61ms
step:421/2330 train_time:25515ms step_avg:60.61ms
step:422/2330 train_time:25578ms step_avg:60.61ms
step:423/2330 train_time:25637ms step_avg:60.61ms
step:424/2330 train_time:25699ms step_avg:60.61ms
step:425/2330 train_time:25757ms step_avg:60.61ms
step:426/2330 train_time:25820ms step_avg:60.61ms
step:427/2330 train_time:25880ms step_avg:60.61ms
step:428/2330 train_time:25942ms step_avg:60.61ms
step:429/2330 train_time:26000ms step_avg:60.61ms
step:430/2330 train_time:26063ms step_avg:60.61ms
step:431/2330 train_time:26122ms step_avg:60.61ms
step:432/2330 train_time:26184ms step_avg:60.61ms
step:433/2330 train_time:26244ms step_avg:60.61ms
step:434/2330 train_time:26307ms step_avg:60.62ms
step:435/2330 train_time:26367ms step_avg:60.61ms
step:436/2330 train_time:26429ms step_avg:60.62ms
step:437/2330 train_time:26489ms step_avg:60.62ms
step:438/2330 train_time:26551ms step_avg:60.62ms
step:439/2330 train_time:26609ms step_avg:60.61ms
step:440/2330 train_time:26671ms step_avg:60.62ms
step:441/2330 train_time:26730ms step_avg:60.61ms
step:442/2330 train_time:26792ms step_avg:60.61ms
step:443/2330 train_time:26851ms step_avg:60.61ms
step:444/2330 train_time:26912ms step_avg:60.61ms
step:445/2330 train_time:26971ms step_avg:60.61ms
step:446/2330 train_time:27033ms step_avg:60.61ms
step:447/2330 train_time:27092ms step_avg:60.61ms
step:448/2330 train_time:27154ms step_avg:60.61ms
step:449/2330 train_time:27214ms step_avg:60.61ms
step:450/2330 train_time:27276ms step_avg:60.61ms
step:451/2330 train_time:27335ms step_avg:60.61ms
step:452/2330 train_time:27398ms step_avg:60.61ms
step:453/2330 train_time:27457ms step_avg:60.61ms
step:454/2330 train_time:27519ms step_avg:60.61ms
step:455/2330 train_time:27578ms step_avg:60.61ms
step:456/2330 train_time:27640ms step_avg:60.61ms
step:457/2330 train_time:27698ms step_avg:60.61ms
step:458/2330 train_time:27761ms step_avg:60.61ms
step:459/2330 train_time:27819ms step_avg:60.61ms
step:460/2330 train_time:27881ms step_avg:60.61ms
step:461/2330 train_time:27940ms step_avg:60.61ms
step:462/2330 train_time:28002ms step_avg:60.61ms
step:463/2330 train_time:28062ms step_avg:60.61ms
step:464/2330 train_time:28124ms step_avg:60.61ms
step:465/2330 train_time:28184ms step_avg:60.61ms
step:466/2330 train_time:28246ms step_avg:60.61ms
step:467/2330 train_time:28305ms step_avg:60.61ms
step:468/2330 train_time:28369ms step_avg:60.62ms
step:469/2330 train_time:28428ms step_avg:60.61ms
step:470/2330 train_time:28490ms step_avg:60.62ms
step:471/2330 train_time:28550ms step_avg:60.62ms
step:472/2330 train_time:28611ms step_avg:60.62ms
step:473/2330 train_time:28670ms step_avg:60.61ms
step:474/2330 train_time:28731ms step_avg:60.61ms
step:475/2330 train_time:28789ms step_avg:60.61ms
step:476/2330 train_time:28852ms step_avg:60.61ms
step:477/2330 train_time:28911ms step_avg:60.61ms
step:478/2330 train_time:28972ms step_avg:60.61ms
step:479/2330 train_time:29030ms step_avg:60.61ms
step:480/2330 train_time:29093ms step_avg:60.61ms
step:481/2330 train_time:29152ms step_avg:60.61ms
step:482/2330 train_time:29215ms step_avg:60.61ms
step:483/2330 train_time:29275ms step_avg:60.61ms
step:484/2330 train_time:29337ms step_avg:60.61ms
step:485/2330 train_time:29396ms step_avg:60.61ms
step:486/2330 train_time:29459ms step_avg:60.61ms
step:487/2330 train_time:29517ms step_avg:60.61ms
step:488/2330 train_time:29580ms step_avg:60.61ms
step:489/2330 train_time:29638ms step_avg:60.61ms
step:490/2330 train_time:29700ms step_avg:60.61ms
step:491/2330 train_time:29759ms step_avg:60.61ms
step:492/2330 train_time:29821ms step_avg:60.61ms
step:493/2330 train_time:29880ms step_avg:60.61ms
step:494/2330 train_time:29942ms step_avg:60.61ms
step:495/2330 train_time:30001ms step_avg:60.61ms
step:496/2330 train_time:30064ms step_avg:60.61ms
step:497/2330 train_time:30123ms step_avg:60.61ms
step:498/2330 train_time:30187ms step_avg:60.62ms
step:499/2330 train_time:30246ms step_avg:60.61ms
step:500/2330 train_time:30309ms step_avg:60.62ms
step:500/2330 val_loss:4.3796 train_time:30382ms step_avg:60.76ms
step:501/2330 train_time:30403ms step_avg:60.69ms
step:502/2330 train_time:30433ms step_avg:60.62ms
step:503/2330 train_time:30496ms step_avg:60.63ms
step:504/2330 train_time:30563ms step_avg:60.64ms
step:505/2330 train_time:30625ms step_avg:60.64ms
step:506/2330 train_time:30689ms step_avg:60.65ms
step:507/2330 train_time:30748ms step_avg:60.65ms
step:508/2330 train_time:30808ms step_avg:60.65ms
step:509/2330 train_time:30867ms step_avg:60.64ms
step:510/2330 train_time:30928ms step_avg:60.64ms
step:511/2330 train_time:30987ms step_avg:60.64ms
step:512/2330 train_time:31047ms step_avg:60.64ms
step:513/2330 train_time:31105ms step_avg:60.63ms
step:514/2330 train_time:31166ms step_avg:60.63ms
step:515/2330 train_time:31225ms step_avg:60.63ms
step:516/2330 train_time:31287ms step_avg:60.63ms
step:517/2330 train_time:31345ms step_avg:60.63ms
step:518/2330 train_time:31408ms step_avg:60.63ms
step:519/2330 train_time:31468ms step_avg:60.63ms
step:520/2330 train_time:31532ms step_avg:60.64ms
step:521/2330 train_time:31592ms step_avg:60.64ms
step:522/2330 train_time:31655ms step_avg:60.64ms
step:523/2330 train_time:31714ms step_avg:60.64ms
step:524/2330 train_time:31777ms step_avg:60.64ms
step:525/2330 train_time:31835ms step_avg:60.64ms
step:526/2330 train_time:31897ms step_avg:60.64ms
step:527/2330 train_time:31956ms step_avg:60.64ms
step:528/2330 train_time:32018ms step_avg:60.64ms
step:529/2330 train_time:32077ms step_avg:60.64ms
step:530/2330 train_time:32138ms step_avg:60.64ms
step:531/2330 train_time:32198ms step_avg:60.64ms
step:532/2330 train_time:32259ms step_avg:60.64ms
step:533/2330 train_time:32318ms step_avg:60.63ms
step:534/2330 train_time:32381ms step_avg:60.64ms
step:535/2330 train_time:32441ms step_avg:60.64ms
step:536/2330 train_time:32505ms step_avg:60.64ms
step:537/2330 train_time:32565ms step_avg:60.64ms
step:538/2330 train_time:32628ms step_avg:60.65ms
step:539/2330 train_time:32689ms step_avg:60.65ms
step:540/2330 train_time:32751ms step_avg:60.65ms
step:541/2330 train_time:32809ms step_avg:60.65ms
step:542/2330 train_time:32871ms step_avg:60.65ms
step:543/2330 train_time:32928ms step_avg:60.64ms
step:544/2330 train_time:32990ms step_avg:60.64ms
step:545/2330 train_time:33049ms step_avg:60.64ms
step:546/2330 train_time:33111ms step_avg:60.64ms
step:547/2330 train_time:33170ms step_avg:60.64ms
step:548/2330 train_time:33232ms step_avg:60.64ms
step:549/2330 train_time:33291ms step_avg:60.64ms
step:550/2330 train_time:33354ms step_avg:60.64ms
step:551/2330 train_time:33414ms step_avg:60.64ms
step:552/2330 train_time:33477ms step_avg:60.65ms
step:553/2330 train_time:33536ms step_avg:60.64ms
step:554/2330 train_time:33598ms step_avg:60.65ms
step:555/2330 train_time:33657ms step_avg:60.64ms
step:556/2330 train_time:33720ms step_avg:60.65ms
step:557/2330 train_time:33779ms step_avg:60.65ms
step:558/2330 train_time:33842ms step_avg:60.65ms
step:559/2330 train_time:33901ms step_avg:60.65ms
step:560/2330 train_time:33963ms step_avg:60.65ms
step:561/2330 train_time:34023ms step_avg:60.65ms
step:562/2330 train_time:34085ms step_avg:60.65ms
step:563/2330 train_time:34144ms step_avg:60.65ms
step:564/2330 train_time:34207ms step_avg:60.65ms
step:565/2330 train_time:34265ms step_avg:60.65ms
step:566/2330 train_time:34328ms step_avg:60.65ms
step:567/2330 train_time:34387ms step_avg:60.65ms
step:568/2330 train_time:34448ms step_avg:60.65ms
step:569/2330 train_time:34507ms step_avg:60.65ms
step:570/2330 train_time:34568ms step_avg:60.65ms
step:571/2330 train_time:34627ms step_avg:60.64ms
step:572/2330 train_time:34689ms step_avg:60.65ms
step:573/2330 train_time:34749ms step_avg:60.64ms
step:574/2330 train_time:34812ms step_avg:60.65ms
step:575/2330 train_time:34870ms step_avg:60.64ms
step:576/2330 train_time:34933ms step_avg:60.65ms
step:577/2330 train_time:34993ms step_avg:60.65ms
step:578/2330 train_time:35055ms step_avg:60.65ms
step:579/2330 train_time:35114ms step_avg:60.65ms
step:580/2330 train_time:35176ms step_avg:60.65ms
step:581/2330 train_time:35234ms step_avg:60.64ms
step:582/2330 train_time:35298ms step_avg:60.65ms
step:583/2330 train_time:35356ms step_avg:60.65ms
step:584/2330 train_time:35418ms step_avg:60.65ms
step:585/2330 train_time:35478ms step_avg:60.65ms
step:586/2330 train_time:35540ms step_avg:60.65ms
step:587/2330 train_time:35599ms step_avg:60.65ms
step:588/2330 train_time:35661ms step_avg:60.65ms
step:589/2330 train_time:35720ms step_avg:60.65ms
step:590/2330 train_time:35784ms step_avg:60.65ms
step:591/2330 train_time:35843ms step_avg:60.65ms
step:592/2330 train_time:35906ms step_avg:60.65ms
step:593/2330 train_time:35965ms step_avg:60.65ms
step:594/2330 train_time:36026ms step_avg:60.65ms
step:595/2330 train_time:36085ms step_avg:60.65ms
step:596/2330 train_time:36148ms step_avg:60.65ms
step:597/2330 train_time:36207ms step_avg:60.65ms
step:598/2330 train_time:36269ms step_avg:60.65ms
step:599/2330 train_time:36327ms step_avg:60.65ms
step:600/2330 train_time:36389ms step_avg:60.65ms
step:601/2330 train_time:36448ms step_avg:60.64ms
step:602/2330 train_time:36510ms step_avg:60.65ms
step:603/2330 train_time:36568ms step_avg:60.64ms
step:604/2330 train_time:36631ms step_avg:60.65ms
step:605/2330 train_time:36690ms step_avg:60.64ms
step:606/2330 train_time:36752ms step_avg:60.65ms
step:607/2330 train_time:36811ms step_avg:60.64ms
step:608/2330 train_time:36874ms step_avg:60.65ms
step:609/2330 train_time:36933ms step_avg:60.65ms
step:610/2330 train_time:36995ms step_avg:60.65ms
step:611/2330 train_time:37054ms step_avg:60.65ms
step:612/2330 train_time:37116ms step_avg:60.65ms
step:613/2330 train_time:37175ms step_avg:60.64ms
step:614/2330 train_time:37236ms step_avg:60.65ms
step:615/2330 train_time:37295ms step_avg:60.64ms
step:616/2330 train_time:37357ms step_avg:60.64ms
step:617/2330 train_time:37416ms step_avg:60.64ms
step:618/2330 train_time:37478ms step_avg:60.64ms
step:619/2330 train_time:37537ms step_avg:60.64ms
step:620/2330 train_time:37600ms step_avg:60.65ms
step:621/2330 train_time:37660ms step_avg:60.64ms
step:622/2330 train_time:37722ms step_avg:60.65ms
step:623/2330 train_time:37782ms step_avg:60.64ms
step:624/2330 train_time:37843ms step_avg:60.65ms
step:625/2330 train_time:37903ms step_avg:60.64ms
step:626/2330 train_time:37964ms step_avg:60.65ms
step:627/2330 train_time:38024ms step_avg:60.64ms
step:628/2330 train_time:38087ms step_avg:60.65ms
step:629/2330 train_time:38145ms step_avg:60.64ms
step:630/2330 train_time:38207ms step_avg:60.65ms
step:631/2330 train_time:38265ms step_avg:60.64ms
step:632/2330 train_time:38327ms step_avg:60.64ms
step:633/2330 train_time:38386ms step_avg:60.64ms
step:634/2330 train_time:38449ms step_avg:60.64ms
step:635/2330 train_time:38506ms step_avg:60.64ms
step:636/2330 train_time:38569ms step_avg:60.64ms
step:637/2330 train_time:38628ms step_avg:60.64ms
step:638/2330 train_time:38691ms step_avg:60.64ms
step:639/2330 train_time:38750ms step_avg:60.64ms
step:640/2330 train_time:38813ms step_avg:60.64ms
step:641/2330 train_time:38872ms step_avg:60.64ms
step:642/2330 train_time:38935ms step_avg:60.65ms
step:643/2330 train_time:38994ms step_avg:60.64ms
step:644/2330 train_time:39058ms step_avg:60.65ms
step:645/2330 train_time:39116ms step_avg:60.64ms
step:646/2330 train_time:39177ms step_avg:60.65ms
step:647/2330 train_time:39237ms step_avg:60.64ms
step:648/2330 train_time:39299ms step_avg:60.65ms
step:649/2330 train_time:39358ms step_avg:60.64ms
step:650/2330 train_time:39420ms step_avg:60.65ms
step:651/2330 train_time:39480ms step_avg:60.64ms
step:652/2330 train_time:39542ms step_avg:60.65ms
step:653/2330 train_time:39602ms step_avg:60.65ms
step:654/2330 train_time:39664ms step_avg:60.65ms
step:655/2330 train_time:39723ms step_avg:60.65ms
step:656/2330 train_time:39786ms step_avg:60.65ms
step:657/2330 train_time:39845ms step_avg:60.65ms
step:658/2330 train_time:39907ms step_avg:60.65ms
step:659/2330 train_time:39966ms step_avg:60.65ms
step:660/2330 train_time:40029ms step_avg:60.65ms
step:661/2330 train_time:40087ms step_avg:60.65ms
step:662/2330 train_time:40149ms step_avg:60.65ms
step:663/2330 train_time:40208ms step_avg:60.65ms
step:664/2330 train_time:40270ms step_avg:60.65ms
step:665/2330 train_time:40329ms step_avg:60.64ms
step:666/2330 train_time:40392ms step_avg:60.65ms
step:667/2330 train_time:40449ms step_avg:60.64ms
step:668/2330 train_time:40512ms step_avg:60.65ms
step:669/2330 train_time:40571ms step_avg:60.64ms
step:670/2330 train_time:40634ms step_avg:60.65ms
step:671/2330 train_time:40694ms step_avg:60.65ms
step:672/2330 train_time:40756ms step_avg:60.65ms
step:673/2330 train_time:40815ms step_avg:60.65ms
step:674/2330 train_time:40877ms step_avg:60.65ms
step:675/2330 train_time:40935ms step_avg:60.64ms
step:676/2330 train_time:40997ms step_avg:60.65ms
step:677/2330 train_time:41056ms step_avg:60.64ms
step:678/2330 train_time:41118ms step_avg:60.65ms
step:679/2330 train_time:41177ms step_avg:60.64ms
step:680/2330 train_time:41240ms step_avg:60.65ms
step:681/2330 train_time:41299ms step_avg:60.65ms
step:682/2330 train_time:41361ms step_avg:60.65ms
step:683/2330 train_time:41421ms step_avg:60.65ms
step:684/2330 train_time:41484ms step_avg:60.65ms
step:685/2330 train_time:41544ms step_avg:60.65ms
step:686/2330 train_time:41606ms step_avg:60.65ms
step:687/2330 train_time:41665ms step_avg:60.65ms
step:688/2330 train_time:41727ms step_avg:60.65ms
step:689/2330 train_time:41786ms step_avg:60.65ms
step:690/2330 train_time:41848ms step_avg:60.65ms
step:691/2330 train_time:41906ms step_avg:60.65ms
step:692/2330 train_time:41968ms step_avg:60.65ms
step:693/2330 train_time:42026ms step_avg:60.64ms
step:694/2330 train_time:42088ms step_avg:60.65ms
step:695/2330 train_time:42147ms step_avg:60.64ms
step:696/2330 train_time:42209ms step_avg:60.65ms
step:697/2330 train_time:42268ms step_avg:60.64ms
step:698/2330 train_time:42331ms step_avg:60.65ms
step:699/2330 train_time:42391ms step_avg:60.65ms
step:700/2330 train_time:42454ms step_avg:60.65ms
step:701/2330 train_time:42513ms step_avg:60.65ms
step:702/2330 train_time:42575ms step_avg:60.65ms
step:703/2330 train_time:42634ms step_avg:60.65ms
step:704/2330 train_time:42696ms step_avg:60.65ms
step:705/2330 train_time:42755ms step_avg:60.65ms
step:706/2330 train_time:42817ms step_avg:60.65ms
step:707/2330 train_time:42876ms step_avg:60.64ms
step:708/2330 train_time:42937ms step_avg:60.65ms
step:709/2330 train_time:42996ms step_avg:60.64ms
step:710/2330 train_time:43058ms step_avg:60.64ms
step:711/2330 train_time:43117ms step_avg:60.64ms
step:712/2330 train_time:43180ms step_avg:60.65ms
step:713/2330 train_time:43240ms step_avg:60.64ms
step:714/2330 train_time:43303ms step_avg:60.65ms
step:715/2330 train_time:43362ms step_avg:60.65ms
step:716/2330 train_time:43424ms step_avg:60.65ms
step:717/2330 train_time:43485ms step_avg:60.65ms
step:718/2330 train_time:43547ms step_avg:60.65ms
step:719/2330 train_time:43606ms step_avg:60.65ms
step:720/2330 train_time:43667ms step_avg:60.65ms
step:721/2330 train_time:43725ms step_avg:60.65ms
step:722/2330 train_time:43788ms step_avg:60.65ms
step:723/2330 train_time:43846ms step_avg:60.64ms
step:724/2330 train_time:43908ms step_avg:60.65ms
step:725/2330 train_time:43966ms step_avg:60.64ms
step:726/2330 train_time:44029ms step_avg:60.65ms
step:727/2330 train_time:44088ms step_avg:60.64ms
step:728/2330 train_time:44150ms step_avg:60.65ms
step:729/2330 train_time:44209ms step_avg:60.64ms
step:730/2330 train_time:44272ms step_avg:60.65ms
step:731/2330 train_time:44331ms step_avg:60.64ms
step:732/2330 train_time:44392ms step_avg:60.65ms
step:733/2330 train_time:44452ms step_avg:60.64ms
step:734/2330 train_time:44515ms step_avg:60.65ms
step:735/2330 train_time:44574ms step_avg:60.64ms
step:736/2330 train_time:44636ms step_avg:60.65ms
step:737/2330 train_time:44695ms step_avg:60.64ms
step:738/2330 train_time:44756ms step_avg:60.65ms
step:739/2330 train_time:44815ms step_avg:60.64ms
step:740/2330 train_time:44876ms step_avg:60.64ms
step:741/2330 train_time:44935ms step_avg:60.64ms
step:742/2330 train_time:44998ms step_avg:60.64ms
step:743/2330 train_time:45057ms step_avg:60.64ms
step:744/2330 train_time:45119ms step_avg:60.64ms
step:745/2330 train_time:45179ms step_avg:60.64ms
step:746/2330 train_time:45241ms step_avg:60.65ms
step:747/2330 train_time:45301ms step_avg:60.64ms
step:748/2330 train_time:45364ms step_avg:60.65ms
step:749/2330 train_time:45424ms step_avg:60.65ms
step:750/2330 train_time:45487ms step_avg:60.65ms
step:750/2330 val_loss:4.0143 train_time:45559ms step_avg:60.75ms
step:751/2330 train_time:45580ms step_avg:60.69ms
step:752/2330 train_time:45610ms step_avg:60.65ms
step:753/2330 train_time:45672ms step_avg:60.65ms
step:754/2330 train_time:45735ms step_avg:60.66ms
step:755/2330 train_time:45796ms step_avg:60.66ms
step:756/2330 train_time:45858ms step_avg:60.66ms
step:757/2330 train_time:45917ms step_avg:60.66ms
step:758/2330 train_time:45978ms step_avg:60.66ms
step:759/2330 train_time:46037ms step_avg:60.65ms
step:760/2330 train_time:46098ms step_avg:60.66ms
step:761/2330 train_time:46156ms step_avg:60.65ms
step:762/2330 train_time:46217ms step_avg:60.65ms
step:763/2330 train_time:46275ms step_avg:60.65ms
step:764/2330 train_time:46336ms step_avg:60.65ms
step:765/2330 train_time:46396ms step_avg:60.65ms
step:766/2330 train_time:46457ms step_avg:60.65ms
step:767/2330 train_time:46517ms step_avg:60.65ms
step:768/2330 train_time:46580ms step_avg:60.65ms
step:769/2330 train_time:46642ms step_avg:60.65ms
step:770/2330 train_time:46706ms step_avg:60.66ms
step:771/2330 train_time:46766ms step_avg:60.66ms
step:772/2330 train_time:46831ms step_avg:60.66ms
step:773/2330 train_time:46890ms step_avg:60.66ms
step:774/2330 train_time:46953ms step_avg:60.66ms
step:775/2330 train_time:47012ms step_avg:60.66ms
step:776/2330 train_time:47074ms step_avg:60.66ms
step:777/2330 train_time:47134ms step_avg:60.66ms
step:778/2330 train_time:47196ms step_avg:60.66ms
step:779/2330 train_time:47255ms step_avg:60.66ms
step:780/2330 train_time:47317ms step_avg:60.66ms
step:781/2330 train_time:47376ms step_avg:60.66ms
step:782/2330 train_time:47438ms step_avg:60.66ms
step:783/2330 train_time:47497ms step_avg:60.66ms
step:784/2330 train_time:47560ms step_avg:60.66ms
step:785/2330 train_time:47621ms step_avg:60.66ms
step:786/2330 train_time:47684ms step_avg:60.67ms
step:787/2330 train_time:47746ms step_avg:60.67ms
step:788/2330 train_time:47809ms step_avg:60.67ms
step:789/2330 train_time:47869ms step_avg:60.67ms
step:790/2330 train_time:47933ms step_avg:60.67ms
step:791/2330 train_time:47991ms step_avg:60.67ms
step:792/2330 train_time:48054ms step_avg:60.67ms
step:793/2330 train_time:48113ms step_avg:60.67ms
step:794/2330 train_time:48176ms step_avg:60.67ms
step:795/2330 train_time:48235ms step_avg:60.67ms
step:796/2330 train_time:48297ms step_avg:60.67ms
step:797/2330 train_time:48356ms step_avg:60.67ms
step:798/2330 train_time:48418ms step_avg:60.67ms
step:799/2330 train_time:48477ms step_avg:60.67ms
step:800/2330 train_time:48540ms step_avg:60.68ms
step:801/2330 train_time:48600ms step_avg:60.67ms
step:802/2330 train_time:48664ms step_avg:60.68ms
step:803/2330 train_time:48725ms step_avg:60.68ms
step:804/2330 train_time:48788ms step_avg:60.68ms
step:805/2330 train_time:48849ms step_avg:60.68ms
step:806/2330 train_time:48912ms step_avg:60.68ms
step:807/2330 train_time:48972ms step_avg:60.68ms
step:808/2330 train_time:49034ms step_avg:60.69ms
step:809/2330 train_time:49093ms step_avg:60.68ms
step:810/2330 train_time:49156ms step_avg:60.69ms
step:811/2330 train_time:49215ms step_avg:60.68ms
step:812/2330 train_time:49277ms step_avg:60.69ms
step:813/2330 train_time:49337ms step_avg:60.68ms
step:814/2330 train_time:49398ms step_avg:60.69ms
step:815/2330 train_time:49458ms step_avg:60.68ms
step:816/2330 train_time:49521ms step_avg:60.69ms
step:817/2330 train_time:49581ms step_avg:60.69ms
step:818/2330 train_time:49644ms step_avg:60.69ms
step:819/2330 train_time:49704ms step_avg:60.69ms
step:820/2330 train_time:49768ms step_avg:60.69ms
step:821/2330 train_time:49830ms step_avg:60.69ms
step:822/2330 train_time:49892ms step_avg:60.70ms
step:823/2330 train_time:49952ms step_avg:60.69ms
step:824/2330 train_time:50015ms step_avg:60.70ms
step:825/2330 train_time:50075ms step_avg:60.70ms
step:826/2330 train_time:50138ms step_avg:60.70ms
step:827/2330 train_time:50198ms step_avg:60.70ms
step:828/2330 train_time:50260ms step_avg:60.70ms
step:829/2330 train_time:50319ms step_avg:60.70ms
step:830/2330 train_time:50382ms step_avg:60.70ms
step:831/2330 train_time:50442ms step_avg:60.70ms
step:832/2330 train_time:50504ms step_avg:60.70ms
step:833/2330 train_time:50564ms step_avg:60.70ms
step:834/2330 train_time:50627ms step_avg:60.70ms
step:835/2330 train_time:50687ms step_avg:60.70ms
step:836/2330 train_time:50750ms step_avg:60.71ms
step:837/2330 train_time:50811ms step_avg:60.71ms
step:838/2330 train_time:50874ms step_avg:60.71ms
step:839/2330 train_time:50934ms step_avg:60.71ms
step:840/2330 train_time:50996ms step_avg:60.71ms
step:841/2330 train_time:51056ms step_avg:60.71ms
step:842/2330 train_time:51119ms step_avg:60.71ms
step:843/2330 train_time:51178ms step_avg:60.71ms
step:844/2330 train_time:51241ms step_avg:60.71ms
step:845/2330 train_time:51299ms step_avg:60.71ms
step:846/2330 train_time:51362ms step_avg:60.71ms
step:847/2330 train_time:51422ms step_avg:60.71ms
step:848/2330 train_time:51485ms step_avg:60.71ms
step:849/2330 train_time:51545ms step_avg:60.71ms
step:850/2330 train_time:51608ms step_avg:60.72ms
step:851/2330 train_time:51668ms step_avg:60.71ms
step:852/2330 train_time:51731ms step_avg:60.72ms
step:853/2330 train_time:51790ms step_avg:60.72ms
step:854/2330 train_time:51853ms step_avg:60.72ms
step:855/2330 train_time:51913ms step_avg:60.72ms
step:856/2330 train_time:51976ms step_avg:60.72ms
step:857/2330 train_time:52036ms step_avg:60.72ms
step:858/2330 train_time:52098ms step_avg:60.72ms
step:859/2330 train_time:52159ms step_avg:60.72ms
step:860/2330 train_time:52221ms step_avg:60.72ms
step:861/2330 train_time:52282ms step_avg:60.72ms
step:862/2330 train_time:52344ms step_avg:60.72ms
step:863/2330 train_time:52403ms step_avg:60.72ms
step:864/2330 train_time:52466ms step_avg:60.72ms
step:865/2330 train_time:52526ms step_avg:60.72ms
step:866/2330 train_time:52588ms step_avg:60.73ms
step:867/2330 train_time:52648ms step_avg:60.72ms
step:868/2330 train_time:52711ms step_avg:60.73ms
step:869/2330 train_time:52770ms step_avg:60.73ms
step:870/2330 train_time:52833ms step_avg:60.73ms
step:871/2330 train_time:52892ms step_avg:60.73ms
step:872/2330 train_time:52956ms step_avg:60.73ms
step:873/2330 train_time:53016ms step_avg:60.73ms
step:874/2330 train_time:53079ms step_avg:60.73ms
step:875/2330 train_time:53138ms step_avg:60.73ms
step:876/2330 train_time:53201ms step_avg:60.73ms
step:877/2330 train_time:53260ms step_avg:60.73ms
step:878/2330 train_time:53323ms step_avg:60.73ms
step:879/2330 train_time:53384ms step_avg:60.73ms
step:880/2330 train_time:53446ms step_avg:60.73ms
step:881/2330 train_time:53505ms step_avg:60.73ms
step:882/2330 train_time:53569ms step_avg:60.74ms
step:883/2330 train_time:53630ms step_avg:60.74ms
step:884/2330 train_time:53691ms step_avg:60.74ms
step:885/2330 train_time:53751ms step_avg:60.74ms
step:886/2330 train_time:53813ms step_avg:60.74ms
step:887/2330 train_time:53873ms step_avg:60.74ms
step:888/2330 train_time:53936ms step_avg:60.74ms
step:889/2330 train_time:53995ms step_avg:60.74ms
step:890/2330 train_time:54057ms step_avg:60.74ms
step:891/2330 train_time:54117ms step_avg:60.74ms
step:892/2330 train_time:54180ms step_avg:60.74ms
step:893/2330 train_time:54240ms step_avg:60.74ms
step:894/2330 train_time:54302ms step_avg:60.74ms
step:895/2330 train_time:54362ms step_avg:60.74ms
step:896/2330 train_time:54425ms step_avg:60.74ms
step:897/2330 train_time:54485ms step_avg:60.74ms
step:898/2330 train_time:54548ms step_avg:60.74ms
step:899/2330 train_time:54608ms step_avg:60.74ms
step:900/2330 train_time:54671ms step_avg:60.75ms
step:901/2330 train_time:54730ms step_avg:60.74ms
step:902/2330 train_time:54793ms step_avg:60.75ms
step:903/2330 train_time:54853ms step_avg:60.74ms
step:904/2330 train_time:54916ms step_avg:60.75ms
step:905/2330 train_time:54975ms step_avg:60.75ms
step:906/2330 train_time:55039ms step_avg:60.75ms
step:907/2330 train_time:55097ms step_avg:60.75ms
step:908/2330 train_time:55160ms step_avg:60.75ms
step:909/2330 train_time:55219ms step_avg:60.75ms
step:910/2330 train_time:55282ms step_avg:60.75ms
step:911/2330 train_time:55342ms step_avg:60.75ms
step:912/2330 train_time:55405ms step_avg:60.75ms
step:913/2330 train_time:55465ms step_avg:60.75ms
step:914/2330 train_time:55530ms step_avg:60.75ms
step:915/2330 train_time:55589ms step_avg:60.75ms
step:916/2330 train_time:55651ms step_avg:60.75ms
step:917/2330 train_time:55711ms step_avg:60.75ms
step:918/2330 train_time:55774ms step_avg:60.76ms
step:919/2330 train_time:55834ms step_avg:60.76ms
step:920/2330 train_time:55896ms step_avg:60.76ms
step:921/2330 train_time:55955ms step_avg:60.76ms
step:922/2330 train_time:56018ms step_avg:60.76ms
step:923/2330 train_time:56077ms step_avg:60.76ms
step:924/2330 train_time:56140ms step_avg:60.76ms
step:925/2330 train_time:56200ms step_avg:60.76ms
step:926/2330 train_time:56263ms step_avg:60.76ms
step:927/2330 train_time:56322ms step_avg:60.76ms
step:928/2330 train_time:56385ms step_avg:60.76ms
step:929/2330 train_time:56445ms step_avg:60.76ms
step:930/2330 train_time:56508ms step_avg:60.76ms
step:931/2330 train_time:56568ms step_avg:60.76ms
step:932/2330 train_time:56631ms step_avg:60.76ms
step:933/2330 train_time:56690ms step_avg:60.76ms
step:934/2330 train_time:56754ms step_avg:60.76ms
step:935/2330 train_time:56813ms step_avg:60.76ms
step:936/2330 train_time:56876ms step_avg:60.77ms
step:937/2330 train_time:56936ms step_avg:60.76ms
step:938/2330 train_time:56998ms step_avg:60.77ms
step:939/2330 train_time:57058ms step_avg:60.76ms
step:940/2330 train_time:57120ms step_avg:60.77ms
step:941/2330 train_time:57180ms step_avg:60.77ms
step:942/2330 train_time:57243ms step_avg:60.77ms
step:943/2330 train_time:57302ms step_avg:60.77ms
step:944/2330 train_time:57366ms step_avg:60.77ms
step:945/2330 train_time:57427ms step_avg:60.77ms
step:946/2330 train_time:57490ms step_avg:60.77ms
step:947/2330 train_time:57549ms step_avg:60.77ms
step:948/2330 train_time:57612ms step_avg:60.77ms
step:949/2330 train_time:57672ms step_avg:60.77ms
step:950/2330 train_time:57735ms step_avg:60.77ms
step:951/2330 train_time:57794ms step_avg:60.77ms
step:952/2330 train_time:57857ms step_avg:60.77ms
step:953/2330 train_time:57916ms step_avg:60.77ms
step:954/2330 train_time:57979ms step_avg:60.77ms
step:955/2330 train_time:58039ms step_avg:60.77ms
step:956/2330 train_time:58101ms step_avg:60.78ms
step:957/2330 train_time:58161ms step_avg:60.77ms
step:958/2330 train_time:58224ms step_avg:60.78ms
step:959/2330 train_time:58283ms step_avg:60.77ms
step:960/2330 train_time:58346ms step_avg:60.78ms
step:961/2330 train_time:58406ms step_avg:60.78ms
step:962/2330 train_time:58469ms step_avg:60.78ms
step:963/2330 train_time:58529ms step_avg:60.78ms
step:964/2330 train_time:58592ms step_avg:60.78ms
step:965/2330 train_time:58651ms step_avg:60.78ms
step:966/2330 train_time:58715ms step_avg:60.78ms
step:967/2330 train_time:58775ms step_avg:60.78ms
step:968/2330 train_time:58838ms step_avg:60.78ms
step:969/2330 train_time:58896ms step_avg:60.78ms
step:970/2330 train_time:58959ms step_avg:60.78ms
step:971/2330 train_time:59018ms step_avg:60.78ms
step:972/2330 train_time:59081ms step_avg:60.78ms
step:973/2330 train_time:59141ms step_avg:60.78ms
step:974/2330 train_time:59203ms step_avg:60.78ms
step:975/2330 train_time:59263ms step_avg:60.78ms
step:976/2330 train_time:59326ms step_avg:60.79ms
step:977/2330 train_time:59386ms step_avg:60.78ms
step:978/2330 train_time:59449ms step_avg:60.79ms
step:979/2330 train_time:59508ms step_avg:60.78ms
step:980/2330 train_time:59572ms step_avg:60.79ms
step:981/2330 train_time:59633ms step_avg:60.79ms
step:982/2330 train_time:59696ms step_avg:60.79ms
step:983/2330 train_time:59755ms step_avg:60.79ms
step:984/2330 train_time:59817ms step_avg:60.79ms
step:985/2330 train_time:59877ms step_avg:60.79ms
step:986/2330 train_time:59939ms step_avg:60.79ms
step:987/2330 train_time:59999ms step_avg:60.79ms
step:988/2330 train_time:60062ms step_avg:60.79ms
step:989/2330 train_time:60123ms step_avg:60.79ms
step:990/2330 train_time:60185ms step_avg:60.79ms
step:991/2330 train_time:60245ms step_avg:60.79ms
step:992/2330 train_time:60308ms step_avg:60.79ms
step:993/2330 train_time:60368ms step_avg:60.79ms
step:994/2330 train_time:60431ms step_avg:60.80ms
step:995/2330 train_time:60490ms step_avg:60.79ms
step:996/2330 train_time:60554ms step_avg:60.80ms
step:997/2330 train_time:60614ms step_avg:60.80ms
step:998/2330 train_time:60676ms step_avg:60.80ms
step:999/2330 train_time:60735ms step_avg:60.80ms
step:1000/2330 train_time:60798ms step_avg:60.80ms
step:1000/2330 val_loss:3.8752 train_time:60870ms step_avg:60.87ms
step:1001/2330 train_time:60892ms step_avg:60.83ms
step:1002/2330 train_time:60921ms step_avg:60.80ms
step:1003/2330 train_time:60980ms step_avg:60.80ms
step:1004/2330 train_time:61046ms step_avg:60.80ms
step:1005/2330 train_time:61109ms step_avg:60.80ms
step:1006/2330 train_time:61171ms step_avg:60.81ms
step:1007/2330 train_time:61230ms step_avg:60.80ms
step:1008/2330 train_time:61292ms step_avg:60.81ms
step:1009/2330 train_time:61351ms step_avg:60.80ms
step:1010/2330 train_time:61412ms step_avg:60.80ms
step:1011/2330 train_time:61471ms step_avg:60.80ms
step:1012/2330 train_time:61533ms step_avg:60.80ms
step:1013/2330 train_time:61592ms step_avg:60.80ms
step:1014/2330 train_time:61653ms step_avg:60.80ms
step:1015/2330 train_time:61712ms step_avg:60.80ms
step:1016/2330 train_time:61776ms step_avg:60.80ms
step:1017/2330 train_time:61842ms step_avg:60.81ms
step:1018/2330 train_time:61908ms step_avg:60.81ms
step:1019/2330 train_time:61969ms step_avg:60.81ms
step:1020/2330 train_time:62033ms step_avg:60.82ms
step:1021/2330 train_time:62092ms step_avg:60.82ms
step:1022/2330 train_time:62156ms step_avg:60.82ms
step:1023/2330 train_time:62215ms step_avg:60.82ms
step:1024/2330 train_time:62278ms step_avg:60.82ms
step:1025/2330 train_time:62337ms step_avg:60.82ms
step:1026/2330 train_time:62399ms step_avg:60.82ms
step:1027/2330 train_time:62459ms step_avg:60.82ms
step:1028/2330 train_time:62521ms step_avg:60.82ms
step:1029/2330 train_time:62580ms step_avg:60.82ms
step:1030/2330 train_time:62642ms step_avg:60.82ms
step:1031/2330 train_time:62701ms step_avg:60.82ms
step:1032/2330 train_time:62764ms step_avg:60.82ms
step:1033/2330 train_time:62825ms step_avg:60.82ms
step:1034/2330 train_time:62890ms step_avg:60.82ms
step:1035/2330 train_time:62951ms step_avg:60.82ms
step:1036/2330 train_time:63015ms step_avg:60.83ms
step:1037/2330 train_time:63074ms step_avg:60.82ms
step:1038/2330 train_time:63137ms step_avg:60.83ms
step:1039/2330 train_time:63197ms step_avg:60.82ms
step:1040/2330 train_time:63260ms step_avg:60.83ms
step:1041/2330 train_time:63320ms step_avg:60.83ms
step:1042/2330 train_time:63382ms step_avg:60.83ms
step:1043/2330 train_time:63441ms step_avg:60.83ms
step:1044/2330 train_time:63503ms step_avg:60.83ms
step:1045/2330 train_time:63562ms step_avg:60.83ms
step:1046/2330 train_time:63624ms step_avg:60.83ms
step:1047/2330 train_time:63684ms step_avg:60.83ms
step:1048/2330 train_time:63747ms step_avg:60.83ms
step:1049/2330 train_time:63806ms step_avg:60.83ms
step:1050/2330 train_time:63870ms step_avg:60.83ms
step:1051/2330 train_time:63931ms step_avg:60.83ms
step:1052/2330 train_time:63996ms step_avg:60.83ms
step:1053/2330 train_time:64055ms step_avg:60.83ms
step:1054/2330 train_time:64118ms step_avg:60.83ms
step:1055/2330 train_time:64177ms step_avg:60.83ms
step:1056/2330 train_time:64240ms step_avg:60.83ms
step:1057/2330 train_time:64299ms step_avg:60.83ms
step:1058/2330 train_time:64362ms step_avg:60.83ms
step:1059/2330 train_time:64421ms step_avg:60.83ms
step:1060/2330 train_time:64483ms step_avg:60.83ms
step:1061/2330 train_time:64542ms step_avg:60.83ms
step:1062/2330 train_time:64606ms step_avg:60.83ms
step:1063/2330 train_time:64665ms step_avg:60.83ms
step:1064/2330 train_time:64727ms step_avg:60.83ms
step:1065/2330 train_time:64789ms step_avg:60.83ms
step:1066/2330 train_time:64852ms step_avg:60.84ms
step:1067/2330 train_time:64912ms step_avg:60.84ms
step:1068/2330 train_time:64975ms step_avg:60.84ms
step:1069/2330 train_time:65035ms step_avg:60.84ms
step:1070/2330 train_time:65098ms step_avg:60.84ms
step:1071/2330 train_time:65157ms step_avg:60.84ms
step:1072/2330 train_time:65219ms step_avg:60.84ms
step:1073/2330 train_time:65278ms step_avg:60.84ms
step:1074/2330 train_time:65342ms step_avg:60.84ms
step:1075/2330 train_time:65401ms step_avg:60.84ms
step:1076/2330 train_time:65462ms step_avg:60.84ms
step:1077/2330 train_time:65521ms step_avg:60.84ms
step:1078/2330 train_time:65584ms step_avg:60.84ms
step:1079/2330 train_time:65643ms step_avg:60.84ms
step:1080/2330 train_time:65706ms step_avg:60.84ms
step:1081/2330 train_time:65767ms step_avg:60.84ms
step:1082/2330 train_time:65830ms step_avg:60.84ms
step:1083/2330 train_time:65892ms step_avg:60.84ms
step:1084/2330 train_time:65955ms step_avg:60.84ms
step:1085/2330 train_time:66014ms step_avg:60.84ms
step:1086/2330 train_time:66078ms step_avg:60.85ms
step:1087/2330 train_time:66137ms step_avg:60.84ms
step:1088/2330 train_time:66199ms step_avg:60.84ms
step:1089/2330 train_time:66258ms step_avg:60.84ms
step:1090/2330 train_time:66321ms step_avg:60.84ms
step:1091/2330 train_time:66380ms step_avg:60.84ms
step:1092/2330 train_time:66442ms step_avg:60.84ms
step:1093/2330 train_time:66501ms step_avg:60.84ms
step:1094/2330 train_time:66564ms step_avg:60.84ms
step:1095/2330 train_time:66624ms step_avg:60.84ms
step:1096/2330 train_time:66687ms step_avg:60.85ms
step:1097/2330 train_time:66747ms step_avg:60.85ms
step:1098/2330 train_time:66811ms step_avg:60.85ms
step:1099/2330 train_time:66871ms step_avg:60.85ms
step:1100/2330 train_time:66934ms step_avg:60.85ms
step:1101/2330 train_time:66996ms step_avg:60.85ms
step:1102/2330 train_time:67058ms step_avg:60.85ms
step:1103/2330 train_time:67118ms step_avg:60.85ms
step:1104/2330 train_time:67180ms step_avg:60.85ms
step:1105/2330 train_time:67239ms step_avg:60.85ms
step:1106/2330 train_time:67302ms step_avg:60.85ms
step:1107/2330 train_time:67361ms step_avg:60.85ms
step:1108/2330 train_time:67423ms step_avg:60.85ms
step:1109/2330 train_time:67482ms step_avg:60.85ms
step:1110/2330 train_time:67545ms step_avg:60.85ms
step:1111/2330 train_time:67604ms step_avg:60.85ms
step:1112/2330 train_time:67667ms step_avg:60.85ms
step:1113/2330 train_time:67727ms step_avg:60.85ms
step:1114/2330 train_time:67790ms step_avg:60.85ms
step:1115/2330 train_time:67850ms step_avg:60.85ms
step:1116/2330 train_time:67914ms step_avg:60.86ms
step:1117/2330 train_time:67974ms step_avg:60.85ms
step:1118/2330 train_time:68038ms step_avg:60.86ms
step:1119/2330 train_time:68098ms step_avg:60.86ms
step:1120/2330 train_time:68162ms step_avg:60.86ms
step:1121/2330 train_time:68222ms step_avg:60.86ms
step:1122/2330 train_time:68284ms step_avg:60.86ms
step:1123/2330 train_time:68344ms step_avg:60.86ms
step:1124/2330 train_time:68406ms step_avg:60.86ms
step:1125/2330 train_time:68466ms step_avg:60.86ms
step:1126/2330 train_time:68529ms step_avg:60.86ms
step:1127/2330 train_time:68589ms step_avg:60.86ms
step:1128/2330 train_time:68651ms step_avg:60.86ms
step:1129/2330 train_time:68711ms step_avg:60.86ms
step:1130/2330 train_time:68773ms step_avg:60.86ms
step:1131/2330 train_time:68833ms step_avg:60.86ms
step:1132/2330 train_time:68896ms step_avg:60.86ms
step:1133/2330 train_time:68956ms step_avg:60.86ms
step:1134/2330 train_time:69019ms step_avg:60.86ms
step:1135/2330 train_time:69078ms step_avg:60.86ms
step:1136/2330 train_time:69142ms step_avg:60.86ms
step:1137/2330 train_time:69202ms step_avg:60.86ms
step:1138/2330 train_time:69264ms step_avg:60.86ms
step:1139/2330 train_time:69325ms step_avg:60.86ms
step:1140/2330 train_time:69388ms step_avg:60.87ms
step:1141/2330 train_time:69447ms step_avg:60.87ms
step:1142/2330 train_time:69509ms step_avg:60.87ms
step:1143/2330 train_time:69568ms step_avg:60.86ms
step:1144/2330 train_time:69632ms step_avg:60.87ms
step:1145/2330 train_time:69692ms step_avg:60.87ms
step:1146/2330 train_time:69754ms step_avg:60.87ms
step:1147/2330 train_time:69814ms step_avg:60.87ms
step:1148/2330 train_time:69877ms step_avg:60.87ms
step:1149/2330 train_time:69937ms step_avg:60.87ms
step:1150/2330 train_time:70000ms step_avg:60.87ms
step:1151/2330 train_time:70060ms step_avg:60.87ms
step:1152/2330 train_time:70122ms step_avg:60.87ms
step:1153/2330 train_time:70182ms step_avg:60.87ms
step:1154/2330 train_time:70245ms step_avg:60.87ms
step:1155/2330 train_time:70306ms step_avg:60.87ms
step:1156/2330 train_time:70369ms step_avg:60.87ms
step:1157/2330 train_time:70428ms step_avg:60.87ms
step:1158/2330 train_time:70491ms step_avg:60.87ms
step:1159/2330 train_time:70550ms step_avg:60.87ms
step:1160/2330 train_time:70613ms step_avg:60.87ms
step:1161/2330 train_time:70672ms step_avg:60.87ms
step:1162/2330 train_time:70734ms step_avg:60.87ms
step:1163/2330 train_time:70795ms step_avg:60.87ms
step:1164/2330 train_time:70857ms step_avg:60.87ms
step:1165/2330 train_time:70916ms step_avg:60.87ms
step:1166/2330 train_time:70979ms step_avg:60.87ms
step:1167/2330 train_time:71039ms step_avg:60.87ms
step:1168/2330 train_time:71103ms step_avg:60.88ms
step:1169/2330 train_time:71162ms step_avg:60.87ms
step:1170/2330 train_time:71225ms step_avg:60.88ms
step:1171/2330 train_time:71284ms step_avg:60.87ms
step:1172/2330 train_time:71347ms step_avg:60.88ms
step:1173/2330 train_time:71407ms step_avg:60.88ms
step:1174/2330 train_time:71469ms step_avg:60.88ms
step:1175/2330 train_time:71529ms step_avg:60.88ms
step:1176/2330 train_time:71593ms step_avg:60.88ms
step:1177/2330 train_time:71651ms step_avg:60.88ms
step:1178/2330 train_time:71715ms step_avg:60.88ms
step:1179/2330 train_time:71774ms step_avg:60.88ms
step:1180/2330 train_time:71836ms step_avg:60.88ms
step:1181/2330 train_time:71897ms step_avg:60.88ms
step:1182/2330 train_time:71959ms step_avg:60.88ms
step:1183/2330 train_time:72019ms step_avg:60.88ms
step:1184/2330 train_time:72083ms step_avg:60.88ms
step:1185/2330 train_time:72142ms step_avg:60.88ms
step:1186/2330 train_time:72205ms step_avg:60.88ms
step:1187/2330 train_time:72265ms step_avg:60.88ms
step:1188/2330 train_time:72327ms step_avg:60.88ms
step:1189/2330 train_time:72387ms step_avg:60.88ms
step:1190/2330 train_time:72449ms step_avg:60.88ms
step:1191/2330 train_time:72510ms step_avg:60.88ms
step:1192/2330 train_time:72573ms step_avg:60.88ms
step:1193/2330 train_time:72633ms step_avg:60.88ms
step:1194/2330 train_time:72696ms step_avg:60.88ms
step:1195/2330 train_time:72755ms step_avg:60.88ms
step:1196/2330 train_time:72818ms step_avg:60.88ms
step:1197/2330 train_time:72877ms step_avg:60.88ms
step:1198/2330 train_time:72941ms step_avg:60.89ms
step:1199/2330 train_time:73000ms step_avg:60.88ms
step:1200/2330 train_time:73063ms step_avg:60.89ms
step:1201/2330 train_time:73122ms step_avg:60.88ms
step:1202/2330 train_time:73185ms step_avg:60.89ms
step:1203/2330 train_time:73245ms step_avg:60.89ms
step:1204/2330 train_time:73307ms step_avg:60.89ms
step:1205/2330 train_time:73367ms step_avg:60.89ms
step:1206/2330 train_time:73431ms step_avg:60.89ms
step:1207/2330 train_time:73491ms step_avg:60.89ms
step:1208/2330 train_time:73554ms step_avg:60.89ms
step:1209/2330 train_time:73614ms step_avg:60.89ms
step:1210/2330 train_time:73676ms step_avg:60.89ms
step:1211/2330 train_time:73736ms step_avg:60.89ms
step:1212/2330 train_time:73798ms step_avg:60.89ms
step:1213/2330 train_time:73857ms step_avg:60.89ms
step:1214/2330 train_time:73919ms step_avg:60.89ms
step:1215/2330 train_time:73979ms step_avg:60.89ms
step:1216/2330 train_time:74041ms step_avg:60.89ms
step:1217/2330 train_time:74101ms step_avg:60.89ms
step:1218/2330 train_time:74164ms step_avg:60.89ms
step:1219/2330 train_time:74224ms step_avg:60.89ms
step:1220/2330 train_time:74287ms step_avg:60.89ms
step:1221/2330 train_time:74346ms step_avg:60.89ms
step:1222/2330 train_time:74410ms step_avg:60.89ms
step:1223/2330 train_time:74469ms step_avg:60.89ms
step:1224/2330 train_time:74533ms step_avg:60.89ms
step:1225/2330 train_time:74593ms step_avg:60.89ms
step:1226/2330 train_time:74655ms step_avg:60.89ms
step:1227/2330 train_time:74715ms step_avg:60.89ms
step:1228/2330 train_time:74777ms step_avg:60.89ms
step:1229/2330 train_time:74838ms step_avg:60.89ms
step:1230/2330 train_time:74901ms step_avg:60.89ms
step:1231/2330 train_time:74960ms step_avg:60.89ms
step:1232/2330 train_time:75022ms step_avg:60.89ms
step:1233/2330 train_time:75082ms step_avg:60.89ms
step:1234/2330 train_time:75145ms step_avg:60.90ms
step:1235/2330 train_time:75204ms step_avg:60.89ms
step:1236/2330 train_time:75267ms step_avg:60.90ms
step:1237/2330 train_time:75328ms step_avg:60.90ms
step:1238/2330 train_time:75391ms step_avg:60.90ms
step:1239/2330 train_time:75450ms step_avg:60.90ms
step:1240/2330 train_time:75513ms step_avg:60.90ms
step:1241/2330 train_time:75573ms step_avg:60.90ms
step:1242/2330 train_time:75636ms step_avg:60.90ms
step:1243/2330 train_time:75696ms step_avg:60.90ms
step:1244/2330 train_time:75759ms step_avg:60.90ms
step:1245/2330 train_time:75818ms step_avg:60.90ms
step:1246/2330 train_time:75881ms step_avg:60.90ms
step:1247/2330 train_time:75941ms step_avg:60.90ms
step:1248/2330 train_time:76003ms step_avg:60.90ms
step:1249/2330 train_time:76062ms step_avg:60.90ms
step:1250/2330 train_time:76125ms step_avg:60.90ms
step:1250/2330 val_loss:3.7848 train_time:76198ms step_avg:60.96ms
step:1251/2330 train_time:76218ms step_avg:60.93ms
step:1252/2330 train_time:76250ms step_avg:60.90ms
step:1253/2330 train_time:76315ms step_avg:60.91ms
step:1254/2330 train_time:76382ms step_avg:60.91ms
step:1255/2330 train_time:76443ms step_avg:60.91ms
step:1256/2330 train_time:76506ms step_avg:60.91ms
step:1257/2330 train_time:76565ms step_avg:60.91ms
step:1258/2330 train_time:76627ms step_avg:60.91ms
step:1259/2330 train_time:76686ms step_avg:60.91ms
step:1260/2330 train_time:76748ms step_avg:60.91ms
step:1261/2330 train_time:76807ms step_avg:60.91ms
step:1262/2330 train_time:76869ms step_avg:60.91ms
step:1263/2330 train_time:76929ms step_avg:60.91ms
step:1264/2330 train_time:76991ms step_avg:60.91ms
step:1265/2330 train_time:77049ms step_avg:60.91ms
step:1266/2330 train_time:77114ms step_avg:60.91ms
step:1267/2330 train_time:77173ms step_avg:60.91ms
step:1268/2330 train_time:77237ms step_avg:60.91ms
step:1269/2330 train_time:77299ms step_avg:60.91ms
step:1270/2330 train_time:77363ms step_avg:60.92ms
step:1271/2330 train_time:77423ms step_avg:60.92ms
step:1272/2330 train_time:77486ms step_avg:60.92ms
step:1273/2330 train_time:77545ms step_avg:60.92ms
step:1274/2330 train_time:77607ms step_avg:60.92ms
step:1275/2330 train_time:77666ms step_avg:60.91ms
step:1276/2330 train_time:77728ms step_avg:60.92ms
step:1277/2330 train_time:77788ms step_avg:60.91ms
step:1278/2330 train_time:77850ms step_avg:60.92ms
step:1279/2330 train_time:77909ms step_avg:60.91ms
step:1280/2330 train_time:77971ms step_avg:60.91ms
step:1281/2330 train_time:78030ms step_avg:60.91ms
step:1282/2330 train_time:78093ms step_avg:60.91ms
step:1283/2330 train_time:78154ms step_avg:60.91ms
step:1284/2330 train_time:78217ms step_avg:60.92ms
step:1285/2330 train_time:78277ms step_avg:60.92ms
step:1286/2330 train_time:78340ms step_avg:60.92ms
step:1287/2330 train_time:78400ms step_avg:60.92ms
step:1288/2330 train_time:78462ms step_avg:60.92ms
step:1289/2330 train_time:78523ms step_avg:60.92ms
step:1290/2330 train_time:78584ms step_avg:60.92ms
step:1291/2330 train_time:78643ms step_avg:60.92ms
step:1292/2330 train_time:78705ms step_avg:60.92ms
step:1293/2330 train_time:78764ms step_avg:60.92ms
step:1294/2330 train_time:78827ms step_avg:60.92ms
step:1295/2330 train_time:78886ms step_avg:60.92ms
step:1296/2330 train_time:78948ms step_avg:60.92ms
step:1297/2330 train_time:79008ms step_avg:60.92ms
step:1298/2330 train_time:79070ms step_avg:60.92ms
step:1299/2330 train_time:79131ms step_avg:60.92ms
step:1300/2330 train_time:79194ms step_avg:60.92ms
step:1301/2330 train_time:79256ms step_avg:60.92ms
step:1302/2330 train_time:79320ms step_avg:60.92ms
step:1303/2330 train_time:79379ms step_avg:60.92ms
step:1304/2330 train_time:79442ms step_avg:60.92ms
step:1305/2330 train_time:79501ms step_avg:60.92ms
step:1306/2330 train_time:79564ms step_avg:60.92ms
step:1307/2330 train_time:79623ms step_avg:60.92ms
step:1308/2330 train_time:79685ms step_avg:60.92ms
step:1309/2330 train_time:79744ms step_avg:60.92ms
step:1310/2330 train_time:79807ms step_avg:60.92ms
step:1311/2330 train_time:79865ms step_avg:60.92ms
step:1312/2330 train_time:79928ms step_avg:60.92ms
step:1313/2330 train_time:79987ms step_avg:60.92ms
step:1314/2330 train_time:80050ms step_avg:60.92ms
step:1315/2330 train_time:80110ms step_avg:60.92ms
step:1316/2330 train_time:80173ms step_avg:60.92ms
step:1317/2330 train_time:80234ms step_avg:60.92ms
step:1318/2330 train_time:80297ms step_avg:60.92ms
step:1319/2330 train_time:80357ms step_avg:60.92ms
step:1320/2330 train_time:80420ms step_avg:60.92ms
step:1321/2330 train_time:80479ms step_avg:60.92ms
step:1322/2330 train_time:80542ms step_avg:60.92ms
step:1323/2330 train_time:80602ms step_avg:60.92ms
step:1324/2330 train_time:80664ms step_avg:60.92ms
step:1325/2330 train_time:80723ms step_avg:60.92ms
step:1326/2330 train_time:80786ms step_avg:60.92ms
step:1327/2330 train_time:80846ms step_avg:60.92ms
step:1328/2330 train_time:80908ms step_avg:60.92ms
step:1329/2330 train_time:80967ms step_avg:60.92ms
step:1330/2330 train_time:81030ms step_avg:60.92ms
step:1331/2330 train_time:81089ms step_avg:60.92ms
step:1332/2330 train_time:81153ms step_avg:60.93ms
step:1333/2330 train_time:81214ms step_avg:60.93ms
step:1334/2330 train_time:81276ms step_avg:60.93ms
step:1335/2330 train_time:81336ms step_avg:60.93ms
step:1336/2330 train_time:81399ms step_avg:60.93ms
step:1337/2330 train_time:81459ms step_avg:60.93ms
step:1338/2330 train_time:81521ms step_avg:60.93ms
step:1339/2330 train_time:81580ms step_avg:60.93ms
step:1340/2330 train_time:81642ms step_avg:60.93ms
step:1341/2330 train_time:81702ms step_avg:60.93ms
step:1342/2330 train_time:81764ms step_avg:60.93ms
step:1343/2330 train_time:81823ms step_avg:60.93ms
step:1344/2330 train_time:81886ms step_avg:60.93ms
step:1345/2330 train_time:81945ms step_avg:60.93ms
step:1346/2330 train_time:82008ms step_avg:60.93ms
step:1347/2330 train_time:82067ms step_avg:60.93ms
step:1348/2330 train_time:82130ms step_avg:60.93ms
step:1349/2330 train_time:82190ms step_avg:60.93ms
step:1350/2330 train_time:82253ms step_avg:60.93ms
step:1351/2330 train_time:82314ms step_avg:60.93ms
step:1352/2330 train_time:82377ms step_avg:60.93ms
step:1353/2330 train_time:82436ms step_avg:60.93ms
step:1354/2330 train_time:82499ms step_avg:60.93ms
step:1355/2330 train_time:82558ms step_avg:60.93ms
step:1356/2330 train_time:82623ms step_avg:60.93ms
step:1357/2330 train_time:82681ms step_avg:60.93ms
step:1358/2330 train_time:82744ms step_avg:60.93ms
step:1359/2330 train_time:82803ms step_avg:60.93ms
step:1360/2330 train_time:82865ms step_avg:60.93ms
step:1361/2330 train_time:82925ms step_avg:60.93ms
step:1362/2330 train_time:82987ms step_avg:60.93ms
step:1363/2330 train_time:83047ms step_avg:60.93ms
step:1364/2330 train_time:83110ms step_avg:60.93ms
step:1365/2330 train_time:83169ms step_avg:60.93ms
step:1366/2330 train_time:83232ms step_avg:60.93ms
step:1367/2330 train_time:83293ms step_avg:60.93ms
step:1368/2330 train_time:83356ms step_avg:60.93ms
step:1369/2330 train_time:83416ms step_avg:60.93ms
step:1370/2330 train_time:83479ms step_avg:60.93ms
step:1371/2330 train_time:83538ms step_avg:60.93ms
step:1372/2330 train_time:83600ms step_avg:60.93ms
step:1373/2330 train_time:83660ms step_avg:60.93ms
step:1374/2330 train_time:83724ms step_avg:60.93ms
step:1375/2330 train_time:83782ms step_avg:60.93ms
step:1376/2330 train_time:83845ms step_avg:60.93ms
step:1377/2330 train_time:83904ms step_avg:60.93ms
step:1378/2330 train_time:83966ms step_avg:60.93ms
step:1379/2330 train_time:84026ms step_avg:60.93ms
step:1380/2330 train_time:84088ms step_avg:60.93ms
step:1381/2330 train_time:84148ms step_avg:60.93ms
step:1382/2330 train_time:84211ms step_avg:60.93ms
step:1383/2330 train_time:84271ms step_avg:60.93ms
step:1384/2330 train_time:84334ms step_avg:60.93ms
step:1385/2330 train_time:84393ms step_avg:60.93ms
step:1386/2330 train_time:84457ms step_avg:60.94ms
step:1387/2330 train_time:84517ms step_avg:60.94ms
step:1388/2330 train_time:84580ms step_avg:60.94ms
step:1389/2330 train_time:84640ms step_avg:60.94ms
step:1390/2330 train_time:84702ms step_avg:60.94ms
step:1391/2330 train_time:84761ms step_avg:60.94ms
step:1392/2330 train_time:84825ms step_avg:60.94ms
step:1393/2330 train_time:84884ms step_avg:60.94ms
step:1394/2330 train_time:84946ms step_avg:60.94ms
step:1395/2330 train_time:85006ms step_avg:60.94ms
step:1396/2330 train_time:85069ms step_avg:60.94ms
step:1397/2330 train_time:85129ms step_avg:60.94ms
step:1398/2330 train_time:85191ms step_avg:60.94ms
step:1399/2330 train_time:85252ms step_avg:60.94ms
step:1400/2330 train_time:85315ms step_avg:60.94ms
step:1401/2330 train_time:85374ms step_avg:60.94ms
step:1402/2330 train_time:85437ms step_avg:60.94ms
step:1403/2330 train_time:85497ms step_avg:60.94ms
step:1404/2330 train_time:85560ms step_avg:60.94ms
step:1405/2330 train_time:85620ms step_avg:60.94ms
step:1406/2330 train_time:85682ms step_avg:60.94ms
step:1407/2330 train_time:85741ms step_avg:60.94ms
step:1408/2330 train_time:85803ms step_avg:60.94ms
step:1409/2330 train_time:85863ms step_avg:60.94ms
step:1410/2330 train_time:85926ms step_avg:60.94ms
step:1411/2330 train_time:85986ms step_avg:60.94ms
step:1412/2330 train_time:86048ms step_avg:60.94ms
step:1413/2330 train_time:86108ms step_avg:60.94ms
step:1414/2330 train_time:86171ms step_avg:60.94ms
step:1415/2330 train_time:86230ms step_avg:60.94ms
step:1416/2330 train_time:86293ms step_avg:60.94ms
step:1417/2330 train_time:86353ms step_avg:60.94ms
step:1418/2330 train_time:86417ms step_avg:60.94ms
step:1419/2330 train_time:86475ms step_avg:60.94ms
step:1420/2330 train_time:86538ms step_avg:60.94ms
step:1421/2330 train_time:86598ms step_avg:60.94ms
step:1422/2330 train_time:86660ms step_avg:60.94ms
step:1423/2330 train_time:86720ms step_avg:60.94ms
step:1424/2330 train_time:86782ms step_avg:60.94ms
step:1425/2330 train_time:86841ms step_avg:60.94ms
step:1426/2330 train_time:86904ms step_avg:60.94ms
step:1427/2330 train_time:86963ms step_avg:60.94ms
step:1428/2330 train_time:87027ms step_avg:60.94ms
step:1429/2330 train_time:87086ms step_avg:60.94ms
step:1430/2330 train_time:87149ms step_avg:60.94ms
step:1431/2330 train_time:87208ms step_avg:60.94ms
step:1432/2330 train_time:87271ms step_avg:60.94ms
step:1433/2330 train_time:87331ms step_avg:60.94ms
step:1434/2330 train_time:87394ms step_avg:60.94ms
step:1435/2330 train_time:87454ms step_avg:60.94ms
step:1436/2330 train_time:87518ms step_avg:60.95ms
step:1437/2330 train_time:87577ms step_avg:60.94ms
step:1438/2330 train_time:87639ms step_avg:60.95ms
step:1439/2330 train_time:87699ms step_avg:60.94ms
step:1440/2330 train_time:87761ms step_avg:60.95ms
step:1441/2330 train_time:87821ms step_avg:60.94ms
step:1442/2330 train_time:87883ms step_avg:60.95ms
step:1443/2330 train_time:87943ms step_avg:60.94ms
step:1444/2330 train_time:88006ms step_avg:60.95ms
step:1445/2330 train_time:88066ms step_avg:60.95ms
step:1446/2330 train_time:88129ms step_avg:60.95ms
step:1447/2330 train_time:88188ms step_avg:60.95ms
step:1448/2330 train_time:88250ms step_avg:60.95ms
step:1449/2330 train_time:88310ms step_avg:60.95ms
step:1450/2330 train_time:88372ms step_avg:60.95ms
step:1451/2330 train_time:88432ms step_avg:60.95ms
step:1452/2330 train_time:88495ms step_avg:60.95ms
step:1453/2330 train_time:88556ms step_avg:60.95ms
step:1454/2330 train_time:88618ms step_avg:60.95ms
step:1455/2330 train_time:88677ms step_avg:60.95ms
step:1456/2330 train_time:88740ms step_avg:60.95ms
step:1457/2330 train_time:88799ms step_avg:60.95ms
step:1458/2330 train_time:88862ms step_avg:60.95ms
step:1459/2330 train_time:88922ms step_avg:60.95ms
step:1460/2330 train_time:88984ms step_avg:60.95ms
step:1461/2330 train_time:89043ms step_avg:60.95ms
step:1462/2330 train_time:89106ms step_avg:60.95ms
step:1463/2330 train_time:89166ms step_avg:60.95ms
step:1464/2330 train_time:89229ms step_avg:60.95ms
step:1465/2330 train_time:89288ms step_avg:60.95ms
step:1466/2330 train_time:89352ms step_avg:60.95ms
step:1467/2330 train_time:89412ms step_avg:60.95ms
step:1468/2330 train_time:89475ms step_avg:60.95ms
step:1469/2330 train_time:89535ms step_avg:60.95ms
step:1470/2330 train_time:89598ms step_avg:60.95ms
step:1471/2330 train_time:89657ms step_avg:60.95ms
step:1472/2330 train_time:89720ms step_avg:60.95ms
step:1473/2330 train_time:89779ms step_avg:60.95ms
step:1474/2330 train_time:89842ms step_avg:60.95ms
step:1475/2330 train_time:89901ms step_avg:60.95ms
step:1476/2330 train_time:89963ms step_avg:60.95ms
step:1477/2330 train_time:90022ms step_avg:60.95ms
step:1478/2330 train_time:90086ms step_avg:60.95ms
step:1479/2330 train_time:90145ms step_avg:60.95ms
step:1480/2330 train_time:90208ms step_avg:60.95ms
step:1481/2330 train_time:90267ms step_avg:60.95ms
step:1482/2330 train_time:90330ms step_avg:60.95ms
step:1483/2330 train_time:90389ms step_avg:60.95ms
step:1484/2330 train_time:90452ms step_avg:60.95ms
step:1485/2330 train_time:90513ms step_avg:60.95ms
step:1486/2330 train_time:90576ms step_avg:60.95ms
step:1487/2330 train_time:90635ms step_avg:60.95ms
step:1488/2330 train_time:90698ms step_avg:60.95ms
step:1489/2330 train_time:90757ms step_avg:60.95ms
step:1490/2330 train_time:90820ms step_avg:60.95ms
step:1491/2330 train_time:90879ms step_avg:60.95ms
step:1492/2330 train_time:90942ms step_avg:60.95ms
step:1493/2330 train_time:91001ms step_avg:60.95ms
step:1494/2330 train_time:91064ms step_avg:60.95ms
step:1495/2330 train_time:91124ms step_avg:60.95ms
step:1496/2330 train_time:91187ms step_avg:60.95ms
step:1497/2330 train_time:91246ms step_avg:60.95ms
step:1498/2330 train_time:91308ms step_avg:60.95ms
step:1499/2330 train_time:91368ms step_avg:60.95ms
step:1500/2330 train_time:91430ms step_avg:60.95ms
step:1500/2330 val_loss:3.6851 train_time:91503ms step_avg:61.00ms
step:1501/2330 train_time:91524ms step_avg:60.98ms
step:1502/2330 train_time:91555ms step_avg:60.96ms
step:1503/2330 train_time:91617ms step_avg:60.96ms
step:1504/2330 train_time:91684ms step_avg:60.96ms
step:1505/2330 train_time:91744ms step_avg:60.96ms
step:1506/2330 train_time:91806ms step_avg:60.96ms
step:1507/2330 train_time:91866ms step_avg:60.96ms
step:1508/2330 train_time:91928ms step_avg:60.96ms
step:1509/2330 train_time:91987ms step_avg:60.96ms
step:1510/2330 train_time:92048ms step_avg:60.96ms
step:1511/2330 train_time:92107ms step_avg:60.96ms
step:1512/2330 train_time:92170ms step_avg:60.96ms
step:1513/2330 train_time:92228ms step_avg:60.96ms
step:1514/2330 train_time:92290ms step_avg:60.96ms
step:1515/2330 train_time:92349ms step_avg:60.96ms
step:1516/2330 train_time:92411ms step_avg:60.96ms
step:1517/2330 train_time:92470ms step_avg:60.96ms
step:1518/2330 train_time:92533ms step_avg:60.96ms
step:1519/2330 train_time:92595ms step_avg:60.96ms
step:1520/2330 train_time:92661ms step_avg:60.96ms
step:1521/2330 train_time:92721ms step_avg:60.96ms
step:1522/2330 train_time:92784ms step_avg:60.96ms
step:1523/2330 train_time:92844ms step_avg:60.96ms
step:1524/2330 train_time:92906ms step_avg:60.96ms
step:1525/2330 train_time:92966ms step_avg:60.96ms
step:1526/2330 train_time:93028ms step_avg:60.96ms
step:1527/2330 train_time:93087ms step_avg:60.96ms
step:1528/2330 train_time:93149ms step_avg:60.96ms
step:1529/2330 train_time:93209ms step_avg:60.96ms
step:1530/2330 train_time:93270ms step_avg:60.96ms
step:1531/2330 train_time:93330ms step_avg:60.96ms
step:1532/2330 train_time:93393ms step_avg:60.96ms
step:1533/2330 train_time:93453ms step_avg:60.96ms
step:1534/2330 train_time:93516ms step_avg:60.96ms
step:1535/2330 train_time:93576ms step_avg:60.96ms
step:1536/2330 train_time:93641ms step_avg:60.96ms
step:1537/2330 train_time:93702ms step_avg:60.96ms
step:1538/2330 train_time:93765ms step_avg:60.97ms
step:1539/2330 train_time:93826ms step_avg:60.97ms
step:1540/2330 train_time:93889ms step_avg:60.97ms
step:1541/2330 train_time:93949ms step_avg:60.97ms
step:1542/2330 train_time:94012ms step_avg:60.97ms
step:1543/2330 train_time:94072ms step_avg:60.97ms
step:1544/2330 train_time:94134ms step_avg:60.97ms
step:1545/2330 train_time:94194ms step_avg:60.97ms
step:1546/2330 train_time:94256ms step_avg:60.97ms
step:1547/2330 train_time:94316ms step_avg:60.97ms
step:1548/2330 train_time:94379ms step_avg:60.97ms
step:1549/2330 train_time:94440ms step_avg:60.97ms
step:1550/2330 train_time:94503ms step_avg:60.97ms
step:1551/2330 train_time:94563ms step_avg:60.97ms
step:1552/2330 train_time:94625ms step_avg:60.97ms
step:1553/2330 train_time:94685ms step_avg:60.97ms
step:1554/2330 train_time:94749ms step_avg:60.97ms
step:1555/2330 train_time:94809ms step_avg:60.97ms
step:1556/2330 train_time:94872ms step_avg:60.97ms
step:1557/2330 train_time:94933ms step_avg:60.97ms
step:1558/2330 train_time:94997ms step_avg:60.97ms
step:1559/2330 train_time:95057ms step_avg:60.97ms
step:1560/2330 train_time:95120ms step_avg:60.97ms
step:1561/2330 train_time:95180ms step_avg:60.97ms
step:1562/2330 train_time:95243ms step_avg:60.98ms
step:1563/2330 train_time:95302ms step_avg:60.97ms
step:1564/2330 train_time:95365ms step_avg:60.97ms
step:1565/2330 train_time:95424ms step_avg:60.97ms
step:1566/2330 train_time:95487ms step_avg:60.98ms
step:1567/2330 train_time:95548ms step_avg:60.97ms
step:1568/2330 train_time:95611ms step_avg:60.98ms
step:1569/2330 train_time:95671ms step_avg:60.98ms
step:1570/2330 train_time:95734ms step_avg:60.98ms
step:1571/2330 train_time:95795ms step_avg:60.98ms
step:1572/2330 train_time:95859ms step_avg:60.98ms
step:1573/2330 train_time:95919ms step_avg:60.98ms
step:1574/2330 train_time:95982ms step_avg:60.98ms
step:1575/2330 train_time:96042ms step_avg:60.98ms
step:1576/2330 train_time:96105ms step_avg:60.98ms
step:1577/2330 train_time:96164ms step_avg:60.98ms
step:1578/2330 train_time:96228ms step_avg:60.98ms
step:1579/2330 train_time:96287ms step_avg:60.98ms
step:1580/2330 train_time:96351ms step_avg:60.98ms
step:1581/2330 train_time:96411ms step_avg:60.98ms
step:1582/2330 train_time:96474ms step_avg:60.98ms
step:1583/2330 train_time:96534ms step_avg:60.98ms
step:1584/2330 train_time:96597ms step_avg:60.98ms
step:1585/2330 train_time:96657ms step_avg:60.98ms
step:1586/2330 train_time:96720ms step_avg:60.98ms
step:1587/2330 train_time:96781ms step_avg:60.98ms
step:1588/2330 train_time:96844ms step_avg:60.98ms
step:1589/2330 train_time:96903ms step_avg:60.98ms
step:1590/2330 train_time:96967ms step_avg:60.99ms
step:1591/2330 train_time:97027ms step_avg:60.99ms
step:1592/2330 train_time:97090ms step_avg:60.99ms
step:1593/2330 train_time:97150ms step_avg:60.99ms
step:1594/2330 train_time:97212ms step_avg:60.99ms
step:1595/2330 train_time:97273ms step_avg:60.99ms
step:1596/2330 train_time:97337ms step_avg:60.99ms
step:1597/2330 train_time:97397ms step_avg:60.99ms
step:1598/2330 train_time:97459ms step_avg:60.99ms
step:1599/2330 train_time:97520ms step_avg:60.99ms
step:1600/2330 train_time:97583ms step_avg:60.99ms
step:1601/2330 train_time:97643ms step_avg:60.99ms
step:1602/2330 train_time:97706ms step_avg:60.99ms
step:1603/2330 train_time:97766ms step_avg:60.99ms
step:1604/2330 train_time:97830ms step_avg:60.99ms
step:1605/2330 train_time:97890ms step_avg:60.99ms
step:1606/2330 train_time:97953ms step_avg:60.99ms
step:1607/2330 train_time:98013ms step_avg:60.99ms
step:1608/2330 train_time:98077ms step_avg:60.99ms
step:1609/2330 train_time:98137ms step_avg:60.99ms
step:1610/2330 train_time:98199ms step_avg:60.99ms
step:1611/2330 train_time:98259ms step_avg:60.99ms
step:1612/2330 train_time:98323ms step_avg:60.99ms
step:1613/2330 train_time:98382ms step_avg:60.99ms
step:1614/2330 train_time:98445ms step_avg:60.99ms
step:1615/2330 train_time:98506ms step_avg:60.99ms
step:1616/2330 train_time:98569ms step_avg:61.00ms
step:1617/2330 train_time:98629ms step_avg:60.99ms
step:1618/2330 train_time:98691ms step_avg:61.00ms
step:1619/2330 train_time:98751ms step_avg:61.00ms
step:1620/2330 train_time:98815ms step_avg:61.00ms
step:1621/2330 train_time:98876ms step_avg:61.00ms
step:1622/2330 train_time:98940ms step_avg:61.00ms
step:1623/2330 train_time:98998ms step_avg:61.00ms
step:1624/2330 train_time:99062ms step_avg:61.00ms
step:1625/2330 train_time:99121ms step_avg:61.00ms
step:1626/2330 train_time:99183ms step_avg:61.00ms
step:1627/2330 train_time:99244ms step_avg:61.00ms
step:1628/2330 train_time:99306ms step_avg:61.00ms
step:1629/2330 train_time:99366ms step_avg:61.00ms
step:1630/2330 train_time:99429ms step_avg:61.00ms
step:1631/2330 train_time:99489ms step_avg:61.00ms
step:1632/2330 train_time:99552ms step_avg:61.00ms
step:1633/2330 train_time:99612ms step_avg:61.00ms
step:1634/2330 train_time:99676ms step_avg:61.00ms
step:1635/2330 train_time:99737ms step_avg:61.00ms
step:1636/2330 train_time:99800ms step_avg:61.00ms
step:1637/2330 train_time:99860ms step_avg:61.00ms
step:1638/2330 train_time:99922ms step_avg:61.00ms
step:1639/2330 train_time:99982ms step_avg:61.00ms
step:1640/2330 train_time:100046ms step_avg:61.00ms
step:1641/2330 train_time:100106ms step_avg:61.00ms
step:1642/2330 train_time:100169ms step_avg:61.00ms
step:1643/2330 train_time:100228ms step_avg:61.00ms
step:1644/2330 train_time:100291ms step_avg:61.00ms
step:1645/2330 train_time:100351ms step_avg:61.00ms
step:1646/2330 train_time:100415ms step_avg:61.01ms
step:1647/2330 train_time:100475ms step_avg:61.00ms
step:1648/2330 train_time:100537ms step_avg:61.01ms
step:1649/2330 train_time:100598ms step_avg:61.01ms
step:1650/2330 train_time:100660ms step_avg:61.01ms
step:1651/2330 train_time:100721ms step_avg:61.01ms
step:1652/2330 train_time:100783ms step_avg:61.01ms
step:1653/2330 train_time:100843ms step_avg:61.01ms
step:1654/2330 train_time:100906ms step_avg:61.01ms
step:1655/2330 train_time:100966ms step_avg:61.01ms
step:1656/2330 train_time:101029ms step_avg:61.01ms
step:1657/2330 train_time:101089ms step_avg:61.01ms
step:1658/2330 train_time:101153ms step_avg:61.01ms
step:1659/2330 train_time:101212ms step_avg:61.01ms
step:1660/2330 train_time:101275ms step_avg:61.01ms
step:1661/2330 train_time:101335ms step_avg:61.01ms
step:1662/2330 train_time:101398ms step_avg:61.01ms
step:1663/2330 train_time:101458ms step_avg:61.01ms
step:1664/2330 train_time:101522ms step_avg:61.01ms
step:1665/2330 train_time:101581ms step_avg:61.01ms
step:1666/2330 train_time:101644ms step_avg:61.01ms
step:1667/2330 train_time:101703ms step_avg:61.01ms
step:1668/2330 train_time:101766ms step_avg:61.01ms
step:1669/2330 train_time:101826ms step_avg:61.01ms
step:1670/2330 train_time:101889ms step_avg:61.01ms
step:1671/2330 train_time:101950ms step_avg:61.01ms
step:1672/2330 train_time:102012ms step_avg:61.01ms
step:1673/2330 train_time:102072ms step_avg:61.01ms
step:1674/2330 train_time:102135ms step_avg:61.01ms
step:1675/2330 train_time:102195ms step_avg:61.01ms
step:1676/2330 train_time:102257ms step_avg:61.01ms
step:1677/2330 train_time:102316ms step_avg:61.01ms
step:1678/2330 train_time:102380ms step_avg:61.01ms
step:1679/2330 train_time:102441ms step_avg:61.01ms
step:1680/2330 train_time:102503ms step_avg:61.01ms
step:1681/2330 train_time:102563ms step_avg:61.01ms
step:1682/2330 train_time:102626ms step_avg:61.01ms
step:1683/2330 train_time:102685ms step_avg:61.01ms
step:1684/2330 train_time:102749ms step_avg:61.02ms
step:1685/2330 train_time:102809ms step_avg:61.01ms
step:1686/2330 train_time:102872ms step_avg:61.02ms
step:1687/2330 train_time:102933ms step_avg:61.02ms
step:1688/2330 train_time:102997ms step_avg:61.02ms
step:1689/2330 train_time:103057ms step_avg:61.02ms
step:1690/2330 train_time:103120ms step_avg:61.02ms
step:1691/2330 train_time:103180ms step_avg:61.02ms
step:1692/2330 train_time:103243ms step_avg:61.02ms
step:1693/2330 train_time:103302ms step_avg:61.02ms
step:1694/2330 train_time:103366ms step_avg:61.02ms
step:1695/2330 train_time:103426ms step_avg:61.02ms
step:1696/2330 train_time:103489ms step_avg:61.02ms
step:1697/2330 train_time:103549ms step_avg:61.02ms
step:1698/2330 train_time:103611ms step_avg:61.02ms
step:1699/2330 train_time:103672ms step_avg:61.02ms
step:1700/2330 train_time:103736ms step_avg:61.02ms
step:1701/2330 train_time:103795ms step_avg:61.02ms
step:1702/2330 train_time:103858ms step_avg:61.02ms
step:1703/2330 train_time:103918ms step_avg:61.02ms
step:1704/2330 train_time:103981ms step_avg:61.02ms
step:1705/2330 train_time:104042ms step_avg:61.02ms
step:1706/2330 train_time:104104ms step_avg:61.02ms
step:1707/2330 train_time:104165ms step_avg:61.02ms
step:1708/2330 train_time:104228ms step_avg:61.02ms
step:1709/2330 train_time:104288ms step_avg:61.02ms
step:1710/2330 train_time:104351ms step_avg:61.02ms
step:1711/2330 train_time:104411ms step_avg:61.02ms
step:1712/2330 train_time:104475ms step_avg:61.02ms
step:1713/2330 train_time:104535ms step_avg:61.02ms
step:1714/2330 train_time:104599ms step_avg:61.03ms
step:1715/2330 train_time:104660ms step_avg:61.03ms
step:1716/2330 train_time:104722ms step_avg:61.03ms
step:1717/2330 train_time:104782ms step_avg:61.03ms
step:1718/2330 train_time:104845ms step_avg:61.03ms
step:1719/2330 train_time:104904ms step_avg:61.03ms
step:1720/2330 train_time:104967ms step_avg:61.03ms
step:1721/2330 train_time:105026ms step_avg:61.03ms
step:1722/2330 train_time:105090ms step_avg:61.03ms
step:1723/2330 train_time:105150ms step_avg:61.03ms
step:1724/2330 train_time:105213ms step_avg:61.03ms
step:1725/2330 train_time:105273ms step_avg:61.03ms
step:1726/2330 train_time:105336ms step_avg:61.03ms
step:1727/2330 train_time:105397ms step_avg:61.03ms
step:1728/2330 train_time:105460ms step_avg:61.03ms
step:1729/2330 train_time:105521ms step_avg:61.03ms
step:1730/2330 train_time:105584ms step_avg:61.03ms
step:1731/2330 train_time:105644ms step_avg:61.03ms
step:1732/2330 train_time:105707ms step_avg:61.03ms
step:1733/2330 train_time:105768ms step_avg:61.03ms
step:1734/2330 train_time:105830ms step_avg:61.03ms
step:1735/2330 train_time:105890ms step_avg:61.03ms
step:1736/2330 train_time:105953ms step_avg:61.03ms
step:1737/2330 train_time:106014ms step_avg:61.03ms
step:1738/2330 train_time:106078ms step_avg:61.03ms
step:1739/2330 train_time:106138ms step_avg:61.03ms
step:1740/2330 train_time:106201ms step_avg:61.04ms
step:1741/2330 train_time:106261ms step_avg:61.03ms
step:1742/2330 train_time:106324ms step_avg:61.04ms
step:1743/2330 train_time:106384ms step_avg:61.03ms
step:1744/2330 train_time:106447ms step_avg:61.04ms
step:1745/2330 train_time:106508ms step_avg:61.04ms
step:1746/2330 train_time:106570ms step_avg:61.04ms
step:1747/2330 train_time:106630ms step_avg:61.04ms
step:1748/2330 train_time:106694ms step_avg:61.04ms
step:1749/2330 train_time:106755ms step_avg:61.04ms
step:1750/2330 train_time:106818ms step_avg:61.04ms
step:1750/2330 val_loss:3.6092 train_time:106890ms step_avg:61.08ms
step:1751/2330 train_time:106911ms step_avg:61.06ms
step:1752/2330 train_time:106941ms step_avg:61.04ms
step:1753/2330 train_time:107002ms step_avg:61.04ms
step:1754/2330 train_time:107072ms step_avg:61.04ms
step:1755/2330 train_time:107135ms step_avg:61.05ms
step:1756/2330 train_time:107198ms step_avg:61.05ms
step:1757/2330 train_time:107257ms step_avg:61.05ms
step:1758/2330 train_time:107319ms step_avg:61.05ms
step:1759/2330 train_time:107379ms step_avg:61.05ms
step:1760/2330 train_time:107441ms step_avg:61.05ms
step:1761/2330 train_time:107500ms step_avg:61.05ms
step:1762/2330 train_time:107562ms step_avg:61.05ms
step:1763/2330 train_time:107622ms step_avg:61.05ms
step:1764/2330 train_time:107684ms step_avg:61.05ms
step:1765/2330 train_time:107743ms step_avg:61.04ms
step:1766/2330 train_time:107809ms step_avg:61.05ms
step:1767/2330 train_time:107874ms step_avg:61.05ms
step:1768/2330 train_time:107940ms step_avg:61.05ms
step:1769/2330 train_time:108003ms step_avg:61.05ms
step:1770/2330 train_time:108068ms step_avg:61.06ms
step:1771/2330 train_time:108129ms step_avg:61.06ms
step:1772/2330 train_time:108190ms step_avg:61.06ms
step:1773/2330 train_time:108250ms step_avg:61.05ms
step:1774/2330 train_time:108314ms step_avg:61.06ms
step:1775/2330 train_time:108373ms step_avg:61.05ms
step:1776/2330 train_time:108436ms step_avg:61.06ms
step:1777/2330 train_time:108496ms step_avg:61.06ms
step:1778/2330 train_time:108559ms step_avg:61.06ms
step:1779/2330 train_time:108619ms step_avg:61.06ms
step:1780/2330 train_time:108681ms step_avg:61.06ms
step:1781/2330 train_time:108741ms step_avg:61.06ms
step:1782/2330 train_time:108804ms step_avg:61.06ms
step:1783/2330 train_time:108866ms step_avg:61.06ms
step:1784/2330 train_time:108930ms step_avg:61.06ms
step:1785/2330 train_time:108991ms step_avg:61.06ms
step:1786/2330 train_time:109055ms step_avg:61.06ms
step:1787/2330 train_time:109115ms step_avg:61.06ms
step:1788/2330 train_time:109178ms step_avg:61.06ms
step:1789/2330 train_time:109239ms step_avg:61.06ms
step:1790/2330 train_time:109303ms step_avg:61.06ms
step:1791/2330 train_time:109363ms step_avg:61.06ms
step:1792/2330 train_time:109426ms step_avg:61.06ms
step:1793/2330 train_time:109486ms step_avg:61.06ms
step:1794/2330 train_time:109547ms step_avg:61.06ms
step:1795/2330 train_time:109607ms step_avg:61.06ms
step:1796/2330 train_time:109670ms step_avg:61.06ms
step:1797/2330 train_time:109731ms step_avg:61.06ms
step:1798/2330 train_time:109794ms step_avg:61.06ms
step:1799/2330 train_time:109854ms step_avg:61.06ms
step:1800/2330 train_time:109918ms step_avg:61.07ms
step:1801/2330 train_time:109979ms step_avg:61.07ms
step:1802/2330 train_time:110044ms step_avg:61.07ms
step:1803/2330 train_time:110105ms step_avg:61.07ms
step:1804/2330 train_time:110168ms step_avg:61.07ms
step:1805/2330 train_time:110229ms step_avg:61.07ms
step:1806/2330 train_time:110292ms step_avg:61.07ms
step:1807/2330 train_time:110352ms step_avg:61.07ms
step:1808/2330 train_time:110415ms step_avg:61.07ms
step:1809/2330 train_time:110474ms step_avg:61.07ms
step:1810/2330 train_time:110538ms step_avg:61.07ms
step:1811/2330 train_time:110598ms step_avg:61.07ms
step:1812/2330 train_time:110662ms step_avg:61.07ms
step:1813/2330 train_time:110723ms step_avg:61.07ms
step:1814/2330 train_time:110786ms step_avg:61.07ms
step:1815/2330 train_time:110846ms step_avg:61.07ms
step:1816/2330 train_time:110909ms step_avg:61.07ms
step:1817/2330 train_time:110969ms step_avg:61.07ms
step:1818/2330 train_time:111033ms step_avg:61.07ms
step:1819/2330 train_time:111093ms step_avg:61.07ms
step:1820/2330 train_time:111156ms step_avg:61.07ms
step:1821/2330 train_time:111216ms step_avg:61.07ms
step:1822/2330 train_time:111279ms step_avg:61.08ms
step:1823/2330 train_time:111340ms step_avg:61.08ms
step:1824/2330 train_time:111404ms step_avg:61.08ms
step:1825/2330 train_time:111463ms step_avg:61.08ms
step:1826/2330 train_time:111526ms step_avg:61.08ms
step:1827/2330 train_time:111586ms step_avg:61.08ms
step:1828/2330 train_time:111648ms step_avg:61.08ms
step:1829/2330 train_time:111708ms step_avg:61.08ms
step:1830/2330 train_time:111770ms step_avg:61.08ms
step:1831/2330 train_time:111830ms step_avg:61.08ms
step:1832/2330 train_time:111894ms step_avg:61.08ms
step:1833/2330 train_time:111954ms step_avg:61.08ms
step:1834/2330 train_time:112017ms step_avg:61.08ms
step:1835/2330 train_time:112077ms step_avg:61.08ms
step:1836/2330 train_time:112141ms step_avg:61.08ms
step:1837/2330 train_time:112201ms step_avg:61.08ms
step:1838/2330 train_time:112264ms step_avg:61.08ms
step:1839/2330 train_time:112324ms step_avg:61.08ms
step:1840/2330 train_time:112387ms step_avg:61.08ms
step:1841/2330 train_time:112447ms step_avg:61.08ms
step:1842/2330 train_time:112511ms step_avg:61.08ms
step:1843/2330 train_time:112570ms step_avg:61.08ms
step:1844/2330 train_time:112633ms step_avg:61.08ms
step:1845/2330 train_time:112692ms step_avg:61.08ms
step:1846/2330 train_time:112756ms step_avg:61.08ms
step:1847/2330 train_time:112815ms step_avg:61.08ms
step:1848/2330 train_time:112878ms step_avg:61.08ms
step:1849/2330 train_time:112938ms step_avg:61.08ms
step:1850/2330 train_time:113003ms step_avg:61.08ms
step:1851/2330 train_time:113063ms step_avg:61.08ms
step:1852/2330 train_time:113125ms step_avg:61.08ms
step:1853/2330 train_time:113185ms step_avg:61.08ms
step:1854/2330 train_time:113248ms step_avg:61.08ms
step:1855/2330 train_time:113308ms step_avg:61.08ms
step:1856/2330 train_time:113372ms step_avg:61.08ms
step:1857/2330 train_time:113433ms step_avg:61.08ms
step:1858/2330 train_time:113495ms step_avg:61.08ms
step:1859/2330 train_time:113555ms step_avg:61.08ms
step:1860/2330 train_time:113618ms step_avg:61.09ms
step:1861/2330 train_time:113679ms step_avg:61.08ms
step:1862/2330 train_time:113744ms step_avg:61.09ms
step:1863/2330 train_time:113804ms step_avg:61.09ms
step:1864/2330 train_time:113867ms step_avg:61.09ms
step:1865/2330 train_time:113927ms step_avg:61.09ms
step:1866/2330 train_time:113990ms step_avg:61.09ms
step:1867/2330 train_time:114050ms step_avg:61.09ms
step:1868/2330 train_time:114114ms step_avg:61.09ms
step:1869/2330 train_time:114173ms step_avg:61.09ms
step:1870/2330 train_time:114237ms step_avg:61.09ms
step:1871/2330 train_time:114297ms step_avg:61.09ms
step:1872/2330 train_time:114360ms step_avg:61.09ms
step:1873/2330 train_time:114422ms step_avg:61.09ms
step:1874/2330 train_time:114484ms step_avg:61.09ms
step:1875/2330 train_time:114544ms step_avg:61.09ms
step:1876/2330 train_time:114607ms step_avg:61.09ms
step:1877/2330 train_time:114667ms step_avg:61.09ms
step:1878/2330 train_time:114730ms step_avg:61.09ms
step:1879/2330 train_time:114791ms step_avg:61.09ms
step:1880/2330 train_time:114854ms step_avg:61.09ms
step:1881/2330 train_time:114913ms step_avg:61.09ms
step:1882/2330 train_time:114977ms step_avg:61.09ms
step:1883/2330 train_time:115038ms step_avg:61.09ms
step:1884/2330 train_time:115102ms step_avg:61.09ms
step:1885/2330 train_time:115163ms step_avg:61.09ms
step:1886/2330 train_time:115227ms step_avg:61.10ms
step:1887/2330 train_time:115287ms step_avg:61.10ms
step:1888/2330 train_time:115349ms step_avg:61.10ms
step:1889/2330 train_time:115410ms step_avg:61.10ms
step:1890/2330 train_time:115474ms step_avg:61.10ms
step:1891/2330 train_time:115533ms step_avg:61.10ms
step:1892/2330 train_time:115597ms step_avg:61.10ms
step:1893/2330 train_time:115656ms step_avg:61.10ms
step:1894/2330 train_time:115719ms step_avg:61.10ms
step:1895/2330 train_time:115780ms step_avg:61.10ms
step:1896/2330 train_time:115844ms step_avg:61.10ms
step:1897/2330 train_time:115904ms step_avg:61.10ms
step:1898/2330 train_time:115968ms step_avg:61.10ms
step:1899/2330 train_time:116027ms step_avg:61.10ms
step:1900/2330 train_time:116090ms step_avg:61.10ms
step:1901/2330 train_time:116150ms step_avg:61.10ms
step:1902/2330 train_time:116213ms step_avg:61.10ms
step:1903/2330 train_time:116273ms step_avg:61.10ms
step:1904/2330 train_time:116337ms step_avg:61.10ms
step:1905/2330 train_time:116397ms step_avg:61.10ms
step:1906/2330 train_time:116461ms step_avg:61.10ms
step:1907/2330 train_time:116520ms step_avg:61.10ms
step:1908/2330 train_time:116584ms step_avg:61.10ms
step:1909/2330 train_time:116643ms step_avg:61.10ms
step:1910/2330 train_time:116706ms step_avg:61.10ms
step:1911/2330 train_time:116766ms step_avg:61.10ms
step:1912/2330 train_time:116830ms step_avg:61.10ms
step:1913/2330 train_time:116889ms step_avg:61.10ms
step:1914/2330 train_time:116952ms step_avg:61.10ms
step:1915/2330 train_time:117011ms step_avg:61.10ms
step:1916/2330 train_time:117075ms step_avg:61.10ms
step:1917/2330 train_time:117135ms step_avg:61.10ms
step:1918/2330 train_time:117198ms step_avg:61.10ms
step:1919/2330 train_time:117258ms step_avg:61.10ms
step:1920/2330 train_time:117322ms step_avg:61.11ms
step:1921/2330 train_time:117383ms step_avg:61.10ms
step:1922/2330 train_time:117446ms step_avg:61.11ms
step:1923/2330 train_time:117506ms step_avg:61.11ms
step:1924/2330 train_time:117569ms step_avg:61.11ms
step:1925/2330 train_time:117630ms step_avg:61.11ms
step:1926/2330 train_time:117693ms step_avg:61.11ms
step:1927/2330 train_time:117753ms step_avg:61.11ms
step:1928/2330 train_time:117816ms step_avg:61.11ms
step:1929/2330 train_time:117876ms step_avg:61.11ms
step:1930/2330 train_time:117939ms step_avg:61.11ms
step:1931/2330 train_time:117999ms step_avg:61.11ms
step:1932/2330 train_time:118062ms step_avg:61.11ms
step:1933/2330 train_time:118124ms step_avg:61.11ms
step:1934/2330 train_time:118187ms step_avg:61.11ms
step:1935/2330 train_time:118247ms step_avg:61.11ms
step:1936/2330 train_time:118311ms step_avg:61.11ms
step:1937/2330 train_time:118370ms step_avg:61.11ms
step:1938/2330 train_time:118434ms step_avg:61.11ms
step:1939/2330 train_time:118494ms step_avg:61.11ms
step:1940/2330 train_time:118558ms step_avg:61.11ms
step:1941/2330 train_time:118618ms step_avg:61.11ms
step:1942/2330 train_time:118681ms step_avg:61.11ms
step:1943/2330 train_time:118743ms step_avg:61.11ms
step:1944/2330 train_time:118807ms step_avg:61.11ms
step:1945/2330 train_time:118866ms step_avg:61.11ms
step:1946/2330 train_time:118930ms step_avg:61.12ms
step:1947/2330 train_time:118989ms step_avg:61.11ms
step:1948/2330 train_time:119053ms step_avg:61.12ms
step:1949/2330 train_time:119113ms step_avg:61.11ms
step:1950/2330 train_time:119176ms step_avg:61.12ms
step:1951/2330 train_time:119236ms step_avg:61.12ms
step:1952/2330 train_time:119299ms step_avg:61.12ms
step:1953/2330 train_time:119360ms step_avg:61.12ms
step:1954/2330 train_time:119425ms step_avg:61.12ms
step:1955/2330 train_time:119485ms step_avg:61.12ms
step:1956/2330 train_time:119548ms step_avg:61.12ms
step:1957/2330 train_time:119608ms step_avg:61.12ms
step:1958/2330 train_time:119671ms step_avg:61.12ms
step:1959/2330 train_time:119731ms step_avg:61.12ms
step:1960/2330 train_time:119795ms step_avg:61.12ms
step:1961/2330 train_time:119855ms step_avg:61.12ms
step:1962/2330 train_time:119917ms step_avg:61.12ms
step:1963/2330 train_time:119978ms step_avg:61.12ms
step:1964/2330 train_time:120042ms step_avg:61.12ms
step:1965/2330 train_time:120102ms step_avg:61.12ms
step:1966/2330 train_time:120166ms step_avg:61.12ms
step:1967/2330 train_time:120227ms step_avg:61.12ms
step:1968/2330 train_time:120289ms step_avg:61.12ms
step:1969/2330 train_time:120349ms step_avg:61.12ms
step:1970/2330 train_time:120412ms step_avg:61.12ms
step:1971/2330 train_time:120472ms step_avg:61.12ms
step:1972/2330 train_time:120536ms step_avg:61.12ms
step:1973/2330 train_time:120595ms step_avg:61.12ms
step:1974/2330 train_time:120658ms step_avg:61.12ms
step:1975/2330 train_time:120719ms step_avg:61.12ms
step:1976/2330 train_time:120782ms step_avg:61.12ms
step:1977/2330 train_time:120843ms step_avg:61.12ms
step:1978/2330 train_time:120907ms step_avg:61.13ms
step:1979/2330 train_time:120967ms step_avg:61.13ms
step:1980/2330 train_time:121030ms step_avg:61.13ms
step:1981/2330 train_time:121089ms step_avg:61.13ms
step:1982/2330 train_time:121153ms step_avg:61.13ms
step:1983/2330 train_time:121212ms step_avg:61.13ms
step:1984/2330 train_time:121276ms step_avg:61.13ms
step:1985/2330 train_time:121336ms step_avg:61.13ms
step:1986/2330 train_time:121398ms step_avg:61.13ms
step:1987/2330 train_time:121458ms step_avg:61.13ms
step:1988/2330 train_time:121522ms step_avg:61.13ms
step:1989/2330 train_time:121582ms step_avg:61.13ms
step:1990/2330 train_time:121646ms step_avg:61.13ms
step:1991/2330 train_time:121706ms step_avg:61.13ms
step:1992/2330 train_time:121769ms step_avg:61.13ms
step:1993/2330 train_time:121830ms step_avg:61.13ms
step:1994/2330 train_time:121891ms step_avg:61.13ms
step:1995/2330 train_time:121951ms step_avg:61.13ms
step:1996/2330 train_time:122015ms step_avg:61.13ms
step:1997/2330 train_time:122074ms step_avg:61.13ms
step:1998/2330 train_time:122138ms step_avg:61.13ms
step:1999/2330 train_time:122199ms step_avg:61.13ms
step:2000/2330 train_time:122263ms step_avg:61.13ms
step:2000/2330 val_loss:3.5551 train_time:122337ms step_avg:61.17ms
step:2001/2330 train_time:122358ms step_avg:61.15ms
step:2002/2330 train_time:122389ms step_avg:61.13ms
step:2003/2330 train_time:122454ms step_avg:61.14ms
step:2004/2330 train_time:122521ms step_avg:61.14ms
step:2005/2330 train_time:122585ms step_avg:61.14ms
step:2006/2330 train_time:122649ms step_avg:61.14ms
step:2007/2330 train_time:122707ms step_avg:61.14ms
step:2008/2330 train_time:122769ms step_avg:61.14ms
step:2009/2330 train_time:122829ms step_avg:61.14ms
step:2010/2330 train_time:122892ms step_avg:61.14ms
step:2011/2330 train_time:122951ms step_avg:61.14ms
step:2012/2330 train_time:123013ms step_avg:61.14ms
step:2013/2330 train_time:123073ms step_avg:61.14ms
step:2014/2330 train_time:123135ms step_avg:61.14ms
step:2015/2330 train_time:123194ms step_avg:61.14ms
step:2016/2330 train_time:123256ms step_avg:61.14ms
step:2017/2330 train_time:123316ms step_avg:61.14ms
step:2018/2330 train_time:123381ms step_avg:61.14ms
step:2019/2330 train_time:123444ms step_avg:61.14ms
step:2020/2330 train_time:123508ms step_avg:61.14ms
step:2021/2330 train_time:123569ms step_avg:61.14ms
step:2022/2330 train_time:123633ms step_avg:61.14ms
step:2023/2330 train_time:123693ms step_avg:61.14ms
step:2024/2330 train_time:123756ms step_avg:61.14ms
step:2025/2330 train_time:123815ms step_avg:61.14ms
step:2026/2330 train_time:123879ms step_avg:61.14ms
step:2027/2330 train_time:123939ms step_avg:61.14ms
step:2028/2330 train_time:124001ms step_avg:61.14ms
step:2029/2330 train_time:124060ms step_avg:61.14ms
step:2030/2330 train_time:124123ms step_avg:61.14ms
step:2031/2330 train_time:124182ms step_avg:61.14ms
step:2032/2330 train_time:124246ms step_avg:61.14ms
step:2033/2330 train_time:124305ms step_avg:61.14ms
step:2034/2330 train_time:124368ms step_avg:61.14ms
step:2035/2330 train_time:124429ms step_avg:61.14ms
step:2036/2330 train_time:124493ms step_avg:61.15ms
step:2037/2330 train_time:124554ms step_avg:61.15ms
step:2038/2330 train_time:124616ms step_avg:61.15ms
step:2039/2330 train_time:124677ms step_avg:61.15ms
step:2040/2330 train_time:124741ms step_avg:61.15ms
step:2041/2330 train_time:124801ms step_avg:61.15ms
step:2042/2330 train_time:124863ms step_avg:61.15ms
step:2043/2330 train_time:124923ms step_avg:61.15ms
step:2044/2330 train_time:124986ms step_avg:61.15ms
step:2045/2330 train_time:125046ms step_avg:61.15ms
step:2046/2330 train_time:125109ms step_avg:61.15ms
step:2047/2330 train_time:125169ms step_avg:61.15ms
step:2048/2330 train_time:125231ms step_avg:61.15ms
step:2049/2330 train_time:125291ms step_avg:61.15ms
step:2050/2330 train_time:125354ms step_avg:61.15ms
step:2051/2330 train_time:125414ms step_avg:61.15ms
step:2052/2330 train_time:125478ms step_avg:61.15ms
step:2053/2330 train_time:125538ms step_avg:61.15ms
step:2054/2330 train_time:125602ms step_avg:61.15ms
step:2055/2330 train_time:125662ms step_avg:61.15ms
step:2056/2330 train_time:125726ms step_avg:61.15ms
step:2057/2330 train_time:125785ms step_avg:61.15ms
step:2058/2330 train_time:125849ms step_avg:61.15ms
step:2059/2330 train_time:125909ms step_avg:61.15ms
step:2060/2330 train_time:125972ms step_avg:61.15ms
step:2061/2330 train_time:126032ms step_avg:61.15ms
step:2062/2330 train_time:126095ms step_avg:61.15ms
step:2063/2330 train_time:126155ms step_avg:61.15ms
step:2064/2330 train_time:126217ms step_avg:61.15ms
step:2065/2330 train_time:126278ms step_avg:61.15ms
step:2066/2330 train_time:126341ms step_avg:61.15ms
step:2067/2330 train_time:126401ms step_avg:61.15ms
step:2068/2330 train_time:126463ms step_avg:61.15ms
step:2069/2330 train_time:126524ms step_avg:61.15ms
step:2070/2330 train_time:126586ms step_avg:61.15ms
step:2071/2330 train_time:126647ms step_avg:61.15ms
step:2072/2330 train_time:126711ms step_avg:61.15ms
step:2073/2330 train_time:126771ms step_avg:61.15ms
step:2074/2330 train_time:126834ms step_avg:61.15ms
step:2075/2330 train_time:126893ms step_avg:61.15ms
step:2076/2330 train_time:126956ms step_avg:61.15ms
step:2077/2330 train_time:127016ms step_avg:61.15ms
step:2078/2330 train_time:127080ms step_avg:61.15ms
step:2079/2330 train_time:127140ms step_avg:61.15ms
step:2080/2330 train_time:127203ms step_avg:61.16ms
step:2081/2330 train_time:127263ms step_avg:61.15ms
step:2082/2330 train_time:127326ms step_avg:61.16ms
step:2083/2330 train_time:127386ms step_avg:61.15ms
step:2084/2330 train_time:127449ms step_avg:61.16ms
step:2085/2330 train_time:127509ms step_avg:61.16ms
step:2086/2330 train_time:127572ms step_avg:61.16ms
step:2087/2330 train_time:127631ms step_avg:61.16ms
step:2088/2330 train_time:127695ms step_avg:61.16ms
step:2089/2330 train_time:127756ms step_avg:61.16ms
step:2090/2330 train_time:127820ms step_avg:61.16ms
step:2091/2330 train_time:127881ms step_avg:61.16ms
step:2092/2330 train_time:127945ms step_avg:61.16ms
step:2093/2330 train_time:128004ms step_avg:61.16ms
step:2094/2330 train_time:128067ms step_avg:61.16ms
step:2095/2330 train_time:128126ms step_avg:61.16ms
step:2096/2330 train_time:128189ms step_avg:61.16ms
step:2097/2330 train_time:128249ms step_avg:61.16ms
step:2098/2330 train_time:128313ms step_avg:61.16ms
step:2099/2330 train_time:128373ms step_avg:61.16ms
step:2100/2330 train_time:128436ms step_avg:61.16ms
step:2101/2330 train_time:128497ms step_avg:61.16ms
step:2102/2330 train_time:128560ms step_avg:61.16ms
step:2103/2330 train_time:128620ms step_avg:61.16ms
step:2104/2330 train_time:128684ms step_avg:61.16ms
step:2105/2330 train_time:128744ms step_avg:61.16ms
step:2106/2330 train_time:128806ms step_avg:61.16ms
step:2107/2330 train_time:128866ms step_avg:61.16ms
step:2108/2330 train_time:128929ms step_avg:61.16ms
step:2109/2330 train_time:128989ms step_avg:61.16ms
step:2110/2330 train_time:129052ms step_avg:61.16ms
step:2111/2330 train_time:129111ms step_avg:61.16ms
step:2112/2330 train_time:129175ms step_avg:61.16ms
step:2113/2330 train_time:129235ms step_avg:61.16ms
step:2114/2330 train_time:129298ms step_avg:61.16ms
step:2115/2330 train_time:129357ms step_avg:61.16ms
step:2116/2330 train_time:129421ms step_avg:61.16ms
step:2117/2330 train_time:129481ms step_avg:61.16ms
step:2118/2330 train_time:129544ms step_avg:61.16ms
step:2119/2330 train_time:129604ms step_avg:61.16ms
step:2120/2330 train_time:129667ms step_avg:61.16ms
step:2121/2330 train_time:129726ms step_avg:61.16ms
step:2122/2330 train_time:129790ms step_avg:61.16ms
step:2123/2330 train_time:129851ms step_avg:61.16ms
step:2124/2330 train_time:129913ms step_avg:61.16ms
step:2125/2330 train_time:129973ms step_avg:61.16ms
step:2126/2330 train_time:130035ms step_avg:61.16ms
step:2127/2330 train_time:130095ms step_avg:61.16ms
step:2128/2330 train_time:130159ms step_avg:61.16ms
step:2129/2330 train_time:130219ms step_avg:61.16ms
step:2130/2330 train_time:130283ms step_avg:61.17ms
step:2131/2330 train_time:130344ms step_avg:61.17ms
step:2132/2330 train_time:130407ms step_avg:61.17ms
step:2133/2330 train_time:130466ms step_avg:61.17ms
step:2134/2330 train_time:130530ms step_avg:61.17ms
step:2135/2330 train_time:130589ms step_avg:61.17ms
step:2136/2330 train_time:130653ms step_avg:61.17ms
step:2137/2330 train_time:130713ms step_avg:61.17ms
step:2138/2330 train_time:130777ms step_avg:61.17ms
step:2139/2330 train_time:130837ms step_avg:61.17ms
step:2140/2330 train_time:130901ms step_avg:61.17ms
step:2141/2330 train_time:130961ms step_avg:61.17ms
step:2142/2330 train_time:131024ms step_avg:61.17ms
step:2143/2330 train_time:131085ms step_avg:61.17ms
step:2144/2330 train_time:131149ms step_avg:61.17ms
step:2145/2330 train_time:131208ms step_avg:61.17ms
step:2146/2330 train_time:131271ms step_avg:61.17ms
step:2147/2330 train_time:131331ms step_avg:61.17ms
step:2148/2330 train_time:131394ms step_avg:61.17ms
step:2149/2330 train_time:131454ms step_avg:61.17ms
step:2150/2330 train_time:131517ms step_avg:61.17ms
step:2151/2330 train_time:131577ms step_avg:61.17ms
step:2152/2330 train_time:131642ms step_avg:61.17ms
step:2153/2330 train_time:131702ms step_avg:61.17ms
step:2154/2330 train_time:131764ms step_avg:61.17ms
step:2155/2330 train_time:131825ms step_avg:61.17ms
step:2156/2330 train_time:131888ms step_avg:61.17ms
step:2157/2330 train_time:131948ms step_avg:61.17ms
step:2158/2330 train_time:132011ms step_avg:61.17ms
step:2159/2330 train_time:132071ms step_avg:61.17ms
step:2160/2330 train_time:132134ms step_avg:61.17ms
step:2161/2330 train_time:132194ms step_avg:61.17ms
step:2162/2330 train_time:132257ms step_avg:61.17ms
step:2163/2330 train_time:132318ms step_avg:61.17ms
step:2164/2330 train_time:132381ms step_avg:61.17ms
step:2165/2330 train_time:132442ms step_avg:61.17ms
step:2166/2330 train_time:132505ms step_avg:61.18ms
step:2167/2330 train_time:132564ms step_avg:61.17ms
step:2168/2330 train_time:132627ms step_avg:61.17ms
step:2169/2330 train_time:132687ms step_avg:61.17ms
step:2170/2330 train_time:132750ms step_avg:61.18ms
step:2171/2330 train_time:132810ms step_avg:61.17ms
step:2172/2330 train_time:132873ms step_avg:61.18ms
step:2173/2330 train_time:132934ms step_avg:61.18ms
step:2174/2330 train_time:132998ms step_avg:61.18ms
step:2175/2330 train_time:133058ms step_avg:61.18ms
step:2176/2330 train_time:133120ms step_avg:61.18ms
step:2177/2330 train_time:133180ms step_avg:61.18ms
step:2178/2330 train_time:133243ms step_avg:61.18ms
step:2179/2330 train_time:133304ms step_avg:61.18ms
step:2180/2330 train_time:133367ms step_avg:61.18ms
step:2181/2330 train_time:133427ms step_avg:61.18ms
step:2182/2330 train_time:133489ms step_avg:61.18ms
step:2183/2330 train_time:133549ms step_avg:61.18ms
step:2184/2330 train_time:133612ms step_avg:61.18ms
step:2185/2330 train_time:133673ms step_avg:61.18ms
step:2186/2330 train_time:133735ms step_avg:61.18ms
step:2187/2330 train_time:133795ms step_avg:61.18ms
step:2188/2330 train_time:133858ms step_avg:61.18ms
step:2189/2330 train_time:133918ms step_avg:61.18ms
step:2190/2330 train_time:133981ms step_avg:61.18ms
step:2191/2330 train_time:134043ms step_avg:61.18ms
step:2192/2330 train_time:134106ms step_avg:61.18ms
step:2193/2330 train_time:134166ms step_avg:61.18ms
step:2194/2330 train_time:134228ms step_avg:61.18ms
step:2195/2330 train_time:134288ms step_avg:61.18ms
step:2196/2330 train_time:134351ms step_avg:61.18ms
step:2197/2330 train_time:134411ms step_avg:61.18ms
step:2198/2330 train_time:134475ms step_avg:61.18ms
step:2199/2330 train_time:134534ms step_avg:61.18ms
step:2200/2330 train_time:134598ms step_avg:61.18ms
step:2201/2330 train_time:134659ms step_avg:61.18ms
step:2202/2330 train_time:134721ms step_avg:61.18ms
step:2203/2330 train_time:134781ms step_avg:61.18ms
step:2204/2330 train_time:134845ms step_avg:61.18ms
step:2205/2330 train_time:134904ms step_avg:61.18ms
step:2206/2330 train_time:134967ms step_avg:61.18ms
step:2207/2330 train_time:135026ms step_avg:61.18ms
step:2208/2330 train_time:135089ms step_avg:61.18ms
step:2209/2330 train_time:135150ms step_avg:61.18ms
step:2210/2330 train_time:135214ms step_avg:61.18ms
step:2211/2330 train_time:135273ms step_avg:61.18ms
step:2212/2330 train_time:135336ms step_avg:61.18ms
step:2213/2330 train_time:135396ms step_avg:61.18ms
step:2214/2330 train_time:135460ms step_avg:61.18ms
step:2215/2330 train_time:135519ms step_avg:61.18ms
step:2216/2330 train_time:135582ms step_avg:61.18ms
step:2217/2330 train_time:135644ms step_avg:61.18ms
step:2218/2330 train_time:135705ms step_avg:61.18ms
step:2219/2330 train_time:135765ms step_avg:61.18ms
step:2220/2330 train_time:135828ms step_avg:61.18ms
step:2221/2330 train_time:135888ms step_avg:61.18ms
step:2222/2330 train_time:135951ms step_avg:61.18ms
step:2223/2330 train_time:136010ms step_avg:61.18ms
step:2224/2330 train_time:136074ms step_avg:61.18ms
step:2225/2330 train_time:136134ms step_avg:61.18ms
step:2226/2330 train_time:136197ms step_avg:61.18ms
step:2227/2330 train_time:136258ms step_avg:61.18ms
step:2228/2330 train_time:136321ms step_avg:61.19ms
step:2229/2330 train_time:136381ms step_avg:61.19ms
step:2230/2330 train_time:136445ms step_avg:61.19ms
step:2231/2330 train_time:136504ms step_avg:61.19ms
step:2232/2330 train_time:136567ms step_avg:61.19ms
step:2233/2330 train_time:136628ms step_avg:61.19ms
step:2234/2330 train_time:136690ms step_avg:61.19ms
step:2235/2330 train_time:136751ms step_avg:61.19ms
step:2236/2330 train_time:136815ms step_avg:61.19ms
step:2237/2330 train_time:136874ms step_avg:61.19ms
step:2238/2330 train_time:136937ms step_avg:61.19ms
step:2239/2330 train_time:136996ms step_avg:61.19ms
step:2240/2330 train_time:137060ms step_avg:61.19ms
step:2241/2330 train_time:137121ms step_avg:61.19ms
step:2242/2330 train_time:137184ms step_avg:61.19ms
step:2243/2330 train_time:137244ms step_avg:61.19ms
step:2244/2330 train_time:137307ms step_avg:61.19ms
step:2245/2330 train_time:137366ms step_avg:61.19ms
step:2246/2330 train_time:137430ms step_avg:61.19ms
step:2247/2330 train_time:137490ms step_avg:61.19ms
step:2248/2330 train_time:137553ms step_avg:61.19ms
step:2249/2330 train_time:137612ms step_avg:61.19ms
step:2250/2330 train_time:137676ms step_avg:61.19ms
step:2250/2330 val_loss:3.5139 train_time:137748ms step_avg:61.22ms
step:2251/2330 train_time:137770ms step_avg:61.20ms
step:2252/2330 train_time:137802ms step_avg:61.19ms
step:2253/2330 train_time:137867ms step_avg:61.19ms
step:2254/2330 train_time:137931ms step_avg:61.19ms
step:2255/2330 train_time:137991ms step_avg:61.19ms
step:2256/2330 train_time:138054ms step_avg:61.19ms
step:2257/2330 train_time:138113ms step_avg:61.19ms
step:2258/2330 train_time:138176ms step_avg:61.19ms
step:2259/2330 train_time:138235ms step_avg:61.19ms
step:2260/2330 train_time:138298ms step_avg:61.19ms
step:2261/2330 train_time:138358ms step_avg:61.19ms
step:2262/2330 train_time:138419ms step_avg:61.19ms
step:2263/2330 train_time:138479ms step_avg:61.19ms
step:2264/2330 train_time:138541ms step_avg:61.19ms
step:2265/2330 train_time:138601ms step_avg:61.19ms
step:2266/2330 train_time:138664ms step_avg:61.19ms
step:2267/2330 train_time:138725ms step_avg:61.19ms
step:2268/2330 train_time:138789ms step_avg:61.19ms
step:2269/2330 train_time:138850ms step_avg:61.19ms
step:2270/2330 train_time:138913ms step_avg:61.20ms
step:2271/2330 train_time:138974ms step_avg:61.20ms
step:2272/2330 train_time:139038ms step_avg:61.20ms
step:2273/2330 train_time:139097ms step_avg:61.20ms
step:2274/2330 train_time:139161ms step_avg:61.20ms
step:2275/2330 train_time:139219ms step_avg:61.20ms
step:2276/2330 train_time:139282ms step_avg:61.20ms
step:2277/2330 train_time:139341ms step_avg:61.20ms
step:2278/2330 train_time:139404ms step_avg:61.20ms
step:2279/2330 train_time:139464ms step_avg:61.20ms
step:2280/2330 train_time:139526ms step_avg:61.20ms
step:2281/2330 train_time:139586ms step_avg:61.19ms
step:2282/2330 train_time:139649ms step_avg:61.20ms
step:2283/2330 train_time:139709ms step_avg:61.20ms
step:2284/2330 train_time:139772ms step_avg:61.20ms
step:2285/2330 train_time:139832ms step_avg:61.20ms
step:2286/2330 train_time:139897ms step_avg:61.20ms
step:2287/2330 train_time:139958ms step_avg:61.20ms
step:2288/2330 train_time:140021ms step_avg:61.20ms
step:2289/2330 train_time:140081ms step_avg:61.20ms
step:2290/2330 train_time:140143ms step_avg:61.20ms
step:2291/2330 train_time:140204ms step_avg:61.20ms
step:2292/2330 train_time:140267ms step_avg:61.20ms
step:2293/2330 train_time:140327ms step_avg:61.20ms
step:2294/2330 train_time:140390ms step_avg:61.20ms
step:2295/2330 train_time:140449ms step_avg:61.20ms
step:2296/2330 train_time:140512ms step_avg:61.20ms
step:2297/2330 train_time:140573ms step_avg:61.20ms
step:2298/2330 train_time:140636ms step_avg:61.20ms
step:2299/2330 train_time:140697ms step_avg:61.20ms
step:2300/2330 train_time:140761ms step_avg:61.20ms
step:2301/2330 train_time:140821ms step_avg:61.20ms
step:2302/2330 train_time:140884ms step_avg:61.20ms
step:2303/2330 train_time:140943ms step_avg:61.20ms
step:2304/2330 train_time:141007ms step_avg:61.20ms
step:2305/2330 train_time:141068ms step_avg:61.20ms
step:2306/2330 train_time:141131ms step_avg:61.20ms
step:2307/2330 train_time:141190ms step_avg:61.20ms
step:2308/2330 train_time:141253ms step_avg:61.20ms
step:2309/2330 train_time:141313ms step_avg:61.20ms
step:2310/2330 train_time:141376ms step_avg:61.20ms
step:2311/2330 train_time:141436ms step_avg:61.20ms
step:2312/2330 train_time:141499ms step_avg:61.20ms
step:2313/2330 train_time:141559ms step_avg:61.20ms
step:2314/2330 train_time:141621ms step_avg:61.20ms
step:2315/2330 train_time:141681ms step_avg:61.20ms
step:2316/2330 train_time:141744ms step_avg:61.20ms
step:2317/2330 train_time:141804ms step_avg:61.20ms
step:2318/2330 train_time:141868ms step_avg:61.20ms
step:2319/2330 train_time:141929ms step_avg:61.20ms
step:2320/2330 train_time:141992ms step_avg:61.20ms
step:2321/2330 train_time:142052ms step_avg:61.20ms
step:2322/2330 train_time:142116ms step_avg:61.20ms
step:2323/2330 train_time:142176ms step_avg:61.20ms
step:2324/2330 train_time:142239ms step_avg:61.20ms
step:2325/2330 train_time:142299ms step_avg:61.20ms
step:2326/2330 train_time:142362ms step_avg:61.20ms
step:2327/2330 train_time:142421ms step_avg:61.20ms
step:2328/2330 train_time:142485ms step_avg:61.20ms
step:2329/2330 train_time:142545ms step_avg:61.20ms
step:2330/2330 train_time:142608ms step_avg:61.20ms
step:2330/2330 val_loss:3.5000 train_time:142681ms step_avg:61.24ms
peak memory allocated: 29226 MiB reserved: 44076 MiB
