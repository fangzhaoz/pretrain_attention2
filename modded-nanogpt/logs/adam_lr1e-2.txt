import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "adam_lr1e-2"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-1, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.Adam(
    hidden_matrix_params + gate_params,
    lr=1e-2,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 00:35:50 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   34C    P0             113W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   32C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:100ms step_avg:100.03ms
step:2/2330 train_time:192ms step_avg:96.21ms
step:3/2330 train_time:212ms step_avg:70.67ms
step:4/2330 train_time:233ms step_avg:58.35ms
step:5/2330 train_time:288ms step_avg:57.66ms
step:6/2330 train_time:347ms step_avg:57.91ms
step:7/2330 train_time:402ms step_avg:57.49ms
step:8/2330 train_time:462ms step_avg:57.70ms
step:9/2330 train_time:517ms step_avg:57.39ms
step:10/2330 train_time:576ms step_avg:57.58ms
step:11/2330 train_time:631ms step_avg:57.33ms
step:12/2330 train_time:690ms step_avg:57.49ms
step:13/2330 train_time:745ms step_avg:57.30ms
step:14/2330 train_time:804ms step_avg:57.46ms
step:15/2330 train_time:860ms step_avg:57.30ms
step:16/2330 train_time:919ms step_avg:57.43ms
step:17/2330 train_time:974ms step_avg:57.29ms
step:18/2330 train_time:1034ms step_avg:57.42ms
step:19/2330 train_time:1089ms step_avg:57.30ms
step:20/2330 train_time:1148ms step_avg:57.40ms
step:21/2330 train_time:1204ms step_avg:57.33ms
step:22/2330 train_time:1265ms step_avg:57.51ms
step:23/2330 train_time:1321ms step_avg:57.44ms
step:24/2330 train_time:1382ms step_avg:57.60ms
step:25/2330 train_time:1438ms step_avg:57.51ms
step:26/2330 train_time:1498ms step_avg:57.61ms
step:27/2330 train_time:1553ms step_avg:57.52ms
step:28/2330 train_time:1613ms step_avg:57.62ms
step:29/2330 train_time:1669ms step_avg:57.54ms
step:30/2330 train_time:1729ms step_avg:57.62ms
step:31/2330 train_time:1784ms step_avg:57.55ms
step:32/2330 train_time:1843ms step_avg:57.60ms
step:33/2330 train_time:1898ms step_avg:57.52ms
step:34/2330 train_time:1958ms step_avg:57.59ms
step:35/2330 train_time:2013ms step_avg:57.52ms
step:36/2330 train_time:2074ms step_avg:57.60ms
step:37/2330 train_time:2129ms step_avg:57.54ms
step:38/2330 train_time:2189ms step_avg:57.61ms
step:39/2330 train_time:2245ms step_avg:57.55ms
step:40/2330 train_time:2305ms step_avg:57.62ms
step:41/2330 train_time:2360ms step_avg:57.55ms
step:42/2330 train_time:2421ms step_avg:57.64ms
step:43/2330 train_time:2476ms step_avg:57.58ms
step:44/2330 train_time:2537ms step_avg:57.67ms
step:45/2330 train_time:2593ms step_avg:57.62ms
step:46/2330 train_time:2653ms step_avg:57.68ms
step:47/2330 train_time:2708ms step_avg:57.63ms
step:48/2330 train_time:2768ms step_avg:57.68ms
step:49/2330 train_time:2824ms step_avg:57.64ms
step:50/2330 train_time:2884ms step_avg:57.68ms
step:51/2330 train_time:2939ms step_avg:57.64ms
step:52/2330 train_time:3000ms step_avg:57.68ms
step:53/2330 train_time:3055ms step_avg:57.63ms
step:54/2330 train_time:3115ms step_avg:57.69ms
step:55/2330 train_time:3171ms step_avg:57.65ms
step:56/2330 train_time:3231ms step_avg:57.70ms
step:57/2330 train_time:3286ms step_avg:57.66ms
step:58/2330 train_time:3348ms step_avg:57.72ms
step:59/2330 train_time:3403ms step_avg:57.68ms
step:60/2330 train_time:3464ms step_avg:57.74ms
step:61/2330 train_time:3520ms step_avg:57.70ms
step:62/2330 train_time:3582ms step_avg:57.77ms
step:63/2330 train_time:3637ms step_avg:57.73ms
step:64/2330 train_time:3698ms step_avg:57.78ms
step:65/2330 train_time:3754ms step_avg:57.75ms
step:66/2330 train_time:3814ms step_avg:57.79ms
step:67/2330 train_time:3869ms step_avg:57.75ms
step:68/2330 train_time:3929ms step_avg:57.78ms
step:69/2330 train_time:3984ms step_avg:57.74ms
step:70/2330 train_time:4045ms step_avg:57.78ms
step:71/2330 train_time:4100ms step_avg:57.74ms
step:72/2330 train_time:4162ms step_avg:57.80ms
step:73/2330 train_time:4217ms step_avg:57.77ms
step:74/2330 train_time:4278ms step_avg:57.82ms
step:75/2330 train_time:4334ms step_avg:57.79ms
step:76/2330 train_time:4394ms step_avg:57.82ms
step:77/2330 train_time:4450ms step_avg:57.79ms
step:78/2330 train_time:4510ms step_avg:57.82ms
step:79/2330 train_time:4565ms step_avg:57.78ms
step:80/2330 train_time:4626ms step_avg:57.83ms
step:81/2330 train_time:4682ms step_avg:57.80ms
step:82/2330 train_time:4743ms step_avg:57.84ms
step:83/2330 train_time:4799ms step_avg:57.82ms
step:84/2330 train_time:4861ms step_avg:57.87ms
step:85/2330 train_time:4917ms step_avg:57.85ms
step:86/2330 train_time:4977ms step_avg:57.87ms
step:87/2330 train_time:5033ms step_avg:57.85ms
step:88/2330 train_time:5093ms step_avg:57.87ms
step:89/2330 train_time:5149ms step_avg:57.85ms
step:90/2330 train_time:5208ms step_avg:57.87ms
step:91/2330 train_time:5263ms step_avg:57.84ms
step:92/2330 train_time:5324ms step_avg:57.87ms
step:93/2330 train_time:5379ms step_avg:57.84ms
step:94/2330 train_time:5441ms step_avg:57.88ms
step:95/2330 train_time:5496ms step_avg:57.86ms
step:96/2330 train_time:5557ms step_avg:57.89ms
step:97/2330 train_time:5613ms step_avg:57.86ms
step:98/2330 train_time:5673ms step_avg:57.89ms
step:99/2330 train_time:5729ms step_avg:57.87ms
step:100/2330 train_time:5789ms step_avg:57.89ms
step:101/2330 train_time:5844ms step_avg:57.86ms
step:102/2330 train_time:5905ms step_avg:57.90ms
step:103/2330 train_time:5961ms step_avg:57.87ms
step:104/2330 train_time:6022ms step_avg:57.90ms
step:105/2330 train_time:6078ms step_avg:57.88ms
step:106/2330 train_time:6138ms step_avg:57.91ms
step:107/2330 train_time:6195ms step_avg:57.89ms
step:108/2330 train_time:6254ms step_avg:57.91ms
step:109/2330 train_time:6310ms step_avg:57.89ms
step:110/2330 train_time:6370ms step_avg:57.91ms
step:111/2330 train_time:6426ms step_avg:57.89ms
step:112/2330 train_time:6485ms step_avg:57.90ms
step:113/2330 train_time:6541ms step_avg:57.88ms
step:114/2330 train_time:6601ms step_avg:57.90ms
step:115/2330 train_time:6657ms step_avg:57.89ms
step:116/2330 train_time:6717ms step_avg:57.91ms
step:117/2330 train_time:6773ms step_avg:57.89ms
step:118/2330 train_time:6834ms step_avg:57.91ms
step:119/2330 train_time:6889ms step_avg:57.89ms
step:120/2330 train_time:6949ms step_avg:57.91ms
step:121/2330 train_time:7005ms step_avg:57.90ms
step:122/2330 train_time:7065ms step_avg:57.91ms
step:123/2330 train_time:7120ms step_avg:57.89ms
step:124/2330 train_time:7183ms step_avg:57.93ms
step:125/2330 train_time:7238ms step_avg:57.91ms
step:126/2330 train_time:7301ms step_avg:57.94ms
step:127/2330 train_time:7356ms step_avg:57.92ms
step:128/2330 train_time:7418ms step_avg:57.95ms
step:129/2330 train_time:7474ms step_avg:57.94ms
step:130/2330 train_time:7534ms step_avg:57.95ms
step:131/2330 train_time:7590ms step_avg:57.94ms
step:132/2330 train_time:7650ms step_avg:57.95ms
step:133/2330 train_time:7706ms step_avg:57.94ms
step:134/2330 train_time:7766ms step_avg:57.96ms
step:135/2330 train_time:7822ms step_avg:57.94ms
step:136/2330 train_time:7884ms step_avg:57.97ms
step:137/2330 train_time:7939ms step_avg:57.95ms
step:138/2330 train_time:8002ms step_avg:57.98ms
step:139/2330 train_time:8057ms step_avg:57.96ms
step:140/2330 train_time:8119ms step_avg:57.99ms
step:141/2330 train_time:8175ms step_avg:57.98ms
step:142/2330 train_time:8235ms step_avg:58.00ms
step:143/2330 train_time:8291ms step_avg:57.98ms
step:144/2330 train_time:8351ms step_avg:58.00ms
step:145/2330 train_time:8407ms step_avg:57.98ms
step:146/2330 train_time:8467ms step_avg:58.00ms
step:147/2330 train_time:8523ms step_avg:57.98ms
step:148/2330 train_time:8584ms step_avg:58.00ms
step:149/2330 train_time:8640ms step_avg:57.99ms
step:150/2330 train_time:8701ms step_avg:58.01ms
step:151/2330 train_time:8757ms step_avg:57.99ms
step:152/2330 train_time:8820ms step_avg:58.02ms
step:153/2330 train_time:8876ms step_avg:58.01ms
step:154/2330 train_time:8936ms step_avg:58.03ms
step:155/2330 train_time:8992ms step_avg:58.01ms
step:156/2330 train_time:9052ms step_avg:58.03ms
step:157/2330 train_time:9108ms step_avg:58.01ms
step:158/2330 train_time:9168ms step_avg:58.02ms
step:159/2330 train_time:9223ms step_avg:58.01ms
step:160/2330 train_time:9284ms step_avg:58.03ms
step:161/2330 train_time:9340ms step_avg:58.01ms
step:162/2330 train_time:9402ms step_avg:58.04ms
step:163/2330 train_time:9457ms step_avg:58.02ms
step:164/2330 train_time:9520ms step_avg:58.05ms
step:165/2330 train_time:9575ms step_avg:58.03ms
step:166/2330 train_time:9635ms step_avg:58.04ms
step:167/2330 train_time:9691ms step_avg:58.03ms
step:168/2330 train_time:9752ms step_avg:58.05ms
step:169/2330 train_time:9807ms step_avg:58.03ms
step:170/2330 train_time:9867ms step_avg:58.04ms
step:171/2330 train_time:9923ms step_avg:58.03ms
step:172/2330 train_time:9986ms step_avg:58.06ms
step:173/2330 train_time:10041ms step_avg:58.04ms
step:174/2330 train_time:10102ms step_avg:58.06ms
step:175/2330 train_time:10158ms step_avg:58.05ms
step:176/2330 train_time:10219ms step_avg:58.06ms
step:177/2330 train_time:10275ms step_avg:58.05ms
step:178/2330 train_time:10335ms step_avg:58.06ms
step:179/2330 train_time:10390ms step_avg:58.05ms
step:180/2330 train_time:10450ms step_avg:58.06ms
step:181/2330 train_time:10506ms step_avg:58.04ms
step:182/2330 train_time:10567ms step_avg:58.06ms
step:183/2330 train_time:10623ms step_avg:58.05ms
step:184/2330 train_time:10685ms step_avg:58.07ms
step:185/2330 train_time:10740ms step_avg:58.05ms
step:186/2330 train_time:10802ms step_avg:58.08ms
step:187/2330 train_time:10858ms step_avg:58.07ms
step:188/2330 train_time:10919ms step_avg:58.08ms
step:189/2330 train_time:10976ms step_avg:58.07ms
step:190/2330 train_time:11036ms step_avg:58.09ms
step:191/2330 train_time:11092ms step_avg:58.08ms
step:192/2330 train_time:11152ms step_avg:58.08ms
step:193/2330 train_time:11208ms step_avg:58.07ms
step:194/2330 train_time:11268ms step_avg:58.08ms
step:195/2330 train_time:11323ms step_avg:58.07ms
step:196/2330 train_time:11384ms step_avg:58.08ms
step:197/2330 train_time:11440ms step_avg:58.07ms
step:198/2330 train_time:11501ms step_avg:58.08ms
step:199/2330 train_time:11556ms step_avg:58.07ms
step:200/2330 train_time:11618ms step_avg:58.09ms
step:201/2330 train_time:11674ms step_avg:58.08ms
step:202/2330 train_time:11735ms step_avg:58.09ms
step:203/2330 train_time:11791ms step_avg:58.08ms
step:204/2330 train_time:11851ms step_avg:58.09ms
step:205/2330 train_time:11907ms step_avg:58.08ms
step:206/2330 train_time:11968ms step_avg:58.10ms
step:207/2330 train_time:12024ms step_avg:58.08ms
step:208/2330 train_time:12085ms step_avg:58.10ms
step:209/2330 train_time:12141ms step_avg:58.09ms
step:210/2330 train_time:12202ms step_avg:58.11ms
step:211/2330 train_time:12258ms step_avg:58.10ms
step:212/2330 train_time:12319ms step_avg:58.11ms
step:213/2330 train_time:12376ms step_avg:58.10ms
step:214/2330 train_time:12435ms step_avg:58.11ms
step:215/2330 train_time:12492ms step_avg:58.10ms
step:216/2330 train_time:12551ms step_avg:58.11ms
step:217/2330 train_time:12607ms step_avg:58.09ms
step:218/2330 train_time:12668ms step_avg:58.11ms
step:219/2330 train_time:12724ms step_avg:58.10ms
step:220/2330 train_time:12785ms step_avg:58.11ms
step:221/2330 train_time:12841ms step_avg:58.10ms
step:222/2330 train_time:12902ms step_avg:58.12ms
step:223/2330 train_time:12958ms step_avg:58.11ms
step:224/2330 train_time:13020ms step_avg:58.12ms
step:225/2330 train_time:13075ms step_avg:58.11ms
step:226/2330 train_time:13137ms step_avg:58.13ms
step:227/2330 train_time:13192ms step_avg:58.12ms
step:228/2330 train_time:13253ms step_avg:58.13ms
step:229/2330 train_time:13309ms step_avg:58.12ms
step:230/2330 train_time:13369ms step_avg:58.12ms
step:231/2330 train_time:13424ms step_avg:58.11ms
step:232/2330 train_time:13485ms step_avg:58.13ms
step:233/2330 train_time:13541ms step_avg:58.12ms
step:234/2330 train_time:13603ms step_avg:58.13ms
step:235/2330 train_time:13658ms step_avg:58.12ms
step:236/2330 train_time:13719ms step_avg:58.13ms
step:237/2330 train_time:13775ms step_avg:58.12ms
step:238/2330 train_time:13836ms step_avg:58.13ms
step:239/2330 train_time:13892ms step_avg:58.13ms
step:240/2330 train_time:13952ms step_avg:58.13ms
step:241/2330 train_time:14008ms step_avg:58.12ms
step:242/2330 train_time:14067ms step_avg:58.13ms
step:243/2330 train_time:14123ms step_avg:58.12ms
step:244/2330 train_time:14186ms step_avg:58.14ms
step:245/2330 train_time:14241ms step_avg:58.13ms
step:246/2330 train_time:14303ms step_avg:58.14ms
step:247/2330 train_time:14359ms step_avg:58.13ms
step:248/2330 train_time:14422ms step_avg:58.15ms
step:249/2330 train_time:14478ms step_avg:58.14ms
step:250/2330 train_time:14539ms step_avg:58.16ms
step:250/2330 val_loss:5.5641 train_time:14617ms step_avg:58.47ms
step:251/2330 train_time:14637ms step_avg:58.31ms
step:252/2330 train_time:14659ms step_avg:58.17ms
step:253/2330 train_time:14714ms step_avg:58.16ms
step:254/2330 train_time:14777ms step_avg:58.18ms
step:255/2330 train_time:14834ms step_avg:58.17ms
step:256/2330 train_time:14901ms step_avg:58.21ms
step:257/2330 train_time:14956ms step_avg:58.20ms
step:258/2330 train_time:15019ms step_avg:58.21ms
step:259/2330 train_time:15074ms step_avg:58.20ms
step:260/2330 train_time:15135ms step_avg:58.21ms
step:261/2330 train_time:15191ms step_avg:58.20ms
step:262/2330 train_time:15250ms step_avg:58.21ms
step:263/2330 train_time:15306ms step_avg:58.20ms
step:264/2330 train_time:15366ms step_avg:58.20ms
step:265/2330 train_time:15421ms step_avg:58.19ms
step:266/2330 train_time:15481ms step_avg:58.20ms
step:267/2330 train_time:15537ms step_avg:58.19ms
step:268/2330 train_time:15597ms step_avg:58.20ms
step:269/2330 train_time:15653ms step_avg:58.19ms
step:270/2330 train_time:15715ms step_avg:58.20ms
step:271/2330 train_time:15772ms step_avg:58.20ms
step:272/2330 train_time:15834ms step_avg:58.21ms
step:273/2330 train_time:15891ms step_avg:58.21ms
step:274/2330 train_time:15951ms step_avg:58.21ms
step:275/2330 train_time:16006ms step_avg:58.20ms
step:276/2330 train_time:16068ms step_avg:58.22ms
step:277/2330 train_time:16123ms step_avg:58.21ms
step:278/2330 train_time:16184ms step_avg:58.21ms
step:279/2330 train_time:16239ms step_avg:58.20ms
step:280/2330 train_time:16300ms step_avg:58.21ms
step:281/2330 train_time:16355ms step_avg:58.20ms
step:282/2330 train_time:16415ms step_avg:58.21ms
step:283/2330 train_time:16471ms step_avg:58.20ms
step:284/2330 train_time:16531ms step_avg:58.21ms
step:285/2330 train_time:16586ms step_avg:58.20ms
step:286/2330 train_time:16646ms step_avg:58.20ms
step:287/2330 train_time:16702ms step_avg:58.19ms
step:288/2330 train_time:16763ms step_avg:58.20ms
step:289/2330 train_time:16818ms step_avg:58.20ms
step:290/2330 train_time:16880ms step_avg:58.21ms
step:291/2330 train_time:16935ms step_avg:58.20ms
step:292/2330 train_time:16998ms step_avg:58.21ms
step:293/2330 train_time:17054ms step_avg:58.20ms
step:294/2330 train_time:17115ms step_avg:58.22ms
step:295/2330 train_time:17171ms step_avg:58.21ms
step:296/2330 train_time:17232ms step_avg:58.22ms
step:297/2330 train_time:17288ms step_avg:58.21ms
step:298/2330 train_time:17348ms step_avg:58.22ms
step:299/2330 train_time:17404ms step_avg:58.21ms
step:300/2330 train_time:17464ms step_avg:58.21ms
step:301/2330 train_time:17519ms step_avg:58.20ms
step:302/2330 train_time:17580ms step_avg:58.21ms
step:303/2330 train_time:17635ms step_avg:58.20ms
step:304/2330 train_time:17696ms step_avg:58.21ms
step:305/2330 train_time:17752ms step_avg:58.20ms
step:306/2330 train_time:17813ms step_avg:58.21ms
step:307/2330 train_time:17869ms step_avg:58.21ms
step:308/2330 train_time:17930ms step_avg:58.21ms
step:309/2330 train_time:17985ms step_avg:58.20ms
step:310/2330 train_time:18046ms step_avg:58.21ms
step:311/2330 train_time:18101ms step_avg:58.20ms
step:312/2330 train_time:18163ms step_avg:58.21ms
step:313/2330 train_time:18219ms step_avg:58.21ms
step:314/2330 train_time:18280ms step_avg:58.22ms
step:315/2330 train_time:18335ms step_avg:58.21ms
step:316/2330 train_time:18397ms step_avg:58.22ms
step:317/2330 train_time:18453ms step_avg:58.21ms
step:318/2330 train_time:18513ms step_avg:58.22ms
step:319/2330 train_time:18569ms step_avg:58.21ms
step:320/2330 train_time:18630ms step_avg:58.22ms
step:321/2330 train_time:18686ms step_avg:58.21ms
step:322/2330 train_time:18746ms step_avg:58.22ms
step:323/2330 train_time:18801ms step_avg:58.21ms
step:324/2330 train_time:18863ms step_avg:58.22ms
step:325/2330 train_time:18919ms step_avg:58.21ms
step:326/2330 train_time:18981ms step_avg:58.22ms
step:327/2330 train_time:19037ms step_avg:58.22ms
step:328/2330 train_time:19098ms step_avg:58.23ms
step:329/2330 train_time:19154ms step_avg:58.22ms
step:330/2330 train_time:19216ms step_avg:58.23ms
step:331/2330 train_time:19271ms step_avg:58.22ms
step:332/2330 train_time:19333ms step_avg:58.23ms
step:333/2330 train_time:19389ms step_avg:58.22ms
step:334/2330 train_time:19449ms step_avg:58.23ms
step:335/2330 train_time:19505ms step_avg:58.22ms
step:336/2330 train_time:19565ms step_avg:58.23ms
step:337/2330 train_time:19620ms step_avg:58.22ms
step:338/2330 train_time:19681ms step_avg:58.23ms
step:339/2330 train_time:19737ms step_avg:58.22ms
step:340/2330 train_time:19799ms step_avg:58.23ms
step:341/2330 train_time:19854ms step_avg:58.22ms
step:342/2330 train_time:19915ms step_avg:58.23ms
step:343/2330 train_time:19971ms step_avg:58.22ms
step:344/2330 train_time:20032ms step_avg:58.23ms
step:345/2330 train_time:20089ms step_avg:58.23ms
step:346/2330 train_time:20149ms step_avg:58.23ms
step:347/2330 train_time:20205ms step_avg:58.23ms
step:348/2330 train_time:20265ms step_avg:58.23ms
step:349/2330 train_time:20320ms step_avg:58.22ms
step:350/2330 train_time:20382ms step_avg:58.23ms
step:351/2330 train_time:20437ms step_avg:58.23ms
step:352/2330 train_time:20499ms step_avg:58.24ms
step:353/2330 train_time:20555ms step_avg:58.23ms
step:354/2330 train_time:20616ms step_avg:58.24ms
step:355/2330 train_time:20672ms step_avg:58.23ms
step:356/2330 train_time:20733ms step_avg:58.24ms
step:357/2330 train_time:20790ms step_avg:58.23ms
step:358/2330 train_time:20850ms step_avg:58.24ms
step:359/2330 train_time:20906ms step_avg:58.23ms
step:360/2330 train_time:20966ms step_avg:58.24ms
step:361/2330 train_time:21021ms step_avg:58.23ms
step:362/2330 train_time:21082ms step_avg:58.24ms
step:363/2330 train_time:21138ms step_avg:58.23ms
step:364/2330 train_time:21200ms step_avg:58.24ms
step:365/2330 train_time:21256ms step_avg:58.23ms
step:366/2330 train_time:21317ms step_avg:58.24ms
step:367/2330 train_time:21373ms step_avg:58.24ms
step:368/2330 train_time:21435ms step_avg:58.25ms
step:369/2330 train_time:21491ms step_avg:58.24ms
step:370/2330 train_time:21552ms step_avg:58.25ms
step:371/2330 train_time:21608ms step_avg:58.24ms
step:372/2330 train_time:21669ms step_avg:58.25ms
step:373/2330 train_time:21725ms step_avg:58.24ms
step:374/2330 train_time:21784ms step_avg:58.25ms
step:375/2330 train_time:21840ms step_avg:58.24ms
step:376/2330 train_time:21901ms step_avg:58.25ms
step:377/2330 train_time:21957ms step_avg:58.24ms
step:378/2330 train_time:22018ms step_avg:58.25ms
step:379/2330 train_time:22074ms step_avg:58.24ms
step:380/2330 train_time:22136ms step_avg:58.25ms
step:381/2330 train_time:22192ms step_avg:58.25ms
step:382/2330 train_time:22253ms step_avg:58.25ms
step:383/2330 train_time:22310ms step_avg:58.25ms
step:384/2330 train_time:22370ms step_avg:58.26ms
step:385/2330 train_time:22427ms step_avg:58.25ms
step:386/2330 train_time:22487ms step_avg:58.26ms
step:387/2330 train_time:22543ms step_avg:58.25ms
step:388/2330 train_time:22603ms step_avg:58.26ms
step:389/2330 train_time:22658ms step_avg:58.25ms
step:390/2330 train_time:22721ms step_avg:58.26ms
step:391/2330 train_time:22777ms step_avg:58.25ms
step:392/2330 train_time:22838ms step_avg:58.26ms
step:393/2330 train_time:22894ms step_avg:58.25ms
step:394/2330 train_time:22955ms step_avg:58.26ms
step:395/2330 train_time:23011ms step_avg:58.26ms
step:396/2330 train_time:23073ms step_avg:58.26ms
step:397/2330 train_time:23129ms step_avg:58.26ms
step:398/2330 train_time:23189ms step_avg:58.26ms
step:399/2330 train_time:23244ms step_avg:58.26ms
step:400/2330 train_time:23304ms step_avg:58.26ms
step:401/2330 train_time:23360ms step_avg:58.25ms
step:402/2330 train_time:23421ms step_avg:58.26ms
step:403/2330 train_time:23476ms step_avg:58.25ms
step:404/2330 train_time:23538ms step_avg:58.26ms
step:405/2330 train_time:23594ms step_avg:58.26ms
step:406/2330 train_time:23655ms step_avg:58.26ms
step:407/2330 train_time:23711ms step_avg:58.26ms
step:408/2330 train_time:23773ms step_avg:58.27ms
step:409/2330 train_time:23828ms step_avg:58.26ms
step:410/2330 train_time:23889ms step_avg:58.27ms
step:411/2330 train_time:23945ms step_avg:58.26ms
step:412/2330 train_time:24006ms step_avg:58.27ms
step:413/2330 train_time:24062ms step_avg:58.26ms
step:414/2330 train_time:24122ms step_avg:58.27ms
step:415/2330 train_time:24178ms step_avg:58.26ms
step:416/2330 train_time:24240ms step_avg:58.27ms
step:417/2330 train_time:24295ms step_avg:58.26ms
step:418/2330 train_time:24357ms step_avg:58.27ms
step:419/2330 train_time:24413ms step_avg:58.26ms
step:420/2330 train_time:24473ms step_avg:58.27ms
step:421/2330 train_time:24529ms step_avg:58.26ms
step:422/2330 train_time:24590ms step_avg:58.27ms
step:423/2330 train_time:24645ms step_avg:58.26ms
step:424/2330 train_time:24705ms step_avg:58.27ms
step:425/2330 train_time:24760ms step_avg:58.26ms
step:426/2330 train_time:24822ms step_avg:58.27ms
step:427/2330 train_time:24878ms step_avg:58.26ms
step:428/2330 train_time:24939ms step_avg:58.27ms
step:429/2330 train_time:24994ms step_avg:58.26ms
step:430/2330 train_time:25057ms step_avg:58.27ms
step:431/2330 train_time:25112ms step_avg:58.27ms
step:432/2330 train_time:25174ms step_avg:58.27ms
step:433/2330 train_time:25229ms step_avg:58.27ms
step:434/2330 train_time:25290ms step_avg:58.27ms
step:435/2330 train_time:25345ms step_avg:58.27ms
step:436/2330 train_time:25406ms step_avg:58.27ms
step:437/2330 train_time:25461ms step_avg:58.26ms
step:438/2330 train_time:25521ms step_avg:58.27ms
step:439/2330 train_time:25577ms step_avg:58.26ms
step:440/2330 train_time:25638ms step_avg:58.27ms
step:441/2330 train_time:25694ms step_avg:58.26ms
step:442/2330 train_time:25756ms step_avg:58.27ms
step:443/2330 train_time:25812ms step_avg:58.27ms
step:444/2330 train_time:25873ms step_avg:58.27ms
step:445/2330 train_time:25929ms step_avg:58.27ms
step:446/2330 train_time:25989ms step_avg:58.27ms
step:447/2330 train_time:26045ms step_avg:58.27ms
step:448/2330 train_time:26105ms step_avg:58.27ms
step:449/2330 train_time:26161ms step_avg:58.26ms
step:450/2330 train_time:26221ms step_avg:58.27ms
step:451/2330 train_time:26277ms step_avg:58.26ms
step:452/2330 train_time:26339ms step_avg:58.27ms
step:453/2330 train_time:26394ms step_avg:58.27ms
step:454/2330 train_time:26456ms step_avg:58.27ms
step:455/2330 train_time:26511ms step_avg:58.27ms
step:456/2330 train_time:26572ms step_avg:58.27ms
step:457/2330 train_time:26629ms step_avg:58.27ms
step:458/2330 train_time:26688ms step_avg:58.27ms
step:459/2330 train_time:26744ms step_avg:58.27ms
step:460/2330 train_time:26804ms step_avg:58.27ms
step:461/2330 train_time:26860ms step_avg:58.26ms
step:462/2330 train_time:26921ms step_avg:58.27ms
step:463/2330 train_time:26976ms step_avg:58.26ms
step:464/2330 train_time:27038ms step_avg:58.27ms
step:465/2330 train_time:27094ms step_avg:58.27ms
step:466/2330 train_time:27156ms step_avg:58.27ms
step:467/2330 train_time:27212ms step_avg:58.27ms
step:468/2330 train_time:27273ms step_avg:58.27ms
step:469/2330 train_time:27329ms step_avg:58.27ms
step:470/2330 train_time:27390ms step_avg:58.28ms
step:471/2330 train_time:27446ms step_avg:58.27ms
step:472/2330 train_time:27506ms step_avg:58.28ms
step:473/2330 train_time:27562ms step_avg:58.27ms
step:474/2330 train_time:27622ms step_avg:58.28ms
step:475/2330 train_time:27678ms step_avg:58.27ms
step:476/2330 train_time:27739ms step_avg:58.28ms
step:477/2330 train_time:27795ms step_avg:58.27ms
step:478/2330 train_time:27857ms step_avg:58.28ms
step:479/2330 train_time:27912ms step_avg:58.27ms
step:480/2330 train_time:27973ms step_avg:58.28ms
step:481/2330 train_time:28029ms step_avg:58.27ms
step:482/2330 train_time:28089ms step_avg:58.28ms
step:483/2330 train_time:28145ms step_avg:58.27ms
step:484/2330 train_time:28206ms step_avg:58.28ms
step:485/2330 train_time:28262ms step_avg:58.27ms
step:486/2330 train_time:28322ms step_avg:58.28ms
step:487/2330 train_time:28378ms step_avg:58.27ms
step:488/2330 train_time:28439ms step_avg:58.28ms
step:489/2330 train_time:28495ms step_avg:58.27ms
step:490/2330 train_time:28556ms step_avg:58.28ms
step:491/2330 train_time:28612ms step_avg:58.27ms
step:492/2330 train_time:28672ms step_avg:58.28ms
step:493/2330 train_time:28728ms step_avg:58.27ms
step:494/2330 train_time:28788ms step_avg:58.27ms
step:495/2330 train_time:28844ms step_avg:58.27ms
step:496/2330 train_time:28904ms step_avg:58.27ms
step:497/2330 train_time:28959ms step_avg:58.27ms
step:498/2330 train_time:29020ms step_avg:58.27ms
step:499/2330 train_time:29076ms step_avg:58.27ms
step:500/2330 train_time:29138ms step_avg:58.28ms
step:500/2330 val_loss:4.9120 train_time:29216ms step_avg:58.43ms
step:501/2330 train_time:29236ms step_avg:58.35ms
step:502/2330 train_time:29258ms step_avg:58.28ms
step:503/2330 train_time:29314ms step_avg:58.28ms
step:504/2330 train_time:29378ms step_avg:58.29ms
step:505/2330 train_time:29433ms step_avg:58.28ms
step:506/2330 train_time:29498ms step_avg:58.30ms
step:507/2330 train_time:29553ms step_avg:58.29ms
step:508/2330 train_time:29616ms step_avg:58.30ms
step:509/2330 train_time:29671ms step_avg:58.29ms
step:510/2330 train_time:29732ms step_avg:58.30ms
step:511/2330 train_time:29787ms step_avg:58.29ms
step:512/2330 train_time:29848ms step_avg:58.30ms
step:513/2330 train_time:29903ms step_avg:58.29ms
step:514/2330 train_time:29965ms step_avg:58.30ms
step:515/2330 train_time:30020ms step_avg:58.29ms
step:516/2330 train_time:30081ms step_avg:58.30ms
step:517/2330 train_time:30136ms step_avg:58.29ms
step:518/2330 train_time:30196ms step_avg:58.29ms
step:519/2330 train_time:30251ms step_avg:58.29ms
step:520/2330 train_time:30313ms step_avg:58.29ms
step:521/2330 train_time:30368ms step_avg:58.29ms
step:522/2330 train_time:30431ms step_avg:58.30ms
step:523/2330 train_time:30486ms step_avg:58.29ms
step:524/2330 train_time:30549ms step_avg:58.30ms
step:525/2330 train_time:30605ms step_avg:58.29ms
step:526/2330 train_time:30666ms step_avg:58.30ms
step:527/2330 train_time:30722ms step_avg:58.30ms
step:528/2330 train_time:30784ms step_avg:58.30ms
step:529/2330 train_time:30839ms step_avg:58.30ms
step:530/2330 train_time:30900ms step_avg:58.30ms
step:531/2330 train_time:30955ms step_avg:58.30ms
step:532/2330 train_time:31017ms step_avg:58.30ms
step:533/2330 train_time:31072ms step_avg:58.30ms
step:534/2330 train_time:31133ms step_avg:58.30ms
step:535/2330 train_time:31188ms step_avg:58.30ms
step:536/2330 train_time:31249ms step_avg:58.30ms
step:537/2330 train_time:31305ms step_avg:58.30ms
step:538/2330 train_time:31365ms step_avg:58.30ms
step:539/2330 train_time:31422ms step_avg:58.30ms
step:540/2330 train_time:31483ms step_avg:58.30ms
step:541/2330 train_time:31539ms step_avg:58.30ms
step:542/2330 train_time:31601ms step_avg:58.30ms
step:543/2330 train_time:31657ms step_avg:58.30ms
step:544/2330 train_time:31718ms step_avg:58.30ms
step:545/2330 train_time:31773ms step_avg:58.30ms
step:546/2330 train_time:31834ms step_avg:58.30ms
step:547/2330 train_time:31889ms step_avg:58.30ms
step:548/2330 train_time:31950ms step_avg:58.30ms
step:549/2330 train_time:32005ms step_avg:58.30ms
step:550/2330 train_time:32067ms step_avg:58.30ms
step:551/2330 train_time:32123ms step_avg:58.30ms
step:552/2330 train_time:32184ms step_avg:58.30ms
step:553/2330 train_time:32240ms step_avg:58.30ms
step:554/2330 train_time:32301ms step_avg:58.30ms
step:555/2330 train_time:32356ms step_avg:58.30ms
step:556/2330 train_time:32418ms step_avg:58.31ms
step:557/2330 train_time:32473ms step_avg:58.30ms
step:558/2330 train_time:32534ms step_avg:58.30ms
step:559/2330 train_time:32590ms step_avg:58.30ms
step:560/2330 train_time:32651ms step_avg:58.31ms
step:561/2330 train_time:32707ms step_avg:58.30ms
step:562/2330 train_time:32768ms step_avg:58.31ms
step:563/2330 train_time:32824ms step_avg:58.30ms
step:564/2330 train_time:32885ms step_avg:58.31ms
step:565/2330 train_time:32940ms step_avg:58.30ms
step:566/2330 train_time:33001ms step_avg:58.31ms
step:567/2330 train_time:33057ms step_avg:58.30ms
step:568/2330 train_time:33117ms step_avg:58.30ms
step:569/2330 train_time:33172ms step_avg:58.30ms
step:570/2330 train_time:33232ms step_avg:58.30ms
step:571/2330 train_time:33287ms step_avg:58.30ms
step:572/2330 train_time:33349ms step_avg:58.30ms
step:573/2330 train_time:33405ms step_avg:58.30ms
step:574/2330 train_time:33468ms step_avg:58.31ms
step:575/2330 train_time:33523ms step_avg:58.30ms
step:576/2330 train_time:33585ms step_avg:58.31ms
step:577/2330 train_time:33641ms step_avg:58.30ms
step:578/2330 train_time:33702ms step_avg:58.31ms
step:579/2330 train_time:33758ms step_avg:58.30ms
step:580/2330 train_time:33819ms step_avg:58.31ms
step:581/2330 train_time:33874ms step_avg:58.30ms
step:582/2330 train_time:33935ms step_avg:58.31ms
step:583/2330 train_time:33990ms step_avg:58.30ms
step:584/2330 train_time:34052ms step_avg:58.31ms
step:585/2330 train_time:34108ms step_avg:58.30ms
step:586/2330 train_time:34169ms step_avg:58.31ms
step:587/2330 train_time:34224ms step_avg:58.30ms
step:588/2330 train_time:34285ms step_avg:58.31ms
step:589/2330 train_time:34340ms step_avg:58.30ms
step:590/2330 train_time:34402ms step_avg:58.31ms
step:591/2330 train_time:34458ms step_avg:58.30ms
step:592/2330 train_time:34519ms step_avg:58.31ms
step:593/2330 train_time:34574ms step_avg:58.30ms
step:594/2330 train_time:34635ms step_avg:58.31ms
step:595/2330 train_time:34690ms step_avg:58.30ms
step:596/2330 train_time:34751ms step_avg:58.31ms
step:597/2330 train_time:34806ms step_avg:58.30ms
step:598/2330 train_time:34869ms step_avg:58.31ms
step:599/2330 train_time:34925ms step_avg:58.31ms
step:600/2330 train_time:34987ms step_avg:58.31ms
step:601/2330 train_time:35042ms step_avg:58.31ms
step:602/2330 train_time:35104ms step_avg:58.31ms
step:603/2330 train_time:35160ms step_avg:58.31ms
step:604/2330 train_time:35221ms step_avg:58.31ms
step:605/2330 train_time:35277ms step_avg:58.31ms
step:606/2330 train_time:35337ms step_avg:58.31ms
step:607/2330 train_time:35393ms step_avg:58.31ms
step:608/2330 train_time:35453ms step_avg:58.31ms
step:609/2330 train_time:35509ms step_avg:58.31ms
step:610/2330 train_time:35570ms step_avg:58.31ms
step:611/2330 train_time:35625ms step_avg:58.31ms
step:612/2330 train_time:35686ms step_avg:58.31ms
step:613/2330 train_time:35742ms step_avg:58.31ms
step:614/2330 train_time:35804ms step_avg:58.31ms
step:615/2330 train_time:35860ms step_avg:58.31ms
step:616/2330 train_time:35921ms step_avg:58.31ms
step:617/2330 train_time:35976ms step_avg:58.31ms
step:618/2330 train_time:36037ms step_avg:58.31ms
step:619/2330 train_time:36093ms step_avg:58.31ms
step:620/2330 train_time:36154ms step_avg:58.31ms
step:621/2330 train_time:36209ms step_avg:58.31ms
step:622/2330 train_time:36271ms step_avg:58.31ms
step:623/2330 train_time:36327ms step_avg:58.31ms
step:624/2330 train_time:36389ms step_avg:58.32ms
step:625/2330 train_time:36444ms step_avg:58.31ms
step:626/2330 train_time:36506ms step_avg:58.32ms
step:627/2330 train_time:36561ms step_avg:58.31ms
step:628/2330 train_time:36622ms step_avg:58.32ms
step:629/2330 train_time:36678ms step_avg:58.31ms
step:630/2330 train_time:36739ms step_avg:58.32ms
step:631/2330 train_time:36795ms step_avg:58.31ms
step:632/2330 train_time:36855ms step_avg:58.31ms
step:633/2330 train_time:36910ms step_avg:58.31ms
step:634/2330 train_time:36971ms step_avg:58.31ms
step:635/2330 train_time:37027ms step_avg:58.31ms
step:636/2330 train_time:37089ms step_avg:58.32ms
step:637/2330 train_time:37144ms step_avg:58.31ms
step:638/2330 train_time:37207ms step_avg:58.32ms
step:639/2330 train_time:37263ms step_avg:58.31ms
step:640/2330 train_time:37326ms step_avg:58.32ms
step:641/2330 train_time:37381ms step_avg:58.32ms
step:642/2330 train_time:37443ms step_avg:58.32ms
step:643/2330 train_time:37499ms step_avg:58.32ms
step:644/2330 train_time:37559ms step_avg:58.32ms
step:645/2330 train_time:37615ms step_avg:58.32ms
step:646/2330 train_time:37676ms step_avg:58.32ms
step:647/2330 train_time:37731ms step_avg:58.32ms
step:648/2330 train_time:37792ms step_avg:58.32ms
step:649/2330 train_time:37848ms step_avg:58.32ms
step:650/2330 train_time:37909ms step_avg:58.32ms
step:651/2330 train_time:37964ms step_avg:58.32ms
step:652/2330 train_time:38026ms step_avg:58.32ms
step:653/2330 train_time:38082ms step_avg:58.32ms
step:654/2330 train_time:38144ms step_avg:58.32ms
step:655/2330 train_time:38200ms step_avg:58.32ms
step:656/2330 train_time:38260ms step_avg:58.32ms
step:657/2330 train_time:38316ms step_avg:58.32ms
step:658/2330 train_time:38377ms step_avg:58.32ms
step:659/2330 train_time:38432ms step_avg:58.32ms
step:660/2330 train_time:38493ms step_avg:58.32ms
step:661/2330 train_time:38549ms step_avg:58.32ms
step:662/2330 train_time:38610ms step_avg:58.32ms
step:663/2330 train_time:38665ms step_avg:58.32ms
step:664/2330 train_time:38727ms step_avg:58.32ms
step:665/2330 train_time:38783ms step_avg:58.32ms
step:666/2330 train_time:38845ms step_avg:58.33ms
step:667/2330 train_time:38900ms step_avg:58.32ms
step:668/2330 train_time:38961ms step_avg:58.32ms
step:669/2330 train_time:39017ms step_avg:58.32ms
step:670/2330 train_time:39077ms step_avg:58.32ms
step:671/2330 train_time:39133ms step_avg:58.32ms
step:672/2330 train_time:39193ms step_avg:58.32ms
step:673/2330 train_time:39250ms step_avg:58.32ms
step:674/2330 train_time:39310ms step_avg:58.32ms
step:675/2330 train_time:39365ms step_avg:58.32ms
step:676/2330 train_time:39428ms step_avg:58.33ms
step:677/2330 train_time:39484ms step_avg:58.32ms
step:678/2330 train_time:39545ms step_avg:58.33ms
step:679/2330 train_time:39601ms step_avg:58.32ms
step:680/2330 train_time:39663ms step_avg:58.33ms
step:681/2330 train_time:39718ms step_avg:58.32ms
step:682/2330 train_time:39779ms step_avg:58.33ms
step:683/2330 train_time:39835ms step_avg:58.32ms
step:684/2330 train_time:39896ms step_avg:58.33ms
step:685/2330 train_time:39952ms step_avg:58.32ms
step:686/2330 train_time:40012ms step_avg:58.33ms
step:687/2330 train_time:40067ms step_avg:58.32ms
step:688/2330 train_time:40129ms step_avg:58.33ms
step:689/2330 train_time:40184ms step_avg:58.32ms
step:690/2330 train_time:40246ms step_avg:58.33ms
step:691/2330 train_time:40302ms step_avg:58.32ms
step:692/2330 train_time:40363ms step_avg:58.33ms
step:693/2330 train_time:40419ms step_avg:58.32ms
step:694/2330 train_time:40479ms step_avg:58.33ms
step:695/2330 train_time:40535ms step_avg:58.32ms
step:696/2330 train_time:40597ms step_avg:58.33ms
step:697/2330 train_time:40652ms step_avg:58.32ms
step:698/2330 train_time:40713ms step_avg:58.33ms
step:699/2330 train_time:40768ms step_avg:58.32ms
step:700/2330 train_time:40830ms step_avg:58.33ms
step:701/2330 train_time:40885ms step_avg:58.32ms
step:702/2330 train_time:40947ms step_avg:58.33ms
step:703/2330 train_time:41003ms step_avg:58.33ms
step:704/2330 train_time:41065ms step_avg:58.33ms
step:705/2330 train_time:41120ms step_avg:58.33ms
step:706/2330 train_time:41181ms step_avg:58.33ms
step:707/2330 train_time:41237ms step_avg:58.33ms
step:708/2330 train_time:41298ms step_avg:58.33ms
step:709/2330 train_time:41353ms step_avg:58.33ms
step:710/2330 train_time:41413ms step_avg:58.33ms
step:711/2330 train_time:41469ms step_avg:58.32ms
step:712/2330 train_time:41530ms step_avg:58.33ms
step:713/2330 train_time:41586ms step_avg:58.32ms
step:714/2330 train_time:41647ms step_avg:58.33ms
step:715/2330 train_time:41703ms step_avg:58.33ms
step:716/2330 train_time:41764ms step_avg:58.33ms
step:717/2330 train_time:41820ms step_avg:58.33ms
step:718/2330 train_time:41881ms step_avg:58.33ms
step:719/2330 train_time:41937ms step_avg:58.33ms
step:720/2330 train_time:41997ms step_avg:58.33ms
step:721/2330 train_time:42053ms step_avg:58.33ms
step:722/2330 train_time:42114ms step_avg:58.33ms
step:723/2330 train_time:42169ms step_avg:58.33ms
step:724/2330 train_time:42231ms step_avg:58.33ms
step:725/2330 train_time:42287ms step_avg:58.33ms
step:726/2330 train_time:42349ms step_avg:58.33ms
step:727/2330 train_time:42404ms step_avg:58.33ms
step:728/2330 train_time:42466ms step_avg:58.33ms
step:729/2330 train_time:42522ms step_avg:58.33ms
step:730/2330 train_time:42583ms step_avg:58.33ms
step:731/2330 train_time:42639ms step_avg:58.33ms
step:732/2330 train_time:42700ms step_avg:58.33ms
step:733/2330 train_time:42756ms step_avg:58.33ms
step:734/2330 train_time:42818ms step_avg:58.33ms
step:735/2330 train_time:42873ms step_avg:58.33ms
step:736/2330 train_time:42934ms step_avg:58.33ms
step:737/2330 train_time:42990ms step_avg:58.33ms
step:738/2330 train_time:43050ms step_avg:58.33ms
step:739/2330 train_time:43105ms step_avg:58.33ms
step:740/2330 train_time:43167ms step_avg:58.33ms
step:741/2330 train_time:43223ms step_avg:58.33ms
step:742/2330 train_time:43284ms step_avg:58.33ms
step:743/2330 train_time:43340ms step_avg:58.33ms
step:744/2330 train_time:43400ms step_avg:58.33ms
step:745/2330 train_time:43456ms step_avg:58.33ms
step:746/2330 train_time:43516ms step_avg:58.33ms
step:747/2330 train_time:43572ms step_avg:58.33ms
step:748/2330 train_time:43633ms step_avg:58.33ms
step:749/2330 train_time:43688ms step_avg:58.33ms
step:750/2330 train_time:43749ms step_avg:58.33ms
step:750/2330 val_loss:4.6045 train_time:43827ms step_avg:58.44ms
step:751/2330 train_time:43847ms step_avg:58.38ms
step:752/2330 train_time:43869ms step_avg:58.34ms
step:753/2330 train_time:43925ms step_avg:58.33ms
step:754/2330 train_time:43988ms step_avg:58.34ms
step:755/2330 train_time:44044ms step_avg:58.34ms
step:756/2330 train_time:44107ms step_avg:58.34ms
step:757/2330 train_time:44163ms step_avg:58.34ms
step:758/2330 train_time:44223ms step_avg:58.34ms
step:759/2330 train_time:44278ms step_avg:58.34ms
step:760/2330 train_time:44339ms step_avg:58.34ms
step:761/2330 train_time:44394ms step_avg:58.34ms
step:762/2330 train_time:44455ms step_avg:58.34ms
step:763/2330 train_time:44511ms step_avg:58.34ms
step:764/2330 train_time:44570ms step_avg:58.34ms
step:765/2330 train_time:44627ms step_avg:58.34ms
step:766/2330 train_time:44687ms step_avg:58.34ms
step:767/2330 train_time:44743ms step_avg:58.34ms
step:768/2330 train_time:44804ms step_avg:58.34ms
step:769/2330 train_time:44860ms step_avg:58.34ms
step:770/2330 train_time:44923ms step_avg:58.34ms
step:771/2330 train_time:44980ms step_avg:58.34ms
step:772/2330 train_time:45043ms step_avg:58.35ms
step:773/2330 train_time:45100ms step_avg:58.34ms
step:774/2330 train_time:45162ms step_avg:58.35ms
step:775/2330 train_time:45218ms step_avg:58.35ms
step:776/2330 train_time:45280ms step_avg:58.35ms
step:777/2330 train_time:45336ms step_avg:58.35ms
step:778/2330 train_time:45398ms step_avg:58.35ms
step:779/2330 train_time:45455ms step_avg:58.35ms
step:780/2330 train_time:45516ms step_avg:58.35ms
step:781/2330 train_time:45573ms step_avg:58.35ms
step:782/2330 train_time:45634ms step_avg:58.36ms
step:783/2330 train_time:45690ms step_avg:58.35ms
step:784/2330 train_time:45750ms step_avg:58.36ms
step:785/2330 train_time:45807ms step_avg:58.35ms
step:786/2330 train_time:45869ms step_avg:58.36ms
step:787/2330 train_time:45925ms step_avg:58.35ms
step:788/2330 train_time:45986ms step_avg:58.36ms
step:789/2330 train_time:46043ms step_avg:58.36ms
step:790/2330 train_time:46104ms step_avg:58.36ms
step:791/2330 train_time:46161ms step_avg:58.36ms
step:792/2330 train_time:46223ms step_avg:58.36ms
step:793/2330 train_time:46279ms step_avg:58.36ms
step:794/2330 train_time:46341ms step_avg:58.36ms
step:795/2330 train_time:46398ms step_avg:58.36ms
step:796/2330 train_time:46459ms step_avg:58.37ms
step:797/2330 train_time:46515ms step_avg:58.36ms
step:798/2330 train_time:46577ms step_avg:58.37ms
step:799/2330 train_time:46633ms step_avg:58.36ms
step:800/2330 train_time:46695ms step_avg:58.37ms
step:801/2330 train_time:46752ms step_avg:58.37ms
step:802/2330 train_time:46813ms step_avg:58.37ms
step:803/2330 train_time:46870ms step_avg:58.37ms
step:804/2330 train_time:46931ms step_avg:58.37ms
step:805/2330 train_time:46988ms step_avg:58.37ms
step:806/2330 train_time:47049ms step_avg:58.37ms
step:807/2330 train_time:47106ms step_avg:58.37ms
step:808/2330 train_time:47167ms step_avg:58.37ms
step:809/2330 train_time:47223ms step_avg:58.37ms
step:810/2330 train_time:47285ms step_avg:58.38ms
step:811/2330 train_time:47341ms step_avg:58.37ms
step:812/2330 train_time:47403ms step_avg:58.38ms
step:813/2330 train_time:47459ms step_avg:58.37ms
step:814/2330 train_time:47521ms step_avg:58.38ms
step:815/2330 train_time:47578ms step_avg:58.38ms
step:816/2330 train_time:47640ms step_avg:58.38ms
step:817/2330 train_time:47696ms step_avg:58.38ms
step:818/2330 train_time:47758ms step_avg:58.38ms
step:819/2330 train_time:47814ms step_avg:58.38ms
step:820/2330 train_time:47877ms step_avg:58.39ms
step:821/2330 train_time:47934ms step_avg:58.39ms
step:822/2330 train_time:47997ms step_avg:58.39ms
step:823/2330 train_time:48054ms step_avg:58.39ms
step:824/2330 train_time:48115ms step_avg:58.39ms
step:825/2330 train_time:48172ms step_avg:58.39ms
step:826/2330 train_time:48233ms step_avg:58.39ms
step:827/2330 train_time:48291ms step_avg:58.39ms
step:828/2330 train_time:48351ms step_avg:58.40ms
step:829/2330 train_time:48409ms step_avg:58.39ms
step:830/2330 train_time:48469ms step_avg:58.40ms
step:831/2330 train_time:48526ms step_avg:58.39ms
step:832/2330 train_time:48586ms step_avg:58.40ms
step:833/2330 train_time:48643ms step_avg:58.39ms
step:834/2330 train_time:48704ms step_avg:58.40ms
step:835/2330 train_time:48760ms step_avg:58.40ms
step:836/2330 train_time:48822ms step_avg:58.40ms
step:837/2330 train_time:48878ms step_avg:58.40ms
step:838/2330 train_time:48940ms step_avg:58.40ms
step:839/2330 train_time:48997ms step_avg:58.40ms
step:840/2330 train_time:49059ms step_avg:58.40ms
step:841/2330 train_time:49115ms step_avg:58.40ms
step:842/2330 train_time:49177ms step_avg:58.41ms
step:843/2330 train_time:49234ms step_avg:58.40ms
step:844/2330 train_time:49296ms step_avg:58.41ms
step:845/2330 train_time:49353ms step_avg:58.41ms
step:846/2330 train_time:49415ms step_avg:58.41ms
step:847/2330 train_time:49472ms step_avg:58.41ms
step:848/2330 train_time:49533ms step_avg:58.41ms
step:849/2330 train_time:49590ms step_avg:58.41ms
step:850/2330 train_time:49651ms step_avg:58.41ms
step:851/2330 train_time:49708ms step_avg:58.41ms
step:852/2330 train_time:49768ms step_avg:58.41ms
step:853/2330 train_time:49824ms step_avg:58.41ms
step:854/2330 train_time:49885ms step_avg:58.41ms
step:855/2330 train_time:49941ms step_avg:58.41ms
step:856/2330 train_time:50003ms step_avg:58.41ms
step:857/2330 train_time:50058ms step_avg:58.41ms
step:858/2330 train_time:50121ms step_avg:58.42ms
step:859/2330 train_time:50177ms step_avg:58.41ms
step:860/2330 train_time:50240ms step_avg:58.42ms
step:861/2330 train_time:50297ms step_avg:58.42ms
step:862/2330 train_time:50359ms step_avg:58.42ms
step:863/2330 train_time:50415ms step_avg:58.42ms
step:864/2330 train_time:50478ms step_avg:58.42ms
step:865/2330 train_time:50534ms step_avg:58.42ms
step:866/2330 train_time:50598ms step_avg:58.43ms
step:867/2330 train_time:50654ms step_avg:58.42ms
step:868/2330 train_time:50716ms step_avg:58.43ms
step:869/2330 train_time:50774ms step_avg:58.43ms
step:870/2330 train_time:50835ms step_avg:58.43ms
step:871/2330 train_time:50892ms step_avg:58.43ms
step:872/2330 train_time:50952ms step_avg:58.43ms
step:873/2330 train_time:51009ms step_avg:58.43ms
step:874/2330 train_time:51069ms step_avg:58.43ms
step:875/2330 train_time:51125ms step_avg:58.43ms
step:876/2330 train_time:51186ms step_avg:58.43ms
step:877/2330 train_time:51242ms step_avg:58.43ms
step:878/2330 train_time:51304ms step_avg:58.43ms
step:879/2330 train_time:51360ms step_avg:58.43ms
step:880/2330 train_time:51423ms step_avg:58.44ms
step:881/2330 train_time:51479ms step_avg:58.43ms
step:882/2330 train_time:51542ms step_avg:58.44ms
step:883/2330 train_time:51598ms step_avg:58.44ms
step:884/2330 train_time:51660ms step_avg:58.44ms
step:885/2330 train_time:51716ms step_avg:58.44ms
step:886/2330 train_time:51779ms step_avg:58.44ms
step:887/2330 train_time:51835ms step_avg:58.44ms
step:888/2330 train_time:51898ms step_avg:58.44ms
step:889/2330 train_time:51955ms step_avg:58.44ms
step:890/2330 train_time:52016ms step_avg:58.45ms
step:891/2330 train_time:52073ms step_avg:58.44ms
step:892/2330 train_time:52134ms step_avg:58.45ms
step:893/2330 train_time:52192ms step_avg:58.45ms
step:894/2330 train_time:52252ms step_avg:58.45ms
step:895/2330 train_time:52310ms step_avg:58.45ms
step:896/2330 train_time:52371ms step_avg:58.45ms
step:897/2330 train_time:52427ms step_avg:58.45ms
step:898/2330 train_time:52488ms step_avg:58.45ms
step:899/2330 train_time:52544ms step_avg:58.45ms
step:900/2330 train_time:52605ms step_avg:58.45ms
step:901/2330 train_time:52662ms step_avg:58.45ms
step:902/2330 train_time:52724ms step_avg:58.45ms
step:903/2330 train_time:52780ms step_avg:58.45ms
step:904/2330 train_time:52842ms step_avg:58.45ms
step:905/2330 train_time:52898ms step_avg:58.45ms
step:906/2330 train_time:52960ms step_avg:58.46ms
step:907/2330 train_time:53017ms step_avg:58.45ms
step:908/2330 train_time:53078ms step_avg:58.46ms
step:909/2330 train_time:53135ms step_avg:58.45ms
step:910/2330 train_time:53197ms step_avg:58.46ms
step:911/2330 train_time:53254ms step_avg:58.46ms
step:912/2330 train_time:53316ms step_avg:58.46ms
step:913/2330 train_time:53373ms step_avg:58.46ms
step:914/2330 train_time:53435ms step_avg:58.46ms
step:915/2330 train_time:53492ms step_avg:58.46ms
step:916/2330 train_time:53553ms step_avg:58.46ms
step:917/2330 train_time:53610ms step_avg:58.46ms
step:918/2330 train_time:53671ms step_avg:58.46ms
step:919/2330 train_time:53728ms step_avg:58.46ms
step:920/2330 train_time:53788ms step_avg:58.47ms
step:921/2330 train_time:53844ms step_avg:58.46ms
step:922/2330 train_time:53906ms step_avg:58.47ms
step:923/2330 train_time:53962ms step_avg:58.46ms
step:924/2330 train_time:54024ms step_avg:58.47ms
step:925/2330 train_time:54080ms step_avg:58.47ms
step:926/2330 train_time:54143ms step_avg:58.47ms
step:927/2330 train_time:54199ms step_avg:58.47ms
step:928/2330 train_time:54261ms step_avg:58.47ms
step:929/2330 train_time:54317ms step_avg:58.47ms
step:930/2330 train_time:54379ms step_avg:58.47ms
step:931/2330 train_time:54436ms step_avg:58.47ms
step:932/2330 train_time:54499ms step_avg:58.48ms
step:933/2330 train_time:54556ms step_avg:58.47ms
step:934/2330 train_time:54619ms step_avg:58.48ms
step:935/2330 train_time:54676ms step_avg:58.48ms
step:936/2330 train_time:54738ms step_avg:58.48ms
step:937/2330 train_time:54796ms step_avg:58.48ms
step:938/2330 train_time:54856ms step_avg:58.48ms
step:939/2330 train_time:54913ms step_avg:58.48ms
step:940/2330 train_time:54975ms step_avg:58.48ms
step:941/2330 train_time:55031ms step_avg:58.48ms
step:942/2330 train_time:55092ms step_avg:58.48ms
step:943/2330 train_time:55148ms step_avg:58.48ms
step:944/2330 train_time:55209ms step_avg:58.48ms
step:945/2330 train_time:55266ms step_avg:58.48ms
step:946/2330 train_time:55327ms step_avg:58.49ms
step:947/2330 train_time:55384ms step_avg:58.48ms
step:948/2330 train_time:55445ms step_avg:58.49ms
step:949/2330 train_time:55501ms step_avg:58.48ms
step:950/2330 train_time:55564ms step_avg:58.49ms
step:951/2330 train_time:55619ms step_avg:58.49ms
step:952/2330 train_time:55681ms step_avg:58.49ms
step:953/2330 train_time:55738ms step_avg:58.49ms
step:954/2330 train_time:55801ms step_avg:58.49ms
step:955/2330 train_time:55857ms step_avg:58.49ms
step:956/2330 train_time:55919ms step_avg:58.49ms
step:957/2330 train_time:55975ms step_avg:58.49ms
step:958/2330 train_time:56038ms step_avg:58.49ms
step:959/2330 train_time:56095ms step_avg:58.49ms
step:960/2330 train_time:56156ms step_avg:58.50ms
step:961/2330 train_time:56214ms step_avg:58.50ms
step:962/2330 train_time:56274ms step_avg:58.50ms
step:963/2330 train_time:56331ms step_avg:58.50ms
step:964/2330 train_time:56392ms step_avg:58.50ms
step:965/2330 train_time:56449ms step_avg:58.50ms
step:966/2330 train_time:56509ms step_avg:58.50ms
step:967/2330 train_time:56566ms step_avg:58.50ms
step:968/2330 train_time:56627ms step_avg:58.50ms
step:969/2330 train_time:56683ms step_avg:58.50ms
step:970/2330 train_time:56745ms step_avg:58.50ms
step:971/2330 train_time:56802ms step_avg:58.50ms
step:972/2330 train_time:56863ms step_avg:58.50ms
step:973/2330 train_time:56919ms step_avg:58.50ms
step:974/2330 train_time:56980ms step_avg:58.50ms
step:975/2330 train_time:57036ms step_avg:58.50ms
step:976/2330 train_time:57099ms step_avg:58.50ms
step:977/2330 train_time:57156ms step_avg:58.50ms
step:978/2330 train_time:57217ms step_avg:58.50ms
step:979/2330 train_time:57274ms step_avg:58.50ms
step:980/2330 train_time:57336ms step_avg:58.51ms
step:981/2330 train_time:57392ms step_avg:58.50ms
step:982/2330 train_time:57456ms step_avg:58.51ms
step:983/2330 train_time:57513ms step_avg:58.51ms
step:984/2330 train_time:57574ms step_avg:58.51ms
step:985/2330 train_time:57631ms step_avg:58.51ms
step:986/2330 train_time:57691ms step_avg:58.51ms
step:987/2330 train_time:57748ms step_avg:58.51ms
step:988/2330 train_time:57809ms step_avg:58.51ms
step:989/2330 train_time:57866ms step_avg:58.51ms
step:990/2330 train_time:57926ms step_avg:58.51ms
step:991/2330 train_time:57982ms step_avg:58.51ms
step:992/2330 train_time:58043ms step_avg:58.51ms
step:993/2330 train_time:58099ms step_avg:58.51ms
step:994/2330 train_time:58161ms step_avg:58.51ms
step:995/2330 train_time:58217ms step_avg:58.51ms
step:996/2330 train_time:58280ms step_avg:58.51ms
step:997/2330 train_time:58336ms step_avg:58.51ms
step:998/2330 train_time:58399ms step_avg:58.52ms
step:999/2330 train_time:58456ms step_avg:58.51ms
step:1000/2330 train_time:58518ms step_avg:58.52ms
step:1000/2330 val_loss:4.4194 train_time:58597ms step_avg:58.60ms
step:1001/2330 train_time:58617ms step_avg:58.56ms
step:1002/2330 train_time:58638ms step_avg:58.52ms
step:1003/2330 train_time:58694ms step_avg:58.52ms
step:1004/2330 train_time:58761ms step_avg:58.53ms
step:1005/2330 train_time:58817ms step_avg:58.52ms
step:1006/2330 train_time:58883ms step_avg:58.53ms
step:1007/2330 train_time:58938ms step_avg:58.53ms
step:1008/2330 train_time:59000ms step_avg:58.53ms
step:1009/2330 train_time:59056ms step_avg:58.53ms
step:1010/2330 train_time:59117ms step_avg:58.53ms
step:1011/2330 train_time:59173ms step_avg:58.53ms
step:1012/2330 train_time:59234ms step_avg:58.53ms
step:1013/2330 train_time:59290ms step_avg:58.53ms
step:1014/2330 train_time:59351ms step_avg:58.53ms
step:1015/2330 train_time:59407ms step_avg:58.53ms
step:1016/2330 train_time:59467ms step_avg:58.53ms
step:1017/2330 train_time:59523ms step_avg:58.53ms
step:1018/2330 train_time:59585ms step_avg:58.53ms
step:1019/2330 train_time:59641ms step_avg:58.53ms
step:1020/2330 train_time:59703ms step_avg:58.53ms
step:1021/2330 train_time:59760ms step_avg:58.53ms
step:1022/2330 train_time:59822ms step_avg:58.53ms
step:1023/2330 train_time:59879ms step_avg:58.53ms
step:1024/2330 train_time:59941ms step_avg:58.54ms
step:1025/2330 train_time:59998ms step_avg:58.53ms
step:1026/2330 train_time:60059ms step_avg:58.54ms
step:1027/2330 train_time:60115ms step_avg:58.53ms
step:1028/2330 train_time:60176ms step_avg:58.54ms
step:1029/2330 train_time:60232ms step_avg:58.53ms
step:1030/2330 train_time:60294ms step_avg:58.54ms
step:1031/2330 train_time:60350ms step_avg:58.54ms
step:1032/2330 train_time:60411ms step_avg:58.54ms
step:1033/2330 train_time:60467ms step_avg:58.54ms
step:1034/2330 train_time:60529ms step_avg:58.54ms
step:1035/2330 train_time:60585ms step_avg:58.54ms
step:1036/2330 train_time:60647ms step_avg:58.54ms
step:1037/2330 train_time:60704ms step_avg:58.54ms
step:1038/2330 train_time:60766ms step_avg:58.54ms
step:1039/2330 train_time:60822ms step_avg:58.54ms
step:1040/2330 train_time:60883ms step_avg:58.54ms
step:1041/2330 train_time:60940ms step_avg:58.54ms
step:1042/2330 train_time:61001ms step_avg:58.54ms
step:1043/2330 train_time:61058ms step_avg:58.54ms
step:1044/2330 train_time:61119ms step_avg:58.54ms
step:1045/2330 train_time:61175ms step_avg:58.54ms
step:1046/2330 train_time:61237ms step_avg:58.54ms
step:1047/2330 train_time:61292ms step_avg:58.54ms
step:1048/2330 train_time:61356ms step_avg:58.55ms
step:1049/2330 train_time:61412ms step_avg:58.54ms
step:1050/2330 train_time:61474ms step_avg:58.55ms
step:1051/2330 train_time:61530ms step_avg:58.54ms
step:1052/2330 train_time:61592ms step_avg:58.55ms
step:1053/2330 train_time:61648ms step_avg:58.55ms
step:1054/2330 train_time:61710ms step_avg:58.55ms
step:1055/2330 train_time:61767ms step_avg:58.55ms
step:1056/2330 train_time:61828ms step_avg:58.55ms
step:1057/2330 train_time:61885ms step_avg:58.55ms
step:1058/2330 train_time:61947ms step_avg:58.55ms
step:1059/2330 train_time:62004ms step_avg:58.55ms
step:1060/2330 train_time:62064ms step_avg:58.55ms
step:1061/2330 train_time:62120ms step_avg:58.55ms
step:1062/2330 train_time:62182ms step_avg:58.55ms
step:1063/2330 train_time:62238ms step_avg:58.55ms
step:1064/2330 train_time:62300ms step_avg:58.55ms
step:1065/2330 train_time:62356ms step_avg:58.55ms
step:1066/2330 train_time:62417ms step_avg:58.55ms
step:1067/2330 train_time:62473ms step_avg:58.55ms
step:1068/2330 train_time:62536ms step_avg:58.55ms
step:1069/2330 train_time:62592ms step_avg:58.55ms
step:1070/2330 train_time:62655ms step_avg:58.56ms
step:1071/2330 train_time:62711ms step_avg:58.55ms
step:1072/2330 train_time:62775ms step_avg:58.56ms
step:1073/2330 train_time:62830ms step_avg:58.56ms
step:1074/2330 train_time:62894ms step_avg:58.56ms
step:1075/2330 train_time:62950ms step_avg:58.56ms
step:1076/2330 train_time:63012ms step_avg:58.56ms
step:1077/2330 train_time:63068ms step_avg:58.56ms
step:1078/2330 train_time:63130ms step_avg:58.56ms
step:1079/2330 train_time:63186ms step_avg:58.56ms
step:1080/2330 train_time:63247ms step_avg:58.56ms
step:1081/2330 train_time:63304ms step_avg:58.56ms
step:1082/2330 train_time:63364ms step_avg:58.56ms
step:1083/2330 train_time:63420ms step_avg:58.56ms
step:1084/2330 train_time:63482ms step_avg:58.56ms
step:1085/2330 train_time:63538ms step_avg:58.56ms
step:1086/2330 train_time:63600ms step_avg:58.56ms
step:1087/2330 train_time:63656ms step_avg:58.56ms
step:1088/2330 train_time:63718ms step_avg:58.56ms
step:1089/2330 train_time:63774ms step_avg:58.56ms
step:1090/2330 train_time:63837ms step_avg:58.57ms
step:1091/2330 train_time:63893ms step_avg:58.56ms
step:1092/2330 train_time:63956ms step_avg:58.57ms
step:1093/2330 train_time:64012ms step_avg:58.57ms
step:1094/2330 train_time:64075ms step_avg:58.57ms
step:1095/2330 train_time:64131ms step_avg:58.57ms
step:1096/2330 train_time:64195ms step_avg:58.57ms
step:1097/2330 train_time:64251ms step_avg:58.57ms
step:1098/2330 train_time:64313ms step_avg:58.57ms
step:1099/2330 train_time:64370ms step_avg:58.57ms
step:1100/2330 train_time:64431ms step_avg:58.57ms
step:1101/2330 train_time:64488ms step_avg:58.57ms
step:1102/2330 train_time:64549ms step_avg:58.57ms
step:1103/2330 train_time:64606ms step_avg:58.57ms
step:1104/2330 train_time:64667ms step_avg:58.57ms
step:1105/2330 train_time:64723ms step_avg:58.57ms
step:1106/2330 train_time:64784ms step_avg:58.58ms
step:1107/2330 train_time:64840ms step_avg:58.57ms
step:1108/2330 train_time:64902ms step_avg:58.58ms
step:1109/2330 train_time:64958ms step_avg:58.57ms
step:1110/2330 train_time:65020ms step_avg:58.58ms
step:1111/2330 train_time:65076ms step_avg:58.57ms
step:1112/2330 train_time:65138ms step_avg:58.58ms
step:1113/2330 train_time:65194ms step_avg:58.58ms
step:1114/2330 train_time:65256ms step_avg:58.58ms
step:1115/2330 train_time:65312ms step_avg:58.58ms
step:1116/2330 train_time:65374ms step_avg:58.58ms
step:1117/2330 train_time:65431ms step_avg:58.58ms
step:1118/2330 train_time:65493ms step_avg:58.58ms
step:1119/2330 train_time:65550ms step_avg:58.58ms
step:1120/2330 train_time:65612ms step_avg:58.58ms
step:1121/2330 train_time:65668ms step_avg:58.58ms
step:1122/2330 train_time:65730ms step_avg:58.58ms
step:1123/2330 train_time:65787ms step_avg:58.58ms
step:1124/2330 train_time:65849ms step_avg:58.58ms
step:1125/2330 train_time:65907ms step_avg:58.58ms
step:1126/2330 train_time:65967ms step_avg:58.59ms
step:1127/2330 train_time:66024ms step_avg:58.58ms
step:1128/2330 train_time:66085ms step_avg:58.59ms
step:1129/2330 train_time:66141ms step_avg:58.58ms
step:1130/2330 train_time:66202ms step_avg:58.59ms
step:1131/2330 train_time:66258ms step_avg:58.58ms
step:1132/2330 train_time:66321ms step_avg:58.59ms
step:1133/2330 train_time:66377ms step_avg:58.59ms
step:1134/2330 train_time:66439ms step_avg:58.59ms
step:1135/2330 train_time:66495ms step_avg:58.59ms
step:1136/2330 train_time:66557ms step_avg:58.59ms
step:1137/2330 train_time:66613ms step_avg:58.59ms
step:1138/2330 train_time:66676ms step_avg:58.59ms
step:1139/2330 train_time:66732ms step_avg:58.59ms
step:1140/2330 train_time:66794ms step_avg:58.59ms
step:1141/2330 train_time:66851ms step_avg:58.59ms
step:1142/2330 train_time:66914ms step_avg:58.59ms
step:1143/2330 train_time:66970ms step_avg:58.59ms
step:1144/2330 train_time:67033ms step_avg:58.60ms
step:1145/2330 train_time:67090ms step_avg:58.59ms
step:1146/2330 train_time:67150ms step_avg:58.60ms
step:1147/2330 train_time:67208ms step_avg:58.59ms
step:1148/2330 train_time:67269ms step_avg:58.60ms
step:1149/2330 train_time:67326ms step_avg:58.60ms
step:1150/2330 train_time:67387ms step_avg:58.60ms
step:1151/2330 train_time:67445ms step_avg:58.60ms
step:1152/2330 train_time:67505ms step_avg:58.60ms
step:1153/2330 train_time:67561ms step_avg:58.60ms
step:1154/2330 train_time:67622ms step_avg:58.60ms
step:1155/2330 train_time:67679ms step_avg:58.60ms
step:1156/2330 train_time:67741ms step_avg:58.60ms
step:1157/2330 train_time:67797ms step_avg:58.60ms
step:1158/2330 train_time:67859ms step_avg:58.60ms
step:1159/2330 train_time:67915ms step_avg:58.60ms
step:1160/2330 train_time:67978ms step_avg:58.60ms
step:1161/2330 train_time:68034ms step_avg:58.60ms
step:1162/2330 train_time:68097ms step_avg:58.60ms
step:1163/2330 train_time:68152ms step_avg:58.60ms
step:1164/2330 train_time:68215ms step_avg:58.60ms
step:1165/2330 train_time:68271ms step_avg:58.60ms
step:1166/2330 train_time:68334ms step_avg:58.61ms
step:1167/2330 train_time:68390ms step_avg:58.60ms
step:1168/2330 train_time:68453ms step_avg:58.61ms
step:1169/2330 train_time:68510ms step_avg:58.61ms
step:1170/2330 train_time:68572ms step_avg:58.61ms
step:1171/2330 train_time:68628ms step_avg:58.61ms
step:1172/2330 train_time:68690ms step_avg:58.61ms
step:1173/2330 train_time:68747ms step_avg:58.61ms
step:1174/2330 train_time:68807ms step_avg:58.61ms
step:1175/2330 train_time:68864ms step_avg:58.61ms
step:1176/2330 train_time:68925ms step_avg:58.61ms
step:1177/2330 train_time:68982ms step_avg:58.61ms
step:1178/2330 train_time:69042ms step_avg:58.61ms
step:1179/2330 train_time:69098ms step_avg:58.61ms
step:1180/2330 train_time:69161ms step_avg:58.61ms
step:1181/2330 train_time:69217ms step_avg:58.61ms
step:1182/2330 train_time:69279ms step_avg:58.61ms
step:1183/2330 train_time:69335ms step_avg:58.61ms
step:1184/2330 train_time:69398ms step_avg:58.61ms
step:1185/2330 train_time:69454ms step_avg:58.61ms
step:1186/2330 train_time:69516ms step_avg:58.61ms
step:1187/2330 train_time:69572ms step_avg:58.61ms
step:1188/2330 train_time:69635ms step_avg:58.62ms
step:1189/2330 train_time:69691ms step_avg:58.61ms
step:1190/2330 train_time:69754ms step_avg:58.62ms
step:1191/2330 train_time:69811ms step_avg:58.62ms
step:1192/2330 train_time:69873ms step_avg:58.62ms
step:1193/2330 train_time:69930ms step_avg:58.62ms
step:1194/2330 train_time:69992ms step_avg:58.62ms
step:1195/2330 train_time:70048ms step_avg:58.62ms
step:1196/2330 train_time:70110ms step_avg:58.62ms
step:1197/2330 train_time:70167ms step_avg:58.62ms
step:1198/2330 train_time:70228ms step_avg:58.62ms
step:1199/2330 train_time:70284ms step_avg:58.62ms
step:1200/2330 train_time:70345ms step_avg:58.62ms
step:1201/2330 train_time:70403ms step_avg:58.62ms
step:1202/2330 train_time:70463ms step_avg:58.62ms
step:1203/2330 train_time:70519ms step_avg:58.62ms
step:1204/2330 train_time:70580ms step_avg:58.62ms
step:1205/2330 train_time:70636ms step_avg:58.62ms
step:1206/2330 train_time:70698ms step_avg:58.62ms
step:1207/2330 train_time:70754ms step_avg:58.62ms
step:1208/2330 train_time:70817ms step_avg:58.62ms
step:1209/2330 train_time:70873ms step_avg:58.62ms
step:1210/2330 train_time:70936ms step_avg:58.62ms
step:1211/2330 train_time:70992ms step_avg:58.62ms
step:1212/2330 train_time:71055ms step_avg:58.63ms
step:1213/2330 train_time:71111ms step_avg:58.62ms
step:1214/2330 train_time:71173ms step_avg:58.63ms
step:1215/2330 train_time:71230ms step_avg:58.63ms
step:1216/2330 train_time:71292ms step_avg:58.63ms
step:1217/2330 train_time:71349ms step_avg:58.63ms
step:1218/2330 train_time:71410ms step_avg:58.63ms
step:1219/2330 train_time:71468ms step_avg:58.63ms
step:1220/2330 train_time:71528ms step_avg:58.63ms
step:1221/2330 train_time:71585ms step_avg:58.63ms
step:1222/2330 train_time:71645ms step_avg:58.63ms
step:1223/2330 train_time:71702ms step_avg:58.63ms
step:1224/2330 train_time:71764ms step_avg:58.63ms
step:1225/2330 train_time:71820ms step_avg:58.63ms
step:1226/2330 train_time:71881ms step_avg:58.63ms
step:1227/2330 train_time:71937ms step_avg:58.63ms
step:1228/2330 train_time:71999ms step_avg:58.63ms
step:1229/2330 train_time:72056ms step_avg:58.63ms
step:1230/2330 train_time:72118ms step_avg:58.63ms
step:1231/2330 train_time:72174ms step_avg:58.63ms
step:1232/2330 train_time:72237ms step_avg:58.63ms
step:1233/2330 train_time:72293ms step_avg:58.63ms
step:1234/2330 train_time:72356ms step_avg:58.64ms
step:1235/2330 train_time:72412ms step_avg:58.63ms
step:1236/2330 train_time:72474ms step_avg:58.64ms
step:1237/2330 train_time:72531ms step_avg:58.63ms
step:1238/2330 train_time:72594ms step_avg:58.64ms
step:1239/2330 train_time:72651ms step_avg:58.64ms
step:1240/2330 train_time:72713ms step_avg:58.64ms
step:1241/2330 train_time:72770ms step_avg:58.64ms
step:1242/2330 train_time:72831ms step_avg:58.64ms
step:1243/2330 train_time:72888ms step_avg:58.64ms
step:1244/2330 train_time:72949ms step_avg:58.64ms
step:1245/2330 train_time:73006ms step_avg:58.64ms
step:1246/2330 train_time:73068ms step_avg:58.64ms
step:1247/2330 train_time:73124ms step_avg:58.64ms
step:1248/2330 train_time:73185ms step_avg:58.64ms
step:1249/2330 train_time:73241ms step_avg:58.64ms
step:1250/2330 train_time:73302ms step_avg:58.64ms
step:1250/2330 val_loss:4.3175 train_time:73380ms step_avg:58.70ms
step:1251/2330 train_time:73400ms step_avg:58.67ms
step:1252/2330 train_time:73423ms step_avg:58.64ms
step:1253/2330 train_time:73481ms step_avg:58.64ms
step:1254/2330 train_time:73546ms step_avg:58.65ms
step:1255/2330 train_time:73602ms step_avg:58.65ms
step:1256/2330 train_time:73664ms step_avg:58.65ms
step:1257/2330 train_time:73721ms step_avg:58.65ms
step:1258/2330 train_time:73782ms step_avg:58.65ms
step:1259/2330 train_time:73838ms step_avg:58.65ms
step:1260/2330 train_time:73899ms step_avg:58.65ms
step:1261/2330 train_time:73955ms step_avg:58.65ms
step:1262/2330 train_time:74017ms step_avg:58.65ms
step:1263/2330 train_time:74073ms step_avg:58.65ms
step:1264/2330 train_time:74134ms step_avg:58.65ms
step:1265/2330 train_time:74190ms step_avg:58.65ms
step:1266/2330 train_time:74250ms step_avg:58.65ms
step:1267/2330 train_time:74306ms step_avg:58.65ms
step:1268/2330 train_time:74367ms step_avg:58.65ms
step:1269/2330 train_time:74424ms step_avg:58.65ms
step:1270/2330 train_time:74487ms step_avg:58.65ms
step:1271/2330 train_time:74545ms step_avg:58.65ms
step:1272/2330 train_time:74607ms step_avg:58.65ms
step:1273/2330 train_time:74664ms step_avg:58.65ms
step:1274/2330 train_time:74725ms step_avg:58.65ms
step:1275/2330 train_time:74782ms step_avg:58.65ms
step:1276/2330 train_time:74844ms step_avg:58.65ms
step:1277/2330 train_time:74900ms step_avg:58.65ms
step:1278/2330 train_time:74962ms step_avg:58.66ms
step:1279/2330 train_time:75018ms step_avg:58.65ms
step:1280/2330 train_time:75079ms step_avg:58.66ms
step:1281/2330 train_time:75135ms step_avg:58.65ms
step:1282/2330 train_time:75196ms step_avg:58.66ms
step:1283/2330 train_time:75253ms step_avg:58.65ms
step:1284/2330 train_time:75314ms step_avg:58.66ms
step:1285/2330 train_time:75370ms step_avg:58.65ms
step:1286/2330 train_time:75433ms step_avg:58.66ms
step:1287/2330 train_time:75489ms step_avg:58.66ms
step:1288/2330 train_time:75552ms step_avg:58.66ms
step:1289/2330 train_time:75608ms step_avg:58.66ms
step:1290/2330 train_time:75671ms step_avg:58.66ms
step:1291/2330 train_time:75728ms step_avg:58.66ms
step:1292/2330 train_time:75791ms step_avg:58.66ms
step:1293/2330 train_time:75848ms step_avg:58.66ms
step:1294/2330 train_time:75908ms step_avg:58.66ms
step:1295/2330 train_time:75965ms step_avg:58.66ms
step:1296/2330 train_time:76026ms step_avg:58.66ms
step:1297/2330 train_time:76083ms step_avg:58.66ms
step:1298/2330 train_time:76143ms step_avg:58.66ms
step:1299/2330 train_time:76199ms step_avg:58.66ms
step:1300/2330 train_time:76261ms step_avg:58.66ms
step:1301/2330 train_time:76318ms step_avg:58.66ms
step:1302/2330 train_time:76380ms step_avg:58.66ms
step:1303/2330 train_time:76435ms step_avg:58.66ms
step:1304/2330 train_time:76499ms step_avg:58.66ms
step:1305/2330 train_time:76555ms step_avg:58.66ms
step:1306/2330 train_time:76617ms step_avg:58.67ms
step:1307/2330 train_time:76673ms step_avg:58.66ms
step:1308/2330 train_time:76736ms step_avg:58.67ms
step:1309/2330 train_time:76792ms step_avg:58.66ms
step:1310/2330 train_time:76856ms step_avg:58.67ms
step:1311/2330 train_time:76912ms step_avg:58.67ms
step:1312/2330 train_time:76975ms step_avg:58.67ms
step:1313/2330 train_time:77032ms step_avg:58.67ms
step:1314/2330 train_time:77093ms step_avg:58.67ms
step:1315/2330 train_time:77149ms step_avg:58.67ms
step:1316/2330 train_time:77210ms step_avg:58.67ms
step:1317/2330 train_time:77267ms step_avg:58.67ms
step:1318/2330 train_time:77328ms step_avg:58.67ms
step:1319/2330 train_time:77385ms step_avg:58.67ms
step:1320/2330 train_time:77446ms step_avg:58.67ms
step:1321/2330 train_time:77502ms step_avg:58.67ms
step:1322/2330 train_time:77563ms step_avg:58.67ms
step:1323/2330 train_time:77619ms step_avg:58.67ms
step:1324/2330 train_time:77681ms step_avg:58.67ms
step:1325/2330 train_time:77737ms step_avg:58.67ms
step:1326/2330 train_time:77800ms step_avg:58.67ms
step:1327/2330 train_time:77856ms step_avg:58.67ms
step:1328/2330 train_time:77919ms step_avg:58.67ms
step:1329/2330 train_time:77976ms step_avg:58.67ms
step:1330/2330 train_time:78038ms step_avg:58.67ms
step:1331/2330 train_time:78094ms step_avg:58.67ms
step:1332/2330 train_time:78156ms step_avg:58.68ms
step:1333/2330 train_time:78213ms step_avg:58.67ms
step:1334/2330 train_time:78275ms step_avg:58.68ms
step:1335/2330 train_time:78331ms step_avg:58.68ms
step:1336/2330 train_time:78393ms step_avg:58.68ms
step:1337/2330 train_time:78450ms step_avg:58.68ms
step:1338/2330 train_time:78511ms step_avg:58.68ms
step:1339/2330 train_time:78568ms step_avg:58.68ms
step:1340/2330 train_time:78629ms step_avg:58.68ms
step:1341/2330 train_time:78685ms step_avg:58.68ms
step:1342/2330 train_time:78747ms step_avg:58.68ms
step:1343/2330 train_time:78803ms step_avg:58.68ms
step:1344/2330 train_time:78864ms step_avg:58.68ms
step:1345/2330 train_time:78920ms step_avg:58.68ms
step:1346/2330 train_time:78982ms step_avg:58.68ms
step:1347/2330 train_time:79038ms step_avg:58.68ms
step:1348/2330 train_time:79099ms step_avg:58.68ms
step:1349/2330 train_time:79156ms step_avg:58.68ms
step:1350/2330 train_time:79218ms step_avg:58.68ms
step:1351/2330 train_time:79274ms step_avg:58.68ms
step:1352/2330 train_time:79336ms step_avg:58.68ms
step:1353/2330 train_time:79393ms step_avg:58.68ms
step:1354/2330 train_time:79454ms step_avg:58.68ms
step:1355/2330 train_time:79510ms step_avg:58.68ms
step:1356/2330 train_time:79573ms step_avg:58.68ms
step:1357/2330 train_time:79630ms step_avg:58.68ms
step:1358/2330 train_time:79691ms step_avg:58.68ms
step:1359/2330 train_time:79748ms step_avg:58.68ms
step:1360/2330 train_time:79810ms step_avg:58.68ms
step:1361/2330 train_time:79867ms step_avg:58.68ms
step:1362/2330 train_time:79928ms step_avg:58.68ms
step:1363/2330 train_time:79984ms step_avg:58.68ms
step:1364/2330 train_time:80046ms step_avg:58.68ms
step:1365/2330 train_time:80103ms step_avg:58.68ms
step:1366/2330 train_time:80163ms step_avg:58.68ms
step:1367/2330 train_time:80220ms step_avg:58.68ms
step:1368/2330 train_time:80281ms step_avg:58.68ms
step:1369/2330 train_time:80337ms step_avg:58.68ms
step:1370/2330 train_time:80400ms step_avg:58.69ms
step:1371/2330 train_time:80456ms step_avg:58.68ms
step:1372/2330 train_time:80519ms step_avg:58.69ms
step:1373/2330 train_time:80575ms step_avg:58.69ms
step:1374/2330 train_time:80637ms step_avg:58.69ms
step:1375/2330 train_time:80693ms step_avg:58.69ms
step:1376/2330 train_time:80755ms step_avg:58.69ms
step:1377/2330 train_time:80812ms step_avg:58.69ms
step:1378/2330 train_time:80876ms step_avg:58.69ms
step:1379/2330 train_time:80932ms step_avg:58.69ms
step:1380/2330 train_time:80995ms step_avg:58.69ms
step:1381/2330 train_time:81052ms step_avg:58.69ms
step:1382/2330 train_time:81113ms step_avg:58.69ms
step:1383/2330 train_time:81170ms step_avg:58.69ms
step:1384/2330 train_time:81231ms step_avg:58.69ms
step:1385/2330 train_time:81288ms step_avg:58.69ms
step:1386/2330 train_time:81349ms step_avg:58.69ms
step:1387/2330 train_time:81406ms step_avg:58.69ms
step:1388/2330 train_time:81466ms step_avg:58.69ms
step:1389/2330 train_time:81523ms step_avg:58.69ms
step:1390/2330 train_time:81584ms step_avg:58.69ms
step:1391/2330 train_time:81640ms step_avg:58.69ms
step:1392/2330 train_time:81702ms step_avg:58.69ms
step:1393/2330 train_time:81758ms step_avg:58.69ms
step:1394/2330 train_time:81821ms step_avg:58.69ms
step:1395/2330 train_time:81877ms step_avg:58.69ms
step:1396/2330 train_time:81939ms step_avg:58.70ms
step:1397/2330 train_time:81995ms step_avg:58.69ms
step:1398/2330 train_time:82057ms step_avg:58.70ms
step:1399/2330 train_time:82113ms step_avg:58.69ms
step:1400/2330 train_time:82176ms step_avg:58.70ms
step:1401/2330 train_time:82232ms step_avg:58.70ms
step:1402/2330 train_time:82294ms step_avg:58.70ms
step:1403/2330 train_time:82351ms step_avg:58.70ms
step:1404/2330 train_time:82413ms step_avg:58.70ms
step:1405/2330 train_time:82470ms step_avg:58.70ms
step:1406/2330 train_time:82532ms step_avg:58.70ms
step:1407/2330 train_time:82589ms step_avg:58.70ms
step:1408/2330 train_time:82650ms step_avg:58.70ms
step:1409/2330 train_time:82706ms step_avg:58.70ms
step:1410/2330 train_time:82767ms step_avg:58.70ms
step:1411/2330 train_time:82825ms step_avg:58.70ms
step:1412/2330 train_time:82886ms step_avg:58.70ms
step:1413/2330 train_time:82943ms step_avg:58.70ms
step:1414/2330 train_time:83004ms step_avg:58.70ms
step:1415/2330 train_time:83060ms step_avg:58.70ms
step:1416/2330 train_time:83122ms step_avg:58.70ms
step:1417/2330 train_time:83178ms step_avg:58.70ms
step:1418/2330 train_time:83240ms step_avg:58.70ms
step:1419/2330 train_time:83296ms step_avg:58.70ms
step:1420/2330 train_time:83359ms step_avg:58.70ms
step:1421/2330 train_time:83415ms step_avg:58.70ms
step:1422/2330 train_time:83478ms step_avg:58.70ms
step:1423/2330 train_time:83534ms step_avg:58.70ms
step:1424/2330 train_time:83596ms step_avg:58.71ms
step:1425/2330 train_time:83652ms step_avg:58.70ms
step:1426/2330 train_time:83715ms step_avg:58.71ms
step:1427/2330 train_time:83771ms step_avg:58.70ms
step:1428/2330 train_time:83835ms step_avg:58.71ms
step:1429/2330 train_time:83892ms step_avg:58.71ms
step:1430/2330 train_time:83954ms step_avg:58.71ms
step:1431/2330 train_time:84011ms step_avg:58.71ms
step:1432/2330 train_time:84073ms step_avg:58.71ms
step:1433/2330 train_time:84130ms step_avg:58.71ms
step:1434/2330 train_time:84191ms step_avg:58.71ms
step:1435/2330 train_time:84248ms step_avg:58.71ms
step:1436/2330 train_time:84309ms step_avg:58.71ms
step:1437/2330 train_time:84366ms step_avg:58.71ms
step:1438/2330 train_time:84426ms step_avg:58.71ms
step:1439/2330 train_time:84483ms step_avg:58.71ms
step:1440/2330 train_time:84544ms step_avg:58.71ms
step:1441/2330 train_time:84600ms step_avg:58.71ms
step:1442/2330 train_time:84661ms step_avg:58.71ms
step:1443/2330 train_time:84717ms step_avg:58.71ms
step:1444/2330 train_time:84780ms step_avg:58.71ms
step:1445/2330 train_time:84836ms step_avg:58.71ms
step:1446/2330 train_time:84899ms step_avg:58.71ms
step:1447/2330 train_time:84955ms step_avg:58.71ms
step:1448/2330 train_time:85017ms step_avg:58.71ms
step:1449/2330 train_time:85073ms step_avg:58.71ms
step:1450/2330 train_time:85137ms step_avg:58.72ms
step:1451/2330 train_time:85193ms step_avg:58.71ms
step:1452/2330 train_time:85256ms step_avg:58.72ms
step:1453/2330 train_time:85312ms step_avg:58.71ms
step:1454/2330 train_time:85376ms step_avg:58.72ms
step:1455/2330 train_time:85433ms step_avg:58.72ms
step:1456/2330 train_time:85494ms step_avg:58.72ms
step:1457/2330 train_time:85551ms step_avg:58.72ms
step:1458/2330 train_time:85612ms step_avg:58.72ms
step:1459/2330 train_time:85668ms step_avg:58.72ms
step:1460/2330 train_time:85730ms step_avg:58.72ms
step:1461/2330 train_time:85787ms step_avg:58.72ms
step:1462/2330 train_time:85847ms step_avg:58.72ms
step:1463/2330 train_time:85904ms step_avg:58.72ms
step:1464/2330 train_time:85965ms step_avg:58.72ms
step:1465/2330 train_time:86021ms step_avg:58.72ms
step:1466/2330 train_time:86083ms step_avg:58.72ms
step:1467/2330 train_time:86139ms step_avg:58.72ms
step:1468/2330 train_time:86200ms step_avg:58.72ms
step:1469/2330 train_time:86257ms step_avg:58.72ms
step:1470/2330 train_time:86319ms step_avg:58.72ms
step:1471/2330 train_time:86375ms step_avg:58.72ms
step:1472/2330 train_time:86438ms step_avg:58.72ms
step:1473/2330 train_time:86494ms step_avg:58.72ms
step:1474/2330 train_time:86557ms step_avg:58.72ms
step:1475/2330 train_time:86613ms step_avg:58.72ms
step:1476/2330 train_time:86675ms step_avg:58.72ms
step:1477/2330 train_time:86732ms step_avg:58.72ms
step:1478/2330 train_time:86794ms step_avg:58.72ms
step:1479/2330 train_time:86851ms step_avg:58.72ms
step:1480/2330 train_time:86913ms step_avg:58.73ms
step:1481/2330 train_time:86970ms step_avg:58.72ms
step:1482/2330 train_time:87032ms step_avg:58.73ms
step:1483/2330 train_time:87089ms step_avg:58.72ms
step:1484/2330 train_time:87149ms step_avg:58.73ms
step:1485/2330 train_time:87207ms step_avg:58.73ms
step:1486/2330 train_time:87267ms step_avg:58.73ms
step:1487/2330 train_time:87324ms step_avg:58.72ms
step:1488/2330 train_time:87384ms step_avg:58.73ms
step:1489/2330 train_time:87441ms step_avg:58.72ms
step:1490/2330 train_time:87502ms step_avg:58.73ms
step:1491/2330 train_time:87558ms step_avg:58.72ms
step:1492/2330 train_time:87620ms step_avg:58.73ms
step:1493/2330 train_time:87676ms step_avg:58.72ms
step:1494/2330 train_time:87739ms step_avg:58.73ms
step:1495/2330 train_time:87795ms step_avg:58.73ms
step:1496/2330 train_time:87859ms step_avg:58.73ms
step:1497/2330 train_time:87915ms step_avg:58.73ms
step:1498/2330 train_time:87977ms step_avg:58.73ms
step:1499/2330 train_time:88033ms step_avg:58.73ms
step:1500/2330 train_time:88097ms step_avg:58.73ms
step:1500/2330 val_loss:4.2174 train_time:88175ms step_avg:58.78ms
step:1501/2330 train_time:88195ms step_avg:58.76ms
step:1502/2330 train_time:88217ms step_avg:58.73ms
step:1503/2330 train_time:88277ms step_avg:58.73ms
step:1504/2330 train_time:88340ms step_avg:58.74ms
step:1505/2330 train_time:88398ms step_avg:58.74ms
step:1506/2330 train_time:88459ms step_avg:58.74ms
step:1507/2330 train_time:88516ms step_avg:58.74ms
step:1508/2330 train_time:88577ms step_avg:58.74ms
step:1509/2330 train_time:88633ms step_avg:58.74ms
step:1510/2330 train_time:88693ms step_avg:58.74ms
step:1511/2330 train_time:88749ms step_avg:58.74ms
step:1512/2330 train_time:88811ms step_avg:58.74ms
step:1513/2330 train_time:88867ms step_avg:58.74ms
step:1514/2330 train_time:88929ms step_avg:58.74ms
step:1515/2330 train_time:88985ms step_avg:58.74ms
step:1516/2330 train_time:89046ms step_avg:58.74ms
step:1517/2330 train_time:89102ms step_avg:58.74ms
step:1518/2330 train_time:89163ms step_avg:58.74ms
step:1519/2330 train_time:89220ms step_avg:58.74ms
step:1520/2330 train_time:89283ms step_avg:58.74ms
step:1521/2330 train_time:89340ms step_avg:58.74ms
step:1522/2330 train_time:89402ms step_avg:58.74ms
step:1523/2330 train_time:89459ms step_avg:58.74ms
step:1524/2330 train_time:89521ms step_avg:58.74ms
step:1525/2330 train_time:89578ms step_avg:58.74ms
step:1526/2330 train_time:89638ms step_avg:58.74ms
step:1527/2330 train_time:89694ms step_avg:58.74ms
step:1528/2330 train_time:89756ms step_avg:58.74ms
step:1529/2330 train_time:89813ms step_avg:58.74ms
step:1530/2330 train_time:89874ms step_avg:58.74ms
step:1531/2330 train_time:89930ms step_avg:58.74ms
step:1532/2330 train_time:89992ms step_avg:58.74ms
step:1533/2330 train_time:90049ms step_avg:58.74ms
step:1534/2330 train_time:90111ms step_avg:58.74ms
step:1535/2330 train_time:90167ms step_avg:58.74ms
step:1536/2330 train_time:90230ms step_avg:58.74ms
step:1537/2330 train_time:90286ms step_avg:58.74ms
step:1538/2330 train_time:90351ms step_avg:58.75ms
step:1539/2330 train_time:90407ms step_avg:58.74ms
step:1540/2330 train_time:90472ms step_avg:58.75ms
step:1541/2330 train_time:90528ms step_avg:58.75ms
step:1542/2330 train_time:90594ms step_avg:58.75ms
step:1543/2330 train_time:90650ms step_avg:58.75ms
step:1544/2330 train_time:90712ms step_avg:58.75ms
step:1545/2330 train_time:90769ms step_avg:58.75ms
step:1546/2330 train_time:90831ms step_avg:58.75ms
step:1547/2330 train_time:90888ms step_avg:58.75ms
step:1548/2330 train_time:90949ms step_avg:58.75ms
step:1549/2330 train_time:91006ms step_avg:58.75ms
step:1550/2330 train_time:91068ms step_avg:58.75ms
step:1551/2330 train_time:91126ms step_avg:58.75ms
step:1552/2330 train_time:91188ms step_avg:58.76ms
step:1553/2330 train_time:91244ms step_avg:58.75ms
step:1554/2330 train_time:91308ms step_avg:58.76ms
step:1555/2330 train_time:91365ms step_avg:58.76ms
step:1556/2330 train_time:91430ms step_avg:58.76ms
step:1557/2330 train_time:91487ms step_avg:58.76ms
step:1558/2330 train_time:91550ms step_avg:58.76ms
step:1559/2330 train_time:91607ms step_avg:58.76ms
step:1560/2330 train_time:91670ms step_avg:58.76ms
step:1561/2330 train_time:91727ms step_avg:58.76ms
step:1562/2330 train_time:91789ms step_avg:58.76ms
step:1563/2330 train_time:91846ms step_avg:58.76ms
step:1564/2330 train_time:91908ms step_avg:58.76ms
step:1565/2330 train_time:91964ms step_avg:58.76ms
step:1566/2330 train_time:92028ms step_avg:58.77ms
step:1567/2330 train_time:92084ms step_avg:58.76ms
step:1568/2330 train_time:92146ms step_avg:58.77ms
step:1569/2330 train_time:92203ms step_avg:58.77ms
step:1570/2330 train_time:92266ms step_avg:58.77ms
step:1571/2330 train_time:92323ms step_avg:58.77ms
step:1572/2330 train_time:92387ms step_avg:58.77ms
step:1573/2330 train_time:92445ms step_avg:58.77ms
step:1574/2330 train_time:92506ms step_avg:58.77ms
step:1575/2330 train_time:92563ms step_avg:58.77ms
step:1576/2330 train_time:92625ms step_avg:58.77ms
step:1577/2330 train_time:92683ms step_avg:58.77ms
step:1578/2330 train_time:92745ms step_avg:58.77ms
step:1579/2330 train_time:92803ms step_avg:58.77ms
step:1580/2330 train_time:92863ms step_avg:58.77ms
step:1581/2330 train_time:92921ms step_avg:58.77ms
step:1582/2330 train_time:92982ms step_avg:58.77ms
step:1583/2330 train_time:93039ms step_avg:58.77ms
step:1584/2330 train_time:93100ms step_avg:58.78ms
step:1585/2330 train_time:93157ms step_avg:58.77ms
step:1586/2330 train_time:93218ms step_avg:58.78ms
step:1587/2330 train_time:93274ms step_avg:58.77ms
step:1588/2330 train_time:93337ms step_avg:58.78ms
step:1589/2330 train_time:93393ms step_avg:58.77ms
step:1590/2330 train_time:93456ms step_avg:58.78ms
step:1591/2330 train_time:93513ms step_avg:58.78ms
step:1592/2330 train_time:93575ms step_avg:58.78ms
step:1593/2330 train_time:93632ms step_avg:58.78ms
step:1594/2330 train_time:93694ms step_avg:58.78ms
step:1595/2330 train_time:93751ms step_avg:58.78ms
step:1596/2330 train_time:93814ms step_avg:58.78ms
step:1597/2330 train_time:93871ms step_avg:58.78ms
step:1598/2330 train_time:93933ms step_avg:58.78ms
step:1599/2330 train_time:93989ms step_avg:58.78ms
step:1600/2330 train_time:94053ms step_avg:58.78ms
step:1601/2330 train_time:94110ms step_avg:58.78ms
step:1602/2330 train_time:94172ms step_avg:58.78ms
step:1603/2330 train_time:94229ms step_avg:58.78ms
step:1604/2330 train_time:94291ms step_avg:58.78ms
step:1605/2330 train_time:94347ms step_avg:58.78ms
step:1606/2330 train_time:94410ms step_avg:58.79ms
step:1607/2330 train_time:94467ms step_avg:58.78ms
step:1608/2330 train_time:94530ms step_avg:58.79ms
step:1609/2330 train_time:94587ms step_avg:58.79ms
step:1610/2330 train_time:94650ms step_avg:58.79ms
step:1611/2330 train_time:94706ms step_avg:58.79ms
step:1612/2330 train_time:94770ms step_avg:58.79ms
step:1613/2330 train_time:94828ms step_avg:58.79ms
step:1614/2330 train_time:94890ms step_avg:58.79ms
step:1615/2330 train_time:94947ms step_avg:58.79ms
step:1616/2330 train_time:95009ms step_avg:58.79ms
step:1617/2330 train_time:95066ms step_avg:58.79ms
step:1618/2330 train_time:95129ms step_avg:58.79ms
step:1619/2330 train_time:95186ms step_avg:58.79ms
step:1620/2330 train_time:95248ms step_avg:58.80ms
step:1621/2330 train_time:95305ms step_avg:58.79ms
step:1622/2330 train_time:95368ms step_avg:58.80ms
step:1623/2330 train_time:95425ms step_avg:58.80ms
step:1624/2330 train_time:95488ms step_avg:58.80ms
step:1625/2330 train_time:95544ms step_avg:58.80ms
step:1626/2330 train_time:95607ms step_avg:58.80ms
step:1627/2330 train_time:95664ms step_avg:58.80ms
step:1628/2330 train_time:95727ms step_avg:58.80ms
step:1629/2330 train_time:95784ms step_avg:58.80ms
step:1630/2330 train_time:95847ms step_avg:58.80ms
step:1631/2330 train_time:95903ms step_avg:58.80ms
step:1632/2330 train_time:95967ms step_avg:58.80ms
step:1633/2330 train_time:96025ms step_avg:58.80ms
step:1634/2330 train_time:96086ms step_avg:58.80ms
step:1635/2330 train_time:96143ms step_avg:58.80ms
step:1636/2330 train_time:96206ms step_avg:58.81ms
step:1637/2330 train_time:96263ms step_avg:58.80ms
step:1638/2330 train_time:96324ms step_avg:58.81ms
step:1639/2330 train_time:96381ms step_avg:58.81ms
step:1640/2330 train_time:96443ms step_avg:58.81ms
step:1641/2330 train_time:96501ms step_avg:58.81ms
step:1642/2330 train_time:96562ms step_avg:58.81ms
step:1643/2330 train_time:96619ms step_avg:58.81ms
step:1644/2330 train_time:96681ms step_avg:58.81ms
step:1645/2330 train_time:96738ms step_avg:58.81ms
step:1646/2330 train_time:96799ms step_avg:58.81ms
step:1647/2330 train_time:96855ms step_avg:58.81ms
step:1648/2330 train_time:96917ms step_avg:58.81ms
step:1649/2330 train_time:96975ms step_avg:58.81ms
step:1650/2330 train_time:97036ms step_avg:58.81ms
step:1651/2330 train_time:97092ms step_avg:58.81ms
step:1652/2330 train_time:97155ms step_avg:58.81ms
step:1653/2330 train_time:97212ms step_avg:58.81ms
step:1654/2330 train_time:97275ms step_avg:58.81ms
step:1655/2330 train_time:97331ms step_avg:58.81ms
step:1656/2330 train_time:97394ms step_avg:58.81ms
step:1657/2330 train_time:97450ms step_avg:58.81ms
step:1658/2330 train_time:97513ms step_avg:58.81ms
step:1659/2330 train_time:97570ms step_avg:58.81ms
step:1660/2330 train_time:97634ms step_avg:58.82ms
step:1661/2330 train_time:97690ms step_avg:58.81ms
step:1662/2330 train_time:97754ms step_avg:58.82ms
step:1663/2330 train_time:97810ms step_avg:58.82ms
step:1664/2330 train_time:97873ms step_avg:58.82ms
step:1665/2330 train_time:97929ms step_avg:58.82ms
step:1666/2330 train_time:97992ms step_avg:58.82ms
step:1667/2330 train_time:98049ms step_avg:58.82ms
step:1668/2330 train_time:98112ms step_avg:58.82ms
step:1669/2330 train_time:98168ms step_avg:58.82ms
step:1670/2330 train_time:98231ms step_avg:58.82ms
step:1671/2330 train_time:98288ms step_avg:58.82ms
step:1672/2330 train_time:98350ms step_avg:58.82ms
step:1673/2330 train_time:98407ms step_avg:58.82ms
step:1674/2330 train_time:98469ms step_avg:58.82ms
step:1675/2330 train_time:98526ms step_avg:58.82ms
step:1676/2330 train_time:98589ms step_avg:58.82ms
step:1677/2330 train_time:98645ms step_avg:58.82ms
step:1678/2330 train_time:98709ms step_avg:58.83ms
step:1679/2330 train_time:98767ms step_avg:58.82ms
step:1680/2330 train_time:98829ms step_avg:58.83ms
step:1681/2330 train_time:98886ms step_avg:58.83ms
step:1682/2330 train_time:98948ms step_avg:58.83ms
step:1683/2330 train_time:99005ms step_avg:58.83ms
step:1684/2330 train_time:99067ms step_avg:58.83ms
step:1685/2330 train_time:99125ms step_avg:58.83ms
step:1686/2330 train_time:99187ms step_avg:58.83ms
step:1687/2330 train_time:99244ms step_avg:58.83ms
step:1688/2330 train_time:99307ms step_avg:58.83ms
step:1689/2330 train_time:99365ms step_avg:58.83ms
step:1690/2330 train_time:99426ms step_avg:58.83ms
step:1691/2330 train_time:99483ms step_avg:58.83ms
step:1692/2330 train_time:99545ms step_avg:58.83ms
step:1693/2330 train_time:99602ms step_avg:58.83ms
step:1694/2330 train_time:99664ms step_avg:58.83ms
step:1695/2330 train_time:99722ms step_avg:58.83ms
step:1696/2330 train_time:99783ms step_avg:58.83ms
step:1697/2330 train_time:99841ms step_avg:58.83ms
step:1698/2330 train_time:99902ms step_avg:58.83ms
step:1699/2330 train_time:99959ms step_avg:58.83ms
step:1700/2330 train_time:100020ms step_avg:58.84ms
step:1701/2330 train_time:100077ms step_avg:58.83ms
step:1702/2330 train_time:100139ms step_avg:58.84ms
step:1703/2330 train_time:100196ms step_avg:58.84ms
step:1704/2330 train_time:100257ms step_avg:58.84ms
step:1705/2330 train_time:100314ms step_avg:58.84ms
step:1706/2330 train_time:100376ms step_avg:58.84ms
step:1707/2330 train_time:100432ms step_avg:58.84ms
step:1708/2330 train_time:100495ms step_avg:58.84ms
step:1709/2330 train_time:100551ms step_avg:58.84ms
step:1710/2330 train_time:100614ms step_avg:58.84ms
step:1711/2330 train_time:100671ms step_avg:58.84ms
step:1712/2330 train_time:100734ms step_avg:58.84ms
step:1713/2330 train_time:100791ms step_avg:58.84ms
step:1714/2330 train_time:100853ms step_avg:58.84ms
step:1715/2330 train_time:100910ms step_avg:58.84ms
step:1716/2330 train_time:100973ms step_avg:58.84ms
step:1717/2330 train_time:101029ms step_avg:58.84ms
step:1718/2330 train_time:101092ms step_avg:58.84ms
step:1719/2330 train_time:101148ms step_avg:58.84ms
step:1720/2330 train_time:101210ms step_avg:58.84ms
step:1721/2330 train_time:101267ms step_avg:58.84ms
step:1722/2330 train_time:101329ms step_avg:58.84ms
step:1723/2330 train_time:101386ms step_avg:58.84ms
step:1724/2330 train_time:101448ms step_avg:58.84ms
step:1725/2330 train_time:101505ms step_avg:58.84ms
step:1726/2330 train_time:101569ms step_avg:58.85ms
step:1727/2330 train_time:101626ms step_avg:58.85ms
step:1728/2330 train_time:101688ms step_avg:58.85ms
step:1729/2330 train_time:101745ms step_avg:58.85ms
step:1730/2330 train_time:101807ms step_avg:58.85ms
step:1731/2330 train_time:101864ms step_avg:58.85ms
step:1732/2330 train_time:101928ms step_avg:58.85ms
step:1733/2330 train_time:101984ms step_avg:58.85ms
step:1734/2330 train_time:102048ms step_avg:58.85ms
step:1735/2330 train_time:102105ms step_avg:58.85ms
step:1736/2330 train_time:102167ms step_avg:58.85ms
step:1737/2330 train_time:102224ms step_avg:58.85ms
step:1738/2330 train_time:102286ms step_avg:58.85ms
step:1739/2330 train_time:102343ms step_avg:58.85ms
step:1740/2330 train_time:102405ms step_avg:58.85ms
step:1741/2330 train_time:102462ms step_avg:58.85ms
step:1742/2330 train_time:102525ms step_avg:58.85ms
step:1743/2330 train_time:102583ms step_avg:58.85ms
step:1744/2330 train_time:102644ms step_avg:58.86ms
step:1745/2330 train_time:102702ms step_avg:58.85ms
step:1746/2330 train_time:102763ms step_avg:58.86ms
step:1747/2330 train_time:102820ms step_avg:58.86ms
step:1748/2330 train_time:102881ms step_avg:58.86ms
step:1749/2330 train_time:102939ms step_avg:58.86ms
step:1750/2330 train_time:103000ms step_avg:58.86ms
step:1750/2330 val_loss:4.1286 train_time:103078ms step_avg:58.90ms
step:1751/2330 train_time:103099ms step_avg:58.88ms
step:1752/2330 train_time:103122ms step_avg:58.86ms
step:1753/2330 train_time:103179ms step_avg:58.86ms
step:1754/2330 train_time:103245ms step_avg:58.86ms
step:1755/2330 train_time:103304ms step_avg:58.86ms
step:1756/2330 train_time:103365ms step_avg:58.86ms
step:1757/2330 train_time:103423ms step_avg:58.86ms
step:1758/2330 train_time:103483ms step_avg:58.86ms
step:1759/2330 train_time:103540ms step_avg:58.86ms
step:1760/2330 train_time:103601ms step_avg:58.86ms
step:1761/2330 train_time:103658ms step_avg:58.86ms
step:1762/2330 train_time:103719ms step_avg:58.86ms
step:1763/2330 train_time:103776ms step_avg:58.86ms
step:1764/2330 train_time:103837ms step_avg:58.86ms
step:1765/2330 train_time:103894ms step_avg:58.86ms
step:1766/2330 train_time:103955ms step_avg:58.86ms
step:1767/2330 train_time:104011ms step_avg:58.86ms
step:1768/2330 train_time:104073ms step_avg:58.87ms
step:1769/2330 train_time:104130ms step_avg:58.86ms
step:1770/2330 train_time:104194ms step_avg:58.87ms
step:1771/2330 train_time:104251ms step_avg:58.87ms
step:1772/2330 train_time:104314ms step_avg:58.87ms
step:1773/2330 train_time:104370ms step_avg:58.87ms
step:1774/2330 train_time:104432ms step_avg:58.87ms
step:1775/2330 train_time:104489ms step_avg:58.87ms
step:1776/2330 train_time:104552ms step_avg:58.87ms
step:1777/2330 train_time:104608ms step_avg:58.87ms
step:1778/2330 train_time:104671ms step_avg:58.87ms
step:1779/2330 train_time:104728ms step_avg:58.87ms
step:1780/2330 train_time:104789ms step_avg:58.87ms
step:1781/2330 train_time:104845ms step_avg:58.87ms
step:1782/2330 train_time:104908ms step_avg:58.87ms
step:1783/2330 train_time:104965ms step_avg:58.87ms
step:1784/2330 train_time:105028ms step_avg:58.87ms
step:1785/2330 train_time:105085ms step_avg:58.87ms
step:1786/2330 train_time:105148ms step_avg:58.87ms
step:1787/2330 train_time:105205ms step_avg:58.87ms
step:1788/2330 train_time:105267ms step_avg:58.87ms
step:1789/2330 train_time:105324ms step_avg:58.87ms
step:1790/2330 train_time:105387ms step_avg:58.88ms
step:1791/2330 train_time:105444ms step_avg:58.87ms
step:1792/2330 train_time:105506ms step_avg:58.88ms
step:1793/2330 train_time:105563ms step_avg:58.88ms
step:1794/2330 train_time:105625ms step_avg:58.88ms
step:1795/2330 train_time:105682ms step_avg:58.88ms
step:1796/2330 train_time:105744ms step_avg:58.88ms
step:1797/2330 train_time:105801ms step_avg:58.88ms
step:1798/2330 train_time:105863ms step_avg:58.88ms
step:1799/2330 train_time:105920ms step_avg:58.88ms
step:1800/2330 train_time:105982ms step_avg:58.88ms
step:1801/2330 train_time:106040ms step_avg:58.88ms
step:1802/2330 train_time:106102ms step_avg:58.88ms
step:1803/2330 train_time:106159ms step_avg:58.88ms
step:1804/2330 train_time:106222ms step_avg:58.88ms
step:1805/2330 train_time:106279ms step_avg:58.88ms
step:1806/2330 train_time:106343ms step_avg:58.88ms
step:1807/2330 train_time:106400ms step_avg:58.88ms
step:1808/2330 train_time:106462ms step_avg:58.88ms
step:1809/2330 train_time:106518ms step_avg:58.88ms
step:1810/2330 train_time:106580ms step_avg:58.88ms
step:1811/2330 train_time:106637ms step_avg:58.88ms
step:1812/2330 train_time:106699ms step_avg:58.88ms
step:1813/2330 train_time:106756ms step_avg:58.88ms
step:1814/2330 train_time:106817ms step_avg:58.88ms
step:1815/2330 train_time:106874ms step_avg:58.88ms
step:1816/2330 train_time:106935ms step_avg:58.89ms
step:1817/2330 train_time:106992ms step_avg:58.88ms
step:1818/2330 train_time:107054ms step_avg:58.89ms
step:1819/2330 train_time:107111ms step_avg:58.88ms
step:1820/2330 train_time:107173ms step_avg:58.89ms
step:1821/2330 train_time:107230ms step_avg:58.89ms
step:1822/2330 train_time:107292ms step_avg:58.89ms
step:1823/2330 train_time:107349ms step_avg:58.89ms
step:1824/2330 train_time:107411ms step_avg:58.89ms
step:1825/2330 train_time:107468ms step_avg:58.89ms
step:1826/2330 train_time:107530ms step_avg:58.89ms
step:1827/2330 train_time:107586ms step_avg:58.89ms
step:1828/2330 train_time:107649ms step_avg:58.89ms
step:1829/2330 train_time:107705ms step_avg:58.89ms
step:1830/2330 train_time:107767ms step_avg:58.89ms
step:1831/2330 train_time:107824ms step_avg:58.89ms
step:1832/2330 train_time:107887ms step_avg:58.89ms
step:1833/2330 train_time:107944ms step_avg:58.89ms
step:1834/2330 train_time:108007ms step_avg:58.89ms
step:1835/2330 train_time:108064ms step_avg:58.89ms
step:1836/2330 train_time:108126ms step_avg:58.89ms
step:1837/2330 train_time:108183ms step_avg:58.89ms
step:1838/2330 train_time:108244ms step_avg:58.89ms
step:1839/2330 train_time:108301ms step_avg:58.89ms
step:1840/2330 train_time:108363ms step_avg:58.89ms
step:1841/2330 train_time:108421ms step_avg:58.89ms
step:1842/2330 train_time:108483ms step_avg:58.89ms
step:1843/2330 train_time:108541ms step_avg:58.89ms
step:1844/2330 train_time:108602ms step_avg:58.90ms
step:1845/2330 train_time:108659ms step_avg:58.89ms
step:1846/2330 train_time:108721ms step_avg:58.90ms
step:1847/2330 train_time:108778ms step_avg:58.89ms
step:1848/2330 train_time:108839ms step_avg:58.90ms
step:1849/2330 train_time:108896ms step_avg:58.89ms
step:1850/2330 train_time:108958ms step_avg:58.90ms
step:1851/2330 train_time:109015ms step_avg:58.90ms
step:1852/2330 train_time:109076ms step_avg:58.90ms
step:1853/2330 train_time:109133ms step_avg:58.90ms
step:1854/2330 train_time:109195ms step_avg:58.90ms
step:1855/2330 train_time:109251ms step_avg:58.90ms
step:1856/2330 train_time:109314ms step_avg:58.90ms
step:1857/2330 train_time:109370ms step_avg:58.90ms
step:1858/2330 train_time:109433ms step_avg:58.90ms
step:1859/2330 train_time:109489ms step_avg:58.90ms
step:1860/2330 train_time:109552ms step_avg:58.90ms
step:1861/2330 train_time:109608ms step_avg:58.90ms
step:1862/2330 train_time:109671ms step_avg:58.90ms
step:1863/2330 train_time:109728ms step_avg:58.90ms
step:1864/2330 train_time:109790ms step_avg:58.90ms
step:1865/2330 train_time:109847ms step_avg:58.90ms
step:1866/2330 train_time:109910ms step_avg:58.90ms
step:1867/2330 train_time:109966ms step_avg:58.90ms
step:1868/2330 train_time:110030ms step_avg:58.90ms
step:1869/2330 train_time:110087ms step_avg:58.90ms
step:1870/2330 train_time:110149ms step_avg:58.90ms
step:1871/2330 train_time:110206ms step_avg:58.90ms
step:1872/2330 train_time:110269ms step_avg:58.90ms
step:1873/2330 train_time:110326ms step_avg:58.90ms
step:1874/2330 train_time:110388ms step_avg:58.90ms
step:1875/2330 train_time:110444ms step_avg:58.90ms
step:1876/2330 train_time:110508ms step_avg:58.91ms
step:1877/2330 train_time:110565ms step_avg:58.90ms
step:1878/2330 train_time:110628ms step_avg:58.91ms
step:1879/2330 train_time:110685ms step_avg:58.91ms
step:1880/2330 train_time:110748ms step_avg:58.91ms
step:1881/2330 train_time:110804ms step_avg:58.91ms
step:1882/2330 train_time:110867ms step_avg:58.91ms
step:1883/2330 train_time:110924ms step_avg:58.91ms
step:1884/2330 train_time:110987ms step_avg:58.91ms
step:1885/2330 train_time:111044ms step_avg:58.91ms
step:1886/2330 train_time:111106ms step_avg:58.91ms
step:1887/2330 train_time:111164ms step_avg:58.91ms
step:1888/2330 train_time:111226ms step_avg:58.91ms
step:1889/2330 train_time:111283ms step_avg:58.91ms
step:1890/2330 train_time:111345ms step_avg:58.91ms
step:1891/2330 train_time:111402ms step_avg:58.91ms
step:1892/2330 train_time:111465ms step_avg:58.91ms
step:1893/2330 train_time:111522ms step_avg:58.91ms
step:1894/2330 train_time:111585ms step_avg:58.91ms
step:1895/2330 train_time:111642ms step_avg:58.91ms
step:1896/2330 train_time:111705ms step_avg:58.92ms
step:1897/2330 train_time:111761ms step_avg:58.91ms
step:1898/2330 train_time:111824ms step_avg:58.92ms
step:1899/2330 train_time:111881ms step_avg:58.92ms
step:1900/2330 train_time:111943ms step_avg:58.92ms
step:1901/2330 train_time:112000ms step_avg:58.92ms
step:1902/2330 train_time:112063ms step_avg:58.92ms
step:1903/2330 train_time:112120ms step_avg:58.92ms
step:1904/2330 train_time:112181ms step_avg:58.92ms
step:1905/2330 train_time:112238ms step_avg:58.92ms
step:1906/2330 train_time:112300ms step_avg:58.92ms
step:1907/2330 train_time:112357ms step_avg:58.92ms
step:1908/2330 train_time:112419ms step_avg:58.92ms
step:1909/2330 train_time:112476ms step_avg:58.92ms
step:1910/2330 train_time:112537ms step_avg:58.92ms
step:1911/2330 train_time:112595ms step_avg:58.92ms
step:1912/2330 train_time:112656ms step_avg:58.92ms
step:1913/2330 train_time:112713ms step_avg:58.92ms
step:1914/2330 train_time:112776ms step_avg:58.92ms
step:1915/2330 train_time:112834ms step_avg:58.92ms
step:1916/2330 train_time:112895ms step_avg:58.92ms
step:1917/2330 train_time:112951ms step_avg:58.92ms
step:1918/2330 train_time:113014ms step_avg:58.92ms
step:1919/2330 train_time:113070ms step_avg:58.92ms
step:1920/2330 train_time:113132ms step_avg:58.92ms
step:1921/2330 train_time:113188ms step_avg:58.92ms
step:1922/2330 train_time:113252ms step_avg:58.92ms
step:1923/2330 train_time:113308ms step_avg:58.92ms
step:1924/2330 train_time:113371ms step_avg:58.92ms
step:1925/2330 train_time:113427ms step_avg:58.92ms
step:1926/2330 train_time:113491ms step_avg:58.93ms
step:1927/2330 train_time:113547ms step_avg:58.92ms
step:1928/2330 train_time:113610ms step_avg:58.93ms
step:1929/2330 train_time:113667ms step_avg:58.93ms
step:1930/2330 train_time:113731ms step_avg:58.93ms
step:1931/2330 train_time:113788ms step_avg:58.93ms
step:1932/2330 train_time:113850ms step_avg:58.93ms
step:1933/2330 train_time:113906ms step_avg:58.93ms
step:1934/2330 train_time:113970ms step_avg:58.93ms
step:1935/2330 train_time:114027ms step_avg:58.93ms
step:1936/2330 train_time:114090ms step_avg:58.93ms
step:1937/2330 train_time:114147ms step_avg:58.93ms
step:1938/2330 train_time:114208ms step_avg:58.93ms
step:1939/2330 train_time:114265ms step_avg:58.93ms
step:1940/2330 train_time:114328ms step_avg:58.93ms
step:1941/2330 train_time:114385ms step_avg:58.93ms
step:1942/2330 train_time:114448ms step_avg:58.93ms
step:1943/2330 train_time:114504ms step_avg:58.93ms
step:1944/2330 train_time:114567ms step_avg:58.93ms
step:1945/2330 train_time:114624ms step_avg:58.93ms
step:1946/2330 train_time:114688ms step_avg:58.94ms
step:1947/2330 train_time:114745ms step_avg:58.93ms
step:1948/2330 train_time:114807ms step_avg:58.94ms
step:1949/2330 train_time:114864ms step_avg:58.93ms
step:1950/2330 train_time:114927ms step_avg:58.94ms
step:1951/2330 train_time:114984ms step_avg:58.94ms
step:1952/2330 train_time:115047ms step_avg:58.94ms
step:1953/2330 train_time:115104ms step_avg:58.94ms
step:1954/2330 train_time:115165ms step_avg:58.94ms
step:1955/2330 train_time:115222ms step_avg:58.94ms
step:1956/2330 train_time:115286ms step_avg:58.94ms
step:1957/2330 train_time:115343ms step_avg:58.94ms
step:1958/2330 train_time:115405ms step_avg:58.94ms
step:1959/2330 train_time:115462ms step_avg:58.94ms
step:1960/2330 train_time:115525ms step_avg:58.94ms
step:1961/2330 train_time:115582ms step_avg:58.94ms
step:1962/2330 train_time:115644ms step_avg:58.94ms
step:1963/2330 train_time:115702ms step_avg:58.94ms
step:1964/2330 train_time:115762ms step_avg:58.94ms
step:1965/2330 train_time:115819ms step_avg:58.94ms
step:1966/2330 train_time:115881ms step_avg:58.94ms
step:1967/2330 train_time:115939ms step_avg:58.94ms
step:1968/2330 train_time:116002ms step_avg:58.94ms
step:1969/2330 train_time:116059ms step_avg:58.94ms
step:1970/2330 train_time:116120ms step_avg:58.94ms
step:1971/2330 train_time:116178ms step_avg:58.94ms
step:1972/2330 train_time:116239ms step_avg:58.94ms
step:1973/2330 train_time:116297ms step_avg:58.94ms
step:1974/2330 train_time:116358ms step_avg:58.95ms
step:1975/2330 train_time:116415ms step_avg:58.94ms
step:1976/2330 train_time:116476ms step_avg:58.95ms
step:1977/2330 train_time:116533ms step_avg:58.94ms
step:1978/2330 train_time:116595ms step_avg:58.95ms
step:1979/2330 train_time:116653ms step_avg:58.95ms
step:1980/2330 train_time:116714ms step_avg:58.95ms
step:1981/2330 train_time:116770ms step_avg:58.95ms
step:1982/2330 train_time:116833ms step_avg:58.95ms
step:1983/2330 train_time:116890ms step_avg:58.95ms
step:1984/2330 train_time:116951ms step_avg:58.95ms
step:1985/2330 train_time:117008ms step_avg:58.95ms
step:1986/2330 train_time:117071ms step_avg:58.95ms
step:1987/2330 train_time:117127ms step_avg:58.95ms
step:1988/2330 train_time:117190ms step_avg:58.95ms
step:1989/2330 train_time:117247ms step_avg:58.95ms
step:1990/2330 train_time:117310ms step_avg:58.95ms
step:1991/2330 train_time:117366ms step_avg:58.95ms
step:1992/2330 train_time:117430ms step_avg:58.95ms
step:1993/2330 train_time:117487ms step_avg:58.95ms
step:1994/2330 train_time:117550ms step_avg:58.95ms
step:1995/2330 train_time:117606ms step_avg:58.95ms
step:1996/2330 train_time:117670ms step_avg:58.95ms
step:1997/2330 train_time:117726ms step_avg:58.95ms
step:1998/2330 train_time:117789ms step_avg:58.95ms
step:1999/2330 train_time:117845ms step_avg:58.95ms
step:2000/2330 train_time:117908ms step_avg:58.95ms
step:2000/2330 val_loss:4.0694 train_time:117988ms step_avg:58.99ms
step:2001/2330 train_time:118008ms step_avg:58.97ms
step:2002/2330 train_time:118034ms step_avg:58.96ms
step:2003/2330 train_time:118093ms step_avg:58.96ms
step:2004/2330 train_time:118156ms step_avg:58.96ms
step:2005/2330 train_time:118214ms step_avg:58.96ms
step:2006/2330 train_time:118276ms step_avg:58.96ms
step:2007/2330 train_time:118332ms step_avg:58.96ms
step:2008/2330 train_time:118394ms step_avg:58.96ms
step:2009/2330 train_time:118450ms step_avg:58.96ms
step:2010/2330 train_time:118512ms step_avg:58.96ms
step:2011/2330 train_time:118569ms step_avg:58.96ms
step:2012/2330 train_time:118630ms step_avg:58.96ms
step:2013/2330 train_time:118686ms step_avg:58.96ms
step:2014/2330 train_time:118748ms step_avg:58.96ms
step:2015/2330 train_time:118805ms step_avg:58.96ms
step:2016/2330 train_time:118866ms step_avg:58.96ms
step:2017/2330 train_time:118923ms step_avg:58.96ms
step:2018/2330 train_time:118985ms step_avg:58.96ms
step:2019/2330 train_time:119042ms step_avg:58.96ms
step:2020/2330 train_time:119107ms step_avg:58.96ms
step:2021/2330 train_time:119165ms step_avg:58.96ms
step:2022/2330 train_time:119228ms step_avg:58.97ms
step:2023/2330 train_time:119285ms step_avg:58.96ms
step:2024/2330 train_time:119349ms step_avg:58.97ms
step:2025/2330 train_time:119407ms step_avg:58.97ms
step:2026/2330 train_time:119470ms step_avg:58.97ms
step:2027/2330 train_time:119526ms step_avg:58.97ms
step:2028/2330 train_time:119588ms step_avg:58.97ms
step:2029/2330 train_time:119644ms step_avg:58.97ms
step:2030/2330 train_time:119706ms step_avg:58.97ms
step:2031/2330 train_time:119762ms step_avg:58.97ms
step:2032/2330 train_time:119824ms step_avg:58.97ms
step:2033/2330 train_time:119881ms step_avg:58.97ms
step:2034/2330 train_time:119942ms step_avg:58.97ms
step:2035/2330 train_time:119999ms step_avg:58.97ms
step:2036/2330 train_time:120062ms step_avg:58.97ms
step:2037/2330 train_time:120119ms step_avg:58.97ms
step:2038/2330 train_time:120181ms step_avg:58.97ms
step:2039/2330 train_time:120239ms step_avg:58.97ms
step:2040/2330 train_time:120301ms step_avg:58.97ms
step:2041/2330 train_time:120359ms step_avg:58.97ms
step:2042/2330 train_time:120421ms step_avg:58.97ms
step:2043/2330 train_time:120479ms step_avg:58.97ms
step:2044/2330 train_time:120539ms step_avg:58.97ms
step:2045/2330 train_time:120597ms step_avg:58.97ms
step:2046/2330 train_time:120659ms step_avg:58.97ms
step:2047/2330 train_time:120716ms step_avg:58.97ms
step:2048/2330 train_time:120777ms step_avg:58.97ms
step:2049/2330 train_time:120834ms step_avg:58.97ms
step:2050/2330 train_time:120895ms step_avg:58.97ms
step:2051/2330 train_time:120952ms step_avg:58.97ms
step:2052/2330 train_time:121013ms step_avg:58.97ms
step:2053/2330 train_time:121070ms step_avg:58.97ms
step:2054/2330 train_time:121133ms step_avg:58.97ms
step:2055/2330 train_time:121189ms step_avg:58.97ms
step:2056/2330 train_time:121253ms step_avg:58.98ms
step:2057/2330 train_time:121310ms step_avg:58.97ms
step:2058/2330 train_time:121373ms step_avg:58.98ms
step:2059/2330 train_time:121429ms step_avg:58.97ms
step:2060/2330 train_time:121493ms step_avg:58.98ms
step:2061/2330 train_time:121549ms step_avg:58.98ms
step:2062/2330 train_time:121612ms step_avg:58.98ms
step:2063/2330 train_time:121669ms step_avg:58.98ms
step:2064/2330 train_time:121732ms step_avg:58.98ms
step:2065/2330 train_time:121788ms step_avg:58.98ms
step:2066/2330 train_time:121850ms step_avg:58.98ms
step:2067/2330 train_time:121907ms step_avg:58.98ms
step:2068/2330 train_time:121969ms step_avg:58.98ms
step:2069/2330 train_time:122026ms step_avg:58.98ms
step:2070/2330 train_time:122088ms step_avg:58.98ms
step:2071/2330 train_time:122145ms step_avg:58.98ms
step:2072/2330 train_time:122210ms step_avg:58.98ms
step:2073/2330 train_time:122267ms step_avg:58.98ms
step:2074/2330 train_time:122330ms step_avg:58.98ms
step:2075/2330 train_time:122387ms step_avg:58.98ms
step:2076/2330 train_time:122450ms step_avg:58.98ms
step:2077/2330 train_time:122508ms step_avg:58.98ms
step:2078/2330 train_time:122569ms step_avg:58.98ms
step:2079/2330 train_time:122626ms step_avg:58.98ms
step:2080/2330 train_time:122689ms step_avg:58.98ms
step:2081/2330 train_time:122745ms step_avg:58.98ms
step:2082/2330 train_time:122808ms step_avg:58.99ms
step:2083/2330 train_time:122865ms step_avg:58.98ms
step:2084/2330 train_time:122928ms step_avg:58.99ms
step:2085/2330 train_time:122984ms step_avg:58.99ms
step:2086/2330 train_time:123049ms step_avg:58.99ms
step:2087/2330 train_time:123105ms step_avg:58.99ms
step:2088/2330 train_time:123168ms step_avg:58.99ms
step:2089/2330 train_time:123225ms step_avg:58.99ms
step:2090/2330 train_time:123288ms step_avg:58.99ms
step:2091/2330 train_time:123345ms step_avg:58.99ms
step:2092/2330 train_time:123408ms step_avg:58.99ms
step:2093/2330 train_time:123466ms step_avg:58.99ms
step:2094/2330 train_time:123528ms step_avg:58.99ms
step:2095/2330 train_time:123585ms step_avg:58.99ms
step:2096/2330 train_time:123647ms step_avg:58.99ms
step:2097/2330 train_time:123704ms step_avg:58.99ms
step:2098/2330 train_time:123767ms step_avg:58.99ms
step:2099/2330 train_time:123824ms step_avg:58.99ms
step:2100/2330 train_time:123886ms step_avg:58.99ms
step:2101/2330 train_time:123943ms step_avg:58.99ms
step:2102/2330 train_time:124006ms step_avg:58.99ms
step:2103/2330 train_time:124063ms step_avg:58.99ms
step:2104/2330 train_time:124126ms step_avg:59.00ms
step:2105/2330 train_time:124184ms step_avg:58.99ms
step:2106/2330 train_time:124245ms step_avg:59.00ms
step:2107/2330 train_time:124303ms step_avg:59.00ms
step:2108/2330 train_time:124365ms step_avg:59.00ms
step:2109/2330 train_time:124423ms step_avg:59.00ms
step:2110/2330 train_time:124485ms step_avg:59.00ms
step:2111/2330 train_time:124543ms step_avg:59.00ms
step:2112/2330 train_time:124605ms step_avg:59.00ms
step:2113/2330 train_time:124662ms step_avg:59.00ms
step:2114/2330 train_time:124723ms step_avg:59.00ms
step:2115/2330 train_time:124781ms step_avg:59.00ms
step:2116/2330 train_time:124842ms step_avg:59.00ms
step:2117/2330 train_time:124899ms step_avg:59.00ms
step:2118/2330 train_time:124960ms step_avg:59.00ms
step:2119/2330 train_time:125018ms step_avg:59.00ms
step:2120/2330 train_time:125079ms step_avg:59.00ms
step:2121/2330 train_time:125136ms step_avg:59.00ms
step:2122/2330 train_time:125197ms step_avg:59.00ms
step:2123/2330 train_time:125254ms step_avg:59.00ms
step:2124/2330 train_time:125316ms step_avg:59.00ms
step:2125/2330 train_time:125373ms step_avg:59.00ms
step:2126/2330 train_time:125434ms step_avg:59.00ms
step:2127/2330 train_time:125491ms step_avg:59.00ms
step:2128/2330 train_time:125554ms step_avg:59.00ms
step:2129/2330 train_time:125611ms step_avg:59.00ms
step:2130/2330 train_time:125673ms step_avg:59.00ms
step:2131/2330 train_time:125729ms step_avg:59.00ms
step:2132/2330 train_time:125792ms step_avg:59.00ms
step:2133/2330 train_time:125848ms step_avg:59.00ms
step:2134/2330 train_time:125912ms step_avg:59.00ms
step:2135/2330 train_time:125968ms step_avg:59.00ms
step:2136/2330 train_time:126032ms step_avg:59.00ms
step:2137/2330 train_time:126088ms step_avg:59.00ms
step:2138/2330 train_time:126151ms step_avg:59.00ms
step:2139/2330 train_time:126208ms step_avg:59.00ms
step:2140/2330 train_time:126270ms step_avg:59.00ms
step:2141/2330 train_time:126326ms step_avg:59.00ms
step:2142/2330 train_time:126390ms step_avg:59.01ms
step:2143/2330 train_time:126446ms step_avg:59.00ms
step:2144/2330 train_time:126509ms step_avg:59.01ms
step:2145/2330 train_time:126566ms step_avg:59.00ms
step:2146/2330 train_time:126629ms step_avg:59.01ms
step:2147/2330 train_time:126686ms step_avg:59.01ms
step:2148/2330 train_time:126748ms step_avg:59.01ms
step:2149/2330 train_time:126806ms step_avg:59.01ms
step:2150/2330 train_time:126868ms step_avg:59.01ms
step:2151/2330 train_time:126925ms step_avg:59.01ms
step:2152/2330 train_time:126988ms step_avg:59.01ms
step:2153/2330 train_time:127045ms step_avg:59.01ms
step:2154/2330 train_time:127108ms step_avg:59.01ms
step:2155/2330 train_time:127164ms step_avg:59.01ms
step:2156/2330 train_time:127227ms step_avg:59.01ms
step:2157/2330 train_time:127284ms step_avg:59.01ms
step:2158/2330 train_time:127346ms step_avg:59.01ms
step:2159/2330 train_time:127403ms step_avg:59.01ms
step:2160/2330 train_time:127466ms step_avg:59.01ms
step:2161/2330 train_time:127524ms step_avg:59.01ms
step:2162/2330 train_time:127585ms step_avg:59.01ms
step:2163/2330 train_time:127643ms step_avg:59.01ms
step:2164/2330 train_time:127706ms step_avg:59.01ms
step:2165/2330 train_time:127763ms step_avg:59.01ms
step:2166/2330 train_time:127825ms step_avg:59.01ms
step:2167/2330 train_time:127882ms step_avg:59.01ms
step:2168/2330 train_time:127944ms step_avg:59.01ms
step:2169/2330 train_time:128002ms step_avg:59.01ms
step:2170/2330 train_time:128064ms step_avg:59.02ms
step:2171/2330 train_time:128122ms step_avg:59.02ms
step:2172/2330 train_time:128184ms step_avg:59.02ms
step:2173/2330 train_time:128242ms step_avg:59.02ms
step:2174/2330 train_time:128303ms step_avg:59.02ms
step:2175/2330 train_time:128361ms step_avg:59.02ms
step:2176/2330 train_time:128422ms step_avg:59.02ms
step:2177/2330 train_time:128480ms step_avg:59.02ms
step:2178/2330 train_time:128541ms step_avg:59.02ms
step:2179/2330 train_time:128598ms step_avg:59.02ms
step:2180/2330 train_time:128660ms step_avg:59.02ms
step:2181/2330 train_time:128718ms step_avg:59.02ms
step:2182/2330 train_time:128779ms step_avg:59.02ms
step:2183/2330 train_time:128837ms step_avg:59.02ms
step:2184/2330 train_time:128898ms step_avg:59.02ms
step:2185/2330 train_time:128955ms step_avg:59.02ms
step:2186/2330 train_time:129016ms step_avg:59.02ms
step:2187/2330 train_time:129073ms step_avg:59.02ms
step:2188/2330 train_time:129136ms step_avg:59.02ms
step:2189/2330 train_time:129193ms step_avg:59.02ms
step:2190/2330 train_time:129254ms step_avg:59.02ms
step:2191/2330 train_time:129311ms step_avg:59.02ms
step:2192/2330 train_time:129373ms step_avg:59.02ms
step:2193/2330 train_time:129430ms step_avg:59.02ms
step:2194/2330 train_time:129492ms step_avg:59.02ms
step:2195/2330 train_time:129548ms step_avg:59.02ms
step:2196/2330 train_time:129611ms step_avg:59.02ms
step:2197/2330 train_time:129667ms step_avg:59.02ms
step:2198/2330 train_time:129731ms step_avg:59.02ms
step:2199/2330 train_time:129787ms step_avg:59.02ms
step:2200/2330 train_time:129851ms step_avg:59.02ms
step:2201/2330 train_time:129908ms step_avg:59.02ms
step:2202/2330 train_time:129970ms step_avg:59.02ms
step:2203/2330 train_time:130027ms step_avg:59.02ms
step:2204/2330 train_time:130089ms step_avg:59.02ms
step:2205/2330 train_time:130147ms step_avg:59.02ms
step:2206/2330 train_time:130208ms step_avg:59.02ms
step:2207/2330 train_time:130265ms step_avg:59.02ms
step:2208/2330 train_time:130327ms step_avg:59.03ms
step:2209/2330 train_time:130384ms step_avg:59.02ms
step:2210/2330 train_time:130447ms step_avg:59.03ms
step:2211/2330 train_time:130504ms step_avg:59.02ms
step:2212/2330 train_time:130567ms step_avg:59.03ms
step:2213/2330 train_time:130624ms step_avg:59.03ms
step:2214/2330 train_time:130687ms step_avg:59.03ms
step:2215/2330 train_time:130744ms step_avg:59.03ms
step:2216/2330 train_time:130807ms step_avg:59.03ms
step:2217/2330 train_time:130865ms step_avg:59.03ms
step:2218/2330 train_time:130927ms step_avg:59.03ms
step:2219/2330 train_time:130984ms step_avg:59.03ms
step:2220/2330 train_time:131048ms step_avg:59.03ms
step:2221/2330 train_time:131105ms step_avg:59.03ms
step:2222/2330 train_time:131168ms step_avg:59.03ms
step:2223/2330 train_time:131224ms step_avg:59.03ms
step:2224/2330 train_time:131287ms step_avg:59.03ms
step:2225/2330 train_time:131343ms step_avg:59.03ms
step:2226/2330 train_time:131407ms step_avg:59.03ms
step:2227/2330 train_time:131464ms step_avg:59.03ms
step:2228/2330 train_time:131527ms step_avg:59.03ms
step:2229/2330 train_time:131584ms step_avg:59.03ms
step:2230/2330 train_time:131646ms step_avg:59.03ms
step:2231/2330 train_time:131703ms step_avg:59.03ms
step:2232/2330 train_time:131765ms step_avg:59.03ms
step:2233/2330 train_time:131823ms step_avg:59.03ms
step:2234/2330 train_time:131885ms step_avg:59.04ms
step:2235/2330 train_time:131942ms step_avg:59.03ms
step:2236/2330 train_time:132006ms step_avg:59.04ms
step:2237/2330 train_time:132063ms step_avg:59.04ms
step:2238/2330 train_time:132126ms step_avg:59.04ms
step:2239/2330 train_time:132183ms step_avg:59.04ms
step:2240/2330 train_time:132246ms step_avg:59.04ms
step:2241/2330 train_time:132303ms step_avg:59.04ms
step:2242/2330 train_time:132366ms step_avg:59.04ms
step:2243/2330 train_time:132422ms step_avg:59.04ms
step:2244/2330 train_time:132485ms step_avg:59.04ms
step:2245/2330 train_time:132541ms step_avg:59.04ms
step:2246/2330 train_time:132604ms step_avg:59.04ms
step:2247/2330 train_time:132661ms step_avg:59.04ms
step:2248/2330 train_time:132724ms step_avg:59.04ms
step:2249/2330 train_time:132781ms step_avg:59.04ms
step:2250/2330 train_time:132843ms step_avg:59.04ms
step:2250/2330 val_loss:4.0221 train_time:132923ms step_avg:59.08ms
step:2251/2330 train_time:132943ms step_avg:59.06ms
step:2252/2330 train_time:132967ms step_avg:59.04ms
step:2253/2330 train_time:133027ms step_avg:59.04ms
step:2254/2330 train_time:133095ms step_avg:59.05ms
step:2255/2330 train_time:133152ms step_avg:59.05ms
step:2256/2330 train_time:133215ms step_avg:59.05ms
step:2257/2330 train_time:133271ms step_avg:59.05ms
step:2258/2330 train_time:133334ms step_avg:59.05ms
step:2259/2330 train_time:133391ms step_avg:59.05ms
step:2260/2330 train_time:133452ms step_avg:59.05ms
step:2261/2330 train_time:133509ms step_avg:59.05ms
step:2262/2330 train_time:133570ms step_avg:59.05ms
step:2263/2330 train_time:133626ms step_avg:59.05ms
step:2264/2330 train_time:133688ms step_avg:59.05ms
step:2265/2330 train_time:133744ms step_avg:59.05ms
step:2266/2330 train_time:133806ms step_avg:59.05ms
step:2267/2330 train_time:133862ms step_avg:59.05ms
step:2268/2330 train_time:133926ms step_avg:59.05ms
step:2269/2330 train_time:133985ms step_avg:59.05ms
step:2270/2330 train_time:134050ms step_avg:59.05ms
step:2271/2330 train_time:134109ms step_avg:59.05ms
step:2272/2330 train_time:134171ms step_avg:59.05ms
step:2273/2330 train_time:134229ms step_avg:59.05ms
step:2274/2330 train_time:134290ms step_avg:59.05ms
step:2275/2330 train_time:134347ms step_avg:59.05ms
step:2276/2330 train_time:134409ms step_avg:59.05ms
step:2277/2330 train_time:134466ms step_avg:59.05ms
step:2278/2330 train_time:134528ms step_avg:59.06ms
step:2279/2330 train_time:134584ms step_avg:59.05ms
step:2280/2330 train_time:134646ms step_avg:59.06ms
step:2281/2330 train_time:134702ms step_avg:59.05ms
step:2282/2330 train_time:134764ms step_avg:59.06ms
step:2283/2330 train_time:134820ms step_avg:59.05ms
step:2284/2330 train_time:134881ms step_avg:59.05ms
step:2285/2330 train_time:134939ms step_avg:59.05ms
step:2286/2330 train_time:135001ms step_avg:59.06ms
step:2287/2330 train_time:135059ms step_avg:59.05ms
step:2288/2330 train_time:135121ms step_avg:59.06ms
step:2289/2330 train_time:135178ms step_avg:59.06ms
step:2290/2330 train_time:135240ms step_avg:59.06ms
step:2291/2330 train_time:135297ms step_avg:59.06ms
step:2292/2330 train_time:135359ms step_avg:59.06ms
step:2293/2330 train_time:135415ms step_avg:59.06ms
step:2294/2330 train_time:135478ms step_avg:59.06ms
step:2295/2330 train_time:135534ms step_avg:59.06ms
step:2296/2330 train_time:135596ms step_avg:59.06ms
step:2297/2330 train_time:135652ms step_avg:59.06ms
step:2298/2330 train_time:135714ms step_avg:59.06ms
step:2299/2330 train_time:135771ms step_avg:59.06ms
step:2300/2330 train_time:135833ms step_avg:59.06ms
step:2301/2330 train_time:135889ms step_avg:59.06ms
step:2302/2330 train_time:135953ms step_avg:59.06ms
step:2303/2330 train_time:136010ms step_avg:59.06ms
step:2304/2330 train_time:136073ms step_avg:59.06ms
step:2305/2330 train_time:136129ms step_avg:59.06ms
step:2306/2330 train_time:136193ms step_avg:59.06ms
step:2307/2330 train_time:136250ms step_avg:59.06ms
step:2308/2330 train_time:136313ms step_avg:59.06ms
step:2309/2330 train_time:136370ms step_avg:59.06ms
step:2310/2330 train_time:136432ms step_avg:59.06ms
step:2311/2330 train_time:136490ms step_avg:59.06ms
step:2312/2330 train_time:136552ms step_avg:59.06ms
step:2313/2330 train_time:136608ms step_avg:59.06ms
step:2314/2330 train_time:136670ms step_avg:59.06ms
step:2315/2330 train_time:136727ms step_avg:59.06ms
step:2316/2330 train_time:136789ms step_avg:59.06ms
step:2317/2330 train_time:136846ms step_avg:59.06ms
step:2318/2330 train_time:136908ms step_avg:59.06ms
step:2319/2330 train_time:136965ms step_avg:59.06ms
step:2320/2330 train_time:137029ms step_avg:59.06ms
step:2321/2330 train_time:137086ms step_avg:59.06ms
step:2322/2330 train_time:137150ms step_avg:59.07ms
step:2323/2330 train_time:137207ms step_avg:59.06ms
step:2324/2330 train_time:137270ms step_avg:59.07ms
step:2325/2330 train_time:137327ms step_avg:59.07ms
step:2326/2330 train_time:137390ms step_avg:59.07ms
step:2327/2330 train_time:137447ms step_avg:59.07ms
step:2328/2330 train_time:137508ms step_avg:59.07ms
step:2329/2330 train_time:137566ms step_avg:59.07ms
step:2330/2330 train_time:137627ms step_avg:59.07ms
step:2330/2330 val_loss:4.0077 train_time:137707ms step_avg:59.10ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
