import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = divide_by_norm(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = divide_by_norm(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "gd_7e-4"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=7e-4, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 02:55:27 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   34C    P0             113W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   33C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   33C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   30C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   34C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:80ms step_avg:79.71ms
step:2/2330 train_time:172ms step_avg:85.92ms
step:3/2330 train_time:192ms step_avg:64.03ms
step:4/2330 train_time:221ms step_avg:55.27ms
step:5/2330 train_time:278ms step_avg:55.50ms
step:6/2330 train_time:338ms step_avg:56.26ms
step:7/2330 train_time:396ms step_avg:56.50ms
step:8/2330 train_time:457ms step_avg:57.12ms
step:9/2330 train_time:514ms step_avg:57.12ms
step:10/2330 train_time:575ms step_avg:57.47ms
step:11/2330 train_time:633ms step_avg:57.52ms
step:12/2330 train_time:694ms step_avg:57.79ms
step:13/2330 train_time:751ms step_avg:57.80ms
step:14/2330 train_time:812ms step_avg:58.01ms
step:15/2330 train_time:870ms step_avg:58.03ms
step:16/2330 train_time:932ms step_avg:58.28ms
step:17/2330 train_time:991ms step_avg:58.28ms
step:18/2330 train_time:1056ms step_avg:58.64ms
step:19/2330 train_time:1118ms step_avg:58.82ms
step:20/2330 train_time:1180ms step_avg:59.02ms
step:21/2330 train_time:1240ms step_avg:59.03ms
step:22/2330 train_time:1301ms step_avg:59.13ms
step:23/2330 train_time:1360ms step_avg:59.15ms
step:24/2330 train_time:1422ms step_avg:59.26ms
step:25/2330 train_time:1480ms step_avg:59.22ms
step:26/2330 train_time:1542ms step_avg:59.32ms
step:27/2330 train_time:1601ms step_avg:59.29ms
step:28/2330 train_time:1663ms step_avg:59.39ms
step:29/2330 train_time:1722ms step_avg:59.37ms
step:30/2330 train_time:1784ms step_avg:59.46ms
step:31/2330 train_time:1842ms step_avg:59.42ms
step:32/2330 train_time:1905ms step_avg:59.52ms
step:33/2330 train_time:1964ms step_avg:59.52ms
step:34/2330 train_time:2026ms step_avg:59.59ms
step:35/2330 train_time:2086ms step_avg:59.59ms
step:36/2330 train_time:2149ms step_avg:59.71ms
step:37/2330 train_time:2208ms step_avg:59.68ms
step:38/2330 train_time:2270ms step_avg:59.75ms
step:39/2330 train_time:2329ms step_avg:59.73ms
step:40/2330 train_time:2392ms step_avg:59.81ms
step:41/2330 train_time:2451ms step_avg:59.78ms
step:42/2330 train_time:2514ms step_avg:59.85ms
step:43/2330 train_time:2573ms step_avg:59.83ms
step:44/2330 train_time:2635ms step_avg:59.88ms
step:45/2330 train_time:2695ms step_avg:59.89ms
step:46/2330 train_time:2758ms step_avg:59.96ms
step:47/2330 train_time:2816ms step_avg:59.92ms
step:48/2330 train_time:2878ms step_avg:59.96ms
step:49/2330 train_time:2938ms step_avg:59.95ms
step:50/2330 train_time:2999ms step_avg:59.99ms
step:51/2330 train_time:3059ms step_avg:59.97ms
step:52/2330 train_time:3121ms step_avg:60.01ms
step:53/2330 train_time:3180ms step_avg:59.99ms
step:54/2330 train_time:3241ms step_avg:60.03ms
step:55/2330 train_time:3302ms step_avg:60.04ms
step:56/2330 train_time:3364ms step_avg:60.07ms
step:57/2330 train_time:3423ms step_avg:60.06ms
step:58/2330 train_time:3484ms step_avg:60.08ms
step:59/2330 train_time:3543ms step_avg:60.06ms
step:60/2330 train_time:3605ms step_avg:60.08ms
step:61/2330 train_time:3665ms step_avg:60.08ms
step:62/2330 train_time:3727ms step_avg:60.12ms
step:63/2330 train_time:3786ms step_avg:60.10ms
step:64/2330 train_time:3849ms step_avg:60.14ms
step:65/2330 train_time:3907ms step_avg:60.11ms
step:66/2330 train_time:3970ms step_avg:60.14ms
step:67/2330 train_time:4028ms step_avg:60.12ms
step:68/2330 train_time:4091ms step_avg:60.16ms
step:69/2330 train_time:4150ms step_avg:60.15ms
step:70/2330 train_time:4212ms step_avg:60.17ms
step:71/2330 train_time:4271ms step_avg:60.15ms
step:72/2330 train_time:4333ms step_avg:60.18ms
step:73/2330 train_time:4392ms step_avg:60.17ms
step:74/2330 train_time:4455ms step_avg:60.20ms
step:75/2330 train_time:4515ms step_avg:60.20ms
step:76/2330 train_time:4578ms step_avg:60.23ms
step:77/2330 train_time:4637ms step_avg:60.22ms
step:78/2330 train_time:4699ms step_avg:60.24ms
step:79/2330 train_time:4758ms step_avg:60.23ms
step:80/2330 train_time:4821ms step_avg:60.26ms
step:81/2330 train_time:4880ms step_avg:60.25ms
step:82/2330 train_time:4943ms step_avg:60.28ms
step:83/2330 train_time:5002ms step_avg:60.26ms
step:84/2330 train_time:5063ms step_avg:60.28ms
step:85/2330 train_time:5122ms step_avg:60.26ms
step:86/2330 train_time:5185ms step_avg:60.29ms
step:87/2330 train_time:5244ms step_avg:60.28ms
step:88/2330 train_time:5306ms step_avg:60.30ms
step:89/2330 train_time:5365ms step_avg:60.28ms
step:90/2330 train_time:5427ms step_avg:60.30ms
step:91/2330 train_time:5487ms step_avg:60.29ms
step:92/2330 train_time:5549ms step_avg:60.32ms
step:93/2330 train_time:5609ms step_avg:60.31ms
step:94/2330 train_time:5671ms step_avg:60.33ms
step:95/2330 train_time:5730ms step_avg:60.31ms
step:96/2330 train_time:5792ms step_avg:60.33ms
step:97/2330 train_time:5851ms step_avg:60.32ms
step:98/2330 train_time:5914ms step_avg:60.35ms
step:99/2330 train_time:5974ms step_avg:60.34ms
step:100/2330 train_time:6037ms step_avg:60.37ms
step:101/2330 train_time:6096ms step_avg:60.35ms
step:102/2330 train_time:6158ms step_avg:60.37ms
step:103/2330 train_time:6217ms step_avg:60.36ms
step:104/2330 train_time:6279ms step_avg:60.38ms
step:105/2330 train_time:6338ms step_avg:60.36ms
step:106/2330 train_time:6400ms step_avg:60.38ms
step:107/2330 train_time:6460ms step_avg:60.37ms
step:108/2330 train_time:6522ms step_avg:60.39ms
step:109/2330 train_time:6581ms step_avg:60.37ms
step:110/2330 train_time:6643ms step_avg:60.39ms
step:111/2330 train_time:6702ms step_avg:60.38ms
step:112/2330 train_time:6765ms step_avg:60.40ms
step:113/2330 train_time:6825ms step_avg:60.39ms
step:114/2330 train_time:6887ms step_avg:60.41ms
step:115/2330 train_time:6946ms step_avg:60.40ms
step:116/2330 train_time:7009ms step_avg:60.42ms
step:117/2330 train_time:7068ms step_avg:60.41ms
step:118/2330 train_time:7130ms step_avg:60.42ms
step:119/2330 train_time:7188ms step_avg:60.41ms
step:120/2330 train_time:7251ms step_avg:60.43ms
step:121/2330 train_time:7310ms step_avg:60.41ms
step:122/2330 train_time:7373ms step_avg:60.43ms
step:123/2330 train_time:7432ms step_avg:60.42ms
step:124/2330 train_time:7494ms step_avg:60.43ms
step:125/2330 train_time:7553ms step_avg:60.43ms
step:126/2330 train_time:7616ms step_avg:60.44ms
step:127/2330 train_time:7676ms step_avg:60.44ms
step:128/2330 train_time:7738ms step_avg:60.45ms
step:129/2330 train_time:7797ms step_avg:60.44ms
step:130/2330 train_time:7859ms step_avg:60.46ms
step:131/2330 train_time:7918ms step_avg:60.44ms
step:132/2330 train_time:7980ms step_avg:60.46ms
step:133/2330 train_time:8039ms step_avg:60.45ms
step:134/2330 train_time:8102ms step_avg:60.46ms
step:135/2330 train_time:8162ms step_avg:60.46ms
step:136/2330 train_time:8223ms step_avg:60.46ms
step:137/2330 train_time:8282ms step_avg:60.45ms
step:138/2330 train_time:8344ms step_avg:60.46ms
step:139/2330 train_time:8403ms step_avg:60.45ms
step:140/2330 train_time:8466ms step_avg:60.47ms
step:141/2330 train_time:8525ms step_avg:60.46ms
step:142/2330 train_time:8588ms step_avg:60.48ms
step:143/2330 train_time:8647ms step_avg:60.47ms
step:144/2330 train_time:8710ms step_avg:60.48ms
step:145/2330 train_time:8769ms step_avg:60.48ms
step:146/2330 train_time:8831ms step_avg:60.48ms
step:147/2330 train_time:8890ms step_avg:60.48ms
step:148/2330 train_time:8953ms step_avg:60.49ms
step:149/2330 train_time:9013ms step_avg:60.49ms
step:150/2330 train_time:9075ms step_avg:60.50ms
step:151/2330 train_time:9135ms step_avg:60.49ms
step:152/2330 train_time:9197ms step_avg:60.51ms
step:153/2330 train_time:9257ms step_avg:60.50ms
step:154/2330 train_time:9319ms step_avg:60.51ms
step:155/2330 train_time:9378ms step_avg:60.50ms
step:156/2330 train_time:9440ms step_avg:60.51ms
step:157/2330 train_time:9499ms step_avg:60.50ms
step:158/2330 train_time:9562ms step_avg:60.52ms
step:159/2330 train_time:9621ms step_avg:60.51ms
step:160/2330 train_time:9683ms step_avg:60.52ms
step:161/2330 train_time:9743ms step_avg:60.51ms
step:162/2330 train_time:9804ms step_avg:60.52ms
step:163/2330 train_time:9864ms step_avg:60.52ms
step:164/2330 train_time:9926ms step_avg:60.53ms
step:165/2330 train_time:9985ms step_avg:60.52ms
step:166/2330 train_time:10049ms step_avg:60.53ms
step:167/2330 train_time:10108ms step_avg:60.53ms
step:168/2330 train_time:10170ms step_avg:60.54ms
step:169/2330 train_time:10229ms step_avg:60.53ms
step:170/2330 train_time:10292ms step_avg:60.54ms
step:171/2330 train_time:10352ms step_avg:60.54ms
step:172/2330 train_time:10414ms step_avg:60.55ms
step:173/2330 train_time:10474ms step_avg:60.54ms
step:174/2330 train_time:10536ms step_avg:60.55ms
step:175/2330 train_time:10595ms step_avg:60.54ms
step:176/2330 train_time:10659ms step_avg:60.56ms
step:177/2330 train_time:10718ms step_avg:60.55ms
step:178/2330 train_time:10780ms step_avg:60.56ms
step:179/2330 train_time:10839ms step_avg:60.56ms
step:180/2330 train_time:10902ms step_avg:60.57ms
step:181/2330 train_time:10961ms step_avg:60.56ms
step:182/2330 train_time:11023ms step_avg:60.57ms
step:183/2330 train_time:11083ms step_avg:60.56ms
step:184/2330 train_time:11144ms step_avg:60.57ms
step:185/2330 train_time:11204ms step_avg:60.56ms
step:186/2330 train_time:11268ms step_avg:60.58ms
step:187/2330 train_time:11327ms step_avg:60.57ms
step:188/2330 train_time:11389ms step_avg:60.58ms
step:189/2330 train_time:11448ms step_avg:60.57ms
step:190/2330 train_time:11511ms step_avg:60.58ms
step:191/2330 train_time:11570ms step_avg:60.58ms
step:192/2330 train_time:11632ms step_avg:60.58ms
step:193/2330 train_time:11692ms step_avg:60.58ms
step:194/2330 train_time:11755ms step_avg:60.59ms
step:195/2330 train_time:11815ms step_avg:60.59ms
step:196/2330 train_time:11878ms step_avg:60.60ms
step:197/2330 train_time:11936ms step_avg:60.59ms
step:198/2330 train_time:11999ms step_avg:60.60ms
step:199/2330 train_time:12060ms step_avg:60.60ms
step:200/2330 train_time:12122ms step_avg:60.61ms
step:201/2330 train_time:12181ms step_avg:60.60ms
step:202/2330 train_time:12243ms step_avg:60.61ms
step:203/2330 train_time:12302ms step_avg:60.60ms
step:204/2330 train_time:12366ms step_avg:60.62ms
step:205/2330 train_time:12425ms step_avg:60.61ms
step:206/2330 train_time:12488ms step_avg:60.62ms
step:207/2330 train_time:12548ms step_avg:60.62ms
step:208/2330 train_time:12609ms step_avg:60.62ms
step:209/2330 train_time:12669ms step_avg:60.62ms
step:210/2330 train_time:12731ms step_avg:60.62ms
step:211/2330 train_time:12790ms step_avg:60.62ms
step:212/2330 train_time:12852ms step_avg:60.62ms
step:213/2330 train_time:12912ms step_avg:60.62ms
step:214/2330 train_time:12975ms step_avg:60.63ms
step:215/2330 train_time:13034ms step_avg:60.62ms
step:216/2330 train_time:13097ms step_avg:60.63ms
step:217/2330 train_time:13157ms step_avg:60.63ms
step:218/2330 train_time:13219ms step_avg:60.64ms
step:219/2330 train_time:13279ms step_avg:60.63ms
step:220/2330 train_time:13342ms step_avg:60.64ms
step:221/2330 train_time:13401ms step_avg:60.64ms
step:222/2330 train_time:13464ms step_avg:60.65ms
step:223/2330 train_time:13523ms step_avg:60.64ms
step:224/2330 train_time:13585ms step_avg:60.65ms
step:225/2330 train_time:13644ms step_avg:60.64ms
step:226/2330 train_time:13706ms step_avg:60.65ms
step:227/2330 train_time:13767ms step_avg:60.65ms
step:228/2330 train_time:13829ms step_avg:60.65ms
step:229/2330 train_time:13888ms step_avg:60.65ms
step:230/2330 train_time:13950ms step_avg:60.65ms
step:231/2330 train_time:14009ms step_avg:60.65ms
step:232/2330 train_time:14073ms step_avg:60.66ms
step:233/2330 train_time:14132ms step_avg:60.65ms
step:234/2330 train_time:14194ms step_avg:60.66ms
step:235/2330 train_time:14254ms step_avg:60.66ms
step:236/2330 train_time:14316ms step_avg:60.66ms
step:237/2330 train_time:14376ms step_avg:60.66ms
step:238/2330 train_time:14439ms step_avg:60.67ms
step:239/2330 train_time:14498ms step_avg:60.66ms
step:240/2330 train_time:14561ms step_avg:60.67ms
step:241/2330 train_time:14620ms step_avg:60.67ms
step:242/2330 train_time:14682ms step_avg:60.67ms
step:243/2330 train_time:14741ms step_avg:60.66ms
step:244/2330 train_time:14804ms step_avg:60.67ms
step:245/2330 train_time:14865ms step_avg:60.67ms
step:246/2330 train_time:14926ms step_avg:60.67ms
step:247/2330 train_time:14986ms step_avg:60.67ms
step:248/2330 train_time:15049ms step_avg:60.68ms
step:249/2330 train_time:15109ms step_avg:60.68ms
step:250/2330 train_time:15170ms step_avg:60.68ms
step:250/2330 val_loss:5.5753 train_time:15243ms step_avg:60.97ms
step:251/2330 train_time:15265ms step_avg:60.82ms
step:252/2330 train_time:15294ms step_avg:60.69ms
step:253/2330 train_time:15354ms step_avg:60.69ms
step:254/2330 train_time:15420ms step_avg:60.71ms
step:255/2330 train_time:15484ms step_avg:60.72ms
step:256/2330 train_time:15547ms step_avg:60.73ms
step:257/2330 train_time:15606ms step_avg:60.72ms
step:258/2330 train_time:15668ms step_avg:60.73ms
step:259/2330 train_time:15729ms step_avg:60.73ms
step:260/2330 train_time:15789ms step_avg:60.73ms
step:261/2330 train_time:15847ms step_avg:60.72ms
step:262/2330 train_time:15909ms step_avg:60.72ms
step:263/2330 train_time:15968ms step_avg:60.71ms
step:264/2330 train_time:16030ms step_avg:60.72ms
step:265/2330 train_time:16087ms step_avg:60.71ms
step:266/2330 train_time:16151ms step_avg:60.72ms
step:267/2330 train_time:16212ms step_avg:60.72ms
step:268/2330 train_time:16274ms step_avg:60.72ms
step:269/2330 train_time:16335ms step_avg:60.72ms
step:270/2330 train_time:16397ms step_avg:60.73ms
step:271/2330 train_time:16458ms step_avg:60.73ms
step:272/2330 train_time:16520ms step_avg:60.74ms
step:273/2330 train_time:16579ms step_avg:60.73ms
step:274/2330 train_time:16642ms step_avg:60.74ms
step:275/2330 train_time:16701ms step_avg:60.73ms
step:276/2330 train_time:16763ms step_avg:60.74ms
step:277/2330 train_time:16823ms step_avg:60.73ms
step:278/2330 train_time:16885ms step_avg:60.74ms
step:279/2330 train_time:16944ms step_avg:60.73ms
step:280/2330 train_time:17005ms step_avg:60.73ms
step:281/2330 train_time:17065ms step_avg:60.73ms
step:282/2330 train_time:17128ms step_avg:60.74ms
step:283/2330 train_time:17187ms step_avg:60.73ms
step:284/2330 train_time:17249ms step_avg:60.74ms
step:285/2330 train_time:17310ms step_avg:60.74ms
step:286/2330 train_time:17372ms step_avg:60.74ms
step:287/2330 train_time:17432ms step_avg:60.74ms
step:288/2330 train_time:17494ms step_avg:60.74ms
step:289/2330 train_time:17553ms step_avg:60.74ms
step:290/2330 train_time:17616ms step_avg:60.74ms
step:291/2330 train_time:17675ms step_avg:60.74ms
step:292/2330 train_time:17737ms step_avg:60.74ms
step:293/2330 train_time:17796ms step_avg:60.74ms
step:294/2330 train_time:17859ms step_avg:60.74ms
step:295/2330 train_time:17917ms step_avg:60.74ms
step:296/2330 train_time:17979ms step_avg:60.74ms
step:297/2330 train_time:18038ms step_avg:60.73ms
step:298/2330 train_time:18100ms step_avg:60.74ms
step:299/2330 train_time:18160ms step_avg:60.74ms
step:300/2330 train_time:18223ms step_avg:60.74ms
step:301/2330 train_time:18283ms step_avg:60.74ms
step:302/2330 train_time:18345ms step_avg:60.75ms
step:303/2330 train_time:18405ms step_avg:60.74ms
step:304/2330 train_time:18468ms step_avg:60.75ms
step:305/2330 train_time:18528ms step_avg:60.75ms
step:306/2330 train_time:18590ms step_avg:60.75ms
step:307/2330 train_time:18649ms step_avg:60.75ms
step:308/2330 train_time:18712ms step_avg:60.75ms
step:309/2330 train_time:18771ms step_avg:60.75ms
step:310/2330 train_time:18834ms step_avg:60.75ms
step:311/2330 train_time:18892ms step_avg:60.75ms
step:312/2330 train_time:18955ms step_avg:60.75ms
step:313/2330 train_time:19014ms step_avg:60.75ms
step:314/2330 train_time:19076ms step_avg:60.75ms
step:315/2330 train_time:19135ms step_avg:60.75ms
step:316/2330 train_time:19197ms step_avg:60.75ms
step:317/2330 train_time:19256ms step_avg:60.75ms
step:318/2330 train_time:19319ms step_avg:60.75ms
step:319/2330 train_time:19378ms step_avg:60.75ms
step:320/2330 train_time:19441ms step_avg:60.75ms
step:321/2330 train_time:19500ms step_avg:60.75ms
step:322/2330 train_time:19562ms step_avg:60.75ms
step:323/2330 train_time:19622ms step_avg:60.75ms
step:324/2330 train_time:19685ms step_avg:60.76ms
step:325/2330 train_time:19745ms step_avg:60.76ms
step:326/2330 train_time:19808ms step_avg:60.76ms
step:327/2330 train_time:19868ms step_avg:60.76ms
step:328/2330 train_time:19931ms step_avg:60.76ms
step:329/2330 train_time:19990ms step_avg:60.76ms
step:330/2330 train_time:20052ms step_avg:60.76ms
step:331/2330 train_time:20112ms step_avg:60.76ms
step:332/2330 train_time:20174ms step_avg:60.76ms
step:333/2330 train_time:20234ms step_avg:60.76ms
step:334/2330 train_time:20297ms step_avg:60.77ms
step:335/2330 train_time:20356ms step_avg:60.76ms
step:336/2330 train_time:20418ms step_avg:60.77ms
step:337/2330 train_time:20477ms step_avg:60.76ms
step:338/2330 train_time:20541ms step_avg:60.77ms
step:339/2330 train_time:20600ms step_avg:60.77ms
step:340/2330 train_time:20663ms step_avg:60.77ms
step:341/2330 train_time:20723ms step_avg:60.77ms
step:342/2330 train_time:20787ms step_avg:60.78ms
step:343/2330 train_time:20846ms step_avg:60.78ms
step:344/2330 train_time:20909ms step_avg:60.78ms
step:345/2330 train_time:20968ms step_avg:60.78ms
step:346/2330 train_time:21030ms step_avg:60.78ms
step:347/2330 train_time:21089ms step_avg:60.78ms
step:348/2330 train_time:21152ms step_avg:60.78ms
step:349/2330 train_time:21211ms step_avg:60.78ms
step:350/2330 train_time:21274ms step_avg:60.78ms
step:351/2330 train_time:21334ms step_avg:60.78ms
step:352/2330 train_time:21396ms step_avg:60.78ms
step:353/2330 train_time:21455ms step_avg:60.78ms
step:354/2330 train_time:21518ms step_avg:60.79ms
step:355/2330 train_time:21578ms step_avg:60.78ms
step:356/2330 train_time:21641ms step_avg:60.79ms
step:357/2330 train_time:21699ms step_avg:60.78ms
step:358/2330 train_time:21762ms step_avg:60.79ms
step:359/2330 train_time:21821ms step_avg:60.78ms
step:360/2330 train_time:21884ms step_avg:60.79ms
step:361/2330 train_time:21944ms step_avg:60.79ms
step:362/2330 train_time:22007ms step_avg:60.79ms
step:363/2330 train_time:22066ms step_avg:60.79ms
step:364/2330 train_time:22130ms step_avg:60.80ms
step:365/2330 train_time:22188ms step_avg:60.79ms
step:366/2330 train_time:22251ms step_avg:60.79ms
step:367/2330 train_time:22310ms step_avg:60.79ms
step:368/2330 train_time:22372ms step_avg:60.79ms
step:369/2330 train_time:22433ms step_avg:60.79ms
step:370/2330 train_time:22494ms step_avg:60.80ms
step:371/2330 train_time:22554ms step_avg:60.79ms
step:372/2330 train_time:22616ms step_avg:60.80ms
step:373/2330 train_time:22675ms step_avg:60.79ms
step:374/2330 train_time:22739ms step_avg:60.80ms
step:375/2330 train_time:22797ms step_avg:60.79ms
step:376/2330 train_time:22860ms step_avg:60.80ms
step:377/2330 train_time:22918ms step_avg:60.79ms
step:378/2330 train_time:22981ms step_avg:60.80ms
step:379/2330 train_time:23041ms step_avg:60.79ms
step:380/2330 train_time:23103ms step_avg:60.80ms
step:381/2330 train_time:23163ms step_avg:60.80ms
step:382/2330 train_time:23226ms step_avg:60.80ms
step:383/2330 train_time:23286ms step_avg:60.80ms
step:384/2330 train_time:23349ms step_avg:60.80ms
step:385/2330 train_time:23407ms step_avg:60.80ms
step:386/2330 train_time:23470ms step_avg:60.80ms
step:387/2330 train_time:23530ms step_avg:60.80ms
step:388/2330 train_time:23592ms step_avg:60.80ms
step:389/2330 train_time:23652ms step_avg:60.80ms
step:390/2330 train_time:23714ms step_avg:60.81ms
step:391/2330 train_time:23773ms step_avg:60.80ms
step:392/2330 train_time:23836ms step_avg:60.81ms
step:393/2330 train_time:23895ms step_avg:60.80ms
step:394/2330 train_time:23959ms step_avg:60.81ms
step:395/2330 train_time:24018ms step_avg:60.81ms
step:396/2330 train_time:24080ms step_avg:60.81ms
step:397/2330 train_time:24140ms step_avg:60.80ms
step:398/2330 train_time:24202ms step_avg:60.81ms
step:399/2330 train_time:24260ms step_avg:60.80ms
step:400/2330 train_time:24323ms step_avg:60.81ms
step:401/2330 train_time:24383ms step_avg:60.80ms
step:402/2330 train_time:24445ms step_avg:60.81ms
step:403/2330 train_time:24505ms step_avg:60.81ms
step:404/2330 train_time:24568ms step_avg:60.81ms
step:405/2330 train_time:24628ms step_avg:60.81ms
step:406/2330 train_time:24691ms step_avg:60.82ms
step:407/2330 train_time:24751ms step_avg:60.81ms
step:408/2330 train_time:24813ms step_avg:60.82ms
step:409/2330 train_time:24872ms step_avg:60.81ms
step:410/2330 train_time:24934ms step_avg:60.82ms
step:411/2330 train_time:24994ms step_avg:60.81ms
step:412/2330 train_time:25057ms step_avg:60.82ms
step:413/2330 train_time:25117ms step_avg:60.82ms
step:414/2330 train_time:25179ms step_avg:60.82ms
step:415/2330 train_time:25238ms step_avg:60.81ms
step:416/2330 train_time:25300ms step_avg:60.82ms
step:417/2330 train_time:25359ms step_avg:60.81ms
step:418/2330 train_time:25422ms step_avg:60.82ms
step:419/2330 train_time:25483ms step_avg:60.82ms
step:420/2330 train_time:25545ms step_avg:60.82ms
step:421/2330 train_time:25605ms step_avg:60.82ms
step:422/2330 train_time:25668ms step_avg:60.83ms
step:423/2330 train_time:25728ms step_avg:60.82ms
step:424/2330 train_time:25790ms step_avg:60.83ms
step:425/2330 train_time:25849ms step_avg:60.82ms
step:426/2330 train_time:25913ms step_avg:60.83ms
step:427/2330 train_time:25972ms step_avg:60.83ms
step:428/2330 train_time:26034ms step_avg:60.83ms
step:429/2330 train_time:26093ms step_avg:60.82ms
step:430/2330 train_time:26156ms step_avg:60.83ms
step:431/2330 train_time:26216ms step_avg:60.83ms
step:432/2330 train_time:26278ms step_avg:60.83ms
step:433/2330 train_time:26338ms step_avg:60.83ms
step:434/2330 train_time:26401ms step_avg:60.83ms
step:435/2330 train_time:26460ms step_avg:60.83ms
step:436/2330 train_time:26523ms step_avg:60.83ms
step:437/2330 train_time:26582ms step_avg:60.83ms
step:438/2330 train_time:26645ms step_avg:60.83ms
step:439/2330 train_time:26704ms step_avg:60.83ms
step:440/2330 train_time:26767ms step_avg:60.83ms
step:441/2330 train_time:26827ms step_avg:60.83ms
step:442/2330 train_time:26889ms step_avg:60.83ms
step:443/2330 train_time:26949ms step_avg:60.83ms
step:444/2330 train_time:27011ms step_avg:60.84ms
step:445/2330 train_time:27071ms step_avg:60.83ms
step:446/2330 train_time:27134ms step_avg:60.84ms
step:447/2330 train_time:27193ms step_avg:60.83ms
step:448/2330 train_time:27255ms step_avg:60.84ms
step:449/2330 train_time:27314ms step_avg:60.83ms
step:450/2330 train_time:27376ms step_avg:60.84ms
step:451/2330 train_time:27436ms step_avg:60.83ms
step:452/2330 train_time:27499ms step_avg:60.84ms
step:453/2330 train_time:27558ms step_avg:60.83ms
step:454/2330 train_time:27620ms step_avg:60.84ms
step:455/2330 train_time:27679ms step_avg:60.83ms
step:456/2330 train_time:27742ms step_avg:60.84ms
step:457/2330 train_time:27800ms step_avg:60.83ms
step:458/2330 train_time:27862ms step_avg:60.83ms
step:459/2330 train_time:27923ms step_avg:60.83ms
step:460/2330 train_time:27987ms step_avg:60.84ms
step:461/2330 train_time:28046ms step_avg:60.84ms
step:462/2330 train_time:28109ms step_avg:60.84ms
step:463/2330 train_time:28169ms step_avg:60.84ms
step:464/2330 train_time:28233ms step_avg:60.85ms
step:465/2330 train_time:28292ms step_avg:60.84ms
step:466/2330 train_time:28355ms step_avg:60.85ms
step:467/2330 train_time:28414ms step_avg:60.84ms
step:468/2330 train_time:28477ms step_avg:60.85ms
step:469/2330 train_time:28537ms step_avg:60.85ms
step:470/2330 train_time:28599ms step_avg:60.85ms
step:471/2330 train_time:28658ms step_avg:60.84ms
step:472/2330 train_time:28720ms step_avg:60.85ms
step:473/2330 train_time:28779ms step_avg:60.84ms
step:474/2330 train_time:28841ms step_avg:60.85ms
step:475/2330 train_time:28901ms step_avg:60.84ms
step:476/2330 train_time:28964ms step_avg:60.85ms
step:477/2330 train_time:29025ms step_avg:60.85ms
step:478/2330 train_time:29088ms step_avg:60.85ms
step:479/2330 train_time:29147ms step_avg:60.85ms
step:480/2330 train_time:29211ms step_avg:60.86ms
step:481/2330 train_time:29270ms step_avg:60.85ms
step:482/2330 train_time:29334ms step_avg:60.86ms
step:483/2330 train_time:29393ms step_avg:60.85ms
step:484/2330 train_time:29454ms step_avg:60.86ms
step:485/2330 train_time:29514ms step_avg:60.85ms
step:486/2330 train_time:29576ms step_avg:60.86ms
step:487/2330 train_time:29635ms step_avg:60.85ms
step:488/2330 train_time:29698ms step_avg:60.86ms
step:489/2330 train_time:29757ms step_avg:60.85ms
step:490/2330 train_time:29820ms step_avg:60.86ms
step:491/2330 train_time:29879ms step_avg:60.85ms
step:492/2330 train_time:29942ms step_avg:60.86ms
step:493/2330 train_time:30001ms step_avg:60.85ms
step:494/2330 train_time:30063ms step_avg:60.86ms
step:495/2330 train_time:30124ms step_avg:60.86ms
step:496/2330 train_time:30188ms step_avg:60.86ms
step:497/2330 train_time:30248ms step_avg:60.86ms
step:498/2330 train_time:30311ms step_avg:60.86ms
step:499/2330 train_time:30369ms step_avg:60.86ms
step:500/2330 train_time:30433ms step_avg:60.87ms
step:500/2330 val_loss:5.1668 train_time:30505ms step_avg:61.01ms
step:501/2330 train_time:30528ms step_avg:60.93ms
step:502/2330 train_time:30557ms step_avg:60.87ms
step:503/2330 train_time:30618ms step_avg:60.87ms
step:504/2330 train_time:30685ms step_avg:60.88ms
step:505/2330 train_time:30744ms step_avg:60.88ms
step:506/2330 train_time:30807ms step_avg:60.88ms
step:507/2330 train_time:30868ms step_avg:60.88ms
step:508/2330 train_time:30929ms step_avg:60.88ms
step:509/2330 train_time:30988ms step_avg:60.88ms
step:510/2330 train_time:31050ms step_avg:60.88ms
step:511/2330 train_time:31109ms step_avg:60.88ms
step:512/2330 train_time:31170ms step_avg:60.88ms
step:513/2330 train_time:31229ms step_avg:60.88ms
step:514/2330 train_time:31291ms step_avg:60.88ms
step:515/2330 train_time:31350ms step_avg:60.87ms
step:516/2330 train_time:31413ms step_avg:60.88ms
step:517/2330 train_time:31473ms step_avg:60.88ms
step:518/2330 train_time:31536ms step_avg:60.88ms
step:519/2330 train_time:31596ms step_avg:60.88ms
step:520/2330 train_time:31660ms step_avg:60.88ms
step:521/2330 train_time:31720ms step_avg:60.88ms
step:522/2330 train_time:31784ms step_avg:60.89ms
step:523/2330 train_time:31844ms step_avg:60.89ms
step:524/2330 train_time:31907ms step_avg:60.89ms
step:525/2330 train_time:31966ms step_avg:60.89ms
step:526/2330 train_time:32028ms step_avg:60.89ms
step:527/2330 train_time:32087ms step_avg:60.89ms
step:528/2330 train_time:32149ms step_avg:60.89ms
step:529/2330 train_time:32208ms step_avg:60.88ms
step:530/2330 train_time:32270ms step_avg:60.89ms
step:531/2330 train_time:32330ms step_avg:60.88ms
step:532/2330 train_time:32391ms step_avg:60.89ms
step:533/2330 train_time:32451ms step_avg:60.88ms
step:534/2330 train_time:32514ms step_avg:60.89ms
step:535/2330 train_time:32574ms step_avg:60.89ms
step:536/2330 train_time:32637ms step_avg:60.89ms
step:537/2330 train_time:32695ms step_avg:60.88ms
step:538/2330 train_time:32759ms step_avg:60.89ms
step:539/2330 train_time:32820ms step_avg:60.89ms
step:540/2330 train_time:32884ms step_avg:60.90ms
step:541/2330 train_time:32943ms step_avg:60.89ms
step:542/2330 train_time:33006ms step_avg:60.90ms
step:543/2330 train_time:33065ms step_avg:60.89ms
step:544/2330 train_time:33127ms step_avg:60.90ms
step:545/2330 train_time:33186ms step_avg:60.89ms
step:546/2330 train_time:33249ms step_avg:60.90ms
step:547/2330 train_time:33308ms step_avg:60.89ms
step:548/2330 train_time:33371ms step_avg:60.90ms
step:549/2330 train_time:33430ms step_avg:60.89ms
step:550/2330 train_time:33492ms step_avg:60.90ms
step:551/2330 train_time:33552ms step_avg:60.89ms
step:552/2330 train_time:33616ms step_avg:60.90ms
step:553/2330 train_time:33675ms step_avg:60.90ms
step:554/2330 train_time:33738ms step_avg:60.90ms
step:555/2330 train_time:33798ms step_avg:60.90ms
step:556/2330 train_time:33861ms step_avg:60.90ms
step:557/2330 train_time:33921ms step_avg:60.90ms
step:558/2330 train_time:33984ms step_avg:60.90ms
step:559/2330 train_time:34044ms step_avg:60.90ms
step:560/2330 train_time:34106ms step_avg:60.90ms
step:561/2330 train_time:34165ms step_avg:60.90ms
step:562/2330 train_time:34228ms step_avg:60.90ms
step:563/2330 train_time:34287ms step_avg:60.90ms
step:564/2330 train_time:34349ms step_avg:60.90ms
step:565/2330 train_time:34409ms step_avg:60.90ms
step:566/2330 train_time:34471ms step_avg:60.90ms
step:567/2330 train_time:34531ms step_avg:60.90ms
step:568/2330 train_time:34594ms step_avg:60.90ms
step:569/2330 train_time:34654ms step_avg:60.90ms
step:570/2330 train_time:34716ms step_avg:60.91ms
step:571/2330 train_time:34776ms step_avg:60.90ms
step:572/2330 train_time:34838ms step_avg:60.91ms
step:573/2330 train_time:34898ms step_avg:60.90ms
step:574/2330 train_time:34961ms step_avg:60.91ms
step:575/2330 train_time:35022ms step_avg:60.91ms
step:576/2330 train_time:35085ms step_avg:60.91ms
step:577/2330 train_time:35145ms step_avg:60.91ms
step:578/2330 train_time:35208ms step_avg:60.91ms
step:579/2330 train_time:35269ms step_avg:60.91ms
step:580/2330 train_time:35330ms step_avg:60.91ms
step:581/2330 train_time:35389ms step_avg:60.91ms
step:582/2330 train_time:35451ms step_avg:60.91ms
step:583/2330 train_time:35511ms step_avg:60.91ms
step:584/2330 train_time:35574ms step_avg:60.92ms
step:585/2330 train_time:35633ms step_avg:60.91ms
step:586/2330 train_time:35696ms step_avg:60.91ms
step:587/2330 train_time:35755ms step_avg:60.91ms
step:588/2330 train_time:35817ms step_avg:60.91ms
step:589/2330 train_time:35876ms step_avg:60.91ms
step:590/2330 train_time:35939ms step_avg:60.91ms
step:591/2330 train_time:35999ms step_avg:60.91ms
step:592/2330 train_time:36063ms step_avg:60.92ms
step:593/2330 train_time:36122ms step_avg:60.91ms
step:594/2330 train_time:36185ms step_avg:60.92ms
step:595/2330 train_time:36245ms step_avg:60.92ms
step:596/2330 train_time:36307ms step_avg:60.92ms
step:597/2330 train_time:36367ms step_avg:60.92ms
step:598/2330 train_time:36429ms step_avg:60.92ms
step:599/2330 train_time:36488ms step_avg:60.92ms
step:600/2330 train_time:36551ms step_avg:60.92ms
step:601/2330 train_time:36610ms step_avg:60.92ms
step:602/2330 train_time:36674ms step_avg:60.92ms
step:603/2330 train_time:36734ms step_avg:60.92ms
step:604/2330 train_time:36797ms step_avg:60.92ms
step:605/2330 train_time:36856ms step_avg:60.92ms
step:606/2330 train_time:36919ms step_avg:60.92ms
step:607/2330 train_time:36978ms step_avg:60.92ms
step:608/2330 train_time:37041ms step_avg:60.92ms
step:609/2330 train_time:37101ms step_avg:60.92ms
step:610/2330 train_time:37164ms step_avg:60.92ms
step:611/2330 train_time:37223ms step_avg:60.92ms
step:612/2330 train_time:37286ms step_avg:60.92ms
step:613/2330 train_time:37346ms step_avg:60.92ms
step:614/2330 train_time:37408ms step_avg:60.93ms
step:615/2330 train_time:37469ms step_avg:60.92ms
step:616/2330 train_time:37531ms step_avg:60.93ms
step:617/2330 train_time:37589ms step_avg:60.92ms
step:618/2330 train_time:37652ms step_avg:60.93ms
step:619/2330 train_time:37712ms step_avg:60.92ms
step:620/2330 train_time:37776ms step_avg:60.93ms
step:621/2330 train_time:37836ms step_avg:60.93ms
step:622/2330 train_time:37897ms step_avg:60.93ms
step:623/2330 train_time:37957ms step_avg:60.93ms
step:624/2330 train_time:38020ms step_avg:60.93ms
step:625/2330 train_time:38080ms step_avg:60.93ms
step:626/2330 train_time:38143ms step_avg:60.93ms
step:627/2330 train_time:38203ms step_avg:60.93ms
step:628/2330 train_time:38266ms step_avg:60.93ms
step:629/2330 train_time:38325ms step_avg:60.93ms
step:630/2330 train_time:38388ms step_avg:60.93ms
step:631/2330 train_time:38447ms step_avg:60.93ms
step:632/2330 train_time:38510ms step_avg:60.93ms
step:633/2330 train_time:38569ms step_avg:60.93ms
step:634/2330 train_time:38631ms step_avg:60.93ms
step:635/2330 train_time:38691ms step_avg:60.93ms
step:636/2330 train_time:38754ms step_avg:60.93ms
step:637/2330 train_time:38814ms step_avg:60.93ms
step:638/2330 train_time:38877ms step_avg:60.94ms
step:639/2330 train_time:38936ms step_avg:60.93ms
step:640/2330 train_time:38998ms step_avg:60.93ms
step:641/2330 train_time:39058ms step_avg:60.93ms
step:642/2330 train_time:39121ms step_avg:60.94ms
step:643/2330 train_time:39181ms step_avg:60.93ms
step:644/2330 train_time:39244ms step_avg:60.94ms
step:645/2330 train_time:39303ms step_avg:60.93ms
step:646/2330 train_time:39365ms step_avg:60.94ms
step:647/2330 train_time:39425ms step_avg:60.94ms
step:648/2330 train_time:39488ms step_avg:60.94ms
step:649/2330 train_time:39547ms step_avg:60.93ms
step:650/2330 train_time:39609ms step_avg:60.94ms
step:651/2330 train_time:39669ms step_avg:60.94ms
step:652/2330 train_time:39732ms step_avg:60.94ms
step:653/2330 train_time:39792ms step_avg:60.94ms
step:654/2330 train_time:39855ms step_avg:60.94ms
step:655/2330 train_time:39915ms step_avg:60.94ms
step:656/2330 train_time:39977ms step_avg:60.94ms
step:657/2330 train_time:40037ms step_avg:60.94ms
step:658/2330 train_time:40100ms step_avg:60.94ms
step:659/2330 train_time:40159ms step_avg:60.94ms
step:660/2330 train_time:40221ms step_avg:60.94ms
step:661/2330 train_time:40281ms step_avg:60.94ms
step:662/2330 train_time:40343ms step_avg:60.94ms
step:663/2330 train_time:40402ms step_avg:60.94ms
step:664/2330 train_time:40466ms step_avg:60.94ms
step:665/2330 train_time:40525ms step_avg:60.94ms
step:666/2330 train_time:40588ms step_avg:60.94ms
step:667/2330 train_time:40648ms step_avg:60.94ms
step:668/2330 train_time:40711ms step_avg:60.94ms
step:669/2330 train_time:40771ms step_avg:60.94ms
step:670/2330 train_time:40833ms step_avg:60.95ms
step:671/2330 train_time:40892ms step_avg:60.94ms
step:672/2330 train_time:40956ms step_avg:60.95ms
step:673/2330 train_time:41015ms step_avg:60.94ms
step:674/2330 train_time:41078ms step_avg:60.95ms
step:675/2330 train_time:41138ms step_avg:60.94ms
step:676/2330 train_time:41200ms step_avg:60.95ms
step:677/2330 train_time:41259ms step_avg:60.94ms
step:678/2330 train_time:41322ms step_avg:60.95ms
step:679/2330 train_time:41382ms step_avg:60.95ms
step:680/2330 train_time:41444ms step_avg:60.95ms
step:681/2330 train_time:41505ms step_avg:60.95ms
step:682/2330 train_time:41568ms step_avg:60.95ms
step:683/2330 train_time:41627ms step_avg:60.95ms
step:684/2330 train_time:41690ms step_avg:60.95ms
step:685/2330 train_time:41749ms step_avg:60.95ms
step:686/2330 train_time:41812ms step_avg:60.95ms
step:687/2330 train_time:41873ms step_avg:60.95ms
step:688/2330 train_time:41935ms step_avg:60.95ms
step:689/2330 train_time:41994ms step_avg:60.95ms
step:690/2330 train_time:42057ms step_avg:60.95ms
step:691/2330 train_time:42115ms step_avg:60.95ms
step:692/2330 train_time:42178ms step_avg:60.95ms
step:693/2330 train_time:42237ms step_avg:60.95ms
step:694/2330 train_time:42300ms step_avg:60.95ms
step:695/2330 train_time:42360ms step_avg:60.95ms
step:696/2330 train_time:42423ms step_avg:60.95ms
step:697/2330 train_time:42483ms step_avg:60.95ms
step:698/2330 train_time:42546ms step_avg:60.95ms
step:699/2330 train_time:42605ms step_avg:60.95ms
step:700/2330 train_time:42669ms step_avg:60.96ms
step:701/2330 train_time:42727ms step_avg:60.95ms
step:702/2330 train_time:42790ms step_avg:60.95ms
step:703/2330 train_time:42849ms step_avg:60.95ms
step:704/2330 train_time:42912ms step_avg:60.95ms
step:705/2330 train_time:42973ms step_avg:60.95ms
step:706/2330 train_time:43036ms step_avg:60.96ms
step:707/2330 train_time:43095ms step_avg:60.95ms
step:708/2330 train_time:43158ms step_avg:60.96ms
step:709/2330 train_time:43217ms step_avg:60.96ms
step:710/2330 train_time:43280ms step_avg:60.96ms
step:711/2330 train_time:43339ms step_avg:60.96ms
step:712/2330 train_time:43402ms step_avg:60.96ms
step:713/2330 train_time:43462ms step_avg:60.96ms
step:714/2330 train_time:43525ms step_avg:60.96ms
step:715/2330 train_time:43585ms step_avg:60.96ms
step:716/2330 train_time:43647ms step_avg:60.96ms
step:717/2330 train_time:43708ms step_avg:60.96ms
step:718/2330 train_time:43771ms step_avg:60.96ms
step:719/2330 train_time:43829ms step_avg:60.96ms
step:720/2330 train_time:43892ms step_avg:60.96ms
step:721/2330 train_time:43952ms step_avg:60.96ms
step:722/2330 train_time:44015ms step_avg:60.96ms
step:723/2330 train_time:44075ms step_avg:60.96ms
step:724/2330 train_time:44137ms step_avg:60.96ms
step:725/2330 train_time:44196ms step_avg:60.96ms
step:726/2330 train_time:44258ms step_avg:60.96ms
step:727/2330 train_time:44317ms step_avg:60.96ms
step:728/2330 train_time:44381ms step_avg:60.96ms
step:729/2330 train_time:44440ms step_avg:60.96ms
step:730/2330 train_time:44504ms step_avg:60.96ms
step:731/2330 train_time:44564ms step_avg:60.96ms
step:732/2330 train_time:44626ms step_avg:60.96ms
step:733/2330 train_time:44685ms step_avg:60.96ms
step:734/2330 train_time:44748ms step_avg:60.97ms
step:735/2330 train_time:44808ms step_avg:60.96ms
step:736/2330 train_time:44871ms step_avg:60.97ms
step:737/2330 train_time:44929ms step_avg:60.96ms
step:738/2330 train_time:44993ms step_avg:60.97ms
step:739/2330 train_time:45052ms step_avg:60.96ms
step:740/2330 train_time:45115ms step_avg:60.97ms
step:741/2330 train_time:45174ms step_avg:60.96ms
step:742/2330 train_time:45237ms step_avg:60.97ms
step:743/2330 train_time:45296ms step_avg:60.96ms
step:744/2330 train_time:45358ms step_avg:60.97ms
step:745/2330 train_time:45418ms step_avg:60.96ms
step:746/2330 train_time:45481ms step_avg:60.97ms
step:747/2330 train_time:45541ms step_avg:60.96ms
step:748/2330 train_time:45604ms step_avg:60.97ms
step:749/2330 train_time:45664ms step_avg:60.97ms
step:750/2330 train_time:45726ms step_avg:60.97ms
step:750/2330 val_loss:4.9098 train_time:45799ms step_avg:61.07ms
step:751/2330 train_time:45821ms step_avg:61.01ms
step:752/2330 train_time:45850ms step_avg:60.97ms
step:753/2330 train_time:45913ms step_avg:60.97ms
step:754/2330 train_time:45981ms step_avg:60.98ms
step:755/2330 train_time:46044ms step_avg:60.98ms
step:756/2330 train_time:46107ms step_avg:60.99ms
step:757/2330 train_time:46166ms step_avg:60.99ms
step:758/2330 train_time:46228ms step_avg:60.99ms
step:759/2330 train_time:46288ms step_avg:60.99ms
step:760/2330 train_time:46350ms step_avg:60.99ms
step:761/2330 train_time:46409ms step_avg:60.98ms
step:762/2330 train_time:46470ms step_avg:60.98ms
step:763/2330 train_time:46529ms step_avg:60.98ms
step:764/2330 train_time:46592ms step_avg:60.98ms
step:765/2330 train_time:46652ms step_avg:60.98ms
step:766/2330 train_time:46714ms step_avg:60.98ms
step:767/2330 train_time:46774ms step_avg:60.98ms
step:768/2330 train_time:46838ms step_avg:60.99ms
step:769/2330 train_time:46900ms step_avg:60.99ms
step:770/2330 train_time:46964ms step_avg:60.99ms
step:771/2330 train_time:47024ms step_avg:60.99ms
step:772/2330 train_time:47088ms step_avg:60.99ms
step:773/2330 train_time:47148ms step_avg:60.99ms
step:774/2330 train_time:47212ms step_avg:61.00ms
step:775/2330 train_time:47272ms step_avg:61.00ms
step:776/2330 train_time:47335ms step_avg:61.00ms
step:777/2330 train_time:47395ms step_avg:61.00ms
step:778/2330 train_time:47457ms step_avg:61.00ms
step:779/2330 train_time:47517ms step_avg:61.00ms
step:780/2330 train_time:47580ms step_avg:61.00ms
step:781/2330 train_time:47640ms step_avg:61.00ms
step:782/2330 train_time:47704ms step_avg:61.00ms
step:783/2330 train_time:47763ms step_avg:61.00ms
step:784/2330 train_time:47827ms step_avg:61.00ms
step:785/2330 train_time:47887ms step_avg:61.00ms
step:786/2330 train_time:47952ms step_avg:61.01ms
step:787/2330 train_time:48014ms step_avg:61.01ms
step:788/2330 train_time:48078ms step_avg:61.01ms
step:789/2330 train_time:48138ms step_avg:61.01ms
step:790/2330 train_time:48201ms step_avg:61.01ms
step:791/2330 train_time:48262ms step_avg:61.01ms
step:792/2330 train_time:48325ms step_avg:61.02ms
step:793/2330 train_time:48385ms step_avg:61.01ms
step:794/2330 train_time:48448ms step_avg:61.02ms
step:795/2330 train_time:48509ms step_avg:61.02ms
step:796/2330 train_time:48572ms step_avg:61.02ms
step:797/2330 train_time:48633ms step_avg:61.02ms
step:798/2330 train_time:48697ms step_avg:61.02ms
step:799/2330 train_time:48756ms step_avg:61.02ms
step:800/2330 train_time:48819ms step_avg:61.02ms
step:801/2330 train_time:48880ms step_avg:61.02ms
step:802/2330 train_time:48942ms step_avg:61.03ms
step:803/2330 train_time:49003ms step_avg:61.02ms
step:804/2330 train_time:49066ms step_avg:61.03ms
step:805/2330 train_time:49126ms step_avg:61.03ms
step:806/2330 train_time:49190ms step_avg:61.03ms
step:807/2330 train_time:49250ms step_avg:61.03ms
step:808/2330 train_time:49314ms step_avg:61.03ms
step:809/2330 train_time:49374ms step_avg:61.03ms
step:810/2330 train_time:49438ms step_avg:61.03ms
step:811/2330 train_time:49499ms step_avg:61.03ms
step:812/2330 train_time:49561ms step_avg:61.04ms
step:813/2330 train_time:49620ms step_avg:61.03ms
step:814/2330 train_time:49683ms step_avg:61.04ms
step:815/2330 train_time:49743ms step_avg:61.03ms
step:816/2330 train_time:49807ms step_avg:61.04ms
step:817/2330 train_time:49867ms step_avg:61.04ms
step:818/2330 train_time:49931ms step_avg:61.04ms
step:819/2330 train_time:49993ms step_avg:61.04ms
step:820/2330 train_time:50056ms step_avg:61.04ms
step:821/2330 train_time:50116ms step_avg:61.04ms
step:822/2330 train_time:50179ms step_avg:61.04ms
step:823/2330 train_time:50239ms step_avg:61.04ms
step:824/2330 train_time:50302ms step_avg:61.05ms
step:825/2330 train_time:50362ms step_avg:61.04ms
step:826/2330 train_time:50425ms step_avg:61.05ms
step:827/2330 train_time:50485ms step_avg:61.05ms
step:828/2330 train_time:50548ms step_avg:61.05ms
step:829/2330 train_time:50608ms step_avg:61.05ms
step:830/2330 train_time:50672ms step_avg:61.05ms
step:831/2330 train_time:50733ms step_avg:61.05ms
step:832/2330 train_time:50798ms step_avg:61.05ms
step:833/2330 train_time:50857ms step_avg:61.05ms
step:834/2330 train_time:50919ms step_avg:61.05ms
step:835/2330 train_time:50980ms step_avg:61.05ms
step:836/2330 train_time:51043ms step_avg:61.06ms
step:837/2330 train_time:51104ms step_avg:61.06ms
step:838/2330 train_time:51168ms step_avg:61.06ms
step:839/2330 train_time:51228ms step_avg:61.06ms
step:840/2330 train_time:51293ms step_avg:61.06ms
step:841/2330 train_time:51354ms step_avg:61.06ms
step:842/2330 train_time:51417ms step_avg:61.07ms
step:843/2330 train_time:51478ms step_avg:61.06ms
step:844/2330 train_time:51541ms step_avg:61.07ms
step:845/2330 train_time:51601ms step_avg:61.07ms
step:846/2330 train_time:51664ms step_avg:61.07ms
step:847/2330 train_time:51725ms step_avg:61.07ms
step:848/2330 train_time:51788ms step_avg:61.07ms
step:849/2330 train_time:51849ms step_avg:61.07ms
step:850/2330 train_time:51913ms step_avg:61.07ms
step:851/2330 train_time:51973ms step_avg:61.07ms
step:852/2330 train_time:52037ms step_avg:61.08ms
step:853/2330 train_time:52098ms step_avg:61.08ms
step:854/2330 train_time:52160ms step_avg:61.08ms
step:855/2330 train_time:52221ms step_avg:61.08ms
step:856/2330 train_time:52284ms step_avg:61.08ms
step:857/2330 train_time:52345ms step_avg:61.08ms
step:858/2330 train_time:52408ms step_avg:61.08ms
step:859/2330 train_time:52469ms step_avg:61.08ms
step:860/2330 train_time:52533ms step_avg:61.09ms
step:861/2330 train_time:52596ms step_avg:61.09ms
step:862/2330 train_time:52658ms step_avg:61.09ms
step:863/2330 train_time:52718ms step_avg:61.09ms
step:864/2330 train_time:52781ms step_avg:61.09ms
step:865/2330 train_time:52840ms step_avg:61.09ms
step:866/2330 train_time:52904ms step_avg:61.09ms
step:867/2330 train_time:52965ms step_avg:61.09ms
step:868/2330 train_time:53028ms step_avg:61.09ms
step:869/2330 train_time:53089ms step_avg:61.09ms
step:870/2330 train_time:53152ms step_avg:61.09ms
step:871/2330 train_time:53214ms step_avg:61.09ms
step:872/2330 train_time:53278ms step_avg:61.10ms
step:873/2330 train_time:53338ms step_avg:61.10ms
step:874/2330 train_time:53401ms step_avg:61.10ms
step:875/2330 train_time:53461ms step_avg:61.10ms
step:876/2330 train_time:53524ms step_avg:61.10ms
step:877/2330 train_time:53584ms step_avg:61.10ms
step:878/2330 train_time:53648ms step_avg:61.10ms
step:879/2330 train_time:53708ms step_avg:61.10ms
step:880/2330 train_time:53772ms step_avg:61.10ms
step:881/2330 train_time:53833ms step_avg:61.10ms
step:882/2330 train_time:53897ms step_avg:61.11ms
step:883/2330 train_time:53957ms step_avg:61.11ms
step:884/2330 train_time:54019ms step_avg:61.11ms
step:885/2330 train_time:54079ms step_avg:61.11ms
step:886/2330 train_time:54143ms step_avg:61.11ms
step:887/2330 train_time:54203ms step_avg:61.11ms
step:888/2330 train_time:54267ms step_avg:61.11ms
step:889/2330 train_time:54328ms step_avg:61.11ms
step:890/2330 train_time:54392ms step_avg:61.11ms
step:891/2330 train_time:54453ms step_avg:61.11ms
step:892/2330 train_time:54517ms step_avg:61.12ms
step:893/2330 train_time:54577ms step_avg:61.12ms
step:894/2330 train_time:54641ms step_avg:61.12ms
step:895/2330 train_time:54700ms step_avg:61.12ms
step:896/2330 train_time:54763ms step_avg:61.12ms
step:897/2330 train_time:54823ms step_avg:61.12ms
step:898/2330 train_time:54886ms step_avg:61.12ms
step:899/2330 train_time:54947ms step_avg:61.12ms
step:900/2330 train_time:55011ms step_avg:61.12ms
step:901/2330 train_time:55071ms step_avg:61.12ms
step:902/2330 train_time:55135ms step_avg:61.13ms
step:903/2330 train_time:55196ms step_avg:61.13ms
step:904/2330 train_time:55259ms step_avg:61.13ms
step:905/2330 train_time:55318ms step_avg:61.12ms
step:906/2330 train_time:55381ms step_avg:61.13ms
step:907/2330 train_time:55441ms step_avg:61.13ms
step:908/2330 train_time:55506ms step_avg:61.13ms
step:909/2330 train_time:55565ms step_avg:61.13ms
step:910/2330 train_time:55628ms step_avg:61.13ms
step:911/2330 train_time:55690ms step_avg:61.13ms
step:912/2330 train_time:55753ms step_avg:61.13ms
step:913/2330 train_time:55814ms step_avg:61.13ms
step:914/2330 train_time:55877ms step_avg:61.13ms
step:915/2330 train_time:55937ms step_avg:61.13ms
step:916/2330 train_time:56001ms step_avg:61.14ms
step:917/2330 train_time:56060ms step_avg:61.13ms
step:918/2330 train_time:56123ms step_avg:61.14ms
step:919/2330 train_time:56184ms step_avg:61.14ms
step:920/2330 train_time:56247ms step_avg:61.14ms
step:921/2330 train_time:56308ms step_avg:61.14ms
step:922/2330 train_time:56372ms step_avg:61.14ms
step:923/2330 train_time:56433ms step_avg:61.14ms
step:924/2330 train_time:56496ms step_avg:61.14ms
step:925/2330 train_time:56556ms step_avg:61.14ms
step:926/2330 train_time:56619ms step_avg:61.14ms
step:927/2330 train_time:56679ms step_avg:61.14ms
step:928/2330 train_time:56742ms step_avg:61.14ms
step:929/2330 train_time:56803ms step_avg:61.14ms
step:930/2330 train_time:56865ms step_avg:61.15ms
step:931/2330 train_time:56925ms step_avg:61.14ms
step:932/2330 train_time:56989ms step_avg:61.15ms
step:933/2330 train_time:57049ms step_avg:61.15ms
step:934/2330 train_time:57114ms step_avg:61.15ms
step:935/2330 train_time:57174ms step_avg:61.15ms
step:936/2330 train_time:57237ms step_avg:61.15ms
step:937/2330 train_time:57298ms step_avg:61.15ms
step:938/2330 train_time:57361ms step_avg:61.15ms
step:939/2330 train_time:57420ms step_avg:61.15ms
step:940/2330 train_time:57483ms step_avg:61.15ms
step:941/2330 train_time:57544ms step_avg:61.15ms
step:942/2330 train_time:57607ms step_avg:61.15ms
step:943/2330 train_time:57668ms step_avg:61.15ms
step:944/2330 train_time:57731ms step_avg:61.16ms
step:945/2330 train_time:57791ms step_avg:61.15ms
step:946/2330 train_time:57855ms step_avg:61.16ms
step:947/2330 train_time:57916ms step_avg:61.16ms
step:948/2330 train_time:57979ms step_avg:61.16ms
step:949/2330 train_time:58039ms step_avg:61.16ms
step:950/2330 train_time:58103ms step_avg:61.16ms
step:951/2330 train_time:58162ms step_avg:61.16ms
step:952/2330 train_time:58226ms step_avg:61.16ms
step:953/2330 train_time:58286ms step_avg:61.16ms
step:954/2330 train_time:58350ms step_avg:61.16ms
step:955/2330 train_time:58411ms step_avg:61.16ms
step:956/2330 train_time:58476ms step_avg:61.17ms
step:957/2330 train_time:58536ms step_avg:61.17ms
step:958/2330 train_time:58600ms step_avg:61.17ms
step:959/2330 train_time:58659ms step_avg:61.17ms
step:960/2330 train_time:58722ms step_avg:61.17ms
step:961/2330 train_time:58781ms step_avg:61.17ms
step:962/2330 train_time:58844ms step_avg:61.17ms
step:963/2330 train_time:58905ms step_avg:61.17ms
step:964/2330 train_time:58969ms step_avg:61.17ms
step:965/2330 train_time:59029ms step_avg:61.17ms
step:966/2330 train_time:59095ms step_avg:61.17ms
step:967/2330 train_time:59155ms step_avg:61.17ms
step:968/2330 train_time:59218ms step_avg:61.18ms
step:969/2330 train_time:59278ms step_avg:61.17ms
step:970/2330 train_time:59341ms step_avg:61.18ms
step:971/2330 train_time:59401ms step_avg:61.18ms
step:972/2330 train_time:59465ms step_avg:61.18ms
step:973/2330 train_time:59525ms step_avg:61.18ms
step:974/2330 train_time:59588ms step_avg:61.18ms
step:975/2330 train_time:59649ms step_avg:61.18ms
step:976/2330 train_time:59713ms step_avg:61.18ms
step:977/2330 train_time:59774ms step_avg:61.18ms
step:978/2330 train_time:59838ms step_avg:61.18ms
step:979/2330 train_time:59898ms step_avg:61.18ms
step:980/2330 train_time:59961ms step_avg:61.18ms
step:981/2330 train_time:60021ms step_avg:61.18ms
step:982/2330 train_time:60084ms step_avg:61.19ms
step:983/2330 train_time:60144ms step_avg:61.18ms
step:984/2330 train_time:60207ms step_avg:61.19ms
step:985/2330 train_time:60267ms step_avg:61.19ms
step:986/2330 train_time:60331ms step_avg:61.19ms
step:987/2330 train_time:60392ms step_avg:61.19ms
step:988/2330 train_time:60456ms step_avg:61.19ms
step:989/2330 train_time:60516ms step_avg:61.19ms
step:990/2330 train_time:60579ms step_avg:61.19ms
step:991/2330 train_time:60639ms step_avg:61.19ms
step:992/2330 train_time:60702ms step_avg:61.19ms
step:993/2330 train_time:60761ms step_avg:61.19ms
step:994/2330 train_time:60826ms step_avg:61.19ms
step:995/2330 train_time:60885ms step_avg:61.19ms
step:996/2330 train_time:60949ms step_avg:61.19ms
step:997/2330 train_time:61009ms step_avg:61.19ms
step:998/2330 train_time:61072ms step_avg:61.19ms
step:999/2330 train_time:61134ms step_avg:61.19ms
step:1000/2330 train_time:61197ms step_avg:61.20ms
step:1000/2330 val_loss:4.7371 train_time:61269ms step_avg:61.27ms
step:1001/2330 train_time:61292ms step_avg:61.23ms
step:1002/2330 train_time:61321ms step_avg:61.20ms
step:1003/2330 train_time:61385ms step_avg:61.20ms
step:1004/2330 train_time:61455ms step_avg:61.21ms
step:1005/2330 train_time:61517ms step_avg:61.21ms
step:1006/2330 train_time:61581ms step_avg:61.21ms
step:1007/2330 train_time:61640ms step_avg:61.21ms
step:1008/2330 train_time:61703ms step_avg:61.21ms
step:1009/2330 train_time:61763ms step_avg:61.21ms
step:1010/2330 train_time:61825ms step_avg:61.21ms
step:1011/2330 train_time:61884ms step_avg:61.21ms
step:1012/2330 train_time:61947ms step_avg:61.21ms
step:1013/2330 train_time:62005ms step_avg:61.21ms
step:1014/2330 train_time:62068ms step_avg:61.21ms
step:1015/2330 train_time:62128ms step_avg:61.21ms
step:1016/2330 train_time:62189ms step_avg:61.21ms
step:1017/2330 train_time:62250ms step_avg:61.21ms
step:1018/2330 train_time:62316ms step_avg:61.21ms
step:1019/2330 train_time:62379ms step_avg:61.22ms
step:1020/2330 train_time:62443ms step_avg:61.22ms
step:1021/2330 train_time:62504ms step_avg:61.22ms
step:1022/2330 train_time:62566ms step_avg:61.22ms
step:1023/2330 train_time:62626ms step_avg:61.22ms
step:1024/2330 train_time:62688ms step_avg:61.22ms
step:1025/2330 train_time:62748ms step_avg:61.22ms
step:1026/2330 train_time:62811ms step_avg:61.22ms
step:1027/2330 train_time:62870ms step_avg:61.22ms
step:1028/2330 train_time:62933ms step_avg:61.22ms
step:1029/2330 train_time:62991ms step_avg:61.22ms
step:1030/2330 train_time:63054ms step_avg:61.22ms
step:1031/2330 train_time:63114ms step_avg:61.22ms
step:1032/2330 train_time:63178ms step_avg:61.22ms
step:1033/2330 train_time:63239ms step_avg:61.22ms
step:1034/2330 train_time:63302ms step_avg:61.22ms
step:1035/2330 train_time:63363ms step_avg:61.22ms
step:1036/2330 train_time:63427ms step_avg:61.22ms
step:1037/2330 train_time:63487ms step_avg:61.22ms
step:1038/2330 train_time:63550ms step_avg:61.22ms
step:1039/2330 train_time:63612ms step_avg:61.22ms
step:1040/2330 train_time:63675ms step_avg:61.23ms
step:1041/2330 train_time:63735ms step_avg:61.22ms
step:1042/2330 train_time:63798ms step_avg:61.23ms
step:1043/2330 train_time:63859ms step_avg:61.23ms
step:1044/2330 train_time:63922ms step_avg:61.23ms
step:1045/2330 train_time:63982ms step_avg:61.23ms
step:1046/2330 train_time:64045ms step_avg:61.23ms
step:1047/2330 train_time:64104ms step_avg:61.23ms
step:1048/2330 train_time:64167ms step_avg:61.23ms
step:1049/2330 train_time:64227ms step_avg:61.23ms
step:1050/2330 train_time:64290ms step_avg:61.23ms
step:1051/2330 train_time:64350ms step_avg:61.23ms
step:1052/2330 train_time:64413ms step_avg:61.23ms
step:1053/2330 train_time:64473ms step_avg:61.23ms
step:1054/2330 train_time:64538ms step_avg:61.23ms
step:1055/2330 train_time:64598ms step_avg:61.23ms
step:1056/2330 train_time:64662ms step_avg:61.23ms
step:1057/2330 train_time:64722ms step_avg:61.23ms
step:1058/2330 train_time:64785ms step_avg:61.23ms
step:1059/2330 train_time:64845ms step_avg:61.23ms
step:1060/2330 train_time:64908ms step_avg:61.23ms
step:1061/2330 train_time:64967ms step_avg:61.23ms
step:1062/2330 train_time:65031ms step_avg:61.23ms
step:1063/2330 train_time:65090ms step_avg:61.23ms
step:1064/2330 train_time:65153ms step_avg:61.23ms
step:1065/2330 train_time:65213ms step_avg:61.23ms
step:1066/2330 train_time:65277ms step_avg:61.24ms
step:1067/2330 train_time:65337ms step_avg:61.23ms
step:1068/2330 train_time:65401ms step_avg:61.24ms
step:1069/2330 train_time:65462ms step_avg:61.24ms
step:1070/2330 train_time:65525ms step_avg:61.24ms
step:1071/2330 train_time:65584ms step_avg:61.24ms
step:1072/2330 train_time:65647ms step_avg:61.24ms
step:1073/2330 train_time:65707ms step_avg:61.24ms
step:1074/2330 train_time:65770ms step_avg:61.24ms
step:1075/2330 train_time:65832ms step_avg:61.24ms
step:1076/2330 train_time:65895ms step_avg:61.24ms
step:1077/2330 train_time:65954ms step_avg:61.24ms
step:1078/2330 train_time:66018ms step_avg:61.24ms
step:1079/2330 train_time:66079ms step_avg:61.24ms
step:1080/2330 train_time:66142ms step_avg:61.24ms
step:1081/2330 train_time:66202ms step_avg:61.24ms
step:1082/2330 train_time:66265ms step_avg:61.24ms
step:1083/2330 train_time:66326ms step_avg:61.24ms
step:1084/2330 train_time:66388ms step_avg:61.24ms
step:1085/2330 train_time:66447ms step_avg:61.24ms
step:1086/2330 train_time:66511ms step_avg:61.24ms
step:1087/2330 train_time:66572ms step_avg:61.24ms
step:1088/2330 train_time:66635ms step_avg:61.25ms
step:1089/2330 train_time:66695ms step_avg:61.24ms
step:1090/2330 train_time:66759ms step_avg:61.25ms
step:1091/2330 train_time:66820ms step_avg:61.25ms
step:1092/2330 train_time:66884ms step_avg:61.25ms
step:1093/2330 train_time:66943ms step_avg:61.25ms
step:1094/2330 train_time:67006ms step_avg:61.25ms
step:1095/2330 train_time:67066ms step_avg:61.25ms
step:1096/2330 train_time:67129ms step_avg:61.25ms
step:1097/2330 train_time:67188ms step_avg:61.25ms
step:1098/2330 train_time:67252ms step_avg:61.25ms
step:1099/2330 train_time:67312ms step_avg:61.25ms
step:1100/2330 train_time:67375ms step_avg:61.25ms
step:1101/2330 train_time:67436ms step_avg:61.25ms
step:1102/2330 train_time:67499ms step_avg:61.25ms
step:1103/2330 train_time:67560ms step_avg:61.25ms
step:1104/2330 train_time:67624ms step_avg:61.25ms
step:1105/2330 train_time:67683ms step_avg:61.25ms
step:1106/2330 train_time:67746ms step_avg:61.25ms
step:1107/2330 train_time:67807ms step_avg:61.25ms
step:1108/2330 train_time:67868ms step_avg:61.25ms
step:1109/2330 train_time:67929ms step_avg:61.25ms
step:1110/2330 train_time:67992ms step_avg:61.25ms
step:1111/2330 train_time:68052ms step_avg:61.25ms
step:1112/2330 train_time:68115ms step_avg:61.25ms
step:1113/2330 train_time:68175ms step_avg:61.25ms
step:1114/2330 train_time:68238ms step_avg:61.26ms
step:1115/2330 train_time:68299ms step_avg:61.25ms
step:1116/2330 train_time:68363ms step_avg:61.26ms
step:1117/2330 train_time:68424ms step_avg:61.26ms
step:1118/2330 train_time:68486ms step_avg:61.26ms
step:1119/2330 train_time:68546ms step_avg:61.26ms
step:1120/2330 train_time:68610ms step_avg:61.26ms
step:1121/2330 train_time:68670ms step_avg:61.26ms
step:1122/2330 train_time:68733ms step_avg:61.26ms
step:1123/2330 train_time:68793ms step_avg:61.26ms
step:1124/2330 train_time:68857ms step_avg:61.26ms
step:1125/2330 train_time:68918ms step_avg:61.26ms
step:1126/2330 train_time:68982ms step_avg:61.26ms
step:1127/2330 train_time:69043ms step_avg:61.26ms
step:1128/2330 train_time:69106ms step_avg:61.26ms
step:1129/2330 train_time:69166ms step_avg:61.26ms
step:1130/2330 train_time:69230ms step_avg:61.27ms
step:1131/2330 train_time:69289ms step_avg:61.26ms
step:1132/2330 train_time:69352ms step_avg:61.27ms
step:1133/2330 train_time:69412ms step_avg:61.26ms
step:1134/2330 train_time:69476ms step_avg:61.27ms
step:1135/2330 train_time:69536ms step_avg:61.27ms
step:1136/2330 train_time:69600ms step_avg:61.27ms
step:1137/2330 train_time:69661ms step_avg:61.27ms
step:1138/2330 train_time:69726ms step_avg:61.27ms
step:1139/2330 train_time:69785ms step_avg:61.27ms
step:1140/2330 train_time:69848ms step_avg:61.27ms
step:1141/2330 train_time:69908ms step_avg:61.27ms
step:1142/2330 train_time:69971ms step_avg:61.27ms
step:1143/2330 train_time:70031ms step_avg:61.27ms
step:1144/2330 train_time:70094ms step_avg:61.27ms
step:1145/2330 train_time:70155ms step_avg:61.27ms
step:1146/2330 train_time:70218ms step_avg:61.27ms
step:1147/2330 train_time:70280ms step_avg:61.27ms
step:1148/2330 train_time:70343ms step_avg:61.27ms
step:1149/2330 train_time:70403ms step_avg:61.27ms
step:1150/2330 train_time:70465ms step_avg:61.27ms
step:1151/2330 train_time:70526ms step_avg:61.27ms
step:1152/2330 train_time:70589ms step_avg:61.28ms
step:1153/2330 train_time:70650ms step_avg:61.27ms
step:1154/2330 train_time:70713ms step_avg:61.28ms
step:1155/2330 train_time:70774ms step_avg:61.28ms
step:1156/2330 train_time:70837ms step_avg:61.28ms
step:1157/2330 train_time:70898ms step_avg:61.28ms
step:1158/2330 train_time:70962ms step_avg:61.28ms
step:1159/2330 train_time:71024ms step_avg:61.28ms
step:1160/2330 train_time:71086ms step_avg:61.28ms
step:1161/2330 train_time:71146ms step_avg:61.28ms
step:1162/2330 train_time:71209ms step_avg:61.28ms
step:1163/2330 train_time:71268ms step_avg:61.28ms
step:1164/2330 train_time:71332ms step_avg:61.28ms
step:1165/2330 train_time:71391ms step_avg:61.28ms
step:1166/2330 train_time:71455ms step_avg:61.28ms
step:1167/2330 train_time:71515ms step_avg:61.28ms
step:1168/2330 train_time:71579ms step_avg:61.28ms
step:1169/2330 train_time:71640ms step_avg:61.28ms
step:1170/2330 train_time:71703ms step_avg:61.28ms
step:1171/2330 train_time:71763ms step_avg:61.28ms
step:1172/2330 train_time:71826ms step_avg:61.29ms
step:1173/2330 train_time:71885ms step_avg:61.28ms
step:1174/2330 train_time:71948ms step_avg:61.28ms
step:1175/2330 train_time:72009ms step_avg:61.28ms
step:1176/2330 train_time:72071ms step_avg:61.29ms
step:1177/2330 train_time:72132ms step_avg:61.28ms
step:1178/2330 train_time:72195ms step_avg:61.29ms
step:1179/2330 train_time:72255ms step_avg:61.29ms
step:1180/2330 train_time:72318ms step_avg:61.29ms
step:1181/2330 train_time:72380ms step_avg:61.29ms
step:1182/2330 train_time:72443ms step_avg:61.29ms
step:1183/2330 train_time:72503ms step_avg:61.29ms
step:1184/2330 train_time:72566ms step_avg:61.29ms
step:1185/2330 train_time:72628ms step_avg:61.29ms
step:1186/2330 train_time:72689ms step_avg:61.29ms
step:1187/2330 train_time:72749ms step_avg:61.29ms
step:1188/2330 train_time:72813ms step_avg:61.29ms
step:1189/2330 train_time:72873ms step_avg:61.29ms
step:1190/2330 train_time:72936ms step_avg:61.29ms
step:1191/2330 train_time:72996ms step_avg:61.29ms
step:1192/2330 train_time:73060ms step_avg:61.29ms
step:1193/2330 train_time:73121ms step_avg:61.29ms
step:1194/2330 train_time:73184ms step_avg:61.29ms
step:1195/2330 train_time:73244ms step_avg:61.29ms
step:1196/2330 train_time:73307ms step_avg:61.29ms
step:1197/2330 train_time:73367ms step_avg:61.29ms
step:1198/2330 train_time:73430ms step_avg:61.29ms
step:1199/2330 train_time:73489ms step_avg:61.29ms
step:1200/2330 train_time:73553ms step_avg:61.29ms
step:1201/2330 train_time:73613ms step_avg:61.29ms
step:1202/2330 train_time:73677ms step_avg:61.30ms
step:1203/2330 train_time:73738ms step_avg:61.30ms
step:1204/2330 train_time:73802ms step_avg:61.30ms
step:1205/2330 train_time:73863ms step_avg:61.30ms
step:1206/2330 train_time:73928ms step_avg:61.30ms
step:1207/2330 train_time:73987ms step_avg:61.30ms
step:1208/2330 train_time:74049ms step_avg:61.30ms
step:1209/2330 train_time:74109ms step_avg:61.30ms
step:1210/2330 train_time:74172ms step_avg:61.30ms
step:1211/2330 train_time:74232ms step_avg:61.30ms
step:1212/2330 train_time:74296ms step_avg:61.30ms
step:1213/2330 train_time:74357ms step_avg:61.30ms
step:1214/2330 train_time:74422ms step_avg:61.30ms
step:1215/2330 train_time:74482ms step_avg:61.30ms
step:1216/2330 train_time:74545ms step_avg:61.30ms
step:1217/2330 train_time:74606ms step_avg:61.30ms
step:1218/2330 train_time:74668ms step_avg:61.30ms
step:1219/2330 train_time:74729ms step_avg:61.30ms
step:1220/2330 train_time:74793ms step_avg:61.31ms
step:1221/2330 train_time:74853ms step_avg:61.30ms
step:1222/2330 train_time:74917ms step_avg:61.31ms
step:1223/2330 train_time:74978ms step_avg:61.31ms
step:1224/2330 train_time:75042ms step_avg:61.31ms
step:1225/2330 train_time:75103ms step_avg:61.31ms
step:1226/2330 train_time:75166ms step_avg:61.31ms
step:1227/2330 train_time:75227ms step_avg:61.31ms
step:1228/2330 train_time:75289ms step_avg:61.31ms
step:1229/2330 train_time:75349ms step_avg:61.31ms
step:1230/2330 train_time:75412ms step_avg:61.31ms
step:1231/2330 train_time:75472ms step_avg:61.31ms
step:1232/2330 train_time:75536ms step_avg:61.31ms
step:1233/2330 train_time:75596ms step_avg:61.31ms
step:1234/2330 train_time:75661ms step_avg:61.31ms
step:1235/2330 train_time:75723ms step_avg:61.31ms
step:1236/2330 train_time:75785ms step_avg:61.31ms
step:1237/2330 train_time:75845ms step_avg:61.31ms
step:1238/2330 train_time:75908ms step_avg:61.32ms
step:1239/2330 train_time:75968ms step_avg:61.31ms
step:1240/2330 train_time:76032ms step_avg:61.32ms
step:1241/2330 train_time:76090ms step_avg:61.31ms
step:1242/2330 train_time:76154ms step_avg:61.32ms
step:1243/2330 train_time:76214ms step_avg:61.31ms
step:1244/2330 train_time:76277ms step_avg:61.32ms
step:1245/2330 train_time:76338ms step_avg:61.32ms
step:1246/2330 train_time:76402ms step_avg:61.32ms
step:1247/2330 train_time:76463ms step_avg:61.32ms
step:1248/2330 train_time:76525ms step_avg:61.32ms
step:1249/2330 train_time:76586ms step_avg:61.32ms
step:1250/2330 train_time:76648ms step_avg:61.32ms
step:1250/2330 val_loss:4.5964 train_time:76721ms step_avg:61.38ms
step:1251/2330 train_time:76743ms step_avg:61.35ms
step:1252/2330 train_time:76774ms step_avg:61.32ms
step:1253/2330 train_time:76834ms step_avg:61.32ms
step:1254/2330 train_time:76903ms step_avg:61.33ms
step:1255/2330 train_time:76967ms step_avg:61.33ms
step:1256/2330 train_time:77029ms step_avg:61.33ms
step:1257/2330 train_time:77089ms step_avg:61.33ms
step:1258/2330 train_time:77152ms step_avg:61.33ms
step:1259/2330 train_time:77212ms step_avg:61.33ms
step:1260/2330 train_time:77275ms step_avg:61.33ms
step:1261/2330 train_time:77334ms step_avg:61.33ms
step:1262/2330 train_time:77396ms step_avg:61.33ms
step:1263/2330 train_time:77456ms step_avg:61.33ms
step:1264/2330 train_time:77518ms step_avg:61.33ms
step:1265/2330 train_time:77577ms step_avg:61.33ms
step:1266/2330 train_time:77639ms step_avg:61.33ms
step:1267/2330 train_time:77699ms step_avg:61.33ms
step:1268/2330 train_time:77764ms step_avg:61.33ms
step:1269/2330 train_time:77827ms step_avg:61.33ms
step:1270/2330 train_time:77892ms step_avg:61.33ms
step:1271/2330 train_time:77954ms step_avg:61.33ms
step:1272/2330 train_time:78018ms step_avg:61.33ms
step:1273/2330 train_time:78079ms step_avg:61.34ms
step:1274/2330 train_time:78143ms step_avg:61.34ms
step:1275/2330 train_time:78204ms step_avg:61.34ms
step:1276/2330 train_time:78267ms step_avg:61.34ms
step:1277/2330 train_time:78327ms step_avg:61.34ms
step:1278/2330 train_time:78390ms step_avg:61.34ms
step:1279/2330 train_time:78450ms step_avg:61.34ms
step:1280/2330 train_time:78512ms step_avg:61.34ms
step:1281/2330 train_time:78572ms step_avg:61.34ms
step:1282/2330 train_time:78635ms step_avg:61.34ms
step:1283/2330 train_time:78695ms step_avg:61.34ms
step:1284/2330 train_time:78759ms step_avg:61.34ms
step:1285/2330 train_time:78819ms step_avg:61.34ms
step:1286/2330 train_time:78885ms step_avg:61.34ms
step:1287/2330 train_time:78948ms step_avg:61.34ms
step:1288/2330 train_time:79012ms step_avg:61.34ms
step:1289/2330 train_time:79072ms step_avg:61.34ms
step:1290/2330 train_time:79136ms step_avg:61.35ms
step:1291/2330 train_time:79196ms step_avg:61.34ms
step:1292/2330 train_time:79259ms step_avg:61.35ms
step:1293/2330 train_time:79319ms step_avg:61.35ms
step:1294/2330 train_time:79383ms step_avg:61.35ms
step:1295/2330 train_time:79444ms step_avg:61.35ms
step:1296/2330 train_time:79507ms step_avg:61.35ms
step:1297/2330 train_time:79567ms step_avg:61.35ms
step:1298/2330 train_time:79630ms step_avg:61.35ms
step:1299/2330 train_time:79690ms step_avg:61.35ms
step:1300/2330 train_time:79754ms step_avg:61.35ms
step:1301/2330 train_time:79814ms step_avg:61.35ms
step:1302/2330 train_time:79877ms step_avg:61.35ms
step:1303/2330 train_time:79939ms step_avg:61.35ms
step:1304/2330 train_time:80002ms step_avg:61.35ms
step:1305/2330 train_time:80063ms step_avg:61.35ms
step:1306/2330 train_time:80127ms step_avg:61.35ms
step:1307/2330 train_time:80187ms step_avg:61.35ms
step:1308/2330 train_time:80250ms step_avg:61.35ms
step:1309/2330 train_time:80309ms step_avg:61.35ms
step:1310/2330 train_time:80373ms step_avg:61.35ms
step:1311/2330 train_time:80433ms step_avg:61.35ms
step:1312/2330 train_time:80495ms step_avg:61.35ms
step:1313/2330 train_time:80555ms step_avg:61.35ms
step:1314/2330 train_time:80619ms step_avg:61.35ms
step:1315/2330 train_time:80679ms step_avg:61.35ms
step:1316/2330 train_time:80742ms step_avg:61.35ms
step:1317/2330 train_time:80804ms step_avg:61.35ms
step:1318/2330 train_time:80867ms step_avg:61.36ms
step:1319/2330 train_time:80926ms step_avg:61.35ms
step:1320/2330 train_time:80990ms step_avg:61.36ms
step:1321/2330 train_time:81051ms step_avg:61.36ms
step:1322/2330 train_time:81114ms step_avg:61.36ms
step:1323/2330 train_time:81174ms step_avg:61.36ms
step:1324/2330 train_time:81237ms step_avg:61.36ms
step:1325/2330 train_time:81298ms step_avg:61.36ms
step:1326/2330 train_time:81362ms step_avg:61.36ms
step:1327/2330 train_time:81423ms step_avg:61.36ms
step:1328/2330 train_time:81486ms step_avg:61.36ms
step:1329/2330 train_time:81548ms step_avg:61.36ms
step:1330/2330 train_time:81610ms step_avg:61.36ms
step:1331/2330 train_time:81669ms step_avg:61.36ms
step:1332/2330 train_time:81733ms step_avg:61.36ms
step:1333/2330 train_time:81792ms step_avg:61.36ms
step:1334/2330 train_time:81856ms step_avg:61.36ms
step:1335/2330 train_time:81915ms step_avg:61.36ms
step:1336/2330 train_time:81980ms step_avg:61.36ms
step:1337/2330 train_time:82041ms step_avg:61.36ms
step:1338/2330 train_time:82105ms step_avg:61.36ms
step:1339/2330 train_time:82167ms step_avg:61.36ms
step:1340/2330 train_time:82230ms step_avg:61.37ms
step:1341/2330 train_time:82290ms step_avg:61.36ms
step:1342/2330 train_time:82353ms step_avg:61.37ms
step:1343/2330 train_time:82414ms step_avg:61.37ms
step:1344/2330 train_time:82477ms step_avg:61.37ms
step:1345/2330 train_time:82537ms step_avg:61.37ms
step:1346/2330 train_time:82601ms step_avg:61.37ms
step:1347/2330 train_time:82662ms step_avg:61.37ms
step:1348/2330 train_time:82726ms step_avg:61.37ms
step:1349/2330 train_time:82786ms step_avg:61.37ms
step:1350/2330 train_time:82851ms step_avg:61.37ms
step:1351/2330 train_time:82910ms step_avg:61.37ms
step:1352/2330 train_time:82973ms step_avg:61.37ms
step:1353/2330 train_time:83033ms step_avg:61.37ms
step:1354/2330 train_time:83098ms step_avg:61.37ms
step:1355/2330 train_time:83158ms step_avg:61.37ms
step:1356/2330 train_time:83221ms step_avg:61.37ms
step:1357/2330 train_time:83282ms step_avg:61.37ms
step:1358/2330 train_time:83347ms step_avg:61.37ms
step:1359/2330 train_time:83407ms step_avg:61.37ms
step:1360/2330 train_time:83470ms step_avg:61.38ms
step:1361/2330 train_time:83530ms step_avg:61.37ms
step:1362/2330 train_time:83593ms step_avg:61.38ms
step:1363/2330 train_time:83654ms step_avg:61.38ms
step:1364/2330 train_time:83717ms step_avg:61.38ms
step:1365/2330 train_time:83778ms step_avg:61.38ms
step:1366/2330 train_time:83841ms step_avg:61.38ms
step:1367/2330 train_time:83902ms step_avg:61.38ms
step:1368/2330 train_time:83967ms step_avg:61.38ms
step:1369/2330 train_time:84027ms step_avg:61.38ms
step:1370/2330 train_time:84091ms step_avg:61.38ms
step:1371/2330 train_time:84151ms step_avg:61.38ms
step:1372/2330 train_time:84216ms step_avg:61.38ms
step:1373/2330 train_time:84276ms step_avg:61.38ms
step:1374/2330 train_time:84339ms step_avg:61.38ms
step:1375/2330 train_time:84400ms step_avg:61.38ms
step:1376/2330 train_time:84464ms step_avg:61.38ms
step:1377/2330 train_time:84526ms step_avg:61.38ms
step:1378/2330 train_time:84589ms step_avg:61.39ms
step:1379/2330 train_time:84650ms step_avg:61.39ms
step:1380/2330 train_time:84712ms step_avg:61.39ms
step:1381/2330 train_time:84772ms step_avg:61.38ms
step:1382/2330 train_time:84837ms step_avg:61.39ms
step:1383/2330 train_time:84896ms step_avg:61.39ms
step:1384/2330 train_time:84960ms step_avg:61.39ms
step:1385/2330 train_time:85021ms step_avg:61.39ms
step:1386/2330 train_time:85085ms step_avg:61.39ms
step:1387/2330 train_time:85146ms step_avg:61.39ms
step:1388/2330 train_time:85209ms step_avg:61.39ms
step:1389/2330 train_time:85269ms step_avg:61.39ms
step:1390/2330 train_time:85332ms step_avg:61.39ms
step:1391/2330 train_time:85393ms step_avg:61.39ms
step:1392/2330 train_time:85456ms step_avg:61.39ms
step:1393/2330 train_time:85515ms step_avg:61.39ms
step:1394/2330 train_time:85580ms step_avg:61.39ms
step:1395/2330 train_time:85642ms step_avg:61.39ms
step:1396/2330 train_time:85706ms step_avg:61.39ms
step:1397/2330 train_time:85766ms step_avg:61.39ms
step:1398/2330 train_time:85829ms step_avg:61.39ms
step:1399/2330 train_time:85890ms step_avg:61.39ms
step:1400/2330 train_time:85953ms step_avg:61.40ms
step:1401/2330 train_time:86013ms step_avg:61.39ms
step:1402/2330 train_time:86077ms step_avg:61.40ms
step:1403/2330 train_time:86137ms step_avg:61.40ms
step:1404/2330 train_time:86201ms step_avg:61.40ms
step:1405/2330 train_time:86263ms step_avg:61.40ms
step:1406/2330 train_time:86327ms step_avg:61.40ms
step:1407/2330 train_time:86387ms step_avg:61.40ms
step:1408/2330 train_time:86451ms step_avg:61.40ms
step:1409/2330 train_time:86509ms step_avg:61.40ms
step:1410/2330 train_time:86573ms step_avg:61.40ms
step:1411/2330 train_time:86633ms step_avg:61.40ms
step:1412/2330 train_time:86697ms step_avg:61.40ms
step:1413/2330 train_time:86758ms step_avg:61.40ms
step:1414/2330 train_time:86821ms step_avg:61.40ms
step:1415/2330 train_time:86883ms step_avg:61.40ms
step:1416/2330 train_time:86947ms step_avg:61.40ms
step:1417/2330 train_time:87007ms step_avg:61.40ms
step:1418/2330 train_time:87069ms step_avg:61.40ms
step:1419/2330 train_time:87130ms step_avg:61.40ms
step:1420/2330 train_time:87193ms step_avg:61.40ms
step:1421/2330 train_time:87254ms step_avg:61.40ms
step:1422/2330 train_time:87317ms step_avg:61.40ms
step:1423/2330 train_time:87378ms step_avg:61.40ms
step:1424/2330 train_time:87441ms step_avg:61.41ms
step:1425/2330 train_time:87502ms step_avg:61.40ms
step:1426/2330 train_time:87565ms step_avg:61.41ms
step:1427/2330 train_time:87625ms step_avg:61.41ms
step:1428/2330 train_time:87688ms step_avg:61.41ms
step:1429/2330 train_time:87749ms step_avg:61.41ms
step:1430/2330 train_time:87812ms step_avg:61.41ms
step:1431/2330 train_time:87872ms step_avg:61.41ms
step:1432/2330 train_time:87935ms step_avg:61.41ms
step:1433/2330 train_time:87995ms step_avg:61.41ms
step:1434/2330 train_time:88059ms step_avg:61.41ms
step:1435/2330 train_time:88119ms step_avg:61.41ms
step:1436/2330 train_time:88183ms step_avg:61.41ms
step:1437/2330 train_time:88244ms step_avg:61.41ms
step:1438/2330 train_time:88307ms step_avg:61.41ms
step:1439/2330 train_time:88367ms step_avg:61.41ms
step:1440/2330 train_time:88430ms step_avg:61.41ms
step:1441/2330 train_time:88490ms step_avg:61.41ms
step:1442/2330 train_time:88554ms step_avg:61.41ms
step:1443/2330 train_time:88613ms step_avg:61.41ms
step:1444/2330 train_time:88677ms step_avg:61.41ms
step:1445/2330 train_time:88738ms step_avg:61.41ms
step:1446/2330 train_time:88802ms step_avg:61.41ms
step:1447/2330 train_time:88862ms step_avg:61.41ms
step:1448/2330 train_time:88925ms step_avg:61.41ms
step:1449/2330 train_time:88986ms step_avg:61.41ms
step:1450/2330 train_time:89049ms step_avg:61.41ms
step:1451/2330 train_time:89108ms step_avg:61.41ms
step:1452/2330 train_time:89171ms step_avg:61.41ms
step:1453/2330 train_time:89231ms step_avg:61.41ms
step:1454/2330 train_time:89294ms step_avg:61.41ms
step:1455/2330 train_time:89354ms step_avg:61.41ms
step:1456/2330 train_time:89417ms step_avg:61.41ms
step:1457/2330 train_time:89477ms step_avg:61.41ms
step:1458/2330 train_time:89541ms step_avg:61.41ms
step:1459/2330 train_time:89602ms step_avg:61.41ms
step:1460/2330 train_time:89665ms step_avg:61.41ms
step:1461/2330 train_time:89726ms step_avg:61.41ms
step:1462/2330 train_time:89790ms step_avg:61.42ms
step:1463/2330 train_time:89850ms step_avg:61.42ms
step:1464/2330 train_time:89913ms step_avg:61.42ms
step:1465/2330 train_time:89973ms step_avg:61.41ms
step:1466/2330 train_time:90035ms step_avg:61.42ms
step:1467/2330 train_time:90095ms step_avg:61.41ms
step:1468/2330 train_time:90159ms step_avg:61.42ms
step:1469/2330 train_time:90219ms step_avg:61.42ms
step:1470/2330 train_time:90284ms step_avg:61.42ms
step:1471/2330 train_time:90346ms step_avg:61.42ms
step:1472/2330 train_time:90408ms step_avg:61.42ms
step:1473/2330 train_time:90468ms step_avg:61.42ms
step:1474/2330 train_time:90531ms step_avg:61.42ms
step:1475/2330 train_time:90590ms step_avg:61.42ms
step:1476/2330 train_time:90654ms step_avg:61.42ms
step:1477/2330 train_time:90714ms step_avg:61.42ms
step:1478/2330 train_time:90778ms step_avg:61.42ms
step:1479/2330 train_time:90839ms step_avg:61.42ms
step:1480/2330 train_time:90902ms step_avg:61.42ms
step:1481/2330 train_time:90963ms step_avg:61.42ms
step:1482/2330 train_time:91027ms step_avg:61.42ms
step:1483/2330 train_time:91087ms step_avg:61.42ms
step:1484/2330 train_time:91150ms step_avg:61.42ms
step:1485/2330 train_time:91209ms step_avg:61.42ms
step:1486/2330 train_time:91273ms step_avg:61.42ms
step:1487/2330 train_time:91332ms step_avg:61.42ms
step:1488/2330 train_time:91395ms step_avg:61.42ms
step:1489/2330 train_time:91456ms step_avg:61.42ms
step:1490/2330 train_time:91520ms step_avg:61.42ms
step:1491/2330 train_time:91581ms step_avg:61.42ms
step:1492/2330 train_time:91647ms step_avg:61.43ms
step:1493/2330 train_time:91706ms step_avg:61.42ms
step:1494/2330 train_time:91769ms step_avg:61.43ms
step:1495/2330 train_time:91829ms step_avg:61.42ms
step:1496/2330 train_time:91893ms step_avg:61.43ms
step:1497/2330 train_time:91954ms step_avg:61.43ms
step:1498/2330 train_time:92016ms step_avg:61.43ms
step:1499/2330 train_time:92076ms step_avg:61.43ms
step:1500/2330 train_time:92140ms step_avg:61.43ms
step:1500/2330 val_loss:4.4453 train_time:92214ms step_avg:61.48ms
step:1501/2330 train_time:92238ms step_avg:61.45ms
step:1502/2330 train_time:92267ms step_avg:61.43ms
step:1503/2330 train_time:92330ms step_avg:61.43ms
step:1504/2330 train_time:92398ms step_avg:61.44ms
step:1505/2330 train_time:92458ms step_avg:61.43ms
step:1506/2330 train_time:92522ms step_avg:61.44ms
step:1507/2330 train_time:92582ms step_avg:61.43ms
step:1508/2330 train_time:92644ms step_avg:61.44ms
step:1509/2330 train_time:92703ms step_avg:61.43ms
step:1510/2330 train_time:92766ms step_avg:61.43ms
step:1511/2330 train_time:92825ms step_avg:61.43ms
step:1512/2330 train_time:92888ms step_avg:61.43ms
step:1513/2330 train_time:92947ms step_avg:61.43ms
step:1514/2330 train_time:93010ms step_avg:61.43ms
step:1515/2330 train_time:93070ms step_avg:61.43ms
step:1516/2330 train_time:93132ms step_avg:61.43ms
step:1517/2330 train_time:93192ms step_avg:61.43ms
step:1518/2330 train_time:93256ms step_avg:61.43ms
step:1519/2330 train_time:93318ms step_avg:61.43ms
step:1520/2330 train_time:93382ms step_avg:61.44ms
step:1521/2330 train_time:93442ms step_avg:61.43ms
step:1522/2330 train_time:93507ms step_avg:61.44ms
step:1523/2330 train_time:93568ms step_avg:61.44ms
step:1524/2330 train_time:93631ms step_avg:61.44ms
step:1525/2330 train_time:93691ms step_avg:61.44ms
step:1526/2330 train_time:93754ms step_avg:61.44ms
step:1527/2330 train_time:93813ms step_avg:61.44ms
step:1528/2330 train_time:93876ms step_avg:61.44ms
step:1529/2330 train_time:93937ms step_avg:61.44ms
step:1530/2330 train_time:93999ms step_avg:61.44ms
step:1531/2330 train_time:94059ms step_avg:61.44ms
step:1532/2330 train_time:94123ms step_avg:61.44ms
step:1533/2330 train_time:94183ms step_avg:61.44ms
step:1534/2330 train_time:94247ms step_avg:61.44ms
step:1535/2330 train_time:94309ms step_avg:61.44ms
step:1536/2330 train_time:94375ms step_avg:61.44ms
step:1537/2330 train_time:94435ms step_avg:61.44ms
step:1538/2330 train_time:94500ms step_avg:61.44ms
step:1539/2330 train_time:94560ms step_avg:61.44ms
step:1540/2330 train_time:94625ms step_avg:61.44ms
step:1541/2330 train_time:94684ms step_avg:61.44ms
step:1542/2330 train_time:94748ms step_avg:61.44ms
step:1543/2330 train_time:94810ms step_avg:61.44ms
step:1544/2330 train_time:94874ms step_avg:61.45ms
step:1545/2330 train_time:94934ms step_avg:61.45ms
step:1546/2330 train_time:94997ms step_avg:61.45ms
step:1547/2330 train_time:95057ms step_avg:61.45ms
step:1548/2330 train_time:95120ms step_avg:61.45ms
step:1549/2330 train_time:95182ms step_avg:61.45ms
step:1550/2330 train_time:95246ms step_avg:61.45ms
step:1551/2330 train_time:95306ms step_avg:61.45ms
step:1552/2330 train_time:95371ms step_avg:61.45ms
step:1553/2330 train_time:95433ms step_avg:61.45ms
step:1554/2330 train_time:95496ms step_avg:61.45ms
step:1555/2330 train_time:95558ms step_avg:61.45ms
step:1556/2330 train_time:95621ms step_avg:61.45ms
step:1557/2330 train_time:95682ms step_avg:61.45ms
step:1558/2330 train_time:95745ms step_avg:61.45ms
step:1559/2330 train_time:95805ms step_avg:61.45ms
step:1560/2330 train_time:95869ms step_avg:61.45ms
step:1561/2330 train_time:95930ms step_avg:61.45ms
step:1562/2330 train_time:95994ms step_avg:61.46ms
step:1563/2330 train_time:96055ms step_avg:61.46ms
step:1564/2330 train_time:96117ms step_avg:61.46ms
step:1565/2330 train_time:96178ms step_avg:61.46ms
step:1566/2330 train_time:96241ms step_avg:61.46ms
step:1567/2330 train_time:96302ms step_avg:61.46ms
step:1568/2330 train_time:96366ms step_avg:61.46ms
step:1569/2330 train_time:96426ms step_avg:61.46ms
step:1570/2330 train_time:96491ms step_avg:61.46ms
step:1571/2330 train_time:96553ms step_avg:61.46ms
step:1572/2330 train_time:96616ms step_avg:61.46ms
step:1573/2330 train_time:96677ms step_avg:61.46ms
step:1574/2330 train_time:96740ms step_avg:61.46ms
step:1575/2330 train_time:96801ms step_avg:61.46ms
step:1576/2330 train_time:96865ms step_avg:61.46ms
step:1577/2330 train_time:96925ms step_avg:61.46ms
step:1578/2330 train_time:96989ms step_avg:61.46ms
step:1579/2330 train_time:97050ms step_avg:61.46ms
step:1580/2330 train_time:97116ms step_avg:61.47ms
step:1581/2330 train_time:97177ms step_avg:61.47ms
step:1582/2330 train_time:97239ms step_avg:61.47ms
step:1583/2330 train_time:97299ms step_avg:61.46ms
step:1584/2330 train_time:97362ms step_avg:61.47ms
step:1585/2330 train_time:97423ms step_avg:61.47ms
step:1586/2330 train_time:97487ms step_avg:61.47ms
step:1587/2330 train_time:97549ms step_avg:61.47ms
step:1588/2330 train_time:97613ms step_avg:61.47ms
step:1589/2330 train_time:97674ms step_avg:61.47ms
step:1590/2330 train_time:97738ms step_avg:61.47ms
step:1591/2330 train_time:97798ms step_avg:61.47ms
step:1592/2330 train_time:97862ms step_avg:61.47ms
step:1593/2330 train_time:97923ms step_avg:61.47ms
step:1594/2330 train_time:97987ms step_avg:61.47ms
step:1595/2330 train_time:98047ms step_avg:61.47ms
step:1596/2330 train_time:98111ms step_avg:61.47ms
step:1597/2330 train_time:98172ms step_avg:61.47ms
step:1598/2330 train_time:98235ms step_avg:61.47ms
step:1599/2330 train_time:98296ms step_avg:61.47ms
step:1600/2330 train_time:98360ms step_avg:61.47ms
step:1601/2330 train_time:98420ms step_avg:61.47ms
step:1602/2330 train_time:98483ms step_avg:61.48ms
step:1603/2330 train_time:98544ms step_avg:61.47ms
step:1604/2330 train_time:98608ms step_avg:61.48ms
step:1605/2330 train_time:98669ms step_avg:61.48ms
step:1606/2330 train_time:98733ms step_avg:61.48ms
step:1607/2330 train_time:98794ms step_avg:61.48ms
step:1608/2330 train_time:98857ms step_avg:61.48ms
step:1609/2330 train_time:98918ms step_avg:61.48ms
step:1610/2330 train_time:98982ms step_avg:61.48ms
step:1611/2330 train_time:99042ms step_avg:61.48ms
step:1612/2330 train_time:99106ms step_avg:61.48ms
step:1613/2330 train_time:99166ms step_avg:61.48ms
step:1614/2330 train_time:99231ms step_avg:61.48ms
step:1615/2330 train_time:99292ms step_avg:61.48ms
step:1616/2330 train_time:99355ms step_avg:61.48ms
step:1617/2330 train_time:99415ms step_avg:61.48ms
step:1618/2330 train_time:99479ms step_avg:61.48ms
step:1619/2330 train_time:99540ms step_avg:61.48ms
step:1620/2330 train_time:99603ms step_avg:61.48ms
step:1621/2330 train_time:99663ms step_avg:61.48ms
step:1622/2330 train_time:99727ms step_avg:61.48ms
step:1623/2330 train_time:99788ms step_avg:61.48ms
step:1624/2330 train_time:99852ms step_avg:61.49ms
step:1625/2330 train_time:99913ms step_avg:61.49ms
step:1626/2330 train_time:99978ms step_avg:61.49ms
step:1627/2330 train_time:100037ms step_avg:61.49ms
step:1628/2330 train_time:100101ms step_avg:61.49ms
step:1629/2330 train_time:100161ms step_avg:61.49ms
step:1630/2330 train_time:100224ms step_avg:61.49ms
step:1631/2330 train_time:100285ms step_avg:61.49ms
step:1632/2330 train_time:100349ms step_avg:61.49ms
step:1633/2330 train_time:100411ms step_avg:61.49ms
step:1634/2330 train_time:100475ms step_avg:61.49ms
step:1635/2330 train_time:100535ms step_avg:61.49ms
step:1636/2330 train_time:100599ms step_avg:61.49ms
step:1637/2330 train_time:100658ms step_avg:61.49ms
step:1638/2330 train_time:100722ms step_avg:61.49ms
step:1639/2330 train_time:100782ms step_avg:61.49ms
step:1640/2330 train_time:100846ms step_avg:61.49ms
step:1641/2330 train_time:100908ms step_avg:61.49ms
step:1642/2330 train_time:100973ms step_avg:61.49ms
step:1643/2330 train_time:101033ms step_avg:61.49ms
step:1644/2330 train_time:101097ms step_avg:61.49ms
step:1645/2330 train_time:101157ms step_avg:61.49ms
step:1646/2330 train_time:101221ms step_avg:61.50ms
step:1647/2330 train_time:101281ms step_avg:61.49ms
step:1648/2330 train_time:101345ms step_avg:61.50ms
step:1649/2330 train_time:101406ms step_avg:61.50ms
step:1650/2330 train_time:101471ms step_avg:61.50ms
step:1651/2330 train_time:101532ms step_avg:61.50ms
step:1652/2330 train_time:101596ms step_avg:61.50ms
step:1653/2330 train_time:101657ms step_avg:61.50ms
step:1654/2330 train_time:101720ms step_avg:61.50ms
step:1655/2330 train_time:101781ms step_avg:61.50ms
step:1656/2330 train_time:101844ms step_avg:61.50ms
step:1657/2330 train_time:101904ms step_avg:61.50ms
step:1658/2330 train_time:101969ms step_avg:61.50ms
step:1659/2330 train_time:102030ms step_avg:61.50ms
step:1660/2330 train_time:102093ms step_avg:61.50ms
step:1661/2330 train_time:102154ms step_avg:61.50ms
step:1662/2330 train_time:102219ms step_avg:61.50ms
step:1663/2330 train_time:102280ms step_avg:61.50ms
step:1664/2330 train_time:102343ms step_avg:61.50ms
step:1665/2330 train_time:102403ms step_avg:61.50ms
step:1666/2330 train_time:102467ms step_avg:61.51ms
step:1667/2330 train_time:102528ms step_avg:61.50ms
step:1668/2330 train_time:102592ms step_avg:61.51ms
step:1669/2330 train_time:102653ms step_avg:61.51ms
step:1670/2330 train_time:102717ms step_avg:61.51ms
step:1671/2330 train_time:102779ms step_avg:61.51ms
step:1672/2330 train_time:102841ms step_avg:61.51ms
step:1673/2330 train_time:102901ms step_avg:61.51ms
step:1674/2330 train_time:102964ms step_avg:61.51ms
step:1675/2330 train_time:103025ms step_avg:61.51ms
step:1676/2330 train_time:103089ms step_avg:61.51ms
step:1677/2330 train_time:103151ms step_avg:61.51ms
step:1678/2330 train_time:103216ms step_avg:61.51ms
step:1679/2330 train_time:103277ms step_avg:61.51ms
step:1680/2330 train_time:103340ms step_avg:61.51ms
step:1681/2330 train_time:103400ms step_avg:61.51ms
step:1682/2330 train_time:103463ms step_avg:61.51ms
step:1683/2330 train_time:103525ms step_avg:61.51ms
step:1684/2330 train_time:103589ms step_avg:61.51ms
step:1685/2330 train_time:103650ms step_avg:61.51ms
step:1686/2330 train_time:103715ms step_avg:61.52ms
step:1687/2330 train_time:103776ms step_avg:61.52ms
step:1688/2330 train_time:103839ms step_avg:61.52ms
step:1689/2330 train_time:103900ms step_avg:61.52ms
step:1690/2330 train_time:103964ms step_avg:61.52ms
step:1691/2330 train_time:104024ms step_avg:61.52ms
step:1692/2330 train_time:104088ms step_avg:61.52ms
step:1693/2330 train_time:104149ms step_avg:61.52ms
step:1694/2330 train_time:104214ms step_avg:61.52ms
step:1695/2330 train_time:104275ms step_avg:61.52ms
step:1696/2330 train_time:104338ms step_avg:61.52ms
step:1697/2330 train_time:104398ms step_avg:61.52ms
step:1698/2330 train_time:104463ms step_avg:61.52ms
step:1699/2330 train_time:104523ms step_avg:61.52ms
step:1700/2330 train_time:104586ms step_avg:61.52ms
step:1701/2330 train_time:104647ms step_avg:61.52ms
step:1702/2330 train_time:104712ms step_avg:61.52ms
step:1703/2330 train_time:104774ms step_avg:61.52ms
step:1704/2330 train_time:104837ms step_avg:61.52ms
step:1705/2330 train_time:104897ms step_avg:61.52ms
step:1706/2330 train_time:104961ms step_avg:61.52ms
step:1707/2330 train_time:105021ms step_avg:61.52ms
step:1708/2330 train_time:105086ms step_avg:61.53ms
step:1709/2330 train_time:105145ms step_avg:61.52ms
step:1710/2330 train_time:105210ms step_avg:61.53ms
step:1711/2330 train_time:105272ms step_avg:61.53ms
step:1712/2330 train_time:105335ms step_avg:61.53ms
step:1713/2330 train_time:105396ms step_avg:61.53ms
step:1714/2330 train_time:105459ms step_avg:61.53ms
step:1715/2330 train_time:105520ms step_avg:61.53ms
step:1716/2330 train_time:105583ms step_avg:61.53ms
step:1717/2330 train_time:105643ms step_avg:61.53ms
step:1718/2330 train_time:105707ms step_avg:61.53ms
step:1719/2330 train_time:105769ms step_avg:61.53ms
step:1720/2330 train_time:105833ms step_avg:61.53ms
step:1721/2330 train_time:105893ms step_avg:61.53ms
step:1722/2330 train_time:105957ms step_avg:61.53ms
step:1723/2330 train_time:106017ms step_avg:61.53ms
step:1724/2330 train_time:106081ms step_avg:61.53ms
step:1725/2330 train_time:106141ms step_avg:61.53ms
step:1726/2330 train_time:106205ms step_avg:61.53ms
step:1727/2330 train_time:106266ms step_avg:61.53ms
step:1728/2330 train_time:106329ms step_avg:61.53ms
step:1729/2330 train_time:106392ms step_avg:61.53ms
step:1730/2330 train_time:106455ms step_avg:61.53ms
step:1731/2330 train_time:106517ms step_avg:61.53ms
step:1732/2330 train_time:106580ms step_avg:61.54ms
step:1733/2330 train_time:106640ms step_avg:61.54ms
step:1734/2330 train_time:106704ms step_avg:61.54ms
step:1735/2330 train_time:106764ms step_avg:61.54ms
step:1736/2330 train_time:106828ms step_avg:61.54ms
step:1737/2330 train_time:106889ms step_avg:61.54ms
step:1738/2330 train_time:106953ms step_avg:61.54ms
step:1739/2330 train_time:107015ms step_avg:61.54ms
step:1740/2330 train_time:107079ms step_avg:61.54ms
step:1741/2330 train_time:107139ms step_avg:61.54ms
step:1742/2330 train_time:107202ms step_avg:61.54ms
step:1743/2330 train_time:107262ms step_avg:61.54ms
step:1744/2330 train_time:107327ms step_avg:61.54ms
step:1745/2330 train_time:107388ms step_avg:61.54ms
step:1746/2330 train_time:107452ms step_avg:61.54ms
step:1747/2330 train_time:107514ms step_avg:61.54ms
step:1748/2330 train_time:107578ms step_avg:61.54ms
step:1749/2330 train_time:107639ms step_avg:61.54ms
step:1750/2330 train_time:107701ms step_avg:61.54ms
step:1750/2330 val_loss:4.3104 train_time:107775ms step_avg:61.59ms
step:1751/2330 train_time:107798ms step_avg:61.56ms
step:1752/2330 train_time:107828ms step_avg:61.55ms
step:1753/2330 train_time:107896ms step_avg:61.55ms
step:1754/2330 train_time:107965ms step_avg:61.55ms
step:1755/2330 train_time:108027ms step_avg:61.55ms
step:1756/2330 train_time:108092ms step_avg:61.56ms
step:1757/2330 train_time:108151ms step_avg:61.55ms
step:1758/2330 train_time:108214ms step_avg:61.56ms
step:1759/2330 train_time:108274ms step_avg:61.55ms
step:1760/2330 train_time:108337ms step_avg:61.56ms
step:1761/2330 train_time:108397ms step_avg:61.55ms
step:1762/2330 train_time:108461ms step_avg:61.56ms
step:1763/2330 train_time:108520ms step_avg:61.55ms
step:1764/2330 train_time:108583ms step_avg:61.55ms
step:1765/2330 train_time:108642ms step_avg:61.55ms
step:1766/2330 train_time:108705ms step_avg:61.55ms
step:1767/2330 train_time:108767ms step_avg:61.55ms
step:1768/2330 train_time:108832ms step_avg:61.56ms
step:1769/2330 train_time:108893ms step_avg:61.56ms
step:1770/2330 train_time:108960ms step_avg:61.56ms
step:1771/2330 train_time:109020ms step_avg:61.56ms
step:1772/2330 train_time:109083ms step_avg:61.56ms
step:1773/2330 train_time:109144ms step_avg:61.56ms
step:1774/2330 train_time:109207ms step_avg:61.56ms
step:1775/2330 train_time:109267ms step_avg:61.56ms
step:1776/2330 train_time:109331ms step_avg:61.56ms
step:1777/2330 train_time:109391ms step_avg:61.56ms
step:1778/2330 train_time:109455ms step_avg:61.56ms
step:1779/2330 train_time:109515ms step_avg:61.56ms
step:1780/2330 train_time:109578ms step_avg:61.56ms
step:1781/2330 train_time:109639ms step_avg:61.56ms
step:1782/2330 train_time:109703ms step_avg:61.56ms
step:1783/2330 train_time:109765ms step_avg:61.56ms
step:1784/2330 train_time:109828ms step_avg:61.56ms
step:1785/2330 train_time:109890ms step_avg:61.56ms
step:1786/2330 train_time:109955ms step_avg:61.56ms
step:1787/2330 train_time:110018ms step_avg:61.57ms
step:1788/2330 train_time:110083ms step_avg:61.57ms
step:1789/2330 train_time:110143ms step_avg:61.57ms
step:1790/2330 train_time:110206ms step_avg:61.57ms
step:1791/2330 train_time:110266ms step_avg:61.57ms
step:1792/2330 train_time:110329ms step_avg:61.57ms
step:1793/2330 train_time:110390ms step_avg:61.57ms
step:1794/2330 train_time:110452ms step_avg:61.57ms
step:1795/2330 train_time:110512ms step_avg:61.57ms
step:1796/2330 train_time:110576ms step_avg:61.57ms
step:1797/2330 train_time:110638ms step_avg:61.57ms
step:1798/2330 train_time:110703ms step_avg:61.57ms
step:1799/2330 train_time:110763ms step_avg:61.57ms
step:1800/2330 train_time:110827ms step_avg:61.57ms
step:1801/2330 train_time:110887ms step_avg:61.57ms
step:1802/2330 train_time:110951ms step_avg:61.57ms
step:1803/2330 train_time:111013ms step_avg:61.57ms
step:1804/2330 train_time:111078ms step_avg:61.57ms
step:1805/2330 train_time:111141ms step_avg:61.57ms
step:1806/2330 train_time:111205ms step_avg:61.58ms
step:1807/2330 train_time:111265ms step_avg:61.57ms
step:1808/2330 train_time:111328ms step_avg:61.58ms
step:1809/2330 train_time:111389ms step_avg:61.57ms
step:1810/2330 train_time:111453ms step_avg:61.58ms
step:1811/2330 train_time:111513ms step_avg:61.58ms
step:1812/2330 train_time:111576ms step_avg:61.58ms
step:1813/2330 train_time:111638ms step_avg:61.58ms
step:1814/2330 train_time:111702ms step_avg:61.58ms
step:1815/2330 train_time:111764ms step_avg:61.58ms
step:1816/2330 train_time:111826ms step_avg:61.58ms
step:1817/2330 train_time:111887ms step_avg:61.58ms
step:1818/2330 train_time:111950ms step_avg:61.58ms
step:1819/2330 train_time:112012ms step_avg:61.58ms
step:1820/2330 train_time:112076ms step_avg:61.58ms
step:1821/2330 train_time:112138ms step_avg:61.58ms
step:1822/2330 train_time:112203ms step_avg:61.58ms
step:1823/2330 train_time:112265ms step_avg:61.58ms
step:1824/2330 train_time:112327ms step_avg:61.58ms
step:1825/2330 train_time:112388ms step_avg:61.58ms
step:1826/2330 train_time:112451ms step_avg:61.58ms
step:1827/2330 train_time:112511ms step_avg:61.58ms
step:1828/2330 train_time:112575ms step_avg:61.58ms
step:1829/2330 train_time:112636ms step_avg:61.58ms
step:1830/2330 train_time:112700ms step_avg:61.58ms
step:1831/2330 train_time:112761ms step_avg:61.58ms
step:1832/2330 train_time:112824ms step_avg:61.59ms
step:1833/2330 train_time:112885ms step_avg:61.58ms
step:1834/2330 train_time:112949ms step_avg:61.59ms
step:1835/2330 train_time:113009ms step_avg:61.59ms
step:1836/2330 train_time:113073ms step_avg:61.59ms
step:1837/2330 train_time:113134ms step_avg:61.59ms
step:1838/2330 train_time:113200ms step_avg:61.59ms
step:1839/2330 train_time:113262ms step_avg:61.59ms
step:1840/2330 train_time:113325ms step_avg:61.59ms
step:1841/2330 train_time:113385ms step_avg:61.59ms
step:1842/2330 train_time:113448ms step_avg:61.59ms
step:1843/2330 train_time:113509ms step_avg:61.59ms
step:1844/2330 train_time:113573ms step_avg:61.59ms
step:1845/2330 train_time:113634ms step_avg:61.59ms
step:1846/2330 train_time:113697ms step_avg:61.59ms
step:1847/2330 train_time:113758ms step_avg:61.59ms
step:1848/2330 train_time:113822ms step_avg:61.59ms
step:1849/2330 train_time:113883ms step_avg:61.59ms
step:1850/2330 train_time:113945ms step_avg:61.59ms
step:1851/2330 train_time:114006ms step_avg:61.59ms
step:1852/2330 train_time:114070ms step_avg:61.59ms
step:1853/2330 train_time:114130ms step_avg:61.59ms
step:1854/2330 train_time:114194ms step_avg:61.59ms
step:1855/2330 train_time:114256ms step_avg:61.59ms
step:1856/2330 train_time:114321ms step_avg:61.60ms
step:1857/2330 train_time:114382ms step_avg:61.59ms
step:1858/2330 train_time:114446ms step_avg:61.60ms
step:1859/2330 train_time:114505ms step_avg:61.60ms
step:1860/2330 train_time:114569ms step_avg:61.60ms
step:1861/2330 train_time:114630ms step_avg:61.60ms
step:1862/2330 train_time:114694ms step_avg:61.60ms
step:1863/2330 train_time:114754ms step_avg:61.60ms
step:1864/2330 train_time:114818ms step_avg:61.60ms
step:1865/2330 train_time:114879ms step_avg:61.60ms
step:1866/2330 train_time:114944ms step_avg:61.60ms
step:1867/2330 train_time:115004ms step_avg:61.60ms
step:1868/2330 train_time:115067ms step_avg:61.60ms
step:1869/2330 train_time:115128ms step_avg:61.60ms
step:1870/2330 train_time:115191ms step_avg:61.60ms
step:1871/2330 train_time:115251ms step_avg:61.60ms
step:1872/2330 train_time:115315ms step_avg:61.60ms
step:1873/2330 train_time:115377ms step_avg:61.60ms
step:1874/2330 train_time:115441ms step_avg:61.60ms
step:1875/2330 train_time:115500ms step_avg:61.60ms
step:1876/2330 train_time:115566ms step_avg:61.60ms
step:1877/2330 train_time:115625ms step_avg:61.60ms
step:1878/2330 train_time:115690ms step_avg:61.60ms
step:1879/2330 train_time:115750ms step_avg:61.60ms
step:1880/2330 train_time:115813ms step_avg:61.60ms
step:1881/2330 train_time:115874ms step_avg:61.60ms
step:1882/2330 train_time:115938ms step_avg:61.60ms
step:1883/2330 train_time:116000ms step_avg:61.60ms
step:1884/2330 train_time:116064ms step_avg:61.61ms
step:1885/2330 train_time:116124ms step_avg:61.60ms
step:1886/2330 train_time:116186ms step_avg:61.60ms
step:1887/2330 train_time:116247ms step_avg:61.60ms
step:1888/2330 train_time:116310ms step_avg:61.60ms
step:1889/2330 train_time:116371ms step_avg:61.60ms
step:1890/2330 train_time:116434ms step_avg:61.61ms
step:1891/2330 train_time:116496ms step_avg:61.61ms
step:1892/2330 train_time:116561ms step_avg:61.61ms
step:1893/2330 train_time:116620ms step_avg:61.61ms
step:1894/2330 train_time:116684ms step_avg:61.61ms
step:1895/2330 train_time:116744ms step_avg:61.61ms
step:1896/2330 train_time:116808ms step_avg:61.61ms
step:1897/2330 train_time:116869ms step_avg:61.61ms
step:1898/2330 train_time:116933ms step_avg:61.61ms
step:1899/2330 train_time:116994ms step_avg:61.61ms
step:1900/2330 train_time:117058ms step_avg:61.61ms
step:1901/2330 train_time:117119ms step_avg:61.61ms
step:1902/2330 train_time:117183ms step_avg:61.61ms
step:1903/2330 train_time:117243ms step_avg:61.61ms
step:1904/2330 train_time:117307ms step_avg:61.61ms
step:1905/2330 train_time:117367ms step_avg:61.61ms
step:1906/2330 train_time:117431ms step_avg:61.61ms
step:1907/2330 train_time:117491ms step_avg:61.61ms
step:1908/2330 train_time:117554ms step_avg:61.61ms
step:1909/2330 train_time:117615ms step_avg:61.61ms
step:1910/2330 train_time:117679ms step_avg:61.61ms
step:1911/2330 train_time:117739ms step_avg:61.61ms
step:1912/2330 train_time:117803ms step_avg:61.61ms
step:1913/2330 train_time:117864ms step_avg:61.61ms
step:1914/2330 train_time:117926ms step_avg:61.61ms
step:1915/2330 train_time:117986ms step_avg:61.61ms
step:1916/2330 train_time:118050ms step_avg:61.61ms
step:1917/2330 train_time:118110ms step_avg:61.61ms
step:1918/2330 train_time:118174ms step_avg:61.61ms
step:1919/2330 train_time:118234ms step_avg:61.61ms
step:1920/2330 train_time:118300ms step_avg:61.61ms
step:1921/2330 train_time:118362ms step_avg:61.61ms
step:1922/2330 train_time:118425ms step_avg:61.62ms
step:1923/2330 train_time:118485ms step_avg:61.61ms
step:1924/2330 train_time:118549ms step_avg:61.62ms
step:1925/2330 train_time:118610ms step_avg:61.62ms
step:1926/2330 train_time:118673ms step_avg:61.62ms
step:1927/2330 train_time:118734ms step_avg:61.62ms
step:1928/2330 train_time:118799ms step_avg:61.62ms
step:1929/2330 train_time:118861ms step_avg:61.62ms
step:1930/2330 train_time:118924ms step_avg:61.62ms
step:1931/2330 train_time:118984ms step_avg:61.62ms
step:1932/2330 train_time:119047ms step_avg:61.62ms
step:1933/2330 train_time:119109ms step_avg:61.62ms
step:1934/2330 train_time:119172ms step_avg:61.62ms
step:1935/2330 train_time:119232ms step_avg:61.62ms
step:1936/2330 train_time:119298ms step_avg:61.62ms
step:1937/2330 train_time:119360ms step_avg:61.62ms
step:1938/2330 train_time:119424ms step_avg:61.62ms
step:1939/2330 train_time:119484ms step_avg:61.62ms
step:1940/2330 train_time:119547ms step_avg:61.62ms
step:1941/2330 train_time:119608ms step_avg:61.62ms
step:1942/2330 train_time:119671ms step_avg:61.62ms
step:1943/2330 train_time:119731ms step_avg:61.62ms
step:1944/2330 train_time:119795ms step_avg:61.62ms
step:1945/2330 train_time:119856ms step_avg:61.62ms
step:1946/2330 train_time:119921ms step_avg:61.62ms
step:1947/2330 train_time:119981ms step_avg:61.62ms
step:1948/2330 train_time:120045ms step_avg:61.62ms
step:1949/2330 train_time:120104ms step_avg:61.62ms
step:1950/2330 train_time:120168ms step_avg:61.62ms
step:1951/2330 train_time:120229ms step_avg:61.62ms
step:1952/2330 train_time:120293ms step_avg:61.63ms
step:1953/2330 train_time:120354ms step_avg:61.63ms
step:1954/2330 train_time:120418ms step_avg:61.63ms
step:1955/2330 train_time:120479ms step_avg:61.63ms
step:1956/2330 train_time:120543ms step_avg:61.63ms
step:1957/2330 train_time:120604ms step_avg:61.63ms
step:1958/2330 train_time:120668ms step_avg:61.63ms
step:1959/2330 train_time:120728ms step_avg:61.63ms
step:1960/2330 train_time:120791ms step_avg:61.63ms
step:1961/2330 train_time:120851ms step_avg:61.63ms
step:1962/2330 train_time:120915ms step_avg:61.63ms
step:1963/2330 train_time:120977ms step_avg:61.63ms
step:1964/2330 train_time:121042ms step_avg:61.63ms
step:1965/2330 train_time:121103ms step_avg:61.63ms
step:1966/2330 train_time:121166ms step_avg:61.63ms
step:1967/2330 train_time:121226ms step_avg:61.63ms
step:1968/2330 train_time:121289ms step_avg:61.63ms
step:1969/2330 train_time:121350ms step_avg:61.63ms
step:1970/2330 train_time:121414ms step_avg:61.63ms
step:1971/2330 train_time:121474ms step_avg:61.63ms
step:1972/2330 train_time:121539ms step_avg:61.63ms
step:1973/2330 train_time:121600ms step_avg:61.63ms
step:1974/2330 train_time:121664ms step_avg:61.63ms
step:1975/2330 train_time:121724ms step_avg:61.63ms
step:1976/2330 train_time:121787ms step_avg:61.63ms
step:1977/2330 train_time:121846ms step_avg:61.63ms
step:1978/2330 train_time:121910ms step_avg:61.63ms
step:1979/2330 train_time:121970ms step_avg:61.63ms
step:1980/2330 train_time:122033ms step_avg:61.63ms
step:1981/2330 train_time:122095ms step_avg:61.63ms
step:1982/2330 train_time:122161ms step_avg:61.64ms
step:1983/2330 train_time:122221ms step_avg:61.63ms
step:1984/2330 train_time:122284ms step_avg:61.63ms
step:1985/2330 train_time:122344ms step_avg:61.63ms
step:1986/2330 train_time:122407ms step_avg:61.63ms
step:1987/2330 train_time:122467ms step_avg:61.63ms
step:1988/2330 train_time:122531ms step_avg:61.64ms
step:1989/2330 train_time:122591ms step_avg:61.63ms
step:1990/2330 train_time:122655ms step_avg:61.64ms
step:1991/2330 train_time:122717ms step_avg:61.64ms
step:1992/2330 train_time:122781ms step_avg:61.64ms
step:1993/2330 train_time:122842ms step_avg:61.64ms
step:1994/2330 train_time:122906ms step_avg:61.64ms
step:1995/2330 train_time:122966ms step_avg:61.64ms
step:1996/2330 train_time:123030ms step_avg:61.64ms
step:1997/2330 train_time:123090ms step_avg:61.64ms
step:1998/2330 train_time:123154ms step_avg:61.64ms
step:1999/2330 train_time:123215ms step_avg:61.64ms
step:2000/2330 train_time:123279ms step_avg:61.64ms
step:2000/2330 val_loss:4.1954 train_time:123352ms step_avg:61.68ms
step:2001/2330 train_time:123373ms step_avg:61.66ms
step:2002/2330 train_time:123406ms step_avg:61.64ms
step:2003/2330 train_time:123470ms step_avg:61.64ms
step:2004/2330 train_time:123535ms step_avg:61.64ms
step:2005/2330 train_time:123597ms step_avg:61.64ms
step:2006/2330 train_time:123661ms step_avg:61.65ms
step:2007/2330 train_time:123720ms step_avg:61.64ms
step:2008/2330 train_time:123783ms step_avg:61.65ms
step:2009/2330 train_time:123843ms step_avg:61.64ms
step:2010/2330 train_time:123906ms step_avg:61.64ms
step:2011/2330 train_time:123965ms step_avg:61.64ms
step:2012/2330 train_time:124028ms step_avg:61.64ms
step:2013/2330 train_time:124088ms step_avg:61.64ms
step:2014/2330 train_time:124151ms step_avg:61.64ms
step:2015/2330 train_time:124211ms step_avg:61.64ms
step:2016/2330 train_time:124273ms step_avg:61.64ms
step:2017/2330 train_time:124334ms step_avg:61.64ms
step:2018/2330 train_time:124400ms step_avg:61.65ms
step:2019/2330 train_time:124462ms step_avg:61.65ms
step:2020/2330 train_time:124526ms step_avg:61.65ms
step:2021/2330 train_time:124588ms step_avg:61.65ms
step:2022/2330 train_time:124651ms step_avg:61.65ms
step:2023/2330 train_time:124713ms step_avg:61.65ms
step:2024/2330 train_time:124778ms step_avg:61.65ms
step:2025/2330 train_time:124839ms step_avg:61.65ms
step:2026/2330 train_time:124903ms step_avg:61.65ms
step:2027/2330 train_time:124963ms step_avg:61.65ms
step:2028/2330 train_time:125025ms step_avg:61.65ms
step:2029/2330 train_time:125085ms step_avg:61.65ms
step:2030/2330 train_time:125147ms step_avg:61.65ms
step:2031/2330 train_time:125207ms step_avg:61.65ms
step:2032/2330 train_time:125271ms step_avg:61.65ms
step:2033/2330 train_time:125331ms step_avg:61.65ms
step:2034/2330 train_time:125396ms step_avg:61.65ms
step:2035/2330 train_time:125458ms step_avg:61.65ms
step:2036/2330 train_time:125522ms step_avg:61.65ms
step:2037/2330 train_time:125584ms step_avg:61.65ms
step:2038/2330 train_time:125646ms step_avg:61.65ms
step:2039/2330 train_time:125707ms step_avg:61.65ms
step:2040/2330 train_time:125771ms step_avg:61.65ms
step:2041/2330 train_time:125832ms step_avg:61.65ms
step:2042/2330 train_time:125896ms step_avg:61.65ms
step:2043/2330 train_time:125958ms step_avg:61.65ms
step:2044/2330 train_time:126022ms step_avg:61.65ms
step:2045/2330 train_time:126084ms step_avg:61.65ms
step:2046/2330 train_time:126145ms step_avg:61.65ms
step:2047/2330 train_time:126205ms step_avg:61.65ms
step:2048/2330 train_time:126268ms step_avg:61.65ms
step:2049/2330 train_time:126329ms step_avg:61.65ms
step:2050/2330 train_time:126394ms step_avg:61.66ms
step:2051/2330 train_time:126455ms step_avg:61.66ms
step:2052/2330 train_time:126521ms step_avg:61.66ms
step:2053/2330 train_time:126582ms step_avg:61.66ms
step:2054/2330 train_time:126646ms step_avg:61.66ms
step:2055/2330 train_time:126707ms step_avg:61.66ms
step:2056/2330 train_time:126770ms step_avg:61.66ms
step:2057/2330 train_time:126831ms step_avg:61.66ms
step:2058/2330 train_time:126895ms step_avg:61.66ms
step:2059/2330 train_time:126957ms step_avg:61.66ms
step:2060/2330 train_time:127021ms step_avg:61.66ms
step:2061/2330 train_time:127083ms step_avg:61.66ms
step:2062/2330 train_time:127144ms step_avg:61.66ms
step:2063/2330 train_time:127205ms step_avg:61.66ms
step:2064/2330 train_time:127267ms step_avg:61.66ms
step:2065/2330 train_time:127327ms step_avg:61.66ms
step:2066/2330 train_time:127391ms step_avg:61.66ms
step:2067/2330 train_time:127451ms step_avg:61.66ms
step:2068/2330 train_time:127516ms step_avg:61.66ms
step:2069/2330 train_time:127579ms step_avg:61.66ms
step:2070/2330 train_time:127642ms step_avg:61.66ms
step:2071/2330 train_time:127702ms step_avg:61.66ms
step:2072/2330 train_time:127766ms step_avg:61.66ms
step:2073/2330 train_time:127827ms step_avg:61.66ms
step:2074/2330 train_time:127892ms step_avg:61.66ms
step:2075/2330 train_time:127952ms step_avg:61.66ms
step:2076/2330 train_time:128017ms step_avg:61.67ms
step:2077/2330 train_time:128078ms step_avg:61.67ms
step:2078/2330 train_time:128141ms step_avg:61.67ms
step:2079/2330 train_time:128201ms step_avg:61.66ms
step:2080/2330 train_time:128265ms step_avg:61.67ms
step:2081/2330 train_time:128325ms step_avg:61.67ms
step:2082/2330 train_time:128389ms step_avg:61.67ms
step:2083/2330 train_time:128448ms step_avg:61.67ms
step:2084/2330 train_time:128512ms step_avg:61.67ms
step:2085/2330 train_time:128574ms step_avg:61.67ms
step:2086/2330 train_time:128638ms step_avg:61.67ms
step:2087/2330 train_time:128699ms step_avg:61.67ms
step:2088/2330 train_time:128764ms step_avg:61.67ms
step:2089/2330 train_time:128825ms step_avg:61.67ms
step:2090/2330 train_time:128889ms step_avg:61.67ms
step:2091/2330 train_time:128949ms step_avg:61.67ms
step:2092/2330 train_time:129014ms step_avg:61.67ms
step:2093/2330 train_time:129075ms step_avg:61.67ms
step:2094/2330 train_time:129139ms step_avg:61.67ms
step:2095/2330 train_time:129200ms step_avg:61.67ms
step:2096/2330 train_time:129264ms step_avg:61.67ms
step:2097/2330 train_time:129324ms step_avg:61.67ms
step:2098/2330 train_time:129387ms step_avg:61.67ms
step:2099/2330 train_time:129447ms step_avg:61.67ms
step:2100/2330 train_time:129510ms step_avg:61.67ms
step:2101/2330 train_time:129571ms step_avg:61.67ms
step:2102/2330 train_time:129634ms step_avg:61.67ms
step:2103/2330 train_time:129695ms step_avg:61.67ms
step:2104/2330 train_time:129761ms step_avg:61.67ms
step:2105/2330 train_time:129821ms step_avg:61.67ms
step:2106/2330 train_time:129885ms step_avg:61.67ms
step:2107/2330 train_time:129945ms step_avg:61.67ms
step:2108/2330 train_time:130009ms step_avg:61.67ms
step:2109/2330 train_time:130070ms step_avg:61.67ms
step:2110/2330 train_time:130134ms step_avg:61.67ms
step:2111/2330 train_time:130195ms step_avg:61.67ms
step:2112/2330 train_time:130259ms step_avg:61.68ms
step:2113/2330 train_time:130320ms step_avg:61.68ms
step:2114/2330 train_time:130384ms step_avg:61.68ms
step:2115/2330 train_time:130444ms step_avg:61.68ms
step:2116/2330 train_time:130506ms step_avg:61.68ms
step:2117/2330 train_time:130567ms step_avg:61.68ms
step:2118/2330 train_time:130630ms step_avg:61.68ms
step:2119/2330 train_time:130690ms step_avg:61.68ms
step:2120/2330 train_time:130753ms step_avg:61.68ms
step:2121/2330 train_time:130815ms step_avg:61.68ms
step:2122/2330 train_time:130881ms step_avg:61.68ms
step:2123/2330 train_time:130941ms step_avg:61.68ms
step:2124/2330 train_time:131004ms step_avg:61.68ms
step:2125/2330 train_time:131066ms step_avg:61.68ms
step:2126/2330 train_time:131129ms step_avg:61.68ms
step:2127/2330 train_time:131189ms step_avg:61.68ms
step:2128/2330 train_time:131253ms step_avg:61.68ms
step:2129/2330 train_time:131315ms step_avg:61.68ms
step:2130/2330 train_time:131380ms step_avg:61.68ms
step:2131/2330 train_time:131440ms step_avg:61.68ms
step:2132/2330 train_time:131503ms step_avg:61.68ms
step:2133/2330 train_time:131564ms step_avg:61.68ms
step:2134/2330 train_time:131628ms step_avg:61.68ms
step:2135/2330 train_time:131688ms step_avg:61.68ms
step:2136/2330 train_time:131752ms step_avg:61.68ms
step:2137/2330 train_time:131812ms step_avg:61.68ms
step:2138/2330 train_time:131876ms step_avg:61.68ms
step:2139/2330 train_time:131938ms step_avg:61.68ms
step:2140/2330 train_time:132001ms step_avg:61.68ms
step:2141/2330 train_time:132062ms step_avg:61.68ms
step:2142/2330 train_time:132125ms step_avg:61.68ms
step:2143/2330 train_time:132186ms step_avg:61.68ms
step:2144/2330 train_time:132248ms step_avg:61.68ms
step:2145/2330 train_time:132309ms step_avg:61.68ms
step:2146/2330 train_time:132373ms step_avg:61.68ms
step:2147/2330 train_time:132434ms step_avg:61.68ms
step:2148/2330 train_time:132499ms step_avg:61.68ms
step:2149/2330 train_time:132560ms step_avg:61.68ms
step:2150/2330 train_time:132623ms step_avg:61.69ms
step:2151/2330 train_time:132684ms step_avg:61.68ms
step:2152/2330 train_time:132747ms step_avg:61.69ms
step:2153/2330 train_time:132809ms step_avg:61.69ms
step:2154/2330 train_time:132871ms step_avg:61.69ms
step:2155/2330 train_time:132932ms step_avg:61.69ms
step:2156/2330 train_time:132996ms step_avg:61.69ms
step:2157/2330 train_time:133057ms step_avg:61.69ms
step:2158/2330 train_time:133121ms step_avg:61.69ms
step:2159/2330 train_time:133182ms step_avg:61.69ms
step:2160/2330 train_time:133245ms step_avg:61.69ms
step:2161/2330 train_time:133305ms step_avg:61.69ms
step:2162/2330 train_time:133369ms step_avg:61.69ms
step:2163/2330 train_time:133430ms step_avg:61.69ms
step:2164/2330 train_time:133494ms step_avg:61.69ms
step:2165/2330 train_time:133555ms step_avg:61.69ms
step:2166/2330 train_time:133620ms step_avg:61.69ms
step:2167/2330 train_time:133681ms step_avg:61.69ms
step:2168/2330 train_time:133744ms step_avg:61.69ms
step:2169/2330 train_time:133804ms step_avg:61.69ms
step:2170/2330 train_time:133868ms step_avg:61.69ms
step:2171/2330 train_time:133929ms step_avg:61.69ms
step:2172/2330 train_time:133993ms step_avg:61.69ms
step:2173/2330 train_time:134053ms step_avg:61.69ms
step:2174/2330 train_time:134118ms step_avg:61.69ms
step:2175/2330 train_time:134179ms step_avg:61.69ms
step:2176/2330 train_time:134242ms step_avg:61.69ms
step:2177/2330 train_time:134303ms step_avg:61.69ms
step:2178/2330 train_time:134366ms step_avg:61.69ms
step:2179/2330 train_time:134427ms step_avg:61.69ms
step:2180/2330 train_time:134491ms step_avg:61.69ms
step:2181/2330 train_time:134552ms step_avg:61.69ms
step:2182/2330 train_time:134617ms step_avg:61.69ms
step:2183/2330 train_time:134678ms step_avg:61.69ms
step:2184/2330 train_time:134741ms step_avg:61.69ms
step:2185/2330 train_time:134802ms step_avg:61.69ms
step:2186/2330 train_time:134865ms step_avg:61.69ms
step:2187/2330 train_time:134926ms step_avg:61.69ms
step:2188/2330 train_time:134990ms step_avg:61.70ms
step:2189/2330 train_time:135050ms step_avg:61.69ms
step:2190/2330 train_time:135114ms step_avg:61.70ms
step:2191/2330 train_time:135176ms step_avg:61.70ms
step:2192/2330 train_time:135240ms step_avg:61.70ms
step:2193/2330 train_time:135301ms step_avg:61.70ms
step:2194/2330 train_time:135365ms step_avg:61.70ms
step:2195/2330 train_time:135424ms step_avg:61.70ms
step:2196/2330 train_time:135488ms step_avg:61.70ms
step:2197/2330 train_time:135549ms step_avg:61.70ms
step:2198/2330 train_time:135612ms step_avg:61.70ms
step:2199/2330 train_time:135674ms step_avg:61.70ms
step:2200/2330 train_time:135739ms step_avg:61.70ms
step:2201/2330 train_time:135800ms step_avg:61.70ms
step:2202/2330 train_time:135863ms step_avg:61.70ms
step:2203/2330 train_time:135924ms step_avg:61.70ms
step:2204/2330 train_time:135988ms step_avg:61.70ms
step:2205/2330 train_time:136049ms step_avg:61.70ms
step:2206/2330 train_time:136112ms step_avg:61.70ms
step:2207/2330 train_time:136173ms step_avg:61.70ms
step:2208/2330 train_time:136238ms step_avg:61.70ms
step:2209/2330 train_time:136299ms step_avg:61.70ms
step:2210/2330 train_time:136364ms step_avg:61.70ms
step:2211/2330 train_time:136423ms step_avg:61.70ms
step:2212/2330 train_time:136488ms step_avg:61.70ms
step:2213/2330 train_time:136547ms step_avg:61.70ms
step:2214/2330 train_time:136612ms step_avg:61.70ms
step:2215/2330 train_time:136673ms step_avg:61.70ms
step:2216/2330 train_time:136737ms step_avg:61.70ms
step:2217/2330 train_time:136798ms step_avg:61.70ms
step:2218/2330 train_time:136862ms step_avg:61.71ms
step:2219/2330 train_time:136922ms step_avg:61.70ms
step:2220/2330 train_time:136986ms step_avg:61.71ms
step:2221/2330 train_time:137046ms step_avg:61.70ms
step:2222/2330 train_time:137110ms step_avg:61.71ms
step:2223/2330 train_time:137170ms step_avg:61.70ms
step:2224/2330 train_time:137235ms step_avg:61.71ms
step:2225/2330 train_time:137297ms step_avg:61.71ms
step:2226/2330 train_time:137361ms step_avg:61.71ms
step:2227/2330 train_time:137422ms step_avg:61.71ms
step:2228/2330 train_time:137485ms step_avg:61.71ms
step:2229/2330 train_time:137545ms step_avg:61.71ms
step:2230/2330 train_time:137608ms step_avg:61.71ms
step:2231/2330 train_time:137669ms step_avg:61.71ms
step:2232/2330 train_time:137733ms step_avg:61.71ms
step:2233/2330 train_time:137794ms step_avg:61.71ms
step:2234/2330 train_time:137858ms step_avg:61.71ms
step:2235/2330 train_time:137920ms step_avg:61.71ms
step:2236/2330 train_time:137984ms step_avg:61.71ms
step:2237/2330 train_time:138044ms step_avg:61.71ms
step:2238/2330 train_time:138108ms step_avg:61.71ms
step:2239/2330 train_time:138168ms step_avg:61.71ms
step:2240/2330 train_time:138233ms step_avg:61.71ms
step:2241/2330 train_time:138294ms step_avg:61.71ms
step:2242/2330 train_time:138359ms step_avg:61.71ms
step:2243/2330 train_time:138420ms step_avg:61.71ms
step:2244/2330 train_time:138484ms step_avg:61.71ms
step:2245/2330 train_time:138544ms step_avg:61.71ms
step:2246/2330 train_time:138608ms step_avg:61.71ms
step:2247/2330 train_time:138668ms step_avg:61.71ms
step:2248/2330 train_time:138731ms step_avg:61.71ms
step:2249/2330 train_time:138791ms step_avg:61.71ms
step:2250/2330 train_time:138855ms step_avg:61.71ms
step:2250/2330 val_loss:4.1357 train_time:138931ms step_avg:61.75ms
step:2251/2330 train_time:138952ms step_avg:61.73ms
step:2252/2330 train_time:138983ms step_avg:61.72ms
step:2253/2330 train_time:139047ms step_avg:61.72ms
step:2254/2330 train_time:139115ms step_avg:61.72ms
step:2255/2330 train_time:139176ms step_avg:61.72ms
step:2256/2330 train_time:139240ms step_avg:61.72ms
step:2257/2330 train_time:139302ms step_avg:61.72ms
step:2258/2330 train_time:139364ms step_avg:61.72ms
step:2259/2330 train_time:139423ms step_avg:61.72ms
step:2260/2330 train_time:139486ms step_avg:61.72ms
step:2261/2330 train_time:139546ms step_avg:61.72ms
step:2262/2330 train_time:139608ms step_avg:61.72ms
step:2263/2330 train_time:139668ms step_avg:61.72ms
step:2264/2330 train_time:139730ms step_avg:61.72ms
step:2265/2330 train_time:139790ms step_avg:61.72ms
step:2266/2330 train_time:139852ms step_avg:61.72ms
step:2267/2330 train_time:139913ms step_avg:61.72ms
step:2268/2330 train_time:139978ms step_avg:61.72ms
step:2269/2330 train_time:140040ms step_avg:61.72ms
step:2270/2330 train_time:140105ms step_avg:61.72ms
step:2271/2330 train_time:140167ms step_avg:61.72ms
step:2272/2330 train_time:140231ms step_avg:61.72ms
step:2273/2330 train_time:140291ms step_avg:61.72ms
step:2274/2330 train_time:140355ms step_avg:61.72ms
step:2275/2330 train_time:140415ms step_avg:61.72ms
step:2276/2330 train_time:140479ms step_avg:61.72ms
step:2277/2330 train_time:140541ms step_avg:61.72ms
step:2278/2330 train_time:140604ms step_avg:61.72ms
step:2279/2330 train_time:140663ms step_avg:61.72ms
step:2280/2330 train_time:140726ms step_avg:61.72ms
step:2281/2330 train_time:140787ms step_avg:61.72ms
step:2282/2330 train_time:140849ms step_avg:61.72ms
step:2283/2330 train_time:140909ms step_avg:61.72ms
step:2284/2330 train_time:140974ms step_avg:61.72ms
step:2285/2330 train_time:141034ms step_avg:61.72ms
step:2286/2330 train_time:141100ms step_avg:61.72ms
step:2287/2330 train_time:141161ms step_avg:61.72ms
step:2288/2330 train_time:141224ms step_avg:61.72ms
step:2289/2330 train_time:141284ms step_avg:61.72ms
step:2290/2330 train_time:141349ms step_avg:61.72ms
step:2291/2330 train_time:141409ms step_avg:61.72ms
step:2292/2330 train_time:141472ms step_avg:61.72ms
step:2293/2330 train_time:141534ms step_avg:61.72ms
step:2294/2330 train_time:141598ms step_avg:61.73ms
step:2295/2330 train_time:141659ms step_avg:61.72ms
step:2296/2330 train_time:141722ms step_avg:61.73ms
step:2297/2330 train_time:141783ms step_avg:61.73ms
step:2298/2330 train_time:141846ms step_avg:61.73ms
step:2299/2330 train_time:141906ms step_avg:61.73ms
step:2300/2330 train_time:141969ms step_avg:61.73ms
step:2301/2330 train_time:142029ms step_avg:61.73ms
step:2302/2330 train_time:142093ms step_avg:61.73ms
step:2303/2330 train_time:142155ms step_avg:61.73ms
step:2304/2330 train_time:142220ms step_avg:61.73ms
step:2305/2330 train_time:142281ms step_avg:61.73ms
step:2306/2330 train_time:142345ms step_avg:61.73ms
step:2307/2330 train_time:142405ms step_avg:61.73ms
step:2308/2330 train_time:142469ms step_avg:61.73ms
step:2309/2330 train_time:142529ms step_avg:61.73ms
step:2310/2330 train_time:142593ms step_avg:61.73ms
step:2311/2330 train_time:142653ms step_avg:61.73ms
step:2312/2330 train_time:142718ms step_avg:61.73ms
step:2313/2330 train_time:142779ms step_avg:61.73ms
step:2314/2330 train_time:142843ms step_avg:61.73ms
step:2315/2330 train_time:142903ms step_avg:61.73ms
step:2316/2330 train_time:142966ms step_avg:61.73ms
step:2317/2330 train_time:143026ms step_avg:61.73ms
step:2318/2330 train_time:143089ms step_avg:61.73ms
step:2319/2330 train_time:143151ms step_avg:61.73ms
step:2320/2330 train_time:143215ms step_avg:61.73ms
step:2321/2330 train_time:143277ms step_avg:61.73ms
step:2322/2330 train_time:143341ms step_avg:61.73ms
step:2323/2330 train_time:143403ms step_avg:61.73ms
step:2324/2330 train_time:143465ms step_avg:61.73ms
step:2325/2330 train_time:143525ms step_avg:61.73ms
step:2326/2330 train_time:143588ms step_avg:61.73ms
step:2327/2330 train_time:143649ms step_avg:61.73ms
step:2328/2330 train_time:143713ms step_avg:61.73ms
step:2329/2330 train_time:143774ms step_avg:61.73ms
step:2330/2330 train_time:143838ms step_avg:61.73ms
step:2330/2330 val_loss:4.1233 train_time:143913ms step_avg:61.77ms
peak memory allocated: 29512 MiB reserved: 44076 MiB
