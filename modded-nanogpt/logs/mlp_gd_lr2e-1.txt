import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = divide_by_norm(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = divide_by_norm(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_gd_lr2e-1"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=2e-1, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 05:39:41 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   34C    P0             113W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   32C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   30C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   34C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.05ms
step:1/2330 train_time:70ms step_avg:70.36ms
step:2/2330 train_time:205ms step_avg:102.49ms
step:3/2330 train_time:217ms step_avg:72.41ms
step:4/2330 train_time:229ms step_avg:57.33ms
step:5/2330 train_time:239ms step_avg:47.84ms
step:6/2330 train_time:273ms step_avg:45.57ms
step:7/2330 train_time:295ms step_avg:42.08ms
step:8/2330 train_time:349ms step_avg:43.57ms
step:9/2330 train_time:370ms step_avg:41.14ms
step:10/2330 train_time:426ms step_avg:42.55ms
step:11/2330 train_time:448ms step_avg:40.72ms
step:12/2330 train_time:504ms step_avg:42.00ms
step:13/2330 train_time:526ms step_avg:40.50ms
step:14/2330 train_time:582ms step_avg:41.56ms
step:15/2330 train_time:603ms step_avg:40.21ms
step:16/2330 train_time:658ms step_avg:41.15ms
step:17/2330 train_time:680ms step_avg:40.00ms
step:18/2330 train_time:736ms step_avg:40.86ms
step:19/2330 train_time:757ms step_avg:39.84ms
step:20/2330 train_time:812ms step_avg:40.61ms
step:21/2330 train_time:834ms step_avg:39.73ms
step:22/2330 train_time:889ms step_avg:40.42ms
step:23/2330 train_time:912ms step_avg:39.63ms
step:24/2330 train_time:966ms step_avg:40.27ms
step:25/2330 train_time:989ms step_avg:39.54ms
step:26/2330 train_time:1044ms step_avg:40.14ms
step:27/2330 train_time:1070ms step_avg:39.63ms
step:28/2330 train_time:1130ms step_avg:40.37ms
step:29/2330 train_time:1157ms step_avg:39.89ms
step:30/2330 train_time:1216ms step_avg:40.52ms
step:31/2330 train_time:1239ms step_avg:39.96ms
step:32/2330 train_time:1296ms step_avg:40.49ms
step:33/2330 train_time:1317ms step_avg:39.92ms
step:34/2330 train_time:1374ms step_avg:40.40ms
step:35/2330 train_time:1395ms step_avg:39.86ms
step:36/2330 train_time:1450ms step_avg:40.28ms
step:37/2330 train_time:1473ms step_avg:39.80ms
step:38/2330 train_time:1528ms step_avg:40.21ms
step:39/2330 train_time:1550ms step_avg:39.75ms
step:40/2330 train_time:1605ms step_avg:40.13ms
step:41/2330 train_time:1628ms step_avg:39.70ms
step:42/2330 train_time:1683ms step_avg:40.06ms
step:43/2330 train_time:1704ms step_avg:39.63ms
step:44/2330 train_time:1758ms step_avg:39.97ms
step:45/2330 train_time:1780ms step_avg:39.55ms
step:46/2330 train_time:1835ms step_avg:39.89ms
step:47/2330 train_time:1856ms step_avg:39.49ms
step:48/2330 train_time:1910ms step_avg:39.80ms
step:49/2330 train_time:1932ms step_avg:39.43ms
step:50/2330 train_time:1988ms step_avg:39.75ms
step:51/2330 train_time:2010ms step_avg:39.42ms
step:52/2330 train_time:2067ms step_avg:39.75ms
step:53/2330 train_time:2092ms step_avg:39.47ms
step:54/2330 train_time:2150ms step_avg:39.82ms
step:55/2330 train_time:2174ms step_avg:39.53ms
step:56/2330 train_time:2230ms step_avg:39.83ms
step:57/2330 train_time:2254ms step_avg:39.54ms
step:58/2330 train_time:2310ms step_avg:39.82ms
step:59/2330 train_time:2332ms step_avg:39.53ms
step:60/2330 train_time:2388ms step_avg:39.80ms
step:61/2330 train_time:2411ms step_avg:39.52ms
step:62/2330 train_time:2466ms step_avg:39.77ms
step:63/2330 train_time:2488ms step_avg:39.49ms
step:64/2330 train_time:2544ms step_avg:39.74ms
step:65/2330 train_time:2565ms step_avg:39.46ms
step:66/2330 train_time:2620ms step_avg:39.70ms
step:67/2330 train_time:2642ms step_avg:39.43ms
step:68/2330 train_time:2696ms step_avg:39.65ms
step:69/2330 train_time:2718ms step_avg:39.39ms
step:70/2330 train_time:2773ms step_avg:39.62ms
step:71/2330 train_time:2795ms step_avg:39.36ms
step:72/2330 train_time:2850ms step_avg:39.58ms
step:73/2330 train_time:2872ms step_avg:39.34ms
step:74/2330 train_time:2927ms step_avg:39.55ms
step:75/2330 train_time:2949ms step_avg:39.32ms
step:76/2330 train_time:3005ms step_avg:39.54ms
step:77/2330 train_time:3028ms step_avg:39.32ms
step:78/2330 train_time:3085ms step_avg:39.54ms
step:79/2330 train_time:3107ms step_avg:39.32ms
step:80/2330 train_time:3163ms step_avg:39.54ms
step:81/2330 train_time:3186ms step_avg:39.34ms
step:82/2330 train_time:3242ms step_avg:39.54ms
step:83/2330 train_time:3265ms step_avg:39.33ms
step:84/2330 train_time:3320ms step_avg:39.53ms
step:85/2330 train_time:3342ms step_avg:39.32ms
step:86/2330 train_time:3398ms step_avg:39.51ms
step:87/2330 train_time:3420ms step_avg:39.31ms
step:88/2330 train_time:3475ms step_avg:39.49ms
step:89/2330 train_time:3497ms step_avg:39.29ms
step:90/2330 train_time:3553ms step_avg:39.47ms
step:91/2330 train_time:3575ms step_avg:39.28ms
step:92/2330 train_time:3630ms step_avg:39.45ms
step:93/2330 train_time:3652ms step_avg:39.27ms
step:94/2330 train_time:3707ms step_avg:39.43ms
step:95/2330 train_time:3729ms step_avg:39.25ms
step:96/2330 train_time:3784ms step_avg:39.42ms
step:97/2330 train_time:3806ms step_avg:39.24ms
step:98/2330 train_time:3860ms step_avg:39.39ms
step:99/2330 train_time:3882ms step_avg:39.22ms
step:100/2330 train_time:3938ms step_avg:39.38ms
step:101/2330 train_time:3960ms step_avg:39.20ms
step:102/2330 train_time:4016ms step_avg:39.37ms
step:103/2330 train_time:4038ms step_avg:39.20ms
step:104/2330 train_time:4094ms step_avg:39.36ms
step:105/2330 train_time:4116ms step_avg:39.20ms
step:106/2330 train_time:4172ms step_avg:39.36ms
step:107/2330 train_time:4194ms step_avg:39.20ms
step:108/2330 train_time:4250ms step_avg:39.35ms
step:109/2330 train_time:4273ms step_avg:39.21ms
step:110/2330 train_time:4329ms step_avg:39.36ms
step:111/2330 train_time:4352ms step_avg:39.21ms
step:112/2330 train_time:4408ms step_avg:39.36ms
step:113/2330 train_time:4431ms step_avg:39.21ms
step:114/2330 train_time:4486ms step_avg:39.35ms
step:115/2330 train_time:4509ms step_avg:39.21ms
step:116/2330 train_time:4565ms step_avg:39.35ms
step:117/2330 train_time:4587ms step_avg:39.20ms
step:118/2330 train_time:4643ms step_avg:39.35ms
step:119/2330 train_time:4665ms step_avg:39.20ms
step:120/2330 train_time:4719ms step_avg:39.33ms
step:121/2330 train_time:4741ms step_avg:39.18ms
step:122/2330 train_time:4796ms step_avg:39.32ms
step:123/2330 train_time:4818ms step_avg:39.17ms
step:124/2330 train_time:4874ms step_avg:39.31ms
step:125/2330 train_time:4895ms step_avg:39.16ms
step:126/2330 train_time:4951ms step_avg:39.29ms
step:127/2330 train_time:4973ms step_avg:39.16ms
step:128/2330 train_time:5029ms step_avg:39.29ms
step:129/2330 train_time:5052ms step_avg:39.16ms
step:130/2330 train_time:5108ms step_avg:39.29ms
step:131/2330 train_time:5131ms step_avg:39.17ms
step:132/2330 train_time:5188ms step_avg:39.30ms
step:133/2330 train_time:5211ms step_avg:39.18ms
step:134/2330 train_time:5267ms step_avg:39.31ms
step:135/2330 train_time:5291ms step_avg:39.19ms
step:136/2330 train_time:5348ms step_avg:39.32ms
step:137/2330 train_time:5370ms step_avg:39.20ms
step:138/2330 train_time:5426ms step_avg:39.32ms
step:139/2330 train_time:5449ms step_avg:39.20ms
step:140/2330 train_time:5505ms step_avg:39.32ms
step:141/2330 train_time:5528ms step_avg:39.20ms
step:142/2330 train_time:5583ms step_avg:39.32ms
step:143/2330 train_time:5606ms step_avg:39.20ms
step:144/2330 train_time:5661ms step_avg:39.31ms
step:145/2330 train_time:5682ms step_avg:39.19ms
step:146/2330 train_time:5738ms step_avg:39.30ms
step:147/2330 train_time:5759ms step_avg:39.18ms
step:148/2330 train_time:5815ms step_avg:39.29ms
step:149/2330 train_time:5836ms step_avg:39.17ms
step:150/2330 train_time:5891ms step_avg:39.28ms
step:151/2330 train_time:5914ms step_avg:39.16ms
step:152/2330 train_time:5969ms step_avg:39.27ms
step:153/2330 train_time:5992ms step_avg:39.16ms
step:154/2330 train_time:6048ms step_avg:39.27ms
step:155/2330 train_time:6071ms step_avg:39.17ms
step:156/2330 train_time:6127ms step_avg:39.28ms
step:157/2330 train_time:6150ms step_avg:39.17ms
step:158/2330 train_time:6206ms step_avg:39.28ms
step:159/2330 train_time:6228ms step_avg:39.17ms
step:160/2330 train_time:6285ms step_avg:39.28ms
step:161/2330 train_time:6307ms step_avg:39.17ms
step:162/2330 train_time:6362ms step_avg:39.27ms
step:163/2330 train_time:6384ms step_avg:39.17ms
step:164/2330 train_time:6440ms step_avg:39.27ms
step:165/2330 train_time:6462ms step_avg:39.17ms
step:166/2330 train_time:6518ms step_avg:39.27ms
step:167/2330 train_time:6540ms step_avg:39.16ms
step:168/2330 train_time:6596ms step_avg:39.26ms
step:169/2330 train_time:6617ms step_avg:39.16ms
step:170/2330 train_time:6673ms step_avg:39.25ms
step:171/2330 train_time:6695ms step_avg:39.15ms
step:172/2330 train_time:6750ms step_avg:39.24ms
step:173/2330 train_time:6772ms step_avg:39.15ms
step:174/2330 train_time:6828ms step_avg:39.24ms
step:175/2330 train_time:6850ms step_avg:39.14ms
step:176/2330 train_time:6905ms step_avg:39.23ms
step:177/2330 train_time:6928ms step_avg:39.14ms
step:178/2330 train_time:6984ms step_avg:39.24ms
step:179/2330 train_time:7006ms step_avg:39.14ms
step:180/2330 train_time:7062ms step_avg:39.24ms
step:181/2330 train_time:7084ms step_avg:39.14ms
step:182/2330 train_time:7141ms step_avg:39.23ms
step:183/2330 train_time:7162ms step_avg:39.14ms
step:184/2330 train_time:7219ms step_avg:39.23ms
step:185/2330 train_time:7241ms step_avg:39.14ms
step:186/2330 train_time:7297ms step_avg:39.23ms
step:187/2330 train_time:7319ms step_avg:39.14ms
step:188/2330 train_time:7375ms step_avg:39.23ms
step:189/2330 train_time:7398ms step_avg:39.14ms
step:190/2330 train_time:7454ms step_avg:39.23ms
step:191/2330 train_time:7476ms step_avg:39.14ms
step:192/2330 train_time:7531ms step_avg:39.22ms
step:193/2330 train_time:7553ms step_avg:39.14ms
step:194/2330 train_time:7609ms step_avg:39.22ms
step:195/2330 train_time:7632ms step_avg:39.14ms
step:196/2330 train_time:7688ms step_avg:39.23ms
step:197/2330 train_time:7711ms step_avg:39.14ms
step:198/2330 train_time:7766ms step_avg:39.22ms
step:199/2330 train_time:7789ms step_avg:39.14ms
step:200/2330 train_time:7844ms step_avg:39.22ms
step:201/2330 train_time:7867ms step_avg:39.14ms
step:202/2330 train_time:7923ms step_avg:39.22ms
step:203/2330 train_time:7945ms step_avg:39.14ms
step:204/2330 train_time:8001ms step_avg:39.22ms
step:205/2330 train_time:8023ms step_avg:39.14ms
step:206/2330 train_time:8079ms step_avg:39.22ms
step:207/2330 train_time:8101ms step_avg:39.13ms
step:208/2330 train_time:8157ms step_avg:39.21ms
step:209/2330 train_time:8179ms step_avg:39.13ms
step:210/2330 train_time:8235ms step_avg:39.21ms
step:211/2330 train_time:8257ms step_avg:39.13ms
step:212/2330 train_time:8313ms step_avg:39.21ms
step:213/2330 train_time:8335ms step_avg:39.13ms
step:214/2330 train_time:8391ms step_avg:39.21ms
step:215/2330 train_time:8414ms step_avg:39.13ms
step:216/2330 train_time:8470ms step_avg:39.21ms
step:217/2330 train_time:8493ms step_avg:39.14ms
step:218/2330 train_time:8549ms step_avg:39.21ms
step:219/2330 train_time:8571ms step_avg:39.14ms
step:220/2330 train_time:8627ms step_avg:39.21ms
step:221/2330 train_time:8650ms step_avg:39.14ms
step:222/2330 train_time:8706ms step_avg:39.21ms
step:223/2330 train_time:8728ms step_avg:39.14ms
step:224/2330 train_time:8784ms step_avg:39.21ms
step:225/2330 train_time:8806ms step_avg:39.14ms
step:226/2330 train_time:8862ms step_avg:39.21ms
step:227/2330 train_time:8883ms step_avg:39.13ms
step:228/2330 train_time:8939ms step_avg:39.21ms
step:229/2330 train_time:8961ms step_avg:39.13ms
step:230/2330 train_time:9017ms step_avg:39.20ms
step:231/2330 train_time:9039ms step_avg:39.13ms
step:232/2330 train_time:9095ms step_avg:39.20ms
step:233/2330 train_time:9117ms step_avg:39.13ms
step:234/2330 train_time:9174ms step_avg:39.21ms
step:235/2330 train_time:9196ms step_avg:39.13ms
step:236/2330 train_time:9252ms step_avg:39.20ms
step:237/2330 train_time:9274ms step_avg:39.13ms
step:238/2330 train_time:9330ms step_avg:39.20ms
step:239/2330 train_time:9353ms step_avg:39.13ms
step:240/2330 train_time:9409ms step_avg:39.20ms
step:241/2330 train_time:9431ms step_avg:39.13ms
step:242/2330 train_time:9488ms step_avg:39.21ms
step:243/2330 train_time:9510ms step_avg:39.14ms
step:244/2330 train_time:9566ms step_avg:39.20ms
step:245/2330 train_time:9588ms step_avg:39.14ms
step:246/2330 train_time:9645ms step_avg:39.21ms
step:247/2330 train_time:9667ms step_avg:39.14ms
step:248/2330 train_time:9723ms step_avg:39.20ms
step:249/2330 train_time:9745ms step_avg:39.14ms
step:250/2330 train_time:9800ms step_avg:39.20ms
step:250/2330 val_loss:5.5079 train_time:9894ms step_avg:39.58ms
step:251/2330 train_time:9907ms step_avg:39.47ms
step:252/2330 train_time:9919ms step_avg:39.36ms
step:253/2330 train_time:9930ms step_avg:39.25ms
step:254/2330 train_time:9956ms step_avg:39.20ms
step:255/2330 train_time:9978ms step_avg:39.13ms
step:256/2330 train_time:10033ms step_avg:39.19ms
step:257/2330 train_time:10054ms step_avg:39.12ms
step:258/2330 train_time:10109ms step_avg:39.18ms
step:259/2330 train_time:10131ms step_avg:39.12ms
step:260/2330 train_time:10186ms step_avg:39.18ms
step:261/2330 train_time:10212ms step_avg:39.13ms
step:262/2330 train_time:10273ms step_avg:39.21ms
step:263/2330 train_time:10300ms step_avg:39.16ms
step:264/2330 train_time:10356ms step_avg:39.23ms
step:265/2330 train_time:10379ms step_avg:39.17ms
step:266/2330 train_time:10436ms step_avg:39.23ms
step:267/2330 train_time:10458ms step_avg:39.17ms
step:268/2330 train_time:10514ms step_avg:39.23ms
step:269/2330 train_time:10536ms step_avg:39.17ms
step:270/2330 train_time:10591ms step_avg:39.23ms
step:271/2330 train_time:10613ms step_avg:39.16ms
step:272/2330 train_time:10668ms step_avg:39.22ms
step:273/2330 train_time:10691ms step_avg:39.16ms
step:274/2330 train_time:10747ms step_avg:39.22ms
step:275/2330 train_time:10769ms step_avg:39.16ms
step:276/2330 train_time:10827ms step_avg:39.23ms
step:277/2330 train_time:10849ms step_avg:39.17ms
step:278/2330 train_time:10906ms step_avg:39.23ms
step:279/2330 train_time:10928ms step_avg:39.17ms
step:280/2330 train_time:10984ms step_avg:39.23ms
step:281/2330 train_time:11005ms step_avg:39.16ms
step:282/2330 train_time:11060ms step_avg:39.22ms
step:283/2330 train_time:11082ms step_avg:39.16ms
step:284/2330 train_time:11139ms step_avg:39.22ms
step:285/2330 train_time:11161ms step_avg:39.16ms
step:286/2330 train_time:11219ms step_avg:39.23ms
step:287/2330 train_time:11242ms step_avg:39.17ms
step:288/2330 train_time:11299ms step_avg:39.23ms
step:289/2330 train_time:11321ms step_avg:39.17ms
step:290/2330 train_time:11378ms step_avg:39.23ms
step:291/2330 train_time:11400ms step_avg:39.18ms
step:292/2330 train_time:11456ms step_avg:39.23ms
step:293/2330 train_time:11478ms step_avg:39.17ms
step:294/2330 train_time:11534ms step_avg:39.23ms
step:295/2330 train_time:11556ms step_avg:39.17ms
step:296/2330 train_time:11612ms step_avg:39.23ms
step:297/2330 train_time:11635ms step_avg:39.17ms
step:298/2330 train_time:11690ms step_avg:39.23ms
step:299/2330 train_time:11712ms step_avg:39.17ms
step:300/2330 train_time:11768ms step_avg:39.23ms
step:301/2330 train_time:11791ms step_avg:39.17ms
step:302/2330 train_time:11847ms step_avg:39.23ms
step:303/2330 train_time:11870ms step_avg:39.17ms
step:304/2330 train_time:11925ms step_avg:39.23ms
step:305/2330 train_time:11948ms step_avg:39.17ms
step:306/2330 train_time:12004ms step_avg:39.23ms
step:307/2330 train_time:12026ms step_avg:39.17ms
step:308/2330 train_time:12082ms step_avg:39.23ms
step:309/2330 train_time:12104ms step_avg:39.17ms
step:310/2330 train_time:12160ms step_avg:39.22ms
step:311/2330 train_time:12181ms step_avg:39.17ms
step:312/2330 train_time:12238ms step_avg:39.22ms
step:313/2330 train_time:12260ms step_avg:39.17ms
step:314/2330 train_time:12317ms step_avg:39.23ms
step:315/2330 train_time:12340ms step_avg:39.17ms
step:316/2330 train_time:12396ms step_avg:39.23ms
step:317/2330 train_time:12419ms step_avg:39.18ms
step:318/2330 train_time:12475ms step_avg:39.23ms
step:319/2330 train_time:12497ms step_avg:39.18ms
step:320/2330 train_time:12553ms step_avg:39.23ms
step:321/2330 train_time:12575ms step_avg:39.17ms
step:322/2330 train_time:12630ms step_avg:39.22ms
step:323/2330 train_time:12653ms step_avg:39.17ms
step:324/2330 train_time:12709ms step_avg:39.22ms
step:325/2330 train_time:12731ms step_avg:39.17ms
step:326/2330 train_time:12787ms step_avg:39.22ms
step:327/2330 train_time:12809ms step_avg:39.17ms
step:328/2330 train_time:12865ms step_avg:39.22ms
step:329/2330 train_time:12888ms step_avg:39.17ms
step:330/2330 train_time:12945ms step_avg:39.23ms
step:331/2330 train_time:12967ms step_avg:39.17ms
step:332/2330 train_time:13023ms step_avg:39.23ms
step:333/2330 train_time:13045ms step_avg:39.17ms
step:334/2330 train_time:13101ms step_avg:39.23ms
step:335/2330 train_time:13123ms step_avg:39.17ms
step:336/2330 train_time:13179ms step_avg:39.22ms
step:337/2330 train_time:13200ms step_avg:39.17ms
step:338/2330 train_time:13257ms step_avg:39.22ms
step:339/2330 train_time:13280ms step_avg:39.17ms
step:340/2330 train_time:13336ms step_avg:39.22ms
step:341/2330 train_time:13358ms step_avg:39.17ms
step:342/2330 train_time:13415ms step_avg:39.22ms
step:343/2330 train_time:13437ms step_avg:39.17ms
step:344/2330 train_time:13493ms step_avg:39.22ms
step:345/2330 train_time:13515ms step_avg:39.17ms
step:346/2330 train_time:13570ms step_avg:39.22ms
step:347/2330 train_time:13593ms step_avg:39.17ms
step:348/2330 train_time:13649ms step_avg:39.22ms
step:349/2330 train_time:13671ms step_avg:39.17ms
step:350/2330 train_time:13727ms step_avg:39.22ms
step:351/2330 train_time:13750ms step_avg:39.17ms
step:352/2330 train_time:13806ms step_avg:39.22ms
step:353/2330 train_time:13828ms step_avg:39.17ms
step:354/2330 train_time:13884ms step_avg:39.22ms
step:355/2330 train_time:13905ms step_avg:39.17ms
step:356/2330 train_time:13961ms step_avg:39.22ms
step:357/2330 train_time:13983ms step_avg:39.17ms
step:358/2330 train_time:14038ms step_avg:39.21ms
step:359/2330 train_time:14060ms step_avg:39.17ms
step:360/2330 train_time:14117ms step_avg:39.21ms
step:361/2330 train_time:14139ms step_avg:39.17ms
step:362/2330 train_time:14195ms step_avg:39.21ms
step:363/2330 train_time:14217ms step_avg:39.17ms
step:364/2330 train_time:14274ms step_avg:39.22ms
step:365/2330 train_time:14297ms step_avg:39.17ms
step:366/2330 train_time:14353ms step_avg:39.22ms
step:367/2330 train_time:14376ms step_avg:39.17ms
step:368/2330 train_time:14431ms step_avg:39.22ms
step:369/2330 train_time:14454ms step_avg:39.17ms
step:370/2330 train_time:14510ms step_avg:39.22ms
step:371/2330 train_time:14533ms step_avg:39.17ms
step:372/2330 train_time:14589ms step_avg:39.22ms
step:373/2330 train_time:14612ms step_avg:39.17ms
step:374/2330 train_time:14668ms step_avg:39.22ms
step:375/2330 train_time:14691ms step_avg:39.17ms
step:376/2330 train_time:14747ms step_avg:39.22ms
step:377/2330 train_time:14769ms step_avg:39.18ms
step:378/2330 train_time:14825ms step_avg:39.22ms
step:379/2330 train_time:14847ms step_avg:39.17ms
step:380/2330 train_time:14904ms step_avg:39.22ms
step:381/2330 train_time:14926ms step_avg:39.18ms
step:382/2330 train_time:14982ms step_avg:39.22ms
step:383/2330 train_time:15004ms step_avg:39.17ms
step:384/2330 train_time:15060ms step_avg:39.22ms
step:385/2330 train_time:15081ms step_avg:39.17ms
step:386/2330 train_time:15137ms step_avg:39.22ms
step:387/2330 train_time:15159ms step_avg:39.17ms
step:388/2330 train_time:15216ms step_avg:39.22ms
step:389/2330 train_time:15238ms step_avg:39.17ms
step:390/2330 train_time:15295ms step_avg:39.22ms
step:391/2330 train_time:15317ms step_avg:39.17ms
step:392/2330 train_time:15374ms step_avg:39.22ms
step:393/2330 train_time:15396ms step_avg:39.17ms
step:394/2330 train_time:15452ms step_avg:39.22ms
step:395/2330 train_time:15474ms step_avg:39.17ms
step:396/2330 train_time:15530ms step_avg:39.22ms
step:397/2330 train_time:15552ms step_avg:39.17ms
step:398/2330 train_time:15609ms step_avg:39.22ms
step:399/2330 train_time:15631ms step_avg:39.18ms
step:400/2330 train_time:15688ms step_avg:39.22ms
step:401/2330 train_time:15710ms step_avg:39.18ms
step:402/2330 train_time:15766ms step_avg:39.22ms
step:403/2330 train_time:15789ms step_avg:39.18ms
step:404/2330 train_time:15846ms step_avg:39.22ms
step:405/2330 train_time:15868ms step_avg:39.18ms
step:406/2330 train_time:15923ms step_avg:39.22ms
step:407/2330 train_time:15946ms step_avg:39.18ms
step:408/2330 train_time:16003ms step_avg:39.22ms
step:409/2330 train_time:16026ms step_avg:39.18ms
step:410/2330 train_time:16083ms step_avg:39.23ms
step:411/2330 train_time:16105ms step_avg:39.18ms
step:412/2330 train_time:16160ms step_avg:39.22ms
step:413/2330 train_time:16182ms step_avg:39.18ms
step:414/2330 train_time:16237ms step_avg:39.22ms
step:415/2330 train_time:16260ms step_avg:39.18ms
step:416/2330 train_time:16317ms step_avg:39.22ms
step:417/2330 train_time:16339ms step_avg:39.18ms
step:418/2330 train_time:16396ms step_avg:39.22ms
step:419/2330 train_time:16418ms step_avg:39.18ms
step:420/2330 train_time:16475ms step_avg:39.23ms
step:421/2330 train_time:16497ms step_avg:39.18ms
step:422/2330 train_time:16553ms step_avg:39.23ms
step:423/2330 train_time:16576ms step_avg:39.19ms
step:424/2330 train_time:16631ms step_avg:39.22ms
step:425/2330 train_time:16654ms step_avg:39.19ms
step:426/2330 train_time:16710ms step_avg:39.23ms
step:427/2330 train_time:16733ms step_avg:39.19ms
step:428/2330 train_time:16789ms step_avg:39.23ms
step:429/2330 train_time:16812ms step_avg:39.19ms
step:430/2330 train_time:16868ms step_avg:39.23ms
step:431/2330 train_time:16891ms step_avg:39.19ms
step:432/2330 train_time:16948ms step_avg:39.23ms
step:433/2330 train_time:16972ms step_avg:39.20ms
step:434/2330 train_time:17028ms step_avg:39.23ms
step:435/2330 train_time:17051ms step_avg:39.20ms
step:436/2330 train_time:17108ms step_avg:39.24ms
step:437/2330 train_time:17130ms step_avg:39.20ms
step:438/2330 train_time:17187ms step_avg:39.24ms
step:439/2330 train_time:17209ms step_avg:39.20ms
step:440/2330 train_time:17265ms step_avg:39.24ms
step:441/2330 train_time:17289ms step_avg:39.20ms
step:442/2330 train_time:17345ms step_avg:39.24ms
step:443/2330 train_time:17368ms step_avg:39.20ms
step:444/2330 train_time:17423ms step_avg:39.24ms
step:445/2330 train_time:17446ms step_avg:39.20ms
step:446/2330 train_time:17502ms step_avg:39.24ms
step:447/2330 train_time:17524ms step_avg:39.20ms
step:448/2330 train_time:17580ms step_avg:39.24ms
step:449/2330 train_time:17602ms step_avg:39.20ms
step:450/2330 train_time:17658ms step_avg:39.24ms
step:451/2330 train_time:17680ms step_avg:39.20ms
step:452/2330 train_time:17736ms step_avg:39.24ms
step:453/2330 train_time:17759ms step_avg:39.20ms
step:454/2330 train_time:17816ms step_avg:39.24ms
step:455/2330 train_time:17839ms step_avg:39.21ms
step:456/2330 train_time:17896ms step_avg:39.25ms
step:457/2330 train_time:17919ms step_avg:39.21ms
step:458/2330 train_time:17975ms step_avg:39.25ms
step:459/2330 train_time:17998ms step_avg:39.21ms
step:460/2330 train_time:18054ms step_avg:39.25ms
step:461/2330 train_time:18077ms step_avg:39.21ms
step:462/2330 train_time:18133ms step_avg:39.25ms
step:463/2330 train_time:18155ms step_avg:39.21ms
step:464/2330 train_time:18211ms step_avg:39.25ms
step:465/2330 train_time:18234ms step_avg:39.21ms
step:466/2330 train_time:18290ms step_avg:39.25ms
step:467/2330 train_time:18313ms step_avg:39.21ms
step:468/2330 train_time:18370ms step_avg:39.25ms
step:469/2330 train_time:18393ms step_avg:39.22ms
step:470/2330 train_time:18449ms step_avg:39.25ms
step:471/2330 train_time:18472ms step_avg:39.22ms
step:472/2330 train_time:18529ms step_avg:39.26ms
step:473/2330 train_time:18552ms step_avg:39.22ms
step:474/2330 train_time:18609ms step_avg:39.26ms
step:475/2330 train_time:18632ms step_avg:39.22ms
step:476/2330 train_time:18688ms step_avg:39.26ms
step:477/2330 train_time:18711ms step_avg:39.23ms
step:478/2330 train_time:18767ms step_avg:39.26ms
step:479/2330 train_time:18790ms step_avg:39.23ms
step:480/2330 train_time:18847ms step_avg:39.26ms
step:481/2330 train_time:18870ms step_avg:39.23ms
step:482/2330 train_time:18927ms step_avg:39.27ms
step:483/2330 train_time:18949ms step_avg:39.23ms
step:484/2330 train_time:19005ms step_avg:39.27ms
step:485/2330 train_time:19028ms step_avg:39.23ms
step:486/2330 train_time:19084ms step_avg:39.27ms
step:487/2330 train_time:19107ms step_avg:39.23ms
step:488/2330 train_time:19163ms step_avg:39.27ms
step:489/2330 train_time:19186ms step_avg:39.23ms
step:490/2330 train_time:19242ms step_avg:39.27ms
step:491/2330 train_time:19264ms step_avg:39.23ms
step:492/2330 train_time:19320ms step_avg:39.27ms
step:493/2330 train_time:19342ms step_avg:39.23ms
step:494/2330 train_time:19398ms step_avg:39.27ms
step:495/2330 train_time:19420ms step_avg:39.23ms
step:496/2330 train_time:19477ms step_avg:39.27ms
step:497/2330 train_time:19499ms step_avg:39.23ms
step:498/2330 train_time:19555ms step_avg:39.27ms
step:499/2330 train_time:19577ms step_avg:39.23ms
step:500/2330 train_time:19633ms step_avg:39.27ms
step:500/2330 val_loss:5.3667 train_time:19729ms step_avg:39.46ms
step:501/2330 train_time:19742ms step_avg:39.40ms
step:502/2330 train_time:19753ms step_avg:39.35ms
step:503/2330 train_time:19764ms step_avg:39.29ms
step:504/2330 train_time:19791ms step_avg:39.27ms
step:505/2330 train_time:19813ms step_avg:39.23ms
step:506/2330 train_time:19868ms step_avg:39.26ms
step:507/2330 train_time:19890ms step_avg:39.23ms
step:508/2330 train_time:19945ms step_avg:39.26ms
step:509/2330 train_time:19967ms step_avg:39.23ms
step:510/2330 train_time:20023ms step_avg:39.26ms
step:511/2330 train_time:20050ms step_avg:39.24ms
step:512/2330 train_time:20109ms step_avg:39.27ms
step:513/2330 train_time:20132ms step_avg:39.24ms
step:514/2330 train_time:20189ms step_avg:39.28ms
step:515/2330 train_time:20212ms step_avg:39.25ms
step:516/2330 train_time:20268ms step_avg:39.28ms
step:517/2330 train_time:20291ms step_avg:39.25ms
step:518/2330 train_time:20347ms step_avg:39.28ms
step:519/2330 train_time:20369ms step_avg:39.25ms
step:520/2330 train_time:20425ms step_avg:39.28ms
step:521/2330 train_time:20447ms step_avg:39.25ms
step:522/2330 train_time:20503ms step_avg:39.28ms
step:523/2330 train_time:20524ms step_avg:39.24ms
step:524/2330 train_time:20580ms step_avg:39.27ms
step:525/2330 train_time:20601ms step_avg:39.24ms
step:526/2330 train_time:20658ms step_avg:39.27ms
step:527/2330 train_time:20680ms step_avg:39.24ms
step:528/2330 train_time:20737ms step_avg:39.27ms
step:529/2330 train_time:20759ms step_avg:39.24ms
step:530/2330 train_time:20815ms step_avg:39.27ms
step:531/2330 train_time:20836ms step_avg:39.24ms
step:532/2330 train_time:20891ms step_avg:39.27ms
step:533/2330 train_time:20914ms step_avg:39.24ms
step:534/2330 train_time:20970ms step_avg:39.27ms
step:535/2330 train_time:20993ms step_avg:39.24ms
step:536/2330 train_time:21051ms step_avg:39.28ms
step:537/2330 train_time:21075ms step_avg:39.25ms
step:538/2330 train_time:21131ms step_avg:39.28ms
step:539/2330 train_time:21154ms step_avg:39.25ms
step:540/2330 train_time:21211ms step_avg:39.28ms
step:541/2330 train_time:21234ms step_avg:39.25ms
step:542/2330 train_time:21291ms step_avg:39.28ms
step:543/2330 train_time:21314ms step_avg:39.25ms
step:544/2330 train_time:21370ms step_avg:39.28ms
step:545/2330 train_time:21393ms step_avg:39.25ms
step:546/2330 train_time:21450ms step_avg:39.29ms
step:547/2330 train_time:21473ms step_avg:39.26ms
step:548/2330 train_time:21529ms step_avg:39.29ms
step:549/2330 train_time:21551ms step_avg:39.26ms
step:550/2330 train_time:21607ms step_avg:39.29ms
step:551/2330 train_time:21630ms step_avg:39.26ms
step:552/2330 train_time:21686ms step_avg:39.29ms
step:553/2330 train_time:21708ms step_avg:39.26ms
step:554/2330 train_time:21765ms step_avg:39.29ms
step:555/2330 train_time:21787ms step_avg:39.26ms
step:556/2330 train_time:21842ms step_avg:39.28ms
step:557/2330 train_time:21863ms step_avg:39.25ms
step:558/2330 train_time:21918ms step_avg:39.28ms
step:559/2330 train_time:21941ms step_avg:39.25ms
step:560/2330 train_time:21997ms step_avg:39.28ms
step:561/2330 train_time:22019ms step_avg:39.25ms
step:562/2330 train_time:22077ms step_avg:39.28ms
step:563/2330 train_time:22099ms step_avg:39.25ms
step:564/2330 train_time:22156ms step_avg:39.28ms
step:565/2330 train_time:22178ms step_avg:39.25ms
step:566/2330 train_time:22234ms step_avg:39.28ms
step:567/2330 train_time:22257ms step_avg:39.25ms
step:568/2330 train_time:22313ms step_avg:39.28ms
step:569/2330 train_time:22335ms step_avg:39.25ms
step:570/2330 train_time:22392ms step_avg:39.28ms
step:571/2330 train_time:22414ms step_avg:39.25ms
step:572/2330 train_time:22470ms step_avg:39.28ms
step:573/2330 train_time:22493ms step_avg:39.26ms
step:574/2330 train_time:22550ms step_avg:39.29ms
step:575/2330 train_time:22573ms step_avg:39.26ms
step:576/2330 train_time:22629ms step_avg:39.29ms
step:577/2330 train_time:22652ms step_avg:39.26ms
step:578/2330 train_time:22708ms step_avg:39.29ms
step:579/2330 train_time:22731ms step_avg:39.26ms
step:580/2330 train_time:22788ms step_avg:39.29ms
step:581/2330 train_time:22810ms step_avg:39.26ms
step:582/2330 train_time:22866ms step_avg:39.29ms
step:583/2330 train_time:22889ms step_avg:39.26ms
step:584/2330 train_time:22945ms step_avg:39.29ms
step:585/2330 train_time:22968ms step_avg:39.26ms
step:586/2330 train_time:23023ms step_avg:39.29ms
step:587/2330 train_time:23046ms step_avg:39.26ms
step:588/2330 train_time:23101ms step_avg:39.29ms
step:589/2330 train_time:23123ms step_avg:39.26ms
step:590/2330 train_time:23179ms step_avg:39.29ms
step:591/2330 train_time:23201ms step_avg:39.26ms
step:592/2330 train_time:23258ms step_avg:39.29ms
step:593/2330 train_time:23280ms step_avg:39.26ms
step:594/2330 train_time:23336ms step_avg:39.29ms
step:595/2330 train_time:23359ms step_avg:39.26ms
step:596/2330 train_time:23416ms step_avg:39.29ms
step:597/2330 train_time:23438ms step_avg:39.26ms
step:598/2330 train_time:23495ms step_avg:39.29ms
step:599/2330 train_time:23517ms step_avg:39.26ms
step:600/2330 train_time:23573ms step_avg:39.29ms
step:601/2330 train_time:23595ms step_avg:39.26ms
step:602/2330 train_time:23651ms step_avg:39.29ms
step:603/2330 train_time:23675ms step_avg:39.26ms
step:604/2330 train_time:23730ms step_avg:39.29ms
step:605/2330 train_time:23753ms step_avg:39.26ms
step:606/2330 train_time:23809ms step_avg:39.29ms
step:607/2330 train_time:23832ms step_avg:39.26ms
step:608/2330 train_time:23889ms step_avg:39.29ms
step:609/2330 train_time:23911ms step_avg:39.26ms
step:610/2330 train_time:23968ms step_avg:39.29ms
step:611/2330 train_time:23990ms step_avg:39.26ms
step:612/2330 train_time:24046ms step_avg:39.29ms
step:613/2330 train_time:24069ms step_avg:39.26ms
step:614/2330 train_time:24125ms step_avg:39.29ms
step:615/2330 train_time:24148ms step_avg:39.27ms
step:616/2330 train_time:24205ms step_avg:39.29ms
step:617/2330 train_time:24227ms step_avg:39.27ms
step:618/2330 train_time:24283ms step_avg:39.29ms
step:619/2330 train_time:24305ms step_avg:39.27ms
step:620/2330 train_time:24361ms step_avg:39.29ms
step:621/2330 train_time:24383ms step_avg:39.26ms
step:622/2330 train_time:24440ms step_avg:39.29ms
step:623/2330 train_time:24462ms step_avg:39.26ms
step:624/2330 train_time:24519ms step_avg:39.29ms
step:625/2330 train_time:24540ms step_avg:39.26ms
step:626/2330 train_time:24597ms step_avg:39.29ms
step:627/2330 train_time:24619ms step_avg:39.26ms
step:628/2330 train_time:24675ms step_avg:39.29ms
step:629/2330 train_time:24698ms step_avg:39.26ms
step:630/2330 train_time:24755ms step_avg:39.29ms
step:631/2330 train_time:24777ms step_avg:39.27ms
step:632/2330 train_time:24833ms step_avg:39.29ms
step:633/2330 train_time:24856ms step_avg:39.27ms
step:634/2330 train_time:24913ms step_avg:39.29ms
step:635/2330 train_time:24935ms step_avg:39.27ms
step:636/2330 train_time:24991ms step_avg:39.29ms
step:637/2330 train_time:25014ms step_avg:39.27ms
step:638/2330 train_time:25070ms step_avg:39.29ms
step:639/2330 train_time:25093ms step_avg:39.27ms
step:640/2330 train_time:25149ms step_avg:39.30ms
step:641/2330 train_time:25173ms step_avg:39.27ms
step:642/2330 train_time:25229ms step_avg:39.30ms
step:643/2330 train_time:25253ms step_avg:39.27ms
step:644/2330 train_time:25310ms step_avg:39.30ms
step:645/2330 train_time:25333ms step_avg:39.28ms
step:646/2330 train_time:25389ms step_avg:39.30ms
step:647/2330 train_time:25412ms step_avg:39.28ms
step:648/2330 train_time:25469ms step_avg:39.30ms
step:649/2330 train_time:25492ms step_avg:39.28ms
step:650/2330 train_time:25548ms step_avg:39.30ms
step:651/2330 train_time:25572ms step_avg:39.28ms
step:652/2330 train_time:25628ms step_avg:39.31ms
step:653/2330 train_time:25650ms step_avg:39.28ms
step:654/2330 train_time:25707ms step_avg:39.31ms
step:655/2330 train_time:25729ms step_avg:39.28ms
step:656/2330 train_time:25785ms step_avg:39.31ms
step:657/2330 train_time:25807ms step_avg:39.28ms
step:658/2330 train_time:25862ms step_avg:39.30ms
step:659/2330 train_time:25884ms step_avg:39.28ms
step:660/2330 train_time:25940ms step_avg:39.30ms
step:661/2330 train_time:25962ms step_avg:39.28ms
step:662/2330 train_time:26019ms step_avg:39.30ms
step:663/2330 train_time:26040ms step_avg:39.28ms
step:664/2330 train_time:26097ms step_avg:39.30ms
step:665/2330 train_time:26119ms step_avg:39.28ms
step:666/2330 train_time:26176ms step_avg:39.30ms
step:667/2330 train_time:26199ms step_avg:39.28ms
step:668/2330 train_time:26256ms step_avg:39.30ms
step:669/2330 train_time:26278ms step_avg:39.28ms
step:670/2330 train_time:26334ms step_avg:39.30ms
step:671/2330 train_time:26356ms step_avg:39.28ms
step:672/2330 train_time:26412ms step_avg:39.30ms
step:673/2330 train_time:26435ms step_avg:39.28ms
step:674/2330 train_time:26491ms step_avg:39.30ms
step:675/2330 train_time:26514ms step_avg:39.28ms
step:676/2330 train_time:26570ms step_avg:39.31ms
step:677/2330 train_time:26594ms step_avg:39.28ms
step:678/2330 train_time:26650ms step_avg:39.31ms
step:679/2330 train_time:26673ms step_avg:39.28ms
step:680/2330 train_time:26730ms step_avg:39.31ms
step:681/2330 train_time:26753ms step_avg:39.28ms
step:682/2330 train_time:26809ms step_avg:39.31ms
step:683/2330 train_time:26832ms step_avg:39.29ms
step:684/2330 train_time:26889ms step_avg:39.31ms
step:685/2330 train_time:26911ms step_avg:39.29ms
step:686/2330 train_time:26968ms step_avg:39.31ms
step:687/2330 train_time:26991ms step_avg:39.29ms
step:688/2330 train_time:27047ms step_avg:39.31ms
step:689/2330 train_time:27070ms step_avg:39.29ms
step:690/2330 train_time:27126ms step_avg:39.31ms
step:691/2330 train_time:27149ms step_avg:39.29ms
step:692/2330 train_time:27205ms step_avg:39.31ms
step:693/2330 train_time:27227ms step_avg:39.29ms
step:694/2330 train_time:27284ms step_avg:39.31ms
step:695/2330 train_time:27306ms step_avg:39.29ms
step:696/2330 train_time:27362ms step_avg:39.31ms
step:697/2330 train_time:27384ms step_avg:39.29ms
step:698/2330 train_time:27440ms step_avg:39.31ms
step:699/2330 train_time:27462ms step_avg:39.29ms
step:700/2330 train_time:27519ms step_avg:39.31ms
step:701/2330 train_time:27541ms step_avg:39.29ms
step:702/2330 train_time:27597ms step_avg:39.31ms
step:703/2330 train_time:27619ms step_avg:39.29ms
step:704/2330 train_time:27676ms step_avg:39.31ms
step:705/2330 train_time:27698ms step_avg:39.29ms
step:706/2330 train_time:27755ms step_avg:39.31ms
step:707/2330 train_time:27777ms step_avg:39.29ms
step:708/2330 train_time:27833ms step_avg:39.31ms
step:709/2330 train_time:27856ms step_avg:39.29ms
step:710/2330 train_time:27912ms step_avg:39.31ms
step:711/2330 train_time:27935ms step_avg:39.29ms
step:712/2330 train_time:27991ms step_avg:39.31ms
step:713/2330 train_time:28014ms step_avg:39.29ms
step:714/2330 train_time:28070ms step_avg:39.31ms
step:715/2330 train_time:28093ms step_avg:39.29ms
step:716/2330 train_time:28150ms step_avg:39.32ms
step:717/2330 train_time:28173ms step_avg:39.29ms
step:718/2330 train_time:28229ms step_avg:39.32ms
step:719/2330 train_time:28252ms step_avg:39.29ms
step:720/2330 train_time:28309ms step_avg:39.32ms
step:721/2330 train_time:28332ms step_avg:39.30ms
step:722/2330 train_time:28388ms step_avg:39.32ms
step:723/2330 train_time:28411ms step_avg:39.30ms
step:724/2330 train_time:28467ms step_avg:39.32ms
step:725/2330 train_time:28490ms step_avg:39.30ms
step:726/2330 train_time:28545ms step_avg:39.32ms
step:727/2330 train_time:28568ms step_avg:39.30ms
step:728/2330 train_time:28623ms step_avg:39.32ms
step:729/2330 train_time:28647ms step_avg:39.30ms
step:730/2330 train_time:28703ms step_avg:39.32ms
step:731/2330 train_time:28725ms step_avg:39.30ms
step:732/2330 train_time:28780ms step_avg:39.32ms
step:733/2330 train_time:28802ms step_avg:39.29ms
step:734/2330 train_time:28859ms step_avg:39.32ms
step:735/2330 train_time:28881ms step_avg:39.29ms
step:736/2330 train_time:28938ms step_avg:39.32ms
step:737/2330 train_time:28960ms step_avg:39.29ms
step:738/2330 train_time:29017ms step_avg:39.32ms
step:739/2330 train_time:29039ms step_avg:39.29ms
step:740/2330 train_time:29096ms step_avg:39.32ms
step:741/2330 train_time:29118ms step_avg:39.30ms
step:742/2330 train_time:29175ms step_avg:39.32ms
step:743/2330 train_time:29197ms step_avg:39.30ms
step:744/2330 train_time:29253ms step_avg:39.32ms
step:745/2330 train_time:29276ms step_avg:39.30ms
step:746/2330 train_time:29332ms step_avg:39.32ms
step:747/2330 train_time:29355ms step_avg:39.30ms
step:748/2330 train_time:29411ms step_avg:39.32ms
step:749/2330 train_time:29434ms step_avg:39.30ms
step:750/2330 train_time:29490ms step_avg:39.32ms
step:750/2330 val_loss:5.2933 train_time:29588ms step_avg:39.45ms
step:751/2330 train_time:29601ms step_avg:39.42ms
step:752/2330 train_time:29613ms step_avg:39.38ms
step:753/2330 train_time:29624ms step_avg:39.34ms
step:754/2330 train_time:29652ms step_avg:39.33ms
step:755/2330 train_time:29674ms step_avg:39.30ms
step:756/2330 train_time:29729ms step_avg:39.32ms
step:757/2330 train_time:29750ms step_avg:39.30ms
step:758/2330 train_time:29806ms step_avg:39.32ms
step:759/2330 train_time:29827ms step_avg:39.30ms
step:760/2330 train_time:29883ms step_avg:39.32ms
step:761/2330 train_time:29907ms step_avg:39.30ms
step:762/2330 train_time:29967ms step_avg:39.33ms
step:763/2330 train_time:29989ms step_avg:39.30ms
step:764/2330 train_time:30046ms step_avg:39.33ms
step:765/2330 train_time:30069ms step_avg:39.31ms
step:766/2330 train_time:30125ms step_avg:39.33ms
step:767/2330 train_time:30146ms step_avg:39.30ms
step:768/2330 train_time:30203ms step_avg:39.33ms
step:769/2330 train_time:30225ms step_avg:39.30ms
step:770/2330 train_time:30282ms step_avg:39.33ms
step:771/2330 train_time:30304ms step_avg:39.30ms
step:772/2330 train_time:30359ms step_avg:39.33ms
step:773/2330 train_time:30381ms step_avg:39.30ms
step:774/2330 train_time:30437ms step_avg:39.32ms
step:775/2330 train_time:30459ms step_avg:39.30ms
step:776/2330 train_time:30515ms step_avg:39.32ms
step:777/2330 train_time:30539ms step_avg:39.30ms
step:778/2330 train_time:30595ms step_avg:39.33ms
step:779/2330 train_time:30618ms step_avg:39.30ms
step:780/2330 train_time:30674ms step_avg:39.33ms
step:781/2330 train_time:30697ms step_avg:39.30ms
step:782/2330 train_time:30753ms step_avg:39.33ms
step:783/2330 train_time:30776ms step_avg:39.30ms
step:784/2330 train_time:30832ms step_avg:39.33ms
step:785/2330 train_time:30857ms step_avg:39.31ms
step:786/2330 train_time:30914ms step_avg:39.33ms
step:787/2330 train_time:30938ms step_avg:39.31ms
step:788/2330 train_time:30995ms step_avg:39.33ms
step:789/2330 train_time:31019ms step_avg:39.31ms
step:790/2330 train_time:31076ms step_avg:39.34ms
step:791/2330 train_time:31099ms step_avg:39.32ms
step:792/2330 train_time:31155ms step_avg:39.34ms
step:793/2330 train_time:31178ms step_avg:39.32ms
step:794/2330 train_time:31234ms step_avg:39.34ms
step:795/2330 train_time:31256ms step_avg:39.32ms
step:796/2330 train_time:31313ms step_avg:39.34ms
step:797/2330 train_time:31335ms step_avg:39.32ms
step:798/2330 train_time:31390ms step_avg:39.34ms
step:799/2330 train_time:31413ms step_avg:39.32ms
step:800/2330 train_time:31470ms step_avg:39.34ms
step:801/2330 train_time:31492ms step_avg:39.32ms
step:802/2330 train_time:31548ms step_avg:39.34ms
step:803/2330 train_time:31570ms step_avg:39.32ms
step:804/2330 train_time:31626ms step_avg:39.34ms
step:805/2330 train_time:31648ms step_avg:39.31ms
step:806/2330 train_time:31703ms step_avg:39.33ms
step:807/2330 train_time:31725ms step_avg:39.31ms
step:808/2330 train_time:31782ms step_avg:39.33ms
step:809/2330 train_time:31804ms step_avg:39.31ms
step:810/2330 train_time:31862ms step_avg:39.34ms
step:811/2330 train_time:31884ms step_avg:39.31ms
step:812/2330 train_time:31942ms step_avg:39.34ms
step:813/2330 train_time:31964ms step_avg:39.32ms
step:814/2330 train_time:32021ms step_avg:39.34ms
step:815/2330 train_time:32043ms step_avg:39.32ms
step:816/2330 train_time:32100ms step_avg:39.34ms
step:817/2330 train_time:32122ms step_avg:39.32ms
step:818/2330 train_time:32178ms step_avg:39.34ms
step:819/2330 train_time:32200ms step_avg:39.32ms
step:820/2330 train_time:32256ms step_avg:39.34ms
step:821/2330 train_time:32278ms step_avg:39.32ms
step:822/2330 train_time:32333ms step_avg:39.34ms
step:823/2330 train_time:32357ms step_avg:39.32ms
step:824/2330 train_time:32413ms step_avg:39.34ms
step:825/2330 train_time:32436ms step_avg:39.32ms
step:826/2330 train_time:32492ms step_avg:39.34ms
step:827/2330 train_time:32516ms step_avg:39.32ms
step:828/2330 train_time:32572ms step_avg:39.34ms
step:829/2330 train_time:32595ms step_avg:39.32ms
step:830/2330 train_time:32651ms step_avg:39.34ms
step:831/2330 train_time:32673ms step_avg:39.32ms
step:832/2330 train_time:32730ms step_avg:39.34ms
step:833/2330 train_time:32753ms step_avg:39.32ms
step:834/2330 train_time:32809ms step_avg:39.34ms
step:835/2330 train_time:32831ms step_avg:39.32ms
step:836/2330 train_time:32888ms step_avg:39.34ms
step:837/2330 train_time:32911ms step_avg:39.32ms
step:838/2330 train_time:32967ms step_avg:39.34ms
step:839/2330 train_time:32989ms step_avg:39.32ms
step:840/2330 train_time:33045ms step_avg:39.34ms
step:841/2330 train_time:33067ms step_avg:39.32ms
step:842/2330 train_time:33124ms step_avg:39.34ms
step:843/2330 train_time:33146ms step_avg:39.32ms
step:844/2330 train_time:33205ms step_avg:39.34ms
step:845/2330 train_time:33227ms step_avg:39.32ms
step:846/2330 train_time:33285ms step_avg:39.34ms
step:847/2330 train_time:33307ms step_avg:39.32ms
step:848/2330 train_time:33363ms step_avg:39.34ms
step:849/2330 train_time:33385ms step_avg:39.32ms
step:850/2330 train_time:33441ms step_avg:39.34ms
step:851/2330 train_time:33464ms step_avg:39.32ms
step:852/2330 train_time:33520ms step_avg:39.34ms
step:853/2330 train_time:33543ms step_avg:39.32ms
step:854/2330 train_time:33599ms step_avg:39.34ms
step:855/2330 train_time:33621ms step_avg:39.32ms
step:856/2330 train_time:33677ms step_avg:39.34ms
step:857/2330 train_time:33699ms step_avg:39.32ms
step:858/2330 train_time:33754ms step_avg:39.34ms
step:859/2330 train_time:33777ms step_avg:39.32ms
step:860/2330 train_time:33834ms step_avg:39.34ms
step:861/2330 train_time:33857ms step_avg:39.32ms
step:862/2330 train_time:33913ms step_avg:39.34ms
step:863/2330 train_time:33937ms step_avg:39.32ms
step:864/2330 train_time:33994ms step_avg:39.34ms
step:865/2330 train_time:34017ms step_avg:39.33ms
step:866/2330 train_time:34074ms step_avg:39.35ms
step:867/2330 train_time:34098ms step_avg:39.33ms
step:868/2330 train_time:34154ms step_avg:39.35ms
step:869/2330 train_time:34177ms step_avg:39.33ms
step:870/2330 train_time:34234ms step_avg:39.35ms
step:871/2330 train_time:34257ms step_avg:39.33ms
step:872/2330 train_time:34314ms step_avg:39.35ms
step:873/2330 train_time:34337ms step_avg:39.33ms
step:874/2330 train_time:34393ms step_avg:39.35ms
step:875/2330 train_time:34416ms step_avg:39.33ms
step:876/2330 train_time:34473ms step_avg:39.35ms
step:877/2330 train_time:34495ms step_avg:39.33ms
step:878/2330 train_time:34551ms step_avg:39.35ms
step:879/2330 train_time:34574ms step_avg:39.33ms
step:880/2330 train_time:34630ms step_avg:39.35ms
step:881/2330 train_time:34653ms step_avg:39.33ms
step:882/2330 train_time:34709ms step_avg:39.35ms
step:883/2330 train_time:34731ms step_avg:39.33ms
step:884/2330 train_time:34787ms step_avg:39.35ms
step:885/2330 train_time:34809ms step_avg:39.33ms
step:886/2330 train_time:34864ms step_avg:39.35ms
step:887/2330 train_time:34886ms step_avg:39.33ms
step:888/2330 train_time:34943ms step_avg:39.35ms
step:889/2330 train_time:34965ms step_avg:39.33ms
step:890/2330 train_time:35022ms step_avg:39.35ms
step:891/2330 train_time:35043ms step_avg:39.33ms
step:892/2330 train_time:35100ms step_avg:39.35ms
step:893/2330 train_time:35123ms step_avg:39.33ms
step:894/2330 train_time:35180ms step_avg:39.35ms
step:895/2330 train_time:35203ms step_avg:39.33ms
step:896/2330 train_time:35260ms step_avg:39.35ms
step:897/2330 train_time:35282ms step_avg:39.33ms
step:898/2330 train_time:35338ms step_avg:39.35ms
step:899/2330 train_time:35361ms step_avg:39.33ms
step:900/2330 train_time:35417ms step_avg:39.35ms
step:901/2330 train_time:35438ms step_avg:39.33ms
step:902/2330 train_time:35494ms step_avg:39.35ms
step:903/2330 train_time:35517ms step_avg:39.33ms
step:904/2330 train_time:35574ms step_avg:39.35ms
step:905/2330 train_time:35596ms step_avg:39.33ms
step:906/2330 train_time:35653ms step_avg:39.35ms
step:907/2330 train_time:35676ms step_avg:39.33ms
step:908/2330 train_time:35732ms step_avg:39.35ms
step:909/2330 train_time:35756ms step_avg:39.34ms
step:910/2330 train_time:35812ms step_avg:39.35ms
step:911/2330 train_time:35835ms step_avg:39.34ms
step:912/2330 train_time:35892ms step_avg:39.35ms
step:913/2330 train_time:35915ms step_avg:39.34ms
step:914/2330 train_time:35971ms step_avg:39.36ms
step:915/2330 train_time:35994ms step_avg:39.34ms
step:916/2330 train_time:36050ms step_avg:39.36ms
step:917/2330 train_time:36073ms step_avg:39.34ms
step:918/2330 train_time:36130ms step_avg:39.36ms
step:919/2330 train_time:36152ms step_avg:39.34ms
step:920/2330 train_time:36209ms step_avg:39.36ms
step:921/2330 train_time:36231ms step_avg:39.34ms
step:922/2330 train_time:36287ms step_avg:39.36ms
step:923/2330 train_time:36310ms step_avg:39.34ms
step:924/2330 train_time:36366ms step_avg:39.36ms
step:925/2330 train_time:36388ms step_avg:39.34ms
step:926/2330 train_time:36445ms step_avg:39.36ms
step:927/2330 train_time:36467ms step_avg:39.34ms
step:928/2330 train_time:36523ms step_avg:39.36ms
step:929/2330 train_time:36545ms step_avg:39.34ms
step:930/2330 train_time:36603ms step_avg:39.36ms
step:931/2330 train_time:36625ms step_avg:39.34ms
step:932/2330 train_time:36682ms step_avg:39.36ms
step:933/2330 train_time:36704ms step_avg:39.34ms
step:934/2330 train_time:36761ms step_avg:39.36ms
step:935/2330 train_time:36783ms step_avg:39.34ms
step:936/2330 train_time:36840ms step_avg:39.36ms
step:937/2330 train_time:36862ms step_avg:39.34ms
step:938/2330 train_time:36918ms step_avg:39.36ms
step:939/2330 train_time:36941ms step_avg:39.34ms
step:940/2330 train_time:36997ms step_avg:39.36ms
step:941/2330 train_time:37019ms step_avg:39.34ms
step:942/2330 train_time:37075ms step_avg:39.36ms
step:943/2330 train_time:37098ms step_avg:39.34ms
step:944/2330 train_time:37154ms step_avg:39.36ms
step:945/2330 train_time:37177ms step_avg:39.34ms
step:946/2330 train_time:37234ms step_avg:39.36ms
step:947/2330 train_time:37257ms step_avg:39.34ms
step:948/2330 train_time:37313ms step_avg:39.36ms
step:949/2330 train_time:37336ms step_avg:39.34ms
step:950/2330 train_time:37392ms step_avg:39.36ms
step:951/2330 train_time:37415ms step_avg:39.34ms
step:952/2330 train_time:37472ms step_avg:39.36ms
step:953/2330 train_time:37495ms step_avg:39.34ms
step:954/2330 train_time:37551ms step_avg:39.36ms
step:955/2330 train_time:37574ms step_avg:39.34ms
step:956/2330 train_time:37630ms step_avg:39.36ms
step:957/2330 train_time:37652ms step_avg:39.34ms
step:958/2330 train_time:37708ms step_avg:39.36ms
step:959/2330 train_time:37730ms step_avg:39.34ms
step:960/2330 train_time:37786ms step_avg:39.36ms
step:961/2330 train_time:37808ms step_avg:39.34ms
step:962/2330 train_time:37863ms step_avg:39.36ms
step:963/2330 train_time:37885ms step_avg:39.34ms
step:964/2330 train_time:37941ms step_avg:39.36ms
step:965/2330 train_time:37963ms step_avg:39.34ms
step:966/2330 train_time:38020ms step_avg:39.36ms
step:967/2330 train_time:38042ms step_avg:39.34ms
step:968/2330 train_time:38099ms step_avg:39.36ms
step:969/2330 train_time:38121ms step_avg:39.34ms
step:970/2330 train_time:38177ms step_avg:39.36ms
step:971/2330 train_time:38200ms step_avg:39.34ms
step:972/2330 train_time:38256ms step_avg:39.36ms
step:973/2330 train_time:38279ms step_avg:39.34ms
step:974/2330 train_time:38335ms step_avg:39.36ms
step:975/2330 train_time:38358ms step_avg:39.34ms
step:976/2330 train_time:38414ms step_avg:39.36ms
step:977/2330 train_time:38437ms step_avg:39.34ms
step:978/2330 train_time:38493ms step_avg:39.36ms
step:979/2330 train_time:38516ms step_avg:39.34ms
step:980/2330 train_time:38573ms step_avg:39.36ms
step:981/2330 train_time:38596ms step_avg:39.34ms
step:982/2330 train_time:38653ms step_avg:39.36ms
step:983/2330 train_time:38676ms step_avg:39.34ms
step:984/2330 train_time:38732ms step_avg:39.36ms
step:985/2330 train_time:38756ms step_avg:39.35ms
step:986/2330 train_time:38812ms step_avg:39.36ms
step:987/2330 train_time:38834ms step_avg:39.35ms
step:988/2330 train_time:38890ms step_avg:39.36ms
step:989/2330 train_time:38913ms step_avg:39.35ms
step:990/2330 train_time:38970ms step_avg:39.36ms
step:991/2330 train_time:38992ms step_avg:39.35ms
step:992/2330 train_time:39049ms step_avg:39.36ms
step:993/2330 train_time:39071ms step_avg:39.35ms
step:994/2330 train_time:39128ms step_avg:39.36ms
step:995/2330 train_time:39151ms step_avg:39.35ms
step:996/2330 train_time:39207ms step_avg:39.36ms
step:997/2330 train_time:39229ms step_avg:39.35ms
step:998/2330 train_time:39285ms step_avg:39.36ms
step:999/2330 train_time:39307ms step_avg:39.35ms
step:1000/2330 train_time:39363ms step_avg:39.36ms
step:1000/2330 val_loss:5.2564 train_time:39459ms step_avg:39.46ms
step:1001/2330 train_time:39471ms step_avg:39.43ms
step:1002/2330 train_time:39483ms step_avg:39.40ms
step:1003/2330 train_time:39493ms step_avg:39.37ms
step:1004/2330 train_time:39522ms step_avg:39.36ms
step:1005/2330 train_time:39543ms step_avg:39.35ms
step:1006/2330 train_time:39598ms step_avg:39.36ms
step:1007/2330 train_time:39620ms step_avg:39.34ms
step:1008/2330 train_time:39675ms step_avg:39.36ms
step:1009/2330 train_time:39697ms step_avg:39.34ms
step:1010/2330 train_time:39754ms step_avg:39.36ms
step:1011/2330 train_time:39780ms step_avg:39.35ms
step:1012/2330 train_time:39838ms step_avg:39.37ms
step:1013/2330 train_time:39863ms step_avg:39.35ms
step:1014/2330 train_time:39920ms step_avg:39.37ms
step:1015/2330 train_time:39943ms step_avg:39.35ms
step:1016/2330 train_time:40000ms step_avg:39.37ms
step:1017/2330 train_time:40023ms step_avg:39.35ms
step:1018/2330 train_time:40079ms step_avg:39.37ms
step:1019/2330 train_time:40100ms step_avg:39.35ms
step:1020/2330 train_time:40156ms step_avg:39.37ms
step:1021/2330 train_time:40178ms step_avg:39.35ms
step:1022/2330 train_time:40233ms step_avg:39.37ms
step:1023/2330 train_time:40256ms step_avg:39.35ms
step:1024/2330 train_time:40313ms step_avg:39.37ms
step:1025/2330 train_time:40335ms step_avg:39.35ms
step:1026/2330 train_time:40392ms step_avg:39.37ms
step:1027/2330 train_time:40415ms step_avg:39.35ms
step:1028/2330 train_time:40471ms step_avg:39.37ms
step:1029/2330 train_time:40494ms step_avg:39.35ms
step:1030/2330 train_time:40551ms step_avg:39.37ms
step:1031/2330 train_time:40573ms step_avg:39.35ms
step:1032/2330 train_time:40628ms step_avg:39.37ms
step:1033/2330 train_time:40650ms step_avg:39.35ms
step:1034/2330 train_time:40706ms step_avg:39.37ms
step:1035/2330 train_time:40729ms step_avg:39.35ms
step:1036/2330 train_time:40786ms step_avg:39.37ms
step:1037/2330 train_time:40808ms step_avg:39.35ms
step:1038/2330 train_time:40864ms step_avg:39.37ms
step:1039/2330 train_time:40886ms step_avg:39.35ms
step:1040/2330 train_time:40943ms step_avg:39.37ms
step:1041/2330 train_time:40965ms step_avg:39.35ms
step:1042/2330 train_time:41022ms step_avg:39.37ms
step:1043/2330 train_time:41044ms step_avg:39.35ms
step:1044/2330 train_time:41100ms step_avg:39.37ms
step:1045/2330 train_time:41122ms step_avg:39.35ms
step:1046/2330 train_time:41179ms step_avg:39.37ms
step:1047/2330 train_time:41201ms step_avg:39.35ms
step:1048/2330 train_time:41257ms step_avg:39.37ms
step:1049/2330 train_time:41279ms step_avg:39.35ms
step:1050/2330 train_time:41335ms step_avg:39.37ms
step:1051/2330 train_time:41358ms step_avg:39.35ms
step:1052/2330 train_time:41414ms step_avg:39.37ms
step:1053/2330 train_time:41437ms step_avg:39.35ms
step:1054/2330 train_time:41493ms step_avg:39.37ms
step:1055/2330 train_time:41516ms step_avg:39.35ms
step:1056/2330 train_time:41572ms step_avg:39.37ms
step:1057/2330 train_time:41595ms step_avg:39.35ms
step:1058/2330 train_time:41652ms step_avg:39.37ms
step:1059/2330 train_time:41675ms step_avg:39.35ms
step:1060/2330 train_time:41732ms step_avg:39.37ms
step:1061/2330 train_time:41756ms step_avg:39.35ms
step:1062/2330 train_time:41812ms step_avg:39.37ms
step:1063/2330 train_time:41836ms step_avg:39.36ms
step:1064/2330 train_time:41893ms step_avg:39.37ms
step:1065/2330 train_time:41916ms step_avg:39.36ms
step:1066/2330 train_time:41972ms step_avg:39.37ms
step:1067/2330 train_time:41995ms step_avg:39.36ms
step:1068/2330 train_time:42052ms step_avg:39.37ms
step:1069/2330 train_time:42075ms step_avg:39.36ms
step:1070/2330 train_time:42130ms step_avg:39.37ms
step:1071/2330 train_time:42153ms step_avg:39.36ms
step:1072/2330 train_time:42209ms step_avg:39.37ms
step:1073/2330 train_time:42231ms step_avg:39.36ms
step:1074/2330 train_time:42287ms step_avg:39.37ms
step:1075/2330 train_time:42310ms step_avg:39.36ms
step:1076/2330 train_time:42366ms step_avg:39.37ms
step:1077/2330 train_time:42387ms step_avg:39.36ms
step:1078/2330 train_time:42444ms step_avg:39.37ms
step:1079/2330 train_time:42465ms step_avg:39.36ms
step:1080/2330 train_time:42522ms step_avg:39.37ms
step:1081/2330 train_time:42544ms step_avg:39.36ms
step:1082/2330 train_time:42601ms step_avg:39.37ms
step:1083/2330 train_time:42623ms step_avg:39.36ms
step:1084/2330 train_time:42680ms step_avg:39.37ms
step:1085/2330 train_time:42702ms step_avg:39.36ms
step:1086/2330 train_time:42760ms step_avg:39.37ms
step:1087/2330 train_time:42782ms step_avg:39.36ms
step:1088/2330 train_time:42838ms step_avg:39.37ms
step:1089/2330 train_time:42861ms step_avg:39.36ms
step:1090/2330 train_time:42918ms step_avg:39.37ms
step:1091/2330 train_time:42940ms step_avg:39.36ms
step:1092/2330 train_time:42996ms step_avg:39.37ms
step:1093/2330 train_time:43019ms step_avg:39.36ms
step:1094/2330 train_time:43075ms step_avg:39.37ms
step:1095/2330 train_time:43098ms step_avg:39.36ms
step:1096/2330 train_time:43154ms step_avg:39.37ms
step:1097/2330 train_time:43177ms step_avg:39.36ms
step:1098/2330 train_time:43233ms step_avg:39.37ms
step:1099/2330 train_time:43256ms step_avg:39.36ms
step:1100/2330 train_time:43313ms step_avg:39.38ms
step:1101/2330 train_time:43336ms step_avg:39.36ms
step:1102/2330 train_time:43393ms step_avg:39.38ms
step:1103/2330 train_time:43416ms step_avg:39.36ms
step:1104/2330 train_time:43472ms step_avg:39.38ms
step:1105/2330 train_time:43495ms step_avg:39.36ms
step:1106/2330 train_time:43551ms step_avg:39.38ms
step:1107/2330 train_time:43574ms step_avg:39.36ms
step:1108/2330 train_time:43630ms step_avg:39.38ms
step:1109/2330 train_time:43653ms step_avg:39.36ms
step:1110/2330 train_time:43710ms step_avg:39.38ms
step:1111/2330 train_time:43732ms step_avg:39.36ms
step:1112/2330 train_time:43789ms step_avg:39.38ms
step:1113/2330 train_time:43812ms step_avg:39.36ms
step:1114/2330 train_time:43869ms step_avg:39.38ms
step:1115/2330 train_time:43891ms step_avg:39.36ms
step:1116/2330 train_time:43948ms step_avg:39.38ms
step:1117/2330 train_time:43970ms step_avg:39.36ms
step:1118/2330 train_time:44027ms step_avg:39.38ms
step:1119/2330 train_time:44049ms step_avg:39.36ms
step:1120/2330 train_time:44104ms step_avg:39.38ms
step:1121/2330 train_time:44126ms step_avg:39.36ms
step:1122/2330 train_time:44182ms step_avg:39.38ms
step:1123/2330 train_time:44204ms step_avg:39.36ms
step:1124/2330 train_time:44261ms step_avg:39.38ms
step:1125/2330 train_time:44283ms step_avg:39.36ms
step:1126/2330 train_time:44340ms step_avg:39.38ms
step:1127/2330 train_time:44362ms step_avg:39.36ms
step:1128/2330 train_time:44419ms step_avg:39.38ms
step:1129/2330 train_time:44441ms step_avg:39.36ms
step:1130/2330 train_time:44497ms step_avg:39.38ms
step:1131/2330 train_time:44519ms step_avg:39.36ms
step:1132/2330 train_time:44575ms step_avg:39.38ms
step:1133/2330 train_time:44598ms step_avg:39.36ms
step:1134/2330 train_time:44654ms step_avg:39.38ms
step:1135/2330 train_time:44677ms step_avg:39.36ms
step:1136/2330 train_time:44734ms step_avg:39.38ms
step:1137/2330 train_time:44757ms step_avg:39.36ms
step:1138/2330 train_time:44814ms step_avg:39.38ms
step:1139/2330 train_time:44837ms step_avg:39.37ms
step:1140/2330 train_time:44893ms step_avg:39.38ms
step:1141/2330 train_time:44917ms step_avg:39.37ms
step:1142/2330 train_time:44974ms step_avg:39.38ms
step:1143/2330 train_time:44997ms step_avg:39.37ms
step:1144/2330 train_time:45053ms step_avg:39.38ms
step:1145/2330 train_time:45077ms step_avg:39.37ms
step:1146/2330 train_time:45133ms step_avg:39.38ms
step:1147/2330 train_time:45157ms step_avg:39.37ms
step:1148/2330 train_time:45214ms step_avg:39.38ms
step:1149/2330 train_time:45237ms step_avg:39.37ms
step:1150/2330 train_time:45293ms step_avg:39.39ms
step:1151/2330 train_time:45316ms step_avg:39.37ms
step:1152/2330 train_time:45373ms step_avg:39.39ms
step:1153/2330 train_time:45396ms step_avg:39.37ms
step:1154/2330 train_time:45452ms step_avg:39.39ms
step:1155/2330 train_time:45474ms step_avg:39.37ms
step:1156/2330 train_time:45530ms step_avg:39.39ms
step:1157/2330 train_time:45553ms step_avg:39.37ms
step:1158/2330 train_time:45609ms step_avg:39.39ms
step:1159/2330 train_time:45631ms step_avg:39.37ms
step:1160/2330 train_time:45687ms step_avg:39.39ms
step:1161/2330 train_time:45709ms step_avg:39.37ms
step:1162/2330 train_time:45766ms step_avg:39.39ms
step:1163/2330 train_time:45788ms step_avg:39.37ms
step:1164/2330 train_time:45844ms step_avg:39.38ms
step:1165/2330 train_time:45866ms step_avg:39.37ms
step:1166/2330 train_time:45923ms step_avg:39.39ms
step:1167/2330 train_time:45945ms step_avg:39.37ms
step:1168/2330 train_time:46002ms step_avg:39.39ms
step:1169/2330 train_time:46024ms step_avg:39.37ms
step:1170/2330 train_time:46081ms step_avg:39.39ms
step:1171/2330 train_time:46103ms step_avg:39.37ms
step:1172/2330 train_time:46160ms step_avg:39.39ms
step:1173/2330 train_time:46182ms step_avg:39.37ms
step:1174/2330 train_time:46239ms step_avg:39.39ms
step:1175/2330 train_time:46261ms step_avg:39.37ms
step:1176/2330 train_time:46318ms step_avg:39.39ms
step:1177/2330 train_time:46340ms step_avg:39.37ms
step:1178/2330 train_time:46396ms step_avg:39.38ms
step:1179/2330 train_time:46418ms step_avg:39.37ms
step:1180/2330 train_time:46474ms step_avg:39.38ms
step:1181/2330 train_time:46497ms step_avg:39.37ms
step:1182/2330 train_time:46553ms step_avg:39.38ms
step:1183/2330 train_time:46576ms step_avg:39.37ms
step:1184/2330 train_time:46632ms step_avg:39.39ms
step:1185/2330 train_time:46655ms step_avg:39.37ms
step:1186/2330 train_time:46711ms step_avg:39.39ms
step:1187/2330 train_time:46734ms step_avg:39.37ms
step:1188/2330 train_time:46791ms step_avg:39.39ms
step:1189/2330 train_time:46815ms step_avg:39.37ms
step:1190/2330 train_time:46872ms step_avg:39.39ms
step:1191/2330 train_time:46895ms step_avg:39.37ms
step:1192/2330 train_time:46951ms step_avg:39.39ms
step:1193/2330 train_time:46974ms step_avg:39.37ms
step:1194/2330 train_time:47031ms step_avg:39.39ms
step:1195/2330 train_time:47054ms step_avg:39.38ms
step:1196/2330 train_time:47110ms step_avg:39.39ms
step:1197/2330 train_time:47133ms step_avg:39.38ms
step:1198/2330 train_time:47189ms step_avg:39.39ms
step:1199/2330 train_time:47212ms step_avg:39.38ms
step:1200/2330 train_time:47269ms step_avg:39.39ms
step:1201/2330 train_time:47291ms step_avg:39.38ms
step:1202/2330 train_time:47348ms step_avg:39.39ms
step:1203/2330 train_time:47369ms step_avg:39.38ms
step:1204/2330 train_time:47425ms step_avg:39.39ms
step:1205/2330 train_time:47447ms step_avg:39.38ms
step:1206/2330 train_time:47503ms step_avg:39.39ms
step:1207/2330 train_time:47525ms step_avg:39.37ms
step:1208/2330 train_time:47582ms step_avg:39.39ms
step:1209/2330 train_time:47604ms step_avg:39.37ms
step:1210/2330 train_time:47660ms step_avg:39.39ms
step:1211/2330 train_time:47683ms step_avg:39.37ms
step:1212/2330 train_time:47739ms step_avg:39.39ms
step:1213/2330 train_time:47762ms step_avg:39.37ms
step:1214/2330 train_time:47819ms step_avg:39.39ms
step:1215/2330 train_time:47841ms step_avg:39.38ms
step:1216/2330 train_time:47897ms step_avg:39.39ms
step:1217/2330 train_time:47920ms step_avg:39.38ms
step:1218/2330 train_time:47976ms step_avg:39.39ms
step:1219/2330 train_time:47999ms step_avg:39.38ms
step:1220/2330 train_time:48055ms step_avg:39.39ms
step:1221/2330 train_time:48078ms step_avg:39.38ms
step:1222/2330 train_time:48134ms step_avg:39.39ms
step:1223/2330 train_time:48157ms step_avg:39.38ms
step:1224/2330 train_time:48213ms step_avg:39.39ms
step:1225/2330 train_time:48237ms step_avg:39.38ms
step:1226/2330 train_time:48293ms step_avg:39.39ms
step:1227/2330 train_time:48316ms step_avg:39.38ms
step:1228/2330 train_time:48373ms step_avg:39.39ms
step:1229/2330 train_time:48396ms step_avg:39.38ms
step:1230/2330 train_time:48452ms step_avg:39.39ms
step:1231/2330 train_time:48475ms step_avg:39.38ms
step:1232/2330 train_time:48532ms step_avg:39.39ms
step:1233/2330 train_time:48555ms step_avg:39.38ms
step:1234/2330 train_time:48611ms step_avg:39.39ms
step:1235/2330 train_time:48634ms step_avg:39.38ms
step:1236/2330 train_time:48690ms step_avg:39.39ms
step:1237/2330 train_time:48713ms step_avg:39.38ms
step:1238/2330 train_time:48769ms step_avg:39.39ms
step:1239/2330 train_time:48792ms step_avg:39.38ms
step:1240/2330 train_time:48849ms step_avg:39.39ms
step:1241/2330 train_time:48872ms step_avg:39.38ms
step:1242/2330 train_time:48928ms step_avg:39.39ms
step:1243/2330 train_time:48951ms step_avg:39.38ms
step:1244/2330 train_time:49007ms step_avg:39.39ms
step:1245/2330 train_time:49029ms step_avg:39.38ms
step:1246/2330 train_time:49085ms step_avg:39.39ms
step:1247/2330 train_time:49108ms step_avg:39.38ms
step:1248/2330 train_time:49164ms step_avg:39.39ms
step:1249/2330 train_time:49186ms step_avg:39.38ms
step:1250/2330 train_time:49242ms step_avg:39.39ms
step:1250/2330 val_loss:5.2288 train_time:49338ms step_avg:39.47ms
step:1251/2330 train_time:49350ms step_avg:39.45ms
step:1252/2330 train_time:49363ms step_avg:39.43ms
step:1253/2330 train_time:49372ms step_avg:39.40ms
step:1254/2330 train_time:49400ms step_avg:39.39ms
step:1255/2330 train_time:49421ms step_avg:39.38ms
step:1256/2330 train_time:49477ms step_avg:39.39ms
step:1257/2330 train_time:49499ms step_avg:39.38ms
step:1258/2330 train_time:49554ms step_avg:39.39ms
step:1259/2330 train_time:49576ms step_avg:39.38ms
step:1260/2330 train_time:49632ms step_avg:39.39ms
step:1261/2330 train_time:49660ms step_avg:39.38ms
step:1262/2330 train_time:49720ms step_avg:39.40ms
step:1263/2330 train_time:49744ms step_avg:39.39ms
step:1264/2330 train_time:49801ms step_avg:39.40ms
step:1265/2330 train_time:49824ms step_avg:39.39ms
step:1266/2330 train_time:49880ms step_avg:39.40ms
step:1267/2330 train_time:49903ms step_avg:39.39ms
step:1268/2330 train_time:49959ms step_avg:39.40ms
step:1269/2330 train_time:49981ms step_avg:39.39ms
step:1270/2330 train_time:50037ms step_avg:39.40ms
step:1271/2330 train_time:50059ms step_avg:39.39ms
step:1272/2330 train_time:50114ms step_avg:39.40ms
step:1273/2330 train_time:50136ms step_avg:39.38ms
step:1274/2330 train_time:50192ms step_avg:39.40ms
step:1275/2330 train_time:50214ms step_avg:39.38ms
step:1276/2330 train_time:50272ms step_avg:39.40ms
step:1277/2330 train_time:50294ms step_avg:39.38ms
step:1278/2330 train_time:50350ms step_avg:39.40ms
step:1279/2330 train_time:50373ms step_avg:39.38ms
step:1280/2330 train_time:50429ms step_avg:39.40ms
step:1281/2330 train_time:50452ms step_avg:39.38ms
step:1282/2330 train_time:50508ms step_avg:39.40ms
step:1283/2330 train_time:50530ms step_avg:39.38ms
step:1284/2330 train_time:50587ms step_avg:39.40ms
step:1285/2330 train_time:50611ms step_avg:39.39ms
step:1286/2330 train_time:50669ms step_avg:39.40ms
step:1287/2330 train_time:50692ms step_avg:39.39ms
step:1288/2330 train_time:50749ms step_avg:39.40ms
step:1289/2330 train_time:50772ms step_avg:39.39ms
step:1290/2330 train_time:50829ms step_avg:39.40ms
step:1291/2330 train_time:50851ms step_avg:39.39ms
step:1292/2330 train_time:50907ms step_avg:39.40ms
step:1293/2330 train_time:50930ms step_avg:39.39ms
step:1294/2330 train_time:50986ms step_avg:39.40ms
step:1295/2330 train_time:51009ms step_avg:39.39ms
step:1296/2330 train_time:51065ms step_avg:39.40ms
step:1297/2330 train_time:51086ms step_avg:39.39ms
step:1298/2330 train_time:51142ms step_avg:39.40ms
step:1299/2330 train_time:51164ms step_avg:39.39ms
step:1300/2330 train_time:51220ms step_avg:39.40ms
step:1301/2330 train_time:51243ms step_avg:39.39ms
step:1302/2330 train_time:51299ms step_avg:39.40ms
step:1303/2330 train_time:51321ms step_avg:39.39ms
step:1304/2330 train_time:51378ms step_avg:39.40ms
step:1305/2330 train_time:51400ms step_avg:39.39ms
step:1306/2330 train_time:51456ms step_avg:39.40ms
step:1307/2330 train_time:51477ms step_avg:39.39ms
step:1308/2330 train_time:51534ms step_avg:39.40ms
step:1309/2330 train_time:51558ms step_avg:39.39ms
step:1310/2330 train_time:51614ms step_avg:39.40ms
step:1311/2330 train_time:51638ms step_avg:39.39ms
step:1312/2330 train_time:51694ms step_avg:39.40ms
step:1313/2330 train_time:51718ms step_avg:39.39ms
step:1314/2330 train_time:51774ms step_avg:39.40ms
step:1315/2330 train_time:51797ms step_avg:39.39ms
step:1316/2330 train_time:51853ms step_avg:39.40ms
step:1317/2330 train_time:51876ms step_avg:39.39ms
step:1318/2330 train_time:51932ms step_avg:39.40ms
step:1319/2330 train_time:51956ms step_avg:39.39ms
step:1320/2330 train_time:52012ms step_avg:39.40ms
step:1321/2330 train_time:52035ms step_avg:39.39ms
step:1322/2330 train_time:52091ms step_avg:39.40ms
step:1323/2330 train_time:52114ms step_avg:39.39ms
step:1324/2330 train_time:52171ms step_avg:39.40ms
step:1325/2330 train_time:52193ms step_avg:39.39ms
step:1326/2330 train_time:52249ms step_avg:39.40ms
step:1327/2330 train_time:52272ms step_avg:39.39ms
step:1328/2330 train_time:52328ms step_avg:39.40ms
step:1329/2330 train_time:52351ms step_avg:39.39ms
step:1330/2330 train_time:52407ms step_avg:39.40ms
step:1331/2330 train_time:52429ms step_avg:39.39ms
step:1332/2330 train_time:52485ms step_avg:39.40ms
step:1333/2330 train_time:52507ms step_avg:39.39ms
step:1334/2330 train_time:52563ms step_avg:39.40ms
step:1335/2330 train_time:52584ms step_avg:39.39ms
step:1336/2330 train_time:52641ms step_avg:39.40ms
step:1337/2330 train_time:52663ms step_avg:39.39ms
step:1338/2330 train_time:52720ms step_avg:39.40ms
step:1339/2330 train_time:52742ms step_avg:39.39ms
step:1340/2330 train_time:52799ms step_avg:39.40ms
step:1341/2330 train_time:52821ms step_avg:39.39ms
step:1342/2330 train_time:52878ms step_avg:39.40ms
step:1343/2330 train_time:52901ms step_avg:39.39ms
step:1344/2330 train_time:52959ms step_avg:39.40ms
step:1345/2330 train_time:52980ms step_avg:39.39ms
step:1346/2330 train_time:53037ms step_avg:39.40ms
step:1347/2330 train_time:53060ms step_avg:39.39ms
step:1348/2330 train_time:53116ms step_avg:39.40ms
step:1349/2330 train_time:53138ms step_avg:39.39ms
step:1350/2330 train_time:53194ms step_avg:39.40ms
step:1351/2330 train_time:53217ms step_avg:39.39ms
step:1352/2330 train_time:53273ms step_avg:39.40ms
step:1353/2330 train_time:53296ms step_avg:39.39ms
step:1354/2330 train_time:53352ms step_avg:39.40ms
step:1355/2330 train_time:53375ms step_avg:39.39ms
step:1356/2330 train_time:53431ms step_avg:39.40ms
step:1357/2330 train_time:53454ms step_avg:39.39ms
step:1358/2330 train_time:53510ms step_avg:39.40ms
step:1359/2330 train_time:53532ms step_avg:39.39ms
step:1360/2330 train_time:53589ms step_avg:39.40ms
step:1361/2330 train_time:53612ms step_avg:39.39ms
step:1362/2330 train_time:53669ms step_avg:39.40ms
step:1363/2330 train_time:53691ms step_avg:39.39ms
step:1364/2330 train_time:53747ms step_avg:39.40ms
step:1365/2330 train_time:53769ms step_avg:39.39ms
step:1366/2330 train_time:53826ms step_avg:39.40ms
step:1367/2330 train_time:53848ms step_avg:39.39ms
step:1368/2330 train_time:53904ms step_avg:39.40ms
step:1369/2330 train_time:53926ms step_avg:39.39ms
step:1370/2330 train_time:53982ms step_avg:39.40ms
step:1371/2330 train_time:54004ms step_avg:39.39ms
step:1372/2330 train_time:54061ms step_avg:39.40ms
step:1373/2330 train_time:54083ms step_avg:39.39ms
step:1374/2330 train_time:54140ms step_avg:39.40ms
step:1375/2330 train_time:54161ms step_avg:39.39ms
step:1376/2330 train_time:54218ms step_avg:39.40ms
step:1377/2330 train_time:54240ms step_avg:39.39ms
step:1378/2330 train_time:54297ms step_avg:39.40ms
step:1379/2330 train_time:54320ms step_avg:39.39ms
step:1380/2330 train_time:54376ms step_avg:39.40ms
step:1381/2330 train_time:54398ms step_avg:39.39ms
step:1382/2330 train_time:54454ms step_avg:39.40ms
step:1383/2330 train_time:54476ms step_avg:39.39ms
step:1384/2330 train_time:54533ms step_avg:39.40ms
step:1385/2330 train_time:54556ms step_avg:39.39ms
step:1386/2330 train_time:54613ms step_avg:39.40ms
step:1387/2330 train_time:54636ms step_avg:39.39ms
step:1388/2330 train_time:54692ms step_avg:39.40ms
step:1389/2330 train_time:54716ms step_avg:39.39ms
step:1390/2330 train_time:54773ms step_avg:39.40ms
step:1391/2330 train_time:54796ms step_avg:39.39ms
step:1392/2330 train_time:54853ms step_avg:39.41ms
step:1393/2330 train_time:54876ms step_avg:39.39ms
step:1394/2330 train_time:54933ms step_avg:39.41ms
step:1395/2330 train_time:54957ms step_avg:39.40ms
step:1396/2330 train_time:55013ms step_avg:39.41ms
step:1397/2330 train_time:55036ms step_avg:39.40ms
step:1398/2330 train_time:55093ms step_avg:39.41ms
step:1399/2330 train_time:55116ms step_avg:39.40ms
step:1400/2330 train_time:55172ms step_avg:39.41ms
step:1401/2330 train_time:55195ms step_avg:39.40ms
step:1402/2330 train_time:55251ms step_avg:39.41ms
step:1403/2330 train_time:55274ms step_avg:39.40ms
step:1404/2330 train_time:55331ms step_avg:39.41ms
step:1405/2330 train_time:55354ms step_avg:39.40ms
step:1406/2330 train_time:55410ms step_avg:39.41ms
step:1407/2330 train_time:55433ms step_avg:39.40ms
step:1408/2330 train_time:55489ms step_avg:39.41ms
step:1409/2330 train_time:55511ms step_avg:39.40ms
step:1410/2330 train_time:55568ms step_avg:39.41ms
step:1411/2330 train_time:55590ms step_avg:39.40ms
step:1412/2330 train_time:55647ms step_avg:39.41ms
step:1413/2330 train_time:55670ms step_avg:39.40ms
step:1414/2330 train_time:55726ms step_avg:39.41ms
step:1415/2330 train_time:55749ms step_avg:39.40ms
step:1416/2330 train_time:55805ms step_avg:39.41ms
step:1417/2330 train_time:55828ms step_avg:39.40ms
step:1418/2330 train_time:55884ms step_avg:39.41ms
step:1419/2330 train_time:55905ms step_avg:39.40ms
step:1420/2330 train_time:55962ms step_avg:39.41ms
step:1421/2330 train_time:55984ms step_avg:39.40ms
step:1422/2330 train_time:56042ms step_avg:39.41ms
step:1423/2330 train_time:56064ms step_avg:39.40ms
step:1424/2330 train_time:56122ms step_avg:39.41ms
step:1425/2330 train_time:56144ms step_avg:39.40ms
step:1426/2330 train_time:56200ms step_avg:39.41ms
step:1427/2330 train_time:56223ms step_avg:39.40ms
step:1428/2330 train_time:56279ms step_avg:39.41ms
step:1429/2330 train_time:56302ms step_avg:39.40ms
step:1430/2330 train_time:56358ms step_avg:39.41ms
step:1431/2330 train_time:56381ms step_avg:39.40ms
step:1432/2330 train_time:56438ms step_avg:39.41ms
step:1433/2330 train_time:56461ms step_avg:39.40ms
step:1434/2330 train_time:56517ms step_avg:39.41ms
step:1435/2330 train_time:56539ms step_avg:39.40ms
step:1436/2330 train_time:56595ms step_avg:39.41ms
step:1437/2330 train_time:56618ms step_avg:39.40ms
step:1438/2330 train_time:56674ms step_avg:39.41ms
step:1439/2330 train_time:56697ms step_avg:39.40ms
step:1440/2330 train_time:56753ms step_avg:39.41ms
step:1441/2330 train_time:56777ms step_avg:39.40ms
step:1442/2330 train_time:56833ms step_avg:39.41ms
step:1443/2330 train_time:56857ms step_avg:39.40ms
step:1444/2330 train_time:56913ms step_avg:39.41ms
step:1445/2330 train_time:56936ms step_avg:39.40ms
step:1446/2330 train_time:56992ms step_avg:39.41ms
step:1447/2330 train_time:57016ms step_avg:39.40ms
step:1448/2330 train_time:57072ms step_avg:39.41ms
step:1449/2330 train_time:57095ms step_avg:39.40ms
step:1450/2330 train_time:57152ms step_avg:39.42ms
step:1451/2330 train_time:57176ms step_avg:39.40ms
step:1452/2330 train_time:57232ms step_avg:39.42ms
step:1453/2330 train_time:57255ms step_avg:39.40ms
step:1454/2330 train_time:57312ms step_avg:39.42ms
step:1455/2330 train_time:57335ms step_avg:39.41ms
step:1456/2330 train_time:57391ms step_avg:39.42ms
step:1457/2330 train_time:57414ms step_avg:39.41ms
step:1458/2330 train_time:57470ms step_avg:39.42ms
step:1459/2330 train_time:57493ms step_avg:39.41ms
step:1460/2330 train_time:57549ms step_avg:39.42ms
step:1461/2330 train_time:57571ms step_avg:39.41ms
step:1462/2330 train_time:57627ms step_avg:39.42ms
step:1463/2330 train_time:57650ms step_avg:39.41ms
step:1464/2330 train_time:57706ms step_avg:39.42ms
step:1465/2330 train_time:57728ms step_avg:39.41ms
step:1466/2330 train_time:57784ms step_avg:39.42ms
step:1467/2330 train_time:57806ms step_avg:39.40ms
step:1468/2330 train_time:57862ms step_avg:39.42ms
step:1469/2330 train_time:57884ms step_avg:39.40ms
step:1470/2330 train_time:57940ms step_avg:39.42ms
step:1471/2330 train_time:57963ms step_avg:39.40ms
step:1472/2330 train_time:58020ms step_avg:39.42ms
step:1473/2330 train_time:58042ms step_avg:39.40ms
step:1474/2330 train_time:58099ms step_avg:39.42ms
step:1475/2330 train_time:58121ms step_avg:39.40ms
step:1476/2330 train_time:58178ms step_avg:39.42ms
step:1477/2330 train_time:58200ms step_avg:39.40ms
step:1478/2330 train_time:58256ms step_avg:39.42ms
step:1479/2330 train_time:58278ms step_avg:39.40ms
step:1480/2330 train_time:58335ms step_avg:39.42ms
step:1481/2330 train_time:58358ms step_avg:39.40ms
step:1482/2330 train_time:58414ms step_avg:39.42ms
step:1483/2330 train_time:58437ms step_avg:39.40ms
step:1484/2330 train_time:58493ms step_avg:39.42ms
step:1485/2330 train_time:58516ms step_avg:39.40ms
step:1486/2330 train_time:58572ms step_avg:39.42ms
step:1487/2330 train_time:58595ms step_avg:39.40ms
step:1488/2330 train_time:58652ms step_avg:39.42ms
step:1489/2330 train_time:58675ms step_avg:39.41ms
step:1490/2330 train_time:58732ms step_avg:39.42ms
step:1491/2330 train_time:58755ms step_avg:39.41ms
step:1492/2330 train_time:58811ms step_avg:39.42ms
step:1493/2330 train_time:58834ms step_avg:39.41ms
step:1494/2330 train_time:58890ms step_avg:39.42ms
step:1495/2330 train_time:58914ms step_avg:39.41ms
step:1496/2330 train_time:58971ms step_avg:39.42ms
step:1497/2330 train_time:58995ms step_avg:39.41ms
step:1498/2330 train_time:59051ms step_avg:39.42ms
step:1499/2330 train_time:59074ms step_avg:39.41ms
step:1500/2330 train_time:59131ms step_avg:39.42ms
step:1500/2330 val_loss:5.1929 train_time:59229ms step_avg:39.49ms
step:1501/2330 train_time:59242ms step_avg:39.47ms
step:1502/2330 train_time:59255ms step_avg:39.45ms
step:1503/2330 train_time:59266ms step_avg:39.43ms
step:1504/2330 train_time:59292ms step_avg:39.42ms
step:1505/2330 train_time:59314ms step_avg:39.41ms
step:1506/2330 train_time:59369ms step_avg:39.42ms
step:1507/2330 train_time:59391ms step_avg:39.41ms
step:1508/2330 train_time:59447ms step_avg:39.42ms
step:1509/2330 train_time:59469ms step_avg:39.41ms
step:1510/2330 train_time:59525ms step_avg:39.42ms
step:1511/2330 train_time:59552ms step_avg:39.41ms
step:1512/2330 train_time:59611ms step_avg:39.43ms
step:1513/2330 train_time:59635ms step_avg:39.42ms
step:1514/2330 train_time:59692ms step_avg:39.43ms
step:1515/2330 train_time:59716ms step_avg:39.42ms
step:1516/2330 train_time:59772ms step_avg:39.43ms
step:1517/2330 train_time:59795ms step_avg:39.42ms
step:1518/2330 train_time:59851ms step_avg:39.43ms
step:1519/2330 train_time:59873ms step_avg:39.42ms
step:1520/2330 train_time:59930ms step_avg:39.43ms
step:1521/2330 train_time:59953ms step_avg:39.42ms
step:1522/2330 train_time:60008ms step_avg:39.43ms
step:1523/2330 train_time:60031ms step_avg:39.42ms
step:1524/2330 train_time:60088ms step_avg:39.43ms
step:1525/2330 train_time:60110ms step_avg:39.42ms
step:1526/2330 train_time:60166ms step_avg:39.43ms
step:1527/2330 train_time:60190ms step_avg:39.42ms
step:1528/2330 train_time:60246ms step_avg:39.43ms
step:1529/2330 train_time:60270ms step_avg:39.42ms
step:1530/2330 train_time:60326ms step_avg:39.43ms
step:1531/2330 train_time:60347ms step_avg:39.42ms
step:1532/2330 train_time:60402ms step_avg:39.43ms
step:1533/2330 train_time:60424ms step_avg:39.42ms
step:1534/2330 train_time:60481ms step_avg:39.43ms
step:1535/2330 train_time:60504ms step_avg:39.42ms
step:1536/2330 train_time:60561ms step_avg:39.43ms
step:1537/2330 train_time:60583ms step_avg:39.42ms
step:1538/2330 train_time:60639ms step_avg:39.43ms
step:1539/2330 train_time:60662ms step_avg:39.42ms
step:1540/2330 train_time:60719ms step_avg:39.43ms
step:1541/2330 train_time:60741ms step_avg:39.42ms
step:1542/2330 train_time:60797ms step_avg:39.43ms
step:1543/2330 train_time:60819ms step_avg:39.42ms
step:1544/2330 train_time:60876ms step_avg:39.43ms
step:1545/2330 train_time:60898ms step_avg:39.42ms
step:1546/2330 train_time:60954ms step_avg:39.43ms
step:1547/2330 train_time:60977ms step_avg:39.42ms
step:1548/2330 train_time:61033ms step_avg:39.43ms
step:1549/2330 train_time:61055ms step_avg:39.42ms
step:1550/2330 train_time:61111ms step_avg:39.43ms
step:1551/2330 train_time:61134ms step_avg:39.42ms
step:1552/2330 train_time:61190ms step_avg:39.43ms
step:1553/2330 train_time:61213ms step_avg:39.42ms
step:1554/2330 train_time:61270ms step_avg:39.43ms
step:1555/2330 train_time:61293ms step_avg:39.42ms
step:1556/2330 train_time:61349ms step_avg:39.43ms
step:1557/2330 train_time:61372ms step_avg:39.42ms
step:1558/2330 train_time:61429ms step_avg:39.43ms
step:1559/2330 train_time:61452ms step_avg:39.42ms
step:1560/2330 train_time:61509ms step_avg:39.43ms
step:1561/2330 train_time:61532ms step_avg:39.42ms
step:1562/2330 train_time:61589ms step_avg:39.43ms
step:1563/2330 train_time:61613ms step_avg:39.42ms
step:1564/2330 train_time:61669ms step_avg:39.43ms
step:1565/2330 train_time:61693ms step_avg:39.42ms
step:1566/2330 train_time:61749ms step_avg:39.43ms
step:1567/2330 train_time:61773ms step_avg:39.42ms
step:1568/2330 train_time:61829ms step_avg:39.43ms
step:1569/2330 train_time:61852ms step_avg:39.42ms
step:1570/2330 train_time:61908ms step_avg:39.43ms
step:1571/2330 train_time:61931ms step_avg:39.42ms
step:1572/2330 train_time:61987ms step_avg:39.43ms
step:1573/2330 train_time:62009ms step_avg:39.42ms
step:1574/2330 train_time:62066ms step_avg:39.43ms
step:1575/2330 train_time:62088ms step_avg:39.42ms
step:1576/2330 train_time:62145ms step_avg:39.43ms
step:1577/2330 train_time:62167ms step_avg:39.42ms
step:1578/2330 train_time:62223ms step_avg:39.43ms
step:1579/2330 train_time:62246ms step_avg:39.42ms
step:1580/2330 train_time:62302ms step_avg:39.43ms
step:1581/2330 train_time:62324ms step_avg:39.42ms
step:1582/2330 train_time:62380ms step_avg:39.43ms
step:1583/2330 train_time:62402ms step_avg:39.42ms
step:1584/2330 train_time:62459ms step_avg:39.43ms
step:1585/2330 train_time:62480ms step_avg:39.42ms
step:1586/2330 train_time:62537ms step_avg:39.43ms
step:1587/2330 train_time:62560ms step_avg:39.42ms
step:1588/2330 train_time:62616ms step_avg:39.43ms
step:1589/2330 train_time:62638ms step_avg:39.42ms
step:1590/2330 train_time:62696ms step_avg:39.43ms
step:1591/2330 train_time:62718ms step_avg:39.42ms
step:1592/2330 train_time:62775ms step_avg:39.43ms
step:1593/2330 train_time:62797ms step_avg:39.42ms
step:1594/2330 train_time:62854ms step_avg:39.43ms
step:1595/2330 train_time:62876ms step_avg:39.42ms
step:1596/2330 train_time:62932ms step_avg:39.43ms
step:1597/2330 train_time:62954ms step_avg:39.42ms
step:1598/2330 train_time:63010ms step_avg:39.43ms
step:1599/2330 train_time:63033ms step_avg:39.42ms
step:1600/2330 train_time:63089ms step_avg:39.43ms
step:1601/2330 train_time:63112ms step_avg:39.42ms
step:1602/2330 train_time:63168ms step_avg:39.43ms
step:1603/2330 train_time:63191ms step_avg:39.42ms
step:1604/2330 train_time:63248ms step_avg:39.43ms
step:1605/2330 train_time:63270ms step_avg:39.42ms
step:1606/2330 train_time:63328ms step_avg:39.43ms
step:1607/2330 train_time:63351ms step_avg:39.42ms
step:1608/2330 train_time:63407ms step_avg:39.43ms
step:1609/2330 train_time:63431ms step_avg:39.42ms
step:1610/2330 train_time:63487ms step_avg:39.43ms
step:1611/2330 train_time:63510ms step_avg:39.42ms
step:1612/2330 train_time:63567ms step_avg:39.43ms
step:1613/2330 train_time:63590ms step_avg:39.42ms
step:1614/2330 train_time:63647ms step_avg:39.43ms
step:1615/2330 train_time:63670ms step_avg:39.42ms
step:1616/2330 train_time:63727ms step_avg:39.43ms
step:1617/2330 train_time:63750ms step_avg:39.42ms
step:1618/2330 train_time:63806ms step_avg:39.44ms
step:1619/2330 train_time:63829ms step_avg:39.42ms
step:1620/2330 train_time:63886ms step_avg:39.44ms
step:1621/2330 train_time:63908ms step_avg:39.42ms
step:1622/2330 train_time:63964ms step_avg:39.44ms
step:1623/2330 train_time:63986ms step_avg:39.42ms
step:1624/2330 train_time:64042ms step_avg:39.43ms
step:1625/2330 train_time:64064ms step_avg:39.42ms
step:1626/2330 train_time:64120ms step_avg:39.43ms
step:1627/2330 train_time:64142ms step_avg:39.42ms
step:1628/2330 train_time:64199ms step_avg:39.43ms
step:1629/2330 train_time:64221ms step_avg:39.42ms
step:1630/2330 train_time:64278ms step_avg:39.43ms
step:1631/2330 train_time:64299ms step_avg:39.42ms
step:1632/2330 train_time:64356ms step_avg:39.43ms
step:1633/2330 train_time:64378ms step_avg:39.42ms
step:1634/2330 train_time:64434ms step_avg:39.43ms
step:1635/2330 train_time:64457ms step_avg:39.42ms
step:1636/2330 train_time:64513ms step_avg:39.43ms
step:1637/2330 train_time:64536ms step_avg:39.42ms
step:1638/2330 train_time:64592ms step_avg:39.43ms
step:1639/2330 train_time:64615ms step_avg:39.42ms
step:1640/2330 train_time:64671ms step_avg:39.43ms
step:1641/2330 train_time:64694ms step_avg:39.42ms
step:1642/2330 train_time:64750ms step_avg:39.43ms
step:1643/2330 train_time:64774ms step_avg:39.42ms
step:1644/2330 train_time:64830ms step_avg:39.43ms
step:1645/2330 train_time:64853ms step_avg:39.42ms
step:1646/2330 train_time:64909ms step_avg:39.43ms
step:1647/2330 train_time:64932ms step_avg:39.42ms
step:1648/2330 train_time:64989ms step_avg:39.43ms
step:1649/2330 train_time:65012ms step_avg:39.42ms
step:1650/2330 train_time:65068ms step_avg:39.44ms
step:1651/2330 train_time:65091ms step_avg:39.43ms
step:1652/2330 train_time:65147ms step_avg:39.44ms
step:1653/2330 train_time:65170ms step_avg:39.43ms
step:1654/2330 train_time:65227ms step_avg:39.44ms
step:1655/2330 train_time:65250ms step_avg:39.43ms
step:1656/2330 train_time:65306ms step_avg:39.44ms
step:1657/2330 train_time:65329ms step_avg:39.43ms
step:1658/2330 train_time:65385ms step_avg:39.44ms
step:1659/2330 train_time:65408ms step_avg:39.43ms
step:1660/2330 train_time:65465ms step_avg:39.44ms
step:1661/2330 train_time:65487ms step_avg:39.43ms
step:1662/2330 train_time:65544ms step_avg:39.44ms
step:1663/2330 train_time:65567ms step_avg:39.43ms
step:1664/2330 train_time:65624ms step_avg:39.44ms
step:1665/2330 train_time:65646ms step_avg:39.43ms
step:1666/2330 train_time:65702ms step_avg:39.44ms
step:1667/2330 train_time:65725ms step_avg:39.43ms
step:1668/2330 train_time:65781ms step_avg:39.44ms
step:1669/2330 train_time:65803ms step_avg:39.43ms
step:1670/2330 train_time:65859ms step_avg:39.44ms
step:1671/2330 train_time:65881ms step_avg:39.43ms
step:1672/2330 train_time:65938ms step_avg:39.44ms
step:1673/2330 train_time:65960ms step_avg:39.43ms
step:1674/2330 train_time:66016ms step_avg:39.44ms
step:1675/2330 train_time:66039ms step_avg:39.43ms
step:1676/2330 train_time:66095ms step_avg:39.44ms
step:1677/2330 train_time:66117ms step_avg:39.43ms
step:1678/2330 train_time:66174ms step_avg:39.44ms
step:1679/2330 train_time:66196ms step_avg:39.43ms
step:1680/2330 train_time:66252ms step_avg:39.44ms
step:1681/2330 train_time:66274ms step_avg:39.43ms
step:1682/2330 train_time:66330ms step_avg:39.44ms
step:1683/2330 train_time:66353ms step_avg:39.43ms
step:1684/2330 train_time:66409ms step_avg:39.44ms
step:1685/2330 train_time:66432ms step_avg:39.43ms
step:1686/2330 train_time:66488ms step_avg:39.44ms
step:1687/2330 train_time:66512ms step_avg:39.43ms
step:1688/2330 train_time:66569ms step_avg:39.44ms
step:1689/2330 train_time:66592ms step_avg:39.43ms
step:1690/2330 train_time:66649ms step_avg:39.44ms
step:1691/2330 train_time:66673ms step_avg:39.43ms
step:1692/2330 train_time:66729ms step_avg:39.44ms
step:1693/2330 train_time:66753ms step_avg:39.43ms
step:1694/2330 train_time:66809ms step_avg:39.44ms
step:1695/2330 train_time:66832ms step_avg:39.43ms
step:1696/2330 train_time:66889ms step_avg:39.44ms
step:1697/2330 train_time:66912ms step_avg:39.43ms
step:1698/2330 train_time:66969ms step_avg:39.44ms
step:1699/2330 train_time:66992ms step_avg:39.43ms
step:1700/2330 train_time:67048ms step_avg:39.44ms
step:1701/2330 train_time:67072ms step_avg:39.43ms
step:1702/2330 train_time:67129ms step_avg:39.44ms
step:1703/2330 train_time:67153ms step_avg:39.43ms
step:1704/2330 train_time:67209ms step_avg:39.44ms
step:1705/2330 train_time:67232ms step_avg:39.43ms
step:1706/2330 train_time:67288ms step_avg:39.44ms
step:1707/2330 train_time:67311ms step_avg:39.43ms
step:1708/2330 train_time:67368ms step_avg:39.44ms
step:1709/2330 train_time:67391ms step_avg:39.43ms
step:1710/2330 train_time:67447ms step_avg:39.44ms
step:1711/2330 train_time:67470ms step_avg:39.43ms
step:1712/2330 train_time:67526ms step_avg:39.44ms
step:1713/2330 train_time:67549ms step_avg:39.43ms
step:1714/2330 train_time:67606ms step_avg:39.44ms
step:1715/2330 train_time:67629ms step_avg:39.43ms
step:1716/2330 train_time:67686ms step_avg:39.44ms
step:1717/2330 train_time:67709ms step_avg:39.43ms
step:1718/2330 train_time:67765ms step_avg:39.44ms
step:1719/2330 train_time:67788ms step_avg:39.43ms
step:1720/2330 train_time:67844ms step_avg:39.44ms
step:1721/2330 train_time:67867ms step_avg:39.43ms
step:1722/2330 train_time:67924ms step_avg:39.44ms
step:1723/2330 train_time:67946ms step_avg:39.43ms
step:1724/2330 train_time:68002ms step_avg:39.44ms
step:1725/2330 train_time:68024ms step_avg:39.43ms
step:1726/2330 train_time:68080ms step_avg:39.44ms
step:1727/2330 train_time:68102ms step_avg:39.43ms
step:1728/2330 train_time:68159ms step_avg:39.44ms
step:1729/2330 train_time:68181ms step_avg:39.43ms
step:1730/2330 train_time:68239ms step_avg:39.44ms
step:1731/2330 train_time:68260ms step_avg:39.43ms
step:1732/2330 train_time:68317ms step_avg:39.44ms
step:1733/2330 train_time:68339ms step_avg:39.43ms
step:1734/2330 train_time:68395ms step_avg:39.44ms
step:1735/2330 train_time:68418ms step_avg:39.43ms
step:1736/2330 train_time:68476ms step_avg:39.44ms
step:1737/2330 train_time:68499ms step_avg:39.44ms
step:1738/2330 train_time:68556ms step_avg:39.45ms
step:1739/2330 train_time:68579ms step_avg:39.44ms
step:1740/2330 train_time:68635ms step_avg:39.45ms
step:1741/2330 train_time:68658ms step_avg:39.44ms
step:1742/2330 train_time:68714ms step_avg:39.45ms
step:1743/2330 train_time:68736ms step_avg:39.44ms
step:1744/2330 train_time:68792ms step_avg:39.45ms
step:1745/2330 train_time:68815ms step_avg:39.44ms
step:1746/2330 train_time:68871ms step_avg:39.44ms
step:1747/2330 train_time:68894ms step_avg:39.44ms
step:1748/2330 train_time:68950ms step_avg:39.44ms
step:1749/2330 train_time:68973ms step_avg:39.44ms
step:1750/2330 train_time:69029ms step_avg:39.45ms
step:1750/2330 val_loss:5.1603 train_time:69126ms step_avg:39.50ms
step:1751/2330 train_time:69139ms step_avg:39.49ms
step:1752/2330 train_time:69151ms step_avg:39.47ms
step:1753/2330 train_time:69160ms step_avg:39.45ms
step:1754/2330 train_time:69189ms step_avg:39.45ms
step:1755/2330 train_time:69210ms step_avg:39.44ms
step:1756/2330 train_time:69265ms step_avg:39.44ms
step:1757/2330 train_time:69287ms step_avg:39.44ms
step:1758/2330 train_time:69343ms step_avg:39.44ms
step:1759/2330 train_time:69365ms step_avg:39.43ms
step:1760/2330 train_time:69420ms step_avg:39.44ms
step:1761/2330 train_time:69448ms step_avg:39.44ms
step:1762/2330 train_time:69508ms step_avg:39.45ms
step:1763/2330 train_time:69534ms step_avg:39.44ms
step:1764/2330 train_time:69591ms step_avg:39.45ms
step:1765/2330 train_time:69614ms step_avg:39.44ms
step:1766/2330 train_time:69670ms step_avg:39.45ms
step:1767/2330 train_time:69692ms step_avg:39.44ms
step:1768/2330 train_time:69748ms step_avg:39.45ms
step:1769/2330 train_time:69769ms step_avg:39.44ms
step:1770/2330 train_time:69826ms step_avg:39.45ms
step:1771/2330 train_time:69848ms step_avg:39.44ms
step:1772/2330 train_time:69904ms step_avg:39.45ms
step:1773/2330 train_time:69927ms step_avg:39.44ms
step:1774/2330 train_time:69983ms step_avg:39.45ms
step:1775/2330 train_time:70005ms step_avg:39.44ms
step:1776/2330 train_time:70061ms step_avg:39.45ms
step:1777/2330 train_time:70084ms step_avg:39.44ms
step:1778/2330 train_time:70139ms step_avg:39.45ms
step:1779/2330 train_time:70161ms step_avg:39.44ms
step:1780/2330 train_time:70217ms step_avg:39.45ms
step:1781/2330 train_time:70239ms step_avg:39.44ms
step:1782/2330 train_time:70295ms step_avg:39.45ms
step:1783/2330 train_time:70317ms step_avg:39.44ms
step:1784/2330 train_time:70374ms step_avg:39.45ms
step:1785/2330 train_time:70396ms step_avg:39.44ms
step:1786/2330 train_time:70454ms step_avg:39.45ms
step:1787/2330 train_time:70477ms step_avg:39.44ms
step:1788/2330 train_time:70534ms step_avg:39.45ms
step:1789/2330 train_time:70557ms step_avg:39.44ms
step:1790/2330 train_time:70613ms step_avg:39.45ms
step:1791/2330 train_time:70636ms step_avg:39.44ms
step:1792/2330 train_time:70692ms step_avg:39.45ms
step:1793/2330 train_time:70714ms step_avg:39.44ms
step:1794/2330 train_time:70770ms step_avg:39.45ms
step:1795/2330 train_time:70793ms step_avg:39.44ms
step:1796/2330 train_time:70848ms step_avg:39.45ms
step:1797/2330 train_time:70870ms step_avg:39.44ms
step:1798/2330 train_time:70926ms step_avg:39.45ms
step:1799/2330 train_time:70949ms step_avg:39.44ms
step:1800/2330 train_time:71005ms step_avg:39.45ms
step:1801/2330 train_time:71028ms step_avg:39.44ms
step:1802/2330 train_time:71084ms step_avg:39.45ms
step:1803/2330 train_time:71107ms step_avg:39.44ms
step:1804/2330 train_time:71164ms step_avg:39.45ms
step:1805/2330 train_time:71186ms step_avg:39.44ms
step:1806/2330 train_time:71242ms step_avg:39.45ms
step:1807/2330 train_time:71264ms step_avg:39.44ms
step:1808/2330 train_time:71320ms step_avg:39.45ms
step:1809/2330 train_time:71344ms step_avg:39.44ms
step:1810/2330 train_time:71400ms step_avg:39.45ms
step:1811/2330 train_time:71423ms step_avg:39.44ms
step:1812/2330 train_time:71479ms step_avg:39.45ms
step:1813/2330 train_time:71503ms step_avg:39.44ms
step:1814/2330 train_time:71559ms step_avg:39.45ms
step:1815/2330 train_time:71582ms step_avg:39.44ms
step:1816/2330 train_time:71638ms step_avg:39.45ms
step:1817/2330 train_time:71660ms step_avg:39.44ms
step:1818/2330 train_time:71715ms step_avg:39.45ms
step:1819/2330 train_time:71737ms step_avg:39.44ms
step:1820/2330 train_time:71794ms step_avg:39.45ms
step:1821/2330 train_time:71816ms step_avg:39.44ms
step:1822/2330 train_time:71873ms step_avg:39.45ms
step:1823/2330 train_time:71895ms step_avg:39.44ms
step:1824/2330 train_time:71952ms step_avg:39.45ms
step:1825/2330 train_time:71974ms step_avg:39.44ms
step:1826/2330 train_time:72031ms step_avg:39.45ms
step:1827/2330 train_time:72053ms step_avg:39.44ms
step:1828/2330 train_time:72110ms step_avg:39.45ms
step:1829/2330 train_time:72133ms step_avg:39.44ms
step:1830/2330 train_time:72189ms step_avg:39.45ms
step:1831/2330 train_time:72212ms step_avg:39.44ms
step:1832/2330 train_time:72267ms step_avg:39.45ms
step:1833/2330 train_time:72291ms step_avg:39.44ms
step:1834/2330 train_time:72346ms step_avg:39.45ms
step:1835/2330 train_time:72369ms step_avg:39.44ms
step:1836/2330 train_time:72426ms step_avg:39.45ms
step:1837/2330 train_time:72449ms step_avg:39.44ms
step:1838/2330 train_time:72506ms step_avg:39.45ms
step:1839/2330 train_time:72529ms step_avg:39.44ms
step:1840/2330 train_time:72586ms step_avg:39.45ms
step:1841/2330 train_time:72610ms step_avg:39.44ms
step:1842/2330 train_time:72666ms step_avg:39.45ms
step:1843/2330 train_time:72689ms step_avg:39.44ms
step:1844/2330 train_time:72745ms step_avg:39.45ms
step:1845/2330 train_time:72768ms step_avg:39.44ms
step:1846/2330 train_time:72825ms step_avg:39.45ms
step:1847/2330 train_time:72848ms step_avg:39.44ms
step:1848/2330 train_time:72904ms step_avg:39.45ms
step:1849/2330 train_time:72926ms step_avg:39.44ms
step:1850/2330 train_time:72982ms step_avg:39.45ms
step:1851/2330 train_time:73005ms step_avg:39.44ms
step:1852/2330 train_time:73061ms step_avg:39.45ms
step:1853/2330 train_time:73084ms step_avg:39.44ms
step:1854/2330 train_time:73140ms step_avg:39.45ms
step:1855/2330 train_time:73163ms step_avg:39.44ms
step:1856/2330 train_time:73219ms step_avg:39.45ms
step:1857/2330 train_time:73241ms step_avg:39.44ms
step:1858/2330 train_time:73297ms step_avg:39.45ms
step:1859/2330 train_time:73319ms step_avg:39.44ms
step:1860/2330 train_time:73375ms step_avg:39.45ms
step:1861/2330 train_time:73397ms step_avg:39.44ms
step:1862/2330 train_time:73454ms step_avg:39.45ms
step:1863/2330 train_time:73476ms step_avg:39.44ms
step:1864/2330 train_time:73532ms step_avg:39.45ms
step:1865/2330 train_time:73555ms step_avg:39.44ms
step:1866/2330 train_time:73612ms step_avg:39.45ms
step:1867/2330 train_time:73634ms step_avg:39.44ms
step:1868/2330 train_time:73691ms step_avg:39.45ms
step:1869/2330 train_time:73713ms step_avg:39.44ms
step:1870/2330 train_time:73770ms step_avg:39.45ms
step:1871/2330 train_time:73792ms step_avg:39.44ms
step:1872/2330 train_time:73848ms step_avg:39.45ms
step:1873/2330 train_time:73871ms step_avg:39.44ms
step:1874/2330 train_time:73927ms step_avg:39.45ms
step:1875/2330 train_time:73950ms step_avg:39.44ms
step:1876/2330 train_time:74006ms step_avg:39.45ms
step:1877/2330 train_time:74029ms step_avg:39.44ms
step:1878/2330 train_time:74085ms step_avg:39.45ms
step:1879/2330 train_time:74108ms step_avg:39.44ms
step:1880/2330 train_time:74165ms step_avg:39.45ms
step:1881/2330 train_time:74187ms step_avg:39.44ms
step:1882/2330 train_time:74244ms step_avg:39.45ms
step:1883/2330 train_time:74267ms step_avg:39.44ms
step:1884/2330 train_time:74323ms step_avg:39.45ms
step:1885/2330 train_time:74347ms step_avg:39.44ms
step:1886/2330 train_time:74403ms step_avg:39.45ms
step:1887/2330 train_time:74426ms step_avg:39.44ms
step:1888/2330 train_time:74484ms step_avg:39.45ms
step:1889/2330 train_time:74507ms step_avg:39.44ms
step:1890/2330 train_time:74564ms step_avg:39.45ms
step:1891/2330 train_time:74586ms step_avg:39.44ms
step:1892/2330 train_time:74642ms step_avg:39.45ms
step:1893/2330 train_time:74665ms step_avg:39.44ms
step:1894/2330 train_time:74722ms step_avg:39.45ms
step:1895/2330 train_time:74744ms step_avg:39.44ms
step:1896/2330 train_time:74800ms step_avg:39.45ms
step:1897/2330 train_time:74823ms step_avg:39.44ms
step:1898/2330 train_time:74879ms step_avg:39.45ms
step:1899/2330 train_time:74901ms step_avg:39.44ms
step:1900/2330 train_time:74957ms step_avg:39.45ms
step:1901/2330 train_time:74979ms step_avg:39.44ms
step:1902/2330 train_time:75036ms step_avg:39.45ms
step:1903/2330 train_time:75058ms step_avg:39.44ms
step:1904/2330 train_time:75115ms step_avg:39.45ms
step:1905/2330 train_time:75137ms step_avg:39.44ms
step:1906/2330 train_time:75193ms step_avg:39.45ms
step:1907/2330 train_time:75215ms step_avg:39.44ms
step:1908/2330 train_time:75272ms step_avg:39.45ms
step:1909/2330 train_time:75294ms step_avg:39.44ms
step:1910/2330 train_time:75351ms step_avg:39.45ms
step:1911/2330 train_time:75373ms step_avg:39.44ms
step:1912/2330 train_time:75429ms step_avg:39.45ms
step:1913/2330 train_time:75452ms step_avg:39.44ms
step:1914/2330 train_time:75508ms step_avg:39.45ms
step:1915/2330 train_time:75531ms step_avg:39.44ms
step:1916/2330 train_time:75587ms step_avg:39.45ms
step:1917/2330 train_time:75610ms step_avg:39.44ms
step:1918/2330 train_time:75666ms step_avg:39.45ms
step:1919/2330 train_time:75689ms step_avg:39.44ms
step:1920/2330 train_time:75745ms step_avg:39.45ms
step:1921/2330 train_time:75768ms step_avg:39.44ms
step:1922/2330 train_time:75824ms step_avg:39.45ms
step:1923/2330 train_time:75848ms step_avg:39.44ms
step:1924/2330 train_time:75904ms step_avg:39.45ms
step:1925/2330 train_time:75927ms step_avg:39.44ms
step:1926/2330 train_time:75983ms step_avg:39.45ms
step:1927/2330 train_time:76006ms step_avg:39.44ms
step:1928/2330 train_time:76062ms step_avg:39.45ms
step:1929/2330 train_time:76085ms step_avg:39.44ms
step:1930/2330 train_time:76141ms step_avg:39.45ms
step:1931/2330 train_time:76164ms step_avg:39.44ms
step:1932/2330 train_time:76220ms step_avg:39.45ms
step:1933/2330 train_time:76244ms step_avg:39.44ms
step:1934/2330 train_time:76300ms step_avg:39.45ms
step:1935/2330 train_time:76322ms step_avg:39.44ms
step:1936/2330 train_time:76378ms step_avg:39.45ms
step:1937/2330 train_time:76400ms step_avg:39.44ms
step:1938/2330 train_time:76456ms step_avg:39.45ms
step:1939/2330 train_time:76478ms step_avg:39.44ms
step:1940/2330 train_time:76534ms step_avg:39.45ms
step:1941/2330 train_time:76557ms step_avg:39.44ms
step:1942/2330 train_time:76614ms step_avg:39.45ms
step:1943/2330 train_time:76636ms step_avg:39.44ms
step:1944/2330 train_time:76693ms step_avg:39.45ms
step:1945/2330 train_time:76715ms step_avg:39.44ms
step:1946/2330 train_time:76772ms step_avg:39.45ms
step:1947/2330 train_time:76794ms step_avg:39.44ms
step:1948/2330 train_time:76852ms step_avg:39.45ms
step:1949/2330 train_time:76874ms step_avg:39.44ms
step:1950/2330 train_time:76931ms step_avg:39.45ms
step:1951/2330 train_time:76954ms step_avg:39.44ms
step:1952/2330 train_time:77011ms step_avg:39.45ms
step:1953/2330 train_time:77032ms step_avg:39.44ms
step:1954/2330 train_time:77089ms step_avg:39.45ms
step:1955/2330 train_time:77111ms step_avg:39.44ms
step:1956/2330 train_time:77167ms step_avg:39.45ms
step:1957/2330 train_time:77190ms step_avg:39.44ms
step:1958/2330 train_time:77246ms step_avg:39.45ms
step:1959/2330 train_time:77269ms step_avg:39.44ms
step:1960/2330 train_time:77325ms step_avg:39.45ms
step:1961/2330 train_time:77349ms step_avg:39.44ms
step:1962/2330 train_time:77405ms step_avg:39.45ms
step:1963/2330 train_time:77428ms step_avg:39.44ms
step:1964/2330 train_time:77485ms step_avg:39.45ms
step:1965/2330 train_time:77508ms step_avg:39.44ms
step:1966/2330 train_time:77564ms step_avg:39.45ms
step:1967/2330 train_time:77587ms step_avg:39.44ms
step:1968/2330 train_time:77644ms step_avg:39.45ms
step:1969/2330 train_time:77667ms step_avg:39.45ms
step:1970/2330 train_time:77724ms step_avg:39.45ms
step:1971/2330 train_time:77747ms step_avg:39.45ms
step:1972/2330 train_time:77804ms step_avg:39.45ms
step:1973/2330 train_time:77827ms step_avg:39.45ms
step:1974/2330 train_time:77883ms step_avg:39.45ms
step:1975/2330 train_time:77907ms step_avg:39.45ms
step:1976/2330 train_time:77963ms step_avg:39.46ms
step:1977/2330 train_time:77986ms step_avg:39.45ms
step:1978/2330 train_time:78042ms step_avg:39.46ms
step:1979/2330 train_time:78065ms step_avg:39.45ms
step:1980/2330 train_time:78121ms step_avg:39.45ms
step:1981/2330 train_time:78144ms step_avg:39.45ms
step:1982/2330 train_time:78200ms step_avg:39.45ms
step:1983/2330 train_time:78223ms step_avg:39.45ms
step:1984/2330 train_time:78278ms step_avg:39.45ms
step:1985/2330 train_time:78300ms step_avg:39.45ms
step:1986/2330 train_time:78356ms step_avg:39.45ms
step:1987/2330 train_time:78378ms step_avg:39.45ms
step:1988/2330 train_time:78435ms step_avg:39.45ms
step:1989/2330 train_time:78457ms step_avg:39.45ms
step:1990/2330 train_time:78513ms step_avg:39.45ms
step:1991/2330 train_time:78535ms step_avg:39.45ms
step:1992/2330 train_time:78592ms step_avg:39.45ms
step:1993/2330 train_time:78614ms step_avg:39.45ms
step:1994/2330 train_time:78670ms step_avg:39.45ms
step:1995/2330 train_time:78693ms step_avg:39.45ms
step:1996/2330 train_time:78749ms step_avg:39.45ms
step:1997/2330 train_time:78771ms step_avg:39.44ms
step:1998/2330 train_time:78827ms step_avg:39.45ms
step:1999/2330 train_time:78850ms step_avg:39.44ms
step:2000/2330 train_time:78906ms step_avg:39.45ms
step:2000/2330 val_loss:5.1317 train_time:79003ms step_avg:39.50ms
step:2001/2330 train_time:79015ms step_avg:39.49ms
step:2002/2330 train_time:79027ms step_avg:39.47ms
step:2003/2330 train_time:79036ms step_avg:39.46ms
step:2004/2330 train_time:79066ms step_avg:39.45ms
step:2005/2330 train_time:79088ms step_avg:39.45ms
step:2006/2330 train_time:79143ms step_avg:39.45ms
step:2007/2330 train_time:79165ms step_avg:39.44ms
step:2008/2330 train_time:79220ms step_avg:39.45ms
step:2009/2330 train_time:79241ms step_avg:39.44ms
step:2010/2330 train_time:79299ms step_avg:39.45ms
step:2011/2330 train_time:79323ms step_avg:39.44ms
step:2012/2330 train_time:79383ms step_avg:39.45ms
step:2013/2330 train_time:79405ms step_avg:39.45ms
step:2014/2330 train_time:79463ms step_avg:39.46ms
step:2015/2330 train_time:79485ms step_avg:39.45ms
step:2016/2330 train_time:79541ms step_avg:39.45ms
step:2017/2330 train_time:79562ms step_avg:39.45ms
step:2018/2330 train_time:79618ms step_avg:39.45ms
step:2019/2330 train_time:79640ms step_avg:39.45ms
step:2020/2330 train_time:79696ms step_avg:39.45ms
step:2021/2330 train_time:79718ms step_avg:39.45ms
step:2022/2330 train_time:79774ms step_avg:39.45ms
step:2023/2330 train_time:79796ms step_avg:39.44ms
step:2024/2330 train_time:79852ms step_avg:39.45ms
step:2025/2330 train_time:79874ms step_avg:39.44ms
step:2026/2330 train_time:79931ms step_avg:39.45ms
step:2027/2330 train_time:79955ms step_avg:39.45ms
step:2028/2330 train_time:80011ms step_avg:39.45ms
step:2029/2330 train_time:80034ms step_avg:39.44ms
step:2030/2330 train_time:80090ms step_avg:39.45ms
step:2031/2330 train_time:80112ms step_avg:39.44ms
step:2032/2330 train_time:80168ms step_avg:39.45ms
step:2033/2330 train_time:80191ms step_avg:39.44ms
step:2034/2330 train_time:80247ms step_avg:39.45ms
step:2035/2330 train_time:80271ms step_avg:39.45ms
step:2036/2330 train_time:80330ms step_avg:39.45ms
step:2037/2330 train_time:80354ms step_avg:39.45ms
step:2038/2330 train_time:80411ms step_avg:39.46ms
step:2039/2330 train_time:80434ms step_avg:39.45ms
step:2040/2330 train_time:80491ms step_avg:39.46ms
step:2041/2330 train_time:80514ms step_avg:39.45ms
step:2042/2330 train_time:80570ms step_avg:39.46ms
step:2043/2330 train_time:80593ms step_avg:39.45ms
step:2044/2330 train_time:80649ms step_avg:39.46ms
step:2045/2330 train_time:80672ms step_avg:39.45ms
step:2046/2330 train_time:80728ms step_avg:39.46ms
step:2047/2330 train_time:80751ms step_avg:39.45ms
step:2048/2330 train_time:80806ms step_avg:39.46ms
step:2049/2330 train_time:80829ms step_avg:39.45ms
step:2050/2330 train_time:80885ms step_avg:39.46ms
step:2051/2330 train_time:80907ms step_avg:39.45ms
step:2052/2330 train_time:80963ms step_avg:39.46ms
step:2053/2330 train_time:80985ms step_avg:39.45ms
step:2054/2330 train_time:81041ms step_avg:39.46ms
step:2055/2330 train_time:81063ms step_avg:39.45ms
step:2056/2330 train_time:81119ms step_avg:39.45ms
step:2057/2330 train_time:81141ms step_avg:39.45ms
step:2058/2330 train_time:81199ms step_avg:39.46ms
step:2059/2330 train_time:81222ms step_avg:39.45ms
step:2060/2330 train_time:81279ms step_avg:39.46ms
step:2061/2330 train_time:81301ms step_avg:39.45ms
step:2062/2330 train_time:81358ms step_avg:39.46ms
step:2063/2330 train_time:81381ms step_avg:39.45ms
step:2064/2330 train_time:81438ms step_avg:39.46ms
step:2065/2330 train_time:81460ms step_avg:39.45ms
step:2066/2330 train_time:81518ms step_avg:39.46ms
step:2067/2330 train_time:81540ms step_avg:39.45ms
step:2068/2330 train_time:81596ms step_avg:39.46ms
step:2069/2330 train_time:81618ms step_avg:39.45ms
step:2070/2330 train_time:81675ms step_avg:39.46ms
step:2071/2330 train_time:81697ms step_avg:39.45ms
step:2072/2330 train_time:81753ms step_avg:39.46ms
step:2073/2330 train_time:81776ms step_avg:39.45ms
step:2074/2330 train_time:81832ms step_avg:39.46ms
step:2075/2330 train_time:81854ms step_avg:39.45ms
step:2076/2330 train_time:81910ms step_avg:39.46ms
step:2077/2330 train_time:81933ms step_avg:39.45ms
step:2078/2330 train_time:81989ms step_avg:39.46ms
step:2079/2330 train_time:82012ms step_avg:39.45ms
step:2080/2330 train_time:82068ms step_avg:39.46ms
step:2081/2330 train_time:82091ms step_avg:39.45ms
step:2082/2330 train_time:82147ms step_avg:39.46ms
step:2083/2330 train_time:82170ms step_avg:39.45ms
step:2084/2330 train_time:82227ms step_avg:39.46ms
step:2085/2330 train_time:82250ms step_avg:39.45ms
step:2086/2330 train_time:82307ms step_avg:39.46ms
step:2087/2330 train_time:82330ms step_avg:39.45ms
step:2088/2330 train_time:82386ms step_avg:39.46ms
step:2089/2330 train_time:82409ms step_avg:39.45ms
step:2090/2330 train_time:82466ms step_avg:39.46ms
step:2091/2330 train_time:82489ms step_avg:39.45ms
step:2092/2330 train_time:82546ms step_avg:39.46ms
step:2093/2330 train_time:82568ms step_avg:39.45ms
step:2094/2330 train_time:82624ms step_avg:39.46ms
step:2095/2330 train_time:82646ms step_avg:39.45ms
step:2096/2330 train_time:82702ms step_avg:39.46ms
step:2097/2330 train_time:82724ms step_avg:39.45ms
step:2098/2330 train_time:82781ms step_avg:39.46ms
step:2099/2330 train_time:82803ms step_avg:39.45ms
step:2100/2330 train_time:82860ms step_avg:39.46ms
step:2101/2330 train_time:82881ms step_avg:39.45ms
step:2102/2330 train_time:82938ms step_avg:39.46ms
step:2103/2330 train_time:82960ms step_avg:39.45ms
step:2104/2330 train_time:83017ms step_avg:39.46ms
step:2105/2330 train_time:83039ms step_avg:39.45ms
step:2106/2330 train_time:83096ms step_avg:39.46ms
step:2107/2330 train_time:83119ms step_avg:39.45ms
step:2108/2330 train_time:83176ms step_avg:39.46ms
step:2109/2330 train_time:83198ms step_avg:39.45ms
step:2110/2330 train_time:83255ms step_avg:39.46ms
step:2111/2330 train_time:83278ms step_avg:39.45ms
step:2112/2330 train_time:83335ms step_avg:39.46ms
step:2113/2330 train_time:83358ms step_avg:39.45ms
step:2114/2330 train_time:83415ms step_avg:39.46ms
step:2115/2330 train_time:83437ms step_avg:39.45ms
step:2116/2330 train_time:83494ms step_avg:39.46ms
step:2117/2330 train_time:83516ms step_avg:39.45ms
step:2118/2330 train_time:83572ms step_avg:39.46ms
step:2119/2330 train_time:83595ms step_avg:39.45ms
step:2120/2330 train_time:83651ms step_avg:39.46ms
step:2121/2330 train_time:83674ms step_avg:39.45ms
step:2122/2330 train_time:83730ms step_avg:39.46ms
step:2123/2330 train_time:83753ms step_avg:39.45ms
step:2124/2330 train_time:83809ms step_avg:39.46ms
step:2125/2330 train_time:83833ms step_avg:39.45ms
step:2126/2330 train_time:83890ms step_avg:39.46ms
step:2127/2330 train_time:83913ms step_avg:39.45ms
step:2128/2330 train_time:83970ms step_avg:39.46ms
step:2129/2330 train_time:83992ms step_avg:39.45ms
step:2130/2330 train_time:84049ms step_avg:39.46ms
step:2131/2330 train_time:84072ms step_avg:39.45ms
step:2132/2330 train_time:84128ms step_avg:39.46ms
step:2133/2330 train_time:84151ms step_avg:39.45ms
step:2134/2330 train_time:84207ms step_avg:39.46ms
step:2135/2330 train_time:84230ms step_avg:39.45ms
step:2136/2330 train_time:84287ms step_avg:39.46ms
step:2137/2330 train_time:84310ms step_avg:39.45ms
step:2138/2330 train_time:84366ms step_avg:39.46ms
step:2139/2330 train_time:84389ms step_avg:39.45ms
step:2140/2330 train_time:84445ms step_avg:39.46ms
step:2141/2330 train_time:84468ms step_avg:39.45ms
step:2142/2330 train_time:84525ms step_avg:39.46ms
step:2143/2330 train_time:84548ms step_avg:39.45ms
step:2144/2330 train_time:84604ms step_avg:39.46ms
step:2145/2330 train_time:84627ms step_avg:39.45ms
step:2146/2330 train_time:84682ms step_avg:39.46ms
step:2147/2330 train_time:84704ms step_avg:39.45ms
step:2148/2330 train_time:84760ms step_avg:39.46ms
step:2149/2330 train_time:84783ms step_avg:39.45ms
step:2150/2330 train_time:84839ms step_avg:39.46ms
step:2151/2330 train_time:84862ms step_avg:39.45ms
step:2152/2330 train_time:84919ms step_avg:39.46ms
step:2153/2330 train_time:84941ms step_avg:39.45ms
step:2154/2330 train_time:84997ms step_avg:39.46ms
step:2155/2330 train_time:85019ms step_avg:39.45ms
step:2156/2330 train_time:85076ms step_avg:39.46ms
step:2157/2330 train_time:85098ms step_avg:39.45ms
step:2158/2330 train_time:85155ms step_avg:39.46ms
step:2159/2330 train_time:85177ms step_avg:39.45ms
step:2160/2330 train_time:85234ms step_avg:39.46ms
step:2161/2330 train_time:85257ms step_avg:39.45ms
step:2162/2330 train_time:85313ms step_avg:39.46ms
step:2163/2330 train_time:85336ms step_avg:39.45ms
step:2164/2330 train_time:85392ms step_avg:39.46ms
step:2165/2330 train_time:85415ms step_avg:39.45ms
step:2166/2330 train_time:85471ms step_avg:39.46ms
step:2167/2330 train_time:85495ms step_avg:39.45ms
step:2168/2330 train_time:85551ms step_avg:39.46ms
step:2169/2330 train_time:85575ms step_avg:39.45ms
step:2170/2330 train_time:85631ms step_avg:39.46ms
step:2171/2330 train_time:85654ms step_avg:39.45ms
step:2172/2330 train_time:85710ms step_avg:39.46ms
step:2173/2330 train_time:85733ms step_avg:39.45ms
step:2174/2330 train_time:85789ms step_avg:39.46ms
step:2175/2330 train_time:85813ms step_avg:39.45ms
step:2176/2330 train_time:85869ms step_avg:39.46ms
step:2177/2330 train_time:85892ms step_avg:39.45ms
step:2178/2330 train_time:85949ms step_avg:39.46ms
step:2179/2330 train_time:85971ms step_avg:39.45ms
step:2180/2330 train_time:86028ms step_avg:39.46ms
step:2181/2330 train_time:86050ms step_avg:39.45ms
step:2182/2330 train_time:86107ms step_avg:39.46ms
step:2183/2330 train_time:86129ms step_avg:39.45ms
step:2184/2330 train_time:86186ms step_avg:39.46ms
step:2185/2330 train_time:86209ms step_avg:39.45ms
step:2186/2330 train_time:86265ms step_avg:39.46ms
step:2187/2330 train_time:86288ms step_avg:39.45ms
step:2188/2330 train_time:86343ms step_avg:39.46ms
step:2189/2330 train_time:86366ms step_avg:39.45ms
step:2190/2330 train_time:86422ms step_avg:39.46ms
step:2191/2330 train_time:86443ms step_avg:39.45ms
step:2192/2330 train_time:86499ms step_avg:39.46ms
step:2193/2330 train_time:86521ms step_avg:39.45ms
step:2194/2330 train_time:86578ms step_avg:39.46ms
step:2195/2330 train_time:86600ms step_avg:39.45ms
step:2196/2330 train_time:86657ms step_avg:39.46ms
step:2197/2330 train_time:86679ms step_avg:39.45ms
step:2198/2330 train_time:86736ms step_avg:39.46ms
step:2199/2330 train_time:86758ms step_avg:39.45ms
step:2200/2330 train_time:86815ms step_avg:39.46ms
step:2201/2330 train_time:86837ms step_avg:39.45ms
step:2202/2330 train_time:86894ms step_avg:39.46ms
step:2203/2330 train_time:86916ms step_avg:39.45ms
step:2204/2330 train_time:86972ms step_avg:39.46ms
step:2205/2330 train_time:86995ms step_avg:39.45ms
step:2206/2330 train_time:87051ms step_avg:39.46ms
step:2207/2330 train_time:87073ms step_avg:39.45ms
step:2208/2330 train_time:87130ms step_avg:39.46ms
step:2209/2330 train_time:87153ms step_avg:39.45ms
step:2210/2330 train_time:87209ms step_avg:39.46ms
step:2211/2330 train_time:87232ms step_avg:39.45ms
step:2212/2330 train_time:87289ms step_avg:39.46ms
step:2213/2330 train_time:87311ms step_avg:39.45ms
step:2214/2330 train_time:87368ms step_avg:39.46ms
step:2215/2330 train_time:87391ms step_avg:39.45ms
step:2216/2330 train_time:87447ms step_avg:39.46ms
step:2217/2330 train_time:87470ms step_avg:39.45ms
step:2218/2330 train_time:87528ms step_avg:39.46ms
step:2219/2330 train_time:87549ms step_avg:39.45ms
step:2220/2330 train_time:87606ms step_avg:39.46ms
step:2221/2330 train_time:87628ms step_avg:39.45ms
step:2222/2330 train_time:87684ms step_avg:39.46ms
step:2223/2330 train_time:87707ms step_avg:39.45ms
step:2224/2330 train_time:87764ms step_avg:39.46ms
step:2225/2330 train_time:87786ms step_avg:39.45ms
step:2226/2330 train_time:87842ms step_avg:39.46ms
step:2227/2330 train_time:87864ms step_avg:39.45ms
step:2228/2330 train_time:87920ms step_avg:39.46ms
step:2229/2330 train_time:87942ms step_avg:39.45ms
step:2230/2330 train_time:87999ms step_avg:39.46ms
step:2231/2330 train_time:88021ms step_avg:39.45ms
step:2232/2330 train_time:88078ms step_avg:39.46ms
step:2233/2330 train_time:88100ms step_avg:39.45ms
step:2234/2330 train_time:88158ms step_avg:39.46ms
step:2235/2330 train_time:88180ms step_avg:39.45ms
step:2236/2330 train_time:88237ms step_avg:39.46ms
step:2237/2330 train_time:88260ms step_avg:39.45ms
step:2238/2330 train_time:88317ms step_avg:39.46ms
step:2239/2330 train_time:88339ms step_avg:39.45ms
step:2240/2330 train_time:88396ms step_avg:39.46ms
step:2241/2330 train_time:88418ms step_avg:39.45ms
step:2242/2330 train_time:88474ms step_avg:39.46ms
step:2243/2330 train_time:88496ms step_avg:39.45ms
step:2244/2330 train_time:88552ms step_avg:39.46ms
step:2245/2330 train_time:88575ms step_avg:39.45ms
step:2246/2330 train_time:88630ms step_avg:39.46ms
step:2247/2330 train_time:88654ms step_avg:39.45ms
step:2248/2330 train_time:88710ms step_avg:39.46ms
step:2249/2330 train_time:88734ms step_avg:39.45ms
step:2250/2330 train_time:88790ms step_avg:39.46ms
step:2250/2330 val_loss:5.1085 train_time:88888ms step_avg:39.51ms
step:2251/2330 train_time:88901ms step_avg:39.49ms
step:2252/2330 train_time:88913ms step_avg:39.48ms
step:2253/2330 train_time:88924ms step_avg:39.47ms
step:2254/2330 train_time:88950ms step_avg:39.46ms
step:2255/2330 train_time:88971ms step_avg:39.46ms
step:2256/2330 train_time:89027ms step_avg:39.46ms
step:2257/2330 train_time:89049ms step_avg:39.45ms
step:2258/2330 train_time:89105ms step_avg:39.46ms
step:2259/2330 train_time:89127ms step_avg:39.45ms
step:2260/2330 train_time:89185ms step_avg:39.46ms
step:2261/2330 train_time:89209ms step_avg:39.46ms
step:2262/2330 train_time:89270ms step_avg:39.46ms
step:2263/2330 train_time:89293ms step_avg:39.46ms
step:2264/2330 train_time:89350ms step_avg:39.47ms
step:2265/2330 train_time:89372ms step_avg:39.46ms
step:2266/2330 train_time:89429ms step_avg:39.47ms
step:2267/2330 train_time:89451ms step_avg:39.46ms
step:2268/2330 train_time:89507ms step_avg:39.47ms
step:2269/2330 train_time:89530ms step_avg:39.46ms
step:2270/2330 train_time:89586ms step_avg:39.47ms
step:2271/2330 train_time:89608ms step_avg:39.46ms
step:2272/2330 train_time:89663ms step_avg:39.46ms
step:2273/2330 train_time:89685ms step_avg:39.46ms
step:2274/2330 train_time:89740ms step_avg:39.46ms
step:2275/2330 train_time:89763ms step_avg:39.46ms
step:2276/2330 train_time:89819ms step_avg:39.46ms
step:2277/2330 train_time:89843ms step_avg:39.46ms
step:2278/2330 train_time:89899ms step_avg:39.46ms
step:2279/2330 train_time:89922ms step_avg:39.46ms
step:2280/2330 train_time:89978ms step_avg:39.46ms
step:2281/2330 train_time:90000ms step_avg:39.46ms
step:2282/2330 train_time:90056ms step_avg:39.46ms
step:2283/2330 train_time:90078ms step_avg:39.46ms
step:2284/2330 train_time:90135ms step_avg:39.46ms
step:2285/2330 train_time:90159ms step_avg:39.46ms
step:2286/2330 train_time:90216ms step_avg:39.46ms
step:2287/2330 train_time:90240ms step_avg:39.46ms
step:2288/2330 train_time:90298ms step_avg:39.47ms
step:2289/2330 train_time:90321ms step_avg:39.46ms
step:2290/2330 train_time:90378ms step_avg:39.47ms
step:2291/2330 train_time:90403ms step_avg:39.46ms
step:2292/2330 train_time:90459ms step_avg:39.47ms
step:2293/2330 train_time:90482ms step_avg:39.46ms
step:2294/2330 train_time:90538ms step_avg:39.47ms
step:2295/2330 train_time:90561ms step_avg:39.46ms
step:2296/2330 train_time:90616ms step_avg:39.47ms
step:2297/2330 train_time:90639ms step_avg:39.46ms
step:2298/2330 train_time:90695ms step_avg:39.47ms
step:2299/2330 train_time:90717ms step_avg:39.46ms
step:2300/2330 train_time:90773ms step_avg:39.47ms
step:2301/2330 train_time:90795ms step_avg:39.46ms
step:2302/2330 train_time:90851ms step_avg:39.47ms
step:2303/2330 train_time:90873ms step_avg:39.46ms
step:2304/2330 train_time:90930ms step_avg:39.47ms
step:2305/2330 train_time:90951ms step_avg:39.46ms
step:2306/2330 train_time:91008ms step_avg:39.47ms
step:2307/2330 train_time:91029ms step_avg:39.46ms
step:2308/2330 train_time:91086ms step_avg:39.47ms
step:2309/2330 train_time:91109ms step_avg:39.46ms
step:2310/2330 train_time:91165ms step_avg:39.47ms
step:2311/2330 train_time:91188ms step_avg:39.46ms
step:2312/2330 train_time:91246ms step_avg:39.47ms
step:2313/2330 train_time:91269ms step_avg:39.46ms
step:2314/2330 train_time:91327ms step_avg:39.47ms
step:2315/2330 train_time:91349ms step_avg:39.46ms
step:2316/2330 train_time:91406ms step_avg:39.47ms
step:2317/2330 train_time:91428ms step_avg:39.46ms
step:2318/2330 train_time:91485ms step_avg:39.47ms
step:2319/2330 train_time:91507ms step_avg:39.46ms
step:2320/2330 train_time:91563ms step_avg:39.47ms
step:2321/2330 train_time:91586ms step_avg:39.46ms
step:2322/2330 train_time:91642ms step_avg:39.47ms
step:2323/2330 train_time:91665ms step_avg:39.46ms
step:2324/2330 train_time:91721ms step_avg:39.47ms
step:2325/2330 train_time:91744ms step_avg:39.46ms
step:2326/2330 train_time:91800ms step_avg:39.47ms
step:2327/2330 train_time:91823ms step_avg:39.46ms
step:2328/2330 train_time:91879ms step_avg:39.47ms
step:2329/2330 train_time:91901ms step_avg:39.46ms
step:2330/2330 train_time:91958ms step_avg:39.47ms
step:2330/2330 val_loss:5.1019 train_time:92055ms step_avg:39.51ms
peak memory allocated: 29712 MiB reserved: 38888 MiB
