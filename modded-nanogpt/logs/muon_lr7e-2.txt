import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "muon_lr7e-2"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=7e-2, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Oct 28 23:40:11 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             109W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   27C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   26C    P0             107W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   26C    P0             108W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:91ms step_avg:91.09ms
step:2/2330 train_time:183ms step_avg:91.34ms
step:3/2330 train_time:204ms step_avg:67.98ms
step:4/2330 train_time:239ms step_avg:59.85ms
step:5/2330 train_time:297ms step_avg:59.31ms
step:6/2330 train_time:357ms step_avg:59.51ms
step:7/2330 train_time:416ms step_avg:59.37ms
step:8/2330 train_time:476ms step_avg:59.52ms
step:9/2330 train_time:535ms step_avg:59.43ms
step:10/2330 train_time:595ms step_avg:59.54ms
step:11/2330 train_time:655ms step_avg:59.50ms
step:12/2330 train_time:715ms step_avg:59.60ms
step:13/2330 train_time:774ms step_avg:59.52ms
step:14/2330 train_time:835ms step_avg:59.63ms
step:15/2330 train_time:893ms step_avg:59.55ms
step:16/2330 train_time:954ms step_avg:59.62ms
step:17/2330 train_time:1013ms step_avg:59.56ms
step:18/2330 train_time:1076ms step_avg:59.75ms
step:19/2330 train_time:1138ms step_avg:59.91ms
step:20/2330 train_time:1202ms step_avg:60.10ms
step:21/2330 train_time:1262ms step_avg:60.09ms
step:22/2330 train_time:1324ms step_avg:60.16ms
step:23/2330 train_time:1383ms step_avg:60.15ms
step:24/2330 train_time:1445ms step_avg:60.21ms
step:25/2330 train_time:1504ms step_avg:60.17ms
step:26/2330 train_time:1566ms step_avg:60.22ms
step:27/2330 train_time:1625ms step_avg:60.17ms
step:28/2330 train_time:1686ms step_avg:60.21ms
step:29/2330 train_time:1745ms step_avg:60.18ms
step:30/2330 train_time:1806ms step_avg:60.20ms
step:31/2330 train_time:1865ms step_avg:60.16ms
step:32/2330 train_time:1926ms step_avg:60.18ms
step:33/2330 train_time:1985ms step_avg:60.15ms
step:34/2330 train_time:2048ms step_avg:60.23ms
step:35/2330 train_time:2108ms step_avg:60.24ms
step:36/2330 train_time:2171ms step_avg:60.29ms
step:37/2330 train_time:2231ms step_avg:60.29ms
step:38/2330 train_time:2293ms step_avg:60.34ms
step:39/2330 train_time:2354ms step_avg:60.35ms
step:40/2330 train_time:2415ms step_avg:60.38ms
step:41/2330 train_time:2474ms step_avg:60.35ms
step:42/2330 train_time:2535ms step_avg:60.36ms
step:43/2330 train_time:2594ms step_avg:60.33ms
step:44/2330 train_time:2655ms step_avg:60.35ms
step:45/2330 train_time:2714ms step_avg:60.32ms
step:46/2330 train_time:2775ms step_avg:60.33ms
step:47/2330 train_time:2834ms step_avg:60.30ms
step:48/2330 train_time:2895ms step_avg:60.31ms
step:49/2330 train_time:2954ms step_avg:60.29ms
step:50/2330 train_time:3015ms step_avg:60.31ms
step:51/2330 train_time:3075ms step_avg:60.29ms
step:52/2330 train_time:3137ms step_avg:60.32ms
step:53/2330 train_time:3196ms step_avg:60.30ms
step:54/2330 train_time:3258ms step_avg:60.33ms
step:55/2330 train_time:3317ms step_avg:60.31ms
step:56/2330 train_time:3378ms step_avg:60.32ms
step:57/2330 train_time:3437ms step_avg:60.29ms
step:58/2330 train_time:3498ms step_avg:60.30ms
step:59/2330 train_time:3557ms step_avg:60.28ms
step:60/2330 train_time:3617ms step_avg:60.29ms
step:61/2330 train_time:3677ms step_avg:60.27ms
step:62/2330 train_time:3738ms step_avg:60.29ms
step:63/2330 train_time:3797ms step_avg:60.27ms
step:64/2330 train_time:3858ms step_avg:60.29ms
step:65/2330 train_time:3917ms step_avg:60.27ms
step:66/2330 train_time:3978ms step_avg:60.28ms
step:67/2330 train_time:4037ms step_avg:60.26ms
step:68/2330 train_time:4098ms step_avg:60.27ms
step:69/2330 train_time:4158ms step_avg:60.26ms
step:70/2330 train_time:4218ms step_avg:60.26ms
step:71/2330 train_time:4277ms step_avg:60.24ms
step:72/2330 train_time:4339ms step_avg:60.27ms
step:73/2330 train_time:4398ms step_avg:60.25ms
step:74/2330 train_time:4459ms step_avg:60.26ms
step:75/2330 train_time:4518ms step_avg:60.24ms
step:76/2330 train_time:4579ms step_avg:60.24ms
step:77/2330 train_time:4638ms step_avg:60.23ms
step:78/2330 train_time:4699ms step_avg:60.24ms
step:79/2330 train_time:4758ms step_avg:60.22ms
step:80/2330 train_time:4818ms step_avg:60.23ms
step:81/2330 train_time:4877ms step_avg:60.21ms
step:82/2330 train_time:4938ms step_avg:60.22ms
step:83/2330 train_time:4997ms step_avg:60.20ms
step:84/2330 train_time:5058ms step_avg:60.21ms
step:85/2330 train_time:5117ms step_avg:60.20ms
step:86/2330 train_time:5178ms step_avg:60.21ms
step:87/2330 train_time:5237ms step_avg:60.20ms
step:88/2330 train_time:5298ms step_avg:60.21ms
step:89/2330 train_time:5357ms step_avg:60.19ms
step:90/2330 train_time:5417ms step_avg:60.19ms
step:91/2330 train_time:5476ms step_avg:60.17ms
step:92/2330 train_time:5537ms step_avg:60.18ms
step:93/2330 train_time:5595ms step_avg:60.16ms
step:94/2330 train_time:5656ms step_avg:60.17ms
step:95/2330 train_time:5715ms step_avg:60.16ms
step:96/2330 train_time:5776ms step_avg:60.16ms
step:97/2330 train_time:5835ms step_avg:60.15ms
step:98/2330 train_time:5896ms step_avg:60.17ms
step:99/2330 train_time:5955ms step_avg:60.15ms
step:100/2330 train_time:6016ms step_avg:60.16ms
step:101/2330 train_time:6075ms step_avg:60.15ms
step:102/2330 train_time:6137ms step_avg:60.17ms
step:103/2330 train_time:6196ms step_avg:60.16ms
step:104/2330 train_time:6257ms step_avg:60.16ms
step:105/2330 train_time:6316ms step_avg:60.15ms
step:106/2330 train_time:6376ms step_avg:60.16ms
step:107/2330 train_time:6435ms step_avg:60.14ms
step:108/2330 train_time:6496ms step_avg:60.15ms
step:109/2330 train_time:6555ms step_avg:60.14ms
step:110/2330 train_time:6616ms step_avg:60.14ms
step:111/2330 train_time:6674ms step_avg:60.13ms
step:112/2330 train_time:6735ms step_avg:60.13ms
step:113/2330 train_time:6794ms step_avg:60.12ms
step:114/2330 train_time:6855ms step_avg:60.13ms
step:115/2330 train_time:6914ms step_avg:60.12ms
step:116/2330 train_time:6975ms step_avg:60.13ms
step:117/2330 train_time:7034ms step_avg:60.12ms
step:118/2330 train_time:7096ms step_avg:60.13ms
step:119/2330 train_time:7154ms step_avg:60.12ms
step:120/2330 train_time:7215ms step_avg:60.13ms
step:121/2330 train_time:7274ms step_avg:60.12ms
step:122/2330 train_time:7335ms step_avg:60.12ms
step:123/2330 train_time:7394ms step_avg:60.11ms
step:124/2330 train_time:7455ms step_avg:60.12ms
step:125/2330 train_time:7514ms step_avg:60.11ms
step:126/2330 train_time:7575ms step_avg:60.12ms
step:127/2330 train_time:7634ms step_avg:60.11ms
step:128/2330 train_time:7694ms step_avg:60.11ms
step:129/2330 train_time:7753ms step_avg:60.10ms
step:130/2330 train_time:7814ms step_avg:60.11ms
step:131/2330 train_time:7873ms step_avg:60.10ms
step:132/2330 train_time:7934ms step_avg:60.11ms
step:133/2330 train_time:7993ms step_avg:60.10ms
step:134/2330 train_time:8054ms step_avg:60.11ms
step:135/2330 train_time:8113ms step_avg:60.10ms
step:136/2330 train_time:8174ms step_avg:60.10ms
step:137/2330 train_time:8233ms step_avg:60.10ms
step:138/2330 train_time:8294ms step_avg:60.10ms
step:139/2330 train_time:8353ms step_avg:60.10ms
step:140/2330 train_time:8414ms step_avg:60.10ms
step:141/2330 train_time:8473ms step_avg:60.09ms
step:142/2330 train_time:8534ms step_avg:60.10ms
step:143/2330 train_time:8593ms step_avg:60.09ms
step:144/2330 train_time:8655ms step_avg:60.10ms
step:145/2330 train_time:8713ms step_avg:60.09ms
step:146/2330 train_time:8774ms step_avg:60.10ms
step:147/2330 train_time:8833ms step_avg:60.09ms
step:148/2330 train_time:8894ms step_avg:60.10ms
step:149/2330 train_time:8953ms step_avg:60.09ms
step:150/2330 train_time:9014ms step_avg:60.10ms
step:151/2330 train_time:9073ms step_avg:60.09ms
step:152/2330 train_time:9134ms step_avg:60.09ms
step:153/2330 train_time:9193ms step_avg:60.08ms
step:154/2330 train_time:9254ms step_avg:60.09ms
step:155/2330 train_time:9313ms step_avg:60.08ms
step:156/2330 train_time:9374ms step_avg:60.09ms
step:157/2330 train_time:9432ms step_avg:60.08ms
step:158/2330 train_time:9494ms step_avg:60.09ms
step:159/2330 train_time:9552ms step_avg:60.08ms
step:160/2330 train_time:9613ms step_avg:60.08ms
step:161/2330 train_time:9672ms step_avg:60.07ms
step:162/2330 train_time:9733ms step_avg:60.08ms
step:163/2330 train_time:9792ms step_avg:60.07ms
step:164/2330 train_time:9852ms step_avg:60.08ms
step:165/2330 train_time:9912ms step_avg:60.07ms
step:166/2330 train_time:9973ms step_avg:60.08ms
step:167/2330 train_time:10032ms step_avg:60.07ms
step:168/2330 train_time:10093ms step_avg:60.08ms
step:169/2330 train_time:10153ms step_avg:60.08ms
step:170/2330 train_time:10214ms step_avg:60.08ms
step:171/2330 train_time:10272ms step_avg:60.07ms
step:172/2330 train_time:10334ms step_avg:60.08ms
step:173/2330 train_time:10393ms step_avg:60.08ms
step:174/2330 train_time:10454ms step_avg:60.08ms
step:175/2330 train_time:10513ms step_avg:60.07ms
step:176/2330 train_time:10574ms step_avg:60.08ms
step:177/2330 train_time:10633ms step_avg:60.08ms
step:178/2330 train_time:10694ms step_avg:60.08ms
step:179/2330 train_time:10754ms step_avg:60.08ms
step:180/2330 train_time:10815ms step_avg:60.08ms
step:181/2330 train_time:10873ms step_avg:60.07ms
step:182/2330 train_time:10934ms step_avg:60.08ms
step:183/2330 train_time:10993ms step_avg:60.07ms
step:184/2330 train_time:11055ms step_avg:60.08ms
step:185/2330 train_time:11113ms step_avg:60.07ms
step:186/2330 train_time:11174ms step_avg:60.08ms
step:187/2330 train_time:11233ms step_avg:60.07ms
step:188/2330 train_time:11294ms step_avg:60.07ms
step:189/2330 train_time:11353ms step_avg:60.07ms
step:190/2330 train_time:11414ms step_avg:60.07ms
step:191/2330 train_time:11472ms step_avg:60.07ms
step:192/2330 train_time:11534ms step_avg:60.07ms
step:193/2330 train_time:11593ms step_avg:60.07ms
step:194/2330 train_time:11654ms step_avg:60.07ms
step:195/2330 train_time:11713ms step_avg:60.07ms
step:196/2330 train_time:11774ms step_avg:60.07ms
step:197/2330 train_time:11833ms step_avg:60.07ms
step:198/2330 train_time:11894ms step_avg:60.07ms
step:199/2330 train_time:11953ms step_avg:60.07ms
step:200/2330 train_time:12014ms step_avg:60.07ms
step:201/2330 train_time:12072ms step_avg:60.06ms
step:202/2330 train_time:12134ms step_avg:60.07ms
step:203/2330 train_time:12192ms step_avg:60.06ms
step:204/2330 train_time:12253ms step_avg:60.06ms
step:205/2330 train_time:12312ms step_avg:60.06ms
step:206/2330 train_time:12373ms step_avg:60.06ms
step:207/2330 train_time:12432ms step_avg:60.06ms
step:208/2330 train_time:12493ms step_avg:60.06ms
step:209/2330 train_time:12552ms step_avg:60.06ms
step:210/2330 train_time:12613ms step_avg:60.06ms
step:211/2330 train_time:12672ms step_avg:60.06ms
step:212/2330 train_time:12733ms step_avg:60.06ms
step:213/2330 train_time:12792ms step_avg:60.06ms
step:214/2330 train_time:12854ms step_avg:60.06ms
step:215/2330 train_time:12913ms step_avg:60.06ms
step:216/2330 train_time:12974ms step_avg:60.06ms
step:217/2330 train_time:13033ms step_avg:60.06ms
step:218/2330 train_time:13094ms step_avg:60.06ms
step:219/2330 train_time:13153ms step_avg:60.06ms
step:220/2330 train_time:13214ms step_avg:60.06ms
step:221/2330 train_time:13273ms step_avg:60.06ms
step:222/2330 train_time:13334ms step_avg:60.06ms
step:223/2330 train_time:13392ms step_avg:60.06ms
step:224/2330 train_time:13453ms step_avg:60.06ms
step:225/2330 train_time:13512ms step_avg:60.05ms
step:226/2330 train_time:13573ms step_avg:60.06ms
step:227/2330 train_time:13632ms step_avg:60.05ms
step:228/2330 train_time:13694ms step_avg:60.06ms
step:229/2330 train_time:13752ms step_avg:60.05ms
step:230/2330 train_time:13813ms step_avg:60.06ms
step:231/2330 train_time:13872ms step_avg:60.05ms
step:232/2330 train_time:13934ms step_avg:60.06ms
step:233/2330 train_time:13993ms step_avg:60.06ms
step:234/2330 train_time:14054ms step_avg:60.06ms
step:235/2330 train_time:14113ms step_avg:60.05ms
step:236/2330 train_time:14174ms step_avg:60.06ms
step:237/2330 train_time:14233ms step_avg:60.05ms
step:238/2330 train_time:14294ms step_avg:60.06ms
step:239/2330 train_time:14353ms step_avg:60.05ms
step:240/2330 train_time:14414ms step_avg:60.06ms
step:241/2330 train_time:14472ms step_avg:60.05ms
step:242/2330 train_time:14534ms step_avg:60.06ms
step:243/2330 train_time:14593ms step_avg:60.05ms
step:244/2330 train_time:14654ms step_avg:60.06ms
step:245/2330 train_time:14713ms step_avg:60.05ms
step:246/2330 train_time:14774ms step_avg:60.06ms
step:247/2330 train_time:14832ms step_avg:60.05ms
step:248/2330 train_time:14893ms step_avg:60.05ms
step:249/2330 train_time:14952ms step_avg:60.05ms
step:250/2330 train_time:15013ms step_avg:60.05ms
step:250/2330 val_loss:4.1072 train_time:15076ms step_avg:60.30ms
step:251/2330 train_time:15099ms step_avg:60.16ms
step:252/2330 train_time:15135ms step_avg:60.06ms
step:253/2330 train_time:15197ms step_avg:60.07ms
step:254/2330 train_time:15261ms step_avg:60.08ms
step:255/2330 train_time:15322ms step_avg:60.09ms
step:256/2330 train_time:15384ms step_avg:60.09ms
step:257/2330 train_time:15443ms step_avg:60.09ms
step:258/2330 train_time:15504ms step_avg:60.09ms
step:259/2330 train_time:15562ms step_avg:60.09ms
step:260/2330 train_time:15623ms step_avg:60.09ms
step:261/2330 train_time:15681ms step_avg:60.08ms
step:262/2330 train_time:15741ms step_avg:60.08ms
step:263/2330 train_time:15799ms step_avg:60.07ms
step:264/2330 train_time:15859ms step_avg:60.07ms
step:265/2330 train_time:15917ms step_avg:60.07ms
step:266/2330 train_time:15978ms step_avg:60.07ms
step:267/2330 train_time:16036ms step_avg:60.06ms
step:268/2330 train_time:16097ms step_avg:60.06ms
step:269/2330 train_time:16156ms step_avg:60.06ms
step:270/2330 train_time:16219ms step_avg:60.07ms
step:271/2330 train_time:16280ms step_avg:60.07ms
step:272/2330 train_time:16342ms step_avg:60.08ms
step:273/2330 train_time:16401ms step_avg:60.08ms
step:274/2330 train_time:16462ms step_avg:60.08ms
step:275/2330 train_time:16521ms step_avg:60.08ms
step:276/2330 train_time:16582ms step_avg:60.08ms
step:277/2330 train_time:16640ms step_avg:60.07ms
step:278/2330 train_time:16700ms step_avg:60.07ms
step:279/2330 train_time:16758ms step_avg:60.07ms
step:280/2330 train_time:16819ms step_avg:60.07ms
step:281/2330 train_time:16877ms step_avg:60.06ms
step:282/2330 train_time:16937ms step_avg:60.06ms
step:283/2330 train_time:16995ms step_avg:60.05ms
step:284/2330 train_time:17056ms step_avg:60.05ms
step:285/2330 train_time:17114ms step_avg:60.05ms
step:286/2330 train_time:17175ms step_avg:60.05ms
step:287/2330 train_time:17235ms step_avg:60.05ms
step:288/2330 train_time:17296ms step_avg:60.06ms
step:289/2330 train_time:17355ms step_avg:60.05ms
step:290/2330 train_time:17416ms step_avg:60.06ms
step:291/2330 train_time:17475ms step_avg:60.05ms
step:292/2330 train_time:17536ms step_avg:60.06ms
step:293/2330 train_time:17595ms step_avg:60.05ms
step:294/2330 train_time:17656ms step_avg:60.05ms
step:295/2330 train_time:17714ms step_avg:60.05ms
step:296/2330 train_time:17774ms step_avg:60.05ms
step:297/2330 train_time:17833ms step_avg:60.04ms
step:298/2330 train_time:17894ms step_avg:60.05ms
step:299/2330 train_time:17952ms step_avg:60.04ms
step:300/2330 train_time:18013ms step_avg:60.04ms
step:301/2330 train_time:18071ms step_avg:60.04ms
step:302/2330 train_time:18133ms step_avg:60.04ms
step:303/2330 train_time:18192ms step_avg:60.04ms
step:304/2330 train_time:18253ms step_avg:60.04ms
step:305/2330 train_time:18312ms step_avg:60.04ms
step:306/2330 train_time:18373ms step_avg:60.04ms
step:307/2330 train_time:18432ms step_avg:60.04ms
step:308/2330 train_time:18494ms step_avg:60.04ms
step:309/2330 train_time:18552ms step_avg:60.04ms
step:310/2330 train_time:18613ms step_avg:60.04ms
step:311/2330 train_time:18672ms step_avg:60.04ms
step:312/2330 train_time:18733ms step_avg:60.04ms
step:313/2330 train_time:18792ms step_avg:60.04ms
step:314/2330 train_time:18853ms step_avg:60.04ms
step:315/2330 train_time:18912ms step_avg:60.04ms
step:316/2330 train_time:18972ms step_avg:60.04ms
step:317/2330 train_time:19031ms step_avg:60.04ms
step:318/2330 train_time:19092ms step_avg:60.04ms
step:319/2330 train_time:19151ms step_avg:60.03ms
step:320/2330 train_time:19212ms step_avg:60.04ms
step:321/2330 train_time:19271ms step_avg:60.03ms
step:322/2330 train_time:19333ms step_avg:60.04ms
step:323/2330 train_time:19391ms step_avg:60.04ms
step:324/2330 train_time:19453ms step_avg:60.04ms
step:325/2330 train_time:19512ms step_avg:60.04ms
step:326/2330 train_time:19573ms step_avg:60.04ms
step:327/2330 train_time:19632ms step_avg:60.04ms
step:328/2330 train_time:19693ms step_avg:60.04ms
step:329/2330 train_time:19751ms step_avg:60.03ms
step:330/2330 train_time:19812ms step_avg:60.04ms
step:331/2330 train_time:19871ms step_avg:60.03ms
step:332/2330 train_time:19932ms step_avg:60.04ms
step:333/2330 train_time:19991ms step_avg:60.03ms
step:334/2330 train_time:20052ms step_avg:60.04ms
step:335/2330 train_time:20110ms step_avg:60.03ms
step:336/2330 train_time:20171ms step_avg:60.03ms
step:337/2330 train_time:20231ms step_avg:60.03ms
step:338/2330 train_time:20292ms step_avg:60.04ms
step:339/2330 train_time:20351ms step_avg:60.03ms
step:340/2330 train_time:20412ms step_avg:60.04ms
step:341/2330 train_time:20471ms step_avg:60.03ms
step:342/2330 train_time:20532ms step_avg:60.04ms
step:343/2330 train_time:20592ms step_avg:60.03ms
step:344/2330 train_time:20653ms step_avg:60.04ms
step:345/2330 train_time:20711ms step_avg:60.03ms
step:346/2330 train_time:20772ms step_avg:60.04ms
step:347/2330 train_time:20831ms step_avg:60.03ms
step:348/2330 train_time:20892ms step_avg:60.03ms
step:349/2330 train_time:20951ms step_avg:60.03ms
step:350/2330 train_time:21013ms step_avg:60.04ms
step:351/2330 train_time:21072ms step_avg:60.03ms
step:352/2330 train_time:21133ms step_avg:60.04ms
step:353/2330 train_time:21192ms step_avg:60.03ms
step:354/2330 train_time:21253ms step_avg:60.04ms
step:355/2330 train_time:21312ms step_avg:60.03ms
step:356/2330 train_time:21373ms step_avg:60.04ms
step:357/2330 train_time:21432ms step_avg:60.03ms
step:358/2330 train_time:21493ms step_avg:60.04ms
step:359/2330 train_time:21552ms step_avg:60.03ms
step:360/2330 train_time:21613ms step_avg:60.04ms
step:361/2330 train_time:21672ms step_avg:60.03ms
step:362/2330 train_time:21733ms step_avg:60.04ms
step:363/2330 train_time:21791ms step_avg:60.03ms
step:364/2330 train_time:21853ms step_avg:60.03ms
step:365/2330 train_time:21912ms step_avg:60.03ms
step:366/2330 train_time:21972ms step_avg:60.03ms
step:367/2330 train_time:22031ms step_avg:60.03ms
step:368/2330 train_time:22092ms step_avg:60.03ms
step:369/2330 train_time:22151ms step_avg:60.03ms
step:370/2330 train_time:22213ms step_avg:60.03ms
step:371/2330 train_time:22271ms step_avg:60.03ms
step:372/2330 train_time:22332ms step_avg:60.03ms
step:373/2330 train_time:22391ms step_avg:60.03ms
step:374/2330 train_time:22452ms step_avg:60.03ms
step:375/2330 train_time:22511ms step_avg:60.03ms
step:376/2330 train_time:22572ms step_avg:60.03ms
step:377/2330 train_time:22631ms step_avg:60.03ms
step:378/2330 train_time:22692ms step_avg:60.03ms
step:379/2330 train_time:22751ms step_avg:60.03ms
step:380/2330 train_time:22812ms step_avg:60.03ms
step:381/2330 train_time:22871ms step_avg:60.03ms
step:382/2330 train_time:22933ms step_avg:60.03ms
step:383/2330 train_time:22991ms step_avg:60.03ms
step:384/2330 train_time:23052ms step_avg:60.03ms
step:385/2330 train_time:23111ms step_avg:60.03ms
step:386/2330 train_time:23172ms step_avg:60.03ms
step:387/2330 train_time:23232ms step_avg:60.03ms
step:388/2330 train_time:23292ms step_avg:60.03ms
step:389/2330 train_time:23351ms step_avg:60.03ms
step:390/2330 train_time:23412ms step_avg:60.03ms
step:391/2330 train_time:23472ms step_avg:60.03ms
step:392/2330 train_time:23533ms step_avg:60.03ms
step:393/2330 train_time:23592ms step_avg:60.03ms
step:394/2330 train_time:23652ms step_avg:60.03ms
step:395/2330 train_time:23711ms step_avg:60.03ms
step:396/2330 train_time:23772ms step_avg:60.03ms
step:397/2330 train_time:23832ms step_avg:60.03ms
step:398/2330 train_time:23893ms step_avg:60.03ms
step:399/2330 train_time:23952ms step_avg:60.03ms
step:400/2330 train_time:24012ms step_avg:60.03ms
step:401/2330 train_time:24071ms step_avg:60.03ms
step:402/2330 train_time:24133ms step_avg:60.03ms
step:403/2330 train_time:24192ms step_avg:60.03ms
step:404/2330 train_time:24253ms step_avg:60.03ms
step:405/2330 train_time:24312ms step_avg:60.03ms
step:406/2330 train_time:24373ms step_avg:60.03ms
step:407/2330 train_time:24432ms step_avg:60.03ms
step:408/2330 train_time:24493ms step_avg:60.03ms
step:409/2330 train_time:24552ms step_avg:60.03ms
step:410/2330 train_time:24613ms step_avg:60.03ms
step:411/2330 train_time:24672ms step_avg:60.03ms
step:412/2330 train_time:24733ms step_avg:60.03ms
step:413/2330 train_time:24792ms step_avg:60.03ms
step:414/2330 train_time:24853ms step_avg:60.03ms
step:415/2330 train_time:24911ms step_avg:60.03ms
step:416/2330 train_time:24972ms step_avg:60.03ms
step:417/2330 train_time:25031ms step_avg:60.03ms
step:418/2330 train_time:25092ms step_avg:60.03ms
step:419/2330 train_time:25151ms step_avg:60.03ms
step:420/2330 train_time:25212ms step_avg:60.03ms
step:421/2330 train_time:25271ms step_avg:60.03ms
step:422/2330 train_time:25332ms step_avg:60.03ms
step:423/2330 train_time:25391ms step_avg:60.03ms
step:424/2330 train_time:25452ms step_avg:60.03ms
step:425/2330 train_time:25511ms step_avg:60.03ms
step:426/2330 train_time:25572ms step_avg:60.03ms
step:427/2330 train_time:25631ms step_avg:60.03ms
step:428/2330 train_time:25692ms step_avg:60.03ms
step:429/2330 train_time:25751ms step_avg:60.03ms
step:430/2330 train_time:25812ms step_avg:60.03ms
step:431/2330 train_time:25871ms step_avg:60.03ms
step:432/2330 train_time:25933ms step_avg:60.03ms
step:433/2330 train_time:25992ms step_avg:60.03ms
step:434/2330 train_time:26053ms step_avg:60.03ms
step:435/2330 train_time:26112ms step_avg:60.03ms
step:436/2330 train_time:26173ms step_avg:60.03ms
step:437/2330 train_time:26231ms step_avg:60.03ms
step:438/2330 train_time:26292ms step_avg:60.03ms
step:439/2330 train_time:26351ms step_avg:60.02ms
step:440/2330 train_time:26412ms step_avg:60.03ms
step:441/2330 train_time:26471ms step_avg:60.03ms
step:442/2330 train_time:26533ms step_avg:60.03ms
step:443/2330 train_time:26592ms step_avg:60.03ms
step:444/2330 train_time:26653ms step_avg:60.03ms
step:445/2330 train_time:26712ms step_avg:60.03ms
step:446/2330 train_time:26773ms step_avg:60.03ms
step:447/2330 train_time:26832ms step_avg:60.03ms
step:448/2330 train_time:26893ms step_avg:60.03ms
step:449/2330 train_time:26952ms step_avg:60.03ms
step:450/2330 train_time:27013ms step_avg:60.03ms
step:451/2330 train_time:27071ms step_avg:60.03ms
step:452/2330 train_time:27133ms step_avg:60.03ms
step:453/2330 train_time:27192ms step_avg:60.03ms
step:454/2330 train_time:27252ms step_avg:60.03ms
step:455/2330 train_time:27311ms step_avg:60.02ms
step:456/2330 train_time:27372ms step_avg:60.03ms
step:457/2330 train_time:27432ms step_avg:60.03ms
step:458/2330 train_time:27493ms step_avg:60.03ms
step:459/2330 train_time:27551ms step_avg:60.03ms
step:460/2330 train_time:27613ms step_avg:60.03ms
step:461/2330 train_time:27672ms step_avg:60.03ms
step:462/2330 train_time:27733ms step_avg:60.03ms
step:463/2330 train_time:27792ms step_avg:60.03ms
step:464/2330 train_time:27853ms step_avg:60.03ms
step:465/2330 train_time:27912ms step_avg:60.03ms
step:466/2330 train_time:27973ms step_avg:60.03ms
step:467/2330 train_time:28033ms step_avg:60.03ms
step:468/2330 train_time:28093ms step_avg:60.03ms
step:469/2330 train_time:28152ms step_avg:60.03ms
step:470/2330 train_time:28213ms step_avg:60.03ms
step:471/2330 train_time:28272ms step_avg:60.02ms
step:472/2330 train_time:28333ms step_avg:60.03ms
step:473/2330 train_time:28392ms step_avg:60.02ms
step:474/2330 train_time:28453ms step_avg:60.03ms
step:475/2330 train_time:28512ms step_avg:60.03ms
step:476/2330 train_time:28573ms step_avg:60.03ms
step:477/2330 train_time:28632ms step_avg:60.03ms
step:478/2330 train_time:28693ms step_avg:60.03ms
step:479/2330 train_time:28752ms step_avg:60.02ms
step:480/2330 train_time:28813ms step_avg:60.03ms
step:481/2330 train_time:28872ms step_avg:60.02ms
step:482/2330 train_time:28933ms step_avg:60.03ms
step:483/2330 train_time:28992ms step_avg:60.02ms
step:484/2330 train_time:29053ms step_avg:60.03ms
step:485/2330 train_time:29112ms step_avg:60.02ms
step:486/2330 train_time:29173ms step_avg:60.03ms
step:487/2330 train_time:29232ms step_avg:60.02ms
step:488/2330 train_time:29293ms step_avg:60.03ms
step:489/2330 train_time:29352ms step_avg:60.02ms
step:490/2330 train_time:29412ms step_avg:60.03ms
step:491/2330 train_time:29471ms step_avg:60.02ms
step:492/2330 train_time:29534ms step_avg:60.03ms
step:493/2330 train_time:29592ms step_avg:60.02ms
step:494/2330 train_time:29653ms step_avg:60.03ms
step:495/2330 train_time:29712ms step_avg:60.02ms
step:496/2330 train_time:29773ms step_avg:60.03ms
step:497/2330 train_time:29832ms step_avg:60.02ms
step:498/2330 train_time:29893ms step_avg:60.03ms
step:499/2330 train_time:29951ms step_avg:60.02ms
step:500/2330 train_time:30013ms step_avg:60.03ms
step:500/2330 val_loss:3.8294 train_time:30076ms step_avg:60.15ms
step:501/2330 train_time:30098ms step_avg:60.08ms
step:502/2330 train_time:30135ms step_avg:60.03ms
step:503/2330 train_time:30198ms step_avg:60.04ms
step:504/2330 train_time:30265ms step_avg:60.05ms
step:505/2330 train_time:30325ms step_avg:60.05ms
step:506/2330 train_time:30385ms step_avg:60.05ms
step:507/2330 train_time:30443ms step_avg:60.05ms
step:508/2330 train_time:30504ms step_avg:60.05ms
step:509/2330 train_time:30562ms step_avg:60.04ms
step:510/2330 train_time:30623ms step_avg:60.04ms
step:511/2330 train_time:30680ms step_avg:60.04ms
step:512/2330 train_time:30741ms step_avg:60.04ms
step:513/2330 train_time:30799ms step_avg:60.04ms
step:514/2330 train_time:30859ms step_avg:60.04ms
step:515/2330 train_time:30917ms step_avg:60.03ms
step:516/2330 train_time:30978ms step_avg:60.03ms
step:517/2330 train_time:31038ms step_avg:60.04ms
step:518/2330 train_time:31100ms step_avg:60.04ms
step:519/2330 train_time:31160ms step_avg:60.04ms
step:520/2330 train_time:31222ms step_avg:60.04ms
step:521/2330 train_time:31281ms step_avg:60.04ms
step:522/2330 train_time:31344ms step_avg:60.05ms
step:523/2330 train_time:31402ms step_avg:60.04ms
step:524/2330 train_time:31463ms step_avg:60.04ms
step:525/2330 train_time:31521ms step_avg:60.04ms
step:526/2330 train_time:31582ms step_avg:60.04ms
step:527/2330 train_time:31640ms step_avg:60.04ms
step:528/2330 train_time:31700ms step_avg:60.04ms
step:529/2330 train_time:31758ms step_avg:60.03ms
step:530/2330 train_time:31818ms step_avg:60.03ms
step:531/2330 train_time:31876ms step_avg:60.03ms
step:532/2330 train_time:31937ms step_avg:60.03ms
step:533/2330 train_time:31996ms step_avg:60.03ms
step:534/2330 train_time:32058ms step_avg:60.03ms
step:535/2330 train_time:32117ms step_avg:60.03ms
step:536/2330 train_time:32178ms step_avg:60.03ms
step:537/2330 train_time:32238ms step_avg:60.03ms
step:538/2330 train_time:32299ms step_avg:60.04ms
step:539/2330 train_time:32359ms step_avg:60.04ms
step:540/2330 train_time:32420ms step_avg:60.04ms
step:541/2330 train_time:32479ms step_avg:60.04ms
step:542/2330 train_time:32540ms step_avg:60.04ms
step:543/2330 train_time:32599ms step_avg:60.04ms
step:544/2330 train_time:32660ms step_avg:60.04ms
step:545/2330 train_time:32718ms step_avg:60.03ms
step:546/2330 train_time:32779ms step_avg:60.03ms
step:547/2330 train_time:32837ms step_avg:60.03ms
step:548/2330 train_time:32897ms step_avg:60.03ms
step:549/2330 train_time:32956ms step_avg:60.03ms
step:550/2330 train_time:33017ms step_avg:60.03ms
step:551/2330 train_time:33076ms step_avg:60.03ms
step:552/2330 train_time:33138ms step_avg:60.03ms
step:553/2330 train_time:33197ms step_avg:60.03ms
step:554/2330 train_time:33259ms step_avg:60.03ms
step:555/2330 train_time:33318ms step_avg:60.03ms
step:556/2330 train_time:33379ms step_avg:60.03ms
step:557/2330 train_time:33438ms step_avg:60.03ms
step:558/2330 train_time:33499ms step_avg:60.03ms
step:559/2330 train_time:33559ms step_avg:60.03ms
step:560/2330 train_time:33619ms step_avg:60.03ms
step:561/2330 train_time:33678ms step_avg:60.03ms
step:562/2330 train_time:33739ms step_avg:60.03ms
step:563/2330 train_time:33797ms step_avg:60.03ms
step:564/2330 train_time:33858ms step_avg:60.03ms
step:565/2330 train_time:33916ms step_avg:60.03ms
step:566/2330 train_time:33977ms step_avg:60.03ms
step:567/2330 train_time:34036ms step_avg:60.03ms
step:568/2330 train_time:34097ms step_avg:60.03ms
step:569/2330 train_time:34157ms step_avg:60.03ms
step:570/2330 train_time:34218ms step_avg:60.03ms
step:571/2330 train_time:34278ms step_avg:60.03ms
step:572/2330 train_time:34339ms step_avg:60.03ms
step:573/2330 train_time:34398ms step_avg:60.03ms
step:574/2330 train_time:34459ms step_avg:60.03ms
step:575/2330 train_time:34519ms step_avg:60.03ms
step:576/2330 train_time:34580ms step_avg:60.03ms
step:577/2330 train_time:34638ms step_avg:60.03ms
step:578/2330 train_time:34699ms step_avg:60.03ms
step:579/2330 train_time:34758ms step_avg:60.03ms
step:580/2330 train_time:34819ms step_avg:60.03ms
step:581/2330 train_time:34877ms step_avg:60.03ms
step:582/2330 train_time:34937ms step_avg:60.03ms
step:583/2330 train_time:34996ms step_avg:60.03ms
step:584/2330 train_time:35057ms step_avg:60.03ms
step:585/2330 train_time:35116ms step_avg:60.03ms
step:586/2330 train_time:35177ms step_avg:60.03ms
step:587/2330 train_time:35236ms step_avg:60.03ms
step:588/2330 train_time:35297ms step_avg:60.03ms
step:589/2330 train_time:35357ms step_avg:60.03ms
step:590/2330 train_time:35418ms step_avg:60.03ms
step:591/2330 train_time:35477ms step_avg:60.03ms
step:592/2330 train_time:35538ms step_avg:60.03ms
step:593/2330 train_time:35597ms step_avg:60.03ms
step:594/2330 train_time:35659ms step_avg:60.03ms
step:595/2330 train_time:35718ms step_avg:60.03ms
step:596/2330 train_time:35779ms step_avg:60.03ms
step:597/2330 train_time:35837ms step_avg:60.03ms
step:598/2330 train_time:35898ms step_avg:60.03ms
step:599/2330 train_time:35957ms step_avg:60.03ms
step:600/2330 train_time:36017ms step_avg:60.03ms
step:601/2330 train_time:36076ms step_avg:60.03ms
step:602/2330 train_time:36138ms step_avg:60.03ms
step:603/2330 train_time:36196ms step_avg:60.03ms
step:604/2330 train_time:36259ms step_avg:60.03ms
step:605/2330 train_time:36318ms step_avg:60.03ms
step:606/2330 train_time:36379ms step_avg:60.03ms
step:607/2330 train_time:36438ms step_avg:60.03ms
step:608/2330 train_time:36499ms step_avg:60.03ms
step:609/2330 train_time:36558ms step_avg:60.03ms
step:610/2330 train_time:36619ms step_avg:60.03ms
step:611/2330 train_time:36678ms step_avg:60.03ms
step:612/2330 train_time:36739ms step_avg:60.03ms
step:613/2330 train_time:36798ms step_avg:60.03ms
step:614/2330 train_time:36859ms step_avg:60.03ms
step:615/2330 train_time:36917ms step_avg:60.03ms
step:616/2330 train_time:36978ms step_avg:60.03ms
step:617/2330 train_time:37037ms step_avg:60.03ms
step:618/2330 train_time:37098ms step_avg:60.03ms
step:619/2330 train_time:37158ms step_avg:60.03ms
step:620/2330 train_time:37219ms step_avg:60.03ms
step:621/2330 train_time:37278ms step_avg:60.03ms
step:622/2330 train_time:37339ms step_avg:60.03ms
step:623/2330 train_time:37398ms step_avg:60.03ms
step:624/2330 train_time:37459ms step_avg:60.03ms
step:625/2330 train_time:37518ms step_avg:60.03ms
step:626/2330 train_time:37579ms step_avg:60.03ms
step:627/2330 train_time:37638ms step_avg:60.03ms
step:628/2330 train_time:37699ms step_avg:60.03ms
step:629/2330 train_time:37758ms step_avg:60.03ms
step:630/2330 train_time:37819ms step_avg:60.03ms
step:631/2330 train_time:37878ms step_avg:60.03ms
step:632/2330 train_time:37939ms step_avg:60.03ms
step:633/2330 train_time:37997ms step_avg:60.03ms
step:634/2330 train_time:38059ms step_avg:60.03ms
step:635/2330 train_time:38118ms step_avg:60.03ms
step:636/2330 train_time:38178ms step_avg:60.03ms
step:637/2330 train_time:38237ms step_avg:60.03ms
step:638/2330 train_time:38298ms step_avg:60.03ms
step:639/2330 train_time:38357ms step_avg:60.03ms
step:640/2330 train_time:38418ms step_avg:60.03ms
step:641/2330 train_time:38478ms step_avg:60.03ms
step:642/2330 train_time:38539ms step_avg:60.03ms
step:643/2330 train_time:38598ms step_avg:60.03ms
step:644/2330 train_time:38659ms step_avg:60.03ms
step:645/2330 train_time:38718ms step_avg:60.03ms
step:646/2330 train_time:38778ms step_avg:60.03ms
step:647/2330 train_time:38837ms step_avg:60.03ms
step:648/2330 train_time:38898ms step_avg:60.03ms
step:649/2330 train_time:38957ms step_avg:60.03ms
step:650/2330 train_time:39019ms step_avg:60.03ms
step:651/2330 train_time:39078ms step_avg:60.03ms
step:652/2330 train_time:39139ms step_avg:60.03ms
step:653/2330 train_time:39198ms step_avg:60.03ms
step:654/2330 train_time:39259ms step_avg:60.03ms
step:655/2330 train_time:39317ms step_avg:60.03ms
step:656/2330 train_time:39378ms step_avg:60.03ms
step:657/2330 train_time:39437ms step_avg:60.03ms
step:658/2330 train_time:39498ms step_avg:60.03ms
step:659/2330 train_time:39558ms step_avg:60.03ms
step:660/2330 train_time:39619ms step_avg:60.03ms
step:661/2330 train_time:39678ms step_avg:60.03ms
step:662/2330 train_time:39739ms step_avg:60.03ms
step:663/2330 train_time:39798ms step_avg:60.03ms
step:664/2330 train_time:39859ms step_avg:60.03ms
step:665/2330 train_time:39918ms step_avg:60.03ms
step:666/2330 train_time:39978ms step_avg:60.03ms
step:667/2330 train_time:40038ms step_avg:60.03ms
step:668/2330 train_time:40099ms step_avg:60.03ms
step:669/2330 train_time:40158ms step_avg:60.03ms
step:670/2330 train_time:40219ms step_avg:60.03ms
step:671/2330 train_time:40277ms step_avg:60.03ms
step:672/2330 train_time:40338ms step_avg:60.03ms
step:673/2330 train_time:40398ms step_avg:60.03ms
step:674/2330 train_time:40458ms step_avg:60.03ms
step:675/2330 train_time:40518ms step_avg:60.03ms
step:676/2330 train_time:40579ms step_avg:60.03ms
step:677/2330 train_time:40638ms step_avg:60.03ms
step:678/2330 train_time:40699ms step_avg:60.03ms
step:679/2330 train_time:40758ms step_avg:60.03ms
step:680/2330 train_time:40819ms step_avg:60.03ms
step:681/2330 train_time:40878ms step_avg:60.03ms
step:682/2330 train_time:40939ms step_avg:60.03ms
step:683/2330 train_time:40998ms step_avg:60.03ms
step:684/2330 train_time:41059ms step_avg:60.03ms
step:685/2330 train_time:41118ms step_avg:60.03ms
step:686/2330 train_time:41179ms step_avg:60.03ms
step:687/2330 train_time:41238ms step_avg:60.03ms
step:688/2330 train_time:41299ms step_avg:60.03ms
step:689/2330 train_time:41358ms step_avg:60.03ms
step:690/2330 train_time:41419ms step_avg:60.03ms
step:691/2330 train_time:41478ms step_avg:60.03ms
step:692/2330 train_time:41539ms step_avg:60.03ms
step:693/2330 train_time:41598ms step_avg:60.03ms
step:694/2330 train_time:41660ms step_avg:60.03ms
step:695/2330 train_time:41718ms step_avg:60.03ms
step:696/2330 train_time:41779ms step_avg:60.03ms
step:697/2330 train_time:41838ms step_avg:60.03ms
step:698/2330 train_time:41899ms step_avg:60.03ms
step:699/2330 train_time:41958ms step_avg:60.03ms
step:700/2330 train_time:42019ms step_avg:60.03ms
step:701/2330 train_time:42078ms step_avg:60.03ms
step:702/2330 train_time:42139ms step_avg:60.03ms
step:703/2330 train_time:42198ms step_avg:60.03ms
step:704/2330 train_time:42259ms step_avg:60.03ms
step:705/2330 train_time:42318ms step_avg:60.02ms
step:706/2330 train_time:42379ms step_avg:60.03ms
step:707/2330 train_time:42437ms step_avg:60.02ms
step:708/2330 train_time:42498ms step_avg:60.03ms
step:709/2330 train_time:42557ms step_avg:60.02ms
step:710/2330 train_time:42617ms step_avg:60.02ms
step:711/2330 train_time:42676ms step_avg:60.02ms
step:712/2330 train_time:42738ms step_avg:60.02ms
step:713/2330 train_time:42797ms step_avg:60.02ms
step:714/2330 train_time:42859ms step_avg:60.03ms
step:715/2330 train_time:42918ms step_avg:60.02ms
step:716/2330 train_time:42979ms step_avg:60.03ms
step:717/2330 train_time:43038ms step_avg:60.02ms
step:718/2330 train_time:43098ms step_avg:60.03ms
step:719/2330 train_time:43157ms step_avg:60.02ms
step:720/2330 train_time:43218ms step_avg:60.03ms
step:721/2330 train_time:43277ms step_avg:60.02ms
step:722/2330 train_time:43339ms step_avg:60.03ms
step:723/2330 train_time:43398ms step_avg:60.03ms
step:724/2330 train_time:43459ms step_avg:60.03ms
step:725/2330 train_time:43518ms step_avg:60.03ms
step:726/2330 train_time:43580ms step_avg:60.03ms
step:727/2330 train_time:43639ms step_avg:60.03ms
step:728/2330 train_time:43700ms step_avg:60.03ms
step:729/2330 train_time:43760ms step_avg:60.03ms
step:730/2330 train_time:43821ms step_avg:60.03ms
step:731/2330 train_time:43879ms step_avg:60.03ms
step:732/2330 train_time:43940ms step_avg:60.03ms
step:733/2330 train_time:43998ms step_avg:60.02ms
step:734/2330 train_time:44059ms step_avg:60.03ms
step:735/2330 train_time:44118ms step_avg:60.02ms
step:736/2330 train_time:44179ms step_avg:60.03ms
step:737/2330 train_time:44238ms step_avg:60.02ms
step:738/2330 train_time:44299ms step_avg:60.03ms
step:739/2330 train_time:44358ms step_avg:60.02ms
step:740/2330 train_time:44419ms step_avg:60.03ms
step:741/2330 train_time:44478ms step_avg:60.02ms
step:742/2330 train_time:44540ms step_avg:60.03ms
step:743/2330 train_time:44599ms step_avg:60.03ms
step:744/2330 train_time:44660ms step_avg:60.03ms
step:745/2330 train_time:44719ms step_avg:60.03ms
step:746/2330 train_time:44780ms step_avg:60.03ms
step:747/2330 train_time:44839ms step_avg:60.02ms
step:748/2330 train_time:44900ms step_avg:60.03ms
step:749/2330 train_time:44959ms step_avg:60.03ms
step:750/2330 train_time:45020ms step_avg:60.03ms
step:750/2330 val_loss:3.6997 train_time:45082ms step_avg:60.11ms
step:751/2330 train_time:45105ms step_avg:60.06ms
step:752/2330 train_time:45142ms step_avg:60.03ms
step:753/2330 train_time:45204ms step_avg:60.03ms
step:754/2330 train_time:45266ms step_avg:60.03ms
step:755/2330 train_time:45325ms step_avg:60.03ms
step:756/2330 train_time:45385ms step_avg:60.03ms
step:757/2330 train_time:45444ms step_avg:60.03ms
step:758/2330 train_time:45504ms step_avg:60.03ms
step:759/2330 train_time:45562ms step_avg:60.03ms
step:760/2330 train_time:45623ms step_avg:60.03ms
step:761/2330 train_time:45681ms step_avg:60.03ms
step:762/2330 train_time:45741ms step_avg:60.03ms
step:763/2330 train_time:45801ms step_avg:60.03ms
step:764/2330 train_time:45862ms step_avg:60.03ms
step:765/2330 train_time:45921ms step_avg:60.03ms
step:766/2330 train_time:45983ms step_avg:60.03ms
step:767/2330 train_time:46044ms step_avg:60.03ms
step:768/2330 train_time:46107ms step_avg:60.04ms
step:769/2330 train_time:46168ms step_avg:60.04ms
step:770/2330 train_time:46230ms step_avg:60.04ms
step:771/2330 train_time:46290ms step_avg:60.04ms
step:772/2330 train_time:46353ms step_avg:60.04ms
step:773/2330 train_time:46412ms step_avg:60.04ms
step:774/2330 train_time:46474ms step_avg:60.04ms
step:775/2330 train_time:46533ms step_avg:60.04ms
step:776/2330 train_time:46596ms step_avg:60.05ms
step:777/2330 train_time:46656ms step_avg:60.05ms
step:778/2330 train_time:46717ms step_avg:60.05ms
step:779/2330 train_time:46776ms step_avg:60.05ms
step:780/2330 train_time:46838ms step_avg:60.05ms
step:781/2330 train_time:46897ms step_avg:60.05ms
step:782/2330 train_time:46959ms step_avg:60.05ms
step:783/2330 train_time:47020ms step_avg:60.05ms
step:784/2330 train_time:47082ms step_avg:60.05ms
step:785/2330 train_time:47143ms step_avg:60.05ms
step:786/2330 train_time:47206ms step_avg:60.06ms
step:787/2330 train_time:47266ms step_avg:60.06ms
step:788/2330 train_time:47327ms step_avg:60.06ms
step:789/2330 train_time:47387ms step_avg:60.06ms
step:790/2330 train_time:47448ms step_avg:60.06ms
step:791/2330 train_time:47508ms step_avg:60.06ms
step:792/2330 train_time:47569ms step_avg:60.06ms
step:793/2330 train_time:47628ms step_avg:60.06ms
step:794/2330 train_time:47689ms step_avg:60.06ms
step:795/2330 train_time:47748ms step_avg:60.06ms
step:796/2330 train_time:47810ms step_avg:60.06ms
step:797/2330 train_time:47870ms step_avg:60.06ms
step:798/2330 train_time:47932ms step_avg:60.06ms
step:799/2330 train_time:47992ms step_avg:60.07ms
step:800/2330 train_time:48055ms step_avg:60.07ms
step:801/2330 train_time:48115ms step_avg:60.07ms
step:802/2330 train_time:48178ms step_avg:60.07ms
step:803/2330 train_time:48237ms step_avg:60.07ms
step:804/2330 train_time:48299ms step_avg:60.07ms
step:805/2330 train_time:48360ms step_avg:60.07ms
step:806/2330 train_time:48422ms step_avg:60.08ms
step:807/2330 train_time:48482ms step_avg:60.08ms
step:808/2330 train_time:48544ms step_avg:60.08ms
step:809/2330 train_time:48604ms step_avg:60.08ms
step:810/2330 train_time:48665ms step_avg:60.08ms
step:811/2330 train_time:48725ms step_avg:60.08ms
step:812/2330 train_time:48786ms step_avg:60.08ms
step:813/2330 train_time:48845ms step_avg:60.08ms
step:814/2330 train_time:48907ms step_avg:60.08ms
step:815/2330 train_time:48967ms step_avg:60.08ms
step:816/2330 train_time:49029ms step_avg:60.08ms
step:817/2330 train_time:49089ms step_avg:60.08ms
step:818/2330 train_time:49151ms step_avg:60.09ms
step:819/2330 train_time:49211ms step_avg:60.09ms
step:820/2330 train_time:49273ms step_avg:60.09ms
step:821/2330 train_time:49334ms step_avg:60.09ms
step:822/2330 train_time:49397ms step_avg:60.09ms
step:823/2330 train_time:49456ms step_avg:60.09ms
step:824/2330 train_time:49519ms step_avg:60.10ms
step:825/2330 train_time:49578ms step_avg:60.10ms
step:826/2330 train_time:49640ms step_avg:60.10ms
step:827/2330 train_time:49700ms step_avg:60.10ms
step:828/2330 train_time:49762ms step_avg:60.10ms
step:829/2330 train_time:49822ms step_avg:60.10ms
step:830/2330 train_time:49884ms step_avg:60.10ms
step:831/2330 train_time:49944ms step_avg:60.10ms
step:832/2330 train_time:50006ms step_avg:60.10ms
step:833/2330 train_time:50065ms step_avg:60.10ms
step:834/2330 train_time:50127ms step_avg:60.10ms
step:835/2330 train_time:50187ms step_avg:60.10ms
step:836/2330 train_time:50249ms step_avg:60.11ms
step:837/2330 train_time:50309ms step_avg:60.11ms
step:838/2330 train_time:50371ms step_avg:60.11ms
step:839/2330 train_time:50430ms step_avg:60.11ms
step:840/2330 train_time:50493ms step_avg:60.11ms
step:841/2330 train_time:50553ms step_avg:60.11ms
step:842/2330 train_time:50615ms step_avg:60.11ms
step:843/2330 train_time:50674ms step_avg:60.11ms
step:844/2330 train_time:50736ms step_avg:60.11ms
step:845/2330 train_time:50795ms step_avg:60.11ms
step:846/2330 train_time:50857ms step_avg:60.12ms
step:847/2330 train_time:50918ms step_avg:60.12ms
step:848/2330 train_time:50979ms step_avg:60.12ms
step:849/2330 train_time:51039ms step_avg:60.12ms
step:850/2330 train_time:51102ms step_avg:60.12ms
step:851/2330 train_time:51162ms step_avg:60.12ms
step:852/2330 train_time:51224ms step_avg:60.12ms
step:853/2330 train_time:51284ms step_avg:60.12ms
step:854/2330 train_time:51346ms step_avg:60.12ms
step:855/2330 train_time:51405ms step_avg:60.12ms
step:856/2330 train_time:51466ms step_avg:60.12ms
step:857/2330 train_time:51526ms step_avg:60.12ms
step:858/2330 train_time:51588ms step_avg:60.13ms
step:859/2330 train_time:51647ms step_avg:60.12ms
step:860/2330 train_time:51709ms step_avg:60.13ms
step:861/2330 train_time:51769ms step_avg:60.13ms
step:862/2330 train_time:51830ms step_avg:60.13ms
step:863/2330 train_time:51890ms step_avg:60.13ms
step:864/2330 train_time:51952ms step_avg:60.13ms
step:865/2330 train_time:52012ms step_avg:60.13ms
step:866/2330 train_time:52073ms step_avg:60.13ms
step:867/2330 train_time:52133ms step_avg:60.13ms
step:868/2330 train_time:52196ms step_avg:60.13ms
step:869/2330 train_time:52257ms step_avg:60.13ms
step:870/2330 train_time:52319ms step_avg:60.14ms
step:871/2330 train_time:52379ms step_avg:60.14ms
step:872/2330 train_time:52441ms step_avg:60.14ms
step:873/2330 train_time:52501ms step_avg:60.14ms
step:874/2330 train_time:52563ms step_avg:60.14ms
step:875/2330 train_time:52623ms step_avg:60.14ms
step:876/2330 train_time:52685ms step_avg:60.14ms
step:877/2330 train_time:52745ms step_avg:60.14ms
step:878/2330 train_time:52806ms step_avg:60.14ms
step:879/2330 train_time:52865ms step_avg:60.14ms
step:880/2330 train_time:52927ms step_avg:60.14ms
step:881/2330 train_time:52987ms step_avg:60.14ms
step:882/2330 train_time:53048ms step_avg:60.15ms
step:883/2330 train_time:53107ms step_avg:60.14ms
step:884/2330 train_time:53169ms step_avg:60.15ms
step:885/2330 train_time:53228ms step_avg:60.15ms
step:886/2330 train_time:53290ms step_avg:60.15ms
step:887/2330 train_time:53350ms step_avg:60.15ms
step:888/2330 train_time:53413ms step_avg:60.15ms
step:889/2330 train_time:53473ms step_avg:60.15ms
step:890/2330 train_time:53535ms step_avg:60.15ms
step:891/2330 train_time:53594ms step_avg:60.15ms
step:892/2330 train_time:53657ms step_avg:60.15ms
step:893/2330 train_time:53717ms step_avg:60.15ms
step:894/2330 train_time:53778ms step_avg:60.15ms
step:895/2330 train_time:53838ms step_avg:60.15ms
step:896/2330 train_time:53901ms step_avg:60.16ms
step:897/2330 train_time:53960ms step_avg:60.16ms
step:898/2330 train_time:54022ms step_avg:60.16ms
step:899/2330 train_time:54082ms step_avg:60.16ms
step:900/2330 train_time:54144ms step_avg:60.16ms
step:901/2330 train_time:54204ms step_avg:60.16ms
step:902/2330 train_time:54265ms step_avg:60.16ms
step:903/2330 train_time:54325ms step_avg:60.16ms
step:904/2330 train_time:54387ms step_avg:60.16ms
step:905/2330 train_time:54446ms step_avg:60.16ms
step:906/2330 train_time:54507ms step_avg:60.16ms
step:907/2330 train_time:54566ms step_avg:60.16ms
step:908/2330 train_time:54628ms step_avg:60.16ms
step:909/2330 train_time:54688ms step_avg:60.16ms
step:910/2330 train_time:54749ms step_avg:60.16ms
step:911/2330 train_time:54809ms step_avg:60.16ms
step:912/2330 train_time:54871ms step_avg:60.17ms
step:913/2330 train_time:54931ms step_avg:60.17ms
step:914/2330 train_time:54993ms step_avg:60.17ms
step:915/2330 train_time:55053ms step_avg:60.17ms
step:916/2330 train_time:55115ms step_avg:60.17ms
step:917/2330 train_time:55174ms step_avg:60.17ms
step:918/2330 train_time:55237ms step_avg:60.17ms
step:919/2330 train_time:55297ms step_avg:60.17ms
step:920/2330 train_time:55359ms step_avg:60.17ms
step:921/2330 train_time:55419ms step_avg:60.17ms
step:922/2330 train_time:55480ms step_avg:60.17ms
step:923/2330 train_time:55541ms step_avg:60.17ms
step:924/2330 train_time:55603ms step_avg:60.18ms
step:925/2330 train_time:55663ms step_avg:60.18ms
step:926/2330 train_time:55724ms step_avg:60.18ms
step:927/2330 train_time:55784ms step_avg:60.18ms
step:928/2330 train_time:55846ms step_avg:60.18ms
step:929/2330 train_time:55905ms step_avg:60.18ms
step:930/2330 train_time:55967ms step_avg:60.18ms
step:931/2330 train_time:56026ms step_avg:60.18ms
step:932/2330 train_time:56087ms step_avg:60.18ms
step:933/2330 train_time:56147ms step_avg:60.18ms
step:934/2330 train_time:56209ms step_avg:60.18ms
step:935/2330 train_time:56268ms step_avg:60.18ms
step:936/2330 train_time:56330ms step_avg:60.18ms
step:937/2330 train_time:56389ms step_avg:60.18ms
step:938/2330 train_time:56451ms step_avg:60.18ms
step:939/2330 train_time:56512ms step_avg:60.18ms
step:940/2330 train_time:56573ms step_avg:60.18ms
step:941/2330 train_time:56634ms step_avg:60.18ms
step:942/2330 train_time:56695ms step_avg:60.19ms
step:943/2330 train_time:56755ms step_avg:60.19ms
step:944/2330 train_time:56817ms step_avg:60.19ms
step:945/2330 train_time:56877ms step_avg:60.19ms
step:946/2330 train_time:56939ms step_avg:60.19ms
step:947/2330 train_time:56999ms step_avg:60.19ms
step:948/2330 train_time:57061ms step_avg:60.19ms
step:949/2330 train_time:57121ms step_avg:60.19ms
step:950/2330 train_time:57184ms step_avg:60.19ms
step:951/2330 train_time:57244ms step_avg:60.19ms
step:952/2330 train_time:57306ms step_avg:60.20ms
step:953/2330 train_time:57366ms step_avg:60.19ms
step:954/2330 train_time:57427ms step_avg:60.20ms
step:955/2330 train_time:57486ms step_avg:60.20ms
step:956/2330 train_time:57548ms step_avg:60.20ms
step:957/2330 train_time:57607ms step_avg:60.20ms
step:958/2330 train_time:57669ms step_avg:60.20ms
step:959/2330 train_time:57728ms step_avg:60.20ms
step:960/2330 train_time:57790ms step_avg:60.20ms
step:961/2330 train_time:57850ms step_avg:60.20ms
step:962/2330 train_time:57912ms step_avg:60.20ms
step:963/2330 train_time:57972ms step_avg:60.20ms
step:964/2330 train_time:58034ms step_avg:60.20ms
step:965/2330 train_time:58094ms step_avg:60.20ms
step:966/2330 train_time:58156ms step_avg:60.20ms
step:967/2330 train_time:58216ms step_avg:60.20ms
step:968/2330 train_time:58279ms step_avg:60.21ms
step:969/2330 train_time:58339ms step_avg:60.20ms
step:970/2330 train_time:58401ms step_avg:60.21ms
step:971/2330 train_time:58461ms step_avg:60.21ms
step:972/2330 train_time:58524ms step_avg:60.21ms
step:973/2330 train_time:58583ms step_avg:60.21ms
step:974/2330 train_time:58645ms step_avg:60.21ms
step:975/2330 train_time:58704ms step_avg:60.21ms
step:976/2330 train_time:58766ms step_avg:60.21ms
step:977/2330 train_time:58825ms step_avg:60.21ms
step:978/2330 train_time:58887ms step_avg:60.21ms
step:979/2330 train_time:58947ms step_avg:60.21ms
step:980/2330 train_time:59009ms step_avg:60.21ms
step:981/2330 train_time:59068ms step_avg:60.21ms
step:982/2330 train_time:59130ms step_avg:60.21ms
step:983/2330 train_time:59190ms step_avg:60.21ms
step:984/2330 train_time:59252ms step_avg:60.22ms
step:985/2330 train_time:59312ms step_avg:60.22ms
step:986/2330 train_time:59374ms step_avg:60.22ms
step:987/2330 train_time:59433ms step_avg:60.22ms
step:988/2330 train_time:59496ms step_avg:60.22ms
step:989/2330 train_time:59557ms step_avg:60.22ms
step:990/2330 train_time:59619ms step_avg:60.22ms
step:991/2330 train_time:59678ms step_avg:60.22ms
step:992/2330 train_time:59740ms step_avg:60.22ms
step:993/2330 train_time:59801ms step_avg:60.22ms
step:994/2330 train_time:59863ms step_avg:60.22ms
step:995/2330 train_time:59924ms step_avg:60.22ms
step:996/2330 train_time:59986ms step_avg:60.23ms
step:997/2330 train_time:60045ms step_avg:60.23ms
step:998/2330 train_time:60107ms step_avg:60.23ms
step:999/2330 train_time:60167ms step_avg:60.23ms
step:1000/2330 train_time:60228ms step_avg:60.23ms
step:1000/2330 val_loss:3.5833 train_time:60292ms step_avg:60.29ms
step:1001/2330 train_time:60316ms step_avg:60.26ms
step:1002/2330 train_time:60354ms step_avg:60.23ms
step:1003/2330 train_time:60421ms step_avg:60.24ms
step:1004/2330 train_time:60485ms step_avg:60.24ms
step:1005/2330 train_time:60545ms step_avg:60.24ms
step:1006/2330 train_time:60607ms step_avg:60.25ms
step:1007/2330 train_time:60666ms step_avg:60.24ms
step:1008/2330 train_time:60727ms step_avg:60.25ms
step:1009/2330 train_time:60786ms step_avg:60.24ms
step:1010/2330 train_time:60847ms step_avg:60.24ms
step:1011/2330 train_time:60906ms step_avg:60.24ms
step:1012/2330 train_time:60967ms step_avg:60.24ms
step:1013/2330 train_time:61025ms step_avg:60.24ms
step:1014/2330 train_time:61087ms step_avg:60.24ms
step:1015/2330 train_time:61145ms step_avg:60.24ms
step:1016/2330 train_time:61209ms step_avg:60.25ms
step:1017/2330 train_time:61272ms step_avg:60.25ms
step:1018/2330 train_time:61336ms step_avg:60.25ms
step:1019/2330 train_time:61397ms step_avg:60.25ms
step:1020/2330 train_time:61461ms step_avg:60.26ms
step:1021/2330 train_time:61522ms step_avg:60.26ms
step:1022/2330 train_time:61584ms step_avg:60.26ms
step:1023/2330 train_time:61644ms step_avg:60.26ms
step:1024/2330 train_time:61705ms step_avg:60.26ms
step:1025/2330 train_time:61765ms step_avg:60.26ms
step:1026/2330 train_time:61826ms step_avg:60.26ms
step:1027/2330 train_time:61886ms step_avg:60.26ms
step:1028/2330 train_time:61947ms step_avg:60.26ms
step:1029/2330 train_time:62006ms step_avg:60.26ms
step:1030/2330 train_time:62067ms step_avg:60.26ms
step:1031/2330 train_time:62126ms step_avg:60.26ms
step:1032/2330 train_time:62188ms step_avg:60.26ms
step:1033/2330 train_time:62249ms step_avg:60.26ms
step:1034/2330 train_time:62313ms step_avg:60.26ms
step:1035/2330 train_time:62373ms step_avg:60.26ms
step:1036/2330 train_time:62435ms step_avg:60.27ms
step:1037/2330 train_time:62496ms step_avg:60.27ms
step:1038/2330 train_time:62558ms step_avg:60.27ms
step:1039/2330 train_time:62619ms step_avg:60.27ms
step:1040/2330 train_time:62681ms step_avg:60.27ms
step:1041/2330 train_time:62741ms step_avg:60.27ms
step:1042/2330 train_time:62804ms step_avg:60.27ms
step:1043/2330 train_time:62863ms step_avg:60.27ms
step:1044/2330 train_time:62925ms step_avg:60.27ms
step:1045/2330 train_time:62984ms step_avg:60.27ms
step:1046/2330 train_time:63045ms step_avg:60.27ms
step:1047/2330 train_time:63104ms step_avg:60.27ms
step:1048/2330 train_time:63166ms step_avg:60.27ms
step:1049/2330 train_time:63227ms step_avg:60.27ms
step:1050/2330 train_time:63290ms step_avg:60.28ms
step:1051/2330 train_time:63350ms step_avg:60.28ms
step:1052/2330 train_time:63412ms step_avg:60.28ms
step:1053/2330 train_time:63472ms step_avg:60.28ms
step:1054/2330 train_time:63534ms step_avg:60.28ms
step:1055/2330 train_time:63594ms step_avg:60.28ms
step:1056/2330 train_time:63655ms step_avg:60.28ms
step:1057/2330 train_time:63715ms step_avg:60.28ms
step:1058/2330 train_time:63777ms step_avg:60.28ms
step:1059/2330 train_time:63837ms step_avg:60.28ms
step:1060/2330 train_time:63899ms step_avg:60.28ms
step:1061/2330 train_time:63959ms step_avg:60.28ms
step:1062/2330 train_time:64022ms step_avg:60.28ms
step:1063/2330 train_time:64081ms step_avg:60.28ms
step:1064/2330 train_time:64143ms step_avg:60.28ms
step:1065/2330 train_time:64204ms step_avg:60.29ms
step:1066/2330 train_time:64266ms step_avg:60.29ms
step:1067/2330 train_time:64326ms step_avg:60.29ms
step:1068/2330 train_time:64389ms step_avg:60.29ms
step:1069/2330 train_time:64450ms step_avg:60.29ms
step:1070/2330 train_time:64512ms step_avg:60.29ms
step:1071/2330 train_time:64571ms step_avg:60.29ms
step:1072/2330 train_time:64633ms step_avg:60.29ms
step:1073/2330 train_time:64693ms step_avg:60.29ms
step:1074/2330 train_time:64755ms step_avg:60.29ms
step:1075/2330 train_time:64815ms step_avg:60.29ms
step:1076/2330 train_time:64876ms step_avg:60.29ms
step:1077/2330 train_time:64936ms step_avg:60.29ms
step:1078/2330 train_time:64998ms step_avg:60.30ms
step:1079/2330 train_time:65058ms step_avg:60.29ms
step:1080/2330 train_time:65121ms step_avg:60.30ms
step:1081/2330 train_time:65181ms step_avg:60.30ms
step:1082/2330 train_time:65243ms step_avg:60.30ms
step:1083/2330 train_time:65304ms step_avg:60.30ms
step:1084/2330 train_time:65366ms step_avg:60.30ms
step:1085/2330 train_time:65427ms step_avg:60.30ms
step:1086/2330 train_time:65489ms step_avg:60.30ms
step:1087/2330 train_time:65549ms step_avg:60.30ms
step:1088/2330 train_time:65611ms step_avg:60.30ms
step:1089/2330 train_time:65671ms step_avg:60.30ms
step:1090/2330 train_time:65732ms step_avg:60.30ms
step:1091/2330 train_time:65791ms step_avg:60.30ms
step:1092/2330 train_time:65853ms step_avg:60.30ms
step:1093/2330 train_time:65912ms step_avg:60.30ms
step:1094/2330 train_time:65974ms step_avg:60.31ms
step:1095/2330 train_time:66033ms step_avg:60.30ms
step:1096/2330 train_time:66095ms step_avg:60.31ms
step:1097/2330 train_time:66156ms step_avg:60.31ms
step:1098/2330 train_time:66218ms step_avg:60.31ms
step:1099/2330 train_time:66278ms step_avg:60.31ms
step:1100/2330 train_time:66340ms step_avg:60.31ms
step:1101/2330 train_time:66401ms step_avg:60.31ms
step:1102/2330 train_time:66464ms step_avg:60.31ms
step:1103/2330 train_time:66524ms step_avg:60.31ms
step:1104/2330 train_time:66586ms step_avg:60.31ms
step:1105/2330 train_time:66647ms step_avg:60.31ms
step:1106/2330 train_time:66709ms step_avg:60.32ms
step:1107/2330 train_time:66769ms step_avg:60.31ms
step:1108/2330 train_time:66830ms step_avg:60.32ms
step:1109/2330 train_time:66889ms step_avg:60.32ms
step:1110/2330 train_time:66951ms step_avg:60.32ms
step:1111/2330 train_time:67011ms step_avg:60.32ms
step:1112/2330 train_time:67072ms step_avg:60.32ms
step:1113/2330 train_time:67132ms step_avg:60.32ms
step:1114/2330 train_time:67195ms step_avg:60.32ms
step:1115/2330 train_time:67255ms step_avg:60.32ms
step:1116/2330 train_time:67317ms step_avg:60.32ms
step:1117/2330 train_time:67377ms step_avg:60.32ms
step:1118/2330 train_time:67439ms step_avg:60.32ms
step:1119/2330 train_time:67500ms step_avg:60.32ms
step:1120/2330 train_time:67563ms step_avg:60.32ms
step:1121/2330 train_time:67623ms step_avg:60.32ms
step:1122/2330 train_time:67684ms step_avg:60.32ms
step:1123/2330 train_time:67744ms step_avg:60.32ms
step:1124/2330 train_time:67806ms step_avg:60.33ms
step:1125/2330 train_time:67866ms step_avg:60.33ms
step:1126/2330 train_time:67928ms step_avg:60.33ms
step:1127/2330 train_time:67988ms step_avg:60.33ms
step:1128/2330 train_time:68050ms step_avg:60.33ms
step:1129/2330 train_time:68110ms step_avg:60.33ms
step:1130/2330 train_time:68171ms step_avg:60.33ms
step:1131/2330 train_time:68231ms step_avg:60.33ms
step:1132/2330 train_time:68292ms step_avg:60.33ms
step:1133/2330 train_time:68352ms step_avg:60.33ms
step:1134/2330 train_time:68414ms step_avg:60.33ms
step:1135/2330 train_time:68473ms step_avg:60.33ms
step:1136/2330 train_time:68536ms step_avg:60.33ms
step:1137/2330 train_time:68596ms step_avg:60.33ms
step:1138/2330 train_time:68657ms step_avg:60.33ms
step:1139/2330 train_time:68717ms step_avg:60.33ms
step:1140/2330 train_time:68779ms step_avg:60.33ms
step:1141/2330 train_time:68840ms step_avg:60.33ms
step:1142/2330 train_time:68903ms step_avg:60.34ms
step:1143/2330 train_time:68964ms step_avg:60.34ms
step:1144/2330 train_time:69025ms step_avg:60.34ms
step:1145/2330 train_time:69085ms step_avg:60.34ms
step:1146/2330 train_time:69147ms step_avg:60.34ms
step:1147/2330 train_time:69207ms step_avg:60.34ms
step:1148/2330 train_time:69269ms step_avg:60.34ms
step:1149/2330 train_time:69329ms step_avg:60.34ms
step:1150/2330 train_time:69392ms step_avg:60.34ms
step:1151/2330 train_time:69451ms step_avg:60.34ms
step:1152/2330 train_time:69513ms step_avg:60.34ms
step:1153/2330 train_time:69572ms step_avg:60.34ms
step:1154/2330 train_time:69633ms step_avg:60.34ms
step:1155/2330 train_time:69693ms step_avg:60.34ms
step:1156/2330 train_time:69755ms step_avg:60.34ms
step:1157/2330 train_time:69815ms step_avg:60.34ms
step:1158/2330 train_time:69877ms step_avg:60.34ms
step:1159/2330 train_time:69937ms step_avg:60.34ms
step:1160/2330 train_time:70000ms step_avg:60.34ms
step:1161/2330 train_time:70060ms step_avg:60.34ms
step:1162/2330 train_time:70123ms step_avg:60.35ms
step:1163/2330 train_time:70183ms step_avg:60.35ms
step:1164/2330 train_time:70245ms step_avg:60.35ms
step:1165/2330 train_time:70305ms step_avg:60.35ms
step:1166/2330 train_time:70366ms step_avg:60.35ms
step:1167/2330 train_time:70426ms step_avg:60.35ms
step:1168/2330 train_time:70489ms step_avg:60.35ms
step:1169/2330 train_time:70549ms step_avg:60.35ms
step:1170/2330 train_time:70611ms step_avg:60.35ms
step:1171/2330 train_time:70670ms step_avg:60.35ms
step:1172/2330 train_time:70731ms step_avg:60.35ms
step:1173/2330 train_time:70791ms step_avg:60.35ms
step:1174/2330 train_time:70853ms step_avg:60.35ms
step:1175/2330 train_time:70912ms step_avg:60.35ms
step:1176/2330 train_time:70974ms step_avg:60.35ms
step:1177/2330 train_time:71034ms step_avg:60.35ms
step:1178/2330 train_time:71096ms step_avg:60.35ms
step:1179/2330 train_time:71156ms step_avg:60.35ms
step:1180/2330 train_time:71219ms step_avg:60.35ms
step:1181/2330 train_time:71279ms step_avg:60.35ms
step:1182/2330 train_time:71342ms step_avg:60.36ms
step:1183/2330 train_time:71403ms step_avg:60.36ms
step:1184/2330 train_time:71465ms step_avg:60.36ms
step:1185/2330 train_time:71525ms step_avg:60.36ms
step:1186/2330 train_time:71588ms step_avg:60.36ms
step:1187/2330 train_time:71648ms step_avg:60.36ms
step:1188/2330 train_time:71710ms step_avg:60.36ms
step:1189/2330 train_time:71769ms step_avg:60.36ms
step:1190/2330 train_time:71831ms step_avg:60.36ms
step:1191/2330 train_time:71890ms step_avg:60.36ms
step:1192/2330 train_time:71951ms step_avg:60.36ms
step:1193/2330 train_time:72011ms step_avg:60.36ms
step:1194/2330 train_time:72072ms step_avg:60.36ms
step:1195/2330 train_time:72132ms step_avg:60.36ms
step:1196/2330 train_time:72194ms step_avg:60.36ms
step:1197/2330 train_time:72254ms step_avg:60.36ms
step:1198/2330 train_time:72316ms step_avg:60.36ms
step:1199/2330 train_time:72376ms step_avg:60.36ms
step:1200/2330 train_time:72438ms step_avg:60.37ms
step:1201/2330 train_time:72500ms step_avg:60.37ms
step:1202/2330 train_time:72562ms step_avg:60.37ms
step:1203/2330 train_time:72622ms step_avg:60.37ms
step:1204/2330 train_time:72684ms step_avg:60.37ms
step:1205/2330 train_time:72744ms step_avg:60.37ms
step:1206/2330 train_time:72806ms step_avg:60.37ms
step:1207/2330 train_time:72866ms step_avg:60.37ms
step:1208/2330 train_time:72928ms step_avg:60.37ms
step:1209/2330 train_time:72988ms step_avg:60.37ms
step:1210/2330 train_time:73050ms step_avg:60.37ms
step:1211/2330 train_time:73110ms step_avg:60.37ms
step:1212/2330 train_time:73172ms step_avg:60.37ms
step:1213/2330 train_time:73232ms step_avg:60.37ms
step:1214/2330 train_time:73293ms step_avg:60.37ms
step:1215/2330 train_time:73353ms step_avg:60.37ms
step:1216/2330 train_time:73415ms step_avg:60.37ms
step:1217/2330 train_time:73474ms step_avg:60.37ms
step:1218/2330 train_time:73536ms step_avg:60.37ms
step:1219/2330 train_time:73597ms step_avg:60.37ms
step:1220/2330 train_time:73659ms step_avg:60.38ms
step:1221/2330 train_time:73720ms step_avg:60.38ms
step:1222/2330 train_time:73781ms step_avg:60.38ms
step:1223/2330 train_time:73842ms step_avg:60.38ms
step:1224/2330 train_time:73904ms step_avg:60.38ms
step:1225/2330 train_time:73964ms step_avg:60.38ms
step:1226/2330 train_time:74026ms step_avg:60.38ms
step:1227/2330 train_time:74086ms step_avg:60.38ms
step:1228/2330 train_time:74148ms step_avg:60.38ms
step:1229/2330 train_time:74208ms step_avg:60.38ms
step:1230/2330 train_time:74270ms step_avg:60.38ms
step:1231/2330 train_time:74330ms step_avg:60.38ms
step:1232/2330 train_time:74391ms step_avg:60.38ms
step:1233/2330 train_time:74451ms step_avg:60.38ms
step:1234/2330 train_time:74513ms step_avg:60.38ms
step:1235/2330 train_time:74573ms step_avg:60.38ms
step:1236/2330 train_time:74634ms step_avg:60.38ms
step:1237/2330 train_time:74694ms step_avg:60.38ms
step:1238/2330 train_time:74757ms step_avg:60.38ms
step:1239/2330 train_time:74817ms step_avg:60.39ms
step:1240/2330 train_time:74880ms step_avg:60.39ms
step:1241/2330 train_time:74941ms step_avg:60.39ms
step:1242/2330 train_time:75004ms step_avg:60.39ms
step:1243/2330 train_time:75064ms step_avg:60.39ms
step:1244/2330 train_time:75126ms step_avg:60.39ms
step:1245/2330 train_time:75186ms step_avg:60.39ms
step:1246/2330 train_time:75247ms step_avg:60.39ms
step:1247/2330 train_time:75307ms step_avg:60.39ms
step:1248/2330 train_time:75369ms step_avg:60.39ms
step:1249/2330 train_time:75430ms step_avg:60.39ms
step:1250/2330 train_time:75491ms step_avg:60.39ms
step:1250/2330 val_loss:3.5236 train_time:75555ms step_avg:60.44ms
step:1251/2330 train_time:75578ms step_avg:60.41ms
step:1252/2330 train_time:75617ms step_avg:60.40ms
step:1253/2330 train_time:75680ms step_avg:60.40ms
step:1254/2330 train_time:75743ms step_avg:60.40ms
step:1255/2330 train_time:75803ms step_avg:60.40ms
step:1256/2330 train_time:75865ms step_avg:60.40ms
step:1257/2330 train_time:75924ms step_avg:60.40ms
step:1258/2330 train_time:75985ms step_avg:60.40ms
step:1259/2330 train_time:76044ms step_avg:60.40ms
step:1260/2330 train_time:76105ms step_avg:60.40ms
step:1261/2330 train_time:76165ms step_avg:60.40ms
step:1262/2330 train_time:76226ms step_avg:60.40ms
step:1263/2330 train_time:76285ms step_avg:60.40ms
step:1264/2330 train_time:76346ms step_avg:60.40ms
step:1265/2330 train_time:76405ms step_avg:60.40ms
step:1266/2330 train_time:76467ms step_avg:60.40ms
step:1267/2330 train_time:76530ms step_avg:60.40ms
step:1268/2330 train_time:76593ms step_avg:60.40ms
step:1269/2330 train_time:76654ms step_avg:60.40ms
step:1270/2330 train_time:76716ms step_avg:60.41ms
step:1271/2330 train_time:76776ms step_avg:60.41ms
step:1272/2330 train_time:76838ms step_avg:60.41ms
step:1273/2330 train_time:76898ms step_avg:60.41ms
step:1274/2330 train_time:76960ms step_avg:60.41ms
step:1275/2330 train_time:77020ms step_avg:60.41ms
step:1276/2330 train_time:77081ms step_avg:60.41ms
step:1277/2330 train_time:77141ms step_avg:60.41ms
step:1278/2330 train_time:77203ms step_avg:60.41ms
step:1279/2330 train_time:77263ms step_avg:60.41ms
step:1280/2330 train_time:77324ms step_avg:60.41ms
step:1281/2330 train_time:77384ms step_avg:60.41ms
step:1282/2330 train_time:77446ms step_avg:60.41ms
step:1283/2330 train_time:77506ms step_avg:60.41ms
step:1284/2330 train_time:77569ms step_avg:60.41ms
step:1285/2330 train_time:77630ms step_avg:60.41ms
step:1286/2330 train_time:77691ms step_avg:60.41ms
step:1287/2330 train_time:77752ms step_avg:60.41ms
step:1288/2330 train_time:77814ms step_avg:60.41ms
step:1289/2330 train_time:77874ms step_avg:60.41ms
step:1290/2330 train_time:77936ms step_avg:60.42ms
step:1291/2330 train_time:77995ms step_avg:60.41ms
step:1292/2330 train_time:78058ms step_avg:60.42ms
step:1293/2330 train_time:78118ms step_avg:60.42ms
step:1294/2330 train_time:78180ms step_avg:60.42ms
step:1295/2330 train_time:78240ms step_avg:60.42ms
step:1296/2330 train_time:78302ms step_avg:60.42ms
step:1297/2330 train_time:78362ms step_avg:60.42ms
step:1298/2330 train_time:78424ms step_avg:60.42ms
step:1299/2330 train_time:78484ms step_avg:60.42ms
step:1300/2330 train_time:78547ms step_avg:60.42ms
step:1301/2330 train_time:78607ms step_avg:60.42ms
step:1302/2330 train_time:78669ms step_avg:60.42ms
step:1303/2330 train_time:78728ms step_avg:60.42ms
step:1304/2330 train_time:78790ms step_avg:60.42ms
step:1305/2330 train_time:78849ms step_avg:60.42ms
step:1306/2330 train_time:78911ms step_avg:60.42ms
step:1307/2330 train_time:78970ms step_avg:60.42ms
step:1308/2330 train_time:79032ms step_avg:60.42ms
step:1309/2330 train_time:79092ms step_avg:60.42ms
step:1310/2330 train_time:79155ms step_avg:60.42ms
step:1311/2330 train_time:79215ms step_avg:60.42ms
step:1312/2330 train_time:79277ms step_avg:60.42ms
step:1313/2330 train_time:79337ms step_avg:60.42ms
step:1314/2330 train_time:79400ms step_avg:60.43ms
step:1315/2330 train_time:79460ms step_avg:60.43ms
step:1316/2330 train_time:79522ms step_avg:60.43ms
step:1317/2330 train_time:79582ms step_avg:60.43ms
step:1318/2330 train_time:79644ms step_avg:60.43ms
step:1319/2330 train_time:79704ms step_avg:60.43ms
step:1320/2330 train_time:79766ms step_avg:60.43ms
step:1321/2330 train_time:79826ms step_avg:60.43ms
step:1322/2330 train_time:79888ms step_avg:60.43ms
step:1323/2330 train_time:79947ms step_avg:60.43ms
step:1324/2330 train_time:80009ms step_avg:60.43ms
step:1325/2330 train_time:80068ms step_avg:60.43ms
step:1326/2330 train_time:80131ms step_avg:60.43ms
step:1327/2330 train_time:80191ms step_avg:60.43ms
step:1328/2330 train_time:80252ms step_avg:60.43ms
step:1329/2330 train_time:80312ms step_avg:60.43ms
step:1330/2330 train_time:80374ms step_avg:60.43ms
step:1331/2330 train_time:80434ms step_avg:60.43ms
step:1332/2330 train_time:80496ms step_avg:60.43ms
step:1333/2330 train_time:80557ms step_avg:60.43ms
step:1334/2330 train_time:80621ms step_avg:60.44ms
step:1335/2330 train_time:80681ms step_avg:60.44ms
step:1336/2330 train_time:80744ms step_avg:60.44ms
step:1337/2330 train_time:80804ms step_avg:60.44ms
step:1338/2330 train_time:80865ms step_avg:60.44ms
step:1339/2330 train_time:80925ms step_avg:60.44ms
step:1340/2330 train_time:80987ms step_avg:60.44ms
step:1341/2330 train_time:81047ms step_avg:60.44ms
step:1342/2330 train_time:81109ms step_avg:60.44ms
step:1343/2330 train_time:81168ms step_avg:60.44ms
step:1344/2330 train_time:81229ms step_avg:60.44ms
step:1345/2330 train_time:81289ms step_avg:60.44ms
step:1346/2330 train_time:81351ms step_avg:60.44ms
step:1347/2330 train_time:81411ms step_avg:60.44ms
step:1348/2330 train_time:81473ms step_avg:60.44ms
step:1349/2330 train_time:81533ms step_avg:60.44ms
step:1350/2330 train_time:81595ms step_avg:60.44ms
step:1351/2330 train_time:81656ms step_avg:60.44ms
step:1352/2330 train_time:81719ms step_avg:60.44ms
step:1353/2330 train_time:81780ms step_avg:60.44ms
step:1354/2330 train_time:81842ms step_avg:60.44ms
step:1355/2330 train_time:81903ms step_avg:60.44ms
step:1356/2330 train_time:81964ms step_avg:60.45ms
step:1357/2330 train_time:82024ms step_avg:60.45ms
step:1358/2330 train_time:82086ms step_avg:60.45ms
step:1359/2330 train_time:82146ms step_avg:60.45ms
step:1360/2330 train_time:82208ms step_avg:60.45ms
step:1361/2330 train_time:82267ms step_avg:60.45ms
step:1362/2330 train_time:82329ms step_avg:60.45ms
step:1363/2330 train_time:82389ms step_avg:60.45ms
step:1364/2330 train_time:82451ms step_avg:60.45ms
step:1365/2330 train_time:82511ms step_avg:60.45ms
step:1366/2330 train_time:82573ms step_avg:60.45ms
step:1367/2330 train_time:82633ms step_avg:60.45ms
step:1368/2330 train_time:82695ms step_avg:60.45ms
step:1369/2330 train_time:82755ms step_avg:60.45ms
step:1370/2330 train_time:82819ms step_avg:60.45ms
step:1371/2330 train_time:82879ms step_avg:60.45ms
step:1372/2330 train_time:82940ms step_avg:60.45ms
step:1373/2330 train_time:83000ms step_avg:60.45ms
step:1374/2330 train_time:83063ms step_avg:60.45ms
step:1375/2330 train_time:83123ms step_avg:60.45ms
step:1376/2330 train_time:83185ms step_avg:60.45ms
step:1377/2330 train_time:83245ms step_avg:60.45ms
step:1378/2330 train_time:83306ms step_avg:60.45ms
step:1379/2330 train_time:83367ms step_avg:60.45ms
step:1380/2330 train_time:83429ms step_avg:60.46ms
step:1381/2330 train_time:83489ms step_avg:60.46ms
step:1382/2330 train_time:83550ms step_avg:60.46ms
step:1383/2330 train_time:83610ms step_avg:60.46ms
step:1384/2330 train_time:83671ms step_avg:60.46ms
step:1385/2330 train_time:83731ms step_avg:60.46ms
step:1386/2330 train_time:83793ms step_avg:60.46ms
step:1387/2330 train_time:83854ms step_avg:60.46ms
step:1388/2330 train_time:83917ms step_avg:60.46ms
step:1389/2330 train_time:83977ms step_avg:60.46ms
step:1390/2330 train_time:84039ms step_avg:60.46ms
step:1391/2330 train_time:84100ms step_avg:60.46ms
step:1392/2330 train_time:84162ms step_avg:60.46ms
step:1393/2330 train_time:84222ms step_avg:60.46ms
step:1394/2330 train_time:84284ms step_avg:60.46ms
step:1395/2330 train_time:84344ms step_avg:60.46ms
step:1396/2330 train_time:84407ms step_avg:60.46ms
step:1397/2330 train_time:84467ms step_avg:60.46ms
step:1398/2330 train_time:84529ms step_avg:60.46ms
step:1399/2330 train_time:84588ms step_avg:60.46ms
step:1400/2330 train_time:84650ms step_avg:60.46ms
step:1401/2330 train_time:84709ms step_avg:60.46ms
step:1402/2330 train_time:84771ms step_avg:60.46ms
step:1403/2330 train_time:84831ms step_avg:60.46ms
step:1404/2330 train_time:84892ms step_avg:60.46ms
step:1405/2330 train_time:84953ms step_avg:60.46ms
step:1406/2330 train_time:85015ms step_avg:60.47ms
step:1407/2330 train_time:85076ms step_avg:60.47ms
step:1408/2330 train_time:85138ms step_avg:60.47ms
step:1409/2330 train_time:85198ms step_avg:60.47ms
step:1410/2330 train_time:85260ms step_avg:60.47ms
step:1411/2330 train_time:85320ms step_avg:60.47ms
step:1412/2330 train_time:85383ms step_avg:60.47ms
step:1413/2330 train_time:85443ms step_avg:60.47ms
step:1414/2330 train_time:85505ms step_avg:60.47ms
step:1415/2330 train_time:85565ms step_avg:60.47ms
step:1416/2330 train_time:85627ms step_avg:60.47ms
step:1417/2330 train_time:85687ms step_avg:60.47ms
step:1418/2330 train_time:85749ms step_avg:60.47ms
step:1419/2330 train_time:85809ms step_avg:60.47ms
step:1420/2330 train_time:85871ms step_avg:60.47ms
step:1421/2330 train_time:85930ms step_avg:60.47ms
step:1422/2330 train_time:85993ms step_avg:60.47ms
step:1423/2330 train_time:86052ms step_avg:60.47ms
step:1424/2330 train_time:86114ms step_avg:60.47ms
step:1425/2330 train_time:86174ms step_avg:60.47ms
step:1426/2330 train_time:86237ms step_avg:60.47ms
step:1427/2330 train_time:86298ms step_avg:60.47ms
step:1428/2330 train_time:86360ms step_avg:60.48ms
step:1429/2330 train_time:86421ms step_avg:60.48ms
step:1430/2330 train_time:86482ms step_avg:60.48ms
step:1431/2330 train_time:86543ms step_avg:60.48ms
step:1432/2330 train_time:86605ms step_avg:60.48ms
step:1433/2330 train_time:86665ms step_avg:60.48ms
step:1434/2330 train_time:86727ms step_avg:60.48ms
step:1435/2330 train_time:86788ms step_avg:60.48ms
step:1436/2330 train_time:86849ms step_avg:60.48ms
step:1437/2330 train_time:86909ms step_avg:60.48ms
step:1438/2330 train_time:86971ms step_avg:60.48ms
step:1439/2330 train_time:87030ms step_avg:60.48ms
step:1440/2330 train_time:87092ms step_avg:60.48ms
step:1441/2330 train_time:87152ms step_avg:60.48ms
step:1442/2330 train_time:87214ms step_avg:60.48ms
step:1443/2330 train_time:87274ms step_avg:60.48ms
step:1444/2330 train_time:87336ms step_avg:60.48ms
step:1445/2330 train_time:87396ms step_avg:60.48ms
step:1446/2330 train_time:87458ms step_avg:60.48ms
step:1447/2330 train_time:87519ms step_avg:60.48ms
step:1448/2330 train_time:87581ms step_avg:60.48ms
step:1449/2330 train_time:87642ms step_avg:60.48ms
step:1450/2330 train_time:87704ms step_avg:60.49ms
step:1451/2330 train_time:87765ms step_avg:60.49ms
step:1452/2330 train_time:87827ms step_avg:60.49ms
step:1453/2330 train_time:87887ms step_avg:60.49ms
step:1454/2330 train_time:87948ms step_avg:60.49ms
step:1455/2330 train_time:88008ms step_avg:60.49ms
step:1456/2330 train_time:88069ms step_avg:60.49ms
step:1457/2330 train_time:88129ms step_avg:60.49ms
step:1458/2330 train_time:88191ms step_avg:60.49ms
step:1459/2330 train_time:88251ms step_avg:60.49ms
step:1460/2330 train_time:88314ms step_avg:60.49ms
step:1461/2330 train_time:88374ms step_avg:60.49ms
step:1462/2330 train_time:88436ms step_avg:60.49ms
step:1463/2330 train_time:88496ms step_avg:60.49ms
step:1464/2330 train_time:88558ms step_avg:60.49ms
step:1465/2330 train_time:88619ms step_avg:60.49ms
step:1466/2330 train_time:88681ms step_avg:60.49ms
step:1467/2330 train_time:88741ms step_avg:60.49ms
step:1468/2330 train_time:88804ms step_avg:60.49ms
step:1469/2330 train_time:88864ms step_avg:60.49ms
step:1470/2330 train_time:88927ms step_avg:60.49ms
step:1471/2330 train_time:88987ms step_avg:60.49ms
step:1472/2330 train_time:89049ms step_avg:60.49ms
step:1473/2330 train_time:89108ms step_avg:60.49ms
step:1474/2330 train_time:89169ms step_avg:60.49ms
step:1475/2330 train_time:89229ms step_avg:60.49ms
step:1476/2330 train_time:89291ms step_avg:60.49ms
step:1477/2330 train_time:89351ms step_avg:60.49ms
step:1478/2330 train_time:89413ms step_avg:60.50ms
step:1479/2330 train_time:89473ms step_avg:60.50ms
step:1480/2330 train_time:89535ms step_avg:60.50ms
step:1481/2330 train_time:89596ms step_avg:60.50ms
step:1482/2330 train_time:89659ms step_avg:60.50ms
step:1483/2330 train_time:89720ms step_avg:60.50ms
step:1484/2330 train_time:89782ms step_avg:60.50ms
step:1485/2330 train_time:89842ms step_avg:60.50ms
step:1486/2330 train_time:89905ms step_avg:60.50ms
step:1487/2330 train_time:89965ms step_avg:60.50ms
step:1488/2330 train_time:90027ms step_avg:60.50ms
step:1489/2330 train_time:90087ms step_avg:60.50ms
step:1490/2330 train_time:90148ms step_avg:60.50ms
step:1491/2330 train_time:90209ms step_avg:60.50ms
step:1492/2330 train_time:90270ms step_avg:60.50ms
step:1493/2330 train_time:90330ms step_avg:60.50ms
step:1494/2330 train_time:90391ms step_avg:60.50ms
step:1495/2330 train_time:90452ms step_avg:60.50ms
step:1496/2330 train_time:90514ms step_avg:60.50ms
step:1497/2330 train_time:90574ms step_avg:60.50ms
step:1498/2330 train_time:90636ms step_avg:60.50ms
step:1499/2330 train_time:90697ms step_avg:60.50ms
step:1500/2330 train_time:90760ms step_avg:60.51ms
step:1500/2330 val_loss:3.4589 train_time:90824ms step_avg:60.55ms
step:1501/2330 train_time:90847ms step_avg:60.52ms
step:1502/2330 train_time:90887ms step_avg:60.51ms
step:1503/2330 train_time:90951ms step_avg:60.51ms
step:1504/2330 train_time:91015ms step_avg:60.52ms
step:1505/2330 train_time:91075ms step_avg:60.52ms
step:1506/2330 train_time:91138ms step_avg:60.52ms
step:1507/2330 train_time:91198ms step_avg:60.52ms
step:1508/2330 train_time:91259ms step_avg:60.52ms
step:1509/2330 train_time:91318ms step_avg:60.52ms
step:1510/2330 train_time:91379ms step_avg:60.52ms
step:1511/2330 train_time:91439ms step_avg:60.52ms
step:1512/2330 train_time:91500ms step_avg:60.52ms
step:1513/2330 train_time:91559ms step_avg:60.51ms
step:1514/2330 train_time:91621ms step_avg:60.52ms
step:1515/2330 train_time:91680ms step_avg:60.51ms
step:1516/2330 train_time:91742ms step_avg:60.52ms
step:1517/2330 train_time:91804ms step_avg:60.52ms
step:1518/2330 train_time:91867ms step_avg:60.52ms
step:1519/2330 train_time:91929ms step_avg:60.52ms
step:1520/2330 train_time:91992ms step_avg:60.52ms
step:1521/2330 train_time:92053ms step_avg:60.52ms
step:1522/2330 train_time:92115ms step_avg:60.52ms
step:1523/2330 train_time:92175ms step_avg:60.52ms
step:1524/2330 train_time:92236ms step_avg:60.52ms
step:1525/2330 train_time:92296ms step_avg:60.52ms
step:1526/2330 train_time:92357ms step_avg:60.52ms
step:1527/2330 train_time:92416ms step_avg:60.52ms
step:1528/2330 train_time:92477ms step_avg:60.52ms
step:1529/2330 train_time:92537ms step_avg:60.52ms
step:1530/2330 train_time:92599ms step_avg:60.52ms
step:1531/2330 train_time:92659ms step_avg:60.52ms
step:1532/2330 train_time:92722ms step_avg:60.52ms
step:1533/2330 train_time:92784ms step_avg:60.52ms
step:1534/2330 train_time:92846ms step_avg:60.53ms
step:1535/2330 train_time:92907ms step_avg:60.53ms
step:1536/2330 train_time:92971ms step_avg:60.53ms
step:1537/2330 train_time:93032ms step_avg:60.53ms
step:1538/2330 train_time:93094ms step_avg:60.53ms
step:1539/2330 train_time:93155ms step_avg:60.53ms
step:1540/2330 train_time:93217ms step_avg:60.53ms
step:1541/2330 train_time:93278ms step_avg:60.53ms
step:1542/2330 train_time:93340ms step_avg:60.53ms
step:1543/2330 train_time:93400ms step_avg:60.53ms
step:1544/2330 train_time:93463ms step_avg:60.53ms
step:1545/2330 train_time:93523ms step_avg:60.53ms
step:1546/2330 train_time:93585ms step_avg:60.53ms
step:1547/2330 train_time:93645ms step_avg:60.53ms
step:1548/2330 train_time:93707ms step_avg:60.53ms
step:1549/2330 train_time:93768ms step_avg:60.53ms
step:1550/2330 train_time:93830ms step_avg:60.54ms
step:1551/2330 train_time:93891ms step_avg:60.54ms
step:1552/2330 train_time:93955ms step_avg:60.54ms
step:1553/2330 train_time:94016ms step_avg:60.54ms
step:1554/2330 train_time:94079ms step_avg:60.54ms
step:1555/2330 train_time:94140ms step_avg:60.54ms
step:1556/2330 train_time:94203ms step_avg:60.54ms
step:1557/2330 train_time:94264ms step_avg:60.54ms
step:1558/2330 train_time:94327ms step_avg:60.54ms
step:1559/2330 train_time:94387ms step_avg:60.54ms
step:1560/2330 train_time:94449ms step_avg:60.54ms
step:1561/2330 train_time:94509ms step_avg:60.54ms
step:1562/2330 train_time:94571ms step_avg:60.54ms
step:1563/2330 train_time:94631ms step_avg:60.54ms
step:1564/2330 train_time:94693ms step_avg:60.55ms
step:1565/2330 train_time:94754ms step_avg:60.55ms
step:1566/2330 train_time:94817ms step_avg:60.55ms
step:1567/2330 train_time:94878ms step_avg:60.55ms
step:1568/2330 train_time:94942ms step_avg:60.55ms
step:1569/2330 train_time:95003ms step_avg:60.55ms
step:1570/2330 train_time:95065ms step_avg:60.55ms
step:1571/2330 train_time:95126ms step_avg:60.55ms
step:1572/2330 train_time:95188ms step_avg:60.55ms
step:1573/2330 train_time:95248ms step_avg:60.55ms
step:1574/2330 train_time:95310ms step_avg:60.55ms
step:1575/2330 train_time:95370ms step_avg:60.55ms
step:1576/2330 train_time:95432ms step_avg:60.55ms
step:1577/2330 train_time:95492ms step_avg:60.55ms
step:1578/2330 train_time:95555ms step_avg:60.55ms
step:1579/2330 train_time:95615ms step_avg:60.55ms
step:1580/2330 train_time:95678ms step_avg:60.56ms
step:1581/2330 train_time:95739ms step_avg:60.56ms
step:1582/2330 train_time:95801ms step_avg:60.56ms
step:1583/2330 train_time:95862ms step_avg:60.56ms
step:1584/2330 train_time:95924ms step_avg:60.56ms
step:1585/2330 train_time:95985ms step_avg:60.56ms
step:1586/2330 train_time:96048ms step_avg:60.56ms
step:1587/2330 train_time:96108ms step_avg:60.56ms
step:1588/2330 train_time:96170ms step_avg:60.56ms
step:1589/2330 train_time:96230ms step_avg:60.56ms
step:1590/2330 train_time:96292ms step_avg:60.56ms
step:1591/2330 train_time:96352ms step_avg:60.56ms
step:1592/2330 train_time:96414ms step_avg:60.56ms
step:1593/2330 train_time:96474ms step_avg:60.56ms
step:1594/2330 train_time:96536ms step_avg:60.56ms
step:1595/2330 train_time:96596ms step_avg:60.56ms
step:1596/2330 train_time:96659ms step_avg:60.56ms
step:1597/2330 train_time:96719ms step_avg:60.56ms
step:1598/2330 train_time:96782ms step_avg:60.56ms
step:1599/2330 train_time:96843ms step_avg:60.56ms
step:1600/2330 train_time:96906ms step_avg:60.57ms
step:1601/2330 train_time:96966ms step_avg:60.57ms
step:1602/2330 train_time:97029ms step_avg:60.57ms
step:1603/2330 train_time:97089ms step_avg:60.57ms
step:1604/2330 train_time:97151ms step_avg:60.57ms
step:1605/2330 train_time:97211ms step_avg:60.57ms
step:1606/2330 train_time:97273ms step_avg:60.57ms
step:1607/2330 train_time:97333ms step_avg:60.57ms
step:1608/2330 train_time:97395ms step_avg:60.57ms
step:1609/2330 train_time:97455ms step_avg:60.57ms
step:1610/2330 train_time:97517ms step_avg:60.57ms
step:1611/2330 train_time:97578ms step_avg:60.57ms
step:1612/2330 train_time:97640ms step_avg:60.57ms
step:1613/2330 train_time:97701ms step_avg:60.57ms
step:1614/2330 train_time:97763ms step_avg:60.57ms
step:1615/2330 train_time:97824ms step_avg:60.57ms
step:1616/2330 train_time:97886ms step_avg:60.57ms
step:1617/2330 train_time:97948ms step_avg:60.57ms
step:1618/2330 train_time:98010ms step_avg:60.57ms
step:1619/2330 train_time:98070ms step_avg:60.57ms
step:1620/2330 train_time:98132ms step_avg:60.58ms
step:1621/2330 train_time:98193ms step_avg:60.58ms
step:1622/2330 train_time:98254ms step_avg:60.58ms
step:1623/2330 train_time:98314ms step_avg:60.58ms
step:1624/2330 train_time:98377ms step_avg:60.58ms
step:1625/2330 train_time:98437ms step_avg:60.58ms
step:1626/2330 train_time:98500ms step_avg:60.58ms
step:1627/2330 train_time:98560ms step_avg:60.58ms
step:1628/2330 train_time:98623ms step_avg:60.58ms
step:1629/2330 train_time:98683ms step_avg:60.58ms
step:1630/2330 train_time:98745ms step_avg:60.58ms
step:1631/2330 train_time:98805ms step_avg:60.58ms
step:1632/2330 train_time:98868ms step_avg:60.58ms
step:1633/2330 train_time:98929ms step_avg:60.58ms
step:1634/2330 train_time:98991ms step_avg:60.58ms
step:1635/2330 train_time:99052ms step_avg:60.58ms
step:1636/2330 train_time:99115ms step_avg:60.58ms
step:1637/2330 train_time:99175ms step_avg:60.58ms
step:1638/2330 train_time:99238ms step_avg:60.58ms
step:1639/2330 train_time:99298ms step_avg:60.58ms
step:1640/2330 train_time:99360ms step_avg:60.59ms
step:1641/2330 train_time:99420ms step_avg:60.59ms
step:1642/2330 train_time:99483ms step_avg:60.59ms
step:1643/2330 train_time:99544ms step_avg:60.59ms
step:1644/2330 train_time:99606ms step_avg:60.59ms
step:1645/2330 train_time:99666ms step_avg:60.59ms
step:1646/2330 train_time:99728ms step_avg:60.59ms
step:1647/2330 train_time:99788ms step_avg:60.59ms
step:1648/2330 train_time:99851ms step_avg:60.59ms
step:1649/2330 train_time:99912ms step_avg:60.59ms
step:1650/2330 train_time:99974ms step_avg:60.59ms
step:1651/2330 train_time:100034ms step_avg:60.59ms
step:1652/2330 train_time:100098ms step_avg:60.59ms
step:1653/2330 train_time:100158ms step_avg:60.59ms
step:1654/2330 train_time:100220ms step_avg:60.59ms
step:1655/2330 train_time:100281ms step_avg:60.59ms
step:1656/2330 train_time:100343ms step_avg:60.59ms
step:1657/2330 train_time:100404ms step_avg:60.59ms
step:1658/2330 train_time:100466ms step_avg:60.59ms
step:1659/2330 train_time:100526ms step_avg:60.59ms
step:1660/2330 train_time:100588ms step_avg:60.60ms
step:1661/2330 train_time:100648ms step_avg:60.59ms
step:1662/2330 train_time:100710ms step_avg:60.60ms
step:1663/2330 train_time:100770ms step_avg:60.60ms
step:1664/2330 train_time:100832ms step_avg:60.60ms
step:1665/2330 train_time:100892ms step_avg:60.60ms
step:1666/2330 train_time:100955ms step_avg:60.60ms
step:1667/2330 train_time:101015ms step_avg:60.60ms
step:1668/2330 train_time:101077ms step_avg:60.60ms
step:1669/2330 train_time:101138ms step_avg:60.60ms
step:1670/2330 train_time:101201ms step_avg:60.60ms
step:1671/2330 train_time:101262ms step_avg:60.60ms
step:1672/2330 train_time:101324ms step_avg:60.60ms
step:1673/2330 train_time:101384ms step_avg:60.60ms
step:1674/2330 train_time:101446ms step_avg:60.60ms
step:1675/2330 train_time:101507ms step_avg:60.60ms
step:1676/2330 train_time:101569ms step_avg:60.60ms
step:1677/2330 train_time:101629ms step_avg:60.60ms
step:1678/2330 train_time:101691ms step_avg:60.60ms
step:1679/2330 train_time:101751ms step_avg:60.60ms
step:1680/2330 train_time:101814ms step_avg:60.60ms
step:1681/2330 train_time:101874ms step_avg:60.60ms
step:1682/2330 train_time:101937ms step_avg:60.60ms
step:1683/2330 train_time:101997ms step_avg:60.60ms
step:1684/2330 train_time:102059ms step_avg:60.61ms
step:1685/2330 train_time:102120ms step_avg:60.61ms
step:1686/2330 train_time:102183ms step_avg:60.61ms
step:1687/2330 train_time:102244ms step_avg:60.61ms
step:1688/2330 train_time:102306ms step_avg:60.61ms
step:1689/2330 train_time:102366ms step_avg:60.61ms
step:1690/2330 train_time:102429ms step_avg:60.61ms
step:1691/2330 train_time:102489ms step_avg:60.61ms
step:1692/2330 train_time:102552ms step_avg:60.61ms
step:1693/2330 train_time:102612ms step_avg:60.61ms
step:1694/2330 train_time:102674ms step_avg:60.61ms
step:1695/2330 train_time:102735ms step_avg:60.61ms
step:1696/2330 train_time:102797ms step_avg:60.61ms
step:1697/2330 train_time:102858ms step_avg:60.61ms
step:1698/2330 train_time:102920ms step_avg:60.61ms
step:1699/2330 train_time:102981ms step_avg:60.61ms
step:1700/2330 train_time:103044ms step_avg:60.61ms
step:1701/2330 train_time:103104ms step_avg:60.61ms
step:1702/2330 train_time:103166ms step_avg:60.61ms
step:1703/2330 train_time:103227ms step_avg:60.61ms
step:1704/2330 train_time:103288ms step_avg:60.62ms
step:1705/2330 train_time:103349ms step_avg:60.62ms
step:1706/2330 train_time:103410ms step_avg:60.62ms
step:1707/2330 train_time:103471ms step_avg:60.62ms
step:1708/2330 train_time:103533ms step_avg:60.62ms
step:1709/2330 train_time:103593ms step_avg:60.62ms
step:1710/2330 train_time:103656ms step_avg:60.62ms
step:1711/2330 train_time:103717ms step_avg:60.62ms
step:1712/2330 train_time:103780ms step_avg:60.62ms
step:1713/2330 train_time:103840ms step_avg:60.62ms
step:1714/2330 train_time:103903ms step_avg:60.62ms
step:1715/2330 train_time:103963ms step_avg:60.62ms
step:1716/2330 train_time:104026ms step_avg:60.62ms
step:1717/2330 train_time:104086ms step_avg:60.62ms
step:1718/2330 train_time:104148ms step_avg:60.62ms
step:1719/2330 train_time:104208ms step_avg:60.62ms
step:1720/2330 train_time:104270ms step_avg:60.62ms
step:1721/2330 train_time:104330ms step_avg:60.62ms
step:1722/2330 train_time:104392ms step_avg:60.62ms
step:1723/2330 train_time:104452ms step_avg:60.62ms
step:1724/2330 train_time:104515ms step_avg:60.62ms
step:1725/2330 train_time:104575ms step_avg:60.62ms
step:1726/2330 train_time:104638ms step_avg:60.62ms
step:1727/2330 train_time:104698ms step_avg:60.62ms
step:1728/2330 train_time:104762ms step_avg:60.63ms
step:1729/2330 train_time:104823ms step_avg:60.63ms
step:1730/2330 train_time:104885ms step_avg:60.63ms
step:1731/2330 train_time:104945ms step_avg:60.63ms
step:1732/2330 train_time:105008ms step_avg:60.63ms
step:1733/2330 train_time:105067ms step_avg:60.63ms
step:1734/2330 train_time:105130ms step_avg:60.63ms
step:1735/2330 train_time:105190ms step_avg:60.63ms
step:1736/2330 train_time:105252ms step_avg:60.63ms
step:1737/2330 train_time:105313ms step_avg:60.63ms
step:1738/2330 train_time:105376ms step_avg:60.63ms
step:1739/2330 train_time:105436ms step_avg:60.63ms
step:1740/2330 train_time:105499ms step_avg:60.63ms
step:1741/2330 train_time:105560ms step_avg:60.63ms
step:1742/2330 train_time:105622ms step_avg:60.63ms
step:1743/2330 train_time:105682ms step_avg:60.63ms
step:1744/2330 train_time:105745ms step_avg:60.63ms
step:1745/2330 train_time:105805ms step_avg:60.63ms
step:1746/2330 train_time:105868ms step_avg:60.63ms
step:1747/2330 train_time:105928ms step_avg:60.63ms
step:1748/2330 train_time:105990ms step_avg:60.63ms
step:1749/2330 train_time:106050ms step_avg:60.63ms
step:1750/2330 train_time:106112ms step_avg:60.64ms
step:1750/2330 val_loss:3.3901 train_time:106175ms step_avg:60.67ms
step:1751/2330 train_time:106198ms step_avg:60.65ms
step:1752/2330 train_time:106237ms step_avg:60.64ms
step:1753/2330 train_time:106304ms step_avg:60.64ms
step:1754/2330 train_time:106370ms step_avg:60.64ms
step:1755/2330 train_time:106430ms step_avg:60.64ms
step:1756/2330 train_time:106492ms step_avg:60.64ms
step:1757/2330 train_time:106552ms step_avg:60.64ms
step:1758/2330 train_time:106614ms step_avg:60.64ms
step:1759/2330 train_time:106673ms step_avg:60.64ms
step:1760/2330 train_time:106734ms step_avg:60.64ms
step:1761/2330 train_time:106794ms step_avg:60.64ms
step:1762/2330 train_time:106855ms step_avg:60.64ms
step:1763/2330 train_time:106914ms step_avg:60.64ms
step:1764/2330 train_time:106976ms step_avg:60.64ms
step:1765/2330 train_time:107035ms step_avg:60.64ms
step:1766/2330 train_time:107100ms step_avg:60.65ms
step:1767/2330 train_time:107162ms step_avg:60.65ms
step:1768/2330 train_time:107225ms step_avg:60.65ms
step:1769/2330 train_time:107287ms step_avg:60.65ms
step:1770/2330 train_time:107350ms step_avg:60.65ms
step:1771/2330 train_time:107410ms step_avg:60.65ms
step:1772/2330 train_time:107473ms step_avg:60.65ms
step:1773/2330 train_time:107533ms step_avg:60.65ms
step:1774/2330 train_time:107595ms step_avg:60.65ms
step:1775/2330 train_time:107654ms step_avg:60.65ms
step:1776/2330 train_time:107716ms step_avg:60.65ms
step:1777/2330 train_time:107775ms step_avg:60.65ms
step:1778/2330 train_time:107837ms step_avg:60.65ms
step:1779/2330 train_time:107896ms step_avg:60.65ms
step:1780/2330 train_time:107958ms step_avg:60.65ms
step:1781/2330 train_time:108018ms step_avg:60.65ms
step:1782/2330 train_time:108080ms step_avg:60.65ms
step:1783/2330 train_time:108141ms step_avg:60.65ms
step:1784/2330 train_time:108204ms step_avg:60.65ms
step:1785/2330 train_time:108266ms step_avg:60.65ms
step:1786/2330 train_time:108329ms step_avg:60.65ms
step:1787/2330 train_time:108390ms step_avg:60.65ms
step:1788/2330 train_time:108452ms step_avg:60.66ms
step:1789/2330 train_time:108513ms step_avg:60.66ms
step:1790/2330 train_time:108575ms step_avg:60.66ms
step:1791/2330 train_time:108635ms step_avg:60.66ms
step:1792/2330 train_time:108697ms step_avg:60.66ms
step:1793/2330 train_time:108756ms step_avg:60.66ms
step:1794/2330 train_time:108818ms step_avg:60.66ms
step:1795/2330 train_time:108878ms step_avg:60.66ms
step:1796/2330 train_time:108940ms step_avg:60.66ms
step:1797/2330 train_time:109000ms step_avg:60.66ms
step:1798/2330 train_time:109062ms step_avg:60.66ms
step:1799/2330 train_time:109122ms step_avg:60.66ms
step:1800/2330 train_time:109185ms step_avg:60.66ms
step:1801/2330 train_time:109247ms step_avg:60.66ms
step:1802/2330 train_time:109310ms step_avg:60.66ms
step:1803/2330 train_time:109371ms step_avg:60.66ms
step:1804/2330 train_time:109433ms step_avg:60.66ms
step:1805/2330 train_time:109494ms step_avg:60.66ms
step:1806/2330 train_time:109556ms step_avg:60.66ms
step:1807/2330 train_time:109616ms step_avg:60.66ms
step:1808/2330 train_time:109679ms step_avg:60.66ms
step:1809/2330 train_time:109739ms step_avg:60.66ms
step:1810/2330 train_time:109801ms step_avg:60.66ms
step:1811/2330 train_time:109862ms step_avg:60.66ms
step:1812/2330 train_time:109924ms step_avg:60.66ms
step:1813/2330 train_time:109985ms step_avg:60.66ms
step:1814/2330 train_time:110046ms step_avg:60.67ms
step:1815/2330 train_time:110107ms step_avg:60.67ms
step:1816/2330 train_time:110170ms step_avg:60.67ms
step:1817/2330 train_time:110230ms step_avg:60.67ms
step:1818/2330 train_time:110293ms step_avg:60.67ms
step:1819/2330 train_time:110353ms step_avg:60.67ms
step:1820/2330 train_time:110415ms step_avg:60.67ms
step:1821/2330 train_time:110476ms step_avg:60.67ms
step:1822/2330 train_time:110538ms step_avg:60.67ms
step:1823/2330 train_time:110598ms step_avg:60.67ms
step:1824/2330 train_time:110660ms step_avg:60.67ms
step:1825/2330 train_time:110720ms step_avg:60.67ms
step:1826/2330 train_time:110783ms step_avg:60.67ms
step:1827/2330 train_time:110844ms step_avg:60.67ms
step:1828/2330 train_time:110906ms step_avg:60.67ms
step:1829/2330 train_time:110965ms step_avg:60.67ms
step:1830/2330 train_time:111027ms step_avg:60.67ms
step:1831/2330 train_time:111087ms step_avg:60.67ms
step:1832/2330 train_time:111150ms step_avg:60.67ms
step:1833/2330 train_time:111210ms step_avg:60.67ms
step:1834/2330 train_time:111272ms step_avg:60.67ms
step:1835/2330 train_time:111333ms step_avg:60.67ms
step:1836/2330 train_time:111396ms step_avg:60.67ms
step:1837/2330 train_time:111456ms step_avg:60.67ms
step:1838/2330 train_time:111518ms step_avg:60.67ms
step:1839/2330 train_time:111578ms step_avg:60.67ms
step:1840/2330 train_time:111640ms step_avg:60.67ms
step:1841/2330 train_time:111700ms step_avg:60.67ms
step:1842/2330 train_time:111762ms step_avg:60.67ms
step:1843/2330 train_time:111823ms step_avg:60.67ms
step:1844/2330 train_time:111886ms step_avg:60.68ms
step:1845/2330 train_time:111946ms step_avg:60.68ms
step:1846/2330 train_time:112008ms step_avg:60.68ms
step:1847/2330 train_time:112068ms step_avg:60.68ms
step:1848/2330 train_time:112131ms step_avg:60.68ms
step:1849/2330 train_time:112192ms step_avg:60.68ms
step:1850/2330 train_time:112254ms step_avg:60.68ms
step:1851/2330 train_time:112315ms step_avg:60.68ms
step:1852/2330 train_time:112377ms step_avg:60.68ms
step:1853/2330 train_time:112438ms step_avg:60.68ms
step:1854/2330 train_time:112501ms step_avg:60.68ms
step:1855/2330 train_time:112561ms step_avg:60.68ms
step:1856/2330 train_time:112623ms step_avg:60.68ms
step:1857/2330 train_time:112684ms step_avg:60.68ms
step:1858/2330 train_time:112746ms step_avg:60.68ms
step:1859/2330 train_time:112807ms step_avg:60.68ms
step:1860/2330 train_time:112870ms step_avg:60.68ms
step:1861/2330 train_time:112930ms step_avg:60.68ms
step:1862/2330 train_time:112992ms step_avg:60.68ms
step:1863/2330 train_time:113052ms step_avg:60.68ms
step:1864/2330 train_time:113114ms step_avg:60.68ms
step:1865/2330 train_time:113174ms step_avg:60.68ms
step:1866/2330 train_time:113236ms step_avg:60.68ms
step:1867/2330 train_time:113297ms step_avg:60.68ms
step:1868/2330 train_time:113359ms step_avg:60.68ms
step:1869/2330 train_time:113420ms step_avg:60.68ms
step:1870/2330 train_time:113483ms step_avg:60.69ms
step:1871/2330 train_time:113543ms step_avg:60.69ms
step:1872/2330 train_time:113606ms step_avg:60.69ms
step:1873/2330 train_time:113665ms step_avg:60.69ms
step:1874/2330 train_time:113727ms step_avg:60.69ms
step:1875/2330 train_time:113788ms step_avg:60.69ms
step:1876/2330 train_time:113850ms step_avg:60.69ms
step:1877/2330 train_time:113910ms step_avg:60.69ms
step:1878/2330 train_time:113973ms step_avg:60.69ms
step:1879/2330 train_time:114033ms step_avg:60.69ms
step:1880/2330 train_time:114095ms step_avg:60.69ms
step:1881/2330 train_time:114155ms step_avg:60.69ms
step:1882/2330 train_time:114218ms step_avg:60.69ms
step:1883/2330 train_time:114278ms step_avg:60.69ms
step:1884/2330 train_time:114340ms step_avg:60.69ms
step:1885/2330 train_time:114402ms step_avg:60.69ms
step:1886/2330 train_time:114464ms step_avg:60.69ms
step:1887/2330 train_time:114525ms step_avg:60.69ms
step:1888/2330 train_time:114588ms step_avg:60.69ms
step:1889/2330 train_time:114649ms step_avg:60.69ms
step:1890/2330 train_time:114711ms step_avg:60.69ms
step:1891/2330 train_time:114771ms step_avg:60.69ms
step:1892/2330 train_time:114833ms step_avg:60.69ms
step:1893/2330 train_time:114894ms step_avg:60.69ms
step:1894/2330 train_time:114955ms step_avg:60.69ms
step:1895/2330 train_time:115015ms step_avg:60.69ms
step:1896/2330 train_time:115078ms step_avg:60.70ms
step:1897/2330 train_time:115138ms step_avg:60.69ms
step:1898/2330 train_time:115200ms step_avg:60.70ms
step:1899/2330 train_time:115261ms step_avg:60.70ms
step:1900/2330 train_time:115324ms step_avg:60.70ms
step:1901/2330 train_time:115386ms step_avg:60.70ms
step:1902/2330 train_time:115447ms step_avg:60.70ms
step:1903/2330 train_time:115508ms step_avg:60.70ms
step:1904/2330 train_time:115570ms step_avg:60.70ms
step:1905/2330 train_time:115630ms step_avg:60.70ms
step:1906/2330 train_time:115693ms step_avg:60.70ms
step:1907/2330 train_time:115753ms step_avg:60.70ms
step:1908/2330 train_time:115815ms step_avg:60.70ms
step:1909/2330 train_time:115875ms step_avg:60.70ms
step:1910/2330 train_time:115937ms step_avg:60.70ms
step:1911/2330 train_time:115996ms step_avg:60.70ms
step:1912/2330 train_time:116058ms step_avg:60.70ms
step:1913/2330 train_time:116118ms step_avg:60.70ms
step:1914/2330 train_time:116181ms step_avg:60.70ms
step:1915/2330 train_time:116241ms step_avg:60.70ms
step:1916/2330 train_time:116303ms step_avg:60.70ms
step:1917/2330 train_time:116363ms step_avg:60.70ms
step:1918/2330 train_time:116426ms step_avg:60.70ms
step:1919/2330 train_time:116487ms step_avg:60.70ms
step:1920/2330 train_time:116550ms step_avg:60.70ms
step:1921/2330 train_time:116610ms step_avg:60.70ms
step:1922/2330 train_time:116673ms step_avg:60.70ms
step:1923/2330 train_time:116733ms step_avg:60.70ms
step:1924/2330 train_time:116796ms step_avg:60.70ms
step:1925/2330 train_time:116856ms step_avg:60.70ms
step:1926/2330 train_time:116918ms step_avg:60.70ms
step:1927/2330 train_time:116978ms step_avg:60.70ms
step:1928/2330 train_time:117040ms step_avg:60.71ms
step:1929/2330 train_time:117100ms step_avg:60.70ms
step:1930/2330 train_time:117163ms step_avg:60.71ms
step:1931/2330 train_time:117223ms step_avg:60.71ms
step:1932/2330 train_time:117286ms step_avg:60.71ms
step:1933/2330 train_time:117346ms step_avg:60.71ms
step:1934/2330 train_time:117409ms step_avg:60.71ms
step:1935/2330 train_time:117470ms step_avg:60.71ms
step:1936/2330 train_time:117532ms step_avg:60.71ms
step:1937/2330 train_time:117592ms step_avg:60.71ms
step:1938/2330 train_time:117655ms step_avg:60.71ms
step:1939/2330 train_time:117715ms step_avg:60.71ms
step:1940/2330 train_time:117777ms step_avg:60.71ms
step:1941/2330 train_time:117837ms step_avg:60.71ms
step:1942/2330 train_time:117900ms step_avg:60.71ms
step:1943/2330 train_time:117961ms step_avg:60.71ms
step:1944/2330 train_time:118023ms step_avg:60.71ms
step:1945/2330 train_time:118084ms step_avg:60.71ms
step:1946/2330 train_time:118146ms step_avg:60.71ms
step:1947/2330 train_time:118207ms step_avg:60.71ms
step:1948/2330 train_time:118269ms step_avg:60.71ms
step:1949/2330 train_time:118329ms step_avg:60.71ms
step:1950/2330 train_time:118392ms step_avg:60.71ms
step:1951/2330 train_time:118452ms step_avg:60.71ms
step:1952/2330 train_time:118514ms step_avg:60.71ms
step:1953/2330 train_time:118574ms step_avg:60.71ms
step:1954/2330 train_time:118635ms step_avg:60.71ms
step:1955/2330 train_time:118696ms step_avg:60.71ms
step:1956/2330 train_time:118759ms step_avg:60.72ms
step:1957/2330 train_time:118819ms step_avg:60.71ms
step:1958/2330 train_time:118881ms step_avg:60.72ms
step:1959/2330 train_time:118941ms step_avg:60.72ms
step:1960/2330 train_time:119003ms step_avg:60.72ms
step:1961/2330 train_time:119064ms step_avg:60.72ms
step:1962/2330 train_time:119127ms step_avg:60.72ms
step:1963/2330 train_time:119188ms step_avg:60.72ms
step:1964/2330 train_time:119250ms step_avg:60.72ms
step:1965/2330 train_time:119311ms step_avg:60.72ms
step:1966/2330 train_time:119373ms step_avg:60.72ms
step:1967/2330 train_time:119432ms step_avg:60.72ms
step:1968/2330 train_time:119495ms step_avg:60.72ms
step:1969/2330 train_time:119555ms step_avg:60.72ms
step:1970/2330 train_time:119617ms step_avg:60.72ms
step:1971/2330 train_time:119678ms step_avg:60.72ms
step:1972/2330 train_time:119741ms step_avg:60.72ms
step:1973/2330 train_time:119801ms step_avg:60.72ms
step:1974/2330 train_time:119863ms step_avg:60.72ms
step:1975/2330 train_time:119924ms step_avg:60.72ms
step:1976/2330 train_time:119986ms step_avg:60.72ms
step:1977/2330 train_time:120046ms step_avg:60.72ms
step:1978/2330 train_time:120109ms step_avg:60.72ms
step:1979/2330 train_time:120170ms step_avg:60.72ms
step:1980/2330 train_time:120232ms step_avg:60.72ms
step:1981/2330 train_time:120292ms step_avg:60.72ms
step:1982/2330 train_time:120355ms step_avg:60.72ms
step:1983/2330 train_time:120415ms step_avg:60.72ms
step:1984/2330 train_time:120477ms step_avg:60.72ms
step:1985/2330 train_time:120537ms step_avg:60.72ms
step:1986/2330 train_time:120599ms step_avg:60.72ms
step:1987/2330 train_time:120660ms step_avg:60.72ms
step:1988/2330 train_time:120722ms step_avg:60.73ms
step:1989/2330 train_time:120783ms step_avg:60.73ms
step:1990/2330 train_time:120845ms step_avg:60.73ms
step:1991/2330 train_time:120905ms step_avg:60.73ms
step:1992/2330 train_time:120968ms step_avg:60.73ms
step:1993/2330 train_time:121028ms step_avg:60.73ms
step:1994/2330 train_time:121091ms step_avg:60.73ms
step:1995/2330 train_time:121151ms step_avg:60.73ms
step:1996/2330 train_time:121213ms step_avg:60.73ms
step:1997/2330 train_time:121274ms step_avg:60.73ms
step:1998/2330 train_time:121335ms step_avg:60.73ms
step:1999/2330 train_time:121396ms step_avg:60.73ms
step:2000/2330 train_time:121459ms step_avg:60.73ms
step:2000/2330 val_loss:3.3378 train_time:121523ms step_avg:60.76ms
step:2001/2330 train_time:121544ms step_avg:60.74ms
step:2002/2330 train_time:121584ms step_avg:60.73ms
step:2003/2330 train_time:121651ms step_avg:60.73ms
step:2004/2330 train_time:121715ms step_avg:60.74ms
step:2005/2330 train_time:121775ms step_avg:60.74ms
step:2006/2330 train_time:121837ms step_avg:60.74ms
step:2007/2330 train_time:121897ms step_avg:60.74ms
step:2008/2330 train_time:121959ms step_avg:60.74ms
step:2009/2330 train_time:122019ms step_avg:60.74ms
step:2010/2330 train_time:122080ms step_avg:60.74ms
step:2011/2330 train_time:122140ms step_avg:60.74ms
step:2012/2330 train_time:122202ms step_avg:60.74ms
step:2013/2330 train_time:122261ms step_avg:60.74ms
step:2014/2330 train_time:122323ms step_avg:60.74ms
step:2015/2330 train_time:122382ms step_avg:60.74ms
step:2016/2330 train_time:122445ms step_avg:60.74ms
step:2017/2330 train_time:122507ms step_avg:60.74ms
step:2018/2330 train_time:122571ms step_avg:60.74ms
step:2019/2330 train_time:122633ms step_avg:60.74ms
step:2020/2330 train_time:122697ms step_avg:60.74ms
step:2021/2330 train_time:122758ms step_avg:60.74ms
step:2022/2330 train_time:122821ms step_avg:60.74ms
step:2023/2330 train_time:122881ms step_avg:60.74ms
step:2024/2330 train_time:122944ms step_avg:60.74ms
step:2025/2330 train_time:123004ms step_avg:60.74ms
step:2026/2330 train_time:123066ms step_avg:60.74ms
step:2027/2330 train_time:123126ms step_avg:60.74ms
step:2028/2330 train_time:123188ms step_avg:60.74ms
step:2029/2330 train_time:123248ms step_avg:60.74ms
step:2030/2330 train_time:123310ms step_avg:60.74ms
step:2031/2330 train_time:123370ms step_avg:60.74ms
step:2032/2330 train_time:123432ms step_avg:60.74ms
step:2033/2330 train_time:123493ms step_avg:60.74ms
step:2034/2330 train_time:123556ms step_avg:60.75ms
step:2035/2330 train_time:123617ms step_avg:60.75ms
step:2036/2330 train_time:123679ms step_avg:60.75ms
step:2037/2330 train_time:123741ms step_avg:60.75ms
step:2038/2330 train_time:123804ms step_avg:60.75ms
step:2039/2330 train_time:123865ms step_avg:60.75ms
step:2040/2330 train_time:123928ms step_avg:60.75ms
step:2041/2330 train_time:123989ms step_avg:60.75ms
step:2042/2330 train_time:124051ms step_avg:60.75ms
step:2043/2330 train_time:124111ms step_avg:60.75ms
step:2044/2330 train_time:124173ms step_avg:60.75ms
step:2045/2330 train_time:124233ms step_avg:60.75ms
step:2046/2330 train_time:124295ms step_avg:60.75ms
step:2047/2330 train_time:124355ms step_avg:60.75ms
step:2048/2330 train_time:124416ms step_avg:60.75ms
step:2049/2330 train_time:124476ms step_avg:60.75ms
step:2050/2330 train_time:124540ms step_avg:60.75ms
step:2051/2330 train_time:124601ms step_avg:60.75ms
step:2052/2330 train_time:124664ms step_avg:60.75ms
step:2053/2330 train_time:124725ms step_avg:60.75ms
step:2054/2330 train_time:124789ms step_avg:60.75ms
step:2055/2330 train_time:124850ms step_avg:60.75ms
step:2056/2330 train_time:124913ms step_avg:60.76ms
step:2057/2330 train_time:124973ms step_avg:60.75ms
step:2058/2330 train_time:125035ms step_avg:60.76ms
step:2059/2330 train_time:125096ms step_avg:60.76ms
step:2060/2330 train_time:125158ms step_avg:60.76ms
step:2061/2330 train_time:125218ms step_avg:60.76ms
step:2062/2330 train_time:125279ms step_avg:60.76ms
step:2063/2330 train_time:125340ms step_avg:60.76ms
step:2064/2330 train_time:125403ms step_avg:60.76ms
step:2065/2330 train_time:125463ms step_avg:60.76ms
step:2066/2330 train_time:125526ms step_avg:60.76ms
step:2067/2330 train_time:125588ms step_avg:60.76ms
step:2068/2330 train_time:125651ms step_avg:60.76ms
step:2069/2330 train_time:125712ms step_avg:60.76ms
step:2070/2330 train_time:125775ms step_avg:60.76ms
step:2071/2330 train_time:125835ms step_avg:60.76ms
step:2072/2330 train_time:125897ms step_avg:60.76ms
step:2073/2330 train_time:125957ms step_avg:60.76ms
step:2074/2330 train_time:126020ms step_avg:60.76ms
step:2075/2330 train_time:126081ms step_avg:60.76ms
step:2076/2330 train_time:126143ms step_avg:60.76ms
step:2077/2330 train_time:126204ms step_avg:60.76ms
step:2078/2330 train_time:126266ms step_avg:60.76ms
step:2079/2330 train_time:126326ms step_avg:60.76ms
step:2080/2330 train_time:126389ms step_avg:60.76ms
step:2081/2330 train_time:126449ms step_avg:60.76ms
step:2082/2330 train_time:126512ms step_avg:60.76ms
step:2083/2330 train_time:126572ms step_avg:60.76ms
step:2084/2330 train_time:126634ms step_avg:60.76ms
step:2085/2330 train_time:126695ms step_avg:60.76ms
step:2086/2330 train_time:126757ms step_avg:60.77ms
step:2087/2330 train_time:126817ms step_avg:60.77ms
step:2088/2330 train_time:126880ms step_avg:60.77ms
step:2089/2330 train_time:126941ms step_avg:60.77ms
step:2090/2330 train_time:127004ms step_avg:60.77ms
step:2091/2330 train_time:127064ms step_avg:60.77ms
step:2092/2330 train_time:127127ms step_avg:60.77ms
step:2093/2330 train_time:127187ms step_avg:60.77ms
step:2094/2330 train_time:127250ms step_avg:60.77ms
step:2095/2330 train_time:127309ms step_avg:60.77ms
step:2096/2330 train_time:127372ms step_avg:60.77ms
step:2097/2330 train_time:127433ms step_avg:60.77ms
step:2098/2330 train_time:127495ms step_avg:60.77ms
step:2099/2330 train_time:127555ms step_avg:60.77ms
step:2100/2330 train_time:127617ms step_avg:60.77ms
step:2101/2330 train_time:127677ms step_avg:60.77ms
step:2102/2330 train_time:127740ms step_avg:60.77ms
step:2103/2330 train_time:127801ms step_avg:60.77ms
step:2104/2330 train_time:127864ms step_avg:60.77ms
step:2105/2330 train_time:127924ms step_avg:60.77ms
step:2106/2330 train_time:127986ms step_avg:60.77ms
step:2107/2330 train_time:128047ms step_avg:60.77ms
step:2108/2330 train_time:128109ms step_avg:60.77ms
step:2109/2330 train_time:128169ms step_avg:60.77ms
step:2110/2330 train_time:128232ms step_avg:60.77ms
step:2111/2330 train_time:128292ms step_avg:60.77ms
step:2112/2330 train_time:128355ms step_avg:60.77ms
step:2113/2330 train_time:128415ms step_avg:60.77ms
step:2114/2330 train_time:128477ms step_avg:60.77ms
step:2115/2330 train_time:128537ms step_avg:60.77ms
step:2116/2330 train_time:128599ms step_avg:60.77ms
step:2117/2330 train_time:128660ms step_avg:60.77ms
step:2118/2330 train_time:128723ms step_avg:60.78ms
step:2119/2330 train_time:128784ms step_avg:60.78ms
step:2120/2330 train_time:128846ms step_avg:60.78ms
step:2121/2330 train_time:128907ms step_avg:60.78ms
step:2122/2330 train_time:128970ms step_avg:60.78ms
step:2123/2330 train_time:129031ms step_avg:60.78ms
step:2124/2330 train_time:129093ms step_avg:60.78ms
step:2125/2330 train_time:129153ms step_avg:60.78ms
step:2126/2330 train_time:129216ms step_avg:60.78ms
step:2127/2330 train_time:129275ms step_avg:60.78ms
step:2128/2330 train_time:129338ms step_avg:60.78ms
step:2129/2330 train_time:129398ms step_avg:60.78ms
step:2130/2330 train_time:129460ms step_avg:60.78ms
step:2131/2330 train_time:129521ms step_avg:60.78ms
step:2132/2330 train_time:129583ms step_avg:60.78ms
step:2133/2330 train_time:129643ms step_avg:60.78ms
step:2134/2330 train_time:129706ms step_avg:60.78ms
step:2135/2330 train_time:129766ms step_avg:60.78ms
step:2136/2330 train_time:129829ms step_avg:60.78ms
step:2137/2330 train_time:129890ms step_avg:60.78ms
step:2138/2330 train_time:129953ms step_avg:60.78ms
step:2139/2330 train_time:130013ms step_avg:60.78ms
step:2140/2330 train_time:130076ms step_avg:60.78ms
step:2141/2330 train_time:130136ms step_avg:60.78ms
step:2142/2330 train_time:130199ms step_avg:60.78ms
step:2143/2330 train_time:130258ms step_avg:60.78ms
step:2144/2330 train_time:130320ms step_avg:60.78ms
step:2145/2330 train_time:130381ms step_avg:60.78ms
step:2146/2330 train_time:130443ms step_avg:60.78ms
step:2147/2330 train_time:130505ms step_avg:60.78ms
step:2148/2330 train_time:130568ms step_avg:60.79ms
step:2149/2330 train_time:130628ms step_avg:60.79ms
step:2150/2330 train_time:130691ms step_avg:60.79ms
step:2151/2330 train_time:130751ms step_avg:60.79ms
step:2152/2330 train_time:130814ms step_avg:60.79ms
step:2153/2330 train_time:130875ms step_avg:60.79ms
step:2154/2330 train_time:130937ms step_avg:60.79ms
step:2155/2330 train_time:130996ms step_avg:60.79ms
step:2156/2330 train_time:131059ms step_avg:60.79ms
step:2157/2330 train_time:131119ms step_avg:60.79ms
step:2158/2330 train_time:131182ms step_avg:60.79ms
step:2159/2330 train_time:131242ms step_avg:60.79ms
step:2160/2330 train_time:131305ms step_avg:60.79ms
step:2161/2330 train_time:131366ms step_avg:60.79ms
step:2162/2330 train_time:131428ms step_avg:60.79ms
step:2163/2330 train_time:131489ms step_avg:60.79ms
step:2164/2330 train_time:131551ms step_avg:60.79ms
step:2165/2330 train_time:131612ms step_avg:60.79ms
step:2166/2330 train_time:131674ms step_avg:60.79ms
step:2167/2330 train_time:131734ms step_avg:60.79ms
step:2168/2330 train_time:131796ms step_avg:60.79ms
step:2169/2330 train_time:131856ms step_avg:60.79ms
step:2170/2330 train_time:131919ms step_avg:60.79ms
step:2171/2330 train_time:131979ms step_avg:60.79ms
step:2172/2330 train_time:132042ms step_avg:60.79ms
step:2173/2330 train_time:132103ms step_avg:60.79ms
step:2174/2330 train_time:132165ms step_avg:60.79ms
step:2175/2330 train_time:132226ms step_avg:60.79ms
step:2176/2330 train_time:132288ms step_avg:60.79ms
step:2177/2330 train_time:132349ms step_avg:60.79ms
step:2178/2330 train_time:132412ms step_avg:60.80ms
step:2179/2330 train_time:132472ms step_avg:60.79ms
step:2180/2330 train_time:132534ms step_avg:60.80ms
step:2181/2330 train_time:132595ms step_avg:60.80ms
step:2182/2330 train_time:132657ms step_avg:60.80ms
step:2183/2330 train_time:132718ms step_avg:60.80ms
step:2184/2330 train_time:132780ms step_avg:60.80ms
step:2185/2330 train_time:132841ms step_avg:60.80ms
step:2186/2330 train_time:132904ms step_avg:60.80ms
step:2187/2330 train_time:132964ms step_avg:60.80ms
step:2188/2330 train_time:133027ms step_avg:60.80ms
step:2189/2330 train_time:133087ms step_avg:60.80ms
step:2190/2330 train_time:133150ms step_avg:60.80ms
step:2191/2330 train_time:133211ms step_avg:60.80ms
step:2192/2330 train_time:133273ms step_avg:60.80ms
step:2193/2330 train_time:133333ms step_avg:60.80ms
step:2194/2330 train_time:133395ms step_avg:60.80ms
step:2195/2330 train_time:133455ms step_avg:60.80ms
step:2196/2330 train_time:133518ms step_avg:60.80ms
step:2197/2330 train_time:133578ms step_avg:60.80ms
step:2198/2330 train_time:133640ms step_avg:60.80ms
step:2199/2330 train_time:133700ms step_avg:60.80ms
step:2200/2330 train_time:133763ms step_avg:60.80ms
step:2201/2330 train_time:133824ms step_avg:60.80ms
step:2202/2330 train_time:133886ms step_avg:60.80ms
step:2203/2330 train_time:133947ms step_avg:60.80ms
step:2204/2330 train_time:134010ms step_avg:60.80ms
step:2205/2330 train_time:134071ms step_avg:60.80ms
step:2206/2330 train_time:134133ms step_avg:60.80ms
step:2207/2330 train_time:134194ms step_avg:60.80ms
step:2208/2330 train_time:134256ms step_avg:60.80ms
step:2209/2330 train_time:134317ms step_avg:60.80ms
step:2210/2330 train_time:134379ms step_avg:60.80ms
step:2211/2330 train_time:134439ms step_avg:60.80ms
step:2212/2330 train_time:134502ms step_avg:60.81ms
step:2213/2330 train_time:134563ms step_avg:60.81ms
step:2214/2330 train_time:134625ms step_avg:60.81ms
step:2215/2330 train_time:134686ms step_avg:60.81ms
step:2216/2330 train_time:134749ms step_avg:60.81ms
step:2217/2330 train_time:134809ms step_avg:60.81ms
step:2218/2330 train_time:134872ms step_avg:60.81ms
step:2219/2330 train_time:134932ms step_avg:60.81ms
step:2220/2330 train_time:134994ms step_avg:60.81ms
step:2221/2330 train_time:135054ms step_avg:60.81ms
step:2222/2330 train_time:135116ms step_avg:60.81ms
step:2223/2330 train_time:135176ms step_avg:60.81ms
step:2224/2330 train_time:135238ms step_avg:60.81ms
step:2225/2330 train_time:135299ms step_avg:60.81ms
step:2226/2330 train_time:135361ms step_avg:60.81ms
step:2227/2330 train_time:135422ms step_avg:60.81ms
step:2228/2330 train_time:135484ms step_avg:60.81ms
step:2229/2330 train_time:135545ms step_avg:60.81ms
step:2230/2330 train_time:135608ms step_avg:60.81ms
step:2231/2330 train_time:135669ms step_avg:60.81ms
step:2232/2330 train_time:135731ms step_avg:60.81ms
step:2233/2330 train_time:135791ms step_avg:60.81ms
step:2234/2330 train_time:135854ms step_avg:60.81ms
step:2235/2330 train_time:135914ms step_avg:60.81ms
step:2236/2330 train_time:135977ms step_avg:60.81ms
step:2237/2330 train_time:136037ms step_avg:60.81ms
step:2238/2330 train_time:136099ms step_avg:60.81ms
step:2239/2330 train_time:136159ms step_avg:60.81ms
step:2240/2330 train_time:136222ms step_avg:60.81ms
step:2241/2330 train_time:136282ms step_avg:60.81ms
step:2242/2330 train_time:136345ms step_avg:60.81ms
step:2243/2330 train_time:136406ms step_avg:60.81ms
step:2244/2330 train_time:136469ms step_avg:60.81ms
step:2245/2330 train_time:136529ms step_avg:60.81ms
step:2246/2330 train_time:136592ms step_avg:60.82ms
step:2247/2330 train_time:136652ms step_avg:60.82ms
step:2248/2330 train_time:136715ms step_avg:60.82ms
step:2249/2330 train_time:136775ms step_avg:60.82ms
step:2250/2330 train_time:136837ms step_avg:60.82ms
step:2250/2330 val_loss:3.2967 train_time:136902ms step_avg:60.85ms
step:2251/2330 train_time:136926ms step_avg:60.83ms
step:2252/2330 train_time:136966ms step_avg:60.82ms
step:2253/2330 train_time:137032ms step_avg:60.82ms
step:2254/2330 train_time:137098ms step_avg:60.82ms
step:2255/2330 train_time:137159ms step_avg:60.82ms
step:2256/2330 train_time:137222ms step_avg:60.83ms
step:2257/2330 train_time:137282ms step_avg:60.83ms
step:2258/2330 train_time:137344ms step_avg:60.83ms
step:2259/2330 train_time:137404ms step_avg:60.82ms
step:2260/2330 train_time:137465ms step_avg:60.83ms
step:2261/2330 train_time:137525ms step_avg:60.82ms
step:2262/2330 train_time:137587ms step_avg:60.83ms
step:2263/2330 train_time:137646ms step_avg:60.82ms
step:2264/2330 train_time:137708ms step_avg:60.83ms
step:2265/2330 train_time:137768ms step_avg:60.82ms
step:2266/2330 train_time:137831ms step_avg:60.83ms
step:2267/2330 train_time:137892ms step_avg:60.83ms
step:2268/2330 train_time:137956ms step_avg:60.83ms
step:2269/2330 train_time:138018ms step_avg:60.83ms
step:2270/2330 train_time:138082ms step_avg:60.83ms
step:2271/2330 train_time:138143ms step_avg:60.83ms
step:2272/2330 train_time:138206ms step_avg:60.83ms
step:2273/2330 train_time:138266ms step_avg:60.83ms
step:2274/2330 train_time:138328ms step_avg:60.83ms
step:2275/2330 train_time:138388ms step_avg:60.83ms
step:2276/2330 train_time:138449ms step_avg:60.83ms
step:2277/2330 train_time:138509ms step_avg:60.83ms
step:2278/2330 train_time:138571ms step_avg:60.83ms
step:2279/2330 train_time:138630ms step_avg:60.83ms
step:2280/2330 train_time:138692ms step_avg:60.83ms
step:2281/2330 train_time:138752ms step_avg:60.83ms
step:2282/2330 train_time:138814ms step_avg:60.83ms
step:2283/2330 train_time:138873ms step_avg:60.83ms
step:2284/2330 train_time:138937ms step_avg:60.83ms
step:2285/2330 train_time:138998ms step_avg:60.83ms
step:2286/2330 train_time:139061ms step_avg:60.83ms
step:2287/2330 train_time:139123ms step_avg:60.83ms
step:2288/2330 train_time:139186ms step_avg:60.83ms
step:2289/2330 train_time:139247ms step_avg:60.83ms
step:2290/2330 train_time:139309ms step_avg:60.83ms
step:2291/2330 train_time:139369ms step_avg:60.83ms
step:2292/2330 train_time:139431ms step_avg:60.83ms
step:2293/2330 train_time:139490ms step_avg:60.83ms
step:2294/2330 train_time:139552ms step_avg:60.83ms
step:2295/2330 train_time:139612ms step_avg:60.83ms
step:2296/2330 train_time:139674ms step_avg:60.83ms
step:2297/2330 train_time:139734ms step_avg:60.83ms
step:2298/2330 train_time:139795ms step_avg:60.83ms
step:2299/2330 train_time:139855ms step_avg:60.83ms
step:2300/2330 train_time:139918ms step_avg:60.83ms
step:2301/2330 train_time:139979ms step_avg:60.83ms
step:2302/2330 train_time:140042ms step_avg:60.83ms
step:2303/2330 train_time:140103ms step_avg:60.84ms
step:2304/2330 train_time:140166ms step_avg:60.84ms
step:2305/2330 train_time:140227ms step_avg:60.84ms
step:2306/2330 train_time:140289ms step_avg:60.84ms
step:2307/2330 train_time:140349ms step_avg:60.84ms
step:2308/2330 train_time:140411ms step_avg:60.84ms
step:2309/2330 train_time:140471ms step_avg:60.84ms
step:2310/2330 train_time:140534ms step_avg:60.84ms
step:2311/2330 train_time:140593ms step_avg:60.84ms
step:2312/2330 train_time:140656ms step_avg:60.84ms
step:2313/2330 train_time:140715ms step_avg:60.84ms
step:2314/2330 train_time:140777ms step_avg:60.84ms
step:2315/2330 train_time:140837ms step_avg:60.84ms
step:2316/2330 train_time:140900ms step_avg:60.84ms
step:2317/2330 train_time:140960ms step_avg:60.84ms
step:2318/2330 train_time:141023ms step_avg:60.84ms
step:2319/2330 train_time:141084ms step_avg:60.84ms
step:2320/2330 train_time:141148ms step_avg:60.84ms
step:2321/2330 train_time:141208ms step_avg:60.84ms
step:2322/2330 train_time:141271ms step_avg:60.84ms
step:2323/2330 train_time:141331ms step_avg:60.84ms
step:2324/2330 train_time:141392ms step_avg:60.84ms
step:2325/2330 train_time:141453ms step_avg:60.84ms
step:2326/2330 train_time:141515ms step_avg:60.84ms
step:2327/2330 train_time:141575ms step_avg:60.84ms
step:2328/2330 train_time:141637ms step_avg:60.84ms
step:2329/2330 train_time:141697ms step_avg:60.84ms
step:2330/2330 train_time:141759ms step_avg:60.84ms
step:2330/2330 val_loss:3.2823 train_time:141824ms step_avg:60.87ms
peak memory allocated: 29226 MiB reserved: 44076 MiB
