import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_muon_lr7e-4"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=7e-4, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 03:57:13 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   34C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   32C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:66ms step_avg:65.66ms
step:2/2330 train_time:152ms step_avg:76.22ms
step:3/2330 train_time:170ms step_avg:56.59ms
step:4/2330 train_time:183ms step_avg:45.66ms
step:5/2330 train_time:194ms step_avg:38.74ms
step:6/2330 train_time:223ms step_avg:37.19ms
step:7/2330 train_time:256ms step_avg:36.64ms
step:8/2330 train_time:301ms step_avg:37.61ms
step:9/2330 train_time:336ms step_avg:37.31ms
step:10/2330 train_time:380ms step_avg:37.96ms
step:11/2330 train_time:414ms step_avg:37.66ms
step:12/2330 train_time:458ms step_avg:38.18ms
step:13/2330 train_time:493ms step_avg:37.89ms
step:14/2330 train_time:536ms step_avg:38.31ms
step:15/2330 train_time:571ms step_avg:38.08ms
step:16/2330 train_time:616ms step_avg:38.49ms
step:17/2330 train_time:650ms step_avg:38.26ms
step:18/2330 train_time:694ms step_avg:38.57ms
step:19/2330 train_time:729ms step_avg:38.38ms
step:20/2330 train_time:774ms step_avg:38.70ms
step:21/2330 train_time:809ms step_avg:38.53ms
step:22/2330 train_time:853ms step_avg:38.78ms
step:23/2330 train_time:888ms step_avg:38.60ms
step:24/2330 train_time:932ms step_avg:38.85ms
step:25/2330 train_time:967ms step_avg:38.68ms
step:26/2330 train_time:1016ms step_avg:39.07ms
step:27/2330 train_time:1055ms step_avg:39.06ms
step:28/2330 train_time:1103ms step_avg:39.38ms
step:29/2330 train_time:1140ms step_avg:39.29ms
step:30/2330 train_time:1186ms step_avg:39.52ms
step:31/2330 train_time:1222ms step_avg:39.41ms
step:32/2330 train_time:1267ms step_avg:39.59ms
step:33/2330 train_time:1304ms step_avg:39.50ms
step:34/2330 train_time:1350ms step_avg:39.69ms
step:35/2330 train_time:1386ms step_avg:39.59ms
step:36/2330 train_time:1430ms step_avg:39.73ms
step:37/2330 train_time:1465ms step_avg:39.59ms
step:38/2330 train_time:1510ms step_avg:39.73ms
step:39/2330 train_time:1545ms step_avg:39.61ms
step:40/2330 train_time:1589ms step_avg:39.73ms
step:41/2330 train_time:1624ms step_avg:39.62ms
step:42/2330 train_time:1670ms step_avg:39.75ms
step:43/2330 train_time:1705ms step_avg:39.66ms
step:44/2330 train_time:1750ms step_avg:39.77ms
step:45/2330 train_time:1784ms step_avg:39.65ms
step:46/2330 train_time:1828ms step_avg:39.75ms
step:47/2330 train_time:1863ms step_avg:39.64ms
step:48/2330 train_time:1909ms step_avg:39.76ms
step:49/2330 train_time:1946ms step_avg:39.71ms
step:50/2330 train_time:1993ms step_avg:39.85ms
step:51/2330 train_time:2030ms step_avg:39.80ms
step:52/2330 train_time:2076ms step_avg:39.92ms
step:53/2330 train_time:2112ms step_avg:39.85ms
step:54/2330 train_time:2158ms step_avg:39.96ms
step:55/2330 train_time:2193ms step_avg:39.88ms
step:56/2330 train_time:2239ms step_avg:39.97ms
step:57/2330 train_time:2275ms step_avg:39.92ms
step:58/2330 train_time:2322ms step_avg:40.03ms
step:59/2330 train_time:2358ms step_avg:39.97ms
step:60/2330 train_time:2403ms step_avg:40.04ms
step:61/2330 train_time:2438ms step_avg:39.97ms
step:62/2330 train_time:2483ms step_avg:40.05ms
step:63/2330 train_time:2519ms step_avg:39.98ms
step:64/2330 train_time:2564ms step_avg:40.06ms
step:65/2330 train_time:2599ms step_avg:39.98ms
step:66/2330 train_time:2644ms step_avg:40.06ms
step:67/2330 train_time:2679ms step_avg:39.99ms
step:68/2330 train_time:2724ms step_avg:40.06ms
step:69/2330 train_time:2759ms step_avg:39.99ms
step:70/2330 train_time:2804ms step_avg:40.05ms
step:71/2330 train_time:2839ms step_avg:39.99ms
step:72/2330 train_time:2884ms step_avg:40.05ms
step:73/2330 train_time:2920ms step_avg:40.00ms
step:74/2330 train_time:2965ms step_avg:40.07ms
step:75/2330 train_time:3002ms step_avg:40.02ms
step:76/2330 train_time:3049ms step_avg:40.12ms
step:77/2330 train_time:3086ms step_avg:40.08ms
step:78/2330 train_time:3132ms step_avg:40.15ms
step:79/2330 train_time:3168ms step_avg:40.10ms
step:80/2330 train_time:3215ms step_avg:40.19ms
step:81/2330 train_time:3250ms step_avg:40.13ms
step:82/2330 train_time:3295ms step_avg:40.19ms
step:83/2330 train_time:3331ms step_avg:40.13ms
step:84/2330 train_time:3377ms step_avg:40.20ms
step:85/2330 train_time:3412ms step_avg:40.15ms
step:86/2330 train_time:3458ms step_avg:40.21ms
step:87/2330 train_time:3493ms step_avg:40.15ms
step:88/2330 train_time:3539ms step_avg:40.21ms
step:89/2330 train_time:3573ms step_avg:40.15ms
step:90/2330 train_time:3619ms step_avg:40.21ms
step:91/2330 train_time:3655ms step_avg:40.16ms
step:92/2330 train_time:3700ms step_avg:40.21ms
step:93/2330 train_time:3735ms step_avg:40.16ms
step:94/2330 train_time:3779ms step_avg:40.20ms
step:95/2330 train_time:3816ms step_avg:40.17ms
step:96/2330 train_time:3861ms step_avg:40.22ms
step:97/2330 train_time:3896ms step_avg:40.17ms
step:98/2330 train_time:3941ms step_avg:40.22ms
step:99/2330 train_time:3977ms step_avg:40.17ms
step:100/2330 train_time:4022ms step_avg:40.22ms
step:101/2330 train_time:4059ms step_avg:40.19ms
step:102/2330 train_time:4105ms step_avg:40.25ms
step:103/2330 train_time:4142ms step_avg:40.21ms
step:104/2330 train_time:4188ms step_avg:40.27ms
step:105/2330 train_time:4225ms step_avg:40.24ms
step:106/2330 train_time:4270ms step_avg:40.29ms
step:107/2330 train_time:4307ms step_avg:40.25ms
step:108/2330 train_time:4352ms step_avg:40.30ms
step:109/2330 train_time:4389ms step_avg:40.26ms
step:110/2330 train_time:4433ms step_avg:40.30ms
step:111/2330 train_time:4469ms step_avg:40.26ms
step:112/2330 train_time:4514ms step_avg:40.30ms
step:113/2330 train_time:4550ms step_avg:40.26ms
step:114/2330 train_time:4595ms step_avg:40.31ms
step:115/2330 train_time:4631ms step_avg:40.27ms
step:116/2330 train_time:4675ms step_avg:40.30ms
step:117/2330 train_time:4711ms step_avg:40.26ms
step:118/2330 train_time:4756ms step_avg:40.30ms
step:119/2330 train_time:4791ms step_avg:40.26ms
step:120/2330 train_time:4836ms step_avg:40.30ms
step:121/2330 train_time:4871ms step_avg:40.26ms
step:122/2330 train_time:4917ms step_avg:40.31ms
step:123/2330 train_time:4954ms step_avg:40.28ms
step:124/2330 train_time:4999ms step_avg:40.32ms
step:125/2330 train_time:5034ms step_avg:40.27ms
step:126/2330 train_time:5080ms step_avg:40.32ms
step:127/2330 train_time:5116ms step_avg:40.29ms
step:128/2330 train_time:5162ms step_avg:40.32ms
step:129/2330 train_time:5197ms step_avg:40.29ms
step:130/2330 train_time:5243ms step_avg:40.33ms
step:131/2330 train_time:5279ms step_avg:40.30ms
step:132/2330 train_time:5324ms step_avg:40.33ms
step:133/2330 train_time:5359ms step_avg:40.30ms
step:134/2330 train_time:5405ms step_avg:40.34ms
step:135/2330 train_time:5442ms step_avg:40.31ms
step:136/2330 train_time:5488ms step_avg:40.35ms
step:137/2330 train_time:5525ms step_avg:40.33ms
step:138/2330 train_time:5570ms step_avg:40.36ms
step:139/2330 train_time:5605ms step_avg:40.33ms
step:140/2330 train_time:5651ms step_avg:40.36ms
step:141/2330 train_time:5686ms step_avg:40.32ms
step:142/2330 train_time:5731ms step_avg:40.36ms
step:143/2330 train_time:5766ms step_avg:40.32ms
step:144/2330 train_time:5813ms step_avg:40.37ms
step:145/2330 train_time:5849ms step_avg:40.34ms
step:146/2330 train_time:5894ms step_avg:40.37ms
step:147/2330 train_time:5930ms step_avg:40.34ms
step:148/2330 train_time:5975ms step_avg:40.37ms
step:149/2330 train_time:6011ms step_avg:40.34ms
step:150/2330 train_time:6057ms step_avg:40.38ms
step:151/2330 train_time:6093ms step_avg:40.35ms
step:152/2330 train_time:6139ms step_avg:40.39ms
step:153/2330 train_time:6174ms step_avg:40.35ms
step:154/2330 train_time:6220ms step_avg:40.39ms
step:155/2330 train_time:6257ms step_avg:40.37ms
step:156/2330 train_time:6301ms step_avg:40.39ms
step:157/2330 train_time:6337ms step_avg:40.37ms
step:158/2330 train_time:6382ms step_avg:40.39ms
step:159/2330 train_time:6418ms step_avg:40.36ms
step:160/2330 train_time:6463ms step_avg:40.40ms
step:161/2330 train_time:6500ms step_avg:40.37ms
step:162/2330 train_time:6545ms step_avg:40.40ms
step:163/2330 train_time:6581ms step_avg:40.38ms
step:164/2330 train_time:6627ms step_avg:40.41ms
step:165/2330 train_time:6662ms step_avg:40.38ms
step:166/2330 train_time:6708ms step_avg:40.41ms
step:167/2330 train_time:6745ms step_avg:40.39ms
step:168/2330 train_time:6789ms step_avg:40.41ms
step:169/2330 train_time:6826ms step_avg:40.39ms
step:170/2330 train_time:6871ms step_avg:40.42ms
step:171/2330 train_time:6907ms step_avg:40.39ms
step:172/2330 train_time:6952ms step_avg:40.42ms
step:173/2330 train_time:6989ms step_avg:40.40ms
step:174/2330 train_time:7034ms step_avg:40.43ms
step:175/2330 train_time:7070ms step_avg:40.40ms
step:176/2330 train_time:7115ms step_avg:40.43ms
step:177/2330 train_time:7152ms step_avg:40.40ms
step:178/2330 train_time:7197ms step_avg:40.43ms
step:179/2330 train_time:7233ms step_avg:40.41ms
step:180/2330 train_time:7278ms step_avg:40.43ms
step:181/2330 train_time:7314ms step_avg:40.41ms
step:182/2330 train_time:7360ms step_avg:40.44ms
step:183/2330 train_time:7395ms step_avg:40.41ms
step:184/2330 train_time:7440ms step_avg:40.44ms
step:185/2330 train_time:7476ms step_avg:40.41ms
step:186/2330 train_time:7521ms step_avg:40.43ms
step:187/2330 train_time:7557ms step_avg:40.41ms
step:188/2330 train_time:7601ms step_avg:40.43ms
step:189/2330 train_time:7637ms step_avg:40.41ms
step:190/2330 train_time:7682ms step_avg:40.43ms
step:191/2330 train_time:7718ms step_avg:40.41ms
step:192/2330 train_time:7763ms step_avg:40.43ms
step:193/2330 train_time:7800ms step_avg:40.41ms
step:194/2330 train_time:7845ms step_avg:40.44ms
step:195/2330 train_time:7881ms step_avg:40.42ms
step:196/2330 train_time:7927ms step_avg:40.44ms
step:197/2330 train_time:7964ms step_avg:40.42ms
step:198/2330 train_time:8010ms step_avg:40.46ms
step:199/2330 train_time:8046ms step_avg:40.43ms
step:200/2330 train_time:8091ms step_avg:40.46ms
step:201/2330 train_time:8127ms step_avg:40.43ms
step:202/2330 train_time:8172ms step_avg:40.45ms
step:203/2330 train_time:8207ms step_avg:40.43ms
step:204/2330 train_time:8253ms step_avg:40.45ms
step:205/2330 train_time:8289ms step_avg:40.44ms
step:206/2330 train_time:8335ms step_avg:40.46ms
step:207/2330 train_time:8371ms step_avg:40.44ms
step:208/2330 train_time:8415ms step_avg:40.46ms
step:209/2330 train_time:8451ms step_avg:40.44ms
step:210/2330 train_time:8496ms step_avg:40.46ms
step:211/2330 train_time:8532ms step_avg:40.44ms
step:212/2330 train_time:8579ms step_avg:40.46ms
step:213/2330 train_time:8615ms step_avg:40.44ms
step:214/2330 train_time:8660ms step_avg:40.47ms
step:215/2330 train_time:8695ms step_avg:40.44ms
step:216/2330 train_time:8741ms step_avg:40.47ms
step:217/2330 train_time:8777ms step_avg:40.45ms
step:218/2330 train_time:8822ms step_avg:40.47ms
step:219/2330 train_time:8857ms step_avg:40.44ms
step:220/2330 train_time:8902ms step_avg:40.47ms
step:221/2330 train_time:8938ms step_avg:40.44ms
step:222/2330 train_time:8984ms step_avg:40.47ms
step:223/2330 train_time:9019ms step_avg:40.45ms
step:224/2330 train_time:9065ms step_avg:40.47ms
step:225/2330 train_time:9101ms step_avg:40.45ms
step:226/2330 train_time:9148ms step_avg:40.48ms
step:227/2330 train_time:9184ms step_avg:40.46ms
step:228/2330 train_time:9230ms step_avg:40.48ms
step:229/2330 train_time:9266ms step_avg:40.46ms
step:230/2330 train_time:9312ms step_avg:40.49ms
step:231/2330 train_time:9347ms step_avg:40.46ms
step:232/2330 train_time:9393ms step_avg:40.49ms
step:233/2330 train_time:9429ms step_avg:40.47ms
step:234/2330 train_time:9473ms step_avg:40.48ms
step:235/2330 train_time:9510ms step_avg:40.47ms
step:236/2330 train_time:9555ms step_avg:40.49ms
step:237/2330 train_time:9590ms step_avg:40.46ms
step:238/2330 train_time:9636ms step_avg:40.49ms
step:239/2330 train_time:9671ms step_avg:40.47ms
step:240/2330 train_time:9716ms step_avg:40.48ms
step:241/2330 train_time:9752ms step_avg:40.46ms
step:242/2330 train_time:9798ms step_avg:40.49ms
step:243/2330 train_time:9834ms step_avg:40.47ms
step:244/2330 train_time:9880ms step_avg:40.49ms
step:245/2330 train_time:9916ms step_avg:40.47ms
step:246/2330 train_time:9961ms step_avg:40.49ms
step:247/2330 train_time:9997ms step_avg:40.47ms
step:248/2330 train_time:10042ms step_avg:40.49ms
step:249/2330 train_time:10077ms step_avg:40.47ms
step:250/2330 train_time:10123ms step_avg:40.49ms
step:250/2330 val_loss:5.6442 train_time:10212ms step_avg:40.85ms
step:251/2330 train_time:10226ms step_avg:40.74ms
step:252/2330 train_time:10238ms step_avg:40.63ms
step:253/2330 train_time:10249ms step_avg:40.51ms
step:254/2330 train_time:10287ms step_avg:40.50ms
step:255/2330 train_time:10322ms step_avg:40.48ms
step:256/2330 train_time:10366ms step_avg:40.49ms
step:257/2330 train_time:10401ms step_avg:40.47ms
step:258/2330 train_time:10445ms step_avg:40.48ms
step:259/2330 train_time:10479ms step_avg:40.46ms
step:260/2330 train_time:10526ms step_avg:40.48ms
step:261/2330 train_time:10565ms step_avg:40.48ms
step:262/2330 train_time:10612ms step_avg:40.51ms
step:263/2330 train_time:10649ms step_avg:40.49ms
step:264/2330 train_time:10695ms step_avg:40.51ms
step:265/2330 train_time:10731ms step_avg:40.49ms
step:266/2330 train_time:10776ms step_avg:40.51ms
step:267/2330 train_time:10811ms step_avg:40.49ms
step:268/2330 train_time:10857ms step_avg:40.51ms
step:269/2330 train_time:10892ms step_avg:40.49ms
step:270/2330 train_time:10937ms step_avg:40.51ms
step:271/2330 train_time:10973ms step_avg:40.49ms
step:272/2330 train_time:11017ms step_avg:40.50ms
step:273/2330 train_time:11052ms step_avg:40.48ms
step:274/2330 train_time:11096ms step_avg:40.50ms
step:275/2330 train_time:11132ms step_avg:40.48ms
step:276/2330 train_time:11178ms step_avg:40.50ms
step:277/2330 train_time:11215ms step_avg:40.49ms
step:278/2330 train_time:11260ms step_avg:40.50ms
step:279/2330 train_time:11296ms step_avg:40.49ms
step:280/2330 train_time:11341ms step_avg:40.50ms
step:281/2330 train_time:11376ms step_avg:40.48ms
step:282/2330 train_time:11420ms step_avg:40.50ms
step:283/2330 train_time:11456ms step_avg:40.48ms
step:284/2330 train_time:11503ms step_avg:40.50ms
step:285/2330 train_time:11542ms step_avg:40.50ms
step:286/2330 train_time:11589ms step_avg:40.52ms
step:287/2330 train_time:11625ms step_avg:40.50ms
step:288/2330 train_time:11671ms step_avg:40.52ms
step:289/2330 train_time:11707ms step_avg:40.51ms
step:290/2330 train_time:11753ms step_avg:40.53ms
step:291/2330 train_time:11789ms step_avg:40.51ms
step:292/2330 train_time:11835ms step_avg:40.53ms
step:293/2330 train_time:11870ms step_avg:40.51ms
step:294/2330 train_time:11915ms step_avg:40.53ms
step:295/2330 train_time:11950ms step_avg:40.51ms
step:296/2330 train_time:11995ms step_avg:40.52ms
step:297/2330 train_time:12030ms step_avg:40.50ms
step:298/2330 train_time:12075ms step_avg:40.52ms
step:299/2330 train_time:12110ms step_avg:40.50ms
step:300/2330 train_time:12155ms step_avg:40.52ms
step:301/2330 train_time:12191ms step_avg:40.50ms
step:302/2330 train_time:12236ms step_avg:40.52ms
step:303/2330 train_time:12272ms step_avg:40.50ms
step:304/2330 train_time:12316ms step_avg:40.51ms
step:305/2330 train_time:12352ms step_avg:40.50ms
step:306/2330 train_time:12396ms step_avg:40.51ms
step:307/2330 train_time:12432ms step_avg:40.50ms
step:308/2330 train_time:12479ms step_avg:40.51ms
step:309/2330 train_time:12515ms step_avg:40.50ms
step:310/2330 train_time:12561ms step_avg:40.52ms
step:311/2330 train_time:12599ms step_avg:40.51ms
step:312/2330 train_time:12646ms step_avg:40.53ms
step:313/2330 train_time:12681ms step_avg:40.52ms
step:314/2330 train_time:12728ms step_avg:40.53ms
step:315/2330 train_time:12763ms step_avg:40.52ms
step:316/2330 train_time:12808ms step_avg:40.53ms
step:317/2330 train_time:12844ms step_avg:40.52ms
step:318/2330 train_time:12889ms step_avg:40.53ms
step:319/2330 train_time:12924ms step_avg:40.52ms
step:320/2330 train_time:12970ms step_avg:40.53ms
step:321/2330 train_time:13005ms step_avg:40.51ms
step:322/2330 train_time:13050ms step_avg:40.53ms
step:323/2330 train_time:13085ms step_avg:40.51ms
step:324/2330 train_time:13131ms step_avg:40.53ms
step:325/2330 train_time:13167ms step_avg:40.51ms
step:326/2330 train_time:13212ms step_avg:40.53ms
step:327/2330 train_time:13248ms step_avg:40.51ms
step:328/2330 train_time:13293ms step_avg:40.53ms
step:329/2330 train_time:13328ms step_avg:40.51ms
step:330/2330 train_time:13373ms step_avg:40.53ms
step:331/2330 train_time:13409ms step_avg:40.51ms
step:332/2330 train_time:13454ms step_avg:40.52ms
step:333/2330 train_time:13490ms step_avg:40.51ms
step:334/2330 train_time:13535ms step_avg:40.52ms
step:335/2330 train_time:13571ms step_avg:40.51ms
step:336/2330 train_time:13617ms step_avg:40.53ms
step:337/2330 train_time:13653ms step_avg:40.51ms
step:338/2330 train_time:13700ms step_avg:40.53ms
step:339/2330 train_time:13736ms step_avg:40.52ms
step:340/2330 train_time:13782ms step_avg:40.54ms
step:341/2330 train_time:13818ms step_avg:40.52ms
step:342/2330 train_time:13864ms step_avg:40.54ms
step:343/2330 train_time:13900ms step_avg:40.53ms
step:344/2330 train_time:13946ms step_avg:40.54ms
step:345/2330 train_time:13981ms step_avg:40.53ms
step:346/2330 train_time:14027ms step_avg:40.54ms
step:347/2330 train_time:14062ms step_avg:40.53ms
step:348/2330 train_time:14108ms step_avg:40.54ms
step:349/2330 train_time:14143ms step_avg:40.52ms
step:350/2330 train_time:14189ms step_avg:40.54ms
step:351/2330 train_time:14224ms step_avg:40.52ms
step:352/2330 train_time:14268ms step_avg:40.54ms
step:353/2330 train_time:14305ms step_avg:40.52ms
step:354/2330 train_time:14350ms step_avg:40.54ms
step:355/2330 train_time:14385ms step_avg:40.52ms
step:356/2330 train_time:14431ms step_avg:40.54ms
step:357/2330 train_time:14466ms step_avg:40.52ms
step:358/2330 train_time:14513ms step_avg:40.54ms
step:359/2330 train_time:14548ms step_avg:40.52ms
step:360/2330 train_time:14593ms step_avg:40.54ms
step:361/2330 train_time:14629ms step_avg:40.52ms
step:362/2330 train_time:14675ms step_avg:40.54ms
step:363/2330 train_time:14711ms step_avg:40.53ms
step:364/2330 train_time:14756ms step_avg:40.54ms
step:365/2330 train_time:14792ms step_avg:40.53ms
step:366/2330 train_time:14838ms step_avg:40.54ms
step:367/2330 train_time:14873ms step_avg:40.53ms
step:368/2330 train_time:14919ms step_avg:40.54ms
step:369/2330 train_time:14955ms step_avg:40.53ms
step:370/2330 train_time:15002ms step_avg:40.55ms
step:371/2330 train_time:15039ms step_avg:40.54ms
step:372/2330 train_time:15084ms step_avg:40.55ms
step:373/2330 train_time:15120ms step_avg:40.54ms
step:374/2330 train_time:15164ms step_avg:40.55ms
step:375/2330 train_time:15199ms step_avg:40.53ms
step:376/2330 train_time:15246ms step_avg:40.55ms
step:377/2330 train_time:15282ms step_avg:40.54ms
step:378/2330 train_time:15327ms step_avg:40.55ms
step:379/2330 train_time:15363ms step_avg:40.54ms
step:380/2330 train_time:15408ms step_avg:40.55ms
step:381/2330 train_time:15444ms step_avg:40.54ms
step:382/2330 train_time:15489ms step_avg:40.55ms
step:383/2330 train_time:15525ms step_avg:40.54ms
step:384/2330 train_time:15570ms step_avg:40.55ms
step:385/2330 train_time:15606ms step_avg:40.53ms
step:386/2330 train_time:15651ms step_avg:40.55ms
step:387/2330 train_time:15687ms step_avg:40.53ms
step:388/2330 train_time:15733ms step_avg:40.55ms
step:389/2330 train_time:15769ms step_avg:40.54ms
step:390/2330 train_time:15814ms step_avg:40.55ms
step:391/2330 train_time:15850ms step_avg:40.54ms
step:392/2330 train_time:15895ms step_avg:40.55ms
step:393/2330 train_time:15931ms step_avg:40.54ms
step:394/2330 train_time:15977ms step_avg:40.55ms
step:395/2330 train_time:16012ms step_avg:40.54ms
step:396/2330 train_time:16058ms step_avg:40.55ms
step:397/2330 train_time:16093ms step_avg:40.54ms
step:398/2330 train_time:16139ms step_avg:40.55ms
step:399/2330 train_time:16175ms step_avg:40.54ms
step:400/2330 train_time:16221ms step_avg:40.55ms
step:401/2330 train_time:16257ms step_avg:40.54ms
step:402/2330 train_time:16303ms step_avg:40.56ms
step:403/2330 train_time:16340ms step_avg:40.55ms
step:404/2330 train_time:16385ms step_avg:40.56ms
step:405/2330 train_time:16421ms step_avg:40.54ms
step:406/2330 train_time:16467ms step_avg:40.56ms
step:407/2330 train_time:16503ms step_avg:40.55ms
step:408/2330 train_time:16548ms step_avg:40.56ms
step:409/2330 train_time:16583ms step_avg:40.55ms
step:410/2330 train_time:16628ms step_avg:40.56ms
step:411/2330 train_time:16663ms step_avg:40.54ms
step:412/2330 train_time:16710ms step_avg:40.56ms
step:413/2330 train_time:16745ms step_avg:40.55ms
step:414/2330 train_time:16791ms step_avg:40.56ms
step:415/2330 train_time:16827ms step_avg:40.55ms
step:416/2330 train_time:16872ms step_avg:40.56ms
step:417/2330 train_time:16908ms step_avg:40.55ms
step:418/2330 train_time:16955ms step_avg:40.56ms
step:419/2330 train_time:16990ms step_avg:40.55ms
step:420/2330 train_time:17035ms step_avg:40.56ms
step:421/2330 train_time:17070ms step_avg:40.55ms
step:422/2330 train_time:17115ms step_avg:40.56ms
step:423/2330 train_time:17151ms step_avg:40.55ms
step:424/2330 train_time:17197ms step_avg:40.56ms
step:425/2330 train_time:17233ms step_avg:40.55ms
step:426/2330 train_time:17278ms step_avg:40.56ms
step:427/2330 train_time:17314ms step_avg:40.55ms
step:428/2330 train_time:17359ms step_avg:40.56ms
step:429/2330 train_time:17395ms step_avg:40.55ms
step:430/2330 train_time:17441ms step_avg:40.56ms
step:431/2330 train_time:17478ms step_avg:40.55ms
step:432/2330 train_time:17524ms step_avg:40.56ms
step:433/2330 train_time:17559ms step_avg:40.55ms
step:434/2330 train_time:17606ms step_avg:40.57ms
step:435/2330 train_time:17642ms step_avg:40.56ms
step:436/2330 train_time:17688ms step_avg:40.57ms
step:437/2330 train_time:17723ms step_avg:40.56ms
step:438/2330 train_time:17768ms step_avg:40.57ms
step:439/2330 train_time:17805ms step_avg:40.56ms
step:440/2330 train_time:17851ms step_avg:40.57ms
step:441/2330 train_time:17886ms step_avg:40.56ms
step:442/2330 train_time:17932ms step_avg:40.57ms
step:443/2330 train_time:17968ms step_avg:40.56ms
step:444/2330 train_time:18014ms step_avg:40.57ms
step:445/2330 train_time:18050ms step_avg:40.56ms
step:446/2330 train_time:18095ms step_avg:40.57ms
step:447/2330 train_time:18131ms step_avg:40.56ms
step:448/2330 train_time:18176ms step_avg:40.57ms
step:449/2330 train_time:18212ms step_avg:40.56ms
step:450/2330 train_time:18258ms step_avg:40.57ms
step:451/2330 train_time:18294ms step_avg:40.56ms
step:452/2330 train_time:18340ms step_avg:40.57ms
step:453/2330 train_time:18376ms step_avg:40.56ms
step:454/2330 train_time:18421ms step_avg:40.58ms
step:455/2330 train_time:18457ms step_avg:40.56ms
step:456/2330 train_time:18502ms step_avg:40.57ms
step:457/2330 train_time:18537ms step_avg:40.56ms
step:458/2330 train_time:18583ms step_avg:40.57ms
step:459/2330 train_time:18619ms step_avg:40.57ms
step:460/2330 train_time:18665ms step_avg:40.58ms
step:461/2330 train_time:18702ms step_avg:40.57ms
step:462/2330 train_time:18747ms step_avg:40.58ms
step:463/2330 train_time:18782ms step_avg:40.57ms
step:464/2330 train_time:18828ms step_avg:40.58ms
step:465/2330 train_time:18863ms step_avg:40.57ms
step:466/2330 train_time:18910ms step_avg:40.58ms
step:467/2330 train_time:18945ms step_avg:40.57ms
step:468/2330 train_time:18990ms step_avg:40.58ms
step:469/2330 train_time:19026ms step_avg:40.57ms
step:470/2330 train_time:19071ms step_avg:40.58ms
step:471/2330 train_time:19106ms step_avg:40.56ms
step:472/2330 train_time:19152ms step_avg:40.58ms
step:473/2330 train_time:19188ms step_avg:40.57ms
step:474/2330 train_time:19233ms step_avg:40.58ms
step:475/2330 train_time:19269ms step_avg:40.57ms
step:476/2330 train_time:19314ms step_avg:40.58ms
step:477/2330 train_time:19351ms step_avg:40.57ms
step:478/2330 train_time:19396ms step_avg:40.58ms
step:479/2330 train_time:19433ms step_avg:40.57ms
step:480/2330 train_time:19478ms step_avg:40.58ms
step:481/2330 train_time:19514ms step_avg:40.57ms
step:482/2330 train_time:19559ms step_avg:40.58ms
step:483/2330 train_time:19596ms step_avg:40.57ms
step:484/2330 train_time:19642ms step_avg:40.58ms
step:485/2330 train_time:19678ms step_avg:40.57ms
step:486/2330 train_time:19723ms step_avg:40.58ms
step:487/2330 train_time:19760ms step_avg:40.57ms
step:488/2330 train_time:19806ms step_avg:40.59ms
step:489/2330 train_time:19843ms step_avg:40.58ms
step:490/2330 train_time:19889ms step_avg:40.59ms
step:491/2330 train_time:19925ms step_avg:40.58ms
step:492/2330 train_time:19970ms step_avg:40.59ms
step:493/2330 train_time:20005ms step_avg:40.58ms
step:494/2330 train_time:20051ms step_avg:40.59ms
step:495/2330 train_time:20086ms step_avg:40.58ms
step:496/2330 train_time:20131ms step_avg:40.59ms
step:497/2330 train_time:20166ms step_avg:40.58ms
step:498/2330 train_time:20212ms step_avg:40.59ms
step:499/2330 train_time:20248ms step_avg:40.58ms
step:500/2330 train_time:20293ms step_avg:40.59ms
step:500/2330 val_loss:5.4250 train_time:20383ms step_avg:40.77ms
step:501/2330 train_time:20396ms step_avg:40.71ms
step:502/2330 train_time:20408ms step_avg:40.65ms
step:503/2330 train_time:20419ms step_avg:40.59ms
step:504/2330 train_time:20456ms step_avg:40.59ms
step:505/2330 train_time:20491ms step_avg:40.58ms
step:506/2330 train_time:20536ms step_avg:40.58ms
step:507/2330 train_time:20571ms step_avg:40.57ms
step:508/2330 train_time:20615ms step_avg:40.58ms
step:509/2330 train_time:20651ms step_avg:40.57ms
step:510/2330 train_time:20702ms step_avg:40.59ms
step:511/2330 train_time:20741ms step_avg:40.59ms
step:512/2330 train_time:20789ms step_avg:40.60ms
step:513/2330 train_time:20826ms step_avg:40.60ms
step:514/2330 train_time:20872ms step_avg:40.61ms
step:515/2330 train_time:20908ms step_avg:40.60ms
step:516/2330 train_time:20954ms step_avg:40.61ms
step:517/2330 train_time:20989ms step_avg:40.60ms
step:518/2330 train_time:21034ms step_avg:40.61ms
step:519/2330 train_time:21069ms step_avg:40.60ms
step:520/2330 train_time:21114ms step_avg:40.60ms
step:521/2330 train_time:21149ms step_avg:40.59ms
step:522/2330 train_time:21194ms step_avg:40.60ms
step:523/2330 train_time:21229ms step_avg:40.59ms
step:524/2330 train_time:21274ms step_avg:40.60ms
step:525/2330 train_time:21311ms step_avg:40.59ms
step:526/2330 train_time:21357ms step_avg:40.60ms
step:527/2330 train_time:21392ms step_avg:40.59ms
step:528/2330 train_time:21437ms step_avg:40.60ms
step:529/2330 train_time:21472ms step_avg:40.59ms
step:530/2330 train_time:21517ms step_avg:40.60ms
step:531/2330 train_time:21552ms step_avg:40.59ms
step:532/2330 train_time:21597ms step_avg:40.60ms
step:533/2330 train_time:21634ms step_avg:40.59ms
step:534/2330 train_time:21681ms step_avg:40.60ms
step:535/2330 train_time:21718ms step_avg:40.59ms
step:536/2330 train_time:21764ms step_avg:40.60ms
step:537/2330 train_time:21801ms step_avg:40.60ms
step:538/2330 train_time:21849ms step_avg:40.61ms
step:539/2330 train_time:21886ms step_avg:40.60ms
step:540/2330 train_time:21931ms step_avg:40.61ms
step:541/2330 train_time:21967ms step_avg:40.60ms
step:542/2330 train_time:22012ms step_avg:40.61ms
step:543/2330 train_time:22047ms step_avg:40.60ms
step:544/2330 train_time:22092ms step_avg:40.61ms
step:545/2330 train_time:22127ms step_avg:40.60ms
step:546/2330 train_time:22172ms step_avg:40.61ms
step:547/2330 train_time:22207ms step_avg:40.60ms
step:548/2330 train_time:22252ms step_avg:40.61ms
step:549/2330 train_time:22287ms step_avg:40.60ms
step:550/2330 train_time:22333ms step_avg:40.61ms
step:551/2330 train_time:22369ms step_avg:40.60ms
step:552/2330 train_time:22413ms step_avg:40.60ms
step:553/2330 train_time:22449ms step_avg:40.59ms
step:554/2330 train_time:22494ms step_avg:40.60ms
step:555/2330 train_time:22529ms step_avg:40.59ms
step:556/2330 train_time:22574ms step_avg:40.60ms
step:557/2330 train_time:22611ms step_avg:40.59ms
step:558/2330 train_time:22657ms step_avg:40.60ms
step:559/2330 train_time:22694ms step_avg:40.60ms
step:560/2330 train_time:22740ms step_avg:40.61ms
step:561/2330 train_time:22777ms step_avg:40.60ms
step:562/2330 train_time:22823ms step_avg:40.61ms
step:563/2330 train_time:22858ms step_avg:40.60ms
step:564/2330 train_time:22904ms step_avg:40.61ms
step:565/2330 train_time:22941ms step_avg:40.60ms
step:566/2330 train_time:22987ms step_avg:40.61ms
step:567/2330 train_time:23023ms step_avg:40.61ms
step:568/2330 train_time:23069ms step_avg:40.61ms
step:569/2330 train_time:23104ms step_avg:40.60ms
step:570/2330 train_time:23148ms step_avg:40.61ms
step:571/2330 train_time:23184ms step_avg:40.60ms
step:572/2330 train_time:23229ms step_avg:40.61ms
step:573/2330 train_time:23265ms step_avg:40.60ms
step:574/2330 train_time:23310ms step_avg:40.61ms
step:575/2330 train_time:23345ms step_avg:40.60ms
step:576/2330 train_time:23391ms step_avg:40.61ms
step:577/2330 train_time:23426ms step_avg:40.60ms
step:578/2330 train_time:23472ms step_avg:40.61ms
step:579/2330 train_time:23508ms step_avg:40.60ms
step:580/2330 train_time:23554ms step_avg:40.61ms
step:581/2330 train_time:23589ms step_avg:40.60ms
step:582/2330 train_time:23635ms step_avg:40.61ms
step:583/2330 train_time:23671ms step_avg:40.60ms
step:584/2330 train_time:23717ms step_avg:40.61ms
step:585/2330 train_time:23752ms step_avg:40.60ms
step:586/2330 train_time:23798ms step_avg:40.61ms
step:587/2330 train_time:23834ms step_avg:40.60ms
step:588/2330 train_time:23880ms step_avg:40.61ms
step:589/2330 train_time:23916ms step_avg:40.60ms
step:590/2330 train_time:23961ms step_avg:40.61ms
step:591/2330 train_time:23997ms step_avg:40.60ms
step:592/2330 train_time:24043ms step_avg:40.61ms
step:593/2330 train_time:24078ms step_avg:40.60ms
step:594/2330 train_time:24123ms step_avg:40.61ms
step:595/2330 train_time:24159ms step_avg:40.60ms
step:596/2330 train_time:24204ms step_avg:40.61ms
step:597/2330 train_time:24240ms step_avg:40.60ms
step:598/2330 train_time:24286ms step_avg:40.61ms
step:599/2330 train_time:24322ms step_avg:40.60ms
step:600/2330 train_time:24369ms step_avg:40.61ms
step:601/2330 train_time:24405ms step_avg:40.61ms
step:602/2330 train_time:24450ms step_avg:40.61ms
step:603/2330 train_time:24485ms step_avg:40.61ms
step:604/2330 train_time:24530ms step_avg:40.61ms
step:605/2330 train_time:24566ms step_avg:40.61ms
step:606/2330 train_time:24612ms step_avg:40.61ms
step:607/2330 train_time:24648ms step_avg:40.61ms
step:608/2330 train_time:24694ms step_avg:40.61ms
step:609/2330 train_time:24730ms step_avg:40.61ms
step:610/2330 train_time:24776ms step_avg:40.62ms
step:611/2330 train_time:24812ms step_avg:40.61ms
step:612/2330 train_time:24858ms step_avg:40.62ms
step:613/2330 train_time:24894ms step_avg:40.61ms
step:614/2330 train_time:24939ms step_avg:40.62ms
step:615/2330 train_time:24975ms step_avg:40.61ms
step:616/2330 train_time:25021ms step_avg:40.62ms
step:617/2330 train_time:25057ms step_avg:40.61ms
step:618/2330 train_time:25102ms step_avg:40.62ms
step:619/2330 train_time:25138ms step_avg:40.61ms
step:620/2330 train_time:25183ms step_avg:40.62ms
step:621/2330 train_time:25219ms step_avg:40.61ms
step:622/2330 train_time:25266ms step_avg:40.62ms
step:623/2330 train_time:25304ms step_avg:40.62ms
step:624/2330 train_time:25349ms step_avg:40.62ms
step:625/2330 train_time:25384ms step_avg:40.61ms
step:626/2330 train_time:25429ms step_avg:40.62ms
step:627/2330 train_time:25465ms step_avg:40.61ms
step:628/2330 train_time:25510ms step_avg:40.62ms
step:629/2330 train_time:25547ms step_avg:40.62ms
step:630/2330 train_time:25593ms step_avg:40.62ms
step:631/2330 train_time:25628ms step_avg:40.61ms
step:632/2330 train_time:25673ms step_avg:40.62ms
step:633/2330 train_time:25710ms step_avg:40.62ms
step:634/2330 train_time:25755ms step_avg:40.62ms
step:635/2330 train_time:25791ms step_avg:40.62ms
step:636/2330 train_time:25837ms step_avg:40.62ms
step:637/2330 train_time:25873ms step_avg:40.62ms
step:638/2330 train_time:25919ms step_avg:40.63ms
step:639/2330 train_time:25955ms step_avg:40.62ms
step:640/2330 train_time:26001ms step_avg:40.63ms
step:641/2330 train_time:26036ms step_avg:40.62ms
step:642/2330 train_time:26081ms step_avg:40.62ms
step:643/2330 train_time:26117ms step_avg:40.62ms
step:644/2330 train_time:26161ms step_avg:40.62ms
step:645/2330 train_time:26198ms step_avg:40.62ms
step:646/2330 train_time:26243ms step_avg:40.62ms
step:647/2330 train_time:26279ms step_avg:40.62ms
step:648/2330 train_time:26324ms step_avg:40.62ms
step:649/2330 train_time:26361ms step_avg:40.62ms
step:650/2330 train_time:26408ms step_avg:40.63ms
step:651/2330 train_time:26444ms step_avg:40.62ms
step:652/2330 train_time:26488ms step_avg:40.63ms
step:653/2330 train_time:26524ms step_avg:40.62ms
step:654/2330 train_time:26569ms step_avg:40.63ms
step:655/2330 train_time:26606ms step_avg:40.62ms
step:656/2330 train_time:26651ms step_avg:40.63ms
step:657/2330 train_time:26687ms step_avg:40.62ms
step:658/2330 train_time:26733ms step_avg:40.63ms
step:659/2330 train_time:26769ms step_avg:40.62ms
step:660/2330 train_time:26816ms step_avg:40.63ms
step:661/2330 train_time:26851ms step_avg:40.62ms
step:662/2330 train_time:26897ms step_avg:40.63ms
step:663/2330 train_time:26933ms step_avg:40.62ms
step:664/2330 train_time:26979ms step_avg:40.63ms
step:665/2330 train_time:27015ms step_avg:40.62ms
step:666/2330 train_time:27061ms step_avg:40.63ms
step:667/2330 train_time:27096ms step_avg:40.62ms
step:668/2330 train_time:27141ms step_avg:40.63ms
step:669/2330 train_time:27177ms step_avg:40.62ms
step:670/2330 train_time:27222ms step_avg:40.63ms
step:671/2330 train_time:27259ms step_avg:40.62ms
step:672/2330 train_time:27305ms step_avg:40.63ms
step:673/2330 train_time:27341ms step_avg:40.63ms
step:674/2330 train_time:27387ms step_avg:40.63ms
step:675/2330 train_time:27423ms step_avg:40.63ms
step:676/2330 train_time:27468ms step_avg:40.63ms
step:677/2330 train_time:27504ms step_avg:40.63ms
step:678/2330 train_time:27549ms step_avg:40.63ms
step:679/2330 train_time:27586ms step_avg:40.63ms
step:680/2330 train_time:27632ms step_avg:40.64ms
step:681/2330 train_time:27668ms step_avg:40.63ms
step:682/2330 train_time:27714ms step_avg:40.64ms
step:683/2330 train_time:27749ms step_avg:40.63ms
step:684/2330 train_time:27795ms step_avg:40.64ms
step:685/2330 train_time:27830ms step_avg:40.63ms
step:686/2330 train_time:27875ms step_avg:40.63ms
step:687/2330 train_time:27911ms step_avg:40.63ms
step:688/2330 train_time:27956ms step_avg:40.63ms
step:689/2330 train_time:27991ms step_avg:40.63ms
step:690/2330 train_time:28037ms step_avg:40.63ms
step:691/2330 train_time:28073ms step_avg:40.63ms
step:692/2330 train_time:28120ms step_avg:40.64ms
step:693/2330 train_time:28155ms step_avg:40.63ms
step:694/2330 train_time:28201ms step_avg:40.64ms
step:695/2330 train_time:28237ms step_avg:40.63ms
step:696/2330 train_time:28283ms step_avg:40.64ms
step:697/2330 train_time:28319ms step_avg:40.63ms
step:698/2330 train_time:28365ms step_avg:40.64ms
step:699/2330 train_time:28401ms step_avg:40.63ms
step:700/2330 train_time:28447ms step_avg:40.64ms
step:701/2330 train_time:28483ms step_avg:40.63ms
step:702/2330 train_time:28527ms step_avg:40.64ms
step:703/2330 train_time:28563ms step_avg:40.63ms
step:704/2330 train_time:28609ms step_avg:40.64ms
step:705/2330 train_time:28645ms step_avg:40.63ms
step:706/2330 train_time:28691ms step_avg:40.64ms
step:707/2330 train_time:28727ms step_avg:40.63ms
step:708/2330 train_time:28772ms step_avg:40.64ms
step:709/2330 train_time:28808ms step_avg:40.63ms
step:710/2330 train_time:28854ms step_avg:40.64ms
step:711/2330 train_time:28890ms step_avg:40.63ms
step:712/2330 train_time:28935ms step_avg:40.64ms
step:713/2330 train_time:28971ms step_avg:40.63ms
step:714/2330 train_time:29016ms step_avg:40.64ms
step:715/2330 train_time:29052ms step_avg:40.63ms
step:716/2330 train_time:29098ms step_avg:40.64ms
step:717/2330 train_time:29134ms step_avg:40.63ms
step:718/2330 train_time:29180ms step_avg:40.64ms
step:719/2330 train_time:29216ms step_avg:40.63ms
step:720/2330 train_time:29261ms step_avg:40.64ms
step:721/2330 train_time:29297ms step_avg:40.63ms
step:722/2330 train_time:29343ms step_avg:40.64ms
step:723/2330 train_time:29379ms step_avg:40.64ms
step:724/2330 train_time:29424ms step_avg:40.64ms
step:725/2330 train_time:29460ms step_avg:40.63ms
step:726/2330 train_time:29506ms step_avg:40.64ms
step:727/2330 train_time:29542ms step_avg:40.64ms
step:728/2330 train_time:29587ms step_avg:40.64ms
step:729/2330 train_time:29623ms step_avg:40.64ms
step:730/2330 train_time:29669ms step_avg:40.64ms
step:731/2330 train_time:29705ms step_avg:40.64ms
step:732/2330 train_time:29752ms step_avg:40.64ms
step:733/2330 train_time:29787ms step_avg:40.64ms
step:734/2330 train_time:29833ms step_avg:40.64ms
step:735/2330 train_time:29869ms step_avg:40.64ms
step:736/2330 train_time:29915ms step_avg:40.65ms
step:737/2330 train_time:29951ms step_avg:40.64ms
step:738/2330 train_time:29997ms step_avg:40.65ms
step:739/2330 train_time:30032ms step_avg:40.64ms
step:740/2330 train_time:30077ms step_avg:40.64ms
step:741/2330 train_time:30113ms step_avg:40.64ms
step:742/2330 train_time:30159ms step_avg:40.65ms
step:743/2330 train_time:30194ms step_avg:40.64ms
step:744/2330 train_time:30240ms step_avg:40.65ms
step:745/2330 train_time:30276ms step_avg:40.64ms
step:746/2330 train_time:30322ms step_avg:40.65ms
step:747/2330 train_time:30358ms step_avg:40.64ms
step:748/2330 train_time:30404ms step_avg:40.65ms
step:749/2330 train_time:30440ms step_avg:40.64ms
step:750/2330 train_time:30485ms step_avg:40.65ms
step:750/2330 val_loss:5.2906 train_time:30575ms step_avg:40.77ms
step:751/2330 train_time:30588ms step_avg:40.73ms
step:752/2330 train_time:30600ms step_avg:40.69ms
step:753/2330 train_time:30610ms step_avg:40.65ms
step:754/2330 train_time:30648ms step_avg:40.65ms
step:755/2330 train_time:30683ms step_avg:40.64ms
step:756/2330 train_time:30728ms step_avg:40.64ms
step:757/2330 train_time:30763ms step_avg:40.64ms
step:758/2330 train_time:30808ms step_avg:40.64ms
step:759/2330 train_time:30843ms step_avg:40.64ms
step:760/2330 train_time:30891ms step_avg:40.65ms
step:761/2330 train_time:30932ms step_avg:40.65ms
step:762/2330 train_time:30979ms step_avg:40.66ms
step:763/2330 train_time:31017ms step_avg:40.65ms
step:764/2330 train_time:31063ms step_avg:40.66ms
step:765/2330 train_time:31100ms step_avg:40.65ms
step:766/2330 train_time:31144ms step_avg:40.66ms
step:767/2330 train_time:31180ms step_avg:40.65ms
step:768/2330 train_time:31225ms step_avg:40.66ms
step:769/2330 train_time:31261ms step_avg:40.65ms
step:770/2330 train_time:31307ms step_avg:40.66ms
step:771/2330 train_time:31342ms step_avg:40.65ms
step:772/2330 train_time:31387ms step_avg:40.66ms
step:773/2330 train_time:31423ms step_avg:40.65ms
step:774/2330 train_time:31470ms step_avg:40.66ms
step:775/2330 train_time:31506ms step_avg:40.65ms
step:776/2330 train_time:31550ms step_avg:40.66ms
step:777/2330 train_time:31585ms step_avg:40.65ms
step:778/2330 train_time:31630ms step_avg:40.66ms
step:779/2330 train_time:31666ms step_avg:40.65ms
step:780/2330 train_time:31710ms step_avg:40.65ms
step:781/2330 train_time:31746ms step_avg:40.65ms
step:782/2330 train_time:31791ms step_avg:40.65ms
step:783/2330 train_time:31828ms step_avg:40.65ms
step:784/2330 train_time:31875ms step_avg:40.66ms
step:785/2330 train_time:31912ms step_avg:40.65ms
step:786/2330 train_time:31959ms step_avg:40.66ms
step:787/2330 train_time:31995ms step_avg:40.65ms
step:788/2330 train_time:32042ms step_avg:40.66ms
step:789/2330 train_time:32078ms step_avg:40.66ms
step:790/2330 train_time:32122ms step_avg:40.66ms
step:791/2330 train_time:32158ms step_avg:40.65ms
step:792/2330 train_time:32203ms step_avg:40.66ms
step:793/2330 train_time:32239ms step_avg:40.65ms
step:794/2330 train_time:32284ms step_avg:40.66ms
step:795/2330 train_time:32320ms step_avg:40.65ms
step:796/2330 train_time:32365ms step_avg:40.66ms
step:797/2330 train_time:32401ms step_avg:40.65ms
step:798/2330 train_time:32447ms step_avg:40.66ms
step:799/2330 train_time:32483ms step_avg:40.66ms
step:800/2330 train_time:32529ms step_avg:40.66ms
step:801/2330 train_time:32564ms step_avg:40.65ms
step:802/2330 train_time:32609ms step_avg:40.66ms
step:803/2330 train_time:32644ms step_avg:40.65ms
step:804/2330 train_time:32689ms step_avg:40.66ms
step:805/2330 train_time:32725ms step_avg:40.65ms
step:806/2330 train_time:32770ms step_avg:40.66ms
step:807/2330 train_time:32807ms step_avg:40.65ms
step:808/2330 train_time:32854ms step_avg:40.66ms
step:809/2330 train_time:32890ms step_avg:40.66ms
step:810/2330 train_time:32936ms step_avg:40.66ms
step:811/2330 train_time:32972ms step_avg:40.66ms
step:812/2330 train_time:33019ms step_avg:40.66ms
step:813/2330 train_time:33055ms step_avg:40.66ms
step:814/2330 train_time:33101ms step_avg:40.66ms
step:815/2330 train_time:33136ms step_avg:40.66ms
step:816/2330 train_time:33182ms step_avg:40.66ms
step:817/2330 train_time:33218ms step_avg:40.66ms
step:818/2330 train_time:33263ms step_avg:40.66ms
step:819/2330 train_time:33298ms step_avg:40.66ms
step:820/2330 train_time:33342ms step_avg:40.66ms
step:821/2330 train_time:33378ms step_avg:40.66ms
step:822/2330 train_time:33423ms step_avg:40.66ms
step:823/2330 train_time:33459ms step_avg:40.66ms
step:824/2330 train_time:33505ms step_avg:40.66ms
step:825/2330 train_time:33540ms step_avg:40.65ms
step:826/2330 train_time:33585ms step_avg:40.66ms
step:827/2330 train_time:33620ms step_avg:40.65ms
step:828/2330 train_time:33667ms step_avg:40.66ms
step:829/2330 train_time:33703ms step_avg:40.65ms
step:830/2330 train_time:33748ms step_avg:40.66ms
step:831/2330 train_time:33785ms step_avg:40.66ms
step:832/2330 train_time:33832ms step_avg:40.66ms
step:833/2330 train_time:33869ms step_avg:40.66ms
step:834/2330 train_time:33915ms step_avg:40.67ms
step:835/2330 train_time:33951ms step_avg:40.66ms
step:836/2330 train_time:33996ms step_avg:40.67ms
step:837/2330 train_time:34032ms step_avg:40.66ms
step:838/2330 train_time:34077ms step_avg:40.67ms
step:839/2330 train_time:34113ms step_avg:40.66ms
step:840/2330 train_time:34159ms step_avg:40.67ms
step:841/2330 train_time:34195ms step_avg:40.66ms
step:842/2330 train_time:34241ms step_avg:40.67ms
step:843/2330 train_time:34277ms step_avg:40.66ms
step:844/2330 train_time:34322ms step_avg:40.67ms
step:845/2330 train_time:34358ms step_avg:40.66ms
step:846/2330 train_time:34403ms step_avg:40.67ms
step:847/2330 train_time:34438ms step_avg:40.66ms
step:848/2330 train_time:34484ms step_avg:40.67ms
step:849/2330 train_time:34519ms step_avg:40.66ms
step:850/2330 train_time:34564ms step_avg:40.66ms
step:851/2330 train_time:34600ms step_avg:40.66ms
step:852/2330 train_time:34644ms step_avg:40.66ms
step:853/2330 train_time:34681ms step_avg:40.66ms
step:854/2330 train_time:34727ms step_avg:40.66ms
step:855/2330 train_time:34764ms step_avg:40.66ms
step:856/2330 train_time:34810ms step_avg:40.67ms
step:857/2330 train_time:34847ms step_avg:40.66ms
step:858/2330 train_time:34893ms step_avg:40.67ms
step:859/2330 train_time:34928ms step_avg:40.66ms
step:860/2330 train_time:34973ms step_avg:40.67ms
step:861/2330 train_time:35009ms step_avg:40.66ms
step:862/2330 train_time:35056ms step_avg:40.67ms
step:863/2330 train_time:35091ms step_avg:40.66ms
step:864/2330 train_time:35137ms step_avg:40.67ms
step:865/2330 train_time:35173ms step_avg:40.66ms
step:866/2330 train_time:35219ms step_avg:40.67ms
step:867/2330 train_time:35255ms step_avg:40.66ms
step:868/2330 train_time:35301ms step_avg:40.67ms
step:869/2330 train_time:35337ms step_avg:40.66ms
step:870/2330 train_time:35382ms step_avg:40.67ms
step:871/2330 train_time:35418ms step_avg:40.66ms
step:872/2330 train_time:35463ms step_avg:40.67ms
step:873/2330 train_time:35499ms step_avg:40.66ms
step:874/2330 train_time:35543ms step_avg:40.67ms
step:875/2330 train_time:35579ms step_avg:40.66ms
step:876/2330 train_time:35625ms step_avg:40.67ms
step:877/2330 train_time:35661ms step_avg:40.66ms
step:878/2330 train_time:35707ms step_avg:40.67ms
step:879/2330 train_time:35744ms step_avg:40.66ms
step:880/2330 train_time:35789ms step_avg:40.67ms
step:881/2330 train_time:35825ms step_avg:40.66ms
step:882/2330 train_time:35871ms step_avg:40.67ms
step:883/2330 train_time:35907ms step_avg:40.66ms
step:884/2330 train_time:35952ms step_avg:40.67ms
step:885/2330 train_time:35988ms step_avg:40.66ms
step:886/2330 train_time:36034ms step_avg:40.67ms
step:887/2330 train_time:36070ms step_avg:40.66ms
step:888/2330 train_time:36115ms step_avg:40.67ms
step:889/2330 train_time:36150ms step_avg:40.66ms
step:890/2330 train_time:36196ms step_avg:40.67ms
step:891/2330 train_time:36232ms step_avg:40.66ms
step:892/2330 train_time:36278ms step_avg:40.67ms
step:893/2330 train_time:36314ms step_avg:40.66ms
step:894/2330 train_time:36360ms step_avg:40.67ms
step:895/2330 train_time:36396ms step_avg:40.67ms
step:896/2330 train_time:36441ms step_avg:40.67ms
step:897/2330 train_time:36476ms step_avg:40.66ms
step:898/2330 train_time:36521ms step_avg:40.67ms
step:899/2330 train_time:36557ms step_avg:40.66ms
step:900/2330 train_time:36602ms step_avg:40.67ms
step:901/2330 train_time:36638ms step_avg:40.66ms
step:902/2330 train_time:36683ms step_avg:40.67ms
step:903/2330 train_time:36719ms step_avg:40.66ms
step:904/2330 train_time:36765ms step_avg:40.67ms
step:905/2330 train_time:36801ms step_avg:40.66ms
step:906/2330 train_time:36847ms step_avg:40.67ms
step:907/2330 train_time:36883ms step_avg:40.66ms
step:908/2330 train_time:36929ms step_avg:40.67ms
step:909/2330 train_time:36966ms step_avg:40.67ms
step:910/2330 train_time:37011ms step_avg:40.67ms
step:911/2330 train_time:37047ms step_avg:40.67ms
step:912/2330 train_time:37092ms step_avg:40.67ms
step:913/2330 train_time:37128ms step_avg:40.67ms
step:914/2330 train_time:37174ms step_avg:40.67ms
step:915/2330 train_time:37209ms step_avg:40.67ms
step:916/2330 train_time:37255ms step_avg:40.67ms
step:917/2330 train_time:37291ms step_avg:40.67ms
step:918/2330 train_time:37337ms step_avg:40.67ms
step:919/2330 train_time:37373ms step_avg:40.67ms
step:920/2330 train_time:37419ms step_avg:40.67ms
step:921/2330 train_time:37455ms step_avg:40.67ms
step:922/2330 train_time:37500ms step_avg:40.67ms
step:923/2330 train_time:37536ms step_avg:40.67ms
step:924/2330 train_time:37581ms step_avg:40.67ms
step:925/2330 train_time:37617ms step_avg:40.67ms
step:926/2330 train_time:37663ms step_avg:40.67ms
step:927/2330 train_time:37698ms step_avg:40.67ms
step:928/2330 train_time:37743ms step_avg:40.67ms
step:929/2330 train_time:37779ms step_avg:40.67ms
step:930/2330 train_time:37824ms step_avg:40.67ms
step:931/2330 train_time:37861ms step_avg:40.67ms
step:932/2330 train_time:37907ms step_avg:40.67ms
step:933/2330 train_time:37944ms step_avg:40.67ms
step:934/2330 train_time:37989ms step_avg:40.67ms
step:935/2330 train_time:38025ms step_avg:40.67ms
step:936/2330 train_time:38070ms step_avg:40.67ms
step:937/2330 train_time:38106ms step_avg:40.67ms
step:938/2330 train_time:38152ms step_avg:40.67ms
step:939/2330 train_time:38187ms step_avg:40.67ms
step:940/2330 train_time:38234ms step_avg:40.67ms
step:941/2330 train_time:38270ms step_avg:40.67ms
step:942/2330 train_time:38315ms step_avg:40.67ms
step:943/2330 train_time:38351ms step_avg:40.67ms
step:944/2330 train_time:38396ms step_avg:40.67ms
step:945/2330 train_time:38433ms step_avg:40.67ms
step:946/2330 train_time:38478ms step_avg:40.67ms
step:947/2330 train_time:38513ms step_avg:40.67ms
step:948/2330 train_time:38559ms step_avg:40.67ms
step:949/2330 train_time:38595ms step_avg:40.67ms
step:950/2330 train_time:38641ms step_avg:40.67ms
step:951/2330 train_time:38676ms step_avg:40.67ms
step:952/2330 train_time:38722ms step_avg:40.67ms
step:953/2330 train_time:38758ms step_avg:40.67ms
step:954/2330 train_time:38803ms step_avg:40.67ms
step:955/2330 train_time:38838ms step_avg:40.67ms
step:956/2330 train_time:38883ms step_avg:40.67ms
step:957/2330 train_time:38920ms step_avg:40.67ms
step:958/2330 train_time:38966ms step_avg:40.67ms
step:959/2330 train_time:39002ms step_avg:40.67ms
step:960/2330 train_time:39047ms step_avg:40.67ms
step:961/2330 train_time:39084ms step_avg:40.67ms
step:962/2330 train_time:39129ms step_avg:40.68ms
step:963/2330 train_time:39166ms step_avg:40.67ms
step:964/2330 train_time:39210ms step_avg:40.67ms
step:965/2330 train_time:39247ms step_avg:40.67ms
step:966/2330 train_time:39292ms step_avg:40.68ms
step:967/2330 train_time:39329ms step_avg:40.67ms
step:968/2330 train_time:39375ms step_avg:40.68ms
step:969/2330 train_time:39411ms step_avg:40.67ms
step:970/2330 train_time:39457ms step_avg:40.68ms
step:971/2330 train_time:39492ms step_avg:40.67ms
step:972/2330 train_time:39537ms step_avg:40.68ms
step:973/2330 train_time:39573ms step_avg:40.67ms
step:974/2330 train_time:39618ms step_avg:40.68ms
step:975/2330 train_time:39654ms step_avg:40.67ms
step:976/2330 train_time:39699ms step_avg:40.68ms
step:977/2330 train_time:39735ms step_avg:40.67ms
step:978/2330 train_time:39782ms step_avg:40.68ms
step:979/2330 train_time:39817ms step_avg:40.67ms
step:980/2330 train_time:39863ms step_avg:40.68ms
step:981/2330 train_time:39899ms step_avg:40.67ms
step:982/2330 train_time:39943ms step_avg:40.68ms
step:983/2330 train_time:39979ms step_avg:40.67ms
step:984/2330 train_time:40025ms step_avg:40.68ms
step:985/2330 train_time:40061ms step_avg:40.67ms
step:986/2330 train_time:40108ms step_avg:40.68ms
step:987/2330 train_time:40143ms step_avg:40.67ms
step:988/2330 train_time:40189ms step_avg:40.68ms
step:989/2330 train_time:40226ms step_avg:40.67ms
step:990/2330 train_time:40271ms step_avg:40.68ms
step:991/2330 train_time:40308ms step_avg:40.67ms
step:992/2330 train_time:40353ms step_avg:40.68ms
step:993/2330 train_time:40389ms step_avg:40.67ms
step:994/2330 train_time:40433ms step_avg:40.68ms
step:995/2330 train_time:40470ms step_avg:40.67ms
step:996/2330 train_time:40515ms step_avg:40.68ms
step:997/2330 train_time:40551ms step_avg:40.67ms
step:998/2330 train_time:40597ms step_avg:40.68ms
step:999/2330 train_time:40632ms step_avg:40.67ms
step:1000/2330 train_time:40678ms step_avg:40.68ms
step:1000/2330 val_loss:5.2421 train_time:40766ms step_avg:40.77ms
step:1001/2330 train_time:40780ms step_avg:40.74ms
step:1002/2330 train_time:40793ms step_avg:40.71ms
step:1003/2330 train_time:40803ms step_avg:40.68ms
step:1004/2330 train_time:40842ms step_avg:40.68ms
step:1005/2330 train_time:40876ms step_avg:40.67ms
step:1006/2330 train_time:40920ms step_avg:40.68ms
step:1007/2330 train_time:40956ms step_avg:40.67ms
step:1008/2330 train_time:41000ms step_avg:40.67ms
step:1009/2330 train_time:41036ms step_avg:40.67ms
step:1010/2330 train_time:41087ms step_avg:40.68ms
step:1011/2330 train_time:41127ms step_avg:40.68ms
step:1012/2330 train_time:41175ms step_avg:40.69ms
step:1013/2330 train_time:41210ms step_avg:40.68ms
step:1014/2330 train_time:41256ms step_avg:40.69ms
step:1015/2330 train_time:41291ms step_avg:40.68ms
step:1016/2330 train_time:41337ms step_avg:40.69ms
step:1017/2330 train_time:41373ms step_avg:40.68ms
step:1018/2330 train_time:41417ms step_avg:40.68ms
step:1019/2330 train_time:41453ms step_avg:40.68ms
step:1020/2330 train_time:41499ms step_avg:40.68ms
step:1021/2330 train_time:41534ms step_avg:40.68ms
step:1022/2330 train_time:41579ms step_avg:40.68ms
step:1023/2330 train_time:41614ms step_avg:40.68ms
step:1024/2330 train_time:41659ms step_avg:40.68ms
step:1025/2330 train_time:41696ms step_avg:40.68ms
step:1026/2330 train_time:41742ms step_avg:40.68ms
step:1027/2330 train_time:41778ms step_avg:40.68ms
step:1028/2330 train_time:41823ms step_avg:40.68ms
step:1029/2330 train_time:41859ms step_avg:40.68ms
step:1030/2330 train_time:41903ms step_avg:40.68ms
step:1031/2330 train_time:41938ms step_avg:40.68ms
step:1032/2330 train_time:41983ms step_avg:40.68ms
step:1033/2330 train_time:42021ms step_avg:40.68ms
step:1034/2330 train_time:42070ms step_avg:40.69ms
step:1035/2330 train_time:42107ms step_avg:40.68ms
step:1036/2330 train_time:42154ms step_avg:40.69ms
step:1037/2330 train_time:42190ms step_avg:40.68ms
step:1038/2330 train_time:42236ms step_avg:40.69ms
step:1039/2330 train_time:42273ms step_avg:40.69ms
step:1040/2330 train_time:42318ms step_avg:40.69ms
step:1041/2330 train_time:42355ms step_avg:40.69ms
step:1042/2330 train_time:42403ms step_avg:40.69ms
step:1043/2330 train_time:42438ms step_avg:40.69ms
step:1044/2330 train_time:42483ms step_avg:40.69ms
step:1045/2330 train_time:42519ms step_avg:40.69ms
step:1046/2330 train_time:42564ms step_avg:40.69ms
step:1047/2330 train_time:42600ms step_avg:40.69ms
step:1048/2330 train_time:42645ms step_avg:40.69ms
step:1049/2330 train_time:42679ms step_avg:40.69ms
step:1050/2330 train_time:42725ms step_avg:40.69ms
step:1051/2330 train_time:42760ms step_avg:40.68ms
step:1052/2330 train_time:42805ms step_avg:40.69ms
step:1053/2330 train_time:42839ms step_avg:40.68ms
step:1054/2330 train_time:42884ms step_avg:40.69ms
step:1055/2330 train_time:42920ms step_avg:40.68ms
step:1056/2330 train_time:42967ms step_avg:40.69ms
step:1057/2330 train_time:43003ms step_avg:40.68ms
step:1058/2330 train_time:43050ms step_avg:40.69ms
step:1059/2330 train_time:43086ms step_avg:40.69ms
step:1060/2330 train_time:43133ms step_avg:40.69ms
step:1061/2330 train_time:43170ms step_avg:40.69ms
step:1062/2330 train_time:43215ms step_avg:40.69ms
step:1063/2330 train_time:43251ms step_avg:40.69ms
step:1064/2330 train_time:43297ms step_avg:40.69ms
step:1065/2330 train_time:43333ms step_avg:40.69ms
step:1066/2330 train_time:43379ms step_avg:40.69ms
step:1067/2330 train_time:43415ms step_avg:40.69ms
step:1068/2330 train_time:43462ms step_avg:40.69ms
step:1069/2330 train_time:43498ms step_avg:40.69ms
step:1070/2330 train_time:43544ms step_avg:40.70ms
step:1071/2330 train_time:43579ms step_avg:40.69ms
step:1072/2330 train_time:43624ms step_avg:40.69ms
step:1073/2330 train_time:43659ms step_avg:40.69ms
step:1074/2330 train_time:43705ms step_avg:40.69ms
step:1075/2330 train_time:43739ms step_avg:40.69ms
step:1076/2330 train_time:43784ms step_avg:40.69ms
step:1077/2330 train_time:43819ms step_avg:40.69ms
step:1078/2330 train_time:43864ms step_avg:40.69ms
step:1079/2330 train_time:43900ms step_avg:40.69ms
step:1080/2330 train_time:43945ms step_avg:40.69ms
step:1081/2330 train_time:43982ms step_avg:40.69ms
step:1082/2330 train_time:44028ms step_avg:40.69ms
step:1083/2330 train_time:44064ms step_avg:40.69ms
step:1084/2330 train_time:44111ms step_avg:40.69ms
step:1085/2330 train_time:44147ms step_avg:40.69ms
step:1086/2330 train_time:44193ms step_avg:40.69ms
step:1087/2330 train_time:44229ms step_avg:40.69ms
step:1088/2330 train_time:44275ms step_avg:40.69ms
step:1089/2330 train_time:44311ms step_avg:40.69ms
step:1090/2330 train_time:44356ms step_avg:40.69ms
step:1091/2330 train_time:44391ms step_avg:40.69ms
step:1092/2330 train_time:44437ms step_avg:40.69ms
step:1093/2330 train_time:44473ms step_avg:40.69ms
step:1094/2330 train_time:44518ms step_avg:40.69ms
step:1095/2330 train_time:44555ms step_avg:40.69ms
step:1096/2330 train_time:44601ms step_avg:40.69ms
step:1097/2330 train_time:44637ms step_avg:40.69ms
step:1098/2330 train_time:44682ms step_avg:40.69ms
step:1099/2330 train_time:44717ms step_avg:40.69ms
step:1100/2330 train_time:44763ms step_avg:40.69ms
step:1101/2330 train_time:44798ms step_avg:40.69ms
step:1102/2330 train_time:44843ms step_avg:40.69ms
step:1103/2330 train_time:44879ms step_avg:40.69ms
step:1104/2330 train_time:44925ms step_avg:40.69ms
step:1105/2330 train_time:44961ms step_avg:40.69ms
step:1106/2330 train_time:45007ms step_avg:40.69ms
step:1107/2330 train_time:45044ms step_avg:40.69ms
step:1108/2330 train_time:45091ms step_avg:40.70ms
step:1109/2330 train_time:45127ms step_avg:40.69ms
step:1110/2330 train_time:45172ms step_avg:40.70ms
step:1111/2330 train_time:45208ms step_avg:40.69ms
step:1112/2330 train_time:45254ms step_avg:40.70ms
step:1113/2330 train_time:45290ms step_avg:40.69ms
step:1114/2330 train_time:45335ms step_avg:40.70ms
step:1115/2330 train_time:45371ms step_avg:40.69ms
step:1116/2330 train_time:45416ms step_avg:40.70ms
step:1117/2330 train_time:45452ms step_avg:40.69ms
step:1118/2330 train_time:45497ms step_avg:40.69ms
step:1119/2330 train_time:45533ms step_avg:40.69ms
step:1120/2330 train_time:45578ms step_avg:40.69ms
step:1121/2330 train_time:45614ms step_avg:40.69ms
step:1122/2330 train_time:45660ms step_avg:40.70ms
step:1123/2330 train_time:45696ms step_avg:40.69ms
step:1124/2330 train_time:45742ms step_avg:40.70ms
step:1125/2330 train_time:45777ms step_avg:40.69ms
step:1126/2330 train_time:45822ms step_avg:40.69ms
step:1127/2330 train_time:45858ms step_avg:40.69ms
step:1128/2330 train_time:45904ms step_avg:40.69ms
step:1129/2330 train_time:45939ms step_avg:40.69ms
step:1130/2330 train_time:45985ms step_avg:40.69ms
step:1131/2330 train_time:46021ms step_avg:40.69ms
step:1132/2330 train_time:46068ms step_avg:40.70ms
step:1133/2330 train_time:46104ms step_avg:40.69ms
step:1134/2330 train_time:46149ms step_avg:40.70ms
step:1135/2330 train_time:46187ms step_avg:40.69ms
step:1136/2330 train_time:46232ms step_avg:40.70ms
step:1137/2330 train_time:46268ms step_avg:40.69ms
step:1138/2330 train_time:46314ms step_avg:40.70ms
step:1139/2330 train_time:46349ms step_avg:40.69ms
step:1140/2330 train_time:46395ms step_avg:40.70ms
step:1141/2330 train_time:46430ms step_avg:40.69ms
step:1142/2330 train_time:46475ms step_avg:40.70ms
step:1143/2330 train_time:46511ms step_avg:40.69ms
step:1144/2330 train_time:46556ms step_avg:40.70ms
step:1145/2330 train_time:46591ms step_avg:40.69ms
step:1146/2330 train_time:46636ms step_avg:40.70ms
step:1147/2330 train_time:46673ms step_avg:40.69ms
step:1148/2330 train_time:46718ms step_avg:40.70ms
step:1149/2330 train_time:46754ms step_avg:40.69ms
step:1150/2330 train_time:46800ms step_avg:40.70ms
step:1151/2330 train_time:46836ms step_avg:40.69ms
step:1152/2330 train_time:46881ms step_avg:40.70ms
step:1153/2330 train_time:46917ms step_avg:40.69ms
step:1154/2330 train_time:46964ms step_avg:40.70ms
step:1155/2330 train_time:47000ms step_avg:40.69ms
step:1156/2330 train_time:47046ms step_avg:40.70ms
step:1157/2330 train_time:47083ms step_avg:40.69ms
step:1158/2330 train_time:47128ms step_avg:40.70ms
step:1159/2330 train_time:47164ms step_avg:40.69ms
step:1160/2330 train_time:47210ms step_avg:40.70ms
step:1161/2330 train_time:47246ms step_avg:40.69ms
step:1162/2330 train_time:47291ms step_avg:40.70ms
step:1163/2330 train_time:47327ms step_avg:40.69ms
step:1164/2330 train_time:47372ms step_avg:40.70ms
step:1165/2330 train_time:47408ms step_avg:40.69ms
step:1166/2330 train_time:47454ms step_avg:40.70ms
step:1167/2330 train_time:47490ms step_avg:40.69ms
step:1168/2330 train_time:47535ms step_avg:40.70ms
step:1169/2330 train_time:47571ms step_avg:40.69ms
step:1170/2330 train_time:47616ms step_avg:40.70ms
step:1171/2330 train_time:47652ms step_avg:40.69ms
step:1172/2330 train_time:47697ms step_avg:40.70ms
step:1173/2330 train_time:47733ms step_avg:40.69ms
step:1174/2330 train_time:47778ms step_avg:40.70ms
step:1175/2330 train_time:47813ms step_avg:40.69ms
step:1176/2330 train_time:47858ms step_avg:40.70ms
step:1177/2330 train_time:47895ms step_avg:40.69ms
step:1178/2330 train_time:47941ms step_avg:40.70ms
step:1179/2330 train_time:47979ms step_avg:40.69ms
step:1180/2330 train_time:48024ms step_avg:40.70ms
step:1181/2330 train_time:48059ms step_avg:40.69ms
step:1182/2330 train_time:48105ms step_avg:40.70ms
step:1183/2330 train_time:48142ms step_avg:40.69ms
step:1184/2330 train_time:48187ms step_avg:40.70ms
step:1185/2330 train_time:48224ms step_avg:40.70ms
step:1186/2330 train_time:48270ms step_avg:40.70ms
step:1187/2330 train_time:48306ms step_avg:40.70ms
step:1188/2330 train_time:48352ms step_avg:40.70ms
step:1189/2330 train_time:48387ms step_avg:40.70ms
step:1190/2330 train_time:48432ms step_avg:40.70ms
step:1191/2330 train_time:48468ms step_avg:40.69ms
step:1192/2330 train_time:48514ms step_avg:40.70ms
step:1193/2330 train_time:48550ms step_avg:40.70ms
step:1194/2330 train_time:48595ms step_avg:40.70ms
step:1195/2330 train_time:48631ms step_avg:40.70ms
step:1196/2330 train_time:48676ms step_avg:40.70ms
step:1197/2330 train_time:48712ms step_avg:40.69ms
step:1198/2330 train_time:48757ms step_avg:40.70ms
step:1199/2330 train_time:48792ms step_avg:40.69ms
step:1200/2330 train_time:48838ms step_avg:40.70ms
step:1201/2330 train_time:48874ms step_avg:40.69ms
step:1202/2330 train_time:48919ms step_avg:40.70ms
step:1203/2330 train_time:48956ms step_avg:40.69ms
step:1204/2330 train_time:49002ms step_avg:40.70ms
step:1205/2330 train_time:49038ms step_avg:40.70ms
step:1206/2330 train_time:49084ms step_avg:40.70ms
step:1207/2330 train_time:49120ms step_avg:40.70ms
step:1208/2330 train_time:49166ms step_avg:40.70ms
step:1209/2330 train_time:49203ms step_avg:40.70ms
step:1210/2330 train_time:49249ms step_avg:40.70ms
step:1211/2330 train_time:49284ms step_avg:40.70ms
step:1212/2330 train_time:49329ms step_avg:40.70ms
step:1213/2330 train_time:49365ms step_avg:40.70ms
step:1214/2330 train_time:49411ms step_avg:40.70ms
step:1215/2330 train_time:49447ms step_avg:40.70ms
step:1216/2330 train_time:49493ms step_avg:40.70ms
step:1217/2330 train_time:49529ms step_avg:40.70ms
step:1218/2330 train_time:49574ms step_avg:40.70ms
step:1219/2330 train_time:49610ms step_avg:40.70ms
step:1220/2330 train_time:49655ms step_avg:40.70ms
step:1221/2330 train_time:49691ms step_avg:40.70ms
step:1222/2330 train_time:49736ms step_avg:40.70ms
step:1223/2330 train_time:49771ms step_avg:40.70ms
step:1224/2330 train_time:49817ms step_avg:40.70ms
step:1225/2330 train_time:49853ms step_avg:40.70ms
step:1226/2330 train_time:49899ms step_avg:40.70ms
step:1227/2330 train_time:49934ms step_avg:40.70ms
step:1228/2330 train_time:49979ms step_avg:40.70ms
step:1229/2330 train_time:50015ms step_avg:40.70ms
step:1230/2330 train_time:50062ms step_avg:40.70ms
step:1231/2330 train_time:50099ms step_avg:40.70ms
step:1232/2330 train_time:50144ms step_avg:40.70ms
step:1233/2330 train_time:50180ms step_avg:40.70ms
step:1234/2330 train_time:50225ms step_avg:40.70ms
step:1235/2330 train_time:50261ms step_avg:40.70ms
step:1236/2330 train_time:50307ms step_avg:40.70ms
step:1237/2330 train_time:50343ms step_avg:40.70ms
step:1238/2330 train_time:50388ms step_avg:40.70ms
step:1239/2330 train_time:50424ms step_avg:40.70ms
step:1240/2330 train_time:50470ms step_avg:40.70ms
step:1241/2330 train_time:50506ms step_avg:40.70ms
step:1242/2330 train_time:50551ms step_avg:40.70ms
step:1243/2330 train_time:50587ms step_avg:40.70ms
step:1244/2330 train_time:50633ms step_avg:40.70ms
step:1245/2330 train_time:50668ms step_avg:40.70ms
step:1246/2330 train_time:50715ms step_avg:40.70ms
step:1247/2330 train_time:50750ms step_avg:40.70ms
step:1248/2330 train_time:50795ms step_avg:40.70ms
step:1249/2330 train_time:50830ms step_avg:40.70ms
step:1250/2330 train_time:50875ms step_avg:40.70ms
step:1250/2330 val_loss:5.2211 train_time:50964ms step_avg:40.77ms
step:1251/2330 train_time:50977ms step_avg:40.75ms
step:1252/2330 train_time:50989ms step_avg:40.73ms
step:1253/2330 train_time:51001ms step_avg:40.70ms
step:1254/2330 train_time:51040ms step_avg:40.70ms
step:1255/2330 train_time:51075ms step_avg:40.70ms
step:1256/2330 train_time:51120ms step_avg:40.70ms
step:1257/2330 train_time:51154ms step_avg:40.70ms
step:1258/2330 train_time:51198ms step_avg:40.70ms
step:1259/2330 train_time:51233ms step_avg:40.69ms
step:1260/2330 train_time:51279ms step_avg:40.70ms
step:1261/2330 train_time:51319ms step_avg:40.70ms
step:1262/2330 train_time:51366ms step_avg:40.70ms
step:1263/2330 train_time:51402ms step_avg:40.70ms
step:1264/2330 train_time:51447ms step_avg:40.70ms
step:1265/2330 train_time:51483ms step_avg:40.70ms
step:1266/2330 train_time:51528ms step_avg:40.70ms
step:1267/2330 train_time:51564ms step_avg:40.70ms
step:1268/2330 train_time:51608ms step_avg:40.70ms
step:1269/2330 train_time:51643ms step_avg:40.70ms
step:1270/2330 train_time:51688ms step_avg:40.70ms
step:1271/2330 train_time:51724ms step_avg:40.70ms
step:1272/2330 train_time:51769ms step_avg:40.70ms
step:1273/2330 train_time:51804ms step_avg:40.69ms
step:1274/2330 train_time:51848ms step_avg:40.70ms
step:1275/2330 train_time:51885ms step_avg:40.69ms
step:1276/2330 train_time:51932ms step_avg:40.70ms
step:1277/2330 train_time:51970ms step_avg:40.70ms
step:1278/2330 train_time:52016ms step_avg:40.70ms
step:1279/2330 train_time:52053ms step_avg:40.70ms
step:1280/2330 train_time:52098ms step_avg:40.70ms
step:1281/2330 train_time:52133ms step_avg:40.70ms
step:1282/2330 train_time:52177ms step_avg:40.70ms
step:1283/2330 train_time:52214ms step_avg:40.70ms
step:1284/2330 train_time:52260ms step_avg:40.70ms
step:1285/2330 train_time:52298ms step_avg:40.70ms
step:1286/2330 train_time:52344ms step_avg:40.70ms
step:1287/2330 train_time:52380ms step_avg:40.70ms
step:1288/2330 train_time:52426ms step_avg:40.70ms
step:1289/2330 train_time:52462ms step_avg:40.70ms
step:1290/2330 train_time:52507ms step_avg:40.70ms
step:1291/2330 train_time:52542ms step_avg:40.70ms
step:1292/2330 train_time:52587ms step_avg:40.70ms
step:1293/2330 train_time:52622ms step_avg:40.70ms
step:1294/2330 train_time:52667ms step_avg:40.70ms
step:1295/2330 train_time:52703ms step_avg:40.70ms
step:1296/2330 train_time:52747ms step_avg:40.70ms
step:1297/2330 train_time:52782ms step_avg:40.70ms
step:1298/2330 train_time:52829ms step_avg:40.70ms
step:1299/2330 train_time:52865ms step_avg:40.70ms
step:1300/2330 train_time:52910ms step_avg:40.70ms
step:1301/2330 train_time:52946ms step_avg:40.70ms
step:1302/2330 train_time:52992ms step_avg:40.70ms
step:1303/2330 train_time:53028ms step_avg:40.70ms
step:1304/2330 train_time:53074ms step_avg:40.70ms
step:1305/2330 train_time:53110ms step_avg:40.70ms
step:1306/2330 train_time:53156ms step_avg:40.70ms
step:1307/2330 train_time:53192ms step_avg:40.70ms
step:1308/2330 train_time:53238ms step_avg:40.70ms
step:1309/2330 train_time:53274ms step_avg:40.70ms
step:1310/2330 train_time:53321ms step_avg:40.70ms
step:1311/2330 train_time:53358ms step_avg:40.70ms
step:1312/2330 train_time:53403ms step_avg:40.70ms
step:1313/2330 train_time:53439ms step_avg:40.70ms
step:1314/2330 train_time:53485ms step_avg:40.70ms
step:1315/2330 train_time:53520ms step_avg:40.70ms
step:1316/2330 train_time:53566ms step_avg:40.70ms
step:1317/2330 train_time:53601ms step_avg:40.70ms
step:1318/2330 train_time:53646ms step_avg:40.70ms
step:1319/2330 train_time:53681ms step_avg:40.70ms
step:1320/2330 train_time:53726ms step_avg:40.70ms
step:1321/2330 train_time:53763ms step_avg:40.70ms
step:1322/2330 train_time:53809ms step_avg:40.70ms
step:1323/2330 train_time:53845ms step_avg:40.70ms
step:1324/2330 train_time:53890ms step_avg:40.70ms
step:1325/2330 train_time:53925ms step_avg:40.70ms
step:1326/2330 train_time:53970ms step_avg:40.70ms
step:1327/2330 train_time:54007ms step_avg:40.70ms
step:1328/2330 train_time:54052ms step_avg:40.70ms
step:1329/2330 train_time:54088ms step_avg:40.70ms
step:1330/2330 train_time:54132ms step_avg:40.70ms
step:1331/2330 train_time:54168ms step_avg:40.70ms
step:1332/2330 train_time:54214ms step_avg:40.70ms
step:1333/2330 train_time:54252ms step_avg:40.70ms
step:1334/2330 train_time:54299ms step_avg:40.70ms
step:1335/2330 train_time:54335ms step_avg:40.70ms
step:1336/2330 train_time:54380ms step_avg:40.70ms
step:1337/2330 train_time:54417ms step_avg:40.70ms
step:1338/2330 train_time:54463ms step_avg:40.70ms
step:1339/2330 train_time:54499ms step_avg:40.70ms
step:1340/2330 train_time:54544ms step_avg:40.70ms
step:1341/2330 train_time:54580ms step_avg:40.70ms
step:1342/2330 train_time:54625ms step_avg:40.70ms
step:1343/2330 train_time:54660ms step_avg:40.70ms
step:1344/2330 train_time:54705ms step_avg:40.70ms
step:1345/2330 train_time:54742ms step_avg:40.70ms
step:1346/2330 train_time:54787ms step_avg:40.70ms
step:1347/2330 train_time:54823ms step_avg:40.70ms
step:1348/2330 train_time:54869ms step_avg:40.70ms
step:1349/2330 train_time:54905ms step_avg:40.70ms
step:1350/2330 train_time:54951ms step_avg:40.70ms
step:1351/2330 train_time:54987ms step_avg:40.70ms
step:1352/2330 train_time:55032ms step_avg:40.70ms
step:1353/2330 train_time:55068ms step_avg:40.70ms
step:1354/2330 train_time:55114ms step_avg:40.70ms
step:1355/2330 train_time:55151ms step_avg:40.70ms
step:1356/2330 train_time:55197ms step_avg:40.71ms
step:1357/2330 train_time:55233ms step_avg:40.70ms
step:1358/2330 train_time:55278ms step_avg:40.71ms
step:1359/2330 train_time:55315ms step_avg:40.70ms
step:1360/2330 train_time:55361ms step_avg:40.71ms
step:1361/2330 train_time:55397ms step_avg:40.70ms
step:1362/2330 train_time:55442ms step_avg:40.71ms
step:1363/2330 train_time:55478ms step_avg:40.70ms
step:1364/2330 train_time:55523ms step_avg:40.71ms
step:1365/2330 train_time:55559ms step_avg:40.70ms
step:1366/2330 train_time:55604ms step_avg:40.71ms
step:1367/2330 train_time:55640ms step_avg:40.70ms
step:1368/2330 train_time:55685ms step_avg:40.71ms
step:1369/2330 train_time:55720ms step_avg:40.70ms
step:1370/2330 train_time:55767ms step_avg:40.71ms
step:1371/2330 train_time:55803ms step_avg:40.70ms
step:1372/2330 train_time:55848ms step_avg:40.71ms
step:1373/2330 train_time:55884ms step_avg:40.70ms
step:1374/2330 train_time:55929ms step_avg:40.71ms
step:1375/2330 train_time:55965ms step_avg:40.70ms
step:1376/2330 train_time:56010ms step_avg:40.70ms
step:1377/2330 train_time:56046ms step_avg:40.70ms
step:1378/2330 train_time:56092ms step_avg:40.71ms
step:1379/2330 train_time:56129ms step_avg:40.70ms
step:1380/2330 train_time:56175ms step_avg:40.71ms
step:1381/2330 train_time:56212ms step_avg:40.70ms
step:1382/2330 train_time:56257ms step_avg:40.71ms
step:1383/2330 train_time:56294ms step_avg:40.70ms
step:1384/2330 train_time:56339ms step_avg:40.71ms
step:1385/2330 train_time:56375ms step_avg:40.70ms
step:1386/2330 train_time:56421ms step_avg:40.71ms
step:1387/2330 train_time:56457ms step_avg:40.70ms
step:1388/2330 train_time:56502ms step_avg:40.71ms
step:1389/2330 train_time:56538ms step_avg:40.70ms
step:1390/2330 train_time:56582ms step_avg:40.71ms
step:1391/2330 train_time:56619ms step_avg:40.70ms
step:1392/2330 train_time:56664ms step_avg:40.71ms
step:1393/2330 train_time:56700ms step_avg:40.70ms
step:1394/2330 train_time:56745ms step_avg:40.71ms
step:1395/2330 train_time:56781ms step_avg:40.70ms
step:1396/2330 train_time:56827ms step_avg:40.71ms
step:1397/2330 train_time:56863ms step_avg:40.70ms
step:1398/2330 train_time:56908ms step_avg:40.71ms
step:1399/2330 train_time:56944ms step_avg:40.70ms
step:1400/2330 train_time:56989ms step_avg:40.71ms
step:1401/2330 train_time:57025ms step_avg:40.70ms
step:1402/2330 train_time:57071ms step_avg:40.71ms
step:1403/2330 train_time:57107ms step_avg:40.70ms
step:1404/2330 train_time:57152ms step_avg:40.71ms
step:1405/2330 train_time:57189ms step_avg:40.70ms
step:1406/2330 train_time:57235ms step_avg:40.71ms
step:1407/2330 train_time:57272ms step_avg:40.71ms
step:1408/2330 train_time:57318ms step_avg:40.71ms
step:1409/2330 train_time:57355ms step_avg:40.71ms
step:1410/2330 train_time:57400ms step_avg:40.71ms
step:1411/2330 train_time:57435ms step_avg:40.71ms
step:1412/2330 train_time:57481ms step_avg:40.71ms
step:1413/2330 train_time:57518ms step_avg:40.71ms
step:1414/2330 train_time:57563ms step_avg:40.71ms
step:1415/2330 train_time:57599ms step_avg:40.71ms
step:1416/2330 train_time:57644ms step_avg:40.71ms
step:1417/2330 train_time:57681ms step_avg:40.71ms
step:1418/2330 train_time:57726ms step_avg:40.71ms
step:1419/2330 train_time:57762ms step_avg:40.71ms
step:1420/2330 train_time:57808ms step_avg:40.71ms
step:1421/2330 train_time:57844ms step_avg:40.71ms
step:1422/2330 train_time:57889ms step_avg:40.71ms
step:1423/2330 train_time:57924ms step_avg:40.71ms
step:1424/2330 train_time:57970ms step_avg:40.71ms
step:1425/2330 train_time:58005ms step_avg:40.71ms
step:1426/2330 train_time:58050ms step_avg:40.71ms
step:1427/2330 train_time:58087ms step_avg:40.71ms
step:1428/2330 train_time:58132ms step_avg:40.71ms
step:1429/2330 train_time:58168ms step_avg:40.71ms
step:1430/2330 train_time:58214ms step_avg:40.71ms
step:1431/2330 train_time:58252ms step_avg:40.71ms
step:1432/2330 train_time:58297ms step_avg:40.71ms
step:1433/2330 train_time:58333ms step_avg:40.71ms
step:1434/2330 train_time:58378ms step_avg:40.71ms
step:1435/2330 train_time:58414ms step_avg:40.71ms
step:1436/2330 train_time:58460ms step_avg:40.71ms
step:1437/2330 train_time:58496ms step_avg:40.71ms
step:1438/2330 train_time:58541ms step_avg:40.71ms
step:1439/2330 train_time:58577ms step_avg:40.71ms
step:1440/2330 train_time:58623ms step_avg:40.71ms
step:1441/2330 train_time:58660ms step_avg:40.71ms
step:1442/2330 train_time:58705ms step_avg:40.71ms
step:1443/2330 train_time:58741ms step_avg:40.71ms
step:1444/2330 train_time:58786ms step_avg:40.71ms
step:1445/2330 train_time:58821ms step_avg:40.71ms
step:1446/2330 train_time:58867ms step_avg:40.71ms
step:1447/2330 train_time:58902ms step_avg:40.71ms
step:1448/2330 train_time:58948ms step_avg:40.71ms
step:1449/2330 train_time:58984ms step_avg:40.71ms
step:1450/2330 train_time:59030ms step_avg:40.71ms
step:1451/2330 train_time:59066ms step_avg:40.71ms
step:1452/2330 train_time:59110ms step_avg:40.71ms
step:1453/2330 train_time:59147ms step_avg:40.71ms
step:1454/2330 train_time:59193ms step_avg:40.71ms
step:1455/2330 train_time:59229ms step_avg:40.71ms
step:1456/2330 train_time:59275ms step_avg:40.71ms
step:1457/2330 train_time:59312ms step_avg:40.71ms
step:1458/2330 train_time:59357ms step_avg:40.71ms
step:1459/2330 train_time:59392ms step_avg:40.71ms
step:1460/2330 train_time:59438ms step_avg:40.71ms
step:1461/2330 train_time:59474ms step_avg:40.71ms
step:1462/2330 train_time:59520ms step_avg:40.71ms
step:1463/2330 train_time:59556ms step_avg:40.71ms
step:1464/2330 train_time:59601ms step_avg:40.71ms
step:1465/2330 train_time:59637ms step_avg:40.71ms
step:1466/2330 train_time:59682ms step_avg:40.71ms
step:1467/2330 train_time:59719ms step_avg:40.71ms
step:1468/2330 train_time:59766ms step_avg:40.71ms
step:1469/2330 train_time:59801ms step_avg:40.71ms
step:1470/2330 train_time:59845ms step_avg:40.71ms
step:1471/2330 train_time:59882ms step_avg:40.71ms
step:1472/2330 train_time:59927ms step_avg:40.71ms
step:1473/2330 train_time:59963ms step_avg:40.71ms
step:1474/2330 train_time:60009ms step_avg:40.71ms
step:1475/2330 train_time:60045ms step_avg:40.71ms
step:1476/2330 train_time:60090ms step_avg:40.71ms
step:1477/2330 train_time:60125ms step_avg:40.71ms
step:1478/2330 train_time:60170ms step_avg:40.71ms
step:1479/2330 train_time:60206ms step_avg:40.71ms
step:1480/2330 train_time:60252ms step_avg:40.71ms
step:1481/2330 train_time:60288ms step_avg:40.71ms
step:1482/2330 train_time:60334ms step_avg:40.71ms
step:1483/2330 train_time:60371ms step_avg:40.71ms
step:1484/2330 train_time:60417ms step_avg:40.71ms
step:1485/2330 train_time:60454ms step_avg:40.71ms
step:1486/2330 train_time:60499ms step_avg:40.71ms
step:1487/2330 train_time:60536ms step_avg:40.71ms
step:1488/2330 train_time:60581ms step_avg:40.71ms
step:1489/2330 train_time:60617ms step_avg:40.71ms
step:1490/2330 train_time:60663ms step_avg:40.71ms
step:1491/2330 train_time:60699ms step_avg:40.71ms
step:1492/2330 train_time:60744ms step_avg:40.71ms
step:1493/2330 train_time:60780ms step_avg:40.71ms
step:1494/2330 train_time:60825ms step_avg:40.71ms
step:1495/2330 train_time:60860ms step_avg:40.71ms
step:1496/2330 train_time:60905ms step_avg:40.71ms
step:1497/2330 train_time:60942ms step_avg:40.71ms
step:1498/2330 train_time:60987ms step_avg:40.71ms
step:1499/2330 train_time:61023ms step_avg:40.71ms
step:1500/2330 train_time:61068ms step_avg:40.71ms
step:1500/2330 val_loss:5.1673 train_time:61157ms step_avg:40.77ms
step:1501/2330 train_time:61171ms step_avg:40.75ms
step:1502/2330 train_time:61183ms step_avg:40.73ms
step:1503/2330 train_time:61193ms step_avg:40.71ms
step:1504/2330 train_time:61233ms step_avg:40.71ms
step:1505/2330 train_time:61268ms step_avg:40.71ms
step:1506/2330 train_time:61312ms step_avg:40.71ms
step:1507/2330 train_time:61347ms step_avg:40.71ms
step:1508/2330 train_time:61392ms step_avg:40.71ms
step:1509/2330 train_time:61427ms step_avg:40.71ms
step:1510/2330 train_time:61474ms step_avg:40.71ms
step:1511/2330 train_time:61515ms step_avg:40.71ms
step:1512/2330 train_time:61564ms step_avg:40.72ms
step:1513/2330 train_time:61599ms step_avg:40.71ms
step:1514/2330 train_time:61645ms step_avg:40.72ms
step:1515/2330 train_time:61682ms step_avg:40.71ms
step:1516/2330 train_time:61727ms step_avg:40.72ms
step:1517/2330 train_time:61763ms step_avg:40.71ms
step:1518/2330 train_time:61807ms step_avg:40.72ms
step:1519/2330 train_time:61842ms step_avg:40.71ms
step:1520/2330 train_time:61887ms step_avg:40.72ms
step:1521/2330 train_time:61922ms step_avg:40.71ms
step:1522/2330 train_time:61966ms step_avg:40.71ms
step:1523/2330 train_time:62002ms step_avg:40.71ms
step:1524/2330 train_time:62047ms step_avg:40.71ms
step:1525/2330 train_time:62083ms step_avg:40.71ms
step:1526/2330 train_time:62130ms step_avg:40.71ms
step:1527/2330 train_time:62358ms step_avg:40.84ms
step:1528/2330 train_time:62426ms step_avg:40.85ms
step:1529/2330 train_time:62461ms step_avg:40.85ms
step:1530/2330 train_time:62505ms step_avg:40.85ms
step:1531/2330 train_time:62539ms step_avg:40.85ms
step:1532/2330 train_time:62584ms step_avg:40.85ms
step:1533/2330 train_time:62619ms step_avg:40.85ms
step:1534/2330 train_time:62663ms step_avg:40.85ms
step:1535/2330 train_time:62698ms step_avg:40.85ms
step:1536/2330 train_time:62743ms step_avg:40.85ms
step:1537/2330 train_time:62777ms step_avg:40.84ms
step:1538/2330 train_time:62822ms step_avg:40.85ms
step:1539/2330 train_time:62857ms step_avg:40.84ms
step:1540/2330 train_time:62902ms step_avg:40.85ms
step:1541/2330 train_time:62937ms step_avg:40.84ms
step:1542/2330 train_time:62981ms step_avg:40.84ms
step:1543/2330 train_time:63016ms step_avg:40.84ms
step:1544/2330 train_time:63060ms step_avg:40.84ms
step:1545/2330 train_time:63095ms step_avg:40.84ms
step:1546/2330 train_time:63140ms step_avg:40.84ms
step:1547/2330 train_time:63176ms step_avg:40.84ms
step:1548/2330 train_time:63221ms step_avg:40.84ms
step:1549/2330 train_time:63262ms step_avg:40.84ms
step:1550/2330 train_time:63316ms step_avg:40.85ms
step:1551/2330 train_time:63352ms step_avg:40.85ms
step:1552/2330 train_time:63398ms step_avg:40.85ms
step:1553/2330 train_time:63437ms step_avg:40.85ms
step:1554/2330 train_time:63486ms step_avg:40.85ms
step:1555/2330 train_time:63522ms step_avg:40.85ms
step:1556/2330 train_time:63567ms step_avg:40.85ms
step:1557/2330 train_time:63602ms step_avg:40.85ms
step:1558/2330 train_time:63647ms step_avg:40.85ms
step:1559/2330 train_time:63683ms step_avg:40.85ms
step:1560/2330 train_time:63727ms step_avg:40.85ms
step:1561/2330 train_time:63763ms step_avg:40.85ms
step:1562/2330 train_time:63808ms step_avg:40.85ms
step:1563/2330 train_time:63843ms step_avg:40.85ms
step:1564/2330 train_time:63888ms step_avg:40.85ms
step:1565/2330 train_time:63923ms step_avg:40.85ms
step:1566/2330 train_time:63968ms step_avg:40.85ms
step:1567/2330 train_time:64004ms step_avg:40.84ms
step:1568/2330 train_time:64048ms step_avg:40.85ms
step:1569/2330 train_time:64083ms step_avg:40.84ms
step:1570/2330 train_time:64129ms step_avg:40.85ms
step:1571/2330 train_time:64165ms step_avg:40.84ms
step:1572/2330 train_time:64211ms step_avg:40.85ms
step:1573/2330 train_time:64247ms step_avg:40.84ms
step:1574/2330 train_time:64294ms step_avg:40.85ms
step:1575/2330 train_time:64331ms step_avg:40.84ms
step:1576/2330 train_time:64377ms step_avg:40.85ms
step:1577/2330 train_time:64413ms step_avg:40.85ms
step:1578/2330 train_time:64461ms step_avg:40.85ms
step:1579/2330 train_time:64499ms step_avg:40.85ms
step:1580/2330 train_time:64545ms step_avg:40.85ms
step:1581/2330 train_time:64582ms step_avg:40.85ms
step:1582/2330 train_time:64627ms step_avg:40.85ms
step:1583/2330 train_time:64662ms step_avg:40.85ms
step:1584/2330 train_time:64707ms step_avg:40.85ms
step:1585/2330 train_time:64742ms step_avg:40.85ms
step:1586/2330 train_time:64787ms step_avg:40.85ms
step:1587/2330 train_time:64823ms step_avg:40.85ms
step:1588/2330 train_time:64868ms step_avg:40.85ms
step:1589/2330 train_time:64902ms step_avg:40.84ms
step:1590/2330 train_time:64948ms step_avg:40.85ms
step:1591/2330 train_time:64983ms step_avg:40.84ms
step:1592/2330 train_time:65029ms step_avg:40.85ms
step:1593/2330 train_time:65064ms step_avg:40.84ms
step:1594/2330 train_time:65110ms step_avg:40.85ms
step:1595/2330 train_time:65145ms step_avg:40.84ms
step:1596/2330 train_time:65191ms step_avg:40.85ms
step:1597/2330 train_time:65226ms step_avg:40.84ms
step:1598/2330 train_time:65272ms step_avg:40.85ms
step:1599/2330 train_time:65308ms step_avg:40.84ms
step:1600/2330 train_time:65354ms step_avg:40.85ms
step:1601/2330 train_time:65390ms step_avg:40.84ms
step:1602/2330 train_time:65436ms step_avg:40.85ms
step:1603/2330 train_time:65472ms step_avg:40.84ms
step:1604/2330 train_time:65518ms step_avg:40.85ms
step:1605/2330 train_time:65554ms step_avg:40.84ms
step:1606/2330 train_time:65601ms step_avg:40.85ms
step:1607/2330 train_time:65637ms step_avg:40.84ms
step:1608/2330 train_time:65683ms step_avg:40.85ms
step:1609/2330 train_time:65719ms step_avg:40.84ms
step:1610/2330 train_time:65764ms step_avg:40.85ms
step:1611/2330 train_time:65799ms step_avg:40.84ms
step:1612/2330 train_time:65844ms step_avg:40.85ms
step:1613/2330 train_time:65879ms step_avg:40.84ms
step:1614/2330 train_time:65924ms step_avg:40.84ms
step:1615/2330 train_time:65959ms step_avg:40.84ms
step:1616/2330 train_time:66004ms step_avg:40.84ms
step:1617/2330 train_time:66039ms step_avg:40.84ms
step:1618/2330 train_time:66086ms step_avg:40.84ms
step:1619/2330 train_time:66121ms step_avg:40.84ms
step:1620/2330 train_time:66166ms step_avg:40.84ms
step:1621/2330 train_time:66201ms step_avg:40.84ms
step:1622/2330 train_time:66247ms step_avg:40.84ms
step:1623/2330 train_time:66285ms step_avg:40.84ms
step:1624/2330 train_time:66331ms step_avg:40.84ms
step:1625/2330 train_time:66367ms step_avg:40.84ms
step:1626/2330 train_time:66414ms step_avg:40.84ms
step:1627/2330 train_time:66450ms step_avg:40.84ms
step:1628/2330 train_time:66495ms step_avg:40.84ms
step:1629/2330 train_time:66531ms step_avg:40.84ms
step:1630/2330 train_time:66576ms step_avg:40.84ms
step:1631/2330 train_time:66612ms step_avg:40.84ms
step:1632/2330 train_time:66659ms step_avg:40.84ms
step:1633/2330 train_time:66696ms step_avg:40.84ms
step:1634/2330 train_time:66741ms step_avg:40.85ms
step:1635/2330 train_time:66777ms step_avg:40.84ms
step:1636/2330 train_time:66823ms step_avg:40.85ms
step:1637/2330 train_time:66858ms step_avg:40.84ms
step:1638/2330 train_time:66903ms step_avg:40.84ms
step:1639/2330 train_time:66939ms step_avg:40.84ms
step:1640/2330 train_time:66984ms step_avg:40.84ms
step:1641/2330 train_time:67020ms step_avg:40.84ms
step:1642/2330 train_time:67064ms step_avg:40.84ms
step:1643/2330 train_time:67101ms step_avg:40.84ms
step:1644/2330 train_time:67146ms step_avg:40.84ms
step:1645/2330 train_time:67182ms step_avg:40.84ms
step:1646/2330 train_time:67229ms step_avg:40.84ms
step:1647/2330 train_time:67264ms step_avg:40.84ms
step:1648/2330 train_time:67310ms step_avg:40.84ms
step:1649/2330 train_time:67346ms step_avg:40.84ms
step:1650/2330 train_time:67392ms step_avg:40.84ms
step:1651/2330 train_time:67428ms step_avg:40.84ms
step:1652/2330 train_time:67474ms step_avg:40.84ms
step:1653/2330 train_time:67509ms step_avg:40.84ms
step:1654/2330 train_time:67555ms step_avg:40.84ms
step:1655/2330 train_time:67591ms step_avg:40.84ms
step:1656/2330 train_time:67637ms step_avg:40.84ms
step:1657/2330 train_time:67673ms step_avg:40.84ms
step:1658/2330 train_time:67718ms step_avg:40.84ms
step:1659/2330 train_time:67754ms step_avg:40.84ms
step:1660/2330 train_time:67799ms step_avg:40.84ms
step:1661/2330 train_time:67835ms step_avg:40.84ms
step:1662/2330 train_time:67880ms step_avg:40.84ms
step:1663/2330 train_time:67916ms step_avg:40.84ms
step:1664/2330 train_time:67962ms step_avg:40.84ms
step:1665/2330 train_time:67998ms step_avg:40.84ms
step:1666/2330 train_time:68043ms step_avg:40.84ms
step:1667/2330 train_time:68079ms step_avg:40.84ms
step:1668/2330 train_time:68126ms step_avg:40.84ms
step:1669/2330 train_time:68161ms step_avg:40.84ms
step:1670/2330 train_time:68207ms step_avg:40.84ms
step:1671/2330 train_time:68242ms step_avg:40.84ms
step:1672/2330 train_time:68289ms step_avg:40.84ms
step:1673/2330 train_time:68325ms step_avg:40.84ms
step:1674/2330 train_time:68371ms step_avg:40.84ms
step:1675/2330 train_time:68407ms step_avg:40.84ms
step:1676/2330 train_time:68452ms step_avg:40.84ms
step:1677/2330 train_time:68489ms step_avg:40.84ms
step:1678/2330 train_time:68535ms step_avg:40.84ms
step:1679/2330 train_time:68570ms step_avg:40.84ms
step:1680/2330 train_time:68615ms step_avg:40.84ms
step:1681/2330 train_time:68651ms step_avg:40.84ms
step:1682/2330 train_time:68696ms step_avg:40.84ms
step:1683/2330 train_time:68732ms step_avg:40.84ms
step:1684/2330 train_time:68778ms step_avg:40.84ms
step:1685/2330 train_time:68813ms step_avg:40.84ms
step:1686/2330 train_time:68860ms step_avg:40.84ms
step:1687/2330 train_time:68895ms step_avg:40.84ms
step:1688/2330 train_time:68940ms step_avg:40.84ms
step:1689/2330 train_time:68978ms step_avg:40.84ms
step:1690/2330 train_time:69024ms step_avg:40.84ms
step:1691/2330 train_time:69059ms step_avg:40.84ms
step:1692/2330 train_time:69105ms step_avg:40.84ms
step:1693/2330 train_time:69141ms step_avg:40.84ms
step:1694/2330 train_time:69186ms step_avg:40.84ms
step:1695/2330 train_time:69222ms step_avg:40.84ms
step:1696/2330 train_time:69268ms step_avg:40.84ms
step:1697/2330 train_time:69304ms step_avg:40.84ms
step:1698/2330 train_time:69349ms step_avg:40.84ms
step:1699/2330 train_time:69385ms step_avg:40.84ms
step:1700/2330 train_time:69431ms step_avg:40.84ms
step:1701/2330 train_time:69467ms step_avg:40.84ms
step:1702/2330 train_time:69513ms step_avg:40.84ms
step:1703/2330 train_time:69548ms step_avg:40.84ms
step:1704/2330 train_time:69594ms step_avg:40.84ms
step:1705/2330 train_time:69630ms step_avg:40.84ms
step:1706/2330 train_time:69675ms step_avg:40.84ms
step:1707/2330 train_time:69711ms step_avg:40.84ms
step:1708/2330 train_time:69756ms step_avg:40.84ms
step:1709/2330 train_time:69792ms step_avg:40.84ms
step:1710/2330 train_time:69837ms step_avg:40.84ms
step:1711/2330 train_time:69873ms step_avg:40.84ms
step:1712/2330 train_time:69919ms step_avg:40.84ms
step:1713/2330 train_time:69955ms step_avg:40.84ms
step:1714/2330 train_time:70001ms step_avg:40.84ms
step:1715/2330 train_time:70038ms step_avg:40.84ms
step:1716/2330 train_time:70084ms step_avg:40.84ms
step:1717/2330 train_time:70120ms step_avg:40.84ms
step:1718/2330 train_time:70166ms step_avg:40.84ms
step:1719/2330 train_time:70201ms step_avg:40.84ms
step:1720/2330 train_time:70246ms step_avg:40.84ms
step:1721/2330 train_time:70282ms step_avg:40.84ms
step:1722/2330 train_time:70328ms step_avg:40.84ms
step:1723/2330 train_time:70364ms step_avg:40.84ms
step:1724/2330 train_time:70410ms step_avg:40.84ms
step:1725/2330 train_time:70446ms step_avg:40.84ms
step:1726/2330 train_time:70492ms step_avg:40.84ms
step:1727/2330 train_time:70527ms step_avg:40.84ms
step:1728/2330 train_time:70572ms step_avg:40.84ms
step:1729/2330 train_time:70608ms step_avg:40.84ms
step:1730/2330 train_time:70654ms step_avg:40.84ms
step:1731/2330 train_time:70690ms step_avg:40.84ms
step:1732/2330 train_time:70736ms step_avg:40.84ms
step:1733/2330 train_time:70771ms step_avg:40.84ms
step:1734/2330 train_time:70816ms step_avg:40.84ms
step:1735/2330 train_time:70852ms step_avg:40.84ms
step:1736/2330 train_time:70896ms step_avg:40.84ms
step:1737/2330 train_time:70933ms step_avg:40.84ms
step:1738/2330 train_time:70978ms step_avg:40.84ms
step:1739/2330 train_time:71014ms step_avg:40.84ms
step:1740/2330 train_time:71060ms step_avg:40.84ms
step:1741/2330 train_time:71096ms step_avg:40.84ms
step:1742/2330 train_time:71142ms step_avg:40.84ms
step:1743/2330 train_time:71178ms step_avg:40.84ms
step:1744/2330 train_time:71224ms step_avg:40.84ms
step:1745/2330 train_time:71261ms step_avg:40.84ms
step:1746/2330 train_time:71307ms step_avg:40.84ms
step:1747/2330 train_time:71343ms step_avg:40.84ms
step:1748/2330 train_time:71389ms step_avg:40.84ms
step:1749/2330 train_time:71425ms step_avg:40.84ms
step:1750/2330 train_time:71471ms step_avg:40.84ms
step:1750/2330 val_loss:5.1263 train_time:71558ms step_avg:40.89ms
step:1751/2330 train_time:71572ms step_avg:40.87ms
step:1752/2330 train_time:71584ms step_avg:40.86ms
step:1753/2330 train_time:71594ms step_avg:40.84ms
step:1754/2330 train_time:71631ms step_avg:40.84ms
step:1755/2330 train_time:71665ms step_avg:40.83ms
step:1756/2330 train_time:71710ms step_avg:40.84ms
step:1757/2330 train_time:71745ms step_avg:40.83ms
step:1758/2330 train_time:71789ms step_avg:40.84ms
step:1759/2330 train_time:71825ms step_avg:40.83ms
step:1760/2330 train_time:71870ms step_avg:40.83ms
step:1761/2330 train_time:71906ms step_avg:40.83ms
step:1762/2330 train_time:71955ms step_avg:40.84ms
step:1763/2330 train_time:71995ms step_avg:40.84ms
step:1764/2330 train_time:72041ms step_avg:40.84ms
step:1765/2330 train_time:72076ms step_avg:40.84ms
step:1766/2330 train_time:72120ms step_avg:40.84ms
step:1767/2330 train_time:72155ms step_avg:40.84ms
step:1768/2330 train_time:72200ms step_avg:40.84ms
step:1769/2330 train_time:72234ms step_avg:40.83ms
step:1770/2330 train_time:72279ms step_avg:40.84ms
step:1771/2330 train_time:72314ms step_avg:40.83ms
step:1772/2330 train_time:72358ms step_avg:40.83ms
step:1773/2330 train_time:72393ms step_avg:40.83ms
step:1774/2330 train_time:72439ms step_avg:40.83ms
step:1775/2330 train_time:72479ms step_avg:40.83ms
step:1776/2330 train_time:72529ms step_avg:40.84ms
step:1777/2330 train_time:72566ms step_avg:40.84ms
step:1778/2330 train_time:72612ms step_avg:40.84ms
step:1779/2330 train_time:72647ms step_avg:40.84ms
step:1780/2330 train_time:72693ms step_avg:40.84ms
step:1781/2330 train_time:72730ms step_avg:40.84ms
step:1782/2330 train_time:72776ms step_avg:40.84ms
step:1783/2330 train_time:72812ms step_avg:40.84ms
step:1784/2330 train_time:72859ms step_avg:40.84ms
step:1785/2330 train_time:72895ms step_avg:40.84ms
step:1786/2330 train_time:72940ms step_avg:40.84ms
step:1787/2330 train_time:72976ms step_avg:40.84ms
step:1788/2330 train_time:73020ms step_avg:40.84ms
step:1789/2330 train_time:73055ms step_avg:40.84ms
step:1790/2330 train_time:73100ms step_avg:40.84ms
step:1791/2330 train_time:73135ms step_avg:40.83ms
step:1792/2330 train_time:73180ms step_avg:40.84ms
step:1793/2330 train_time:73215ms step_avg:40.83ms
step:1794/2330 train_time:73259ms step_avg:40.84ms
step:1795/2330 train_time:73294ms step_avg:40.83ms
step:1796/2330 train_time:73339ms step_avg:40.83ms
step:1797/2330 train_time:73375ms step_avg:40.83ms
step:1798/2330 train_time:73422ms step_avg:40.84ms
step:1799/2330 train_time:73459ms step_avg:40.83ms
step:1800/2330 train_time:73506ms step_avg:40.84ms
step:1801/2330 train_time:73543ms step_avg:40.83ms
step:1802/2330 train_time:73589ms step_avg:40.84ms
step:1803/2330 train_time:73625ms step_avg:40.83ms
step:1804/2330 train_time:73671ms step_avg:40.84ms
step:1805/2330 train_time:73707ms step_avg:40.83ms
step:1806/2330 train_time:73752ms step_avg:40.84ms
step:1807/2330 train_time:73788ms step_avg:40.83ms
step:1808/2330 train_time:73834ms step_avg:40.84ms
step:1809/2330 train_time:73870ms step_avg:40.83ms
step:1810/2330 train_time:73916ms step_avg:40.84ms
step:1811/2330 train_time:73952ms step_avg:40.84ms
step:1812/2330 train_time:73998ms step_avg:40.84ms
step:1813/2330 train_time:74034ms step_avg:40.84ms
step:1814/2330 train_time:74080ms step_avg:40.84ms
step:1815/2330 train_time:74115ms step_avg:40.83ms
step:1816/2330 train_time:74159ms step_avg:40.84ms
step:1817/2330 train_time:74194ms step_avg:40.83ms
step:1818/2330 train_time:74238ms step_avg:40.83ms
step:1819/2330 train_time:74274ms step_avg:40.83ms
step:1820/2330 train_time:74319ms step_avg:40.83ms
step:1821/2330 train_time:74355ms step_avg:40.83ms
step:1822/2330 train_time:74401ms step_avg:40.83ms
step:1823/2330 train_time:74437ms step_avg:40.83ms
step:1824/2330 train_time:74485ms step_avg:40.84ms
step:1825/2330 train_time:74522ms step_avg:40.83ms
step:1826/2330 train_time:74568ms step_avg:40.84ms
step:1827/2330 train_time:74603ms step_avg:40.83ms
step:1828/2330 train_time:74649ms step_avg:40.84ms
step:1829/2330 train_time:74684ms step_avg:40.83ms
step:1830/2330 train_time:74731ms step_avg:40.84ms
step:1831/2330 train_time:74767ms step_avg:40.83ms
step:1832/2330 train_time:74813ms step_avg:40.84ms
step:1833/2330 train_time:74849ms step_avg:40.83ms
step:1834/2330 train_time:74894ms step_avg:40.84ms
step:1835/2330 train_time:74930ms step_avg:40.83ms
step:1836/2330 train_time:74976ms step_avg:40.84ms
step:1837/2330 train_time:75013ms step_avg:40.83ms
step:1838/2330 train_time:75058ms step_avg:40.84ms
step:1839/2330 train_time:75093ms step_avg:40.83ms
step:1840/2330 train_time:75139ms step_avg:40.84ms
step:1841/2330 train_time:75174ms step_avg:40.83ms
step:1842/2330 train_time:75219ms step_avg:40.84ms
step:1843/2330 train_time:75255ms step_avg:40.83ms
step:1844/2330 train_time:75299ms step_avg:40.83ms
step:1845/2330 train_time:75335ms step_avg:40.83ms
step:1846/2330 train_time:75380ms step_avg:40.83ms
step:1847/2330 train_time:75416ms step_avg:40.83ms
step:1848/2330 train_time:75463ms step_avg:40.84ms
step:1849/2330 train_time:75499ms step_avg:40.83ms
step:1850/2330 train_time:75546ms step_avg:40.84ms
step:1851/2330 train_time:75581ms step_avg:40.83ms
step:1852/2330 train_time:75627ms step_avg:40.84ms
step:1853/2330 train_time:75664ms step_avg:40.83ms
step:1854/2330 train_time:75709ms step_avg:40.84ms
step:1855/2330 train_time:75746ms step_avg:40.83ms
step:1856/2330 train_time:75792ms step_avg:40.84ms
step:1857/2330 train_time:75827ms step_avg:40.83ms
step:1858/2330 train_time:75872ms step_avg:40.84ms
step:1859/2330 train_time:75908ms step_avg:40.83ms
step:1860/2330 train_time:75953ms step_avg:40.84ms
step:1861/2330 train_time:75989ms step_avg:40.83ms
step:1862/2330 train_time:76035ms step_avg:40.84ms
step:1863/2330 train_time:76071ms step_avg:40.83ms
step:1864/2330 train_time:76116ms step_avg:40.83ms
step:1865/2330 train_time:76153ms step_avg:40.83ms
step:1866/2330 train_time:76198ms step_avg:40.83ms
step:1867/2330 train_time:76234ms step_avg:40.83ms
step:1868/2330 train_time:76280ms step_avg:40.83ms
step:1869/2330 train_time:76315ms step_avg:40.83ms
step:1870/2330 train_time:76360ms step_avg:40.83ms
step:1871/2330 train_time:76395ms step_avg:40.83ms
step:1872/2330 train_time:76442ms step_avg:40.83ms
step:1873/2330 train_time:76478ms step_avg:40.83ms
step:1874/2330 train_time:76523ms step_avg:40.83ms
step:1875/2330 train_time:76559ms step_avg:40.83ms
step:1876/2330 train_time:76605ms step_avg:40.83ms
step:1877/2330 train_time:76641ms step_avg:40.83ms
step:1878/2330 train_time:76688ms step_avg:40.83ms
step:1879/2330 train_time:76723ms step_avg:40.83ms
step:1880/2330 train_time:76769ms step_avg:40.83ms
step:1881/2330 train_time:76805ms step_avg:40.83ms
step:1882/2330 train_time:76850ms step_avg:40.83ms
step:1883/2330 train_time:76886ms step_avg:40.83ms
step:1884/2330 train_time:76931ms step_avg:40.83ms
step:1885/2330 train_time:76967ms step_avg:40.83ms
step:1886/2330 train_time:77012ms step_avg:40.83ms
step:1887/2330 train_time:77048ms step_avg:40.83ms
step:1888/2330 train_time:77093ms step_avg:40.83ms
step:1889/2330 train_time:77128ms step_avg:40.83ms
step:1890/2330 train_time:77173ms step_avg:40.83ms
step:1891/2330 train_time:77209ms step_avg:40.83ms
step:1892/2330 train_time:77254ms step_avg:40.83ms
step:1893/2330 train_time:77291ms step_avg:40.83ms
step:1894/2330 train_time:77337ms step_avg:40.83ms
step:1895/2330 train_time:77374ms step_avg:40.83ms
step:1896/2330 train_time:77420ms step_avg:40.83ms
step:1897/2330 train_time:77456ms step_avg:40.83ms
step:1898/2330 train_time:77501ms step_avg:40.83ms
step:1899/2330 train_time:77538ms step_avg:40.83ms
step:1900/2330 train_time:77584ms step_avg:40.83ms
step:1901/2330 train_time:77619ms step_avg:40.83ms
step:1902/2330 train_time:77665ms step_avg:40.83ms
step:1903/2330 train_time:77702ms step_avg:40.83ms
step:1904/2330 train_time:77748ms step_avg:40.83ms
step:1905/2330 train_time:77783ms step_avg:40.83ms
step:1906/2330 train_time:77829ms step_avg:40.83ms
step:1907/2330 train_time:77864ms step_avg:40.83ms
step:1908/2330 train_time:77910ms step_avg:40.83ms
step:1909/2330 train_time:77946ms step_avg:40.83ms
step:1910/2330 train_time:77992ms step_avg:40.83ms
step:1911/2330 train_time:78028ms step_avg:40.83ms
step:1912/2330 train_time:78073ms step_avg:40.83ms
step:1913/2330 train_time:78108ms step_avg:40.83ms
step:1914/2330 train_time:78154ms step_avg:40.83ms
step:1915/2330 train_time:78190ms step_avg:40.83ms
step:1916/2330 train_time:78236ms step_avg:40.83ms
step:1917/2330 train_time:78271ms step_avg:40.83ms
step:1918/2330 train_time:78318ms step_avg:40.83ms
step:1919/2330 train_time:78354ms step_avg:40.83ms
step:1920/2330 train_time:78399ms step_avg:40.83ms
step:1921/2330 train_time:78435ms step_avg:40.83ms
step:1922/2330 train_time:78481ms step_avg:40.83ms
step:1923/2330 train_time:78517ms step_avg:40.83ms
step:1924/2330 train_time:78562ms step_avg:40.83ms
step:1925/2330 train_time:78599ms step_avg:40.83ms
step:1926/2330 train_time:78644ms step_avg:40.83ms
step:1927/2330 train_time:78682ms step_avg:40.83ms
step:1928/2330 train_time:78727ms step_avg:40.83ms
step:1929/2330 train_time:78762ms step_avg:40.83ms
step:1930/2330 train_time:78808ms step_avg:40.83ms
step:1931/2330 train_time:78843ms step_avg:40.83ms
step:1932/2330 train_time:78888ms step_avg:40.83ms
step:1933/2330 train_time:78923ms step_avg:40.83ms
step:1934/2330 train_time:78969ms step_avg:40.83ms
step:1935/2330 train_time:79005ms step_avg:40.83ms
step:1936/2330 train_time:79051ms step_avg:40.83ms
step:1937/2330 train_time:79086ms step_avg:40.83ms
step:1938/2330 train_time:79131ms step_avg:40.83ms
step:1939/2330 train_time:79167ms step_avg:40.83ms
step:1940/2330 train_time:79212ms step_avg:40.83ms
step:1941/2330 train_time:79248ms step_avg:40.83ms
step:1942/2330 train_time:79293ms step_avg:40.83ms
step:1943/2330 train_time:79329ms step_avg:40.83ms
step:1944/2330 train_time:79375ms step_avg:40.83ms
step:1945/2330 train_time:79412ms step_avg:40.83ms
step:1946/2330 train_time:79458ms step_avg:40.83ms
step:1947/2330 train_time:79495ms step_avg:40.83ms
step:1948/2330 train_time:79541ms step_avg:40.83ms
step:1949/2330 train_time:79576ms step_avg:40.83ms
step:1950/2330 train_time:79622ms step_avg:40.83ms
step:1951/2330 train_time:79659ms step_avg:40.83ms
step:1952/2330 train_time:79705ms step_avg:40.83ms
step:1953/2330 train_time:79742ms step_avg:40.83ms
step:1954/2330 train_time:79787ms step_avg:40.83ms
step:1955/2330 train_time:79823ms step_avg:40.83ms
step:1956/2330 train_time:79868ms step_avg:40.83ms
step:1957/2330 train_time:79904ms step_avg:40.83ms
step:1958/2330 train_time:79948ms step_avg:40.83ms
step:1959/2330 train_time:79985ms step_avg:40.83ms
step:1960/2330 train_time:80030ms step_avg:40.83ms
step:1961/2330 train_time:80065ms step_avg:40.83ms
step:1962/2330 train_time:80110ms step_avg:40.83ms
step:1963/2330 train_time:80146ms step_avg:40.83ms
step:1964/2330 train_time:80191ms step_avg:40.83ms
step:1965/2330 train_time:80226ms step_avg:40.83ms
step:1966/2330 train_time:80272ms step_avg:40.83ms
step:1967/2330 train_time:80308ms step_avg:40.83ms
step:1968/2330 train_time:80354ms step_avg:40.83ms
step:1969/2330 train_time:80390ms step_avg:40.83ms
step:1970/2330 train_time:80436ms step_avg:40.83ms
step:1971/2330 train_time:80472ms step_avg:40.83ms
step:1972/2330 train_time:80518ms step_avg:40.83ms
step:1973/2330 train_time:80555ms step_avg:40.83ms
step:1974/2330 train_time:80600ms step_avg:40.83ms
step:1975/2330 train_time:80637ms step_avg:40.83ms
step:1976/2330 train_time:80683ms step_avg:40.83ms
step:1977/2330 train_time:80720ms step_avg:40.83ms
step:1978/2330 train_time:80765ms step_avg:40.83ms
step:1979/2330 train_time:80801ms step_avg:40.83ms
step:1980/2330 train_time:80847ms step_avg:40.83ms
step:1981/2330 train_time:80883ms step_avg:40.83ms
step:1982/2330 train_time:80928ms step_avg:40.83ms
step:1983/2330 train_time:80963ms step_avg:40.83ms
step:1984/2330 train_time:81008ms step_avg:40.83ms
step:1985/2330 train_time:81044ms step_avg:40.83ms
step:1986/2330 train_time:81089ms step_avg:40.83ms
step:1987/2330 train_time:81124ms step_avg:40.83ms
step:1988/2330 train_time:81170ms step_avg:40.83ms
step:1989/2330 train_time:81206ms step_avg:40.83ms
step:1990/2330 train_time:81252ms step_avg:40.83ms
step:1991/2330 train_time:81287ms step_avg:40.83ms
step:1992/2330 train_time:81331ms step_avg:40.83ms
step:1993/2330 train_time:81368ms step_avg:40.83ms
step:1994/2330 train_time:81414ms step_avg:40.83ms
step:1995/2330 train_time:81450ms step_avg:40.83ms
step:1996/2330 train_time:81496ms step_avg:40.83ms
step:1997/2330 train_time:81532ms step_avg:40.83ms
step:1998/2330 train_time:81579ms step_avg:40.83ms
step:1999/2330 train_time:81615ms step_avg:40.83ms
step:2000/2330 train_time:81660ms step_avg:40.83ms
step:2000/2330 val_loss:5.0952 train_time:81752ms step_avg:40.88ms
step:2001/2330 train_time:81766ms step_avg:40.86ms
step:2002/2330 train_time:81778ms step_avg:40.85ms
step:2003/2330 train_time:81788ms step_avg:40.83ms
step:2004/2330 train_time:81827ms step_avg:40.83ms
step:2005/2330 train_time:81862ms step_avg:40.83ms
step:2006/2330 train_time:81907ms step_avg:40.83ms
step:2007/2330 train_time:81942ms step_avg:40.83ms
step:2008/2330 train_time:81986ms step_avg:40.83ms
step:2009/2330 train_time:82021ms step_avg:40.83ms
step:2010/2330 train_time:82068ms step_avg:40.83ms
step:2011/2330 train_time:82106ms step_avg:40.83ms
step:2012/2330 train_time:82155ms step_avg:40.83ms
step:2013/2330 train_time:82195ms step_avg:40.83ms
step:2014/2330 train_time:82240ms step_avg:40.83ms
step:2015/2330 train_time:82277ms step_avg:40.83ms
step:2016/2330 train_time:82322ms step_avg:40.83ms
step:2017/2330 train_time:82357ms step_avg:40.83ms
step:2018/2330 train_time:82403ms step_avg:40.83ms
step:2019/2330 train_time:82437ms step_avg:40.83ms
step:2020/2330 train_time:82482ms step_avg:40.83ms
step:2021/2330 train_time:82518ms step_avg:40.83ms
step:2022/2330 train_time:82563ms step_avg:40.83ms
step:2023/2330 train_time:82597ms step_avg:40.83ms
step:2024/2330 train_time:82643ms step_avg:40.83ms
step:2025/2330 train_time:82680ms step_avg:40.83ms
step:2026/2330 train_time:82726ms step_avg:40.83ms
step:2027/2330 train_time:82762ms step_avg:40.83ms
step:2028/2330 train_time:82807ms step_avg:40.83ms
step:2029/2330 train_time:82842ms step_avg:40.83ms
step:2030/2330 train_time:82886ms step_avg:40.83ms
step:2031/2330 train_time:82921ms step_avg:40.83ms
step:2032/2330 train_time:82966ms step_avg:40.83ms
step:2033/2330 train_time:83003ms step_avg:40.83ms
step:2034/2330 train_time:83049ms step_avg:40.83ms
step:2035/2330 train_time:83086ms step_avg:40.83ms
step:2036/2330 train_time:83133ms step_avg:40.83ms
step:2037/2330 train_time:83170ms step_avg:40.83ms
step:2038/2330 train_time:83217ms step_avg:40.83ms
step:2039/2330 train_time:83253ms step_avg:40.83ms
step:2040/2330 train_time:83299ms step_avg:40.83ms
step:2041/2330 train_time:83334ms step_avg:40.83ms
step:2042/2330 train_time:83378ms step_avg:40.83ms
step:2043/2330 train_time:83415ms step_avg:40.83ms
step:2044/2330 train_time:83459ms step_avg:40.83ms
step:2045/2330 train_time:83494ms step_avg:40.83ms
step:2046/2330 train_time:83540ms step_avg:40.83ms
step:2047/2330 train_time:83574ms step_avg:40.83ms
step:2048/2330 train_time:83620ms step_avg:40.83ms
step:2049/2330 train_time:83656ms step_avg:40.83ms
step:2050/2330 train_time:83701ms step_avg:40.83ms
step:2051/2330 train_time:83737ms step_avg:40.83ms
step:2052/2330 train_time:83782ms step_avg:40.83ms
step:2053/2330 train_time:83818ms step_avg:40.83ms
step:2054/2330 train_time:83863ms step_avg:40.83ms
step:2055/2330 train_time:83898ms step_avg:40.83ms
step:2056/2330 train_time:83945ms step_avg:40.83ms
step:2057/2330 train_time:83981ms step_avg:40.83ms
step:2058/2330 train_time:84026ms step_avg:40.83ms
step:2059/2330 train_time:84063ms step_avg:40.83ms
step:2060/2330 train_time:84108ms step_avg:40.83ms
step:2061/2330 train_time:84145ms step_avg:40.83ms
step:2062/2330 train_time:84191ms step_avg:40.83ms
step:2063/2330 train_time:84227ms step_avg:40.83ms
step:2064/2330 train_time:84274ms step_avg:40.83ms
step:2065/2330 train_time:84310ms step_avg:40.83ms
step:2066/2330 train_time:84356ms step_avg:40.83ms
step:2067/2330 train_time:84392ms step_avg:40.83ms
step:2068/2330 train_time:84437ms step_avg:40.83ms
step:2069/2330 train_time:84473ms step_avg:40.83ms
step:2070/2330 train_time:84518ms step_avg:40.83ms
step:2071/2330 train_time:84553ms step_avg:40.83ms
step:2072/2330 train_time:84599ms step_avg:40.83ms
step:2073/2330 train_time:84636ms step_avg:40.83ms
step:2074/2330 train_time:84681ms step_avg:40.83ms
step:2075/2330 train_time:84717ms step_avg:40.83ms
step:2076/2330 train_time:84761ms step_avg:40.83ms
step:2077/2330 train_time:84797ms step_avg:40.83ms
step:2078/2330 train_time:84842ms step_avg:40.83ms
step:2079/2330 train_time:84878ms step_avg:40.83ms
step:2080/2330 train_time:84924ms step_avg:40.83ms
step:2081/2330 train_time:84959ms step_avg:40.83ms
step:2082/2330 train_time:85005ms step_avg:40.83ms
step:2083/2330 train_time:85041ms step_avg:40.83ms
step:2084/2330 train_time:85088ms step_avg:40.83ms
step:2085/2330 train_time:85123ms step_avg:40.83ms
step:2086/2330 train_time:85168ms step_avg:40.83ms
step:2087/2330 train_time:85204ms step_avg:40.83ms
step:2088/2330 train_time:85249ms step_avg:40.83ms
step:2089/2330 train_time:85285ms step_avg:40.83ms
step:2090/2330 train_time:85330ms step_avg:40.83ms
step:2091/2330 train_time:85367ms step_avg:40.83ms
step:2092/2330 train_time:85412ms step_avg:40.83ms
step:2093/2330 train_time:85447ms step_avg:40.83ms
step:2094/2330 train_time:85493ms step_avg:40.83ms
step:2095/2330 train_time:85530ms step_avg:40.83ms
step:2096/2330 train_time:85575ms step_avg:40.83ms
step:2097/2330 train_time:85610ms step_avg:40.83ms
step:2098/2330 train_time:85656ms step_avg:40.83ms
step:2099/2330 train_time:85693ms step_avg:40.83ms
step:2100/2330 train_time:85738ms step_avg:40.83ms
step:2101/2330 train_time:85774ms step_avg:40.83ms
step:2102/2330 train_time:85820ms step_avg:40.83ms
step:2103/2330 train_time:85856ms step_avg:40.83ms
step:2104/2330 train_time:85902ms step_avg:40.83ms
step:2105/2330 train_time:85938ms step_avg:40.83ms
step:2106/2330 train_time:85984ms step_avg:40.83ms
step:2107/2330 train_time:86019ms step_avg:40.83ms
step:2108/2330 train_time:86065ms step_avg:40.83ms
step:2109/2330 train_time:86100ms step_avg:40.83ms
step:2110/2330 train_time:86146ms step_avg:40.83ms
step:2111/2330 train_time:86181ms step_avg:40.82ms
step:2112/2330 train_time:86226ms step_avg:40.83ms
step:2113/2330 train_time:86262ms step_avg:40.82ms
step:2114/2330 train_time:86308ms step_avg:40.83ms
step:2115/2330 train_time:86344ms step_avg:40.82ms
step:2116/2330 train_time:86389ms step_avg:40.83ms
step:2117/2330 train_time:86425ms step_avg:40.82ms
step:2118/2330 train_time:86470ms step_avg:40.83ms
step:2119/2330 train_time:86506ms step_avg:40.82ms
step:2120/2330 train_time:86552ms step_avg:40.83ms
step:2121/2330 train_time:86589ms step_avg:40.82ms
step:2122/2330 train_time:86635ms step_avg:40.83ms
step:2123/2330 train_time:86671ms step_avg:40.82ms
step:2124/2330 train_time:86716ms step_avg:40.83ms
step:2125/2330 train_time:86752ms step_avg:40.82ms
step:2126/2330 train_time:86798ms step_avg:40.83ms
step:2127/2330 train_time:86834ms step_avg:40.82ms
step:2128/2330 train_time:86881ms step_avg:40.83ms
step:2129/2330 train_time:86917ms step_avg:40.83ms
step:2130/2330 train_time:86963ms step_avg:40.83ms
step:2131/2330 train_time:86999ms step_avg:40.83ms
step:2132/2330 train_time:87044ms step_avg:40.83ms
step:2133/2330 train_time:87080ms step_avg:40.83ms
step:2134/2330 train_time:87125ms step_avg:40.83ms
step:2135/2330 train_time:87161ms step_avg:40.82ms
step:2136/2330 train_time:87206ms step_avg:40.83ms
step:2137/2330 train_time:87242ms step_avg:40.82ms
step:2138/2330 train_time:87288ms step_avg:40.83ms
step:2139/2330 train_time:87323ms step_avg:40.82ms
step:2140/2330 train_time:87369ms step_avg:40.83ms
step:2141/2330 train_time:87405ms step_avg:40.82ms
step:2142/2330 train_time:87450ms step_avg:40.83ms
step:2143/2330 train_time:87486ms step_avg:40.82ms
step:2144/2330 train_time:87532ms step_avg:40.83ms
step:2145/2330 train_time:87568ms step_avg:40.82ms
step:2146/2330 train_time:87613ms step_avg:40.83ms
step:2147/2330 train_time:87649ms step_avg:40.82ms
step:2148/2330 train_time:87695ms step_avg:40.83ms
step:2149/2330 train_time:87732ms step_avg:40.82ms
step:2150/2330 train_time:87777ms step_avg:40.83ms
step:2151/2330 train_time:87812ms step_avg:40.82ms
step:2152/2330 train_time:87858ms step_avg:40.83ms
step:2153/2330 train_time:87894ms step_avg:40.82ms
step:2154/2330 train_time:87939ms step_avg:40.83ms
step:2155/2330 train_time:87975ms step_avg:40.82ms
step:2156/2330 train_time:88020ms step_avg:40.83ms
step:2157/2330 train_time:88057ms step_avg:40.82ms
step:2158/2330 train_time:88103ms step_avg:40.83ms
step:2159/2330 train_time:88139ms step_avg:40.82ms
step:2160/2330 train_time:88184ms step_avg:40.83ms
step:2161/2330 train_time:88219ms step_avg:40.82ms
step:2162/2330 train_time:88265ms step_avg:40.83ms
step:2163/2330 train_time:88301ms step_avg:40.82ms
step:2164/2330 train_time:88348ms step_avg:40.83ms
step:2165/2330 train_time:88383ms step_avg:40.82ms
step:2166/2330 train_time:88429ms step_avg:40.83ms
step:2167/2330 train_time:88465ms step_avg:40.82ms
step:2168/2330 train_time:88510ms step_avg:40.83ms
step:2169/2330 train_time:88545ms step_avg:40.82ms
step:2170/2330 train_time:88591ms step_avg:40.83ms
step:2171/2330 train_time:88627ms step_avg:40.82ms
step:2172/2330 train_time:88672ms step_avg:40.83ms
step:2173/2330 train_time:88709ms step_avg:40.82ms
step:2174/2330 train_time:88754ms step_avg:40.83ms
step:2175/2330 train_time:88791ms step_avg:40.82ms
step:2176/2330 train_time:88837ms step_avg:40.83ms
step:2177/2330 train_time:88873ms step_avg:40.82ms
step:2178/2330 train_time:88919ms step_avg:40.83ms
step:2179/2330 train_time:88954ms step_avg:40.82ms
step:2180/2330 train_time:89001ms step_avg:40.83ms
step:2181/2330 train_time:89037ms step_avg:40.82ms
step:2182/2330 train_time:89083ms step_avg:40.83ms
step:2183/2330 train_time:89118ms step_avg:40.82ms
step:2184/2330 train_time:89163ms step_avg:40.83ms
step:2185/2330 train_time:89199ms step_avg:40.82ms
step:2186/2330 train_time:89244ms step_avg:40.83ms
step:2187/2330 train_time:89280ms step_avg:40.82ms
step:2188/2330 train_time:89324ms step_avg:40.82ms
step:2189/2330 train_time:89361ms step_avg:40.82ms
step:2190/2330 train_time:89406ms step_avg:40.82ms
step:2191/2330 train_time:89442ms step_avg:40.82ms
step:2192/2330 train_time:89487ms step_avg:40.82ms
step:2193/2330 train_time:89522ms step_avg:40.82ms
step:2194/2330 train_time:89568ms step_avg:40.82ms
step:2195/2330 train_time:89604ms step_avg:40.82ms
step:2196/2330 train_time:89649ms step_avg:40.82ms
step:2197/2330 train_time:89685ms step_avg:40.82ms
step:2198/2330 train_time:89731ms step_avg:40.82ms
step:2199/2330 train_time:89767ms step_avg:40.82ms
step:2200/2330 train_time:89812ms step_avg:40.82ms
step:2201/2330 train_time:89848ms step_avg:40.82ms
step:2202/2330 train_time:89895ms step_avg:40.82ms
step:2203/2330 train_time:89932ms step_avg:40.82ms
step:2204/2330 train_time:89977ms step_avg:40.82ms
step:2205/2330 train_time:90013ms step_avg:40.82ms
step:2206/2330 train_time:90059ms step_avg:40.82ms
step:2207/2330 train_time:90096ms step_avg:40.82ms
step:2208/2330 train_time:90142ms step_avg:40.83ms
step:2209/2330 train_time:90177ms step_avg:40.82ms
step:2210/2330 train_time:90222ms step_avg:40.82ms
step:2211/2330 train_time:90258ms step_avg:40.82ms
step:2212/2330 train_time:90303ms step_avg:40.82ms
step:2213/2330 train_time:90339ms step_avg:40.82ms
step:2214/2330 train_time:90385ms step_avg:40.82ms
step:2215/2330 train_time:90421ms step_avg:40.82ms
step:2216/2330 train_time:90467ms step_avg:40.82ms
step:2217/2330 train_time:90502ms step_avg:40.82ms
step:2218/2330 train_time:90548ms step_avg:40.82ms
step:2219/2330 train_time:90583ms step_avg:40.82ms
step:2220/2330 train_time:90629ms step_avg:40.82ms
step:2221/2330 train_time:90665ms step_avg:40.82ms
step:2222/2330 train_time:90711ms step_avg:40.82ms
step:2223/2330 train_time:90747ms step_avg:40.82ms
step:2224/2330 train_time:90792ms step_avg:40.82ms
step:2225/2330 train_time:90829ms step_avg:40.82ms
step:2226/2330 train_time:90875ms step_avg:40.82ms
step:2227/2330 train_time:90911ms step_avg:40.82ms
step:2228/2330 train_time:90956ms step_avg:40.82ms
step:2229/2330 train_time:90992ms step_avg:40.82ms
step:2230/2330 train_time:91038ms step_avg:40.82ms
step:2231/2330 train_time:91075ms step_avg:40.82ms
step:2232/2330 train_time:91120ms step_avg:40.82ms
step:2233/2330 train_time:91156ms step_avg:40.82ms
step:2234/2330 train_time:91202ms step_avg:40.82ms
step:2235/2330 train_time:91237ms step_avg:40.82ms
step:2236/2330 train_time:91282ms step_avg:40.82ms
step:2237/2330 train_time:91318ms step_avg:40.82ms
step:2238/2330 train_time:91363ms step_avg:40.82ms
step:2239/2330 train_time:91399ms step_avg:40.82ms
step:2240/2330 train_time:91445ms step_avg:40.82ms
step:2241/2330 train_time:91481ms step_avg:40.82ms
step:2242/2330 train_time:91527ms step_avg:40.82ms
step:2243/2330 train_time:91563ms step_avg:40.82ms
step:2244/2330 train_time:91608ms step_avg:40.82ms
step:2245/2330 train_time:91645ms step_avg:40.82ms
step:2246/2330 train_time:91689ms step_avg:40.82ms
step:2247/2330 train_time:91724ms step_avg:40.82ms
step:2248/2330 train_time:91770ms step_avg:40.82ms
step:2249/2330 train_time:91805ms step_avg:40.82ms
step:2250/2330 train_time:91851ms step_avg:40.82ms
step:2250/2330 val_loss:5.0698 train_time:91943ms step_avg:40.86ms
step:2251/2330 train_time:91956ms step_avg:40.85ms
step:2252/2330 train_time:91968ms step_avg:40.84ms
step:2253/2330 train_time:91979ms step_avg:40.82ms
step:2254/2330 train_time:92016ms step_avg:40.82ms
step:2255/2330 train_time:92050ms step_avg:40.82ms
step:2256/2330 train_time:92095ms step_avg:40.82ms
step:2257/2330 train_time:92130ms step_avg:40.82ms
step:2258/2330 train_time:92175ms step_avg:40.82ms
step:2259/2330 train_time:92210ms step_avg:40.82ms
step:2260/2330 train_time:92257ms step_avg:40.82ms
step:2261/2330 train_time:92298ms step_avg:40.82ms
step:2262/2330 train_time:92346ms step_avg:40.82ms
step:2263/2330 train_time:92383ms step_avg:40.82ms
step:2264/2330 train_time:92429ms step_avg:40.83ms
step:2265/2330 train_time:92464ms step_avg:40.82ms
step:2266/2330 train_time:92510ms step_avg:40.83ms
step:2267/2330 train_time:92545ms step_avg:40.82ms
step:2268/2330 train_time:92590ms step_avg:40.82ms
step:2269/2330 train_time:92625ms step_avg:40.82ms
step:2270/2330 train_time:92671ms step_avg:40.82ms
step:2271/2330 train_time:92706ms step_avg:40.82ms
step:2272/2330 train_time:92751ms step_avg:40.82ms
step:2273/2330 train_time:92786ms step_avg:40.82ms
step:2274/2330 train_time:92831ms step_avg:40.82ms
step:2275/2330 train_time:92867ms step_avg:40.82ms
step:2276/2330 train_time:92913ms step_avg:40.82ms
step:2277/2330 train_time:92948ms step_avg:40.82ms
step:2278/2330 train_time:92993ms step_avg:40.82ms
step:2279/2330 train_time:93029ms step_avg:40.82ms
step:2280/2330 train_time:93074ms step_avg:40.82ms
step:2281/2330 train_time:93109ms step_avg:40.82ms
step:2282/2330 train_time:93154ms step_avg:40.82ms
step:2283/2330 train_time:93190ms step_avg:40.82ms
step:2284/2330 train_time:93236ms step_avg:40.82ms
step:2285/2330 train_time:93274ms step_avg:40.82ms
step:2286/2330 train_time:93322ms step_avg:40.82ms
step:2287/2330 train_time:93360ms step_avg:40.82ms
step:2288/2330 train_time:93406ms step_avg:40.82ms
step:2289/2330 train_time:93442ms step_avg:40.82ms
step:2290/2330 train_time:93486ms step_avg:40.82ms
step:2291/2330 train_time:93522ms step_avg:40.82ms
step:2292/2330 train_time:93566ms step_avg:40.82ms
step:2293/2330 train_time:93602ms step_avg:40.82ms
step:2294/2330 train_time:93648ms step_avg:40.82ms
step:2295/2330 train_time:93684ms step_avg:40.82ms
step:2296/2330 train_time:93729ms step_avg:40.82ms
step:2297/2330 train_time:93764ms step_avg:40.82ms
step:2298/2330 train_time:93809ms step_avg:40.82ms
step:2299/2330 train_time:93845ms step_avg:40.82ms
step:2300/2330 train_time:93890ms step_avg:40.82ms
step:2301/2330 train_time:93925ms step_avg:40.82ms
step:2302/2330 train_time:93970ms step_avg:40.82ms
step:2303/2330 train_time:94005ms step_avg:40.82ms
step:2304/2330 train_time:94050ms step_avg:40.82ms
step:2305/2330 train_time:94086ms step_avg:40.82ms
step:2306/2330 train_time:94132ms step_avg:40.82ms
step:2307/2330 train_time:94168ms step_avg:40.82ms
step:2308/2330 train_time:94214ms step_avg:40.82ms
step:2309/2330 train_time:94250ms step_avg:40.82ms
step:2310/2330 train_time:94297ms step_avg:40.82ms
step:2311/2330 train_time:94334ms step_avg:40.82ms
step:2312/2330 train_time:94380ms step_avg:40.82ms
step:2313/2330 train_time:94416ms step_avg:40.82ms
step:2314/2330 train_time:94463ms step_avg:40.82ms
step:2315/2330 train_time:94498ms step_avg:40.82ms
step:2316/2330 train_time:94543ms step_avg:40.82ms
step:2317/2330 train_time:94581ms step_avg:40.82ms
step:2318/2330 train_time:94626ms step_avg:40.82ms
step:2319/2330 train_time:94662ms step_avg:40.82ms
step:2320/2330 train_time:94707ms step_avg:40.82ms
step:2321/2330 train_time:94743ms step_avg:40.82ms
step:2322/2330 train_time:94788ms step_avg:40.82ms
step:2323/2330 train_time:94823ms step_avg:40.82ms
step:2324/2330 train_time:94868ms step_avg:40.82ms
step:2325/2330 train_time:94903ms step_avg:40.82ms
step:2326/2330 train_time:94949ms step_avg:40.82ms
step:2327/2330 train_time:94985ms step_avg:40.82ms
step:2328/2330 train_time:95030ms step_avg:40.82ms
step:2329/2330 train_time:95065ms step_avg:40.82ms
step:2330/2330 train_time:95111ms step_avg:40.82ms
step:2330/2330 val_loss:5.0634 train_time:95202ms step_avg:40.86ms
peak memory allocated: 29226 MiB reserved: 38888 MiB
