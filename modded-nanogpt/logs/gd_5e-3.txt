import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = divide_by_norm(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = divide_by_norm(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "gd_5e-3"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=5e-3, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 03:08:19 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   34C    P0             113W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   32C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.05ms
step:1/2330 train_time:90ms step_avg:90.39ms
step:2/2330 train_time:198ms step_avg:98.82ms
step:3/2330 train_time:220ms step_avg:73.27ms
step:4/2330 train_time:247ms step_avg:61.76ms
step:5/2330 train_time:303ms step_avg:60.64ms
step:6/2330 train_time:363ms step_avg:60.51ms
step:7/2330 train_time:421ms step_avg:60.13ms
step:8/2330 train_time:482ms step_avg:60.21ms
step:9/2330 train_time:539ms step_avg:59.93ms
step:10/2330 train_time:600ms step_avg:60.02ms
step:11/2330 train_time:658ms step_avg:59.84ms
step:12/2330 train_time:719ms step_avg:59.94ms
step:13/2330 train_time:777ms step_avg:59.77ms
step:14/2330 train_time:838ms step_avg:59.84ms
step:15/2330 train_time:896ms step_avg:59.75ms
step:16/2330 train_time:957ms step_avg:59.83ms
step:17/2330 train_time:1015ms step_avg:59.73ms
step:18/2330 train_time:1078ms step_avg:59.88ms
step:19/2330 train_time:1139ms step_avg:59.93ms
step:20/2330 train_time:1204ms step_avg:60.20ms
step:21/2330 train_time:1265ms step_avg:60.23ms
step:22/2330 train_time:1327ms step_avg:60.33ms
step:23/2330 train_time:1387ms step_avg:60.28ms
step:24/2330 train_time:1448ms step_avg:60.35ms
step:25/2330 train_time:1508ms step_avg:60.33ms
step:26/2330 train_time:1571ms step_avg:60.40ms
step:27/2330 train_time:1629ms step_avg:60.32ms
step:28/2330 train_time:1691ms step_avg:60.38ms
step:29/2330 train_time:1750ms step_avg:60.35ms
step:30/2330 train_time:1812ms step_avg:60.38ms
step:31/2330 train_time:1872ms step_avg:60.38ms
step:32/2330 train_time:1934ms step_avg:60.45ms
step:33/2330 train_time:1993ms step_avg:60.39ms
step:34/2330 train_time:2054ms step_avg:60.42ms
step:35/2330 train_time:2114ms step_avg:60.40ms
step:36/2330 train_time:2175ms step_avg:60.43ms
step:37/2330 train_time:2235ms step_avg:60.40ms
step:38/2330 train_time:2298ms step_avg:60.47ms
step:39/2330 train_time:2358ms step_avg:60.46ms
step:40/2330 train_time:2419ms step_avg:60.48ms
step:41/2330 train_time:2478ms step_avg:60.44ms
step:42/2330 train_time:2541ms step_avg:60.50ms
step:43/2330 train_time:2601ms step_avg:60.48ms
step:44/2330 train_time:2663ms step_avg:60.53ms
step:45/2330 train_time:2721ms step_avg:60.47ms
step:46/2330 train_time:2783ms step_avg:60.50ms
step:47/2330 train_time:2841ms step_avg:60.45ms
step:48/2330 train_time:2904ms step_avg:60.50ms
step:49/2330 train_time:2963ms step_avg:60.46ms
step:50/2330 train_time:3024ms step_avg:60.48ms
step:51/2330 train_time:3083ms step_avg:60.45ms
step:52/2330 train_time:3145ms step_avg:60.49ms
step:53/2330 train_time:3205ms step_avg:60.47ms
step:54/2330 train_time:3268ms step_avg:60.52ms
step:55/2330 train_time:3327ms step_avg:60.50ms
step:56/2330 train_time:3390ms step_avg:60.54ms
step:57/2330 train_time:3452ms step_avg:60.56ms
step:58/2330 train_time:3514ms step_avg:60.58ms
step:59/2330 train_time:3573ms step_avg:60.55ms
step:60/2330 train_time:3635ms step_avg:60.58ms
step:61/2330 train_time:3694ms step_avg:60.55ms
step:62/2330 train_time:3755ms step_avg:60.57ms
step:63/2330 train_time:3814ms step_avg:60.54ms
step:64/2330 train_time:3876ms step_avg:60.56ms
step:65/2330 train_time:3935ms step_avg:60.54ms
step:66/2330 train_time:3998ms step_avg:60.57ms
step:67/2330 train_time:4058ms step_avg:60.56ms
step:68/2330 train_time:4119ms step_avg:60.57ms
step:69/2330 train_time:4178ms step_avg:60.55ms
step:70/2330 train_time:4241ms step_avg:60.59ms
step:71/2330 train_time:4300ms step_avg:60.57ms
step:72/2330 train_time:4362ms step_avg:60.59ms
step:73/2330 train_time:4422ms step_avg:60.57ms
step:74/2330 train_time:4484ms step_avg:60.59ms
step:75/2330 train_time:4542ms step_avg:60.56ms
step:76/2330 train_time:4605ms step_avg:60.59ms
step:77/2330 train_time:4663ms step_avg:60.56ms
step:78/2330 train_time:4725ms step_avg:60.58ms
step:79/2330 train_time:4784ms step_avg:60.56ms
step:80/2330 train_time:4848ms step_avg:60.60ms
step:81/2330 train_time:4907ms step_avg:60.58ms
step:82/2330 train_time:4970ms step_avg:60.61ms
step:83/2330 train_time:5029ms step_avg:60.60ms
step:84/2330 train_time:5092ms step_avg:60.62ms
step:85/2330 train_time:5152ms step_avg:60.61ms
step:86/2330 train_time:5214ms step_avg:60.62ms
step:87/2330 train_time:5273ms step_avg:60.60ms
step:88/2330 train_time:5334ms step_avg:60.62ms
step:89/2330 train_time:5393ms step_avg:60.60ms
step:90/2330 train_time:5456ms step_avg:60.62ms
step:91/2330 train_time:5514ms step_avg:60.60ms
step:92/2330 train_time:5576ms step_avg:60.61ms
step:93/2330 train_time:5635ms step_avg:60.60ms
step:94/2330 train_time:5697ms step_avg:60.61ms
step:95/2330 train_time:5757ms step_avg:60.60ms
step:96/2330 train_time:5818ms step_avg:60.61ms
step:97/2330 train_time:5878ms step_avg:60.60ms
step:98/2330 train_time:5941ms step_avg:60.62ms
step:99/2330 train_time:5999ms step_avg:60.60ms
step:100/2330 train_time:6061ms step_avg:60.61ms
step:101/2330 train_time:6120ms step_avg:60.59ms
step:102/2330 train_time:6182ms step_avg:60.61ms
step:103/2330 train_time:6241ms step_avg:60.59ms
step:104/2330 train_time:6303ms step_avg:60.60ms
step:105/2330 train_time:6362ms step_avg:60.59ms
step:106/2330 train_time:6424ms step_avg:60.61ms
step:107/2330 train_time:6484ms step_avg:60.59ms
step:108/2330 train_time:6546ms step_avg:60.61ms
step:109/2330 train_time:6605ms step_avg:60.60ms
step:110/2330 train_time:6668ms step_avg:60.61ms
step:111/2330 train_time:6727ms step_avg:60.60ms
step:112/2330 train_time:6790ms step_avg:60.63ms
step:113/2330 train_time:6851ms step_avg:60.63ms
step:114/2330 train_time:6913ms step_avg:60.64ms
step:115/2330 train_time:6972ms step_avg:60.63ms
step:116/2330 train_time:7034ms step_avg:60.64ms
step:117/2330 train_time:7094ms step_avg:60.63ms
step:118/2330 train_time:7156ms step_avg:60.64ms
step:119/2330 train_time:7215ms step_avg:60.63ms
step:120/2330 train_time:7277ms step_avg:60.64ms
step:121/2330 train_time:7335ms step_avg:60.62ms
step:122/2330 train_time:7398ms step_avg:60.64ms
step:123/2330 train_time:7457ms step_avg:60.63ms
step:124/2330 train_time:7519ms step_avg:60.64ms
step:125/2330 train_time:7578ms step_avg:60.63ms
step:126/2330 train_time:7641ms step_avg:60.64ms
step:127/2330 train_time:7700ms step_avg:60.63ms
step:128/2330 train_time:7763ms step_avg:60.65ms
step:129/2330 train_time:7822ms step_avg:60.64ms
step:130/2330 train_time:7885ms step_avg:60.65ms
step:131/2330 train_time:7943ms step_avg:60.63ms
step:132/2330 train_time:8005ms step_avg:60.65ms
step:133/2330 train_time:8065ms step_avg:60.64ms
step:134/2330 train_time:8126ms step_avg:60.65ms
step:135/2330 train_time:8186ms step_avg:60.64ms
step:136/2330 train_time:8248ms step_avg:60.65ms
step:137/2330 train_time:8309ms step_avg:60.65ms
step:138/2330 train_time:8372ms step_avg:60.66ms
step:139/2330 train_time:8431ms step_avg:60.66ms
step:140/2330 train_time:8493ms step_avg:60.67ms
step:141/2330 train_time:8553ms step_avg:60.66ms
step:142/2330 train_time:8615ms step_avg:60.67ms
step:143/2330 train_time:8675ms step_avg:60.66ms
step:144/2330 train_time:8736ms step_avg:60.67ms
step:145/2330 train_time:8795ms step_avg:60.65ms
step:146/2330 train_time:8858ms step_avg:60.67ms
step:147/2330 train_time:8916ms step_avg:60.66ms
step:148/2330 train_time:8979ms step_avg:60.67ms
step:149/2330 train_time:9039ms step_avg:60.66ms
step:150/2330 train_time:9102ms step_avg:60.68ms
step:151/2330 train_time:9161ms step_avg:60.67ms
step:152/2330 train_time:9223ms step_avg:60.68ms
step:153/2330 train_time:9282ms step_avg:60.67ms
step:154/2330 train_time:9344ms step_avg:60.68ms
step:155/2330 train_time:9403ms step_avg:60.66ms
step:156/2330 train_time:9465ms step_avg:60.67ms
step:157/2330 train_time:9524ms step_avg:60.66ms
step:158/2330 train_time:9587ms step_avg:60.68ms
step:159/2330 train_time:9647ms step_avg:60.67ms
step:160/2330 train_time:9709ms step_avg:60.68ms
step:161/2330 train_time:9769ms step_avg:60.68ms
step:162/2330 train_time:9832ms step_avg:60.69ms
step:163/2330 train_time:9892ms step_avg:60.69ms
step:164/2330 train_time:9954ms step_avg:60.70ms
step:165/2330 train_time:10014ms step_avg:60.69ms
step:166/2330 train_time:10076ms step_avg:60.70ms
step:167/2330 train_time:10135ms step_avg:60.69ms
step:168/2330 train_time:10197ms step_avg:60.69ms
step:169/2330 train_time:10255ms step_avg:60.68ms
step:170/2330 train_time:10317ms step_avg:60.69ms
step:171/2330 train_time:10377ms step_avg:60.68ms
step:172/2330 train_time:10439ms step_avg:60.69ms
step:173/2330 train_time:10498ms step_avg:60.68ms
step:174/2330 train_time:10561ms step_avg:60.69ms
step:175/2330 train_time:10620ms step_avg:60.68ms
step:176/2330 train_time:10682ms step_avg:60.69ms
step:177/2330 train_time:10742ms step_avg:60.69ms
step:178/2330 train_time:10804ms step_avg:60.70ms
step:179/2330 train_time:10863ms step_avg:60.69ms
step:180/2330 train_time:10926ms step_avg:60.70ms
step:181/2330 train_time:10985ms step_avg:60.69ms
step:182/2330 train_time:11048ms step_avg:60.70ms
step:183/2330 train_time:11107ms step_avg:60.69ms
step:184/2330 train_time:11169ms step_avg:60.70ms
step:185/2330 train_time:11228ms step_avg:60.69ms
step:186/2330 train_time:11292ms step_avg:60.71ms
step:187/2330 train_time:11352ms step_avg:60.71ms
step:188/2330 train_time:11414ms step_avg:60.71ms
step:189/2330 train_time:11474ms step_avg:60.71ms
step:190/2330 train_time:11536ms step_avg:60.71ms
step:191/2330 train_time:11594ms step_avg:60.70ms
step:192/2330 train_time:11657ms step_avg:60.71ms
step:193/2330 train_time:11715ms step_avg:60.70ms
step:194/2330 train_time:11778ms step_avg:60.71ms
step:195/2330 train_time:11837ms step_avg:60.70ms
step:196/2330 train_time:11901ms step_avg:60.72ms
step:197/2330 train_time:11960ms step_avg:60.71ms
step:198/2330 train_time:12022ms step_avg:60.72ms
step:199/2330 train_time:12081ms step_avg:60.71ms
step:200/2330 train_time:12143ms step_avg:60.72ms
step:201/2330 train_time:12202ms step_avg:60.71ms
step:202/2330 train_time:12265ms step_avg:60.72ms
step:203/2330 train_time:12323ms step_avg:60.71ms
step:204/2330 train_time:12386ms step_avg:60.71ms
step:205/2330 train_time:12445ms step_avg:60.71ms
step:206/2330 train_time:12509ms step_avg:60.72ms
step:207/2330 train_time:12569ms step_avg:60.72ms
step:208/2330 train_time:12631ms step_avg:60.73ms
step:209/2330 train_time:12691ms step_avg:60.72ms
step:210/2330 train_time:12754ms step_avg:60.74ms
step:211/2330 train_time:12814ms step_avg:60.73ms
step:212/2330 train_time:12876ms step_avg:60.74ms
step:213/2330 train_time:12935ms step_avg:60.73ms
step:214/2330 train_time:12997ms step_avg:60.73ms
step:215/2330 train_time:13057ms step_avg:60.73ms
step:216/2330 train_time:13119ms step_avg:60.73ms
step:217/2330 train_time:13178ms step_avg:60.73ms
step:218/2330 train_time:13240ms step_avg:60.73ms
step:219/2330 train_time:13299ms step_avg:60.73ms
step:220/2330 train_time:13362ms step_avg:60.73ms
step:221/2330 train_time:13421ms step_avg:60.73ms
step:222/2330 train_time:13484ms step_avg:60.74ms
step:223/2330 train_time:13543ms step_avg:60.73ms
step:224/2330 train_time:13605ms step_avg:60.74ms
step:225/2330 train_time:13664ms step_avg:60.73ms
step:226/2330 train_time:13727ms step_avg:60.74ms
step:227/2330 train_time:13786ms step_avg:60.73ms
step:228/2330 train_time:13849ms step_avg:60.74ms
step:229/2330 train_time:13909ms step_avg:60.74ms
step:230/2330 train_time:13972ms step_avg:60.75ms
step:231/2330 train_time:14032ms step_avg:60.75ms
step:232/2330 train_time:14095ms step_avg:60.75ms
step:233/2330 train_time:14155ms step_avg:60.75ms
step:234/2330 train_time:14216ms step_avg:60.75ms
step:235/2330 train_time:14275ms step_avg:60.75ms
step:236/2330 train_time:14338ms step_avg:60.75ms
step:237/2330 train_time:14397ms step_avg:60.75ms
step:238/2330 train_time:14460ms step_avg:60.75ms
step:239/2330 train_time:14519ms step_avg:60.75ms
step:240/2330 train_time:14582ms step_avg:60.76ms
step:241/2330 train_time:14641ms step_avg:60.75ms
step:242/2330 train_time:14704ms step_avg:60.76ms
step:243/2330 train_time:14763ms step_avg:60.75ms
step:244/2330 train_time:14825ms step_avg:60.76ms
step:245/2330 train_time:14884ms step_avg:60.75ms
step:246/2330 train_time:14946ms step_avg:60.76ms
step:247/2330 train_time:15006ms step_avg:60.75ms
step:248/2330 train_time:15069ms step_avg:60.76ms
step:249/2330 train_time:15129ms step_avg:60.76ms
step:250/2330 train_time:15191ms step_avg:60.77ms
step:250/2330 val_loss:5.1753 train_time:15264ms step_avg:61.06ms
step:251/2330 train_time:15286ms step_avg:60.90ms
step:252/2330 train_time:15315ms step_avg:60.77ms
step:253/2330 train_time:15375ms step_avg:60.77ms
step:254/2330 train_time:15445ms step_avg:60.81ms
step:255/2330 train_time:15508ms step_avg:60.82ms
step:256/2330 train_time:15571ms step_avg:60.83ms
step:257/2330 train_time:15631ms step_avg:60.82ms
step:258/2330 train_time:15693ms step_avg:60.83ms
step:259/2330 train_time:15751ms step_avg:60.82ms
step:260/2330 train_time:15813ms step_avg:60.82ms
step:261/2330 train_time:15871ms step_avg:60.81ms
step:262/2330 train_time:15932ms step_avg:60.81ms
step:263/2330 train_time:15992ms step_avg:60.81ms
step:264/2330 train_time:16053ms step_avg:60.81ms
step:265/2330 train_time:16113ms step_avg:60.80ms
step:266/2330 train_time:16174ms step_avg:60.81ms
step:267/2330 train_time:16234ms step_avg:60.80ms
step:268/2330 train_time:16296ms step_avg:60.81ms
step:269/2330 train_time:16356ms step_avg:60.80ms
step:270/2330 train_time:16421ms step_avg:60.82ms
step:271/2330 train_time:16481ms step_avg:60.82ms
step:272/2330 train_time:16544ms step_avg:60.82ms
step:273/2330 train_time:16603ms step_avg:60.82ms
step:274/2330 train_time:16666ms step_avg:60.82ms
step:275/2330 train_time:16725ms step_avg:60.82ms
step:276/2330 train_time:16787ms step_avg:60.82ms
step:277/2330 train_time:16848ms step_avg:60.82ms
step:278/2330 train_time:16910ms step_avg:60.83ms
step:279/2330 train_time:16969ms step_avg:60.82ms
step:280/2330 train_time:17030ms step_avg:60.82ms
step:281/2330 train_time:17090ms step_avg:60.82ms
step:282/2330 train_time:17151ms step_avg:60.82ms
step:283/2330 train_time:17210ms step_avg:60.81ms
step:284/2330 train_time:17273ms step_avg:60.82ms
step:285/2330 train_time:17333ms step_avg:60.82ms
step:286/2330 train_time:17396ms step_avg:60.83ms
step:287/2330 train_time:17456ms step_avg:60.82ms
step:288/2330 train_time:17519ms step_avg:60.83ms
step:289/2330 train_time:17579ms step_avg:60.83ms
step:290/2330 train_time:17642ms step_avg:60.83ms
step:291/2330 train_time:17700ms step_avg:60.83ms
step:292/2330 train_time:17762ms step_avg:60.83ms
step:293/2330 train_time:17821ms step_avg:60.82ms
step:294/2330 train_time:17883ms step_avg:60.83ms
step:295/2330 train_time:17942ms step_avg:60.82ms
step:296/2330 train_time:18004ms step_avg:60.83ms
step:297/2330 train_time:18064ms step_avg:60.82ms
step:298/2330 train_time:18126ms step_avg:60.83ms
step:299/2330 train_time:18185ms step_avg:60.82ms
step:300/2330 train_time:18248ms step_avg:60.83ms
step:301/2330 train_time:18308ms step_avg:60.83ms
step:302/2330 train_time:18372ms step_avg:60.83ms
step:303/2330 train_time:18432ms step_avg:60.83ms
step:304/2330 train_time:18496ms step_avg:60.84ms
step:305/2330 train_time:18554ms step_avg:60.83ms
step:306/2330 train_time:18616ms step_avg:60.84ms
step:307/2330 train_time:18675ms step_avg:60.83ms
step:308/2330 train_time:18738ms step_avg:60.84ms
step:309/2330 train_time:18796ms step_avg:60.83ms
step:310/2330 train_time:18858ms step_avg:60.83ms
step:311/2330 train_time:18917ms step_avg:60.83ms
step:312/2330 train_time:18980ms step_avg:60.83ms
step:313/2330 train_time:19039ms step_avg:60.83ms
step:314/2330 train_time:19101ms step_avg:60.83ms
step:315/2330 train_time:19160ms step_avg:60.82ms
step:316/2330 train_time:19222ms step_avg:60.83ms
step:317/2330 train_time:19281ms step_avg:60.82ms
step:318/2330 train_time:19345ms step_avg:60.83ms
step:319/2330 train_time:19406ms step_avg:60.83ms
step:320/2330 train_time:19469ms step_avg:60.84ms
step:321/2330 train_time:19529ms step_avg:60.84ms
step:322/2330 train_time:19591ms step_avg:60.84ms
step:323/2330 train_time:19650ms step_avg:60.84ms
step:324/2330 train_time:19713ms step_avg:60.84ms
step:325/2330 train_time:19772ms step_avg:60.84ms
step:326/2330 train_time:19834ms step_avg:60.84ms
step:327/2330 train_time:19893ms step_avg:60.84ms
step:328/2330 train_time:19956ms step_avg:60.84ms
step:329/2330 train_time:20015ms step_avg:60.83ms
step:330/2330 train_time:20077ms step_avg:60.84ms
step:331/2330 train_time:20137ms step_avg:60.84ms
step:332/2330 train_time:20200ms step_avg:60.84ms
step:333/2330 train_time:20259ms step_avg:60.84ms
step:334/2330 train_time:20323ms step_avg:60.85ms
step:335/2330 train_time:20381ms step_avg:60.84ms
step:336/2330 train_time:20444ms step_avg:60.84ms
step:337/2330 train_time:20504ms step_avg:60.84ms
step:338/2330 train_time:20566ms step_avg:60.85ms
step:339/2330 train_time:20626ms step_avg:60.84ms
step:340/2330 train_time:20688ms step_avg:60.85ms
step:341/2330 train_time:20748ms step_avg:60.85ms
step:342/2330 train_time:20812ms step_avg:60.85ms
step:343/2330 train_time:20871ms step_avg:60.85ms
step:344/2330 train_time:20934ms step_avg:60.85ms
step:345/2330 train_time:20992ms step_avg:60.85ms
step:346/2330 train_time:21055ms step_avg:60.85ms
step:347/2330 train_time:21114ms step_avg:60.85ms
step:348/2330 train_time:21175ms step_avg:60.85ms
step:349/2330 train_time:21234ms step_avg:60.84ms
step:350/2330 train_time:21298ms step_avg:60.85ms
step:351/2330 train_time:21357ms step_avg:60.85ms
step:352/2330 train_time:21420ms step_avg:60.85ms
step:353/2330 train_time:21479ms step_avg:60.85ms
step:354/2330 train_time:21542ms step_avg:60.85ms
step:355/2330 train_time:21600ms step_avg:60.84ms
step:356/2330 train_time:21663ms step_avg:60.85ms
step:357/2330 train_time:21722ms step_avg:60.84ms
step:358/2330 train_time:21784ms step_avg:60.85ms
step:359/2330 train_time:21844ms step_avg:60.85ms
step:360/2330 train_time:21908ms step_avg:60.86ms
step:361/2330 train_time:21968ms step_avg:60.85ms
step:362/2330 train_time:22031ms step_avg:60.86ms
step:363/2330 train_time:22091ms step_avg:60.86ms
step:364/2330 train_time:22153ms step_avg:60.86ms
step:365/2330 train_time:22212ms step_avg:60.86ms
step:366/2330 train_time:22274ms step_avg:60.86ms
step:367/2330 train_time:22333ms step_avg:60.85ms
step:368/2330 train_time:22395ms step_avg:60.86ms
step:369/2330 train_time:22455ms step_avg:60.85ms
step:370/2330 train_time:22517ms step_avg:60.86ms
step:371/2330 train_time:22576ms step_avg:60.85ms
step:372/2330 train_time:22639ms step_avg:60.86ms
step:373/2330 train_time:22698ms step_avg:60.85ms
step:374/2330 train_time:22761ms step_avg:60.86ms
step:375/2330 train_time:22819ms step_avg:60.85ms
step:376/2330 train_time:22882ms step_avg:60.86ms
step:377/2330 train_time:22941ms step_avg:60.85ms
step:378/2330 train_time:23004ms step_avg:60.86ms
step:379/2330 train_time:23064ms step_avg:60.86ms
step:380/2330 train_time:23127ms step_avg:60.86ms
step:381/2330 train_time:23187ms step_avg:60.86ms
step:382/2330 train_time:23250ms step_avg:60.86ms
step:383/2330 train_time:23311ms step_avg:60.86ms
step:384/2330 train_time:23373ms step_avg:60.87ms
step:385/2330 train_time:23432ms step_avg:60.86ms
step:386/2330 train_time:23494ms step_avg:60.87ms
step:387/2330 train_time:23554ms step_avg:60.86ms
step:388/2330 train_time:23616ms step_avg:60.87ms
step:389/2330 train_time:23676ms step_avg:60.86ms
step:390/2330 train_time:23738ms step_avg:60.87ms
step:391/2330 train_time:23798ms step_avg:60.86ms
step:392/2330 train_time:23862ms step_avg:60.87ms
step:393/2330 train_time:23921ms step_avg:60.87ms
step:394/2330 train_time:23984ms step_avg:60.87ms
step:395/2330 train_time:24043ms step_avg:60.87ms
step:396/2330 train_time:24105ms step_avg:60.87ms
step:397/2330 train_time:24164ms step_avg:60.87ms
step:398/2330 train_time:24227ms step_avg:60.87ms
step:399/2330 train_time:24287ms step_avg:60.87ms
step:400/2330 train_time:24351ms step_avg:60.88ms
step:401/2330 train_time:24410ms step_avg:60.87ms
step:402/2330 train_time:24472ms step_avg:60.88ms
step:403/2330 train_time:24532ms step_avg:60.87ms
step:404/2330 train_time:24594ms step_avg:60.88ms
step:405/2330 train_time:24654ms step_avg:60.87ms
step:406/2330 train_time:24715ms step_avg:60.88ms
step:407/2330 train_time:24775ms step_avg:60.87ms
step:408/2330 train_time:24837ms step_avg:60.88ms
step:409/2330 train_time:24896ms step_avg:60.87ms
step:410/2330 train_time:24959ms step_avg:60.88ms
step:411/2330 train_time:25019ms step_avg:60.87ms
step:412/2330 train_time:25081ms step_avg:60.88ms
step:413/2330 train_time:25140ms step_avg:60.87ms
step:414/2330 train_time:25202ms step_avg:60.87ms
step:415/2330 train_time:25261ms step_avg:60.87ms
step:416/2330 train_time:25323ms step_avg:60.87ms
step:417/2330 train_time:25383ms step_avg:60.87ms
step:418/2330 train_time:25445ms step_avg:60.87ms
step:419/2330 train_time:25505ms step_avg:60.87ms
step:420/2330 train_time:25568ms step_avg:60.88ms
step:421/2330 train_time:25629ms step_avg:60.88ms
step:422/2330 train_time:25692ms step_avg:60.88ms
step:423/2330 train_time:25751ms step_avg:60.88ms
step:424/2330 train_time:25814ms step_avg:60.88ms
step:425/2330 train_time:25873ms step_avg:60.88ms
step:426/2330 train_time:25935ms step_avg:60.88ms
step:427/2330 train_time:25995ms step_avg:60.88ms
step:428/2330 train_time:26057ms step_avg:60.88ms
step:429/2330 train_time:26116ms step_avg:60.88ms
step:430/2330 train_time:26179ms step_avg:60.88ms
step:431/2330 train_time:26238ms step_avg:60.88ms
step:432/2330 train_time:26301ms step_avg:60.88ms
step:433/2330 train_time:26360ms step_avg:60.88ms
step:434/2330 train_time:26423ms step_avg:60.88ms
step:435/2330 train_time:26481ms step_avg:60.88ms
step:436/2330 train_time:26545ms step_avg:60.88ms
step:437/2330 train_time:26605ms step_avg:60.88ms
step:438/2330 train_time:26668ms step_avg:60.89ms
step:439/2330 train_time:26728ms step_avg:60.88ms
step:440/2330 train_time:26791ms step_avg:60.89ms
step:441/2330 train_time:26851ms step_avg:60.89ms
step:442/2330 train_time:26913ms step_avg:60.89ms
step:443/2330 train_time:26972ms step_avg:60.88ms
step:444/2330 train_time:27034ms step_avg:60.89ms
step:445/2330 train_time:27094ms step_avg:60.88ms
step:446/2330 train_time:27155ms step_avg:60.89ms
step:447/2330 train_time:27215ms step_avg:60.88ms
step:448/2330 train_time:27278ms step_avg:60.89ms
step:449/2330 train_time:27337ms step_avg:60.88ms
step:450/2330 train_time:27399ms step_avg:60.89ms
step:451/2330 train_time:27459ms step_avg:60.88ms
step:452/2330 train_time:27521ms step_avg:60.89ms
step:453/2330 train_time:27581ms step_avg:60.89ms
step:454/2330 train_time:27644ms step_avg:60.89ms
step:455/2330 train_time:27704ms step_avg:60.89ms
step:456/2330 train_time:27766ms step_avg:60.89ms
step:457/2330 train_time:27826ms step_avg:60.89ms
step:458/2330 train_time:27890ms step_avg:60.89ms
step:459/2330 train_time:27950ms step_avg:60.89ms
step:460/2330 train_time:28013ms step_avg:60.90ms
step:461/2330 train_time:28072ms step_avg:60.89ms
step:462/2330 train_time:28134ms step_avg:60.90ms
step:463/2330 train_time:28193ms step_avg:60.89ms
step:464/2330 train_time:28255ms step_avg:60.89ms
step:465/2330 train_time:28315ms step_avg:60.89ms
step:466/2330 train_time:28377ms step_avg:60.90ms
step:467/2330 train_time:28436ms step_avg:60.89ms
step:468/2330 train_time:28500ms step_avg:60.90ms
step:469/2330 train_time:28558ms step_avg:60.89ms
step:470/2330 train_time:28622ms step_avg:60.90ms
step:471/2330 train_time:28681ms step_avg:60.89ms
step:472/2330 train_time:28743ms step_avg:60.90ms
step:473/2330 train_time:28802ms step_avg:60.89ms
step:474/2330 train_time:28865ms step_avg:60.90ms
step:475/2330 train_time:28925ms step_avg:60.89ms
step:476/2330 train_time:28988ms step_avg:60.90ms
step:477/2330 train_time:29048ms step_avg:60.90ms
step:478/2330 train_time:29112ms step_avg:60.90ms
step:479/2330 train_time:29172ms step_avg:60.90ms
step:480/2330 train_time:29233ms step_avg:60.90ms
step:481/2330 train_time:29293ms step_avg:60.90ms
step:482/2330 train_time:29356ms step_avg:60.90ms
step:483/2330 train_time:29416ms step_avg:60.90ms
step:484/2330 train_time:29478ms step_avg:60.91ms
step:485/2330 train_time:29538ms step_avg:60.90ms
step:486/2330 train_time:29601ms step_avg:60.91ms
step:487/2330 train_time:29659ms step_avg:60.90ms
step:488/2330 train_time:29722ms step_avg:60.91ms
step:489/2330 train_time:29782ms step_avg:60.90ms
step:490/2330 train_time:29844ms step_avg:60.91ms
step:491/2330 train_time:29904ms step_avg:60.90ms
step:492/2330 train_time:29966ms step_avg:60.91ms
step:493/2330 train_time:30026ms step_avg:60.90ms
step:494/2330 train_time:30089ms step_avg:60.91ms
step:495/2330 train_time:30149ms step_avg:60.91ms
step:496/2330 train_time:30212ms step_avg:60.91ms
step:497/2330 train_time:30271ms step_avg:60.91ms
step:498/2330 train_time:30334ms step_avg:60.91ms
step:499/2330 train_time:30393ms step_avg:60.91ms
step:500/2330 train_time:30455ms step_avg:60.91ms
step:500/2330 val_loss:4.7116 train_time:30528ms step_avg:61.06ms
step:501/2330 train_time:30551ms step_avg:60.98ms
step:502/2330 train_time:30581ms step_avg:60.92ms
step:503/2330 train_time:30641ms step_avg:60.92ms
step:504/2330 train_time:30706ms step_avg:60.92ms
step:505/2330 train_time:30767ms step_avg:60.92ms
step:506/2330 train_time:30831ms step_avg:60.93ms
step:507/2330 train_time:30891ms step_avg:60.93ms
step:508/2330 train_time:30954ms step_avg:60.93ms
step:509/2330 train_time:31013ms step_avg:60.93ms
step:510/2330 train_time:31075ms step_avg:60.93ms
step:511/2330 train_time:31135ms step_avg:60.93ms
step:512/2330 train_time:31196ms step_avg:60.93ms
step:513/2330 train_time:31255ms step_avg:60.93ms
step:514/2330 train_time:31316ms step_avg:60.93ms
step:515/2330 train_time:31375ms step_avg:60.92ms
step:516/2330 train_time:31437ms step_avg:60.92ms
step:517/2330 train_time:31498ms step_avg:60.92ms
step:518/2330 train_time:31561ms step_avg:60.93ms
step:519/2330 train_time:31622ms step_avg:60.93ms
step:520/2330 train_time:31685ms step_avg:60.93ms
step:521/2330 train_time:31745ms step_avg:60.93ms
step:522/2330 train_time:31808ms step_avg:60.94ms
step:523/2330 train_time:31867ms step_avg:60.93ms
step:524/2330 train_time:31930ms step_avg:60.93ms
step:525/2330 train_time:31989ms step_avg:60.93ms
step:526/2330 train_time:32052ms step_avg:60.94ms
step:527/2330 train_time:32111ms step_avg:60.93ms
step:528/2330 train_time:32173ms step_avg:60.93ms
step:529/2330 train_time:32232ms step_avg:60.93ms
step:530/2330 train_time:32295ms step_avg:60.93ms
step:531/2330 train_time:32354ms step_avg:60.93ms
step:532/2330 train_time:32416ms step_avg:60.93ms
step:533/2330 train_time:32475ms step_avg:60.93ms
step:534/2330 train_time:32538ms step_avg:60.93ms
step:535/2330 train_time:32599ms step_avg:60.93ms
step:536/2330 train_time:32662ms step_avg:60.94ms
step:537/2330 train_time:32722ms step_avg:60.94ms
step:538/2330 train_time:32784ms step_avg:60.94ms
step:539/2330 train_time:32844ms step_avg:60.94ms
step:540/2330 train_time:32907ms step_avg:60.94ms
step:541/2330 train_time:32966ms step_avg:60.94ms
step:542/2330 train_time:33029ms step_avg:60.94ms
step:543/2330 train_time:33088ms step_avg:60.94ms
step:544/2330 train_time:33151ms step_avg:60.94ms
step:545/2330 train_time:33209ms step_avg:60.93ms
step:546/2330 train_time:33271ms step_avg:60.94ms
step:547/2330 train_time:33330ms step_avg:60.93ms
step:548/2330 train_time:33392ms step_avg:60.93ms
step:549/2330 train_time:33452ms step_avg:60.93ms
step:550/2330 train_time:33514ms step_avg:60.93ms
step:551/2330 train_time:33575ms step_avg:60.93ms
step:552/2330 train_time:33639ms step_avg:60.94ms
step:553/2330 train_time:33699ms step_avg:60.94ms
step:554/2330 train_time:33762ms step_avg:60.94ms
step:555/2330 train_time:33821ms step_avg:60.94ms
step:556/2330 train_time:33883ms step_avg:60.94ms
step:557/2330 train_time:33945ms step_avg:60.94ms
step:558/2330 train_time:34006ms step_avg:60.94ms
step:559/2330 train_time:34064ms step_avg:60.94ms
step:560/2330 train_time:34127ms step_avg:60.94ms
step:561/2330 train_time:34186ms step_avg:60.94ms
step:562/2330 train_time:34249ms step_avg:60.94ms
step:563/2330 train_time:34308ms step_avg:60.94ms
step:564/2330 train_time:34371ms step_avg:60.94ms
step:565/2330 train_time:34429ms step_avg:60.94ms
step:566/2330 train_time:34492ms step_avg:60.94ms
step:567/2330 train_time:34552ms step_avg:60.94ms
step:568/2330 train_time:34615ms step_avg:60.94ms
step:569/2330 train_time:34676ms step_avg:60.94ms
step:570/2330 train_time:34741ms step_avg:60.95ms
step:571/2330 train_time:34799ms step_avg:60.94ms
step:572/2330 train_time:34862ms step_avg:60.95ms
step:573/2330 train_time:34922ms step_avg:60.95ms
step:574/2330 train_time:34983ms step_avg:60.95ms
step:575/2330 train_time:35042ms step_avg:60.94ms
step:576/2330 train_time:35104ms step_avg:60.94ms
step:577/2330 train_time:35164ms step_avg:60.94ms
step:578/2330 train_time:35226ms step_avg:60.94ms
step:579/2330 train_time:35285ms step_avg:60.94ms
step:580/2330 train_time:35349ms step_avg:60.95ms
step:581/2330 train_time:35408ms step_avg:60.94ms
step:582/2330 train_time:35470ms step_avg:60.95ms
step:583/2330 train_time:35530ms step_avg:60.94ms
step:584/2330 train_time:35593ms step_avg:60.95ms
step:585/2330 train_time:35653ms step_avg:60.95ms
step:586/2330 train_time:35715ms step_avg:60.95ms
step:587/2330 train_time:35776ms step_avg:60.95ms
step:588/2330 train_time:35839ms step_avg:60.95ms
step:589/2330 train_time:35899ms step_avg:60.95ms
step:590/2330 train_time:35962ms step_avg:60.95ms
step:591/2330 train_time:36021ms step_avg:60.95ms
step:592/2330 train_time:36083ms step_avg:60.95ms
step:593/2330 train_time:36143ms step_avg:60.95ms
step:594/2330 train_time:36204ms step_avg:60.95ms
step:595/2330 train_time:36263ms step_avg:60.95ms
step:596/2330 train_time:36326ms step_avg:60.95ms
step:597/2330 train_time:36386ms step_avg:60.95ms
step:598/2330 train_time:36450ms step_avg:60.95ms
step:599/2330 train_time:36508ms step_avg:60.95ms
step:600/2330 train_time:36571ms step_avg:60.95ms
step:601/2330 train_time:36630ms step_avg:60.95ms
step:602/2330 train_time:36693ms step_avg:60.95ms
step:603/2330 train_time:36753ms step_avg:60.95ms
step:604/2330 train_time:36816ms step_avg:60.95ms
step:605/2330 train_time:36876ms step_avg:60.95ms
step:606/2330 train_time:36940ms step_avg:60.96ms
step:607/2330 train_time:37000ms step_avg:60.95ms
step:608/2330 train_time:37063ms step_avg:60.96ms
step:609/2330 train_time:37121ms step_avg:60.95ms
step:610/2330 train_time:37184ms step_avg:60.96ms
step:611/2330 train_time:37244ms step_avg:60.96ms
step:612/2330 train_time:37306ms step_avg:60.96ms
step:613/2330 train_time:37365ms step_avg:60.95ms
step:614/2330 train_time:37428ms step_avg:60.96ms
step:615/2330 train_time:37487ms step_avg:60.96ms
step:616/2330 train_time:37550ms step_avg:60.96ms
step:617/2330 train_time:37610ms step_avg:60.96ms
step:618/2330 train_time:37672ms step_avg:60.96ms
step:619/2330 train_time:37732ms step_avg:60.96ms
step:620/2330 train_time:37795ms step_avg:60.96ms
step:621/2330 train_time:37855ms step_avg:60.96ms
step:622/2330 train_time:37918ms step_avg:60.96ms
step:623/2330 train_time:37978ms step_avg:60.96ms
step:624/2330 train_time:38041ms step_avg:60.96ms
step:625/2330 train_time:38101ms step_avg:60.96ms
step:626/2330 train_time:38163ms step_avg:60.96ms
step:627/2330 train_time:38223ms step_avg:60.96ms
step:628/2330 train_time:38285ms step_avg:60.96ms
step:629/2330 train_time:38345ms step_avg:60.96ms
step:630/2330 train_time:38407ms step_avg:60.96ms
step:631/2330 train_time:38466ms step_avg:60.96ms
step:632/2330 train_time:38529ms step_avg:60.96ms
step:633/2330 train_time:38588ms step_avg:60.96ms
step:634/2330 train_time:38651ms step_avg:60.96ms
step:635/2330 train_time:38711ms step_avg:60.96ms
step:636/2330 train_time:38773ms step_avg:60.96ms
step:637/2330 train_time:38833ms step_avg:60.96ms
step:638/2330 train_time:38895ms step_avg:60.96ms
step:639/2330 train_time:38955ms step_avg:60.96ms
step:640/2330 train_time:39018ms step_avg:60.97ms
step:641/2330 train_time:39078ms step_avg:60.96ms
step:642/2330 train_time:39142ms step_avg:60.97ms
step:643/2330 train_time:39201ms step_avg:60.97ms
step:644/2330 train_time:39263ms step_avg:60.97ms
step:645/2330 train_time:39323ms step_avg:60.97ms
step:646/2330 train_time:39384ms step_avg:60.97ms
step:647/2330 train_time:39444ms step_avg:60.96ms
step:648/2330 train_time:39506ms step_avg:60.97ms
step:649/2330 train_time:39566ms step_avg:60.96ms
step:650/2330 train_time:39629ms step_avg:60.97ms
step:651/2330 train_time:39688ms step_avg:60.97ms
step:652/2330 train_time:39751ms step_avg:60.97ms
step:653/2330 train_time:39811ms step_avg:60.97ms
step:654/2330 train_time:39873ms step_avg:60.97ms
step:655/2330 train_time:39933ms step_avg:60.97ms
step:656/2330 train_time:39996ms step_avg:60.97ms
step:657/2330 train_time:40055ms step_avg:60.97ms
step:658/2330 train_time:40118ms step_avg:60.97ms
step:659/2330 train_time:40178ms step_avg:60.97ms
step:660/2330 train_time:40242ms step_avg:60.97ms
step:661/2330 train_time:40302ms step_avg:60.97ms
step:662/2330 train_time:40363ms step_avg:60.97ms
step:663/2330 train_time:40422ms step_avg:60.97ms
step:664/2330 train_time:40486ms step_avg:60.97ms
step:665/2330 train_time:40545ms step_avg:60.97ms
step:666/2330 train_time:40609ms step_avg:60.97ms
step:667/2330 train_time:40668ms step_avg:60.97ms
step:668/2330 train_time:40731ms step_avg:60.97ms
step:669/2330 train_time:40790ms step_avg:60.97ms
step:670/2330 train_time:40852ms step_avg:60.97ms
step:671/2330 train_time:40912ms step_avg:60.97ms
step:672/2330 train_time:40975ms step_avg:60.97ms
step:673/2330 train_time:41035ms step_avg:60.97ms
step:674/2330 train_time:41098ms step_avg:60.98ms
step:675/2330 train_time:41159ms step_avg:60.98ms
step:676/2330 train_time:41221ms step_avg:60.98ms
step:677/2330 train_time:41280ms step_avg:60.98ms
step:678/2330 train_time:41343ms step_avg:60.98ms
step:679/2330 train_time:41403ms step_avg:60.98ms
step:680/2330 train_time:41464ms step_avg:60.98ms
step:681/2330 train_time:41525ms step_avg:60.98ms
step:682/2330 train_time:41587ms step_avg:60.98ms
step:683/2330 train_time:41646ms step_avg:60.98ms
step:684/2330 train_time:41710ms step_avg:60.98ms
step:685/2330 train_time:41769ms step_avg:60.98ms
step:686/2330 train_time:41832ms step_avg:60.98ms
step:687/2330 train_time:41891ms step_avg:60.98ms
step:688/2330 train_time:41953ms step_avg:60.98ms
step:689/2330 train_time:42013ms step_avg:60.98ms
step:690/2330 train_time:42077ms step_avg:60.98ms
step:691/2330 train_time:42139ms step_avg:60.98ms
step:692/2330 train_time:42201ms step_avg:60.98ms
step:693/2330 train_time:42260ms step_avg:60.98ms
step:694/2330 train_time:42323ms step_avg:60.98ms
step:695/2330 train_time:42382ms step_avg:60.98ms
step:696/2330 train_time:42445ms step_avg:60.98ms
step:697/2330 train_time:42504ms step_avg:60.98ms
step:698/2330 train_time:42565ms step_avg:60.98ms
step:699/2330 train_time:42625ms step_avg:60.98ms
step:700/2330 train_time:42688ms step_avg:60.98ms
step:701/2330 train_time:42747ms step_avg:60.98ms
step:702/2330 train_time:42810ms step_avg:60.98ms
step:703/2330 train_time:42869ms step_avg:60.98ms
step:704/2330 train_time:42932ms step_avg:60.98ms
step:705/2330 train_time:42992ms step_avg:60.98ms
step:706/2330 train_time:43055ms step_avg:60.98ms
step:707/2330 train_time:43115ms step_avg:60.98ms
step:708/2330 train_time:43179ms step_avg:60.99ms
step:709/2330 train_time:43240ms step_avg:60.99ms
step:710/2330 train_time:43302ms step_avg:60.99ms
step:711/2330 train_time:43361ms step_avg:60.99ms
step:712/2330 train_time:43424ms step_avg:60.99ms
step:713/2330 train_time:43483ms step_avg:60.99ms
step:714/2330 train_time:43546ms step_avg:60.99ms
step:715/2330 train_time:43605ms step_avg:60.99ms
step:716/2330 train_time:43667ms step_avg:60.99ms
step:717/2330 train_time:43726ms step_avg:60.99ms
step:718/2330 train_time:43789ms step_avg:60.99ms
step:719/2330 train_time:43849ms step_avg:60.99ms
step:720/2330 train_time:43911ms step_avg:60.99ms
step:721/2330 train_time:43970ms step_avg:60.98ms
step:722/2330 train_time:44032ms step_avg:60.99ms
step:723/2330 train_time:44093ms step_avg:60.99ms
step:724/2330 train_time:44156ms step_avg:60.99ms
step:725/2330 train_time:44216ms step_avg:60.99ms
step:726/2330 train_time:44280ms step_avg:60.99ms
step:727/2330 train_time:44341ms step_avg:60.99ms
step:728/2330 train_time:44402ms step_avg:60.99ms
step:729/2330 train_time:44462ms step_avg:60.99ms
step:730/2330 train_time:44525ms step_avg:60.99ms
step:731/2330 train_time:44584ms step_avg:60.99ms
step:732/2330 train_time:44647ms step_avg:60.99ms
step:733/2330 train_time:44705ms step_avg:60.99ms
step:734/2330 train_time:44768ms step_avg:60.99ms
step:735/2330 train_time:44827ms step_avg:60.99ms
step:736/2330 train_time:44890ms step_avg:60.99ms
step:737/2330 train_time:44950ms step_avg:60.99ms
step:738/2330 train_time:45013ms step_avg:60.99ms
step:739/2330 train_time:45072ms step_avg:60.99ms
step:740/2330 train_time:45134ms step_avg:60.99ms
step:741/2330 train_time:45195ms step_avg:60.99ms
step:742/2330 train_time:45259ms step_avg:61.00ms
step:743/2330 train_time:45319ms step_avg:60.99ms
step:744/2330 train_time:45382ms step_avg:61.00ms
step:745/2330 train_time:45442ms step_avg:61.00ms
step:746/2330 train_time:45504ms step_avg:61.00ms
step:747/2330 train_time:45562ms step_avg:60.99ms
step:748/2330 train_time:45624ms step_avg:61.00ms
step:749/2330 train_time:45684ms step_avg:60.99ms
step:750/2330 train_time:45747ms step_avg:61.00ms
step:750/2330 val_loss:4.4308 train_time:45818ms step_avg:61.09ms
step:751/2330 train_time:45841ms step_avg:61.04ms
step:752/2330 train_time:45870ms step_avg:61.00ms
step:753/2330 train_time:45931ms step_avg:61.00ms
step:754/2330 train_time:46000ms step_avg:61.01ms
step:755/2330 train_time:46060ms step_avg:61.01ms
step:756/2330 train_time:46122ms step_avg:61.01ms
step:757/2330 train_time:46181ms step_avg:61.01ms
step:758/2330 train_time:46243ms step_avg:61.01ms
step:759/2330 train_time:46302ms step_avg:61.00ms
step:760/2330 train_time:46364ms step_avg:61.00ms
step:761/2330 train_time:46421ms step_avg:61.00ms
step:762/2330 train_time:46485ms step_avg:61.00ms
step:763/2330 train_time:46543ms step_avg:61.00ms
step:764/2330 train_time:46605ms step_avg:61.00ms
step:765/2330 train_time:46664ms step_avg:61.00ms
step:766/2330 train_time:46726ms step_avg:61.00ms
step:767/2330 train_time:46786ms step_avg:61.00ms
step:768/2330 train_time:46851ms step_avg:61.00ms
step:769/2330 train_time:46912ms step_avg:61.00ms
step:770/2330 train_time:46977ms step_avg:61.01ms
step:771/2330 train_time:47037ms step_avg:61.01ms
step:772/2330 train_time:47102ms step_avg:61.01ms
step:773/2330 train_time:47162ms step_avg:61.01ms
step:774/2330 train_time:47224ms step_avg:61.01ms
step:775/2330 train_time:47285ms step_avg:61.01ms
step:776/2330 train_time:47348ms step_avg:61.02ms
step:777/2330 train_time:47407ms step_avg:61.01ms
step:778/2330 train_time:47471ms step_avg:61.02ms
step:779/2330 train_time:47531ms step_avg:61.02ms
step:780/2330 train_time:47595ms step_avg:61.02ms
step:781/2330 train_time:47654ms step_avg:61.02ms
step:782/2330 train_time:47718ms step_avg:61.02ms
step:783/2330 train_time:47780ms step_avg:61.02ms
step:784/2330 train_time:47841ms step_avg:61.02ms
step:785/2330 train_time:47901ms step_avg:61.02ms
step:786/2330 train_time:47965ms step_avg:61.02ms
step:787/2330 train_time:48025ms step_avg:61.02ms
step:788/2330 train_time:48089ms step_avg:61.03ms
step:789/2330 train_time:48149ms step_avg:61.02ms
step:790/2330 train_time:48213ms step_avg:61.03ms
step:791/2330 train_time:48274ms step_avg:61.03ms
step:792/2330 train_time:48338ms step_avg:61.03ms
step:793/2330 train_time:48397ms step_avg:61.03ms
step:794/2330 train_time:48460ms step_avg:61.03ms
step:795/2330 train_time:48519ms step_avg:61.03ms
step:796/2330 train_time:48582ms step_avg:61.03ms
step:797/2330 train_time:48642ms step_avg:61.03ms
step:798/2330 train_time:48705ms step_avg:61.03ms
step:799/2330 train_time:48766ms step_avg:61.03ms
step:800/2330 train_time:48828ms step_avg:61.03ms
step:801/2330 train_time:48888ms step_avg:61.03ms
step:802/2330 train_time:48952ms step_avg:61.04ms
step:803/2330 train_time:49013ms step_avg:61.04ms
step:804/2330 train_time:49078ms step_avg:61.04ms
step:805/2330 train_time:49138ms step_avg:61.04ms
step:806/2330 train_time:49201ms step_avg:61.04ms
step:807/2330 train_time:49261ms step_avg:61.04ms
step:808/2330 train_time:49324ms step_avg:61.04ms
step:809/2330 train_time:49384ms step_avg:61.04ms
step:810/2330 train_time:49447ms step_avg:61.05ms
step:811/2330 train_time:49507ms step_avg:61.04ms
step:812/2330 train_time:49570ms step_avg:61.05ms
step:813/2330 train_time:49631ms step_avg:61.05ms
step:814/2330 train_time:49695ms step_avg:61.05ms
step:815/2330 train_time:49755ms step_avg:61.05ms
step:816/2330 train_time:49818ms step_avg:61.05ms
step:817/2330 train_time:49878ms step_avg:61.05ms
step:818/2330 train_time:49940ms step_avg:61.05ms
step:819/2330 train_time:50000ms step_avg:61.05ms
step:820/2330 train_time:50064ms step_avg:61.05ms
step:821/2330 train_time:50123ms step_avg:61.05ms
step:822/2330 train_time:50188ms step_avg:61.06ms
step:823/2330 train_time:50247ms step_avg:61.05ms
step:824/2330 train_time:50311ms step_avg:61.06ms
step:825/2330 train_time:50372ms step_avg:61.06ms
step:826/2330 train_time:50435ms step_avg:61.06ms
step:827/2330 train_time:50496ms step_avg:61.06ms
step:828/2330 train_time:50559ms step_avg:61.06ms
step:829/2330 train_time:50619ms step_avg:61.06ms
step:830/2330 train_time:50682ms step_avg:61.06ms
step:831/2330 train_time:50742ms step_avg:61.06ms
step:832/2330 train_time:50804ms step_avg:61.06ms
step:833/2330 train_time:50863ms step_avg:61.06ms
step:834/2330 train_time:50926ms step_avg:61.06ms
step:835/2330 train_time:50987ms step_avg:61.06ms
step:836/2330 train_time:51051ms step_avg:61.07ms
step:837/2330 train_time:51110ms step_avg:61.06ms
step:838/2330 train_time:51175ms step_avg:61.07ms
step:839/2330 train_time:51235ms step_avg:61.07ms
step:840/2330 train_time:51298ms step_avg:61.07ms
step:841/2330 train_time:51358ms step_avg:61.07ms
step:842/2330 train_time:51421ms step_avg:61.07ms
step:843/2330 train_time:51481ms step_avg:61.07ms
step:844/2330 train_time:51544ms step_avg:61.07ms
step:845/2330 train_time:51604ms step_avg:61.07ms
step:846/2330 train_time:51667ms step_avg:61.07ms
step:847/2330 train_time:51727ms step_avg:61.07ms
step:848/2330 train_time:51791ms step_avg:61.07ms
step:849/2330 train_time:51851ms step_avg:61.07ms
step:850/2330 train_time:51915ms step_avg:61.08ms
step:851/2330 train_time:51976ms step_avg:61.08ms
step:852/2330 train_time:52039ms step_avg:61.08ms
step:853/2330 train_time:52099ms step_avg:61.08ms
step:854/2330 train_time:52162ms step_avg:61.08ms
step:855/2330 train_time:52223ms step_avg:61.08ms
step:856/2330 train_time:52286ms step_avg:61.08ms
step:857/2330 train_time:52347ms step_avg:61.08ms
step:858/2330 train_time:52410ms step_avg:61.08ms
step:859/2330 train_time:52470ms step_avg:61.08ms
step:860/2330 train_time:52533ms step_avg:61.09ms
step:861/2330 train_time:52594ms step_avg:61.08ms
step:862/2330 train_time:52656ms step_avg:61.09ms
step:863/2330 train_time:52717ms step_avg:61.09ms
step:864/2330 train_time:52780ms step_avg:61.09ms
step:865/2330 train_time:52840ms step_avg:61.09ms
step:866/2330 train_time:52903ms step_avg:61.09ms
step:867/2330 train_time:52964ms step_avg:61.09ms
step:868/2330 train_time:53027ms step_avg:61.09ms
step:869/2330 train_time:53088ms step_avg:61.09ms
step:870/2330 train_time:53152ms step_avg:61.09ms
step:871/2330 train_time:53213ms step_avg:61.09ms
step:872/2330 train_time:53277ms step_avg:61.10ms
step:873/2330 train_time:53337ms step_avg:61.10ms
step:874/2330 train_time:53400ms step_avg:61.10ms
step:875/2330 train_time:53460ms step_avg:61.10ms
step:876/2330 train_time:53522ms step_avg:61.10ms
step:877/2330 train_time:53583ms step_avg:61.10ms
step:878/2330 train_time:53647ms step_avg:61.10ms
step:879/2330 train_time:53706ms step_avg:61.10ms
step:880/2330 train_time:53769ms step_avg:61.10ms
step:881/2330 train_time:53829ms step_avg:61.10ms
step:882/2330 train_time:53893ms step_avg:61.10ms
step:883/2330 train_time:53955ms step_avg:61.10ms
step:884/2330 train_time:54019ms step_avg:61.11ms
step:885/2330 train_time:54080ms step_avg:61.11ms
step:886/2330 train_time:54142ms step_avg:61.11ms
step:887/2330 train_time:54202ms step_avg:61.11ms
step:888/2330 train_time:54266ms step_avg:61.11ms
step:889/2330 train_time:54325ms step_avg:61.11ms
step:890/2330 train_time:54388ms step_avg:61.11ms
step:891/2330 train_time:54448ms step_avg:61.11ms
step:892/2330 train_time:54512ms step_avg:61.11ms
step:893/2330 train_time:54573ms step_avg:61.11ms
step:894/2330 train_time:54637ms step_avg:61.12ms
step:895/2330 train_time:54697ms step_avg:61.11ms
step:896/2330 train_time:54760ms step_avg:61.12ms
step:897/2330 train_time:54820ms step_avg:61.11ms
step:898/2330 train_time:54883ms step_avg:61.12ms
step:899/2330 train_time:54943ms step_avg:61.12ms
step:900/2330 train_time:55007ms step_avg:61.12ms
step:901/2330 train_time:55066ms step_avg:61.12ms
step:902/2330 train_time:55129ms step_avg:61.12ms
step:903/2330 train_time:55190ms step_avg:61.12ms
step:904/2330 train_time:55253ms step_avg:61.12ms
step:905/2330 train_time:55314ms step_avg:61.12ms
step:906/2330 train_time:55378ms step_avg:61.12ms
step:907/2330 train_time:55437ms step_avg:61.12ms
step:908/2330 train_time:55500ms step_avg:61.12ms
step:909/2330 train_time:55560ms step_avg:61.12ms
step:910/2330 train_time:55622ms step_avg:61.12ms
step:911/2330 train_time:55683ms step_avg:61.12ms
step:912/2330 train_time:55745ms step_avg:61.12ms
step:913/2330 train_time:55805ms step_avg:61.12ms
step:914/2330 train_time:55868ms step_avg:61.12ms
step:915/2330 train_time:55928ms step_avg:61.12ms
step:916/2330 train_time:55992ms step_avg:61.13ms
step:917/2330 train_time:56053ms step_avg:61.13ms
step:918/2330 train_time:56116ms step_avg:61.13ms
step:919/2330 train_time:56177ms step_avg:61.13ms
step:920/2330 train_time:56239ms step_avg:61.13ms
step:921/2330 train_time:56299ms step_avg:61.13ms
step:922/2330 train_time:56362ms step_avg:61.13ms
step:923/2330 train_time:56422ms step_avg:61.13ms
step:924/2330 train_time:56485ms step_avg:61.13ms
step:925/2330 train_time:56546ms step_avg:61.13ms
step:926/2330 train_time:56609ms step_avg:61.13ms
step:927/2330 train_time:56669ms step_avg:61.13ms
step:928/2330 train_time:56732ms step_avg:61.13ms
step:929/2330 train_time:56794ms step_avg:61.13ms
step:930/2330 train_time:56857ms step_avg:61.14ms
step:931/2330 train_time:56917ms step_avg:61.14ms
step:932/2330 train_time:56981ms step_avg:61.14ms
step:933/2330 train_time:57040ms step_avg:61.14ms
step:934/2330 train_time:57103ms step_avg:61.14ms
step:935/2330 train_time:57163ms step_avg:61.14ms
step:936/2330 train_time:57226ms step_avg:61.14ms
step:937/2330 train_time:57286ms step_avg:61.14ms
step:938/2330 train_time:57350ms step_avg:61.14ms
step:939/2330 train_time:57409ms step_avg:61.14ms
step:940/2330 train_time:57474ms step_avg:61.14ms
step:941/2330 train_time:57534ms step_avg:61.14ms
step:942/2330 train_time:57598ms step_avg:61.14ms
step:943/2330 train_time:57658ms step_avg:61.14ms
step:944/2330 train_time:57722ms step_avg:61.15ms
step:945/2330 train_time:57782ms step_avg:61.14ms
step:946/2330 train_time:57846ms step_avg:61.15ms
step:947/2330 train_time:57906ms step_avg:61.15ms
step:948/2330 train_time:57968ms step_avg:61.15ms
step:949/2330 train_time:58029ms step_avg:61.15ms
step:950/2330 train_time:58092ms step_avg:61.15ms
step:951/2330 train_time:58152ms step_avg:61.15ms
step:952/2330 train_time:58216ms step_avg:61.15ms
step:953/2330 train_time:58277ms step_avg:61.15ms
step:954/2330 train_time:58340ms step_avg:61.15ms
step:955/2330 train_time:58400ms step_avg:61.15ms
step:956/2330 train_time:58463ms step_avg:61.15ms
step:957/2330 train_time:58523ms step_avg:61.15ms
step:958/2330 train_time:58586ms step_avg:61.15ms
step:959/2330 train_time:58646ms step_avg:61.15ms
step:960/2330 train_time:58709ms step_avg:61.16ms
step:961/2330 train_time:58770ms step_avg:61.15ms
step:962/2330 train_time:58833ms step_avg:61.16ms
step:963/2330 train_time:58894ms step_avg:61.16ms
step:964/2330 train_time:58958ms step_avg:61.16ms
step:965/2330 train_time:59017ms step_avg:61.16ms
step:966/2330 train_time:59081ms step_avg:61.16ms
step:967/2330 train_time:59140ms step_avg:61.16ms
step:968/2330 train_time:59203ms step_avg:61.16ms
step:969/2330 train_time:59263ms step_avg:61.16ms
step:970/2330 train_time:59326ms step_avg:61.16ms
step:971/2330 train_time:59387ms step_avg:61.16ms
step:972/2330 train_time:59450ms step_avg:61.16ms
step:973/2330 train_time:59511ms step_avg:61.16ms
step:974/2330 train_time:59576ms step_avg:61.17ms
step:975/2330 train_time:59637ms step_avg:61.17ms
step:976/2330 train_time:59700ms step_avg:61.17ms
step:977/2330 train_time:59759ms step_avg:61.17ms
step:978/2330 train_time:59822ms step_avg:61.17ms
step:979/2330 train_time:59883ms step_avg:61.17ms
step:980/2330 train_time:59946ms step_avg:61.17ms
step:981/2330 train_time:60006ms step_avg:61.17ms
step:982/2330 train_time:60069ms step_avg:61.17ms
step:983/2330 train_time:60129ms step_avg:61.17ms
step:984/2330 train_time:60193ms step_avg:61.17ms
step:985/2330 train_time:60255ms step_avg:61.17ms
step:986/2330 train_time:60318ms step_avg:61.17ms
step:987/2330 train_time:60379ms step_avg:61.17ms
step:988/2330 train_time:60441ms step_avg:61.18ms
step:989/2330 train_time:60501ms step_avg:61.17ms
step:990/2330 train_time:60564ms step_avg:61.18ms
step:991/2330 train_time:60624ms step_avg:61.18ms
step:992/2330 train_time:60688ms step_avg:61.18ms
step:993/2330 train_time:60748ms step_avg:61.18ms
step:994/2330 train_time:60811ms step_avg:61.18ms
step:995/2330 train_time:60873ms step_avg:61.18ms
step:996/2330 train_time:60936ms step_avg:61.18ms
step:997/2330 train_time:60996ms step_avg:61.18ms
step:998/2330 train_time:61058ms step_avg:61.18ms
step:999/2330 train_time:61119ms step_avg:61.18ms
step:1000/2330 train_time:61182ms step_avg:61.18ms
step:1000/2330 val_loss:4.1723 train_time:61255ms step_avg:61.26ms
step:1001/2330 train_time:61278ms step_avg:61.22ms
step:1002/2330 train_time:61306ms step_avg:61.18ms
step:1003/2330 train_time:61368ms step_avg:61.18ms
step:1004/2330 train_time:61438ms step_avg:61.19ms
step:1005/2330 train_time:61502ms step_avg:61.20ms
step:1006/2330 train_time:61565ms step_avg:61.20ms
step:1007/2330 train_time:61625ms step_avg:61.20ms
step:1008/2330 train_time:61688ms step_avg:61.20ms
step:1009/2330 train_time:61747ms step_avg:61.20ms
step:1010/2330 train_time:61810ms step_avg:61.20ms
step:1011/2330 train_time:61869ms step_avg:61.20ms
step:1012/2330 train_time:61932ms step_avg:61.20ms
step:1013/2330 train_time:61991ms step_avg:61.20ms
step:1014/2330 train_time:62052ms step_avg:61.20ms
step:1015/2330 train_time:62112ms step_avg:61.19ms
step:1016/2330 train_time:62175ms step_avg:61.20ms
step:1017/2330 train_time:62238ms step_avg:61.20ms
step:1018/2330 train_time:62303ms step_avg:61.20ms
step:1019/2330 train_time:62364ms step_avg:61.20ms
step:1020/2330 train_time:62429ms step_avg:61.20ms
step:1021/2330 train_time:62490ms step_avg:61.21ms
step:1022/2330 train_time:62554ms step_avg:61.21ms
step:1023/2330 train_time:62614ms step_avg:61.21ms
step:1024/2330 train_time:62677ms step_avg:61.21ms
step:1025/2330 train_time:62738ms step_avg:61.21ms
step:1026/2330 train_time:62802ms step_avg:61.21ms
step:1027/2330 train_time:62862ms step_avg:61.21ms
step:1028/2330 train_time:62924ms step_avg:61.21ms
step:1029/2330 train_time:62984ms step_avg:61.21ms
step:1030/2330 train_time:63046ms step_avg:61.21ms
step:1031/2330 train_time:63106ms step_avg:61.21ms
step:1032/2330 train_time:63169ms step_avg:61.21ms
step:1033/2330 train_time:63231ms step_avg:61.21ms
step:1034/2330 train_time:63295ms step_avg:61.21ms
step:1035/2330 train_time:63355ms step_avg:61.21ms
step:1036/2330 train_time:63418ms step_avg:61.21ms
step:1037/2330 train_time:63479ms step_avg:61.21ms
step:1038/2330 train_time:63542ms step_avg:61.22ms
step:1039/2330 train_time:63601ms step_avg:61.21ms
step:1040/2330 train_time:63665ms step_avg:61.22ms
step:1041/2330 train_time:63726ms step_avg:61.22ms
step:1042/2330 train_time:63789ms step_avg:61.22ms
step:1043/2330 train_time:63849ms step_avg:61.22ms
step:1044/2330 train_time:63911ms step_avg:61.22ms
step:1045/2330 train_time:63971ms step_avg:61.22ms
step:1046/2330 train_time:64033ms step_avg:61.22ms
step:1047/2330 train_time:64093ms step_avg:61.22ms
step:1048/2330 train_time:64156ms step_avg:61.22ms
step:1049/2330 train_time:64215ms step_avg:61.22ms
step:1050/2330 train_time:64278ms step_avg:61.22ms
step:1051/2330 train_time:64338ms step_avg:61.22ms
step:1052/2330 train_time:64401ms step_avg:61.22ms
step:1053/2330 train_time:64461ms step_avg:61.22ms
step:1054/2330 train_time:64525ms step_avg:61.22ms
step:1055/2330 train_time:64585ms step_avg:61.22ms
step:1056/2330 train_time:64648ms step_avg:61.22ms
step:1057/2330 train_time:64709ms step_avg:61.22ms
step:1058/2330 train_time:64772ms step_avg:61.22ms
step:1059/2330 train_time:64832ms step_avg:61.22ms
step:1060/2330 train_time:64894ms step_avg:61.22ms
step:1061/2330 train_time:64953ms step_avg:61.22ms
step:1062/2330 train_time:65016ms step_avg:61.22ms
step:1063/2330 train_time:65076ms step_avg:61.22ms
step:1064/2330 train_time:65139ms step_avg:61.22ms
step:1065/2330 train_time:65199ms step_avg:61.22ms
step:1066/2330 train_time:65263ms step_avg:61.22ms
step:1067/2330 train_time:65322ms step_avg:61.22ms
step:1068/2330 train_time:65386ms step_avg:61.22ms
step:1069/2330 train_time:65447ms step_avg:61.22ms
step:1070/2330 train_time:65511ms step_avg:61.22ms
step:1071/2330 train_time:65571ms step_avg:61.22ms
step:1072/2330 train_time:65634ms step_avg:61.23ms
step:1073/2330 train_time:65693ms step_avg:61.22ms
step:1074/2330 train_time:65756ms step_avg:61.23ms
step:1075/2330 train_time:65817ms step_avg:61.22ms
step:1076/2330 train_time:65880ms step_avg:61.23ms
step:1077/2330 train_time:65940ms step_avg:61.23ms
step:1078/2330 train_time:66003ms step_avg:61.23ms
step:1079/2330 train_time:66062ms step_avg:61.23ms
step:1080/2330 train_time:66125ms step_avg:61.23ms
step:1081/2330 train_time:66186ms step_avg:61.23ms
step:1082/2330 train_time:66249ms step_avg:61.23ms
step:1083/2330 train_time:66309ms step_avg:61.23ms
step:1084/2330 train_time:66373ms step_avg:61.23ms
step:1085/2330 train_time:66433ms step_avg:61.23ms
step:1086/2330 train_time:66496ms step_avg:61.23ms
step:1087/2330 train_time:66557ms step_avg:61.23ms
step:1088/2330 train_time:66620ms step_avg:61.23ms
step:1089/2330 train_time:66680ms step_avg:61.23ms
step:1090/2330 train_time:66743ms step_avg:61.23ms
step:1091/2330 train_time:66803ms step_avg:61.23ms
step:1092/2330 train_time:66867ms step_avg:61.23ms
step:1093/2330 train_time:66927ms step_avg:61.23ms
step:1094/2330 train_time:66990ms step_avg:61.23ms
step:1095/2330 train_time:67050ms step_avg:61.23ms
step:1096/2330 train_time:67114ms step_avg:61.24ms
step:1097/2330 train_time:67174ms step_avg:61.23ms
step:1098/2330 train_time:67237ms step_avg:61.24ms
step:1099/2330 train_time:67296ms step_avg:61.23ms
step:1100/2330 train_time:67359ms step_avg:61.24ms
step:1101/2330 train_time:67419ms step_avg:61.23ms
step:1102/2330 train_time:67482ms step_avg:61.24ms
step:1103/2330 train_time:67542ms step_avg:61.24ms
step:1104/2330 train_time:67606ms step_avg:61.24ms
step:1105/2330 train_time:67666ms step_avg:61.24ms
step:1106/2330 train_time:67730ms step_avg:61.24ms
step:1107/2330 train_time:67790ms step_avg:61.24ms
step:1108/2330 train_time:67853ms step_avg:61.24ms
step:1109/2330 train_time:67913ms step_avg:61.24ms
step:1110/2330 train_time:67975ms step_avg:61.24ms
step:1111/2330 train_time:68036ms step_avg:61.24ms
step:1112/2330 train_time:68098ms step_avg:61.24ms
step:1113/2330 train_time:68158ms step_avg:61.24ms
step:1114/2330 train_time:68221ms step_avg:61.24ms
step:1115/2330 train_time:68281ms step_avg:61.24ms
step:1116/2330 train_time:68345ms step_avg:61.24ms
step:1117/2330 train_time:68405ms step_avg:61.24ms
step:1118/2330 train_time:68467ms step_avg:61.24ms
step:1119/2330 train_time:68529ms step_avg:61.24ms
step:1120/2330 train_time:68592ms step_avg:61.24ms
step:1121/2330 train_time:68653ms step_avg:61.24ms
step:1122/2330 train_time:68716ms step_avg:61.24ms
step:1123/2330 train_time:68775ms step_avg:61.24ms
step:1124/2330 train_time:68839ms step_avg:61.24ms
step:1125/2330 train_time:68898ms step_avg:61.24ms
step:1126/2330 train_time:68961ms step_avg:61.24ms
step:1127/2330 train_time:69021ms step_avg:61.24ms
step:1128/2330 train_time:69084ms step_avg:61.24ms
step:1129/2330 train_time:69144ms step_avg:61.24ms
step:1130/2330 train_time:69208ms step_avg:61.25ms
step:1131/2330 train_time:69269ms step_avg:61.25ms
step:1132/2330 train_time:69333ms step_avg:61.25ms
step:1133/2330 train_time:69392ms step_avg:61.25ms
step:1134/2330 train_time:69455ms step_avg:61.25ms
step:1135/2330 train_time:69515ms step_avg:61.25ms
step:1136/2330 train_time:69578ms step_avg:61.25ms
step:1137/2330 train_time:69638ms step_avg:61.25ms
step:1138/2330 train_time:69700ms step_avg:61.25ms
step:1139/2330 train_time:69761ms step_avg:61.25ms
step:1140/2330 train_time:69824ms step_avg:61.25ms
step:1141/2330 train_time:69884ms step_avg:61.25ms
step:1142/2330 train_time:69948ms step_avg:61.25ms
step:1143/2330 train_time:70008ms step_avg:61.25ms
step:1144/2330 train_time:70072ms step_avg:61.25ms
step:1145/2330 train_time:70131ms step_avg:61.25ms
step:1146/2330 train_time:70194ms step_avg:61.25ms
step:1147/2330 train_time:70254ms step_avg:61.25ms
step:1148/2330 train_time:70317ms step_avg:61.25ms
step:1149/2330 train_time:70378ms step_avg:61.25ms
step:1150/2330 train_time:70441ms step_avg:61.25ms
step:1151/2330 train_time:70500ms step_avg:61.25ms
step:1152/2330 train_time:70564ms step_avg:61.25ms
step:1153/2330 train_time:70623ms step_avg:61.25ms
step:1154/2330 train_time:70687ms step_avg:61.25ms
step:1155/2330 train_time:70748ms step_avg:61.25ms
step:1156/2330 train_time:70811ms step_avg:61.26ms
step:1157/2330 train_time:70871ms step_avg:61.25ms
step:1158/2330 train_time:70934ms step_avg:61.26ms
step:1159/2330 train_time:70994ms step_avg:61.25ms
step:1160/2330 train_time:71057ms step_avg:61.26ms
step:1161/2330 train_time:71117ms step_avg:61.25ms
step:1162/2330 train_time:71181ms step_avg:61.26ms
step:1163/2330 train_time:71241ms step_avg:61.26ms
step:1164/2330 train_time:71304ms step_avg:61.26ms
step:1165/2330 train_time:71364ms step_avg:61.26ms
step:1166/2330 train_time:71428ms step_avg:61.26ms
step:1167/2330 train_time:71488ms step_avg:61.26ms
step:1168/2330 train_time:71552ms step_avg:61.26ms
step:1169/2330 train_time:71612ms step_avg:61.26ms
step:1170/2330 train_time:71675ms step_avg:61.26ms
step:1171/2330 train_time:71735ms step_avg:61.26ms
step:1172/2330 train_time:71799ms step_avg:61.26ms
step:1173/2330 train_time:71859ms step_avg:61.26ms
step:1174/2330 train_time:71922ms step_avg:61.26ms
step:1175/2330 train_time:71982ms step_avg:61.26ms
step:1176/2330 train_time:72045ms step_avg:61.26ms
step:1177/2330 train_time:72105ms step_avg:61.26ms
step:1178/2330 train_time:72168ms step_avg:61.26ms
step:1179/2330 train_time:72229ms step_avg:61.26ms
step:1180/2330 train_time:72292ms step_avg:61.26ms
step:1181/2330 train_time:72353ms step_avg:61.26ms
step:1182/2330 train_time:72415ms step_avg:61.26ms
step:1183/2330 train_time:72475ms step_avg:61.26ms
step:1184/2330 train_time:72538ms step_avg:61.27ms
step:1185/2330 train_time:72599ms step_avg:61.26ms
step:1186/2330 train_time:72662ms step_avg:61.27ms
step:1187/2330 train_time:72721ms step_avg:61.26ms
step:1188/2330 train_time:72784ms step_avg:61.27ms
step:1189/2330 train_time:72844ms step_avg:61.27ms
step:1190/2330 train_time:72907ms step_avg:61.27ms
step:1191/2330 train_time:72967ms step_avg:61.27ms
step:1192/2330 train_time:73031ms step_avg:61.27ms
step:1193/2330 train_time:73090ms step_avg:61.27ms
step:1194/2330 train_time:73153ms step_avg:61.27ms
step:1195/2330 train_time:73213ms step_avg:61.27ms
step:1196/2330 train_time:73277ms step_avg:61.27ms
step:1197/2330 train_time:73337ms step_avg:61.27ms
step:1198/2330 train_time:73400ms step_avg:61.27ms
step:1199/2330 train_time:73460ms step_avg:61.27ms
step:1200/2330 train_time:73523ms step_avg:61.27ms
step:1201/2330 train_time:73583ms step_avg:61.27ms
step:1202/2330 train_time:73648ms step_avg:61.27ms
step:1203/2330 train_time:73708ms step_avg:61.27ms
step:1204/2330 train_time:73770ms step_avg:61.27ms
step:1205/2330 train_time:73831ms step_avg:61.27ms
step:1206/2330 train_time:73894ms step_avg:61.27ms
step:1207/2330 train_time:73953ms step_avg:61.27ms
step:1208/2330 train_time:74016ms step_avg:61.27ms
step:1209/2330 train_time:74076ms step_avg:61.27ms
step:1210/2330 train_time:74139ms step_avg:61.27ms
step:1211/2330 train_time:74199ms step_avg:61.27ms
step:1212/2330 train_time:74262ms step_avg:61.27ms
step:1213/2330 train_time:74322ms step_avg:61.27ms
step:1214/2330 train_time:74385ms step_avg:61.27ms
step:1215/2330 train_time:74445ms step_avg:61.27ms
step:1216/2330 train_time:74508ms step_avg:61.27ms
step:1217/2330 train_time:74568ms step_avg:61.27ms
step:1218/2330 train_time:74633ms step_avg:61.27ms
step:1219/2330 train_time:74692ms step_avg:61.27ms
step:1220/2330 train_time:74754ms step_avg:61.27ms
step:1221/2330 train_time:74814ms step_avg:61.27ms
step:1222/2330 train_time:74876ms step_avg:61.27ms
step:1223/2330 train_time:74936ms step_avg:61.27ms
step:1224/2330 train_time:75000ms step_avg:61.27ms
step:1225/2330 train_time:75060ms step_avg:61.27ms
step:1226/2330 train_time:75123ms step_avg:61.27ms
step:1227/2330 train_time:75183ms step_avg:61.27ms
step:1228/2330 train_time:75246ms step_avg:61.28ms
step:1229/2330 train_time:75306ms step_avg:61.27ms
step:1230/2330 train_time:75369ms step_avg:61.28ms
step:1231/2330 train_time:75430ms step_avg:61.27ms
step:1232/2330 train_time:75492ms step_avg:61.28ms
step:1233/2330 train_time:75553ms step_avg:61.28ms
step:1234/2330 train_time:75616ms step_avg:61.28ms
step:1235/2330 train_time:75676ms step_avg:61.28ms
step:1236/2330 train_time:75739ms step_avg:61.28ms
step:1237/2330 train_time:75799ms step_avg:61.28ms
step:1238/2330 train_time:75862ms step_avg:61.28ms
step:1239/2330 train_time:75922ms step_avg:61.28ms
step:1240/2330 train_time:75986ms step_avg:61.28ms
step:1241/2330 train_time:76047ms step_avg:61.28ms
step:1242/2330 train_time:76111ms step_avg:61.28ms
step:1243/2330 train_time:76171ms step_avg:61.28ms
step:1244/2330 train_time:76234ms step_avg:61.28ms
step:1245/2330 train_time:76293ms step_avg:61.28ms
step:1246/2330 train_time:76356ms step_avg:61.28ms
step:1247/2330 train_time:76417ms step_avg:61.28ms
step:1248/2330 train_time:76480ms step_avg:61.28ms
step:1249/2330 train_time:76540ms step_avg:61.28ms
step:1250/2330 train_time:76603ms step_avg:61.28ms
step:1250/2330 val_loss:4.0252 train_time:76677ms step_avg:61.34ms
step:1251/2330 train_time:76700ms step_avg:61.31ms
step:1252/2330 train_time:76730ms step_avg:61.29ms
step:1253/2330 train_time:76794ms step_avg:61.29ms
step:1254/2330 train_time:76862ms step_avg:61.29ms
step:1255/2330 train_time:76923ms step_avg:61.29ms
step:1256/2330 train_time:76986ms step_avg:61.29ms
step:1257/2330 train_time:77046ms step_avg:61.29ms
step:1258/2330 train_time:77107ms step_avg:61.29ms
step:1259/2330 train_time:77167ms step_avg:61.29ms
step:1260/2330 train_time:77230ms step_avg:61.29ms
step:1261/2330 train_time:77289ms step_avg:61.29ms
step:1262/2330 train_time:77352ms step_avg:61.29ms
step:1263/2330 train_time:77411ms step_avg:61.29ms
step:1264/2330 train_time:77473ms step_avg:61.29ms
step:1265/2330 train_time:77533ms step_avg:61.29ms
step:1266/2330 train_time:77595ms step_avg:61.29ms
step:1267/2330 train_time:77656ms step_avg:61.29ms
step:1268/2330 train_time:77719ms step_avg:61.29ms
step:1269/2330 train_time:77781ms step_avg:61.29ms
step:1270/2330 train_time:77846ms step_avg:61.30ms
step:1271/2330 train_time:77907ms step_avg:61.30ms
step:1272/2330 train_time:77971ms step_avg:61.30ms
step:1273/2330 train_time:78030ms step_avg:61.30ms
step:1274/2330 train_time:78092ms step_avg:61.30ms
step:1275/2330 train_time:78153ms step_avg:61.30ms
step:1276/2330 train_time:78215ms step_avg:61.30ms
step:1277/2330 train_time:78275ms step_avg:61.30ms
step:1278/2330 train_time:78338ms step_avg:61.30ms
step:1279/2330 train_time:78398ms step_avg:61.30ms
step:1280/2330 train_time:78461ms step_avg:61.30ms
step:1281/2330 train_time:78520ms step_avg:61.30ms
step:1282/2330 train_time:78583ms step_avg:61.30ms
step:1283/2330 train_time:78643ms step_avg:61.30ms
step:1284/2330 train_time:78706ms step_avg:61.30ms
step:1285/2330 train_time:78767ms step_avg:61.30ms
step:1286/2330 train_time:78832ms step_avg:61.30ms
step:1287/2330 train_time:78892ms step_avg:61.30ms
step:1288/2330 train_time:78957ms step_avg:61.30ms
step:1289/2330 train_time:79017ms step_avg:61.30ms
step:1290/2330 train_time:79081ms step_avg:61.30ms
step:1291/2330 train_time:79141ms step_avg:61.30ms
step:1292/2330 train_time:79205ms step_avg:61.30ms
step:1293/2330 train_time:79264ms step_avg:61.30ms
step:1294/2330 train_time:79327ms step_avg:61.30ms
step:1295/2330 train_time:79387ms step_avg:61.30ms
step:1296/2330 train_time:79451ms step_avg:61.30ms
step:1297/2330 train_time:79511ms step_avg:61.30ms
step:1298/2330 train_time:79574ms step_avg:61.30ms
step:1299/2330 train_time:79633ms step_avg:61.30ms
step:1300/2330 train_time:79697ms step_avg:61.31ms
step:1301/2330 train_time:79757ms step_avg:61.30ms
step:1302/2330 train_time:79819ms step_avg:61.31ms
step:1303/2330 train_time:79880ms step_avg:61.30ms
step:1304/2330 train_time:79943ms step_avg:61.31ms
step:1305/2330 train_time:80003ms step_avg:61.31ms
step:1306/2330 train_time:80067ms step_avg:61.31ms
step:1307/2330 train_time:80128ms step_avg:61.31ms
step:1308/2330 train_time:80191ms step_avg:61.31ms
step:1309/2330 train_time:80251ms step_avg:61.31ms
step:1310/2330 train_time:80314ms step_avg:61.31ms
step:1311/2330 train_time:80374ms step_avg:61.31ms
step:1312/2330 train_time:80437ms step_avg:61.31ms
step:1313/2330 train_time:80497ms step_avg:61.31ms
step:1314/2330 train_time:80561ms step_avg:61.31ms
step:1315/2330 train_time:80621ms step_avg:61.31ms
step:1316/2330 train_time:80685ms step_avg:61.31ms
step:1317/2330 train_time:80745ms step_avg:61.31ms
step:1318/2330 train_time:80807ms step_avg:61.31ms
step:1319/2330 train_time:80869ms step_avg:61.31ms
step:1320/2330 train_time:80934ms step_avg:61.31ms
step:1321/2330 train_time:80994ms step_avg:61.31ms
step:1322/2330 train_time:81057ms step_avg:61.31ms
step:1323/2330 train_time:81117ms step_avg:61.31ms
step:1324/2330 train_time:81180ms step_avg:61.31ms
step:1325/2330 train_time:81240ms step_avg:61.31ms
step:1326/2330 train_time:81303ms step_avg:61.31ms
step:1327/2330 train_time:81364ms step_avg:61.31ms
step:1328/2330 train_time:81426ms step_avg:61.32ms
step:1329/2330 train_time:81486ms step_avg:61.31ms
step:1330/2330 train_time:81549ms step_avg:61.32ms
step:1331/2330 train_time:81610ms step_avg:61.32ms
step:1332/2330 train_time:81673ms step_avg:61.32ms
step:1333/2330 train_time:81734ms step_avg:61.32ms
step:1334/2330 train_time:81796ms step_avg:61.32ms
step:1335/2330 train_time:81856ms step_avg:61.32ms
step:1336/2330 train_time:81919ms step_avg:61.32ms
step:1337/2330 train_time:81979ms step_avg:61.32ms
step:1338/2330 train_time:82043ms step_avg:61.32ms
step:1339/2330 train_time:82102ms step_avg:61.32ms
step:1340/2330 train_time:82166ms step_avg:61.32ms
step:1341/2330 train_time:82226ms step_avg:61.32ms
step:1342/2330 train_time:82289ms step_avg:61.32ms
step:1343/2330 train_time:82350ms step_avg:61.32ms
step:1344/2330 train_time:82412ms step_avg:61.32ms
step:1345/2330 train_time:82472ms step_avg:61.32ms
step:1346/2330 train_time:82535ms step_avg:61.32ms
step:1347/2330 train_time:82595ms step_avg:61.32ms
step:1348/2330 train_time:82658ms step_avg:61.32ms
step:1349/2330 train_time:82717ms step_avg:61.32ms
step:1350/2330 train_time:82781ms step_avg:61.32ms
step:1351/2330 train_time:82841ms step_avg:61.32ms
step:1352/2330 train_time:82904ms step_avg:61.32ms
step:1353/2330 train_time:82964ms step_avg:61.32ms
step:1354/2330 train_time:83027ms step_avg:61.32ms
step:1355/2330 train_time:83087ms step_avg:61.32ms
step:1356/2330 train_time:83150ms step_avg:61.32ms
step:1357/2330 train_time:83211ms step_avg:61.32ms
step:1358/2330 train_time:83272ms step_avg:61.32ms
step:1359/2330 train_time:83332ms step_avg:61.32ms
step:1360/2330 train_time:83396ms step_avg:61.32ms
step:1361/2330 train_time:83456ms step_avg:61.32ms
step:1362/2330 train_time:83518ms step_avg:61.32ms
step:1363/2330 train_time:83578ms step_avg:61.32ms
step:1364/2330 train_time:83641ms step_avg:61.32ms
step:1365/2330 train_time:83701ms step_avg:61.32ms
step:1366/2330 train_time:83764ms step_avg:61.32ms
step:1367/2330 train_time:83824ms step_avg:61.32ms
step:1368/2330 train_time:83887ms step_avg:61.32ms
step:1369/2330 train_time:83948ms step_avg:61.32ms
step:1370/2330 train_time:84011ms step_avg:61.32ms
step:1371/2330 train_time:84071ms step_avg:61.32ms
step:1372/2330 train_time:84134ms step_avg:61.32ms
step:1373/2330 train_time:84193ms step_avg:61.32ms
step:1374/2330 train_time:84256ms step_avg:61.32ms
step:1375/2330 train_time:84316ms step_avg:61.32ms
step:1376/2330 train_time:84379ms step_avg:61.32ms
step:1377/2330 train_time:84439ms step_avg:61.32ms
step:1378/2330 train_time:84502ms step_avg:61.32ms
step:1379/2330 train_time:84562ms step_avg:61.32ms
step:1380/2330 train_time:84626ms step_avg:61.32ms
step:1381/2330 train_time:84686ms step_avg:61.32ms
step:1382/2330 train_time:84750ms step_avg:61.32ms
step:1383/2330 train_time:84810ms step_avg:61.32ms
step:1384/2330 train_time:84873ms step_avg:61.32ms
step:1385/2330 train_time:84933ms step_avg:61.32ms
step:1386/2330 train_time:84996ms step_avg:61.32ms
step:1387/2330 train_time:85055ms step_avg:61.32ms
step:1388/2330 train_time:85118ms step_avg:61.32ms
step:1389/2330 train_time:85179ms step_avg:61.32ms
step:1390/2330 train_time:85242ms step_avg:61.33ms
step:1391/2330 train_time:85303ms step_avg:61.32ms
step:1392/2330 train_time:85366ms step_avg:61.33ms
step:1393/2330 train_time:85426ms step_avg:61.32ms
step:1394/2330 train_time:85490ms step_avg:61.33ms
step:1395/2330 train_time:85551ms step_avg:61.33ms
step:1396/2330 train_time:85615ms step_avg:61.33ms
step:1397/2330 train_time:85675ms step_avg:61.33ms
step:1398/2330 train_time:85737ms step_avg:61.33ms
step:1399/2330 train_time:85797ms step_avg:61.33ms
step:1400/2330 train_time:85860ms step_avg:61.33ms
step:1401/2330 train_time:85921ms step_avg:61.33ms
step:1402/2330 train_time:85984ms step_avg:61.33ms
step:1403/2330 train_time:86043ms step_avg:61.33ms
step:1404/2330 train_time:86107ms step_avg:61.33ms
step:1405/2330 train_time:86167ms step_avg:61.33ms
step:1406/2330 train_time:86231ms step_avg:61.33ms
step:1407/2330 train_time:86292ms step_avg:61.33ms
step:1408/2330 train_time:86356ms step_avg:61.33ms
step:1409/2330 train_time:86416ms step_avg:61.33ms
step:1410/2330 train_time:86478ms step_avg:61.33ms
step:1411/2330 train_time:86537ms step_avg:61.33ms
step:1412/2330 train_time:86600ms step_avg:61.33ms
step:1413/2330 train_time:86660ms step_avg:61.33ms
step:1414/2330 train_time:86723ms step_avg:61.33ms
step:1415/2330 train_time:86783ms step_avg:61.33ms
step:1416/2330 train_time:86846ms step_avg:61.33ms
step:1417/2330 train_time:86907ms step_avg:61.33ms
step:1418/2330 train_time:86970ms step_avg:61.33ms
step:1419/2330 train_time:87030ms step_avg:61.33ms
step:1420/2330 train_time:87093ms step_avg:61.33ms
step:1421/2330 train_time:87154ms step_avg:61.33ms
step:1422/2330 train_time:87216ms step_avg:61.33ms
step:1423/2330 train_time:87276ms step_avg:61.33ms
step:1424/2330 train_time:87338ms step_avg:61.33ms
step:1425/2330 train_time:87398ms step_avg:61.33ms
step:1426/2330 train_time:87462ms step_avg:61.33ms
step:1427/2330 train_time:87521ms step_avg:61.33ms
step:1428/2330 train_time:87586ms step_avg:61.33ms
step:1429/2330 train_time:87646ms step_avg:61.33ms
step:1430/2330 train_time:87709ms step_avg:61.33ms
step:1431/2330 train_time:87769ms step_avg:61.33ms
step:1432/2330 train_time:87833ms step_avg:61.34ms
step:1433/2330 train_time:87893ms step_avg:61.34ms
step:1434/2330 train_time:87956ms step_avg:61.34ms
step:1435/2330 train_time:88015ms step_avg:61.33ms
step:1436/2330 train_time:88078ms step_avg:61.34ms
step:1437/2330 train_time:88138ms step_avg:61.33ms
step:1438/2330 train_time:88201ms step_avg:61.34ms
step:1439/2330 train_time:88262ms step_avg:61.34ms
step:1440/2330 train_time:88326ms step_avg:61.34ms
step:1441/2330 train_time:88385ms step_avg:61.34ms
step:1442/2330 train_time:88449ms step_avg:61.34ms
step:1443/2330 train_time:88509ms step_avg:61.34ms
step:1444/2330 train_time:88573ms step_avg:61.34ms
step:1445/2330 train_time:88633ms step_avg:61.34ms
step:1446/2330 train_time:88696ms step_avg:61.34ms
step:1447/2330 train_time:88756ms step_avg:61.34ms
step:1448/2330 train_time:88818ms step_avg:61.34ms
step:1449/2330 train_time:88879ms step_avg:61.34ms
step:1450/2330 train_time:88942ms step_avg:61.34ms
step:1451/2330 train_time:89001ms step_avg:61.34ms
step:1452/2330 train_time:89064ms step_avg:61.34ms
step:1453/2330 train_time:89124ms step_avg:61.34ms
step:1454/2330 train_time:89187ms step_avg:61.34ms
step:1455/2330 train_time:89247ms step_avg:61.34ms
step:1456/2330 train_time:89311ms step_avg:61.34ms
step:1457/2330 train_time:89370ms step_avg:61.34ms
step:1458/2330 train_time:89433ms step_avg:61.34ms
step:1459/2330 train_time:89493ms step_avg:61.34ms
step:1460/2330 train_time:89556ms step_avg:61.34ms
step:1461/2330 train_time:89616ms step_avg:61.34ms
step:1462/2330 train_time:89679ms step_avg:61.34ms
step:1463/2330 train_time:89738ms step_avg:61.34ms
step:1464/2330 train_time:89801ms step_avg:61.34ms
step:1465/2330 train_time:89862ms step_avg:61.34ms
step:1466/2330 train_time:89925ms step_avg:61.34ms
step:1467/2330 train_time:89985ms step_avg:61.34ms
step:1468/2330 train_time:90047ms step_avg:61.34ms
step:1469/2330 train_time:90108ms step_avg:61.34ms
step:1470/2330 train_time:90171ms step_avg:61.34ms
step:1471/2330 train_time:90230ms step_avg:61.34ms
step:1472/2330 train_time:90293ms step_avg:61.34ms
step:1473/2330 train_time:90354ms step_avg:61.34ms
step:1474/2330 train_time:90416ms step_avg:61.34ms
step:1475/2330 train_time:90476ms step_avg:61.34ms
step:1476/2330 train_time:90539ms step_avg:61.34ms
step:1477/2330 train_time:90598ms step_avg:61.34ms
step:1478/2330 train_time:90662ms step_avg:61.34ms
step:1479/2330 train_time:90722ms step_avg:61.34ms
step:1480/2330 train_time:90785ms step_avg:61.34ms
step:1481/2330 train_time:90845ms step_avg:61.34ms
step:1482/2330 train_time:90907ms step_avg:61.34ms
step:1483/2330 train_time:90968ms step_avg:61.34ms
step:1484/2330 train_time:91031ms step_avg:61.34ms
step:1485/2330 train_time:91091ms step_avg:61.34ms
step:1486/2330 train_time:91154ms step_avg:61.34ms
step:1487/2330 train_time:91214ms step_avg:61.34ms
step:1488/2330 train_time:91276ms step_avg:61.34ms
step:1489/2330 train_time:91336ms step_avg:61.34ms
step:1490/2330 train_time:91398ms step_avg:61.34ms
step:1491/2330 train_time:91459ms step_avg:61.34ms
step:1492/2330 train_time:91522ms step_avg:61.34ms
step:1493/2330 train_time:91583ms step_avg:61.34ms
step:1494/2330 train_time:91646ms step_avg:61.34ms
step:1495/2330 train_time:91706ms step_avg:61.34ms
step:1496/2330 train_time:91770ms step_avg:61.34ms
step:1497/2330 train_time:91830ms step_avg:61.34ms
step:1498/2330 train_time:91893ms step_avg:61.34ms
step:1499/2330 train_time:91954ms step_avg:61.34ms
step:1500/2330 train_time:92016ms step_avg:61.34ms
step:1500/2330 val_loss:3.9328 train_time:92090ms step_avg:61.39ms
step:1501/2330 train_time:92110ms step_avg:61.37ms
step:1502/2330 train_time:92142ms step_avg:61.35ms
step:1503/2330 train_time:92203ms step_avg:61.35ms
step:1504/2330 train_time:92270ms step_avg:61.35ms
step:1505/2330 train_time:92332ms step_avg:61.35ms
step:1506/2330 train_time:92396ms step_avg:61.35ms
step:1507/2330 train_time:92456ms step_avg:61.35ms
step:1508/2330 train_time:92518ms step_avg:61.35ms
step:1509/2330 train_time:92579ms step_avg:61.35ms
step:1510/2330 train_time:92641ms step_avg:61.35ms
step:1511/2330 train_time:92700ms step_avg:61.35ms
step:1512/2330 train_time:92762ms step_avg:61.35ms
step:1513/2330 train_time:92821ms step_avg:61.35ms
step:1514/2330 train_time:92885ms step_avg:61.35ms
step:1515/2330 train_time:92944ms step_avg:61.35ms
step:1516/2330 train_time:93009ms step_avg:61.35ms
step:1517/2330 train_time:93069ms step_avg:61.35ms
step:1518/2330 train_time:93133ms step_avg:61.35ms
step:1519/2330 train_time:93196ms step_avg:61.35ms
step:1520/2330 train_time:93258ms step_avg:61.35ms
step:1521/2330 train_time:93320ms step_avg:61.35ms
step:1522/2330 train_time:93383ms step_avg:61.36ms
step:1523/2330 train_time:93443ms step_avg:61.35ms
step:1524/2330 train_time:93506ms step_avg:61.36ms
step:1525/2330 train_time:93566ms step_avg:61.35ms
step:1526/2330 train_time:93628ms step_avg:61.36ms
step:1527/2330 train_time:93688ms step_avg:61.35ms
step:1528/2330 train_time:93752ms step_avg:61.36ms
step:1529/2330 train_time:93812ms step_avg:61.36ms
step:1530/2330 train_time:93875ms step_avg:61.36ms
step:1531/2330 train_time:93936ms step_avg:61.36ms
step:1532/2330 train_time:94000ms step_avg:61.36ms
step:1533/2330 train_time:94061ms step_avg:61.36ms
step:1534/2330 train_time:94124ms step_avg:61.36ms
step:1535/2330 train_time:94185ms step_avg:61.36ms
step:1536/2330 train_time:94249ms step_avg:61.36ms
step:1537/2330 train_time:94310ms step_avg:61.36ms
step:1538/2330 train_time:94375ms step_avg:61.36ms
step:1539/2330 train_time:94437ms step_avg:61.36ms
step:1540/2330 train_time:94501ms step_avg:61.36ms
step:1541/2330 train_time:94562ms step_avg:61.36ms
step:1542/2330 train_time:94625ms step_avg:61.37ms
step:1543/2330 train_time:94686ms step_avg:61.37ms
step:1544/2330 train_time:94749ms step_avg:61.37ms
step:1545/2330 train_time:94809ms step_avg:61.36ms
step:1546/2330 train_time:94872ms step_avg:61.37ms
step:1547/2330 train_time:94933ms step_avg:61.37ms
step:1548/2330 train_time:94997ms step_avg:61.37ms
step:1549/2330 train_time:95058ms step_avg:61.37ms
step:1550/2330 train_time:95122ms step_avg:61.37ms
step:1551/2330 train_time:95182ms step_avg:61.37ms
step:1552/2330 train_time:95245ms step_avg:61.37ms
step:1553/2330 train_time:95306ms step_avg:61.37ms
step:1554/2330 train_time:95370ms step_avg:61.37ms
step:1555/2330 train_time:95430ms step_avg:61.37ms
step:1556/2330 train_time:95495ms step_avg:61.37ms
step:1557/2330 train_time:95557ms step_avg:61.37ms
step:1558/2330 train_time:95621ms step_avg:61.37ms
step:1559/2330 train_time:95682ms step_avg:61.37ms
step:1560/2330 train_time:95744ms step_avg:61.37ms
step:1561/2330 train_time:95805ms step_avg:61.37ms
step:1562/2330 train_time:95868ms step_avg:61.38ms
step:1563/2330 train_time:95928ms step_avg:61.37ms
step:1564/2330 train_time:95992ms step_avg:61.38ms
step:1565/2330 train_time:96053ms step_avg:61.38ms
step:1566/2330 train_time:96117ms step_avg:61.38ms
step:1567/2330 train_time:96180ms step_avg:61.38ms
step:1568/2330 train_time:96243ms step_avg:61.38ms
step:1569/2330 train_time:96304ms step_avg:61.38ms
step:1570/2330 train_time:96367ms step_avg:61.38ms
step:1571/2330 train_time:96427ms step_avg:61.38ms
step:1572/2330 train_time:96491ms step_avg:61.38ms
step:1573/2330 train_time:96552ms step_avg:61.38ms
step:1574/2330 train_time:96616ms step_avg:61.38ms
step:1575/2330 train_time:96679ms step_avg:61.38ms
step:1576/2330 train_time:96742ms step_avg:61.38ms
step:1577/2330 train_time:96802ms step_avg:61.38ms
step:1578/2330 train_time:96866ms step_avg:61.39ms
step:1579/2330 train_time:96926ms step_avg:61.38ms
step:1580/2330 train_time:96990ms step_avg:61.39ms
step:1581/2330 train_time:97050ms step_avg:61.39ms
step:1582/2330 train_time:97113ms step_avg:61.39ms
step:1583/2330 train_time:97174ms step_avg:61.39ms
step:1584/2330 train_time:97239ms step_avg:61.39ms
step:1585/2330 train_time:97300ms step_avg:61.39ms
step:1586/2330 train_time:97363ms step_avg:61.39ms
step:1587/2330 train_time:97424ms step_avg:61.39ms
step:1588/2330 train_time:97488ms step_avg:61.39ms
step:1589/2330 train_time:97548ms step_avg:61.39ms
step:1590/2330 train_time:97613ms step_avg:61.39ms
step:1591/2330 train_time:97674ms step_avg:61.39ms
step:1592/2330 train_time:97738ms step_avg:61.39ms
step:1593/2330 train_time:97799ms step_avg:61.39ms
step:1594/2330 train_time:97864ms step_avg:61.39ms
step:1595/2330 train_time:97924ms step_avg:61.39ms
step:1596/2330 train_time:97988ms step_avg:61.40ms
step:1597/2330 train_time:98047ms step_avg:61.39ms
step:1598/2330 train_time:98110ms step_avg:61.40ms
step:1599/2330 train_time:98171ms step_avg:61.39ms
step:1600/2330 train_time:98235ms step_avg:61.40ms
step:1601/2330 train_time:98297ms step_avg:61.40ms
step:1602/2330 train_time:98361ms step_avg:61.40ms
step:1603/2330 train_time:98422ms step_avg:61.40ms
step:1604/2330 train_time:98486ms step_avg:61.40ms
step:1605/2330 train_time:98546ms step_avg:61.40ms
step:1606/2330 train_time:98610ms step_avg:61.40ms
step:1607/2330 train_time:98670ms step_avg:61.40ms
step:1608/2330 train_time:98733ms step_avg:61.40ms
step:1609/2330 train_time:98795ms step_avg:61.40ms
step:1610/2330 train_time:98859ms step_avg:61.40ms
step:1611/2330 train_time:98921ms step_avg:61.40ms
step:1612/2330 train_time:98985ms step_avg:61.40ms
step:1613/2330 train_time:99044ms step_avg:61.40ms
step:1614/2330 train_time:99108ms step_avg:61.41ms
step:1615/2330 train_time:99169ms step_avg:61.40ms
step:1616/2330 train_time:99232ms step_avg:61.41ms
step:1617/2330 train_time:99293ms step_avg:61.41ms
step:1618/2330 train_time:99357ms step_avg:61.41ms
step:1619/2330 train_time:99418ms step_avg:61.41ms
step:1620/2330 train_time:99483ms step_avg:61.41ms
step:1621/2330 train_time:99543ms step_avg:61.41ms
step:1622/2330 train_time:99607ms step_avg:61.41ms
step:1623/2330 train_time:99667ms step_avg:61.41ms
step:1624/2330 train_time:99731ms step_avg:61.41ms
step:1625/2330 train_time:99791ms step_avg:61.41ms
step:1626/2330 train_time:99854ms step_avg:61.41ms
step:1627/2330 train_time:99916ms step_avg:61.41ms
step:1628/2330 train_time:99979ms step_avg:61.41ms
step:1629/2330 train_time:100041ms step_avg:61.41ms
step:1630/2330 train_time:100104ms step_avg:61.41ms
step:1631/2330 train_time:100163ms step_avg:61.41ms
step:1632/2330 train_time:100227ms step_avg:61.41ms
step:1633/2330 train_time:100288ms step_avg:61.41ms
step:1634/2330 train_time:100351ms step_avg:61.41ms
step:1635/2330 train_time:100412ms step_avg:61.41ms
step:1636/2330 train_time:100476ms step_avg:61.42ms
step:1637/2330 train_time:100536ms step_avg:61.42ms
step:1638/2330 train_time:100600ms step_avg:61.42ms
step:1639/2330 train_time:100661ms step_avg:61.42ms
step:1640/2330 train_time:100724ms step_avg:61.42ms
step:1641/2330 train_time:100785ms step_avg:61.42ms
step:1642/2330 train_time:100849ms step_avg:61.42ms
step:1643/2330 train_time:100909ms step_avg:61.42ms
step:1644/2330 train_time:100974ms step_avg:61.42ms
step:1645/2330 train_time:101035ms step_avg:61.42ms
step:1646/2330 train_time:101099ms step_avg:61.42ms
step:1647/2330 train_time:101161ms step_avg:61.42ms
step:1648/2330 train_time:101223ms step_avg:61.42ms
step:1649/2330 train_time:101284ms step_avg:61.42ms
step:1650/2330 train_time:101346ms step_avg:61.42ms
step:1651/2330 train_time:101407ms step_avg:61.42ms
step:1652/2330 train_time:101472ms step_avg:61.42ms
step:1653/2330 train_time:101532ms step_avg:61.42ms
step:1654/2330 train_time:101597ms step_avg:61.42ms
step:1655/2330 train_time:101658ms step_avg:61.42ms
step:1656/2330 train_time:101721ms step_avg:61.43ms
step:1657/2330 train_time:101783ms step_avg:61.43ms
step:1658/2330 train_time:101846ms step_avg:61.43ms
step:1659/2330 train_time:101906ms step_avg:61.43ms
step:1660/2330 train_time:101970ms step_avg:61.43ms
step:1661/2330 train_time:102030ms step_avg:61.43ms
step:1662/2330 train_time:102094ms step_avg:61.43ms
step:1663/2330 train_time:102155ms step_avg:61.43ms
step:1664/2330 train_time:102220ms step_avg:61.43ms
step:1665/2330 train_time:102280ms step_avg:61.43ms
step:1666/2330 train_time:102343ms step_avg:61.43ms
step:1667/2330 train_time:102403ms step_avg:61.43ms
step:1668/2330 train_time:102467ms step_avg:61.43ms
step:1669/2330 train_time:102527ms step_avg:61.43ms
step:1670/2330 train_time:102591ms step_avg:61.43ms
step:1671/2330 train_time:102651ms step_avg:61.43ms
step:1672/2330 train_time:102716ms step_avg:61.43ms
step:1673/2330 train_time:102778ms step_avg:61.43ms
step:1674/2330 train_time:102840ms step_avg:61.43ms
step:1675/2330 train_time:102901ms step_avg:61.43ms
step:1676/2330 train_time:102965ms step_avg:61.43ms
step:1677/2330 train_time:103025ms step_avg:61.43ms
step:1678/2330 train_time:103090ms step_avg:61.44ms
step:1679/2330 train_time:103150ms step_avg:61.44ms
step:1680/2330 train_time:103214ms step_avg:61.44ms
step:1681/2330 train_time:103276ms step_avg:61.44ms
step:1682/2330 train_time:103339ms step_avg:61.44ms
step:1683/2330 train_time:103400ms step_avg:61.44ms
step:1684/2330 train_time:103464ms step_avg:61.44ms
step:1685/2330 train_time:103523ms step_avg:61.44ms
step:1686/2330 train_time:103587ms step_avg:61.44ms
step:1687/2330 train_time:103647ms step_avg:61.44ms
step:1688/2330 train_time:103711ms step_avg:61.44ms
step:1689/2330 train_time:103771ms step_avg:61.44ms
step:1690/2330 train_time:103835ms step_avg:61.44ms
step:1691/2330 train_time:103896ms step_avg:61.44ms
step:1692/2330 train_time:103961ms step_avg:61.44ms
step:1693/2330 train_time:104022ms step_avg:61.44ms
step:1694/2330 train_time:104086ms step_avg:61.44ms
step:1695/2330 train_time:104146ms step_avg:61.44ms
step:1696/2330 train_time:104210ms step_avg:61.44ms
step:1697/2330 train_time:104270ms step_avg:61.44ms
step:1698/2330 train_time:104333ms step_avg:61.44ms
step:1699/2330 train_time:104396ms step_avg:61.45ms
step:1700/2330 train_time:104460ms step_avg:61.45ms
step:1701/2330 train_time:104520ms step_avg:61.45ms
step:1702/2330 train_time:104585ms step_avg:61.45ms
step:1703/2330 train_time:104643ms step_avg:61.45ms
step:1704/2330 train_time:104707ms step_avg:61.45ms
step:1705/2330 train_time:104767ms step_avg:61.45ms
step:1706/2330 train_time:104832ms step_avg:61.45ms
step:1707/2330 train_time:104892ms step_avg:61.45ms
step:1708/2330 train_time:104956ms step_avg:61.45ms
step:1709/2330 train_time:105017ms step_avg:61.45ms
step:1710/2330 train_time:105082ms step_avg:61.45ms
step:1711/2330 train_time:105141ms step_avg:61.45ms
step:1712/2330 train_time:105204ms step_avg:61.45ms
step:1713/2330 train_time:105264ms step_avg:61.45ms
step:1714/2330 train_time:105329ms step_avg:61.45ms
step:1715/2330 train_time:105390ms step_avg:61.45ms
step:1716/2330 train_time:105453ms step_avg:61.45ms
step:1717/2330 train_time:105514ms step_avg:61.45ms
step:1718/2330 train_time:105580ms step_avg:61.45ms
step:1719/2330 train_time:105640ms step_avg:61.45ms
step:1720/2330 train_time:105702ms step_avg:61.45ms
step:1721/2330 train_time:105763ms step_avg:61.45ms
step:1722/2330 train_time:105827ms step_avg:61.46ms
step:1723/2330 train_time:105887ms step_avg:61.46ms
step:1724/2330 train_time:105952ms step_avg:61.46ms
step:1725/2330 train_time:106012ms step_avg:61.46ms
step:1726/2330 train_time:106077ms step_avg:61.46ms
step:1727/2330 train_time:106138ms step_avg:61.46ms
step:1728/2330 train_time:106202ms step_avg:61.46ms
step:1729/2330 train_time:106262ms step_avg:61.46ms
step:1730/2330 train_time:106325ms step_avg:61.46ms
step:1731/2330 train_time:106386ms step_avg:61.46ms
step:1732/2330 train_time:106450ms step_avg:61.46ms
step:1733/2330 train_time:106510ms step_avg:61.46ms
step:1734/2330 train_time:106574ms step_avg:61.46ms
step:1735/2330 train_time:106635ms step_avg:61.46ms
step:1736/2330 train_time:106699ms step_avg:61.46ms
step:1737/2330 train_time:106760ms step_avg:61.46ms
step:1738/2330 train_time:106823ms step_avg:61.46ms
step:1739/2330 train_time:106885ms step_avg:61.46ms
step:1740/2330 train_time:106949ms step_avg:61.46ms
step:1741/2330 train_time:107009ms step_avg:61.46ms
step:1742/2330 train_time:107072ms step_avg:61.47ms
step:1743/2330 train_time:107133ms step_avg:61.46ms
step:1744/2330 train_time:107198ms step_avg:61.47ms
step:1745/2330 train_time:107259ms step_avg:61.47ms
step:1746/2330 train_time:107323ms step_avg:61.47ms
step:1747/2330 train_time:107384ms step_avg:61.47ms
step:1748/2330 train_time:107447ms step_avg:61.47ms
step:1749/2330 train_time:107508ms step_avg:61.47ms
step:1750/2330 train_time:107571ms step_avg:61.47ms
step:1750/2330 val_loss:3.8773 train_time:107645ms step_avg:61.51ms
step:1751/2330 train_time:107666ms step_avg:61.49ms
step:1752/2330 train_time:107697ms step_avg:61.47ms
step:1753/2330 train_time:107757ms step_avg:61.47ms
step:1754/2330 train_time:107827ms step_avg:61.47ms
step:1755/2330 train_time:107890ms step_avg:61.48ms
step:1756/2330 train_time:107954ms step_avg:61.48ms
step:1757/2330 train_time:108014ms step_avg:61.48ms
step:1758/2330 train_time:108077ms step_avg:61.48ms
step:1759/2330 train_time:108138ms step_avg:61.48ms
step:1760/2330 train_time:108201ms step_avg:61.48ms
step:1761/2330 train_time:108260ms step_avg:61.48ms
step:1762/2330 train_time:108323ms step_avg:61.48ms
step:1763/2330 train_time:108382ms step_avg:61.48ms
step:1764/2330 train_time:108445ms step_avg:61.48ms
step:1765/2330 train_time:108505ms step_avg:61.48ms
step:1766/2330 train_time:108571ms step_avg:61.48ms
step:1767/2330 train_time:108635ms step_avg:61.48ms
step:1768/2330 train_time:108702ms step_avg:61.48ms
step:1769/2330 train_time:108765ms step_avg:61.48ms
step:1770/2330 train_time:108830ms step_avg:61.49ms
step:1771/2330 train_time:108890ms step_avg:61.48ms
step:1772/2330 train_time:108952ms step_avg:61.49ms
step:1773/2330 train_time:109013ms step_avg:61.48ms
step:1774/2330 train_time:109077ms step_avg:61.49ms
step:1775/2330 train_time:109137ms step_avg:61.49ms
step:1776/2330 train_time:109200ms step_avg:61.49ms
step:1777/2330 train_time:109261ms step_avg:61.49ms
step:1778/2330 train_time:109325ms step_avg:61.49ms
step:1779/2330 train_time:109384ms step_avg:61.49ms
step:1780/2330 train_time:109446ms step_avg:61.49ms
step:1781/2330 train_time:109507ms step_avg:61.49ms
step:1782/2330 train_time:109570ms step_avg:61.49ms
step:1783/2330 train_time:109632ms step_avg:61.49ms
step:1784/2330 train_time:109697ms step_avg:61.49ms
step:1785/2330 train_time:109759ms step_avg:61.49ms
step:1786/2330 train_time:109825ms step_avg:61.49ms
step:1787/2330 train_time:109886ms step_avg:61.49ms
step:1788/2330 train_time:109949ms step_avg:61.49ms
step:1789/2330 train_time:110008ms step_avg:61.49ms
step:1790/2330 train_time:110072ms step_avg:61.49ms
step:1791/2330 train_time:110133ms step_avg:61.49ms
step:1792/2330 train_time:110196ms step_avg:61.49ms
step:1793/2330 train_time:110257ms step_avg:61.49ms
step:1794/2330 train_time:110320ms step_avg:61.49ms
step:1795/2330 train_time:110380ms step_avg:61.49ms
step:1796/2330 train_time:110444ms step_avg:61.49ms
step:1797/2330 train_time:110505ms step_avg:61.49ms
step:1798/2330 train_time:110569ms step_avg:61.50ms
step:1799/2330 train_time:110629ms step_avg:61.49ms
step:1800/2330 train_time:110694ms step_avg:61.50ms
step:1801/2330 train_time:110755ms step_avg:61.50ms
step:1802/2330 train_time:110820ms step_avg:61.50ms
step:1803/2330 train_time:110883ms step_avg:61.50ms
step:1804/2330 train_time:110946ms step_avg:61.50ms
step:1805/2330 train_time:111006ms step_avg:61.50ms
step:1806/2330 train_time:111070ms step_avg:61.50ms
step:1807/2330 train_time:111131ms step_avg:61.50ms
step:1808/2330 train_time:111195ms step_avg:61.50ms
step:1809/2330 train_time:111255ms step_avg:61.50ms
step:1810/2330 train_time:111319ms step_avg:61.50ms
step:1811/2330 train_time:111380ms step_avg:61.50ms
step:1812/2330 train_time:111443ms step_avg:61.50ms
step:1813/2330 train_time:111504ms step_avg:61.50ms
step:1814/2330 train_time:111569ms step_avg:61.50ms
step:1815/2330 train_time:111630ms step_avg:61.50ms
step:1816/2330 train_time:111693ms step_avg:61.50ms
step:1817/2330 train_time:111754ms step_avg:61.50ms
step:1818/2330 train_time:111817ms step_avg:61.51ms
step:1819/2330 train_time:111880ms step_avg:61.51ms
step:1820/2330 train_time:111945ms step_avg:61.51ms
step:1821/2330 train_time:112006ms step_avg:61.51ms
step:1822/2330 train_time:112070ms step_avg:61.51ms
step:1823/2330 train_time:112131ms step_avg:61.51ms
step:1824/2330 train_time:112195ms step_avg:61.51ms
step:1825/2330 train_time:112254ms step_avg:61.51ms
step:1826/2330 train_time:112317ms step_avg:61.51ms
step:1827/2330 train_time:112378ms step_avg:61.51ms
step:1828/2330 train_time:112442ms step_avg:61.51ms
step:1829/2330 train_time:112503ms step_avg:61.51ms
step:1830/2330 train_time:112568ms step_avg:61.51ms
step:1831/2330 train_time:112629ms step_avg:61.51ms
step:1832/2330 train_time:112691ms step_avg:61.51ms
step:1833/2330 train_time:112752ms step_avg:61.51ms
step:1834/2330 train_time:112815ms step_avg:61.51ms
step:1835/2330 train_time:112877ms step_avg:61.51ms
step:1836/2330 train_time:112942ms step_avg:61.52ms
step:1837/2330 train_time:113004ms step_avg:61.52ms
step:1838/2330 train_time:113067ms step_avg:61.52ms
step:1839/2330 train_time:113128ms step_avg:61.52ms
step:1840/2330 train_time:113192ms step_avg:61.52ms
step:1841/2330 train_time:113252ms step_avg:61.52ms
step:1842/2330 train_time:113316ms step_avg:61.52ms
step:1843/2330 train_time:113376ms step_avg:61.52ms
step:1844/2330 train_time:113440ms step_avg:61.52ms
step:1845/2330 train_time:113501ms step_avg:61.52ms
step:1846/2330 train_time:113566ms step_avg:61.52ms
step:1847/2330 train_time:113627ms step_avg:61.52ms
step:1848/2330 train_time:113689ms step_avg:61.52ms
step:1849/2330 train_time:113749ms step_avg:61.52ms
step:1850/2330 train_time:113813ms step_avg:61.52ms
step:1851/2330 train_time:113873ms step_avg:61.52ms
step:1852/2330 train_time:113938ms step_avg:61.52ms
step:1853/2330 train_time:114000ms step_avg:61.52ms
step:1854/2330 train_time:114065ms step_avg:61.52ms
step:1855/2330 train_time:114127ms step_avg:61.52ms
step:1856/2330 train_time:114189ms step_avg:61.52ms
step:1857/2330 train_time:114250ms step_avg:61.52ms
step:1858/2330 train_time:114314ms step_avg:61.53ms
step:1859/2330 train_time:114375ms step_avg:61.52ms
step:1860/2330 train_time:114438ms step_avg:61.53ms
step:1861/2330 train_time:114498ms step_avg:61.53ms
step:1862/2330 train_time:114563ms step_avg:61.53ms
step:1863/2330 train_time:114624ms step_avg:61.53ms
step:1864/2330 train_time:114688ms step_avg:61.53ms
step:1865/2330 train_time:114747ms step_avg:61.53ms
step:1866/2330 train_time:114812ms step_avg:61.53ms
step:1867/2330 train_time:114873ms step_avg:61.53ms
step:1868/2330 train_time:114937ms step_avg:61.53ms
step:1869/2330 train_time:114998ms step_avg:61.53ms
step:1870/2330 train_time:115064ms step_avg:61.53ms
step:1871/2330 train_time:115125ms step_avg:61.53ms
step:1872/2330 train_time:115188ms step_avg:61.53ms
step:1873/2330 train_time:115248ms step_avg:61.53ms
step:1874/2330 train_time:115311ms step_avg:61.53ms
step:1875/2330 train_time:115372ms step_avg:61.53ms
step:1876/2330 train_time:115437ms step_avg:61.53ms
step:1877/2330 train_time:115497ms step_avg:61.53ms
step:1878/2330 train_time:115561ms step_avg:61.53ms
step:1879/2330 train_time:115623ms step_avg:61.53ms
step:1880/2330 train_time:115686ms step_avg:61.53ms
step:1881/2330 train_time:115746ms step_avg:61.53ms
step:1882/2330 train_time:115809ms step_avg:61.53ms
step:1883/2330 train_time:115870ms step_avg:61.53ms
step:1884/2330 train_time:115934ms step_avg:61.54ms
step:1885/2330 train_time:115995ms step_avg:61.54ms
step:1886/2330 train_time:116060ms step_avg:61.54ms
step:1887/2330 train_time:116122ms step_avg:61.54ms
step:1888/2330 train_time:116185ms step_avg:61.54ms
step:1889/2330 train_time:116245ms step_avg:61.54ms
step:1890/2330 train_time:116309ms step_avg:61.54ms
step:1891/2330 train_time:116370ms step_avg:61.54ms
step:1892/2330 train_time:116434ms step_avg:61.54ms
step:1893/2330 train_time:116494ms step_avg:61.54ms
step:1894/2330 train_time:116558ms step_avg:61.54ms
step:1895/2330 train_time:116619ms step_avg:61.54ms
step:1896/2330 train_time:116683ms step_avg:61.54ms
step:1897/2330 train_time:116744ms step_avg:61.54ms
step:1898/2330 train_time:116808ms step_avg:61.54ms
step:1899/2330 train_time:116868ms step_avg:61.54ms
step:1900/2330 train_time:116932ms step_avg:61.54ms
step:1901/2330 train_time:116993ms step_avg:61.54ms
step:1902/2330 train_time:117057ms step_avg:61.54ms
step:1903/2330 train_time:117117ms step_avg:61.54ms
step:1904/2330 train_time:117181ms step_avg:61.54ms
step:1905/2330 train_time:117242ms step_avg:61.54ms
step:1906/2330 train_time:117308ms step_avg:61.55ms
step:1907/2330 train_time:117367ms step_avg:61.55ms
step:1908/2330 train_time:117432ms step_avg:61.55ms
step:1909/2330 train_time:117491ms step_avg:61.55ms
step:1910/2330 train_time:117555ms step_avg:61.55ms
step:1911/2330 train_time:117615ms step_avg:61.55ms
step:1912/2330 train_time:117678ms step_avg:61.55ms
step:1913/2330 train_time:117740ms step_avg:61.55ms
step:1914/2330 train_time:117804ms step_avg:61.55ms
step:1915/2330 train_time:117865ms step_avg:61.55ms
step:1916/2330 train_time:117929ms step_avg:61.55ms
step:1917/2330 train_time:117989ms step_avg:61.55ms
step:1918/2330 train_time:118052ms step_avg:61.55ms
step:1919/2330 train_time:118113ms step_avg:61.55ms
step:1920/2330 train_time:118177ms step_avg:61.55ms
step:1921/2330 train_time:118237ms step_avg:61.55ms
step:1922/2330 train_time:118302ms step_avg:61.55ms
step:1923/2330 train_time:118365ms step_avg:61.55ms
step:1924/2330 train_time:118428ms step_avg:61.55ms
step:1925/2330 train_time:118488ms step_avg:61.55ms
step:1926/2330 train_time:118552ms step_avg:61.55ms
step:1927/2330 train_time:118612ms step_avg:61.55ms
step:1928/2330 train_time:118677ms step_avg:61.55ms
step:1929/2330 train_time:118737ms step_avg:61.55ms
step:1930/2330 train_time:118801ms step_avg:61.55ms
step:1931/2330 train_time:118862ms step_avg:61.55ms
step:1932/2330 train_time:118927ms step_avg:61.56ms
step:1933/2330 train_time:118986ms step_avg:61.56ms
step:1934/2330 train_time:119050ms step_avg:61.56ms
step:1935/2330 train_time:119111ms step_avg:61.56ms
step:1936/2330 train_time:119175ms step_avg:61.56ms
step:1937/2330 train_time:119235ms step_avg:61.56ms
step:1938/2330 train_time:119300ms step_avg:61.56ms
step:1939/2330 train_time:119361ms step_avg:61.56ms
step:1940/2330 train_time:119426ms step_avg:61.56ms
step:1941/2330 train_time:119486ms step_avg:61.56ms
step:1942/2330 train_time:119549ms step_avg:61.56ms
step:1943/2330 train_time:119610ms step_avg:61.56ms
step:1944/2330 train_time:119674ms step_avg:61.56ms
step:1945/2330 train_time:119734ms step_avg:61.56ms
step:1946/2330 train_time:119799ms step_avg:61.56ms
step:1947/2330 train_time:119860ms step_avg:61.56ms
step:1948/2330 train_time:119925ms step_avg:61.56ms
step:1949/2330 train_time:119985ms step_avg:61.56ms
step:1950/2330 train_time:120048ms step_avg:61.56ms
step:1951/2330 train_time:120108ms step_avg:61.56ms
step:1952/2330 train_time:120172ms step_avg:61.56ms
step:1953/2330 train_time:120233ms step_avg:61.56ms
step:1954/2330 train_time:120297ms step_avg:61.56ms
step:1955/2330 train_time:120358ms step_avg:61.56ms
step:1956/2330 train_time:120423ms step_avg:61.57ms
step:1957/2330 train_time:120485ms step_avg:61.57ms
step:1958/2330 train_time:120547ms step_avg:61.57ms
step:1959/2330 train_time:120607ms step_avg:61.57ms
step:1960/2330 train_time:120671ms step_avg:61.57ms
step:1961/2330 train_time:120731ms step_avg:61.57ms
step:1962/2330 train_time:120795ms step_avg:61.57ms
step:1963/2330 train_time:120857ms step_avg:61.57ms
step:1964/2330 train_time:120920ms step_avg:61.57ms
step:1965/2330 train_time:120981ms step_avg:61.57ms
step:1966/2330 train_time:121045ms step_avg:61.57ms
step:1967/2330 train_time:121107ms step_avg:61.57ms
step:1968/2330 train_time:121169ms step_avg:61.57ms
step:1969/2330 train_time:121230ms step_avg:61.57ms
step:1970/2330 train_time:121293ms step_avg:61.57ms
step:1971/2330 train_time:121354ms step_avg:61.57ms
step:1972/2330 train_time:121417ms step_avg:61.57ms
step:1973/2330 train_time:121480ms step_avg:61.57ms
step:1974/2330 train_time:121544ms step_avg:61.57ms
step:1975/2330 train_time:121605ms step_avg:61.57ms
step:1976/2330 train_time:121668ms step_avg:61.57ms
step:1977/2330 train_time:121729ms step_avg:61.57ms
step:1978/2330 train_time:121792ms step_avg:61.57ms
step:1979/2330 train_time:121853ms step_avg:61.57ms
step:1980/2330 train_time:121917ms step_avg:61.57ms
step:1981/2330 train_time:121976ms step_avg:61.57ms
step:1982/2330 train_time:122041ms step_avg:61.57ms
step:1983/2330 train_time:122101ms step_avg:61.57ms
step:1984/2330 train_time:122165ms step_avg:61.58ms
step:1985/2330 train_time:122225ms step_avg:61.57ms
step:1986/2330 train_time:122289ms step_avg:61.58ms
step:1987/2330 train_time:122349ms step_avg:61.57ms
step:1988/2330 train_time:122412ms step_avg:61.58ms
step:1989/2330 train_time:122473ms step_avg:61.57ms
step:1990/2330 train_time:122537ms step_avg:61.58ms
step:1991/2330 train_time:122599ms step_avg:61.58ms
step:1992/2330 train_time:122664ms step_avg:61.58ms
step:1993/2330 train_time:122725ms step_avg:61.58ms
step:1994/2330 train_time:122788ms step_avg:61.58ms
step:1995/2330 train_time:122847ms step_avg:61.58ms
step:1996/2330 train_time:122911ms step_avg:61.58ms
step:1997/2330 train_time:122972ms step_avg:61.58ms
step:1998/2330 train_time:123037ms step_avg:61.58ms
step:1999/2330 train_time:123097ms step_avg:61.58ms
step:2000/2330 train_time:123161ms step_avg:61.58ms
step:2000/2330 val_loss:3.7901 train_time:123236ms step_avg:61.62ms
step:2001/2330 train_time:123259ms step_avg:61.60ms
step:2002/2330 train_time:123289ms step_avg:61.58ms
step:2003/2330 train_time:123351ms step_avg:61.58ms
step:2004/2330 train_time:123420ms step_avg:61.59ms
step:2005/2330 train_time:123482ms step_avg:61.59ms
step:2006/2330 train_time:123546ms step_avg:61.59ms
step:2007/2330 train_time:123606ms step_avg:61.59ms
step:2008/2330 train_time:123668ms step_avg:61.59ms
step:2009/2330 train_time:123728ms step_avg:61.59ms
step:2010/2330 train_time:123791ms step_avg:61.59ms
step:2011/2330 train_time:123851ms step_avg:61.59ms
step:2012/2330 train_time:123914ms step_avg:61.59ms
step:2013/2330 train_time:123973ms step_avg:61.59ms
step:2014/2330 train_time:124035ms step_avg:61.59ms
step:2015/2330 train_time:124096ms step_avg:61.59ms
step:2016/2330 train_time:124159ms step_avg:61.59ms
step:2017/2330 train_time:124220ms step_avg:61.59ms
step:2018/2330 train_time:124285ms step_avg:61.59ms
step:2019/2330 train_time:124348ms step_avg:61.59ms
step:2020/2330 train_time:124412ms step_avg:61.59ms
step:2021/2330 train_time:124473ms step_avg:61.59ms
step:2022/2330 train_time:124537ms step_avg:61.59ms
step:2023/2330 train_time:124598ms step_avg:61.59ms
step:2024/2330 train_time:124662ms step_avg:61.59ms
step:2025/2330 train_time:124724ms step_avg:61.59ms
step:2026/2330 train_time:124787ms step_avg:61.59ms
step:2027/2330 train_time:124846ms step_avg:61.59ms
step:2028/2330 train_time:124909ms step_avg:61.59ms
step:2029/2330 train_time:124968ms step_avg:61.59ms
step:2030/2330 train_time:125031ms step_avg:61.59ms
step:2031/2330 train_time:125091ms step_avg:61.59ms
step:2032/2330 train_time:125155ms step_avg:61.59ms
step:2033/2330 train_time:125215ms step_avg:61.59ms
step:2034/2330 train_time:125280ms step_avg:61.59ms
step:2035/2330 train_time:125344ms step_avg:61.59ms
step:2036/2330 train_time:125407ms step_avg:61.59ms
step:2037/2330 train_time:125467ms step_avg:61.59ms
step:2038/2330 train_time:125531ms step_avg:61.60ms
step:2039/2330 train_time:125592ms step_avg:61.60ms
step:2040/2330 train_time:125657ms step_avg:61.60ms
step:2041/2330 train_time:125717ms step_avg:61.60ms
step:2042/2330 train_time:125782ms step_avg:61.60ms
step:2043/2330 train_time:125843ms step_avg:61.60ms
step:2044/2330 train_time:125906ms step_avg:61.60ms
step:2045/2330 train_time:125966ms step_avg:61.60ms
step:2046/2330 train_time:126030ms step_avg:61.60ms
step:2047/2330 train_time:126089ms step_avg:61.60ms
step:2048/2330 train_time:126153ms step_avg:61.60ms
step:2049/2330 train_time:126213ms step_avg:61.60ms
step:2050/2330 train_time:126276ms step_avg:61.60ms
step:2051/2330 train_time:126338ms step_avg:61.60ms
step:2052/2330 train_time:126402ms step_avg:61.60ms
step:2053/2330 train_time:126463ms step_avg:61.60ms
step:2054/2330 train_time:126528ms step_avg:61.60ms
step:2055/2330 train_time:126588ms step_avg:61.60ms
step:2056/2330 train_time:126653ms step_avg:61.60ms
step:2057/2330 train_time:126714ms step_avg:61.60ms
step:2058/2330 train_time:126777ms step_avg:61.60ms
step:2059/2330 train_time:126838ms step_avg:61.60ms
step:2060/2330 train_time:126902ms step_avg:61.60ms
step:2061/2330 train_time:126963ms step_avg:61.60ms
step:2062/2330 train_time:127028ms step_avg:61.60ms
step:2063/2330 train_time:127088ms step_avg:61.60ms
step:2064/2330 train_time:127152ms step_avg:61.60ms
step:2065/2330 train_time:127211ms step_avg:61.60ms
step:2066/2330 train_time:127274ms step_avg:61.60ms
step:2067/2330 train_time:127334ms step_avg:61.60ms
step:2068/2330 train_time:127398ms step_avg:61.60ms
step:2069/2330 train_time:127460ms step_avg:61.60ms
step:2070/2330 train_time:127525ms step_avg:61.61ms
step:2071/2330 train_time:127587ms step_avg:61.61ms
step:2072/2330 train_time:127651ms step_avg:61.61ms
step:2073/2330 train_time:127710ms step_avg:61.61ms
step:2074/2330 train_time:127773ms step_avg:61.61ms
step:2075/2330 train_time:127834ms step_avg:61.61ms
step:2076/2330 train_time:127898ms step_avg:61.61ms
step:2077/2330 train_time:127958ms step_avg:61.61ms
step:2078/2330 train_time:128023ms step_avg:61.61ms
step:2079/2330 train_time:128083ms step_avg:61.61ms
step:2080/2330 train_time:128149ms step_avg:61.61ms
step:2081/2330 train_time:128207ms step_avg:61.61ms
step:2082/2330 train_time:128271ms step_avg:61.61ms
step:2083/2330 train_time:128331ms step_avg:61.61ms
step:2084/2330 train_time:128395ms step_avg:61.61ms
step:2085/2330 train_time:128456ms step_avg:61.61ms
step:2086/2330 train_time:128520ms step_avg:61.61ms
step:2087/2330 train_time:128581ms step_avg:61.61ms
step:2088/2330 train_time:128646ms step_avg:61.61ms
step:2089/2330 train_time:128707ms step_avg:61.61ms
step:2090/2330 train_time:128769ms step_avg:61.61ms
step:2091/2330 train_time:128829ms step_avg:61.61ms
step:2092/2330 train_time:128894ms step_avg:61.61ms
step:2093/2330 train_time:128954ms step_avg:61.61ms
step:2094/2330 train_time:129018ms step_avg:61.61ms
step:2095/2330 train_time:129079ms step_avg:61.61ms
step:2096/2330 train_time:129144ms step_avg:61.61ms
step:2097/2330 train_time:129204ms step_avg:61.61ms
step:2098/2330 train_time:129268ms step_avg:61.61ms
step:2099/2330 train_time:129328ms step_avg:61.61ms
step:2100/2330 train_time:129392ms step_avg:61.62ms
step:2101/2330 train_time:129453ms step_avg:61.62ms
step:2102/2330 train_time:129517ms step_avg:61.62ms
step:2103/2330 train_time:129579ms step_avg:61.62ms
step:2104/2330 train_time:129644ms step_avg:61.62ms
step:2105/2330 train_time:129704ms step_avg:61.62ms
step:2106/2330 train_time:129767ms step_avg:61.62ms
step:2107/2330 train_time:129828ms step_avg:61.62ms
step:2108/2330 train_time:129891ms step_avg:61.62ms
step:2109/2330 train_time:129951ms step_avg:61.62ms
step:2110/2330 train_time:130014ms step_avg:61.62ms
step:2111/2330 train_time:130074ms step_avg:61.62ms
step:2112/2330 train_time:130137ms step_avg:61.62ms
step:2113/2330 train_time:130198ms step_avg:61.62ms
step:2114/2330 train_time:130264ms step_avg:61.62ms
step:2115/2330 train_time:130324ms step_avg:61.62ms
step:2116/2330 train_time:130387ms step_avg:61.62ms
step:2117/2330 train_time:130447ms step_avg:61.62ms
step:2118/2330 train_time:130511ms step_avg:61.62ms
step:2119/2330 train_time:130570ms step_avg:61.62ms
step:2120/2330 train_time:130635ms step_avg:61.62ms
step:2121/2330 train_time:130695ms step_avg:61.62ms
step:2122/2330 train_time:130760ms step_avg:61.62ms
step:2123/2330 train_time:130821ms step_avg:61.62ms
step:2124/2330 train_time:130886ms step_avg:61.62ms
step:2125/2330 train_time:130947ms step_avg:61.62ms
step:2126/2330 train_time:131010ms step_avg:61.62ms
step:2127/2330 train_time:131069ms step_avg:61.62ms
step:2128/2330 train_time:131133ms step_avg:61.62ms
step:2129/2330 train_time:131193ms step_avg:61.62ms
step:2130/2330 train_time:131258ms step_avg:61.62ms
step:2131/2330 train_time:131318ms step_avg:61.62ms
step:2132/2330 train_time:131383ms step_avg:61.62ms
step:2133/2330 train_time:131445ms step_avg:61.62ms
step:2134/2330 train_time:131508ms step_avg:61.63ms
step:2135/2330 train_time:131568ms step_avg:61.62ms
step:2136/2330 train_time:131632ms step_avg:61.63ms
step:2137/2330 train_time:131690ms step_avg:61.62ms
step:2138/2330 train_time:131755ms step_avg:61.63ms
step:2139/2330 train_time:131815ms step_avg:61.62ms
step:2140/2330 train_time:131880ms step_avg:61.63ms
step:2141/2330 train_time:131942ms step_avg:61.63ms
step:2142/2330 train_time:132006ms step_avg:61.63ms
step:2143/2330 train_time:132066ms step_avg:61.63ms
step:2144/2330 train_time:132130ms step_avg:61.63ms
step:2145/2330 train_time:132190ms step_avg:61.63ms
step:2146/2330 train_time:132254ms step_avg:61.63ms
step:2147/2330 train_time:132314ms step_avg:61.63ms
step:2148/2330 train_time:132378ms step_avg:61.63ms
step:2149/2330 train_time:132439ms step_avg:61.63ms
step:2150/2330 train_time:132504ms step_avg:61.63ms
step:2151/2330 train_time:132564ms step_avg:61.63ms
step:2152/2330 train_time:132629ms step_avg:61.63ms
step:2153/2330 train_time:132690ms step_avg:61.63ms
step:2154/2330 train_time:132752ms step_avg:61.63ms
step:2155/2330 train_time:132812ms step_avg:61.63ms
step:2156/2330 train_time:132875ms step_avg:61.63ms
step:2157/2330 train_time:132936ms step_avg:61.63ms
step:2158/2330 train_time:133001ms step_avg:61.63ms
step:2159/2330 train_time:133062ms step_avg:61.63ms
step:2160/2330 train_time:133127ms step_avg:61.63ms
step:2161/2330 train_time:133187ms step_avg:61.63ms
step:2162/2330 train_time:133251ms step_avg:61.63ms
step:2163/2330 train_time:133310ms step_avg:61.63ms
step:2164/2330 train_time:133374ms step_avg:61.63ms
step:2165/2330 train_time:133433ms step_avg:61.63ms
step:2166/2330 train_time:133497ms step_avg:61.63ms
step:2167/2330 train_time:133558ms step_avg:61.63ms
step:2168/2330 train_time:133622ms step_avg:61.63ms
step:2169/2330 train_time:133683ms step_avg:61.63ms
step:2170/2330 train_time:133748ms step_avg:61.64ms
step:2171/2330 train_time:133808ms step_avg:61.63ms
step:2172/2330 train_time:133871ms step_avg:61.63ms
step:2173/2330 train_time:133931ms step_avg:61.63ms
step:2174/2330 train_time:133995ms step_avg:61.64ms
step:2175/2330 train_time:134055ms step_avg:61.63ms
step:2176/2330 train_time:134119ms step_avg:61.64ms
step:2177/2330 train_time:134181ms step_avg:61.64ms
step:2178/2330 train_time:134246ms step_avg:61.64ms
step:2179/2330 train_time:134305ms step_avg:61.64ms
step:2180/2330 train_time:134370ms step_avg:61.64ms
step:2181/2330 train_time:134430ms step_avg:61.64ms
step:2182/2330 train_time:134494ms step_avg:61.64ms
step:2183/2330 train_time:134555ms step_avg:61.64ms
step:2184/2330 train_time:134619ms step_avg:61.64ms
step:2185/2330 train_time:134681ms step_avg:61.64ms
step:2186/2330 train_time:134746ms step_avg:61.64ms
step:2187/2330 train_time:134806ms step_avg:61.64ms
step:2188/2330 train_time:134869ms step_avg:61.64ms
step:2189/2330 train_time:134930ms step_avg:61.64ms
step:2190/2330 train_time:134993ms step_avg:61.64ms
step:2191/2330 train_time:135054ms step_avg:61.64ms
step:2192/2330 train_time:135117ms step_avg:61.64ms
step:2193/2330 train_time:135179ms step_avg:61.64ms
step:2194/2330 train_time:135244ms step_avg:61.64ms
step:2195/2330 train_time:135303ms step_avg:61.64ms
step:2196/2330 train_time:135367ms step_avg:61.64ms
step:2197/2330 train_time:135427ms step_avg:61.64ms
step:2198/2330 train_time:135491ms step_avg:61.64ms
step:2199/2330 train_time:135552ms step_avg:61.64ms
step:2200/2330 train_time:135616ms step_avg:61.64ms
step:2201/2330 train_time:135677ms step_avg:61.64ms
step:2202/2330 train_time:135741ms step_avg:61.64ms
step:2203/2330 train_time:135802ms step_avg:61.64ms
step:2204/2330 train_time:135866ms step_avg:61.65ms
step:2205/2330 train_time:135928ms step_avg:61.65ms
step:2206/2330 train_time:135990ms step_avg:61.65ms
step:2207/2330 train_time:136051ms step_avg:61.65ms
step:2208/2330 train_time:136115ms step_avg:61.65ms
step:2209/2330 train_time:136175ms step_avg:61.65ms
step:2210/2330 train_time:136239ms step_avg:61.65ms
step:2211/2330 train_time:136300ms step_avg:61.65ms
step:2212/2330 train_time:136363ms step_avg:61.65ms
step:2213/2330 train_time:136424ms step_avg:61.65ms
step:2214/2330 train_time:136488ms step_avg:61.65ms
step:2215/2330 train_time:136550ms step_avg:61.65ms
step:2216/2330 train_time:136612ms step_avg:61.65ms
step:2217/2330 train_time:136672ms step_avg:61.65ms
step:2218/2330 train_time:136736ms step_avg:61.65ms
step:2219/2330 train_time:136796ms step_avg:61.65ms
step:2220/2330 train_time:136861ms step_avg:61.65ms
step:2221/2330 train_time:136924ms step_avg:61.65ms
step:2222/2330 train_time:136987ms step_avg:61.65ms
step:2223/2330 train_time:137048ms step_avg:61.65ms
step:2224/2330 train_time:137111ms step_avg:61.65ms
step:2225/2330 train_time:137171ms step_avg:61.65ms
step:2226/2330 train_time:137235ms step_avg:61.65ms
step:2227/2330 train_time:137295ms step_avg:61.65ms
step:2228/2330 train_time:137359ms step_avg:61.65ms
step:2229/2330 train_time:137420ms step_avg:61.65ms
step:2230/2330 train_time:137485ms step_avg:61.65ms
step:2231/2330 train_time:137546ms step_avg:61.65ms
step:2232/2330 train_time:137609ms step_avg:61.65ms
step:2233/2330 train_time:137670ms step_avg:61.65ms
step:2234/2330 train_time:137734ms step_avg:61.65ms
step:2235/2330 train_time:137794ms step_avg:61.65ms
step:2236/2330 train_time:137859ms step_avg:61.65ms
step:2237/2330 train_time:137920ms step_avg:61.65ms
step:2238/2330 train_time:137985ms step_avg:61.66ms
step:2239/2330 train_time:138046ms step_avg:61.66ms
step:2240/2330 train_time:138109ms step_avg:61.66ms
step:2241/2330 train_time:138169ms step_avg:61.66ms
step:2242/2330 train_time:138233ms step_avg:61.66ms
step:2243/2330 train_time:138293ms step_avg:61.66ms
step:2244/2330 train_time:138357ms step_avg:61.66ms
step:2245/2330 train_time:138418ms step_avg:61.66ms
step:2246/2330 train_time:138482ms step_avg:61.66ms
step:2247/2330 train_time:138545ms step_avg:61.66ms
step:2248/2330 train_time:138608ms step_avg:61.66ms
step:2249/2330 train_time:138668ms step_avg:61.66ms
step:2250/2330 train_time:138731ms step_avg:61.66ms
step:2250/2330 val_loss:3.7406 train_time:138806ms step_avg:61.69ms
step:2251/2330 train_time:138828ms step_avg:61.67ms
step:2252/2330 train_time:138861ms step_avg:61.66ms
step:2253/2330 train_time:138924ms step_avg:61.66ms
step:2254/2330 train_time:138989ms step_avg:61.66ms
step:2255/2330 train_time:139049ms step_avg:61.66ms
step:2256/2330 train_time:139113ms step_avg:61.66ms
step:2257/2330 train_time:139173ms step_avg:61.66ms
step:2258/2330 train_time:139236ms step_avg:61.66ms
step:2259/2330 train_time:139296ms step_avg:61.66ms
step:2260/2330 train_time:139360ms step_avg:61.66ms
step:2261/2330 train_time:139419ms step_avg:61.66ms
step:2262/2330 train_time:139482ms step_avg:61.66ms
step:2263/2330 train_time:139542ms step_avg:61.66ms
step:2264/2330 train_time:139605ms step_avg:61.66ms
step:2265/2330 train_time:139666ms step_avg:61.66ms
step:2266/2330 train_time:139728ms step_avg:61.66ms
step:2267/2330 train_time:139790ms step_avg:61.66ms
step:2268/2330 train_time:139857ms step_avg:61.67ms
step:2269/2330 train_time:139918ms step_avg:61.67ms
step:2270/2330 train_time:139983ms step_avg:61.67ms
step:2271/2330 train_time:140045ms step_avg:61.67ms
step:2272/2330 train_time:140109ms step_avg:61.67ms
step:2273/2330 train_time:140170ms step_avg:61.67ms
step:2274/2330 train_time:140233ms step_avg:61.67ms
step:2275/2330 train_time:140292ms step_avg:61.67ms
step:2276/2330 train_time:140357ms step_avg:61.67ms
step:2277/2330 train_time:140417ms step_avg:61.67ms
step:2278/2330 train_time:140479ms step_avg:61.67ms
step:2279/2330 train_time:140540ms step_avg:61.67ms
step:2280/2330 train_time:140603ms step_avg:61.67ms
step:2281/2330 train_time:140664ms step_avg:61.67ms
step:2282/2330 train_time:140727ms step_avg:61.67ms
step:2283/2330 train_time:140787ms step_avg:61.67ms
step:2284/2330 train_time:140851ms step_avg:61.67ms
step:2285/2330 train_time:140912ms step_avg:61.67ms
step:2286/2330 train_time:140976ms step_avg:61.67ms
step:2287/2330 train_time:141036ms step_avg:61.67ms
step:2288/2330 train_time:141101ms step_avg:61.67ms
step:2289/2330 train_time:141163ms step_avg:61.67ms
step:2290/2330 train_time:141226ms step_avg:61.67ms
step:2291/2330 train_time:141287ms step_avg:61.67ms
step:2292/2330 train_time:141349ms step_avg:61.67ms
step:2293/2330 train_time:141411ms step_avg:61.67ms
step:2294/2330 train_time:141475ms step_avg:61.67ms
step:2295/2330 train_time:141534ms step_avg:61.67ms
step:2296/2330 train_time:141598ms step_avg:61.67ms
step:2297/2330 train_time:141659ms step_avg:61.67ms
step:2298/2330 train_time:141723ms step_avg:61.67ms
step:2299/2330 train_time:141783ms step_avg:61.67ms
step:2300/2330 train_time:141847ms step_avg:61.67ms
step:2301/2330 train_time:141907ms step_avg:61.67ms
step:2302/2330 train_time:141972ms step_avg:61.67ms
step:2303/2330 train_time:142032ms step_avg:61.67ms
step:2304/2330 train_time:142097ms step_avg:61.67ms
step:2305/2330 train_time:142158ms step_avg:61.67ms
step:2306/2330 train_time:142222ms step_avg:61.67ms
step:2307/2330 train_time:142283ms step_avg:61.67ms
step:2308/2330 train_time:142348ms step_avg:61.68ms
step:2309/2330 train_time:142408ms step_avg:61.68ms
step:2310/2330 train_time:142471ms step_avg:61.68ms
step:2311/2330 train_time:142532ms step_avg:61.68ms
step:2312/2330 train_time:142595ms step_avg:61.68ms
step:2313/2330 train_time:142656ms step_avg:61.68ms
step:2314/2330 train_time:142719ms step_avg:61.68ms
step:2315/2330 train_time:142780ms step_avg:61.68ms
step:2316/2330 train_time:142845ms step_avg:61.68ms
step:2317/2330 train_time:142906ms step_avg:61.68ms
step:2318/2330 train_time:142970ms step_avg:61.68ms
step:2319/2330 train_time:143029ms step_avg:61.68ms
step:2320/2330 train_time:143093ms step_avg:61.68ms
step:2321/2330 train_time:143153ms step_avg:61.68ms
step:2322/2330 train_time:143217ms step_avg:61.68ms
step:2323/2330 train_time:143279ms step_avg:61.68ms
step:2324/2330 train_time:143342ms step_avg:61.68ms
step:2325/2330 train_time:143403ms step_avg:61.68ms
step:2326/2330 train_time:143467ms step_avg:61.68ms
step:2327/2330 train_time:143527ms step_avg:61.68ms
step:2328/2330 train_time:143591ms step_avg:61.68ms
step:2329/2330 train_time:143652ms step_avg:61.68ms
step:2330/2330 train_time:143715ms step_avg:61.68ms
step:2330/2330 val_loss:3.7275 train_time:143790ms step_avg:61.71ms
peak memory allocated: 29226 MiB reserved: 44076 MiB
