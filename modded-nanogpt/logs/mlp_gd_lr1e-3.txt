import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = divide_by_norm(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = divide_by_norm(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_gd_lr1e-3"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-3, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 05:12:10 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   32C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   30C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   30C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   28C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   31C    P0             115W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:73ms step_avg:72.83ms
step:2/2330 train_time:136ms step_avg:67.90ms
step:3/2330 train_time:149ms step_avg:49.57ms
step:4/2330 train_time:162ms step_avg:40.39ms
step:5/2330 train_time:173ms step_avg:34.54ms
step:6/2330 train_time:196ms step_avg:32.73ms
step:7/2330 train_time:217ms step_avg:31.07ms
step:8/2330 train_time:272ms step_avg:34.05ms
step:9/2330 train_time:295ms step_avg:32.76ms
step:10/2330 train_time:350ms step_avg:35.05ms
step:11/2330 train_time:372ms step_avg:33.84ms
step:12/2330 train_time:427ms step_avg:35.62ms
step:13/2330 train_time:449ms step_avg:34.52ms
step:14/2330 train_time:504ms step_avg:35.98ms
step:15/2330 train_time:526ms step_avg:35.05ms
step:16/2330 train_time:580ms step_avg:36.28ms
step:17/2330 train_time:603ms step_avg:35.45ms
step:18/2330 train_time:659ms step_avg:36.59ms
step:19/2330 train_time:681ms step_avg:35.83ms
step:20/2330 train_time:736ms step_avg:36.81ms
step:21/2330 train_time:758ms step_avg:36.11ms
step:22/2330 train_time:814ms step_avg:37.01ms
step:23/2330 train_time:837ms step_avg:36.38ms
step:24/2330 train_time:893ms step_avg:37.19ms
step:25/2330 train_time:915ms step_avg:36.58ms
step:26/2330 train_time:974ms step_avg:37.47ms
step:27/2330 train_time:999ms step_avg:37.01ms
step:28/2330 train_time:1062ms step_avg:37.94ms
step:29/2330 train_time:1087ms step_avg:37.47ms
step:30/2330 train_time:1145ms step_avg:38.16ms
step:31/2330 train_time:1168ms step_avg:37.69ms
step:32/2330 train_time:1225ms step_avg:38.27ms
step:33/2330 train_time:1248ms step_avg:37.81ms
step:34/2330 train_time:1304ms step_avg:38.35ms
step:35/2330 train_time:1327ms step_avg:37.90ms
step:36/2330 train_time:1382ms step_avg:38.40ms
step:37/2330 train_time:1405ms step_avg:37.97ms
step:38/2330 train_time:1460ms step_avg:38.43ms
step:39/2330 train_time:1483ms step_avg:38.01ms
step:40/2330 train_time:1538ms step_avg:38.46ms
step:41/2330 train_time:1561ms step_avg:38.07ms
step:42/2330 train_time:1616ms step_avg:38.49ms
step:43/2330 train_time:1639ms step_avg:38.12ms
step:44/2330 train_time:1695ms step_avg:38.53ms
step:45/2330 train_time:1717ms step_avg:38.16ms
step:46/2330 train_time:1774ms step_avg:38.56ms
step:47/2330 train_time:1796ms step_avg:38.21ms
step:48/2330 train_time:1852ms step_avg:38.59ms
step:49/2330 train_time:1875ms step_avg:38.27ms
step:50/2330 train_time:1933ms step_avg:38.65ms
step:51/2330 train_time:1956ms step_avg:38.36ms
step:52/2330 train_time:2016ms step_avg:38.78ms
step:53/2330 train_time:2041ms step_avg:38.50ms
step:54/2330 train_time:2100ms step_avg:38.89ms
step:55/2330 train_time:2123ms step_avg:38.61ms
step:56/2330 train_time:2181ms step_avg:38.94ms
step:57/2330 train_time:2204ms step_avg:38.67ms
step:58/2330 train_time:2262ms step_avg:39.00ms
step:59/2330 train_time:2285ms step_avg:38.73ms
step:60/2330 train_time:2342ms step_avg:39.03ms
step:61/2330 train_time:2364ms step_avg:38.76ms
step:62/2330 train_time:2420ms step_avg:39.03ms
step:63/2330 train_time:2443ms step_avg:38.78ms
step:64/2330 train_time:2499ms step_avg:39.04ms
step:65/2330 train_time:2521ms step_avg:38.79ms
step:66/2330 train_time:2578ms step_avg:39.05ms
step:67/2330 train_time:2600ms step_avg:38.81ms
step:68/2330 train_time:2656ms step_avg:39.06ms
step:69/2330 train_time:2678ms step_avg:38.82ms
step:70/2330 train_time:2735ms step_avg:39.07ms
step:71/2330 train_time:2758ms step_avg:38.85ms
step:72/2330 train_time:2815ms step_avg:39.10ms
step:73/2330 train_time:2839ms step_avg:38.89ms
step:74/2330 train_time:2896ms step_avg:39.13ms
step:75/2330 train_time:2918ms step_avg:38.91ms
step:76/2330 train_time:2976ms step_avg:39.16ms
step:77/2330 train_time:3000ms step_avg:38.97ms
step:78/2330 train_time:3059ms step_avg:39.22ms
step:79/2330 train_time:3083ms step_avg:39.03ms
step:80/2330 train_time:3141ms step_avg:39.26ms
step:81/2330 train_time:3164ms step_avg:39.07ms
step:82/2330 train_time:3222ms step_avg:39.29ms
step:83/2330 train_time:3245ms step_avg:39.10ms
step:84/2330 train_time:3302ms step_avg:39.30ms
step:85/2330 train_time:3325ms step_avg:39.12ms
step:86/2330 train_time:3381ms step_avg:39.31ms
step:87/2330 train_time:3404ms step_avg:39.13ms
step:88/2330 train_time:3461ms step_avg:39.32ms
step:89/2330 train_time:3483ms step_avg:39.14ms
step:90/2330 train_time:3540ms step_avg:39.33ms
step:91/2330 train_time:3562ms step_avg:39.15ms
step:92/2330 train_time:3619ms step_avg:39.33ms
step:93/2330 train_time:3642ms step_avg:39.16ms
step:94/2330 train_time:3698ms step_avg:39.35ms
step:95/2330 train_time:3721ms step_avg:39.17ms
step:96/2330 train_time:3778ms step_avg:39.36ms
step:97/2330 train_time:3803ms step_avg:39.20ms
step:98/2330 train_time:3859ms step_avg:39.38ms
step:99/2330 train_time:3883ms step_avg:39.22ms
step:100/2330 train_time:3939ms step_avg:39.39ms
step:101/2330 train_time:3963ms step_avg:39.24ms
step:102/2330 train_time:4021ms step_avg:39.42ms
step:103/2330 train_time:4045ms step_avg:39.28ms
step:104/2330 train_time:4102ms step_avg:39.44ms
step:105/2330 train_time:4126ms step_avg:39.29ms
step:106/2330 train_time:4182ms step_avg:39.45ms
step:107/2330 train_time:4205ms step_avg:39.30ms
step:108/2330 train_time:4262ms step_avg:39.46ms
step:109/2330 train_time:4285ms step_avg:39.31ms
step:110/2330 train_time:4342ms step_avg:39.47ms
step:111/2330 train_time:4365ms step_avg:39.32ms
step:112/2330 train_time:4421ms step_avg:39.48ms
step:113/2330 train_time:4444ms step_avg:39.33ms
step:114/2330 train_time:4500ms step_avg:39.48ms
step:115/2330 train_time:4523ms step_avg:39.33ms
step:116/2330 train_time:4579ms step_avg:39.48ms
step:117/2330 train_time:4602ms step_avg:39.34ms
step:118/2330 train_time:4659ms step_avg:39.48ms
step:119/2330 train_time:4682ms step_avg:39.35ms
step:120/2330 train_time:4739ms step_avg:39.49ms
step:121/2330 train_time:4762ms step_avg:39.35ms
step:122/2330 train_time:4818ms step_avg:39.49ms
step:123/2330 train_time:4842ms step_avg:39.37ms
step:124/2330 train_time:4899ms step_avg:39.51ms
step:125/2330 train_time:4923ms step_avg:39.39ms
step:126/2330 train_time:4980ms step_avg:39.53ms
step:127/2330 train_time:5004ms step_avg:39.40ms
step:128/2330 train_time:5061ms step_avg:39.54ms
step:129/2330 train_time:5084ms step_avg:39.41ms
step:130/2330 train_time:5141ms step_avg:39.54ms
step:131/2330 train_time:5164ms step_avg:39.42ms
step:132/2330 train_time:5221ms step_avg:39.55ms
step:133/2330 train_time:5244ms step_avg:39.43ms
step:134/2330 train_time:5300ms step_avg:39.55ms
step:135/2330 train_time:5323ms step_avg:39.43ms
step:136/2330 train_time:5380ms step_avg:39.56ms
step:137/2330 train_time:5403ms step_avg:39.44ms
step:138/2330 train_time:5460ms step_avg:39.57ms
step:139/2330 train_time:5483ms step_avg:39.45ms
step:140/2330 train_time:5540ms step_avg:39.57ms
step:141/2330 train_time:5563ms step_avg:39.45ms
step:142/2330 train_time:5619ms step_avg:39.57ms
step:143/2330 train_time:5643ms step_avg:39.46ms
step:144/2330 train_time:5700ms step_avg:39.58ms
step:145/2330 train_time:5722ms step_avg:39.47ms
step:146/2330 train_time:5779ms step_avg:39.58ms
step:147/2330 train_time:5802ms step_avg:39.47ms
step:148/2330 train_time:5859ms step_avg:39.59ms
step:149/2330 train_time:5882ms step_avg:39.48ms
step:150/2330 train_time:5940ms step_avg:39.60ms
step:151/2330 train_time:5964ms step_avg:39.49ms
step:152/2330 train_time:6021ms step_avg:39.61ms
step:153/2330 train_time:6045ms step_avg:39.51ms
step:154/2330 train_time:6101ms step_avg:39.62ms
step:155/2330 train_time:6124ms step_avg:39.51ms
step:156/2330 train_time:6181ms step_avg:39.62ms
step:157/2330 train_time:6204ms step_avg:39.52ms
step:158/2330 train_time:6261ms step_avg:39.63ms
step:159/2330 train_time:6284ms step_avg:39.52ms
step:160/2330 train_time:6341ms step_avg:39.63ms
step:161/2330 train_time:6365ms step_avg:39.53ms
step:162/2330 train_time:6421ms step_avg:39.64ms
step:163/2330 train_time:6445ms step_avg:39.54ms
step:164/2330 train_time:6501ms step_avg:39.64ms
step:165/2330 train_time:6524ms step_avg:39.54ms
step:166/2330 train_time:6581ms step_avg:39.65ms
step:167/2330 train_time:6605ms step_avg:39.55ms
step:168/2330 train_time:6661ms step_avg:39.65ms
step:169/2330 train_time:6685ms step_avg:39.56ms
step:170/2330 train_time:6741ms step_avg:39.65ms
step:171/2330 train_time:6764ms step_avg:39.55ms
step:172/2330 train_time:6820ms step_avg:39.65ms
step:173/2330 train_time:6843ms step_avg:39.56ms
step:174/2330 train_time:6900ms step_avg:39.66ms
step:175/2330 train_time:6923ms step_avg:39.56ms
step:176/2330 train_time:6981ms step_avg:39.66ms
step:177/2330 train_time:7005ms step_avg:39.57ms
step:178/2330 train_time:7062ms step_avg:39.67ms
step:179/2330 train_time:7084ms step_avg:39.58ms
step:180/2330 train_time:7141ms step_avg:39.67ms
step:181/2330 train_time:7164ms step_avg:39.58ms
step:182/2330 train_time:7221ms step_avg:39.68ms
step:183/2330 train_time:7244ms step_avg:39.58ms
step:184/2330 train_time:7300ms step_avg:39.68ms
step:185/2330 train_time:7323ms step_avg:39.58ms
step:186/2330 train_time:7380ms step_avg:39.68ms
step:187/2330 train_time:7404ms step_avg:39.59ms
step:188/2330 train_time:7461ms step_avg:39.68ms
step:189/2330 train_time:7484ms step_avg:39.60ms
step:190/2330 train_time:7541ms step_avg:39.69ms
step:191/2330 train_time:7564ms step_avg:39.60ms
step:192/2330 train_time:7620ms step_avg:39.69ms
step:193/2330 train_time:7644ms step_avg:39.61ms
step:194/2330 train_time:7700ms step_avg:39.69ms
step:195/2330 train_time:7724ms step_avg:39.61ms
step:196/2330 train_time:7780ms step_avg:39.69ms
step:197/2330 train_time:7803ms step_avg:39.61ms
step:198/2330 train_time:7860ms step_avg:39.70ms
step:199/2330 train_time:7883ms step_avg:39.61ms
step:200/2330 train_time:7940ms step_avg:39.70ms
step:201/2330 train_time:7963ms step_avg:39.62ms
step:202/2330 train_time:8021ms step_avg:39.71ms
step:203/2330 train_time:8044ms step_avg:39.63ms
step:204/2330 train_time:8101ms step_avg:39.71ms
step:205/2330 train_time:8124ms step_avg:39.63ms
step:206/2330 train_time:8180ms step_avg:39.71ms
step:207/2330 train_time:8204ms step_avg:39.63ms
step:208/2330 train_time:8260ms step_avg:39.71ms
step:209/2330 train_time:8284ms step_avg:39.64ms
step:210/2330 train_time:8341ms step_avg:39.72ms
step:211/2330 train_time:8365ms step_avg:39.64ms
step:212/2330 train_time:8421ms step_avg:39.72ms
step:213/2330 train_time:8444ms step_avg:39.64ms
step:214/2330 train_time:8501ms step_avg:39.72ms
step:215/2330 train_time:8524ms step_avg:39.65ms
step:216/2330 train_time:8581ms step_avg:39.73ms
step:217/2330 train_time:8604ms step_avg:39.65ms
step:218/2330 train_time:8661ms step_avg:39.73ms
step:219/2330 train_time:8684ms step_avg:39.65ms
step:220/2330 train_time:8740ms step_avg:39.73ms
step:221/2330 train_time:8763ms step_avg:39.65ms
step:222/2330 train_time:8820ms step_avg:39.73ms
step:223/2330 train_time:8844ms step_avg:39.66ms
step:224/2330 train_time:8901ms step_avg:39.73ms
step:225/2330 train_time:8924ms step_avg:39.66ms
step:226/2330 train_time:8980ms step_avg:39.73ms
step:227/2330 train_time:9003ms step_avg:39.66ms
step:228/2330 train_time:9061ms step_avg:39.74ms
step:229/2330 train_time:9083ms step_avg:39.67ms
step:230/2330 train_time:9140ms step_avg:39.74ms
step:231/2330 train_time:9164ms step_avg:39.67ms
step:232/2330 train_time:9222ms step_avg:39.75ms
step:233/2330 train_time:9245ms step_avg:39.68ms
step:234/2330 train_time:9301ms step_avg:39.75ms
step:235/2330 train_time:9325ms step_avg:39.68ms
step:236/2330 train_time:9381ms step_avg:39.75ms
step:237/2330 train_time:9404ms step_avg:39.68ms
step:238/2330 train_time:9461ms step_avg:39.75ms
step:239/2330 train_time:9485ms step_avg:39.68ms
step:240/2330 train_time:9541ms step_avg:39.75ms
step:241/2330 train_time:9565ms step_avg:39.69ms
step:242/2330 train_time:9622ms step_avg:39.76ms
step:243/2330 train_time:9645ms step_avg:39.69ms
step:244/2330 train_time:9702ms step_avg:39.76ms
step:245/2330 train_time:9725ms step_avg:39.69ms
step:246/2330 train_time:9781ms step_avg:39.76ms
step:247/2330 train_time:9805ms step_avg:39.70ms
step:248/2330 train_time:9862ms step_avg:39.76ms
step:249/2330 train_time:9885ms step_avg:39.70ms
step:250/2330 train_time:9941ms step_avg:39.76ms
step:250/2330 val_loss:5.7250 train_time:10039ms step_avg:40.16ms
step:251/2330 train_time:10052ms step_avg:40.05ms
step:252/2330 train_time:10064ms step_avg:39.94ms
step:253/2330 train_time:10073ms step_avg:39.81ms
step:254/2330 train_time:10102ms step_avg:39.77ms
step:255/2330 train_time:10125ms step_avg:39.70ms
step:256/2330 train_time:10181ms step_avg:39.77ms
step:257/2330 train_time:10203ms step_avg:39.70ms
step:258/2330 train_time:10259ms step_avg:39.76ms
step:259/2330 train_time:10282ms step_avg:39.70ms
step:260/2330 train_time:10341ms step_avg:39.77ms
step:261/2330 train_time:10366ms step_avg:39.72ms
step:262/2330 train_time:10427ms step_avg:39.80ms
step:263/2330 train_time:10452ms step_avg:39.74ms
step:264/2330 train_time:10509ms step_avg:39.81ms
step:265/2330 train_time:10533ms step_avg:39.75ms
step:266/2330 train_time:10589ms step_avg:39.81ms
step:267/2330 train_time:10612ms step_avg:39.75ms
step:268/2330 train_time:10668ms step_avg:39.81ms
step:269/2330 train_time:10691ms step_avg:39.74ms
step:270/2330 train_time:10748ms step_avg:39.81ms
step:271/2330 train_time:10771ms step_avg:39.74ms
step:272/2330 train_time:10826ms step_avg:39.80ms
step:273/2330 train_time:10850ms step_avg:39.74ms
step:274/2330 train_time:10905ms step_avg:39.80ms
step:275/2330 train_time:10928ms step_avg:39.74ms
step:276/2330 train_time:10986ms step_avg:39.80ms
step:277/2330 train_time:11009ms step_avg:39.74ms
step:278/2330 train_time:11065ms step_avg:39.80ms
step:279/2330 train_time:11088ms step_avg:39.74ms
step:280/2330 train_time:11144ms step_avg:39.80ms
step:281/2330 train_time:11168ms step_avg:39.74ms
step:282/2330 train_time:11224ms step_avg:39.80ms
step:283/2330 train_time:11247ms step_avg:39.74ms
step:284/2330 train_time:11305ms step_avg:39.81ms
step:285/2330 train_time:11330ms step_avg:39.76ms
step:286/2330 train_time:11388ms step_avg:39.82ms
step:287/2330 train_time:11413ms step_avg:39.77ms
step:288/2330 train_time:11470ms step_avg:39.83ms
step:289/2330 train_time:11494ms step_avg:39.77ms
step:290/2330 train_time:11550ms step_avg:39.83ms
step:291/2330 train_time:11573ms step_avg:39.77ms
step:292/2330 train_time:11630ms step_avg:39.83ms
step:293/2330 train_time:11653ms step_avg:39.77ms
step:294/2330 train_time:11709ms step_avg:39.83ms
step:295/2330 train_time:11732ms step_avg:39.77ms
step:296/2330 train_time:11788ms step_avg:39.82ms
step:297/2330 train_time:11811ms step_avg:39.77ms
step:298/2330 train_time:11867ms step_avg:39.82ms
step:299/2330 train_time:11891ms step_avg:39.77ms
step:300/2330 train_time:11947ms step_avg:39.82ms
step:301/2330 train_time:11970ms step_avg:39.77ms
step:302/2330 train_time:12026ms step_avg:39.82ms
step:303/2330 train_time:12049ms step_avg:39.76ms
step:304/2330 train_time:12105ms step_avg:39.82ms
step:305/2330 train_time:12128ms step_avg:39.77ms
step:306/2330 train_time:12185ms step_avg:39.82ms
step:307/2330 train_time:12209ms step_avg:39.77ms
step:308/2330 train_time:12266ms step_avg:39.83ms
step:309/2330 train_time:12290ms step_avg:39.77ms
step:310/2330 train_time:12348ms step_avg:39.83ms
step:311/2330 train_time:12372ms step_avg:39.78ms
step:312/2330 train_time:12428ms step_avg:39.83ms
step:313/2330 train_time:12452ms step_avg:39.78ms
step:314/2330 train_time:12508ms step_avg:39.83ms
step:315/2330 train_time:12533ms step_avg:39.79ms
step:316/2330 train_time:12589ms step_avg:39.84ms
step:317/2330 train_time:12612ms step_avg:39.79ms
step:318/2330 train_time:12668ms step_avg:39.84ms
step:319/2330 train_time:12691ms step_avg:39.78ms
step:320/2330 train_time:12747ms step_avg:39.84ms
step:321/2330 train_time:12771ms step_avg:39.78ms
step:322/2330 train_time:12827ms step_avg:39.84ms
step:323/2330 train_time:12850ms step_avg:39.78ms
step:324/2330 train_time:12907ms step_avg:39.84ms
step:325/2330 train_time:12930ms step_avg:39.78ms
step:326/2330 train_time:12986ms step_avg:39.83ms
step:327/2330 train_time:13010ms step_avg:39.78ms
step:328/2330 train_time:13066ms step_avg:39.83ms
step:329/2330 train_time:13089ms step_avg:39.78ms
step:330/2330 train_time:13146ms step_avg:39.84ms
step:331/2330 train_time:13170ms step_avg:39.79ms
step:332/2330 train_time:13226ms step_avg:39.84ms
step:333/2330 train_time:13249ms step_avg:39.79ms
step:334/2330 train_time:13305ms step_avg:39.84ms
step:335/2330 train_time:13328ms step_avg:39.79ms
step:336/2330 train_time:13385ms step_avg:39.84ms
step:337/2330 train_time:13410ms step_avg:39.79ms
step:338/2330 train_time:13467ms step_avg:39.84ms
step:339/2330 train_time:13491ms step_avg:39.80ms
step:340/2330 train_time:13548ms step_avg:39.85ms
step:341/2330 train_time:13572ms step_avg:39.80ms
step:342/2330 train_time:13628ms step_avg:39.85ms
step:343/2330 train_time:13651ms step_avg:39.80ms
step:344/2330 train_time:13708ms step_avg:39.85ms
step:345/2330 train_time:13731ms step_avg:39.80ms
step:346/2330 train_time:13787ms step_avg:39.85ms
step:347/2330 train_time:13811ms step_avg:39.80ms
step:348/2330 train_time:13867ms step_avg:39.85ms
step:349/2330 train_time:13890ms step_avg:39.80ms
step:350/2330 train_time:13947ms step_avg:39.85ms
step:351/2330 train_time:13970ms step_avg:39.80ms
step:352/2330 train_time:14026ms step_avg:39.85ms
step:353/2330 train_time:14050ms step_avg:39.80ms
step:354/2330 train_time:14107ms step_avg:39.85ms
step:355/2330 train_time:14130ms step_avg:39.80ms
step:356/2330 train_time:14187ms step_avg:39.85ms
step:357/2330 train_time:14210ms step_avg:39.81ms
step:358/2330 train_time:14267ms step_avg:39.85ms
step:359/2330 train_time:14291ms step_avg:39.81ms
step:360/2330 train_time:14348ms step_avg:39.85ms
step:361/2330 train_time:14371ms step_avg:39.81ms
step:362/2330 train_time:14428ms step_avg:39.86ms
step:363/2330 train_time:14452ms step_avg:39.81ms
step:364/2330 train_time:14509ms step_avg:39.86ms
step:365/2330 train_time:14532ms step_avg:39.81ms
step:366/2330 train_time:14588ms step_avg:39.86ms
step:367/2330 train_time:14612ms step_avg:39.81ms
step:368/2330 train_time:14668ms step_avg:39.86ms
step:369/2330 train_time:14691ms step_avg:39.81ms
step:370/2330 train_time:14748ms step_avg:39.86ms
step:371/2330 train_time:14771ms step_avg:39.82ms
step:372/2330 train_time:14828ms step_avg:39.86ms
step:373/2330 train_time:14851ms step_avg:39.81ms
step:374/2330 train_time:14907ms step_avg:39.86ms
step:375/2330 train_time:14930ms step_avg:39.81ms
step:376/2330 train_time:14986ms step_avg:39.86ms
step:377/2330 train_time:15010ms step_avg:39.81ms
step:378/2330 train_time:15066ms step_avg:39.86ms
step:379/2330 train_time:15090ms step_avg:39.81ms
step:380/2330 train_time:15146ms step_avg:39.86ms
step:381/2330 train_time:15170ms step_avg:39.82ms
step:382/2330 train_time:15226ms step_avg:39.86ms
step:383/2330 train_time:15249ms step_avg:39.82ms
step:384/2330 train_time:15306ms step_avg:39.86ms
step:385/2330 train_time:15330ms step_avg:39.82ms
step:386/2330 train_time:15387ms step_avg:39.86ms
step:387/2330 train_time:15410ms step_avg:39.82ms
step:388/2330 train_time:15467ms step_avg:39.86ms
step:389/2330 train_time:15490ms step_avg:39.82ms
step:390/2330 train_time:15547ms step_avg:39.86ms
step:391/2330 train_time:15570ms step_avg:39.82ms
step:392/2330 train_time:15627ms step_avg:39.86ms
step:393/2330 train_time:15650ms step_avg:39.82ms
step:394/2330 train_time:15707ms step_avg:39.86ms
step:395/2330 train_time:15730ms step_avg:39.82ms
step:396/2330 train_time:15786ms step_avg:39.86ms
step:397/2330 train_time:15810ms step_avg:39.82ms
step:398/2330 train_time:15866ms step_avg:39.87ms
step:399/2330 train_time:15890ms step_avg:39.82ms
step:400/2330 train_time:15946ms step_avg:39.87ms
step:401/2330 train_time:15970ms step_avg:39.83ms
step:402/2330 train_time:16026ms step_avg:39.87ms
step:403/2330 train_time:16049ms step_avg:39.83ms
step:404/2330 train_time:16106ms step_avg:39.87ms
step:405/2330 train_time:16130ms step_avg:39.83ms
step:406/2330 train_time:16187ms step_avg:39.87ms
step:407/2330 train_time:16210ms step_avg:39.83ms
step:408/2330 train_time:16267ms step_avg:39.87ms
step:409/2330 train_time:16290ms step_avg:39.83ms
step:410/2330 train_time:16347ms step_avg:39.87ms
step:411/2330 train_time:16371ms step_avg:39.83ms
step:412/2330 train_time:16427ms step_avg:39.87ms
step:413/2330 train_time:16451ms step_avg:39.83ms
step:414/2330 train_time:16508ms step_avg:39.87ms
step:415/2330 train_time:16531ms step_avg:39.83ms
step:416/2330 train_time:16587ms step_avg:39.87ms
step:417/2330 train_time:16611ms step_avg:39.83ms
step:418/2330 train_time:16667ms step_avg:39.87ms
step:419/2330 train_time:16690ms step_avg:39.83ms
step:420/2330 train_time:16747ms step_avg:39.87ms
step:421/2330 train_time:16771ms step_avg:39.84ms
step:422/2330 train_time:16828ms step_avg:39.88ms
step:423/2330 train_time:16851ms step_avg:39.84ms
step:424/2330 train_time:16907ms step_avg:39.88ms
step:425/2330 train_time:16930ms step_avg:39.84ms
step:426/2330 train_time:16987ms step_avg:39.87ms
step:427/2330 train_time:17010ms step_avg:39.84ms
step:428/2330 train_time:17066ms step_avg:39.87ms
step:429/2330 train_time:17090ms step_avg:39.84ms
step:430/2330 train_time:17148ms step_avg:39.88ms
step:431/2330 train_time:17172ms step_avg:39.84ms
step:432/2330 train_time:17228ms step_avg:39.88ms
step:433/2330 train_time:17252ms step_avg:39.84ms
step:434/2330 train_time:17308ms step_avg:39.88ms
step:435/2330 train_time:17331ms step_avg:39.84ms
step:436/2330 train_time:17388ms step_avg:39.88ms
step:437/2330 train_time:17411ms step_avg:39.84ms
step:438/2330 train_time:17468ms step_avg:39.88ms
step:439/2330 train_time:17491ms step_avg:39.84ms
step:440/2330 train_time:17548ms step_avg:39.88ms
step:441/2330 train_time:17571ms step_avg:39.84ms
step:442/2330 train_time:17628ms step_avg:39.88ms
step:443/2330 train_time:17651ms step_avg:39.84ms
step:444/2330 train_time:17708ms step_avg:39.88ms
step:445/2330 train_time:17731ms step_avg:39.84ms
step:446/2330 train_time:17788ms step_avg:39.88ms
step:447/2330 train_time:17811ms step_avg:39.85ms
step:448/2330 train_time:17867ms step_avg:39.88ms
step:449/2330 train_time:17891ms step_avg:39.85ms
step:450/2330 train_time:17947ms step_avg:39.88ms
step:451/2330 train_time:17971ms step_avg:39.85ms
step:452/2330 train_time:18027ms step_avg:39.88ms
step:453/2330 train_time:18051ms step_avg:39.85ms
step:454/2330 train_time:18107ms step_avg:39.88ms
step:455/2330 train_time:18131ms step_avg:39.85ms
step:456/2330 train_time:18188ms step_avg:39.89ms
step:457/2330 train_time:18211ms step_avg:39.85ms
step:458/2330 train_time:18268ms step_avg:39.89ms
step:459/2330 train_time:18291ms step_avg:39.85ms
step:460/2330 train_time:18348ms step_avg:39.89ms
step:461/2330 train_time:18372ms step_avg:39.85ms
step:462/2330 train_time:18428ms step_avg:39.89ms
step:463/2330 train_time:18452ms step_avg:39.85ms
step:464/2330 train_time:18509ms step_avg:39.89ms
step:465/2330 train_time:18532ms step_avg:39.85ms
step:466/2330 train_time:18589ms step_avg:39.89ms
step:467/2330 train_time:18611ms step_avg:39.85ms
step:468/2330 train_time:18668ms step_avg:39.89ms
step:469/2330 train_time:18692ms step_avg:39.86ms
step:470/2330 train_time:18749ms step_avg:39.89ms
step:471/2330 train_time:18772ms step_avg:39.86ms
step:472/2330 train_time:18829ms step_avg:39.89ms
step:473/2330 train_time:18852ms step_avg:39.86ms
step:474/2330 train_time:18909ms step_avg:39.89ms
step:475/2330 train_time:18932ms step_avg:39.86ms
step:476/2330 train_time:18989ms step_avg:39.89ms
step:477/2330 train_time:19013ms step_avg:39.86ms
step:478/2330 train_time:19070ms step_avg:39.90ms
step:479/2330 train_time:19092ms step_avg:39.86ms
step:480/2330 train_time:19149ms step_avg:39.89ms
step:481/2330 train_time:19172ms step_avg:39.86ms
step:482/2330 train_time:19228ms step_avg:39.89ms
step:483/2330 train_time:19251ms step_avg:39.86ms
step:484/2330 train_time:19309ms step_avg:39.89ms
step:485/2330 train_time:19332ms step_avg:39.86ms
step:486/2330 train_time:19389ms step_avg:39.89ms
step:487/2330 train_time:19412ms step_avg:39.86ms
step:488/2330 train_time:19468ms step_avg:39.89ms
step:489/2330 train_time:19492ms step_avg:39.86ms
step:490/2330 train_time:19548ms step_avg:39.89ms
step:491/2330 train_time:19572ms step_avg:39.86ms
step:492/2330 train_time:19628ms step_avg:39.89ms
step:493/2330 train_time:19651ms step_avg:39.86ms
step:494/2330 train_time:19708ms step_avg:39.90ms
step:495/2330 train_time:19732ms step_avg:39.86ms
step:496/2330 train_time:19789ms step_avg:39.90ms
step:497/2330 train_time:19812ms step_avg:39.86ms
step:498/2330 train_time:19868ms step_avg:39.90ms
step:499/2330 train_time:19892ms step_avg:39.86ms
step:500/2330 train_time:19948ms step_avg:39.90ms
step:500/2330 val_loss:5.6426 train_time:20047ms step_avg:40.09ms
step:501/2330 train_time:20059ms step_avg:40.04ms
step:502/2330 train_time:20071ms step_avg:39.98ms
step:503/2330 train_time:20081ms step_avg:39.92ms
step:504/2330 train_time:20111ms step_avg:39.90ms
step:505/2330 train_time:20133ms step_avg:39.87ms
step:506/2330 train_time:20189ms step_avg:39.90ms
step:507/2330 train_time:20211ms step_avg:39.86ms
step:508/2330 train_time:20268ms step_avg:39.90ms
step:509/2330 train_time:20291ms step_avg:39.87ms
step:510/2330 train_time:20349ms step_avg:39.90ms
step:511/2330 train_time:20374ms step_avg:39.87ms
step:512/2330 train_time:20435ms step_avg:39.91ms
step:513/2330 train_time:20461ms step_avg:39.89ms
step:514/2330 train_time:20518ms step_avg:39.92ms
step:515/2330 train_time:20540ms step_avg:39.88ms
step:516/2330 train_time:20597ms step_avg:39.92ms
step:517/2330 train_time:20619ms step_avg:39.88ms
step:518/2330 train_time:20675ms step_avg:39.91ms
step:519/2330 train_time:20698ms step_avg:39.88ms
step:520/2330 train_time:20754ms step_avg:39.91ms
step:521/2330 train_time:20778ms step_avg:39.88ms
step:522/2330 train_time:20833ms step_avg:39.91ms
step:523/2330 train_time:20857ms step_avg:39.88ms
step:524/2330 train_time:20912ms step_avg:39.91ms
step:525/2330 train_time:20935ms step_avg:39.88ms
step:526/2330 train_time:20993ms step_avg:39.91ms
step:527/2330 train_time:21017ms step_avg:39.88ms
step:528/2330 train_time:21074ms step_avg:39.91ms
step:529/2330 train_time:21097ms step_avg:39.88ms
step:530/2330 train_time:21154ms step_avg:39.91ms
step:531/2330 train_time:21178ms step_avg:39.88ms
step:532/2330 train_time:21234ms step_avg:39.91ms
step:533/2330 train_time:21258ms step_avg:39.88ms
step:534/2330 train_time:21316ms step_avg:39.92ms
step:535/2330 train_time:21341ms step_avg:39.89ms
step:536/2330 train_time:21399ms step_avg:39.92ms
step:537/2330 train_time:21422ms step_avg:39.89ms
step:538/2330 train_time:21479ms step_avg:39.92ms
step:539/2330 train_time:21502ms step_avg:39.89ms
step:540/2330 train_time:21559ms step_avg:39.92ms
step:541/2330 train_time:21582ms step_avg:39.89ms
step:542/2330 train_time:21639ms step_avg:39.92ms
step:543/2330 train_time:21661ms step_avg:39.89ms
step:544/2330 train_time:21719ms step_avg:39.92ms
step:545/2330 train_time:21741ms step_avg:39.89ms
step:546/2330 train_time:21799ms step_avg:39.93ms
step:547/2330 train_time:21821ms step_avg:39.89ms
step:548/2330 train_time:21877ms step_avg:39.92ms
step:549/2330 train_time:21900ms step_avg:39.89ms
step:550/2330 train_time:21959ms step_avg:39.92ms
step:551/2330 train_time:21981ms step_avg:39.89ms
step:552/2330 train_time:22038ms step_avg:39.92ms
step:553/2330 train_time:22061ms step_avg:39.89ms
step:554/2330 train_time:22119ms step_avg:39.93ms
step:555/2330 train_time:22143ms step_avg:39.90ms
step:556/2330 train_time:22200ms step_avg:39.93ms
step:557/2330 train_time:22223ms step_avg:39.90ms
step:558/2330 train_time:22280ms step_avg:39.93ms
step:559/2330 train_time:22303ms step_avg:39.90ms
step:560/2330 train_time:22361ms step_avg:39.93ms
step:561/2330 train_time:22384ms step_avg:39.90ms
step:562/2330 train_time:22442ms step_avg:39.93ms
step:563/2330 train_time:22465ms step_avg:39.90ms
step:564/2330 train_time:22524ms step_avg:39.94ms
step:565/2330 train_time:22546ms step_avg:39.91ms
step:566/2330 train_time:22604ms step_avg:39.94ms
step:567/2330 train_time:22627ms step_avg:39.91ms
step:568/2330 train_time:22685ms step_avg:39.94ms
step:569/2330 train_time:22709ms step_avg:39.91ms
step:570/2330 train_time:22767ms step_avg:39.94ms
step:571/2330 train_time:22790ms step_avg:39.91ms
step:572/2330 train_time:22847ms step_avg:39.94ms
step:573/2330 train_time:22870ms step_avg:39.91ms
step:574/2330 train_time:22927ms step_avg:39.94ms
step:575/2330 train_time:22951ms step_avg:39.91ms
step:576/2330 train_time:23007ms step_avg:39.94ms
step:577/2330 train_time:23030ms step_avg:39.91ms
step:578/2330 train_time:23088ms step_avg:39.94ms
step:579/2330 train_time:23111ms step_avg:39.91ms
step:580/2330 train_time:23168ms step_avg:39.95ms
step:581/2330 train_time:23192ms step_avg:39.92ms
step:582/2330 train_time:23249ms step_avg:39.95ms
step:583/2330 train_time:23273ms step_avg:39.92ms
step:584/2330 train_time:23330ms step_avg:39.95ms
step:585/2330 train_time:23354ms step_avg:39.92ms
step:586/2330 train_time:23411ms step_avg:39.95ms
step:587/2330 train_time:23435ms step_avg:39.92ms
step:588/2330 train_time:23492ms step_avg:39.95ms
step:589/2330 train_time:23517ms step_avg:39.93ms
step:590/2330 train_time:23574ms step_avg:39.96ms
step:591/2330 train_time:23597ms step_avg:39.93ms
step:592/2330 train_time:23654ms step_avg:39.96ms
step:593/2330 train_time:23678ms step_avg:39.93ms
step:594/2330 train_time:23734ms step_avg:39.96ms
step:595/2330 train_time:23758ms step_avg:39.93ms
step:596/2330 train_time:23814ms step_avg:39.96ms
step:597/2330 train_time:23837ms step_avg:39.93ms
step:598/2330 train_time:23894ms step_avg:39.96ms
step:599/2330 train_time:23917ms step_avg:39.93ms
step:600/2330 train_time:23973ms step_avg:39.96ms
step:601/2330 train_time:23997ms step_avg:39.93ms
step:602/2330 train_time:24053ms step_avg:39.96ms
step:603/2330 train_time:24077ms step_avg:39.93ms
step:604/2330 train_time:24134ms step_avg:39.96ms
step:605/2330 train_time:24157ms step_avg:39.93ms
step:606/2330 train_time:24213ms step_avg:39.96ms
step:607/2330 train_time:24238ms step_avg:39.93ms
step:608/2330 train_time:24294ms step_avg:39.96ms
step:609/2330 train_time:24318ms step_avg:39.93ms
step:610/2330 train_time:24376ms step_avg:39.96ms
step:611/2330 train_time:24400ms step_avg:39.93ms
step:612/2330 train_time:24457ms step_avg:39.96ms
step:613/2330 train_time:24480ms step_avg:39.93ms
step:614/2330 train_time:24537ms step_avg:39.96ms
step:615/2330 train_time:24559ms step_avg:39.93ms
step:616/2330 train_time:24616ms step_avg:39.96ms
step:617/2330 train_time:24638ms step_avg:39.93ms
step:618/2330 train_time:24695ms step_avg:39.96ms
step:619/2330 train_time:24718ms step_avg:39.93ms
step:620/2330 train_time:24775ms step_avg:39.96ms
step:621/2330 train_time:24798ms step_avg:39.93ms
step:622/2330 train_time:24854ms step_avg:39.96ms
step:623/2330 train_time:24877ms step_avg:39.93ms
step:624/2330 train_time:24934ms step_avg:39.96ms
step:625/2330 train_time:24958ms step_avg:39.93ms
step:626/2330 train_time:25015ms step_avg:39.96ms
step:627/2330 train_time:25038ms step_avg:39.93ms
step:628/2330 train_time:25095ms step_avg:39.96ms
step:629/2330 train_time:25119ms step_avg:39.93ms
step:630/2330 train_time:25176ms step_avg:39.96ms
step:631/2330 train_time:25200ms step_avg:39.94ms
step:632/2330 train_time:25259ms step_avg:39.97ms
step:633/2330 train_time:25281ms step_avg:39.94ms
step:634/2330 train_time:25339ms step_avg:39.97ms
step:635/2330 train_time:25361ms step_avg:39.94ms
step:636/2330 train_time:25419ms step_avg:39.97ms
step:637/2330 train_time:25442ms step_avg:39.94ms
step:638/2330 train_time:25499ms step_avg:39.97ms
step:639/2330 train_time:25522ms step_avg:39.94ms
step:640/2330 train_time:25579ms step_avg:39.97ms
step:641/2330 train_time:25602ms step_avg:39.94ms
step:642/2330 train_time:25659ms step_avg:39.97ms
step:643/2330 train_time:25682ms step_avg:39.94ms
step:644/2330 train_time:25740ms step_avg:39.97ms
step:645/2330 train_time:25763ms step_avg:39.94ms
step:646/2330 train_time:25820ms step_avg:39.97ms
step:647/2330 train_time:25843ms step_avg:39.94ms
step:648/2330 train_time:25902ms step_avg:39.97ms
step:649/2330 train_time:25925ms step_avg:39.95ms
step:650/2330 train_time:25983ms step_avg:39.97ms
step:651/2330 train_time:26006ms step_avg:39.95ms
step:652/2330 train_time:26064ms step_avg:39.97ms
step:653/2330 train_time:26087ms step_avg:39.95ms
step:654/2330 train_time:26145ms step_avg:39.98ms
step:655/2330 train_time:26169ms step_avg:39.95ms
step:656/2330 train_time:26227ms step_avg:39.98ms
step:657/2330 train_time:26251ms step_avg:39.96ms
step:658/2330 train_time:26309ms step_avg:39.98ms
step:659/2330 train_time:26332ms step_avg:39.96ms
step:660/2330 train_time:26389ms step_avg:39.98ms
step:661/2330 train_time:26412ms step_avg:39.96ms
step:662/2330 train_time:26470ms step_avg:39.99ms
step:663/2330 train_time:26494ms step_avg:39.96ms
step:664/2330 train_time:26550ms step_avg:39.99ms
step:665/2330 train_time:26574ms step_avg:39.96ms
step:666/2330 train_time:26631ms step_avg:39.99ms
step:667/2330 train_time:26654ms step_avg:39.96ms
step:668/2330 train_time:26711ms step_avg:39.99ms
step:669/2330 train_time:26734ms step_avg:39.96ms
step:670/2330 train_time:26791ms step_avg:39.99ms
step:671/2330 train_time:26814ms step_avg:39.96ms
step:672/2330 train_time:26871ms step_avg:39.99ms
step:673/2330 train_time:26894ms step_avg:39.96ms
step:674/2330 train_time:26951ms step_avg:39.99ms
step:675/2330 train_time:26975ms step_avg:39.96ms
step:676/2330 train_time:27032ms step_avg:39.99ms
step:677/2330 train_time:27056ms step_avg:39.96ms
step:678/2330 train_time:27112ms step_avg:39.99ms
step:679/2330 train_time:27136ms step_avg:39.97ms
step:680/2330 train_time:27194ms step_avg:39.99ms
step:681/2330 train_time:27218ms step_avg:39.97ms
step:682/2330 train_time:27275ms step_avg:39.99ms
step:683/2330 train_time:27299ms step_avg:39.97ms
step:684/2330 train_time:27356ms step_avg:39.99ms
step:685/2330 train_time:27379ms step_avg:39.97ms
step:686/2330 train_time:27436ms step_avg:39.99ms
step:687/2330 train_time:27458ms step_avg:39.97ms
step:688/2330 train_time:27515ms step_avg:39.99ms
step:689/2330 train_time:27538ms step_avg:39.97ms
step:690/2330 train_time:27595ms step_avg:39.99ms
step:691/2330 train_time:27617ms step_avg:39.97ms
step:692/2330 train_time:27674ms step_avg:39.99ms
step:693/2330 train_time:27697ms step_avg:39.97ms
step:694/2330 train_time:27754ms step_avg:39.99ms
step:695/2330 train_time:27778ms step_avg:39.97ms
step:696/2330 train_time:27833ms step_avg:39.99ms
step:697/2330 train_time:27857ms step_avg:39.97ms
step:698/2330 train_time:27914ms step_avg:39.99ms
step:699/2330 train_time:27937ms step_avg:39.97ms
step:700/2330 train_time:27994ms step_avg:39.99ms
step:701/2330 train_time:28017ms step_avg:39.97ms
step:702/2330 train_time:28074ms step_avg:39.99ms
step:703/2330 train_time:28098ms step_avg:39.97ms
step:704/2330 train_time:28154ms step_avg:39.99ms
step:705/2330 train_time:28178ms step_avg:39.97ms
step:706/2330 train_time:28235ms step_avg:39.99ms
step:707/2330 train_time:28258ms step_avg:39.97ms
step:708/2330 train_time:28314ms step_avg:39.99ms
step:709/2330 train_time:28339ms step_avg:39.97ms
step:710/2330 train_time:28396ms step_avg:39.99ms
step:711/2330 train_time:28419ms step_avg:39.97ms
step:712/2330 train_time:28477ms step_avg:40.00ms
step:713/2330 train_time:28499ms step_avg:39.97ms
step:714/2330 train_time:28558ms step_avg:40.00ms
step:715/2330 train_time:28580ms step_avg:39.97ms
step:716/2330 train_time:28639ms step_avg:40.00ms
step:717/2330 train_time:28661ms step_avg:39.97ms
step:718/2330 train_time:28719ms step_avg:40.00ms
step:719/2330 train_time:28741ms step_avg:39.97ms
step:720/2330 train_time:28799ms step_avg:40.00ms
step:721/2330 train_time:28822ms step_avg:39.97ms
step:722/2330 train_time:28880ms step_avg:40.00ms
step:723/2330 train_time:28902ms step_avg:39.98ms
step:724/2330 train_time:28960ms step_avg:40.00ms
step:725/2330 train_time:28983ms step_avg:39.98ms
step:726/2330 train_time:29040ms step_avg:40.00ms
step:727/2330 train_time:29064ms step_avg:39.98ms
step:728/2330 train_time:29121ms step_avg:40.00ms
step:729/2330 train_time:29145ms step_avg:39.98ms
step:730/2330 train_time:29202ms step_avg:40.00ms
step:731/2330 train_time:29225ms step_avg:39.98ms
step:732/2330 train_time:29283ms step_avg:40.00ms
step:733/2330 train_time:29306ms step_avg:39.98ms
step:734/2330 train_time:29364ms step_avg:40.01ms
step:735/2330 train_time:29388ms step_avg:39.98ms
step:736/2330 train_time:29445ms step_avg:40.01ms
step:737/2330 train_time:29469ms step_avg:39.98ms
step:738/2330 train_time:29528ms step_avg:40.01ms
step:739/2330 train_time:29551ms step_avg:39.99ms
step:740/2330 train_time:29609ms step_avg:40.01ms
step:741/2330 train_time:29632ms step_avg:39.99ms
step:742/2330 train_time:29689ms step_avg:40.01ms
step:743/2330 train_time:29712ms step_avg:39.99ms
step:744/2330 train_time:29770ms step_avg:40.01ms
step:745/2330 train_time:29793ms step_avg:39.99ms
step:746/2330 train_time:29850ms step_avg:40.01ms
step:747/2330 train_time:29874ms step_avg:39.99ms
step:748/2330 train_time:29930ms step_avg:40.01ms
step:749/2330 train_time:29954ms step_avg:39.99ms
step:750/2330 train_time:30011ms step_avg:40.02ms
step:750/2330 val_loss:5.5452 train_time:30112ms step_avg:40.15ms
step:751/2330 train_time:30124ms step_avg:40.11ms
step:752/2330 train_time:30136ms step_avg:40.07ms
step:753/2330 train_time:30145ms step_avg:40.03ms
step:754/2330 train_time:30176ms step_avg:40.02ms
step:755/2330 train_time:30198ms step_avg:40.00ms
step:756/2330 train_time:30255ms step_avg:40.02ms
step:757/2330 train_time:30277ms step_avg:40.00ms
step:758/2330 train_time:30333ms step_avg:40.02ms
step:759/2330 train_time:30355ms step_avg:39.99ms
step:760/2330 train_time:30414ms step_avg:40.02ms
step:761/2330 train_time:30440ms step_avg:40.00ms
step:762/2330 train_time:30502ms step_avg:40.03ms
step:763/2330 train_time:30526ms step_avg:40.01ms
step:764/2330 train_time:30585ms step_avg:40.03ms
step:765/2330 train_time:30610ms step_avg:40.01ms
step:766/2330 train_time:30667ms step_avg:40.04ms
step:767/2330 train_time:30689ms step_avg:40.01ms
step:768/2330 train_time:30746ms step_avg:40.03ms
step:769/2330 train_time:30769ms step_avg:40.01ms
step:770/2330 train_time:30826ms step_avg:40.03ms
step:771/2330 train_time:30850ms step_avg:40.01ms
step:772/2330 train_time:30906ms step_avg:40.03ms
step:773/2330 train_time:30929ms step_avg:40.01ms
step:774/2330 train_time:30986ms step_avg:40.03ms
step:775/2330 train_time:31009ms step_avg:40.01ms
step:776/2330 train_time:31066ms step_avg:40.03ms
step:777/2330 train_time:31090ms step_avg:40.01ms
step:778/2330 train_time:31148ms step_avg:40.04ms
step:779/2330 train_time:31172ms step_avg:40.02ms
step:780/2330 train_time:31229ms step_avg:40.04ms
step:781/2330 train_time:31252ms step_avg:40.02ms
step:782/2330 train_time:31308ms step_avg:40.04ms
step:783/2330 train_time:31332ms step_avg:40.02ms
step:784/2330 train_time:31390ms step_avg:40.04ms
step:785/2330 train_time:31414ms step_avg:40.02ms
step:786/2330 train_time:31472ms step_avg:40.04ms
step:787/2330 train_time:31497ms step_avg:40.02ms
step:788/2330 train_time:31555ms step_avg:40.04ms
step:789/2330 train_time:31577ms step_avg:40.02ms
step:790/2330 train_time:31635ms step_avg:40.04ms
step:791/2330 train_time:31657ms step_avg:40.02ms
step:792/2330 train_time:31716ms step_avg:40.05ms
step:793/2330 train_time:31739ms step_avg:40.02ms
step:794/2330 train_time:31796ms step_avg:40.04ms
step:795/2330 train_time:31818ms step_avg:40.02ms
step:796/2330 train_time:31875ms step_avg:40.04ms
step:797/2330 train_time:31898ms step_avg:40.02ms
step:798/2330 train_time:31955ms step_avg:40.04ms
step:799/2330 train_time:31978ms step_avg:40.02ms
step:800/2330 train_time:32036ms step_avg:40.04ms
step:801/2330 train_time:32058ms step_avg:40.02ms
step:802/2330 train_time:32116ms step_avg:40.05ms
step:803/2330 train_time:32139ms step_avg:40.02ms
step:804/2330 train_time:32196ms step_avg:40.04ms
step:805/2330 train_time:32219ms step_avg:40.02ms
step:806/2330 train_time:32277ms step_avg:40.05ms
step:807/2330 train_time:32300ms step_avg:40.02ms
step:808/2330 train_time:32358ms step_avg:40.05ms
step:809/2330 train_time:32381ms step_avg:40.03ms
step:810/2330 train_time:32440ms step_avg:40.05ms
step:811/2330 train_time:32463ms step_avg:40.03ms
step:812/2330 train_time:32521ms step_avg:40.05ms
step:813/2330 train_time:32545ms step_avg:40.03ms
step:814/2330 train_time:32604ms step_avg:40.05ms
step:815/2330 train_time:32627ms step_avg:40.03ms
step:816/2330 train_time:32684ms step_avg:40.05ms
step:817/2330 train_time:32707ms step_avg:40.03ms
step:818/2330 train_time:32764ms step_avg:40.05ms
step:819/2330 train_time:32787ms step_avg:40.03ms
step:820/2330 train_time:32844ms step_avg:40.05ms
step:821/2330 train_time:32867ms step_avg:40.03ms
step:822/2330 train_time:32925ms step_avg:40.05ms
step:823/2330 train_time:32948ms step_avg:40.03ms
step:824/2330 train_time:33005ms step_avg:40.06ms
step:825/2330 train_time:33029ms step_avg:40.04ms
step:826/2330 train_time:33087ms step_avg:40.06ms
step:827/2330 train_time:33111ms step_avg:40.04ms
step:828/2330 train_time:33167ms step_avg:40.06ms
step:829/2330 train_time:33191ms step_avg:40.04ms
step:830/2330 train_time:33249ms step_avg:40.06ms
step:831/2330 train_time:33272ms step_avg:40.04ms
step:832/2330 train_time:33329ms step_avg:40.06ms
step:833/2330 train_time:33353ms step_avg:40.04ms
step:834/2330 train_time:33410ms step_avg:40.06ms
step:835/2330 train_time:33435ms step_avg:40.04ms
step:836/2330 train_time:33492ms step_avg:40.06ms
step:837/2330 train_time:33516ms step_avg:40.04ms
step:838/2330 train_time:33573ms step_avg:40.06ms
step:839/2330 train_time:33596ms step_avg:40.04ms
step:840/2330 train_time:33654ms step_avg:40.06ms
step:841/2330 train_time:33677ms step_avg:40.04ms
step:842/2330 train_time:33735ms step_avg:40.07ms
step:843/2330 train_time:33757ms step_avg:40.04ms
step:844/2330 train_time:33814ms step_avg:40.06ms
step:845/2330 train_time:33837ms step_avg:40.04ms
step:846/2330 train_time:33895ms step_avg:40.07ms
step:847/2330 train_time:33918ms step_avg:40.04ms
step:848/2330 train_time:33976ms step_avg:40.07ms
step:849/2330 train_time:33998ms step_avg:40.05ms
step:850/2330 train_time:34057ms step_avg:40.07ms
step:851/2330 train_time:34079ms step_avg:40.05ms
step:852/2330 train_time:34138ms step_avg:40.07ms
step:853/2330 train_time:34160ms step_avg:40.05ms
step:854/2330 train_time:34218ms step_avg:40.07ms
step:855/2330 train_time:34241ms step_avg:40.05ms
step:856/2330 train_time:34298ms step_avg:40.07ms
step:857/2330 train_time:34322ms step_avg:40.05ms
step:858/2330 train_time:34381ms step_avg:40.07ms
step:859/2330 train_time:34404ms step_avg:40.05ms
step:860/2330 train_time:34462ms step_avg:40.07ms
step:861/2330 train_time:34485ms step_avg:40.05ms
step:862/2330 train_time:34543ms step_avg:40.07ms
step:863/2330 train_time:34566ms step_avg:40.05ms
step:864/2330 train_time:34624ms step_avg:40.07ms
step:865/2330 train_time:34648ms step_avg:40.06ms
step:866/2330 train_time:34705ms step_avg:40.08ms
step:867/2330 train_time:34729ms step_avg:40.06ms
step:868/2330 train_time:34787ms step_avg:40.08ms
step:869/2330 train_time:34810ms step_avg:40.06ms
step:870/2330 train_time:34868ms step_avg:40.08ms
step:871/2330 train_time:34891ms step_avg:40.06ms
step:872/2330 train_time:34948ms step_avg:40.08ms
step:873/2330 train_time:34971ms step_avg:40.06ms
step:874/2330 train_time:35028ms step_avg:40.08ms
step:875/2330 train_time:35052ms step_avg:40.06ms
step:876/2330 train_time:35108ms step_avg:40.08ms
step:877/2330 train_time:35131ms step_avg:40.06ms
step:878/2330 train_time:35188ms step_avg:40.08ms
step:879/2330 train_time:35211ms step_avg:40.06ms
step:880/2330 train_time:35269ms step_avg:40.08ms
step:881/2330 train_time:35294ms step_avg:40.06ms
step:882/2330 train_time:35351ms step_avg:40.08ms
step:883/2330 train_time:35375ms step_avg:40.06ms
step:884/2330 train_time:35431ms step_avg:40.08ms
step:885/2330 train_time:35455ms step_avg:40.06ms
step:886/2330 train_time:35511ms step_avg:40.08ms
step:887/2330 train_time:35535ms step_avg:40.06ms
step:888/2330 train_time:35591ms step_avg:40.08ms
step:889/2330 train_time:35615ms step_avg:40.06ms
step:890/2330 train_time:35671ms step_avg:40.08ms
step:891/2330 train_time:35695ms step_avg:40.06ms
step:892/2330 train_time:35753ms step_avg:40.08ms
step:893/2330 train_time:35777ms step_avg:40.06ms
step:894/2330 train_time:35835ms step_avg:40.08ms
step:895/2330 train_time:35858ms step_avg:40.06ms
step:896/2330 train_time:35916ms step_avg:40.08ms
step:897/2330 train_time:35938ms step_avg:40.07ms
step:898/2330 train_time:35996ms step_avg:40.08ms
step:899/2330 train_time:36019ms step_avg:40.07ms
step:900/2330 train_time:36076ms step_avg:40.08ms
step:901/2330 train_time:36099ms step_avg:40.07ms
step:902/2330 train_time:36157ms step_avg:40.09ms
step:903/2330 train_time:36180ms step_avg:40.07ms
step:904/2330 train_time:36238ms step_avg:40.09ms
step:905/2330 train_time:36261ms step_avg:40.07ms
step:906/2330 train_time:36319ms step_avg:40.09ms
step:907/2330 train_time:36341ms step_avg:40.07ms
step:908/2330 train_time:36399ms step_avg:40.09ms
step:909/2330 train_time:36422ms step_avg:40.07ms
step:910/2330 train_time:36479ms step_avg:40.09ms
step:911/2330 train_time:36502ms step_avg:40.07ms
step:912/2330 train_time:36560ms step_avg:40.09ms
step:913/2330 train_time:36583ms step_avg:40.07ms
step:914/2330 train_time:36641ms step_avg:40.09ms
step:915/2330 train_time:36664ms step_avg:40.07ms
step:916/2330 train_time:36722ms step_avg:40.09ms
step:917/2330 train_time:36745ms step_avg:40.07ms
step:918/2330 train_time:36803ms step_avg:40.09ms
step:919/2330 train_time:36826ms step_avg:40.07ms
step:920/2330 train_time:36884ms step_avg:40.09ms
step:921/2330 train_time:36908ms step_avg:40.07ms
step:922/2330 train_time:36965ms step_avg:40.09ms
step:923/2330 train_time:36989ms step_avg:40.07ms
step:924/2330 train_time:37046ms step_avg:40.09ms
step:925/2330 train_time:37069ms step_avg:40.07ms
step:926/2330 train_time:37127ms step_avg:40.09ms
step:927/2330 train_time:37150ms step_avg:40.08ms
step:928/2330 train_time:37207ms step_avg:40.09ms
step:929/2330 train_time:37230ms step_avg:40.08ms
step:930/2330 train_time:37287ms step_avg:40.09ms
step:931/2330 train_time:37311ms step_avg:40.08ms
step:932/2330 train_time:37368ms step_avg:40.09ms
step:933/2330 train_time:37392ms step_avg:40.08ms
step:934/2330 train_time:37449ms step_avg:40.10ms
step:935/2330 train_time:37473ms step_avg:40.08ms
step:936/2330 train_time:37530ms step_avg:40.10ms
step:937/2330 train_time:37554ms step_avg:40.08ms
step:938/2330 train_time:37611ms step_avg:40.10ms
step:939/2330 train_time:37635ms step_avg:40.08ms
step:940/2330 train_time:37692ms step_avg:40.10ms
step:941/2330 train_time:37716ms step_avg:40.08ms
step:942/2330 train_time:37772ms step_avg:40.10ms
step:943/2330 train_time:37796ms step_avg:40.08ms
step:944/2330 train_time:37853ms step_avg:40.10ms
step:945/2330 train_time:37876ms step_avg:40.08ms
step:946/2330 train_time:37934ms step_avg:40.10ms
step:947/2330 train_time:37956ms step_avg:40.08ms
step:948/2330 train_time:38015ms step_avg:40.10ms
step:949/2330 train_time:38038ms step_avg:40.08ms
step:950/2330 train_time:38096ms step_avg:40.10ms
step:951/2330 train_time:38119ms step_avg:40.08ms
step:952/2330 train_time:38178ms step_avg:40.10ms
step:953/2330 train_time:38200ms step_avg:40.08ms
step:954/2330 train_time:38258ms step_avg:40.10ms
step:955/2330 train_time:38281ms step_avg:40.08ms
step:956/2330 train_time:38339ms step_avg:40.10ms
step:957/2330 train_time:38362ms step_avg:40.09ms
step:958/2330 train_time:38420ms step_avg:40.10ms
step:959/2330 train_time:38443ms step_avg:40.09ms
step:960/2330 train_time:38500ms step_avg:40.10ms
step:961/2330 train_time:38524ms step_avg:40.09ms
step:962/2330 train_time:38582ms step_avg:40.11ms
step:963/2330 train_time:38605ms step_avg:40.09ms
step:964/2330 train_time:38663ms step_avg:40.11ms
step:965/2330 train_time:38685ms step_avg:40.09ms
step:966/2330 train_time:38743ms step_avg:40.11ms
step:967/2330 train_time:38766ms step_avg:40.09ms
step:968/2330 train_time:38824ms step_avg:40.11ms
step:969/2330 train_time:38848ms step_avg:40.09ms
step:970/2330 train_time:38906ms step_avg:40.11ms
step:971/2330 train_time:38930ms step_avg:40.09ms
step:972/2330 train_time:38987ms step_avg:40.11ms
step:973/2330 train_time:39011ms step_avg:40.09ms
step:974/2330 train_time:39068ms step_avg:40.11ms
step:975/2330 train_time:39092ms step_avg:40.09ms
step:976/2330 train_time:39149ms step_avg:40.11ms
step:977/2330 train_time:39172ms step_avg:40.09ms
step:978/2330 train_time:39229ms step_avg:40.11ms
step:979/2330 train_time:39253ms step_avg:40.09ms
step:980/2330 train_time:39309ms step_avg:40.11ms
step:981/2330 train_time:39333ms step_avg:40.10ms
step:982/2330 train_time:39390ms step_avg:40.11ms
step:983/2330 train_time:39414ms step_avg:40.10ms
step:984/2330 train_time:39471ms step_avg:40.11ms
step:985/2330 train_time:39496ms step_avg:40.10ms
step:986/2330 train_time:39553ms step_avg:40.11ms
step:987/2330 train_time:39577ms step_avg:40.10ms
step:988/2330 train_time:39634ms step_avg:40.12ms
step:989/2330 train_time:39657ms step_avg:40.10ms
step:990/2330 train_time:39715ms step_avg:40.12ms
step:991/2330 train_time:39737ms step_avg:40.10ms
step:992/2330 train_time:39795ms step_avg:40.12ms
step:993/2330 train_time:39817ms step_avg:40.10ms
step:994/2330 train_time:39875ms step_avg:40.12ms
step:995/2330 train_time:39898ms step_avg:40.10ms
step:996/2330 train_time:39956ms step_avg:40.12ms
step:997/2330 train_time:39980ms step_avg:40.10ms
step:998/2330 train_time:40038ms step_avg:40.12ms
step:999/2330 train_time:40061ms step_avg:40.10ms
step:1000/2330 train_time:40119ms step_avg:40.12ms
step:1000/2330 val_loss:5.4791 train_time:40218ms step_avg:40.22ms
step:1001/2330 train_time:40230ms step_avg:40.19ms
step:1002/2330 train_time:40243ms step_avg:40.16ms
step:1003/2330 train_time:40253ms step_avg:40.13ms
step:1004/2330 train_time:40282ms step_avg:40.12ms
step:1005/2330 train_time:40304ms step_avg:40.10ms
step:1006/2330 train_time:40361ms step_avg:40.12ms
step:1007/2330 train_time:40383ms step_avg:40.10ms
step:1008/2330 train_time:40440ms step_avg:40.12ms
step:1009/2330 train_time:40462ms step_avg:40.10ms
step:1010/2330 train_time:40521ms step_avg:40.12ms
step:1011/2330 train_time:40550ms step_avg:40.11ms
step:1012/2330 train_time:40614ms step_avg:40.13ms
step:1013/2330 train_time:40640ms step_avg:40.12ms
step:1014/2330 train_time:40700ms step_avg:40.14ms
step:1015/2330 train_time:40723ms step_avg:40.12ms
step:1016/2330 train_time:40781ms step_avg:40.14ms
step:1017/2330 train_time:40804ms step_avg:40.12ms
step:1018/2330 train_time:40860ms step_avg:40.14ms
step:1019/2330 train_time:40883ms step_avg:40.12ms
step:1020/2330 train_time:40941ms step_avg:40.14ms
step:1021/2330 train_time:40963ms step_avg:40.12ms
step:1022/2330 train_time:41020ms step_avg:40.14ms
step:1023/2330 train_time:41043ms step_avg:40.12ms
step:1024/2330 train_time:41099ms step_avg:40.14ms
step:1025/2330 train_time:41123ms step_avg:40.12ms
step:1026/2330 train_time:41182ms step_avg:40.14ms
step:1027/2330 train_time:41205ms step_avg:40.12ms
step:1028/2330 train_time:41262ms step_avg:40.14ms
step:1029/2330 train_time:41285ms step_avg:40.12ms
step:1030/2330 train_time:41343ms step_avg:40.14ms
step:1031/2330 train_time:41366ms step_avg:40.12ms
step:1032/2330 train_time:41422ms step_avg:40.14ms
step:1033/2330 train_time:41445ms step_avg:40.12ms
step:1034/2330 train_time:41506ms step_avg:40.14ms
step:1035/2330 train_time:41531ms step_avg:40.13ms
step:1036/2330 train_time:41590ms step_avg:40.14ms
step:1037/2330 train_time:41615ms step_avg:40.13ms
step:1038/2330 train_time:41672ms step_avg:40.15ms
step:1039/2330 train_time:41695ms step_avg:40.13ms
step:1040/2330 train_time:41753ms step_avg:40.15ms
step:1041/2330 train_time:41776ms step_avg:40.13ms
step:1042/2330 train_time:41833ms step_avg:40.15ms
step:1043/2330 train_time:41856ms step_avg:40.13ms
step:1044/2330 train_time:41913ms step_avg:40.15ms
step:1045/2330 train_time:41935ms step_avg:40.13ms
step:1046/2330 train_time:41993ms step_avg:40.15ms
step:1047/2330 train_time:42015ms step_avg:40.13ms
step:1048/2330 train_time:42073ms step_avg:40.15ms
step:1049/2330 train_time:42096ms step_avg:40.13ms
step:1050/2330 train_time:42154ms step_avg:40.15ms
step:1051/2330 train_time:42177ms step_avg:40.13ms
step:1052/2330 train_time:42234ms step_avg:40.15ms
step:1053/2330 train_time:42257ms step_avg:40.13ms
step:1054/2330 train_time:42314ms step_avg:40.15ms
step:1055/2330 train_time:42337ms step_avg:40.13ms
step:1056/2330 train_time:42395ms step_avg:40.15ms
step:1057/2330 train_time:42418ms step_avg:40.13ms
step:1058/2330 train_time:42477ms step_avg:40.15ms
step:1059/2330 train_time:42500ms step_avg:40.13ms
step:1060/2330 train_time:42559ms step_avg:40.15ms
step:1061/2330 train_time:42583ms step_avg:40.13ms
step:1062/2330 train_time:42642ms step_avg:40.15ms
step:1063/2330 train_time:42666ms step_avg:40.14ms
step:1064/2330 train_time:42724ms step_avg:40.15ms
step:1065/2330 train_time:42748ms step_avg:40.14ms
step:1066/2330 train_time:42805ms step_avg:40.16ms
step:1067/2330 train_time:42828ms step_avg:40.14ms
step:1068/2330 train_time:42885ms step_avg:40.15ms
step:1069/2330 train_time:42909ms step_avg:40.14ms
step:1070/2330 train_time:42966ms step_avg:40.15ms
step:1071/2330 train_time:42990ms step_avg:40.14ms
step:1072/2330 train_time:43046ms step_avg:40.15ms
step:1073/2330 train_time:43070ms step_avg:40.14ms
step:1074/2330 train_time:43127ms step_avg:40.16ms
step:1075/2330 train_time:43151ms step_avg:40.14ms
step:1076/2330 train_time:43208ms step_avg:40.16ms
step:1077/2330 train_time:43231ms step_avg:40.14ms
step:1078/2330 train_time:43288ms step_avg:40.16ms
step:1079/2330 train_time:43311ms step_avg:40.14ms
step:1080/2330 train_time:43368ms step_avg:40.16ms
step:1081/2330 train_time:43391ms step_avg:40.14ms
step:1082/2330 train_time:43448ms step_avg:40.16ms
step:1083/2330 train_time:43472ms step_avg:40.14ms
step:1084/2330 train_time:43529ms step_avg:40.16ms
step:1085/2330 train_time:43552ms step_avg:40.14ms
step:1086/2330 train_time:43608ms step_avg:40.16ms
step:1087/2330 train_time:43632ms step_avg:40.14ms
step:1088/2330 train_time:43690ms step_avg:40.16ms
step:1089/2330 train_time:43714ms step_avg:40.14ms
step:1090/2330 train_time:43771ms step_avg:40.16ms
step:1091/2330 train_time:43794ms step_avg:40.14ms
step:1092/2330 train_time:43852ms step_avg:40.16ms
step:1093/2330 train_time:43875ms step_avg:40.14ms
step:1094/2330 train_time:43932ms step_avg:40.16ms
step:1095/2330 train_time:43956ms step_avg:40.14ms
step:1096/2330 train_time:44014ms step_avg:40.16ms
step:1097/2330 train_time:44037ms step_avg:40.14ms
step:1098/2330 train_time:44094ms step_avg:40.16ms
step:1099/2330 train_time:44116ms step_avg:40.14ms
step:1100/2330 train_time:44174ms step_avg:40.16ms
step:1101/2330 train_time:44196ms step_avg:40.14ms
step:1102/2330 train_time:44254ms step_avg:40.16ms
step:1103/2330 train_time:44277ms step_avg:40.14ms
step:1104/2330 train_time:44335ms step_avg:40.16ms
step:1105/2330 train_time:44357ms step_avg:40.14ms
step:1106/2330 train_time:44415ms step_avg:40.16ms
step:1107/2330 train_time:44438ms step_avg:40.14ms
step:1108/2330 train_time:44496ms step_avg:40.16ms
step:1109/2330 train_time:44519ms step_avg:40.14ms
step:1110/2330 train_time:44577ms step_avg:40.16ms
step:1111/2330 train_time:44601ms step_avg:40.14ms
step:1112/2330 train_time:44659ms step_avg:40.16ms
step:1113/2330 train_time:44683ms step_avg:40.15ms
step:1114/2330 train_time:44741ms step_avg:40.16ms
step:1115/2330 train_time:44765ms step_avg:40.15ms
step:1116/2330 train_time:44824ms step_avg:40.16ms
step:1117/2330 train_time:44847ms step_avg:40.15ms
step:1118/2330 train_time:44905ms step_avg:40.17ms
step:1119/2330 train_time:44928ms step_avg:40.15ms
step:1120/2330 train_time:44985ms step_avg:40.16ms
step:1121/2330 train_time:45008ms step_avg:40.15ms
step:1122/2330 train_time:45066ms step_avg:40.17ms
step:1123/2330 train_time:45089ms step_avg:40.15ms
step:1124/2330 train_time:45146ms step_avg:40.17ms
step:1125/2330 train_time:45169ms step_avg:40.15ms
step:1126/2330 train_time:45226ms step_avg:40.17ms
step:1127/2330 train_time:45250ms step_avg:40.15ms
step:1128/2330 train_time:45307ms step_avg:40.17ms
step:1129/2330 train_time:45330ms step_avg:40.15ms
step:1130/2330 train_time:45386ms step_avg:40.16ms
step:1131/2330 train_time:45410ms step_avg:40.15ms
step:1132/2330 train_time:45467ms step_avg:40.17ms
step:1133/2330 train_time:45491ms step_avg:40.15ms
step:1134/2330 train_time:45548ms step_avg:40.17ms
step:1135/2330 train_time:45572ms step_avg:40.15ms
step:1136/2330 train_time:45629ms step_avg:40.17ms
step:1137/2330 train_time:45653ms step_avg:40.15ms
step:1138/2330 train_time:45710ms step_avg:40.17ms
step:1139/2330 train_time:45735ms step_avg:40.15ms
step:1140/2330 train_time:45793ms step_avg:40.17ms
step:1141/2330 train_time:45817ms step_avg:40.15ms
step:1142/2330 train_time:45875ms step_avg:40.17ms
step:1143/2330 train_time:45897ms step_avg:40.15ms
step:1144/2330 train_time:45954ms step_avg:40.17ms
step:1145/2330 train_time:45977ms step_avg:40.15ms
step:1146/2330 train_time:46036ms step_avg:40.17ms
step:1147/2330 train_time:46058ms step_avg:40.16ms
step:1148/2330 train_time:46116ms step_avg:40.17ms
step:1149/2330 train_time:46139ms step_avg:40.16ms
step:1150/2330 train_time:46197ms step_avg:40.17ms
step:1151/2330 train_time:46220ms step_avg:40.16ms
step:1152/2330 train_time:46279ms step_avg:40.17ms
step:1153/2330 train_time:46301ms step_avg:40.16ms
step:1154/2330 train_time:46359ms step_avg:40.17ms
step:1155/2330 train_time:46383ms step_avg:40.16ms
step:1156/2330 train_time:46441ms step_avg:40.17ms
step:1157/2330 train_time:46464ms step_avg:40.16ms
step:1158/2330 train_time:46522ms step_avg:40.17ms
step:1159/2330 train_time:46546ms step_avg:40.16ms
step:1160/2330 train_time:46603ms step_avg:40.17ms
step:1161/2330 train_time:46627ms step_avg:40.16ms
step:1162/2330 train_time:46684ms step_avg:40.18ms
step:1163/2330 train_time:46708ms step_avg:40.16ms
step:1164/2330 train_time:46765ms step_avg:40.18ms
step:1165/2330 train_time:46789ms step_avg:40.16ms
step:1166/2330 train_time:46846ms step_avg:40.18ms
step:1167/2330 train_time:46869ms step_avg:40.16ms
step:1168/2330 train_time:46927ms step_avg:40.18ms
step:1169/2330 train_time:46950ms step_avg:40.16ms
step:1170/2330 train_time:47007ms step_avg:40.18ms
step:1171/2330 train_time:47031ms step_avg:40.16ms
step:1172/2330 train_time:47087ms step_avg:40.18ms
step:1173/2330 train_time:47111ms step_avg:40.16ms
step:1174/2330 train_time:47168ms step_avg:40.18ms
step:1175/2330 train_time:47192ms step_avg:40.16ms
step:1176/2330 train_time:47248ms step_avg:40.18ms
step:1177/2330 train_time:47272ms step_avg:40.16ms
step:1178/2330 train_time:47328ms step_avg:40.18ms
step:1179/2330 train_time:47352ms step_avg:40.16ms
step:1180/2330 train_time:47409ms step_avg:40.18ms
step:1181/2330 train_time:47432ms step_avg:40.16ms
step:1182/2330 train_time:47489ms step_avg:40.18ms
step:1183/2330 train_time:47513ms step_avg:40.16ms
step:1184/2330 train_time:47570ms step_avg:40.18ms
step:1185/2330 train_time:47594ms step_avg:40.16ms
step:1186/2330 train_time:47651ms step_avg:40.18ms
step:1187/2330 train_time:47673ms step_avg:40.16ms
step:1188/2330 train_time:47732ms step_avg:40.18ms
step:1189/2330 train_time:47754ms step_avg:40.16ms
step:1190/2330 train_time:47812ms step_avg:40.18ms
step:1191/2330 train_time:47835ms step_avg:40.16ms
step:1192/2330 train_time:47893ms step_avg:40.18ms
step:1193/2330 train_time:47916ms step_avg:40.16ms
step:1194/2330 train_time:47973ms step_avg:40.18ms
step:1195/2330 train_time:47996ms step_avg:40.16ms
step:1196/2330 train_time:48052ms step_avg:40.18ms
step:1197/2330 train_time:48075ms step_avg:40.16ms
step:1198/2330 train_time:48133ms step_avg:40.18ms
step:1199/2330 train_time:48155ms step_avg:40.16ms
step:1200/2330 train_time:48213ms step_avg:40.18ms
step:1201/2330 train_time:48236ms step_avg:40.16ms
step:1202/2330 train_time:48293ms step_avg:40.18ms
step:1203/2330 train_time:48315ms step_avg:40.16ms
step:1204/2330 train_time:48373ms step_avg:40.18ms
step:1205/2330 train_time:48396ms step_avg:40.16ms
step:1206/2330 train_time:48453ms step_avg:40.18ms
step:1207/2330 train_time:48476ms step_avg:40.16ms
step:1208/2330 train_time:48534ms step_avg:40.18ms
step:1209/2330 train_time:48556ms step_avg:40.16ms
step:1210/2330 train_time:48614ms step_avg:40.18ms
step:1211/2330 train_time:48637ms step_avg:40.16ms
step:1212/2330 train_time:48695ms step_avg:40.18ms
step:1213/2330 train_time:48718ms step_avg:40.16ms
step:1214/2330 train_time:48777ms step_avg:40.18ms
step:1215/2330 train_time:48799ms step_avg:40.16ms
step:1216/2330 train_time:48856ms step_avg:40.18ms
step:1217/2330 train_time:48879ms step_avg:40.16ms
step:1218/2330 train_time:48937ms step_avg:40.18ms
step:1219/2330 train_time:48960ms step_avg:40.16ms
step:1220/2330 train_time:49018ms step_avg:40.18ms
step:1221/2330 train_time:49041ms step_avg:40.16ms
step:1222/2330 train_time:49098ms step_avg:40.18ms
step:1223/2330 train_time:49121ms step_avg:40.16ms
step:1224/2330 train_time:49179ms step_avg:40.18ms
step:1225/2330 train_time:49201ms step_avg:40.16ms
step:1226/2330 train_time:49259ms step_avg:40.18ms
step:1227/2330 train_time:49282ms step_avg:40.16ms
step:1228/2330 train_time:49340ms step_avg:40.18ms
step:1229/2330 train_time:49364ms step_avg:40.17ms
step:1230/2330 train_time:49422ms step_avg:40.18ms
step:1231/2330 train_time:49445ms step_avg:40.17ms
step:1232/2330 train_time:49503ms step_avg:40.18ms
step:1233/2330 train_time:49526ms step_avg:40.17ms
step:1234/2330 train_time:49584ms step_avg:40.18ms
step:1235/2330 train_time:49608ms step_avg:40.17ms
step:1236/2330 train_time:49665ms step_avg:40.18ms
step:1237/2330 train_time:49689ms step_avg:40.17ms
step:1238/2330 train_time:49746ms step_avg:40.18ms
step:1239/2330 train_time:49770ms step_avg:40.17ms
step:1240/2330 train_time:49827ms step_avg:40.18ms
step:1241/2330 train_time:49851ms step_avg:40.17ms
step:1242/2330 train_time:49908ms step_avg:40.18ms
step:1243/2330 train_time:49932ms step_avg:40.17ms
step:1244/2330 train_time:49989ms step_avg:40.18ms
step:1245/2330 train_time:50013ms step_avg:40.17ms
step:1246/2330 train_time:50070ms step_avg:40.18ms
step:1247/2330 train_time:50093ms step_avg:40.17ms
step:1248/2330 train_time:50151ms step_avg:40.19ms
step:1249/2330 train_time:50174ms step_avg:40.17ms
step:1250/2330 train_time:50231ms step_avg:40.18ms
step:1250/2330 val_loss:5.4501 train_time:50329ms step_avg:40.26ms
step:1251/2330 train_time:50342ms step_avg:40.24ms
step:1252/2330 train_time:50353ms step_avg:40.22ms
step:1253/2330 train_time:50363ms step_avg:40.19ms
step:1254/2330 train_time:50394ms step_avg:40.19ms
step:1255/2330 train_time:50415ms step_avg:40.17ms
step:1256/2330 train_time:50471ms step_avg:40.18ms
step:1257/2330 train_time:50494ms step_avg:40.17ms
step:1258/2330 train_time:50549ms step_avg:40.18ms
step:1259/2330 train_time:50572ms step_avg:40.17ms
step:1260/2330 train_time:50629ms step_avg:40.18ms
step:1261/2330 train_time:50656ms step_avg:40.17ms
step:1262/2330 train_time:50717ms step_avg:40.19ms
step:1263/2330 train_time:50741ms step_avg:40.18ms
step:1264/2330 train_time:50799ms step_avg:40.19ms
step:1265/2330 train_time:50823ms step_avg:40.18ms
step:1266/2330 train_time:50880ms step_avg:40.19ms
step:1267/2330 train_time:50904ms step_avg:40.18ms
step:1268/2330 train_time:50962ms step_avg:40.19ms
step:1269/2330 train_time:50984ms step_avg:40.18ms
step:1270/2330 train_time:51042ms step_avg:40.19ms
step:1271/2330 train_time:51065ms step_avg:40.18ms
step:1272/2330 train_time:51122ms step_avg:40.19ms
step:1273/2330 train_time:51144ms step_avg:40.18ms
step:1274/2330 train_time:51202ms step_avg:40.19ms
step:1275/2330 train_time:51225ms step_avg:40.18ms
step:1276/2330 train_time:51285ms step_avg:40.19ms
step:1277/2330 train_time:51308ms step_avg:40.18ms
step:1278/2330 train_time:51366ms step_avg:40.19ms
step:1279/2330 train_time:51389ms step_avg:40.18ms
step:1280/2330 train_time:51446ms step_avg:40.19ms
step:1281/2330 train_time:51469ms step_avg:40.18ms
step:1282/2330 train_time:51526ms step_avg:40.19ms
step:1283/2330 train_time:51549ms step_avg:40.18ms
step:1284/2330 train_time:51608ms step_avg:40.19ms
step:1285/2330 train_time:51633ms step_avg:40.18ms
step:1286/2330 train_time:51691ms step_avg:40.20ms
step:1287/2330 train_time:51716ms step_avg:40.18ms
step:1288/2330 train_time:51773ms step_avg:40.20ms
step:1289/2330 train_time:51797ms step_avg:40.18ms
step:1290/2330 train_time:51855ms step_avg:40.20ms
step:1291/2330 train_time:51878ms step_avg:40.18ms
step:1292/2330 train_time:51935ms step_avg:40.20ms
step:1293/2330 train_time:51957ms step_avg:40.18ms
step:1294/2330 train_time:52014ms step_avg:40.20ms
step:1295/2330 train_time:52036ms step_avg:40.18ms
step:1296/2330 train_time:52094ms step_avg:40.20ms
step:1297/2330 train_time:52116ms step_avg:40.18ms
step:1298/2330 train_time:52173ms step_avg:40.20ms
step:1299/2330 train_time:52196ms step_avg:40.18ms
step:1300/2330 train_time:52253ms step_avg:40.19ms
step:1301/2330 train_time:52277ms step_avg:40.18ms
step:1302/2330 train_time:52334ms step_avg:40.20ms
step:1303/2330 train_time:52357ms step_avg:40.18ms
step:1304/2330 train_time:52414ms step_avg:40.20ms
step:1305/2330 train_time:52437ms step_avg:40.18ms
step:1306/2330 train_time:52496ms step_avg:40.20ms
step:1307/2330 train_time:52519ms step_avg:40.18ms
step:1308/2330 train_time:52577ms step_avg:40.20ms
step:1309/2330 train_time:52600ms step_avg:40.18ms
step:1310/2330 train_time:52657ms step_avg:40.20ms
step:1311/2330 train_time:52683ms step_avg:40.19ms
step:1312/2330 train_time:52741ms step_avg:40.20ms
step:1313/2330 train_time:52765ms step_avg:40.19ms
step:1314/2330 train_time:52823ms step_avg:40.20ms
step:1315/2330 train_time:52847ms step_avg:40.19ms
step:1316/2330 train_time:52905ms step_avg:40.20ms
step:1317/2330 train_time:52929ms step_avg:40.19ms
step:1318/2330 train_time:52986ms step_avg:40.20ms
step:1319/2330 train_time:53010ms step_avg:40.19ms
step:1320/2330 train_time:53067ms step_avg:40.20ms
step:1321/2330 train_time:53091ms step_avg:40.19ms
step:1322/2330 train_time:53147ms step_avg:40.20ms
step:1323/2330 train_time:53172ms step_avg:40.19ms
step:1324/2330 train_time:53228ms step_avg:40.20ms
step:1325/2330 train_time:53252ms step_avg:40.19ms
step:1326/2330 train_time:53309ms step_avg:40.20ms
step:1327/2330 train_time:53332ms step_avg:40.19ms
step:1328/2330 train_time:53389ms step_avg:40.20ms
step:1329/2330 train_time:53413ms step_avg:40.19ms
step:1330/2330 train_time:53470ms step_avg:40.20ms
step:1331/2330 train_time:53493ms step_avg:40.19ms
step:1332/2330 train_time:53550ms step_avg:40.20ms
step:1333/2330 train_time:53574ms step_avg:40.19ms
step:1334/2330 train_time:53631ms step_avg:40.20ms
step:1335/2330 train_time:53656ms step_avg:40.19ms
step:1336/2330 train_time:53713ms step_avg:40.20ms
step:1337/2330 train_time:53737ms step_avg:40.19ms
step:1338/2330 train_time:53795ms step_avg:40.21ms
step:1339/2330 train_time:53818ms step_avg:40.19ms
step:1340/2330 train_time:53875ms step_avg:40.20ms
step:1341/2330 train_time:53898ms step_avg:40.19ms
step:1342/2330 train_time:53956ms step_avg:40.21ms
step:1343/2330 train_time:53979ms step_avg:40.19ms
step:1344/2330 train_time:54037ms step_avg:40.21ms
step:1345/2330 train_time:54059ms step_avg:40.19ms
step:1346/2330 train_time:54116ms step_avg:40.21ms
step:1347/2330 train_time:54138ms step_avg:40.19ms
step:1348/2330 train_time:54196ms step_avg:40.20ms
step:1349/2330 train_time:54219ms step_avg:40.19ms
step:1350/2330 train_time:54277ms step_avg:40.20ms
step:1351/2330 train_time:54300ms step_avg:40.19ms
step:1352/2330 train_time:54358ms step_avg:40.21ms
step:1353/2330 train_time:54381ms step_avg:40.19ms
step:1354/2330 train_time:54439ms step_avg:40.21ms
step:1355/2330 train_time:54461ms step_avg:40.19ms
step:1356/2330 train_time:54520ms step_avg:40.21ms
step:1357/2330 train_time:54544ms step_avg:40.19ms
step:1358/2330 train_time:54602ms step_avg:40.21ms
step:1359/2330 train_time:54625ms step_avg:40.20ms
step:1360/2330 train_time:54684ms step_avg:40.21ms
step:1361/2330 train_time:54708ms step_avg:40.20ms
step:1362/2330 train_time:54766ms step_avg:40.21ms
step:1363/2330 train_time:54789ms step_avg:40.20ms
step:1364/2330 train_time:54846ms step_avg:40.21ms
step:1365/2330 train_time:54870ms step_avg:40.20ms
step:1366/2330 train_time:54927ms step_avg:40.21ms
step:1367/2330 train_time:54950ms step_avg:40.20ms
step:1368/2330 train_time:55007ms step_avg:40.21ms
step:1369/2330 train_time:55031ms step_avg:40.20ms
step:1370/2330 train_time:55088ms step_avg:40.21ms
step:1371/2330 train_time:55111ms step_avg:40.20ms
step:1372/2330 train_time:55168ms step_avg:40.21ms
step:1373/2330 train_time:55192ms step_avg:40.20ms
step:1374/2330 train_time:55249ms step_avg:40.21ms
step:1375/2330 train_time:55273ms step_avg:40.20ms
step:1376/2330 train_time:55330ms step_avg:40.21ms
step:1377/2330 train_time:55354ms step_avg:40.20ms
step:1378/2330 train_time:55410ms step_avg:40.21ms
step:1379/2330 train_time:55435ms step_avg:40.20ms
step:1380/2330 train_time:55492ms step_avg:40.21ms
step:1381/2330 train_time:55515ms step_avg:40.20ms
step:1382/2330 train_time:55572ms step_avg:40.21ms
step:1383/2330 train_time:55595ms step_avg:40.20ms
step:1384/2330 train_time:55652ms step_avg:40.21ms
step:1385/2330 train_time:55675ms step_avg:40.20ms
step:1386/2330 train_time:55733ms step_avg:40.21ms
step:1387/2330 train_time:55757ms step_avg:40.20ms
step:1388/2330 train_time:55815ms step_avg:40.21ms
step:1389/2330 train_time:55838ms step_avg:40.20ms
step:1390/2330 train_time:55895ms step_avg:40.21ms
step:1391/2330 train_time:55918ms step_avg:40.20ms
step:1392/2330 train_time:55976ms step_avg:40.21ms
step:1393/2330 train_time:55999ms step_avg:40.20ms
step:1394/2330 train_time:56057ms step_avg:40.21ms
step:1395/2330 train_time:56079ms step_avg:40.20ms
step:1396/2330 train_time:56137ms step_avg:40.21ms
step:1397/2330 train_time:56160ms step_avg:40.20ms
step:1398/2330 train_time:56218ms step_avg:40.21ms
step:1399/2330 train_time:56242ms step_avg:40.20ms
step:1400/2330 train_time:56299ms step_avg:40.21ms
step:1401/2330 train_time:56323ms step_avg:40.20ms
step:1402/2330 train_time:56381ms step_avg:40.21ms
step:1403/2330 train_time:56404ms step_avg:40.20ms
step:1404/2330 train_time:56462ms step_avg:40.21ms
step:1405/2330 train_time:56485ms step_avg:40.20ms
step:1406/2330 train_time:56543ms step_avg:40.22ms
step:1407/2330 train_time:56566ms step_avg:40.20ms
step:1408/2330 train_time:56623ms step_avg:40.22ms
step:1409/2330 train_time:56648ms step_avg:40.20ms
step:1410/2330 train_time:56706ms step_avg:40.22ms
step:1411/2330 train_time:56730ms step_avg:40.21ms
step:1412/2330 train_time:56787ms step_avg:40.22ms
step:1413/2330 train_time:56811ms step_avg:40.21ms
step:1414/2330 train_time:56869ms step_avg:40.22ms
step:1415/2330 train_time:56893ms step_avg:40.21ms
step:1416/2330 train_time:56949ms step_avg:40.22ms
step:1417/2330 train_time:56973ms step_avg:40.21ms
step:1418/2330 train_time:57029ms step_avg:40.22ms
step:1419/2330 train_time:57053ms step_avg:40.21ms
step:1420/2330 train_time:57111ms step_avg:40.22ms
step:1421/2330 train_time:57135ms step_avg:40.21ms
step:1422/2330 train_time:57192ms step_avg:40.22ms
step:1423/2330 train_time:57215ms step_avg:40.21ms
step:1424/2330 train_time:57272ms step_avg:40.22ms
step:1425/2330 train_time:57296ms step_avg:40.21ms
step:1426/2330 train_time:57353ms step_avg:40.22ms
step:1427/2330 train_time:57377ms step_avg:40.21ms
step:1428/2330 train_time:57434ms step_avg:40.22ms
step:1429/2330 train_time:57458ms step_avg:40.21ms
step:1430/2330 train_time:57516ms step_avg:40.22ms
step:1431/2330 train_time:57539ms step_avg:40.21ms
step:1432/2330 train_time:57596ms step_avg:40.22ms
step:1433/2330 train_time:57619ms step_avg:40.21ms
step:1434/2330 train_time:57677ms step_avg:40.22ms
step:1435/2330 train_time:57700ms step_avg:40.21ms
step:1436/2330 train_time:57758ms step_avg:40.22ms
step:1437/2330 train_time:57780ms step_avg:40.21ms
step:1438/2330 train_time:57838ms step_avg:40.22ms
step:1439/2330 train_time:57860ms step_avg:40.21ms
step:1440/2330 train_time:57919ms step_avg:40.22ms
step:1441/2330 train_time:57942ms step_avg:40.21ms
step:1442/2330 train_time:58000ms step_avg:40.22ms
step:1443/2330 train_time:58024ms step_avg:40.21ms
step:1444/2330 train_time:58082ms step_avg:40.22ms
step:1445/2330 train_time:58106ms step_avg:40.21ms
step:1446/2330 train_time:58164ms step_avg:40.22ms
step:1447/2330 train_time:58188ms step_avg:40.21ms
step:1448/2330 train_time:58245ms step_avg:40.22ms
step:1449/2330 train_time:58269ms step_avg:40.21ms
step:1450/2330 train_time:58326ms step_avg:40.22ms
step:1451/2330 train_time:58350ms step_avg:40.21ms
step:1452/2330 train_time:58407ms step_avg:40.23ms
step:1453/2330 train_time:58431ms step_avg:40.21ms
step:1454/2330 train_time:58488ms step_avg:40.23ms
step:1455/2330 train_time:58512ms step_avg:40.21ms
step:1456/2330 train_time:58569ms step_avg:40.23ms
step:1457/2330 train_time:58593ms step_avg:40.21ms
step:1458/2330 train_time:58650ms step_avg:40.23ms
step:1459/2330 train_time:58674ms step_avg:40.22ms
step:1460/2330 train_time:58730ms step_avg:40.23ms
step:1461/2330 train_time:58754ms step_avg:40.21ms
step:1462/2330 train_time:58810ms step_avg:40.23ms
step:1463/2330 train_time:58834ms step_avg:40.21ms
step:1464/2330 train_time:58891ms step_avg:40.23ms
step:1465/2330 train_time:58914ms step_avg:40.21ms
step:1466/2330 train_time:58972ms step_avg:40.23ms
step:1467/2330 train_time:58996ms step_avg:40.22ms
step:1468/2330 train_time:59054ms step_avg:40.23ms
step:1469/2330 train_time:59078ms step_avg:40.22ms
step:1470/2330 train_time:59136ms step_avg:40.23ms
step:1471/2330 train_time:59158ms step_avg:40.22ms
step:1472/2330 train_time:59216ms step_avg:40.23ms
step:1473/2330 train_time:59238ms step_avg:40.22ms
step:1474/2330 train_time:59296ms step_avg:40.23ms
step:1475/2330 train_time:59319ms step_avg:40.22ms
step:1476/2330 train_time:59377ms step_avg:40.23ms
step:1477/2330 train_time:59399ms step_avg:40.22ms
step:1478/2330 train_time:59457ms step_avg:40.23ms
step:1479/2330 train_time:59480ms step_avg:40.22ms
step:1480/2330 train_time:59538ms step_avg:40.23ms
step:1481/2330 train_time:59561ms step_avg:40.22ms
step:1482/2330 train_time:59619ms step_avg:40.23ms
step:1483/2330 train_time:59642ms step_avg:40.22ms
step:1484/2330 train_time:59700ms step_avg:40.23ms
step:1485/2330 train_time:59722ms step_avg:40.22ms
step:1486/2330 train_time:59780ms step_avg:40.23ms
step:1487/2330 train_time:59803ms step_avg:40.22ms
step:1488/2330 train_time:59862ms step_avg:40.23ms
step:1489/2330 train_time:59885ms step_avg:40.22ms
step:1490/2330 train_time:59942ms step_avg:40.23ms
step:1491/2330 train_time:59965ms step_avg:40.22ms
step:1492/2330 train_time:60022ms step_avg:40.23ms
step:1493/2330 train_time:60046ms step_avg:40.22ms
step:1494/2330 train_time:60104ms step_avg:40.23ms
step:1495/2330 train_time:60128ms step_avg:40.22ms
step:1496/2330 train_time:60186ms step_avg:40.23ms
step:1497/2330 train_time:60210ms step_avg:40.22ms
step:1498/2330 train_time:60267ms step_avg:40.23ms
step:1499/2330 train_time:60291ms step_avg:40.22ms
step:1500/2330 train_time:60348ms step_avg:40.23ms
step:1500/2330 val_loss:5.3822 train_time:60446ms step_avg:40.30ms
step:1501/2330 train_time:60459ms step_avg:40.28ms
step:1502/2330 train_time:60471ms step_avg:40.26ms
step:1503/2330 train_time:60482ms step_avg:40.24ms
step:1504/2330 train_time:60509ms step_avg:40.23ms
step:1505/2330 train_time:60531ms step_avg:40.22ms
step:1506/2330 train_time:60586ms step_avg:40.23ms
step:1507/2330 train_time:60609ms step_avg:40.22ms
step:1508/2330 train_time:60665ms step_avg:40.23ms
step:1509/2330 train_time:60687ms step_avg:40.22ms
step:1510/2330 train_time:60746ms step_avg:40.23ms
step:1511/2330 train_time:60774ms step_avg:40.22ms
step:1512/2330 train_time:60833ms step_avg:40.23ms
step:1513/2330 train_time:60857ms step_avg:40.22ms
step:1514/2330 train_time:60915ms step_avg:40.23ms
step:1515/2330 train_time:60938ms step_avg:40.22ms
step:1516/2330 train_time:60996ms step_avg:40.23ms
step:1517/2330 train_time:61018ms step_avg:40.22ms
step:1518/2330 train_time:61076ms step_avg:40.23ms
step:1519/2330 train_time:61099ms step_avg:40.22ms
step:1520/2330 train_time:61156ms step_avg:40.23ms
step:1521/2330 train_time:61179ms step_avg:40.22ms
step:1522/2330 train_time:61235ms step_avg:40.23ms
step:1523/2330 train_time:61258ms step_avg:40.22ms
step:1524/2330 train_time:61315ms step_avg:40.23ms
step:1525/2330 train_time:61338ms step_avg:40.22ms
step:1526/2330 train_time:61398ms step_avg:40.23ms
step:1527/2330 train_time:61421ms step_avg:40.22ms
step:1528/2330 train_time:61480ms step_avg:40.24ms
step:1529/2330 train_time:61504ms step_avg:40.22ms
step:1530/2330 train_time:61561ms step_avg:40.24ms
step:1531/2330 train_time:61582ms step_avg:40.22ms
step:1532/2330 train_time:61639ms step_avg:40.23ms
step:1533/2330 train_time:61664ms step_avg:40.22ms
step:1534/2330 train_time:61723ms step_avg:40.24ms
step:1535/2330 train_time:61748ms step_avg:40.23ms
step:1536/2330 train_time:61807ms step_avg:40.24ms
step:1537/2330 train_time:61830ms step_avg:40.23ms
step:1538/2330 train_time:61887ms step_avg:40.24ms
step:1539/2330 train_time:61910ms step_avg:40.23ms
step:1540/2330 train_time:61967ms step_avg:40.24ms
step:1541/2330 train_time:61991ms step_avg:40.23ms
step:1542/2330 train_time:62048ms step_avg:40.24ms
step:1543/2330 train_time:62072ms step_avg:40.23ms
step:1544/2330 train_time:62128ms step_avg:40.24ms
step:1545/2330 train_time:62151ms step_avg:40.23ms
step:1546/2330 train_time:62208ms step_avg:40.24ms
step:1547/2330 train_time:62232ms step_avg:40.23ms
step:1548/2330 train_time:62289ms step_avg:40.24ms
step:1549/2330 train_time:62313ms step_avg:40.23ms
step:1550/2330 train_time:62371ms step_avg:40.24ms
step:1551/2330 train_time:62396ms step_avg:40.23ms
step:1552/2330 train_time:62454ms step_avg:40.24ms
step:1553/2330 train_time:62476ms step_avg:40.23ms
step:1554/2330 train_time:62533ms step_avg:40.24ms
step:1555/2330 train_time:62556ms step_avg:40.23ms
step:1556/2330 train_time:62614ms step_avg:40.24ms
step:1557/2330 train_time:62637ms step_avg:40.23ms
step:1558/2330 train_time:62697ms step_avg:40.24ms
step:1559/2330 train_time:62719ms step_avg:40.23ms
step:1560/2330 train_time:62778ms step_avg:40.24ms
step:1561/2330 train_time:62802ms step_avg:40.23ms
step:1562/2330 train_time:62860ms step_avg:40.24ms
step:1563/2330 train_time:62884ms step_avg:40.23ms
step:1564/2330 train_time:62941ms step_avg:40.24ms
step:1565/2330 train_time:62965ms step_avg:40.23ms
step:1566/2330 train_time:63023ms step_avg:40.24ms
step:1567/2330 train_time:63047ms step_avg:40.23ms
step:1568/2330 train_time:63104ms step_avg:40.24ms
step:1569/2330 train_time:63127ms step_avg:40.23ms
step:1570/2330 train_time:63184ms step_avg:40.24ms
step:1571/2330 train_time:63207ms step_avg:40.23ms
step:1572/2330 train_time:63264ms step_avg:40.24ms
step:1573/2330 train_time:63288ms step_avg:40.23ms
step:1574/2330 train_time:63346ms step_avg:40.25ms
step:1575/2330 train_time:63370ms step_avg:40.23ms
step:1576/2330 train_time:63427ms step_avg:40.25ms
step:1577/2330 train_time:63450ms step_avg:40.23ms
step:1578/2330 train_time:63507ms step_avg:40.25ms
step:1579/2330 train_time:63531ms step_avg:40.23ms
step:1580/2330 train_time:63588ms step_avg:40.25ms
step:1581/2330 train_time:63612ms step_avg:40.24ms
step:1582/2330 train_time:63668ms step_avg:40.25ms
step:1583/2330 train_time:63692ms step_avg:40.23ms
step:1584/2330 train_time:63749ms step_avg:40.25ms
step:1585/2330 train_time:63773ms step_avg:40.24ms
step:1586/2330 train_time:63830ms step_avg:40.25ms
step:1587/2330 train_time:63853ms step_avg:40.24ms
step:1588/2330 train_time:63910ms step_avg:40.25ms
step:1589/2330 train_time:63933ms step_avg:40.23ms
step:1590/2330 train_time:63991ms step_avg:40.25ms
step:1591/2330 train_time:64015ms step_avg:40.24ms
step:1592/2330 train_time:64072ms step_avg:40.25ms
step:1593/2330 train_time:64095ms step_avg:40.24ms
step:1594/2330 train_time:64153ms step_avg:40.25ms
step:1595/2330 train_time:64176ms step_avg:40.24ms
step:1596/2330 train_time:64234ms step_avg:40.25ms
step:1597/2330 train_time:64257ms step_avg:40.24ms
step:1598/2330 train_time:64315ms step_avg:40.25ms
step:1599/2330 train_time:64337ms step_avg:40.24ms
step:1600/2330 train_time:64394ms step_avg:40.25ms
step:1601/2330 train_time:64417ms step_avg:40.24ms
step:1602/2330 train_time:64475ms step_avg:40.25ms
step:1603/2330 train_time:64498ms step_avg:40.24ms
step:1604/2330 train_time:64556ms step_avg:40.25ms
step:1605/2330 train_time:64578ms step_avg:40.24ms
step:1606/2330 train_time:64636ms step_avg:40.25ms
step:1607/2330 train_time:64659ms step_avg:40.24ms
step:1608/2330 train_time:64716ms step_avg:40.25ms
step:1609/2330 train_time:64739ms step_avg:40.24ms
step:1610/2330 train_time:64797ms step_avg:40.25ms
step:1611/2330 train_time:64819ms step_avg:40.24ms
step:1612/2330 train_time:64877ms step_avg:40.25ms
step:1613/2330 train_time:64900ms step_avg:40.24ms
step:1614/2330 train_time:64959ms step_avg:40.25ms
step:1615/2330 train_time:64983ms step_avg:40.24ms
step:1616/2330 train_time:65040ms step_avg:40.25ms
step:1617/2330 train_time:65064ms step_avg:40.24ms
step:1618/2330 train_time:65122ms step_avg:40.25ms
step:1619/2330 train_time:65145ms step_avg:40.24ms
step:1620/2330 train_time:65203ms step_avg:40.25ms
step:1621/2330 train_time:65227ms step_avg:40.24ms
step:1622/2330 train_time:65284ms step_avg:40.25ms
step:1623/2330 train_time:65308ms step_avg:40.24ms
step:1624/2330 train_time:65366ms step_avg:40.25ms
step:1625/2330 train_time:65389ms step_avg:40.24ms
step:1626/2330 train_time:65446ms step_avg:40.25ms
step:1627/2330 train_time:65469ms step_avg:40.24ms
step:1628/2330 train_time:65526ms step_avg:40.25ms
step:1629/2330 train_time:65550ms step_avg:40.24ms
step:1630/2330 train_time:65607ms step_avg:40.25ms
step:1631/2330 train_time:65631ms step_avg:40.24ms
step:1632/2330 train_time:65688ms step_avg:40.25ms
step:1633/2330 train_time:65712ms step_avg:40.24ms
step:1634/2330 train_time:65769ms step_avg:40.25ms
step:1635/2330 train_time:65793ms step_avg:40.24ms
step:1636/2330 train_time:65850ms step_avg:40.25ms
step:1637/2330 train_time:65873ms step_avg:40.24ms
step:1638/2330 train_time:65930ms step_avg:40.25ms
step:1639/2330 train_time:65953ms step_avg:40.24ms
step:1640/2330 train_time:66010ms step_avg:40.25ms
step:1641/2330 train_time:66034ms step_avg:40.24ms
step:1642/2330 train_time:66093ms step_avg:40.25ms
step:1643/2330 train_time:66115ms step_avg:40.24ms
step:1644/2330 train_time:66174ms step_avg:40.25ms
step:1645/2330 train_time:66197ms step_avg:40.24ms
step:1646/2330 train_time:66256ms step_avg:40.25ms
step:1647/2330 train_time:66278ms step_avg:40.24ms
step:1648/2330 train_time:66337ms step_avg:40.25ms
step:1649/2330 train_time:66359ms step_avg:40.24ms
step:1650/2330 train_time:66418ms step_avg:40.25ms
step:1651/2330 train_time:66441ms step_avg:40.24ms
step:1652/2330 train_time:66500ms step_avg:40.25ms
step:1653/2330 train_time:66523ms step_avg:40.24ms
step:1654/2330 train_time:66581ms step_avg:40.25ms
step:1655/2330 train_time:66604ms step_avg:40.24ms
step:1656/2330 train_time:66663ms step_avg:40.26ms
step:1657/2330 train_time:66686ms step_avg:40.24ms
step:1658/2330 train_time:66744ms step_avg:40.26ms
step:1659/2330 train_time:66768ms step_avg:40.25ms
step:1660/2330 train_time:66826ms step_avg:40.26ms
step:1661/2330 train_time:66849ms step_avg:40.25ms
step:1662/2330 train_time:66906ms step_avg:40.26ms
step:1663/2330 train_time:66929ms step_avg:40.25ms
step:1664/2330 train_time:66986ms step_avg:40.26ms
step:1665/2330 train_time:67010ms step_avg:40.25ms
step:1666/2330 train_time:67067ms step_avg:40.26ms
step:1667/2330 train_time:67091ms step_avg:40.25ms
step:1668/2330 train_time:67148ms step_avg:40.26ms
step:1669/2330 train_time:67171ms step_avg:40.25ms
step:1670/2330 train_time:67228ms step_avg:40.26ms
step:1671/2330 train_time:67251ms step_avg:40.25ms
step:1672/2330 train_time:67308ms step_avg:40.26ms
step:1673/2330 train_time:67331ms step_avg:40.25ms
step:1674/2330 train_time:67389ms step_avg:40.26ms
step:1675/2330 train_time:67413ms step_avg:40.25ms
step:1676/2330 train_time:67470ms step_avg:40.26ms
step:1677/2330 train_time:67494ms step_avg:40.25ms
step:1678/2330 train_time:67551ms step_avg:40.26ms
step:1679/2330 train_time:67573ms step_avg:40.25ms
step:1680/2330 train_time:67631ms step_avg:40.26ms
step:1681/2330 train_time:67654ms step_avg:40.25ms
step:1682/2330 train_time:67712ms step_avg:40.26ms
step:1683/2330 train_time:67735ms step_avg:40.25ms
step:1684/2330 train_time:67793ms step_avg:40.26ms
step:1685/2330 train_time:67815ms step_avg:40.25ms
step:1686/2330 train_time:67873ms step_avg:40.26ms
step:1687/2330 train_time:67895ms step_avg:40.25ms
step:1688/2330 train_time:67954ms step_avg:40.26ms
step:1689/2330 train_time:67976ms step_avg:40.25ms
step:1690/2330 train_time:68035ms step_avg:40.26ms
step:1691/2330 train_time:68057ms step_avg:40.25ms
step:1692/2330 train_time:68116ms step_avg:40.26ms
step:1693/2330 train_time:68138ms step_avg:40.25ms
step:1694/2330 train_time:68196ms step_avg:40.26ms
step:1695/2330 train_time:68218ms step_avg:40.25ms
step:1696/2330 train_time:68275ms step_avg:40.26ms
step:1697/2330 train_time:68299ms step_avg:40.25ms
step:1698/2330 train_time:68356ms step_avg:40.26ms
step:1699/2330 train_time:68379ms step_avg:40.25ms
step:1700/2330 train_time:68436ms step_avg:40.26ms
step:1701/2330 train_time:68460ms step_avg:40.25ms
step:1702/2330 train_time:68518ms step_avg:40.26ms
step:1703/2330 train_time:68541ms step_avg:40.25ms
step:1704/2330 train_time:68598ms step_avg:40.26ms
step:1705/2330 train_time:68621ms step_avg:40.25ms
step:1706/2330 train_time:68679ms step_avg:40.26ms
step:1707/2330 train_time:68702ms step_avg:40.25ms
step:1708/2330 train_time:68760ms step_avg:40.26ms
step:1709/2330 train_time:68783ms step_avg:40.25ms
step:1710/2330 train_time:68841ms step_avg:40.26ms
step:1711/2330 train_time:68865ms step_avg:40.25ms
step:1712/2330 train_time:68923ms step_avg:40.26ms
step:1713/2330 train_time:68946ms step_avg:40.25ms
step:1714/2330 train_time:69003ms step_avg:40.26ms
step:1715/2330 train_time:69027ms step_avg:40.25ms
step:1716/2330 train_time:69085ms step_avg:40.26ms
step:1717/2330 train_time:69108ms step_avg:40.25ms
step:1718/2330 train_time:69165ms step_avg:40.26ms
step:1719/2330 train_time:69189ms step_avg:40.25ms
step:1720/2330 train_time:69246ms step_avg:40.26ms
step:1721/2330 train_time:69270ms step_avg:40.25ms
step:1722/2330 train_time:69327ms step_avg:40.26ms
step:1723/2330 train_time:69351ms step_avg:40.25ms
step:1724/2330 train_time:69408ms step_avg:40.26ms
step:1725/2330 train_time:69431ms step_avg:40.25ms
step:1726/2330 train_time:69488ms step_avg:40.26ms
step:1727/2330 train_time:69513ms step_avg:40.25ms
step:1728/2330 train_time:69569ms step_avg:40.26ms
step:1729/2330 train_time:69593ms step_avg:40.25ms
step:1730/2330 train_time:69650ms step_avg:40.26ms
step:1731/2330 train_time:69673ms step_avg:40.25ms
step:1732/2330 train_time:69730ms step_avg:40.26ms
step:1733/2330 train_time:69753ms step_avg:40.25ms
step:1734/2330 train_time:69812ms step_avg:40.26ms
step:1735/2330 train_time:69835ms step_avg:40.25ms
step:1736/2330 train_time:69893ms step_avg:40.26ms
step:1737/2330 train_time:69915ms step_avg:40.25ms
step:1738/2330 train_time:69973ms step_avg:40.26ms
step:1739/2330 train_time:69996ms step_avg:40.25ms
step:1740/2330 train_time:70054ms step_avg:40.26ms
step:1741/2330 train_time:70076ms step_avg:40.25ms
step:1742/2330 train_time:70134ms step_avg:40.26ms
step:1743/2330 train_time:70157ms step_avg:40.25ms
step:1744/2330 train_time:70216ms step_avg:40.26ms
step:1745/2330 train_time:70238ms step_avg:40.25ms
step:1746/2330 train_time:70296ms step_avg:40.26ms
step:1747/2330 train_time:70319ms step_avg:40.25ms
step:1748/2330 train_time:70377ms step_avg:40.26ms
step:1749/2330 train_time:70400ms step_avg:40.25ms
step:1750/2330 train_time:70458ms step_avg:40.26ms
step:1750/2330 val_loss:5.3281 train_time:70559ms step_avg:40.32ms
step:1751/2330 train_time:70571ms step_avg:40.30ms
step:1752/2330 train_time:70582ms step_avg:40.29ms
step:1753/2330 train_time:70592ms step_avg:40.27ms
step:1754/2330 train_time:70621ms step_avg:40.26ms
step:1755/2330 train_time:70643ms step_avg:40.25ms
step:1756/2330 train_time:70699ms step_avg:40.26ms
step:1757/2330 train_time:70722ms step_avg:40.25ms
step:1758/2330 train_time:70778ms step_avg:40.26ms
step:1759/2330 train_time:70800ms step_avg:40.25ms
step:1760/2330 train_time:70859ms step_avg:40.26ms
step:1761/2330 train_time:70886ms step_avg:40.25ms
step:1762/2330 train_time:70947ms step_avg:40.26ms
step:1763/2330 train_time:70971ms step_avg:40.26ms
step:1764/2330 train_time:71029ms step_avg:40.27ms
step:1765/2330 train_time:71052ms step_avg:40.26ms
step:1766/2330 train_time:71109ms step_avg:40.27ms
step:1767/2330 train_time:71132ms step_avg:40.26ms
step:1768/2330 train_time:71189ms step_avg:40.27ms
step:1769/2330 train_time:71211ms step_avg:40.26ms
step:1770/2330 train_time:71268ms step_avg:40.26ms
step:1771/2330 train_time:71291ms step_avg:40.25ms
step:1772/2330 train_time:71347ms step_avg:40.26ms
step:1773/2330 train_time:71370ms step_avg:40.25ms
step:1774/2330 train_time:71427ms step_avg:40.26ms
step:1775/2330 train_time:71449ms step_avg:40.25ms
step:1776/2330 train_time:71508ms step_avg:40.26ms
step:1777/2330 train_time:71533ms step_avg:40.26ms
step:1778/2330 train_time:71592ms step_avg:40.27ms
step:1779/2330 train_time:71615ms step_avg:40.26ms
step:1780/2330 train_time:71672ms step_avg:40.27ms
step:1781/2330 train_time:71695ms step_avg:40.26ms
step:1782/2330 train_time:71753ms step_avg:40.27ms
step:1783/2330 train_time:71776ms step_avg:40.26ms
step:1784/2330 train_time:71835ms step_avg:40.27ms
step:1785/2330 train_time:71858ms step_avg:40.26ms
step:1786/2330 train_time:71917ms step_avg:40.27ms
step:1787/2330 train_time:71941ms step_avg:40.26ms
step:1788/2330 train_time:71999ms step_avg:40.27ms
step:1789/2330 train_time:72023ms step_avg:40.26ms
step:1790/2330 train_time:72080ms step_avg:40.27ms
step:1791/2330 train_time:72104ms step_avg:40.26ms
step:1792/2330 train_time:72161ms step_avg:40.27ms
step:1793/2330 train_time:72184ms step_avg:40.26ms
step:1794/2330 train_time:72241ms step_avg:40.27ms
step:1795/2330 train_time:72264ms step_avg:40.26ms
step:1796/2330 train_time:72320ms step_avg:40.27ms
step:1797/2330 train_time:72344ms step_avg:40.26ms
step:1798/2330 train_time:72401ms step_avg:40.27ms
step:1799/2330 train_time:72424ms step_avg:40.26ms
step:1800/2330 train_time:72482ms step_avg:40.27ms
step:1801/2330 train_time:72507ms step_avg:40.26ms
step:1802/2330 train_time:72565ms step_avg:40.27ms
step:1803/2330 train_time:72589ms step_avg:40.26ms
step:1804/2330 train_time:72645ms step_avg:40.27ms
step:1805/2330 train_time:72668ms step_avg:40.26ms
step:1806/2330 train_time:72725ms step_avg:40.27ms
step:1807/2330 train_time:72749ms step_avg:40.26ms
step:1808/2330 train_time:72806ms step_avg:40.27ms
step:1809/2330 train_time:72829ms step_avg:40.26ms
step:1810/2330 train_time:72887ms step_avg:40.27ms
step:1811/2330 train_time:72911ms step_avg:40.26ms
step:1812/2330 train_time:72968ms step_avg:40.27ms
step:1813/2330 train_time:72992ms step_avg:40.26ms
step:1814/2330 train_time:73050ms step_avg:40.27ms
step:1815/2330 train_time:73072ms step_avg:40.26ms
step:1816/2330 train_time:73130ms step_avg:40.27ms
step:1817/2330 train_time:73153ms step_avg:40.26ms
step:1818/2330 train_time:73211ms step_avg:40.27ms
step:1819/2330 train_time:73234ms step_avg:40.26ms
step:1820/2330 train_time:73291ms step_avg:40.27ms
step:1821/2330 train_time:73315ms step_avg:40.26ms
step:1822/2330 train_time:73373ms step_avg:40.27ms
step:1823/2330 train_time:73395ms step_avg:40.26ms
step:1824/2330 train_time:73452ms step_avg:40.27ms
step:1825/2330 train_time:73476ms step_avg:40.26ms
step:1826/2330 train_time:73534ms step_avg:40.27ms
step:1827/2330 train_time:73556ms step_avg:40.26ms
step:1828/2330 train_time:73613ms step_avg:40.27ms
step:1829/2330 train_time:73638ms step_avg:40.26ms
step:1830/2330 train_time:73695ms step_avg:40.27ms
step:1831/2330 train_time:73719ms step_avg:40.26ms
step:1832/2330 train_time:73777ms step_avg:40.27ms
step:1833/2330 train_time:73800ms step_avg:40.26ms
step:1834/2330 train_time:73857ms step_avg:40.27ms
step:1835/2330 train_time:73881ms step_avg:40.26ms
step:1836/2330 train_time:73938ms step_avg:40.27ms
step:1837/2330 train_time:73961ms step_avg:40.26ms
step:1838/2330 train_time:74020ms step_avg:40.27ms
step:1839/2330 train_time:74043ms step_avg:40.26ms
step:1840/2330 train_time:74101ms step_avg:40.27ms
step:1841/2330 train_time:74124ms step_avg:40.26ms
step:1842/2330 train_time:74182ms step_avg:40.27ms
step:1843/2330 train_time:74205ms step_avg:40.26ms
step:1844/2330 train_time:74262ms step_avg:40.27ms
step:1845/2330 train_time:74286ms step_avg:40.26ms
step:1846/2330 train_time:74343ms step_avg:40.27ms
step:1847/2330 train_time:74366ms step_avg:40.26ms
step:1848/2330 train_time:74423ms step_avg:40.27ms
step:1849/2330 train_time:74447ms step_avg:40.26ms
step:1850/2330 train_time:74504ms step_avg:40.27ms
step:1851/2330 train_time:74528ms step_avg:40.26ms
step:1852/2330 train_time:74584ms step_avg:40.27ms
step:1853/2330 train_time:74608ms step_avg:40.26ms
step:1854/2330 train_time:74665ms step_avg:40.27ms
step:1855/2330 train_time:74688ms step_avg:40.26ms
step:1856/2330 train_time:74746ms step_avg:40.27ms
step:1857/2330 train_time:74770ms step_avg:40.26ms
step:1858/2330 train_time:74827ms step_avg:40.27ms
step:1859/2330 train_time:74850ms step_avg:40.26ms
step:1860/2330 train_time:74907ms step_avg:40.27ms
step:1861/2330 train_time:74930ms step_avg:40.26ms
step:1862/2330 train_time:74987ms step_avg:40.27ms
step:1863/2330 train_time:75011ms step_avg:40.26ms
step:1864/2330 train_time:75068ms step_avg:40.27ms
step:1865/2330 train_time:75091ms step_avg:40.26ms
step:1866/2330 train_time:75149ms step_avg:40.27ms
step:1867/2330 train_time:75172ms step_avg:40.26ms
step:1868/2330 train_time:75230ms step_avg:40.27ms
step:1869/2330 train_time:75253ms step_avg:40.26ms
step:1870/2330 train_time:75311ms step_avg:40.27ms
step:1871/2330 train_time:75334ms step_avg:40.26ms
step:1872/2330 train_time:75392ms step_avg:40.27ms
step:1873/2330 train_time:75415ms step_avg:40.26ms
step:1874/2330 train_time:75472ms step_avg:40.27ms
step:1875/2330 train_time:75495ms step_avg:40.26ms
step:1876/2330 train_time:75554ms step_avg:40.27ms
step:1877/2330 train_time:75576ms step_avg:40.26ms
step:1878/2330 train_time:75633ms step_avg:40.27ms
step:1879/2330 train_time:75656ms step_avg:40.26ms
step:1880/2330 train_time:75714ms step_avg:40.27ms
step:1881/2330 train_time:75737ms step_avg:40.26ms
step:1882/2330 train_time:75795ms step_avg:40.27ms
step:1883/2330 train_time:75818ms step_avg:40.26ms
step:1884/2330 train_time:75875ms step_avg:40.27ms
step:1885/2330 train_time:75897ms step_avg:40.26ms
step:1886/2330 train_time:75956ms step_avg:40.27ms
step:1887/2330 train_time:75979ms step_avg:40.26ms
step:1888/2330 train_time:76036ms step_avg:40.27ms
step:1889/2330 train_time:76060ms step_avg:40.26ms
step:1890/2330 train_time:76117ms step_avg:40.27ms
step:1891/2330 train_time:76141ms step_avg:40.26ms
step:1892/2330 train_time:76199ms step_avg:40.27ms
step:1893/2330 train_time:76223ms step_avg:40.27ms
step:1894/2330 train_time:76281ms step_avg:40.27ms
step:1895/2330 train_time:76304ms step_avg:40.27ms
step:1896/2330 train_time:76362ms step_avg:40.28ms
step:1897/2330 train_time:76386ms step_avg:40.27ms
step:1898/2330 train_time:76443ms step_avg:40.28ms
step:1899/2330 train_time:76466ms step_avg:40.27ms
step:1900/2330 train_time:76524ms step_avg:40.28ms
step:1901/2330 train_time:76547ms step_avg:40.27ms
step:1902/2330 train_time:76604ms step_avg:40.28ms
step:1903/2330 train_time:76627ms step_avg:40.27ms
step:1904/2330 train_time:76684ms step_avg:40.28ms
step:1905/2330 train_time:76707ms step_avg:40.27ms
step:1906/2330 train_time:76765ms step_avg:40.28ms
step:1907/2330 train_time:76789ms step_avg:40.27ms
step:1908/2330 train_time:76846ms step_avg:40.28ms
step:1909/2330 train_time:76870ms step_avg:40.27ms
step:1910/2330 train_time:76927ms step_avg:40.28ms
step:1911/2330 train_time:76950ms step_avg:40.27ms
step:1912/2330 train_time:77008ms step_avg:40.28ms
step:1913/2330 train_time:77032ms step_avg:40.27ms
step:1914/2330 train_time:77089ms step_avg:40.28ms
step:1915/2330 train_time:77113ms step_avg:40.27ms
step:1916/2330 train_time:77170ms step_avg:40.28ms
step:1917/2330 train_time:77194ms step_avg:40.27ms
step:1918/2330 train_time:77252ms step_avg:40.28ms
step:1919/2330 train_time:77275ms step_avg:40.27ms
step:1920/2330 train_time:77333ms step_avg:40.28ms
step:1921/2330 train_time:77356ms step_avg:40.27ms
step:1922/2330 train_time:77414ms step_avg:40.28ms
step:1923/2330 train_time:77438ms step_avg:40.27ms
step:1924/2330 train_time:77495ms step_avg:40.28ms
step:1925/2330 train_time:77518ms step_avg:40.27ms
step:1926/2330 train_time:77576ms step_avg:40.28ms
step:1927/2330 train_time:77600ms step_avg:40.27ms
step:1928/2330 train_time:77658ms step_avg:40.28ms
step:1929/2330 train_time:77681ms step_avg:40.27ms
step:1930/2330 train_time:77739ms step_avg:40.28ms
step:1931/2330 train_time:77762ms step_avg:40.27ms
step:1932/2330 train_time:77820ms step_avg:40.28ms
step:1933/2330 train_time:77844ms step_avg:40.27ms
step:1934/2330 train_time:77901ms step_avg:40.28ms
step:1935/2330 train_time:77925ms step_avg:40.27ms
step:1936/2330 train_time:77982ms step_avg:40.28ms
step:1937/2330 train_time:78006ms step_avg:40.27ms
step:1938/2330 train_time:78063ms step_avg:40.28ms
step:1939/2330 train_time:78086ms step_avg:40.27ms
step:1940/2330 train_time:78144ms step_avg:40.28ms
step:1941/2330 train_time:78167ms step_avg:40.27ms
step:1942/2330 train_time:78224ms step_avg:40.28ms
step:1943/2330 train_time:78248ms step_avg:40.27ms
step:1944/2330 train_time:78304ms step_avg:40.28ms
step:1945/2330 train_time:78328ms step_avg:40.27ms
step:1946/2330 train_time:78385ms step_avg:40.28ms
step:1947/2330 train_time:78408ms step_avg:40.27ms
step:1948/2330 train_time:78466ms step_avg:40.28ms
step:1949/2330 train_time:78490ms step_avg:40.27ms
step:1950/2330 train_time:78547ms step_avg:40.28ms
step:1951/2330 train_time:78570ms step_avg:40.27ms
step:1952/2330 train_time:78627ms step_avg:40.28ms
step:1953/2330 train_time:78651ms step_avg:40.27ms
step:1954/2330 train_time:78708ms step_avg:40.28ms
step:1955/2330 train_time:78732ms step_avg:40.27ms
step:1956/2330 train_time:78789ms step_avg:40.28ms
step:1957/2330 train_time:78813ms step_avg:40.27ms
step:1958/2330 train_time:78871ms step_avg:40.28ms
step:1959/2330 train_time:78894ms step_avg:40.27ms
step:1960/2330 train_time:78951ms step_avg:40.28ms
step:1961/2330 train_time:78974ms step_avg:40.27ms
step:1962/2330 train_time:79032ms step_avg:40.28ms
step:1963/2330 train_time:79056ms step_avg:40.27ms
step:1964/2330 train_time:79113ms step_avg:40.28ms
step:1965/2330 train_time:79136ms step_avg:40.27ms
step:1966/2330 train_time:79194ms step_avg:40.28ms
step:1967/2330 train_time:79217ms step_avg:40.27ms
step:1968/2330 train_time:79276ms step_avg:40.28ms
step:1969/2330 train_time:79298ms step_avg:40.27ms
step:1970/2330 train_time:79356ms step_avg:40.28ms
step:1971/2330 train_time:79379ms step_avg:40.27ms
step:1972/2330 train_time:79437ms step_avg:40.28ms
step:1973/2330 train_time:79460ms step_avg:40.27ms
step:1974/2330 train_time:79517ms step_avg:40.28ms
step:1975/2330 train_time:79540ms step_avg:40.27ms
step:1976/2330 train_time:79598ms step_avg:40.28ms
step:1977/2330 train_time:79622ms step_avg:40.27ms
step:1978/2330 train_time:79679ms step_avg:40.28ms
step:1979/2330 train_time:79703ms step_avg:40.27ms
step:1980/2330 train_time:79760ms step_avg:40.28ms
step:1981/2330 train_time:79784ms step_avg:40.27ms
step:1982/2330 train_time:79841ms step_avg:40.28ms
step:1983/2330 train_time:79865ms step_avg:40.28ms
step:1984/2330 train_time:79922ms step_avg:40.28ms
step:1985/2330 train_time:79945ms step_avg:40.27ms
step:1986/2330 train_time:80003ms step_avg:40.28ms
step:1987/2330 train_time:80027ms step_avg:40.28ms
step:1988/2330 train_time:80085ms step_avg:40.28ms
step:1989/2330 train_time:80109ms step_avg:40.28ms
step:1990/2330 train_time:80165ms step_avg:40.28ms
step:1991/2330 train_time:80190ms step_avg:40.28ms
step:1992/2330 train_time:80247ms step_avg:40.28ms
step:1993/2330 train_time:80270ms step_avg:40.28ms
step:1994/2330 train_time:80328ms step_avg:40.28ms
step:1995/2330 train_time:80351ms step_avg:40.28ms
step:1996/2330 train_time:80409ms step_avg:40.28ms
step:1997/2330 train_time:80432ms step_avg:40.28ms
step:1998/2330 train_time:80490ms step_avg:40.29ms
step:1999/2330 train_time:80513ms step_avg:40.28ms
step:2000/2330 train_time:80570ms step_avg:40.29ms
step:2000/2330 val_loss:5.2863 train_time:80668ms step_avg:40.33ms
step:2001/2330 train_time:80680ms step_avg:40.32ms
step:2002/2330 train_time:80691ms step_avg:40.31ms
step:2003/2330 train_time:80701ms step_avg:40.29ms
step:2004/2330 train_time:80732ms step_avg:40.29ms
step:2005/2330 train_time:80754ms step_avg:40.28ms
step:2006/2330 train_time:80810ms step_avg:40.28ms
step:2007/2330 train_time:80834ms step_avg:40.28ms
step:2008/2330 train_time:80890ms step_avg:40.28ms
step:2009/2330 train_time:80913ms step_avg:40.28ms
step:2010/2330 train_time:80972ms step_avg:40.28ms
step:2011/2330 train_time:81000ms step_avg:40.28ms
step:2012/2330 train_time:81061ms step_avg:40.29ms
step:2013/2330 train_time:81086ms step_avg:40.28ms
step:2014/2330 train_time:81144ms step_avg:40.29ms
step:2015/2330 train_time:81168ms step_avg:40.28ms
step:2016/2330 train_time:81226ms step_avg:40.29ms
step:2017/2330 train_time:81249ms step_avg:40.28ms
step:2018/2330 train_time:81306ms step_avg:40.29ms
step:2019/2330 train_time:81329ms step_avg:40.28ms
step:2020/2330 train_time:81386ms step_avg:40.29ms
step:2021/2330 train_time:81409ms step_avg:40.28ms
step:2022/2330 train_time:81465ms step_avg:40.29ms
step:2023/2330 train_time:81487ms step_avg:40.28ms
step:2024/2330 train_time:81544ms step_avg:40.29ms
step:2025/2330 train_time:81568ms step_avg:40.28ms
step:2026/2330 train_time:81626ms step_avg:40.29ms
step:2027/2330 train_time:81649ms step_avg:40.28ms
step:2028/2330 train_time:81706ms step_avg:40.29ms
step:2029/2330 train_time:81728ms step_avg:40.28ms
step:2030/2330 train_time:81785ms step_avg:40.29ms
step:2031/2330 train_time:81808ms step_avg:40.28ms
step:2032/2330 train_time:81867ms step_avg:40.29ms
step:2033/2330 train_time:81890ms step_avg:40.28ms
step:2034/2330 train_time:81949ms step_avg:40.29ms
step:2035/2330 train_time:81975ms step_avg:40.28ms
step:2036/2330 train_time:82033ms step_avg:40.29ms
step:2037/2330 train_time:82058ms step_avg:40.28ms
step:2038/2330 train_time:82117ms step_avg:40.29ms
step:2039/2330 train_time:82141ms step_avg:40.28ms
step:2040/2330 train_time:82198ms step_avg:40.29ms
step:2041/2330 train_time:82222ms step_avg:40.28ms
step:2042/2330 train_time:82279ms step_avg:40.29ms
step:2043/2330 train_time:82301ms step_avg:40.28ms
step:2044/2330 train_time:82358ms step_avg:40.29ms
step:2045/2330 train_time:82381ms step_avg:40.28ms
step:2046/2330 train_time:82438ms step_avg:40.29ms
step:2047/2330 train_time:82460ms step_avg:40.28ms
step:2048/2330 train_time:82518ms step_avg:40.29ms
step:2049/2330 train_time:82540ms step_avg:40.28ms
step:2050/2330 train_time:82597ms step_avg:40.29ms
step:2051/2330 train_time:82620ms step_avg:40.28ms
step:2052/2330 train_time:82680ms step_avg:40.29ms
step:2053/2330 train_time:82702ms step_avg:40.28ms
step:2054/2330 train_time:82760ms step_avg:40.29ms
step:2055/2330 train_time:82782ms step_avg:40.28ms
step:2056/2330 train_time:82841ms step_avg:40.29ms
step:2057/2330 train_time:82863ms step_avg:40.28ms
step:2058/2330 train_time:82923ms step_avg:40.29ms
step:2059/2330 train_time:82946ms step_avg:40.28ms
step:2060/2330 train_time:83005ms step_avg:40.29ms
step:2061/2330 train_time:83029ms step_avg:40.29ms
step:2062/2330 train_time:83088ms step_avg:40.29ms
step:2063/2330 train_time:83112ms step_avg:40.29ms
step:2064/2330 train_time:83169ms step_avg:40.30ms
step:2065/2330 train_time:83193ms step_avg:40.29ms
step:2066/2330 train_time:83250ms step_avg:40.30ms
step:2067/2330 train_time:83274ms step_avg:40.29ms
step:2068/2330 train_time:83331ms step_avg:40.30ms
step:2069/2330 train_time:83354ms step_avg:40.29ms
step:2070/2330 train_time:83410ms step_avg:40.29ms
step:2071/2330 train_time:83434ms step_avg:40.29ms
step:2072/2330 train_time:83492ms step_avg:40.30ms
step:2073/2330 train_time:83516ms step_avg:40.29ms
step:2074/2330 train_time:83573ms step_avg:40.30ms
step:2075/2330 train_time:83597ms step_avg:40.29ms
step:2076/2330 train_time:83654ms step_avg:40.30ms
step:2077/2330 train_time:83677ms step_avg:40.29ms
step:2078/2330 train_time:83733ms step_avg:40.30ms
step:2079/2330 train_time:83757ms step_avg:40.29ms
step:2080/2330 train_time:83813ms step_avg:40.29ms
step:2081/2330 train_time:83836ms step_avg:40.29ms
step:2082/2330 train_time:83893ms step_avg:40.29ms
step:2083/2330 train_time:83917ms step_avg:40.29ms
step:2084/2330 train_time:83975ms step_avg:40.30ms
step:2085/2330 train_time:83999ms step_avg:40.29ms
step:2086/2330 train_time:84057ms step_avg:40.30ms
step:2087/2330 train_time:84081ms step_avg:40.29ms
step:2088/2330 train_time:84139ms step_avg:40.30ms
step:2089/2330 train_time:84162ms step_avg:40.29ms
step:2090/2330 train_time:84220ms step_avg:40.30ms
step:2091/2330 train_time:84242ms step_avg:40.29ms
step:2092/2330 train_time:84300ms step_avg:40.30ms
step:2093/2330 train_time:84322ms step_avg:40.29ms
step:2094/2330 train_time:84380ms step_avg:40.30ms
step:2095/2330 train_time:84403ms step_avg:40.29ms
step:2096/2330 train_time:84460ms step_avg:40.30ms
step:2097/2330 train_time:84483ms step_avg:40.29ms
step:2098/2330 train_time:84541ms step_avg:40.30ms
step:2099/2330 train_time:84564ms step_avg:40.29ms
step:2100/2330 train_time:84622ms step_avg:40.30ms
step:2101/2330 train_time:84644ms step_avg:40.29ms
step:2102/2330 train_time:84702ms step_avg:40.30ms
step:2103/2330 train_time:84724ms step_avg:40.29ms
step:2104/2330 train_time:84782ms step_avg:40.30ms
step:2105/2330 train_time:84804ms step_avg:40.29ms
step:2106/2330 train_time:84862ms step_avg:40.30ms
step:2107/2330 train_time:84886ms step_avg:40.29ms
step:2108/2330 train_time:84944ms step_avg:40.30ms
step:2109/2330 train_time:84967ms step_avg:40.29ms
step:2110/2330 train_time:85025ms step_avg:40.30ms
step:2111/2330 train_time:85049ms step_avg:40.29ms
step:2112/2330 train_time:85107ms step_avg:40.30ms
step:2113/2330 train_time:85131ms step_avg:40.29ms
step:2114/2330 train_time:85189ms step_avg:40.30ms
step:2115/2330 train_time:85213ms step_avg:40.29ms
step:2116/2330 train_time:85270ms step_avg:40.30ms
step:2117/2330 train_time:85294ms step_avg:40.29ms
step:2118/2330 train_time:85350ms step_avg:40.30ms
step:2119/2330 train_time:85373ms step_avg:40.29ms
step:2120/2330 train_time:85431ms step_avg:40.30ms
step:2121/2330 train_time:85454ms step_avg:40.29ms
step:2122/2330 train_time:85512ms step_avg:40.30ms
step:2123/2330 train_time:85536ms step_avg:40.29ms
step:2124/2330 train_time:85593ms step_avg:40.30ms
step:2125/2330 train_time:85616ms step_avg:40.29ms
step:2126/2330 train_time:85673ms step_avg:40.30ms
step:2127/2330 train_time:85696ms step_avg:40.29ms
step:2128/2330 train_time:85753ms step_avg:40.30ms
step:2129/2330 train_time:85777ms step_avg:40.29ms
step:2130/2330 train_time:85833ms step_avg:40.30ms
step:2131/2330 train_time:85857ms step_avg:40.29ms
step:2132/2330 train_time:85913ms step_avg:40.30ms
step:2133/2330 train_time:85937ms step_avg:40.29ms
step:2134/2330 train_time:85995ms step_avg:40.30ms
step:2135/2330 train_time:86019ms step_avg:40.29ms
step:2136/2330 train_time:86078ms step_avg:40.30ms
step:2137/2330 train_time:86102ms step_avg:40.29ms
step:2138/2330 train_time:86160ms step_avg:40.30ms
step:2139/2330 train_time:86182ms step_avg:40.29ms
step:2140/2330 train_time:86241ms step_avg:40.30ms
step:2141/2330 train_time:86263ms step_avg:40.29ms
step:2142/2330 train_time:86320ms step_avg:40.30ms
step:2143/2330 train_time:86344ms step_avg:40.29ms
step:2144/2330 train_time:86402ms step_avg:40.30ms
step:2145/2330 train_time:86425ms step_avg:40.29ms
step:2146/2330 train_time:86484ms step_avg:40.30ms
step:2147/2330 train_time:86507ms step_avg:40.29ms
step:2148/2330 train_time:86564ms step_avg:40.30ms
step:2149/2330 train_time:86587ms step_avg:40.29ms
step:2150/2330 train_time:86644ms step_avg:40.30ms
step:2151/2330 train_time:86667ms step_avg:40.29ms
step:2152/2330 train_time:86725ms step_avg:40.30ms
step:2153/2330 train_time:86748ms step_avg:40.29ms
step:2154/2330 train_time:86807ms step_avg:40.30ms
step:2155/2330 train_time:86829ms step_avg:40.29ms
step:2156/2330 train_time:86887ms step_avg:40.30ms
step:2157/2330 train_time:86911ms step_avg:40.29ms
step:2158/2330 train_time:86968ms step_avg:40.30ms
step:2159/2330 train_time:86993ms step_avg:40.29ms
step:2160/2330 train_time:87050ms step_avg:40.30ms
step:2161/2330 train_time:87073ms step_avg:40.29ms
step:2162/2330 train_time:87130ms step_avg:40.30ms
step:2163/2330 train_time:87153ms step_avg:40.29ms
step:2164/2330 train_time:87210ms step_avg:40.30ms
step:2165/2330 train_time:87234ms step_avg:40.29ms
step:2166/2330 train_time:87290ms step_avg:40.30ms
step:2167/2330 train_time:87315ms step_avg:40.29ms
step:2168/2330 train_time:87372ms step_avg:40.30ms
step:2169/2330 train_time:87396ms step_avg:40.29ms
step:2170/2330 train_time:87453ms step_avg:40.30ms
step:2171/2330 train_time:87478ms step_avg:40.29ms
step:2172/2330 train_time:87535ms step_avg:40.30ms
step:2173/2330 train_time:87558ms step_avg:40.29ms
step:2174/2330 train_time:87616ms step_avg:40.30ms
step:2175/2330 train_time:87640ms step_avg:40.29ms
step:2176/2330 train_time:87697ms step_avg:40.30ms
step:2177/2330 train_time:87719ms step_avg:40.29ms
step:2178/2330 train_time:87777ms step_avg:40.30ms
step:2179/2330 train_time:87799ms step_avg:40.29ms
step:2180/2330 train_time:87858ms step_avg:40.30ms
step:2181/2330 train_time:87881ms step_avg:40.29ms
step:2182/2330 train_time:87938ms step_avg:40.30ms
step:2183/2330 train_time:87961ms step_avg:40.29ms
step:2184/2330 train_time:88019ms step_avg:40.30ms
step:2185/2330 train_time:88042ms step_avg:40.29ms
step:2186/2330 train_time:88100ms step_avg:40.30ms
step:2187/2330 train_time:88122ms step_avg:40.29ms
step:2188/2330 train_time:88179ms step_avg:40.30ms
step:2189/2330 train_time:88203ms step_avg:40.29ms
step:2190/2330 train_time:88261ms step_avg:40.30ms
step:2191/2330 train_time:88284ms step_avg:40.29ms
step:2192/2330 train_time:88343ms step_avg:40.30ms
step:2193/2330 train_time:88366ms step_avg:40.29ms
step:2194/2330 train_time:88425ms step_avg:40.30ms
step:2195/2330 train_time:88449ms step_avg:40.30ms
step:2196/2330 train_time:88506ms step_avg:40.30ms
step:2197/2330 train_time:88529ms step_avg:40.30ms
step:2198/2330 train_time:88587ms step_avg:40.30ms
step:2199/2330 train_time:88611ms step_avg:40.30ms
step:2200/2330 train_time:88668ms step_avg:40.30ms
step:2201/2330 train_time:88692ms step_avg:40.30ms
step:2202/2330 train_time:88749ms step_avg:40.30ms
step:2203/2330 train_time:88773ms step_avg:40.30ms
step:2204/2330 train_time:88830ms step_avg:40.30ms
step:2205/2330 train_time:88853ms step_avg:40.30ms
step:2206/2330 train_time:88910ms step_avg:40.30ms
step:2207/2330 train_time:88934ms step_avg:40.30ms
step:2208/2330 train_time:88991ms step_avg:40.30ms
step:2209/2330 train_time:89016ms step_avg:40.30ms
step:2210/2330 train_time:89072ms step_avg:40.30ms
step:2211/2330 train_time:89095ms step_avg:40.30ms
step:2212/2330 train_time:89152ms step_avg:40.30ms
step:2213/2330 train_time:89176ms step_avg:40.30ms
step:2214/2330 train_time:89233ms step_avg:40.30ms
step:2215/2330 train_time:89257ms step_avg:40.30ms
step:2216/2330 train_time:89314ms step_avg:40.30ms
step:2217/2330 train_time:89338ms step_avg:40.30ms
step:2218/2330 train_time:89395ms step_avg:40.30ms
step:2219/2330 train_time:89418ms step_avg:40.30ms
step:2220/2330 train_time:89476ms step_avg:40.30ms
step:2221/2330 train_time:89499ms step_avg:40.30ms
step:2222/2330 train_time:89557ms step_avg:40.30ms
step:2223/2330 train_time:89580ms step_avg:40.30ms
step:2224/2330 train_time:89637ms step_avg:40.30ms
step:2225/2330 train_time:89661ms step_avg:40.30ms
step:2226/2330 train_time:89719ms step_avg:40.31ms
step:2227/2330 train_time:89742ms step_avg:40.30ms
step:2228/2330 train_time:89799ms step_avg:40.30ms
step:2229/2330 train_time:89822ms step_avg:40.30ms
step:2230/2330 train_time:89881ms step_avg:40.31ms
step:2231/2330 train_time:89903ms step_avg:40.30ms
step:2232/2330 train_time:89962ms step_avg:40.31ms
step:2233/2330 train_time:89984ms step_avg:40.30ms
step:2234/2330 train_time:90042ms step_avg:40.31ms
step:2235/2330 train_time:90065ms step_avg:40.30ms
step:2236/2330 train_time:90123ms step_avg:40.31ms
step:2237/2330 train_time:90146ms step_avg:40.30ms
step:2238/2330 train_time:90203ms step_avg:40.31ms
step:2239/2330 train_time:90226ms step_avg:40.30ms
step:2240/2330 train_time:90285ms step_avg:40.31ms
step:2241/2330 train_time:90309ms step_avg:40.30ms
step:2242/2330 train_time:90366ms step_avg:40.31ms
step:2243/2330 train_time:90390ms step_avg:40.30ms
step:2244/2330 train_time:90447ms step_avg:40.31ms
step:2245/2330 train_time:90471ms step_avg:40.30ms
step:2246/2330 train_time:90528ms step_avg:40.31ms
step:2247/2330 train_time:90552ms step_avg:40.30ms
step:2248/2330 train_time:90609ms step_avg:40.31ms
step:2249/2330 train_time:90633ms step_avg:40.30ms
step:2250/2330 train_time:90690ms step_avg:40.31ms
step:2250/2330 val_loss:5.2541 train_time:90789ms step_avg:40.35ms
step:2251/2330 train_time:90803ms step_avg:40.34ms
step:2252/2330 train_time:90814ms step_avg:40.33ms
step:2253/2330 train_time:90824ms step_avg:40.31ms
step:2254/2330 train_time:90852ms step_avg:40.31ms
step:2255/2330 train_time:90874ms step_avg:40.30ms
step:2256/2330 train_time:90930ms step_avg:40.31ms
step:2257/2330 train_time:90953ms step_avg:40.30ms
step:2258/2330 train_time:91009ms step_avg:40.31ms
step:2259/2330 train_time:91032ms step_avg:40.30ms
step:2260/2330 train_time:91091ms step_avg:40.31ms
step:2261/2330 train_time:91119ms step_avg:40.30ms
step:2262/2330 train_time:91179ms step_avg:40.31ms
step:2263/2330 train_time:91203ms step_avg:40.30ms
step:2264/2330 train_time:91261ms step_avg:40.31ms
step:2265/2330 train_time:91284ms step_avg:40.30ms
step:2266/2330 train_time:91342ms step_avg:40.31ms
step:2267/2330 train_time:91366ms step_avg:40.30ms
step:2268/2330 train_time:91423ms step_avg:40.31ms
step:2269/2330 train_time:91446ms step_avg:40.30ms
step:2270/2330 train_time:91502ms step_avg:40.31ms
step:2271/2330 train_time:91526ms step_avg:40.30ms
step:2272/2330 train_time:91582ms step_avg:40.31ms
step:2273/2330 train_time:91605ms step_avg:40.30ms
step:2274/2330 train_time:91663ms step_avg:40.31ms
step:2275/2330 train_time:91685ms step_avg:40.30ms
step:2276/2330 train_time:91745ms step_avg:40.31ms
step:2277/2330 train_time:91768ms step_avg:40.30ms
step:2278/2330 train_time:91827ms step_avg:40.31ms
step:2279/2330 train_time:91849ms step_avg:40.30ms
step:2280/2330 train_time:91906ms step_avg:40.31ms
step:2281/2330 train_time:91928ms step_avg:40.30ms
step:2282/2330 train_time:91986ms step_avg:40.31ms
step:2283/2330 train_time:92009ms step_avg:40.30ms
step:2284/2330 train_time:92068ms step_avg:40.31ms
step:2285/2330 train_time:92093ms step_avg:40.30ms
step:2286/2330 train_time:92151ms step_avg:40.31ms
step:2287/2330 train_time:92176ms step_avg:40.30ms
step:2288/2330 train_time:92234ms step_avg:40.31ms
step:2289/2330 train_time:92258ms step_avg:40.30ms
step:2290/2330 train_time:92314ms step_avg:40.31ms
step:2291/2330 train_time:92339ms step_avg:40.31ms
step:2292/2330 train_time:92397ms step_avg:40.31ms
step:2293/2330 train_time:92420ms step_avg:40.31ms
step:2294/2330 train_time:92477ms step_avg:40.31ms
step:2295/2330 train_time:92500ms step_avg:40.30ms
step:2296/2330 train_time:92557ms step_avg:40.31ms
step:2297/2330 train_time:92580ms step_avg:40.30ms
step:2298/2330 train_time:92638ms step_avg:40.31ms
step:2299/2330 train_time:92661ms step_avg:40.30ms
step:2300/2330 train_time:92718ms step_avg:40.31ms
step:2301/2330 train_time:92741ms step_avg:40.30ms
step:2302/2330 train_time:92799ms step_avg:40.31ms
step:2303/2330 train_time:92822ms step_avg:40.30ms
step:2304/2330 train_time:92880ms step_avg:40.31ms
step:2305/2330 train_time:92903ms step_avg:40.30ms
step:2306/2330 train_time:92961ms step_avg:40.31ms
step:2307/2330 train_time:92983ms step_avg:40.30ms
step:2308/2330 train_time:93042ms step_avg:40.31ms
step:2309/2330 train_time:93065ms step_avg:40.31ms
step:2310/2330 train_time:93124ms step_avg:40.31ms
step:2311/2330 train_time:93148ms step_avg:40.31ms
step:2312/2330 train_time:93206ms step_avg:40.31ms
step:2313/2330 train_time:93230ms step_avg:40.31ms
step:2314/2330 train_time:93289ms step_avg:40.31ms
step:2315/2330 train_time:93313ms step_avg:40.31ms
step:2316/2330 train_time:93370ms step_avg:40.32ms
step:2317/2330 train_time:93394ms step_avg:40.31ms
step:2318/2330 train_time:93451ms step_avg:40.32ms
step:2319/2330 train_time:93474ms step_avg:40.31ms
step:2320/2330 train_time:93532ms step_avg:40.32ms
step:2321/2330 train_time:93556ms step_avg:40.31ms
step:2322/2330 train_time:93612ms step_avg:40.32ms
step:2323/2330 train_time:93636ms step_avg:40.31ms
step:2324/2330 train_time:93693ms step_avg:40.32ms
step:2325/2330 train_time:93717ms step_avg:40.31ms
step:2326/2330 train_time:93773ms step_avg:40.32ms
step:2327/2330 train_time:93797ms step_avg:40.31ms
step:2328/2330 train_time:93854ms step_avg:40.32ms
step:2329/2330 train_time:93877ms step_avg:40.31ms
step:2330/2330 train_time:93935ms step_avg:40.32ms
step:2330/2330 val_loss:5.2470 train_time:94034ms step_avg:40.36ms
peak memory allocated: 29226 MiB reserved: 38888 MiB
