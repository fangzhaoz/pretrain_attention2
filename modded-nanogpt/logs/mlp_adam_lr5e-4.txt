import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_adam_lr5e-4"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-1, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizer2 = torch.optim.Adam(
    hidden_matrix_params + gate_params,
    lr=5e-4,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 04:31:25 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   33C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   31C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   31C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             115W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:91ms step_avg:90.82ms
step:2/2330 train_time:145ms step_avg:72.53ms
step:3/2330 train_time:158ms step_avg:52.60ms
step:4/2330 train_time:170ms step_avg:42.44ms
step:5/2330 train_time:180ms step_avg:36.00ms
step:6/2330 train_time:191ms step_avg:31.86ms
step:7/2330 train_time:221ms step_avg:31.57ms
step:8/2330 train_time:262ms step_avg:32.73ms
step:9/2330 train_time:296ms step_avg:32.89ms
step:10/2330 train_time:337ms step_avg:33.68ms
step:11/2330 train_time:372ms step_avg:33.78ms
step:12/2330 train_time:412ms step_avg:34.32ms
step:13/2330 train_time:448ms step_avg:34.44ms
step:14/2330 train_time:488ms step_avg:34.87ms
step:15/2330 train_time:523ms step_avg:34.86ms
step:16/2330 train_time:564ms step_avg:35.22ms
step:17/2330 train_time:598ms step_avg:35.19ms
step:18/2330 train_time:639ms step_avg:35.50ms
step:19/2330 train_time:674ms step_avg:35.46ms
step:20/2330 train_time:714ms step_avg:35.72ms
step:21/2330 train_time:750ms step_avg:35.70ms
step:22/2330 train_time:790ms step_avg:35.92ms
step:23/2330 train_time:825ms step_avg:35.86ms
step:24/2330 train_time:865ms step_avg:36.06ms
step:25/2330 train_time:900ms step_avg:35.99ms
step:26/2330 train_time:940ms step_avg:36.16ms
step:27/2330 train_time:978ms step_avg:36.21ms
step:28/2330 train_time:1021ms step_avg:36.47ms
step:29/2330 train_time:1060ms step_avg:36.56ms
step:30/2330 train_time:1104ms step_avg:36.80ms
step:31/2330 train_time:1142ms step_avg:36.84ms
step:32/2330 train_time:1184ms step_avg:37.00ms
step:33/2330 train_time:1220ms step_avg:36.97ms
step:34/2330 train_time:1262ms step_avg:37.12ms
step:35/2330 train_time:1297ms step_avg:37.05ms
step:36/2330 train_time:1338ms step_avg:37.17ms
step:37/2330 train_time:1373ms step_avg:37.12ms
step:38/2330 train_time:1414ms step_avg:37.22ms
step:39/2330 train_time:1449ms step_avg:37.17ms
step:40/2330 train_time:1490ms step_avg:37.25ms
step:41/2330 train_time:1526ms step_avg:37.21ms
step:42/2330 train_time:1566ms step_avg:37.30ms
step:43/2330 train_time:1601ms step_avg:37.23ms
step:44/2330 train_time:1642ms step_avg:37.31ms
step:45/2330 train_time:1677ms step_avg:37.26ms
step:46/2330 train_time:1717ms step_avg:37.33ms
step:47/2330 train_time:1753ms step_avg:37.30ms
step:48/2330 train_time:1793ms step_avg:37.36ms
step:49/2330 train_time:1829ms step_avg:37.33ms
step:50/2330 train_time:1870ms step_avg:37.39ms
step:51/2330 train_time:1905ms step_avg:37.35ms
step:52/2330 train_time:1946ms step_avg:37.42ms
step:53/2330 train_time:1984ms step_avg:37.43ms
step:54/2330 train_time:2025ms step_avg:37.51ms
step:55/2330 train_time:2063ms step_avg:37.50ms
step:56/2330 train_time:2104ms step_avg:37.58ms
step:57/2330 train_time:2142ms step_avg:37.57ms
step:58/2330 train_time:2183ms step_avg:37.64ms
step:59/2330 train_time:2220ms step_avg:37.62ms
step:60/2330 train_time:2261ms step_avg:37.69ms
step:61/2330 train_time:2297ms step_avg:37.66ms
step:62/2330 train_time:2339ms step_avg:37.73ms
step:63/2330 train_time:2375ms step_avg:37.70ms
step:64/2330 train_time:2416ms step_avg:37.75ms
step:65/2330 train_time:2451ms step_avg:37.71ms
step:66/2330 train_time:2492ms step_avg:37.76ms
step:67/2330 train_time:2528ms step_avg:37.73ms
step:68/2330 train_time:2569ms step_avg:37.78ms
step:69/2330 train_time:2603ms step_avg:37.73ms
step:70/2330 train_time:2644ms step_avg:37.77ms
step:71/2330 train_time:2680ms step_avg:37.74ms
step:72/2330 train_time:2721ms step_avg:37.80ms
step:73/2330 train_time:2756ms step_avg:37.76ms
step:74/2330 train_time:2798ms step_avg:37.81ms
step:75/2330 train_time:2832ms step_avg:37.76ms
step:76/2330 train_time:2873ms step_avg:37.80ms
step:77/2330 train_time:2909ms step_avg:37.78ms
step:78/2330 train_time:2951ms step_avg:37.83ms
step:79/2330 train_time:2987ms step_avg:37.81ms
step:80/2330 train_time:3029ms step_avg:37.86ms
step:81/2330 train_time:3065ms step_avg:37.84ms
step:82/2330 train_time:3107ms step_avg:37.89ms
step:83/2330 train_time:3143ms step_avg:37.87ms
step:84/2330 train_time:3184ms step_avg:37.91ms
step:85/2330 train_time:3221ms step_avg:37.89ms
step:86/2330 train_time:3263ms step_avg:37.94ms
step:87/2330 train_time:3299ms step_avg:37.91ms
step:88/2330 train_time:3341ms step_avg:37.96ms
step:89/2330 train_time:3376ms step_avg:37.93ms
step:90/2330 train_time:3417ms step_avg:37.97ms
step:91/2330 train_time:3453ms step_avg:37.94ms
step:92/2330 train_time:3494ms step_avg:37.98ms
step:93/2330 train_time:3529ms step_avg:37.95ms
step:94/2330 train_time:3570ms step_avg:37.98ms
step:95/2330 train_time:3605ms step_avg:37.95ms
step:96/2330 train_time:3646ms step_avg:37.98ms
step:97/2330 train_time:3681ms step_avg:37.95ms
step:98/2330 train_time:3722ms step_avg:37.98ms
step:99/2330 train_time:3758ms step_avg:37.96ms
step:100/2330 train_time:3799ms step_avg:37.99ms
step:101/2330 train_time:3834ms step_avg:37.96ms
step:102/2330 train_time:3875ms step_avg:37.99ms
step:103/2330 train_time:3911ms step_avg:37.97ms
step:104/2330 train_time:3952ms step_avg:38.00ms
step:105/2330 train_time:3989ms step_avg:37.99ms
step:106/2330 train_time:4030ms step_avg:38.02ms
step:107/2330 train_time:4066ms step_avg:38.00ms
step:108/2330 train_time:4107ms step_avg:38.03ms
step:109/2330 train_time:4143ms step_avg:38.01ms
step:110/2330 train_time:4185ms step_avg:38.04ms
step:111/2330 train_time:4221ms step_avg:38.03ms
step:112/2330 train_time:4263ms step_avg:38.06ms
step:113/2330 train_time:4298ms step_avg:38.04ms
step:114/2330 train_time:4340ms step_avg:38.07ms
step:115/2330 train_time:4376ms step_avg:38.06ms
step:116/2330 train_time:4418ms step_avg:38.09ms
step:117/2330 train_time:4454ms step_avg:38.06ms
step:118/2330 train_time:4495ms step_avg:38.09ms
step:119/2330 train_time:4530ms step_avg:38.07ms
step:120/2330 train_time:4571ms step_avg:38.09ms
step:121/2330 train_time:4607ms step_avg:38.07ms
step:122/2330 train_time:4648ms step_avg:38.10ms
step:123/2330 train_time:4683ms step_avg:38.07ms
step:124/2330 train_time:4724ms step_avg:38.10ms
step:125/2330 train_time:4759ms step_avg:38.07ms
step:126/2330 train_time:4800ms step_avg:38.10ms
step:127/2330 train_time:4835ms step_avg:38.07ms
step:128/2330 train_time:4877ms step_avg:38.10ms
step:129/2330 train_time:4912ms step_avg:38.08ms
step:130/2330 train_time:4953ms step_avg:38.10ms
step:131/2330 train_time:4990ms step_avg:38.09ms
step:132/2330 train_time:5031ms step_avg:38.11ms
step:133/2330 train_time:5066ms step_avg:38.09ms
step:134/2330 train_time:5108ms step_avg:38.12ms
step:135/2330 train_time:5143ms step_avg:38.09ms
step:136/2330 train_time:5184ms step_avg:38.12ms
step:137/2330 train_time:5219ms step_avg:38.10ms
step:138/2330 train_time:5260ms step_avg:38.12ms
step:139/2330 train_time:5296ms step_avg:38.10ms
step:140/2330 train_time:5337ms step_avg:38.12ms
step:141/2330 train_time:5374ms step_avg:38.11ms
step:142/2330 train_time:5415ms step_avg:38.13ms
step:143/2330 train_time:5451ms step_avg:38.12ms
step:144/2330 train_time:5491ms step_avg:38.13ms
step:145/2330 train_time:5527ms step_avg:38.12ms
step:146/2330 train_time:5568ms step_avg:38.14ms
step:147/2330 train_time:5603ms step_avg:38.12ms
step:148/2330 train_time:5644ms step_avg:38.14ms
step:149/2330 train_time:5680ms step_avg:38.12ms
step:150/2330 train_time:5721ms step_avg:38.14ms
step:151/2330 train_time:5757ms step_avg:38.13ms
step:152/2330 train_time:5798ms step_avg:38.15ms
step:153/2330 train_time:5834ms step_avg:38.13ms
step:154/2330 train_time:5875ms step_avg:38.15ms
step:155/2330 train_time:5910ms step_avg:38.13ms
step:156/2330 train_time:5951ms step_avg:38.15ms
step:157/2330 train_time:5989ms step_avg:38.14ms
step:158/2330 train_time:6029ms step_avg:38.16ms
step:159/2330 train_time:6065ms step_avg:38.15ms
step:160/2330 train_time:6106ms step_avg:38.17ms
step:161/2330 train_time:6142ms step_avg:38.15ms
step:162/2330 train_time:6182ms step_avg:38.16ms
step:163/2330 train_time:6218ms step_avg:38.15ms
step:164/2330 train_time:6259ms step_avg:38.17ms
step:165/2330 train_time:6295ms step_avg:38.15ms
step:166/2330 train_time:6337ms step_avg:38.17ms
step:167/2330 train_time:6372ms step_avg:38.16ms
step:168/2330 train_time:6413ms step_avg:38.17ms
step:169/2330 train_time:6448ms step_avg:38.16ms
step:170/2330 train_time:6490ms step_avg:38.17ms
step:171/2330 train_time:6525ms step_avg:38.16ms
step:172/2330 train_time:6567ms step_avg:38.18ms
step:173/2330 train_time:6602ms step_avg:38.16ms
step:174/2330 train_time:6643ms step_avg:38.18ms
step:175/2330 train_time:6678ms step_avg:38.16ms
step:176/2330 train_time:6719ms step_avg:38.18ms
step:177/2330 train_time:6754ms step_avg:38.16ms
step:178/2330 train_time:6795ms step_avg:38.18ms
step:179/2330 train_time:6832ms step_avg:38.17ms
step:180/2330 train_time:6872ms step_avg:38.18ms
step:181/2330 train_time:6909ms step_avg:38.17ms
step:182/2330 train_time:6950ms step_avg:38.19ms
step:183/2330 train_time:6985ms step_avg:38.17ms
step:184/2330 train_time:7027ms step_avg:38.19ms
step:185/2330 train_time:7061ms step_avg:38.17ms
step:186/2330 train_time:7103ms step_avg:38.19ms
step:187/2330 train_time:7138ms step_avg:38.17ms
step:188/2330 train_time:7179ms step_avg:38.19ms
step:189/2330 train_time:7215ms step_avg:38.17ms
step:190/2330 train_time:7256ms step_avg:38.19ms
step:191/2330 train_time:7292ms step_avg:38.18ms
step:192/2330 train_time:7333ms step_avg:38.19ms
step:193/2330 train_time:7368ms step_avg:38.18ms
step:194/2330 train_time:7409ms step_avg:38.19ms
step:195/2330 train_time:7444ms step_avg:38.18ms
step:196/2330 train_time:7486ms step_avg:38.19ms
step:197/2330 train_time:7521ms step_avg:38.18ms
step:198/2330 train_time:7562ms step_avg:38.19ms
step:199/2330 train_time:7597ms step_avg:38.18ms
step:200/2330 train_time:7639ms step_avg:38.19ms
step:201/2330 train_time:7674ms step_avg:38.18ms
step:202/2330 train_time:7715ms step_avg:38.19ms
step:203/2330 train_time:7751ms step_avg:38.18ms
step:204/2330 train_time:7791ms step_avg:38.19ms
step:205/2330 train_time:7827ms step_avg:38.18ms
step:206/2330 train_time:7868ms step_avg:38.20ms
step:207/2330 train_time:7903ms step_avg:38.18ms
step:208/2330 train_time:7945ms step_avg:38.20ms
step:209/2330 train_time:7980ms step_avg:38.18ms
step:210/2330 train_time:8021ms step_avg:38.20ms
step:211/2330 train_time:8057ms step_avg:38.19ms
step:212/2330 train_time:8098ms step_avg:38.20ms
step:213/2330 train_time:8134ms step_avg:38.19ms
step:214/2330 train_time:8175ms step_avg:38.20ms
step:215/2330 train_time:8211ms step_avg:38.19ms
step:216/2330 train_time:8252ms step_avg:38.20ms
step:217/2330 train_time:8288ms step_avg:38.19ms
step:218/2330 train_time:8329ms step_avg:38.20ms
step:219/2330 train_time:8364ms step_avg:38.19ms
step:220/2330 train_time:8405ms step_avg:38.20ms
step:221/2330 train_time:8440ms step_avg:38.19ms
step:222/2330 train_time:8481ms step_avg:38.20ms
step:223/2330 train_time:8516ms step_avg:38.19ms
step:224/2330 train_time:8557ms step_avg:38.20ms
step:225/2330 train_time:8593ms step_avg:38.19ms
step:226/2330 train_time:8634ms step_avg:38.20ms
step:227/2330 train_time:8669ms step_avg:38.19ms
step:228/2330 train_time:8710ms step_avg:38.20ms
step:229/2330 train_time:8746ms step_avg:38.19ms
step:230/2330 train_time:8787ms step_avg:38.20ms
step:231/2330 train_time:8822ms step_avg:38.19ms
step:232/2330 train_time:8863ms step_avg:38.20ms
step:233/2330 train_time:8898ms step_avg:38.19ms
step:234/2330 train_time:8940ms step_avg:38.21ms
step:235/2330 train_time:8975ms step_avg:38.19ms
step:236/2330 train_time:9016ms step_avg:38.21ms
step:237/2330 train_time:9052ms step_avg:38.20ms
step:238/2330 train_time:9093ms step_avg:38.20ms
step:239/2330 train_time:9129ms step_avg:38.20ms
step:240/2330 train_time:9170ms step_avg:38.21ms
step:241/2330 train_time:9206ms step_avg:38.20ms
step:242/2330 train_time:9247ms step_avg:38.21ms
step:243/2330 train_time:9282ms step_avg:38.20ms
step:244/2330 train_time:9323ms step_avg:38.21ms
step:245/2330 train_time:9358ms step_avg:38.20ms
step:246/2330 train_time:9400ms step_avg:38.21ms
step:247/2330 train_time:9435ms step_avg:38.20ms
step:248/2330 train_time:9476ms step_avg:38.21ms
step:249/2330 train_time:9512ms step_avg:38.20ms
step:250/2330 train_time:9553ms step_avg:38.21ms
step:250/2330 val_loss:5.5609 train_time:9665ms step_avg:38.66ms
step:251/2330 train_time:9676ms step_avg:38.55ms
step:252/2330 train_time:9688ms step_avg:38.44ms
step:253/2330 train_time:9697ms step_avg:38.33ms
step:254/2330 train_time:9709ms step_avg:38.22ms
step:255/2330 train_time:9742ms step_avg:38.20ms
step:256/2330 train_time:9782ms step_avg:38.21ms
step:257/2330 train_time:9817ms step_avg:38.20ms
step:258/2330 train_time:9858ms step_avg:38.21ms
step:259/2330 train_time:9892ms step_avg:38.19ms
step:260/2330 train_time:9933ms step_avg:38.20ms
step:261/2330 train_time:9970ms step_avg:38.20ms
step:262/2330 train_time:10012ms step_avg:38.21ms
step:263/2330 train_time:10056ms step_avg:38.23ms
step:264/2330 train_time:10097ms step_avg:38.25ms
step:265/2330 train_time:10133ms step_avg:38.24ms
step:266/2330 train_time:10174ms step_avg:38.25ms
step:267/2330 train_time:10210ms step_avg:38.24ms
step:268/2330 train_time:10251ms step_avg:38.25ms
step:269/2330 train_time:10287ms step_avg:38.24ms
step:270/2330 train_time:10328ms step_avg:38.25ms
step:271/2330 train_time:10363ms step_avg:38.24ms
step:272/2330 train_time:10404ms step_avg:38.25ms
step:273/2330 train_time:10439ms step_avg:38.24ms
step:274/2330 train_time:10479ms step_avg:38.25ms
step:275/2330 train_time:10515ms step_avg:38.23ms
step:276/2330 train_time:10555ms step_avg:38.24ms
step:277/2330 train_time:10592ms step_avg:38.24ms
step:278/2330 train_time:10634ms step_avg:38.25ms
step:279/2330 train_time:10670ms step_avg:38.24ms
step:280/2330 train_time:10712ms step_avg:38.26ms
step:281/2330 train_time:10747ms step_avg:38.25ms
step:282/2330 train_time:10789ms step_avg:38.26ms
step:283/2330 train_time:10823ms step_avg:38.24ms
step:284/2330 train_time:10864ms step_avg:38.25ms
step:285/2330 train_time:10900ms step_avg:38.25ms
step:286/2330 train_time:10941ms step_avg:38.26ms
step:287/2330 train_time:10980ms step_avg:38.26ms
step:288/2330 train_time:11021ms step_avg:38.27ms
step:289/2330 train_time:11059ms step_avg:38.27ms
step:290/2330 train_time:11100ms step_avg:38.28ms
step:291/2330 train_time:11138ms step_avg:38.27ms
step:292/2330 train_time:11178ms step_avg:38.28ms
step:293/2330 train_time:11215ms step_avg:38.27ms
step:294/2330 train_time:11255ms step_avg:38.28ms
step:295/2330 train_time:11290ms step_avg:38.27ms
step:296/2330 train_time:11331ms step_avg:38.28ms
step:297/2330 train_time:11367ms step_avg:38.27ms
step:298/2330 train_time:11408ms step_avg:38.28ms
step:299/2330 train_time:11442ms step_avg:38.27ms
step:300/2330 train_time:11482ms step_avg:38.27ms
step:301/2330 train_time:11518ms step_avg:38.27ms
step:302/2330 train_time:11559ms step_avg:38.27ms
step:303/2330 train_time:11595ms step_avg:38.27ms
step:304/2330 train_time:11636ms step_avg:38.28ms
step:305/2330 train_time:11672ms step_avg:38.27ms
step:306/2330 train_time:11713ms step_avg:38.28ms
step:307/2330 train_time:11748ms step_avg:38.27ms
step:308/2330 train_time:11789ms step_avg:38.28ms
step:309/2330 train_time:11824ms step_avg:38.27ms
step:310/2330 train_time:11866ms step_avg:38.28ms
step:311/2330 train_time:11902ms step_avg:38.27ms
step:312/2330 train_time:11943ms step_avg:38.28ms
step:313/2330 train_time:11980ms step_avg:38.27ms
step:314/2330 train_time:12021ms step_avg:38.28ms
step:315/2330 train_time:12058ms step_avg:38.28ms
step:316/2330 train_time:12099ms step_avg:38.29ms
step:317/2330 train_time:12136ms step_avg:38.28ms
step:318/2330 train_time:12177ms step_avg:38.29ms
step:319/2330 train_time:12212ms step_avg:38.28ms
step:320/2330 train_time:12253ms step_avg:38.29ms
step:321/2330 train_time:12289ms step_avg:38.28ms
step:322/2330 train_time:12330ms step_avg:38.29ms
step:323/2330 train_time:12365ms step_avg:38.28ms
step:324/2330 train_time:12406ms step_avg:38.29ms
step:325/2330 train_time:12442ms step_avg:38.28ms
step:326/2330 train_time:12482ms step_avg:38.29ms
step:327/2330 train_time:12517ms step_avg:38.28ms
step:328/2330 train_time:12558ms step_avg:38.29ms
step:329/2330 train_time:12594ms step_avg:38.28ms
step:330/2330 train_time:12634ms step_avg:38.29ms
step:331/2330 train_time:12670ms step_avg:38.28ms
step:332/2330 train_time:12711ms step_avg:38.29ms
step:333/2330 train_time:12747ms step_avg:38.28ms
step:334/2330 train_time:12788ms step_avg:38.29ms
step:335/2330 train_time:12824ms step_avg:38.28ms
step:336/2330 train_time:12866ms step_avg:38.29ms
step:337/2330 train_time:12901ms step_avg:38.28ms
step:338/2330 train_time:12942ms step_avg:38.29ms
step:339/2330 train_time:12979ms step_avg:38.29ms
step:340/2330 train_time:13020ms step_avg:38.29ms
step:341/2330 train_time:13056ms step_avg:38.29ms
step:342/2330 train_time:13097ms step_avg:38.30ms
step:343/2330 train_time:13133ms step_avg:38.29ms
step:344/2330 train_time:13173ms step_avg:38.29ms
step:345/2330 train_time:13210ms step_avg:38.29ms
step:346/2330 train_time:13251ms step_avg:38.30ms
step:347/2330 train_time:13286ms step_avg:38.29ms
step:348/2330 train_time:13328ms step_avg:38.30ms
step:349/2330 train_time:13363ms step_avg:38.29ms
step:350/2330 train_time:13403ms step_avg:38.30ms
step:351/2330 train_time:13440ms step_avg:38.29ms
step:352/2330 train_time:13481ms step_avg:38.30ms
step:353/2330 train_time:13516ms step_avg:38.29ms
step:354/2330 train_time:13557ms step_avg:38.30ms
step:355/2330 train_time:13592ms step_avg:38.29ms
step:356/2330 train_time:13633ms step_avg:38.30ms
step:357/2330 train_time:13669ms step_avg:38.29ms
step:358/2330 train_time:13710ms step_avg:38.30ms
step:359/2330 train_time:13746ms step_avg:38.29ms
step:360/2330 train_time:13787ms step_avg:38.30ms
step:361/2330 train_time:13823ms step_avg:38.29ms
step:362/2330 train_time:13864ms step_avg:38.30ms
step:363/2330 train_time:13900ms step_avg:38.29ms
step:364/2330 train_time:13941ms step_avg:38.30ms
step:365/2330 train_time:13977ms step_avg:38.29ms
step:366/2330 train_time:14019ms step_avg:38.30ms
step:367/2330 train_time:14054ms step_avg:38.29ms
step:368/2330 train_time:14095ms step_avg:38.30ms
step:369/2330 train_time:14131ms step_avg:38.29ms
step:370/2330 train_time:14172ms step_avg:38.30ms
step:371/2330 train_time:14208ms step_avg:38.30ms
step:372/2330 train_time:14250ms step_avg:38.31ms
step:373/2330 train_time:14285ms step_avg:38.30ms
step:374/2330 train_time:14326ms step_avg:38.31ms
step:375/2330 train_time:14362ms step_avg:38.30ms
step:376/2330 train_time:14403ms step_avg:38.30ms
step:377/2330 train_time:14439ms step_avg:38.30ms
step:378/2330 train_time:14480ms step_avg:38.31ms
step:379/2330 train_time:14516ms step_avg:38.30ms
step:380/2330 train_time:14557ms step_avg:38.31ms
step:381/2330 train_time:14592ms step_avg:38.30ms
step:382/2330 train_time:14634ms step_avg:38.31ms
step:383/2330 train_time:14668ms step_avg:38.30ms
step:384/2330 train_time:14710ms step_avg:38.31ms
step:385/2330 train_time:14745ms step_avg:38.30ms
step:386/2330 train_time:14786ms step_avg:38.31ms
step:387/2330 train_time:14821ms step_avg:38.30ms
step:388/2330 train_time:14862ms step_avg:38.30ms
step:389/2330 train_time:14898ms step_avg:38.30ms
step:390/2330 train_time:14940ms step_avg:38.31ms
step:391/2330 train_time:14976ms step_avg:38.30ms
step:392/2330 train_time:15017ms step_avg:38.31ms
step:393/2330 train_time:15053ms step_avg:38.30ms
step:394/2330 train_time:15094ms step_avg:38.31ms
step:395/2330 train_time:15130ms step_avg:38.30ms
step:396/2330 train_time:15171ms step_avg:38.31ms
step:397/2330 train_time:15206ms step_avg:38.30ms
step:398/2330 train_time:15248ms step_avg:38.31ms
step:399/2330 train_time:15283ms step_avg:38.30ms
step:400/2330 train_time:15324ms step_avg:38.31ms
step:401/2330 train_time:15359ms step_avg:38.30ms
step:402/2330 train_time:15400ms step_avg:38.31ms
step:403/2330 train_time:15436ms step_avg:38.30ms
step:404/2330 train_time:15478ms step_avg:38.31ms
step:405/2330 train_time:15512ms step_avg:38.30ms
step:406/2330 train_time:15553ms step_avg:38.31ms
step:407/2330 train_time:15589ms step_avg:38.30ms
step:408/2330 train_time:15630ms step_avg:38.31ms
step:409/2330 train_time:15665ms step_avg:38.30ms
step:410/2330 train_time:15707ms step_avg:38.31ms
step:411/2330 train_time:15743ms step_avg:38.30ms
step:412/2330 train_time:15783ms step_avg:38.31ms
step:413/2330 train_time:15819ms step_avg:38.30ms
step:414/2330 train_time:15861ms step_avg:38.31ms
step:415/2330 train_time:15896ms step_avg:38.30ms
step:416/2330 train_time:15938ms step_avg:38.31ms
step:417/2330 train_time:15973ms step_avg:38.31ms
step:418/2330 train_time:16014ms step_avg:38.31ms
step:419/2330 train_time:16050ms step_avg:38.31ms
step:420/2330 train_time:16092ms step_avg:38.31ms
step:421/2330 train_time:16128ms step_avg:38.31ms
step:422/2330 train_time:16169ms step_avg:38.32ms
step:423/2330 train_time:16205ms step_avg:38.31ms
step:424/2330 train_time:16246ms step_avg:38.32ms
step:425/2330 train_time:16282ms step_avg:38.31ms
step:426/2330 train_time:16322ms step_avg:38.32ms
step:427/2330 train_time:16358ms step_avg:38.31ms
step:428/2330 train_time:16400ms step_avg:38.32ms
step:429/2330 train_time:16435ms step_avg:38.31ms
step:430/2330 train_time:16476ms step_avg:38.32ms
step:431/2330 train_time:16511ms step_avg:38.31ms
step:432/2330 train_time:16553ms step_avg:38.32ms
step:433/2330 train_time:16588ms step_avg:38.31ms
step:434/2330 train_time:16630ms step_avg:38.32ms
step:435/2330 train_time:16665ms step_avg:38.31ms
step:436/2330 train_time:16706ms step_avg:38.32ms
step:437/2330 train_time:16742ms step_avg:38.31ms
step:438/2330 train_time:16783ms step_avg:38.32ms
step:439/2330 train_time:16820ms step_avg:38.31ms
step:440/2330 train_time:16860ms step_avg:38.32ms
step:441/2330 train_time:16897ms step_avg:38.31ms
step:442/2330 train_time:16938ms step_avg:38.32ms
step:443/2330 train_time:16973ms step_avg:38.31ms
step:444/2330 train_time:17014ms step_avg:38.32ms
step:445/2330 train_time:17051ms step_avg:38.32ms
step:446/2330 train_time:17092ms step_avg:38.32ms
step:447/2330 train_time:17127ms step_avg:38.32ms
step:448/2330 train_time:17168ms step_avg:38.32ms
step:449/2330 train_time:17205ms step_avg:38.32ms
step:450/2330 train_time:17246ms step_avg:38.33ms
step:451/2330 train_time:17282ms step_avg:38.32ms
step:452/2330 train_time:17322ms step_avg:38.32ms
step:453/2330 train_time:17358ms step_avg:38.32ms
step:454/2330 train_time:17400ms step_avg:38.33ms
step:455/2330 train_time:17435ms step_avg:38.32ms
step:456/2330 train_time:17476ms step_avg:38.32ms
step:457/2330 train_time:17511ms step_avg:38.32ms
step:458/2330 train_time:17552ms step_avg:38.32ms
step:459/2330 train_time:17588ms step_avg:38.32ms
step:460/2330 train_time:17630ms step_avg:38.33ms
step:461/2330 train_time:17666ms step_avg:38.32ms
step:462/2330 train_time:17707ms step_avg:38.33ms
step:463/2330 train_time:17743ms step_avg:38.32ms
step:464/2330 train_time:17783ms step_avg:38.33ms
step:465/2330 train_time:17820ms step_avg:38.32ms
step:466/2330 train_time:17861ms step_avg:38.33ms
step:467/2330 train_time:17897ms step_avg:38.32ms
step:468/2330 train_time:17939ms step_avg:38.33ms
step:469/2330 train_time:17974ms step_avg:38.32ms
step:470/2330 train_time:18015ms step_avg:38.33ms
step:471/2330 train_time:18050ms step_avg:38.32ms
step:472/2330 train_time:18091ms step_avg:38.33ms
step:473/2330 train_time:18127ms step_avg:38.32ms
step:474/2330 train_time:18168ms step_avg:38.33ms
step:475/2330 train_time:18204ms step_avg:38.32ms
step:476/2330 train_time:18245ms step_avg:38.33ms
step:477/2330 train_time:18282ms step_avg:38.33ms
step:478/2330 train_time:18322ms step_avg:38.33ms
step:479/2330 train_time:18359ms step_avg:38.33ms
step:480/2330 train_time:18400ms step_avg:38.33ms
step:481/2330 train_time:18436ms step_avg:38.33ms
step:482/2330 train_time:18477ms step_avg:38.33ms
step:483/2330 train_time:18512ms step_avg:38.33ms
step:484/2330 train_time:18553ms step_avg:38.33ms
step:485/2330 train_time:18589ms step_avg:38.33ms
step:486/2330 train_time:18630ms step_avg:38.33ms
step:487/2330 train_time:18666ms step_avg:38.33ms
step:488/2330 train_time:18707ms step_avg:38.33ms
step:489/2330 train_time:18742ms step_avg:38.33ms
step:490/2330 train_time:18783ms step_avg:38.33ms
step:491/2330 train_time:18819ms step_avg:38.33ms
step:492/2330 train_time:18860ms step_avg:38.33ms
step:493/2330 train_time:18895ms step_avg:38.33ms
step:494/2330 train_time:18937ms step_avg:38.33ms
step:495/2330 train_time:18972ms step_avg:38.33ms
step:496/2330 train_time:19013ms step_avg:38.33ms
step:497/2330 train_time:19050ms step_avg:38.33ms
step:498/2330 train_time:19091ms step_avg:38.34ms
step:499/2330 train_time:19127ms step_avg:38.33ms
step:500/2330 train_time:19168ms step_avg:38.34ms
step:500/2330 val_loss:5.4171 train_time:19282ms step_avg:38.56ms
step:501/2330 train_time:19294ms step_avg:38.51ms
step:502/2330 train_time:19304ms step_avg:38.45ms
step:503/2330 train_time:19313ms step_avg:38.40ms
step:504/2330 train_time:19324ms step_avg:38.34ms
step:505/2330 train_time:19360ms step_avg:38.34ms
step:506/2330 train_time:19401ms step_avg:38.34ms
step:507/2330 train_time:19435ms step_avg:38.33ms
step:508/2330 train_time:19476ms step_avg:38.34ms
step:509/2330 train_time:19511ms step_avg:38.33ms
step:510/2330 train_time:19551ms step_avg:38.34ms
step:511/2330 train_time:19589ms step_avg:38.33ms
step:512/2330 train_time:19630ms step_avg:38.34ms
step:513/2330 train_time:19671ms step_avg:38.35ms
step:514/2330 train_time:19714ms step_avg:38.35ms
step:515/2330 train_time:19749ms step_avg:38.35ms
step:516/2330 train_time:19790ms step_avg:38.35ms
step:517/2330 train_time:19826ms step_avg:38.35ms
step:518/2330 train_time:19867ms step_avg:38.35ms
step:519/2330 train_time:19902ms step_avg:38.35ms
step:520/2330 train_time:19943ms step_avg:38.35ms
step:521/2330 train_time:19977ms step_avg:38.34ms
step:522/2330 train_time:20018ms step_avg:38.35ms
step:523/2330 train_time:20054ms step_avg:38.34ms
step:524/2330 train_time:20095ms step_avg:38.35ms
step:525/2330 train_time:20130ms step_avg:38.34ms
step:526/2330 train_time:20171ms step_avg:38.35ms
step:527/2330 train_time:20207ms step_avg:38.34ms
step:528/2330 train_time:20248ms step_avg:38.35ms
step:529/2330 train_time:20285ms step_avg:38.35ms
step:530/2330 train_time:20325ms step_avg:38.35ms
step:531/2330 train_time:20361ms step_avg:38.35ms
step:532/2330 train_time:20402ms step_avg:38.35ms
step:533/2330 train_time:20437ms step_avg:38.34ms
step:534/2330 train_time:20478ms step_avg:38.35ms
step:535/2330 train_time:20514ms step_avg:38.34ms
step:536/2330 train_time:20556ms step_avg:38.35ms
step:537/2330 train_time:20593ms step_avg:38.35ms
step:538/2330 train_time:20635ms step_avg:38.35ms
step:539/2330 train_time:20672ms step_avg:38.35ms
step:540/2330 train_time:20714ms step_avg:38.36ms
step:541/2330 train_time:20750ms step_avg:38.36ms
step:542/2330 train_time:20791ms step_avg:38.36ms
step:543/2330 train_time:20828ms step_avg:38.36ms
step:544/2330 train_time:20869ms step_avg:38.36ms
step:545/2330 train_time:20905ms step_avg:38.36ms
step:546/2330 train_time:20945ms step_avg:38.36ms
step:547/2330 train_time:20980ms step_avg:38.36ms
step:548/2330 train_time:21021ms step_avg:38.36ms
step:549/2330 train_time:21057ms step_avg:38.35ms
step:550/2330 train_time:21098ms step_avg:38.36ms
step:551/2330 train_time:21134ms step_avg:38.36ms
step:552/2330 train_time:21175ms step_avg:38.36ms
step:553/2330 train_time:21210ms step_avg:38.35ms
step:554/2330 train_time:21251ms step_avg:38.36ms
step:555/2330 train_time:21287ms step_avg:38.36ms
step:556/2330 train_time:21328ms step_avg:38.36ms
step:557/2330 train_time:21364ms step_avg:38.35ms
step:558/2330 train_time:21404ms step_avg:38.36ms
step:559/2330 train_time:21440ms step_avg:38.35ms
step:560/2330 train_time:21481ms step_avg:38.36ms
step:561/2330 train_time:21518ms step_avg:38.36ms
step:562/2330 train_time:21559ms step_avg:38.36ms
step:563/2330 train_time:21596ms step_avg:38.36ms
step:564/2330 train_time:21638ms step_avg:38.37ms
step:565/2330 train_time:21674ms step_avg:38.36ms
step:566/2330 train_time:21716ms step_avg:38.37ms
step:567/2330 train_time:21752ms step_avg:38.36ms
step:568/2330 train_time:21794ms step_avg:38.37ms
step:569/2330 train_time:21829ms step_avg:38.36ms
step:570/2330 train_time:21870ms step_avg:38.37ms
step:571/2330 train_time:21907ms step_avg:38.37ms
step:572/2330 train_time:21947ms step_avg:38.37ms
step:573/2330 train_time:21983ms step_avg:38.36ms
step:574/2330 train_time:22023ms step_avg:38.37ms
step:575/2330 train_time:22059ms step_avg:38.36ms
step:576/2330 train_time:22100ms step_avg:38.37ms
step:577/2330 train_time:22136ms step_avg:38.36ms
step:578/2330 train_time:22177ms step_avg:38.37ms
step:579/2330 train_time:22213ms step_avg:38.36ms
step:580/2330 train_time:22254ms step_avg:38.37ms
step:581/2330 train_time:22290ms step_avg:38.36ms
step:582/2330 train_time:22331ms step_avg:38.37ms
step:583/2330 train_time:22367ms step_avg:38.37ms
step:584/2330 train_time:22408ms step_avg:38.37ms
step:585/2330 train_time:22445ms step_avg:38.37ms
step:586/2330 train_time:22485ms step_avg:38.37ms
step:587/2330 train_time:22522ms step_avg:38.37ms
step:588/2330 train_time:22563ms step_avg:38.37ms
step:589/2330 train_time:22599ms step_avg:38.37ms
step:590/2330 train_time:22641ms step_avg:38.37ms
step:591/2330 train_time:22676ms step_avg:38.37ms
step:592/2330 train_time:22718ms step_avg:38.38ms
step:593/2330 train_time:22754ms step_avg:38.37ms
step:594/2330 train_time:22796ms step_avg:38.38ms
step:595/2330 train_time:22832ms step_avg:38.37ms
step:596/2330 train_time:22873ms step_avg:38.38ms
step:597/2330 train_time:22909ms step_avg:38.37ms
step:598/2330 train_time:22950ms step_avg:38.38ms
step:599/2330 train_time:22986ms step_avg:38.37ms
step:600/2330 train_time:23027ms step_avg:38.38ms
step:601/2330 train_time:23063ms step_avg:38.37ms
step:602/2330 train_time:23104ms step_avg:38.38ms
step:603/2330 train_time:23139ms step_avg:38.37ms
step:604/2330 train_time:23180ms step_avg:38.38ms
step:605/2330 train_time:23216ms step_avg:38.37ms
step:606/2330 train_time:23258ms step_avg:38.38ms
step:607/2330 train_time:23294ms step_avg:38.38ms
step:608/2330 train_time:23336ms step_avg:38.38ms
step:609/2330 train_time:23371ms step_avg:38.38ms
step:610/2330 train_time:23412ms step_avg:38.38ms
step:611/2330 train_time:23448ms step_avg:38.38ms
step:612/2330 train_time:23489ms step_avg:38.38ms
step:613/2330 train_time:23525ms step_avg:38.38ms
step:614/2330 train_time:23566ms step_avg:38.38ms
step:615/2330 train_time:23603ms step_avg:38.38ms
step:616/2330 train_time:23644ms step_avg:38.38ms
step:617/2330 train_time:23680ms step_avg:38.38ms
step:618/2330 train_time:23721ms step_avg:38.38ms
step:619/2330 train_time:23757ms step_avg:38.38ms
step:620/2330 train_time:23798ms step_avg:38.38ms
step:621/2330 train_time:23834ms step_avg:38.38ms
step:622/2330 train_time:23876ms step_avg:38.39ms
step:623/2330 train_time:23911ms step_avg:38.38ms
step:624/2330 train_time:23953ms step_avg:38.39ms
step:625/2330 train_time:23989ms step_avg:38.38ms
step:626/2330 train_time:24030ms step_avg:38.39ms
step:627/2330 train_time:24066ms step_avg:38.38ms
step:628/2330 train_time:24107ms step_avg:38.39ms
step:629/2330 train_time:24142ms step_avg:38.38ms
step:630/2330 train_time:24183ms step_avg:38.39ms
step:631/2330 train_time:24220ms step_avg:38.38ms
step:632/2330 train_time:24261ms step_avg:38.39ms
step:633/2330 train_time:24296ms step_avg:38.38ms
step:634/2330 train_time:24338ms step_avg:38.39ms
step:635/2330 train_time:24374ms step_avg:38.38ms
step:636/2330 train_time:24416ms step_avg:38.39ms
step:637/2330 train_time:24451ms step_avg:38.38ms
step:638/2330 train_time:24492ms step_avg:38.39ms
step:639/2330 train_time:24529ms step_avg:38.39ms
step:640/2330 train_time:24570ms step_avg:38.39ms
step:641/2330 train_time:24607ms step_avg:38.39ms
step:642/2330 train_time:24648ms step_avg:38.39ms
step:643/2330 train_time:24685ms step_avg:38.39ms
step:644/2330 train_time:24726ms step_avg:38.39ms
step:645/2330 train_time:24761ms step_avg:38.39ms
step:646/2330 train_time:24802ms step_avg:38.39ms
step:647/2330 train_time:24838ms step_avg:38.39ms
step:648/2330 train_time:24879ms step_avg:38.39ms
step:649/2330 train_time:24915ms step_avg:38.39ms
step:650/2330 train_time:24957ms step_avg:38.39ms
step:651/2330 train_time:24992ms step_avg:38.39ms
step:652/2330 train_time:25033ms step_avg:38.39ms
step:653/2330 train_time:25069ms step_avg:38.39ms
step:654/2330 train_time:25110ms step_avg:38.39ms
step:655/2330 train_time:25146ms step_avg:38.39ms
step:656/2330 train_time:25187ms step_avg:38.40ms
step:657/2330 train_time:25223ms step_avg:38.39ms
step:658/2330 train_time:25264ms step_avg:38.39ms
step:659/2330 train_time:25299ms step_avg:38.39ms
step:660/2330 train_time:25340ms step_avg:38.39ms
step:661/2330 train_time:25376ms step_avg:38.39ms
step:662/2330 train_time:25417ms step_avg:38.39ms
step:663/2330 train_time:25454ms step_avg:38.39ms
step:664/2330 train_time:25495ms step_avg:38.40ms
step:665/2330 train_time:25532ms step_avg:38.39ms
step:666/2330 train_time:25573ms step_avg:38.40ms
step:667/2330 train_time:25608ms step_avg:38.39ms
step:668/2330 train_time:25649ms step_avg:38.40ms
step:669/2330 train_time:25686ms step_avg:38.39ms
step:670/2330 train_time:25727ms step_avg:38.40ms
step:671/2330 train_time:25763ms step_avg:38.40ms
step:672/2330 train_time:25805ms step_avg:38.40ms
step:673/2330 train_time:25840ms step_avg:38.39ms
step:674/2330 train_time:25881ms step_avg:38.40ms
step:675/2330 train_time:25917ms step_avg:38.40ms
step:676/2330 train_time:25958ms step_avg:38.40ms
step:677/2330 train_time:25994ms step_avg:38.40ms
step:678/2330 train_time:26036ms step_avg:38.40ms
step:679/2330 train_time:26071ms step_avg:38.40ms
step:680/2330 train_time:26112ms step_avg:38.40ms
step:681/2330 train_time:26149ms step_avg:38.40ms
step:682/2330 train_time:26189ms step_avg:38.40ms
step:683/2330 train_time:26225ms step_avg:38.40ms
step:684/2330 train_time:26266ms step_avg:38.40ms
step:685/2330 train_time:26301ms step_avg:38.40ms
step:686/2330 train_time:26342ms step_avg:38.40ms
step:687/2330 train_time:26378ms step_avg:38.40ms
step:688/2330 train_time:26419ms step_avg:38.40ms
step:689/2330 train_time:26455ms step_avg:38.40ms
step:690/2330 train_time:26497ms step_avg:38.40ms
step:691/2330 train_time:26533ms step_avg:38.40ms
step:692/2330 train_time:26574ms step_avg:38.40ms
step:693/2330 train_time:26611ms step_avg:38.40ms
step:694/2330 train_time:26652ms step_avg:38.40ms
step:695/2330 train_time:26689ms step_avg:38.40ms
step:696/2330 train_time:26730ms step_avg:38.40ms
step:697/2330 train_time:26767ms step_avg:38.40ms
step:698/2330 train_time:26808ms step_avg:38.41ms
step:699/2330 train_time:26845ms step_avg:38.40ms
step:700/2330 train_time:26885ms step_avg:38.41ms
step:701/2330 train_time:26921ms step_avg:38.40ms
step:702/2330 train_time:26962ms step_avg:38.41ms
step:703/2330 train_time:26997ms step_avg:38.40ms
step:704/2330 train_time:27039ms step_avg:38.41ms
step:705/2330 train_time:27074ms step_avg:38.40ms
step:706/2330 train_time:27116ms step_avg:38.41ms
step:707/2330 train_time:27151ms step_avg:38.40ms
step:708/2330 train_time:27192ms step_avg:38.41ms
step:709/2330 train_time:27228ms step_avg:38.40ms
step:710/2330 train_time:27269ms step_avg:38.41ms
step:711/2330 train_time:27305ms step_avg:38.40ms
step:712/2330 train_time:27346ms step_avg:38.41ms
step:713/2330 train_time:27381ms step_avg:38.40ms
step:714/2330 train_time:27422ms step_avg:38.41ms
step:715/2330 train_time:27458ms step_avg:38.40ms
step:716/2330 train_time:27499ms step_avg:38.41ms
step:717/2330 train_time:27535ms step_avg:38.40ms
step:718/2330 train_time:27576ms step_avg:38.41ms
step:719/2330 train_time:27612ms step_avg:38.40ms
step:720/2330 train_time:27654ms step_avg:38.41ms
step:721/2330 train_time:27690ms step_avg:38.40ms
step:722/2330 train_time:27731ms step_avg:38.41ms
step:723/2330 train_time:27768ms step_avg:38.41ms
step:724/2330 train_time:27808ms step_avg:38.41ms
step:725/2330 train_time:27846ms step_avg:38.41ms
step:726/2330 train_time:27886ms step_avg:38.41ms
step:727/2330 train_time:27922ms step_avg:38.41ms
step:728/2330 train_time:27963ms step_avg:38.41ms
step:729/2330 train_time:27998ms step_avg:38.41ms
step:730/2330 train_time:28040ms step_avg:38.41ms
step:731/2330 train_time:28075ms step_avg:38.41ms
step:732/2330 train_time:28116ms step_avg:38.41ms
step:733/2330 train_time:28152ms step_avg:38.41ms
step:734/2330 train_time:28194ms step_avg:38.41ms
step:735/2330 train_time:28230ms step_avg:38.41ms
step:736/2330 train_time:28271ms step_avg:38.41ms
step:737/2330 train_time:28308ms step_avg:38.41ms
step:738/2330 train_time:28349ms step_avg:38.41ms
step:739/2330 train_time:28385ms step_avg:38.41ms
step:740/2330 train_time:28426ms step_avg:38.41ms
step:741/2330 train_time:28462ms step_avg:38.41ms
step:742/2330 train_time:28503ms step_avg:38.41ms
step:743/2330 train_time:28538ms step_avg:38.41ms
step:744/2330 train_time:28579ms step_avg:38.41ms
step:745/2330 train_time:28615ms step_avg:38.41ms
step:746/2330 train_time:28657ms step_avg:38.41ms
step:747/2330 train_time:28692ms step_avg:38.41ms
step:748/2330 train_time:28734ms step_avg:38.41ms
step:749/2330 train_time:28770ms step_avg:38.41ms
step:750/2330 train_time:28812ms step_avg:38.42ms
step:750/2330 val_loss:5.3575 train_time:28925ms step_avg:38.57ms
step:751/2330 train_time:28936ms step_avg:38.53ms
step:752/2330 train_time:28947ms step_avg:38.49ms
step:753/2330 train_time:28956ms step_avg:38.45ms
step:754/2330 train_time:28968ms step_avg:38.42ms
step:755/2330 train_time:29003ms step_avg:38.41ms
step:756/2330 train_time:29044ms step_avg:38.42ms
step:757/2330 train_time:29079ms step_avg:38.41ms
step:758/2330 train_time:29119ms step_avg:38.42ms
step:759/2330 train_time:29154ms step_avg:38.41ms
step:760/2330 train_time:29195ms step_avg:38.41ms
step:761/2330 train_time:29233ms step_avg:38.41ms
step:762/2330 train_time:29275ms step_avg:38.42ms
step:763/2330 train_time:29316ms step_avg:38.42ms
step:764/2330 train_time:29357ms step_avg:38.42ms
step:765/2330 train_time:29395ms step_avg:38.42ms
step:766/2330 train_time:29437ms step_avg:38.43ms
step:767/2330 train_time:29473ms step_avg:38.43ms
step:768/2330 train_time:29513ms step_avg:38.43ms
step:769/2330 train_time:29548ms step_avg:38.42ms
step:770/2330 train_time:29590ms step_avg:38.43ms
step:771/2330 train_time:29624ms step_avg:38.42ms
step:772/2330 train_time:29665ms step_avg:38.43ms
step:773/2330 train_time:29700ms step_avg:38.42ms
step:774/2330 train_time:29741ms step_avg:38.43ms
step:775/2330 train_time:29776ms step_avg:38.42ms
step:776/2330 train_time:29817ms step_avg:38.42ms
step:777/2330 train_time:29852ms step_avg:38.42ms
step:778/2330 train_time:29894ms step_avg:38.42ms
step:779/2330 train_time:29929ms step_avg:38.42ms
step:780/2330 train_time:29969ms step_avg:38.42ms
step:781/2330 train_time:30005ms step_avg:38.42ms
step:782/2330 train_time:30046ms step_avg:38.42ms
step:783/2330 train_time:30082ms step_avg:38.42ms
step:784/2330 train_time:30123ms step_avg:38.42ms
step:785/2330 train_time:30158ms step_avg:38.42ms
step:786/2330 train_time:30199ms step_avg:38.42ms
step:787/2330 train_time:30239ms step_avg:38.42ms
step:788/2330 train_time:30281ms step_avg:38.43ms
step:789/2330 train_time:30318ms step_avg:38.43ms
step:790/2330 train_time:30359ms step_avg:38.43ms
step:791/2330 train_time:30398ms step_avg:38.43ms
step:792/2330 train_time:30438ms step_avg:38.43ms
step:793/2330 train_time:30477ms step_avg:38.43ms
step:794/2330 train_time:30518ms step_avg:38.44ms
step:795/2330 train_time:30555ms step_avg:38.43ms
step:796/2330 train_time:30595ms step_avg:38.44ms
step:797/2330 train_time:30632ms step_avg:38.43ms
step:798/2330 train_time:30672ms step_avg:38.44ms
step:799/2330 train_time:30708ms step_avg:38.43ms
step:800/2330 train_time:30748ms step_avg:38.44ms
step:801/2330 train_time:30784ms step_avg:38.43ms
step:802/2330 train_time:30825ms step_avg:38.43ms
step:803/2330 train_time:30861ms step_avg:38.43ms
step:804/2330 train_time:30902ms step_avg:38.44ms
step:805/2330 train_time:30937ms step_avg:38.43ms
step:806/2330 train_time:30977ms step_avg:38.43ms
step:807/2330 train_time:31014ms step_avg:38.43ms
step:808/2330 train_time:31055ms step_avg:38.43ms
step:809/2330 train_time:31091ms step_avg:38.43ms
step:810/2330 train_time:31131ms step_avg:38.43ms
step:811/2330 train_time:31168ms step_avg:38.43ms
step:812/2330 train_time:31209ms step_avg:38.43ms
step:813/2330 train_time:31246ms step_avg:38.43ms
step:814/2330 train_time:31288ms step_avg:38.44ms
step:815/2330 train_time:31324ms step_avg:38.43ms
step:816/2330 train_time:31366ms step_avg:38.44ms
step:817/2330 train_time:31402ms step_avg:38.44ms
step:818/2330 train_time:31444ms step_avg:38.44ms
step:819/2330 train_time:31480ms step_avg:38.44ms
step:820/2330 train_time:31522ms step_avg:38.44ms
step:821/2330 train_time:31558ms step_avg:38.44ms
step:822/2330 train_time:31599ms step_avg:38.44ms
step:823/2330 train_time:31636ms step_avg:38.44ms
step:824/2330 train_time:31676ms step_avg:38.44ms
step:825/2330 train_time:31712ms step_avg:38.44ms
step:826/2330 train_time:31753ms step_avg:38.44ms
step:827/2330 train_time:31788ms step_avg:38.44ms
step:828/2330 train_time:31829ms step_avg:38.44ms
step:829/2330 train_time:31864ms step_avg:38.44ms
step:830/2330 train_time:31905ms step_avg:38.44ms
step:831/2330 train_time:31942ms step_avg:38.44ms
step:832/2330 train_time:31982ms step_avg:38.44ms
step:833/2330 train_time:32018ms step_avg:38.44ms
step:834/2330 train_time:32058ms step_avg:38.44ms
step:835/2330 train_time:32095ms step_avg:38.44ms
step:836/2330 train_time:32136ms step_avg:38.44ms
step:837/2330 train_time:32172ms step_avg:38.44ms
step:838/2330 train_time:32213ms step_avg:38.44ms
step:839/2330 train_time:32249ms step_avg:38.44ms
step:840/2330 train_time:32289ms step_avg:38.44ms
step:841/2330 train_time:32327ms step_avg:38.44ms
step:842/2330 train_time:32368ms step_avg:38.44ms
step:843/2330 train_time:32404ms step_avg:38.44ms
step:844/2330 train_time:32446ms step_avg:38.44ms
step:845/2330 train_time:32482ms step_avg:38.44ms
step:846/2330 train_time:32524ms step_avg:38.44ms
step:847/2330 train_time:32560ms step_avg:38.44ms
step:848/2330 train_time:32601ms step_avg:38.44ms
step:849/2330 train_time:32637ms step_avg:38.44ms
step:850/2330 train_time:32678ms step_avg:38.44ms
step:851/2330 train_time:32714ms step_avg:38.44ms
step:852/2330 train_time:32755ms step_avg:38.44ms
step:853/2330 train_time:32790ms step_avg:38.44ms
step:854/2330 train_time:32831ms step_avg:38.44ms
step:855/2330 train_time:32867ms step_avg:38.44ms
step:856/2330 train_time:32908ms step_avg:38.44ms
step:857/2330 train_time:32944ms step_avg:38.44ms
step:858/2330 train_time:32985ms step_avg:38.44ms
step:859/2330 train_time:33022ms step_avg:38.44ms
step:860/2330 train_time:33063ms step_avg:38.45ms
step:861/2330 train_time:33099ms step_avg:38.44ms
step:862/2330 train_time:33139ms step_avg:38.44ms
step:863/2330 train_time:33176ms step_avg:38.44ms
step:864/2330 train_time:33217ms step_avg:38.45ms
step:865/2330 train_time:33254ms step_avg:38.44ms
step:866/2330 train_time:33295ms step_avg:38.45ms
step:867/2330 train_time:33332ms step_avg:38.45ms
step:868/2330 train_time:33373ms step_avg:38.45ms
step:869/2330 train_time:33408ms step_avg:38.44ms
step:870/2330 train_time:33449ms step_avg:38.45ms
step:871/2330 train_time:33486ms step_avg:38.45ms
step:872/2330 train_time:33527ms step_avg:38.45ms
step:873/2330 train_time:33563ms step_avg:38.45ms
step:874/2330 train_time:33605ms step_avg:38.45ms
step:875/2330 train_time:33640ms step_avg:38.45ms
step:876/2330 train_time:33682ms step_avg:38.45ms
step:877/2330 train_time:33717ms step_avg:38.45ms
step:878/2330 train_time:33757ms step_avg:38.45ms
step:879/2330 train_time:33793ms step_avg:38.45ms
step:880/2330 train_time:33834ms step_avg:38.45ms
step:881/2330 train_time:33870ms step_avg:38.44ms
step:882/2330 train_time:33911ms step_avg:38.45ms
step:883/2330 train_time:33947ms step_avg:38.44ms
step:884/2330 train_time:33988ms step_avg:38.45ms
step:885/2330 train_time:34023ms step_avg:38.44ms
step:886/2330 train_time:34065ms step_avg:38.45ms
step:887/2330 train_time:34100ms step_avg:38.44ms
step:888/2330 train_time:34142ms step_avg:38.45ms
step:889/2330 train_time:34178ms step_avg:38.45ms
step:890/2330 train_time:34219ms step_avg:38.45ms
step:891/2330 train_time:34257ms step_avg:38.45ms
step:892/2330 train_time:34298ms step_avg:38.45ms
step:893/2330 train_time:34334ms step_avg:38.45ms
step:894/2330 train_time:34375ms step_avg:38.45ms
step:895/2330 train_time:34411ms step_avg:38.45ms
step:896/2330 train_time:34452ms step_avg:38.45ms
step:897/2330 train_time:34488ms step_avg:38.45ms
step:898/2330 train_time:34529ms step_avg:38.45ms
step:899/2330 train_time:34565ms step_avg:38.45ms
step:900/2330 train_time:34606ms step_avg:38.45ms
step:901/2330 train_time:34643ms step_avg:38.45ms
step:902/2330 train_time:34685ms step_avg:38.45ms
step:903/2330 train_time:34721ms step_avg:38.45ms
step:904/2330 train_time:34762ms step_avg:38.45ms
step:905/2330 train_time:34798ms step_avg:38.45ms
step:906/2330 train_time:34838ms step_avg:38.45ms
step:907/2330 train_time:34875ms step_avg:38.45ms
step:908/2330 train_time:34916ms step_avg:38.45ms
step:909/2330 train_time:34951ms step_avg:38.45ms
step:910/2330 train_time:34992ms step_avg:38.45ms
step:911/2330 train_time:35027ms step_avg:38.45ms
step:912/2330 train_time:35068ms step_avg:38.45ms
step:913/2330 train_time:35104ms step_avg:38.45ms
step:914/2330 train_time:35145ms step_avg:38.45ms
step:915/2330 train_time:35181ms step_avg:38.45ms
step:916/2330 train_time:35223ms step_avg:38.45ms
step:917/2330 train_time:35259ms step_avg:38.45ms
step:918/2330 train_time:35300ms step_avg:38.45ms
step:919/2330 train_time:35336ms step_avg:38.45ms
step:920/2330 train_time:35377ms step_avg:38.45ms
step:921/2330 train_time:35414ms step_avg:38.45ms
step:922/2330 train_time:35455ms step_avg:38.45ms
step:923/2330 train_time:35491ms step_avg:38.45ms
step:924/2330 train_time:35532ms step_avg:38.45ms
step:925/2330 train_time:35567ms step_avg:38.45ms
step:926/2330 train_time:35609ms step_avg:38.45ms
step:927/2330 train_time:35644ms step_avg:38.45ms
step:928/2330 train_time:35686ms step_avg:38.45ms
step:929/2330 train_time:35722ms step_avg:38.45ms
step:930/2330 train_time:35764ms step_avg:38.46ms
step:931/2330 train_time:35799ms step_avg:38.45ms
step:932/2330 train_time:35841ms step_avg:38.46ms
step:933/2330 train_time:35876ms step_avg:38.45ms
step:934/2330 train_time:35917ms step_avg:38.45ms
step:935/2330 train_time:35953ms step_avg:38.45ms
step:936/2330 train_time:35994ms step_avg:38.46ms
step:937/2330 train_time:36030ms step_avg:38.45ms
step:938/2330 train_time:36071ms step_avg:38.46ms
step:939/2330 train_time:36106ms step_avg:38.45ms
step:940/2330 train_time:36147ms step_avg:38.45ms
step:941/2330 train_time:36184ms step_avg:38.45ms
step:942/2330 train_time:36225ms step_avg:38.46ms
step:943/2330 train_time:36261ms step_avg:38.45ms
step:944/2330 train_time:36302ms step_avg:38.46ms
step:945/2330 train_time:36338ms step_avg:38.45ms
step:946/2330 train_time:36379ms step_avg:38.46ms
step:947/2330 train_time:36416ms step_avg:38.45ms
step:948/2330 train_time:36457ms step_avg:38.46ms
step:949/2330 train_time:36493ms step_avg:38.45ms
step:950/2330 train_time:36534ms step_avg:38.46ms
step:951/2330 train_time:36570ms step_avg:38.45ms
step:952/2330 train_time:36611ms step_avg:38.46ms
step:953/2330 train_time:36647ms step_avg:38.45ms
step:954/2330 train_time:36690ms step_avg:38.46ms
step:955/2330 train_time:36725ms step_avg:38.46ms
step:956/2330 train_time:36766ms step_avg:38.46ms
step:957/2330 train_time:36802ms step_avg:38.46ms
step:958/2330 train_time:36843ms step_avg:38.46ms
step:959/2330 train_time:36879ms step_avg:38.46ms
step:960/2330 train_time:36920ms step_avg:38.46ms
step:961/2330 train_time:36957ms step_avg:38.46ms
step:962/2330 train_time:36997ms step_avg:38.46ms
step:963/2330 train_time:37034ms step_avg:38.46ms
step:964/2330 train_time:37075ms step_avg:38.46ms
step:965/2330 train_time:37110ms step_avg:38.46ms
step:966/2330 train_time:37151ms step_avg:38.46ms
step:967/2330 train_time:37187ms step_avg:38.46ms
step:968/2330 train_time:37228ms step_avg:38.46ms
step:969/2330 train_time:37264ms step_avg:38.46ms
step:970/2330 train_time:37305ms step_avg:38.46ms
step:971/2330 train_time:37342ms step_avg:38.46ms
step:972/2330 train_time:37383ms step_avg:38.46ms
step:973/2330 train_time:37420ms step_avg:38.46ms
step:974/2330 train_time:37461ms step_avg:38.46ms
step:975/2330 train_time:37497ms step_avg:38.46ms
step:976/2330 train_time:37538ms step_avg:38.46ms
step:977/2330 train_time:37575ms step_avg:38.46ms
step:978/2330 train_time:37616ms step_avg:38.46ms
step:979/2330 train_time:37653ms step_avg:38.46ms
step:980/2330 train_time:37694ms step_avg:38.46ms
step:981/2330 train_time:37730ms step_avg:38.46ms
step:982/2330 train_time:37771ms step_avg:38.46ms
step:983/2330 train_time:37806ms step_avg:38.46ms
step:984/2330 train_time:37847ms step_avg:38.46ms
step:985/2330 train_time:37884ms step_avg:38.46ms
step:986/2330 train_time:37925ms step_avg:38.46ms
step:987/2330 train_time:37960ms step_avg:38.46ms
step:988/2330 train_time:38001ms step_avg:38.46ms
step:989/2330 train_time:38038ms step_avg:38.46ms
step:990/2330 train_time:38078ms step_avg:38.46ms
step:991/2330 train_time:38115ms step_avg:38.46ms
step:992/2330 train_time:38155ms step_avg:38.46ms
step:993/2330 train_time:38192ms step_avg:38.46ms
step:994/2330 train_time:38233ms step_avg:38.46ms
step:995/2330 train_time:38268ms step_avg:38.46ms
step:996/2330 train_time:38309ms step_avg:38.46ms
step:997/2330 train_time:38345ms step_avg:38.46ms
step:998/2330 train_time:38387ms step_avg:38.46ms
step:999/2330 train_time:38423ms step_avg:38.46ms
step:1000/2330 train_time:38464ms step_avg:38.46ms
step:1000/2330 val_loss:5.3173 train_time:38579ms step_avg:38.58ms
step:1001/2330 train_time:38590ms step_avg:38.55ms
step:1002/2330 train_time:38602ms step_avg:38.52ms
step:1003/2330 train_time:38610ms step_avg:38.50ms
step:1004/2330 train_time:38622ms step_avg:38.47ms
step:1005/2330 train_time:38656ms step_avg:38.46ms
step:1006/2330 train_time:38697ms step_avg:38.47ms
step:1007/2330 train_time:38731ms step_avg:38.46ms
step:1008/2330 train_time:38772ms step_avg:38.46ms
step:1009/2330 train_time:38807ms step_avg:38.46ms
step:1010/2330 train_time:38848ms step_avg:38.46ms
step:1011/2330 train_time:38884ms step_avg:38.46ms
step:1012/2330 train_time:38927ms step_avg:38.46ms
step:1013/2330 train_time:38969ms step_avg:38.47ms
step:1014/2330 train_time:39011ms step_avg:38.47ms
step:1015/2330 train_time:39049ms step_avg:38.47ms
step:1016/2330 train_time:39090ms step_avg:38.47ms
step:1017/2330 train_time:39126ms step_avg:38.47ms
step:1018/2330 train_time:39167ms step_avg:38.47ms
step:1019/2330 train_time:39203ms step_avg:38.47ms
step:1020/2330 train_time:39243ms step_avg:38.47ms
step:1021/2330 train_time:39279ms step_avg:38.47ms
step:1022/2330 train_time:39320ms step_avg:38.47ms
step:1023/2330 train_time:39355ms step_avg:38.47ms
step:1024/2330 train_time:39395ms step_avg:38.47ms
step:1025/2330 train_time:39431ms step_avg:38.47ms
step:1026/2330 train_time:39472ms step_avg:38.47ms
step:1027/2330 train_time:39508ms step_avg:38.47ms
step:1028/2330 train_time:39549ms step_avg:38.47ms
step:1029/2330 train_time:39585ms step_avg:38.47ms
step:1030/2330 train_time:39626ms step_avg:38.47ms
step:1031/2330 train_time:39661ms step_avg:38.47ms
step:1032/2330 train_time:39702ms step_avg:38.47ms
step:1033/2330 train_time:39736ms step_avg:38.47ms
step:1034/2330 train_time:39777ms step_avg:38.47ms
step:1035/2330 train_time:39812ms step_avg:38.47ms
step:1036/2330 train_time:39854ms step_avg:38.47ms
step:1037/2330 train_time:39892ms step_avg:38.47ms
step:1038/2330 train_time:39934ms step_avg:38.47ms
step:1039/2330 train_time:39973ms step_avg:38.47ms
step:1040/2330 train_time:40014ms step_avg:38.48ms
step:1041/2330 train_time:40050ms step_avg:38.47ms
step:1042/2330 train_time:40092ms step_avg:38.48ms
step:1043/2330 train_time:40128ms step_avg:38.47ms
step:1044/2330 train_time:40169ms step_avg:38.48ms
step:1045/2330 train_time:40205ms step_avg:38.47ms
step:1046/2330 train_time:40246ms step_avg:38.48ms
step:1047/2330 train_time:40283ms step_avg:38.47ms
step:1048/2330 train_time:40324ms step_avg:38.48ms
step:1049/2330 train_time:40360ms step_avg:38.47ms
step:1050/2330 train_time:40401ms step_avg:38.48ms
step:1051/2330 train_time:40436ms step_avg:38.47ms
step:1052/2330 train_time:40477ms step_avg:38.48ms
step:1053/2330 train_time:40512ms step_avg:38.47ms
step:1054/2330 train_time:40554ms step_avg:38.48ms
step:1055/2330 train_time:40589ms step_avg:38.47ms
step:1056/2330 train_time:40631ms step_avg:38.48ms
step:1057/2330 train_time:40666ms step_avg:38.47ms
step:1058/2330 train_time:40707ms step_avg:38.48ms
step:1059/2330 train_time:40742ms step_avg:38.47ms
step:1060/2330 train_time:40783ms step_avg:38.47ms
step:1061/2330 train_time:40820ms step_avg:38.47ms
step:1062/2330 train_time:40861ms step_avg:38.48ms
step:1063/2330 train_time:40898ms step_avg:38.47ms
step:1064/2330 train_time:40939ms step_avg:38.48ms
step:1065/2330 train_time:40975ms step_avg:38.47ms
step:1066/2330 train_time:41017ms step_avg:38.48ms
step:1067/2330 train_time:41053ms step_avg:38.48ms
step:1068/2330 train_time:41094ms step_avg:38.48ms
step:1069/2330 train_time:41131ms step_avg:38.48ms
step:1070/2330 train_time:41173ms step_avg:38.48ms
step:1071/2330 train_time:41208ms step_avg:38.48ms
step:1072/2330 train_time:41250ms step_avg:38.48ms
step:1073/2330 train_time:41285ms step_avg:38.48ms
step:1074/2330 train_time:41326ms step_avg:38.48ms
step:1075/2330 train_time:41363ms step_avg:38.48ms
step:1076/2330 train_time:41403ms step_avg:38.48ms
step:1077/2330 train_time:41440ms step_avg:38.48ms
step:1078/2330 train_time:41481ms step_avg:38.48ms
step:1079/2330 train_time:41516ms step_avg:38.48ms
step:1080/2330 train_time:41557ms step_avg:38.48ms
step:1081/2330 train_time:41592ms step_avg:38.48ms
step:1082/2330 train_time:41634ms step_avg:38.48ms
step:1083/2330 train_time:41669ms step_avg:38.48ms
step:1084/2330 train_time:41710ms step_avg:38.48ms
step:1085/2330 train_time:41746ms step_avg:38.48ms
step:1086/2330 train_time:41787ms step_avg:38.48ms
step:1087/2330 train_time:41824ms step_avg:38.48ms
step:1088/2330 train_time:41865ms step_avg:38.48ms
step:1089/2330 train_time:41901ms step_avg:38.48ms
step:1090/2330 train_time:41942ms step_avg:38.48ms
step:1091/2330 train_time:41979ms step_avg:38.48ms
step:1092/2330 train_time:42020ms step_avg:38.48ms
step:1093/2330 train_time:42057ms step_avg:38.48ms
step:1094/2330 train_time:42098ms step_avg:38.48ms
step:1095/2330 train_time:42134ms step_avg:38.48ms
step:1096/2330 train_time:42176ms step_avg:38.48ms
step:1097/2330 train_time:42211ms step_avg:38.48ms
step:1098/2330 train_time:42253ms step_avg:38.48ms
step:1099/2330 train_time:42289ms step_avg:38.48ms
step:1100/2330 train_time:42331ms step_avg:38.48ms
step:1101/2330 train_time:42367ms step_avg:38.48ms
step:1102/2330 train_time:42408ms step_avg:38.48ms
step:1103/2330 train_time:42444ms step_avg:38.48ms
step:1104/2330 train_time:42485ms step_avg:38.48ms
step:1105/2330 train_time:42522ms step_avg:38.48ms
step:1106/2330 train_time:42563ms step_avg:38.48ms
step:1107/2330 train_time:42599ms step_avg:38.48ms
step:1108/2330 train_time:42640ms step_avg:38.48ms
step:1109/2330 train_time:42675ms step_avg:38.48ms
step:1110/2330 train_time:42717ms step_avg:38.48ms
step:1111/2330 train_time:42752ms step_avg:38.48ms
step:1112/2330 train_time:42793ms step_avg:38.48ms
step:1113/2330 train_time:42829ms step_avg:38.48ms
step:1114/2330 train_time:42871ms step_avg:38.48ms
step:1115/2330 train_time:42907ms step_avg:38.48ms
step:1116/2330 train_time:42949ms step_avg:38.48ms
step:1117/2330 train_time:42986ms step_avg:38.48ms
step:1118/2330 train_time:43026ms step_avg:38.49ms
step:1119/2330 train_time:43064ms step_avg:38.48ms
step:1120/2330 train_time:43104ms step_avg:38.49ms
step:1121/2330 train_time:43142ms step_avg:38.49ms
step:1122/2330 train_time:43183ms step_avg:38.49ms
step:1123/2330 train_time:43220ms step_avg:38.49ms
step:1124/2330 train_time:43261ms step_avg:38.49ms
step:1125/2330 train_time:43297ms step_avg:38.49ms
step:1126/2330 train_time:43338ms step_avg:38.49ms
step:1127/2330 train_time:43374ms step_avg:38.49ms
step:1128/2330 train_time:43415ms step_avg:38.49ms
step:1129/2330 train_time:43451ms step_avg:38.49ms
step:1130/2330 train_time:43492ms step_avg:38.49ms
step:1131/2330 train_time:43528ms step_avg:38.49ms
step:1132/2330 train_time:43569ms step_avg:38.49ms
step:1133/2330 train_time:43606ms step_avg:38.49ms
step:1134/2330 train_time:43647ms step_avg:38.49ms
step:1135/2330 train_time:43682ms step_avg:38.49ms
step:1136/2330 train_time:43723ms step_avg:38.49ms
step:1137/2330 train_time:43760ms step_avg:38.49ms
step:1138/2330 train_time:43801ms step_avg:38.49ms
step:1139/2330 train_time:43836ms step_avg:38.49ms
step:1140/2330 train_time:43878ms step_avg:38.49ms
step:1141/2330 train_time:43913ms step_avg:38.49ms
step:1142/2330 train_time:43954ms step_avg:38.49ms
step:1143/2330 train_time:43991ms step_avg:38.49ms
step:1144/2330 train_time:44032ms step_avg:38.49ms
step:1145/2330 train_time:44068ms step_avg:38.49ms
step:1146/2330 train_time:44110ms step_avg:38.49ms
step:1147/2330 train_time:44145ms step_avg:38.49ms
step:1148/2330 train_time:44187ms step_avg:38.49ms
step:1149/2330 train_time:44222ms step_avg:38.49ms
step:1150/2330 train_time:44263ms step_avg:38.49ms
step:1151/2330 train_time:44300ms step_avg:38.49ms
step:1152/2330 train_time:44341ms step_avg:38.49ms
step:1153/2330 train_time:44378ms step_avg:38.49ms
step:1154/2330 train_time:44419ms step_avg:38.49ms
step:1155/2330 train_time:44455ms step_avg:38.49ms
step:1156/2330 train_time:44495ms step_avg:38.49ms
step:1157/2330 train_time:44532ms step_avg:38.49ms
step:1158/2330 train_time:44573ms step_avg:38.49ms
step:1159/2330 train_time:44609ms step_avg:38.49ms
step:1160/2330 train_time:44650ms step_avg:38.49ms
step:1161/2330 train_time:44687ms step_avg:38.49ms
step:1162/2330 train_time:44728ms step_avg:38.49ms
step:1163/2330 train_time:44765ms step_avg:38.49ms
step:1164/2330 train_time:44805ms step_avg:38.49ms
step:1165/2330 train_time:44842ms step_avg:38.49ms
step:1166/2330 train_time:44883ms step_avg:38.49ms
step:1167/2330 train_time:44920ms step_avg:38.49ms
step:1168/2330 train_time:44961ms step_avg:38.49ms
step:1169/2330 train_time:44997ms step_avg:38.49ms
step:1170/2330 train_time:45038ms step_avg:38.49ms
step:1171/2330 train_time:45073ms step_avg:38.49ms
step:1172/2330 train_time:45115ms step_avg:38.49ms
step:1173/2330 train_time:45150ms step_avg:38.49ms
step:1174/2330 train_time:45192ms step_avg:38.49ms
step:1175/2330 train_time:45228ms step_avg:38.49ms
step:1176/2330 train_time:45271ms step_avg:38.50ms
step:1177/2330 train_time:45306ms step_avg:38.49ms
step:1178/2330 train_time:45348ms step_avg:38.50ms
step:1179/2330 train_time:45384ms step_avg:38.49ms
step:1180/2330 train_time:45425ms step_avg:38.50ms
step:1181/2330 train_time:45462ms step_avg:38.49ms
step:1182/2330 train_time:45502ms step_avg:38.50ms
step:1183/2330 train_time:45539ms step_avg:38.49ms
step:1184/2330 train_time:45580ms step_avg:38.50ms
step:1185/2330 train_time:45614ms step_avg:38.49ms
step:1186/2330 train_time:45656ms step_avg:38.50ms
step:1187/2330 train_time:45691ms step_avg:38.49ms
step:1188/2330 train_time:45733ms step_avg:38.50ms
step:1189/2330 train_time:45769ms step_avg:38.49ms
step:1190/2330 train_time:45811ms step_avg:38.50ms
step:1191/2330 train_time:45846ms step_avg:38.49ms
step:1192/2330 train_time:45887ms step_avg:38.50ms
step:1193/2330 train_time:45924ms step_avg:38.49ms
step:1194/2330 train_time:45965ms step_avg:38.50ms
step:1195/2330 train_time:46001ms step_avg:38.49ms
step:1196/2330 train_time:46042ms step_avg:38.50ms
step:1197/2330 train_time:46079ms step_avg:38.50ms
step:1198/2330 train_time:46120ms step_avg:38.50ms
step:1199/2330 train_time:46156ms step_avg:38.50ms
step:1200/2330 train_time:46197ms step_avg:38.50ms
step:1201/2330 train_time:46233ms step_avg:38.50ms
step:1202/2330 train_time:46275ms step_avg:38.50ms
step:1203/2330 train_time:46310ms step_avg:38.50ms
step:1204/2330 train_time:46352ms step_avg:38.50ms
step:1205/2330 train_time:46387ms step_avg:38.50ms
step:1206/2330 train_time:46429ms step_avg:38.50ms
step:1207/2330 train_time:46466ms step_avg:38.50ms
step:1208/2330 train_time:46507ms step_avg:38.50ms
step:1209/2330 train_time:46544ms step_avg:38.50ms
step:1210/2330 train_time:46584ms step_avg:38.50ms
step:1211/2330 train_time:46622ms step_avg:38.50ms
step:1212/2330 train_time:46662ms step_avg:38.50ms
step:1213/2330 train_time:46699ms step_avg:38.50ms
step:1214/2330 train_time:46740ms step_avg:38.50ms
step:1215/2330 train_time:46775ms step_avg:38.50ms
step:1216/2330 train_time:46816ms step_avg:38.50ms
step:1217/2330 train_time:46851ms step_avg:38.50ms
step:1218/2330 train_time:46893ms step_avg:38.50ms
step:1219/2330 train_time:46929ms step_avg:38.50ms
step:1220/2330 train_time:46971ms step_avg:38.50ms
step:1221/2330 train_time:47007ms step_avg:38.50ms
step:1222/2330 train_time:47048ms step_avg:38.50ms
step:1223/2330 train_time:47085ms step_avg:38.50ms
step:1224/2330 train_time:47126ms step_avg:38.50ms
step:1225/2330 train_time:47163ms step_avg:38.50ms
step:1226/2330 train_time:47203ms step_avg:38.50ms
step:1227/2330 train_time:47240ms step_avg:38.50ms
step:1228/2330 train_time:47281ms step_avg:38.50ms
step:1229/2330 train_time:47317ms step_avg:38.50ms
step:1230/2330 train_time:47358ms step_avg:38.50ms
step:1231/2330 train_time:47393ms step_avg:38.50ms
step:1232/2330 train_time:47435ms step_avg:38.50ms
step:1233/2330 train_time:47470ms step_avg:38.50ms
step:1234/2330 train_time:47512ms step_avg:38.50ms
step:1235/2330 train_time:47548ms step_avg:38.50ms
step:1236/2330 train_time:47590ms step_avg:38.50ms
step:1237/2330 train_time:47625ms step_avg:38.50ms
step:1238/2330 train_time:47667ms step_avg:38.50ms
step:1239/2330 train_time:47704ms step_avg:38.50ms
step:1240/2330 train_time:47744ms step_avg:38.50ms
step:1241/2330 train_time:47781ms step_avg:38.50ms
step:1242/2330 train_time:47822ms step_avg:38.50ms
step:1243/2330 train_time:47857ms step_avg:38.50ms
step:1244/2330 train_time:47899ms step_avg:38.50ms
step:1245/2330 train_time:47933ms step_avg:38.50ms
step:1246/2330 train_time:47974ms step_avg:38.50ms
step:1247/2330 train_time:48010ms step_avg:38.50ms
step:1248/2330 train_time:48052ms step_avg:38.50ms
step:1249/2330 train_time:48088ms step_avg:38.50ms
step:1250/2330 train_time:48130ms step_avg:38.50ms
step:1250/2330 val_loss:5.2867 train_time:48244ms step_avg:38.59ms
step:1251/2330 train_time:48255ms step_avg:38.57ms
step:1252/2330 train_time:48265ms step_avg:38.55ms
step:1253/2330 train_time:48274ms step_avg:38.53ms
step:1254/2330 train_time:48286ms step_avg:38.51ms
step:1255/2330 train_time:48321ms step_avg:38.50ms
step:1256/2330 train_time:48362ms step_avg:38.50ms
step:1257/2330 train_time:48397ms step_avg:38.50ms
step:1258/2330 train_time:48437ms step_avg:38.50ms
step:1259/2330 train_time:48472ms step_avg:38.50ms
step:1260/2330 train_time:48513ms step_avg:38.50ms
step:1261/2330 train_time:48552ms step_avg:38.50ms
step:1262/2330 train_time:48594ms step_avg:38.51ms
step:1263/2330 train_time:48632ms step_avg:38.51ms
step:1264/2330 train_time:48673ms step_avg:38.51ms
step:1265/2330 train_time:48709ms step_avg:38.51ms
step:1266/2330 train_time:48750ms step_avg:38.51ms
step:1267/2330 train_time:48786ms step_avg:38.51ms
step:1268/2330 train_time:48827ms step_avg:38.51ms
step:1269/2330 train_time:48863ms step_avg:38.50ms
step:1270/2330 train_time:48904ms step_avg:38.51ms
step:1271/2330 train_time:48939ms step_avg:38.50ms
step:1272/2330 train_time:48979ms step_avg:38.51ms
step:1273/2330 train_time:49015ms step_avg:38.50ms
step:1274/2330 train_time:49056ms step_avg:38.51ms
step:1275/2330 train_time:49091ms step_avg:38.50ms
step:1276/2330 train_time:49132ms step_avg:38.51ms
step:1277/2330 train_time:49169ms step_avg:38.50ms
step:1278/2330 train_time:49210ms step_avg:38.51ms
step:1279/2330 train_time:49247ms step_avg:38.50ms
step:1280/2330 train_time:49289ms step_avg:38.51ms
step:1281/2330 train_time:49324ms step_avg:38.50ms
step:1282/2330 train_time:49366ms step_avg:38.51ms
step:1283/2330 train_time:49401ms step_avg:38.50ms
step:1284/2330 train_time:49443ms step_avg:38.51ms
step:1285/2330 train_time:49480ms step_avg:38.51ms
step:1286/2330 train_time:49521ms step_avg:38.51ms
step:1287/2330 train_time:49559ms step_avg:38.51ms
step:1288/2330 train_time:49600ms step_avg:38.51ms
step:1289/2330 train_time:49638ms step_avg:38.51ms
step:1290/2330 train_time:49678ms step_avg:38.51ms
step:1291/2330 train_time:49716ms step_avg:38.51ms
step:1292/2330 train_time:49756ms step_avg:38.51ms
step:1293/2330 train_time:49793ms step_avg:38.51ms
step:1294/2330 train_time:49834ms step_avg:38.51ms
step:1295/2330 train_time:49868ms step_avg:38.51ms
step:1296/2330 train_time:49909ms step_avg:38.51ms
step:1297/2330 train_time:49944ms step_avg:38.51ms
step:1298/2330 train_time:49985ms step_avg:38.51ms
step:1299/2330 train_time:50022ms step_avg:38.51ms
step:1300/2330 train_time:50063ms step_avg:38.51ms
step:1301/2330 train_time:50098ms step_avg:38.51ms
step:1302/2330 train_time:50139ms step_avg:38.51ms
step:1303/2330 train_time:50176ms step_avg:38.51ms
step:1304/2330 train_time:50217ms step_avg:38.51ms
step:1305/2330 train_time:50253ms step_avg:38.51ms
step:1306/2330 train_time:50293ms step_avg:38.51ms
step:1307/2330 train_time:50329ms step_avg:38.51ms
step:1308/2330 train_time:50370ms step_avg:38.51ms
step:1309/2330 train_time:50406ms step_avg:38.51ms
step:1310/2330 train_time:50448ms step_avg:38.51ms
step:1311/2330 train_time:50485ms step_avg:38.51ms
step:1312/2330 train_time:50527ms step_avg:38.51ms
step:1313/2330 train_time:50564ms step_avg:38.51ms
step:1314/2330 train_time:50605ms step_avg:38.51ms
step:1315/2330 train_time:50642ms step_avg:38.51ms
step:1316/2330 train_time:50684ms step_avg:38.51ms
step:1317/2330 train_time:50721ms step_avg:38.51ms
step:1318/2330 train_time:50763ms step_avg:38.51ms
step:1319/2330 train_time:50799ms step_avg:38.51ms
step:1320/2330 train_time:50839ms step_avg:38.51ms
step:1321/2330 train_time:50876ms step_avg:38.51ms
step:1322/2330 train_time:50916ms step_avg:38.51ms
step:1323/2330 train_time:50951ms step_avg:38.51ms
step:1324/2330 train_time:50992ms step_avg:38.51ms
step:1325/2330 train_time:51028ms step_avg:38.51ms
step:1326/2330 train_time:51068ms step_avg:38.51ms
step:1327/2330 train_time:51105ms step_avg:38.51ms
step:1328/2330 train_time:51146ms step_avg:38.51ms
step:1329/2330 train_time:51183ms step_avg:38.51ms
step:1330/2330 train_time:51224ms step_avg:38.51ms
step:1331/2330 train_time:51260ms step_avg:38.51ms
step:1332/2330 train_time:51301ms step_avg:38.51ms
step:1333/2330 train_time:51338ms step_avg:38.51ms
step:1334/2330 train_time:51378ms step_avg:38.51ms
step:1335/2330 train_time:51415ms step_avg:38.51ms
step:1336/2330 train_time:51456ms step_avg:38.52ms
step:1337/2330 train_time:51493ms step_avg:38.51ms
step:1338/2330 train_time:51534ms step_avg:38.52ms
step:1339/2330 train_time:51569ms step_avg:38.51ms
step:1340/2330 train_time:51610ms step_avg:38.52ms
step:1341/2330 train_time:51648ms step_avg:38.51ms
step:1342/2330 train_time:51689ms step_avg:38.52ms
step:1343/2330 train_time:51726ms step_avg:38.52ms
step:1344/2330 train_time:51767ms step_avg:38.52ms
step:1345/2330 train_time:51804ms step_avg:38.52ms
step:1346/2330 train_time:51846ms step_avg:38.52ms
step:1347/2330 train_time:51881ms step_avg:38.52ms
step:1348/2330 train_time:51922ms step_avg:38.52ms
step:1349/2330 train_time:51957ms step_avg:38.52ms
step:1350/2330 train_time:51998ms step_avg:38.52ms
step:1351/2330 train_time:52034ms step_avg:38.52ms
step:1352/2330 train_time:52075ms step_avg:38.52ms
step:1353/2330 train_time:52111ms step_avg:38.52ms
step:1354/2330 train_time:52152ms step_avg:38.52ms
step:1355/2330 train_time:52187ms step_avg:38.51ms
step:1356/2330 train_time:52229ms step_avg:38.52ms
step:1357/2330 train_time:52264ms step_avg:38.51ms
step:1358/2330 train_time:52306ms step_avg:38.52ms
step:1359/2330 train_time:52341ms step_avg:38.51ms
step:1360/2330 train_time:52383ms step_avg:38.52ms
step:1361/2330 train_time:52420ms step_avg:38.52ms
step:1362/2330 train_time:52460ms step_avg:38.52ms
step:1363/2330 train_time:52498ms step_avg:38.52ms
step:1364/2330 train_time:52538ms step_avg:38.52ms
step:1365/2330 train_time:52576ms step_avg:38.52ms
step:1366/2330 train_time:52616ms step_avg:38.52ms
step:1367/2330 train_time:52653ms step_avg:38.52ms
step:1368/2330 train_time:52695ms step_avg:38.52ms
step:1369/2330 train_time:52731ms step_avg:38.52ms
step:1370/2330 train_time:52772ms step_avg:38.52ms
step:1371/2330 train_time:52807ms step_avg:38.52ms
step:1372/2330 train_time:52848ms step_avg:38.52ms
step:1373/2330 train_time:52885ms step_avg:38.52ms
step:1374/2330 train_time:52926ms step_avg:38.52ms
step:1375/2330 train_time:52962ms step_avg:38.52ms
step:1376/2330 train_time:53003ms step_avg:38.52ms
step:1377/2330 train_time:53038ms step_avg:38.52ms
step:1378/2330 train_time:53079ms step_avg:38.52ms
step:1379/2330 train_time:53115ms step_avg:38.52ms
step:1380/2330 train_time:53156ms step_avg:38.52ms
step:1381/2330 train_time:53192ms step_avg:38.52ms
step:1382/2330 train_time:53232ms step_avg:38.52ms
step:1383/2330 train_time:53268ms step_avg:38.52ms
step:1384/2330 train_time:53308ms step_avg:38.52ms
step:1385/2330 train_time:53345ms step_avg:38.52ms
step:1386/2330 train_time:53386ms step_avg:38.52ms
step:1387/2330 train_time:53422ms step_avg:38.52ms
step:1388/2330 train_time:53464ms step_avg:38.52ms
step:1389/2330 train_time:53500ms step_avg:38.52ms
step:1390/2330 train_time:53541ms step_avg:38.52ms
step:1391/2330 train_time:53578ms step_avg:38.52ms
step:1392/2330 train_time:53619ms step_avg:38.52ms
step:1393/2330 train_time:53656ms step_avg:38.52ms
step:1394/2330 train_time:53697ms step_avg:38.52ms
step:1395/2330 train_time:53734ms step_avg:38.52ms
step:1396/2330 train_time:53775ms step_avg:38.52ms
step:1397/2330 train_time:53811ms step_avg:38.52ms
step:1398/2330 train_time:53852ms step_avg:38.52ms
step:1399/2330 train_time:53887ms step_avg:38.52ms
step:1400/2330 train_time:53928ms step_avg:38.52ms
step:1401/2330 train_time:53964ms step_avg:38.52ms
step:1402/2330 train_time:54005ms step_avg:38.52ms
step:1403/2330 train_time:54042ms step_avg:38.52ms
step:1404/2330 train_time:54083ms step_avg:38.52ms
step:1405/2330 train_time:54120ms step_avg:38.52ms
step:1406/2330 train_time:54160ms step_avg:38.52ms
step:1407/2330 train_time:54197ms step_avg:38.52ms
step:1408/2330 train_time:54238ms step_avg:38.52ms
step:1409/2330 train_time:54274ms step_avg:38.52ms
step:1410/2330 train_time:54315ms step_avg:38.52ms
step:1411/2330 train_time:54351ms step_avg:38.52ms
step:1412/2330 train_time:54392ms step_avg:38.52ms
step:1413/2330 train_time:54428ms step_avg:38.52ms
step:1414/2330 train_time:54470ms step_avg:38.52ms
step:1415/2330 train_time:54505ms step_avg:38.52ms
step:1416/2330 train_time:54547ms step_avg:38.52ms
step:1417/2330 train_time:54582ms step_avg:38.52ms
step:1418/2330 train_time:54624ms step_avg:38.52ms
step:1419/2330 train_time:54660ms step_avg:38.52ms
step:1420/2330 train_time:54702ms step_avg:38.52ms
step:1421/2330 train_time:54738ms step_avg:38.52ms
step:1422/2330 train_time:54779ms step_avg:38.52ms
step:1423/2330 train_time:54816ms step_avg:38.52ms
step:1424/2330 train_time:54857ms step_avg:38.52ms
step:1425/2330 train_time:54893ms step_avg:38.52ms
step:1426/2330 train_time:54934ms step_avg:38.52ms
step:1427/2330 train_time:54969ms step_avg:38.52ms
step:1428/2330 train_time:55010ms step_avg:38.52ms
step:1429/2330 train_time:55046ms step_avg:38.52ms
step:1430/2330 train_time:55087ms step_avg:38.52ms
step:1431/2330 train_time:55123ms step_avg:38.52ms
step:1432/2330 train_time:55165ms step_avg:38.52ms
step:1433/2330 train_time:55200ms step_avg:38.52ms
step:1434/2330 train_time:55241ms step_avg:38.52ms
step:1435/2330 train_time:55278ms step_avg:38.52ms
step:1436/2330 train_time:55319ms step_avg:38.52ms
step:1437/2330 train_time:55355ms step_avg:38.52ms
step:1438/2330 train_time:55396ms step_avg:38.52ms
step:1439/2330 train_time:55433ms step_avg:38.52ms
step:1440/2330 train_time:55474ms step_avg:38.52ms
step:1441/2330 train_time:55509ms step_avg:38.52ms
step:1442/2330 train_time:55551ms step_avg:38.52ms
step:1443/2330 train_time:55586ms step_avg:38.52ms
step:1444/2330 train_time:55628ms step_avg:38.52ms
step:1445/2330 train_time:55664ms step_avg:38.52ms
step:1446/2330 train_time:55706ms step_avg:38.52ms
step:1447/2330 train_time:55743ms step_avg:38.52ms
step:1448/2330 train_time:55784ms step_avg:38.53ms
step:1449/2330 train_time:55821ms step_avg:38.52ms
step:1450/2330 train_time:55862ms step_avg:38.53ms
step:1451/2330 train_time:55898ms step_avg:38.52ms
step:1452/2330 train_time:55939ms step_avg:38.53ms
step:1453/2330 train_time:55975ms step_avg:38.52ms
step:1454/2330 train_time:56016ms step_avg:38.53ms
step:1455/2330 train_time:56052ms step_avg:38.52ms
step:1456/2330 train_time:56093ms step_avg:38.53ms
step:1457/2330 train_time:56128ms step_avg:38.52ms
step:1458/2330 train_time:56169ms step_avg:38.52ms
step:1459/2330 train_time:56206ms step_avg:38.52ms
step:1460/2330 train_time:56247ms step_avg:38.53ms
step:1461/2330 train_time:56283ms step_avg:38.52ms
step:1462/2330 train_time:56324ms step_avg:38.53ms
step:1463/2330 train_time:56360ms step_avg:38.52ms
step:1464/2330 train_time:56401ms step_avg:38.52ms
step:1465/2330 train_time:56437ms step_avg:38.52ms
step:1466/2330 train_time:56478ms step_avg:38.53ms
step:1467/2330 train_time:56515ms step_avg:38.52ms
step:1468/2330 train_time:56556ms step_avg:38.53ms
step:1469/2330 train_time:56593ms step_avg:38.53ms
step:1470/2330 train_time:56635ms step_avg:38.53ms
step:1471/2330 train_time:56670ms step_avg:38.52ms
step:1472/2330 train_time:56711ms step_avg:38.53ms
step:1473/2330 train_time:56747ms step_avg:38.52ms
step:1474/2330 train_time:56789ms step_avg:38.53ms
step:1475/2330 train_time:56825ms step_avg:38.53ms
step:1476/2330 train_time:56867ms step_avg:38.53ms
step:1477/2330 train_time:56903ms step_avg:38.53ms
step:1478/2330 train_time:56944ms step_avg:38.53ms
step:1479/2330 train_time:56980ms step_avg:38.53ms
step:1480/2330 train_time:57021ms step_avg:38.53ms
step:1481/2330 train_time:57058ms step_avg:38.53ms
step:1482/2330 train_time:57098ms step_avg:38.53ms
step:1483/2330 train_time:57135ms step_avg:38.53ms
step:1484/2330 train_time:57175ms step_avg:38.53ms
step:1485/2330 train_time:57212ms step_avg:38.53ms
step:1486/2330 train_time:57253ms step_avg:38.53ms
step:1487/2330 train_time:57289ms step_avg:38.53ms
step:1488/2330 train_time:57330ms step_avg:38.53ms
step:1489/2330 train_time:57366ms step_avg:38.53ms
step:1490/2330 train_time:57408ms step_avg:38.53ms
step:1491/2330 train_time:57444ms step_avg:38.53ms
step:1492/2330 train_time:57485ms step_avg:38.53ms
step:1493/2330 train_time:57522ms step_avg:38.53ms
step:1494/2330 train_time:57564ms step_avg:38.53ms
step:1495/2330 train_time:57599ms step_avg:38.53ms
step:1496/2330 train_time:57640ms step_avg:38.53ms
step:1497/2330 train_time:57678ms step_avg:38.53ms
step:1498/2330 train_time:57719ms step_avg:38.53ms
step:1499/2330 train_time:57756ms step_avg:38.53ms
step:1500/2330 train_time:57797ms step_avg:38.53ms
step:1500/2330 val_loss:5.2496 train_time:57910ms step_avg:38.61ms
step:1501/2330 train_time:57921ms step_avg:38.59ms
step:1502/2330 train_time:57933ms step_avg:38.57ms
step:1503/2330 train_time:57942ms step_avg:38.55ms
step:1504/2330 train_time:57953ms step_avg:38.53ms
step:1505/2330 train_time:57987ms step_avg:38.53ms
step:1506/2330 train_time:58028ms step_avg:38.53ms
step:1507/2330 train_time:58063ms step_avg:38.53ms
step:1508/2330 train_time:58104ms step_avg:38.53ms
step:1509/2330 train_time:58140ms step_avg:38.53ms
step:1510/2330 train_time:58180ms step_avg:38.53ms
step:1511/2330 train_time:58221ms step_avg:38.53ms
step:1512/2330 train_time:58263ms step_avg:38.53ms
step:1513/2330 train_time:58303ms step_avg:38.53ms
step:1514/2330 train_time:58344ms step_avg:38.54ms
step:1515/2330 train_time:58382ms step_avg:38.54ms
step:1516/2330 train_time:58423ms step_avg:38.54ms
step:1517/2330 train_time:58460ms step_avg:38.54ms
step:1518/2330 train_time:58500ms step_avg:38.54ms
step:1519/2330 train_time:58536ms step_avg:38.54ms
step:1520/2330 train_time:58577ms step_avg:38.54ms
step:1521/2330 train_time:58612ms step_avg:38.54ms
step:1522/2330 train_time:58653ms step_avg:38.54ms
step:1523/2330 train_time:58688ms step_avg:38.53ms
step:1524/2330 train_time:58729ms step_avg:38.54ms
step:1525/2330 train_time:58764ms step_avg:38.53ms
step:1526/2330 train_time:58805ms step_avg:38.54ms
step:1527/2330 train_time:58842ms step_avg:38.53ms
step:1528/2330 train_time:58883ms step_avg:38.54ms
step:1529/2330 train_time:58921ms step_avg:38.54ms
step:1530/2330 train_time:58962ms step_avg:38.54ms
step:1531/2330 train_time:58997ms step_avg:38.54ms
step:1532/2330 train_time:59038ms step_avg:38.54ms
step:1533/2330 train_time:59073ms step_avg:38.53ms
step:1534/2330 train_time:59114ms step_avg:38.54ms
step:1535/2330 train_time:59150ms step_avg:38.53ms
step:1536/2330 train_time:59192ms step_avg:38.54ms
step:1537/2330 train_time:59229ms step_avg:38.54ms
step:1538/2330 train_time:59271ms step_avg:38.54ms
step:1539/2330 train_time:59308ms step_avg:38.54ms
step:1540/2330 train_time:59350ms step_avg:38.54ms
step:1541/2330 train_time:59388ms step_avg:38.54ms
step:1542/2330 train_time:59430ms step_avg:38.54ms
step:1543/2330 train_time:59466ms step_avg:38.54ms
step:1544/2330 train_time:59508ms step_avg:38.54ms
step:1545/2330 train_time:59543ms step_avg:38.54ms
step:1546/2330 train_time:59584ms step_avg:38.54ms
step:1547/2330 train_time:59620ms step_avg:38.54ms
step:1548/2330 train_time:59660ms step_avg:38.54ms
step:1549/2330 train_time:59697ms step_avg:38.54ms
step:1550/2330 train_time:59737ms step_avg:38.54ms
step:1551/2330 train_time:59773ms step_avg:38.54ms
step:1552/2330 train_time:59814ms step_avg:38.54ms
step:1553/2330 train_time:59850ms step_avg:38.54ms
step:1554/2330 train_time:59891ms step_avg:38.54ms
step:1555/2330 train_time:59927ms step_avg:38.54ms
step:1556/2330 train_time:59968ms step_avg:38.54ms
step:1557/2330 train_time:60004ms step_avg:38.54ms
step:1558/2330 train_time:60045ms step_avg:38.54ms
step:1559/2330 train_time:60081ms step_avg:38.54ms
step:1560/2330 train_time:60122ms step_avg:38.54ms
step:1561/2330 train_time:60159ms step_avg:38.54ms
step:1562/2330 train_time:60200ms step_avg:38.54ms
step:1563/2330 train_time:60238ms step_avg:38.54ms
step:1564/2330 train_time:60278ms step_avg:38.54ms
step:1565/2330 train_time:60315ms step_avg:38.54ms
step:1566/2330 train_time:60356ms step_avg:38.54ms
step:1567/2330 train_time:60392ms step_avg:38.54ms
step:1568/2330 train_time:60433ms step_avg:38.54ms
step:1569/2330 train_time:60470ms step_avg:38.54ms
step:1570/2330 train_time:60511ms step_avg:38.54ms
step:1571/2330 train_time:60547ms step_avg:38.54ms
step:1572/2330 train_time:60589ms step_avg:38.54ms
step:1573/2330 train_time:60624ms step_avg:38.54ms
step:1574/2330 train_time:60665ms step_avg:38.54ms
step:1575/2330 train_time:60702ms step_avg:38.54ms
step:1576/2330 train_time:60742ms step_avg:38.54ms
step:1577/2330 train_time:60779ms step_avg:38.54ms
step:1578/2330 train_time:60820ms step_avg:38.54ms
step:1579/2330 train_time:60856ms step_avg:38.54ms
step:1580/2330 train_time:60896ms step_avg:38.54ms
step:1581/2330 train_time:60932ms step_avg:38.54ms
step:1582/2330 train_time:60972ms step_avg:38.54ms
step:1583/2330 train_time:61009ms step_avg:38.54ms
step:1584/2330 train_time:61050ms step_avg:38.54ms
step:1585/2330 train_time:61086ms step_avg:38.54ms
step:1586/2330 train_time:61128ms step_avg:38.54ms
step:1587/2330 train_time:61164ms step_avg:38.54ms
step:1588/2330 train_time:61205ms step_avg:38.54ms
step:1589/2330 train_time:61243ms step_avg:38.54ms
step:1590/2330 train_time:61284ms step_avg:38.54ms
step:1591/2330 train_time:61322ms step_avg:38.54ms
step:1592/2330 train_time:61362ms step_avg:38.54ms
step:1593/2330 train_time:61400ms step_avg:38.54ms
step:1594/2330 train_time:61441ms step_avg:38.55ms
step:1595/2330 train_time:61478ms step_avg:38.54ms
step:1596/2330 train_time:61519ms step_avg:38.55ms
step:1597/2330 train_time:61555ms step_avg:38.54ms
step:1598/2330 train_time:61596ms step_avg:38.55ms
step:1599/2330 train_time:61632ms step_avg:38.54ms
step:1600/2330 train_time:61673ms step_avg:38.55ms
step:1601/2330 train_time:61709ms step_avg:38.54ms
step:1602/2330 train_time:61750ms step_avg:38.55ms
step:1603/2330 train_time:61786ms step_avg:38.54ms
step:1604/2330 train_time:61828ms step_avg:38.55ms
step:1605/2330 train_time:61863ms step_avg:38.54ms
step:1606/2330 train_time:61904ms step_avg:38.55ms
step:1607/2330 train_time:61940ms step_avg:38.54ms
step:1608/2330 train_time:61981ms step_avg:38.55ms
step:1609/2330 train_time:62018ms step_avg:38.54ms
step:1610/2330 train_time:62059ms step_avg:38.55ms
step:1611/2330 train_time:62096ms step_avg:38.54ms
step:1612/2330 train_time:62136ms step_avg:38.55ms
step:1613/2330 train_time:62172ms step_avg:38.54ms
step:1614/2330 train_time:62213ms step_avg:38.55ms
step:1615/2330 train_time:62249ms step_avg:38.54ms
step:1616/2330 train_time:62291ms step_avg:38.55ms
step:1617/2330 train_time:62327ms step_avg:38.54ms
step:1618/2330 train_time:62369ms step_avg:38.55ms
step:1619/2330 train_time:62405ms step_avg:38.55ms
step:1620/2330 train_time:62446ms step_avg:38.55ms
step:1621/2330 train_time:62482ms step_avg:38.55ms
step:1622/2330 train_time:62524ms step_avg:38.55ms
step:1623/2330 train_time:62559ms step_avg:38.55ms
step:1624/2330 train_time:62600ms step_avg:38.55ms
step:1625/2330 train_time:62636ms step_avg:38.55ms
step:1626/2330 train_time:62677ms step_avg:38.55ms
step:1627/2330 train_time:62712ms step_avg:38.54ms
step:1628/2330 train_time:62754ms step_avg:38.55ms
step:1629/2330 train_time:62789ms step_avg:38.54ms
step:1630/2330 train_time:62831ms step_avg:38.55ms
step:1631/2330 train_time:62866ms step_avg:38.54ms
step:1632/2330 train_time:62908ms step_avg:38.55ms
step:1633/2330 train_time:62943ms step_avg:38.54ms
step:1634/2330 train_time:62985ms step_avg:38.55ms
step:1635/2330 train_time:63021ms step_avg:38.54ms
step:1636/2330 train_time:63061ms step_avg:38.55ms
step:1637/2330 train_time:63099ms step_avg:38.55ms
step:1638/2330 train_time:63140ms step_avg:38.55ms
step:1639/2330 train_time:63177ms step_avg:38.55ms
step:1640/2330 train_time:63218ms step_avg:38.55ms
step:1641/2330 train_time:63254ms step_avg:38.55ms
step:1642/2330 train_time:63295ms step_avg:38.55ms
step:1643/2330 train_time:63331ms step_avg:38.55ms
step:1644/2330 train_time:63372ms step_avg:38.55ms
step:1645/2330 train_time:63408ms step_avg:38.55ms
step:1646/2330 train_time:63450ms step_avg:38.55ms
step:1647/2330 train_time:63486ms step_avg:38.55ms
step:1648/2330 train_time:63528ms step_avg:38.55ms
step:1649/2330 train_time:63563ms step_avg:38.55ms
step:1650/2330 train_time:63605ms step_avg:38.55ms
step:1651/2330 train_time:63642ms step_avg:38.55ms
step:1652/2330 train_time:63682ms step_avg:38.55ms
step:1653/2330 train_time:63719ms step_avg:38.55ms
step:1654/2330 train_time:63760ms step_avg:38.55ms
step:1655/2330 train_time:63796ms step_avg:38.55ms
step:1656/2330 train_time:63837ms step_avg:38.55ms
step:1657/2330 train_time:63873ms step_avg:38.55ms
step:1658/2330 train_time:63914ms step_avg:38.55ms
step:1659/2330 train_time:63951ms step_avg:38.55ms
step:1660/2330 train_time:63992ms step_avg:38.55ms
step:1661/2330 train_time:64028ms step_avg:38.55ms
step:1662/2330 train_time:64070ms step_avg:38.55ms
step:1663/2330 train_time:64106ms step_avg:38.55ms
step:1664/2330 train_time:64148ms step_avg:38.55ms
step:1665/2330 train_time:64184ms step_avg:38.55ms
step:1666/2330 train_time:64226ms step_avg:38.55ms
step:1667/2330 train_time:64262ms step_avg:38.55ms
step:1668/2330 train_time:64303ms step_avg:38.55ms
step:1669/2330 train_time:64340ms step_avg:38.55ms
step:1670/2330 train_time:64381ms step_avg:38.55ms
step:1671/2330 train_time:64418ms step_avg:38.55ms
step:1672/2330 train_time:64459ms step_avg:38.55ms
step:1673/2330 train_time:64496ms step_avg:38.55ms
step:1674/2330 train_time:64537ms step_avg:38.55ms
step:1675/2330 train_time:64573ms step_avg:38.55ms
step:1676/2330 train_time:64614ms step_avg:38.55ms
step:1677/2330 train_time:64650ms step_avg:38.55ms
step:1678/2330 train_time:64691ms step_avg:38.55ms
step:1679/2330 train_time:64727ms step_avg:38.55ms
step:1680/2330 train_time:64768ms step_avg:38.55ms
step:1681/2330 train_time:64804ms step_avg:38.55ms
step:1682/2330 train_time:64845ms step_avg:38.55ms
step:1683/2330 train_time:64882ms step_avg:38.55ms
step:1684/2330 train_time:64923ms step_avg:38.55ms
step:1685/2330 train_time:64959ms step_avg:38.55ms
step:1686/2330 train_time:65000ms step_avg:38.55ms
step:1687/2330 train_time:65036ms step_avg:38.55ms
step:1688/2330 train_time:65077ms step_avg:38.55ms
step:1689/2330 train_time:65114ms step_avg:38.55ms
step:1690/2330 train_time:65154ms step_avg:38.55ms
step:1691/2330 train_time:65191ms step_avg:38.55ms
step:1692/2330 train_time:65232ms step_avg:38.55ms
step:1693/2330 train_time:65268ms step_avg:38.55ms
step:1694/2330 train_time:65310ms step_avg:38.55ms
step:1695/2330 train_time:65346ms step_avg:38.55ms
step:1696/2330 train_time:65388ms step_avg:38.55ms
step:1697/2330 train_time:65424ms step_avg:38.55ms
step:1698/2330 train_time:65465ms step_avg:38.55ms
step:1699/2330 train_time:65501ms step_avg:38.55ms
step:1700/2330 train_time:65542ms step_avg:38.55ms
step:1701/2330 train_time:65578ms step_avg:38.55ms
step:1702/2330 train_time:65619ms step_avg:38.55ms
step:1703/2330 train_time:65654ms step_avg:38.55ms
step:1704/2330 train_time:65695ms step_avg:38.55ms
step:1705/2330 train_time:65731ms step_avg:38.55ms
step:1706/2330 train_time:65772ms step_avg:38.55ms
step:1707/2330 train_time:65808ms step_avg:38.55ms
step:1708/2330 train_time:65850ms step_avg:38.55ms
step:1709/2330 train_time:65885ms step_avg:38.55ms
step:1710/2330 train_time:65927ms step_avg:38.55ms
step:1711/2330 train_time:65963ms step_avg:38.55ms
step:1712/2330 train_time:66004ms step_avg:38.55ms
step:1713/2330 train_time:66042ms step_avg:38.55ms
step:1714/2330 train_time:66083ms step_avg:38.55ms
step:1715/2330 train_time:66120ms step_avg:38.55ms
step:1716/2330 train_time:66161ms step_avg:38.56ms
step:1717/2330 train_time:66198ms step_avg:38.55ms
step:1718/2330 train_time:66239ms step_avg:38.56ms
step:1719/2330 train_time:66274ms step_avg:38.55ms
step:1720/2330 train_time:66315ms step_avg:38.56ms
step:1721/2330 train_time:66351ms step_avg:38.55ms
step:1722/2330 train_time:66393ms step_avg:38.56ms
step:1723/2330 train_time:66428ms step_avg:38.55ms
step:1724/2330 train_time:66470ms step_avg:38.56ms
step:1725/2330 train_time:66506ms step_avg:38.55ms
step:1726/2330 train_time:66547ms step_avg:38.56ms
step:1727/2330 train_time:66583ms step_avg:38.55ms
step:1728/2330 train_time:66624ms step_avg:38.56ms
step:1729/2330 train_time:66659ms step_avg:38.55ms
step:1730/2330 train_time:66700ms step_avg:38.55ms
step:1731/2330 train_time:66737ms step_avg:38.55ms
step:1732/2330 train_time:66778ms step_avg:38.56ms
step:1733/2330 train_time:66814ms step_avg:38.55ms
step:1734/2330 train_time:66855ms step_avg:38.56ms
step:1735/2330 train_time:66890ms step_avg:38.55ms
step:1736/2330 train_time:66932ms step_avg:38.56ms
step:1737/2330 train_time:66968ms step_avg:38.55ms
step:1738/2330 train_time:67010ms step_avg:38.56ms
step:1739/2330 train_time:67046ms step_avg:38.55ms
step:1740/2330 train_time:67088ms step_avg:38.56ms
step:1741/2330 train_time:67124ms step_avg:38.56ms
step:1742/2330 train_time:67166ms step_avg:38.56ms
step:1743/2330 train_time:67203ms step_avg:38.56ms
step:1744/2330 train_time:67244ms step_avg:38.56ms
step:1745/2330 train_time:67281ms step_avg:38.56ms
step:1746/2330 train_time:67321ms step_avg:38.56ms
step:1747/2330 train_time:67358ms step_avg:38.56ms
step:1748/2330 train_time:67399ms step_avg:38.56ms
step:1749/2330 train_time:67435ms step_avg:38.56ms
step:1750/2330 train_time:67476ms step_avg:38.56ms
step:1750/2330 val_loss:5.2113 train_time:67589ms step_avg:38.62ms
step:1751/2330 train_time:67601ms step_avg:38.61ms
step:1752/2330 train_time:67611ms step_avg:38.59ms
step:1753/2330 train_time:67621ms step_avg:38.57ms
step:1754/2330 train_time:67633ms step_avg:38.56ms
step:1755/2330 train_time:67666ms step_avg:38.56ms
step:1756/2330 train_time:67707ms step_avg:38.56ms
step:1757/2330 train_time:67742ms step_avg:38.56ms
step:1758/2330 train_time:67783ms step_avg:38.56ms
step:1759/2330 train_time:67818ms step_avg:38.56ms
step:1760/2330 train_time:67859ms step_avg:38.56ms
step:1761/2330 train_time:67896ms step_avg:38.56ms
step:1762/2330 train_time:67937ms step_avg:38.56ms
step:1763/2330 train_time:67982ms step_avg:38.56ms
step:1764/2330 train_time:68023ms step_avg:38.56ms
step:1765/2330 train_time:68063ms step_avg:38.56ms
step:1766/2330 train_time:68104ms step_avg:38.56ms
step:1767/2330 train_time:68140ms step_avg:38.56ms
step:1768/2330 train_time:68181ms step_avg:38.56ms
step:1769/2330 train_time:68216ms step_avg:38.56ms
step:1770/2330 train_time:68257ms step_avg:38.56ms
step:1771/2330 train_time:68293ms step_avg:38.56ms
step:1772/2330 train_time:68334ms step_avg:38.56ms
step:1773/2330 train_time:68369ms step_avg:38.56ms
step:1774/2330 train_time:68410ms step_avg:38.56ms
step:1775/2330 train_time:68444ms step_avg:38.56ms
step:1776/2330 train_time:68486ms step_avg:38.56ms
step:1777/2330 train_time:68522ms step_avg:38.56ms
step:1778/2330 train_time:68564ms step_avg:38.56ms
step:1779/2330 train_time:68599ms step_avg:38.56ms
step:1780/2330 train_time:68641ms step_avg:38.56ms
step:1781/2330 train_time:68676ms step_avg:38.56ms
step:1782/2330 train_time:68717ms step_avg:38.56ms
step:1783/2330 train_time:68752ms step_avg:38.56ms
step:1784/2330 train_time:68793ms step_avg:38.56ms
step:1785/2330 train_time:68830ms step_avg:38.56ms
step:1786/2330 train_time:68872ms step_avg:38.56ms
step:1787/2330 train_time:68908ms step_avg:38.56ms
step:1788/2330 train_time:68950ms step_avg:38.56ms
step:1789/2330 train_time:68987ms step_avg:38.56ms
step:1790/2330 train_time:69028ms step_avg:38.56ms
step:1791/2330 train_time:69066ms step_avg:38.56ms
step:1792/2330 train_time:69106ms step_avg:38.56ms
step:1793/2330 train_time:69144ms step_avg:38.56ms
step:1794/2330 train_time:69185ms step_avg:38.56ms
step:1795/2330 train_time:69222ms step_avg:38.56ms
step:1796/2330 train_time:69263ms step_avg:38.57ms
step:1797/2330 train_time:69299ms step_avg:38.56ms
step:1798/2330 train_time:69340ms step_avg:38.56ms
step:1799/2330 train_time:69376ms step_avg:38.56ms
step:1800/2330 train_time:69416ms step_avg:38.56ms
step:1801/2330 train_time:69453ms step_avg:38.56ms
step:1802/2330 train_time:69494ms step_avg:38.56ms
step:1803/2330 train_time:69529ms step_avg:38.56ms
step:1804/2330 train_time:69571ms step_avg:38.56ms
step:1805/2330 train_time:69606ms step_avg:38.56ms
step:1806/2330 train_time:69647ms step_avg:38.56ms
step:1807/2330 train_time:69683ms step_avg:38.56ms
step:1808/2330 train_time:69725ms step_avg:38.56ms
step:1809/2330 train_time:69760ms step_avg:38.56ms
step:1810/2330 train_time:69802ms step_avg:38.56ms
step:1811/2330 train_time:69837ms step_avg:38.56ms
step:1812/2330 train_time:69879ms step_avg:38.56ms
step:1813/2330 train_time:69916ms step_avg:38.56ms
step:1814/2330 train_time:69956ms step_avg:38.56ms
step:1815/2330 train_time:69993ms step_avg:38.56ms
step:1816/2330 train_time:70034ms step_avg:38.57ms
step:1817/2330 train_time:70073ms step_avg:38.57ms
step:1818/2330 train_time:70114ms step_avg:38.57ms
step:1819/2330 train_time:70151ms step_avg:38.57ms
step:1820/2330 train_time:70192ms step_avg:38.57ms
step:1821/2330 train_time:70228ms step_avg:38.57ms
step:1822/2330 train_time:70269ms step_avg:38.57ms
step:1823/2330 train_time:70304ms step_avg:38.57ms
step:1824/2330 train_time:70345ms step_avg:38.57ms
step:1825/2330 train_time:70381ms step_avg:38.57ms
step:1826/2330 train_time:70422ms step_avg:38.57ms
step:1827/2330 train_time:70458ms step_avg:38.56ms
step:1828/2330 train_time:70499ms step_avg:38.57ms
step:1829/2330 train_time:70534ms step_avg:38.56ms
step:1830/2330 train_time:70575ms step_avg:38.57ms
step:1831/2330 train_time:70611ms step_avg:38.56ms
step:1832/2330 train_time:70652ms step_avg:38.57ms
step:1833/2330 train_time:70688ms step_avg:38.56ms
step:1834/2330 train_time:70729ms step_avg:38.57ms
step:1835/2330 train_time:70766ms step_avg:38.56ms
step:1836/2330 train_time:70807ms step_avg:38.57ms
step:1837/2330 train_time:70843ms step_avg:38.56ms
step:1838/2330 train_time:70885ms step_avg:38.57ms
step:1839/2330 train_time:70921ms step_avg:38.57ms
step:1840/2330 train_time:70963ms step_avg:38.57ms
step:1841/2330 train_time:70999ms step_avg:38.57ms
step:1842/2330 train_time:71040ms step_avg:38.57ms
step:1843/2330 train_time:71078ms step_avg:38.57ms
step:1844/2330 train_time:71119ms step_avg:38.57ms
step:1845/2330 train_time:71156ms step_avg:38.57ms
step:1846/2330 train_time:71197ms step_avg:38.57ms
step:1847/2330 train_time:71234ms step_avg:38.57ms
step:1848/2330 train_time:71275ms step_avg:38.57ms
step:1849/2330 train_time:71312ms step_avg:38.57ms
step:1850/2330 train_time:71352ms step_avg:38.57ms
step:1851/2330 train_time:71388ms step_avg:38.57ms
step:1852/2330 train_time:71429ms step_avg:38.57ms
step:1853/2330 train_time:71464ms step_avg:38.57ms
step:1854/2330 train_time:71505ms step_avg:38.57ms
step:1855/2330 train_time:71541ms step_avg:38.57ms
step:1856/2330 train_time:71582ms step_avg:38.57ms
step:1857/2330 train_time:71618ms step_avg:38.57ms
step:1858/2330 train_time:71659ms step_avg:38.57ms
step:1859/2330 train_time:71696ms step_avg:38.57ms
step:1860/2330 train_time:71736ms step_avg:38.57ms
step:1861/2330 train_time:71774ms step_avg:38.57ms
step:1862/2330 train_time:71815ms step_avg:38.57ms
step:1863/2330 train_time:71851ms step_avg:38.57ms
step:1864/2330 train_time:71892ms step_avg:38.57ms
step:1865/2330 train_time:71928ms step_avg:38.57ms
step:1866/2330 train_time:71969ms step_avg:38.57ms
step:1867/2330 train_time:72006ms step_avg:38.57ms
step:1868/2330 train_time:72047ms step_avg:38.57ms
step:1869/2330 train_time:72083ms step_avg:38.57ms
step:1870/2330 train_time:72125ms step_avg:38.57ms
step:1871/2330 train_time:72161ms step_avg:38.57ms
step:1872/2330 train_time:72202ms step_avg:38.57ms
step:1873/2330 train_time:72238ms step_avg:38.57ms
step:1874/2330 train_time:72280ms step_avg:38.57ms
step:1875/2330 train_time:72316ms step_avg:38.57ms
step:1876/2330 train_time:72357ms step_avg:38.57ms
step:1877/2330 train_time:72394ms step_avg:38.57ms
step:1878/2330 train_time:72434ms step_avg:38.57ms
step:1879/2330 train_time:72471ms step_avg:38.57ms
step:1880/2330 train_time:72511ms step_avg:38.57ms
step:1881/2330 train_time:72547ms step_avg:38.57ms
step:1882/2330 train_time:72588ms step_avg:38.57ms
step:1883/2330 train_time:72624ms step_avg:38.57ms
step:1884/2330 train_time:72665ms step_avg:38.57ms
step:1885/2330 train_time:72701ms step_avg:38.57ms
step:1886/2330 train_time:72742ms step_avg:38.57ms
step:1887/2330 train_time:72778ms step_avg:38.57ms
step:1888/2330 train_time:72819ms step_avg:38.57ms
step:1889/2330 train_time:72856ms step_avg:38.57ms
step:1890/2330 train_time:72896ms step_avg:38.57ms
step:1891/2330 train_time:72933ms step_avg:38.57ms
step:1892/2330 train_time:72974ms step_avg:38.57ms
step:1893/2330 train_time:73011ms step_avg:38.57ms
step:1894/2330 train_time:73052ms step_avg:38.57ms
step:1895/2330 train_time:73088ms step_avg:38.57ms
step:1896/2330 train_time:73129ms step_avg:38.57ms
step:1897/2330 train_time:73165ms step_avg:38.57ms
step:1898/2330 train_time:73206ms step_avg:38.57ms
step:1899/2330 train_time:73243ms step_avg:38.57ms
step:1900/2330 train_time:73284ms step_avg:38.57ms
step:1901/2330 train_time:73321ms step_avg:38.57ms
step:1902/2330 train_time:73362ms step_avg:38.57ms
step:1903/2330 train_time:73397ms step_avg:38.57ms
step:1904/2330 train_time:73438ms step_avg:38.57ms
step:1905/2330 train_time:73475ms step_avg:38.57ms
step:1906/2330 train_time:73516ms step_avg:38.57ms
step:1907/2330 train_time:73552ms step_avg:38.57ms
step:1908/2330 train_time:73593ms step_avg:38.57ms
step:1909/2330 train_time:73629ms step_avg:38.57ms
step:1910/2330 train_time:73670ms step_avg:38.57ms
step:1911/2330 train_time:73705ms step_avg:38.57ms
step:1912/2330 train_time:73746ms step_avg:38.57ms
step:1913/2330 train_time:73782ms step_avg:38.57ms
step:1914/2330 train_time:73824ms step_avg:38.57ms
step:1915/2330 train_time:73860ms step_avg:38.57ms
step:1916/2330 train_time:73901ms step_avg:38.57ms
step:1917/2330 train_time:73937ms step_avg:38.57ms
step:1918/2330 train_time:73979ms step_avg:38.57ms
step:1919/2330 train_time:74015ms step_avg:38.57ms
step:1920/2330 train_time:74056ms step_avg:38.57ms
step:1921/2330 train_time:74093ms step_avg:38.57ms
step:1922/2330 train_time:74134ms step_avg:38.57ms
step:1923/2330 train_time:74170ms step_avg:38.57ms
step:1924/2330 train_time:74211ms step_avg:38.57ms
step:1925/2330 train_time:74247ms step_avg:38.57ms
step:1926/2330 train_time:74288ms step_avg:38.57ms
step:1927/2330 train_time:74324ms step_avg:38.57ms
step:1928/2330 train_time:74366ms step_avg:38.57ms
step:1929/2330 train_time:74402ms step_avg:38.57ms
step:1930/2330 train_time:74444ms step_avg:38.57ms
step:1931/2330 train_time:74479ms step_avg:38.57ms
step:1932/2330 train_time:74521ms step_avg:38.57ms
step:1933/2330 train_time:74557ms step_avg:38.57ms
step:1934/2330 train_time:74598ms step_avg:38.57ms
step:1935/2330 train_time:74634ms step_avg:38.57ms
step:1936/2330 train_time:74675ms step_avg:38.57ms
step:1937/2330 train_time:74712ms step_avg:38.57ms
step:1938/2330 train_time:74752ms step_avg:38.57ms
step:1939/2330 train_time:74788ms step_avg:38.57ms
step:1940/2330 train_time:74829ms step_avg:38.57ms
step:1941/2330 train_time:74865ms step_avg:38.57ms
step:1942/2330 train_time:74905ms step_avg:38.57ms
step:1943/2330 train_time:74942ms step_avg:38.57ms
step:1944/2330 train_time:74984ms step_avg:38.57ms
step:1945/2330 train_time:75021ms step_avg:38.57ms
step:1946/2330 train_time:75063ms step_avg:38.57ms
step:1947/2330 train_time:75099ms step_avg:38.57ms
step:1948/2330 train_time:75140ms step_avg:38.57ms
step:1949/2330 train_time:75178ms step_avg:38.57ms
step:1950/2330 train_time:75218ms step_avg:38.57ms
step:1951/2330 train_time:75256ms step_avg:38.57ms
step:1952/2330 train_time:75296ms step_avg:38.57ms
step:1953/2330 train_time:75333ms step_avg:38.57ms
step:1954/2330 train_time:75374ms step_avg:38.57ms
step:1955/2330 train_time:75410ms step_avg:38.57ms
step:1956/2330 train_time:75451ms step_avg:38.57ms
step:1957/2330 train_time:75486ms step_avg:38.57ms
step:1958/2330 train_time:75528ms step_avg:38.57ms
step:1959/2330 train_time:75563ms step_avg:38.57ms
step:1960/2330 train_time:75605ms step_avg:38.57ms
step:1961/2330 train_time:75641ms step_avg:38.57ms
step:1962/2330 train_time:75682ms step_avg:38.57ms
step:1963/2330 train_time:75719ms step_avg:38.57ms
step:1964/2330 train_time:75760ms step_avg:38.57ms
step:1965/2330 train_time:75796ms step_avg:38.57ms
step:1966/2330 train_time:75837ms step_avg:38.57ms
step:1967/2330 train_time:75874ms step_avg:38.57ms
step:1968/2330 train_time:75914ms step_avg:38.57ms
step:1969/2330 train_time:75951ms step_avg:38.57ms
step:1970/2330 train_time:75992ms step_avg:38.57ms
step:1971/2330 train_time:76028ms step_avg:38.57ms
step:1972/2330 train_time:76069ms step_avg:38.57ms
step:1973/2330 train_time:76105ms step_avg:38.57ms
step:1974/2330 train_time:76146ms step_avg:38.57ms
step:1975/2330 train_time:76182ms step_avg:38.57ms
step:1976/2330 train_time:76224ms step_avg:38.57ms
step:1977/2330 train_time:76260ms step_avg:38.57ms
step:1978/2330 train_time:76301ms step_avg:38.57ms
step:1979/2330 train_time:76337ms step_avg:38.57ms
step:1980/2330 train_time:76378ms step_avg:38.57ms
step:1981/2330 train_time:76415ms step_avg:38.57ms
step:1982/2330 train_time:76455ms step_avg:38.57ms
step:1983/2330 train_time:76492ms step_avg:38.57ms
step:1984/2330 train_time:76533ms step_avg:38.58ms
step:1985/2330 train_time:76569ms step_avg:38.57ms
step:1986/2330 train_time:76610ms step_avg:38.58ms
step:1987/2330 train_time:76645ms step_avg:38.57ms
step:1988/2330 train_time:76688ms step_avg:38.58ms
step:1989/2330 train_time:76723ms step_avg:38.57ms
step:1990/2330 train_time:76765ms step_avg:38.58ms
step:1991/2330 train_time:76801ms step_avg:38.57ms
step:1992/2330 train_time:76843ms step_avg:38.58ms
step:1993/2330 train_time:76879ms step_avg:38.57ms
step:1994/2330 train_time:76920ms step_avg:38.58ms
step:1995/2330 train_time:76957ms step_avg:38.57ms
step:1996/2330 train_time:76997ms step_avg:38.58ms
step:1997/2330 train_time:77036ms step_avg:38.58ms
step:1998/2330 train_time:77076ms step_avg:38.58ms
step:1999/2330 train_time:77114ms step_avg:38.58ms
step:2000/2330 train_time:77154ms step_avg:38.58ms
step:2000/2330 val_loss:5.1786 train_time:77268ms step_avg:38.63ms
step:2001/2330 train_time:77280ms step_avg:38.62ms
step:2002/2330 train_time:77291ms step_avg:38.61ms
step:2003/2330 train_time:77300ms step_avg:38.59ms
step:2004/2330 train_time:77311ms step_avg:38.58ms
step:2005/2330 train_time:77347ms step_avg:38.58ms
step:2006/2330 train_time:77388ms step_avg:38.58ms
step:2007/2330 train_time:77422ms step_avg:38.58ms
step:2008/2330 train_time:77463ms step_avg:38.58ms
step:2009/2330 train_time:77498ms step_avg:38.58ms
step:2010/2330 train_time:77539ms step_avg:38.58ms
step:2011/2330 train_time:77578ms step_avg:38.58ms
step:2012/2330 train_time:77619ms step_avg:38.58ms
step:2013/2330 train_time:77662ms step_avg:38.58ms
step:2014/2330 train_time:77703ms step_avg:38.58ms
step:2015/2330 train_time:77742ms step_avg:38.58ms
step:2016/2330 train_time:77782ms step_avg:38.58ms
step:2017/2330 train_time:77819ms step_avg:38.58ms
step:2018/2330 train_time:77860ms step_avg:38.58ms
step:2019/2330 train_time:77897ms step_avg:38.58ms
step:2020/2330 train_time:77937ms step_avg:38.58ms
step:2021/2330 train_time:77972ms step_avg:38.58ms
step:2022/2330 train_time:78013ms step_avg:38.58ms
step:2023/2330 train_time:78049ms step_avg:38.58ms
step:2024/2330 train_time:78090ms step_avg:38.58ms
step:2025/2330 train_time:78126ms step_avg:38.58ms
step:2026/2330 train_time:78167ms step_avg:38.58ms
step:2027/2330 train_time:78203ms step_avg:38.58ms
step:2028/2330 train_time:78244ms step_avg:38.58ms
step:2029/2330 train_time:78281ms step_avg:38.58ms
step:2030/2330 train_time:78321ms step_avg:38.58ms
step:2031/2330 train_time:78357ms step_avg:38.58ms
step:2032/2330 train_time:78398ms step_avg:38.58ms
step:2033/2330 train_time:78433ms step_avg:38.58ms
step:2034/2330 train_time:78474ms step_avg:38.58ms
step:2035/2330 train_time:78510ms step_avg:38.58ms
step:2036/2330 train_time:78552ms step_avg:38.58ms
step:2037/2330 train_time:78589ms step_avg:38.58ms
step:2038/2330 train_time:78631ms step_avg:38.58ms
step:2039/2330 train_time:78668ms step_avg:38.58ms
step:2040/2330 train_time:78710ms step_avg:38.58ms
step:2041/2330 train_time:78747ms step_avg:38.58ms
step:2042/2330 train_time:78789ms step_avg:38.58ms
step:2043/2330 train_time:78824ms step_avg:38.58ms
step:2044/2330 train_time:78866ms step_avg:38.58ms
step:2045/2330 train_time:78902ms step_avg:38.58ms
step:2046/2330 train_time:78943ms step_avg:38.58ms
step:2047/2330 train_time:78979ms step_avg:38.58ms
step:2048/2330 train_time:79020ms step_avg:38.58ms
step:2049/2330 train_time:79056ms step_avg:38.58ms
step:2050/2330 train_time:79096ms step_avg:38.58ms
step:2051/2330 train_time:79131ms step_avg:38.58ms
step:2052/2330 train_time:79172ms step_avg:38.58ms
step:2053/2330 train_time:79207ms step_avg:38.58ms
step:2054/2330 train_time:79248ms step_avg:38.58ms
step:2055/2330 train_time:79284ms step_avg:38.58ms
step:2056/2330 train_time:79325ms step_avg:38.58ms
step:2057/2330 train_time:79361ms step_avg:38.58ms
step:2058/2330 train_time:79402ms step_avg:38.58ms
step:2059/2330 train_time:79438ms step_avg:38.58ms
step:2060/2330 train_time:79479ms step_avg:38.58ms
step:2061/2330 train_time:79517ms step_avg:38.58ms
step:2062/2330 train_time:79557ms step_avg:38.58ms
step:2063/2330 train_time:79594ms step_avg:38.58ms
step:2064/2330 train_time:79635ms step_avg:38.58ms
step:2065/2330 train_time:79672ms step_avg:38.58ms
step:2066/2330 train_time:79713ms step_avg:38.58ms
step:2067/2330 train_time:79750ms step_avg:38.58ms
step:2068/2330 train_time:79791ms step_avg:38.58ms
step:2069/2330 train_time:79828ms step_avg:38.58ms
step:2070/2330 train_time:79870ms step_avg:38.58ms
step:2071/2330 train_time:79905ms step_avg:38.58ms
step:2072/2330 train_time:79946ms step_avg:38.58ms
step:2073/2330 train_time:79982ms step_avg:38.58ms
step:2074/2330 train_time:80023ms step_avg:38.58ms
step:2075/2330 train_time:80058ms step_avg:38.58ms
step:2076/2330 train_time:80100ms step_avg:38.58ms
step:2077/2330 train_time:80135ms step_avg:38.58ms
step:2078/2330 train_time:80176ms step_avg:38.58ms
step:2079/2330 train_time:80212ms step_avg:38.58ms
step:2080/2330 train_time:80253ms step_avg:38.58ms
step:2081/2330 train_time:80288ms step_avg:38.58ms
step:2082/2330 train_time:80330ms step_avg:38.58ms
step:2083/2330 train_time:80365ms step_avg:38.58ms
step:2084/2330 train_time:80407ms step_avg:38.58ms
step:2085/2330 train_time:80443ms step_avg:38.58ms
step:2086/2330 train_time:80485ms step_avg:38.58ms
step:2087/2330 train_time:80522ms step_avg:38.58ms
step:2088/2330 train_time:80563ms step_avg:38.58ms
step:2089/2330 train_time:80600ms step_avg:38.58ms
step:2090/2330 train_time:80641ms step_avg:38.58ms
step:2091/2330 train_time:80679ms step_avg:38.58ms
step:2092/2330 train_time:80719ms step_avg:38.58ms
step:2093/2330 train_time:80757ms step_avg:38.58ms
step:2094/2330 train_time:80797ms step_avg:38.59ms
step:2095/2330 train_time:80834ms step_avg:38.58ms
step:2096/2330 train_time:80875ms step_avg:38.59ms
step:2097/2330 train_time:80911ms step_avg:38.58ms
step:2098/2330 train_time:80952ms step_avg:38.59ms
step:2099/2330 train_time:80989ms step_avg:38.58ms
step:2100/2330 train_time:81030ms step_avg:38.59ms
step:2101/2330 train_time:81066ms step_avg:38.58ms
step:2102/2330 train_time:81108ms step_avg:38.59ms
step:2103/2330 train_time:81143ms step_avg:38.58ms
step:2104/2330 train_time:81184ms step_avg:38.59ms
step:2105/2330 train_time:81221ms step_avg:38.58ms
step:2106/2330 train_time:81262ms step_avg:38.59ms
step:2107/2330 train_time:81298ms step_avg:38.58ms
step:2108/2330 train_time:81339ms step_avg:38.59ms
step:2109/2330 train_time:81375ms step_avg:38.58ms
step:2110/2330 train_time:81416ms step_avg:38.59ms
step:2111/2330 train_time:81451ms step_avg:38.58ms
step:2112/2330 train_time:81492ms step_avg:38.59ms
step:2113/2330 train_time:81528ms step_avg:38.58ms
step:2114/2330 train_time:81570ms step_avg:38.59ms
step:2115/2330 train_time:81606ms step_avg:38.58ms
step:2116/2330 train_time:81647ms step_avg:38.59ms
step:2117/2330 train_time:81684ms step_avg:38.58ms
step:2118/2330 train_time:81726ms step_avg:38.59ms
step:2119/2330 train_time:81762ms step_avg:38.59ms
step:2120/2330 train_time:81803ms step_avg:38.59ms
step:2121/2330 train_time:81840ms step_avg:38.59ms
step:2122/2330 train_time:81881ms step_avg:38.59ms
step:2123/2330 train_time:81918ms step_avg:38.59ms
step:2124/2330 train_time:81958ms step_avg:38.59ms
step:2125/2330 train_time:81994ms step_avg:38.59ms
step:2126/2330 train_time:82035ms step_avg:38.59ms
step:2127/2330 train_time:82071ms step_avg:38.59ms
step:2128/2330 train_time:82112ms step_avg:38.59ms
step:2129/2330 train_time:82147ms step_avg:38.59ms
step:2130/2330 train_time:82189ms step_avg:38.59ms
step:2131/2330 train_time:82225ms step_avg:38.58ms
step:2132/2330 train_time:82266ms step_avg:38.59ms
step:2133/2330 train_time:82301ms step_avg:38.58ms
step:2134/2330 train_time:82342ms step_avg:38.59ms
step:2135/2330 train_time:82379ms step_avg:38.59ms
step:2136/2330 train_time:82420ms step_avg:38.59ms
step:2137/2330 train_time:82456ms step_avg:38.58ms
step:2138/2330 train_time:82497ms step_avg:38.59ms
step:2139/2330 train_time:82533ms step_avg:38.58ms
step:2140/2330 train_time:82574ms step_avg:38.59ms
step:2141/2330 train_time:82610ms step_avg:38.58ms
step:2142/2330 train_time:82651ms step_avg:38.59ms
step:2143/2330 train_time:82688ms step_avg:38.59ms
step:2144/2330 train_time:82730ms step_avg:38.59ms
step:2145/2330 train_time:82765ms step_avg:38.59ms
step:2146/2330 train_time:82807ms step_avg:38.59ms
step:2147/2330 train_time:82844ms step_avg:38.59ms
step:2148/2330 train_time:82886ms step_avg:38.59ms
step:2149/2330 train_time:82922ms step_avg:38.59ms
step:2150/2330 train_time:82963ms step_avg:38.59ms
step:2151/2330 train_time:83000ms step_avg:38.59ms
step:2152/2330 train_time:83040ms step_avg:38.59ms
step:2153/2330 train_time:83077ms step_avg:38.59ms
step:2154/2330 train_time:83118ms step_avg:38.59ms
step:2155/2330 train_time:83153ms step_avg:38.59ms
step:2156/2330 train_time:83195ms step_avg:38.59ms
step:2157/2330 train_time:83230ms step_avg:38.59ms
step:2158/2330 train_time:83271ms step_avg:38.59ms
step:2159/2330 train_time:83308ms step_avg:38.59ms
step:2160/2330 train_time:83349ms step_avg:38.59ms
step:2161/2330 train_time:83386ms step_avg:38.59ms
step:2162/2330 train_time:83427ms step_avg:38.59ms
step:2163/2330 train_time:83463ms step_avg:38.59ms
step:2164/2330 train_time:83504ms step_avg:38.59ms
step:2165/2330 train_time:83541ms step_avg:38.59ms
step:2166/2330 train_time:83582ms step_avg:38.59ms
step:2167/2330 train_time:83620ms step_avg:38.59ms
step:2168/2330 train_time:83661ms step_avg:38.59ms
step:2169/2330 train_time:83697ms step_avg:38.59ms
step:2170/2330 train_time:83738ms step_avg:38.59ms
step:2171/2330 train_time:83774ms step_avg:38.59ms
step:2172/2330 train_time:83815ms step_avg:38.59ms
step:2173/2330 train_time:83851ms step_avg:38.59ms
step:2174/2330 train_time:83892ms step_avg:38.59ms
step:2175/2330 train_time:83928ms step_avg:38.59ms
step:2176/2330 train_time:83970ms step_avg:38.59ms
step:2177/2330 train_time:84005ms step_avg:38.59ms
step:2178/2330 train_time:84046ms step_avg:38.59ms
step:2179/2330 train_time:84082ms step_avg:38.59ms
step:2180/2330 train_time:84123ms step_avg:38.59ms
step:2181/2330 train_time:84159ms step_avg:38.59ms
step:2182/2330 train_time:84201ms step_avg:38.59ms
step:2183/2330 train_time:84236ms step_avg:38.59ms
step:2184/2330 train_time:84277ms step_avg:38.59ms
step:2185/2330 train_time:84313ms step_avg:38.59ms
step:2186/2330 train_time:84354ms step_avg:38.59ms
step:2187/2330 train_time:84390ms step_avg:38.59ms
step:2188/2330 train_time:84431ms step_avg:38.59ms
step:2189/2330 train_time:84467ms step_avg:38.59ms
step:2190/2330 train_time:84509ms step_avg:38.59ms
step:2191/2330 train_time:84545ms step_avg:38.59ms
step:2192/2330 train_time:84587ms step_avg:38.59ms
step:2193/2330 train_time:84623ms step_avg:38.59ms
step:2194/2330 train_time:84664ms step_avg:38.59ms
step:2195/2330 train_time:84701ms step_avg:38.59ms
step:2196/2330 train_time:84742ms step_avg:38.59ms
step:2197/2330 train_time:84779ms step_avg:38.59ms
step:2198/2330 train_time:84819ms step_avg:38.59ms
step:2199/2330 train_time:84856ms step_avg:38.59ms
step:2200/2330 train_time:84897ms step_avg:38.59ms
step:2201/2330 train_time:84932ms step_avg:38.59ms
step:2202/2330 train_time:84973ms step_avg:38.59ms
step:2203/2330 train_time:85009ms step_avg:38.59ms
step:2204/2330 train_time:85051ms step_avg:38.59ms
step:2205/2330 train_time:85088ms step_avg:38.59ms
step:2206/2330 train_time:85129ms step_avg:38.59ms
step:2207/2330 train_time:85165ms step_avg:38.59ms
step:2208/2330 train_time:85206ms step_avg:38.59ms
step:2209/2330 train_time:85242ms step_avg:38.59ms
step:2210/2330 train_time:85282ms step_avg:38.59ms
step:2211/2330 train_time:85320ms step_avg:38.59ms
step:2212/2330 train_time:85360ms step_avg:38.59ms
step:2213/2330 train_time:85396ms step_avg:38.59ms
step:2214/2330 train_time:85438ms step_avg:38.59ms
step:2215/2330 train_time:85473ms step_avg:38.59ms
step:2216/2330 train_time:85514ms step_avg:38.59ms
step:2217/2330 train_time:85550ms step_avg:38.59ms
step:2218/2330 train_time:85592ms step_avg:38.59ms
step:2219/2330 train_time:85627ms step_avg:38.59ms
step:2220/2330 train_time:85669ms step_avg:38.59ms
step:2221/2330 train_time:85705ms step_avg:38.59ms
step:2222/2330 train_time:85747ms step_avg:38.59ms
step:2223/2330 train_time:85783ms step_avg:38.59ms
step:2224/2330 train_time:85825ms step_avg:38.59ms
step:2225/2330 train_time:85862ms step_avg:38.59ms
step:2226/2330 train_time:85902ms step_avg:38.59ms
step:2227/2330 train_time:85939ms step_avg:38.59ms
step:2228/2330 train_time:85980ms step_avg:38.59ms
step:2229/2330 train_time:86017ms step_avg:38.59ms
step:2230/2330 train_time:86057ms step_avg:38.59ms
step:2231/2330 train_time:86094ms step_avg:38.59ms
step:2232/2330 train_time:86134ms step_avg:38.59ms
step:2233/2330 train_time:86170ms step_avg:38.59ms
step:2234/2330 train_time:86211ms step_avg:38.59ms
step:2235/2330 train_time:86248ms step_avg:38.59ms
step:2236/2330 train_time:86290ms step_avg:38.59ms
step:2237/2330 train_time:86325ms step_avg:38.59ms
step:2238/2330 train_time:86366ms step_avg:38.59ms
step:2239/2330 train_time:86402ms step_avg:38.59ms
step:2240/2330 train_time:86442ms step_avg:38.59ms
step:2241/2330 train_time:86479ms step_avg:38.59ms
step:2242/2330 train_time:86520ms step_avg:38.59ms
step:2243/2330 train_time:86556ms step_avg:38.59ms
step:2244/2330 train_time:86598ms step_avg:38.59ms
step:2245/2330 train_time:86633ms step_avg:38.59ms
step:2246/2330 train_time:86674ms step_avg:38.59ms
step:2247/2330 train_time:86711ms step_avg:38.59ms
step:2248/2330 train_time:86752ms step_avg:38.59ms
step:2249/2330 train_time:86788ms step_avg:38.59ms
step:2250/2330 train_time:86830ms step_avg:38.59ms
step:2250/2330 val_loss:5.1504 train_time:86943ms step_avg:38.64ms
step:2251/2330 train_time:86956ms step_avg:38.63ms
step:2252/2330 train_time:86968ms step_avg:38.62ms
step:2253/2330 train_time:86978ms step_avg:38.61ms
step:2254/2330 train_time:86989ms step_avg:38.59ms
step:2255/2330 train_time:87021ms step_avg:38.59ms
step:2256/2330 train_time:87061ms step_avg:38.59ms
step:2257/2330 train_time:87097ms step_avg:38.59ms
step:2258/2330 train_time:87138ms step_avg:38.59ms
step:2259/2330 train_time:87173ms step_avg:38.59ms
step:2260/2330 train_time:87214ms step_avg:38.59ms
step:2261/2330 train_time:87250ms step_avg:38.59ms
step:2262/2330 train_time:87292ms step_avg:38.59ms
step:2263/2330 train_time:87333ms step_avg:38.59ms
step:2264/2330 train_time:87375ms step_avg:38.59ms
step:2265/2330 train_time:87413ms step_avg:38.59ms
step:2266/2330 train_time:87454ms step_avg:38.59ms
step:2267/2330 train_time:87488ms step_avg:38.59ms
step:2268/2330 train_time:87529ms step_avg:38.59ms
step:2269/2330 train_time:87565ms step_avg:38.59ms
step:2270/2330 train_time:87606ms step_avg:38.59ms
step:2271/2330 train_time:87642ms step_avg:38.59ms
step:2272/2330 train_time:87683ms step_avg:38.59ms
step:2273/2330 train_time:87719ms step_avg:38.59ms
step:2274/2330 train_time:87759ms step_avg:38.59ms
step:2275/2330 train_time:87795ms step_avg:38.59ms
step:2276/2330 train_time:87836ms step_avg:38.59ms
step:2277/2330 train_time:87872ms step_avg:38.59ms
step:2278/2330 train_time:87914ms step_avg:38.59ms
step:2279/2330 train_time:87949ms step_avg:38.59ms
step:2280/2330 train_time:87991ms step_avg:38.59ms
step:2281/2330 train_time:88026ms step_avg:38.59ms
step:2282/2330 train_time:88068ms step_avg:38.59ms
step:2283/2330 train_time:88103ms step_avg:38.59ms
step:2284/2330 train_time:88144ms step_avg:38.59ms
step:2285/2330 train_time:88180ms step_avg:38.59ms
step:2286/2330 train_time:88221ms step_avg:38.59ms
step:2287/2330 train_time:88259ms step_avg:38.59ms
step:2288/2330 train_time:88300ms step_avg:38.59ms
step:2289/2330 train_time:88339ms step_avg:38.59ms
step:2290/2330 train_time:88380ms step_avg:38.59ms
step:2291/2330 train_time:88416ms step_avg:38.59ms
step:2292/2330 train_time:88457ms step_avg:38.59ms
step:2293/2330 train_time:88494ms step_avg:38.59ms
step:2294/2330 train_time:88534ms step_avg:38.59ms
step:2295/2330 train_time:88570ms step_avg:38.59ms
step:2296/2330 train_time:88611ms step_avg:38.59ms
step:2297/2330 train_time:88648ms step_avg:38.59ms
step:2298/2330 train_time:88689ms step_avg:38.59ms
step:2299/2330 train_time:88725ms step_avg:38.59ms
step:2300/2330 train_time:88766ms step_avg:38.59ms
step:2301/2330 train_time:88802ms step_avg:38.59ms
step:2302/2330 train_time:88843ms step_avg:38.59ms
step:2303/2330 train_time:88880ms step_avg:38.59ms
step:2304/2330 train_time:88920ms step_avg:38.59ms
step:2305/2330 train_time:88957ms step_avg:38.59ms
step:2306/2330 train_time:88998ms step_avg:38.59ms
step:2307/2330 train_time:89033ms step_avg:38.59ms
step:2308/2330 train_time:89074ms step_avg:38.59ms
step:2309/2330 train_time:89110ms step_avg:38.59ms
step:2310/2330 train_time:89151ms step_avg:38.59ms
step:2311/2330 train_time:89188ms step_avg:38.59ms
step:2312/2330 train_time:89230ms step_avg:38.59ms
step:2313/2330 train_time:89266ms step_avg:38.59ms
step:2314/2330 train_time:89308ms step_avg:38.59ms
step:2315/2330 train_time:89345ms step_avg:38.59ms
step:2316/2330 train_time:89386ms step_avg:38.60ms
step:2317/2330 train_time:89423ms step_avg:38.59ms
step:2318/2330 train_time:89465ms step_avg:38.60ms
step:2319/2330 train_time:89502ms step_avg:38.59ms
step:2320/2330 train_time:89542ms step_avg:38.60ms
step:2321/2330 train_time:89580ms step_avg:38.60ms
step:2322/2330 train_time:89620ms step_avg:38.60ms
step:2323/2330 train_time:89657ms step_avg:38.60ms
step:2324/2330 train_time:89698ms step_avg:38.60ms
step:2325/2330 train_time:89736ms step_avg:38.60ms
step:2326/2330 train_time:89777ms step_avg:38.60ms
step:2327/2330 train_time:89812ms step_avg:38.60ms
step:2328/2330 train_time:89853ms step_avg:38.60ms
step:2329/2330 train_time:89888ms step_avg:38.60ms
step:2330/2330 train_time:89929ms step_avg:38.60ms
step:2330/2330 val_loss:5.1431 train_time:90043ms step_avg:38.64ms
peak memory allocated: 27165 MiB reserved: 38888 MiB
