import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = divide_by_norm(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = divide_by_norm(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "gd_7e-2"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=7e-2, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 03:29:05 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   34C    P0             113W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   32C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:102ms step_avg:102.27ms
step:2/2330 train_time:211ms step_avg:105.55ms
step:3/2330 train_time:233ms step_avg:77.73ms
step:4/2330 train_time:260ms step_avg:65.11ms
step:5/2330 train_time:317ms step_avg:63.35ms
step:6/2330 train_time:377ms step_avg:62.82ms
step:7/2330 train_time:434ms step_avg:62.04ms
step:8/2330 train_time:495ms step_avg:61.87ms
step:9/2330 train_time:553ms step_avg:61.43ms
step:10/2330 train_time:614ms step_avg:61.45ms
step:11/2330 train_time:673ms step_avg:61.15ms
step:12/2330 train_time:734ms step_avg:61.14ms
step:13/2330 train_time:791ms step_avg:60.87ms
step:14/2330 train_time:852ms step_avg:60.88ms
step:15/2330 train_time:911ms step_avg:60.73ms
step:16/2330 train_time:972ms step_avg:60.72ms
step:17/2330 train_time:1030ms step_avg:60.59ms
step:18/2330 train_time:1092ms step_avg:60.65ms
step:19/2330 train_time:1155ms step_avg:60.79ms
step:20/2330 train_time:1221ms step_avg:61.07ms
step:21/2330 train_time:1282ms step_avg:61.06ms
step:22/2330 train_time:1344ms step_avg:61.10ms
step:23/2330 train_time:1403ms step_avg:60.99ms
step:24/2330 train_time:1464ms step_avg:61.02ms
step:25/2330 train_time:1523ms step_avg:60.91ms
step:26/2330 train_time:1584ms step_avg:60.94ms
step:27/2330 train_time:1643ms step_avg:60.86ms
step:28/2330 train_time:1706ms step_avg:60.91ms
step:29/2330 train_time:1764ms step_avg:60.84ms
step:30/2330 train_time:1826ms step_avg:60.87ms
step:31/2330 train_time:1885ms step_avg:60.79ms
step:32/2330 train_time:1947ms step_avg:60.84ms
step:33/2330 train_time:2006ms step_avg:60.78ms
step:34/2330 train_time:2067ms step_avg:60.80ms
step:35/2330 train_time:2127ms step_avg:60.76ms
step:36/2330 train_time:2189ms step_avg:60.80ms
step:37/2330 train_time:2248ms step_avg:60.74ms
step:38/2330 train_time:2309ms step_avg:60.77ms
step:39/2330 train_time:2368ms step_avg:60.71ms
step:40/2330 train_time:2429ms step_avg:60.72ms
step:41/2330 train_time:2487ms step_avg:60.67ms
step:42/2330 train_time:2549ms step_avg:60.70ms
step:43/2330 train_time:2608ms step_avg:60.65ms
step:44/2330 train_time:2670ms step_avg:60.69ms
step:45/2330 train_time:2729ms step_avg:60.64ms
step:46/2330 train_time:2791ms step_avg:60.68ms
step:47/2330 train_time:2850ms step_avg:60.64ms
step:48/2330 train_time:2912ms step_avg:60.67ms
step:49/2330 train_time:2972ms step_avg:60.65ms
step:50/2330 train_time:3034ms step_avg:60.67ms
step:51/2330 train_time:3092ms step_avg:60.63ms
step:52/2330 train_time:3154ms step_avg:60.66ms
step:53/2330 train_time:3213ms step_avg:60.63ms
step:54/2330 train_time:3275ms step_avg:60.65ms
step:55/2330 train_time:3333ms step_avg:60.61ms
step:56/2330 train_time:3396ms step_avg:60.64ms
step:57/2330 train_time:3454ms step_avg:60.60ms
step:58/2330 train_time:3517ms step_avg:60.63ms
step:59/2330 train_time:3576ms step_avg:60.60ms
step:60/2330 train_time:3637ms step_avg:60.62ms
step:61/2330 train_time:3695ms step_avg:60.58ms
step:62/2330 train_time:3757ms step_avg:60.60ms
step:63/2330 train_time:3816ms step_avg:60.58ms
step:64/2330 train_time:3878ms step_avg:60.59ms
step:65/2330 train_time:3937ms step_avg:60.57ms
step:66/2330 train_time:3999ms step_avg:60.59ms
step:67/2330 train_time:4058ms step_avg:60.57ms
step:68/2330 train_time:4120ms step_avg:60.59ms
step:69/2330 train_time:4179ms step_avg:60.56ms
step:70/2330 train_time:4240ms step_avg:60.58ms
step:71/2330 train_time:4299ms step_avg:60.56ms
step:72/2330 train_time:4361ms step_avg:60.57ms
step:73/2330 train_time:4420ms step_avg:60.54ms
step:74/2330 train_time:4481ms step_avg:60.56ms
step:75/2330 train_time:4539ms step_avg:60.52ms
step:76/2330 train_time:4601ms step_avg:60.54ms
step:77/2330 train_time:4659ms step_avg:60.51ms
step:78/2330 train_time:4721ms step_avg:60.52ms
step:79/2330 train_time:4779ms step_avg:60.50ms
step:80/2330 train_time:4841ms step_avg:60.51ms
step:81/2330 train_time:4900ms step_avg:60.49ms
step:82/2330 train_time:4961ms step_avg:60.50ms
step:83/2330 train_time:5020ms step_avg:60.48ms
step:84/2330 train_time:5082ms step_avg:60.50ms
step:85/2330 train_time:5142ms step_avg:60.49ms
step:86/2330 train_time:5205ms step_avg:60.52ms
step:87/2330 train_time:5263ms step_avg:60.50ms
step:88/2330 train_time:5325ms step_avg:60.51ms
step:89/2330 train_time:5384ms step_avg:60.49ms
step:90/2330 train_time:5446ms step_avg:60.51ms
step:91/2330 train_time:5506ms step_avg:60.51ms
step:92/2330 train_time:5567ms step_avg:60.51ms
step:93/2330 train_time:5625ms step_avg:60.49ms
step:94/2330 train_time:5689ms step_avg:60.52ms
step:95/2330 train_time:5746ms step_avg:60.49ms
step:96/2330 train_time:5809ms step_avg:60.51ms
step:97/2330 train_time:5867ms step_avg:60.48ms
step:98/2330 train_time:5928ms step_avg:60.48ms
step:99/2330 train_time:5986ms step_avg:60.47ms
step:100/2330 train_time:6048ms step_avg:60.48ms
step:101/2330 train_time:6108ms step_avg:60.47ms
step:102/2330 train_time:6169ms step_avg:60.48ms
step:103/2330 train_time:6229ms step_avg:60.47ms
step:104/2330 train_time:6290ms step_avg:60.48ms
step:105/2330 train_time:6349ms step_avg:60.47ms
step:106/2330 train_time:6413ms step_avg:60.50ms
step:107/2330 train_time:6471ms step_avg:60.48ms
step:108/2330 train_time:6535ms step_avg:60.51ms
step:109/2330 train_time:6594ms step_avg:60.49ms
step:110/2330 train_time:6656ms step_avg:60.51ms
step:111/2330 train_time:6714ms step_avg:60.49ms
step:112/2330 train_time:6777ms step_avg:60.51ms
step:113/2330 train_time:6835ms step_avg:60.49ms
step:114/2330 train_time:6897ms step_avg:60.50ms
step:115/2330 train_time:6955ms step_avg:60.48ms
step:116/2330 train_time:7017ms step_avg:60.49ms
step:117/2330 train_time:7076ms step_avg:60.48ms
step:118/2330 train_time:7138ms step_avg:60.49ms
step:119/2330 train_time:7197ms step_avg:60.48ms
step:120/2330 train_time:7259ms step_avg:60.49ms
step:121/2330 train_time:7319ms step_avg:60.49ms
step:122/2330 train_time:7381ms step_avg:60.50ms
step:123/2330 train_time:7439ms step_avg:60.48ms
step:124/2330 train_time:7501ms step_avg:60.49ms
step:125/2330 train_time:7560ms step_avg:60.48ms
step:126/2330 train_time:7623ms step_avg:60.50ms
step:127/2330 train_time:7682ms step_avg:60.48ms
step:128/2330 train_time:7743ms step_avg:60.49ms
step:129/2330 train_time:7802ms step_avg:60.48ms
step:130/2330 train_time:7863ms step_avg:60.49ms
step:131/2330 train_time:7922ms step_avg:60.47ms
step:132/2330 train_time:7984ms step_avg:60.48ms
step:133/2330 train_time:8042ms step_avg:60.47ms
step:134/2330 train_time:8105ms step_avg:60.48ms
step:135/2330 train_time:8163ms step_avg:60.47ms
step:136/2330 train_time:8226ms step_avg:60.48ms
step:137/2330 train_time:8284ms step_avg:60.47ms
step:138/2330 train_time:8346ms step_avg:60.48ms
step:139/2330 train_time:8406ms step_avg:60.47ms
step:140/2330 train_time:8467ms step_avg:60.48ms
step:141/2330 train_time:8526ms step_avg:60.46ms
step:142/2330 train_time:8587ms step_avg:60.47ms
step:143/2330 train_time:8646ms step_avg:60.46ms
step:144/2330 train_time:8709ms step_avg:60.48ms
step:145/2330 train_time:8767ms step_avg:60.46ms
step:146/2330 train_time:8829ms step_avg:60.47ms
step:147/2330 train_time:8888ms step_avg:60.47ms
step:148/2330 train_time:8951ms step_avg:60.48ms
step:149/2330 train_time:9009ms step_avg:60.46ms
step:150/2330 train_time:9071ms step_avg:60.47ms
step:151/2330 train_time:9129ms step_avg:60.46ms
step:152/2330 train_time:9192ms step_avg:60.47ms
step:153/2330 train_time:9250ms step_avg:60.46ms
step:154/2330 train_time:9313ms step_avg:60.48ms
step:155/2330 train_time:9372ms step_avg:60.46ms
step:156/2330 train_time:9435ms step_avg:60.48ms
step:157/2330 train_time:9494ms step_avg:60.47ms
step:158/2330 train_time:9555ms step_avg:60.48ms
step:159/2330 train_time:9615ms step_avg:60.47ms
step:160/2330 train_time:9677ms step_avg:60.48ms
step:161/2330 train_time:9735ms step_avg:60.47ms
step:162/2330 train_time:9797ms step_avg:60.48ms
step:163/2330 train_time:9855ms step_avg:60.46ms
step:164/2330 train_time:9918ms step_avg:60.48ms
step:165/2330 train_time:9976ms step_avg:60.46ms
step:166/2330 train_time:10038ms step_avg:60.47ms
step:167/2330 train_time:10097ms step_avg:60.46ms
step:168/2330 train_time:10160ms step_avg:60.48ms
step:169/2330 train_time:10220ms step_avg:60.47ms
step:170/2330 train_time:10282ms step_avg:60.48ms
step:171/2330 train_time:10341ms step_avg:60.47ms
step:172/2330 train_time:10403ms step_avg:60.48ms
step:173/2330 train_time:10462ms step_avg:60.48ms
step:174/2330 train_time:10524ms step_avg:60.48ms
step:175/2330 train_time:10583ms step_avg:60.48ms
step:176/2330 train_time:10645ms step_avg:60.48ms
step:177/2330 train_time:10704ms step_avg:60.47ms
step:178/2330 train_time:10766ms step_avg:60.48ms
step:179/2330 train_time:10825ms step_avg:60.48ms
step:180/2330 train_time:10887ms step_avg:60.48ms
step:181/2330 train_time:10946ms step_avg:60.48ms
step:182/2330 train_time:11009ms step_avg:60.49ms
step:183/2330 train_time:11066ms step_avg:60.47ms
step:184/2330 train_time:11128ms step_avg:60.48ms
step:185/2330 train_time:11186ms step_avg:60.47ms
step:186/2330 train_time:11248ms step_avg:60.47ms
step:187/2330 train_time:11308ms step_avg:60.47ms
step:188/2330 train_time:11369ms step_avg:60.47ms
step:189/2330 train_time:11428ms step_avg:60.46ms
step:190/2330 train_time:11490ms step_avg:60.47ms
step:191/2330 train_time:11549ms step_avg:60.47ms
step:192/2330 train_time:11612ms step_avg:60.48ms
step:193/2330 train_time:11670ms step_avg:60.47ms
step:194/2330 train_time:11732ms step_avg:60.48ms
step:195/2330 train_time:11792ms step_avg:60.47ms
step:196/2330 train_time:11854ms step_avg:60.48ms
step:197/2330 train_time:11913ms step_avg:60.47ms
step:198/2330 train_time:11975ms step_avg:60.48ms
step:199/2330 train_time:12033ms step_avg:60.47ms
step:200/2330 train_time:12095ms step_avg:60.48ms
step:201/2330 train_time:12154ms step_avg:60.47ms
step:202/2330 train_time:12216ms step_avg:60.47ms
step:203/2330 train_time:12274ms step_avg:60.47ms
step:204/2330 train_time:12337ms step_avg:60.47ms
step:205/2330 train_time:12395ms step_avg:60.46ms
step:206/2330 train_time:12457ms step_avg:60.47ms
step:207/2330 train_time:12516ms step_avg:60.46ms
step:208/2330 train_time:12578ms step_avg:60.47ms
step:209/2330 train_time:12638ms step_avg:60.47ms
step:210/2330 train_time:12699ms step_avg:60.47ms
step:211/2330 train_time:12758ms step_avg:60.47ms
step:212/2330 train_time:12820ms step_avg:60.47ms
step:213/2330 train_time:12880ms step_avg:60.47ms
step:214/2330 train_time:12941ms step_avg:60.47ms
step:215/2330 train_time:13000ms step_avg:60.47ms
step:216/2330 train_time:13062ms step_avg:60.47ms
step:217/2330 train_time:13122ms step_avg:60.47ms
step:218/2330 train_time:13184ms step_avg:60.48ms
step:219/2330 train_time:13243ms step_avg:60.47ms
step:220/2330 train_time:13305ms step_avg:60.48ms
step:221/2330 train_time:13364ms step_avg:60.47ms
step:222/2330 train_time:13426ms step_avg:60.48ms
step:223/2330 train_time:13484ms step_avg:60.47ms
step:224/2330 train_time:13546ms step_avg:60.48ms
step:225/2330 train_time:13606ms step_avg:60.47ms
step:226/2330 train_time:13667ms step_avg:60.47ms
step:227/2330 train_time:13726ms step_avg:60.47ms
step:228/2330 train_time:13788ms step_avg:60.48ms
step:229/2330 train_time:13847ms step_avg:60.47ms
step:230/2330 train_time:13908ms step_avg:60.47ms
step:231/2330 train_time:13968ms step_avg:60.47ms
step:232/2330 train_time:14029ms step_avg:60.47ms
step:233/2330 train_time:14088ms step_avg:60.46ms
step:234/2330 train_time:14149ms step_avg:60.47ms
step:235/2330 train_time:14209ms step_avg:60.46ms
step:236/2330 train_time:14271ms step_avg:60.47ms
step:237/2330 train_time:14330ms step_avg:60.46ms
step:238/2330 train_time:14392ms step_avg:60.47ms
step:239/2330 train_time:14451ms step_avg:60.46ms
step:240/2330 train_time:14514ms step_avg:60.48ms
step:241/2330 train_time:14572ms step_avg:60.47ms
step:242/2330 train_time:14634ms step_avg:60.47ms
step:243/2330 train_time:14692ms step_avg:60.46ms
step:244/2330 train_time:14755ms step_avg:60.47ms
step:245/2330 train_time:14814ms step_avg:60.47ms
step:246/2330 train_time:14876ms step_avg:60.47ms
step:247/2330 train_time:14934ms step_avg:60.46ms
step:248/2330 train_time:14996ms step_avg:60.47ms
step:249/2330 train_time:15055ms step_avg:60.46ms
step:250/2330 train_time:15117ms step_avg:60.47ms
step:250/2330 val_loss:4.7123 train_time:15189ms step_avg:60.75ms
step:251/2330 train_time:15212ms step_avg:60.60ms
step:252/2330 train_time:15239ms step_avg:60.47ms
step:253/2330 train_time:15298ms step_avg:60.47ms
step:254/2330 train_time:15366ms step_avg:60.49ms
step:255/2330 train_time:15430ms step_avg:60.51ms
step:256/2330 train_time:15493ms step_avg:60.52ms
step:257/2330 train_time:15553ms step_avg:60.52ms
step:258/2330 train_time:15614ms step_avg:60.52ms
step:259/2330 train_time:15673ms step_avg:60.51ms
step:260/2330 train_time:15733ms step_avg:60.51ms
step:261/2330 train_time:15791ms step_avg:60.50ms
step:262/2330 train_time:15852ms step_avg:60.51ms
step:263/2330 train_time:15911ms step_avg:60.50ms
step:264/2330 train_time:15973ms step_avg:60.51ms
step:265/2330 train_time:16031ms step_avg:60.49ms
step:266/2330 train_time:16093ms step_avg:60.50ms
step:267/2330 train_time:16153ms step_avg:60.50ms
step:268/2330 train_time:16216ms step_avg:60.51ms
step:269/2330 train_time:16276ms step_avg:60.50ms
step:270/2330 train_time:16338ms step_avg:60.51ms
step:271/2330 train_time:16399ms step_avg:60.51ms
step:272/2330 train_time:16462ms step_avg:60.52ms
step:273/2330 train_time:16520ms step_avg:60.51ms
step:274/2330 train_time:16583ms step_avg:60.52ms
step:275/2330 train_time:16642ms step_avg:60.51ms
step:276/2330 train_time:16704ms step_avg:60.52ms
step:277/2330 train_time:16761ms step_avg:60.51ms
step:278/2330 train_time:16823ms step_avg:60.51ms
step:279/2330 train_time:16882ms step_avg:60.51ms
step:280/2330 train_time:16943ms step_avg:60.51ms
step:281/2330 train_time:17002ms step_avg:60.51ms
step:282/2330 train_time:17064ms step_avg:60.51ms
step:283/2330 train_time:17123ms step_avg:60.50ms
step:284/2330 train_time:17186ms step_avg:60.51ms
step:285/2330 train_time:17244ms step_avg:60.51ms
step:286/2330 train_time:17307ms step_avg:60.51ms
step:287/2330 train_time:17367ms step_avg:60.51ms
step:288/2330 train_time:17428ms step_avg:60.52ms
step:289/2330 train_time:17487ms step_avg:60.51ms
step:290/2330 train_time:17549ms step_avg:60.51ms
step:291/2330 train_time:17609ms step_avg:60.51ms
step:292/2330 train_time:17672ms step_avg:60.52ms
step:293/2330 train_time:17730ms step_avg:60.51ms
step:294/2330 train_time:17791ms step_avg:60.51ms
step:295/2330 train_time:17850ms step_avg:60.51ms
step:296/2330 train_time:17911ms step_avg:60.51ms
step:297/2330 train_time:17970ms step_avg:60.50ms
step:298/2330 train_time:18032ms step_avg:60.51ms
step:299/2330 train_time:18091ms step_avg:60.50ms
step:300/2330 train_time:18152ms step_avg:60.51ms
step:301/2330 train_time:18211ms step_avg:60.50ms
step:302/2330 train_time:18274ms step_avg:60.51ms
step:303/2330 train_time:18332ms step_avg:60.50ms
step:304/2330 train_time:18395ms step_avg:60.51ms
step:305/2330 train_time:18453ms step_avg:60.50ms
step:306/2330 train_time:18516ms step_avg:60.51ms
step:307/2330 train_time:18574ms step_avg:60.50ms
step:308/2330 train_time:18635ms step_avg:60.50ms
step:309/2330 train_time:18694ms step_avg:60.50ms
step:310/2330 train_time:18757ms step_avg:60.51ms
step:311/2330 train_time:18816ms step_avg:60.50ms
step:312/2330 train_time:18878ms step_avg:60.51ms
step:313/2330 train_time:18936ms step_avg:60.50ms
step:314/2330 train_time:18998ms step_avg:60.50ms
step:315/2330 train_time:19057ms step_avg:60.50ms
step:316/2330 train_time:19119ms step_avg:60.50ms
step:317/2330 train_time:19178ms step_avg:60.50ms
step:318/2330 train_time:19240ms step_avg:60.50ms
step:319/2330 train_time:19299ms step_avg:60.50ms
step:320/2330 train_time:19361ms step_avg:60.50ms
step:321/2330 train_time:19419ms step_avg:60.50ms
step:322/2330 train_time:19481ms step_avg:60.50ms
step:323/2330 train_time:19539ms step_avg:60.49ms
step:324/2330 train_time:19601ms step_avg:60.50ms
step:325/2330 train_time:19660ms step_avg:60.49ms
step:326/2330 train_time:19721ms step_avg:60.50ms
step:327/2330 train_time:19780ms step_avg:60.49ms
step:328/2330 train_time:19842ms step_avg:60.50ms
step:329/2330 train_time:19901ms step_avg:60.49ms
step:330/2330 train_time:19962ms step_avg:60.49ms
step:331/2330 train_time:20021ms step_avg:60.49ms
step:332/2330 train_time:20083ms step_avg:60.49ms
step:333/2330 train_time:20141ms step_avg:60.48ms
step:334/2330 train_time:20203ms step_avg:60.49ms
step:335/2330 train_time:20262ms step_avg:60.48ms
step:336/2330 train_time:20323ms step_avg:60.49ms
step:337/2330 train_time:20383ms step_avg:60.48ms
step:338/2330 train_time:20445ms step_avg:60.49ms
step:339/2330 train_time:20504ms step_avg:60.49ms
step:340/2330 train_time:20566ms step_avg:60.49ms
step:341/2330 train_time:20625ms step_avg:60.48ms
step:342/2330 train_time:20686ms step_avg:60.48ms
step:343/2330 train_time:20744ms step_avg:60.48ms
step:344/2330 train_time:20806ms step_avg:60.48ms
step:345/2330 train_time:20866ms step_avg:60.48ms
step:346/2330 train_time:20927ms step_avg:60.48ms
step:347/2330 train_time:20985ms step_avg:60.48ms
step:348/2330 train_time:21047ms step_avg:60.48ms
step:349/2330 train_time:21106ms step_avg:60.47ms
step:350/2330 train_time:21168ms step_avg:60.48ms
step:351/2330 train_time:21227ms step_avg:60.48ms
step:352/2330 train_time:21289ms step_avg:60.48ms
step:353/2330 train_time:21348ms step_avg:60.48ms
step:354/2330 train_time:21410ms step_avg:60.48ms
step:355/2330 train_time:21470ms step_avg:60.48ms
step:356/2330 train_time:21532ms step_avg:60.48ms
step:357/2330 train_time:21590ms step_avg:60.48ms
step:358/2330 train_time:21651ms step_avg:60.48ms
step:359/2330 train_time:21710ms step_avg:60.47ms
step:360/2330 train_time:21772ms step_avg:60.48ms
step:361/2330 train_time:21832ms step_avg:60.48ms
step:362/2330 train_time:21893ms step_avg:60.48ms
step:363/2330 train_time:21952ms step_avg:60.47ms
step:364/2330 train_time:22014ms step_avg:60.48ms
step:365/2330 train_time:22073ms step_avg:60.47ms
step:366/2330 train_time:22134ms step_avg:60.48ms
step:367/2330 train_time:22193ms step_avg:60.47ms
step:368/2330 train_time:22255ms step_avg:60.48ms
step:369/2330 train_time:22314ms step_avg:60.47ms
step:370/2330 train_time:22377ms step_avg:60.48ms
step:371/2330 train_time:22435ms step_avg:60.47ms
step:372/2330 train_time:22497ms step_avg:60.48ms
step:373/2330 train_time:22556ms step_avg:60.47ms
step:374/2330 train_time:22618ms step_avg:60.48ms
step:375/2330 train_time:22676ms step_avg:60.47ms
step:376/2330 train_time:22738ms step_avg:60.47ms
step:377/2330 train_time:22797ms step_avg:60.47ms
step:378/2330 train_time:22860ms step_avg:60.48ms
step:379/2330 train_time:22919ms step_avg:60.47ms
step:380/2330 train_time:22981ms step_avg:60.48ms
step:381/2330 train_time:23039ms step_avg:60.47ms
step:382/2330 train_time:23102ms step_avg:60.48ms
step:383/2330 train_time:23160ms step_avg:60.47ms
step:384/2330 train_time:23221ms step_avg:60.47ms
step:385/2330 train_time:23280ms step_avg:60.47ms
step:386/2330 train_time:23341ms step_avg:60.47ms
step:387/2330 train_time:23400ms step_avg:60.47ms
step:388/2330 train_time:23462ms step_avg:60.47ms
step:389/2330 train_time:23520ms step_avg:60.46ms
step:390/2330 train_time:23583ms step_avg:60.47ms
step:391/2330 train_time:23642ms step_avg:60.46ms
step:392/2330 train_time:23704ms step_avg:60.47ms
step:393/2330 train_time:23762ms step_avg:60.46ms
step:394/2330 train_time:23824ms step_avg:60.47ms
step:395/2330 train_time:23884ms step_avg:60.47ms
step:396/2330 train_time:23946ms step_avg:60.47ms
step:397/2330 train_time:24005ms step_avg:60.47ms
step:398/2330 train_time:24066ms step_avg:60.47ms
step:399/2330 train_time:24125ms step_avg:60.46ms
step:400/2330 train_time:24187ms step_avg:60.47ms
step:401/2330 train_time:24246ms step_avg:60.46ms
step:402/2330 train_time:24307ms step_avg:60.47ms
step:403/2330 train_time:24367ms step_avg:60.46ms
step:404/2330 train_time:24429ms step_avg:60.47ms
step:405/2330 train_time:24489ms step_avg:60.47ms
step:406/2330 train_time:24550ms step_avg:60.47ms
step:407/2330 train_time:24609ms step_avg:60.46ms
step:408/2330 train_time:24671ms step_avg:60.47ms
step:409/2330 train_time:24730ms step_avg:60.46ms
step:410/2330 train_time:24792ms step_avg:60.47ms
step:411/2330 train_time:24850ms step_avg:60.46ms
step:412/2330 train_time:24913ms step_avg:60.47ms
step:413/2330 train_time:24972ms step_avg:60.46ms
step:414/2330 train_time:25033ms step_avg:60.47ms
step:415/2330 train_time:25092ms step_avg:60.46ms
step:416/2330 train_time:25153ms step_avg:60.46ms
step:417/2330 train_time:25213ms step_avg:60.46ms
step:418/2330 train_time:25274ms step_avg:60.47ms
step:419/2330 train_time:25332ms step_avg:60.46ms
step:420/2330 train_time:25395ms step_avg:60.46ms
step:421/2330 train_time:25454ms step_avg:60.46ms
step:422/2330 train_time:25515ms step_avg:60.46ms
step:423/2330 train_time:25574ms step_avg:60.46ms
step:424/2330 train_time:25636ms step_avg:60.46ms
step:425/2330 train_time:25695ms step_avg:60.46ms
step:426/2330 train_time:25757ms step_avg:60.46ms
step:427/2330 train_time:25816ms step_avg:60.46ms
step:428/2330 train_time:25878ms step_avg:60.46ms
step:429/2330 train_time:25937ms step_avg:60.46ms
step:430/2330 train_time:25999ms step_avg:60.46ms
step:431/2330 train_time:26057ms step_avg:60.46ms
step:432/2330 train_time:26120ms step_avg:60.46ms
step:433/2330 train_time:26179ms step_avg:60.46ms
step:434/2330 train_time:26241ms step_avg:60.46ms
step:435/2330 train_time:26300ms step_avg:60.46ms
step:436/2330 train_time:26362ms step_avg:60.46ms
step:437/2330 train_time:26420ms step_avg:60.46ms
step:438/2330 train_time:26481ms step_avg:60.46ms
step:439/2330 train_time:26540ms step_avg:60.46ms
step:440/2330 train_time:26602ms step_avg:60.46ms
step:441/2330 train_time:26660ms step_avg:60.45ms
step:442/2330 train_time:26722ms step_avg:60.46ms
step:443/2330 train_time:26782ms step_avg:60.46ms
step:444/2330 train_time:26844ms step_avg:60.46ms
step:445/2330 train_time:26902ms step_avg:60.45ms
step:446/2330 train_time:26964ms step_avg:60.46ms
step:447/2330 train_time:27024ms step_avg:60.46ms
step:448/2330 train_time:27086ms step_avg:60.46ms
step:449/2330 train_time:27145ms step_avg:60.46ms
step:450/2330 train_time:27206ms step_avg:60.46ms
step:451/2330 train_time:27265ms step_avg:60.45ms
step:452/2330 train_time:27327ms step_avg:60.46ms
step:453/2330 train_time:27385ms step_avg:60.45ms
step:454/2330 train_time:27447ms step_avg:60.46ms
step:455/2330 train_time:27506ms step_avg:60.45ms
step:456/2330 train_time:27568ms step_avg:60.46ms
step:457/2330 train_time:27626ms step_avg:60.45ms
step:458/2330 train_time:27689ms step_avg:60.46ms
step:459/2330 train_time:27748ms step_avg:60.45ms
step:460/2330 train_time:27810ms step_avg:60.46ms
step:461/2330 train_time:27869ms step_avg:60.45ms
step:462/2330 train_time:27933ms step_avg:60.46ms
step:463/2330 train_time:27992ms step_avg:60.46ms
step:464/2330 train_time:28054ms step_avg:60.46ms
step:465/2330 train_time:28113ms step_avg:60.46ms
step:466/2330 train_time:28175ms step_avg:60.46ms
step:467/2330 train_time:28233ms step_avg:60.46ms
step:468/2330 train_time:28295ms step_avg:60.46ms
step:469/2330 train_time:28353ms step_avg:60.45ms
step:470/2330 train_time:28415ms step_avg:60.46ms
step:471/2330 train_time:28474ms step_avg:60.45ms
step:472/2330 train_time:28535ms step_avg:60.46ms
step:473/2330 train_time:28594ms step_avg:60.45ms
step:474/2330 train_time:28656ms step_avg:60.46ms
step:475/2330 train_time:28715ms step_avg:60.45ms
step:476/2330 train_time:28778ms step_avg:60.46ms
step:477/2330 train_time:28836ms step_avg:60.45ms
step:478/2330 train_time:28898ms step_avg:60.46ms
step:479/2330 train_time:28957ms step_avg:60.45ms
step:480/2330 train_time:29020ms step_avg:60.46ms
step:481/2330 train_time:29079ms step_avg:60.45ms
step:482/2330 train_time:29141ms step_avg:60.46ms
step:483/2330 train_time:29199ms step_avg:60.45ms
step:484/2330 train_time:29262ms step_avg:60.46ms
step:485/2330 train_time:29320ms step_avg:60.45ms
step:486/2330 train_time:29382ms step_avg:60.46ms
step:487/2330 train_time:29439ms step_avg:60.45ms
step:488/2330 train_time:29501ms step_avg:60.45ms
step:489/2330 train_time:29560ms step_avg:60.45ms
step:490/2330 train_time:29623ms step_avg:60.45ms
step:491/2330 train_time:29681ms step_avg:60.45ms
step:492/2330 train_time:29743ms step_avg:60.45ms
step:493/2330 train_time:29802ms step_avg:60.45ms
step:494/2330 train_time:29865ms step_avg:60.45ms
step:495/2330 train_time:29924ms step_avg:60.45ms
step:496/2330 train_time:29985ms step_avg:60.45ms
step:497/2330 train_time:30045ms step_avg:60.45ms
step:498/2330 train_time:30107ms step_avg:60.46ms
step:499/2330 train_time:30165ms step_avg:60.45ms
step:500/2330 train_time:30226ms step_avg:60.45ms
step:500/2330 val_loss:4.1244 train_time:30297ms step_avg:60.59ms
step:501/2330 train_time:30319ms step_avg:60.52ms
step:502/2330 train_time:30349ms step_avg:60.46ms
step:503/2330 train_time:30410ms step_avg:60.46ms
step:504/2330 train_time:30477ms step_avg:60.47ms
step:505/2330 train_time:30536ms step_avg:60.47ms
step:506/2330 train_time:30598ms step_avg:60.47ms
step:507/2330 train_time:30657ms step_avg:60.47ms
step:508/2330 train_time:30718ms step_avg:60.47ms
step:509/2330 train_time:30776ms step_avg:60.46ms
step:510/2330 train_time:30837ms step_avg:60.46ms
step:511/2330 train_time:30895ms step_avg:60.46ms
step:512/2330 train_time:30956ms step_avg:60.46ms
step:513/2330 train_time:31015ms step_avg:60.46ms
step:514/2330 train_time:31076ms step_avg:60.46ms
step:515/2330 train_time:31134ms step_avg:60.46ms
step:516/2330 train_time:31195ms step_avg:60.46ms
step:517/2330 train_time:31254ms step_avg:60.45ms
step:518/2330 train_time:31317ms step_avg:60.46ms
step:519/2330 train_time:31378ms step_avg:60.46ms
step:520/2330 train_time:31443ms step_avg:60.47ms
step:521/2330 train_time:31502ms step_avg:60.46ms
step:522/2330 train_time:31564ms step_avg:60.47ms
step:523/2330 train_time:31623ms step_avg:60.46ms
step:524/2330 train_time:31685ms step_avg:60.47ms
step:525/2330 train_time:31743ms step_avg:60.46ms
step:526/2330 train_time:31806ms step_avg:60.47ms
step:527/2330 train_time:31864ms step_avg:60.46ms
step:528/2330 train_time:31926ms step_avg:60.47ms
step:529/2330 train_time:31984ms step_avg:60.46ms
step:530/2330 train_time:32046ms step_avg:60.47ms
step:531/2330 train_time:32106ms step_avg:60.46ms
step:532/2330 train_time:32167ms step_avg:60.47ms
step:533/2330 train_time:32226ms step_avg:60.46ms
step:534/2330 train_time:32288ms step_avg:60.47ms
step:535/2330 train_time:32349ms step_avg:60.46ms
step:536/2330 train_time:32411ms step_avg:60.47ms
step:537/2330 train_time:32470ms step_avg:60.47ms
step:538/2330 train_time:32531ms step_avg:60.47ms
step:539/2330 train_time:32589ms step_avg:60.46ms
step:540/2330 train_time:32652ms step_avg:60.47ms
step:541/2330 train_time:32711ms step_avg:60.46ms
step:542/2330 train_time:32772ms step_avg:60.47ms
step:543/2330 train_time:32831ms step_avg:60.46ms
step:544/2330 train_time:32893ms step_avg:60.47ms
step:545/2330 train_time:32952ms step_avg:60.46ms
step:546/2330 train_time:33014ms step_avg:60.47ms
step:547/2330 train_time:33073ms step_avg:60.46ms
step:548/2330 train_time:33135ms step_avg:60.47ms
step:549/2330 train_time:33193ms step_avg:60.46ms
step:550/2330 train_time:33255ms step_avg:60.46ms
step:551/2330 train_time:33314ms step_avg:60.46ms
step:552/2330 train_time:33376ms step_avg:60.46ms
step:553/2330 train_time:33435ms step_avg:60.46ms
step:554/2330 train_time:33497ms step_avg:60.46ms
step:555/2330 train_time:33555ms step_avg:60.46ms
step:556/2330 train_time:33618ms step_avg:60.46ms
step:557/2330 train_time:33676ms step_avg:60.46ms
step:558/2330 train_time:33737ms step_avg:60.46ms
step:559/2330 train_time:33796ms step_avg:60.46ms
step:560/2330 train_time:33857ms step_avg:60.46ms
step:561/2330 train_time:33916ms step_avg:60.46ms
step:562/2330 train_time:33978ms step_avg:60.46ms
step:563/2330 train_time:34037ms step_avg:60.46ms
step:564/2330 train_time:34099ms step_avg:60.46ms
step:565/2330 train_time:34158ms step_avg:60.46ms
step:566/2330 train_time:34219ms step_avg:60.46ms
step:567/2330 train_time:34278ms step_avg:60.46ms
step:568/2330 train_time:34341ms step_avg:60.46ms
step:569/2330 train_time:34400ms step_avg:60.46ms
step:570/2330 train_time:34462ms step_avg:60.46ms
step:571/2330 train_time:34521ms step_avg:60.46ms
step:572/2330 train_time:34584ms step_avg:60.46ms
step:573/2330 train_time:34642ms step_avg:60.46ms
step:574/2330 train_time:34703ms step_avg:60.46ms
step:575/2330 train_time:34762ms step_avg:60.46ms
step:576/2330 train_time:34823ms step_avg:60.46ms
step:577/2330 train_time:34883ms step_avg:60.46ms
step:578/2330 train_time:34944ms step_avg:60.46ms
step:579/2330 train_time:35002ms step_avg:60.45ms
step:580/2330 train_time:35064ms step_avg:60.46ms
step:581/2330 train_time:35123ms step_avg:60.45ms
step:582/2330 train_time:35185ms step_avg:60.46ms
step:583/2330 train_time:35245ms step_avg:60.45ms
step:584/2330 train_time:35307ms step_avg:60.46ms
step:585/2330 train_time:35366ms step_avg:60.46ms
step:586/2330 train_time:35429ms step_avg:60.46ms
step:587/2330 train_time:35488ms step_avg:60.46ms
step:588/2330 train_time:35549ms step_avg:60.46ms
step:589/2330 train_time:35609ms step_avg:60.46ms
step:590/2330 train_time:35670ms step_avg:60.46ms
step:591/2330 train_time:35729ms step_avg:60.45ms
step:592/2330 train_time:35790ms step_avg:60.46ms
step:593/2330 train_time:35849ms step_avg:60.45ms
step:594/2330 train_time:35911ms step_avg:60.46ms
step:595/2330 train_time:35969ms step_avg:60.45ms
step:596/2330 train_time:36032ms step_avg:60.46ms
step:597/2330 train_time:36090ms step_avg:60.45ms
step:598/2330 train_time:36152ms step_avg:60.45ms
step:599/2330 train_time:36211ms step_avg:60.45ms
step:600/2330 train_time:36273ms step_avg:60.45ms
step:601/2330 train_time:36332ms step_avg:60.45ms
step:602/2330 train_time:36394ms step_avg:60.46ms
step:603/2330 train_time:36453ms step_avg:60.45ms
step:604/2330 train_time:36515ms step_avg:60.46ms
step:605/2330 train_time:36574ms step_avg:60.45ms
step:606/2330 train_time:36636ms step_avg:60.46ms
step:607/2330 train_time:36695ms step_avg:60.45ms
step:608/2330 train_time:36757ms step_avg:60.46ms
step:609/2330 train_time:36816ms step_avg:60.45ms
step:610/2330 train_time:36877ms step_avg:60.45ms
step:611/2330 train_time:36936ms step_avg:60.45ms
step:612/2330 train_time:36997ms step_avg:60.45ms
step:613/2330 train_time:37056ms step_avg:60.45ms
step:614/2330 train_time:37118ms step_avg:60.45ms
step:615/2330 train_time:37176ms step_avg:60.45ms
step:616/2330 train_time:37238ms step_avg:60.45ms
step:617/2330 train_time:37296ms step_avg:60.45ms
step:618/2330 train_time:37359ms step_avg:60.45ms
step:619/2330 train_time:37418ms step_avg:60.45ms
step:620/2330 train_time:37479ms step_avg:60.45ms
step:621/2330 train_time:37538ms step_avg:60.45ms
step:622/2330 train_time:37600ms step_avg:60.45ms
step:623/2330 train_time:37658ms step_avg:60.45ms
step:624/2330 train_time:37721ms step_avg:60.45ms
step:625/2330 train_time:37779ms step_avg:60.45ms
step:626/2330 train_time:37841ms step_avg:60.45ms
step:627/2330 train_time:37900ms step_avg:60.45ms
step:628/2330 train_time:37961ms step_avg:60.45ms
step:629/2330 train_time:38021ms step_avg:60.45ms
step:630/2330 train_time:38083ms step_avg:60.45ms
step:631/2330 train_time:38142ms step_avg:60.45ms
step:632/2330 train_time:38204ms step_avg:60.45ms
step:633/2330 train_time:38263ms step_avg:60.45ms
step:634/2330 train_time:38326ms step_avg:60.45ms
step:635/2330 train_time:38384ms step_avg:60.45ms
step:636/2330 train_time:38446ms step_avg:60.45ms
step:637/2330 train_time:38506ms step_avg:60.45ms
step:638/2330 train_time:38568ms step_avg:60.45ms
step:639/2330 train_time:38627ms step_avg:60.45ms
step:640/2330 train_time:38688ms step_avg:60.45ms
step:641/2330 train_time:38747ms step_avg:60.45ms
step:642/2330 train_time:38810ms step_avg:60.45ms
step:643/2330 train_time:38868ms step_avg:60.45ms
step:644/2330 train_time:38930ms step_avg:60.45ms
step:645/2330 train_time:38989ms step_avg:60.45ms
step:646/2330 train_time:39051ms step_avg:60.45ms
step:647/2330 train_time:39109ms step_avg:60.45ms
step:648/2330 train_time:39171ms step_avg:60.45ms
step:649/2330 train_time:39229ms step_avg:60.45ms
step:650/2330 train_time:39291ms step_avg:60.45ms
step:651/2330 train_time:39349ms step_avg:60.44ms
step:652/2330 train_time:39412ms step_avg:60.45ms
step:653/2330 train_time:39471ms step_avg:60.44ms
step:654/2330 train_time:39532ms step_avg:60.45ms
step:655/2330 train_time:39591ms step_avg:60.44ms
step:656/2330 train_time:39653ms step_avg:60.45ms
step:657/2330 train_time:39712ms step_avg:60.44ms
step:658/2330 train_time:39774ms step_avg:60.45ms
step:659/2330 train_time:39833ms step_avg:60.44ms
step:660/2330 train_time:39894ms step_avg:60.45ms
step:661/2330 train_time:39953ms step_avg:60.44ms
step:662/2330 train_time:40015ms step_avg:60.45ms
step:663/2330 train_time:40074ms step_avg:60.44ms
step:664/2330 train_time:40137ms step_avg:60.45ms
step:665/2330 train_time:40195ms step_avg:60.44ms
step:666/2330 train_time:40257ms step_avg:60.45ms
step:667/2330 train_time:40316ms step_avg:60.44ms
step:668/2330 train_time:40379ms step_avg:60.45ms
step:669/2330 train_time:40437ms step_avg:60.44ms
step:670/2330 train_time:40499ms step_avg:60.45ms
step:671/2330 train_time:40557ms step_avg:60.44ms
step:672/2330 train_time:40619ms step_avg:60.45ms
step:673/2330 train_time:40678ms step_avg:60.44ms
step:674/2330 train_time:40740ms step_avg:60.45ms
step:675/2330 train_time:40798ms step_avg:60.44ms
step:676/2330 train_time:40860ms step_avg:60.44ms
step:677/2330 train_time:40920ms step_avg:60.44ms
step:678/2330 train_time:40982ms step_avg:60.45ms
step:679/2330 train_time:41041ms step_avg:60.44ms
step:680/2330 train_time:41102ms step_avg:60.44ms
step:681/2330 train_time:41161ms step_avg:60.44ms
step:682/2330 train_time:41223ms step_avg:60.44ms
step:683/2330 train_time:41282ms step_avg:60.44ms
step:684/2330 train_time:41344ms step_avg:60.44ms
step:685/2330 train_time:41403ms step_avg:60.44ms
step:686/2330 train_time:41464ms step_avg:60.44ms
step:687/2330 train_time:41523ms step_avg:60.44ms
step:688/2330 train_time:41584ms step_avg:60.44ms
step:689/2330 train_time:41643ms step_avg:60.44ms
step:690/2330 train_time:41706ms step_avg:60.44ms
step:691/2330 train_time:41765ms step_avg:60.44ms
step:692/2330 train_time:41827ms step_avg:60.44ms
step:693/2330 train_time:41887ms step_avg:60.44ms
step:694/2330 train_time:41949ms step_avg:60.45ms
step:695/2330 train_time:42008ms step_avg:60.44ms
step:696/2330 train_time:42070ms step_avg:60.44ms
step:697/2330 train_time:42129ms step_avg:60.44ms
step:698/2330 train_time:42192ms step_avg:60.45ms
step:699/2330 train_time:42251ms step_avg:60.44ms
step:700/2330 train_time:42312ms step_avg:60.45ms
step:701/2330 train_time:42371ms step_avg:60.44ms
step:702/2330 train_time:42434ms step_avg:60.45ms
step:703/2330 train_time:42491ms step_avg:60.44ms
step:704/2330 train_time:42554ms step_avg:60.45ms
step:705/2330 train_time:42613ms step_avg:60.44ms
step:706/2330 train_time:42675ms step_avg:60.45ms
step:707/2330 train_time:42734ms step_avg:60.44ms
step:708/2330 train_time:42796ms step_avg:60.45ms
step:709/2330 train_time:42855ms step_avg:60.44ms
step:710/2330 train_time:42918ms step_avg:60.45ms
step:711/2330 train_time:42977ms step_avg:60.45ms
step:712/2330 train_time:43039ms step_avg:60.45ms
step:713/2330 train_time:43098ms step_avg:60.45ms
step:714/2330 train_time:43160ms step_avg:60.45ms
step:715/2330 train_time:43219ms step_avg:60.45ms
step:716/2330 train_time:43280ms step_avg:60.45ms
step:717/2330 train_time:43339ms step_avg:60.44ms
step:718/2330 train_time:43401ms step_avg:60.45ms
step:719/2330 train_time:43459ms step_avg:60.44ms
step:720/2330 train_time:43521ms step_avg:60.45ms
step:721/2330 train_time:43580ms step_avg:60.44ms
step:722/2330 train_time:43641ms step_avg:60.45ms
step:723/2330 train_time:43700ms step_avg:60.44ms
step:724/2330 train_time:43762ms step_avg:60.44ms
step:725/2330 train_time:43821ms step_avg:60.44ms
step:726/2330 train_time:43883ms step_avg:60.45ms
step:727/2330 train_time:43943ms step_avg:60.44ms
step:728/2330 train_time:44004ms step_avg:60.45ms
step:729/2330 train_time:44063ms step_avg:60.44ms
step:730/2330 train_time:44125ms step_avg:60.45ms
step:731/2330 train_time:44184ms step_avg:60.44ms
step:732/2330 train_time:44246ms step_avg:60.44ms
step:733/2330 train_time:44305ms step_avg:60.44ms
step:734/2330 train_time:44367ms step_avg:60.45ms
step:735/2330 train_time:44425ms step_avg:60.44ms
step:736/2330 train_time:44488ms step_avg:60.45ms
step:737/2330 train_time:44547ms step_avg:60.44ms
step:738/2330 train_time:44609ms step_avg:60.45ms
step:739/2330 train_time:44668ms step_avg:60.44ms
step:740/2330 train_time:44730ms step_avg:60.45ms
step:741/2330 train_time:44790ms step_avg:60.45ms
step:742/2330 train_time:44851ms step_avg:60.45ms
step:743/2330 train_time:44910ms step_avg:60.44ms
step:744/2330 train_time:44972ms step_avg:60.45ms
step:745/2330 train_time:45032ms step_avg:60.45ms
step:746/2330 train_time:45093ms step_avg:60.45ms
step:747/2330 train_time:45151ms step_avg:60.44ms
step:748/2330 train_time:45213ms step_avg:60.45ms
step:749/2330 train_time:45273ms step_avg:60.44ms
step:750/2330 train_time:45336ms step_avg:60.45ms
step:750/2330 val_loss:3.9547 train_time:45408ms step_avg:60.54ms
step:751/2330 train_time:45430ms step_avg:60.49ms
step:752/2330 train_time:45460ms step_avg:60.45ms
step:753/2330 train_time:45522ms step_avg:60.45ms
step:754/2330 train_time:45588ms step_avg:60.46ms
step:755/2330 train_time:45648ms step_avg:60.46ms
step:756/2330 train_time:45709ms step_avg:60.46ms
step:757/2330 train_time:45768ms step_avg:60.46ms
step:758/2330 train_time:45829ms step_avg:60.46ms
step:759/2330 train_time:45888ms step_avg:60.46ms
step:760/2330 train_time:45949ms step_avg:60.46ms
step:761/2330 train_time:46007ms step_avg:60.46ms
step:762/2330 train_time:46068ms step_avg:60.46ms
step:763/2330 train_time:46126ms step_avg:60.45ms
step:764/2330 train_time:46187ms step_avg:60.45ms
step:765/2330 train_time:46246ms step_avg:60.45ms
step:766/2330 train_time:46307ms step_avg:60.45ms
step:767/2330 train_time:46368ms step_avg:60.45ms
step:768/2330 train_time:46432ms step_avg:60.46ms
step:769/2330 train_time:46494ms step_avg:60.46ms
step:770/2330 train_time:46559ms step_avg:60.47ms
step:771/2330 train_time:46619ms step_avg:60.47ms
step:772/2330 train_time:46682ms step_avg:60.47ms
step:773/2330 train_time:46742ms step_avg:60.47ms
step:774/2330 train_time:46804ms step_avg:60.47ms
step:775/2330 train_time:46864ms step_avg:60.47ms
step:776/2330 train_time:46926ms step_avg:60.47ms
step:777/2330 train_time:46986ms step_avg:60.47ms
step:778/2330 train_time:47048ms step_avg:60.47ms
step:779/2330 train_time:47107ms step_avg:60.47ms
step:780/2330 train_time:47169ms step_avg:60.47ms
step:781/2330 train_time:47227ms step_avg:60.47ms
step:782/2330 train_time:47289ms step_avg:60.47ms
step:783/2330 train_time:47350ms step_avg:60.47ms
step:784/2330 train_time:47412ms step_avg:60.47ms
step:785/2330 train_time:47472ms step_avg:60.47ms
step:786/2330 train_time:47536ms step_avg:60.48ms
step:787/2330 train_time:47596ms step_avg:60.48ms
step:788/2330 train_time:47659ms step_avg:60.48ms
step:789/2330 train_time:47718ms step_avg:60.48ms
step:790/2330 train_time:47781ms step_avg:60.48ms
step:791/2330 train_time:47841ms step_avg:60.48ms
step:792/2330 train_time:47904ms step_avg:60.48ms
step:793/2330 train_time:47963ms step_avg:60.48ms
step:794/2330 train_time:48025ms step_avg:60.48ms
step:795/2330 train_time:48084ms step_avg:60.48ms
step:796/2330 train_time:48147ms step_avg:60.49ms
step:797/2330 train_time:48206ms step_avg:60.48ms
step:798/2330 train_time:48268ms step_avg:60.49ms
step:799/2330 train_time:48328ms step_avg:60.49ms
step:800/2330 train_time:48392ms step_avg:60.49ms
step:801/2330 train_time:48453ms step_avg:60.49ms
step:802/2330 train_time:48516ms step_avg:60.49ms
step:803/2330 train_time:48575ms step_avg:60.49ms
step:804/2330 train_time:48638ms step_avg:60.49ms
step:805/2330 train_time:48697ms step_avg:60.49ms
step:806/2330 train_time:48760ms step_avg:60.50ms
step:807/2330 train_time:48819ms step_avg:60.49ms
step:808/2330 train_time:48882ms step_avg:60.50ms
step:809/2330 train_time:48941ms step_avg:60.50ms
step:810/2330 train_time:49003ms step_avg:60.50ms
step:811/2330 train_time:49063ms step_avg:60.50ms
step:812/2330 train_time:49125ms step_avg:60.50ms
step:813/2330 train_time:49184ms step_avg:60.50ms
step:814/2330 train_time:49247ms step_avg:60.50ms
step:815/2330 train_time:49307ms step_avg:60.50ms
step:816/2330 train_time:49369ms step_avg:60.50ms
step:817/2330 train_time:49429ms step_avg:60.50ms
step:818/2330 train_time:49493ms step_avg:60.50ms
step:819/2330 train_time:49553ms step_avg:60.50ms
step:820/2330 train_time:49616ms step_avg:60.51ms
step:821/2330 train_time:49675ms step_avg:60.51ms
step:822/2330 train_time:49738ms step_avg:60.51ms
step:823/2330 train_time:49797ms step_avg:60.51ms
step:824/2330 train_time:49860ms step_avg:60.51ms
step:825/2330 train_time:49919ms step_avg:60.51ms
step:826/2330 train_time:49981ms step_avg:60.51ms
step:827/2330 train_time:50042ms step_avg:60.51ms
step:828/2330 train_time:50104ms step_avg:60.51ms
step:829/2330 train_time:50163ms step_avg:60.51ms
step:830/2330 train_time:50226ms step_avg:60.51ms
step:831/2330 train_time:50286ms step_avg:60.51ms
step:832/2330 train_time:50350ms step_avg:60.52ms
step:833/2330 train_time:50410ms step_avg:60.52ms
step:834/2330 train_time:50473ms step_avg:60.52ms
step:835/2330 train_time:50532ms step_avg:60.52ms
step:836/2330 train_time:50595ms step_avg:60.52ms
step:837/2330 train_time:50655ms step_avg:60.52ms
step:838/2330 train_time:50717ms step_avg:60.52ms
step:839/2330 train_time:50776ms step_avg:60.52ms
step:840/2330 train_time:50839ms step_avg:60.52ms
step:841/2330 train_time:50898ms step_avg:60.52ms
step:842/2330 train_time:50961ms step_avg:60.52ms
step:843/2330 train_time:51021ms step_avg:60.52ms
step:844/2330 train_time:51084ms step_avg:60.53ms
step:845/2330 train_time:51142ms step_avg:60.52ms
step:846/2330 train_time:51206ms step_avg:60.53ms
step:847/2330 train_time:51266ms step_avg:60.53ms
step:848/2330 train_time:51329ms step_avg:60.53ms
step:849/2330 train_time:51389ms step_avg:60.53ms
step:850/2330 train_time:51452ms step_avg:60.53ms
step:851/2330 train_time:51511ms step_avg:60.53ms
step:852/2330 train_time:51574ms step_avg:60.53ms
step:853/2330 train_time:51633ms step_avg:60.53ms
step:854/2330 train_time:51697ms step_avg:60.53ms
step:855/2330 train_time:51757ms step_avg:60.53ms
step:856/2330 train_time:51819ms step_avg:60.54ms
step:857/2330 train_time:51878ms step_avg:60.53ms
step:858/2330 train_time:51941ms step_avg:60.54ms
step:859/2330 train_time:52001ms step_avg:60.54ms
step:860/2330 train_time:52063ms step_avg:60.54ms
step:861/2330 train_time:52122ms step_avg:60.54ms
step:862/2330 train_time:52185ms step_avg:60.54ms
step:863/2330 train_time:52245ms step_avg:60.54ms
step:864/2330 train_time:52307ms step_avg:60.54ms
step:865/2330 train_time:52368ms step_avg:60.54ms
step:866/2330 train_time:52431ms step_avg:60.54ms
step:867/2330 train_time:52491ms step_avg:60.54ms
step:868/2330 train_time:52554ms step_avg:60.55ms
step:869/2330 train_time:52613ms step_avg:60.54ms
step:870/2330 train_time:52676ms step_avg:60.55ms
step:871/2330 train_time:52735ms step_avg:60.55ms
step:872/2330 train_time:52797ms step_avg:60.55ms
step:873/2330 train_time:52857ms step_avg:60.55ms
step:874/2330 train_time:52919ms step_avg:60.55ms
step:875/2330 train_time:52979ms step_avg:60.55ms
step:876/2330 train_time:53041ms step_avg:60.55ms
step:877/2330 train_time:53100ms step_avg:60.55ms
step:878/2330 train_time:53163ms step_avg:60.55ms
step:879/2330 train_time:53223ms step_avg:60.55ms
step:880/2330 train_time:53286ms step_avg:60.55ms
step:881/2330 train_time:53346ms step_avg:60.55ms
step:882/2330 train_time:53409ms step_avg:60.55ms
step:883/2330 train_time:53469ms step_avg:60.55ms
step:884/2330 train_time:53531ms step_avg:60.56ms
step:885/2330 train_time:53592ms step_avg:60.56ms
step:886/2330 train_time:53654ms step_avg:60.56ms
step:887/2330 train_time:53713ms step_avg:60.56ms
step:888/2330 train_time:53775ms step_avg:60.56ms
step:889/2330 train_time:53834ms step_avg:60.56ms
step:890/2330 train_time:53896ms step_avg:60.56ms
step:891/2330 train_time:53956ms step_avg:60.56ms
step:892/2330 train_time:54019ms step_avg:60.56ms
step:893/2330 train_time:54078ms step_avg:60.56ms
step:894/2330 train_time:54141ms step_avg:60.56ms
step:895/2330 train_time:54200ms step_avg:60.56ms
step:896/2330 train_time:54263ms step_avg:60.56ms
step:897/2330 train_time:54323ms step_avg:60.56ms
step:898/2330 train_time:54386ms step_avg:60.56ms
step:899/2330 train_time:54446ms step_avg:60.56ms
step:900/2330 train_time:54510ms step_avg:60.57ms
step:901/2330 train_time:54570ms step_avg:60.57ms
step:902/2330 train_time:54633ms step_avg:60.57ms
step:903/2330 train_time:54693ms step_avg:60.57ms
step:904/2330 train_time:54755ms step_avg:60.57ms
step:905/2330 train_time:54814ms step_avg:60.57ms
step:906/2330 train_time:54876ms step_avg:60.57ms
step:907/2330 train_time:54935ms step_avg:60.57ms
step:908/2330 train_time:54998ms step_avg:60.57ms
step:909/2330 train_time:55057ms step_avg:60.57ms
step:910/2330 train_time:55119ms step_avg:60.57ms
step:911/2330 train_time:55179ms step_avg:60.57ms
step:912/2330 train_time:55242ms step_avg:60.57ms
step:913/2330 train_time:55302ms step_avg:60.57ms
step:914/2330 train_time:55364ms step_avg:60.57ms
step:915/2330 train_time:55424ms step_avg:60.57ms
step:916/2330 train_time:55487ms step_avg:60.58ms
step:917/2330 train_time:55548ms step_avg:60.58ms
step:918/2330 train_time:55610ms step_avg:60.58ms
step:919/2330 train_time:55670ms step_avg:60.58ms
step:920/2330 train_time:55732ms step_avg:60.58ms
step:921/2330 train_time:55792ms step_avg:60.58ms
step:922/2330 train_time:55856ms step_avg:60.58ms
step:923/2330 train_time:55914ms step_avg:60.58ms
step:924/2330 train_time:55976ms step_avg:60.58ms
step:925/2330 train_time:56035ms step_avg:60.58ms
step:926/2330 train_time:56098ms step_avg:60.58ms
step:927/2330 train_time:56157ms step_avg:60.58ms
step:928/2330 train_time:56220ms step_avg:60.58ms
step:929/2330 train_time:56279ms step_avg:60.58ms
step:930/2330 train_time:56342ms step_avg:60.58ms
step:931/2330 train_time:56401ms step_avg:60.58ms
step:932/2330 train_time:56465ms step_avg:60.58ms
step:933/2330 train_time:56525ms step_avg:60.58ms
step:934/2330 train_time:56588ms step_avg:60.59ms
step:935/2330 train_time:56648ms step_avg:60.59ms
step:936/2330 train_time:56710ms step_avg:60.59ms
step:937/2330 train_time:56771ms step_avg:60.59ms
step:938/2330 train_time:56833ms step_avg:60.59ms
step:939/2330 train_time:56892ms step_avg:60.59ms
step:940/2330 train_time:56955ms step_avg:60.59ms
step:941/2330 train_time:57014ms step_avg:60.59ms
step:942/2330 train_time:57076ms step_avg:60.59ms
step:943/2330 train_time:57135ms step_avg:60.59ms
step:944/2330 train_time:57198ms step_avg:60.59ms
step:945/2330 train_time:57258ms step_avg:60.59ms
step:946/2330 train_time:57321ms step_avg:60.59ms
step:947/2330 train_time:57380ms step_avg:60.59ms
step:948/2330 train_time:57443ms step_avg:60.59ms
step:949/2330 train_time:57503ms step_avg:60.59ms
step:950/2330 train_time:57566ms step_avg:60.60ms
step:951/2330 train_time:57625ms step_avg:60.59ms
step:952/2330 train_time:57689ms step_avg:60.60ms
step:953/2330 train_time:57750ms step_avg:60.60ms
step:954/2330 train_time:57812ms step_avg:60.60ms
step:955/2330 train_time:57872ms step_avg:60.60ms
step:956/2330 train_time:57935ms step_avg:60.60ms
step:957/2330 train_time:57993ms step_avg:60.60ms
step:958/2330 train_time:58056ms step_avg:60.60ms
step:959/2330 train_time:58116ms step_avg:60.60ms
step:960/2330 train_time:58178ms step_avg:60.60ms
step:961/2330 train_time:58238ms step_avg:60.60ms
step:962/2330 train_time:58301ms step_avg:60.60ms
step:963/2330 train_time:58360ms step_avg:60.60ms
step:964/2330 train_time:58423ms step_avg:60.60ms
step:965/2330 train_time:58482ms step_avg:60.60ms
step:966/2330 train_time:58545ms step_avg:60.61ms
step:967/2330 train_time:58605ms step_avg:60.61ms
step:968/2330 train_time:58668ms step_avg:60.61ms
step:969/2330 train_time:58727ms step_avg:60.61ms
step:970/2330 train_time:58791ms step_avg:60.61ms
step:971/2330 train_time:58851ms step_avg:60.61ms
step:972/2330 train_time:58914ms step_avg:60.61ms
step:973/2330 train_time:58973ms step_avg:60.61ms
step:974/2330 train_time:59036ms step_avg:60.61ms
step:975/2330 train_time:59095ms step_avg:60.61ms
step:976/2330 train_time:59158ms step_avg:60.61ms
step:977/2330 train_time:59217ms step_avg:60.61ms
step:978/2330 train_time:59279ms step_avg:60.61ms
step:979/2330 train_time:59338ms step_avg:60.61ms
step:980/2330 train_time:59401ms step_avg:60.61ms
step:981/2330 train_time:59460ms step_avg:60.61ms
step:982/2330 train_time:59524ms step_avg:60.61ms
step:983/2330 train_time:59584ms step_avg:60.61ms
step:984/2330 train_time:59647ms step_avg:60.62ms
step:985/2330 train_time:59707ms step_avg:60.62ms
step:986/2330 train_time:59769ms step_avg:60.62ms
step:987/2330 train_time:59829ms step_avg:60.62ms
step:988/2330 train_time:59892ms step_avg:60.62ms
step:989/2330 train_time:59953ms step_avg:60.62ms
step:990/2330 train_time:60014ms step_avg:60.62ms
step:991/2330 train_time:60074ms step_avg:60.62ms
step:992/2330 train_time:60136ms step_avg:60.62ms
step:993/2330 train_time:60195ms step_avg:60.62ms
step:994/2330 train_time:60257ms step_avg:60.62ms
step:995/2330 train_time:60316ms step_avg:60.62ms
step:996/2330 train_time:60379ms step_avg:60.62ms
step:997/2330 train_time:60439ms step_avg:60.62ms
step:998/2330 train_time:60501ms step_avg:60.62ms
step:999/2330 train_time:60561ms step_avg:60.62ms
step:1000/2330 train_time:60624ms step_avg:60.62ms
step:1000/2330 val_loss:3.7785 train_time:60697ms step_avg:60.70ms
step:1001/2330 train_time:60719ms step_avg:60.66ms
step:1002/2330 train_time:60749ms step_avg:60.63ms
step:1003/2330 train_time:60810ms step_avg:60.63ms
step:1004/2330 train_time:60881ms step_avg:60.64ms
step:1005/2330 train_time:60941ms step_avg:60.64ms
step:1006/2330 train_time:61003ms step_avg:60.64ms
step:1007/2330 train_time:61062ms step_avg:60.64ms
step:1008/2330 train_time:61123ms step_avg:60.64ms
step:1009/2330 train_time:61182ms step_avg:60.64ms
step:1010/2330 train_time:61245ms step_avg:60.64ms
step:1011/2330 train_time:61303ms step_avg:60.64ms
step:1012/2330 train_time:61365ms step_avg:60.64ms
step:1013/2330 train_time:61423ms step_avg:60.63ms
step:1014/2330 train_time:61485ms step_avg:60.64ms
step:1015/2330 train_time:61544ms step_avg:60.63ms
step:1016/2330 train_time:61606ms step_avg:60.64ms
step:1017/2330 train_time:61666ms step_avg:60.64ms
step:1018/2330 train_time:61731ms step_avg:60.64ms
step:1019/2330 train_time:61793ms step_avg:60.64ms
step:1020/2330 train_time:61858ms step_avg:60.64ms
step:1021/2330 train_time:61917ms step_avg:60.64ms
step:1022/2330 train_time:61980ms step_avg:60.65ms
step:1023/2330 train_time:62040ms step_avg:60.65ms
step:1024/2330 train_time:62103ms step_avg:60.65ms
step:1025/2330 train_time:62162ms step_avg:60.65ms
step:1026/2330 train_time:62224ms step_avg:60.65ms
step:1027/2330 train_time:62283ms step_avg:60.65ms
step:1028/2330 train_time:62345ms step_avg:60.65ms
step:1029/2330 train_time:62403ms step_avg:60.64ms
step:1030/2330 train_time:62466ms step_avg:60.65ms
step:1031/2330 train_time:62524ms step_avg:60.64ms
step:1032/2330 train_time:62587ms step_avg:60.65ms
step:1033/2330 train_time:62647ms step_avg:60.65ms
step:1034/2330 train_time:62710ms step_avg:60.65ms
step:1035/2330 train_time:62771ms step_avg:60.65ms
step:1036/2330 train_time:62835ms step_avg:60.65ms
step:1037/2330 train_time:62895ms step_avg:60.65ms
step:1038/2330 train_time:62957ms step_avg:60.65ms
step:1039/2330 train_time:63017ms step_avg:60.65ms
step:1040/2330 train_time:63079ms step_avg:60.65ms
step:1041/2330 train_time:63139ms step_avg:60.65ms
step:1042/2330 train_time:63201ms step_avg:60.65ms
step:1043/2330 train_time:63260ms step_avg:60.65ms
step:1044/2330 train_time:63323ms step_avg:60.65ms
step:1045/2330 train_time:63383ms step_avg:60.65ms
step:1046/2330 train_time:63445ms step_avg:60.65ms
step:1047/2330 train_time:63504ms step_avg:60.65ms
step:1048/2330 train_time:63567ms step_avg:60.66ms
step:1049/2330 train_time:63627ms step_avg:60.65ms
step:1050/2330 train_time:63690ms step_avg:60.66ms
step:1051/2330 train_time:63750ms step_avg:60.66ms
step:1052/2330 train_time:63812ms step_avg:60.66ms
step:1053/2330 train_time:63871ms step_avg:60.66ms
step:1054/2330 train_time:63934ms step_avg:60.66ms
step:1055/2330 train_time:63994ms step_avg:60.66ms
step:1056/2330 train_time:64056ms step_avg:60.66ms
step:1057/2330 train_time:64115ms step_avg:60.66ms
step:1058/2330 train_time:64178ms step_avg:60.66ms
step:1059/2330 train_time:64237ms step_avg:60.66ms
step:1060/2330 train_time:64300ms step_avg:60.66ms
step:1061/2330 train_time:64360ms step_avg:60.66ms
step:1062/2330 train_time:64424ms step_avg:60.66ms
step:1063/2330 train_time:64484ms step_avg:60.66ms
step:1064/2330 train_time:64547ms step_avg:60.66ms
step:1065/2330 train_time:64607ms step_avg:60.66ms
step:1066/2330 train_time:64670ms step_avg:60.67ms
step:1067/2330 train_time:64730ms step_avg:60.67ms
step:1068/2330 train_time:64792ms step_avg:60.67ms
step:1069/2330 train_time:64853ms step_avg:60.67ms
step:1070/2330 train_time:64915ms step_avg:60.67ms
step:1071/2330 train_time:64975ms step_avg:60.67ms
step:1072/2330 train_time:65037ms step_avg:60.67ms
step:1073/2330 train_time:65096ms step_avg:60.67ms
step:1074/2330 train_time:65159ms step_avg:60.67ms
step:1075/2330 train_time:65219ms step_avg:60.67ms
step:1076/2330 train_time:65281ms step_avg:60.67ms
step:1077/2330 train_time:65340ms step_avg:60.67ms
step:1078/2330 train_time:65403ms step_avg:60.67ms
step:1079/2330 train_time:65463ms step_avg:60.67ms
step:1080/2330 train_time:65525ms step_avg:60.67ms
step:1081/2330 train_time:65586ms step_avg:60.67ms
step:1082/2330 train_time:65649ms step_avg:60.67ms
step:1083/2330 train_time:65708ms step_avg:60.67ms
step:1084/2330 train_time:65771ms step_avg:60.67ms
step:1085/2330 train_time:65830ms step_avg:60.67ms
step:1086/2330 train_time:65893ms step_avg:60.68ms
step:1087/2330 train_time:65954ms step_avg:60.67ms
step:1088/2330 train_time:66016ms step_avg:60.68ms
step:1089/2330 train_time:66075ms step_avg:60.67ms
step:1090/2330 train_time:66138ms step_avg:60.68ms
step:1091/2330 train_time:66197ms step_avg:60.68ms
step:1092/2330 train_time:66260ms step_avg:60.68ms
step:1093/2330 train_time:66319ms step_avg:60.68ms
step:1094/2330 train_time:66381ms step_avg:60.68ms
step:1095/2330 train_time:66441ms step_avg:60.68ms
step:1096/2330 train_time:66505ms step_avg:60.68ms
step:1097/2330 train_time:66565ms step_avg:60.68ms
step:1098/2330 train_time:66628ms step_avg:60.68ms
step:1099/2330 train_time:66688ms step_avg:60.68ms
step:1100/2330 train_time:66751ms step_avg:60.68ms
step:1101/2330 train_time:66810ms step_avg:60.68ms
step:1102/2330 train_time:66872ms step_avg:60.68ms
step:1103/2330 train_time:66931ms step_avg:60.68ms
step:1104/2330 train_time:66994ms step_avg:60.68ms
step:1105/2330 train_time:67054ms step_avg:60.68ms
step:1106/2330 train_time:67116ms step_avg:60.68ms
step:1107/2330 train_time:67176ms step_avg:60.68ms
step:1108/2330 train_time:67238ms step_avg:60.68ms
step:1109/2330 train_time:67298ms step_avg:60.68ms
step:1110/2330 train_time:67360ms step_avg:60.68ms
step:1111/2330 train_time:67420ms step_avg:60.68ms
step:1112/2330 train_time:67482ms step_avg:60.69ms
step:1113/2330 train_time:67542ms step_avg:60.68ms
step:1114/2330 train_time:67606ms step_avg:60.69ms
step:1115/2330 train_time:67666ms step_avg:60.69ms
step:1116/2330 train_time:67729ms step_avg:60.69ms
step:1117/2330 train_time:67789ms step_avg:60.69ms
step:1118/2330 train_time:67852ms step_avg:60.69ms
step:1119/2330 train_time:67912ms step_avg:60.69ms
step:1120/2330 train_time:67974ms step_avg:60.69ms
step:1121/2330 train_time:68034ms step_avg:60.69ms
step:1122/2330 train_time:68096ms step_avg:60.69ms
step:1123/2330 train_time:68156ms step_avg:60.69ms
step:1124/2330 train_time:68218ms step_avg:60.69ms
step:1125/2330 train_time:68277ms step_avg:60.69ms
step:1126/2330 train_time:68339ms step_avg:60.69ms
step:1127/2330 train_time:68398ms step_avg:60.69ms
step:1128/2330 train_time:68461ms step_avg:60.69ms
step:1129/2330 train_time:68521ms step_avg:60.69ms
step:1130/2330 train_time:68584ms step_avg:60.69ms
step:1131/2330 train_time:68645ms step_avg:60.69ms
step:1132/2330 train_time:68707ms step_avg:60.70ms
step:1133/2330 train_time:68767ms step_avg:60.69ms
step:1134/2330 train_time:68830ms step_avg:60.70ms
step:1135/2330 train_time:68890ms step_avg:60.70ms
step:1136/2330 train_time:68953ms step_avg:60.70ms
step:1137/2330 train_time:69011ms step_avg:60.70ms
step:1138/2330 train_time:69073ms step_avg:60.70ms
step:1139/2330 train_time:69133ms step_avg:60.70ms
step:1140/2330 train_time:69196ms step_avg:60.70ms
step:1141/2330 train_time:69255ms step_avg:60.70ms
step:1142/2330 train_time:69317ms step_avg:60.70ms
step:1143/2330 train_time:69377ms step_avg:60.70ms
step:1144/2330 train_time:69440ms step_avg:60.70ms
step:1145/2330 train_time:69499ms step_avg:60.70ms
step:1146/2330 train_time:69563ms step_avg:60.70ms
step:1147/2330 train_time:69623ms step_avg:60.70ms
step:1148/2330 train_time:69686ms step_avg:60.70ms
step:1149/2330 train_time:69746ms step_avg:60.70ms
step:1150/2330 train_time:69809ms step_avg:60.70ms
step:1151/2330 train_time:69869ms step_avg:60.70ms
step:1152/2330 train_time:69932ms step_avg:60.71ms
step:1153/2330 train_time:69992ms step_avg:60.70ms
step:1154/2330 train_time:70055ms step_avg:60.71ms
step:1155/2330 train_time:70114ms step_avg:60.70ms
step:1156/2330 train_time:70176ms step_avg:60.71ms
step:1157/2330 train_time:70236ms step_avg:60.70ms
step:1158/2330 train_time:70297ms step_avg:60.71ms
step:1159/2330 train_time:70357ms step_avg:60.70ms
step:1160/2330 train_time:70419ms step_avg:60.71ms
step:1161/2330 train_time:70479ms step_avg:60.71ms
step:1162/2330 train_time:70543ms step_avg:60.71ms
step:1163/2330 train_time:70601ms step_avg:60.71ms
step:1164/2330 train_time:70665ms step_avg:60.71ms
step:1165/2330 train_time:70724ms step_avg:60.71ms
step:1166/2330 train_time:70787ms step_avg:60.71ms
step:1167/2330 train_time:70849ms step_avg:60.71ms
step:1168/2330 train_time:70912ms step_avg:60.71ms
step:1169/2330 train_time:70971ms step_avg:60.71ms
step:1170/2330 train_time:71034ms step_avg:60.71ms
step:1171/2330 train_time:71093ms step_avg:60.71ms
step:1172/2330 train_time:71156ms step_avg:60.71ms
step:1173/2330 train_time:71215ms step_avg:60.71ms
step:1174/2330 train_time:71277ms step_avg:60.71ms
step:1175/2330 train_time:71336ms step_avg:60.71ms
step:1176/2330 train_time:71399ms step_avg:60.71ms
step:1177/2330 train_time:71459ms step_avg:60.71ms
step:1178/2330 train_time:71521ms step_avg:60.71ms
step:1179/2330 train_time:71581ms step_avg:60.71ms
step:1180/2330 train_time:71644ms step_avg:60.72ms
step:1181/2330 train_time:71704ms step_avg:60.71ms
step:1182/2330 train_time:71767ms step_avg:60.72ms
step:1183/2330 train_time:71827ms step_avg:60.72ms
step:1184/2330 train_time:71890ms step_avg:60.72ms
step:1185/2330 train_time:71950ms step_avg:60.72ms
step:1186/2330 train_time:72012ms step_avg:60.72ms
step:1187/2330 train_time:72071ms step_avg:60.72ms
step:1188/2330 train_time:72134ms step_avg:60.72ms
step:1189/2330 train_time:72193ms step_avg:60.72ms
step:1190/2330 train_time:72256ms step_avg:60.72ms
step:1191/2330 train_time:72315ms step_avg:60.72ms
step:1192/2330 train_time:72377ms step_avg:60.72ms
step:1193/2330 train_time:72437ms step_avg:60.72ms
step:1194/2330 train_time:72499ms step_avg:60.72ms
step:1195/2330 train_time:72559ms step_avg:60.72ms
step:1196/2330 train_time:72621ms step_avg:60.72ms
step:1197/2330 train_time:72682ms step_avg:60.72ms
step:1198/2330 train_time:72746ms step_avg:60.72ms
step:1199/2330 train_time:72806ms step_avg:60.72ms
step:1200/2330 train_time:72868ms step_avg:60.72ms
step:1201/2330 train_time:72928ms step_avg:60.72ms
step:1202/2330 train_time:72991ms step_avg:60.72ms
step:1203/2330 train_time:73051ms step_avg:60.72ms
step:1204/2330 train_time:73112ms step_avg:60.72ms
step:1205/2330 train_time:73171ms step_avg:60.72ms
step:1206/2330 train_time:73233ms step_avg:60.72ms
step:1207/2330 train_time:73293ms step_avg:60.72ms
step:1208/2330 train_time:73356ms step_avg:60.72ms
step:1209/2330 train_time:73415ms step_avg:60.72ms
step:1210/2330 train_time:73478ms step_avg:60.73ms
step:1211/2330 train_time:73538ms step_avg:60.72ms
step:1212/2330 train_time:73600ms step_avg:60.73ms
step:1213/2330 train_time:73660ms step_avg:60.73ms
step:1214/2330 train_time:73722ms step_avg:60.73ms
step:1215/2330 train_time:73783ms step_avg:60.73ms
step:1216/2330 train_time:73846ms step_avg:60.73ms
step:1217/2330 train_time:73906ms step_avg:60.73ms
step:1218/2330 train_time:73969ms step_avg:60.73ms
step:1219/2330 train_time:74029ms step_avg:60.73ms
step:1220/2330 train_time:74092ms step_avg:60.73ms
step:1221/2330 train_time:74152ms step_avg:60.73ms
step:1222/2330 train_time:74213ms step_avg:60.73ms
step:1223/2330 train_time:74272ms step_avg:60.73ms
step:1224/2330 train_time:74335ms step_avg:60.73ms
step:1225/2330 train_time:74394ms step_avg:60.73ms
step:1226/2330 train_time:74457ms step_avg:60.73ms
step:1227/2330 train_time:74516ms step_avg:60.73ms
step:1228/2330 train_time:74579ms step_avg:60.73ms
step:1229/2330 train_time:74639ms step_avg:60.73ms
step:1230/2330 train_time:74702ms step_avg:60.73ms
step:1231/2330 train_time:74761ms step_avg:60.73ms
step:1232/2330 train_time:74824ms step_avg:60.73ms
step:1233/2330 train_time:74885ms step_avg:60.73ms
step:1234/2330 train_time:74948ms step_avg:60.74ms
step:1235/2330 train_time:75008ms step_avg:60.73ms
step:1236/2330 train_time:75070ms step_avg:60.74ms
step:1237/2330 train_time:75130ms step_avg:60.74ms
step:1238/2330 train_time:75192ms step_avg:60.74ms
step:1239/2330 train_time:75251ms step_avg:60.74ms
step:1240/2330 train_time:75313ms step_avg:60.74ms
step:1241/2330 train_time:75372ms step_avg:60.73ms
step:1242/2330 train_time:75435ms step_avg:60.74ms
step:1243/2330 train_time:75494ms step_avg:60.74ms
step:1244/2330 train_time:75556ms step_avg:60.74ms
step:1245/2330 train_time:75617ms step_avg:60.74ms
step:1246/2330 train_time:75680ms step_avg:60.74ms
step:1247/2330 train_time:75740ms step_avg:60.74ms
step:1248/2330 train_time:75802ms step_avg:60.74ms
step:1249/2330 train_time:75862ms step_avg:60.74ms
step:1250/2330 train_time:75925ms step_avg:60.74ms
step:1250/2330 val_loss:3.6990 train_time:75999ms step_avg:60.80ms
step:1251/2330 train_time:76021ms step_avg:60.77ms
step:1252/2330 train_time:76052ms step_avg:60.74ms
step:1253/2330 train_time:76115ms step_avg:60.75ms
step:1254/2330 train_time:76181ms step_avg:60.75ms
step:1255/2330 train_time:76240ms step_avg:60.75ms
step:1256/2330 train_time:76303ms step_avg:60.75ms
step:1257/2330 train_time:76363ms step_avg:60.75ms
step:1258/2330 train_time:76425ms step_avg:60.75ms
step:1259/2330 train_time:76484ms step_avg:60.75ms
step:1260/2330 train_time:76546ms step_avg:60.75ms
step:1261/2330 train_time:76604ms step_avg:60.75ms
step:1262/2330 train_time:76666ms step_avg:60.75ms
step:1263/2330 train_time:76725ms step_avg:60.75ms
step:1264/2330 train_time:76786ms step_avg:60.75ms
step:1265/2330 train_time:76845ms step_avg:60.75ms
step:1266/2330 train_time:76907ms step_avg:60.75ms
step:1267/2330 train_time:76967ms step_avg:60.75ms
step:1268/2330 train_time:77031ms step_avg:60.75ms
step:1269/2330 train_time:77093ms step_avg:60.75ms
step:1270/2330 train_time:77156ms step_avg:60.75ms
step:1271/2330 train_time:77216ms step_avg:60.75ms
step:1272/2330 train_time:77280ms step_avg:60.75ms
step:1273/2330 train_time:77338ms step_avg:60.75ms
step:1274/2330 train_time:77401ms step_avg:60.75ms
step:1275/2330 train_time:77460ms step_avg:60.75ms
step:1276/2330 train_time:77522ms step_avg:60.75ms
step:1277/2330 train_time:77582ms step_avg:60.75ms
step:1278/2330 train_time:77643ms step_avg:60.75ms
step:1279/2330 train_time:77703ms step_avg:60.75ms
step:1280/2330 train_time:77765ms step_avg:60.75ms
step:1281/2330 train_time:77824ms step_avg:60.75ms
step:1282/2330 train_time:77887ms step_avg:60.75ms
step:1283/2330 train_time:77946ms step_avg:60.75ms
step:1284/2330 train_time:78008ms step_avg:60.75ms
step:1285/2330 train_time:78068ms step_avg:60.75ms
step:1286/2330 train_time:78131ms step_avg:60.76ms
step:1287/2330 train_time:78192ms step_avg:60.75ms
step:1288/2330 train_time:78255ms step_avg:60.76ms
step:1289/2330 train_time:78315ms step_avg:60.76ms
step:1290/2330 train_time:78378ms step_avg:60.76ms
step:1291/2330 train_time:78437ms step_avg:60.76ms
step:1292/2330 train_time:78500ms step_avg:60.76ms
step:1293/2330 train_time:78558ms step_avg:60.76ms
step:1294/2330 train_time:78621ms step_avg:60.76ms
step:1295/2330 train_time:78680ms step_avg:60.76ms
step:1296/2330 train_time:78741ms step_avg:60.76ms
step:1297/2330 train_time:78800ms step_avg:60.76ms
step:1298/2330 train_time:78863ms step_avg:60.76ms
step:1299/2330 train_time:78922ms step_avg:60.76ms
step:1300/2330 train_time:78986ms step_avg:60.76ms
step:1301/2330 train_time:79046ms step_avg:60.76ms
step:1302/2330 train_time:79109ms step_avg:60.76ms
step:1303/2330 train_time:79169ms step_avg:60.76ms
step:1304/2330 train_time:79232ms step_avg:60.76ms
step:1305/2330 train_time:79292ms step_avg:60.76ms
step:1306/2330 train_time:79355ms step_avg:60.76ms
step:1307/2330 train_time:79415ms step_avg:60.76ms
step:1308/2330 train_time:79478ms step_avg:60.76ms
step:1309/2330 train_time:79537ms step_avg:60.76ms
step:1310/2330 train_time:79600ms step_avg:60.76ms
step:1311/2330 train_time:79660ms step_avg:60.76ms
step:1312/2330 train_time:79722ms step_avg:60.76ms
step:1313/2330 train_time:79781ms step_avg:60.76ms
step:1314/2330 train_time:79843ms step_avg:60.76ms
step:1315/2330 train_time:79902ms step_avg:60.76ms
step:1316/2330 train_time:79965ms step_avg:60.76ms
step:1317/2330 train_time:80024ms step_avg:60.76ms
step:1318/2330 train_time:80087ms step_avg:60.76ms
step:1319/2330 train_time:80148ms step_avg:60.76ms
step:1320/2330 train_time:80210ms step_avg:60.77ms
step:1321/2330 train_time:80269ms step_avg:60.76ms
step:1322/2330 train_time:80332ms step_avg:60.77ms
step:1323/2330 train_time:80393ms step_avg:60.77ms
step:1324/2330 train_time:80455ms step_avg:60.77ms
step:1325/2330 train_time:80516ms step_avg:60.77ms
step:1326/2330 train_time:80579ms step_avg:60.77ms
step:1327/2330 train_time:80639ms step_avg:60.77ms
step:1328/2330 train_time:80701ms step_avg:60.77ms
step:1329/2330 train_time:80760ms step_avg:60.77ms
step:1330/2330 train_time:80822ms step_avg:60.77ms
step:1331/2330 train_time:80881ms step_avg:60.77ms
step:1332/2330 train_time:80944ms step_avg:60.77ms
step:1333/2330 train_time:81003ms step_avg:60.77ms
step:1334/2330 train_time:81067ms step_avg:60.77ms
step:1335/2330 train_time:81126ms step_avg:60.77ms
step:1336/2330 train_time:81189ms step_avg:60.77ms
step:1337/2330 train_time:81249ms step_avg:60.77ms
step:1338/2330 train_time:81311ms step_avg:60.77ms
step:1339/2330 train_time:81371ms step_avg:60.77ms
step:1340/2330 train_time:81435ms step_avg:60.77ms
step:1341/2330 train_time:81495ms step_avg:60.77ms
step:1342/2330 train_time:81557ms step_avg:60.77ms
step:1343/2330 train_time:81617ms step_avg:60.77ms
step:1344/2330 train_time:81681ms step_avg:60.77ms
step:1345/2330 train_time:81740ms step_avg:60.77ms
step:1346/2330 train_time:81802ms step_avg:60.77ms
step:1347/2330 train_time:81861ms step_avg:60.77ms
step:1348/2330 train_time:81925ms step_avg:60.77ms
step:1349/2330 train_time:81985ms step_avg:60.77ms
step:1350/2330 train_time:82047ms step_avg:60.78ms
step:1351/2330 train_time:82106ms step_avg:60.77ms
step:1352/2330 train_time:82169ms step_avg:60.78ms
step:1353/2330 train_time:82229ms step_avg:60.78ms
step:1354/2330 train_time:82292ms step_avg:60.78ms
step:1355/2330 train_time:82351ms step_avg:60.78ms
step:1356/2330 train_time:82414ms step_avg:60.78ms
step:1357/2330 train_time:82475ms step_avg:60.78ms
step:1358/2330 train_time:82537ms step_avg:60.78ms
step:1359/2330 train_time:82597ms step_avg:60.78ms
step:1360/2330 train_time:82659ms step_avg:60.78ms
step:1361/2330 train_time:82719ms step_avg:60.78ms
step:1362/2330 train_time:82781ms step_avg:60.78ms
step:1363/2330 train_time:82839ms step_avg:60.78ms
step:1364/2330 train_time:82903ms step_avg:60.78ms
step:1365/2330 train_time:82962ms step_avg:60.78ms
step:1366/2330 train_time:83024ms step_avg:60.78ms
step:1367/2330 train_time:83084ms step_avg:60.78ms
step:1368/2330 train_time:83147ms step_avg:60.78ms
step:1369/2330 train_time:83206ms step_avg:60.78ms
step:1370/2330 train_time:83269ms step_avg:60.78ms
step:1371/2330 train_time:83328ms step_avg:60.78ms
step:1372/2330 train_time:83391ms step_avg:60.78ms
step:1373/2330 train_time:83450ms step_avg:60.78ms
step:1374/2330 train_time:83513ms step_avg:60.78ms
step:1375/2330 train_time:83573ms step_avg:60.78ms
step:1376/2330 train_time:83636ms step_avg:60.78ms
step:1377/2330 train_time:83695ms step_avg:60.78ms
step:1378/2330 train_time:83757ms step_avg:60.78ms
step:1379/2330 train_time:83818ms step_avg:60.78ms
step:1380/2330 train_time:83881ms step_avg:60.78ms
step:1381/2330 train_time:83939ms step_avg:60.78ms
step:1382/2330 train_time:84002ms step_avg:60.78ms
step:1383/2330 train_time:84061ms step_avg:60.78ms
step:1384/2330 train_time:84124ms step_avg:60.78ms
step:1385/2330 train_time:84183ms step_avg:60.78ms
step:1386/2330 train_time:84246ms step_avg:60.78ms
step:1387/2330 train_time:84306ms step_avg:60.78ms
step:1388/2330 train_time:84369ms step_avg:60.78ms
step:1389/2330 train_time:84429ms step_avg:60.78ms
step:1390/2330 train_time:84491ms step_avg:60.79ms
step:1391/2330 train_time:84552ms step_avg:60.78ms
step:1392/2330 train_time:84615ms step_avg:60.79ms
step:1393/2330 train_time:84676ms step_avg:60.79ms
step:1394/2330 train_time:84738ms step_avg:60.79ms
step:1395/2330 train_time:84797ms step_avg:60.79ms
step:1396/2330 train_time:84860ms step_avg:60.79ms
step:1397/2330 train_time:84919ms step_avg:60.79ms
step:1398/2330 train_time:84982ms step_avg:60.79ms
step:1399/2330 train_time:85041ms step_avg:60.79ms
step:1400/2330 train_time:85104ms step_avg:60.79ms
step:1401/2330 train_time:85163ms step_avg:60.79ms
step:1402/2330 train_time:85226ms step_avg:60.79ms
step:1403/2330 train_time:85286ms step_avg:60.79ms
step:1404/2330 train_time:85349ms step_avg:60.79ms
step:1405/2330 train_time:85409ms step_avg:60.79ms
step:1406/2330 train_time:85472ms step_avg:60.79ms
step:1407/2330 train_time:85531ms step_avg:60.79ms
step:1408/2330 train_time:85594ms step_avg:60.79ms
step:1409/2330 train_time:85654ms step_avg:60.79ms
step:1410/2330 train_time:85717ms step_avg:60.79ms
step:1411/2330 train_time:85778ms step_avg:60.79ms
step:1412/2330 train_time:85840ms step_avg:60.79ms
step:1413/2330 train_time:85899ms step_avg:60.79ms
step:1414/2330 train_time:85961ms step_avg:60.79ms
step:1415/2330 train_time:86021ms step_avg:60.79ms
step:1416/2330 train_time:86084ms step_avg:60.79ms
step:1417/2330 train_time:86143ms step_avg:60.79ms
step:1418/2330 train_time:86205ms step_avg:60.79ms
step:1419/2330 train_time:86265ms step_avg:60.79ms
step:1420/2330 train_time:86327ms step_avg:60.79ms
step:1421/2330 train_time:86387ms step_avg:60.79ms
step:1422/2330 train_time:86450ms step_avg:60.79ms
step:1423/2330 train_time:86509ms step_avg:60.79ms
step:1424/2330 train_time:86572ms step_avg:60.79ms
step:1425/2330 train_time:86631ms step_avg:60.79ms
step:1426/2330 train_time:86694ms step_avg:60.80ms
step:1427/2330 train_time:86754ms step_avg:60.79ms
step:1428/2330 train_time:86817ms step_avg:60.80ms
step:1429/2330 train_time:86878ms step_avg:60.80ms
step:1430/2330 train_time:86941ms step_avg:60.80ms
step:1431/2330 train_time:87000ms step_avg:60.80ms
step:1432/2330 train_time:87063ms step_avg:60.80ms
step:1433/2330 train_time:87122ms step_avg:60.80ms
step:1434/2330 train_time:87186ms step_avg:60.80ms
step:1435/2330 train_time:87244ms step_avg:60.80ms
step:1436/2330 train_time:87307ms step_avg:60.80ms
step:1437/2330 train_time:87366ms step_avg:60.80ms
step:1438/2330 train_time:87428ms step_avg:60.80ms
step:1439/2330 train_time:87488ms step_avg:60.80ms
step:1440/2330 train_time:87551ms step_avg:60.80ms
step:1441/2330 train_time:87610ms step_avg:60.80ms
step:1442/2330 train_time:87674ms step_avg:60.80ms
step:1443/2330 train_time:87734ms step_avg:60.80ms
step:1444/2330 train_time:87797ms step_avg:60.80ms
step:1445/2330 train_time:87857ms step_avg:60.80ms
step:1446/2330 train_time:87919ms step_avg:60.80ms
step:1447/2330 train_time:87980ms step_avg:60.80ms
step:1448/2330 train_time:88041ms step_avg:60.80ms
step:1449/2330 train_time:88101ms step_avg:60.80ms
step:1450/2330 train_time:88164ms step_avg:60.80ms
step:1451/2330 train_time:88223ms step_avg:60.80ms
step:1452/2330 train_time:88286ms step_avg:60.80ms
step:1453/2330 train_time:88346ms step_avg:60.80ms
step:1454/2330 train_time:88408ms step_avg:60.80ms
step:1455/2330 train_time:88467ms step_avg:60.80ms
step:1456/2330 train_time:88530ms step_avg:60.80ms
step:1457/2330 train_time:88589ms step_avg:60.80ms
step:1458/2330 train_time:88651ms step_avg:60.80ms
step:1459/2330 train_time:88711ms step_avg:60.80ms
step:1460/2330 train_time:88775ms step_avg:60.80ms
step:1461/2330 train_time:88834ms step_avg:60.80ms
step:1462/2330 train_time:88897ms step_avg:60.81ms
step:1463/2330 train_time:88957ms step_avg:60.80ms
step:1464/2330 train_time:89021ms step_avg:60.81ms
step:1465/2330 train_time:89081ms step_avg:60.81ms
step:1466/2330 train_time:89143ms step_avg:60.81ms
step:1467/2330 train_time:89203ms step_avg:60.81ms
step:1468/2330 train_time:89265ms step_avg:60.81ms
step:1469/2330 train_time:89324ms step_avg:60.81ms
step:1470/2330 train_time:89387ms step_avg:60.81ms
step:1471/2330 train_time:89446ms step_avg:60.81ms
step:1472/2330 train_time:89508ms step_avg:60.81ms
step:1473/2330 train_time:89568ms step_avg:60.81ms
step:1474/2330 train_time:89630ms step_avg:60.81ms
step:1475/2330 train_time:89690ms step_avg:60.81ms
step:1476/2330 train_time:89753ms step_avg:60.81ms
step:1477/2330 train_time:89812ms step_avg:60.81ms
step:1478/2330 train_time:89876ms step_avg:60.81ms
step:1479/2330 train_time:89936ms step_avg:60.81ms
step:1480/2330 train_time:89999ms step_avg:60.81ms
step:1481/2330 train_time:90059ms step_avg:60.81ms
step:1482/2330 train_time:90121ms step_avg:60.81ms
step:1483/2330 train_time:90180ms step_avg:60.81ms
step:1484/2330 train_time:90242ms step_avg:60.81ms
step:1485/2330 train_time:90301ms step_avg:60.81ms
step:1486/2330 train_time:90363ms step_avg:60.81ms
step:1487/2330 train_time:90422ms step_avg:60.81ms
step:1488/2330 train_time:90485ms step_avg:60.81ms
step:1489/2330 train_time:90546ms step_avg:60.81ms
step:1490/2330 train_time:90609ms step_avg:60.81ms
step:1491/2330 train_time:90668ms step_avg:60.81ms
step:1492/2330 train_time:90731ms step_avg:60.81ms
step:1493/2330 train_time:90790ms step_avg:60.81ms
step:1494/2330 train_time:90853ms step_avg:60.81ms
step:1495/2330 train_time:90914ms step_avg:60.81ms
step:1496/2330 train_time:90976ms step_avg:60.81ms
step:1497/2330 train_time:91035ms step_avg:60.81ms
step:1498/2330 train_time:91098ms step_avg:60.81ms
step:1499/2330 train_time:91158ms step_avg:60.81ms
step:1500/2330 train_time:91222ms step_avg:60.81ms
step:1500/2330 val_loss:3.6186 train_time:91294ms step_avg:60.86ms
step:1501/2330 train_time:91316ms step_avg:60.84ms
step:1502/2330 train_time:91345ms step_avg:60.82ms
step:1503/2330 train_time:91408ms step_avg:60.82ms
step:1504/2330 train_time:91472ms step_avg:60.82ms
step:1505/2330 train_time:91532ms step_avg:60.82ms
step:1506/2330 train_time:91596ms step_avg:60.82ms
step:1507/2330 train_time:91655ms step_avg:60.82ms
step:1508/2330 train_time:91717ms step_avg:60.82ms
step:1509/2330 train_time:91777ms step_avg:60.82ms
step:1510/2330 train_time:91839ms step_avg:60.82ms
step:1511/2330 train_time:91898ms step_avg:60.82ms
step:1512/2330 train_time:91960ms step_avg:60.82ms
step:1513/2330 train_time:92018ms step_avg:60.82ms
step:1514/2330 train_time:92080ms step_avg:60.82ms
step:1515/2330 train_time:92138ms step_avg:60.82ms
step:1516/2330 train_time:92202ms step_avg:60.82ms
step:1517/2330 train_time:92262ms step_avg:60.82ms
step:1518/2330 train_time:92326ms step_avg:60.82ms
step:1519/2330 train_time:92386ms step_avg:60.82ms
step:1520/2330 train_time:92448ms step_avg:60.82ms
step:1521/2330 train_time:92509ms step_avg:60.82ms
step:1522/2330 train_time:92571ms step_avg:60.82ms
step:1523/2330 train_time:92631ms step_avg:60.82ms
step:1524/2330 train_time:92694ms step_avg:60.82ms
step:1525/2330 train_time:92753ms step_avg:60.82ms
step:1526/2330 train_time:92815ms step_avg:60.82ms
step:1527/2330 train_time:92874ms step_avg:60.82ms
step:1528/2330 train_time:92937ms step_avg:60.82ms
step:1529/2330 train_time:92997ms step_avg:60.82ms
step:1530/2330 train_time:93058ms step_avg:60.82ms
step:1531/2330 train_time:93118ms step_avg:60.82ms
step:1532/2330 train_time:93181ms step_avg:60.82ms
step:1533/2330 train_time:93242ms step_avg:60.82ms
step:1534/2330 train_time:93305ms step_avg:60.82ms
step:1535/2330 train_time:93365ms step_avg:60.82ms
step:1536/2330 train_time:93428ms step_avg:60.83ms
step:1537/2330 train_time:93488ms step_avg:60.83ms
step:1538/2330 train_time:93552ms step_avg:60.83ms
step:1539/2330 train_time:93612ms step_avg:60.83ms
step:1540/2330 train_time:93675ms step_avg:60.83ms
step:1541/2330 train_time:93735ms step_avg:60.83ms
step:1542/2330 train_time:93797ms step_avg:60.83ms
step:1543/2330 train_time:93857ms step_avg:60.83ms
step:1544/2330 train_time:93920ms step_avg:60.83ms
step:1545/2330 train_time:93979ms step_avg:60.83ms
step:1546/2330 train_time:94042ms step_avg:60.83ms
step:1547/2330 train_time:94102ms step_avg:60.83ms
step:1548/2330 train_time:94164ms step_avg:60.83ms
step:1549/2330 train_time:94225ms step_avg:60.83ms
step:1550/2330 train_time:94287ms step_avg:60.83ms
step:1551/2330 train_time:94347ms step_avg:60.83ms
step:1552/2330 train_time:94411ms step_avg:60.83ms
step:1553/2330 train_time:94470ms step_avg:60.83ms
step:1554/2330 train_time:94533ms step_avg:60.83ms
step:1555/2330 train_time:94593ms step_avg:60.83ms
step:1556/2330 train_time:94656ms step_avg:60.83ms
step:1557/2330 train_time:94717ms step_avg:60.83ms
step:1558/2330 train_time:94779ms step_avg:60.83ms
step:1559/2330 train_time:94839ms step_avg:60.83ms
step:1560/2330 train_time:94901ms step_avg:60.83ms
step:1561/2330 train_time:94961ms step_avg:60.83ms
step:1562/2330 train_time:95023ms step_avg:60.83ms
step:1563/2330 train_time:95082ms step_avg:60.83ms
step:1564/2330 train_time:95145ms step_avg:60.83ms
step:1565/2330 train_time:95205ms step_avg:60.83ms
step:1566/2330 train_time:95268ms step_avg:60.83ms
step:1567/2330 train_time:95328ms step_avg:60.83ms
step:1568/2330 train_time:95391ms step_avg:60.84ms
step:1569/2330 train_time:95451ms step_avg:60.84ms
step:1570/2330 train_time:95515ms step_avg:60.84ms
step:1571/2330 train_time:95575ms step_avg:60.84ms
step:1572/2330 train_time:95637ms step_avg:60.84ms
step:1573/2330 train_time:95697ms step_avg:60.84ms
step:1574/2330 train_time:95759ms step_avg:60.84ms
step:1575/2330 train_time:95818ms step_avg:60.84ms
step:1576/2330 train_time:95881ms step_avg:60.84ms
step:1577/2330 train_time:95941ms step_avg:60.84ms
step:1578/2330 train_time:96005ms step_avg:60.84ms
step:1579/2330 train_time:96064ms step_avg:60.84ms
step:1580/2330 train_time:96127ms step_avg:60.84ms
step:1581/2330 train_time:96186ms step_avg:60.84ms
step:1582/2330 train_time:96248ms step_avg:60.84ms
step:1583/2330 train_time:96309ms step_avg:60.84ms
step:1584/2330 train_time:96372ms step_avg:60.84ms
step:1585/2330 train_time:96431ms step_avg:60.84ms
step:1586/2330 train_time:96494ms step_avg:60.84ms
step:1587/2330 train_time:96554ms step_avg:60.84ms
step:1588/2330 train_time:96616ms step_avg:60.84ms
step:1589/2330 train_time:96676ms step_avg:60.84ms
step:1590/2330 train_time:96739ms step_avg:60.84ms
step:1591/2330 train_time:96800ms step_avg:60.84ms
step:1592/2330 train_time:96862ms step_avg:60.84ms
step:1593/2330 train_time:96923ms step_avg:60.84ms
step:1594/2330 train_time:96985ms step_avg:60.84ms
step:1595/2330 train_time:97045ms step_avg:60.84ms
step:1596/2330 train_time:97107ms step_avg:60.84ms
step:1597/2330 train_time:97166ms step_avg:60.84ms
step:1598/2330 train_time:97229ms step_avg:60.84ms
step:1599/2330 train_time:97289ms step_avg:60.84ms
step:1600/2330 train_time:97353ms step_avg:60.85ms
step:1601/2330 train_time:97413ms step_avg:60.84ms
step:1602/2330 train_time:97475ms step_avg:60.85ms
step:1603/2330 train_time:97535ms step_avg:60.85ms
step:1604/2330 train_time:97598ms step_avg:60.85ms
step:1605/2330 train_time:97657ms step_avg:60.85ms
step:1606/2330 train_time:97720ms step_avg:60.85ms
step:1607/2330 train_time:97780ms step_avg:60.85ms
step:1608/2330 train_time:97843ms step_avg:60.85ms
step:1609/2330 train_time:97903ms step_avg:60.85ms
step:1610/2330 train_time:97965ms step_avg:60.85ms
step:1611/2330 train_time:98024ms step_avg:60.85ms
step:1612/2330 train_time:98087ms step_avg:60.85ms
step:1613/2330 train_time:98146ms step_avg:60.85ms
step:1614/2330 train_time:98210ms step_avg:60.85ms
step:1615/2330 train_time:98270ms step_avg:60.85ms
step:1616/2330 train_time:98333ms step_avg:60.85ms
step:1617/2330 train_time:98392ms step_avg:60.85ms
step:1618/2330 train_time:98455ms step_avg:60.85ms
step:1619/2330 train_time:98515ms step_avg:60.85ms
step:1620/2330 train_time:98578ms step_avg:60.85ms
step:1621/2330 train_time:98638ms step_avg:60.85ms
step:1622/2330 train_time:98701ms step_avg:60.85ms
step:1623/2330 train_time:98760ms step_avg:60.85ms
step:1624/2330 train_time:98823ms step_avg:60.85ms
step:1625/2330 train_time:98883ms step_avg:60.85ms
step:1626/2330 train_time:98946ms step_avg:60.85ms
step:1627/2330 train_time:99007ms step_avg:60.85ms
step:1628/2330 train_time:99069ms step_avg:60.85ms
step:1629/2330 train_time:99129ms step_avg:60.85ms
step:1630/2330 train_time:99192ms step_avg:60.85ms
step:1631/2330 train_time:99251ms step_avg:60.85ms
step:1632/2330 train_time:99315ms step_avg:60.85ms
step:1633/2330 train_time:99375ms step_avg:60.85ms
step:1634/2330 train_time:99438ms step_avg:60.86ms
step:1635/2330 train_time:99497ms step_avg:60.85ms
step:1636/2330 train_time:99560ms step_avg:60.86ms
step:1637/2330 train_time:99621ms step_avg:60.86ms
step:1638/2330 train_time:99683ms step_avg:60.86ms
step:1639/2330 train_time:99743ms step_avg:60.86ms
step:1640/2330 train_time:99807ms step_avg:60.86ms
step:1641/2330 train_time:99865ms step_avg:60.86ms
step:1642/2330 train_time:99929ms step_avg:60.86ms
step:1643/2330 train_time:99989ms step_avg:60.86ms
step:1644/2330 train_time:100051ms step_avg:60.86ms
step:1645/2330 train_time:100112ms step_avg:60.86ms
step:1646/2330 train_time:100174ms step_avg:60.86ms
step:1647/2330 train_time:100234ms step_avg:60.86ms
step:1648/2330 train_time:100298ms step_avg:60.86ms
step:1649/2330 train_time:100357ms step_avg:60.86ms
step:1650/2330 train_time:100419ms step_avg:60.86ms
step:1651/2330 train_time:100479ms step_avg:60.86ms
step:1652/2330 train_time:100542ms step_avg:60.86ms
step:1653/2330 train_time:100602ms step_avg:60.86ms
step:1654/2330 train_time:100664ms step_avg:60.86ms
step:1655/2330 train_time:100724ms step_avg:60.86ms
step:1656/2330 train_time:100787ms step_avg:60.86ms
step:1657/2330 train_time:100847ms step_avg:60.86ms
step:1658/2330 train_time:100909ms step_avg:60.86ms
step:1659/2330 train_time:100969ms step_avg:60.86ms
step:1660/2330 train_time:101031ms step_avg:60.86ms
step:1661/2330 train_time:101090ms step_avg:60.86ms
step:1662/2330 train_time:101154ms step_avg:60.86ms
step:1663/2330 train_time:101214ms step_avg:60.86ms
step:1664/2330 train_time:101277ms step_avg:60.86ms
step:1665/2330 train_time:101336ms step_avg:60.86ms
step:1666/2330 train_time:101400ms step_avg:60.86ms
step:1667/2330 train_time:101459ms step_avg:60.86ms
step:1668/2330 train_time:101522ms step_avg:60.86ms
step:1669/2330 train_time:101582ms step_avg:60.86ms
step:1670/2330 train_time:101644ms step_avg:60.86ms
step:1671/2330 train_time:101704ms step_avg:60.86ms
step:1672/2330 train_time:101767ms step_avg:60.87ms
step:1673/2330 train_time:101826ms step_avg:60.86ms
step:1674/2330 train_time:101889ms step_avg:60.87ms
step:1675/2330 train_time:101948ms step_avg:60.86ms
step:1676/2330 train_time:102011ms step_avg:60.87ms
step:1677/2330 train_time:102071ms step_avg:60.87ms
step:1678/2330 train_time:102134ms step_avg:60.87ms
step:1679/2330 train_time:102194ms step_avg:60.87ms
step:1680/2330 train_time:102256ms step_avg:60.87ms
step:1681/2330 train_time:102316ms step_avg:60.87ms
step:1682/2330 train_time:102379ms step_avg:60.87ms
step:1683/2330 train_time:102439ms step_avg:60.87ms
step:1684/2330 train_time:102503ms step_avg:60.87ms
step:1685/2330 train_time:102563ms step_avg:60.87ms
step:1686/2330 train_time:102624ms step_avg:60.87ms
step:1687/2330 train_time:102685ms step_avg:60.87ms
step:1688/2330 train_time:102748ms step_avg:60.87ms
step:1689/2330 train_time:102808ms step_avg:60.87ms
step:1690/2330 train_time:102870ms step_avg:60.87ms
step:1691/2330 train_time:102930ms step_avg:60.87ms
step:1692/2330 train_time:102992ms step_avg:60.87ms
step:1693/2330 train_time:103052ms step_avg:60.87ms
step:1694/2330 train_time:103116ms step_avg:60.87ms
step:1695/2330 train_time:103175ms step_avg:60.87ms
step:1696/2330 train_time:103238ms step_avg:60.87ms
step:1697/2330 train_time:103298ms step_avg:60.87ms
step:1698/2330 train_time:103361ms step_avg:60.87ms
step:1699/2330 train_time:103421ms step_avg:60.87ms
step:1700/2330 train_time:103483ms step_avg:60.87ms
step:1701/2330 train_time:103543ms step_avg:60.87ms
step:1702/2330 train_time:103607ms step_avg:60.87ms
step:1703/2330 train_time:103665ms step_avg:60.87ms
step:1704/2330 train_time:103728ms step_avg:60.87ms
step:1705/2330 train_time:103788ms step_avg:60.87ms
step:1706/2330 train_time:103851ms step_avg:60.87ms
step:1707/2330 train_time:103911ms step_avg:60.87ms
step:1708/2330 train_time:103975ms step_avg:60.88ms
step:1709/2330 train_time:104035ms step_avg:60.87ms
step:1710/2330 train_time:104098ms step_avg:60.88ms
step:1711/2330 train_time:104157ms step_avg:60.88ms
step:1712/2330 train_time:104220ms step_avg:60.88ms
step:1713/2330 train_time:104279ms step_avg:60.88ms
step:1714/2330 train_time:104342ms step_avg:60.88ms
step:1715/2330 train_time:104402ms step_avg:60.88ms
step:1716/2330 train_time:104464ms step_avg:60.88ms
step:1717/2330 train_time:104524ms step_avg:60.88ms
step:1718/2330 train_time:104586ms step_avg:60.88ms
step:1719/2330 train_time:104645ms step_avg:60.88ms
step:1720/2330 train_time:104709ms step_avg:60.88ms
step:1721/2330 train_time:104768ms step_avg:60.88ms
step:1722/2330 train_time:104831ms step_avg:60.88ms
step:1723/2330 train_time:104891ms step_avg:60.88ms
step:1724/2330 train_time:104954ms step_avg:60.88ms
step:1725/2330 train_time:105014ms step_avg:60.88ms
step:1726/2330 train_time:105077ms step_avg:60.88ms
step:1727/2330 train_time:105137ms step_avg:60.88ms
step:1728/2330 train_time:105200ms step_avg:60.88ms
step:1729/2330 train_time:105261ms step_avg:60.88ms
step:1730/2330 train_time:105323ms step_avg:60.88ms
step:1731/2330 train_time:105383ms step_avg:60.88ms
step:1732/2330 train_time:105445ms step_avg:60.88ms
step:1733/2330 train_time:105506ms step_avg:60.88ms
step:1734/2330 train_time:105567ms step_avg:60.88ms
step:1735/2330 train_time:105628ms step_avg:60.88ms
step:1736/2330 train_time:105690ms step_avg:60.88ms
step:1737/2330 train_time:105750ms step_avg:60.88ms
step:1738/2330 train_time:105812ms step_avg:60.88ms
step:1739/2330 train_time:105872ms step_avg:60.88ms
step:1740/2330 train_time:105935ms step_avg:60.88ms
step:1741/2330 train_time:105995ms step_avg:60.88ms
step:1742/2330 train_time:106058ms step_avg:60.88ms
step:1743/2330 train_time:106117ms step_avg:60.88ms
step:1744/2330 train_time:106180ms step_avg:60.88ms
step:1745/2330 train_time:106240ms step_avg:60.88ms
step:1746/2330 train_time:106304ms step_avg:60.88ms
step:1747/2330 train_time:106363ms step_avg:60.88ms
step:1748/2330 train_time:106426ms step_avg:60.88ms
step:1749/2330 train_time:106485ms step_avg:60.88ms
step:1750/2330 train_time:106548ms step_avg:60.88ms
step:1750/2330 val_loss:3.5433 train_time:106621ms step_avg:60.93ms
step:1751/2330 train_time:106642ms step_avg:60.90ms
step:1752/2330 train_time:106672ms step_avg:60.89ms
step:1753/2330 train_time:106734ms step_avg:60.89ms
step:1754/2330 train_time:106803ms step_avg:60.89ms
step:1755/2330 train_time:106863ms step_avg:60.89ms
step:1756/2330 train_time:106926ms step_avg:60.89ms
step:1757/2330 train_time:106986ms step_avg:60.89ms
step:1758/2330 train_time:107047ms step_avg:60.89ms
step:1759/2330 train_time:107107ms step_avg:60.89ms
step:1760/2330 train_time:107169ms step_avg:60.89ms
step:1761/2330 train_time:107228ms step_avg:60.89ms
step:1762/2330 train_time:107290ms step_avg:60.89ms
step:1763/2330 train_time:107350ms step_avg:60.89ms
step:1764/2330 train_time:107412ms step_avg:60.89ms
step:1765/2330 train_time:107470ms step_avg:60.89ms
step:1766/2330 train_time:107533ms step_avg:60.89ms
step:1767/2330 train_time:107595ms step_avg:60.89ms
step:1768/2330 train_time:107661ms step_avg:60.89ms
step:1769/2330 train_time:107724ms step_avg:60.90ms
step:1770/2330 train_time:107787ms step_avg:60.90ms
step:1771/2330 train_time:107847ms step_avg:60.90ms
step:1772/2330 train_time:107909ms step_avg:60.90ms
step:1773/2330 train_time:107969ms step_avg:60.90ms
step:1774/2330 train_time:108031ms step_avg:60.90ms
step:1775/2330 train_time:108091ms step_avg:60.90ms
step:1776/2330 train_time:108153ms step_avg:60.90ms
step:1777/2330 train_time:108214ms step_avg:60.90ms
step:1778/2330 train_time:108276ms step_avg:60.90ms
step:1779/2330 train_time:108336ms step_avg:60.90ms
step:1780/2330 train_time:108397ms step_avg:60.90ms
step:1781/2330 train_time:108456ms step_avg:60.90ms
step:1782/2330 train_time:108520ms step_avg:60.90ms
step:1783/2330 train_time:108580ms step_avg:60.90ms
step:1784/2330 train_time:108644ms step_avg:60.90ms
step:1785/2330 train_time:108705ms step_avg:60.90ms
step:1786/2330 train_time:108768ms step_avg:60.90ms
step:1787/2330 train_time:108828ms step_avg:60.90ms
step:1788/2330 train_time:108891ms step_avg:60.90ms
step:1789/2330 train_time:108952ms step_avg:60.90ms
step:1790/2330 train_time:109015ms step_avg:60.90ms
step:1791/2330 train_time:109075ms step_avg:60.90ms
step:1792/2330 train_time:109138ms step_avg:60.90ms
step:1793/2330 train_time:109198ms step_avg:60.90ms
step:1794/2330 train_time:109260ms step_avg:60.90ms
step:1795/2330 train_time:109319ms step_avg:60.90ms
step:1796/2330 train_time:109381ms step_avg:60.90ms
step:1797/2330 train_time:109440ms step_avg:60.90ms
step:1798/2330 train_time:109503ms step_avg:60.90ms
step:1799/2330 train_time:109563ms step_avg:60.90ms
step:1800/2330 train_time:109626ms step_avg:60.90ms
step:1801/2330 train_time:109687ms step_avg:60.90ms
step:1802/2330 train_time:109750ms step_avg:60.90ms
step:1803/2330 train_time:109811ms step_avg:60.90ms
step:1804/2330 train_time:109874ms step_avg:60.91ms
step:1805/2330 train_time:109933ms step_avg:60.90ms
step:1806/2330 train_time:109996ms step_avg:60.91ms
step:1807/2330 train_time:110056ms step_avg:60.91ms
step:1808/2330 train_time:110119ms step_avg:60.91ms
step:1809/2330 train_time:110179ms step_avg:60.91ms
step:1810/2330 train_time:110242ms step_avg:60.91ms
step:1811/2330 train_time:110301ms step_avg:60.91ms
step:1812/2330 train_time:110365ms step_avg:60.91ms
step:1813/2330 train_time:110424ms step_avg:60.91ms
step:1814/2330 train_time:110487ms step_avg:60.91ms
step:1815/2330 train_time:110547ms step_avg:60.91ms
step:1816/2330 train_time:110610ms step_avg:60.91ms
step:1817/2330 train_time:110669ms step_avg:60.91ms
step:1818/2330 train_time:110733ms step_avg:60.91ms
step:1819/2330 train_time:110793ms step_avg:60.91ms
step:1820/2330 train_time:110857ms step_avg:60.91ms
step:1821/2330 train_time:110918ms step_avg:60.91ms
step:1822/2330 train_time:110981ms step_avg:60.91ms
step:1823/2330 train_time:111040ms step_avg:60.91ms
step:1824/2330 train_time:111103ms step_avg:60.91ms
step:1825/2330 train_time:111163ms step_avg:60.91ms
step:1826/2330 train_time:111225ms step_avg:60.91ms
step:1827/2330 train_time:111285ms step_avg:60.91ms
step:1828/2330 train_time:111347ms step_avg:60.91ms
step:1829/2330 train_time:111407ms step_avg:60.91ms
step:1830/2330 train_time:111470ms step_avg:60.91ms
step:1831/2330 train_time:111529ms step_avg:60.91ms
step:1832/2330 train_time:111592ms step_avg:60.91ms
step:1833/2330 train_time:111652ms step_avg:60.91ms
step:1834/2330 train_time:111714ms step_avg:60.91ms
step:1835/2330 train_time:111774ms step_avg:60.91ms
step:1836/2330 train_time:111838ms step_avg:60.91ms
step:1837/2330 train_time:111898ms step_avg:60.91ms
step:1838/2330 train_time:111960ms step_avg:60.91ms
step:1839/2330 train_time:112020ms step_avg:60.91ms
step:1840/2330 train_time:112083ms step_avg:60.91ms
step:1841/2330 train_time:112142ms step_avg:60.91ms
step:1842/2330 train_time:112206ms step_avg:60.92ms
step:1843/2330 train_time:112265ms step_avg:60.91ms
step:1844/2330 train_time:112328ms step_avg:60.92ms
step:1845/2330 train_time:112387ms step_avg:60.91ms
step:1846/2330 train_time:112450ms step_avg:60.92ms
step:1847/2330 train_time:112510ms step_avg:60.91ms
step:1848/2330 train_time:112572ms step_avg:60.92ms
step:1849/2330 train_time:112632ms step_avg:60.91ms
step:1850/2330 train_time:112696ms step_avg:60.92ms
step:1851/2330 train_time:112755ms step_avg:60.92ms
step:1852/2330 train_time:112818ms step_avg:60.92ms
step:1853/2330 train_time:112878ms step_avg:60.92ms
step:1854/2330 train_time:112941ms step_avg:60.92ms
step:1855/2330 train_time:113001ms step_avg:60.92ms
step:1856/2330 train_time:113063ms step_avg:60.92ms
step:1857/2330 train_time:113123ms step_avg:60.92ms
step:1858/2330 train_time:113185ms step_avg:60.92ms
step:1859/2330 train_time:113246ms step_avg:60.92ms
step:1860/2330 train_time:113308ms step_avg:60.92ms
step:1861/2330 train_time:113368ms step_avg:60.92ms
step:1862/2330 train_time:113431ms step_avg:60.92ms
step:1863/2330 train_time:113490ms step_avg:60.92ms
step:1864/2330 train_time:113553ms step_avg:60.92ms
step:1865/2330 train_time:113612ms step_avg:60.92ms
step:1866/2330 train_time:113675ms step_avg:60.92ms
step:1867/2330 train_time:113736ms step_avg:60.92ms
step:1868/2330 train_time:113799ms step_avg:60.92ms
step:1869/2330 train_time:113859ms step_avg:60.92ms
step:1870/2330 train_time:113922ms step_avg:60.92ms
step:1871/2330 train_time:113982ms step_avg:60.92ms
step:1872/2330 train_time:114045ms step_avg:60.92ms
step:1873/2330 train_time:114106ms step_avg:60.92ms
step:1874/2330 train_time:114168ms step_avg:60.92ms
step:1875/2330 train_time:114227ms step_avg:60.92ms
step:1876/2330 train_time:114290ms step_avg:60.92ms
step:1877/2330 train_time:114349ms step_avg:60.92ms
step:1878/2330 train_time:114413ms step_avg:60.92ms
step:1879/2330 train_time:114472ms step_avg:60.92ms
step:1880/2330 train_time:114534ms step_avg:60.92ms
step:1881/2330 train_time:114594ms step_avg:60.92ms
step:1882/2330 train_time:114657ms step_avg:60.92ms
step:1883/2330 train_time:114717ms step_avg:60.92ms
step:1884/2330 train_time:114779ms step_avg:60.92ms
step:1885/2330 train_time:114839ms step_avg:60.92ms
step:1886/2330 train_time:114902ms step_avg:60.92ms
step:1887/2330 train_time:114962ms step_avg:60.92ms
step:1888/2330 train_time:115024ms step_avg:60.92ms
step:1889/2330 train_time:115084ms step_avg:60.92ms
step:1890/2330 train_time:115146ms step_avg:60.92ms
step:1891/2330 train_time:115207ms step_avg:60.92ms
step:1892/2330 train_time:115269ms step_avg:60.92ms
step:1893/2330 train_time:115329ms step_avg:60.92ms
step:1894/2330 train_time:115393ms step_avg:60.93ms
step:1895/2330 train_time:115453ms step_avg:60.92ms
step:1896/2330 train_time:115516ms step_avg:60.93ms
step:1897/2330 train_time:115576ms step_avg:60.93ms
step:1898/2330 train_time:115639ms step_avg:60.93ms
step:1899/2330 train_time:115699ms step_avg:60.93ms
step:1900/2330 train_time:115762ms step_avg:60.93ms
step:1901/2330 train_time:115821ms step_avg:60.93ms
step:1902/2330 train_time:115884ms step_avg:60.93ms
step:1903/2330 train_time:115944ms step_avg:60.93ms
step:1904/2330 train_time:116007ms step_avg:60.93ms
step:1905/2330 train_time:116066ms step_avg:60.93ms
step:1906/2330 train_time:116128ms step_avg:60.93ms
step:1907/2330 train_time:116189ms step_avg:60.93ms
step:1908/2330 train_time:116252ms step_avg:60.93ms
step:1909/2330 train_time:116311ms step_avg:60.93ms
step:1910/2330 train_time:116375ms step_avg:60.93ms
step:1911/2330 train_time:116434ms step_avg:60.93ms
step:1912/2330 train_time:116497ms step_avg:60.93ms
step:1913/2330 train_time:116557ms step_avg:60.93ms
step:1914/2330 train_time:116621ms step_avg:60.93ms
step:1915/2330 train_time:116680ms step_avg:60.93ms
step:1916/2330 train_time:116743ms step_avg:60.93ms
step:1917/2330 train_time:116802ms step_avg:60.93ms
step:1918/2330 train_time:116866ms step_avg:60.93ms
step:1919/2330 train_time:116925ms step_avg:60.93ms
step:1920/2330 train_time:116988ms step_avg:60.93ms
step:1921/2330 train_time:117047ms step_avg:60.93ms
step:1922/2330 train_time:117111ms step_avg:60.93ms
step:1923/2330 train_time:117170ms step_avg:60.93ms
step:1924/2330 train_time:117233ms step_avg:60.93ms
step:1925/2330 train_time:117293ms step_avg:60.93ms
step:1926/2330 train_time:117355ms step_avg:60.93ms
step:1927/2330 train_time:117416ms step_avg:60.93ms
step:1928/2330 train_time:117480ms step_avg:60.93ms
step:1929/2330 train_time:117540ms step_avg:60.93ms
step:1930/2330 train_time:117602ms step_avg:60.93ms
step:1931/2330 train_time:117662ms step_avg:60.93ms
step:1932/2330 train_time:117724ms step_avg:60.93ms
step:1933/2330 train_time:117785ms step_avg:60.93ms
step:1934/2330 train_time:117847ms step_avg:60.93ms
step:1935/2330 train_time:117908ms step_avg:60.93ms
step:1936/2330 train_time:117971ms step_avg:60.94ms
step:1937/2330 train_time:118030ms step_avg:60.93ms
step:1938/2330 train_time:118094ms step_avg:60.94ms
step:1939/2330 train_time:118154ms step_avg:60.94ms
step:1940/2330 train_time:118217ms step_avg:60.94ms
step:1941/2330 train_time:118277ms step_avg:60.94ms
step:1942/2330 train_time:118339ms step_avg:60.94ms
step:1943/2330 train_time:118399ms step_avg:60.94ms
step:1944/2330 train_time:118462ms step_avg:60.94ms
step:1945/2330 train_time:118522ms step_avg:60.94ms
step:1946/2330 train_time:118586ms step_avg:60.94ms
step:1947/2330 train_time:118645ms step_avg:60.94ms
step:1948/2330 train_time:118709ms step_avg:60.94ms
step:1949/2330 train_time:118768ms step_avg:60.94ms
step:1950/2330 train_time:118830ms step_avg:60.94ms
step:1951/2330 train_time:118891ms step_avg:60.94ms
step:1952/2330 train_time:118953ms step_avg:60.94ms
step:1953/2330 train_time:119014ms step_avg:60.94ms
step:1954/2330 train_time:119076ms step_avg:60.94ms
step:1955/2330 train_time:119136ms step_avg:60.94ms
step:1956/2330 train_time:119199ms step_avg:60.94ms
step:1957/2330 train_time:119259ms step_avg:60.94ms
step:1958/2330 train_time:119322ms step_avg:60.94ms
step:1959/2330 train_time:119382ms step_avg:60.94ms
step:1960/2330 train_time:119445ms step_avg:60.94ms
step:1961/2330 train_time:119504ms step_avg:60.94ms
step:1962/2330 train_time:119567ms step_avg:60.94ms
step:1963/2330 train_time:119627ms step_avg:60.94ms
step:1964/2330 train_time:119690ms step_avg:60.94ms
step:1965/2330 train_time:119750ms step_avg:60.94ms
step:1966/2330 train_time:119813ms step_avg:60.94ms
step:1967/2330 train_time:119874ms step_avg:60.94ms
step:1968/2330 train_time:119935ms step_avg:60.94ms
step:1969/2330 train_time:119995ms step_avg:60.94ms
step:1970/2330 train_time:120059ms step_avg:60.94ms
step:1971/2330 train_time:120119ms step_avg:60.94ms
step:1972/2330 train_time:120181ms step_avg:60.94ms
step:1973/2330 train_time:120241ms step_avg:60.94ms
step:1974/2330 train_time:120305ms step_avg:60.94ms
step:1975/2330 train_time:120364ms step_avg:60.94ms
step:1976/2330 train_time:120426ms step_avg:60.94ms
step:1977/2330 train_time:120486ms step_avg:60.94ms
step:1978/2330 train_time:120549ms step_avg:60.94ms
step:1979/2330 train_time:120611ms step_avg:60.95ms
step:1980/2330 train_time:120673ms step_avg:60.95ms
step:1981/2330 train_time:120732ms step_avg:60.94ms
step:1982/2330 train_time:120796ms step_avg:60.95ms
step:1983/2330 train_time:120854ms step_avg:60.95ms
step:1984/2330 train_time:120917ms step_avg:60.95ms
step:1985/2330 train_time:120977ms step_avg:60.95ms
step:1986/2330 train_time:121040ms step_avg:60.95ms
step:1987/2330 train_time:121099ms step_avg:60.95ms
step:1988/2330 train_time:121162ms step_avg:60.95ms
step:1989/2330 train_time:121222ms step_avg:60.95ms
step:1990/2330 train_time:121285ms step_avg:60.95ms
step:1991/2330 train_time:121345ms step_avg:60.95ms
step:1992/2330 train_time:121408ms step_avg:60.95ms
step:1993/2330 train_time:121468ms step_avg:60.95ms
step:1994/2330 train_time:121530ms step_avg:60.95ms
step:1995/2330 train_time:121590ms step_avg:60.95ms
step:1996/2330 train_time:121653ms step_avg:60.95ms
step:1997/2330 train_time:121712ms step_avg:60.95ms
step:1998/2330 train_time:121776ms step_avg:60.95ms
step:1999/2330 train_time:121835ms step_avg:60.95ms
step:2000/2330 train_time:121898ms step_avg:60.95ms
step:2000/2330 val_loss:3.4930 train_time:121972ms step_avg:60.99ms
step:2001/2330 train_time:121993ms step_avg:60.97ms
step:2002/2330 train_time:122024ms step_avg:60.95ms
step:2003/2330 train_time:122086ms step_avg:60.95ms
step:2004/2330 train_time:122153ms step_avg:60.95ms
step:2005/2330 train_time:122213ms step_avg:60.95ms
step:2006/2330 train_time:122276ms step_avg:60.96ms
step:2007/2330 train_time:122335ms step_avg:60.95ms
step:2008/2330 train_time:122397ms step_avg:60.95ms
step:2009/2330 train_time:122457ms step_avg:60.95ms
step:2010/2330 train_time:122519ms step_avg:60.95ms
step:2011/2330 train_time:122578ms step_avg:60.95ms
step:2012/2330 train_time:122641ms step_avg:60.95ms
step:2013/2330 train_time:122700ms step_avg:60.95ms
step:2014/2330 train_time:122763ms step_avg:60.95ms
step:2015/2330 train_time:122822ms step_avg:60.95ms
step:2016/2330 train_time:122885ms step_avg:60.95ms
step:2017/2330 train_time:122946ms step_avg:60.95ms
step:2018/2330 train_time:123009ms step_avg:60.96ms
step:2019/2330 train_time:123070ms step_avg:60.96ms
step:2020/2330 train_time:123135ms step_avg:60.96ms
step:2021/2330 train_time:123196ms step_avg:60.96ms
step:2022/2330 train_time:123259ms step_avg:60.96ms
step:2023/2330 train_time:123319ms step_avg:60.96ms
step:2024/2330 train_time:123381ms step_avg:60.96ms
step:2025/2330 train_time:123441ms step_avg:60.96ms
step:2026/2330 train_time:123505ms step_avg:60.96ms
step:2027/2330 train_time:123564ms step_avg:60.96ms
step:2028/2330 train_time:123626ms step_avg:60.96ms
step:2029/2330 train_time:123686ms step_avg:60.96ms
step:2030/2330 train_time:123748ms step_avg:60.96ms
step:2031/2330 train_time:123807ms step_avg:60.96ms
step:2032/2330 train_time:123870ms step_avg:60.96ms
step:2033/2330 train_time:123930ms step_avg:60.96ms
step:2034/2330 train_time:123995ms step_avg:60.96ms
step:2035/2330 train_time:124055ms step_avg:60.96ms
step:2036/2330 train_time:124118ms step_avg:60.96ms
step:2037/2330 train_time:124178ms step_avg:60.96ms
step:2038/2330 train_time:124243ms step_avg:60.96ms
step:2039/2330 train_time:124303ms step_avg:60.96ms
step:2040/2330 train_time:124366ms step_avg:60.96ms
step:2041/2330 train_time:124425ms step_avg:60.96ms
step:2042/2330 train_time:124489ms step_avg:60.96ms
step:2043/2330 train_time:124549ms step_avg:60.96ms
step:2044/2330 train_time:124611ms step_avg:60.96ms
step:2045/2330 train_time:124670ms step_avg:60.96ms
step:2046/2330 train_time:124733ms step_avg:60.96ms
step:2047/2330 train_time:124792ms step_avg:60.96ms
step:2048/2330 train_time:124855ms step_avg:60.96ms
step:2049/2330 train_time:124914ms step_avg:60.96ms
step:2050/2330 train_time:124978ms step_avg:60.96ms
step:2051/2330 train_time:125039ms step_avg:60.96ms
step:2052/2330 train_time:125102ms step_avg:60.97ms
step:2053/2330 train_time:125162ms step_avg:60.97ms
step:2054/2330 train_time:125225ms step_avg:60.97ms
step:2055/2330 train_time:125286ms step_avg:60.97ms
step:2056/2330 train_time:125350ms step_avg:60.97ms
step:2057/2330 train_time:125409ms step_avg:60.97ms
step:2058/2330 train_time:125472ms step_avg:60.97ms
step:2059/2330 train_time:125533ms step_avg:60.97ms
step:2060/2330 train_time:125595ms step_avg:60.97ms
step:2061/2330 train_time:125655ms step_avg:60.97ms
step:2062/2330 train_time:125717ms step_avg:60.97ms
step:2063/2330 train_time:125777ms step_avg:60.97ms
step:2064/2330 train_time:125839ms step_avg:60.97ms
step:2065/2330 train_time:125900ms step_avg:60.97ms
step:2066/2330 train_time:125962ms step_avg:60.97ms
step:2067/2330 train_time:126022ms step_avg:60.97ms
step:2068/2330 train_time:126085ms step_avg:60.97ms
step:2069/2330 train_time:126147ms step_avg:60.97ms
step:2070/2330 train_time:126209ms step_avg:60.97ms
step:2071/2330 train_time:126269ms step_avg:60.97ms
step:2072/2330 train_time:126333ms step_avg:60.97ms
step:2073/2330 train_time:126393ms step_avg:60.97ms
step:2074/2330 train_time:126455ms step_avg:60.97ms
step:2075/2330 train_time:126515ms step_avg:60.97ms
step:2076/2330 train_time:126576ms step_avg:60.97ms
step:2077/2330 train_time:126636ms step_avg:60.97ms
step:2078/2330 train_time:126699ms step_avg:60.97ms
step:2079/2330 train_time:126758ms step_avg:60.97ms
step:2080/2330 train_time:126821ms step_avg:60.97ms
step:2081/2330 train_time:126881ms step_avg:60.97ms
step:2082/2330 train_time:126944ms step_avg:60.97ms
step:2083/2330 train_time:127004ms step_avg:60.97ms
step:2084/2330 train_time:127067ms step_avg:60.97ms
step:2085/2330 train_time:127128ms step_avg:60.97ms
step:2086/2330 train_time:127191ms step_avg:60.97ms
step:2087/2330 train_time:127251ms step_avg:60.97ms
step:2088/2330 train_time:127315ms step_avg:60.97ms
step:2089/2330 train_time:127375ms step_avg:60.97ms
step:2090/2330 train_time:127438ms step_avg:60.98ms
step:2091/2330 train_time:127497ms step_avg:60.97ms
step:2092/2330 train_time:127560ms step_avg:60.97ms
step:2093/2330 train_time:127619ms step_avg:60.97ms
step:2094/2330 train_time:127683ms step_avg:60.98ms
step:2095/2330 train_time:127742ms step_avg:60.97ms
step:2096/2330 train_time:127805ms step_avg:60.98ms
step:2097/2330 train_time:127864ms step_avg:60.97ms
step:2098/2330 train_time:127927ms step_avg:60.98ms
step:2099/2330 train_time:127987ms step_avg:60.98ms
step:2100/2330 train_time:128050ms step_avg:60.98ms
step:2101/2330 train_time:128110ms step_avg:60.98ms
step:2102/2330 train_time:128173ms step_avg:60.98ms
step:2103/2330 train_time:128234ms step_avg:60.98ms
step:2104/2330 train_time:128297ms step_avg:60.98ms
step:2105/2330 train_time:128357ms step_avg:60.98ms
step:2106/2330 train_time:128420ms step_avg:60.98ms
step:2107/2330 train_time:128479ms step_avg:60.98ms
step:2108/2330 train_time:128544ms step_avg:60.98ms
step:2109/2330 train_time:128603ms step_avg:60.98ms
step:2110/2330 train_time:128666ms step_avg:60.98ms
step:2111/2330 train_time:128726ms step_avg:60.98ms
step:2112/2330 train_time:128789ms step_avg:60.98ms
step:2113/2330 train_time:128847ms step_avg:60.98ms
step:2114/2330 train_time:128910ms step_avg:60.98ms
step:2115/2330 train_time:128969ms step_avg:60.98ms
step:2116/2330 train_time:129033ms step_avg:60.98ms
step:2117/2330 train_time:129093ms step_avg:60.98ms
step:2118/2330 train_time:129156ms step_avg:60.98ms
step:2119/2330 train_time:129216ms step_avg:60.98ms
step:2120/2330 train_time:129279ms step_avg:60.98ms
step:2121/2330 train_time:129339ms step_avg:60.98ms
step:2122/2330 train_time:129402ms step_avg:60.98ms
step:2123/2330 train_time:129463ms step_avg:60.98ms
step:2124/2330 train_time:129525ms step_avg:60.98ms
step:2125/2330 train_time:129584ms step_avg:60.98ms
step:2126/2330 train_time:129647ms step_avg:60.98ms
step:2127/2330 train_time:129707ms step_avg:60.98ms
step:2128/2330 train_time:129770ms step_avg:60.98ms
step:2129/2330 train_time:129830ms step_avg:60.98ms
step:2130/2330 train_time:129893ms step_avg:60.98ms
step:2131/2330 train_time:129953ms step_avg:60.98ms
step:2132/2330 train_time:130016ms step_avg:60.98ms
step:2133/2330 train_time:130076ms step_avg:60.98ms
step:2134/2330 train_time:130139ms step_avg:60.98ms
step:2135/2330 train_time:130198ms step_avg:60.98ms
step:2136/2330 train_time:130262ms step_avg:60.98ms
step:2137/2330 train_time:130321ms step_avg:60.98ms
step:2138/2330 train_time:130385ms step_avg:60.98ms
step:2139/2330 train_time:130446ms step_avg:60.98ms
step:2140/2330 train_time:130509ms step_avg:60.99ms
step:2141/2330 train_time:130568ms step_avg:60.98ms
step:2142/2330 train_time:130632ms step_avg:60.99ms
step:2143/2330 train_time:130692ms step_avg:60.99ms
step:2144/2330 train_time:130754ms step_avg:60.99ms
step:2145/2330 train_time:130814ms step_avg:60.99ms
step:2146/2330 train_time:130876ms step_avg:60.99ms
step:2147/2330 train_time:130936ms step_avg:60.99ms
step:2148/2330 train_time:130999ms step_avg:60.99ms
step:2149/2330 train_time:131058ms step_avg:60.99ms
step:2150/2330 train_time:131121ms step_avg:60.99ms
step:2151/2330 train_time:131181ms step_avg:60.99ms
step:2152/2330 train_time:131245ms step_avg:60.99ms
step:2153/2330 train_time:131306ms step_avg:60.99ms
step:2154/2330 train_time:131368ms step_avg:60.99ms
step:2155/2330 train_time:131427ms step_avg:60.99ms
step:2156/2330 train_time:131490ms step_avg:60.99ms
step:2157/2330 train_time:131551ms step_avg:60.99ms
step:2158/2330 train_time:131614ms step_avg:60.99ms
step:2159/2330 train_time:131674ms step_avg:60.99ms
step:2160/2330 train_time:131737ms step_avg:60.99ms
step:2161/2330 train_time:131797ms step_avg:60.99ms
step:2162/2330 train_time:131859ms step_avg:60.99ms
step:2163/2330 train_time:131919ms step_avg:60.99ms
step:2164/2330 train_time:131982ms step_avg:60.99ms
step:2165/2330 train_time:132042ms step_avg:60.99ms
step:2166/2330 train_time:132105ms step_avg:60.99ms
step:2167/2330 train_time:132164ms step_avg:60.99ms
step:2168/2330 train_time:132227ms step_avg:60.99ms
step:2169/2330 train_time:132287ms step_avg:60.99ms
step:2170/2330 train_time:132350ms step_avg:60.99ms
step:2171/2330 train_time:132411ms step_avg:60.99ms
step:2172/2330 train_time:132474ms step_avg:60.99ms
step:2173/2330 train_time:132534ms step_avg:60.99ms
step:2174/2330 train_time:132596ms step_avg:60.99ms
step:2175/2330 train_time:132655ms step_avg:60.99ms
step:2176/2330 train_time:132719ms step_avg:60.99ms
step:2177/2330 train_time:132778ms step_avg:60.99ms
step:2178/2330 train_time:132841ms step_avg:60.99ms
step:2179/2330 train_time:132901ms step_avg:60.99ms
step:2180/2330 train_time:132964ms step_avg:60.99ms
step:2181/2330 train_time:133023ms step_avg:60.99ms
step:2182/2330 train_time:133086ms step_avg:60.99ms
step:2183/2330 train_time:133147ms step_avg:60.99ms
step:2184/2330 train_time:133211ms step_avg:60.99ms
step:2185/2330 train_time:133271ms step_avg:60.99ms
step:2186/2330 train_time:133334ms step_avg:60.99ms
step:2187/2330 train_time:133393ms step_avg:60.99ms
step:2188/2330 train_time:133456ms step_avg:60.99ms
step:2189/2330 train_time:133515ms step_avg:60.99ms
step:2190/2330 train_time:133578ms step_avg:60.99ms
step:2191/2330 train_time:133639ms step_avg:60.99ms
step:2192/2330 train_time:133701ms step_avg:61.00ms
step:2193/2330 train_time:133762ms step_avg:60.99ms
step:2194/2330 train_time:133825ms step_avg:61.00ms
step:2195/2330 train_time:133884ms step_avg:60.99ms
step:2196/2330 train_time:133947ms step_avg:61.00ms
step:2197/2330 train_time:134007ms step_avg:61.00ms
step:2198/2330 train_time:134070ms step_avg:61.00ms
step:2199/2330 train_time:134131ms step_avg:61.00ms
step:2200/2330 train_time:134193ms step_avg:61.00ms
step:2201/2330 train_time:134253ms step_avg:61.00ms
step:2202/2330 train_time:134315ms step_avg:61.00ms
step:2203/2330 train_time:134376ms step_avg:61.00ms
step:2204/2330 train_time:134440ms step_avg:61.00ms
step:2205/2330 train_time:134499ms step_avg:61.00ms
step:2206/2330 train_time:134563ms step_avg:61.00ms
step:2207/2330 train_time:134622ms step_avg:61.00ms
step:2208/2330 train_time:134685ms step_avg:61.00ms
step:2209/2330 train_time:134746ms step_avg:61.00ms
step:2210/2330 train_time:134809ms step_avg:61.00ms
step:2211/2330 train_time:134869ms step_avg:61.00ms
step:2212/2330 train_time:134932ms step_avg:61.00ms
step:2213/2330 train_time:134992ms step_avg:61.00ms
step:2214/2330 train_time:135055ms step_avg:61.00ms
step:2215/2330 train_time:135114ms step_avg:61.00ms
step:2216/2330 train_time:135177ms step_avg:61.00ms
step:2217/2330 train_time:135237ms step_avg:61.00ms
step:2218/2330 train_time:135299ms step_avg:61.00ms
step:2219/2330 train_time:135359ms step_avg:61.00ms
step:2220/2330 train_time:135422ms step_avg:61.00ms
step:2221/2330 train_time:135481ms step_avg:61.00ms
step:2222/2330 train_time:135545ms step_avg:61.00ms
step:2223/2330 train_time:135604ms step_avg:61.00ms
step:2224/2330 train_time:135667ms step_avg:61.00ms
step:2225/2330 train_time:135727ms step_avg:61.00ms
step:2226/2330 train_time:135789ms step_avg:61.00ms
step:2227/2330 train_time:135849ms step_avg:61.00ms
step:2228/2330 train_time:135912ms step_avg:61.00ms
step:2229/2330 train_time:135971ms step_avg:61.00ms
step:2230/2330 train_time:136035ms step_avg:61.00ms
step:2231/2330 train_time:136095ms step_avg:61.00ms
step:2232/2330 train_time:136157ms step_avg:61.00ms
step:2233/2330 train_time:136218ms step_avg:61.00ms
step:2234/2330 train_time:136280ms step_avg:61.00ms
step:2235/2330 train_time:136340ms step_avg:61.00ms
step:2236/2330 train_time:136403ms step_avg:61.00ms
step:2237/2330 train_time:136463ms step_avg:61.00ms
step:2238/2330 train_time:136526ms step_avg:61.00ms
step:2239/2330 train_time:136585ms step_avg:61.00ms
step:2240/2330 train_time:136648ms step_avg:61.00ms
step:2241/2330 train_time:136708ms step_avg:61.00ms
step:2242/2330 train_time:136770ms step_avg:61.00ms
step:2243/2330 train_time:136830ms step_avg:61.00ms
step:2244/2330 train_time:136894ms step_avg:61.00ms
step:2245/2330 train_time:136953ms step_avg:61.00ms
step:2246/2330 train_time:137016ms step_avg:61.00ms
step:2247/2330 train_time:137076ms step_avg:61.00ms
step:2248/2330 train_time:137139ms step_avg:61.00ms
step:2249/2330 train_time:137198ms step_avg:61.00ms
step:2250/2330 train_time:137261ms step_avg:61.01ms
step:2250/2330 val_loss:3.4490 train_time:137335ms step_avg:61.04ms
step:2251/2330 train_time:137356ms step_avg:61.02ms
step:2252/2330 train_time:137388ms step_avg:61.01ms
step:2253/2330 train_time:137449ms step_avg:61.01ms
step:2254/2330 train_time:137515ms step_avg:61.01ms
step:2255/2330 train_time:137577ms step_avg:61.01ms
step:2256/2330 train_time:137640ms step_avg:61.01ms
step:2257/2330 train_time:137699ms step_avg:61.01ms
step:2258/2330 train_time:137761ms step_avg:61.01ms
step:2259/2330 train_time:137821ms step_avg:61.01ms
step:2260/2330 train_time:137883ms step_avg:61.01ms
step:2261/2330 train_time:137942ms step_avg:61.01ms
step:2262/2330 train_time:138004ms step_avg:61.01ms
step:2263/2330 train_time:138063ms step_avg:61.01ms
step:2264/2330 train_time:138125ms step_avg:61.01ms
step:2265/2330 train_time:138185ms step_avg:61.01ms
step:2266/2330 train_time:138247ms step_avg:61.01ms
step:2267/2330 train_time:138308ms step_avg:61.01ms
step:2268/2330 train_time:138372ms step_avg:61.01ms
step:2269/2330 train_time:138434ms step_avg:61.01ms
step:2270/2330 train_time:138499ms step_avg:61.01ms
step:2271/2330 train_time:138560ms step_avg:61.01ms
step:2272/2330 train_time:138624ms step_avg:61.01ms
step:2273/2330 train_time:138682ms step_avg:61.01ms
step:2274/2330 train_time:138746ms step_avg:61.01ms
step:2275/2330 train_time:138804ms step_avg:61.01ms
step:2276/2330 train_time:138868ms step_avg:61.01ms
step:2277/2330 train_time:138927ms step_avg:61.01ms
step:2278/2330 train_time:138989ms step_avg:61.01ms
step:2279/2330 train_time:139048ms step_avg:61.01ms
step:2280/2330 train_time:139111ms step_avg:61.01ms
step:2281/2330 train_time:139171ms step_avg:61.01ms
step:2282/2330 train_time:139234ms step_avg:61.01ms
step:2283/2330 train_time:139294ms step_avg:61.01ms
step:2284/2330 train_time:139358ms step_avg:61.01ms
step:2285/2330 train_time:139417ms step_avg:61.01ms
step:2286/2330 train_time:139481ms step_avg:61.02ms
step:2287/2330 train_time:139541ms step_avg:61.01ms
step:2288/2330 train_time:139604ms step_avg:61.02ms
step:2289/2330 train_time:139664ms step_avg:61.02ms
step:2290/2330 train_time:139727ms step_avg:61.02ms
step:2291/2330 train_time:139787ms step_avg:61.02ms
step:2292/2330 train_time:139849ms step_avg:61.02ms
step:2293/2330 train_time:139909ms step_avg:61.02ms
step:2294/2330 train_time:139972ms step_avg:61.02ms
step:2295/2330 train_time:140031ms step_avg:61.02ms
step:2296/2330 train_time:140094ms step_avg:61.02ms
step:2297/2330 train_time:140154ms step_avg:61.02ms
step:2298/2330 train_time:140216ms step_avg:61.02ms
step:2299/2330 train_time:140276ms step_avg:61.02ms
step:2300/2330 train_time:140338ms step_avg:61.02ms
step:2301/2330 train_time:140399ms step_avg:61.02ms
step:2302/2330 train_time:140462ms step_avg:61.02ms
step:2303/2330 train_time:140522ms step_avg:61.02ms
step:2304/2330 train_time:140585ms step_avg:61.02ms
step:2305/2330 train_time:140644ms step_avg:61.02ms
step:2306/2330 train_time:140708ms step_avg:61.02ms
step:2307/2330 train_time:140768ms step_avg:61.02ms
step:2308/2330 train_time:140832ms step_avg:61.02ms
step:2309/2330 train_time:140891ms step_avg:61.02ms
step:2310/2330 train_time:140953ms step_avg:61.02ms
step:2311/2330 train_time:141013ms step_avg:61.02ms
step:2312/2330 train_time:141075ms step_avg:61.02ms
step:2313/2330 train_time:141135ms step_avg:61.02ms
step:2314/2330 train_time:141197ms step_avg:61.02ms
step:2315/2330 train_time:141257ms step_avg:61.02ms
step:2316/2330 train_time:141320ms step_avg:61.02ms
step:2317/2330 train_time:141380ms step_avg:61.02ms
step:2318/2330 train_time:141443ms step_avg:61.02ms
step:2319/2330 train_time:141503ms step_avg:61.02ms
step:2320/2330 train_time:141567ms step_avg:61.02ms
step:2321/2330 train_time:141627ms step_avg:61.02ms
step:2322/2330 train_time:141690ms step_avg:61.02ms
step:2323/2330 train_time:141751ms step_avg:61.02ms
step:2324/2330 train_time:141814ms step_avg:61.02ms
step:2325/2330 train_time:141874ms step_avg:61.02ms
step:2326/2330 train_time:141937ms step_avg:61.02ms
step:2327/2330 train_time:141997ms step_avg:61.02ms
step:2328/2330 train_time:142059ms step_avg:61.02ms
step:2329/2330 train_time:142119ms step_avg:61.02ms
step:2330/2330 train_time:142181ms step_avg:61.02ms
step:2330/2330 val_loss:3.4349 train_time:142254ms step_avg:61.05ms
peak memory allocated: 29226 MiB reserved: 44076 MiB
