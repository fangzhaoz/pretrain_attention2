import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "muon_lr1e-3"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-3, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Oct 28 22:29:19 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   34C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   32C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:74ms step_avg:74.31ms
step:2/2330 train_time:175ms step_avg:87.26ms
step:3/2330 train_time:198ms step_avg:65.87ms
step:4/2330 train_time:232ms step_avg:57.90ms
step:5/2330 train_time:289ms step_avg:57.77ms
step:6/2330 train_time:349ms step_avg:58.21ms
step:7/2330 train_time:407ms step_avg:58.20ms
step:8/2330 train_time:468ms step_avg:58.51ms
step:9/2330 train_time:527ms step_avg:58.57ms
step:10/2330 train_time:589ms step_avg:58.89ms
step:11/2330 train_time:648ms step_avg:58.91ms
step:12/2330 train_time:709ms step_avg:59.10ms
step:13/2330 train_time:769ms step_avg:59.12ms
step:14/2330 train_time:829ms step_avg:59.25ms
step:15/2330 train_time:889ms step_avg:59.24ms
step:16/2330 train_time:950ms step_avg:59.37ms
step:17/2330 train_time:1012ms step_avg:59.55ms
step:18/2330 train_time:1078ms step_avg:59.90ms
step:19/2330 train_time:1141ms step_avg:60.03ms
step:20/2330 train_time:1204ms step_avg:60.19ms
step:21/2330 train_time:1264ms step_avg:60.18ms
step:22/2330 train_time:1326ms step_avg:60.25ms
step:23/2330 train_time:1385ms step_avg:60.20ms
step:24/2330 train_time:1446ms step_avg:60.24ms
step:25/2330 train_time:1505ms step_avg:60.21ms
step:26/2330 train_time:1566ms step_avg:60.24ms
step:27/2330 train_time:1625ms step_avg:60.20ms
step:28/2330 train_time:1688ms step_avg:60.28ms
step:29/2330 train_time:1747ms step_avg:60.25ms
step:30/2330 train_time:1809ms step_avg:60.29ms
step:31/2330 train_time:1867ms step_avg:60.23ms
step:32/2330 train_time:1929ms step_avg:60.28ms
step:33/2330 train_time:1990ms step_avg:60.30ms
step:34/2330 train_time:2053ms step_avg:60.39ms
step:35/2330 train_time:2116ms step_avg:60.45ms
step:36/2330 train_time:2180ms step_avg:60.54ms
step:37/2330 train_time:2240ms step_avg:60.53ms
step:38/2330 train_time:2302ms step_avg:60.58ms
step:39/2330 train_time:2362ms step_avg:60.55ms
step:40/2330 train_time:2423ms step_avg:60.58ms
step:41/2330 train_time:2483ms step_avg:60.55ms
step:42/2330 train_time:2544ms step_avg:60.57ms
step:43/2330 train_time:2603ms step_avg:60.55ms
step:44/2330 train_time:2665ms step_avg:60.57ms
step:45/2330 train_time:2725ms step_avg:60.55ms
step:46/2330 train_time:2787ms step_avg:60.58ms
step:47/2330 train_time:2846ms step_avg:60.55ms
step:48/2330 train_time:2907ms step_avg:60.57ms
step:49/2330 train_time:2968ms step_avg:60.56ms
step:50/2330 train_time:3030ms step_avg:60.60ms
step:51/2330 train_time:3091ms step_avg:60.61ms
step:52/2330 train_time:3154ms step_avg:60.65ms
step:53/2330 train_time:3215ms step_avg:60.66ms
step:54/2330 train_time:3278ms step_avg:60.70ms
step:55/2330 train_time:3337ms step_avg:60.67ms
step:56/2330 train_time:3398ms step_avg:60.68ms
step:57/2330 train_time:3458ms step_avg:60.66ms
step:58/2330 train_time:3520ms step_avg:60.68ms
step:59/2330 train_time:3579ms step_avg:60.67ms
step:60/2330 train_time:3641ms step_avg:60.68ms
step:61/2330 train_time:3700ms step_avg:60.65ms
step:62/2330 train_time:3761ms step_avg:60.67ms
step:63/2330 train_time:3821ms step_avg:60.65ms
step:64/2330 train_time:3884ms step_avg:60.68ms
step:65/2330 train_time:3944ms step_avg:60.67ms
step:66/2330 train_time:4006ms step_avg:60.70ms
step:67/2330 train_time:4067ms step_avg:60.70ms
step:68/2330 train_time:4130ms step_avg:60.74ms
step:69/2330 train_time:4191ms step_avg:60.74ms
step:70/2330 train_time:4253ms step_avg:60.75ms
step:71/2330 train_time:4313ms step_avg:60.74ms
step:72/2330 train_time:4375ms step_avg:60.76ms
step:73/2330 train_time:4435ms step_avg:60.76ms
step:74/2330 train_time:4497ms step_avg:60.77ms
step:75/2330 train_time:4556ms step_avg:60.75ms
step:76/2330 train_time:4617ms step_avg:60.75ms
step:77/2330 train_time:4677ms step_avg:60.74ms
step:78/2330 train_time:4738ms step_avg:60.74ms
step:79/2330 train_time:4797ms step_avg:60.72ms
step:80/2330 train_time:4858ms step_avg:60.73ms
step:81/2330 train_time:4917ms step_avg:60.71ms
step:82/2330 train_time:4980ms step_avg:60.73ms
step:83/2330 train_time:5040ms step_avg:60.72ms
step:84/2330 train_time:5102ms step_avg:60.74ms
step:85/2330 train_time:5163ms step_avg:60.74ms
step:86/2330 train_time:5225ms step_avg:60.76ms
step:87/2330 train_time:5286ms step_avg:60.76ms
step:88/2330 train_time:5349ms step_avg:60.78ms
step:89/2330 train_time:5408ms step_avg:60.77ms
step:90/2330 train_time:5470ms step_avg:60.78ms
step:91/2330 train_time:5530ms step_avg:60.77ms
step:92/2330 train_time:5592ms step_avg:60.78ms
step:93/2330 train_time:5652ms step_avg:60.77ms
step:94/2330 train_time:5714ms step_avg:60.78ms
step:95/2330 train_time:5774ms step_avg:60.78ms
step:96/2330 train_time:5836ms step_avg:60.79ms
step:97/2330 train_time:5897ms step_avg:60.79ms
step:98/2330 train_time:5958ms step_avg:60.80ms
step:99/2330 train_time:6018ms step_avg:60.78ms
step:100/2330 train_time:6079ms step_avg:60.79ms
step:101/2330 train_time:6139ms step_avg:60.78ms
step:102/2330 train_time:6201ms step_avg:60.80ms
step:103/2330 train_time:6260ms step_avg:60.78ms
step:104/2330 train_time:6323ms step_avg:60.79ms
step:105/2330 train_time:6382ms step_avg:60.79ms
step:106/2330 train_time:6444ms step_avg:60.80ms
step:107/2330 train_time:6505ms step_avg:60.79ms
step:108/2330 train_time:6567ms step_avg:60.80ms
step:109/2330 train_time:6626ms step_avg:60.79ms
step:110/2330 train_time:6689ms step_avg:60.81ms
step:111/2330 train_time:6749ms step_avg:60.80ms
step:112/2330 train_time:6810ms step_avg:60.81ms
step:113/2330 train_time:6871ms step_avg:60.80ms
step:114/2330 train_time:6932ms step_avg:60.81ms
step:115/2330 train_time:6992ms step_avg:60.80ms
step:116/2330 train_time:7055ms step_avg:60.82ms
step:117/2330 train_time:7115ms step_avg:60.81ms
step:118/2330 train_time:7177ms step_avg:60.82ms
step:119/2330 train_time:7236ms step_avg:60.81ms
step:120/2330 train_time:7298ms step_avg:60.81ms
step:121/2330 train_time:7357ms step_avg:60.80ms
step:122/2330 train_time:7418ms step_avg:60.81ms
step:123/2330 train_time:7478ms step_avg:60.79ms
step:124/2330 train_time:7539ms step_avg:60.80ms
step:125/2330 train_time:7599ms step_avg:60.80ms
step:126/2330 train_time:7662ms step_avg:60.81ms
step:127/2330 train_time:7722ms step_avg:60.80ms
step:128/2330 train_time:7783ms step_avg:60.81ms
step:129/2330 train_time:7844ms step_avg:60.81ms
step:130/2330 train_time:7906ms step_avg:60.82ms
step:131/2330 train_time:7966ms step_avg:60.81ms
step:132/2330 train_time:8029ms step_avg:60.82ms
step:133/2330 train_time:8089ms step_avg:60.82ms
step:134/2330 train_time:8151ms step_avg:60.83ms
step:135/2330 train_time:8211ms step_avg:60.82ms
step:136/2330 train_time:8274ms step_avg:60.84ms
step:137/2330 train_time:8334ms step_avg:60.83ms
step:138/2330 train_time:8396ms step_avg:60.84ms
step:139/2330 train_time:8456ms step_avg:60.83ms
step:140/2330 train_time:8518ms step_avg:60.84ms
step:141/2330 train_time:8578ms step_avg:60.84ms
step:142/2330 train_time:8639ms step_avg:60.84ms
step:143/2330 train_time:8699ms step_avg:60.84ms
step:144/2330 train_time:8762ms step_avg:60.84ms
step:145/2330 train_time:8821ms step_avg:60.84ms
step:146/2330 train_time:8884ms step_avg:60.85ms
step:147/2330 train_time:8944ms step_avg:60.84ms
step:148/2330 train_time:9007ms step_avg:60.85ms
step:149/2330 train_time:9066ms step_avg:60.85ms
step:150/2330 train_time:9129ms step_avg:60.86ms
step:151/2330 train_time:9190ms step_avg:60.86ms
step:152/2330 train_time:9252ms step_avg:60.87ms
step:153/2330 train_time:9312ms step_avg:60.86ms
step:154/2330 train_time:9374ms step_avg:60.87ms
step:155/2330 train_time:9434ms step_avg:60.86ms
step:156/2330 train_time:9496ms step_avg:60.87ms
step:157/2330 train_time:9556ms step_avg:60.86ms
step:158/2330 train_time:9618ms step_avg:60.87ms
step:159/2330 train_time:9678ms step_avg:60.87ms
step:160/2330 train_time:9739ms step_avg:60.87ms
step:161/2330 train_time:9799ms step_avg:60.87ms
step:162/2330 train_time:9861ms step_avg:60.87ms
step:163/2330 train_time:9920ms step_avg:60.86ms
step:164/2330 train_time:9982ms step_avg:60.87ms
step:165/2330 train_time:10042ms step_avg:60.86ms
step:166/2330 train_time:10104ms step_avg:60.87ms
step:167/2330 train_time:10165ms step_avg:60.87ms
step:168/2330 train_time:10227ms step_avg:60.88ms
step:169/2330 train_time:10288ms step_avg:60.88ms
step:170/2330 train_time:10350ms step_avg:60.88ms
step:171/2330 train_time:10410ms step_avg:60.88ms
step:172/2330 train_time:10472ms step_avg:60.89ms
step:173/2330 train_time:10533ms step_avg:60.88ms
step:174/2330 train_time:10595ms step_avg:60.89ms
step:175/2330 train_time:10655ms step_avg:60.88ms
step:176/2330 train_time:10717ms step_avg:60.89ms
step:177/2330 train_time:10776ms step_avg:60.88ms
step:178/2330 train_time:10837ms step_avg:60.88ms
step:179/2330 train_time:10898ms step_avg:60.88ms
step:180/2330 train_time:10959ms step_avg:60.88ms
step:181/2330 train_time:11019ms step_avg:60.88ms
step:182/2330 train_time:11080ms step_avg:60.88ms
step:183/2330 train_time:11140ms step_avg:60.87ms
step:184/2330 train_time:11202ms step_avg:60.88ms
step:185/2330 train_time:11262ms step_avg:60.88ms
step:186/2330 train_time:11325ms step_avg:60.89ms
step:187/2330 train_time:11385ms step_avg:60.88ms
step:188/2330 train_time:11447ms step_avg:60.89ms
step:189/2330 train_time:11508ms step_avg:60.89ms
step:190/2330 train_time:11570ms step_avg:60.89ms
step:191/2330 train_time:11630ms step_avg:60.89ms
step:192/2330 train_time:11692ms step_avg:60.90ms
step:193/2330 train_time:11752ms step_avg:60.89ms
step:194/2330 train_time:11815ms step_avg:60.90ms
step:195/2330 train_time:11876ms step_avg:60.90ms
step:196/2330 train_time:11938ms step_avg:60.91ms
step:197/2330 train_time:11997ms step_avg:60.90ms
step:198/2330 train_time:12059ms step_avg:60.90ms
step:199/2330 train_time:12119ms step_avg:60.90ms
step:200/2330 train_time:12180ms step_avg:60.90ms
step:201/2330 train_time:12240ms step_avg:60.90ms
step:202/2330 train_time:12302ms step_avg:60.90ms
step:203/2330 train_time:12362ms step_avg:60.90ms
step:204/2330 train_time:12424ms step_avg:60.90ms
step:205/2330 train_time:12485ms step_avg:60.90ms
step:206/2330 train_time:12547ms step_avg:60.91ms
step:207/2330 train_time:12608ms step_avg:60.91ms
step:208/2330 train_time:12670ms step_avg:60.92ms
step:209/2330 train_time:12730ms step_avg:60.91ms
step:210/2330 train_time:12792ms step_avg:60.92ms
step:211/2330 train_time:12853ms step_avg:60.91ms
step:212/2330 train_time:12915ms step_avg:60.92ms
step:213/2330 train_time:12976ms step_avg:60.92ms
step:214/2330 train_time:13038ms step_avg:60.92ms
step:215/2330 train_time:13098ms step_avg:60.92ms
step:216/2330 train_time:13161ms step_avg:60.93ms
step:217/2330 train_time:13220ms step_avg:60.92ms
step:218/2330 train_time:13282ms step_avg:60.93ms
step:219/2330 train_time:13342ms step_avg:60.92ms
step:220/2330 train_time:13404ms step_avg:60.93ms
step:221/2330 train_time:13464ms step_avg:60.92ms
step:222/2330 train_time:13527ms step_avg:60.93ms
step:223/2330 train_time:13587ms step_avg:60.93ms
step:224/2330 train_time:13649ms step_avg:60.93ms
step:225/2330 train_time:13710ms step_avg:60.93ms
step:226/2330 train_time:13772ms step_avg:60.94ms
step:227/2330 train_time:13832ms step_avg:60.93ms
step:228/2330 train_time:13894ms step_avg:60.94ms
step:229/2330 train_time:13954ms step_avg:60.94ms
step:230/2330 train_time:14016ms step_avg:60.94ms
step:231/2330 train_time:14076ms step_avg:60.93ms
step:232/2330 train_time:14138ms step_avg:60.94ms
step:233/2330 train_time:14198ms step_avg:60.94ms
step:234/2330 train_time:14260ms step_avg:60.94ms
step:235/2330 train_time:14319ms step_avg:60.93ms
step:236/2330 train_time:14381ms step_avg:60.94ms
step:237/2330 train_time:14442ms step_avg:60.94ms
step:238/2330 train_time:14504ms step_avg:60.94ms
step:239/2330 train_time:14565ms step_avg:60.94ms
step:240/2330 train_time:14627ms step_avg:60.95ms
step:241/2330 train_time:14687ms step_avg:60.94ms
step:242/2330 train_time:14750ms step_avg:60.95ms
step:243/2330 train_time:14810ms step_avg:60.95ms
step:244/2330 train_time:14872ms step_avg:60.95ms
step:245/2330 train_time:14932ms step_avg:60.95ms
step:246/2330 train_time:14994ms step_avg:60.95ms
step:247/2330 train_time:15054ms step_avg:60.95ms
step:248/2330 train_time:15116ms step_avg:60.95ms
step:249/2330 train_time:15176ms step_avg:60.95ms
step:250/2330 train_time:15238ms step_avg:60.95ms
step:250/2330 val_loss:4.6502 train_time:15302ms step_avg:61.21ms
step:251/2330 train_time:15325ms step_avg:61.05ms
step:252/2330 train_time:15362ms step_avg:60.96ms
step:253/2330 train_time:15427ms step_avg:60.98ms
step:254/2330 train_time:15492ms step_avg:60.99ms
step:255/2330 train_time:15553ms step_avg:60.99ms
step:256/2330 train_time:15615ms step_avg:60.99ms
step:257/2330 train_time:15674ms step_avg:60.99ms
step:258/2330 train_time:15735ms step_avg:60.99ms
step:259/2330 train_time:15795ms step_avg:60.98ms
step:260/2330 train_time:15856ms step_avg:60.98ms
step:261/2330 train_time:15914ms step_avg:60.97ms
step:262/2330 train_time:15976ms step_avg:60.98ms
step:263/2330 train_time:16034ms step_avg:60.97ms
step:264/2330 train_time:16096ms step_avg:60.97ms
step:265/2330 train_time:16154ms step_avg:60.96ms
step:266/2330 train_time:16216ms step_avg:60.96ms
step:267/2330 train_time:16275ms step_avg:60.96ms
step:268/2330 train_time:16338ms step_avg:60.96ms
step:269/2330 train_time:16400ms step_avg:60.96ms
step:270/2330 train_time:16463ms step_avg:60.97ms
step:271/2330 train_time:16524ms step_avg:60.97ms
step:272/2330 train_time:16587ms step_avg:60.98ms
step:273/2330 train_time:16647ms step_avg:60.98ms
step:274/2330 train_time:16709ms step_avg:60.98ms
step:275/2330 train_time:16768ms step_avg:60.98ms
step:276/2330 train_time:16830ms step_avg:60.98ms
step:277/2330 train_time:16890ms step_avg:60.97ms
step:278/2330 train_time:16952ms step_avg:60.98ms
step:279/2330 train_time:17012ms step_avg:60.98ms
step:280/2330 train_time:17074ms step_avg:60.98ms
step:281/2330 train_time:17133ms step_avg:60.97ms
step:282/2330 train_time:17194ms step_avg:60.97ms
step:283/2330 train_time:17254ms step_avg:60.97ms
step:284/2330 train_time:17315ms step_avg:60.97ms
step:285/2330 train_time:17374ms step_avg:60.96ms
step:286/2330 train_time:17436ms step_avg:60.97ms
step:287/2330 train_time:17496ms step_avg:60.96ms
step:288/2330 train_time:17558ms step_avg:60.97ms
step:289/2330 train_time:17618ms step_avg:60.96ms
step:290/2330 train_time:17681ms step_avg:60.97ms
step:291/2330 train_time:17741ms step_avg:60.97ms
step:292/2330 train_time:17803ms step_avg:60.97ms
step:293/2330 train_time:17864ms step_avg:60.97ms
step:294/2330 train_time:17926ms step_avg:60.97ms
step:295/2330 train_time:17986ms step_avg:60.97ms
step:296/2330 train_time:18048ms step_avg:60.97ms
step:297/2330 train_time:18108ms step_avg:60.97ms
step:298/2330 train_time:18170ms step_avg:60.97ms
step:299/2330 train_time:18230ms step_avg:60.97ms
step:300/2330 train_time:18292ms step_avg:60.97ms
step:301/2330 train_time:18353ms step_avg:60.97ms
step:302/2330 train_time:18414ms step_avg:60.97ms
step:303/2330 train_time:18474ms step_avg:60.97ms
step:304/2330 train_time:18536ms step_avg:60.97ms
step:305/2330 train_time:18595ms step_avg:60.97ms
step:306/2330 train_time:18656ms step_avg:60.97ms
step:307/2330 train_time:18716ms step_avg:60.96ms
step:308/2330 train_time:18779ms step_avg:60.97ms
step:309/2330 train_time:18839ms step_avg:60.97ms
step:310/2330 train_time:18901ms step_avg:60.97ms
step:311/2330 train_time:18962ms step_avg:60.97ms
step:312/2330 train_time:19025ms step_avg:60.98ms
step:313/2330 train_time:19085ms step_avg:60.97ms
step:314/2330 train_time:19147ms step_avg:60.98ms
step:315/2330 train_time:19206ms step_avg:60.97ms
step:316/2330 train_time:19268ms step_avg:60.98ms
step:317/2330 train_time:19329ms step_avg:60.97ms
step:318/2330 train_time:19391ms step_avg:60.98ms
step:319/2330 train_time:19452ms step_avg:60.98ms
step:320/2330 train_time:19513ms step_avg:60.98ms
step:321/2330 train_time:19573ms step_avg:60.98ms
step:322/2330 train_time:19635ms step_avg:60.98ms
step:323/2330 train_time:19695ms step_avg:60.97ms
step:324/2330 train_time:19758ms step_avg:60.98ms
step:325/2330 train_time:19818ms step_avg:60.98ms
step:326/2330 train_time:19880ms step_avg:60.98ms
step:327/2330 train_time:19940ms step_avg:60.98ms
step:328/2330 train_time:20001ms step_avg:60.98ms
step:329/2330 train_time:20061ms step_avg:60.98ms
step:330/2330 train_time:20123ms step_avg:60.98ms
step:331/2330 train_time:20183ms step_avg:60.98ms
step:332/2330 train_time:20245ms step_avg:60.98ms
step:333/2330 train_time:20306ms step_avg:60.98ms
step:334/2330 train_time:20368ms step_avg:60.98ms
step:335/2330 train_time:20428ms step_avg:60.98ms
step:336/2330 train_time:20490ms step_avg:60.98ms
step:337/2330 train_time:20551ms step_avg:60.98ms
step:338/2330 train_time:20613ms step_avg:60.98ms
step:339/2330 train_time:20673ms step_avg:60.98ms
step:340/2330 train_time:20735ms step_avg:60.98ms
step:341/2330 train_time:20795ms step_avg:60.98ms
step:342/2330 train_time:20857ms step_avg:60.98ms
step:343/2330 train_time:20916ms step_avg:60.98ms
step:344/2330 train_time:20978ms step_avg:60.98ms
step:345/2330 train_time:21038ms step_avg:60.98ms
step:346/2330 train_time:21100ms step_avg:60.98ms
step:347/2330 train_time:21160ms step_avg:60.98ms
step:348/2330 train_time:21222ms step_avg:60.98ms
step:349/2330 train_time:21282ms step_avg:60.98ms
step:350/2330 train_time:21345ms step_avg:60.99ms
step:351/2330 train_time:21406ms step_avg:60.98ms
step:352/2330 train_time:21468ms step_avg:60.99ms
step:353/2330 train_time:21528ms step_avg:60.99ms
step:354/2330 train_time:21590ms step_avg:60.99ms
step:355/2330 train_time:21651ms step_avg:60.99ms
step:356/2330 train_time:21714ms step_avg:60.99ms
step:357/2330 train_time:21774ms step_avg:60.99ms
step:358/2330 train_time:21836ms step_avg:60.99ms
step:359/2330 train_time:21895ms step_avg:60.99ms
step:360/2330 train_time:21957ms step_avg:60.99ms
step:361/2330 train_time:22017ms step_avg:60.99ms
step:362/2330 train_time:22079ms step_avg:60.99ms
step:363/2330 train_time:22139ms step_avg:60.99ms
step:364/2330 train_time:22201ms step_avg:60.99ms
step:365/2330 train_time:22262ms step_avg:60.99ms
step:366/2330 train_time:22324ms step_avg:60.99ms
step:367/2330 train_time:22384ms step_avg:60.99ms
step:368/2330 train_time:22446ms step_avg:61.00ms
step:369/2330 train_time:22506ms step_avg:60.99ms
step:370/2330 train_time:22569ms step_avg:61.00ms
step:371/2330 train_time:22628ms step_avg:60.99ms
step:372/2330 train_time:22691ms step_avg:61.00ms
step:373/2330 train_time:22752ms step_avg:61.00ms
step:374/2330 train_time:22815ms step_avg:61.00ms
step:375/2330 train_time:22875ms step_avg:61.00ms
step:376/2330 train_time:22936ms step_avg:61.00ms
step:377/2330 train_time:22996ms step_avg:61.00ms
step:378/2330 train_time:23058ms step_avg:61.00ms
step:379/2330 train_time:23117ms step_avg:61.00ms
step:380/2330 train_time:23179ms step_avg:61.00ms
step:381/2330 train_time:23239ms step_avg:60.99ms
step:382/2330 train_time:23301ms step_avg:61.00ms
step:383/2330 train_time:23361ms step_avg:61.00ms
step:384/2330 train_time:23424ms step_avg:61.00ms
step:385/2330 train_time:23485ms step_avg:61.00ms
step:386/2330 train_time:23548ms step_avg:61.01ms
step:387/2330 train_time:23609ms step_avg:61.00ms
step:388/2330 train_time:23672ms step_avg:61.01ms
step:389/2330 train_time:23732ms step_avg:61.01ms
step:390/2330 train_time:23794ms step_avg:61.01ms
step:391/2330 train_time:23855ms step_avg:61.01ms
step:392/2330 train_time:23916ms step_avg:61.01ms
step:393/2330 train_time:23975ms step_avg:61.01ms
step:394/2330 train_time:24038ms step_avg:61.01ms
step:395/2330 train_time:24098ms step_avg:61.01ms
step:396/2330 train_time:24160ms step_avg:61.01ms
step:397/2330 train_time:24219ms step_avg:61.01ms
step:398/2330 train_time:24281ms step_avg:61.01ms
step:399/2330 train_time:24340ms step_avg:61.00ms
step:400/2330 train_time:24403ms step_avg:61.01ms
step:401/2330 train_time:24463ms step_avg:61.01ms
step:402/2330 train_time:24526ms step_avg:61.01ms
step:403/2330 train_time:24587ms step_avg:61.01ms
step:404/2330 train_time:24650ms step_avg:61.01ms
step:405/2330 train_time:24710ms step_avg:61.01ms
step:406/2330 train_time:24773ms step_avg:61.02ms
step:407/2330 train_time:24832ms step_avg:61.01ms
step:408/2330 train_time:24894ms step_avg:61.01ms
step:409/2330 train_time:24953ms step_avg:61.01ms
step:410/2330 train_time:25015ms step_avg:61.01ms
step:411/2330 train_time:25075ms step_avg:61.01ms
step:412/2330 train_time:25137ms step_avg:61.01ms
step:413/2330 train_time:25196ms step_avg:61.01ms
step:414/2330 train_time:25258ms step_avg:61.01ms
step:415/2330 train_time:25318ms step_avg:61.01ms
step:416/2330 train_time:25380ms step_avg:61.01ms
step:417/2330 train_time:25440ms step_avg:61.01ms
step:418/2330 train_time:25503ms step_avg:61.01ms
step:419/2330 train_time:25564ms step_avg:61.01ms
step:420/2330 train_time:25627ms step_avg:61.02ms
step:421/2330 train_time:25687ms step_avg:61.01ms
step:422/2330 train_time:25750ms step_avg:61.02ms
step:423/2330 train_time:25810ms step_avg:61.02ms
step:424/2330 train_time:25872ms step_avg:61.02ms
step:425/2330 train_time:25932ms step_avg:61.02ms
step:426/2330 train_time:25994ms step_avg:61.02ms
step:427/2330 train_time:26053ms step_avg:61.02ms
step:428/2330 train_time:26115ms step_avg:61.02ms
step:429/2330 train_time:26174ms step_avg:61.01ms
step:430/2330 train_time:26236ms step_avg:61.01ms
step:431/2330 train_time:26296ms step_avg:61.01ms
step:432/2330 train_time:26357ms step_avg:61.01ms
step:433/2330 train_time:26417ms step_avg:61.01ms
step:434/2330 train_time:26480ms step_avg:61.01ms
step:435/2330 train_time:26540ms step_avg:61.01ms
step:436/2330 train_time:26602ms step_avg:61.01ms
step:437/2330 train_time:26663ms step_avg:61.01ms
step:438/2330 train_time:26726ms step_avg:61.02ms
step:439/2330 train_time:26787ms step_avg:61.02ms
step:440/2330 train_time:26849ms step_avg:61.02ms
step:441/2330 train_time:26909ms step_avg:61.02ms
step:442/2330 train_time:26972ms step_avg:61.02ms
step:443/2330 train_time:27032ms step_avg:61.02ms
step:444/2330 train_time:27094ms step_avg:61.02ms
step:445/2330 train_time:27154ms step_avg:61.02ms
step:446/2330 train_time:27216ms step_avg:61.02ms
step:447/2330 train_time:27275ms step_avg:61.02ms
step:448/2330 train_time:27336ms step_avg:61.02ms
step:449/2330 train_time:27396ms step_avg:61.02ms
step:450/2330 train_time:27458ms step_avg:61.02ms
step:451/2330 train_time:27517ms step_avg:61.01ms
step:452/2330 train_time:27580ms step_avg:61.02ms
step:453/2330 train_time:27641ms step_avg:61.02ms
step:454/2330 train_time:27703ms step_avg:61.02ms
step:455/2330 train_time:27764ms step_avg:61.02ms
step:456/2330 train_time:27826ms step_avg:61.02ms
step:457/2330 train_time:27887ms step_avg:61.02ms
step:458/2330 train_time:27949ms step_avg:61.02ms
step:459/2330 train_time:28009ms step_avg:61.02ms
step:460/2330 train_time:28071ms step_avg:61.02ms
step:461/2330 train_time:28131ms step_avg:61.02ms
step:462/2330 train_time:28193ms step_avg:61.02ms
step:463/2330 train_time:28253ms step_avg:61.02ms
step:464/2330 train_time:28315ms step_avg:61.02ms
step:465/2330 train_time:28374ms step_avg:61.02ms
step:466/2330 train_time:28436ms step_avg:61.02ms
step:467/2330 train_time:28497ms step_avg:61.02ms
step:468/2330 train_time:28559ms step_avg:61.02ms
step:469/2330 train_time:28618ms step_avg:61.02ms
step:470/2330 train_time:28680ms step_avg:61.02ms
step:471/2330 train_time:28740ms step_avg:61.02ms
step:472/2330 train_time:28803ms step_avg:61.02ms
step:473/2330 train_time:28863ms step_avg:61.02ms
step:474/2330 train_time:28926ms step_avg:61.02ms
step:475/2330 train_time:28987ms step_avg:61.02ms
step:476/2330 train_time:29049ms step_avg:61.03ms
step:477/2330 train_time:29109ms step_avg:61.02ms
step:478/2330 train_time:29172ms step_avg:61.03ms
step:479/2330 train_time:29232ms step_avg:61.03ms
step:480/2330 train_time:29294ms step_avg:61.03ms
step:481/2330 train_time:29354ms step_avg:61.03ms
step:482/2330 train_time:29416ms step_avg:61.03ms
step:483/2330 train_time:29475ms step_avg:61.03ms
step:484/2330 train_time:29538ms step_avg:61.03ms
step:485/2330 train_time:29598ms step_avg:61.03ms
step:486/2330 train_time:29660ms step_avg:61.03ms
step:487/2330 train_time:29720ms step_avg:61.03ms
step:488/2330 train_time:29782ms step_avg:61.03ms
step:489/2330 train_time:29842ms step_avg:61.03ms
step:490/2330 train_time:29905ms step_avg:61.03ms
step:491/2330 train_time:29966ms step_avg:61.03ms
step:492/2330 train_time:30028ms step_avg:61.03ms
step:493/2330 train_time:30088ms step_avg:61.03ms
step:494/2330 train_time:30150ms step_avg:61.03ms
step:495/2330 train_time:30210ms step_avg:61.03ms
step:496/2330 train_time:30272ms step_avg:61.03ms
step:497/2330 train_time:30333ms step_avg:61.03ms
step:498/2330 train_time:30394ms step_avg:61.03ms
step:499/2330 train_time:30454ms step_avg:61.03ms
step:500/2330 train_time:30516ms step_avg:61.03ms
step:500/2330 val_loss:4.1622 train_time:30580ms step_avg:61.16ms
step:501/2330 train_time:30602ms step_avg:61.08ms
step:502/2330 train_time:30639ms step_avg:61.03ms
step:503/2330 train_time:30705ms step_avg:61.04ms
step:504/2330 train_time:30769ms step_avg:61.05ms
step:505/2330 train_time:30829ms step_avg:61.05ms
step:506/2330 train_time:30891ms step_avg:61.05ms
step:507/2330 train_time:30950ms step_avg:61.04ms
step:508/2330 train_time:31011ms step_avg:61.05ms
step:509/2330 train_time:31070ms step_avg:61.04ms
step:510/2330 train_time:31131ms step_avg:61.04ms
step:511/2330 train_time:31190ms step_avg:61.04ms
step:512/2330 train_time:31251ms step_avg:61.04ms
step:513/2330 train_time:31310ms step_avg:61.03ms
step:514/2330 train_time:31372ms step_avg:61.04ms
step:515/2330 train_time:31432ms step_avg:61.03ms
step:516/2330 train_time:31494ms step_avg:61.03ms
step:517/2330 train_time:31555ms step_avg:61.03ms
step:518/2330 train_time:31619ms step_avg:61.04ms
step:519/2330 train_time:31682ms step_avg:61.04ms
step:520/2330 train_time:31744ms step_avg:61.05ms
step:521/2330 train_time:31805ms step_avg:61.05ms
step:522/2330 train_time:31867ms step_avg:61.05ms
step:523/2330 train_time:31928ms step_avg:61.05ms
step:524/2330 train_time:31990ms step_avg:61.05ms
step:525/2330 train_time:32049ms step_avg:61.05ms
step:526/2330 train_time:32110ms step_avg:61.05ms
step:527/2330 train_time:32170ms step_avg:61.04ms
step:528/2330 train_time:32231ms step_avg:61.04ms
step:529/2330 train_time:32290ms step_avg:61.04ms
step:530/2330 train_time:32351ms step_avg:61.04ms
step:531/2330 train_time:32410ms step_avg:61.04ms
step:532/2330 train_time:32472ms step_avg:61.04ms
step:533/2330 train_time:32533ms step_avg:61.04ms
step:534/2330 train_time:32596ms step_avg:61.04ms
step:535/2330 train_time:32657ms step_avg:61.04ms
step:536/2330 train_time:32721ms step_avg:61.05ms
step:537/2330 train_time:32781ms step_avg:61.05ms
step:538/2330 train_time:32843ms step_avg:61.05ms
step:539/2330 train_time:32904ms step_avg:61.05ms
step:540/2330 train_time:32966ms step_avg:61.05ms
step:541/2330 train_time:33026ms step_avg:61.05ms
step:542/2330 train_time:33087ms step_avg:61.05ms
step:543/2330 train_time:33147ms step_avg:61.04ms
step:544/2330 train_time:33209ms step_avg:61.05ms
step:545/2330 train_time:33269ms step_avg:61.04ms
step:546/2330 train_time:33330ms step_avg:61.04ms
step:547/2330 train_time:33389ms step_avg:61.04ms
step:548/2330 train_time:33451ms step_avg:61.04ms
step:549/2330 train_time:33511ms step_avg:61.04ms
step:550/2330 train_time:33573ms step_avg:61.04ms
step:551/2330 train_time:33634ms step_avg:61.04ms
step:552/2330 train_time:33696ms step_avg:61.04ms
step:553/2330 train_time:33757ms step_avg:61.04ms
step:554/2330 train_time:33820ms step_avg:61.05ms
step:555/2330 train_time:33881ms step_avg:61.05ms
step:556/2330 train_time:33942ms step_avg:61.05ms
step:557/2330 train_time:34003ms step_avg:61.05ms
step:558/2330 train_time:34064ms step_avg:61.05ms
step:559/2330 train_time:34125ms step_avg:61.05ms
step:560/2330 train_time:34187ms step_avg:61.05ms
step:561/2330 train_time:34247ms step_avg:61.05ms
step:562/2330 train_time:34309ms step_avg:61.05ms
step:563/2330 train_time:34369ms step_avg:61.05ms
step:564/2330 train_time:34430ms step_avg:61.05ms
step:565/2330 train_time:34490ms step_avg:61.04ms
step:566/2330 train_time:34552ms step_avg:61.05ms
step:567/2330 train_time:34612ms step_avg:61.04ms
step:568/2330 train_time:34674ms step_avg:61.04ms
step:569/2330 train_time:34734ms step_avg:61.04ms
step:570/2330 train_time:34797ms step_avg:61.05ms
step:571/2330 train_time:34859ms step_avg:61.05ms
step:572/2330 train_time:34922ms step_avg:61.05ms
step:573/2330 train_time:34981ms step_avg:61.05ms
step:574/2330 train_time:35043ms step_avg:61.05ms
step:575/2330 train_time:35103ms step_avg:61.05ms
step:576/2330 train_time:35165ms step_avg:61.05ms
step:577/2330 train_time:35224ms step_avg:61.05ms
step:578/2330 train_time:35287ms step_avg:61.05ms
step:579/2330 train_time:35347ms step_avg:61.05ms
step:580/2330 train_time:35409ms step_avg:61.05ms
step:581/2330 train_time:35468ms step_avg:61.05ms
step:582/2330 train_time:35530ms step_avg:61.05ms
step:583/2330 train_time:35590ms step_avg:61.05ms
step:584/2330 train_time:35652ms step_avg:61.05ms
step:585/2330 train_time:35712ms step_avg:61.05ms
step:586/2330 train_time:35774ms step_avg:61.05ms
step:587/2330 train_time:35834ms step_avg:61.05ms
step:588/2330 train_time:35897ms step_avg:61.05ms
step:589/2330 train_time:35958ms step_avg:61.05ms
step:590/2330 train_time:36020ms step_avg:61.05ms
step:591/2330 train_time:36079ms step_avg:61.05ms
step:592/2330 train_time:36142ms step_avg:61.05ms
step:593/2330 train_time:36202ms step_avg:61.05ms
step:594/2330 train_time:36264ms step_avg:61.05ms
step:595/2330 train_time:36325ms step_avg:61.05ms
step:596/2330 train_time:36387ms step_avg:61.05ms
step:597/2330 train_time:36447ms step_avg:61.05ms
step:598/2330 train_time:36509ms step_avg:61.05ms
step:599/2330 train_time:36569ms step_avg:61.05ms
step:600/2330 train_time:36631ms step_avg:61.05ms
step:601/2330 train_time:36692ms step_avg:61.05ms
step:602/2330 train_time:36754ms step_avg:61.05ms
step:603/2330 train_time:36814ms step_avg:61.05ms
step:604/2330 train_time:36876ms step_avg:61.05ms
step:605/2330 train_time:36936ms step_avg:61.05ms
step:606/2330 train_time:36999ms step_avg:61.05ms
step:607/2330 train_time:37060ms step_avg:61.05ms
step:608/2330 train_time:37123ms step_avg:61.06ms
step:609/2330 train_time:37183ms step_avg:61.06ms
step:610/2330 train_time:37245ms step_avg:61.06ms
step:611/2330 train_time:37306ms step_avg:61.06ms
step:612/2330 train_time:37368ms step_avg:61.06ms
step:613/2330 train_time:37429ms step_avg:61.06ms
step:614/2330 train_time:37491ms step_avg:61.06ms
step:615/2330 train_time:37551ms step_avg:61.06ms
step:616/2330 train_time:37613ms step_avg:61.06ms
step:617/2330 train_time:37673ms step_avg:61.06ms
step:618/2330 train_time:37735ms step_avg:61.06ms
step:619/2330 train_time:37795ms step_avg:61.06ms
step:620/2330 train_time:37857ms step_avg:61.06ms
step:621/2330 train_time:37917ms step_avg:61.06ms
step:622/2330 train_time:37979ms step_avg:61.06ms
step:623/2330 train_time:38040ms step_avg:61.06ms
step:624/2330 train_time:38103ms step_avg:61.06ms
step:625/2330 train_time:38163ms step_avg:61.06ms
step:626/2330 train_time:38226ms step_avg:61.06ms
step:627/2330 train_time:38286ms step_avg:61.06ms
step:628/2330 train_time:38347ms step_avg:61.06ms
step:629/2330 train_time:38408ms step_avg:61.06ms
step:630/2330 train_time:38470ms step_avg:61.06ms
step:631/2330 train_time:38530ms step_avg:61.06ms
step:632/2330 train_time:38592ms step_avg:61.06ms
step:633/2330 train_time:38652ms step_avg:61.06ms
step:634/2330 train_time:38715ms step_avg:61.07ms
step:635/2330 train_time:38774ms step_avg:61.06ms
step:636/2330 train_time:38835ms step_avg:61.06ms
step:637/2330 train_time:38895ms step_avg:61.06ms
step:638/2330 train_time:38957ms step_avg:61.06ms
step:639/2330 train_time:39018ms step_avg:61.06ms
step:640/2330 train_time:39080ms step_avg:61.06ms
step:641/2330 train_time:39141ms step_avg:61.06ms
step:642/2330 train_time:39204ms step_avg:61.06ms
step:643/2330 train_time:39263ms step_avg:61.06ms
step:644/2330 train_time:39326ms step_avg:61.06ms
step:645/2330 train_time:39386ms step_avg:61.06ms
step:646/2330 train_time:39448ms step_avg:61.07ms
step:647/2330 train_time:39508ms step_avg:61.06ms
step:648/2330 train_time:39570ms step_avg:61.06ms
step:649/2330 train_time:39631ms step_avg:61.06ms
step:650/2330 train_time:39693ms step_avg:61.07ms
step:651/2330 train_time:39753ms step_avg:61.06ms
step:652/2330 train_time:39816ms step_avg:61.07ms
step:653/2330 train_time:39875ms step_avg:61.06ms
step:654/2330 train_time:39937ms step_avg:61.07ms
step:655/2330 train_time:39997ms step_avg:61.06ms
step:656/2330 train_time:40060ms step_avg:61.07ms
step:657/2330 train_time:40122ms step_avg:61.07ms
step:658/2330 train_time:40184ms step_avg:61.07ms
step:659/2330 train_time:40245ms step_avg:61.07ms
step:660/2330 train_time:40307ms step_avg:61.07ms
step:661/2330 train_time:40366ms step_avg:61.07ms
step:662/2330 train_time:40428ms step_avg:61.07ms
step:663/2330 train_time:40488ms step_avg:61.07ms
step:664/2330 train_time:40550ms step_avg:61.07ms
step:665/2330 train_time:40610ms step_avg:61.07ms
step:666/2330 train_time:40672ms step_avg:61.07ms
step:667/2330 train_time:40732ms step_avg:61.07ms
step:668/2330 train_time:40794ms step_avg:61.07ms
step:669/2330 train_time:40854ms step_avg:61.07ms
step:670/2330 train_time:40916ms step_avg:61.07ms
step:671/2330 train_time:40976ms step_avg:61.07ms
step:672/2330 train_time:41038ms step_avg:61.07ms
step:673/2330 train_time:41099ms step_avg:61.07ms
step:674/2330 train_time:41161ms step_avg:61.07ms
step:675/2330 train_time:41222ms step_avg:61.07ms
step:676/2330 train_time:41285ms step_avg:61.07ms
step:677/2330 train_time:41344ms step_avg:61.07ms
step:678/2330 train_time:41406ms step_avg:61.07ms
step:679/2330 train_time:41466ms step_avg:61.07ms
step:680/2330 train_time:41529ms step_avg:61.07ms
step:681/2330 train_time:41590ms step_avg:61.07ms
step:682/2330 train_time:41651ms step_avg:61.07ms
step:683/2330 train_time:41711ms step_avg:61.07ms
step:684/2330 train_time:41772ms step_avg:61.07ms
step:685/2330 train_time:41832ms step_avg:61.07ms
step:686/2330 train_time:41894ms step_avg:61.07ms
step:687/2330 train_time:41954ms step_avg:61.07ms
step:688/2330 train_time:42016ms step_avg:61.07ms
step:689/2330 train_time:42076ms step_avg:61.07ms
step:690/2330 train_time:42139ms step_avg:61.07ms
step:691/2330 train_time:42201ms step_avg:61.07ms
step:692/2330 train_time:42263ms step_avg:61.07ms
step:693/2330 train_time:42323ms step_avg:61.07ms
step:694/2330 train_time:42387ms step_avg:61.08ms
step:695/2330 train_time:42446ms step_avg:61.07ms
step:696/2330 train_time:42509ms step_avg:61.08ms
step:697/2330 train_time:42569ms step_avg:61.07ms
step:698/2330 train_time:42630ms step_avg:61.07ms
step:699/2330 train_time:42691ms step_avg:61.07ms
step:700/2330 train_time:42753ms step_avg:61.08ms
step:701/2330 train_time:42813ms step_avg:61.07ms
step:702/2330 train_time:42874ms step_avg:61.07ms
step:703/2330 train_time:42934ms step_avg:61.07ms
step:704/2330 train_time:42996ms step_avg:61.07ms
step:705/2330 train_time:43056ms step_avg:61.07ms
step:706/2330 train_time:43119ms step_avg:61.08ms
step:707/2330 train_time:43180ms step_avg:61.07ms
step:708/2330 train_time:43242ms step_avg:61.08ms
step:709/2330 train_time:43302ms step_avg:61.07ms
step:710/2330 train_time:43365ms step_avg:61.08ms
step:711/2330 train_time:43425ms step_avg:61.08ms
step:712/2330 train_time:43487ms step_avg:61.08ms
step:713/2330 train_time:43548ms step_avg:61.08ms
step:714/2330 train_time:43610ms step_avg:61.08ms
step:715/2330 train_time:43670ms step_avg:61.08ms
step:716/2330 train_time:43731ms step_avg:61.08ms
step:717/2330 train_time:43791ms step_avg:61.08ms
step:718/2330 train_time:43853ms step_avg:61.08ms
step:719/2330 train_time:43914ms step_avg:61.08ms
step:720/2330 train_time:43976ms step_avg:61.08ms
step:721/2330 train_time:44036ms step_avg:61.08ms
step:722/2330 train_time:44100ms step_avg:61.08ms
step:723/2330 train_time:44160ms step_avg:61.08ms
step:724/2330 train_time:44223ms step_avg:61.08ms
step:725/2330 train_time:44283ms step_avg:61.08ms
step:726/2330 train_time:44345ms step_avg:61.08ms
step:727/2330 train_time:44405ms step_avg:61.08ms
step:728/2330 train_time:44467ms step_avg:61.08ms
step:729/2330 train_time:44528ms step_avg:61.08ms
step:730/2330 train_time:44590ms step_avg:61.08ms
step:731/2330 train_time:44649ms step_avg:61.08ms
step:732/2330 train_time:44712ms step_avg:61.08ms
step:733/2330 train_time:44773ms step_avg:61.08ms
step:734/2330 train_time:44835ms step_avg:61.08ms
step:735/2330 train_time:44895ms step_avg:61.08ms
step:736/2330 train_time:44956ms step_avg:61.08ms
step:737/2330 train_time:45016ms step_avg:61.08ms
step:738/2330 train_time:45079ms step_avg:61.08ms
step:739/2330 train_time:45139ms step_avg:61.08ms
step:740/2330 train_time:45202ms step_avg:61.08ms
step:741/2330 train_time:45262ms step_avg:61.08ms
step:742/2330 train_time:45324ms step_avg:61.08ms
step:743/2330 train_time:45384ms step_avg:61.08ms
step:744/2330 train_time:45446ms step_avg:61.08ms
step:745/2330 train_time:45506ms step_avg:61.08ms
step:746/2330 train_time:45568ms step_avg:61.08ms
step:747/2330 train_time:45629ms step_avg:61.08ms
step:748/2330 train_time:45691ms step_avg:61.08ms
step:749/2330 train_time:45751ms step_avg:61.08ms
step:750/2330 train_time:45813ms step_avg:61.08ms
step:750/2330 val_loss:3.9578 train_time:45877ms step_avg:61.17ms
step:751/2330 train_time:45902ms step_avg:61.12ms
step:752/2330 train_time:45938ms step_avg:61.09ms
step:753/2330 train_time:46003ms step_avg:61.09ms
step:754/2330 train_time:46069ms step_avg:61.10ms
step:755/2330 train_time:46129ms step_avg:61.10ms
step:756/2330 train_time:46191ms step_avg:61.10ms
step:757/2330 train_time:46250ms step_avg:61.10ms
step:758/2330 train_time:46311ms step_avg:61.10ms
step:759/2330 train_time:46370ms step_avg:61.09ms
step:760/2330 train_time:46431ms step_avg:61.09ms
step:761/2330 train_time:46490ms step_avg:61.09ms
step:762/2330 train_time:46551ms step_avg:61.09ms
step:763/2330 train_time:46610ms step_avg:61.09ms
step:764/2330 train_time:46672ms step_avg:61.09ms
step:765/2330 train_time:46731ms step_avg:61.09ms
step:766/2330 train_time:46795ms step_avg:61.09ms
step:767/2330 train_time:46857ms step_avg:61.09ms
step:768/2330 train_time:46920ms step_avg:61.09ms
step:769/2330 train_time:46982ms step_avg:61.10ms
step:770/2330 train_time:47046ms step_avg:61.10ms
step:771/2330 train_time:47108ms step_avg:61.10ms
step:772/2330 train_time:47171ms step_avg:61.10ms
step:773/2330 train_time:47231ms step_avg:61.10ms
step:774/2330 train_time:47294ms step_avg:61.10ms
step:775/2330 train_time:47354ms step_avg:61.10ms
step:776/2330 train_time:47416ms step_avg:61.10ms
step:777/2330 train_time:47476ms step_avg:61.10ms
step:778/2330 train_time:47539ms step_avg:61.10ms
step:779/2330 train_time:47600ms step_avg:61.10ms
step:780/2330 train_time:47662ms step_avg:61.11ms
step:781/2330 train_time:47722ms step_avg:61.10ms
step:782/2330 train_time:47785ms step_avg:61.11ms
step:783/2330 train_time:47846ms step_avg:61.11ms
step:784/2330 train_time:47909ms step_avg:61.11ms
step:785/2330 train_time:47970ms step_avg:61.11ms
step:786/2330 train_time:48034ms step_avg:61.11ms
step:787/2330 train_time:48095ms step_avg:61.11ms
step:788/2330 train_time:48159ms step_avg:61.12ms
step:789/2330 train_time:48219ms step_avg:61.11ms
step:790/2330 train_time:48282ms step_avg:61.12ms
step:791/2330 train_time:48343ms step_avg:61.12ms
step:792/2330 train_time:48405ms step_avg:61.12ms
step:793/2330 train_time:48466ms step_avg:61.12ms
step:794/2330 train_time:48528ms step_avg:61.12ms
step:795/2330 train_time:48588ms step_avg:61.12ms
step:796/2330 train_time:48651ms step_avg:61.12ms
step:797/2330 train_time:48711ms step_avg:61.12ms
step:798/2330 train_time:48773ms step_avg:61.12ms
step:799/2330 train_time:48834ms step_avg:61.12ms
step:800/2330 train_time:48898ms step_avg:61.12ms
step:801/2330 train_time:48959ms step_avg:61.12ms
step:802/2330 train_time:49022ms step_avg:61.13ms
step:803/2330 train_time:49084ms step_avg:61.13ms
step:804/2330 train_time:49147ms step_avg:61.13ms
step:805/2330 train_time:49208ms step_avg:61.13ms
step:806/2330 train_time:49270ms step_avg:61.13ms
step:807/2330 train_time:49331ms step_avg:61.13ms
step:808/2330 train_time:49393ms step_avg:61.13ms
step:809/2330 train_time:49453ms step_avg:61.13ms
step:810/2330 train_time:49516ms step_avg:61.13ms
step:811/2330 train_time:49577ms step_avg:61.13ms
step:812/2330 train_time:49640ms step_avg:61.13ms
step:813/2330 train_time:49700ms step_avg:61.13ms
step:814/2330 train_time:49763ms step_avg:61.13ms
step:815/2330 train_time:49823ms step_avg:61.13ms
step:816/2330 train_time:49886ms step_avg:61.13ms
step:817/2330 train_time:49946ms step_avg:61.13ms
step:818/2330 train_time:50009ms step_avg:61.14ms
step:819/2330 train_time:50070ms step_avg:61.14ms
step:820/2330 train_time:50133ms step_avg:61.14ms
step:821/2330 train_time:50194ms step_avg:61.14ms
step:822/2330 train_time:50257ms step_avg:61.14ms
step:823/2330 train_time:50318ms step_avg:61.14ms
step:824/2330 train_time:50382ms step_avg:61.14ms
step:825/2330 train_time:50443ms step_avg:61.14ms
step:826/2330 train_time:50505ms step_avg:61.14ms
step:827/2330 train_time:50567ms step_avg:61.14ms
step:828/2330 train_time:50629ms step_avg:61.15ms
step:829/2330 train_time:50689ms step_avg:61.14ms
step:830/2330 train_time:50752ms step_avg:61.15ms
step:831/2330 train_time:50812ms step_avg:61.15ms
step:832/2330 train_time:50874ms step_avg:61.15ms
step:833/2330 train_time:50935ms step_avg:61.15ms
step:834/2330 train_time:50998ms step_avg:61.15ms
step:835/2330 train_time:51059ms step_avg:61.15ms
step:836/2330 train_time:51123ms step_avg:61.15ms
step:837/2330 train_time:51183ms step_avg:61.15ms
step:838/2330 train_time:51247ms step_avg:61.15ms
step:839/2330 train_time:51308ms step_avg:61.15ms
step:840/2330 train_time:51371ms step_avg:61.16ms
step:841/2330 train_time:51431ms step_avg:61.15ms
step:842/2330 train_time:51493ms step_avg:61.16ms
step:843/2330 train_time:51555ms step_avg:61.16ms
step:844/2330 train_time:51618ms step_avg:61.16ms
step:845/2330 train_time:51678ms step_avg:61.16ms
step:846/2330 train_time:51742ms step_avg:61.16ms
step:847/2330 train_time:51802ms step_avg:61.16ms
step:848/2330 train_time:51865ms step_avg:61.16ms
step:849/2330 train_time:51927ms step_avg:61.16ms
step:850/2330 train_time:51989ms step_avg:61.16ms
step:851/2330 train_time:52050ms step_avg:61.16ms
step:852/2330 train_time:52112ms step_avg:61.16ms
step:853/2330 train_time:52172ms step_avg:61.16ms
step:854/2330 train_time:52235ms step_avg:61.16ms
step:855/2330 train_time:52295ms step_avg:61.16ms
step:856/2330 train_time:52359ms step_avg:61.17ms
step:857/2330 train_time:52419ms step_avg:61.17ms
step:858/2330 train_time:52482ms step_avg:61.17ms
step:859/2330 train_time:52543ms step_avg:61.17ms
step:860/2330 train_time:52606ms step_avg:61.17ms
step:861/2330 train_time:52667ms step_avg:61.17ms
step:862/2330 train_time:52729ms step_avg:61.17ms
step:863/2330 train_time:52789ms step_avg:61.17ms
step:864/2330 train_time:52852ms step_avg:61.17ms
step:865/2330 train_time:52912ms step_avg:61.17ms
step:866/2330 train_time:52975ms step_avg:61.17ms
step:867/2330 train_time:53036ms step_avg:61.17ms
step:868/2330 train_time:53099ms step_avg:61.17ms
step:869/2330 train_time:53159ms step_avg:61.17ms
step:870/2330 train_time:53222ms step_avg:61.17ms
step:871/2330 train_time:53283ms step_avg:61.17ms
step:872/2330 train_time:53345ms step_avg:61.18ms
step:873/2330 train_time:53406ms step_avg:61.18ms
step:874/2330 train_time:53469ms step_avg:61.18ms
step:875/2330 train_time:53529ms step_avg:61.18ms
step:876/2330 train_time:53592ms step_avg:61.18ms
step:877/2330 train_time:53653ms step_avg:61.18ms
step:878/2330 train_time:53715ms step_avg:61.18ms
step:879/2330 train_time:53776ms step_avg:61.18ms
step:880/2330 train_time:53839ms step_avg:61.18ms
step:881/2330 train_time:53900ms step_avg:61.18ms
step:882/2330 train_time:53963ms step_avg:61.18ms
step:883/2330 train_time:54024ms step_avg:61.18ms
step:884/2330 train_time:54087ms step_avg:61.18ms
step:885/2330 train_time:54148ms step_avg:61.18ms
step:886/2330 train_time:54211ms step_avg:61.19ms
step:887/2330 train_time:54271ms step_avg:61.18ms
step:888/2330 train_time:54333ms step_avg:61.19ms
step:889/2330 train_time:54394ms step_avg:61.19ms
step:890/2330 train_time:54457ms step_avg:61.19ms
step:891/2330 train_time:54518ms step_avg:61.19ms
step:892/2330 train_time:54581ms step_avg:61.19ms
step:893/2330 train_time:54642ms step_avg:61.19ms
step:894/2330 train_time:54705ms step_avg:61.19ms
step:895/2330 train_time:54766ms step_avg:61.19ms
step:896/2330 train_time:54829ms step_avg:61.19ms
step:897/2330 train_time:54889ms step_avg:61.19ms
step:898/2330 train_time:54951ms step_avg:61.19ms
step:899/2330 train_time:55011ms step_avg:61.19ms
step:900/2330 train_time:55074ms step_avg:61.19ms
step:901/2330 train_time:55135ms step_avg:61.19ms
step:902/2330 train_time:55197ms step_avg:61.19ms
step:903/2330 train_time:55258ms step_avg:61.19ms
step:904/2330 train_time:55320ms step_avg:61.19ms
step:905/2330 train_time:55381ms step_avg:61.19ms
step:906/2330 train_time:55445ms step_avg:61.20ms
step:907/2330 train_time:55505ms step_avg:61.20ms
step:908/2330 train_time:55568ms step_avg:61.20ms
step:909/2330 train_time:55629ms step_avg:61.20ms
step:910/2330 train_time:55691ms step_avg:61.20ms
step:911/2330 train_time:55752ms step_avg:61.20ms
step:912/2330 train_time:55814ms step_avg:61.20ms
step:913/2330 train_time:55875ms step_avg:61.20ms
step:914/2330 train_time:55939ms step_avg:61.20ms
step:915/2330 train_time:55999ms step_avg:61.20ms
step:916/2330 train_time:56062ms step_avg:61.20ms
step:917/2330 train_time:56123ms step_avg:61.20ms
step:918/2330 train_time:56186ms step_avg:61.20ms
step:919/2330 train_time:56246ms step_avg:61.20ms
step:920/2330 train_time:56309ms step_avg:61.21ms
step:921/2330 train_time:56369ms step_avg:61.20ms
step:922/2330 train_time:56432ms step_avg:61.21ms
step:923/2330 train_time:56492ms step_avg:61.20ms
step:924/2330 train_time:56555ms step_avg:61.21ms
step:925/2330 train_time:56616ms step_avg:61.21ms
step:926/2330 train_time:56679ms step_avg:61.21ms
step:927/2330 train_time:56740ms step_avg:61.21ms
step:928/2330 train_time:56803ms step_avg:61.21ms
step:929/2330 train_time:56863ms step_avg:61.21ms
step:930/2330 train_time:56926ms step_avg:61.21ms
step:931/2330 train_time:56987ms step_avg:61.21ms
step:932/2330 train_time:57050ms step_avg:61.21ms
step:933/2330 train_time:57110ms step_avg:61.21ms
step:934/2330 train_time:57173ms step_avg:61.21ms
step:935/2330 train_time:57233ms step_avg:61.21ms
step:936/2330 train_time:57295ms step_avg:61.21ms
step:937/2330 train_time:57356ms step_avg:61.21ms
step:938/2330 train_time:57419ms step_avg:61.21ms
step:939/2330 train_time:57479ms step_avg:61.21ms
step:940/2330 train_time:57542ms step_avg:61.22ms
step:941/2330 train_time:57603ms step_avg:61.21ms
step:942/2330 train_time:57666ms step_avg:61.22ms
step:943/2330 train_time:57727ms step_avg:61.22ms
step:944/2330 train_time:57790ms step_avg:61.22ms
step:945/2330 train_time:57850ms step_avg:61.22ms
step:946/2330 train_time:57912ms step_avg:61.22ms
step:947/2330 train_time:57972ms step_avg:61.22ms
step:948/2330 train_time:58035ms step_avg:61.22ms
step:949/2330 train_time:58095ms step_avg:61.22ms
step:950/2330 train_time:58158ms step_avg:61.22ms
step:951/2330 train_time:58218ms step_avg:61.22ms
step:952/2330 train_time:58281ms step_avg:61.22ms
step:953/2330 train_time:58342ms step_avg:61.22ms
step:954/2330 train_time:58405ms step_avg:61.22ms
step:955/2330 train_time:58466ms step_avg:61.22ms
step:956/2330 train_time:58530ms step_avg:61.22ms
step:957/2330 train_time:58590ms step_avg:61.22ms
step:958/2330 train_time:58653ms step_avg:61.22ms
step:959/2330 train_time:58713ms step_avg:61.22ms
step:960/2330 train_time:58776ms step_avg:61.23ms
step:961/2330 train_time:58838ms step_avg:61.23ms
step:962/2330 train_time:58901ms step_avg:61.23ms
step:963/2330 train_time:58961ms step_avg:61.23ms
step:964/2330 train_time:59024ms step_avg:61.23ms
step:965/2330 train_time:59085ms step_avg:61.23ms
step:966/2330 train_time:59148ms step_avg:61.23ms
step:967/2330 train_time:59208ms step_avg:61.23ms
step:968/2330 train_time:59271ms step_avg:61.23ms
step:969/2330 train_time:59331ms step_avg:61.23ms
step:970/2330 train_time:59394ms step_avg:61.23ms
step:971/2330 train_time:59455ms step_avg:61.23ms
step:972/2330 train_time:59519ms step_avg:61.23ms
step:973/2330 train_time:59580ms step_avg:61.23ms
step:974/2330 train_time:59643ms step_avg:61.24ms
step:975/2330 train_time:59704ms step_avg:61.24ms
step:976/2330 train_time:59768ms step_avg:61.24ms
step:977/2330 train_time:59828ms step_avg:61.24ms
step:978/2330 train_time:59890ms step_avg:61.24ms
step:979/2330 train_time:59951ms step_avg:61.24ms
step:980/2330 train_time:60014ms step_avg:61.24ms
step:981/2330 train_time:60074ms step_avg:61.24ms
step:982/2330 train_time:60138ms step_avg:61.24ms
step:983/2330 train_time:60198ms step_avg:61.24ms
step:984/2330 train_time:60261ms step_avg:61.24ms
step:985/2330 train_time:60321ms step_avg:61.24ms
step:986/2330 train_time:60384ms step_avg:61.24ms
step:987/2330 train_time:60445ms step_avg:61.24ms
step:988/2330 train_time:60508ms step_avg:61.24ms
step:989/2330 train_time:60568ms step_avg:61.24ms
step:990/2330 train_time:60631ms step_avg:61.24ms
step:991/2330 train_time:60691ms step_avg:61.24ms
step:992/2330 train_time:60754ms step_avg:61.24ms
step:993/2330 train_time:60815ms step_avg:61.24ms
step:994/2330 train_time:60878ms step_avg:61.25ms
step:995/2330 train_time:60940ms step_avg:61.25ms
step:996/2330 train_time:61003ms step_avg:61.25ms
step:997/2330 train_time:61063ms step_avg:61.25ms
step:998/2330 train_time:61126ms step_avg:61.25ms
step:999/2330 train_time:61187ms step_avg:61.25ms
step:1000/2330 train_time:61249ms step_avg:61.25ms
step:1000/2330 val_loss:3.8011 train_time:61314ms step_avg:61.31ms
step:1001/2330 train_time:61336ms step_avg:61.27ms
step:1002/2330 train_time:61377ms step_avg:61.25ms
step:1003/2330 train_time:61444ms step_avg:61.26ms
step:1004/2330 train_time:61509ms step_avg:61.26ms
step:1005/2330 train_time:61571ms step_avg:61.26ms
step:1006/2330 train_time:61634ms step_avg:61.27ms
step:1007/2330 train_time:61693ms step_avg:61.26ms
step:1008/2330 train_time:61755ms step_avg:61.26ms
step:1009/2330 train_time:61815ms step_avg:61.26ms
step:1010/2330 train_time:61877ms step_avg:61.26ms
step:1011/2330 train_time:61936ms step_avg:61.26ms
step:1012/2330 train_time:61998ms step_avg:61.26ms
step:1013/2330 train_time:62058ms step_avg:61.26ms
step:1014/2330 train_time:62119ms step_avg:61.26ms
step:1015/2330 train_time:62179ms step_avg:61.26ms
step:1016/2330 train_time:62245ms step_avg:61.26ms
step:1017/2330 train_time:62308ms step_avg:61.27ms
step:1018/2330 train_time:62371ms step_avg:61.27ms
step:1019/2330 train_time:62433ms step_avg:61.27ms
step:1020/2330 train_time:62497ms step_avg:61.27ms
step:1021/2330 train_time:62558ms step_avg:61.27ms
step:1022/2330 train_time:62621ms step_avg:61.27ms
step:1023/2330 train_time:62682ms step_avg:61.27ms
step:1024/2330 train_time:62745ms step_avg:61.27ms
step:1025/2330 train_time:62805ms step_avg:61.27ms
step:1026/2330 train_time:62868ms step_avg:61.27ms
step:1027/2330 train_time:62928ms step_avg:61.27ms
step:1028/2330 train_time:62992ms step_avg:61.28ms
step:1029/2330 train_time:63052ms step_avg:61.28ms
step:1030/2330 train_time:63115ms step_avg:61.28ms
step:1031/2330 train_time:63174ms step_avg:61.27ms
step:1032/2330 train_time:63237ms step_avg:61.28ms
step:1033/2330 train_time:63298ms step_avg:61.28ms
step:1034/2330 train_time:63362ms step_avg:61.28ms
step:1035/2330 train_time:63424ms step_avg:61.28ms
step:1036/2330 train_time:63487ms step_avg:61.28ms
step:1037/2330 train_time:63548ms step_avg:61.28ms
step:1038/2330 train_time:63611ms step_avg:61.28ms
step:1039/2330 train_time:63672ms step_avg:61.28ms
step:1040/2330 train_time:63734ms step_avg:61.28ms
step:1041/2330 train_time:63794ms step_avg:61.28ms
step:1042/2330 train_time:63857ms step_avg:61.28ms
step:1043/2330 train_time:63917ms step_avg:61.28ms
step:1044/2330 train_time:63980ms step_avg:61.28ms
step:1045/2330 train_time:64041ms step_avg:61.28ms
step:1046/2330 train_time:64103ms step_avg:61.28ms
step:1047/2330 train_time:64165ms step_avg:61.28ms
step:1048/2330 train_time:64228ms step_avg:61.29ms
step:1049/2330 train_time:64289ms step_avg:61.29ms
step:1050/2330 train_time:64353ms step_avg:61.29ms
step:1051/2330 train_time:64414ms step_avg:61.29ms
step:1052/2330 train_time:64477ms step_avg:61.29ms
step:1053/2330 train_time:64538ms step_avg:61.29ms
step:1054/2330 train_time:64600ms step_avg:61.29ms
step:1055/2330 train_time:64662ms step_avg:61.29ms
step:1056/2330 train_time:64725ms step_avg:61.29ms
step:1057/2330 train_time:64786ms step_avg:61.29ms
step:1058/2330 train_time:64849ms step_avg:61.29ms
step:1059/2330 train_time:64909ms step_avg:61.29ms
step:1060/2330 train_time:64972ms step_avg:61.29ms
step:1061/2330 train_time:65033ms step_avg:61.29ms
step:1062/2330 train_time:65095ms step_avg:61.29ms
step:1063/2330 train_time:65155ms step_avg:61.29ms
step:1064/2330 train_time:65218ms step_avg:61.30ms
step:1065/2330 train_time:65279ms step_avg:61.29ms
step:1066/2330 train_time:65342ms step_avg:61.30ms
step:1067/2330 train_time:65404ms step_avg:61.30ms
step:1068/2330 train_time:65467ms step_avg:61.30ms
step:1069/2330 train_time:65528ms step_avg:61.30ms
step:1070/2330 train_time:65592ms step_avg:61.30ms
step:1071/2330 train_time:65652ms step_avg:61.30ms
step:1072/2330 train_time:65715ms step_avg:61.30ms
step:1073/2330 train_time:65775ms step_avg:61.30ms
step:1074/2330 train_time:65838ms step_avg:61.30ms
step:1075/2330 train_time:65899ms step_avg:61.30ms
step:1076/2330 train_time:65962ms step_avg:61.30ms
step:1077/2330 train_time:66023ms step_avg:61.30ms
step:1078/2330 train_time:66085ms step_avg:61.30ms
step:1079/2330 train_time:66145ms step_avg:61.30ms
step:1080/2330 train_time:66209ms step_avg:61.30ms
step:1081/2330 train_time:66270ms step_avg:61.30ms
step:1082/2330 train_time:66333ms step_avg:61.31ms
step:1083/2330 train_time:66394ms step_avg:61.31ms
step:1084/2330 train_time:66457ms step_avg:61.31ms
step:1085/2330 train_time:66517ms step_avg:61.31ms
step:1086/2330 train_time:66580ms step_avg:61.31ms
step:1087/2330 train_time:66641ms step_avg:61.31ms
step:1088/2330 train_time:66705ms step_avg:61.31ms
step:1089/2330 train_time:66765ms step_avg:61.31ms
step:1090/2330 train_time:66828ms step_avg:61.31ms
step:1091/2330 train_time:66890ms step_avg:61.31ms
step:1092/2330 train_time:66953ms step_avg:61.31ms
step:1093/2330 train_time:67013ms step_avg:61.31ms
step:1094/2330 train_time:67076ms step_avg:61.31ms
step:1095/2330 train_time:67135ms step_avg:61.31ms
step:1096/2330 train_time:67198ms step_avg:61.31ms
step:1097/2330 train_time:67259ms step_avg:61.31ms
step:1098/2330 train_time:67322ms step_avg:61.31ms
step:1099/2330 train_time:67383ms step_avg:61.31ms
step:1100/2330 train_time:67446ms step_avg:61.31ms
step:1101/2330 train_time:67507ms step_avg:61.31ms
step:1102/2330 train_time:67570ms step_avg:61.32ms
step:1103/2330 train_time:67631ms step_avg:61.32ms
step:1104/2330 train_time:67694ms step_avg:61.32ms
step:1105/2330 train_time:67754ms step_avg:61.32ms
step:1106/2330 train_time:67817ms step_avg:61.32ms
step:1107/2330 train_time:67878ms step_avg:61.32ms
step:1108/2330 train_time:67941ms step_avg:61.32ms
step:1109/2330 train_time:68001ms step_avg:61.32ms
step:1110/2330 train_time:68065ms step_avg:61.32ms
step:1111/2330 train_time:68125ms step_avg:61.32ms
step:1112/2330 train_time:68189ms step_avg:61.32ms
step:1113/2330 train_time:68251ms step_avg:61.32ms
step:1114/2330 train_time:68314ms step_avg:61.32ms
step:1115/2330 train_time:68374ms step_avg:61.32ms
step:1116/2330 train_time:68436ms step_avg:61.32ms
step:1117/2330 train_time:68496ms step_avg:61.32ms
step:1118/2330 train_time:68559ms step_avg:61.32ms
step:1119/2330 train_time:68620ms step_avg:61.32ms
step:1120/2330 train_time:68684ms step_avg:61.33ms
step:1121/2330 train_time:68745ms step_avg:61.32ms
step:1122/2330 train_time:68808ms step_avg:61.33ms
step:1123/2330 train_time:68868ms step_avg:61.33ms
step:1124/2330 train_time:68932ms step_avg:61.33ms
step:1125/2330 train_time:68993ms step_avg:61.33ms
step:1126/2330 train_time:69056ms step_avg:61.33ms
step:1127/2330 train_time:69116ms step_avg:61.33ms
step:1128/2330 train_time:69179ms step_avg:61.33ms
step:1129/2330 train_time:69240ms step_avg:61.33ms
step:1130/2330 train_time:69303ms step_avg:61.33ms
step:1131/2330 train_time:69364ms step_avg:61.33ms
step:1132/2330 train_time:69426ms step_avg:61.33ms
step:1133/2330 train_time:69488ms step_avg:61.33ms
step:1134/2330 train_time:69552ms step_avg:61.33ms
step:1135/2330 train_time:69613ms step_avg:61.33ms
step:1136/2330 train_time:69675ms step_avg:61.33ms
step:1137/2330 train_time:69735ms step_avg:61.33ms
step:1138/2330 train_time:69798ms step_avg:61.33ms
step:1139/2330 train_time:69859ms step_avg:61.33ms
step:1140/2330 train_time:69922ms step_avg:61.34ms
step:1141/2330 train_time:69984ms step_avg:61.34ms
step:1142/2330 train_time:70047ms step_avg:61.34ms
step:1143/2330 train_time:70107ms step_avg:61.34ms
step:1144/2330 train_time:70171ms step_avg:61.34ms
step:1145/2330 train_time:70231ms step_avg:61.34ms
step:1146/2330 train_time:70294ms step_avg:61.34ms
step:1147/2330 train_time:70355ms step_avg:61.34ms
step:1148/2330 train_time:70417ms step_avg:61.34ms
step:1149/2330 train_time:70477ms step_avg:61.34ms
step:1150/2330 train_time:70540ms step_avg:61.34ms
step:1151/2330 train_time:70601ms step_avg:61.34ms
step:1152/2330 train_time:70664ms step_avg:61.34ms
step:1153/2330 train_time:70725ms step_avg:61.34ms
step:1154/2330 train_time:70788ms step_avg:61.34ms
step:1155/2330 train_time:70848ms step_avg:61.34ms
step:1156/2330 train_time:70911ms step_avg:61.34ms
step:1157/2330 train_time:70973ms step_avg:61.34ms
step:1158/2330 train_time:71036ms step_avg:61.34ms
step:1159/2330 train_time:71097ms step_avg:61.34ms
step:1160/2330 train_time:71159ms step_avg:61.34ms
step:1161/2330 train_time:71221ms step_avg:61.34ms
step:1162/2330 train_time:71284ms step_avg:61.35ms
step:1163/2330 train_time:71344ms step_avg:61.35ms
step:1164/2330 train_time:71408ms step_avg:61.35ms
step:1165/2330 train_time:71470ms step_avg:61.35ms
step:1166/2330 train_time:71533ms step_avg:61.35ms
step:1167/2330 train_time:71593ms step_avg:61.35ms
step:1168/2330 train_time:71656ms step_avg:61.35ms
step:1169/2330 train_time:71716ms step_avg:61.35ms
step:1170/2330 train_time:71779ms step_avg:61.35ms
step:1171/2330 train_time:71840ms step_avg:61.35ms
step:1172/2330 train_time:71904ms step_avg:61.35ms
step:1173/2330 train_time:71965ms step_avg:61.35ms
step:1174/2330 train_time:72028ms step_avg:61.35ms
step:1175/2330 train_time:72090ms step_avg:61.35ms
step:1176/2330 train_time:72153ms step_avg:61.35ms
step:1177/2330 train_time:72213ms step_avg:61.35ms
step:1178/2330 train_time:72276ms step_avg:61.35ms
step:1179/2330 train_time:72336ms step_avg:61.35ms
step:1180/2330 train_time:72398ms step_avg:61.35ms
step:1181/2330 train_time:72459ms step_avg:61.35ms
step:1182/2330 train_time:72522ms step_avg:61.36ms
step:1183/2330 train_time:72583ms step_avg:61.36ms
step:1184/2330 train_time:72646ms step_avg:61.36ms
step:1185/2330 train_time:72707ms step_avg:61.36ms
step:1186/2330 train_time:72771ms step_avg:61.36ms
step:1187/2330 train_time:72832ms step_avg:61.36ms
step:1188/2330 train_time:72895ms step_avg:61.36ms
step:1189/2330 train_time:72955ms step_avg:61.36ms
step:1190/2330 train_time:73019ms step_avg:61.36ms
step:1191/2330 train_time:73080ms step_avg:61.36ms
step:1192/2330 train_time:73143ms step_avg:61.36ms
step:1193/2330 train_time:73205ms step_avg:61.36ms
step:1194/2330 train_time:73268ms step_avg:61.36ms
step:1195/2330 train_time:73328ms step_avg:61.36ms
step:1196/2330 train_time:73391ms step_avg:61.36ms
step:1197/2330 train_time:73453ms step_avg:61.36ms
step:1198/2330 train_time:73515ms step_avg:61.36ms
step:1199/2330 train_time:73575ms step_avg:61.36ms
step:1200/2330 train_time:73638ms step_avg:61.37ms
step:1201/2330 train_time:73699ms step_avg:61.36ms
step:1202/2330 train_time:73762ms step_avg:61.37ms
step:1203/2330 train_time:73822ms step_avg:61.37ms
step:1204/2330 train_time:73886ms step_avg:61.37ms
step:1205/2330 train_time:73947ms step_avg:61.37ms
step:1206/2330 train_time:74011ms step_avg:61.37ms
step:1207/2330 train_time:74073ms step_avg:61.37ms
step:1208/2330 train_time:74135ms step_avg:61.37ms
step:1209/2330 train_time:74195ms step_avg:61.37ms
step:1210/2330 train_time:74258ms step_avg:61.37ms
step:1211/2330 train_time:74319ms step_avg:61.37ms
step:1212/2330 train_time:74382ms step_avg:61.37ms
step:1213/2330 train_time:74443ms step_avg:61.37ms
step:1214/2330 train_time:74506ms step_avg:61.37ms
step:1215/2330 train_time:74566ms step_avg:61.37ms
step:1216/2330 train_time:74630ms step_avg:61.37ms
step:1217/2330 train_time:74691ms step_avg:61.37ms
step:1218/2330 train_time:74755ms step_avg:61.38ms
step:1219/2330 train_time:74815ms step_avg:61.37ms
step:1220/2330 train_time:74877ms step_avg:61.37ms
step:1221/2330 train_time:74938ms step_avg:61.37ms
step:1222/2330 train_time:75001ms step_avg:61.38ms
step:1223/2330 train_time:75061ms step_avg:61.37ms
step:1224/2330 train_time:75124ms step_avg:61.38ms
step:1225/2330 train_time:75185ms step_avg:61.38ms
step:1226/2330 train_time:75248ms step_avg:61.38ms
step:1227/2330 train_time:75311ms step_avg:61.38ms
step:1228/2330 train_time:75373ms step_avg:61.38ms
step:1229/2330 train_time:75434ms step_avg:61.38ms
step:1230/2330 train_time:75496ms step_avg:61.38ms
step:1231/2330 train_time:75557ms step_avg:61.38ms
step:1232/2330 train_time:75619ms step_avg:61.38ms
step:1233/2330 train_time:75680ms step_avg:61.38ms
step:1234/2330 train_time:75743ms step_avg:61.38ms
step:1235/2330 train_time:75804ms step_avg:61.38ms
step:1236/2330 train_time:75867ms step_avg:61.38ms
step:1237/2330 train_time:75928ms step_avg:61.38ms
step:1238/2330 train_time:75992ms step_avg:61.38ms
step:1239/2330 train_time:76052ms step_avg:61.38ms
step:1240/2330 train_time:76115ms step_avg:61.38ms
step:1241/2330 train_time:76176ms step_avg:61.38ms
step:1242/2330 train_time:76239ms step_avg:61.38ms
step:1243/2330 train_time:76300ms step_avg:61.38ms
step:1244/2330 train_time:76363ms step_avg:61.39ms
step:1245/2330 train_time:76424ms step_avg:61.38ms
step:1246/2330 train_time:76486ms step_avg:61.39ms
step:1247/2330 train_time:76547ms step_avg:61.39ms
step:1248/2330 train_time:76611ms step_avg:61.39ms
step:1249/2330 train_time:76672ms step_avg:61.39ms
step:1250/2330 train_time:76734ms step_avg:61.39ms
step:1250/2330 val_loss:3.7534 train_time:76799ms step_avg:61.44ms
step:1251/2330 train_time:76822ms step_avg:61.41ms
step:1252/2330 train_time:76859ms step_avg:61.39ms
step:1253/2330 train_time:76922ms step_avg:61.39ms
step:1254/2330 train_time:76987ms step_avg:61.39ms
step:1255/2330 train_time:77047ms step_avg:61.39ms
step:1256/2330 train_time:77111ms step_avg:61.39ms
step:1257/2330 train_time:77171ms step_avg:61.39ms
step:1258/2330 train_time:77233ms step_avg:61.39ms
step:1259/2330 train_time:77294ms step_avg:61.39ms
step:1260/2330 train_time:77356ms step_avg:61.39ms
step:1261/2330 train_time:77416ms step_avg:61.39ms
step:1262/2330 train_time:77478ms step_avg:61.39ms
step:1263/2330 train_time:77538ms step_avg:61.39ms
step:1264/2330 train_time:77600ms step_avg:61.39ms
step:1265/2330 train_time:77660ms step_avg:61.39ms
step:1266/2330 train_time:77723ms step_avg:61.39ms
step:1267/2330 train_time:77784ms step_avg:61.39ms
step:1268/2330 train_time:77848ms step_avg:61.39ms
step:1269/2330 train_time:77910ms step_avg:61.40ms
step:1270/2330 train_time:77975ms step_avg:61.40ms
step:1271/2330 train_time:78036ms step_avg:61.40ms
step:1272/2330 train_time:78098ms step_avg:61.40ms
step:1273/2330 train_time:78159ms step_avg:61.40ms
step:1274/2330 train_time:78221ms step_avg:61.40ms
step:1275/2330 train_time:78281ms step_avg:61.40ms
step:1276/2330 train_time:78344ms step_avg:61.40ms
step:1277/2330 train_time:78404ms step_avg:61.40ms
step:1278/2330 train_time:78467ms step_avg:61.40ms
step:1279/2330 train_time:78527ms step_avg:61.40ms
step:1280/2330 train_time:78590ms step_avg:61.40ms
step:1281/2330 train_time:78651ms step_avg:61.40ms
step:1282/2330 train_time:78714ms step_avg:61.40ms
step:1283/2330 train_time:78775ms step_avg:61.40ms
step:1284/2330 train_time:78839ms step_avg:61.40ms
step:1285/2330 train_time:78900ms step_avg:61.40ms
step:1286/2330 train_time:78963ms step_avg:61.40ms
step:1287/2330 train_time:79024ms step_avg:61.40ms
step:1288/2330 train_time:79087ms step_avg:61.40ms
step:1289/2330 train_time:79148ms step_avg:61.40ms
step:1290/2330 train_time:79210ms step_avg:61.40ms
step:1291/2330 train_time:79270ms step_avg:61.40ms
step:1292/2330 train_time:79333ms step_avg:61.40ms
step:1293/2330 train_time:79393ms step_avg:61.40ms
step:1294/2330 train_time:79456ms step_avg:61.40ms
step:1295/2330 train_time:79517ms step_avg:61.40ms
step:1296/2330 train_time:79579ms step_avg:61.40ms
step:1297/2330 train_time:79639ms step_avg:61.40ms
step:1298/2330 train_time:79702ms step_avg:61.40ms
step:1299/2330 train_time:79763ms step_avg:61.40ms
step:1300/2330 train_time:79825ms step_avg:61.40ms
step:1301/2330 train_time:79886ms step_avg:61.40ms
step:1302/2330 train_time:79949ms step_avg:61.41ms
step:1303/2330 train_time:80011ms step_avg:61.40ms
step:1304/2330 train_time:80075ms step_avg:61.41ms
step:1305/2330 train_time:80135ms step_avg:61.41ms
step:1306/2330 train_time:80198ms step_avg:61.41ms
step:1307/2330 train_time:80258ms step_avg:61.41ms
step:1308/2330 train_time:80321ms step_avg:61.41ms
step:1309/2330 train_time:80380ms step_avg:61.41ms
step:1310/2330 train_time:80442ms step_avg:61.41ms
step:1311/2330 train_time:80503ms step_avg:61.41ms
step:1312/2330 train_time:80566ms step_avg:61.41ms
step:1313/2330 train_time:80626ms step_avg:61.41ms
step:1314/2330 train_time:80689ms step_avg:61.41ms
step:1315/2330 train_time:80750ms step_avg:61.41ms
step:1316/2330 train_time:80813ms step_avg:61.41ms
step:1317/2330 train_time:80874ms step_avg:61.41ms
step:1318/2330 train_time:80937ms step_avg:61.41ms
step:1319/2330 train_time:80998ms step_avg:61.41ms
step:1320/2330 train_time:81060ms step_avg:61.41ms
step:1321/2330 train_time:81121ms step_avg:61.41ms
step:1322/2330 train_time:81184ms step_avg:61.41ms
step:1323/2330 train_time:81244ms step_avg:61.41ms
step:1324/2330 train_time:81307ms step_avg:61.41ms
step:1325/2330 train_time:81369ms step_avg:61.41ms
step:1326/2330 train_time:81431ms step_avg:61.41ms
step:1327/2330 train_time:81492ms step_avg:61.41ms
step:1328/2330 train_time:81555ms step_avg:61.41ms
step:1329/2330 train_time:81616ms step_avg:61.41ms
step:1330/2330 train_time:81679ms step_avg:61.41ms
step:1331/2330 train_time:81739ms step_avg:61.41ms
step:1332/2330 train_time:81802ms step_avg:61.41ms
step:1333/2330 train_time:81863ms step_avg:61.41ms
step:1334/2330 train_time:81927ms step_avg:61.41ms
step:1335/2330 train_time:81987ms step_avg:61.41ms
step:1336/2330 train_time:82050ms step_avg:61.41ms
step:1337/2330 train_time:82111ms step_avg:61.41ms
step:1338/2330 train_time:82174ms step_avg:61.42ms
step:1339/2330 train_time:82236ms step_avg:61.42ms
step:1340/2330 train_time:82298ms step_avg:61.42ms
step:1341/2330 train_time:82358ms step_avg:61.42ms
step:1342/2330 train_time:82421ms step_avg:61.42ms
step:1343/2330 train_time:82481ms step_avg:61.42ms
step:1344/2330 train_time:82544ms step_avg:61.42ms
step:1345/2330 train_time:82604ms step_avg:61.42ms
step:1346/2330 train_time:82668ms step_avg:61.42ms
step:1347/2330 train_time:82729ms step_avg:61.42ms
step:1348/2330 train_time:82792ms step_avg:61.42ms
step:1349/2330 train_time:82854ms step_avg:61.42ms
step:1350/2330 train_time:82918ms step_avg:61.42ms
step:1351/2330 train_time:82978ms step_avg:61.42ms
step:1352/2330 train_time:83041ms step_avg:61.42ms
step:1353/2330 train_time:83102ms step_avg:61.42ms
step:1354/2330 train_time:83165ms step_avg:61.42ms
step:1355/2330 train_time:83226ms step_avg:61.42ms
step:1356/2330 train_time:83290ms step_avg:61.42ms
step:1357/2330 train_time:83350ms step_avg:61.42ms
step:1358/2330 train_time:83413ms step_avg:61.42ms
step:1359/2330 train_time:83474ms step_avg:61.42ms
step:1360/2330 train_time:83537ms step_avg:61.42ms
step:1361/2330 train_time:83598ms step_avg:61.42ms
step:1362/2330 train_time:83660ms step_avg:61.42ms
step:1363/2330 train_time:83720ms step_avg:61.42ms
step:1364/2330 train_time:83783ms step_avg:61.42ms
step:1365/2330 train_time:83844ms step_avg:61.42ms
step:1366/2330 train_time:83907ms step_avg:61.42ms
step:1367/2330 train_time:83967ms step_avg:61.42ms
step:1368/2330 train_time:84031ms step_avg:61.43ms
step:1369/2330 train_time:84092ms step_avg:61.43ms
step:1370/2330 train_time:84156ms step_avg:61.43ms
step:1371/2330 train_time:84216ms step_avg:61.43ms
step:1372/2330 train_time:84279ms step_avg:61.43ms
step:1373/2330 train_time:84339ms step_avg:61.43ms
step:1374/2330 train_time:84402ms step_avg:61.43ms
step:1375/2330 train_time:84461ms step_avg:61.43ms
step:1376/2330 train_time:84524ms step_avg:61.43ms
step:1377/2330 train_time:84585ms step_avg:61.43ms
step:1378/2330 train_time:84647ms step_avg:61.43ms
step:1379/2330 train_time:84708ms step_avg:61.43ms
step:1380/2330 train_time:84771ms step_avg:61.43ms
step:1381/2330 train_time:84832ms step_avg:61.43ms
step:1382/2330 train_time:84895ms step_avg:61.43ms
step:1383/2330 train_time:84956ms step_avg:61.43ms
step:1384/2330 train_time:85018ms step_avg:61.43ms
step:1385/2330 train_time:85079ms step_avg:61.43ms
step:1386/2330 train_time:85142ms step_avg:61.43ms
step:1387/2330 train_time:85202ms step_avg:61.43ms
step:1388/2330 train_time:85266ms step_avg:61.43ms
step:1389/2330 train_time:85327ms step_avg:61.43ms
step:1390/2330 train_time:85390ms step_avg:61.43ms
step:1391/2330 train_time:85450ms step_avg:61.43ms
step:1392/2330 train_time:85513ms step_avg:61.43ms
step:1393/2330 train_time:85575ms step_avg:61.43ms
step:1394/2330 train_time:85637ms step_avg:61.43ms
step:1395/2330 train_time:85698ms step_avg:61.43ms
step:1396/2330 train_time:85760ms step_avg:61.43ms
step:1397/2330 train_time:85820ms step_avg:61.43ms
step:1398/2330 train_time:85884ms step_avg:61.43ms
step:1399/2330 train_time:85945ms step_avg:61.43ms
step:1400/2330 train_time:86007ms step_avg:61.43ms
step:1401/2330 train_time:86068ms step_avg:61.43ms
step:1402/2330 train_time:86131ms step_avg:61.43ms
step:1403/2330 train_time:86192ms step_avg:61.43ms
step:1404/2330 train_time:86255ms step_avg:61.44ms
step:1405/2330 train_time:86316ms step_avg:61.44ms
step:1406/2330 train_time:86379ms step_avg:61.44ms
step:1407/2330 train_time:86439ms step_avg:61.43ms
step:1408/2330 train_time:86502ms step_avg:61.44ms
step:1409/2330 train_time:86563ms step_avg:61.44ms
step:1410/2330 train_time:86626ms step_avg:61.44ms
step:1411/2330 train_time:86687ms step_avg:61.44ms
step:1412/2330 train_time:86749ms step_avg:61.44ms
step:1413/2330 train_time:86809ms step_avg:61.44ms
step:1414/2330 train_time:86873ms step_avg:61.44ms
step:1415/2330 train_time:86934ms step_avg:61.44ms
step:1416/2330 train_time:86997ms step_avg:61.44ms
step:1417/2330 train_time:87057ms step_avg:61.44ms
step:1418/2330 train_time:87120ms step_avg:61.44ms
step:1419/2330 train_time:87181ms step_avg:61.44ms
step:1420/2330 train_time:87243ms step_avg:61.44ms
step:1421/2330 train_time:87304ms step_avg:61.44ms
step:1422/2330 train_time:87367ms step_avg:61.44ms
step:1423/2330 train_time:87428ms step_avg:61.44ms
step:1424/2330 train_time:87492ms step_avg:61.44ms
step:1425/2330 train_time:87552ms step_avg:61.44ms
step:1426/2330 train_time:87616ms step_avg:61.44ms
step:1427/2330 train_time:87677ms step_avg:61.44ms
step:1428/2330 train_time:87739ms step_avg:61.44ms
step:1429/2330 train_time:87799ms step_avg:61.44ms
step:1430/2330 train_time:87862ms step_avg:61.44ms
step:1431/2330 train_time:87922ms step_avg:61.44ms
step:1432/2330 train_time:87985ms step_avg:61.44ms
step:1433/2330 train_time:88046ms step_avg:61.44ms
step:1434/2330 train_time:88109ms step_avg:61.44ms
step:1435/2330 train_time:88170ms step_avg:61.44ms
step:1436/2330 train_time:88233ms step_avg:61.44ms
step:1437/2330 train_time:88294ms step_avg:61.44ms
step:1438/2330 train_time:88358ms step_avg:61.44ms
step:1439/2330 train_time:88418ms step_avg:61.44ms
step:1440/2330 train_time:88480ms step_avg:61.44ms
step:1441/2330 train_time:88541ms step_avg:61.44ms
step:1442/2330 train_time:88603ms step_avg:61.44ms
step:1443/2330 train_time:88664ms step_avg:61.44ms
step:1444/2330 train_time:88727ms step_avg:61.45ms
step:1445/2330 train_time:88788ms step_avg:61.45ms
step:1446/2330 train_time:88851ms step_avg:61.45ms
step:1447/2330 train_time:88911ms step_avg:61.45ms
step:1448/2330 train_time:88976ms step_avg:61.45ms
step:1449/2330 train_time:89037ms step_avg:61.45ms
step:1450/2330 train_time:89099ms step_avg:61.45ms
step:1451/2330 train_time:89159ms step_avg:61.45ms
step:1452/2330 train_time:89221ms step_avg:61.45ms
step:1453/2330 train_time:89281ms step_avg:61.45ms
step:1454/2330 train_time:89344ms step_avg:61.45ms
step:1455/2330 train_time:89405ms step_avg:61.45ms
step:1456/2330 train_time:89468ms step_avg:61.45ms
step:1457/2330 train_time:89529ms step_avg:61.45ms
step:1458/2330 train_time:89591ms step_avg:61.45ms
step:1459/2330 train_time:89652ms step_avg:61.45ms
step:1460/2330 train_time:89715ms step_avg:61.45ms
step:1461/2330 train_time:89776ms step_avg:61.45ms
step:1462/2330 train_time:89838ms step_avg:61.45ms
step:1463/2330 train_time:89899ms step_avg:61.45ms
step:1464/2330 train_time:89961ms step_avg:61.45ms
step:1465/2330 train_time:90022ms step_avg:61.45ms
step:1466/2330 train_time:90085ms step_avg:61.45ms
step:1467/2330 train_time:90146ms step_avg:61.45ms
step:1468/2330 train_time:90209ms step_avg:61.45ms
step:1469/2330 train_time:90269ms step_avg:61.45ms
step:1470/2330 train_time:90332ms step_avg:61.45ms
step:1471/2330 train_time:90393ms step_avg:61.45ms
step:1472/2330 train_time:90456ms step_avg:61.45ms
step:1473/2330 train_time:90517ms step_avg:61.45ms
step:1474/2330 train_time:90580ms step_avg:61.45ms
step:1475/2330 train_time:90640ms step_avg:61.45ms
step:1476/2330 train_time:90703ms step_avg:61.45ms
step:1477/2330 train_time:90763ms step_avg:61.45ms
step:1478/2330 train_time:90826ms step_avg:61.45ms
step:1479/2330 train_time:90888ms step_avg:61.45ms
step:1480/2330 train_time:90951ms step_avg:61.45ms
step:1481/2330 train_time:91013ms step_avg:61.45ms
step:1482/2330 train_time:91076ms step_avg:61.46ms
step:1483/2330 train_time:91137ms step_avg:61.45ms
step:1484/2330 train_time:91199ms step_avg:61.45ms
step:1485/2330 train_time:91259ms step_avg:61.45ms
step:1486/2330 train_time:91321ms step_avg:61.45ms
step:1487/2330 train_time:91382ms step_avg:61.45ms
step:1488/2330 train_time:91445ms step_avg:61.46ms
step:1489/2330 train_time:91506ms step_avg:61.45ms
step:1490/2330 train_time:91569ms step_avg:61.46ms
step:1491/2330 train_time:91630ms step_avg:61.46ms
step:1492/2330 train_time:91693ms step_avg:61.46ms
step:1493/2330 train_time:91753ms step_avg:61.46ms
step:1494/2330 train_time:91816ms step_avg:61.46ms
step:1495/2330 train_time:91877ms step_avg:61.46ms
step:1496/2330 train_time:91940ms step_avg:61.46ms
step:1497/2330 train_time:92000ms step_avg:61.46ms
step:1498/2330 train_time:92062ms step_avg:61.46ms
step:1499/2330 train_time:92123ms step_avg:61.46ms
step:1500/2330 train_time:92187ms step_avg:61.46ms
step:1500/2330 val_loss:3.6429 train_time:92251ms step_avg:61.50ms
step:1501/2330 train_time:92275ms step_avg:61.48ms
step:1502/2330 train_time:92314ms step_avg:61.46ms
step:1503/2330 train_time:92381ms step_avg:61.46ms
step:1504/2330 train_time:92446ms step_avg:61.47ms
step:1505/2330 train_time:92506ms step_avg:61.47ms
step:1506/2330 train_time:92569ms step_avg:61.47ms
step:1507/2330 train_time:92629ms step_avg:61.47ms
step:1508/2330 train_time:92691ms step_avg:61.47ms
step:1509/2330 train_time:92751ms step_avg:61.46ms
step:1510/2330 train_time:92813ms step_avg:61.47ms
step:1511/2330 train_time:92873ms step_avg:61.46ms
step:1512/2330 train_time:92935ms step_avg:61.47ms
step:1513/2330 train_time:92996ms step_avg:61.46ms
step:1514/2330 train_time:93057ms step_avg:61.46ms
step:1515/2330 train_time:93117ms step_avg:61.46ms
step:1516/2330 train_time:93181ms step_avg:61.46ms
step:1517/2330 train_time:93243ms step_avg:61.47ms
step:1518/2330 train_time:93309ms step_avg:61.47ms
step:1519/2330 train_time:93371ms step_avg:61.47ms
step:1520/2330 train_time:93435ms step_avg:61.47ms
step:1521/2330 train_time:93498ms step_avg:61.47ms
step:1522/2330 train_time:93561ms step_avg:61.47ms
step:1523/2330 train_time:93622ms step_avg:61.47ms
step:1524/2330 train_time:93684ms step_avg:61.47ms
step:1525/2330 train_time:93744ms step_avg:61.47ms
step:1526/2330 train_time:93807ms step_avg:61.47ms
step:1527/2330 train_time:93867ms step_avg:61.47ms
step:1528/2330 train_time:93929ms step_avg:61.47ms
step:1529/2330 train_time:93990ms step_avg:61.47ms
step:1530/2330 train_time:94053ms step_avg:61.47ms
step:1531/2330 train_time:94115ms step_avg:61.47ms
step:1532/2330 train_time:94178ms step_avg:61.47ms
step:1533/2330 train_time:94240ms step_avg:61.47ms
step:1534/2330 train_time:94304ms step_avg:61.48ms
step:1535/2330 train_time:94366ms step_avg:61.48ms
step:1536/2330 train_time:94430ms step_avg:61.48ms
step:1537/2330 train_time:94491ms step_avg:61.48ms
step:1538/2330 train_time:94555ms step_avg:61.48ms
step:1539/2330 train_time:94617ms step_avg:61.48ms
step:1540/2330 train_time:94680ms step_avg:61.48ms
step:1541/2330 train_time:94742ms step_avg:61.48ms
step:1542/2330 train_time:94806ms step_avg:61.48ms
step:1543/2330 train_time:94867ms step_avg:61.48ms
step:1544/2330 train_time:94929ms step_avg:61.48ms
step:1545/2330 train_time:94989ms step_avg:61.48ms
step:1546/2330 train_time:95053ms step_avg:61.48ms
step:1547/2330 train_time:95114ms step_avg:61.48ms
step:1548/2330 train_time:95178ms step_avg:61.48ms
step:1549/2330 train_time:95239ms step_avg:61.48ms
step:1550/2330 train_time:95304ms step_avg:61.49ms
step:1551/2330 train_time:95365ms step_avg:61.49ms
step:1552/2330 train_time:95428ms step_avg:61.49ms
step:1553/2330 train_time:95489ms step_avg:61.49ms
step:1554/2330 train_time:95552ms step_avg:61.49ms
step:1555/2330 train_time:95614ms step_avg:61.49ms
step:1556/2330 train_time:95678ms step_avg:61.49ms
step:1557/2330 train_time:95739ms step_avg:61.49ms
step:1558/2330 train_time:95803ms step_avg:61.49ms
step:1559/2330 train_time:95864ms step_avg:61.49ms
step:1560/2330 train_time:95927ms step_avg:61.49ms
step:1561/2330 train_time:95987ms step_avg:61.49ms
step:1562/2330 train_time:96050ms step_avg:61.49ms
step:1563/2330 train_time:96112ms step_avg:61.49ms
step:1564/2330 train_time:96175ms step_avg:61.49ms
step:1565/2330 train_time:96236ms step_avg:61.49ms
step:1566/2330 train_time:96299ms step_avg:61.49ms
step:1567/2330 train_time:96360ms step_avg:61.49ms
step:1568/2330 train_time:96423ms step_avg:61.49ms
step:1569/2330 train_time:96484ms step_avg:61.49ms
step:1570/2330 train_time:96548ms step_avg:61.50ms
step:1571/2330 train_time:96609ms step_avg:61.50ms
step:1572/2330 train_time:96673ms step_avg:61.50ms
step:1573/2330 train_time:96734ms step_avg:61.50ms
step:1574/2330 train_time:96798ms step_avg:61.50ms
step:1575/2330 train_time:96860ms step_avg:61.50ms
step:1576/2330 train_time:96923ms step_avg:61.50ms
step:1577/2330 train_time:96984ms step_avg:61.50ms
step:1578/2330 train_time:97047ms step_avg:61.50ms
step:1579/2330 train_time:97108ms step_avg:61.50ms
step:1580/2330 train_time:97171ms step_avg:61.50ms
step:1581/2330 train_time:97233ms step_avg:61.50ms
step:1582/2330 train_time:97296ms step_avg:61.50ms
step:1583/2330 train_time:97357ms step_avg:61.50ms
step:1584/2330 train_time:97420ms step_avg:61.50ms
step:1585/2330 train_time:97482ms step_avg:61.50ms
step:1586/2330 train_time:97546ms step_avg:61.50ms
step:1587/2330 train_time:97607ms step_avg:61.50ms
step:1588/2330 train_time:97671ms step_avg:61.51ms
step:1589/2330 train_time:97733ms step_avg:61.51ms
step:1590/2330 train_time:97798ms step_avg:61.51ms
step:1591/2330 train_time:97859ms step_avg:61.51ms
step:1592/2330 train_time:97923ms step_avg:61.51ms
step:1593/2330 train_time:97984ms step_avg:61.51ms
step:1594/2330 train_time:98047ms step_avg:61.51ms
step:1595/2330 train_time:98109ms step_avg:61.51ms
step:1596/2330 train_time:98172ms step_avg:61.51ms
step:1597/2330 train_time:98233ms step_avg:61.51ms
step:1598/2330 train_time:98296ms step_avg:61.51ms
step:1599/2330 train_time:98357ms step_avg:61.51ms
step:1600/2330 train_time:98421ms step_avg:61.51ms
step:1601/2330 train_time:98483ms step_avg:61.51ms
step:1602/2330 train_time:98546ms step_avg:61.51ms
step:1603/2330 train_time:98607ms step_avg:61.51ms
step:1604/2330 train_time:98671ms step_avg:61.52ms
step:1605/2330 train_time:98733ms step_avg:61.52ms
step:1606/2330 train_time:98797ms step_avg:61.52ms
step:1607/2330 train_time:98858ms step_avg:61.52ms
step:1608/2330 train_time:98922ms step_avg:61.52ms
step:1609/2330 train_time:98983ms step_avg:61.52ms
step:1610/2330 train_time:99047ms step_avg:61.52ms
step:1611/2330 train_time:99108ms step_avg:61.52ms
step:1612/2330 train_time:99171ms step_avg:61.52ms
step:1613/2330 train_time:99231ms step_avg:61.52ms
step:1614/2330 train_time:99295ms step_avg:61.52ms
step:1615/2330 train_time:99357ms step_avg:61.52ms
step:1616/2330 train_time:99421ms step_avg:61.52ms
step:1617/2330 train_time:99483ms step_avg:61.52ms
step:1618/2330 train_time:99547ms step_avg:61.52ms
step:1619/2330 train_time:99608ms step_avg:61.52ms
step:1620/2330 train_time:99671ms step_avg:61.53ms
step:1621/2330 train_time:99733ms step_avg:61.53ms
step:1622/2330 train_time:99797ms step_avg:61.53ms
step:1623/2330 train_time:99859ms step_avg:61.53ms
step:1624/2330 train_time:99922ms step_avg:61.53ms
step:1625/2330 train_time:99984ms step_avg:61.53ms
step:1626/2330 train_time:100047ms step_avg:61.53ms
step:1627/2330 train_time:100108ms step_avg:61.53ms
step:1628/2330 train_time:100171ms step_avg:61.53ms
step:1629/2330 train_time:100232ms step_avg:61.53ms
step:1630/2330 train_time:100295ms step_avg:61.53ms
step:1631/2330 train_time:100357ms step_avg:61.53ms
step:1632/2330 train_time:100421ms step_avg:61.53ms
step:1633/2330 train_time:100482ms step_avg:61.53ms
step:1634/2330 train_time:100545ms step_avg:61.53ms
step:1635/2330 train_time:100606ms step_avg:61.53ms
step:1636/2330 train_time:100669ms step_avg:61.53ms
step:1637/2330 train_time:100731ms step_avg:61.53ms
step:1638/2330 train_time:100795ms step_avg:61.54ms
step:1639/2330 train_time:100856ms step_avg:61.54ms
step:1640/2330 train_time:100920ms step_avg:61.54ms
step:1641/2330 train_time:100981ms step_avg:61.54ms
step:1642/2330 train_time:101045ms step_avg:61.54ms
step:1643/2330 train_time:101106ms step_avg:61.54ms
step:1644/2330 train_time:101169ms step_avg:61.54ms
step:1645/2330 train_time:101230ms step_avg:61.54ms
step:1646/2330 train_time:101294ms step_avg:61.54ms
step:1647/2330 train_time:101355ms step_avg:61.54ms
step:1648/2330 train_time:101420ms step_avg:61.54ms
step:1649/2330 train_time:101481ms step_avg:61.54ms
step:1650/2330 train_time:101544ms step_avg:61.54ms
step:1651/2330 train_time:101606ms step_avg:61.54ms
step:1652/2330 train_time:101669ms step_avg:61.54ms
step:1653/2330 train_time:101730ms step_avg:61.54ms
step:1654/2330 train_time:101794ms step_avg:61.54ms
step:1655/2330 train_time:101856ms step_avg:61.54ms
step:1656/2330 train_time:101919ms step_avg:61.55ms
step:1657/2330 train_time:101981ms step_avg:61.55ms
step:1658/2330 train_time:102045ms step_avg:61.55ms
step:1659/2330 train_time:102106ms step_avg:61.55ms
step:1660/2330 train_time:102169ms step_avg:61.55ms
step:1661/2330 train_time:102230ms step_avg:61.55ms
step:1662/2330 train_time:102293ms step_avg:61.55ms
step:1663/2330 train_time:102355ms step_avg:61.55ms
step:1664/2330 train_time:102419ms step_avg:61.55ms
step:1665/2330 train_time:102481ms step_avg:61.55ms
step:1666/2330 train_time:102544ms step_avg:61.55ms
step:1667/2330 train_time:102606ms step_avg:61.55ms
step:1668/2330 train_time:102669ms step_avg:61.55ms
step:1669/2330 train_time:102730ms step_avg:61.55ms
step:1670/2330 train_time:102793ms step_avg:61.55ms
step:1671/2330 train_time:102855ms step_avg:61.55ms
step:1672/2330 train_time:102919ms step_avg:61.55ms
step:1673/2330 train_time:102981ms step_avg:61.55ms
step:1674/2330 train_time:103044ms step_avg:61.56ms
step:1675/2330 train_time:103105ms step_avg:61.56ms
step:1676/2330 train_time:103168ms step_avg:61.56ms
step:1677/2330 train_time:103229ms step_avg:61.56ms
step:1678/2330 train_time:103292ms step_avg:61.56ms
step:1679/2330 train_time:103355ms step_avg:61.56ms
step:1680/2330 train_time:103419ms step_avg:61.56ms
step:1681/2330 train_time:103480ms step_avg:61.56ms
step:1682/2330 train_time:103544ms step_avg:61.56ms
step:1683/2330 train_time:103605ms step_avg:61.56ms
step:1684/2330 train_time:103668ms step_avg:61.56ms
step:1685/2330 train_time:103729ms step_avg:61.56ms
step:1686/2330 train_time:103793ms step_avg:61.56ms
step:1687/2330 train_time:103856ms step_avg:61.56ms
step:1688/2330 train_time:103920ms step_avg:61.56ms
step:1689/2330 train_time:103982ms step_avg:61.56ms
step:1690/2330 train_time:104046ms step_avg:61.57ms
step:1691/2330 train_time:104107ms step_avg:61.57ms
step:1692/2330 train_time:104170ms step_avg:61.57ms
step:1693/2330 train_time:104231ms step_avg:61.57ms
step:1694/2330 train_time:104294ms step_avg:61.57ms
step:1695/2330 train_time:104355ms step_avg:61.57ms
step:1696/2330 train_time:104418ms step_avg:61.57ms
step:1697/2330 train_time:104479ms step_avg:61.57ms
step:1698/2330 train_time:104543ms step_avg:61.57ms
step:1699/2330 train_time:104604ms step_avg:61.57ms
step:1700/2330 train_time:104667ms step_avg:61.57ms
step:1701/2330 train_time:104728ms step_avg:61.57ms
step:1702/2330 train_time:104793ms step_avg:61.57ms
step:1703/2330 train_time:104854ms step_avg:61.57ms
step:1704/2330 train_time:104917ms step_avg:61.57ms
step:1705/2330 train_time:104979ms step_avg:61.57ms
step:1706/2330 train_time:105042ms step_avg:61.57ms
step:1707/2330 train_time:105104ms step_avg:61.57ms
step:1708/2330 train_time:105167ms step_avg:61.57ms
step:1709/2330 train_time:105227ms step_avg:61.57ms
step:1710/2330 train_time:105290ms step_avg:61.57ms
step:1711/2330 train_time:105352ms step_avg:61.57ms
step:1712/2330 train_time:105416ms step_avg:61.57ms
step:1713/2330 train_time:105478ms step_avg:61.57ms
step:1714/2330 train_time:105542ms step_avg:61.58ms
step:1715/2330 train_time:105603ms step_avg:61.58ms
step:1716/2330 train_time:105666ms step_avg:61.58ms
step:1717/2330 train_time:105727ms step_avg:61.58ms
step:1718/2330 train_time:105790ms step_avg:61.58ms
step:1719/2330 train_time:105851ms step_avg:61.58ms
step:1720/2330 train_time:105916ms step_avg:61.58ms
step:1721/2330 train_time:105977ms step_avg:61.58ms
step:1722/2330 train_time:106041ms step_avg:61.58ms
step:1723/2330 train_time:106103ms step_avg:61.58ms
step:1724/2330 train_time:106166ms step_avg:61.58ms
step:1725/2330 train_time:106227ms step_avg:61.58ms
step:1726/2330 train_time:106289ms step_avg:61.58ms
step:1727/2330 train_time:106351ms step_avg:61.58ms
step:1728/2330 train_time:106415ms step_avg:61.58ms
step:1729/2330 train_time:106477ms step_avg:61.58ms
step:1730/2330 train_time:106540ms step_avg:61.58ms
step:1731/2330 train_time:106601ms step_avg:61.58ms
step:1732/2330 train_time:106664ms step_avg:61.58ms
step:1733/2330 train_time:106725ms step_avg:61.58ms
step:1734/2330 train_time:106789ms step_avg:61.59ms
step:1735/2330 train_time:106850ms step_avg:61.58ms
step:1736/2330 train_time:106914ms step_avg:61.59ms
step:1737/2330 train_time:106976ms step_avg:61.59ms
step:1738/2330 train_time:107039ms step_avg:61.59ms
step:1739/2330 train_time:107101ms step_avg:61.59ms
step:1740/2330 train_time:107165ms step_avg:61.59ms
step:1741/2330 train_time:107226ms step_avg:61.59ms
step:1742/2330 train_time:107289ms step_avg:61.59ms
step:1743/2330 train_time:107350ms step_avg:61.59ms
step:1744/2330 train_time:107413ms step_avg:61.59ms
step:1745/2330 train_time:107474ms step_avg:61.59ms
step:1746/2330 train_time:107538ms step_avg:61.59ms
step:1747/2330 train_time:107600ms step_avg:61.59ms
step:1748/2330 train_time:107663ms step_avg:61.59ms
step:1749/2330 train_time:107724ms step_avg:61.59ms
step:1750/2330 train_time:107787ms step_avg:61.59ms
step:1750/2330 val_loss:3.5706 train_time:107852ms step_avg:61.63ms
step:1751/2330 train_time:107875ms step_avg:61.61ms
step:1752/2330 train_time:107915ms step_avg:61.60ms
step:1753/2330 train_time:107986ms step_avg:61.60ms
step:1754/2330 train_time:108055ms step_avg:61.60ms
step:1755/2330 train_time:108117ms step_avg:61.61ms
step:1756/2330 train_time:108179ms step_avg:61.61ms
step:1757/2330 train_time:108240ms step_avg:61.60ms
step:1758/2330 train_time:108302ms step_avg:61.61ms
step:1759/2330 train_time:108362ms step_avg:61.60ms
step:1760/2330 train_time:108424ms step_avg:61.60ms
step:1761/2330 train_time:108485ms step_avg:61.60ms
step:1762/2330 train_time:108548ms step_avg:61.60ms
step:1763/2330 train_time:108608ms step_avg:61.60ms
step:1764/2330 train_time:108670ms step_avg:61.60ms
step:1765/2330 train_time:108731ms step_avg:61.60ms
step:1766/2330 train_time:108796ms step_avg:61.61ms
step:1767/2330 train_time:108858ms step_avg:61.61ms
step:1768/2330 train_time:108923ms step_avg:61.61ms
step:1769/2330 train_time:108987ms step_avg:61.61ms
step:1770/2330 train_time:109053ms step_avg:61.61ms
step:1771/2330 train_time:109116ms step_avg:61.61ms
step:1772/2330 train_time:109179ms step_avg:61.61ms
step:1773/2330 train_time:109239ms step_avg:61.61ms
step:1774/2330 train_time:109302ms step_avg:61.61ms
step:1775/2330 train_time:109362ms step_avg:61.61ms
step:1776/2330 train_time:109425ms step_avg:61.61ms
step:1777/2330 train_time:109485ms step_avg:61.61ms
step:1778/2330 train_time:109548ms step_avg:61.61ms
step:1779/2330 train_time:109609ms step_avg:61.61ms
step:1780/2330 train_time:109672ms step_avg:61.61ms
step:1781/2330 train_time:109734ms step_avg:61.61ms
step:1782/2330 train_time:109797ms step_avg:61.61ms
step:1783/2330 train_time:109860ms step_avg:61.62ms
step:1784/2330 train_time:109925ms step_avg:61.62ms
step:1785/2330 train_time:109988ms step_avg:61.62ms
step:1786/2330 train_time:110053ms step_avg:61.62ms
step:1787/2330 train_time:110115ms step_avg:61.62ms
step:1788/2330 train_time:110179ms step_avg:61.62ms
step:1789/2330 train_time:110240ms step_avg:61.62ms
step:1790/2330 train_time:110303ms step_avg:61.62ms
step:1791/2330 train_time:110364ms step_avg:61.62ms
step:1792/2330 train_time:110426ms step_avg:61.62ms
step:1793/2330 train_time:110486ms step_avg:61.62ms
step:1794/2330 train_time:110549ms step_avg:61.62ms
step:1795/2330 train_time:110610ms step_avg:61.62ms
step:1796/2330 train_time:110673ms step_avg:61.62ms
step:1797/2330 train_time:110734ms step_avg:61.62ms
step:1798/2330 train_time:110798ms step_avg:61.62ms
step:1799/2330 train_time:110859ms step_avg:61.62ms
step:1800/2330 train_time:110924ms step_avg:61.62ms
step:1801/2330 train_time:110987ms step_avg:61.63ms
step:1802/2330 train_time:111052ms step_avg:61.63ms
step:1803/2330 train_time:111114ms step_avg:61.63ms
step:1804/2330 train_time:111178ms step_avg:61.63ms
step:1805/2330 train_time:111240ms step_avg:61.63ms
step:1806/2330 train_time:111303ms step_avg:61.63ms
step:1807/2330 train_time:111365ms step_avg:61.63ms
step:1808/2330 train_time:111428ms step_avg:61.63ms
step:1809/2330 train_time:111488ms step_avg:61.63ms
step:1810/2330 train_time:111551ms step_avg:61.63ms
step:1811/2330 train_time:111612ms step_avg:61.63ms
step:1812/2330 train_time:111675ms step_avg:61.63ms
step:1813/2330 train_time:111737ms step_avg:61.63ms
step:1814/2330 train_time:111801ms step_avg:61.63ms
step:1815/2330 train_time:111863ms step_avg:61.63ms
step:1816/2330 train_time:111926ms step_avg:61.63ms
step:1817/2330 train_time:111988ms step_avg:61.63ms
step:1818/2330 train_time:112053ms step_avg:61.64ms
step:1819/2330 train_time:112114ms step_avg:61.64ms
step:1820/2330 train_time:112178ms step_avg:61.64ms
step:1821/2330 train_time:112239ms step_avg:61.64ms
step:1822/2330 train_time:112302ms step_avg:61.64ms
step:1823/2330 train_time:112363ms step_avg:61.64ms
step:1824/2330 train_time:112426ms step_avg:61.64ms
step:1825/2330 train_time:112486ms step_avg:61.64ms
step:1826/2330 train_time:112550ms step_avg:61.64ms
step:1827/2330 train_time:112612ms step_avg:61.64ms
step:1828/2330 train_time:112675ms step_avg:61.64ms
step:1829/2330 train_time:112736ms step_avg:61.64ms
step:1830/2330 train_time:112801ms step_avg:61.64ms
step:1831/2330 train_time:112862ms step_avg:61.64ms
step:1832/2330 train_time:112925ms step_avg:61.64ms
step:1833/2330 train_time:112987ms step_avg:61.64ms
step:1834/2330 train_time:113052ms step_avg:61.64ms
step:1835/2330 train_time:113114ms step_avg:61.64ms
step:1836/2330 train_time:113178ms step_avg:61.64ms
step:1837/2330 train_time:113239ms step_avg:61.64ms
step:1838/2330 train_time:113303ms step_avg:61.64ms
step:1839/2330 train_time:113364ms step_avg:61.64ms
step:1840/2330 train_time:113427ms step_avg:61.65ms
step:1841/2330 train_time:113488ms step_avg:61.64ms
step:1842/2330 train_time:113551ms step_avg:61.65ms
step:1843/2330 train_time:113613ms step_avg:61.65ms
step:1844/2330 train_time:113676ms step_avg:61.65ms
step:1845/2330 train_time:113738ms step_avg:61.65ms
step:1846/2330 train_time:113802ms step_avg:61.65ms
step:1847/2330 train_time:113863ms step_avg:61.65ms
step:1848/2330 train_time:113926ms step_avg:61.65ms
step:1849/2330 train_time:113988ms step_avg:61.65ms
step:1850/2330 train_time:114052ms step_avg:61.65ms
step:1851/2330 train_time:114113ms step_avg:61.65ms
step:1852/2330 train_time:114177ms step_avg:61.65ms
step:1853/2330 train_time:114239ms step_avg:61.65ms
step:1854/2330 train_time:114302ms step_avg:61.65ms
step:1855/2330 train_time:114363ms step_avg:61.65ms
step:1856/2330 train_time:114426ms step_avg:61.65ms
step:1857/2330 train_time:114486ms step_avg:61.65ms
step:1858/2330 train_time:114550ms step_avg:61.65ms
step:1859/2330 train_time:114612ms step_avg:61.65ms
step:1860/2330 train_time:114675ms step_avg:61.65ms
step:1861/2330 train_time:114737ms step_avg:61.65ms
step:1862/2330 train_time:114801ms step_avg:61.65ms
step:1863/2330 train_time:114861ms step_avg:61.65ms
step:1864/2330 train_time:114925ms step_avg:61.65ms
step:1865/2330 train_time:114988ms step_avg:61.66ms
step:1866/2330 train_time:115052ms step_avg:61.66ms
step:1867/2330 train_time:115114ms step_avg:61.66ms
step:1868/2330 train_time:115177ms step_avg:61.66ms
step:1869/2330 train_time:115239ms step_avg:61.66ms
step:1870/2330 train_time:115302ms step_avg:61.66ms
step:1871/2330 train_time:115363ms step_avg:61.66ms
step:1872/2330 train_time:115426ms step_avg:61.66ms
step:1873/2330 train_time:115487ms step_avg:61.66ms
step:1874/2330 train_time:115551ms step_avg:61.66ms
step:1875/2330 train_time:115613ms step_avg:61.66ms
step:1876/2330 train_time:115676ms step_avg:61.66ms
step:1877/2330 train_time:115737ms step_avg:61.66ms
step:1878/2330 train_time:115801ms step_avg:61.66ms
step:1879/2330 train_time:115862ms step_avg:61.66ms
step:1880/2330 train_time:115925ms step_avg:61.66ms
step:1881/2330 train_time:115988ms step_avg:61.66ms
step:1882/2330 train_time:116051ms step_avg:61.66ms
step:1883/2330 train_time:116113ms step_avg:61.66ms
step:1884/2330 train_time:116177ms step_avg:61.66ms
step:1885/2330 train_time:116238ms step_avg:61.66ms
step:1886/2330 train_time:116302ms step_avg:61.67ms
step:1887/2330 train_time:116363ms step_avg:61.67ms
step:1888/2330 train_time:116426ms step_avg:61.67ms
step:1889/2330 train_time:116487ms step_avg:61.67ms
step:1890/2330 train_time:116551ms step_avg:61.67ms
step:1891/2330 train_time:116612ms step_avg:61.67ms
step:1892/2330 train_time:116676ms step_avg:61.67ms
step:1893/2330 train_time:116737ms step_avg:61.67ms
step:1894/2330 train_time:116801ms step_avg:61.67ms
step:1895/2330 train_time:116861ms step_avg:61.67ms
step:1896/2330 train_time:116925ms step_avg:61.67ms
step:1897/2330 train_time:116986ms step_avg:61.67ms
step:1898/2330 train_time:117050ms step_avg:61.67ms
step:1899/2330 train_time:117112ms step_avg:61.67ms
step:1900/2330 train_time:117176ms step_avg:61.67ms
step:1901/2330 train_time:117242ms step_avg:61.67ms
step:1902/2330 train_time:117302ms step_avg:61.67ms
step:1903/2330 train_time:117363ms step_avg:61.67ms
step:1904/2330 train_time:117426ms step_avg:61.67ms
step:1905/2330 train_time:117486ms step_avg:61.67ms
step:1906/2330 train_time:117550ms step_avg:61.67ms
step:1907/2330 train_time:117612ms step_avg:61.67ms
step:1908/2330 train_time:117675ms step_avg:61.67ms
step:1909/2330 train_time:117737ms step_avg:61.67ms
step:1910/2330 train_time:117800ms step_avg:61.68ms
step:1911/2330 train_time:117861ms step_avg:61.68ms
step:1912/2330 train_time:117924ms step_avg:61.68ms
step:1913/2330 train_time:117987ms step_avg:61.68ms
step:1914/2330 train_time:118051ms step_avg:61.68ms
step:1915/2330 train_time:118113ms step_avg:61.68ms
step:1916/2330 train_time:118176ms step_avg:61.68ms
step:1917/2330 train_time:118237ms step_avg:61.68ms
step:1918/2330 train_time:118301ms step_avg:61.68ms
step:1919/2330 train_time:118362ms step_avg:61.68ms
step:1920/2330 train_time:118425ms step_avg:61.68ms
step:1921/2330 train_time:118487ms step_avg:61.68ms
step:1922/2330 train_time:118550ms step_avg:61.68ms
step:1923/2330 train_time:118612ms step_avg:61.68ms
step:1924/2330 train_time:118675ms step_avg:61.68ms
step:1925/2330 train_time:118736ms step_avg:61.68ms
step:1926/2330 train_time:118800ms step_avg:61.68ms
step:1927/2330 train_time:118861ms step_avg:61.68ms
step:1928/2330 train_time:118925ms step_avg:61.68ms
step:1929/2330 train_time:118987ms step_avg:61.68ms
step:1930/2330 train_time:119051ms step_avg:61.68ms
step:1931/2330 train_time:119113ms step_avg:61.68ms
step:1932/2330 train_time:119177ms step_avg:61.69ms
step:1933/2330 train_time:119238ms step_avg:61.69ms
step:1934/2330 train_time:119302ms step_avg:61.69ms
step:1935/2330 train_time:119363ms step_avg:61.69ms
step:1936/2330 train_time:119427ms step_avg:61.69ms
step:1937/2330 train_time:119488ms step_avg:61.69ms
step:1938/2330 train_time:119553ms step_avg:61.69ms
step:1939/2330 train_time:119614ms step_avg:61.69ms
step:1940/2330 train_time:119676ms step_avg:61.69ms
step:1941/2330 train_time:119738ms step_avg:61.69ms
step:1942/2330 train_time:119802ms step_avg:61.69ms
step:1943/2330 train_time:119863ms step_avg:61.69ms
step:1944/2330 train_time:119926ms step_avg:61.69ms
step:1945/2330 train_time:119987ms step_avg:61.69ms
step:1946/2330 train_time:120050ms step_avg:61.69ms
step:1947/2330 train_time:120112ms step_avg:61.69ms
step:1948/2330 train_time:120175ms step_avg:61.69ms
step:1949/2330 train_time:120237ms step_avg:61.69ms
step:1950/2330 train_time:120301ms step_avg:61.69ms
step:1951/2330 train_time:120362ms step_avg:61.69ms
step:1952/2330 train_time:120425ms step_avg:61.69ms
step:1953/2330 train_time:120486ms step_avg:61.69ms
step:1954/2330 train_time:120549ms step_avg:61.69ms
step:1955/2330 train_time:120611ms step_avg:61.69ms
step:1956/2330 train_time:120676ms step_avg:61.70ms
step:1957/2330 train_time:120737ms step_avg:61.70ms
step:1958/2330 train_time:120801ms step_avg:61.70ms
step:1959/2330 train_time:120863ms step_avg:61.70ms
step:1960/2330 train_time:120926ms step_avg:61.70ms
step:1961/2330 train_time:120987ms step_avg:61.70ms
step:1962/2330 train_time:121050ms step_avg:61.70ms
step:1963/2330 train_time:121112ms step_avg:61.70ms
step:1964/2330 train_time:121175ms step_avg:61.70ms
step:1965/2330 train_time:121237ms step_avg:61.70ms
step:1966/2330 train_time:121301ms step_avg:61.70ms
step:1967/2330 train_time:121362ms step_avg:61.70ms
step:1968/2330 train_time:121425ms step_avg:61.70ms
step:1969/2330 train_time:121486ms step_avg:61.70ms
step:1970/2330 train_time:121550ms step_avg:61.70ms
step:1971/2330 train_time:121612ms step_avg:61.70ms
step:1972/2330 train_time:121675ms step_avg:61.70ms
step:1973/2330 train_time:121737ms step_avg:61.70ms
step:1974/2330 train_time:121801ms step_avg:61.70ms
step:1975/2330 train_time:121861ms step_avg:61.70ms
step:1976/2330 train_time:121925ms step_avg:61.70ms
step:1977/2330 train_time:121986ms step_avg:61.70ms
step:1978/2330 train_time:122050ms step_avg:61.70ms
step:1979/2330 train_time:122112ms step_avg:61.70ms
step:1980/2330 train_time:122175ms step_avg:61.70ms
step:1981/2330 train_time:122237ms step_avg:61.70ms
step:1982/2330 train_time:122300ms step_avg:61.71ms
step:1983/2330 train_time:122361ms step_avg:61.71ms
step:1984/2330 train_time:122424ms step_avg:61.71ms
step:1985/2330 train_time:122487ms step_avg:61.71ms
step:1986/2330 train_time:122551ms step_avg:61.71ms
step:1987/2330 train_time:122614ms step_avg:61.71ms
step:1988/2330 train_time:122677ms step_avg:61.71ms
step:1989/2330 train_time:122738ms step_avg:61.71ms
step:1990/2330 train_time:122802ms step_avg:61.71ms
step:1991/2330 train_time:122863ms step_avg:61.71ms
step:1992/2330 train_time:122926ms step_avg:61.71ms
step:1993/2330 train_time:122987ms step_avg:61.71ms
step:1994/2330 train_time:123051ms step_avg:61.71ms
step:1995/2330 train_time:123114ms step_avg:61.71ms
step:1996/2330 train_time:123177ms step_avg:61.71ms
step:1997/2330 train_time:123239ms step_avg:61.71ms
step:1998/2330 train_time:123303ms step_avg:61.71ms
step:1999/2330 train_time:123363ms step_avg:61.71ms
step:2000/2330 train_time:123427ms step_avg:61.71ms
step:2000/2330 val_loss:3.5208 train_time:123492ms step_avg:61.75ms
step:2001/2330 train_time:123515ms step_avg:61.73ms
step:2002/2330 train_time:123557ms step_avg:61.72ms
step:2003/2330 train_time:123622ms step_avg:61.72ms
step:2004/2330 train_time:123686ms step_avg:61.72ms
step:2005/2330 train_time:123748ms step_avg:61.72ms
step:2006/2330 train_time:123811ms step_avg:61.72ms
step:2007/2330 train_time:123872ms step_avg:61.72ms
step:2008/2330 train_time:123935ms step_avg:61.72ms
step:2009/2330 train_time:123996ms step_avg:61.72ms
step:2010/2330 train_time:124058ms step_avg:61.72ms
step:2011/2330 train_time:124119ms step_avg:61.72ms
step:2012/2330 train_time:124182ms step_avg:61.72ms
step:2013/2330 train_time:124242ms step_avg:61.72ms
step:2014/2330 train_time:124304ms step_avg:61.72ms
step:2015/2330 train_time:124365ms step_avg:61.72ms
step:2016/2330 train_time:124428ms step_avg:61.72ms
step:2017/2330 train_time:124493ms step_avg:61.72ms
step:2018/2330 train_time:124559ms step_avg:61.72ms
step:2019/2330 train_time:124622ms step_avg:61.72ms
step:2020/2330 train_time:124685ms step_avg:61.73ms
step:2021/2330 train_time:124747ms step_avg:61.73ms
step:2022/2330 train_time:124810ms step_avg:61.73ms
step:2023/2330 train_time:124871ms step_avg:61.73ms
step:2024/2330 train_time:124934ms step_avg:61.73ms
step:2025/2330 train_time:124995ms step_avg:61.73ms
step:2026/2330 train_time:125058ms step_avg:61.73ms
step:2027/2330 train_time:125119ms step_avg:61.73ms
step:2028/2330 train_time:125182ms step_avg:61.73ms
step:2029/2330 train_time:125242ms step_avg:61.73ms
step:2030/2330 train_time:125304ms step_avg:61.73ms
step:2031/2330 train_time:125365ms step_avg:61.73ms
step:2032/2330 train_time:125429ms step_avg:61.73ms
step:2033/2330 train_time:125492ms step_avg:61.73ms
step:2034/2330 train_time:125557ms step_avg:61.73ms
step:2035/2330 train_time:125618ms step_avg:61.73ms
step:2036/2330 train_time:125681ms step_avg:61.73ms
step:2037/2330 train_time:125743ms step_avg:61.73ms
step:2038/2330 train_time:125806ms step_avg:61.73ms
step:2039/2330 train_time:125868ms step_avg:61.73ms
step:2040/2330 train_time:125932ms step_avg:61.73ms
step:2041/2330 train_time:125994ms step_avg:61.73ms
step:2042/2330 train_time:126058ms step_avg:61.73ms
step:2043/2330 train_time:126118ms step_avg:61.73ms
step:2044/2330 train_time:126182ms step_avg:61.73ms
step:2045/2330 train_time:126242ms step_avg:61.73ms
step:2046/2330 train_time:126305ms step_avg:61.73ms
step:2047/2330 train_time:126366ms step_avg:61.73ms
step:2048/2330 train_time:126429ms step_avg:61.73ms
step:2049/2330 train_time:126492ms step_avg:61.73ms
step:2050/2330 train_time:126555ms step_avg:61.73ms
step:2051/2330 train_time:126617ms step_avg:61.73ms
step:2052/2330 train_time:126681ms step_avg:61.74ms
step:2053/2330 train_time:126743ms step_avg:61.74ms
step:2054/2330 train_time:126806ms step_avg:61.74ms
step:2055/2330 train_time:126868ms step_avg:61.74ms
step:2056/2330 train_time:126933ms step_avg:61.74ms
step:2057/2330 train_time:126995ms step_avg:61.74ms
step:2058/2330 train_time:127058ms step_avg:61.74ms
step:2059/2330 train_time:127120ms step_avg:61.74ms
step:2060/2330 train_time:127183ms step_avg:61.74ms
step:2061/2330 train_time:127243ms step_avg:61.74ms
step:2062/2330 train_time:127306ms step_avg:61.74ms
step:2063/2330 train_time:127366ms step_avg:61.74ms
step:2064/2330 train_time:127430ms step_avg:61.74ms
step:2065/2330 train_time:127493ms step_avg:61.74ms
step:2066/2330 train_time:127556ms step_avg:61.74ms
step:2067/2330 train_time:127618ms step_avg:61.74ms
step:2068/2330 train_time:127682ms step_avg:61.74ms
step:2069/2330 train_time:127743ms step_avg:61.74ms
step:2070/2330 train_time:127806ms step_avg:61.74ms
step:2071/2330 train_time:127868ms step_avg:61.74ms
step:2072/2330 train_time:127932ms step_avg:61.74ms
step:2073/2330 train_time:127994ms step_avg:61.74ms
step:2074/2330 train_time:128057ms step_avg:61.74ms
step:2075/2330 train_time:128119ms step_avg:61.74ms
step:2076/2330 train_time:128182ms step_avg:61.74ms
step:2077/2330 train_time:128243ms step_avg:61.74ms
step:2078/2330 train_time:128306ms step_avg:61.74ms
step:2079/2330 train_time:128367ms step_avg:61.74ms
step:2080/2330 train_time:128431ms step_avg:61.75ms
step:2081/2330 train_time:128493ms step_avg:61.75ms
step:2082/2330 train_time:128557ms step_avg:61.75ms
step:2083/2330 train_time:128618ms step_avg:61.75ms
step:2084/2330 train_time:128681ms step_avg:61.75ms
step:2085/2330 train_time:128743ms step_avg:61.75ms
step:2086/2330 train_time:128806ms step_avg:61.75ms
step:2087/2330 train_time:128868ms step_avg:61.75ms
step:2088/2330 train_time:128932ms step_avg:61.75ms
step:2089/2330 train_time:128995ms step_avg:61.75ms
step:2090/2330 train_time:129059ms step_avg:61.75ms
step:2091/2330 train_time:129121ms step_avg:61.75ms
step:2092/2330 train_time:129184ms step_avg:61.75ms
step:2093/2330 train_time:129245ms step_avg:61.75ms
step:2094/2330 train_time:129309ms step_avg:61.75ms
step:2095/2330 train_time:129370ms step_avg:61.75ms
step:2096/2330 train_time:129434ms step_avg:61.75ms
step:2097/2330 train_time:129496ms step_avg:61.75ms
step:2098/2330 train_time:129560ms step_avg:61.75ms
step:2099/2330 train_time:129621ms step_avg:61.75ms
step:2100/2330 train_time:129684ms step_avg:61.75ms
step:2101/2330 train_time:129746ms step_avg:61.75ms
step:2102/2330 train_time:129809ms step_avg:61.76ms
step:2103/2330 train_time:129871ms step_avg:61.76ms
step:2104/2330 train_time:129936ms step_avg:61.76ms
step:2105/2330 train_time:129997ms step_avg:61.76ms
step:2106/2330 train_time:130061ms step_avg:61.76ms
step:2107/2330 train_time:130122ms step_avg:61.76ms
step:2108/2330 train_time:130186ms step_avg:61.76ms
step:2109/2330 train_time:130247ms step_avg:61.76ms
step:2110/2330 train_time:130311ms step_avg:61.76ms
step:2111/2330 train_time:130372ms step_avg:61.76ms
step:2112/2330 train_time:130436ms step_avg:61.76ms
step:2113/2330 train_time:130497ms step_avg:61.76ms
step:2114/2330 train_time:130560ms step_avg:61.76ms
step:2115/2330 train_time:130622ms step_avg:61.76ms
step:2116/2330 train_time:130685ms step_avg:61.76ms
step:2117/2330 train_time:130746ms step_avg:61.76ms
step:2118/2330 train_time:130810ms step_avg:61.76ms
step:2119/2330 train_time:130873ms step_avg:61.76ms
step:2120/2330 train_time:130937ms step_avg:61.76ms
step:2121/2330 train_time:130997ms step_avg:61.76ms
step:2122/2330 train_time:131060ms step_avg:61.76ms
step:2123/2330 train_time:131122ms step_avg:61.76ms
step:2124/2330 train_time:131185ms step_avg:61.76ms
step:2125/2330 train_time:131246ms step_avg:61.76ms
step:2126/2330 train_time:131310ms step_avg:61.76ms
step:2127/2330 train_time:131373ms step_avg:61.76ms
step:2128/2330 train_time:131437ms step_avg:61.77ms
step:2129/2330 train_time:131498ms step_avg:61.77ms
step:2130/2330 train_time:131561ms step_avg:61.77ms
step:2131/2330 train_time:131623ms step_avg:61.77ms
step:2132/2330 train_time:131686ms step_avg:61.77ms
step:2133/2330 train_time:131748ms step_avg:61.77ms
step:2134/2330 train_time:131813ms step_avg:61.77ms
step:2135/2330 train_time:131874ms step_avg:61.77ms
step:2136/2330 train_time:131938ms step_avg:61.77ms
step:2137/2330 train_time:131999ms step_avg:61.77ms
step:2138/2330 train_time:132062ms step_avg:61.77ms
step:2139/2330 train_time:132123ms step_avg:61.77ms
step:2140/2330 train_time:132186ms step_avg:61.77ms
step:2141/2330 train_time:132247ms step_avg:61.77ms
step:2142/2330 train_time:132312ms step_avg:61.77ms
step:2143/2330 train_time:132374ms step_avg:61.77ms
step:2144/2330 train_time:132439ms step_avg:61.77ms
step:2145/2330 train_time:132500ms step_avg:61.77ms
step:2146/2330 train_time:132563ms step_avg:61.77ms
step:2147/2330 train_time:132625ms step_avg:61.77ms
step:2148/2330 train_time:132689ms step_avg:61.77ms
step:2149/2330 train_time:132750ms step_avg:61.77ms
step:2150/2330 train_time:132814ms step_avg:61.77ms
step:2151/2330 train_time:132876ms step_avg:61.77ms
step:2152/2330 train_time:132940ms step_avg:61.78ms
step:2153/2330 train_time:133000ms step_avg:61.77ms
step:2154/2330 train_time:133064ms step_avg:61.78ms
step:2155/2330 train_time:133126ms step_avg:61.78ms
step:2156/2330 train_time:133189ms step_avg:61.78ms
step:2157/2330 train_time:133250ms step_avg:61.78ms
step:2158/2330 train_time:133314ms step_avg:61.78ms
step:2159/2330 train_time:133377ms step_avg:61.78ms
step:2160/2330 train_time:133440ms step_avg:61.78ms
step:2161/2330 train_time:133501ms step_avg:61.78ms
step:2162/2330 train_time:133565ms step_avg:61.78ms
step:2163/2330 train_time:133626ms step_avg:61.78ms
step:2164/2330 train_time:133690ms step_avg:61.78ms
step:2165/2330 train_time:133752ms step_avg:61.78ms
step:2166/2330 train_time:133816ms step_avg:61.78ms
step:2167/2330 train_time:133877ms step_avg:61.78ms
step:2168/2330 train_time:133941ms step_avg:61.78ms
step:2169/2330 train_time:134002ms step_avg:61.78ms
step:2170/2330 train_time:134065ms step_avg:61.78ms
step:2171/2330 train_time:134126ms step_avg:61.78ms
step:2172/2330 train_time:134190ms step_avg:61.78ms
step:2173/2330 train_time:134252ms step_avg:61.78ms
step:2174/2330 train_time:134316ms step_avg:61.78ms
step:2175/2330 train_time:134378ms step_avg:61.78ms
step:2176/2330 train_time:134441ms step_avg:61.78ms
step:2177/2330 train_time:134502ms step_avg:61.78ms
step:2178/2330 train_time:134565ms step_avg:61.78ms
step:2179/2330 train_time:134627ms step_avg:61.78ms
step:2180/2330 train_time:134692ms step_avg:61.79ms
step:2181/2330 train_time:134754ms step_avg:61.79ms
step:2182/2330 train_time:134818ms step_avg:61.79ms
step:2183/2330 train_time:134880ms step_avg:61.79ms
step:2184/2330 train_time:134944ms step_avg:61.79ms
step:2185/2330 train_time:135006ms step_avg:61.79ms
step:2186/2330 train_time:135069ms step_avg:61.79ms
step:2187/2330 train_time:135129ms step_avg:61.79ms
step:2188/2330 train_time:135192ms step_avg:61.79ms
step:2189/2330 train_time:135254ms step_avg:61.79ms
step:2190/2330 train_time:135318ms step_avg:61.79ms
step:2191/2330 train_time:135380ms step_avg:61.79ms
step:2192/2330 train_time:135443ms step_avg:61.79ms
step:2193/2330 train_time:135504ms step_avg:61.79ms
step:2194/2330 train_time:135566ms step_avg:61.79ms
step:2195/2330 train_time:135628ms step_avg:61.79ms
step:2196/2330 train_time:135691ms step_avg:61.79ms
step:2197/2330 train_time:135753ms step_avg:61.79ms
step:2198/2330 train_time:135817ms step_avg:61.79ms
step:2199/2330 train_time:135879ms step_avg:61.79ms
step:2200/2330 train_time:135942ms step_avg:61.79ms
step:2201/2330 train_time:136003ms step_avg:61.79ms
step:2202/2330 train_time:136066ms step_avg:61.79ms
step:2203/2330 train_time:136127ms step_avg:61.79ms
step:2204/2330 train_time:136191ms step_avg:61.79ms
step:2205/2330 train_time:136253ms step_avg:61.79ms
step:2206/2330 train_time:136317ms step_avg:61.79ms
step:2207/2330 train_time:136378ms step_avg:61.79ms
step:2208/2330 train_time:136441ms step_avg:61.79ms
step:2209/2330 train_time:136502ms step_avg:61.79ms
step:2210/2330 train_time:136566ms step_avg:61.79ms
step:2211/2330 train_time:136628ms step_avg:61.79ms
step:2212/2330 train_time:136692ms step_avg:61.80ms
step:2213/2330 train_time:136754ms step_avg:61.80ms
step:2214/2330 train_time:136818ms step_avg:61.80ms
step:2215/2330 train_time:136880ms step_avg:61.80ms
step:2216/2330 train_time:136944ms step_avg:61.80ms
step:2217/2330 train_time:137006ms step_avg:61.80ms
step:2218/2330 train_time:137069ms step_avg:61.80ms
step:2219/2330 train_time:137130ms step_avg:61.80ms
step:2220/2330 train_time:137194ms step_avg:61.80ms
step:2221/2330 train_time:137255ms step_avg:61.80ms
step:2222/2330 train_time:137319ms step_avg:61.80ms
step:2223/2330 train_time:137379ms step_avg:61.80ms
step:2224/2330 train_time:137443ms step_avg:61.80ms
step:2225/2330 train_time:137504ms step_avg:61.80ms
step:2226/2330 train_time:137567ms step_avg:61.80ms
step:2227/2330 train_time:137628ms step_avg:61.80ms
step:2228/2330 train_time:137692ms step_avg:61.80ms
step:2229/2330 train_time:137754ms step_avg:61.80ms
step:2230/2330 train_time:137817ms step_avg:61.80ms
step:2231/2330 train_time:137879ms step_avg:61.80ms
step:2232/2330 train_time:137943ms step_avg:61.80ms
step:2233/2330 train_time:138003ms step_avg:61.80ms
step:2234/2330 train_time:138066ms step_avg:61.80ms
step:2235/2330 train_time:138129ms step_avg:61.80ms
step:2236/2330 train_time:138193ms step_avg:61.80ms
step:2237/2330 train_time:138255ms step_avg:61.80ms
step:2238/2330 train_time:138318ms step_avg:61.80ms
step:2239/2330 train_time:138379ms step_avg:61.80ms
step:2240/2330 train_time:138444ms step_avg:61.81ms
step:2241/2330 train_time:138505ms step_avg:61.80ms
step:2242/2330 train_time:138568ms step_avg:61.81ms
step:2243/2330 train_time:138629ms step_avg:61.81ms
step:2244/2330 train_time:138693ms step_avg:61.81ms
step:2245/2330 train_time:138755ms step_avg:61.81ms
step:2246/2330 train_time:138819ms step_avg:61.81ms
step:2247/2330 train_time:138880ms step_avg:61.81ms
step:2248/2330 train_time:138943ms step_avg:61.81ms
step:2249/2330 train_time:139004ms step_avg:61.81ms
step:2250/2330 train_time:139067ms step_avg:61.81ms
step:2250/2330 val_loss:3.4831 train_time:139133ms step_avg:61.84ms
step:2251/2330 train_time:139157ms step_avg:61.82ms
step:2252/2330 train_time:139196ms step_avg:61.81ms
step:2253/2330 train_time:139263ms step_avg:61.81ms
step:2254/2330 train_time:139328ms step_avg:61.81ms
step:2255/2330 train_time:139389ms step_avg:61.81ms
step:2256/2330 train_time:139453ms step_avg:61.81ms
step:2257/2330 train_time:139513ms step_avg:61.81ms
step:2258/2330 train_time:139576ms step_avg:61.81ms
step:2259/2330 train_time:139636ms step_avg:61.81ms
step:2260/2330 train_time:139699ms step_avg:61.81ms
step:2261/2330 train_time:139760ms step_avg:61.81ms
step:2262/2330 train_time:139822ms step_avg:61.81ms
step:2263/2330 train_time:139883ms step_avg:61.81ms
step:2264/2330 train_time:139946ms step_avg:61.81ms
step:2265/2330 train_time:140006ms step_avg:61.81ms
step:2266/2330 train_time:140071ms step_avg:61.81ms
step:2267/2330 train_time:140134ms step_avg:61.81ms
step:2268/2330 train_time:140200ms step_avg:61.82ms
step:2269/2330 train_time:140262ms step_avg:61.82ms
step:2270/2330 train_time:140326ms step_avg:61.82ms
step:2271/2330 train_time:140388ms step_avg:61.82ms
step:2272/2330 train_time:140452ms step_avg:61.82ms
step:2273/2330 train_time:140513ms step_avg:61.82ms
step:2274/2330 train_time:140576ms step_avg:61.82ms
step:2275/2330 train_time:140637ms step_avg:61.82ms
step:2276/2330 train_time:140700ms step_avg:61.82ms
step:2277/2330 train_time:140760ms step_avg:61.82ms
step:2278/2330 train_time:140823ms step_avg:61.82ms
step:2279/2330 train_time:140884ms step_avg:61.82ms
step:2280/2330 train_time:140948ms step_avg:61.82ms
step:2281/2330 train_time:141008ms step_avg:61.82ms
step:2282/2330 train_time:141072ms step_avg:61.82ms
step:2283/2330 train_time:141134ms step_avg:61.82ms
step:2284/2330 train_time:141198ms step_avg:61.82ms
step:2285/2330 train_time:141261ms step_avg:61.82ms
step:2286/2330 train_time:141325ms step_avg:61.82ms
step:2287/2330 train_time:141387ms step_avg:61.82ms
step:2288/2330 train_time:141450ms step_avg:61.82ms
step:2289/2330 train_time:141511ms step_avg:61.82ms
step:2290/2330 train_time:141576ms step_avg:61.82ms
step:2291/2330 train_time:141636ms step_avg:61.82ms
step:2292/2330 train_time:141699ms step_avg:61.82ms
step:2293/2330 train_time:141760ms step_avg:61.82ms
step:2294/2330 train_time:141823ms step_avg:61.82ms
step:2295/2330 train_time:141884ms step_avg:61.82ms
step:2296/2330 train_time:141947ms step_avg:61.82ms
step:2297/2330 train_time:142007ms step_avg:61.82ms
step:2298/2330 train_time:142071ms step_avg:61.82ms
step:2299/2330 train_time:142133ms step_avg:61.82ms
step:2300/2330 train_time:142198ms step_avg:61.83ms
step:2301/2330 train_time:142260ms step_avg:61.83ms
step:2302/2330 train_time:142324ms step_avg:61.83ms
step:2303/2330 train_time:142386ms step_avg:61.83ms
step:2304/2330 train_time:142450ms step_avg:61.83ms
step:2305/2330 train_time:142513ms step_avg:61.83ms
step:2306/2330 train_time:142575ms step_avg:61.83ms
step:2307/2330 train_time:142637ms step_avg:61.83ms
step:2308/2330 train_time:142700ms step_avg:61.83ms
step:2309/2330 train_time:142761ms step_avg:61.83ms
step:2310/2330 train_time:142825ms step_avg:61.83ms
step:2311/2330 train_time:142886ms step_avg:61.83ms
step:2312/2330 train_time:142949ms step_avg:61.83ms
step:2313/2330 train_time:143010ms step_avg:61.83ms
step:2314/2330 train_time:143073ms step_avg:61.83ms
step:2315/2330 train_time:143136ms step_avg:61.83ms
step:2316/2330 train_time:143199ms step_avg:61.83ms
step:2317/2330 train_time:143260ms step_avg:61.83ms
step:2318/2330 train_time:143324ms step_avg:61.83ms
step:2319/2330 train_time:143386ms step_avg:61.83ms
step:2320/2330 train_time:143450ms step_avg:61.83ms
step:2321/2330 train_time:143513ms step_avg:61.83ms
step:2322/2330 train_time:143577ms step_avg:61.83ms
step:2323/2330 train_time:143639ms step_avg:61.83ms
step:2324/2330 train_time:143702ms step_avg:61.83ms
step:2325/2330 train_time:143763ms step_avg:61.83ms
step:2326/2330 train_time:143826ms step_avg:61.83ms
step:2327/2330 train_time:143886ms step_avg:61.83ms
step:2328/2330 train_time:143950ms step_avg:61.83ms
step:2329/2330 train_time:144011ms step_avg:61.83ms
step:2330/2330 train_time:144075ms step_avg:61.83ms
step:2330/2330 val_loss:3.4715 train_time:144142ms step_avg:61.86ms
peak memory allocated: 29512 MiB reserved: 44076 MiB
