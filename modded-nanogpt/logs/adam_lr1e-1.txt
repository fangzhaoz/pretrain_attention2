import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "adam_lr1e-1"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-1, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.Adam(
    hidden_matrix_params + gate_params,
    lr=1e-1,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 00:51:05 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   34C    P0             113W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   32C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   30C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   30C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   34C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:76ms step_avg:75.68ms
step:2/2330 train_time:169ms step_avg:84.35ms
step:3/2330 train_time:188ms step_avg:62.78ms
step:4/2330 train_time:209ms step_avg:52.37ms
step:5/2330 train_time:264ms step_avg:52.89ms
step:6/2330 train_time:323ms step_avg:53.91ms
step:7/2330 train_time:378ms step_avg:54.04ms
step:8/2330 train_time:437ms step_avg:54.62ms
step:9/2330 train_time:492ms step_avg:54.65ms
step:10/2330 train_time:551ms step_avg:55.05ms
step:11/2330 train_time:605ms step_avg:55.04ms
step:12/2330 train_time:664ms step_avg:55.35ms
step:13/2330 train_time:719ms step_avg:55.32ms
step:14/2330 train_time:778ms step_avg:55.56ms
step:15/2330 train_time:833ms step_avg:55.53ms
step:16/2330 train_time:892ms step_avg:55.73ms
step:17/2330 train_time:947ms step_avg:55.69ms
step:18/2330 train_time:1005ms step_avg:55.85ms
step:19/2330 train_time:1060ms step_avg:55.80ms
step:20/2330 train_time:1119ms step_avg:55.97ms
step:21/2330 train_time:1175ms step_avg:55.93ms
step:22/2330 train_time:1236ms step_avg:56.17ms
step:23/2330 train_time:1291ms step_avg:56.14ms
step:24/2330 train_time:1350ms step_avg:56.27ms
step:25/2330 train_time:1406ms step_avg:56.23ms
step:26/2330 train_time:1466ms step_avg:56.37ms
step:27/2330 train_time:1521ms step_avg:56.32ms
step:28/2330 train_time:1580ms step_avg:56.44ms
step:29/2330 train_time:1636ms step_avg:56.40ms
step:30/2330 train_time:1695ms step_avg:56.49ms
step:31/2330 train_time:1750ms step_avg:56.45ms
step:32/2330 train_time:1809ms step_avg:56.53ms
step:33/2330 train_time:1864ms step_avg:56.49ms
step:34/2330 train_time:1924ms step_avg:56.58ms
step:35/2330 train_time:1979ms step_avg:56.53ms
step:36/2330 train_time:2038ms step_avg:56.61ms
step:37/2330 train_time:2093ms step_avg:56.57ms
step:38/2330 train_time:2153ms step_avg:56.65ms
step:39/2330 train_time:2208ms step_avg:56.62ms
step:40/2330 train_time:2268ms step_avg:56.70ms
step:41/2330 train_time:2323ms step_avg:56.66ms
step:42/2330 train_time:2384ms step_avg:56.75ms
step:43/2330 train_time:2439ms step_avg:56.72ms
step:44/2330 train_time:2499ms step_avg:56.80ms
step:45/2330 train_time:2554ms step_avg:56.76ms
step:46/2330 train_time:2615ms step_avg:56.85ms
step:47/2330 train_time:2671ms step_avg:56.82ms
step:48/2330 train_time:2730ms step_avg:56.88ms
step:49/2330 train_time:2786ms step_avg:56.85ms
step:50/2330 train_time:2845ms step_avg:56.90ms
step:51/2330 train_time:2901ms step_avg:56.87ms
step:52/2330 train_time:2960ms step_avg:56.92ms
step:53/2330 train_time:3015ms step_avg:56.89ms
step:54/2330 train_time:3075ms step_avg:56.95ms
step:55/2330 train_time:3131ms step_avg:56.92ms
step:56/2330 train_time:3191ms step_avg:56.97ms
step:57/2330 train_time:3246ms step_avg:56.95ms
step:58/2330 train_time:3306ms step_avg:56.99ms
step:59/2330 train_time:3361ms step_avg:56.97ms
step:60/2330 train_time:3421ms step_avg:57.02ms
step:61/2330 train_time:3477ms step_avg:57.01ms
step:62/2330 train_time:3537ms step_avg:57.05ms
step:63/2330 train_time:3592ms step_avg:57.02ms
step:64/2330 train_time:3653ms step_avg:57.08ms
step:65/2330 train_time:3709ms step_avg:57.06ms
step:66/2330 train_time:3768ms step_avg:57.08ms
step:67/2330 train_time:3823ms step_avg:57.06ms
step:68/2330 train_time:3883ms step_avg:57.10ms
step:69/2330 train_time:3938ms step_avg:57.07ms
step:70/2330 train_time:3998ms step_avg:57.11ms
step:71/2330 train_time:4053ms step_avg:57.09ms
step:72/2330 train_time:4113ms step_avg:57.13ms
step:73/2330 train_time:4169ms step_avg:57.10ms
step:74/2330 train_time:4229ms step_avg:57.15ms
step:75/2330 train_time:4285ms step_avg:57.14ms
step:76/2330 train_time:4345ms step_avg:57.17ms
step:77/2330 train_time:4401ms step_avg:57.16ms
step:78/2330 train_time:4460ms step_avg:57.18ms
step:79/2330 train_time:4515ms step_avg:57.15ms
step:80/2330 train_time:4576ms step_avg:57.20ms
step:81/2330 train_time:4631ms step_avg:57.17ms
step:82/2330 train_time:4692ms step_avg:57.22ms
step:83/2330 train_time:4747ms step_avg:57.20ms
step:84/2330 train_time:4807ms step_avg:57.23ms
step:85/2330 train_time:4862ms step_avg:57.20ms
step:86/2330 train_time:4922ms step_avg:57.23ms
step:87/2330 train_time:4977ms step_avg:57.21ms
step:88/2330 train_time:5037ms step_avg:57.24ms
step:89/2330 train_time:5092ms step_avg:57.21ms
step:90/2330 train_time:5152ms step_avg:57.25ms
step:91/2330 train_time:5208ms step_avg:57.23ms
step:92/2330 train_time:5267ms step_avg:57.25ms
step:93/2330 train_time:5322ms step_avg:57.23ms
step:94/2330 train_time:5382ms step_avg:57.26ms
step:95/2330 train_time:5438ms step_avg:57.24ms
step:96/2330 train_time:5497ms step_avg:57.26ms
step:97/2330 train_time:5553ms step_avg:57.24ms
step:98/2330 train_time:5612ms step_avg:57.26ms
step:99/2330 train_time:5667ms step_avg:57.25ms
step:100/2330 train_time:5727ms step_avg:57.27ms
step:101/2330 train_time:5783ms step_avg:57.25ms
step:102/2330 train_time:5843ms step_avg:57.28ms
step:103/2330 train_time:5898ms step_avg:57.27ms
step:104/2330 train_time:5957ms step_avg:57.28ms
step:105/2330 train_time:6013ms step_avg:57.26ms
step:106/2330 train_time:6073ms step_avg:57.29ms
step:107/2330 train_time:6129ms step_avg:57.28ms
step:108/2330 train_time:6189ms step_avg:57.30ms
step:109/2330 train_time:6244ms step_avg:57.28ms
step:110/2330 train_time:6304ms step_avg:57.31ms
step:111/2330 train_time:6359ms step_avg:57.29ms
step:112/2330 train_time:6419ms step_avg:57.32ms
step:113/2330 train_time:6475ms step_avg:57.30ms
step:114/2330 train_time:6535ms step_avg:57.33ms
step:115/2330 train_time:6591ms step_avg:57.31ms
step:116/2330 train_time:6651ms step_avg:57.34ms
step:117/2330 train_time:6707ms step_avg:57.32ms
step:118/2330 train_time:6766ms step_avg:57.34ms
step:119/2330 train_time:6822ms step_avg:57.33ms
step:120/2330 train_time:6882ms step_avg:57.35ms
step:121/2330 train_time:6938ms step_avg:57.34ms
step:122/2330 train_time:6998ms step_avg:57.36ms
step:123/2330 train_time:7053ms step_avg:57.34ms
step:124/2330 train_time:7114ms step_avg:57.37ms
step:125/2330 train_time:7170ms step_avg:57.36ms
step:126/2330 train_time:7230ms step_avg:57.38ms
step:127/2330 train_time:7285ms step_avg:57.36ms
step:128/2330 train_time:7345ms step_avg:57.38ms
step:129/2330 train_time:7401ms step_avg:57.37ms
step:130/2330 train_time:7460ms step_avg:57.39ms
step:131/2330 train_time:7516ms step_avg:57.37ms
step:132/2330 train_time:7576ms step_avg:57.39ms
step:133/2330 train_time:7631ms step_avg:57.38ms
step:134/2330 train_time:7690ms step_avg:57.39ms
step:135/2330 train_time:7746ms step_avg:57.38ms
step:136/2330 train_time:7806ms step_avg:57.39ms
step:137/2330 train_time:7861ms step_avg:57.38ms
step:138/2330 train_time:7921ms step_avg:57.40ms
step:139/2330 train_time:7976ms step_avg:57.38ms
step:140/2330 train_time:8036ms step_avg:57.40ms
step:141/2330 train_time:8092ms step_avg:57.39ms
step:142/2330 train_time:8152ms step_avg:57.41ms
step:143/2330 train_time:8207ms step_avg:57.39ms
step:144/2330 train_time:8267ms step_avg:57.41ms
step:145/2330 train_time:8323ms step_avg:57.40ms
step:146/2330 train_time:8383ms step_avg:57.42ms
step:147/2330 train_time:8439ms step_avg:57.41ms
step:148/2330 train_time:8499ms step_avg:57.42ms
step:149/2330 train_time:8554ms step_avg:57.41ms
step:150/2330 train_time:8614ms step_avg:57.43ms
step:151/2330 train_time:8670ms step_avg:57.42ms
step:152/2330 train_time:8730ms step_avg:57.43ms
step:153/2330 train_time:8785ms step_avg:57.42ms
step:154/2330 train_time:8845ms step_avg:57.44ms
step:155/2330 train_time:8900ms step_avg:57.42ms
step:156/2330 train_time:8961ms step_avg:57.44ms
step:157/2330 train_time:9016ms step_avg:57.43ms
step:158/2330 train_time:9078ms step_avg:57.45ms
step:159/2330 train_time:9133ms step_avg:57.44ms
step:160/2330 train_time:9195ms step_avg:57.47ms
step:161/2330 train_time:9251ms step_avg:57.46ms
step:162/2330 train_time:9311ms step_avg:57.48ms
step:163/2330 train_time:9367ms step_avg:57.46ms
step:164/2330 train_time:9426ms step_avg:57.48ms
step:165/2330 train_time:9482ms step_avg:57.47ms
step:166/2330 train_time:9542ms step_avg:57.48ms
step:167/2330 train_time:9598ms step_avg:57.47ms
step:168/2330 train_time:9658ms step_avg:57.49ms
step:169/2330 train_time:9714ms step_avg:57.48ms
step:170/2330 train_time:9774ms step_avg:57.50ms
step:171/2330 train_time:9830ms step_avg:57.49ms
step:172/2330 train_time:9890ms step_avg:57.50ms
step:173/2330 train_time:9947ms step_avg:57.49ms
step:174/2330 train_time:10006ms step_avg:57.51ms
step:175/2330 train_time:10061ms step_avg:57.49ms
step:176/2330 train_time:10122ms step_avg:57.51ms
step:177/2330 train_time:10177ms step_avg:57.50ms
step:178/2330 train_time:10240ms step_avg:57.53ms
step:179/2330 train_time:10296ms step_avg:57.52ms
step:180/2330 train_time:10357ms step_avg:57.54ms
step:181/2330 train_time:10412ms step_avg:57.52ms
step:182/2330 train_time:10472ms step_avg:57.54ms
step:183/2330 train_time:10528ms step_avg:57.53ms
step:184/2330 train_time:10588ms step_avg:57.54ms
step:185/2330 train_time:10643ms step_avg:57.53ms
step:186/2330 train_time:10705ms step_avg:57.55ms
step:187/2330 train_time:10761ms step_avg:57.54ms
step:188/2330 train_time:10821ms step_avg:57.56ms
step:189/2330 train_time:10876ms step_avg:57.55ms
step:190/2330 train_time:10938ms step_avg:57.57ms
step:191/2330 train_time:10994ms step_avg:57.56ms
step:192/2330 train_time:11054ms step_avg:57.57ms
step:193/2330 train_time:11111ms step_avg:57.57ms
step:194/2330 train_time:11170ms step_avg:57.58ms
step:195/2330 train_time:11227ms step_avg:57.57ms
step:196/2330 train_time:11287ms step_avg:57.59ms
step:197/2330 train_time:11342ms step_avg:57.58ms
step:198/2330 train_time:11402ms step_avg:57.59ms
step:199/2330 train_time:11458ms step_avg:57.58ms
step:200/2330 train_time:11519ms step_avg:57.59ms
step:201/2330 train_time:11574ms step_avg:57.58ms
step:202/2330 train_time:11636ms step_avg:57.60ms
step:203/2330 train_time:11691ms step_avg:57.59ms
step:204/2330 train_time:11751ms step_avg:57.60ms
step:205/2330 train_time:11807ms step_avg:57.59ms
step:206/2330 train_time:11867ms step_avg:57.61ms
step:207/2330 train_time:11923ms step_avg:57.60ms
step:208/2330 train_time:11984ms step_avg:57.62ms
step:209/2330 train_time:12040ms step_avg:57.61ms
step:210/2330 train_time:12100ms step_avg:57.62ms
step:211/2330 train_time:12156ms step_avg:57.61ms
step:212/2330 train_time:12218ms step_avg:57.63ms
step:213/2330 train_time:12273ms step_avg:57.62ms
step:214/2330 train_time:12334ms step_avg:57.63ms
step:215/2330 train_time:12390ms step_avg:57.63ms
step:216/2330 train_time:12450ms step_avg:57.64ms
step:217/2330 train_time:12506ms step_avg:57.63ms
step:218/2330 train_time:12565ms step_avg:57.64ms
step:219/2330 train_time:12621ms step_avg:57.63ms
step:220/2330 train_time:12681ms step_avg:57.64ms
step:221/2330 train_time:12737ms step_avg:57.63ms
step:222/2330 train_time:12799ms step_avg:57.65ms
step:223/2330 train_time:12855ms step_avg:57.64ms
step:224/2330 train_time:12916ms step_avg:57.66ms
step:225/2330 train_time:12972ms step_avg:57.65ms
step:226/2330 train_time:13031ms step_avg:57.66ms
step:227/2330 train_time:13087ms step_avg:57.65ms
step:228/2330 train_time:13147ms step_avg:57.66ms
step:229/2330 train_time:13203ms step_avg:57.65ms
step:230/2330 train_time:13263ms step_avg:57.67ms
step:231/2330 train_time:13319ms step_avg:57.66ms
step:232/2330 train_time:13380ms step_avg:57.67ms
step:233/2330 train_time:13436ms step_avg:57.67ms
step:234/2330 train_time:13497ms step_avg:57.68ms
step:235/2330 train_time:13553ms step_avg:57.67ms
step:236/2330 train_time:13614ms step_avg:57.69ms
step:237/2330 train_time:13670ms step_avg:57.68ms
step:238/2330 train_time:13729ms step_avg:57.69ms
step:239/2330 train_time:13786ms step_avg:57.68ms
step:240/2330 train_time:13846ms step_avg:57.69ms
step:241/2330 train_time:13901ms step_avg:57.68ms
step:242/2330 train_time:13962ms step_avg:57.69ms
step:243/2330 train_time:14017ms step_avg:57.68ms
step:244/2330 train_time:14079ms step_avg:57.70ms
step:245/2330 train_time:14134ms step_avg:57.69ms
step:246/2330 train_time:14195ms step_avg:57.70ms
step:247/2330 train_time:14252ms step_avg:57.70ms
step:248/2330 train_time:14313ms step_avg:57.71ms
step:249/2330 train_time:14369ms step_avg:57.71ms
step:250/2330 train_time:14428ms step_avg:57.71ms
step:250/2330 val_loss:6.4675 train_time:14505ms step_avg:58.02ms
step:251/2330 train_time:14526ms step_avg:57.87ms
step:252/2330 train_time:14547ms step_avg:57.73ms
step:253/2330 train_time:14602ms step_avg:57.72ms
step:254/2330 train_time:14668ms step_avg:57.75ms
step:255/2330 train_time:14723ms step_avg:57.74ms
step:256/2330 train_time:14790ms step_avg:57.77ms
step:257/2330 train_time:14845ms step_avg:57.76ms
step:258/2330 train_time:14907ms step_avg:57.78ms
step:259/2330 train_time:14963ms step_avg:57.77ms
step:260/2330 train_time:15024ms step_avg:57.78ms
step:261/2330 train_time:15079ms step_avg:57.77ms
step:262/2330 train_time:15139ms step_avg:57.78ms
step:263/2330 train_time:15195ms step_avg:57.78ms
step:264/2330 train_time:15255ms step_avg:57.78ms
step:265/2330 train_time:15310ms step_avg:57.77ms
step:266/2330 train_time:15370ms step_avg:57.78ms
step:267/2330 train_time:15426ms step_avg:57.77ms
step:268/2330 train_time:15486ms step_avg:57.78ms
step:269/2330 train_time:15541ms step_avg:57.77ms
step:270/2330 train_time:15602ms step_avg:57.79ms
step:271/2330 train_time:15658ms step_avg:57.78ms
step:272/2330 train_time:15721ms step_avg:57.80ms
step:273/2330 train_time:15777ms step_avg:57.79ms
step:274/2330 train_time:15840ms step_avg:57.81ms
step:275/2330 train_time:15895ms step_avg:57.80ms
step:276/2330 train_time:15958ms step_avg:57.82ms
step:277/2330 train_time:16013ms step_avg:57.81ms
step:278/2330 train_time:16073ms step_avg:57.82ms
step:279/2330 train_time:16130ms step_avg:57.81ms
step:280/2330 train_time:16190ms step_avg:57.82ms
step:281/2330 train_time:16245ms step_avg:57.81ms
step:282/2330 train_time:16305ms step_avg:57.82ms
step:283/2330 train_time:16360ms step_avg:57.81ms
step:284/2330 train_time:16420ms step_avg:57.82ms
step:285/2330 train_time:16477ms step_avg:57.81ms
step:286/2330 train_time:16538ms step_avg:57.83ms
step:287/2330 train_time:16594ms step_avg:57.82ms
step:288/2330 train_time:16655ms step_avg:57.83ms
step:289/2330 train_time:16711ms step_avg:57.82ms
step:290/2330 train_time:16772ms step_avg:57.83ms
step:291/2330 train_time:16827ms step_avg:57.83ms
step:292/2330 train_time:16889ms step_avg:57.84ms
step:293/2330 train_time:16945ms step_avg:57.83ms
step:294/2330 train_time:17007ms step_avg:57.85ms
step:295/2330 train_time:17063ms step_avg:57.84ms
step:296/2330 train_time:17124ms step_avg:57.85ms
step:297/2330 train_time:17180ms step_avg:57.84ms
step:298/2330 train_time:17241ms step_avg:57.86ms
step:299/2330 train_time:17296ms step_avg:57.85ms
step:300/2330 train_time:17357ms step_avg:57.86ms
step:301/2330 train_time:17412ms step_avg:57.85ms
step:302/2330 train_time:17473ms step_avg:57.86ms
step:303/2330 train_time:17529ms step_avg:57.85ms
step:304/2330 train_time:17589ms step_avg:57.86ms
step:305/2330 train_time:17645ms step_avg:57.85ms
step:306/2330 train_time:17706ms step_avg:57.86ms
step:307/2330 train_time:17761ms step_avg:57.85ms
step:308/2330 train_time:17824ms step_avg:57.87ms
step:309/2330 train_time:17880ms step_avg:57.86ms
step:310/2330 train_time:17942ms step_avg:57.88ms
step:311/2330 train_time:17997ms step_avg:57.87ms
step:312/2330 train_time:18059ms step_avg:57.88ms
step:313/2330 train_time:18115ms step_avg:57.88ms
step:314/2330 train_time:18177ms step_avg:57.89ms
step:315/2330 train_time:18233ms step_avg:57.88ms
step:316/2330 train_time:18293ms step_avg:57.89ms
step:317/2330 train_time:18349ms step_avg:57.88ms
step:318/2330 train_time:18408ms step_avg:57.89ms
step:319/2330 train_time:18464ms step_avg:57.88ms
step:320/2330 train_time:18524ms step_avg:57.89ms
step:321/2330 train_time:18580ms step_avg:57.88ms
step:322/2330 train_time:18641ms step_avg:57.89ms
step:323/2330 train_time:18697ms step_avg:57.89ms
step:324/2330 train_time:18759ms step_avg:57.90ms
step:325/2330 train_time:18815ms step_avg:57.89ms
step:326/2330 train_time:18877ms step_avg:57.91ms
step:327/2330 train_time:18933ms step_avg:57.90ms
step:328/2330 train_time:18993ms step_avg:57.91ms
step:329/2330 train_time:19049ms step_avg:57.90ms
step:330/2330 train_time:19110ms step_avg:57.91ms
step:331/2330 train_time:19166ms step_avg:57.90ms
step:332/2330 train_time:19226ms step_avg:57.91ms
step:333/2330 train_time:19282ms step_avg:57.90ms
step:334/2330 train_time:19343ms step_avg:57.91ms
step:335/2330 train_time:19399ms step_avg:57.91ms
step:336/2330 train_time:19461ms step_avg:57.92ms
step:337/2330 train_time:19516ms step_avg:57.91ms
step:338/2330 train_time:19577ms step_avg:57.92ms
step:339/2330 train_time:19633ms step_avg:57.91ms
step:340/2330 train_time:19693ms step_avg:57.92ms
step:341/2330 train_time:19749ms step_avg:57.91ms
step:342/2330 train_time:19809ms step_avg:57.92ms
step:343/2330 train_time:19865ms step_avg:57.92ms
step:344/2330 train_time:19926ms step_avg:57.93ms
step:345/2330 train_time:19982ms step_avg:57.92ms
step:346/2330 train_time:20044ms step_avg:57.93ms
step:347/2330 train_time:20100ms step_avg:57.92ms
step:348/2330 train_time:20162ms step_avg:57.94ms
step:349/2330 train_time:20217ms step_avg:57.93ms
step:350/2330 train_time:20280ms step_avg:57.94ms
step:351/2330 train_time:20335ms step_avg:57.93ms
step:352/2330 train_time:20397ms step_avg:57.95ms
step:353/2330 train_time:20452ms step_avg:57.94ms
step:354/2330 train_time:20514ms step_avg:57.95ms
step:355/2330 train_time:20569ms step_avg:57.94ms
step:356/2330 train_time:20629ms step_avg:57.95ms
step:357/2330 train_time:20685ms step_avg:57.94ms
step:358/2330 train_time:20745ms step_avg:57.95ms
step:359/2330 train_time:20801ms step_avg:57.94ms
step:360/2330 train_time:20862ms step_avg:57.95ms
step:361/2330 train_time:20918ms step_avg:57.94ms
step:362/2330 train_time:20980ms step_avg:57.96ms
step:363/2330 train_time:21036ms step_avg:57.95ms
step:364/2330 train_time:21097ms step_avg:57.96ms
step:365/2330 train_time:21153ms step_avg:57.95ms
step:366/2330 train_time:21213ms step_avg:57.96ms
step:367/2330 train_time:21270ms step_avg:57.96ms
step:368/2330 train_time:21331ms step_avg:57.96ms
step:369/2330 train_time:21387ms step_avg:57.96ms
step:370/2330 train_time:21448ms step_avg:57.97ms
step:371/2330 train_time:21503ms step_avg:57.96ms
step:372/2330 train_time:21565ms step_avg:57.97ms
step:373/2330 train_time:21621ms step_avg:57.96ms
step:374/2330 train_time:21681ms step_avg:57.97ms
step:375/2330 train_time:21737ms step_avg:57.96ms
step:376/2330 train_time:21799ms step_avg:57.98ms
step:377/2330 train_time:21855ms step_avg:57.97ms
step:378/2330 train_time:21916ms step_avg:57.98ms
step:379/2330 train_time:21972ms step_avg:57.97ms
step:380/2330 train_time:22032ms step_avg:57.98ms
step:381/2330 train_time:22088ms step_avg:57.97ms
step:382/2330 train_time:22149ms step_avg:57.98ms
step:383/2330 train_time:22205ms step_avg:57.98ms
step:384/2330 train_time:22266ms step_avg:57.99ms
step:385/2330 train_time:22322ms step_avg:57.98ms
step:386/2330 train_time:22384ms step_avg:57.99ms
step:387/2330 train_time:22440ms step_avg:57.98ms
step:388/2330 train_time:22501ms step_avg:57.99ms
step:389/2330 train_time:22556ms step_avg:57.98ms
step:390/2330 train_time:22618ms step_avg:58.00ms
step:391/2330 train_time:22674ms step_avg:57.99ms
step:392/2330 train_time:22735ms step_avg:58.00ms
step:393/2330 train_time:22791ms step_avg:57.99ms
step:394/2330 train_time:22852ms step_avg:58.00ms
step:395/2330 train_time:22908ms step_avg:57.99ms
step:396/2330 train_time:22968ms step_avg:58.00ms
step:397/2330 train_time:23024ms step_avg:58.00ms
step:398/2330 train_time:23085ms step_avg:58.00ms
step:399/2330 train_time:23141ms step_avg:58.00ms
step:400/2330 train_time:23204ms step_avg:58.01ms
step:401/2330 train_time:23260ms step_avg:58.01ms
step:402/2330 train_time:23322ms step_avg:58.02ms
step:403/2330 train_time:23378ms step_avg:58.01ms
step:404/2330 train_time:23439ms step_avg:58.02ms
step:405/2330 train_time:23495ms step_avg:58.01ms
step:406/2330 train_time:23556ms step_avg:58.02ms
step:407/2330 train_time:23612ms step_avg:58.01ms
step:408/2330 train_time:23672ms step_avg:58.02ms
step:409/2330 train_time:23727ms step_avg:58.01ms
step:410/2330 train_time:23788ms step_avg:58.02ms
step:411/2330 train_time:23844ms step_avg:58.01ms
step:412/2330 train_time:23905ms step_avg:58.02ms
step:413/2330 train_time:23961ms step_avg:58.02ms
step:414/2330 train_time:24022ms step_avg:58.02ms
step:415/2330 train_time:24078ms step_avg:58.02ms
step:416/2330 train_time:24139ms step_avg:58.03ms
step:417/2330 train_time:24195ms step_avg:58.02ms
step:418/2330 train_time:24256ms step_avg:58.03ms
step:419/2330 train_time:24313ms step_avg:58.03ms
step:420/2330 train_time:24373ms step_avg:58.03ms
step:421/2330 train_time:24429ms step_avg:58.03ms
step:422/2330 train_time:24490ms step_avg:58.03ms
step:423/2330 train_time:24546ms step_avg:58.03ms
step:424/2330 train_time:24607ms step_avg:58.03ms
step:425/2330 train_time:24662ms step_avg:58.03ms
step:426/2330 train_time:24724ms step_avg:58.04ms
step:427/2330 train_time:24781ms step_avg:58.03ms
step:428/2330 train_time:24842ms step_avg:58.04ms
step:429/2330 train_time:24898ms step_avg:58.04ms
step:430/2330 train_time:24959ms step_avg:58.05ms
step:431/2330 train_time:25015ms step_avg:58.04ms
step:432/2330 train_time:25077ms step_avg:58.05ms
step:433/2330 train_time:25133ms step_avg:58.04ms
step:434/2330 train_time:25194ms step_avg:58.05ms
step:435/2330 train_time:25250ms step_avg:58.05ms
step:436/2330 train_time:25310ms step_avg:58.05ms
step:437/2330 train_time:25366ms step_avg:58.05ms
step:438/2330 train_time:25427ms step_avg:58.05ms
step:439/2330 train_time:25483ms step_avg:58.05ms
step:440/2330 train_time:25545ms step_avg:58.06ms
step:441/2330 train_time:25601ms step_avg:58.05ms
step:442/2330 train_time:25662ms step_avg:58.06ms
step:443/2330 train_time:25719ms step_avg:58.06ms
step:444/2330 train_time:25780ms step_avg:58.06ms
step:445/2330 train_time:25836ms step_avg:58.06ms
step:446/2330 train_time:25896ms step_avg:58.06ms
step:447/2330 train_time:25953ms step_avg:58.06ms
step:448/2330 train_time:26013ms step_avg:58.06ms
step:449/2330 train_time:26069ms step_avg:58.06ms
step:450/2330 train_time:26129ms step_avg:58.07ms
step:451/2330 train_time:26186ms step_avg:58.06ms
step:452/2330 train_time:26246ms step_avg:58.07ms
step:453/2330 train_time:26302ms step_avg:58.06ms
step:454/2330 train_time:26363ms step_avg:58.07ms
step:455/2330 train_time:26419ms step_avg:58.06ms
step:456/2330 train_time:26481ms step_avg:58.07ms
step:457/2330 train_time:26537ms step_avg:58.07ms
step:458/2330 train_time:26599ms step_avg:58.08ms
step:459/2330 train_time:26655ms step_avg:58.07ms
step:460/2330 train_time:26716ms step_avg:58.08ms
step:461/2330 train_time:26773ms step_avg:58.08ms
step:462/2330 train_time:26834ms step_avg:58.08ms
step:463/2330 train_time:26890ms step_avg:58.08ms
step:464/2330 train_time:26951ms step_avg:58.08ms
step:465/2330 train_time:27007ms step_avg:58.08ms
step:466/2330 train_time:27067ms step_avg:58.08ms
step:467/2330 train_time:27124ms step_avg:58.08ms
step:468/2330 train_time:27184ms step_avg:58.09ms
step:469/2330 train_time:27240ms step_avg:58.08ms
step:470/2330 train_time:27302ms step_avg:58.09ms
step:471/2330 train_time:27358ms step_avg:58.08ms
step:472/2330 train_time:27419ms step_avg:58.09ms
step:473/2330 train_time:27475ms step_avg:58.09ms
step:474/2330 train_time:27536ms step_avg:58.09ms
step:475/2330 train_time:27592ms step_avg:58.09ms
step:476/2330 train_time:27652ms step_avg:58.09ms
step:477/2330 train_time:27708ms step_avg:58.09ms
step:478/2330 train_time:27768ms step_avg:58.09ms
step:479/2330 train_time:27824ms step_avg:58.09ms
step:480/2330 train_time:27885ms step_avg:58.09ms
step:481/2330 train_time:27941ms step_avg:58.09ms
step:482/2330 train_time:28003ms step_avg:58.10ms
step:483/2330 train_time:28059ms step_avg:58.09ms
step:484/2330 train_time:28120ms step_avg:58.10ms
step:485/2330 train_time:28176ms step_avg:58.10ms
step:486/2330 train_time:28238ms step_avg:58.10ms
step:487/2330 train_time:28294ms step_avg:58.10ms
step:488/2330 train_time:28354ms step_avg:58.10ms
step:489/2330 train_time:28411ms step_avg:58.10ms
step:490/2330 train_time:28471ms step_avg:58.10ms
step:491/2330 train_time:28528ms step_avg:58.10ms
step:492/2330 train_time:28588ms step_avg:58.11ms
step:493/2330 train_time:28644ms step_avg:58.10ms
step:494/2330 train_time:28706ms step_avg:58.11ms
step:495/2330 train_time:28762ms step_avg:58.10ms
step:496/2330 train_time:28822ms step_avg:58.11ms
step:497/2330 train_time:28879ms step_avg:58.11ms
step:498/2330 train_time:28940ms step_avg:58.11ms
step:499/2330 train_time:28996ms step_avg:58.11ms
step:500/2330 train_time:29057ms step_avg:58.11ms
step:500/2330 val_loss:5.7436 train_time:29134ms step_avg:58.27ms
step:501/2330 train_time:29153ms step_avg:58.19ms
step:502/2330 train_time:29176ms step_avg:58.12ms
step:503/2330 train_time:29232ms step_avg:58.12ms
step:504/2330 train_time:29297ms step_avg:58.13ms
step:505/2330 train_time:29353ms step_avg:58.12ms
step:506/2330 train_time:29415ms step_avg:58.13ms
step:507/2330 train_time:29471ms step_avg:58.13ms
step:508/2330 train_time:29533ms step_avg:58.14ms
step:509/2330 train_time:29589ms step_avg:58.13ms
step:510/2330 train_time:29650ms step_avg:58.14ms
step:511/2330 train_time:29706ms step_avg:58.13ms
step:512/2330 train_time:29766ms step_avg:58.14ms
step:513/2330 train_time:29822ms step_avg:58.13ms
step:514/2330 train_time:29882ms step_avg:58.14ms
step:515/2330 train_time:29938ms step_avg:58.13ms
step:516/2330 train_time:29998ms step_avg:58.14ms
step:517/2330 train_time:30054ms step_avg:58.13ms
step:518/2330 train_time:30114ms step_avg:58.14ms
step:519/2330 train_time:30171ms step_avg:58.13ms
step:520/2330 train_time:30233ms step_avg:58.14ms
step:521/2330 train_time:30289ms step_avg:58.14ms
step:522/2330 train_time:30352ms step_avg:58.15ms
step:523/2330 train_time:30408ms step_avg:58.14ms
step:524/2330 train_time:30471ms step_avg:58.15ms
step:525/2330 train_time:30527ms step_avg:58.15ms
step:526/2330 train_time:30588ms step_avg:58.15ms
step:527/2330 train_time:30644ms step_avg:58.15ms
step:528/2330 train_time:30705ms step_avg:58.15ms
step:529/2330 train_time:30760ms step_avg:58.15ms
step:530/2330 train_time:30822ms step_avg:58.15ms
step:531/2330 train_time:30878ms step_avg:58.15ms
step:532/2330 train_time:30938ms step_avg:58.15ms
step:533/2330 train_time:30993ms step_avg:58.15ms
step:534/2330 train_time:31054ms step_avg:58.15ms
step:535/2330 train_time:31111ms step_avg:58.15ms
step:536/2330 train_time:31171ms step_avg:58.15ms
step:537/2330 train_time:31228ms step_avg:58.15ms
step:538/2330 train_time:31289ms step_avg:58.16ms
step:539/2330 train_time:31345ms step_avg:58.15ms
step:540/2330 train_time:31408ms step_avg:58.16ms
step:541/2330 train_time:31464ms step_avg:58.16ms
step:542/2330 train_time:31526ms step_avg:58.17ms
step:543/2330 train_time:31582ms step_avg:58.16ms
step:544/2330 train_time:31644ms step_avg:58.17ms
step:545/2330 train_time:31699ms step_avg:58.16ms
step:546/2330 train_time:31761ms step_avg:58.17ms
step:547/2330 train_time:31817ms step_avg:58.17ms
step:548/2330 train_time:31878ms step_avg:58.17ms
step:549/2330 train_time:31934ms step_avg:58.17ms
step:550/2330 train_time:31994ms step_avg:58.17ms
step:551/2330 train_time:32050ms step_avg:58.17ms
step:552/2330 train_time:32111ms step_avg:58.17ms
step:553/2330 train_time:32167ms step_avg:58.17ms
step:554/2330 train_time:32229ms step_avg:58.18ms
step:555/2330 train_time:32285ms step_avg:58.17ms
step:556/2330 train_time:32348ms step_avg:58.18ms
step:557/2330 train_time:32404ms step_avg:58.18ms
step:558/2330 train_time:32467ms step_avg:58.18ms
step:559/2330 train_time:32523ms step_avg:58.18ms
step:560/2330 train_time:32585ms step_avg:58.19ms
step:561/2330 train_time:32640ms step_avg:58.18ms
step:562/2330 train_time:32703ms step_avg:58.19ms
step:563/2330 train_time:32759ms step_avg:58.19ms
step:564/2330 train_time:32820ms step_avg:58.19ms
step:565/2330 train_time:32877ms step_avg:58.19ms
step:566/2330 train_time:32937ms step_avg:58.19ms
step:567/2330 train_time:32994ms step_avg:58.19ms
step:568/2330 train_time:33054ms step_avg:58.19ms
step:569/2330 train_time:33110ms step_avg:58.19ms
step:570/2330 train_time:33170ms step_avg:58.19ms
step:571/2330 train_time:33227ms step_avg:58.19ms
step:572/2330 train_time:33287ms step_avg:58.19ms
step:573/2330 train_time:33343ms step_avg:58.19ms
step:574/2330 train_time:33405ms step_avg:58.20ms
step:575/2330 train_time:33461ms step_avg:58.19ms
step:576/2330 train_time:33523ms step_avg:58.20ms
step:577/2330 train_time:33579ms step_avg:58.20ms
step:578/2330 train_time:33640ms step_avg:58.20ms
step:579/2330 train_time:33696ms step_avg:58.20ms
step:580/2330 train_time:33758ms step_avg:58.20ms
step:581/2330 train_time:33814ms step_avg:58.20ms
step:582/2330 train_time:33875ms step_avg:58.20ms
step:583/2330 train_time:33931ms step_avg:58.20ms
step:584/2330 train_time:33991ms step_avg:58.20ms
step:585/2330 train_time:34047ms step_avg:58.20ms
step:586/2330 train_time:34109ms step_avg:58.21ms
step:587/2330 train_time:34165ms step_avg:58.20ms
step:588/2330 train_time:34225ms step_avg:58.21ms
step:589/2330 train_time:34282ms step_avg:58.20ms
step:590/2330 train_time:34342ms step_avg:58.21ms
step:591/2330 train_time:34398ms step_avg:58.20ms
step:592/2330 train_time:34459ms step_avg:58.21ms
step:593/2330 train_time:34517ms step_avg:58.21ms
step:594/2330 train_time:34576ms step_avg:58.21ms
step:595/2330 train_time:34632ms step_avg:58.21ms
step:596/2330 train_time:34693ms step_avg:58.21ms
step:597/2330 train_time:34749ms step_avg:58.21ms
step:598/2330 train_time:34811ms step_avg:58.21ms
step:599/2330 train_time:34866ms step_avg:58.21ms
step:600/2330 train_time:34928ms step_avg:58.21ms
step:601/2330 train_time:34984ms step_avg:58.21ms
step:602/2330 train_time:35046ms step_avg:58.22ms
step:603/2330 train_time:35102ms step_avg:58.21ms
step:604/2330 train_time:35163ms step_avg:58.22ms
step:605/2330 train_time:35219ms step_avg:58.21ms
step:606/2330 train_time:35280ms step_avg:58.22ms
step:607/2330 train_time:35336ms step_avg:58.21ms
step:608/2330 train_time:35397ms step_avg:58.22ms
step:609/2330 train_time:35453ms step_avg:58.22ms
step:610/2330 train_time:35514ms step_avg:58.22ms
step:611/2330 train_time:35570ms step_avg:58.22ms
step:612/2330 train_time:35632ms step_avg:58.22ms
step:613/2330 train_time:35688ms step_avg:58.22ms
step:614/2330 train_time:35750ms step_avg:58.22ms
step:615/2330 train_time:35806ms step_avg:58.22ms
step:616/2330 train_time:35867ms step_avg:58.23ms
step:617/2330 train_time:35923ms step_avg:58.22ms
step:618/2330 train_time:35984ms step_avg:58.23ms
step:619/2330 train_time:36040ms step_avg:58.22ms
step:620/2330 train_time:36101ms step_avg:58.23ms
step:621/2330 train_time:36157ms step_avg:58.22ms
step:622/2330 train_time:36218ms step_avg:58.23ms
step:623/2330 train_time:36274ms step_avg:58.23ms
step:624/2330 train_time:36335ms step_avg:58.23ms
step:625/2330 train_time:36391ms step_avg:58.23ms
step:626/2330 train_time:36452ms step_avg:58.23ms
step:627/2330 train_time:36508ms step_avg:58.23ms
step:628/2330 train_time:36569ms step_avg:58.23ms
step:629/2330 train_time:36626ms step_avg:58.23ms
step:630/2330 train_time:36687ms step_avg:58.23ms
step:631/2330 train_time:36743ms step_avg:58.23ms
step:632/2330 train_time:36805ms step_avg:58.24ms
step:633/2330 train_time:36861ms step_avg:58.23ms
step:634/2330 train_time:36922ms step_avg:58.24ms
step:635/2330 train_time:36978ms step_avg:58.23ms
step:636/2330 train_time:37038ms step_avg:58.24ms
step:637/2330 train_time:37094ms step_avg:58.23ms
step:638/2330 train_time:37155ms step_avg:58.24ms
step:639/2330 train_time:37210ms step_avg:58.23ms
step:640/2330 train_time:37272ms step_avg:58.24ms
step:641/2330 train_time:37327ms step_avg:58.23ms
step:642/2330 train_time:37390ms step_avg:58.24ms
step:643/2330 train_time:37445ms step_avg:58.24ms
step:644/2330 train_time:37507ms step_avg:58.24ms
step:645/2330 train_time:37563ms step_avg:58.24ms
step:646/2330 train_time:37625ms step_avg:58.24ms
step:647/2330 train_time:37681ms step_avg:58.24ms
step:648/2330 train_time:37743ms step_avg:58.25ms
step:649/2330 train_time:37799ms step_avg:58.24ms
step:650/2330 train_time:37861ms step_avg:58.25ms
step:651/2330 train_time:37917ms step_avg:58.24ms
step:652/2330 train_time:37978ms step_avg:58.25ms
step:653/2330 train_time:38035ms step_avg:58.25ms
step:654/2330 train_time:38095ms step_avg:58.25ms
step:655/2330 train_time:38152ms step_avg:58.25ms
step:656/2330 train_time:38213ms step_avg:58.25ms
step:657/2330 train_time:38269ms step_avg:58.25ms
step:658/2330 train_time:38330ms step_avg:58.25ms
step:659/2330 train_time:38387ms step_avg:58.25ms
step:660/2330 train_time:38447ms step_avg:58.25ms
step:661/2330 train_time:38503ms step_avg:58.25ms
step:662/2330 train_time:38565ms step_avg:58.26ms
step:663/2330 train_time:38622ms step_avg:58.25ms
step:664/2330 train_time:38683ms step_avg:58.26ms
step:665/2330 train_time:38739ms step_avg:58.25ms
step:666/2330 train_time:38801ms step_avg:58.26ms
step:667/2330 train_time:38857ms step_avg:58.26ms
step:668/2330 train_time:38919ms step_avg:58.26ms
step:669/2330 train_time:38975ms step_avg:58.26ms
step:670/2330 train_time:39036ms step_avg:58.26ms
step:671/2330 train_time:39093ms step_avg:58.26ms
step:672/2330 train_time:39154ms step_avg:58.26ms
step:673/2330 train_time:39210ms step_avg:58.26ms
step:674/2330 train_time:39271ms step_avg:58.26ms
step:675/2330 train_time:39327ms step_avg:58.26ms
step:676/2330 train_time:39388ms step_avg:58.27ms
step:677/2330 train_time:39444ms step_avg:58.26ms
step:678/2330 train_time:39505ms step_avg:58.27ms
step:679/2330 train_time:39562ms step_avg:58.27ms
step:680/2330 train_time:39624ms step_avg:58.27ms
step:681/2330 train_time:39680ms step_avg:58.27ms
step:682/2330 train_time:39741ms step_avg:58.27ms
step:683/2330 train_time:39797ms step_avg:58.27ms
step:684/2330 train_time:39859ms step_avg:58.27ms
step:685/2330 train_time:39916ms step_avg:58.27ms
step:686/2330 train_time:39976ms step_avg:58.27ms
step:687/2330 train_time:40032ms step_avg:58.27ms
step:688/2330 train_time:40093ms step_avg:58.27ms
step:689/2330 train_time:40149ms step_avg:58.27ms
step:690/2330 train_time:40210ms step_avg:58.27ms
step:691/2330 train_time:40266ms step_avg:58.27ms
step:692/2330 train_time:40326ms step_avg:58.27ms
step:693/2330 train_time:40382ms step_avg:58.27ms
step:694/2330 train_time:40444ms step_avg:58.28ms
step:695/2330 train_time:40500ms step_avg:58.27ms
step:696/2330 train_time:40561ms step_avg:58.28ms
step:697/2330 train_time:40617ms step_avg:58.27ms
step:698/2330 train_time:40679ms step_avg:58.28ms
step:699/2330 train_time:40736ms step_avg:58.28ms
step:700/2330 train_time:40796ms step_avg:58.28ms
step:701/2330 train_time:40853ms step_avg:58.28ms
step:702/2330 train_time:40913ms step_avg:58.28ms
step:703/2330 train_time:40969ms step_avg:58.28ms
step:704/2330 train_time:41030ms step_avg:58.28ms
step:705/2330 train_time:41087ms step_avg:58.28ms
step:706/2330 train_time:41148ms step_avg:58.28ms
step:707/2330 train_time:41204ms step_avg:58.28ms
step:708/2330 train_time:41266ms step_avg:58.28ms
step:709/2330 train_time:41322ms step_avg:58.28ms
step:710/2330 train_time:41383ms step_avg:58.29ms
step:711/2330 train_time:41439ms step_avg:58.28ms
step:712/2330 train_time:41500ms step_avg:58.29ms
step:713/2330 train_time:41557ms step_avg:58.28ms
step:714/2330 train_time:41617ms step_avg:58.29ms
step:715/2330 train_time:41674ms step_avg:58.28ms
step:716/2330 train_time:41735ms step_avg:58.29ms
step:717/2330 train_time:41791ms step_avg:58.29ms
step:718/2330 train_time:41852ms step_avg:58.29ms
step:719/2330 train_time:41908ms step_avg:58.29ms
step:720/2330 train_time:41970ms step_avg:58.29ms
step:721/2330 train_time:42027ms step_avg:58.29ms
step:722/2330 train_time:42087ms step_avg:58.29ms
step:723/2330 train_time:42143ms step_avg:58.29ms
step:724/2330 train_time:42205ms step_avg:58.29ms
step:725/2330 train_time:42262ms step_avg:58.29ms
step:726/2330 train_time:42323ms step_avg:58.30ms
step:727/2330 train_time:42379ms step_avg:58.29ms
step:728/2330 train_time:42440ms step_avg:58.30ms
step:729/2330 train_time:42496ms step_avg:58.29ms
step:730/2330 train_time:42556ms step_avg:58.30ms
step:731/2330 train_time:42612ms step_avg:58.29ms
step:732/2330 train_time:42673ms step_avg:58.30ms
step:733/2330 train_time:42730ms step_avg:58.29ms
step:734/2330 train_time:42791ms step_avg:58.30ms
step:735/2330 train_time:42847ms step_avg:58.29ms
step:736/2330 train_time:42909ms step_avg:58.30ms
step:737/2330 train_time:42965ms step_avg:58.30ms
step:738/2330 train_time:43026ms step_avg:58.30ms
step:739/2330 train_time:43082ms step_avg:58.30ms
step:740/2330 train_time:43144ms step_avg:58.30ms
step:741/2330 train_time:43200ms step_avg:58.30ms
step:742/2330 train_time:43260ms step_avg:58.30ms
step:743/2330 train_time:43317ms step_avg:58.30ms
step:744/2330 train_time:43378ms step_avg:58.30ms
step:745/2330 train_time:43434ms step_avg:58.30ms
step:746/2330 train_time:43494ms step_avg:58.30ms
step:747/2330 train_time:43550ms step_avg:58.30ms
step:748/2330 train_time:43612ms step_avg:58.30ms
step:749/2330 train_time:43668ms step_avg:58.30ms
step:750/2330 train_time:43729ms step_avg:58.31ms
step:750/2330 val_loss:5.3037 train_time:43807ms step_avg:58.41ms
step:751/2330 train_time:43826ms step_avg:58.36ms
step:752/2330 train_time:43849ms step_avg:58.31ms
step:753/2330 train_time:43905ms step_avg:58.31ms
step:754/2330 train_time:43968ms step_avg:58.31ms
step:755/2330 train_time:44025ms step_avg:58.31ms
step:756/2330 train_time:44088ms step_avg:58.32ms
step:757/2330 train_time:44143ms step_avg:58.31ms
step:758/2330 train_time:44206ms step_avg:58.32ms
step:759/2330 train_time:44262ms step_avg:58.32ms
step:760/2330 train_time:44323ms step_avg:58.32ms
step:761/2330 train_time:44378ms step_avg:58.32ms
step:762/2330 train_time:44439ms step_avg:58.32ms
step:763/2330 train_time:44495ms step_avg:58.32ms
step:764/2330 train_time:44555ms step_avg:58.32ms
step:765/2330 train_time:44612ms step_avg:58.32ms
step:766/2330 train_time:44671ms step_avg:58.32ms
step:767/2330 train_time:44728ms step_avg:58.32ms
step:768/2330 train_time:44789ms step_avg:58.32ms
step:769/2330 train_time:44846ms step_avg:58.32ms
step:770/2330 train_time:44909ms step_avg:58.32ms
step:771/2330 train_time:44966ms step_avg:58.32ms
step:772/2330 train_time:45029ms step_avg:58.33ms
step:773/2330 train_time:45087ms step_avg:58.33ms
step:774/2330 train_time:45149ms step_avg:58.33ms
step:775/2330 train_time:45206ms step_avg:58.33ms
step:776/2330 train_time:45268ms step_avg:58.34ms
step:777/2330 train_time:45325ms step_avg:58.33ms
step:778/2330 train_time:45386ms step_avg:58.34ms
step:779/2330 train_time:45443ms step_avg:58.33ms
step:780/2330 train_time:45504ms step_avg:58.34ms
step:781/2330 train_time:45561ms step_avg:58.34ms
step:782/2330 train_time:45622ms step_avg:58.34ms
step:783/2330 train_time:45679ms step_avg:58.34ms
step:784/2330 train_time:45740ms step_avg:58.34ms
step:785/2330 train_time:45797ms step_avg:58.34ms
step:786/2330 train_time:45859ms step_avg:58.35ms
step:787/2330 train_time:45916ms step_avg:58.34ms
step:788/2330 train_time:45979ms step_avg:58.35ms
step:789/2330 train_time:46036ms step_avg:58.35ms
step:790/2330 train_time:46099ms step_avg:58.35ms
step:791/2330 train_time:46156ms step_avg:58.35ms
step:792/2330 train_time:46218ms step_avg:58.36ms
step:793/2330 train_time:46275ms step_avg:58.35ms
step:794/2330 train_time:46337ms step_avg:58.36ms
step:795/2330 train_time:46394ms step_avg:58.36ms
step:796/2330 train_time:46456ms step_avg:58.36ms
step:797/2330 train_time:46512ms step_avg:58.36ms
step:798/2330 train_time:46574ms step_avg:58.36ms
step:799/2330 train_time:46631ms step_avg:58.36ms
step:800/2330 train_time:46692ms step_avg:58.37ms
step:801/2330 train_time:46749ms step_avg:58.36ms
step:802/2330 train_time:46811ms step_avg:58.37ms
step:803/2330 train_time:46868ms step_avg:58.37ms
step:804/2330 train_time:46929ms step_avg:58.37ms
step:805/2330 train_time:46987ms step_avg:58.37ms
step:806/2330 train_time:47049ms step_avg:58.37ms
step:807/2330 train_time:47107ms step_avg:58.37ms
step:808/2330 train_time:47169ms step_avg:58.38ms
step:809/2330 train_time:47226ms step_avg:58.38ms
step:810/2330 train_time:47288ms step_avg:58.38ms
step:811/2330 train_time:47345ms step_avg:58.38ms
step:812/2330 train_time:47406ms step_avg:58.38ms
step:813/2330 train_time:47463ms step_avg:58.38ms
step:814/2330 train_time:47524ms step_avg:58.38ms
step:815/2330 train_time:47582ms step_avg:58.38ms
step:816/2330 train_time:47643ms step_avg:58.39ms
step:817/2330 train_time:47699ms step_avg:58.38ms
step:818/2330 train_time:47762ms step_avg:58.39ms
step:819/2330 train_time:47818ms step_avg:58.39ms
step:820/2330 train_time:47881ms step_avg:58.39ms
step:821/2330 train_time:47938ms step_avg:58.39ms
step:822/2330 train_time:48001ms step_avg:58.40ms
step:823/2330 train_time:48057ms step_avg:58.39ms
step:824/2330 train_time:48120ms step_avg:58.40ms
step:825/2330 train_time:48176ms step_avg:58.40ms
step:826/2330 train_time:48239ms step_avg:58.40ms
step:827/2330 train_time:48295ms step_avg:58.40ms
step:828/2330 train_time:48357ms step_avg:58.40ms
step:829/2330 train_time:48414ms step_avg:58.40ms
step:830/2330 train_time:48476ms step_avg:58.41ms
step:831/2330 train_time:48534ms step_avg:58.40ms
step:832/2330 train_time:48595ms step_avg:58.41ms
step:833/2330 train_time:48651ms step_avg:58.40ms
step:834/2330 train_time:48712ms step_avg:58.41ms
step:835/2330 train_time:48770ms step_avg:58.41ms
step:836/2330 train_time:48831ms step_avg:58.41ms
step:837/2330 train_time:48889ms step_avg:58.41ms
step:838/2330 train_time:48950ms step_avg:58.41ms
step:839/2330 train_time:49008ms step_avg:58.41ms
step:840/2330 train_time:49069ms step_avg:58.42ms
step:841/2330 train_time:49126ms step_avg:58.41ms
step:842/2330 train_time:49188ms step_avg:58.42ms
step:843/2330 train_time:49245ms step_avg:58.42ms
step:844/2330 train_time:49307ms step_avg:58.42ms
step:845/2330 train_time:49364ms step_avg:58.42ms
step:846/2330 train_time:49426ms step_avg:58.42ms
step:847/2330 train_time:49483ms step_avg:58.42ms
step:848/2330 train_time:49545ms step_avg:58.43ms
step:849/2330 train_time:49601ms step_avg:58.42ms
step:850/2330 train_time:49663ms step_avg:58.43ms
step:851/2330 train_time:49719ms step_avg:58.42ms
step:852/2330 train_time:49781ms step_avg:58.43ms
step:853/2330 train_time:49838ms step_avg:58.43ms
step:854/2330 train_time:49901ms step_avg:58.43ms
step:855/2330 train_time:49957ms step_avg:58.43ms
step:856/2330 train_time:50020ms step_avg:58.43ms
step:857/2330 train_time:50076ms step_avg:58.43ms
step:858/2330 train_time:50139ms step_avg:58.44ms
step:859/2330 train_time:50196ms step_avg:58.44ms
step:860/2330 train_time:50258ms step_avg:58.44ms
step:861/2330 train_time:50315ms step_avg:58.44ms
step:862/2330 train_time:50377ms step_avg:58.44ms
step:863/2330 train_time:50433ms step_avg:58.44ms
step:864/2330 train_time:50495ms step_avg:58.44ms
step:865/2330 train_time:50552ms step_avg:58.44ms
step:866/2330 train_time:50614ms step_avg:58.45ms
step:867/2330 train_time:50672ms step_avg:58.44ms
step:868/2330 train_time:50733ms step_avg:58.45ms
step:869/2330 train_time:50790ms step_avg:58.45ms
step:870/2330 train_time:50851ms step_avg:58.45ms
step:871/2330 train_time:50908ms step_avg:58.45ms
step:872/2330 train_time:50970ms step_avg:58.45ms
step:873/2330 train_time:51027ms step_avg:58.45ms
step:874/2330 train_time:51089ms step_avg:58.45ms
step:875/2330 train_time:51145ms step_avg:58.45ms
step:876/2330 train_time:51207ms step_avg:58.46ms
step:877/2330 train_time:51264ms step_avg:58.45ms
step:878/2330 train_time:51326ms step_avg:58.46ms
step:879/2330 train_time:51383ms step_avg:58.46ms
step:880/2330 train_time:51445ms step_avg:58.46ms
step:881/2330 train_time:51501ms step_avg:58.46ms
step:882/2330 train_time:51563ms step_avg:58.46ms
step:883/2330 train_time:51620ms step_avg:58.46ms
step:884/2330 train_time:51682ms step_avg:58.46ms
step:885/2330 train_time:51739ms step_avg:58.46ms
step:886/2330 train_time:51802ms step_avg:58.47ms
step:887/2330 train_time:51858ms step_avg:58.46ms
step:888/2330 train_time:51921ms step_avg:58.47ms
step:889/2330 train_time:51977ms step_avg:58.47ms
step:890/2330 train_time:52040ms step_avg:58.47ms
step:891/2330 train_time:52096ms step_avg:58.47ms
step:892/2330 train_time:52158ms step_avg:58.47ms
step:893/2330 train_time:52215ms step_avg:58.47ms
step:894/2330 train_time:52277ms step_avg:58.48ms
step:895/2330 train_time:52334ms step_avg:58.47ms
step:896/2330 train_time:52395ms step_avg:58.48ms
step:897/2330 train_time:52452ms step_avg:58.48ms
step:898/2330 train_time:52514ms step_avg:58.48ms
step:899/2330 train_time:52571ms step_avg:58.48ms
step:900/2330 train_time:52633ms step_avg:58.48ms
step:901/2330 train_time:52691ms step_avg:58.48ms
step:902/2330 train_time:52752ms step_avg:58.48ms
step:903/2330 train_time:52809ms step_avg:58.48ms
step:904/2330 train_time:52870ms step_avg:58.48ms
step:905/2330 train_time:52927ms step_avg:58.48ms
step:906/2330 train_time:52988ms step_avg:58.49ms
step:907/2330 train_time:53045ms step_avg:58.48ms
step:908/2330 train_time:53107ms step_avg:58.49ms
step:909/2330 train_time:53164ms step_avg:58.49ms
step:910/2330 train_time:53225ms step_avg:58.49ms
step:911/2330 train_time:53282ms step_avg:58.49ms
step:912/2330 train_time:53344ms step_avg:58.49ms
step:913/2330 train_time:53400ms step_avg:58.49ms
step:914/2330 train_time:53462ms step_avg:58.49ms
step:915/2330 train_time:53519ms step_avg:58.49ms
step:916/2330 train_time:53581ms step_avg:58.49ms
step:917/2330 train_time:53638ms step_avg:58.49ms
step:918/2330 train_time:53700ms step_avg:58.50ms
step:919/2330 train_time:53757ms step_avg:58.49ms
step:920/2330 train_time:53819ms step_avg:58.50ms
step:921/2330 train_time:53876ms step_avg:58.50ms
step:922/2330 train_time:53939ms step_avg:58.50ms
step:923/2330 train_time:53995ms step_avg:58.50ms
step:924/2330 train_time:54058ms step_avg:58.50ms
step:925/2330 train_time:54114ms step_avg:58.50ms
step:926/2330 train_time:54177ms step_avg:58.51ms
step:927/2330 train_time:54234ms step_avg:58.50ms
step:928/2330 train_time:54296ms step_avg:58.51ms
step:929/2330 train_time:54352ms step_avg:58.51ms
step:930/2330 train_time:54414ms step_avg:58.51ms
step:931/2330 train_time:54472ms step_avg:58.51ms
step:932/2330 train_time:54532ms step_avg:58.51ms
step:933/2330 train_time:54590ms step_avg:58.51ms
step:934/2330 train_time:54650ms step_avg:58.51ms
step:935/2330 train_time:54708ms step_avg:58.51ms
step:936/2330 train_time:54768ms step_avg:58.51ms
step:937/2330 train_time:54826ms step_avg:58.51ms
step:938/2330 train_time:54887ms step_avg:58.51ms
step:939/2330 train_time:54944ms step_avg:58.51ms
step:940/2330 train_time:55005ms step_avg:58.52ms
step:941/2330 train_time:55062ms step_avg:58.51ms
step:942/2330 train_time:55124ms step_avg:58.52ms
step:943/2330 train_time:55180ms step_avg:58.52ms
step:944/2330 train_time:55243ms step_avg:58.52ms
step:945/2330 train_time:55299ms step_avg:58.52ms
step:946/2330 train_time:55362ms step_avg:58.52ms
step:947/2330 train_time:55418ms step_avg:58.52ms
step:948/2330 train_time:55481ms step_avg:58.52ms
step:949/2330 train_time:55537ms step_avg:58.52ms
step:950/2330 train_time:55599ms step_avg:58.53ms
step:951/2330 train_time:55656ms step_avg:58.52ms
step:952/2330 train_time:55718ms step_avg:58.53ms
step:953/2330 train_time:55775ms step_avg:58.53ms
step:954/2330 train_time:55837ms step_avg:58.53ms
step:955/2330 train_time:55894ms step_avg:58.53ms
step:956/2330 train_time:55957ms step_avg:58.53ms
step:957/2330 train_time:56014ms step_avg:58.53ms
step:958/2330 train_time:56076ms step_avg:58.53ms
step:959/2330 train_time:56133ms step_avg:58.53ms
step:960/2330 train_time:56194ms step_avg:58.54ms
step:961/2330 train_time:56251ms step_avg:58.53ms
step:962/2330 train_time:56312ms step_avg:58.54ms
step:963/2330 train_time:56369ms step_avg:58.53ms
step:964/2330 train_time:56429ms step_avg:58.54ms
step:965/2330 train_time:56487ms step_avg:58.54ms
step:966/2330 train_time:56549ms step_avg:58.54ms
step:967/2330 train_time:56606ms step_avg:58.54ms
step:968/2330 train_time:56667ms step_avg:58.54ms
step:969/2330 train_time:56724ms step_avg:58.54ms
step:970/2330 train_time:56786ms step_avg:58.54ms
step:971/2330 train_time:56842ms step_avg:58.54ms
step:972/2330 train_time:56905ms step_avg:58.54ms
step:973/2330 train_time:56961ms step_avg:58.54ms
step:974/2330 train_time:57023ms step_avg:58.55ms
step:975/2330 train_time:57080ms step_avg:58.54ms
step:976/2330 train_time:57143ms step_avg:58.55ms
step:977/2330 train_time:57199ms step_avg:58.55ms
step:978/2330 train_time:57262ms step_avg:58.55ms
step:979/2330 train_time:57318ms step_avg:58.55ms
step:980/2330 train_time:57382ms step_avg:58.55ms
step:981/2330 train_time:57438ms step_avg:58.55ms
step:982/2330 train_time:57501ms step_avg:58.56ms
step:983/2330 train_time:57558ms step_avg:58.55ms
step:984/2330 train_time:57620ms step_avg:58.56ms
step:985/2330 train_time:57677ms step_avg:58.56ms
step:986/2330 train_time:57738ms step_avg:58.56ms
step:987/2330 train_time:57794ms step_avg:58.56ms
step:988/2330 train_time:57858ms step_avg:58.56ms
step:989/2330 train_time:57914ms step_avg:58.56ms
step:990/2330 train_time:57976ms step_avg:58.56ms
step:991/2330 train_time:58034ms step_avg:58.56ms
step:992/2330 train_time:58097ms step_avg:58.57ms
step:993/2330 train_time:58153ms step_avg:58.56ms
step:994/2330 train_time:58215ms step_avg:58.57ms
step:995/2330 train_time:58273ms step_avg:58.57ms
step:996/2330 train_time:58334ms step_avg:58.57ms
step:997/2330 train_time:58391ms step_avg:58.57ms
step:998/2330 train_time:58452ms step_avg:58.57ms
step:999/2330 train_time:58509ms step_avg:58.57ms
step:1000/2330 train_time:58571ms step_avg:58.57ms
step:1000/2330 val_loss:4.9839 train_time:58649ms step_avg:58.65ms
step:1001/2330 train_time:58669ms step_avg:58.61ms
step:1002/2330 train_time:58691ms step_avg:58.57ms
step:1003/2330 train_time:58750ms step_avg:58.57ms
step:1004/2330 train_time:58816ms step_avg:58.58ms
step:1005/2330 train_time:58873ms step_avg:58.58ms
step:1006/2330 train_time:58938ms step_avg:58.59ms
step:1007/2330 train_time:58995ms step_avg:58.58ms
step:1008/2330 train_time:59057ms step_avg:58.59ms
step:1009/2330 train_time:59113ms step_avg:58.59ms
step:1010/2330 train_time:59174ms step_avg:58.59ms
step:1011/2330 train_time:59231ms step_avg:58.59ms
step:1012/2330 train_time:59292ms step_avg:58.59ms
step:1013/2330 train_time:59348ms step_avg:58.59ms
step:1014/2330 train_time:59409ms step_avg:58.59ms
step:1015/2330 train_time:59465ms step_avg:58.59ms
step:1016/2330 train_time:59526ms step_avg:58.59ms
step:1017/2330 train_time:59585ms step_avg:58.59ms
step:1018/2330 train_time:59646ms step_avg:58.59ms
step:1019/2330 train_time:59703ms step_avg:58.59ms
step:1020/2330 train_time:59767ms step_avg:58.60ms
step:1021/2330 train_time:59824ms step_avg:58.59ms
step:1022/2330 train_time:59887ms step_avg:58.60ms
step:1023/2330 train_time:59944ms step_avg:58.60ms
step:1024/2330 train_time:60008ms step_avg:58.60ms
step:1025/2330 train_time:60064ms step_avg:58.60ms
step:1026/2330 train_time:60127ms step_avg:58.60ms
step:1027/2330 train_time:60184ms step_avg:58.60ms
step:1028/2330 train_time:60246ms step_avg:58.61ms
step:1029/2330 train_time:60303ms step_avg:58.60ms
step:1030/2330 train_time:60364ms step_avg:58.61ms
step:1031/2330 train_time:60420ms step_avg:58.60ms
step:1032/2330 train_time:60482ms step_avg:58.61ms
step:1033/2330 train_time:60538ms step_avg:58.60ms
step:1034/2330 train_time:60600ms step_avg:58.61ms
step:1035/2330 train_time:60656ms step_avg:58.61ms
step:1036/2330 train_time:60719ms step_avg:58.61ms
step:1037/2330 train_time:60777ms step_avg:58.61ms
step:1038/2330 train_time:60840ms step_avg:58.61ms
step:1039/2330 train_time:60898ms step_avg:58.61ms
step:1040/2330 train_time:60959ms step_avg:58.61ms
step:1041/2330 train_time:61017ms step_avg:58.61ms
step:1042/2330 train_time:61079ms step_avg:58.62ms
step:1043/2330 train_time:61136ms step_avg:58.62ms
step:1044/2330 train_time:61198ms step_avg:58.62ms
step:1045/2330 train_time:61255ms step_avg:58.62ms
step:1046/2330 train_time:61315ms step_avg:58.62ms
step:1047/2330 train_time:61373ms step_avg:58.62ms
step:1048/2330 train_time:61434ms step_avg:58.62ms
step:1049/2330 train_time:61491ms step_avg:58.62ms
step:1050/2330 train_time:61552ms step_avg:58.62ms
step:1051/2330 train_time:61608ms step_avg:58.62ms
step:1052/2330 train_time:61670ms step_avg:58.62ms
step:1053/2330 train_time:61726ms step_avg:58.62ms
step:1054/2330 train_time:61789ms step_avg:58.62ms
step:1055/2330 train_time:61845ms step_avg:58.62ms
step:1056/2330 train_time:61909ms step_avg:58.63ms
step:1057/2330 train_time:61966ms step_avg:58.62ms
step:1058/2330 train_time:62029ms step_avg:58.63ms
step:1059/2330 train_time:62085ms step_avg:58.63ms
step:1060/2330 train_time:62149ms step_avg:58.63ms
step:1061/2330 train_time:62205ms step_avg:58.63ms
step:1062/2330 train_time:62268ms step_avg:58.63ms
step:1063/2330 train_time:62325ms step_avg:58.63ms
step:1064/2330 train_time:62388ms step_avg:58.64ms
step:1065/2330 train_time:62445ms step_avg:58.63ms
step:1066/2330 train_time:62507ms step_avg:58.64ms
step:1067/2330 train_time:62564ms step_avg:58.64ms
step:1068/2330 train_time:62625ms step_avg:58.64ms
step:1069/2330 train_time:62681ms step_avg:58.64ms
step:1070/2330 train_time:62744ms step_avg:58.64ms
step:1071/2330 train_time:62800ms step_avg:58.64ms
step:1072/2330 train_time:62863ms step_avg:58.64ms
step:1073/2330 train_time:62919ms step_avg:58.64ms
step:1074/2330 train_time:62983ms step_avg:58.64ms
step:1075/2330 train_time:63040ms step_avg:58.64ms
step:1076/2330 train_time:63104ms step_avg:58.65ms
step:1077/2330 train_time:63160ms step_avg:58.64ms
step:1078/2330 train_time:63223ms step_avg:58.65ms
step:1079/2330 train_time:63280ms step_avg:58.65ms
step:1080/2330 train_time:63342ms step_avg:58.65ms
step:1081/2330 train_time:63399ms step_avg:58.65ms
step:1082/2330 train_time:63462ms step_avg:58.65ms
step:1083/2330 train_time:63519ms step_avg:58.65ms
step:1084/2330 train_time:63581ms step_avg:58.65ms
step:1085/2330 train_time:63638ms step_avg:58.65ms
step:1086/2330 train_time:63699ms step_avg:58.65ms
step:1087/2330 train_time:63755ms step_avg:58.65ms
step:1088/2330 train_time:63817ms step_avg:58.66ms
step:1089/2330 train_time:63874ms step_avg:58.65ms
step:1090/2330 train_time:63935ms step_avg:58.66ms
step:1091/2330 train_time:63992ms step_avg:58.65ms
step:1092/2330 train_time:64054ms step_avg:58.66ms
step:1093/2330 train_time:64111ms step_avg:58.66ms
step:1094/2330 train_time:64173ms step_avg:58.66ms
step:1095/2330 train_time:64231ms step_avg:58.66ms
step:1096/2330 train_time:64292ms step_avg:58.66ms
step:1097/2330 train_time:64351ms step_avg:58.66ms
step:1098/2330 train_time:64412ms step_avg:58.66ms
step:1099/2330 train_time:64469ms step_avg:58.66ms
step:1100/2330 train_time:64530ms step_avg:58.66ms
step:1101/2330 train_time:64587ms step_avg:58.66ms
step:1102/2330 train_time:64649ms step_avg:58.67ms
step:1103/2330 train_time:64705ms step_avg:58.66ms
step:1104/2330 train_time:64768ms step_avg:58.67ms
step:1105/2330 train_time:64824ms step_avg:58.66ms
step:1106/2330 train_time:64886ms step_avg:58.67ms
step:1107/2330 train_time:64943ms step_avg:58.67ms
step:1108/2330 train_time:65005ms step_avg:58.67ms
step:1109/2330 train_time:65062ms step_avg:58.67ms
step:1110/2330 train_time:65125ms step_avg:58.67ms
step:1111/2330 train_time:65181ms step_avg:58.67ms
step:1112/2330 train_time:65244ms step_avg:58.67ms
step:1113/2330 train_time:65301ms step_avg:58.67ms
step:1114/2330 train_time:65363ms step_avg:58.67ms
step:1115/2330 train_time:65420ms step_avg:58.67ms
step:1116/2330 train_time:65483ms step_avg:58.68ms
step:1117/2330 train_time:65540ms step_avg:58.67ms
step:1118/2330 train_time:65602ms step_avg:58.68ms
step:1119/2330 train_time:65659ms step_avg:58.68ms
step:1120/2330 train_time:65721ms step_avg:58.68ms
step:1121/2330 train_time:65778ms step_avg:58.68ms
step:1122/2330 train_time:65840ms step_avg:58.68ms
step:1123/2330 train_time:65897ms step_avg:58.68ms
step:1124/2330 train_time:65958ms step_avg:58.68ms
step:1125/2330 train_time:66016ms step_avg:58.68ms
step:1126/2330 train_time:66077ms step_avg:58.68ms
step:1127/2330 train_time:66135ms step_avg:58.68ms
step:1128/2330 train_time:66196ms step_avg:58.68ms
step:1129/2330 train_time:66254ms step_avg:58.68ms
step:1130/2330 train_time:66315ms step_avg:58.69ms
step:1131/2330 train_time:66372ms step_avg:58.68ms
step:1132/2330 train_time:66434ms step_avg:58.69ms
step:1133/2330 train_time:66492ms step_avg:58.69ms
step:1134/2330 train_time:66553ms step_avg:58.69ms
step:1135/2330 train_time:66611ms step_avg:58.69ms
step:1136/2330 train_time:66671ms step_avg:58.69ms
step:1137/2330 train_time:66728ms step_avg:58.69ms
step:1138/2330 train_time:66789ms step_avg:58.69ms
step:1139/2330 train_time:66846ms step_avg:58.69ms
step:1140/2330 train_time:66909ms step_avg:58.69ms
step:1141/2330 train_time:66965ms step_avg:58.69ms
step:1142/2330 train_time:67029ms step_avg:58.69ms
step:1143/2330 train_time:67085ms step_avg:58.69ms
step:1144/2330 train_time:67148ms step_avg:58.70ms
step:1145/2330 train_time:67204ms step_avg:58.69ms
step:1146/2330 train_time:67267ms step_avg:58.70ms
step:1147/2330 train_time:67323ms step_avg:58.69ms
step:1148/2330 train_time:67386ms step_avg:58.70ms
step:1149/2330 train_time:67443ms step_avg:58.70ms
step:1150/2330 train_time:67505ms step_avg:58.70ms
step:1151/2330 train_time:67562ms step_avg:58.70ms
step:1152/2330 train_time:67624ms step_avg:58.70ms
step:1153/2330 train_time:67681ms step_avg:58.70ms
step:1154/2330 train_time:67743ms step_avg:58.70ms
step:1155/2330 train_time:67800ms step_avg:58.70ms
step:1156/2330 train_time:67863ms step_avg:58.71ms
step:1157/2330 train_time:67920ms step_avg:58.70ms
step:1158/2330 train_time:67982ms step_avg:58.71ms
step:1159/2330 train_time:68039ms step_avg:58.71ms
step:1160/2330 train_time:68102ms step_avg:58.71ms
step:1161/2330 train_time:68159ms step_avg:58.71ms
step:1162/2330 train_time:68222ms step_avg:58.71ms
step:1163/2330 train_time:68280ms step_avg:58.71ms
step:1164/2330 train_time:68342ms step_avg:58.71ms
step:1165/2330 train_time:68398ms step_avg:58.71ms
step:1166/2330 train_time:68460ms step_avg:58.71ms
step:1167/2330 train_time:68518ms step_avg:58.71ms
step:1168/2330 train_time:68580ms step_avg:58.72ms
step:1169/2330 train_time:68637ms step_avg:58.71ms
step:1170/2330 train_time:68699ms step_avg:58.72ms
step:1171/2330 train_time:68756ms step_avg:58.72ms
step:1172/2330 train_time:68817ms step_avg:58.72ms
step:1173/2330 train_time:68875ms step_avg:58.72ms
step:1174/2330 train_time:68936ms step_avg:58.72ms
step:1175/2330 train_time:68993ms step_avg:58.72ms
step:1176/2330 train_time:69054ms step_avg:58.72ms
step:1177/2330 train_time:69112ms step_avg:58.72ms
step:1178/2330 train_time:69174ms step_avg:58.72ms
step:1179/2330 train_time:69232ms step_avg:58.72ms
step:1180/2330 train_time:69293ms step_avg:58.72ms
step:1181/2330 train_time:69351ms step_avg:58.72ms
step:1182/2330 train_time:69411ms step_avg:58.72ms
step:1183/2330 train_time:69468ms step_avg:58.72ms
step:1184/2330 train_time:69530ms step_avg:58.72ms
step:1185/2330 train_time:69587ms step_avg:58.72ms
step:1186/2330 train_time:69650ms step_avg:58.73ms
step:1187/2330 train_time:69706ms step_avg:58.72ms
step:1188/2330 train_time:69769ms step_avg:58.73ms
step:1189/2330 train_time:69826ms step_avg:58.73ms
step:1190/2330 train_time:69888ms step_avg:58.73ms
step:1191/2330 train_time:69944ms step_avg:58.73ms
step:1192/2330 train_time:70008ms step_avg:58.73ms
step:1193/2330 train_time:70064ms step_avg:58.73ms
step:1194/2330 train_time:70127ms step_avg:58.73ms
step:1195/2330 train_time:70183ms step_avg:58.73ms
step:1196/2330 train_time:70246ms step_avg:58.73ms
step:1197/2330 train_time:70302ms step_avg:58.73ms
step:1198/2330 train_time:70365ms step_avg:58.74ms
step:1199/2330 train_time:70422ms step_avg:58.73ms
step:1200/2330 train_time:70484ms step_avg:58.74ms
step:1201/2330 train_time:70541ms step_avg:58.73ms
step:1202/2330 train_time:70604ms step_avg:58.74ms
step:1203/2330 train_time:70661ms step_avg:58.74ms
step:1204/2330 train_time:70724ms step_avg:58.74ms
step:1205/2330 train_time:70781ms step_avg:58.74ms
step:1206/2330 train_time:70843ms step_avg:58.74ms
step:1207/2330 train_time:70899ms step_avg:58.74ms
step:1208/2330 train_time:70962ms step_avg:58.74ms
step:1209/2330 train_time:71019ms step_avg:58.74ms
step:1210/2330 train_time:71082ms step_avg:58.75ms
step:1211/2330 train_time:71138ms step_avg:58.74ms
step:1212/2330 train_time:71201ms step_avg:58.75ms
step:1213/2330 train_time:71258ms step_avg:58.75ms
step:1214/2330 train_time:71321ms step_avg:58.75ms
step:1215/2330 train_time:71378ms step_avg:58.75ms
step:1216/2330 train_time:71440ms step_avg:58.75ms
step:1217/2330 train_time:71497ms step_avg:58.75ms
step:1218/2330 train_time:71558ms step_avg:58.75ms
step:1219/2330 train_time:71615ms step_avg:58.75ms
step:1220/2330 train_time:71677ms step_avg:58.75ms
step:1221/2330 train_time:71734ms step_avg:58.75ms
step:1222/2330 train_time:71795ms step_avg:58.75ms
step:1223/2330 train_time:71852ms step_avg:58.75ms
step:1224/2330 train_time:71914ms step_avg:58.75ms
step:1225/2330 train_time:71971ms step_avg:58.75ms
step:1226/2330 train_time:72032ms step_avg:58.75ms
step:1227/2330 train_time:72090ms step_avg:58.75ms
step:1228/2330 train_time:72151ms step_avg:58.75ms
step:1229/2330 train_time:72208ms step_avg:58.75ms
step:1230/2330 train_time:72270ms step_avg:58.76ms
step:1231/2330 train_time:72326ms step_avg:58.75ms
step:1232/2330 train_time:72389ms step_avg:58.76ms
step:1233/2330 train_time:72445ms step_avg:58.76ms
step:1234/2330 train_time:72509ms step_avg:58.76ms
step:1235/2330 train_time:72565ms step_avg:58.76ms
step:1236/2330 train_time:72627ms step_avg:58.76ms
step:1237/2330 train_time:72684ms step_avg:58.76ms
step:1238/2330 train_time:72746ms step_avg:58.76ms
step:1239/2330 train_time:72802ms step_avg:58.76ms
step:1240/2330 train_time:72865ms step_avg:58.76ms
step:1241/2330 train_time:72921ms step_avg:58.76ms
step:1242/2330 train_time:72984ms step_avg:58.76ms
step:1243/2330 train_time:73041ms step_avg:58.76ms
step:1244/2330 train_time:73103ms step_avg:58.76ms
step:1245/2330 train_time:73159ms step_avg:58.76ms
step:1246/2330 train_time:73224ms step_avg:58.77ms
step:1247/2330 train_time:73281ms step_avg:58.77ms
step:1248/2330 train_time:73343ms step_avg:58.77ms
step:1249/2330 train_time:73400ms step_avg:58.77ms
step:1250/2330 train_time:73463ms step_avg:58.77ms
step:1250/2330 val_loss:4.7092 train_time:73543ms step_avg:58.83ms
step:1251/2330 train_time:73562ms step_avg:58.80ms
step:1252/2330 train_time:73585ms step_avg:58.77ms
step:1253/2330 train_time:73643ms step_avg:58.77ms
step:1254/2330 train_time:73708ms step_avg:58.78ms
step:1255/2330 train_time:73764ms step_avg:58.78ms
step:1256/2330 train_time:73828ms step_avg:58.78ms
step:1257/2330 train_time:73884ms step_avg:58.78ms
step:1258/2330 train_time:73946ms step_avg:58.78ms
step:1259/2330 train_time:74002ms step_avg:58.78ms
step:1260/2330 train_time:74064ms step_avg:58.78ms
step:1261/2330 train_time:74120ms step_avg:58.78ms
step:1262/2330 train_time:74182ms step_avg:58.78ms
step:1263/2330 train_time:74238ms step_avg:58.78ms
step:1264/2330 train_time:74299ms step_avg:58.78ms
step:1265/2330 train_time:74356ms step_avg:58.78ms
step:1266/2330 train_time:74417ms step_avg:58.78ms
step:1267/2330 train_time:74473ms step_avg:58.78ms
step:1268/2330 train_time:74536ms step_avg:58.78ms
step:1269/2330 train_time:74594ms step_avg:58.78ms
step:1270/2330 train_time:74657ms step_avg:58.79ms
step:1271/2330 train_time:74715ms step_avg:58.78ms
step:1272/2330 train_time:74779ms step_avg:58.79ms
step:1273/2330 train_time:74835ms step_avg:58.79ms
step:1274/2330 train_time:74898ms step_avg:58.79ms
step:1275/2330 train_time:74955ms step_avg:58.79ms
step:1276/2330 train_time:75018ms step_avg:58.79ms
step:1277/2330 train_time:75075ms step_avg:58.79ms
step:1278/2330 train_time:75138ms step_avg:58.79ms
step:1279/2330 train_time:75195ms step_avg:58.79ms
step:1280/2330 train_time:75256ms step_avg:58.79ms
step:1281/2330 train_time:75313ms step_avg:58.79ms
step:1282/2330 train_time:75375ms step_avg:58.79ms
step:1283/2330 train_time:75431ms step_avg:58.79ms
step:1284/2330 train_time:75493ms step_avg:58.80ms
step:1285/2330 train_time:75551ms step_avg:58.79ms
step:1286/2330 train_time:75613ms step_avg:58.80ms
step:1287/2330 train_time:75671ms step_avg:58.80ms
step:1288/2330 train_time:75732ms step_avg:58.80ms
step:1289/2330 train_time:75790ms step_avg:58.80ms
step:1290/2330 train_time:75851ms step_avg:58.80ms
step:1291/2330 train_time:75909ms step_avg:58.80ms
step:1292/2330 train_time:75971ms step_avg:58.80ms
step:1293/2330 train_time:76029ms step_avg:58.80ms
step:1294/2330 train_time:76089ms step_avg:58.80ms
step:1295/2330 train_time:76146ms step_avg:58.80ms
step:1296/2330 train_time:76208ms step_avg:58.80ms
step:1297/2330 train_time:76265ms step_avg:58.80ms
step:1298/2330 train_time:76327ms step_avg:58.80ms
step:1299/2330 train_time:76383ms step_avg:58.80ms
step:1300/2330 train_time:76445ms step_avg:58.80ms
step:1301/2330 train_time:76501ms step_avg:58.80ms
step:1302/2330 train_time:76564ms step_avg:58.80ms
step:1303/2330 train_time:76620ms step_avg:58.80ms
step:1304/2330 train_time:76684ms step_avg:58.81ms
step:1305/2330 train_time:76740ms step_avg:58.80ms
step:1306/2330 train_time:76804ms step_avg:58.81ms
step:1307/2330 train_time:76860ms step_avg:58.81ms
step:1308/2330 train_time:76924ms step_avg:58.81ms
step:1309/2330 train_time:76980ms step_avg:58.81ms
step:1310/2330 train_time:77044ms step_avg:58.81ms
step:1311/2330 train_time:77100ms step_avg:58.81ms
step:1312/2330 train_time:77163ms step_avg:58.81ms
step:1313/2330 train_time:77219ms step_avg:58.81ms
step:1314/2330 train_time:77282ms step_avg:58.81ms
step:1315/2330 train_time:77338ms step_avg:58.81ms
step:1316/2330 train_time:77401ms step_avg:58.82ms
step:1317/2330 train_time:77457ms step_avg:58.81ms
step:1318/2330 train_time:77519ms step_avg:58.82ms
step:1319/2330 train_time:77575ms step_avg:58.81ms
step:1320/2330 train_time:77638ms step_avg:58.82ms
step:1321/2330 train_time:77694ms step_avg:58.81ms
step:1322/2330 train_time:77757ms step_avg:58.82ms
step:1323/2330 train_time:77815ms step_avg:58.82ms
step:1324/2330 train_time:77877ms step_avg:58.82ms
step:1325/2330 train_time:77934ms step_avg:58.82ms
step:1326/2330 train_time:77997ms step_avg:58.82ms
step:1327/2330 train_time:78053ms step_avg:58.82ms
step:1328/2330 train_time:78116ms step_avg:58.82ms
step:1329/2330 train_time:78174ms step_avg:58.82ms
step:1330/2330 train_time:78235ms step_avg:58.82ms
step:1331/2330 train_time:78293ms step_avg:58.82ms
step:1332/2330 train_time:78355ms step_avg:58.83ms
step:1333/2330 train_time:78413ms step_avg:58.82ms
step:1334/2330 train_time:78474ms step_avg:58.83ms
step:1335/2330 train_time:78531ms step_avg:58.82ms
step:1336/2330 train_time:78593ms step_avg:58.83ms
step:1337/2330 train_time:78650ms step_avg:58.83ms
step:1338/2330 train_time:78711ms step_avg:58.83ms
step:1339/2330 train_time:78768ms step_avg:58.83ms
step:1340/2330 train_time:78829ms step_avg:58.83ms
step:1341/2330 train_time:78887ms step_avg:58.83ms
step:1342/2330 train_time:78949ms step_avg:58.83ms
step:1343/2330 train_time:79006ms step_avg:58.83ms
step:1344/2330 train_time:79067ms step_avg:58.83ms
step:1345/2330 train_time:79124ms step_avg:58.83ms
step:1346/2330 train_time:79186ms step_avg:58.83ms
step:1347/2330 train_time:79242ms step_avg:58.83ms
step:1348/2330 train_time:79304ms step_avg:58.83ms
step:1349/2330 train_time:79361ms step_avg:58.83ms
step:1350/2330 train_time:79423ms step_avg:58.83ms
step:1351/2330 train_time:79480ms step_avg:58.83ms
step:1352/2330 train_time:79543ms step_avg:58.83ms
step:1353/2330 train_time:79599ms step_avg:58.83ms
step:1354/2330 train_time:79662ms step_avg:58.83ms
step:1355/2330 train_time:79719ms step_avg:58.83ms
step:1356/2330 train_time:79782ms step_avg:58.84ms
step:1357/2330 train_time:79838ms step_avg:58.83ms
step:1358/2330 train_time:79901ms step_avg:58.84ms
step:1359/2330 train_time:79957ms step_avg:58.84ms
step:1360/2330 train_time:80021ms step_avg:58.84ms
step:1361/2330 train_time:80077ms step_avg:58.84ms
step:1362/2330 train_time:80141ms step_avg:58.84ms
step:1363/2330 train_time:80197ms step_avg:58.84ms
step:1364/2330 train_time:80260ms step_avg:58.84ms
step:1365/2330 train_time:80316ms step_avg:58.84ms
step:1366/2330 train_time:80379ms step_avg:58.84ms
step:1367/2330 train_time:80436ms step_avg:58.84ms
step:1368/2330 train_time:80499ms step_avg:58.84ms
step:1369/2330 train_time:80556ms step_avg:58.84ms
step:1370/2330 train_time:80618ms step_avg:58.85ms
step:1371/2330 train_time:80675ms step_avg:58.84ms
step:1372/2330 train_time:80737ms step_avg:58.85ms
step:1373/2330 train_time:80794ms step_avg:58.84ms
step:1374/2330 train_time:80857ms step_avg:58.85ms
step:1375/2330 train_time:80914ms step_avg:58.85ms
step:1376/2330 train_time:80975ms step_avg:58.85ms
step:1377/2330 train_time:81032ms step_avg:58.85ms
step:1378/2330 train_time:81095ms step_avg:58.85ms
step:1379/2330 train_time:81152ms step_avg:58.85ms
step:1380/2330 train_time:81214ms step_avg:58.85ms
step:1381/2330 train_time:81271ms step_avg:58.85ms
step:1382/2330 train_time:81332ms step_avg:58.85ms
step:1383/2330 train_time:81390ms step_avg:58.85ms
step:1384/2330 train_time:81451ms step_avg:58.85ms
step:1385/2330 train_time:81509ms step_avg:58.85ms
step:1386/2330 train_time:81570ms step_avg:58.85ms
step:1387/2330 train_time:81627ms step_avg:58.85ms
step:1388/2330 train_time:81688ms step_avg:58.85ms
step:1389/2330 train_time:81745ms step_avg:58.85ms
step:1390/2330 train_time:81807ms step_avg:58.85ms
step:1391/2330 train_time:81864ms step_avg:58.85ms
step:1392/2330 train_time:81925ms step_avg:58.85ms
step:1393/2330 train_time:81982ms step_avg:58.85ms
step:1394/2330 train_time:82044ms step_avg:58.86ms
step:1395/2330 train_time:82101ms step_avg:58.85ms
step:1396/2330 train_time:82164ms step_avg:58.86ms
step:1397/2330 train_time:82220ms step_avg:58.86ms
step:1398/2330 train_time:82283ms step_avg:58.86ms
step:1399/2330 train_time:82339ms step_avg:58.86ms
step:1400/2330 train_time:82402ms step_avg:58.86ms
step:1401/2330 train_time:82458ms step_avg:58.86ms
step:1402/2330 train_time:82521ms step_avg:58.86ms
step:1403/2330 train_time:82577ms step_avg:58.86ms
step:1404/2330 train_time:82640ms step_avg:58.86ms
step:1405/2330 train_time:82697ms step_avg:58.86ms
step:1406/2330 train_time:82759ms step_avg:58.86ms
step:1407/2330 train_time:82815ms step_avg:58.86ms
step:1408/2330 train_time:82878ms step_avg:58.86ms
step:1409/2330 train_time:82934ms step_avg:58.86ms
step:1410/2330 train_time:82998ms step_avg:58.86ms
step:1411/2330 train_time:83055ms step_avg:58.86ms
step:1412/2330 train_time:83118ms step_avg:58.87ms
step:1413/2330 train_time:83174ms step_avg:58.86ms
step:1414/2330 train_time:83237ms step_avg:58.87ms
step:1415/2330 train_time:83294ms step_avg:58.87ms
step:1416/2330 train_time:83358ms step_avg:58.87ms
step:1417/2330 train_time:83415ms step_avg:58.87ms
step:1418/2330 train_time:83478ms step_avg:58.87ms
step:1419/2330 train_time:83534ms step_avg:58.87ms
step:1420/2330 train_time:83597ms step_avg:58.87ms
step:1421/2330 train_time:83654ms step_avg:58.87ms
step:1422/2330 train_time:83716ms step_avg:58.87ms
step:1423/2330 train_time:83773ms step_avg:58.87ms
step:1424/2330 train_time:83835ms step_avg:58.87ms
step:1425/2330 train_time:83892ms step_avg:58.87ms
step:1426/2330 train_time:83954ms step_avg:58.87ms
step:1427/2330 train_time:84012ms step_avg:58.87ms
step:1428/2330 train_time:84074ms step_avg:58.88ms
step:1429/2330 train_time:84131ms step_avg:58.87ms
step:1430/2330 train_time:84192ms step_avg:58.88ms
step:1431/2330 train_time:84249ms step_avg:58.87ms
step:1432/2330 train_time:84311ms step_avg:58.88ms
step:1433/2330 train_time:84369ms step_avg:58.88ms
step:1434/2330 train_time:84429ms step_avg:58.88ms
step:1435/2330 train_time:84487ms step_avg:58.88ms
step:1436/2330 train_time:84548ms step_avg:58.88ms
step:1437/2330 train_time:84606ms step_avg:58.88ms
step:1438/2330 train_time:84667ms step_avg:58.88ms
step:1439/2330 train_time:84724ms step_avg:58.88ms
step:1440/2330 train_time:84785ms step_avg:58.88ms
step:1441/2330 train_time:84842ms step_avg:58.88ms
step:1442/2330 train_time:84904ms step_avg:58.88ms
step:1443/2330 train_time:84960ms step_avg:58.88ms
step:1444/2330 train_time:85023ms step_avg:58.88ms
step:1445/2330 train_time:85080ms step_avg:58.88ms
step:1446/2330 train_time:85142ms step_avg:58.88ms
step:1447/2330 train_time:85198ms step_avg:58.88ms
step:1448/2330 train_time:85262ms step_avg:58.88ms
step:1449/2330 train_time:85319ms step_avg:58.88ms
step:1450/2330 train_time:85382ms step_avg:58.88ms
step:1451/2330 train_time:85438ms step_avg:58.88ms
step:1452/2330 train_time:85500ms step_avg:58.88ms
step:1453/2330 train_time:85557ms step_avg:58.88ms
step:1454/2330 train_time:85620ms step_avg:58.89ms
step:1455/2330 train_time:85676ms step_avg:58.88ms
step:1456/2330 train_time:85738ms step_avg:58.89ms
step:1457/2330 train_time:85795ms step_avg:58.88ms
step:1458/2330 train_time:85858ms step_avg:58.89ms
step:1459/2330 train_time:85914ms step_avg:58.89ms
step:1460/2330 train_time:85978ms step_avg:58.89ms
step:1461/2330 train_time:86034ms step_avg:58.89ms
step:1462/2330 train_time:86098ms step_avg:58.89ms
step:1463/2330 train_time:86155ms step_avg:58.89ms
step:1464/2330 train_time:86217ms step_avg:58.89ms
step:1465/2330 train_time:86274ms step_avg:58.89ms
step:1466/2330 train_time:86337ms step_avg:58.89ms
step:1467/2330 train_time:86394ms step_avg:58.89ms
step:1468/2330 train_time:86456ms step_avg:58.89ms
step:1469/2330 train_time:86513ms step_avg:58.89ms
step:1470/2330 train_time:86575ms step_avg:58.89ms
step:1471/2330 train_time:86632ms step_avg:58.89ms
step:1472/2330 train_time:86694ms step_avg:58.90ms
step:1473/2330 train_time:86751ms step_avg:58.89ms
step:1474/2330 train_time:86813ms step_avg:58.90ms
step:1475/2330 train_time:86871ms step_avg:58.90ms
step:1476/2330 train_time:86932ms step_avg:58.90ms
step:1477/2330 train_time:86989ms step_avg:58.90ms
step:1478/2330 train_time:87050ms step_avg:58.90ms
step:1479/2330 train_time:87108ms step_avg:58.90ms
step:1480/2330 train_time:87169ms step_avg:58.90ms
step:1481/2330 train_time:87226ms step_avg:58.90ms
step:1482/2330 train_time:87288ms step_avg:58.90ms
step:1483/2330 train_time:87345ms step_avg:58.90ms
step:1484/2330 train_time:87406ms step_avg:58.90ms
step:1485/2330 train_time:87463ms step_avg:58.90ms
step:1486/2330 train_time:87525ms step_avg:58.90ms
step:1487/2330 train_time:87582ms step_avg:58.90ms
step:1488/2330 train_time:87644ms step_avg:58.90ms
step:1489/2330 train_time:87700ms step_avg:58.90ms
step:1490/2330 train_time:87763ms step_avg:58.90ms
step:1491/2330 train_time:87819ms step_avg:58.90ms
step:1492/2330 train_time:87883ms step_avg:58.90ms
step:1493/2330 train_time:87939ms step_avg:58.90ms
step:1494/2330 train_time:88002ms step_avg:58.90ms
step:1495/2330 train_time:88058ms step_avg:58.90ms
step:1496/2330 train_time:88121ms step_avg:58.90ms
step:1497/2330 train_time:88177ms step_avg:58.90ms
step:1498/2330 train_time:88240ms step_avg:58.91ms
step:1499/2330 train_time:88297ms step_avg:58.90ms
step:1500/2330 train_time:88360ms step_avg:58.91ms
step:1500/2330 val_loss:4.5316 train_time:88440ms step_avg:58.96ms
step:1501/2330 train_time:88458ms step_avg:58.93ms
step:1502/2330 train_time:88483ms step_avg:58.91ms
step:1503/2330 train_time:88543ms step_avg:58.91ms
step:1504/2330 train_time:88607ms step_avg:58.91ms
step:1505/2330 train_time:88664ms step_avg:58.91ms
step:1506/2330 train_time:88727ms step_avg:58.92ms
step:1507/2330 train_time:88784ms step_avg:58.91ms
step:1508/2330 train_time:88845ms step_avg:58.92ms
step:1509/2330 train_time:88901ms step_avg:58.91ms
step:1510/2330 train_time:88962ms step_avg:58.92ms
step:1511/2330 train_time:89018ms step_avg:58.91ms
step:1512/2330 train_time:89080ms step_avg:58.92ms
step:1513/2330 train_time:89136ms step_avg:58.91ms
step:1514/2330 train_time:89198ms step_avg:58.92ms
step:1515/2330 train_time:89254ms step_avg:58.91ms
step:1516/2330 train_time:89315ms step_avg:58.91ms
step:1517/2330 train_time:89372ms step_avg:58.91ms
step:1518/2330 train_time:89433ms step_avg:58.91ms
step:1519/2330 train_time:89490ms step_avg:58.91ms
step:1520/2330 train_time:89554ms step_avg:58.92ms
step:1521/2330 train_time:89611ms step_avg:58.92ms
step:1522/2330 train_time:89675ms step_avg:58.92ms
step:1523/2330 train_time:89732ms step_avg:58.92ms
step:1524/2330 train_time:89797ms step_avg:58.92ms
step:1525/2330 train_time:89853ms step_avg:58.92ms
step:1526/2330 train_time:89916ms step_avg:58.92ms
step:1527/2330 train_time:89973ms step_avg:58.92ms
step:1528/2330 train_time:90034ms step_avg:58.92ms
step:1529/2330 train_time:90092ms step_avg:58.92ms
step:1530/2330 train_time:90153ms step_avg:58.92ms
step:1531/2330 train_time:90210ms step_avg:58.92ms
step:1532/2330 train_time:90272ms step_avg:58.92ms
step:1533/2330 train_time:90329ms step_avg:58.92ms
step:1534/2330 train_time:90392ms step_avg:58.93ms
step:1535/2330 train_time:90450ms step_avg:58.92ms
step:1536/2330 train_time:90513ms step_avg:58.93ms
step:1537/2330 train_time:90570ms step_avg:58.93ms
step:1538/2330 train_time:90636ms step_avg:58.93ms
step:1539/2330 train_time:90693ms step_avg:58.93ms
step:1540/2330 train_time:90757ms step_avg:58.93ms
step:1541/2330 train_time:90814ms step_avg:58.93ms
step:1542/2330 train_time:90878ms step_avg:58.94ms
step:1543/2330 train_time:90935ms step_avg:58.93ms
step:1544/2330 train_time:90997ms step_avg:58.94ms
step:1545/2330 train_time:91054ms step_avg:58.93ms
step:1546/2330 train_time:91117ms step_avg:58.94ms
step:1547/2330 train_time:91174ms step_avg:58.94ms
step:1548/2330 train_time:91236ms step_avg:58.94ms
step:1549/2330 train_time:91293ms step_avg:58.94ms
step:1550/2330 train_time:91355ms step_avg:58.94ms
step:1551/2330 train_time:91412ms step_avg:58.94ms
step:1552/2330 train_time:91475ms step_avg:58.94ms
step:1553/2330 train_time:91532ms step_avg:58.94ms
step:1554/2330 train_time:91596ms step_avg:58.94ms
step:1555/2330 train_time:91653ms step_avg:58.94ms
step:1556/2330 train_time:91718ms step_avg:58.94ms
step:1557/2330 train_time:91774ms step_avg:58.94ms
step:1558/2330 train_time:91839ms step_avg:58.95ms
step:1559/2330 train_time:91896ms step_avg:58.95ms
step:1560/2330 train_time:91959ms step_avg:58.95ms
step:1561/2330 train_time:92017ms step_avg:58.95ms
step:1562/2330 train_time:92080ms step_avg:58.95ms
step:1563/2330 train_time:92136ms step_avg:58.95ms
step:1564/2330 train_time:92198ms step_avg:58.95ms
step:1565/2330 train_time:92255ms step_avg:58.95ms
step:1566/2330 train_time:92318ms step_avg:58.95ms
step:1567/2330 train_time:92375ms step_avg:58.95ms
step:1568/2330 train_time:92437ms step_avg:58.95ms
step:1569/2330 train_time:92493ms step_avg:58.95ms
step:1570/2330 train_time:92557ms step_avg:58.95ms
step:1571/2330 train_time:92614ms step_avg:58.95ms
step:1572/2330 train_time:92679ms step_avg:58.96ms
step:1573/2330 train_time:92736ms step_avg:58.95ms
step:1574/2330 train_time:92799ms step_avg:58.96ms
step:1575/2330 train_time:92856ms step_avg:58.96ms
step:1576/2330 train_time:92919ms step_avg:58.96ms
step:1577/2330 train_time:92976ms step_avg:58.96ms
step:1578/2330 train_time:93041ms step_avg:58.96ms
step:1579/2330 train_time:93098ms step_avg:58.96ms
step:1580/2330 train_time:93160ms step_avg:58.96ms
step:1581/2330 train_time:93217ms step_avg:58.96ms
step:1582/2330 train_time:93279ms step_avg:58.96ms
step:1583/2330 train_time:93336ms step_avg:58.96ms
step:1584/2330 train_time:93399ms step_avg:58.96ms
step:1585/2330 train_time:93456ms step_avg:58.96ms
step:1586/2330 train_time:93518ms step_avg:58.96ms
step:1587/2330 train_time:93574ms step_avg:58.96ms
step:1588/2330 train_time:93640ms step_avg:58.97ms
step:1589/2330 train_time:93697ms step_avg:58.97ms
step:1590/2330 train_time:93761ms step_avg:58.97ms
step:1591/2330 train_time:93818ms step_avg:58.97ms
step:1592/2330 train_time:93881ms step_avg:58.97ms
step:1593/2330 train_time:93938ms step_avg:58.97ms
step:1594/2330 train_time:94002ms step_avg:58.97ms
step:1595/2330 train_time:94059ms step_avg:58.97ms
step:1596/2330 train_time:94121ms step_avg:58.97ms
step:1597/2330 train_time:94178ms step_avg:58.97ms
step:1598/2330 train_time:94240ms step_avg:58.97ms
step:1599/2330 train_time:94297ms step_avg:58.97ms
step:1600/2330 train_time:94361ms step_avg:58.98ms
step:1601/2330 train_time:94418ms step_avg:58.97ms
step:1602/2330 train_time:94481ms step_avg:58.98ms
step:1603/2330 train_time:94538ms step_avg:58.98ms
step:1604/2330 train_time:94601ms step_avg:58.98ms
step:1605/2330 train_time:94658ms step_avg:58.98ms
step:1606/2330 train_time:94721ms step_avg:58.98ms
step:1607/2330 train_time:94778ms step_avg:58.98ms
step:1608/2330 train_time:94841ms step_avg:58.98ms
step:1609/2330 train_time:94897ms step_avg:58.98ms
step:1610/2330 train_time:94960ms step_avg:58.98ms
step:1611/2330 train_time:95017ms step_avg:58.98ms
step:1612/2330 train_time:95080ms step_avg:58.98ms
step:1613/2330 train_time:95136ms step_avg:58.98ms
step:1614/2330 train_time:95200ms step_avg:58.98ms
step:1615/2330 train_time:95257ms step_avg:58.98ms
step:1616/2330 train_time:95319ms step_avg:58.98ms
step:1617/2330 train_time:95376ms step_avg:58.98ms
step:1618/2330 train_time:95439ms step_avg:58.99ms
step:1619/2330 train_time:95495ms step_avg:58.98ms
step:1620/2330 train_time:95558ms step_avg:58.99ms
step:1621/2330 train_time:95614ms step_avg:58.98ms
step:1622/2330 train_time:95678ms step_avg:58.99ms
step:1623/2330 train_time:95735ms step_avg:58.99ms
step:1624/2330 train_time:95799ms step_avg:58.99ms
step:1625/2330 train_time:95856ms step_avg:58.99ms
step:1626/2330 train_time:95919ms step_avg:58.99ms
step:1627/2330 train_time:95976ms step_avg:58.99ms
step:1628/2330 train_time:96039ms step_avg:58.99ms
step:1629/2330 train_time:96096ms step_avg:58.99ms
step:1630/2330 train_time:96160ms step_avg:58.99ms
step:1631/2330 train_time:96216ms step_avg:58.99ms
step:1632/2330 train_time:96280ms step_avg:58.99ms
step:1633/2330 train_time:96336ms step_avg:58.99ms
step:1634/2330 train_time:96400ms step_avg:59.00ms
step:1635/2330 train_time:96457ms step_avg:58.99ms
step:1636/2330 train_time:96520ms step_avg:59.00ms
step:1637/2330 train_time:96577ms step_avg:59.00ms
step:1638/2330 train_time:96640ms step_avg:59.00ms
step:1639/2330 train_time:96696ms step_avg:59.00ms
step:1640/2330 train_time:96761ms step_avg:59.00ms
step:1641/2330 train_time:96818ms step_avg:59.00ms
step:1642/2330 train_time:96881ms step_avg:59.00ms
step:1643/2330 train_time:96937ms step_avg:59.00ms
step:1644/2330 train_time:97001ms step_avg:59.00ms
step:1645/2330 train_time:97058ms step_avg:59.00ms
step:1646/2330 train_time:97120ms step_avg:59.00ms
step:1647/2330 train_time:97177ms step_avg:59.00ms
step:1648/2330 train_time:97241ms step_avg:59.01ms
step:1649/2330 train_time:97297ms step_avg:59.00ms
step:1650/2330 train_time:97360ms step_avg:59.01ms
step:1651/2330 train_time:97417ms step_avg:59.00ms
step:1652/2330 train_time:97480ms step_avg:59.01ms
step:1653/2330 train_time:97537ms step_avg:59.01ms
step:1654/2330 train_time:97600ms step_avg:59.01ms
step:1655/2330 train_time:97656ms step_avg:59.01ms
step:1656/2330 train_time:97720ms step_avg:59.01ms
step:1657/2330 train_time:97777ms step_avg:59.01ms
step:1658/2330 train_time:97840ms step_avg:59.01ms
step:1659/2330 train_time:97897ms step_avg:59.01ms
step:1660/2330 train_time:97961ms step_avg:59.01ms
step:1661/2330 train_time:98017ms step_avg:59.01ms
step:1662/2330 train_time:98081ms step_avg:59.01ms
step:1663/2330 train_time:98137ms step_avg:59.01ms
step:1664/2330 train_time:98200ms step_avg:59.01ms
step:1665/2330 train_time:98258ms step_avg:59.01ms
step:1666/2330 train_time:98320ms step_avg:59.02ms
step:1667/2330 train_time:98376ms step_avg:59.01ms
step:1668/2330 train_time:98440ms step_avg:59.02ms
step:1669/2330 train_time:98496ms step_avg:59.02ms
step:1670/2330 train_time:98560ms step_avg:59.02ms
step:1671/2330 train_time:98617ms step_avg:59.02ms
step:1672/2330 train_time:98682ms step_avg:59.02ms
step:1673/2330 train_time:98739ms step_avg:59.02ms
step:1674/2330 train_time:98801ms step_avg:59.02ms
step:1675/2330 train_time:98858ms step_avg:59.02ms
step:1676/2330 train_time:98921ms step_avg:59.02ms
step:1677/2330 train_time:98977ms step_avg:59.02ms
step:1678/2330 train_time:99041ms step_avg:59.02ms
step:1679/2330 train_time:99097ms step_avg:59.02ms
step:1680/2330 train_time:99160ms step_avg:59.02ms
step:1681/2330 train_time:99217ms step_avg:59.02ms
step:1682/2330 train_time:99280ms step_avg:59.03ms
step:1683/2330 train_time:99338ms step_avg:59.02ms
step:1684/2330 train_time:99400ms step_avg:59.03ms
step:1685/2330 train_time:99457ms step_avg:59.03ms
step:1686/2330 train_time:99520ms step_avg:59.03ms
step:1687/2330 train_time:99576ms step_avg:59.03ms
step:1688/2330 train_time:99640ms step_avg:59.03ms
step:1689/2330 train_time:99697ms step_avg:59.03ms
step:1690/2330 train_time:99761ms step_avg:59.03ms
step:1691/2330 train_time:99817ms step_avg:59.03ms
step:1692/2330 train_time:99881ms step_avg:59.03ms
step:1693/2330 train_time:99937ms step_avg:59.03ms
step:1694/2330 train_time:100001ms step_avg:59.03ms
step:1695/2330 train_time:100058ms step_avg:59.03ms
step:1696/2330 train_time:100120ms step_avg:59.03ms
step:1697/2330 train_time:100176ms step_avg:59.03ms
step:1698/2330 train_time:100239ms step_avg:59.03ms
step:1699/2330 train_time:100296ms step_avg:59.03ms
step:1700/2330 train_time:100358ms step_avg:59.03ms
step:1701/2330 train_time:100415ms step_avg:59.03ms
step:1702/2330 train_time:100479ms step_avg:59.04ms
step:1703/2330 train_time:100535ms step_avg:59.03ms
step:1704/2330 train_time:100598ms step_avg:59.04ms
step:1705/2330 train_time:100654ms step_avg:59.03ms
step:1706/2330 train_time:100719ms step_avg:59.04ms
step:1707/2330 train_time:100775ms step_avg:59.04ms
step:1708/2330 train_time:100839ms step_avg:59.04ms
step:1709/2330 train_time:100896ms step_avg:59.04ms
step:1710/2330 train_time:100960ms step_avg:59.04ms
step:1711/2330 train_time:101017ms step_avg:59.04ms
step:1712/2330 train_time:101080ms step_avg:59.04ms
step:1713/2330 train_time:101137ms step_avg:59.04ms
step:1714/2330 train_time:101201ms step_avg:59.04ms
step:1715/2330 train_time:101258ms step_avg:59.04ms
step:1716/2330 train_time:101321ms step_avg:59.04ms
step:1717/2330 train_time:101378ms step_avg:59.04ms
step:1718/2330 train_time:101441ms step_avg:59.05ms
step:1719/2330 train_time:101497ms step_avg:59.04ms
step:1720/2330 train_time:101560ms step_avg:59.05ms
step:1721/2330 train_time:101616ms step_avg:59.04ms
step:1722/2330 train_time:101680ms step_avg:59.05ms
step:1723/2330 train_time:101736ms step_avg:59.05ms
step:1724/2330 train_time:101800ms step_avg:59.05ms
step:1725/2330 train_time:101856ms step_avg:59.05ms
step:1726/2330 train_time:101920ms step_avg:59.05ms
step:1727/2330 train_time:101977ms step_avg:59.05ms
step:1728/2330 train_time:102041ms step_avg:59.05ms
step:1729/2330 train_time:102097ms step_avg:59.05ms
step:1730/2330 train_time:102161ms step_avg:59.05ms
step:1731/2330 train_time:102217ms step_avg:59.05ms
step:1732/2330 train_time:102281ms step_avg:59.05ms
step:1733/2330 train_time:102338ms step_avg:59.05ms
step:1734/2330 train_time:102401ms step_avg:59.06ms
step:1735/2330 train_time:102458ms step_avg:59.05ms
step:1736/2330 train_time:102521ms step_avg:59.06ms
step:1737/2330 train_time:102578ms step_avg:59.05ms
step:1738/2330 train_time:102641ms step_avg:59.06ms
step:1739/2330 train_time:102698ms step_avg:59.06ms
step:1740/2330 train_time:102761ms step_avg:59.06ms
step:1741/2330 train_time:102818ms step_avg:59.06ms
step:1742/2330 train_time:102881ms step_avg:59.06ms
step:1743/2330 train_time:102938ms step_avg:59.06ms
step:1744/2330 train_time:103001ms step_avg:59.06ms
step:1745/2330 train_time:103059ms step_avg:59.06ms
step:1746/2330 train_time:103122ms step_avg:59.06ms
step:1747/2330 train_time:103178ms step_avg:59.06ms
step:1748/2330 train_time:103242ms step_avg:59.06ms
step:1749/2330 train_time:103299ms step_avg:59.06ms
step:1750/2330 train_time:103361ms step_avg:59.06ms
step:1750/2330 val_loss:4.4010 train_time:103441ms step_avg:59.11ms
step:1751/2330 train_time:103460ms step_avg:59.09ms
step:1752/2330 train_time:103483ms step_avg:59.07ms
step:1753/2330 train_time:103540ms step_avg:59.06ms
step:1754/2330 train_time:103607ms step_avg:59.07ms
step:1755/2330 train_time:103664ms step_avg:59.07ms
step:1756/2330 train_time:103728ms step_avg:59.07ms
step:1757/2330 train_time:103785ms step_avg:59.07ms
step:1758/2330 train_time:103848ms step_avg:59.07ms
step:1759/2330 train_time:103904ms step_avg:59.07ms
step:1760/2330 train_time:103967ms step_avg:59.07ms
step:1761/2330 train_time:104024ms step_avg:59.07ms
step:1762/2330 train_time:104086ms step_avg:59.07ms
step:1763/2330 train_time:104142ms step_avg:59.07ms
step:1764/2330 train_time:104204ms step_avg:59.07ms
step:1765/2330 train_time:104261ms step_avg:59.07ms
step:1766/2330 train_time:104322ms step_avg:59.07ms
step:1767/2330 train_time:104379ms step_avg:59.07ms
step:1768/2330 train_time:104442ms step_avg:59.07ms
step:1769/2330 train_time:104500ms step_avg:59.07ms
step:1770/2330 train_time:104564ms step_avg:59.08ms
step:1771/2330 train_time:104623ms step_avg:59.08ms
step:1772/2330 train_time:104685ms step_avg:59.08ms
step:1773/2330 train_time:104742ms step_avg:59.08ms
step:1774/2330 train_time:104804ms step_avg:59.08ms
step:1775/2330 train_time:104862ms step_avg:59.08ms
step:1776/2330 train_time:104922ms step_avg:59.08ms
step:1777/2330 train_time:104980ms step_avg:59.08ms
step:1778/2330 train_time:105041ms step_avg:59.08ms
step:1779/2330 train_time:105099ms step_avg:59.08ms
step:1780/2330 train_time:105159ms step_avg:59.08ms
step:1781/2330 train_time:105217ms step_avg:59.08ms
step:1782/2330 train_time:105278ms step_avg:59.08ms
step:1783/2330 train_time:105336ms step_avg:59.08ms
step:1784/2330 train_time:105397ms step_avg:59.08ms
step:1785/2330 train_time:105455ms step_avg:59.08ms
step:1786/2330 train_time:105516ms step_avg:59.08ms
step:1787/2330 train_time:105574ms step_avg:59.08ms
step:1788/2330 train_time:105636ms step_avg:59.08ms
step:1789/2330 train_time:105694ms step_avg:59.08ms
step:1790/2330 train_time:105756ms step_avg:59.08ms
step:1791/2330 train_time:105814ms step_avg:59.08ms
step:1792/2330 train_time:105875ms step_avg:59.08ms
step:1793/2330 train_time:105933ms step_avg:59.08ms
step:1794/2330 train_time:105995ms step_avg:59.08ms
step:1795/2330 train_time:106051ms step_avg:59.08ms
step:1796/2330 train_time:106114ms step_avg:59.08ms
step:1797/2330 train_time:106170ms step_avg:59.08ms
step:1798/2330 train_time:106233ms step_avg:59.08ms
step:1799/2330 train_time:106290ms step_avg:59.08ms
step:1800/2330 train_time:106352ms step_avg:59.08ms
step:1801/2330 train_time:106409ms step_avg:59.08ms
step:1802/2330 train_time:106471ms step_avg:59.08ms
step:1803/2330 train_time:106528ms step_avg:59.08ms
step:1804/2330 train_time:106591ms step_avg:59.09ms
step:1805/2330 train_time:106648ms step_avg:59.08ms
step:1806/2330 train_time:106711ms step_avg:59.09ms
step:1807/2330 train_time:106768ms step_avg:59.09ms
step:1808/2330 train_time:106832ms step_avg:59.09ms
step:1809/2330 train_time:106889ms step_avg:59.09ms
step:1810/2330 train_time:106952ms step_avg:59.09ms
step:1811/2330 train_time:107009ms step_avg:59.09ms
step:1812/2330 train_time:107073ms step_avg:59.09ms
step:1813/2330 train_time:107129ms step_avg:59.09ms
step:1814/2330 train_time:107192ms step_avg:59.09ms
step:1815/2330 train_time:107249ms step_avg:59.09ms
step:1816/2330 train_time:107311ms step_avg:59.09ms
step:1817/2330 train_time:107368ms step_avg:59.09ms
step:1818/2330 train_time:107431ms step_avg:59.09ms
step:1819/2330 train_time:107488ms step_avg:59.09ms
step:1820/2330 train_time:107550ms step_avg:59.09ms
step:1821/2330 train_time:107607ms step_avg:59.09ms
step:1822/2330 train_time:107670ms step_avg:59.09ms
step:1823/2330 train_time:107727ms step_avg:59.09ms
step:1824/2330 train_time:107790ms step_avg:59.10ms
step:1825/2330 train_time:107847ms step_avg:59.09ms
step:1826/2330 train_time:107910ms step_avg:59.10ms
step:1827/2330 train_time:107967ms step_avg:59.10ms
step:1828/2330 train_time:108030ms step_avg:59.10ms
step:1829/2330 train_time:108087ms step_avg:59.10ms
step:1830/2330 train_time:108150ms step_avg:59.10ms
step:1831/2330 train_time:108207ms step_avg:59.10ms
step:1832/2330 train_time:108270ms step_avg:59.10ms
step:1833/2330 train_time:108327ms step_avg:59.10ms
step:1834/2330 train_time:108389ms step_avg:59.10ms
step:1835/2330 train_time:108446ms step_avg:59.10ms
step:1836/2330 train_time:108508ms step_avg:59.10ms
step:1837/2330 train_time:108565ms step_avg:59.10ms
step:1838/2330 train_time:108628ms step_avg:59.10ms
step:1839/2330 train_time:108684ms step_avg:59.10ms
step:1840/2330 train_time:108749ms step_avg:59.10ms
step:1841/2330 train_time:108806ms step_avg:59.10ms
step:1842/2330 train_time:108869ms step_avg:59.10ms
step:1843/2330 train_time:108926ms step_avg:59.10ms
step:1844/2330 train_time:108988ms step_avg:59.10ms
step:1845/2330 train_time:109046ms step_avg:59.10ms
step:1846/2330 train_time:109108ms step_avg:59.11ms
step:1847/2330 train_time:109165ms step_avg:59.10ms
step:1848/2330 train_time:109228ms step_avg:59.11ms
step:1849/2330 train_time:109285ms step_avg:59.10ms
step:1850/2330 train_time:109348ms step_avg:59.11ms
step:1851/2330 train_time:109404ms step_avg:59.11ms
step:1852/2330 train_time:109468ms step_avg:59.11ms
step:1853/2330 train_time:109524ms step_avg:59.11ms
step:1854/2330 train_time:109588ms step_avg:59.11ms
step:1855/2330 train_time:109644ms step_avg:59.11ms
step:1856/2330 train_time:109708ms step_avg:59.11ms
step:1857/2330 train_time:109764ms step_avg:59.11ms
step:1858/2330 train_time:109828ms step_avg:59.11ms
step:1859/2330 train_time:109885ms step_avg:59.11ms
step:1860/2330 train_time:109949ms step_avg:59.11ms
step:1861/2330 train_time:110005ms step_avg:59.11ms
step:1862/2330 train_time:110069ms step_avg:59.11ms
step:1863/2330 train_time:110126ms step_avg:59.11ms
step:1864/2330 train_time:110188ms step_avg:59.11ms
step:1865/2330 train_time:110245ms step_avg:59.11ms
step:1866/2330 train_time:110308ms step_avg:59.11ms
step:1867/2330 train_time:110365ms step_avg:59.11ms
step:1868/2330 train_time:110428ms step_avg:59.12ms
step:1869/2330 train_time:110485ms step_avg:59.11ms
step:1870/2330 train_time:110548ms step_avg:59.12ms
step:1871/2330 train_time:110604ms step_avg:59.12ms
step:1872/2330 train_time:110668ms step_avg:59.12ms
step:1873/2330 train_time:110724ms step_avg:59.12ms
step:1874/2330 train_time:110788ms step_avg:59.12ms
step:1875/2330 train_time:110845ms step_avg:59.12ms
step:1876/2330 train_time:110908ms step_avg:59.12ms
step:1877/2330 train_time:110965ms step_avg:59.12ms
step:1878/2330 train_time:111028ms step_avg:59.12ms
step:1879/2330 train_time:111086ms step_avg:59.12ms
step:1880/2330 train_time:111148ms step_avg:59.12ms
step:1881/2330 train_time:111205ms step_avg:59.12ms
step:1882/2330 train_time:111267ms step_avg:59.12ms
step:1883/2330 train_time:111324ms step_avg:59.12ms
step:1884/2330 train_time:111387ms step_avg:59.12ms
step:1885/2330 train_time:111445ms step_avg:59.12ms
step:1886/2330 train_time:111507ms step_avg:59.12ms
step:1887/2330 train_time:111564ms step_avg:59.12ms
step:1888/2330 train_time:111627ms step_avg:59.12ms
step:1889/2330 train_time:111684ms step_avg:59.12ms
step:1890/2330 train_time:111747ms step_avg:59.13ms
step:1891/2330 train_time:111804ms step_avg:59.12ms
step:1892/2330 train_time:111867ms step_avg:59.13ms
step:1893/2330 train_time:111924ms step_avg:59.13ms
step:1894/2330 train_time:111987ms step_avg:59.13ms
step:1895/2330 train_time:112044ms step_avg:59.13ms
step:1896/2330 train_time:112107ms step_avg:59.13ms
step:1897/2330 train_time:112164ms step_avg:59.13ms
step:1898/2330 train_time:112226ms step_avg:59.13ms
step:1899/2330 train_time:112283ms step_avg:59.13ms
step:1900/2330 train_time:112346ms step_avg:59.13ms
step:1901/2330 train_time:112403ms step_avg:59.13ms
step:1902/2330 train_time:112467ms step_avg:59.13ms
step:1903/2330 train_time:112524ms step_avg:59.13ms
step:1904/2330 train_time:112587ms step_avg:59.13ms
step:1905/2330 train_time:112643ms step_avg:59.13ms
step:1906/2330 train_time:112706ms step_avg:59.13ms
step:1907/2330 train_time:112763ms step_avg:59.13ms
step:1908/2330 train_time:112826ms step_avg:59.13ms
step:1909/2330 train_time:112883ms step_avg:59.13ms
step:1910/2330 train_time:112946ms step_avg:59.13ms
step:1911/2330 train_time:113003ms step_avg:59.13ms
step:1912/2330 train_time:113066ms step_avg:59.14ms
step:1913/2330 train_time:113123ms step_avg:59.13ms
step:1914/2330 train_time:113186ms step_avg:59.14ms
step:1915/2330 train_time:113243ms step_avg:59.13ms
step:1916/2330 train_time:113306ms step_avg:59.14ms
step:1917/2330 train_time:113364ms step_avg:59.14ms
step:1918/2330 train_time:113425ms step_avg:59.14ms
step:1919/2330 train_time:113483ms step_avg:59.14ms
step:1920/2330 train_time:113545ms step_avg:59.14ms
step:1921/2330 train_time:113603ms step_avg:59.14ms
step:1922/2330 train_time:113665ms step_avg:59.14ms
step:1923/2330 train_time:113722ms step_avg:59.14ms
step:1924/2330 train_time:113786ms step_avg:59.14ms
step:1925/2330 train_time:113843ms step_avg:59.14ms
step:1926/2330 train_time:113905ms step_avg:59.14ms
step:1927/2330 train_time:113962ms step_avg:59.14ms
step:1928/2330 train_time:114026ms step_avg:59.14ms
step:1929/2330 train_time:114083ms step_avg:59.14ms
step:1930/2330 train_time:114146ms step_avg:59.14ms
step:1931/2330 train_time:114203ms step_avg:59.14ms
step:1932/2330 train_time:114266ms step_avg:59.14ms
step:1933/2330 train_time:114323ms step_avg:59.14ms
step:1934/2330 train_time:114386ms step_avg:59.14ms
step:1935/2330 train_time:114442ms step_avg:59.14ms
step:1936/2330 train_time:114506ms step_avg:59.15ms
step:1937/2330 train_time:114563ms step_avg:59.14ms
step:1938/2330 train_time:114625ms step_avg:59.15ms
step:1939/2330 train_time:114682ms step_avg:59.14ms
step:1940/2330 train_time:114746ms step_avg:59.15ms
step:1941/2330 train_time:114803ms step_avg:59.15ms
step:1942/2330 train_time:114865ms step_avg:59.15ms
step:1943/2330 train_time:114922ms step_avg:59.15ms
step:1944/2330 train_time:114986ms step_avg:59.15ms
step:1945/2330 train_time:115043ms step_avg:59.15ms
step:1946/2330 train_time:115107ms step_avg:59.15ms
step:1947/2330 train_time:115164ms step_avg:59.15ms
step:1948/2330 train_time:115226ms step_avg:59.15ms
step:1949/2330 train_time:115284ms step_avg:59.15ms
step:1950/2330 train_time:115345ms step_avg:59.15ms
step:1951/2330 train_time:115402ms step_avg:59.15ms
step:1952/2330 train_time:115465ms step_avg:59.15ms
step:1953/2330 train_time:115522ms step_avg:59.15ms
step:1954/2330 train_time:115585ms step_avg:59.15ms
step:1955/2330 train_time:115642ms step_avg:59.15ms
step:1956/2330 train_time:115705ms step_avg:59.15ms
step:1957/2330 train_time:115762ms step_avg:59.15ms
step:1958/2330 train_time:115825ms step_avg:59.15ms
step:1959/2330 train_time:115883ms step_avg:59.15ms
step:1960/2330 train_time:115946ms step_avg:59.16ms
step:1961/2330 train_time:116003ms step_avg:59.15ms
step:1962/2330 train_time:116066ms step_avg:59.16ms
step:1963/2330 train_time:116123ms step_avg:59.16ms
step:1964/2330 train_time:116187ms step_avg:59.16ms
step:1965/2330 train_time:116243ms step_avg:59.16ms
step:1966/2330 train_time:116307ms step_avg:59.16ms
step:1967/2330 train_time:116364ms step_avg:59.16ms
step:1968/2330 train_time:116427ms step_avg:59.16ms
step:1969/2330 train_time:116483ms step_avg:59.16ms
step:1970/2330 train_time:116547ms step_avg:59.16ms
step:1971/2330 train_time:116604ms step_avg:59.16ms
step:1972/2330 train_time:116667ms step_avg:59.16ms
step:1973/2330 train_time:116724ms step_avg:59.16ms
step:1974/2330 train_time:116788ms step_avg:59.16ms
step:1975/2330 train_time:116845ms step_avg:59.16ms
step:1976/2330 train_time:116907ms step_avg:59.16ms
step:1977/2330 train_time:116964ms step_avg:59.16ms
step:1978/2330 train_time:117026ms step_avg:59.16ms
step:1979/2330 train_time:117084ms step_avg:59.16ms
step:1980/2330 train_time:117146ms step_avg:59.16ms
step:1981/2330 train_time:117203ms step_avg:59.16ms
step:1982/2330 train_time:117267ms step_avg:59.17ms
step:1983/2330 train_time:117324ms step_avg:59.16ms
step:1984/2330 train_time:117386ms step_avg:59.17ms
step:1985/2330 train_time:117442ms step_avg:59.16ms
step:1986/2330 train_time:117505ms step_avg:59.17ms
step:1987/2330 train_time:117562ms step_avg:59.17ms
step:1988/2330 train_time:117625ms step_avg:59.17ms
step:1989/2330 train_time:117682ms step_avg:59.17ms
step:1990/2330 train_time:117745ms step_avg:59.17ms
step:1991/2330 train_time:117802ms step_avg:59.17ms
step:1992/2330 train_time:117864ms step_avg:59.17ms
step:1993/2330 train_time:117922ms step_avg:59.17ms
step:1994/2330 train_time:117984ms step_avg:59.17ms
step:1995/2330 train_time:118041ms step_avg:59.17ms
step:1996/2330 train_time:118104ms step_avg:59.17ms
step:1997/2330 train_time:118162ms step_avg:59.17ms
step:1998/2330 train_time:118224ms step_avg:59.17ms
step:1999/2330 train_time:118281ms step_avg:59.17ms
step:2000/2330 train_time:118343ms step_avg:59.17ms
step:2000/2330 val_loss:4.2999 train_time:118422ms step_avg:59.21ms
step:2001/2330 train_time:118441ms step_avg:59.19ms
step:2002/2330 train_time:118465ms step_avg:59.17ms
step:2003/2330 train_time:118523ms step_avg:59.17ms
step:2004/2330 train_time:118588ms step_avg:59.18ms
step:2005/2330 train_time:118646ms step_avg:59.17ms
step:2006/2330 train_time:118710ms step_avg:59.18ms
step:2007/2330 train_time:118766ms step_avg:59.18ms
step:2008/2330 train_time:118828ms step_avg:59.18ms
step:2009/2330 train_time:118885ms step_avg:59.18ms
step:2010/2330 train_time:118948ms step_avg:59.18ms
step:2011/2330 train_time:119005ms step_avg:59.18ms
step:2012/2330 train_time:119066ms step_avg:59.18ms
step:2013/2330 train_time:119123ms step_avg:59.18ms
step:2014/2330 train_time:119185ms step_avg:59.18ms
step:2015/2330 train_time:119241ms step_avg:59.18ms
step:2016/2330 train_time:119303ms step_avg:59.18ms
step:2017/2330 train_time:119360ms step_avg:59.18ms
step:2018/2330 train_time:119423ms step_avg:59.18ms
step:2019/2330 train_time:119481ms step_avg:59.18ms
step:2020/2330 train_time:119544ms step_avg:59.18ms
step:2021/2330 train_time:119601ms step_avg:59.18ms
step:2022/2330 train_time:119666ms step_avg:59.18ms
step:2023/2330 train_time:119723ms step_avg:59.18ms
step:2024/2330 train_time:119785ms step_avg:59.18ms
step:2025/2330 train_time:119842ms step_avg:59.18ms
step:2026/2330 train_time:119905ms step_avg:59.18ms
step:2027/2330 train_time:119962ms step_avg:59.18ms
step:2028/2330 train_time:120024ms step_avg:59.18ms
step:2029/2330 train_time:120081ms step_avg:59.18ms
step:2030/2330 train_time:120141ms step_avg:59.18ms
step:2031/2330 train_time:120198ms step_avg:59.18ms
step:2032/2330 train_time:120260ms step_avg:59.18ms
step:2033/2330 train_time:120317ms step_avg:59.18ms
step:2034/2330 train_time:120379ms step_avg:59.18ms
step:2035/2330 train_time:120436ms step_avg:59.18ms
step:2036/2330 train_time:120499ms step_avg:59.18ms
step:2037/2330 train_time:120556ms step_avg:59.18ms
step:2038/2330 train_time:120620ms step_avg:59.19ms
step:2039/2330 train_time:120678ms step_avg:59.18ms
step:2040/2330 train_time:120742ms step_avg:59.19ms
step:2041/2330 train_time:120798ms step_avg:59.19ms
step:2042/2330 train_time:120862ms step_avg:59.19ms
step:2043/2330 train_time:120919ms step_avg:59.19ms
step:2044/2330 train_time:120982ms step_avg:59.19ms
step:2045/2330 train_time:121038ms step_avg:59.19ms
step:2046/2330 train_time:121101ms step_avg:59.19ms
step:2047/2330 train_time:121157ms step_avg:59.19ms
step:2048/2330 train_time:121219ms step_avg:59.19ms
step:2049/2330 train_time:121276ms step_avg:59.19ms
step:2050/2330 train_time:121338ms step_avg:59.19ms
step:2051/2330 train_time:121395ms step_avg:59.19ms
step:2052/2330 train_time:121459ms step_avg:59.19ms
step:2053/2330 train_time:121515ms step_avg:59.19ms
step:2054/2330 train_time:121580ms step_avg:59.19ms
step:2055/2330 train_time:121637ms step_avg:59.19ms
step:2056/2330 train_time:121701ms step_avg:59.19ms
step:2057/2330 train_time:121757ms step_avg:59.19ms
step:2058/2330 train_time:121823ms step_avg:59.19ms
step:2059/2330 train_time:121879ms step_avg:59.19ms
step:2060/2330 train_time:121943ms step_avg:59.20ms
step:2061/2330 train_time:122000ms step_avg:59.19ms
step:2062/2330 train_time:122063ms step_avg:59.20ms
step:2063/2330 train_time:122120ms step_avg:59.20ms
step:2064/2330 train_time:122182ms step_avg:59.20ms
step:2065/2330 train_time:122239ms step_avg:59.20ms
step:2066/2330 train_time:122301ms step_avg:59.20ms
step:2067/2330 train_time:122357ms step_avg:59.20ms
step:2068/2330 train_time:122421ms step_avg:59.20ms
step:2069/2330 train_time:122478ms step_avg:59.20ms
step:2070/2330 train_time:122542ms step_avg:59.20ms
step:2071/2330 train_time:122598ms step_avg:59.20ms
step:2072/2330 train_time:122663ms step_avg:59.20ms
step:2073/2330 train_time:122721ms step_avg:59.20ms
step:2074/2330 train_time:122784ms step_avg:59.20ms
step:2075/2330 train_time:122841ms step_avg:59.20ms
step:2076/2330 train_time:122904ms step_avg:59.20ms
step:2077/2330 train_time:122961ms step_avg:59.20ms
step:2078/2330 train_time:123024ms step_avg:59.20ms
step:2079/2330 train_time:123080ms step_avg:59.20ms
step:2080/2330 train_time:123143ms step_avg:59.20ms
step:2081/2330 train_time:123200ms step_avg:59.20ms
step:2082/2330 train_time:123263ms step_avg:59.20ms
step:2083/2330 train_time:123319ms step_avg:59.20ms
step:2084/2330 train_time:123382ms step_avg:59.20ms
step:2085/2330 train_time:123439ms step_avg:59.20ms
step:2086/2330 train_time:123502ms step_avg:59.21ms
step:2087/2330 train_time:123559ms step_avg:59.20ms
step:2088/2330 train_time:123622ms step_avg:59.21ms
step:2089/2330 train_time:123679ms step_avg:59.20ms
step:2090/2330 train_time:123744ms step_avg:59.21ms
step:2091/2330 train_time:123800ms step_avg:59.21ms
step:2092/2330 train_time:123864ms step_avg:59.21ms
step:2093/2330 train_time:123921ms step_avg:59.21ms
step:2094/2330 train_time:123985ms step_avg:59.21ms
step:2095/2330 train_time:124041ms step_avg:59.21ms
step:2096/2330 train_time:124104ms step_avg:59.21ms
step:2097/2330 train_time:124160ms step_avg:59.21ms
step:2098/2330 train_time:124223ms step_avg:59.21ms
step:2099/2330 train_time:124279ms step_avg:59.21ms
step:2100/2330 train_time:124342ms step_avg:59.21ms
step:2101/2330 train_time:124399ms step_avg:59.21ms
step:2102/2330 train_time:124462ms step_avg:59.21ms
step:2103/2330 train_time:124519ms step_avg:59.21ms
step:2104/2330 train_time:124582ms step_avg:59.21ms
step:2105/2330 train_time:124639ms step_avg:59.21ms
step:2106/2330 train_time:124702ms step_avg:59.21ms
step:2107/2330 train_time:124758ms step_avg:59.21ms
step:2108/2330 train_time:124822ms step_avg:59.21ms
step:2109/2330 train_time:124880ms step_avg:59.21ms
step:2110/2330 train_time:124943ms step_avg:59.21ms
step:2111/2330 train_time:125000ms step_avg:59.21ms
step:2112/2330 train_time:125062ms step_avg:59.22ms
step:2113/2330 train_time:125119ms step_avg:59.21ms
step:2114/2330 train_time:125182ms step_avg:59.22ms
step:2115/2330 train_time:125238ms step_avg:59.21ms
step:2116/2330 train_time:125301ms step_avg:59.22ms
step:2117/2330 train_time:125358ms step_avg:59.21ms
step:2118/2330 train_time:125420ms step_avg:59.22ms
step:2119/2330 train_time:125477ms step_avg:59.22ms
step:2120/2330 train_time:125540ms step_avg:59.22ms
step:2121/2330 train_time:125596ms step_avg:59.22ms
step:2122/2330 train_time:125659ms step_avg:59.22ms
step:2123/2330 train_time:125717ms step_avg:59.22ms
step:2124/2330 train_time:125779ms step_avg:59.22ms
step:2125/2330 train_time:125836ms step_avg:59.22ms
step:2126/2330 train_time:125900ms step_avg:59.22ms
step:2127/2330 train_time:125957ms step_avg:59.22ms
step:2128/2330 train_time:126021ms step_avg:59.22ms
step:2129/2330 train_time:126078ms step_avg:59.22ms
step:2130/2330 train_time:126141ms step_avg:59.22ms
step:2131/2330 train_time:126198ms step_avg:59.22ms
step:2132/2330 train_time:126261ms step_avg:59.22ms
step:2133/2330 train_time:126318ms step_avg:59.22ms
step:2134/2330 train_time:126381ms step_avg:59.22ms
step:2135/2330 train_time:126437ms step_avg:59.22ms
step:2136/2330 train_time:126500ms step_avg:59.22ms
step:2137/2330 train_time:126556ms step_avg:59.22ms
step:2138/2330 train_time:126619ms step_avg:59.22ms
step:2139/2330 train_time:126676ms step_avg:59.22ms
step:2140/2330 train_time:126739ms step_avg:59.22ms
step:2141/2330 train_time:126796ms step_avg:59.22ms
step:2142/2330 train_time:126859ms step_avg:59.22ms
step:2143/2330 train_time:126917ms step_avg:59.22ms
step:2144/2330 train_time:126979ms step_avg:59.23ms
step:2145/2330 train_time:127036ms step_avg:59.22ms
step:2146/2330 train_time:127099ms step_avg:59.23ms
step:2147/2330 train_time:127156ms step_avg:59.22ms
step:2148/2330 train_time:127219ms step_avg:59.23ms
step:2149/2330 train_time:127276ms step_avg:59.23ms
step:2150/2330 train_time:127339ms step_avg:59.23ms
step:2151/2330 train_time:127396ms step_avg:59.23ms
step:2152/2330 train_time:127458ms step_avg:59.23ms
step:2153/2330 train_time:127515ms step_avg:59.23ms
step:2154/2330 train_time:127578ms step_avg:59.23ms
step:2155/2330 train_time:127634ms step_avg:59.23ms
step:2156/2330 train_time:127698ms step_avg:59.23ms
step:2157/2330 train_time:127754ms step_avg:59.23ms
step:2158/2330 train_time:127818ms step_avg:59.23ms
step:2159/2330 train_time:127875ms step_avg:59.23ms
step:2160/2330 train_time:127939ms step_avg:59.23ms
step:2161/2330 train_time:127995ms step_avg:59.23ms
step:2162/2330 train_time:128059ms step_avg:59.23ms
step:2163/2330 train_time:128116ms step_avg:59.23ms
step:2164/2330 train_time:128179ms step_avg:59.23ms
step:2165/2330 train_time:128235ms step_avg:59.23ms
step:2166/2330 train_time:128299ms step_avg:59.23ms
step:2167/2330 train_time:128356ms step_avg:59.23ms
step:2168/2330 train_time:128419ms step_avg:59.23ms
step:2169/2330 train_time:128475ms step_avg:59.23ms
step:2170/2330 train_time:128538ms step_avg:59.23ms
step:2171/2330 train_time:128595ms step_avg:59.23ms
step:2172/2330 train_time:128658ms step_avg:59.23ms
step:2173/2330 train_time:128715ms step_avg:59.23ms
step:2174/2330 train_time:128778ms step_avg:59.24ms
step:2175/2330 train_time:128835ms step_avg:59.23ms
step:2176/2330 train_time:128898ms step_avg:59.24ms
step:2177/2330 train_time:128955ms step_avg:59.24ms
step:2178/2330 train_time:129018ms step_avg:59.24ms
step:2179/2330 train_time:129075ms step_avg:59.24ms
step:2180/2330 train_time:129138ms step_avg:59.24ms
step:2181/2330 train_time:129195ms step_avg:59.24ms
step:2182/2330 train_time:129260ms step_avg:59.24ms
step:2183/2330 train_time:129317ms step_avg:59.24ms
step:2184/2330 train_time:129379ms step_avg:59.24ms
step:2185/2330 train_time:129436ms step_avg:59.24ms
step:2186/2330 train_time:129498ms step_avg:59.24ms
step:2187/2330 train_time:129555ms step_avg:59.24ms
step:2188/2330 train_time:129618ms step_avg:59.24ms
step:2189/2330 train_time:129675ms step_avg:59.24ms
step:2190/2330 train_time:129738ms step_avg:59.24ms
step:2191/2330 train_time:129794ms step_avg:59.24ms
step:2192/2330 train_time:129858ms step_avg:59.24ms
step:2193/2330 train_time:129914ms step_avg:59.24ms
step:2194/2330 train_time:129978ms step_avg:59.24ms
step:2195/2330 train_time:130034ms step_avg:59.24ms
step:2196/2330 train_time:130098ms step_avg:59.24ms
step:2197/2330 train_time:130155ms step_avg:59.24ms
step:2198/2330 train_time:130219ms step_avg:59.24ms
step:2199/2330 train_time:130276ms step_avg:59.24ms
step:2200/2330 train_time:130339ms step_avg:59.25ms
step:2201/2330 train_time:130397ms step_avg:59.24ms
step:2202/2330 train_time:130460ms step_avg:59.25ms
step:2203/2330 train_time:130517ms step_avg:59.25ms
step:2204/2330 train_time:130580ms step_avg:59.25ms
step:2205/2330 train_time:130637ms step_avg:59.25ms
step:2206/2330 train_time:130700ms step_avg:59.25ms
step:2207/2330 train_time:130757ms step_avg:59.25ms
step:2208/2330 train_time:130820ms step_avg:59.25ms
step:2209/2330 train_time:130877ms step_avg:59.25ms
step:2210/2330 train_time:130940ms step_avg:59.25ms
step:2211/2330 train_time:130997ms step_avg:59.25ms
step:2212/2330 train_time:131059ms step_avg:59.25ms
step:2213/2330 train_time:131116ms step_avg:59.25ms
step:2214/2330 train_time:131180ms step_avg:59.25ms
step:2215/2330 train_time:131237ms step_avg:59.25ms
step:2216/2330 train_time:131301ms step_avg:59.25ms
step:2217/2330 train_time:131358ms step_avg:59.25ms
step:2218/2330 train_time:131421ms step_avg:59.25ms
step:2219/2330 train_time:131478ms step_avg:59.25ms
step:2220/2330 train_time:131541ms step_avg:59.25ms
step:2221/2330 train_time:131598ms step_avg:59.25ms
step:2222/2330 train_time:131661ms step_avg:59.25ms
step:2223/2330 train_time:131718ms step_avg:59.25ms
step:2224/2330 train_time:131782ms step_avg:59.25ms
step:2225/2330 train_time:131838ms step_avg:59.25ms
step:2226/2330 train_time:131901ms step_avg:59.25ms
step:2227/2330 train_time:131958ms step_avg:59.25ms
step:2228/2330 train_time:132021ms step_avg:59.26ms
step:2229/2330 train_time:132078ms step_avg:59.25ms
step:2230/2330 train_time:132140ms step_avg:59.26ms
step:2231/2330 train_time:132197ms step_avg:59.25ms
step:2232/2330 train_time:132260ms step_avg:59.26ms
step:2233/2330 train_time:132317ms step_avg:59.26ms
step:2234/2330 train_time:132381ms step_avg:59.26ms
step:2235/2330 train_time:132437ms step_avg:59.26ms
step:2236/2330 train_time:132501ms step_avg:59.26ms
step:2237/2330 train_time:132557ms step_avg:59.26ms
step:2238/2330 train_time:132621ms step_avg:59.26ms
step:2239/2330 train_time:132678ms step_avg:59.26ms
step:2240/2330 train_time:132742ms step_avg:59.26ms
step:2241/2330 train_time:132799ms step_avg:59.26ms
step:2242/2330 train_time:132861ms step_avg:59.26ms
step:2243/2330 train_time:132918ms step_avg:59.26ms
step:2244/2330 train_time:132980ms step_avg:59.26ms
step:2245/2330 train_time:133037ms step_avg:59.26ms
step:2246/2330 train_time:133100ms step_avg:59.26ms
step:2247/2330 train_time:133157ms step_avg:59.26ms
step:2248/2330 train_time:133220ms step_avg:59.26ms
step:2249/2330 train_time:133277ms step_avg:59.26ms
step:2250/2330 train_time:133339ms step_avg:59.26ms
step:2250/2330 val_loss:4.2287 train_time:133420ms step_avg:59.30ms
step:2251/2330 train_time:133438ms step_avg:59.28ms
step:2252/2330 train_time:133464ms step_avg:59.26ms
step:2253/2330 train_time:133523ms step_avg:59.26ms
step:2254/2330 train_time:133589ms step_avg:59.27ms
step:2255/2330 train_time:133647ms step_avg:59.27ms
step:2256/2330 train_time:133709ms step_avg:59.27ms
step:2257/2330 train_time:133766ms step_avg:59.27ms
step:2258/2330 train_time:133828ms step_avg:59.27ms
step:2259/2330 train_time:133885ms step_avg:59.27ms
step:2260/2330 train_time:133947ms step_avg:59.27ms
step:2261/2330 train_time:134005ms step_avg:59.27ms
step:2262/2330 train_time:134065ms step_avg:59.27ms
step:2263/2330 train_time:134122ms step_avg:59.27ms
step:2264/2330 train_time:134183ms step_avg:59.27ms
step:2265/2330 train_time:134240ms step_avg:59.27ms
step:2266/2330 train_time:134302ms step_avg:59.27ms
step:2267/2330 train_time:134358ms step_avg:59.27ms
step:2268/2330 train_time:134424ms step_avg:59.27ms
step:2269/2330 train_time:134481ms step_avg:59.27ms
step:2270/2330 train_time:134547ms step_avg:59.27ms
step:2271/2330 train_time:134603ms step_avg:59.27ms
step:2272/2330 train_time:134669ms step_avg:59.27ms
step:2273/2330 train_time:134726ms step_avg:59.27ms
step:2274/2330 train_time:134789ms step_avg:59.27ms
step:2275/2330 train_time:134846ms step_avg:59.27ms
step:2276/2330 train_time:134908ms step_avg:59.27ms
step:2277/2330 train_time:134964ms step_avg:59.27ms
step:2278/2330 train_time:135026ms step_avg:59.27ms
step:2279/2330 train_time:135082ms step_avg:59.27ms
step:2280/2330 train_time:135144ms step_avg:59.27ms
step:2281/2330 train_time:135201ms step_avg:59.27ms
step:2282/2330 train_time:135263ms step_avg:59.27ms
step:2283/2330 train_time:135319ms step_avg:59.27ms
step:2284/2330 train_time:135383ms step_avg:59.27ms
step:2285/2330 train_time:135440ms step_avg:59.27ms
step:2286/2330 train_time:135503ms step_avg:59.28ms
step:2287/2330 train_time:135561ms step_avg:59.27ms
step:2288/2330 train_time:135627ms step_avg:59.28ms
step:2289/2330 train_time:135684ms step_avg:59.28ms
step:2290/2330 train_time:135747ms step_avg:59.28ms
step:2291/2330 train_time:135804ms step_avg:59.28ms
step:2292/2330 train_time:135867ms step_avg:59.28ms
step:2293/2330 train_time:135924ms step_avg:59.28ms
step:2294/2330 train_time:135987ms step_avg:59.28ms
step:2295/2330 train_time:136044ms step_avg:59.28ms
step:2296/2330 train_time:136106ms step_avg:59.28ms
step:2297/2330 train_time:136162ms step_avg:59.28ms
step:2298/2330 train_time:136224ms step_avg:59.28ms
step:2299/2330 train_time:136281ms step_avg:59.28ms
step:2300/2330 train_time:136343ms step_avg:59.28ms
step:2301/2330 train_time:136400ms step_avg:59.28ms
step:2302/2330 train_time:136463ms step_avg:59.28ms
step:2303/2330 train_time:136521ms step_avg:59.28ms
step:2304/2330 train_time:136585ms step_avg:59.28ms
step:2305/2330 train_time:136642ms step_avg:59.28ms
step:2306/2330 train_time:136706ms step_avg:59.28ms
step:2307/2330 train_time:136762ms step_avg:59.28ms
step:2308/2330 train_time:136826ms step_avg:59.28ms
step:2309/2330 train_time:136884ms step_avg:59.28ms
step:2310/2330 train_time:136947ms step_avg:59.28ms
step:2311/2330 train_time:137004ms step_avg:59.28ms
step:2312/2330 train_time:137066ms step_avg:59.28ms
step:2313/2330 train_time:137122ms step_avg:59.28ms
step:2314/2330 train_time:137185ms step_avg:59.28ms
step:2315/2330 train_time:137241ms step_avg:59.28ms
step:2316/2330 train_time:137303ms step_avg:59.28ms
step:2317/2330 train_time:137360ms step_avg:59.28ms
step:2318/2330 train_time:137423ms step_avg:59.29ms
step:2319/2330 train_time:137480ms step_avg:59.28ms
step:2320/2330 train_time:137544ms step_avg:59.29ms
step:2321/2330 train_time:137602ms step_avg:59.29ms
step:2322/2330 train_time:137666ms step_avg:59.29ms
step:2323/2330 train_time:137723ms step_avg:59.29ms
step:2324/2330 train_time:137787ms step_avg:59.29ms
step:2325/2330 train_time:137844ms step_avg:59.29ms
step:2326/2330 train_time:137907ms step_avg:59.29ms
step:2327/2330 train_time:137964ms step_avg:59.29ms
step:2328/2330 train_time:138026ms step_avg:59.29ms
step:2329/2330 train_time:138083ms step_avg:59.29ms
step:2330/2330 train_time:138146ms step_avg:59.29ms
step:2330/2330 val_loss:4.2111 train_time:138225ms step_avg:59.32ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
