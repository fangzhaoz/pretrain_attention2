import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = divide_by_norm(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = divide_by_norm(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_gd_lr5e-3"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=5e-3, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 05:18:27 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   33C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   32C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.05ms
step:1/2330 train_time:76ms step_avg:76.15ms
step:2/2330 train_time:152ms step_avg:75.80ms
step:3/2330 train_time:165ms step_avg:54.89ms
step:4/2330 train_time:178ms step_avg:44.39ms
step:5/2330 train_time:190ms step_avg:37.90ms
step:6/2330 train_time:212ms step_avg:35.28ms
step:7/2330 train_time:233ms step_avg:33.27ms
step:8/2330 train_time:288ms step_avg:35.96ms
step:9/2330 train_time:309ms step_avg:34.32ms
step:10/2330 train_time:364ms step_avg:36.40ms
step:11/2330 train_time:386ms step_avg:35.10ms
step:12/2330 train_time:441ms step_avg:36.78ms
step:13/2330 train_time:464ms step_avg:35.67ms
step:14/2330 train_time:519ms step_avg:37.10ms
step:15/2330 train_time:542ms step_avg:36.13ms
step:16/2330 train_time:598ms step_avg:37.36ms
step:17/2330 train_time:620ms step_avg:36.48ms
step:18/2330 train_time:676ms step_avg:37.56ms
step:19/2330 train_time:698ms step_avg:36.75ms
step:20/2330 train_time:754ms step_avg:37.69ms
step:21/2330 train_time:776ms step_avg:36.95ms
step:22/2330 train_time:832ms step_avg:37.84ms
step:23/2330 train_time:854ms step_avg:37.14ms
step:24/2330 train_time:911ms step_avg:37.94ms
step:25/2330 train_time:932ms step_avg:37.29ms
step:26/2330 train_time:991ms step_avg:38.10ms
step:27/2330 train_time:1015ms step_avg:37.61ms
step:28/2330 train_time:1079ms step_avg:38.53ms
step:29/2330 train_time:1103ms step_avg:38.03ms
step:30/2330 train_time:1162ms step_avg:38.73ms
step:31/2330 train_time:1186ms step_avg:38.24ms
step:32/2330 train_time:1242ms step_avg:38.83ms
step:33/2330 train_time:1265ms step_avg:38.35ms
step:34/2330 train_time:1322ms step_avg:38.87ms
step:35/2330 train_time:1344ms step_avg:38.41ms
step:36/2330 train_time:1400ms step_avg:38.90ms
step:37/2330 train_time:1423ms step_avg:38.46ms
step:38/2330 train_time:1479ms step_avg:38.93ms
step:39/2330 train_time:1502ms step_avg:38.51ms
step:40/2330 train_time:1558ms step_avg:38.95ms
step:41/2330 train_time:1581ms step_avg:38.56ms
step:42/2330 train_time:1638ms step_avg:38.99ms
step:43/2330 train_time:1660ms step_avg:38.61ms
step:44/2330 train_time:1716ms step_avg:39.01ms
step:45/2330 train_time:1739ms step_avg:38.64ms
step:46/2330 train_time:1795ms step_avg:39.02ms
step:47/2330 train_time:1818ms step_avg:38.68ms
step:48/2330 train_time:1875ms step_avg:39.07ms
step:49/2330 train_time:1899ms step_avg:38.75ms
step:50/2330 train_time:1957ms step_avg:39.15ms
step:51/2330 train_time:1982ms step_avg:38.86ms
step:52/2330 train_time:2040ms step_avg:39.24ms
step:53/2330 train_time:2065ms step_avg:38.96ms
step:54/2330 train_time:2123ms step_avg:39.31ms
step:55/2330 train_time:2147ms step_avg:39.04ms
step:56/2330 train_time:2204ms step_avg:39.37ms
step:57/2330 train_time:2228ms step_avg:39.08ms
step:58/2330 train_time:2284ms step_avg:39.38ms
step:59/2330 train_time:2307ms step_avg:39.11ms
step:60/2330 train_time:2363ms step_avg:39.39ms
step:61/2330 train_time:2386ms step_avg:39.12ms
step:62/2330 train_time:2443ms step_avg:39.40ms
step:63/2330 train_time:2466ms step_avg:39.14ms
step:64/2330 train_time:2522ms step_avg:39.40ms
step:65/2330 train_time:2545ms step_avg:39.15ms
step:66/2330 train_time:2601ms step_avg:39.41ms
step:67/2330 train_time:2624ms step_avg:39.16ms
step:68/2330 train_time:2680ms step_avg:39.42ms
step:69/2330 train_time:2703ms step_avg:39.18ms
step:70/2330 train_time:2759ms step_avg:39.42ms
step:71/2330 train_time:2783ms step_avg:39.20ms
step:72/2330 train_time:2840ms step_avg:39.44ms
step:73/2330 train_time:2863ms step_avg:39.22ms
step:74/2330 train_time:2920ms step_avg:39.46ms
step:75/2330 train_time:2944ms step_avg:39.25ms
step:76/2330 train_time:3002ms step_avg:39.50ms
step:77/2330 train_time:3026ms step_avg:39.30ms
step:78/2330 train_time:3084ms step_avg:39.54ms
step:79/2330 train_time:3108ms step_avg:39.35ms
step:80/2330 train_time:3166ms step_avg:39.57ms
step:81/2330 train_time:3189ms step_avg:39.37ms
step:82/2330 train_time:3246ms step_avg:39.58ms
step:83/2330 train_time:3269ms step_avg:39.38ms
step:84/2330 train_time:3325ms step_avg:39.59ms
step:85/2330 train_time:3348ms step_avg:39.39ms
step:86/2330 train_time:3404ms step_avg:39.58ms
step:87/2330 train_time:3427ms step_avg:39.39ms
step:88/2330 train_time:3483ms step_avg:39.58ms
step:89/2330 train_time:3506ms step_avg:39.39ms
step:90/2330 train_time:3562ms step_avg:39.58ms
step:91/2330 train_time:3585ms step_avg:39.40ms
step:92/2330 train_time:3641ms step_avg:39.58ms
step:93/2330 train_time:3665ms step_avg:39.40ms
step:94/2330 train_time:3721ms step_avg:39.59ms
step:95/2330 train_time:3744ms step_avg:39.41ms
step:96/2330 train_time:3801ms step_avg:39.59ms
step:97/2330 train_time:3825ms step_avg:39.43ms
step:98/2330 train_time:3883ms step_avg:39.62ms
step:99/2330 train_time:3906ms step_avg:39.46ms
step:100/2330 train_time:3963ms step_avg:39.63ms
step:101/2330 train_time:3986ms step_avg:39.47ms
step:102/2330 train_time:4044ms step_avg:39.64ms
step:103/2330 train_time:4067ms step_avg:39.49ms
step:104/2330 train_time:4125ms step_avg:39.66ms
step:105/2330 train_time:4148ms step_avg:39.51ms
step:106/2330 train_time:4205ms step_avg:39.67ms
step:107/2330 train_time:4228ms step_avg:39.52ms
step:108/2330 train_time:4285ms step_avg:39.68ms
step:109/2330 train_time:4309ms step_avg:39.53ms
step:110/2330 train_time:4365ms step_avg:39.68ms
step:111/2330 train_time:4388ms step_avg:39.53ms
step:112/2330 train_time:4445ms step_avg:39.68ms
step:113/2330 train_time:4468ms step_avg:39.54ms
step:114/2330 train_time:4523ms step_avg:39.68ms
step:115/2330 train_time:4547ms step_avg:39.54ms
step:116/2330 train_time:4604ms step_avg:39.69ms
step:117/2330 train_time:4627ms step_avg:39.55ms
step:118/2330 train_time:4685ms step_avg:39.70ms
step:119/2330 train_time:4708ms step_avg:39.56ms
step:120/2330 train_time:4764ms step_avg:39.70ms
step:121/2330 train_time:4788ms step_avg:39.57ms
step:122/2330 train_time:4844ms step_avg:39.70ms
step:123/2330 train_time:4867ms step_avg:39.57ms
step:124/2330 train_time:4923ms step_avg:39.71ms
step:125/2330 train_time:4947ms step_avg:39.58ms
step:126/2330 train_time:5004ms step_avg:39.71ms
step:127/2330 train_time:5028ms step_avg:39.59ms
step:128/2330 train_time:5084ms step_avg:39.72ms
step:129/2330 train_time:5108ms step_avg:39.59ms
step:130/2330 train_time:5164ms step_avg:39.73ms
step:131/2330 train_time:5188ms step_avg:39.60ms
step:132/2330 train_time:5244ms step_avg:39.73ms
step:133/2330 train_time:5267ms step_avg:39.60ms
step:134/2330 train_time:5324ms step_avg:39.73ms
step:135/2330 train_time:5347ms step_avg:39.61ms
step:136/2330 train_time:5404ms step_avg:39.73ms
step:137/2330 train_time:5427ms step_avg:39.61ms
step:138/2330 train_time:5484ms step_avg:39.74ms
step:139/2330 train_time:5507ms step_avg:39.62ms
step:140/2330 train_time:5563ms step_avg:39.74ms
step:141/2330 train_time:5586ms step_avg:39.62ms
step:142/2330 train_time:5642ms step_avg:39.73ms
step:143/2330 train_time:5665ms step_avg:39.62ms
step:144/2330 train_time:5722ms step_avg:39.74ms
step:145/2330 train_time:5746ms step_avg:39.62ms
step:146/2330 train_time:5802ms step_avg:39.74ms
step:147/2330 train_time:5825ms step_avg:39.63ms
step:148/2330 train_time:5882ms step_avg:39.74ms
step:149/2330 train_time:5906ms step_avg:39.64ms
step:150/2330 train_time:5963ms step_avg:39.75ms
step:151/2330 train_time:5986ms step_avg:39.64ms
step:152/2330 train_time:6043ms step_avg:39.76ms
step:153/2330 train_time:6067ms step_avg:39.65ms
step:154/2330 train_time:6123ms step_avg:39.76ms
step:155/2330 train_time:6146ms step_avg:39.65ms
step:156/2330 train_time:6203ms step_avg:39.76ms
step:157/2330 train_time:6226ms step_avg:39.66ms
step:158/2330 train_time:6284ms step_avg:39.77ms
step:159/2330 train_time:6307ms step_avg:39.67ms
step:160/2330 train_time:6364ms step_avg:39.77ms
step:161/2330 train_time:6387ms step_avg:39.67ms
step:162/2330 train_time:6443ms step_avg:39.77ms
step:163/2330 train_time:6467ms step_avg:39.67ms
step:164/2330 train_time:6523ms step_avg:39.77ms
step:165/2330 train_time:6546ms step_avg:39.67ms
step:166/2330 train_time:6603ms step_avg:39.78ms
step:167/2330 train_time:6626ms step_avg:39.68ms
step:168/2330 train_time:6683ms step_avg:39.78ms
step:169/2330 train_time:6706ms step_avg:39.68ms
step:170/2330 train_time:6763ms step_avg:39.78ms
step:171/2330 train_time:6786ms step_avg:39.68ms
step:172/2330 train_time:6843ms step_avg:39.78ms
step:173/2330 train_time:6866ms step_avg:39.69ms
step:174/2330 train_time:6923ms step_avg:39.79ms
step:175/2330 train_time:6946ms step_avg:39.69ms
step:176/2330 train_time:7003ms step_avg:39.79ms
step:177/2330 train_time:7027ms step_avg:39.70ms
step:178/2330 train_time:7084ms step_avg:39.80ms
step:179/2330 train_time:7106ms step_avg:39.70ms
step:180/2330 train_time:7163ms step_avg:39.80ms
step:181/2330 train_time:7186ms step_avg:39.70ms
step:182/2330 train_time:7244ms step_avg:39.80ms
step:183/2330 train_time:7267ms step_avg:39.71ms
step:184/2330 train_time:7323ms step_avg:39.80ms
step:185/2330 train_time:7347ms step_avg:39.71ms
step:186/2330 train_time:7403ms step_avg:39.80ms
step:187/2330 train_time:7427ms step_avg:39.71ms
step:188/2330 train_time:7483ms step_avg:39.80ms
step:189/2330 train_time:7506ms step_avg:39.71ms
step:190/2330 train_time:7563ms step_avg:39.80ms
step:191/2330 train_time:7586ms step_avg:39.72ms
step:192/2330 train_time:7643ms step_avg:39.81ms
step:193/2330 train_time:7666ms step_avg:39.72ms
step:194/2330 train_time:7722ms step_avg:39.81ms
step:195/2330 train_time:7746ms step_avg:39.72ms
step:196/2330 train_time:7802ms step_avg:39.81ms
step:197/2330 train_time:7825ms step_avg:39.72ms
step:198/2330 train_time:7883ms step_avg:39.81ms
step:199/2330 train_time:7906ms step_avg:39.73ms
step:200/2330 train_time:7963ms step_avg:39.81ms
step:201/2330 train_time:7986ms step_avg:39.73ms
step:202/2330 train_time:8043ms step_avg:39.82ms
step:203/2330 train_time:8066ms step_avg:39.73ms
step:204/2330 train_time:8123ms step_avg:39.82ms
step:205/2330 train_time:8146ms step_avg:39.73ms
step:206/2330 train_time:8202ms step_avg:39.82ms
step:207/2330 train_time:8225ms step_avg:39.74ms
step:208/2330 train_time:8283ms step_avg:39.82ms
step:209/2330 train_time:8306ms step_avg:39.74ms
step:210/2330 train_time:8363ms step_avg:39.82ms
step:211/2330 train_time:8386ms step_avg:39.74ms
step:212/2330 train_time:8442ms step_avg:39.82ms
step:213/2330 train_time:8465ms step_avg:39.74ms
step:214/2330 train_time:8522ms step_avg:39.82ms
step:215/2330 train_time:8545ms step_avg:39.74ms
step:216/2330 train_time:8602ms step_avg:39.82ms
step:217/2330 train_time:8625ms step_avg:39.75ms
step:218/2330 train_time:8682ms step_avg:39.82ms
step:219/2330 train_time:8705ms step_avg:39.75ms
step:220/2330 train_time:8762ms step_avg:39.83ms
step:221/2330 train_time:8786ms step_avg:39.75ms
step:222/2330 train_time:8842ms step_avg:39.83ms
step:223/2330 train_time:8866ms step_avg:39.76ms
step:224/2330 train_time:8923ms step_avg:39.83ms
step:225/2330 train_time:8946ms step_avg:39.76ms
step:226/2330 train_time:9002ms step_avg:39.83ms
step:227/2330 train_time:9026ms step_avg:39.76ms
step:228/2330 train_time:9083ms step_avg:39.84ms
step:229/2330 train_time:9106ms step_avg:39.76ms
step:230/2330 train_time:9163ms step_avg:39.84ms
step:231/2330 train_time:9185ms step_avg:39.76ms
step:232/2330 train_time:9243ms step_avg:39.84ms
step:233/2330 train_time:9266ms step_avg:39.77ms
step:234/2330 train_time:9323ms step_avg:39.84ms
step:235/2330 train_time:9346ms step_avg:39.77ms
step:236/2330 train_time:9403ms step_avg:39.84ms
step:237/2330 train_time:9426ms step_avg:39.77ms
step:238/2330 train_time:9482ms step_avg:39.84ms
step:239/2330 train_time:9505ms step_avg:39.77ms
step:240/2330 train_time:9562ms step_avg:39.84ms
step:241/2330 train_time:9585ms step_avg:39.77ms
step:242/2330 train_time:9642ms step_avg:39.84ms
step:243/2330 train_time:9665ms step_avg:39.77ms
step:244/2330 train_time:9721ms step_avg:39.84ms
step:245/2330 train_time:9745ms step_avg:39.78ms
step:246/2330 train_time:9802ms step_avg:39.84ms
step:247/2330 train_time:9826ms step_avg:39.78ms
step:248/2330 train_time:9883ms step_avg:39.85ms
step:249/2330 train_time:9906ms step_avg:39.78ms
step:250/2330 train_time:9963ms step_avg:39.85ms
step:250/2330 val_loss:5.6483 train_time:10060ms step_avg:40.24ms
step:251/2330 train_time:10073ms step_avg:40.13ms
step:252/2330 train_time:10085ms step_avg:40.02ms
step:253/2330 train_time:10096ms step_avg:39.90ms
step:254/2330 train_time:10123ms step_avg:39.85ms
step:255/2330 train_time:10145ms step_avg:39.78ms
step:256/2330 train_time:10201ms step_avg:39.85ms
step:257/2330 train_time:10224ms step_avg:39.78ms
step:258/2330 train_time:10280ms step_avg:39.84ms
step:259/2330 train_time:10302ms step_avg:39.78ms
step:260/2330 train_time:10359ms step_avg:39.84ms
step:261/2330 train_time:10386ms step_avg:39.79ms
step:262/2330 train_time:10449ms step_avg:39.88ms
step:263/2330 train_time:10473ms step_avg:39.82ms
step:264/2330 train_time:10531ms step_avg:39.89ms
step:265/2330 train_time:10554ms step_avg:39.83ms
step:266/2330 train_time:10610ms step_avg:39.89ms
step:267/2330 train_time:10632ms step_avg:39.82ms
step:268/2330 train_time:10689ms step_avg:39.88ms
step:269/2330 train_time:10711ms step_avg:39.82ms
step:270/2330 train_time:10767ms step_avg:39.88ms
step:271/2330 train_time:10789ms step_avg:39.81ms
step:272/2330 train_time:10845ms step_avg:39.87ms
step:273/2330 train_time:10869ms step_avg:39.81ms
step:274/2330 train_time:10925ms step_avg:39.87ms
step:275/2330 train_time:10949ms step_avg:39.81ms
step:276/2330 train_time:11006ms step_avg:39.88ms
step:277/2330 train_time:11030ms step_avg:39.82ms
step:278/2330 train_time:11086ms step_avg:39.88ms
step:279/2330 train_time:11109ms step_avg:39.82ms
step:280/2330 train_time:11166ms step_avg:39.88ms
step:281/2330 train_time:11189ms step_avg:39.82ms
step:282/2330 train_time:11245ms step_avg:39.87ms
step:283/2330 train_time:11268ms step_avg:39.82ms
step:284/2330 train_time:11325ms step_avg:39.88ms
step:285/2330 train_time:11350ms step_avg:39.83ms
step:286/2330 train_time:11408ms step_avg:39.89ms
step:287/2330 train_time:11433ms step_avg:39.83ms
step:288/2330 train_time:11490ms step_avg:39.90ms
step:289/2330 train_time:11514ms step_avg:39.84ms
step:290/2330 train_time:11572ms step_avg:39.90ms
step:291/2330 train_time:11594ms step_avg:39.84ms
step:292/2330 train_time:11651ms step_avg:39.90ms
step:293/2330 train_time:11673ms step_avg:39.84ms
step:294/2330 train_time:11730ms step_avg:39.90ms
step:295/2330 train_time:11752ms step_avg:39.84ms
step:296/2330 train_time:11810ms step_avg:39.90ms
step:297/2330 train_time:11832ms step_avg:39.84ms
step:298/2330 train_time:11889ms step_avg:39.90ms
step:299/2330 train_time:11911ms step_avg:39.84ms
step:300/2330 train_time:11968ms step_avg:39.89ms
step:301/2330 train_time:11991ms step_avg:39.84ms
step:302/2330 train_time:12047ms step_avg:39.89ms
step:303/2330 train_time:12070ms step_avg:39.84ms
step:304/2330 train_time:12127ms step_avg:39.89ms
step:305/2330 train_time:12150ms step_avg:39.84ms
step:306/2330 train_time:12207ms step_avg:39.89ms
step:307/2330 train_time:12230ms step_avg:39.84ms
step:308/2330 train_time:12287ms step_avg:39.89ms
step:309/2330 train_time:12311ms step_avg:39.84ms
step:310/2330 train_time:12369ms step_avg:39.90ms
step:311/2330 train_time:12393ms step_avg:39.85ms
step:312/2330 train_time:12451ms step_avg:39.91ms
step:313/2330 train_time:12473ms step_avg:39.85ms
step:314/2330 train_time:12530ms step_avg:39.91ms
step:315/2330 train_time:12554ms step_avg:39.85ms
step:316/2330 train_time:12611ms step_avg:39.91ms
step:317/2330 train_time:12633ms step_avg:39.85ms
step:318/2330 train_time:12690ms step_avg:39.91ms
step:319/2330 train_time:12713ms step_avg:39.85ms
step:320/2330 train_time:12770ms step_avg:39.91ms
step:321/2330 train_time:12792ms step_avg:39.85ms
step:322/2330 train_time:12850ms step_avg:39.91ms
step:323/2330 train_time:12872ms step_avg:39.85ms
step:324/2330 train_time:12930ms step_avg:39.91ms
step:325/2330 train_time:12952ms step_avg:39.85ms
step:326/2330 train_time:13010ms step_avg:39.91ms
step:327/2330 train_time:13032ms step_avg:39.85ms
step:328/2330 train_time:13090ms step_avg:39.91ms
step:329/2330 train_time:13112ms step_avg:39.85ms
step:330/2330 train_time:13169ms step_avg:39.91ms
step:331/2330 train_time:13191ms step_avg:39.85ms
step:332/2330 train_time:13249ms step_avg:39.91ms
step:333/2330 train_time:13271ms step_avg:39.85ms
step:334/2330 train_time:13328ms step_avg:39.90ms
step:335/2330 train_time:13352ms step_avg:39.86ms
step:336/2330 train_time:13410ms step_avg:39.91ms
step:337/2330 train_time:13434ms step_avg:39.86ms
step:338/2330 train_time:13492ms step_avg:39.92ms
step:339/2330 train_time:13515ms step_avg:39.87ms
step:340/2330 train_time:13572ms step_avg:39.92ms
step:341/2330 train_time:13595ms step_avg:39.87ms
step:342/2330 train_time:13652ms step_avg:39.92ms
step:343/2330 train_time:13675ms step_avg:39.87ms
step:344/2330 train_time:13732ms step_avg:39.92ms
step:345/2330 train_time:13754ms step_avg:39.87ms
step:346/2330 train_time:13811ms step_avg:39.92ms
step:347/2330 train_time:13834ms step_avg:39.87ms
step:348/2330 train_time:13890ms step_avg:39.91ms
step:349/2330 train_time:13913ms step_avg:39.87ms
step:350/2330 train_time:13971ms step_avg:39.92ms
step:351/2330 train_time:13993ms step_avg:39.87ms
step:352/2330 train_time:14051ms step_avg:39.92ms
step:353/2330 train_time:14073ms step_avg:39.87ms
step:354/2330 train_time:14131ms step_avg:39.92ms
step:355/2330 train_time:14154ms step_avg:39.87ms
step:356/2330 train_time:14212ms step_avg:39.92ms
step:357/2330 train_time:14235ms step_avg:39.87ms
step:358/2330 train_time:14292ms step_avg:39.92ms
step:359/2330 train_time:14316ms step_avg:39.88ms
step:360/2330 train_time:14372ms step_avg:39.92ms
step:361/2330 train_time:14395ms step_avg:39.88ms
step:362/2330 train_time:14452ms step_avg:39.92ms
step:363/2330 train_time:14474ms step_avg:39.87ms
step:364/2330 train_time:14532ms step_avg:39.92ms
step:365/2330 train_time:14554ms step_avg:39.87ms
step:366/2330 train_time:14612ms step_avg:39.92ms
step:367/2330 train_time:14634ms step_avg:39.87ms
step:368/2330 train_time:14692ms step_avg:39.92ms
step:369/2330 train_time:14714ms step_avg:39.87ms
step:370/2330 train_time:14772ms step_avg:39.92ms
step:371/2330 train_time:14794ms step_avg:39.88ms
step:372/2330 train_time:14851ms step_avg:39.92ms
step:373/2330 train_time:14873ms step_avg:39.87ms
step:374/2330 train_time:14931ms step_avg:39.92ms
step:375/2330 train_time:14953ms step_avg:39.87ms
step:376/2330 train_time:15010ms step_avg:39.92ms
step:377/2330 train_time:15033ms step_avg:39.88ms
step:378/2330 train_time:15091ms step_avg:39.92ms
step:379/2330 train_time:15114ms step_avg:39.88ms
step:380/2330 train_time:15171ms step_avg:39.92ms
step:381/2330 train_time:15194ms step_avg:39.88ms
step:382/2330 train_time:15252ms step_avg:39.93ms
step:383/2330 train_time:15274ms step_avg:39.88ms
step:384/2330 train_time:15332ms step_avg:39.93ms
step:385/2330 train_time:15355ms step_avg:39.88ms
step:386/2330 train_time:15411ms step_avg:39.93ms
step:387/2330 train_time:15435ms step_avg:39.88ms
step:388/2330 train_time:15492ms step_avg:39.93ms
step:389/2330 train_time:15515ms step_avg:39.88ms
step:390/2330 train_time:15572ms step_avg:39.93ms
step:391/2330 train_time:15594ms step_avg:39.88ms
step:392/2330 train_time:15651ms step_avg:39.93ms
step:393/2330 train_time:15674ms step_avg:39.88ms
step:394/2330 train_time:15732ms step_avg:39.93ms
step:395/2330 train_time:15754ms step_avg:39.88ms
step:396/2330 train_time:15812ms step_avg:39.93ms
step:397/2330 train_time:15834ms step_avg:39.88ms
step:398/2330 train_time:15892ms step_avg:39.93ms
step:399/2330 train_time:15914ms step_avg:39.88ms
step:400/2330 train_time:15971ms step_avg:39.93ms
step:401/2330 train_time:15994ms step_avg:39.88ms
step:402/2330 train_time:16052ms step_avg:39.93ms
step:403/2330 train_time:16074ms step_avg:39.88ms
step:404/2330 train_time:16131ms step_avg:39.93ms
step:405/2330 train_time:16154ms step_avg:39.89ms
step:406/2330 train_time:16212ms step_avg:39.93ms
step:407/2330 train_time:16235ms step_avg:39.89ms
step:408/2330 train_time:16292ms step_avg:39.93ms
step:409/2330 train_time:16315ms step_avg:39.89ms
step:410/2330 train_time:16372ms step_avg:39.93ms
step:411/2330 train_time:16395ms step_avg:39.89ms
step:412/2330 train_time:16452ms step_avg:39.93ms
step:413/2330 train_time:16474ms step_avg:39.89ms
step:414/2330 train_time:16532ms step_avg:39.93ms
step:415/2330 train_time:16554ms step_avg:39.89ms
step:416/2330 train_time:16611ms step_avg:39.93ms
step:417/2330 train_time:16634ms step_avg:39.89ms
step:418/2330 train_time:16691ms step_avg:39.93ms
step:419/2330 train_time:16714ms step_avg:39.89ms
step:420/2330 train_time:16771ms step_avg:39.93ms
step:421/2330 train_time:16794ms step_avg:39.89ms
step:422/2330 train_time:16851ms step_avg:39.93ms
step:423/2330 train_time:16873ms step_avg:39.89ms
step:424/2330 train_time:16930ms step_avg:39.93ms
step:425/2330 train_time:16952ms step_avg:39.89ms
step:426/2330 train_time:17011ms step_avg:39.93ms
step:427/2330 train_time:17033ms step_avg:39.89ms
step:428/2330 train_time:17090ms step_avg:39.93ms
step:429/2330 train_time:17113ms step_avg:39.89ms
step:430/2330 train_time:17171ms step_avg:39.93ms
step:431/2330 train_time:17194ms step_avg:39.89ms
step:432/2330 train_time:17251ms step_avg:39.93ms
step:433/2330 train_time:17273ms step_avg:39.89ms
step:434/2330 train_time:17331ms step_avg:39.93ms
step:435/2330 train_time:17354ms step_avg:39.89ms
step:436/2330 train_time:17411ms step_avg:39.93ms
step:437/2330 train_time:17433ms step_avg:39.89ms
step:438/2330 train_time:17490ms step_avg:39.93ms
step:439/2330 train_time:17513ms step_avg:39.89ms
step:440/2330 train_time:17569ms step_avg:39.93ms
step:441/2330 train_time:17592ms step_avg:39.89ms
step:442/2330 train_time:17650ms step_avg:39.93ms
step:443/2330 train_time:17673ms step_avg:39.89ms
step:444/2330 train_time:17731ms step_avg:39.94ms
step:445/2330 train_time:17754ms step_avg:39.90ms
step:446/2330 train_time:17812ms step_avg:39.94ms
step:447/2330 train_time:17834ms step_avg:39.90ms
step:448/2330 train_time:17891ms step_avg:39.94ms
step:449/2330 train_time:17914ms step_avg:39.90ms
step:450/2330 train_time:17971ms step_avg:39.94ms
step:451/2330 train_time:17994ms step_avg:39.90ms
step:452/2330 train_time:18052ms step_avg:39.94ms
step:453/2330 train_time:18075ms step_avg:39.90ms
step:454/2330 train_time:18132ms step_avg:39.94ms
step:455/2330 train_time:18154ms step_avg:39.90ms
step:456/2330 train_time:18212ms step_avg:39.94ms
step:457/2330 train_time:18235ms step_avg:39.90ms
step:458/2330 train_time:18292ms step_avg:39.94ms
step:459/2330 train_time:18314ms step_avg:39.90ms
step:460/2330 train_time:18372ms step_avg:39.94ms
step:461/2330 train_time:18394ms step_avg:39.90ms
step:462/2330 train_time:18452ms step_avg:39.94ms
step:463/2330 train_time:18475ms step_avg:39.90ms
step:464/2330 train_time:18532ms step_avg:39.94ms
step:465/2330 train_time:18554ms step_avg:39.90ms
step:466/2330 train_time:18611ms step_avg:39.94ms
step:467/2330 train_time:18633ms step_avg:39.90ms
step:468/2330 train_time:18691ms step_avg:39.94ms
step:469/2330 train_time:18713ms step_avg:39.90ms
step:470/2330 train_time:18771ms step_avg:39.94ms
step:471/2330 train_time:18793ms step_avg:39.90ms
step:472/2330 train_time:18851ms step_avg:39.94ms
step:473/2330 train_time:18873ms step_avg:39.90ms
step:474/2330 train_time:18930ms step_avg:39.94ms
step:475/2330 train_time:18953ms step_avg:39.90ms
step:476/2330 train_time:19011ms step_avg:39.94ms
step:477/2330 train_time:19033ms step_avg:39.90ms
step:478/2330 train_time:19090ms step_avg:39.94ms
step:479/2330 train_time:19113ms step_avg:39.90ms
step:480/2330 train_time:19171ms step_avg:39.94ms
step:481/2330 train_time:19193ms step_avg:39.90ms
step:482/2330 train_time:19250ms step_avg:39.94ms
step:483/2330 train_time:19273ms step_avg:39.90ms
step:484/2330 train_time:19330ms step_avg:39.94ms
step:485/2330 train_time:19352ms step_avg:39.90ms
step:486/2330 train_time:19410ms step_avg:39.94ms
step:487/2330 train_time:19433ms step_avg:39.90ms
step:488/2330 train_time:19491ms step_avg:39.94ms
step:489/2330 train_time:19513ms step_avg:39.90ms
step:490/2330 train_time:19571ms step_avg:39.94ms
step:491/2330 train_time:19594ms step_avg:39.91ms
step:492/2330 train_time:19650ms step_avg:39.94ms
step:493/2330 train_time:19673ms step_avg:39.90ms
step:494/2330 train_time:19730ms step_avg:39.94ms
step:495/2330 train_time:19753ms step_avg:39.90ms
step:496/2330 train_time:19811ms step_avg:39.94ms
step:497/2330 train_time:19833ms step_avg:39.91ms
step:498/2330 train_time:19890ms step_avg:39.94ms
step:499/2330 train_time:19913ms step_avg:39.91ms
step:500/2330 train_time:19971ms step_avg:39.94ms
step:500/2330 val_loss:5.5078 train_time:20068ms step_avg:40.14ms
step:501/2330 train_time:20080ms step_avg:40.08ms
step:502/2330 train_time:20092ms step_avg:40.02ms
step:503/2330 train_time:20103ms step_avg:39.97ms
step:504/2330 train_time:20132ms step_avg:39.94ms
step:505/2330 train_time:20154ms step_avg:39.91ms
step:506/2330 train_time:20210ms step_avg:39.94ms
step:507/2330 train_time:20232ms step_avg:39.91ms
step:508/2330 train_time:20289ms step_avg:39.94ms
step:509/2330 train_time:20311ms step_avg:39.90ms
step:510/2330 train_time:20371ms step_avg:39.94ms
step:511/2330 train_time:20396ms step_avg:39.91ms
step:512/2330 train_time:20458ms step_avg:39.96ms
step:513/2330 train_time:20482ms step_avg:39.93ms
step:514/2330 train_time:20539ms step_avg:39.96ms
step:515/2330 train_time:20562ms step_avg:39.93ms
step:516/2330 train_time:20618ms step_avg:39.96ms
step:517/2330 train_time:20643ms step_avg:39.93ms
step:518/2330 train_time:20699ms step_avg:39.96ms
step:519/2330 train_time:20722ms step_avg:39.93ms
step:520/2330 train_time:20778ms step_avg:39.96ms
step:521/2330 train_time:20801ms step_avg:39.93ms
step:522/2330 train_time:20858ms step_avg:39.96ms
step:523/2330 train_time:20881ms step_avg:39.92ms
step:524/2330 train_time:20937ms step_avg:39.96ms
step:525/2330 train_time:20960ms step_avg:39.92ms
step:526/2330 train_time:21018ms step_avg:39.96ms
step:527/2330 train_time:21042ms step_avg:39.93ms
step:528/2330 train_time:21100ms step_avg:39.96ms
step:529/2330 train_time:21124ms step_avg:39.93ms
step:530/2330 train_time:21180ms step_avg:39.96ms
step:531/2330 train_time:21203ms step_avg:39.93ms
step:532/2330 train_time:21259ms step_avg:39.96ms
step:533/2330 train_time:21283ms step_avg:39.93ms
step:534/2330 train_time:21341ms step_avg:39.97ms
step:535/2330 train_time:21367ms step_avg:39.94ms
step:536/2330 train_time:21426ms step_avg:39.97ms
step:537/2330 train_time:21449ms step_avg:39.94ms
step:538/2330 train_time:21506ms step_avg:39.97ms
step:539/2330 train_time:21529ms step_avg:39.94ms
step:540/2330 train_time:21587ms step_avg:39.98ms
step:541/2330 train_time:21609ms step_avg:39.94ms
step:542/2330 train_time:21666ms step_avg:39.97ms
step:543/2330 train_time:21688ms step_avg:39.94ms
step:544/2330 train_time:21745ms step_avg:39.97ms
step:545/2330 train_time:21768ms step_avg:39.94ms
step:546/2330 train_time:21826ms step_avg:39.97ms
step:547/2330 train_time:21848ms step_avg:39.94ms
step:548/2330 train_time:21907ms step_avg:39.98ms
step:549/2330 train_time:21929ms step_avg:39.94ms
step:550/2330 train_time:21987ms step_avg:39.98ms
step:551/2330 train_time:22009ms step_avg:39.94ms
step:552/2330 train_time:22068ms step_avg:39.98ms
step:553/2330 train_time:22090ms step_avg:39.95ms
step:554/2330 train_time:22148ms step_avg:39.98ms
step:555/2330 train_time:22171ms step_avg:39.95ms
step:556/2330 train_time:22228ms step_avg:39.98ms
step:557/2330 train_time:22251ms step_avg:39.95ms
step:558/2330 train_time:22308ms step_avg:39.98ms
step:559/2330 train_time:22332ms step_avg:39.95ms
step:560/2330 train_time:22391ms step_avg:39.98ms
step:561/2330 train_time:22414ms step_avg:39.95ms
step:562/2330 train_time:22472ms step_avg:39.99ms
step:563/2330 train_time:22495ms step_avg:39.96ms
step:564/2330 train_time:22553ms step_avg:39.99ms
step:565/2330 train_time:22576ms step_avg:39.96ms
step:566/2330 train_time:22634ms step_avg:39.99ms
step:567/2330 train_time:22658ms step_avg:39.96ms
step:568/2330 train_time:22716ms step_avg:39.99ms
step:569/2330 train_time:22739ms step_avg:39.96ms
step:570/2330 train_time:22796ms step_avg:39.99ms
step:571/2330 train_time:22820ms step_avg:39.96ms
step:572/2330 train_time:22877ms step_avg:39.99ms
step:573/2330 train_time:22900ms step_avg:39.97ms
step:574/2330 train_time:22957ms step_avg:40.00ms
step:575/2330 train_time:22980ms step_avg:39.97ms
step:576/2330 train_time:23037ms step_avg:39.99ms
step:577/2330 train_time:23060ms step_avg:39.97ms
step:578/2330 train_time:23117ms step_avg:39.99ms
step:579/2330 train_time:23140ms step_avg:39.97ms
step:580/2330 train_time:23197ms step_avg:40.00ms
step:581/2330 train_time:23221ms step_avg:39.97ms
step:582/2330 train_time:23277ms step_avg:40.00ms
step:583/2330 train_time:23301ms step_avg:39.97ms
step:584/2330 train_time:23359ms step_avg:40.00ms
step:585/2330 train_time:23383ms step_avg:39.97ms
step:586/2330 train_time:23440ms step_avg:40.00ms
step:587/2330 train_time:23465ms step_avg:39.97ms
step:588/2330 train_time:23521ms step_avg:40.00ms
step:589/2330 train_time:23545ms step_avg:39.97ms
step:590/2330 train_time:23603ms step_avg:40.00ms
step:591/2330 train_time:23626ms step_avg:39.98ms
step:592/2330 train_time:23684ms step_avg:40.01ms
step:593/2330 train_time:23706ms step_avg:39.98ms
step:594/2330 train_time:23764ms step_avg:40.01ms
step:595/2330 train_time:23787ms step_avg:39.98ms
step:596/2330 train_time:23845ms step_avg:40.01ms
step:597/2330 train_time:23867ms step_avg:39.98ms
step:598/2330 train_time:23925ms step_avg:40.01ms
step:599/2330 train_time:23947ms step_avg:39.98ms
step:600/2330 train_time:24004ms step_avg:40.01ms
step:601/2330 train_time:24027ms step_avg:39.98ms
step:602/2330 train_time:24084ms step_avg:40.01ms
step:603/2330 train_time:24107ms step_avg:39.98ms
step:604/2330 train_time:24164ms step_avg:40.01ms
step:605/2330 train_time:24187ms step_avg:39.98ms
step:606/2330 train_time:24245ms step_avg:40.01ms
step:607/2330 train_time:24268ms step_avg:39.98ms
step:608/2330 train_time:24326ms step_avg:40.01ms
step:609/2330 train_time:24348ms step_avg:39.98ms
step:610/2330 train_time:24406ms step_avg:40.01ms
step:611/2330 train_time:24429ms step_avg:39.98ms
step:612/2330 train_time:24486ms step_avg:40.01ms
step:613/2330 train_time:24509ms step_avg:39.98ms
step:614/2330 train_time:24566ms step_avg:40.01ms
step:615/2330 train_time:24589ms step_avg:39.98ms
step:616/2330 train_time:24646ms step_avg:40.01ms
step:617/2330 train_time:24669ms step_avg:39.98ms
step:618/2330 train_time:24726ms step_avg:40.01ms
step:619/2330 train_time:24749ms step_avg:39.98ms
step:620/2330 train_time:24806ms step_avg:40.01ms
step:621/2330 train_time:24828ms step_avg:39.98ms
step:622/2330 train_time:24886ms step_avg:40.01ms
step:623/2330 train_time:24908ms step_avg:39.98ms
step:624/2330 train_time:24965ms step_avg:40.01ms
step:625/2330 train_time:24988ms step_avg:39.98ms
step:626/2330 train_time:25047ms step_avg:40.01ms
step:627/2330 train_time:25069ms step_avg:39.98ms
step:628/2330 train_time:25127ms step_avg:40.01ms
step:629/2330 train_time:25150ms step_avg:39.98ms
step:630/2330 train_time:25208ms step_avg:40.01ms
step:631/2330 train_time:25231ms step_avg:39.99ms
step:632/2330 train_time:25289ms step_avg:40.01ms
step:633/2330 train_time:25312ms step_avg:39.99ms
step:634/2330 train_time:25370ms step_avg:40.02ms
step:635/2330 train_time:25399ms step_avg:40.00ms
step:636/2330 train_time:25452ms step_avg:40.02ms
step:637/2330 train_time:25475ms step_avg:39.99ms
step:638/2330 train_time:25532ms step_avg:40.02ms
step:639/2330 train_time:25554ms step_avg:39.99ms
step:640/2330 train_time:25612ms step_avg:40.02ms
step:641/2330 train_time:25635ms step_avg:39.99ms
step:642/2330 train_time:25692ms step_avg:40.02ms
step:643/2330 train_time:25715ms step_avg:39.99ms
step:644/2330 train_time:25773ms step_avg:40.02ms
step:645/2330 train_time:25797ms step_avg:39.99ms
step:646/2330 train_time:25855ms step_avg:40.02ms
step:647/2330 train_time:25879ms step_avg:40.00ms
step:648/2330 train_time:25936ms step_avg:40.02ms
step:649/2330 train_time:25960ms step_avg:40.00ms
step:650/2330 train_time:26017ms step_avg:40.03ms
step:651/2330 train_time:26041ms step_avg:40.00ms
step:652/2330 train_time:26098ms step_avg:40.03ms
step:653/2330 train_time:26121ms step_avg:40.00ms
step:654/2330 train_time:26178ms step_avg:40.03ms
step:655/2330 train_time:26202ms step_avg:40.00ms
step:656/2330 train_time:26260ms step_avg:40.03ms
step:657/2330 train_time:26283ms step_avg:40.00ms
step:658/2330 train_time:26340ms step_avg:40.03ms
step:659/2330 train_time:26364ms step_avg:40.01ms
step:660/2330 train_time:26421ms step_avg:40.03ms
step:661/2330 train_time:26444ms step_avg:40.01ms
step:662/2330 train_time:26501ms step_avg:40.03ms
step:663/2330 train_time:26525ms step_avg:40.01ms
step:664/2330 train_time:26583ms step_avg:40.03ms
step:665/2330 train_time:26607ms step_avg:40.01ms
step:666/2330 train_time:26665ms step_avg:40.04ms
step:667/2330 train_time:26688ms step_avg:40.01ms
step:668/2330 train_time:26745ms step_avg:40.04ms
step:669/2330 train_time:26768ms step_avg:40.01ms
step:670/2330 train_time:26827ms step_avg:40.04ms
step:671/2330 train_time:26849ms step_avg:40.01ms
step:672/2330 train_time:26908ms step_avg:40.04ms
step:673/2330 train_time:26930ms step_avg:40.02ms
step:674/2330 train_time:26988ms step_avg:40.04ms
step:675/2330 train_time:27011ms step_avg:40.02ms
step:676/2330 train_time:27069ms step_avg:40.04ms
step:677/2330 train_time:27092ms step_avg:40.02ms
step:678/2330 train_time:27150ms step_avg:40.04ms
step:679/2330 train_time:27174ms step_avg:40.02ms
step:680/2330 train_time:27232ms step_avg:40.05ms
step:681/2330 train_time:27254ms step_avg:40.02ms
step:682/2330 train_time:27313ms step_avg:40.05ms
step:683/2330 train_time:27336ms step_avg:40.02ms
step:684/2330 train_time:27393ms step_avg:40.05ms
step:685/2330 train_time:27416ms step_avg:40.02ms
step:686/2330 train_time:27474ms step_avg:40.05ms
step:687/2330 train_time:27498ms step_avg:40.03ms
step:688/2330 train_time:27555ms step_avg:40.05ms
step:689/2330 train_time:27579ms step_avg:40.03ms
step:690/2330 train_time:27636ms step_avg:40.05ms
step:691/2330 train_time:27660ms step_avg:40.03ms
step:692/2330 train_time:27717ms step_avg:40.05ms
step:693/2330 train_time:27740ms step_avg:40.03ms
step:694/2330 train_time:27797ms step_avg:40.05ms
step:695/2330 train_time:27821ms step_avg:40.03ms
step:696/2330 train_time:27878ms step_avg:40.05ms
step:697/2330 train_time:27901ms step_avg:40.03ms
step:698/2330 train_time:27958ms step_avg:40.05ms
step:699/2330 train_time:27982ms step_avg:40.03ms
step:700/2330 train_time:28039ms step_avg:40.06ms
step:701/2330 train_time:28063ms step_avg:40.03ms
step:702/2330 train_time:28119ms step_avg:40.06ms
step:703/2330 train_time:28143ms step_avg:40.03ms
step:704/2330 train_time:28199ms step_avg:40.06ms
step:705/2330 train_time:28224ms step_avg:40.03ms
step:706/2330 train_time:28281ms step_avg:40.06ms
step:707/2330 train_time:28304ms step_avg:40.03ms
step:708/2330 train_time:28362ms step_avg:40.06ms
step:709/2330 train_time:28386ms step_avg:40.04ms
step:710/2330 train_time:28444ms step_avg:40.06ms
step:711/2330 train_time:28467ms step_avg:40.04ms
step:712/2330 train_time:28525ms step_avg:40.06ms
step:713/2330 train_time:28547ms step_avg:40.04ms
step:714/2330 train_time:28606ms step_avg:40.07ms
step:715/2330 train_time:28629ms step_avg:40.04ms
step:716/2330 train_time:28686ms step_avg:40.06ms
step:717/2330 train_time:28708ms step_avg:40.04ms
step:718/2330 train_time:28765ms step_avg:40.06ms
step:719/2330 train_time:28788ms step_avg:40.04ms
step:720/2330 train_time:28847ms step_avg:40.07ms
step:721/2330 train_time:28870ms step_avg:40.04ms
step:722/2330 train_time:28928ms step_avg:40.07ms
step:723/2330 train_time:28950ms step_avg:40.04ms
step:724/2330 train_time:29008ms step_avg:40.07ms
step:725/2330 train_time:29031ms step_avg:40.04ms
step:726/2330 train_time:29088ms step_avg:40.07ms
step:727/2330 train_time:29110ms step_avg:40.04ms
step:728/2330 train_time:29169ms step_avg:40.07ms
step:729/2330 train_time:29191ms step_avg:40.04ms
step:730/2330 train_time:29249ms step_avg:40.07ms
step:731/2330 train_time:29272ms step_avg:40.04ms
step:732/2330 train_time:29330ms step_avg:40.07ms
step:733/2330 train_time:29354ms step_avg:40.05ms
step:734/2330 train_time:29412ms step_avg:40.07ms
step:735/2330 train_time:29435ms step_avg:40.05ms
step:736/2330 train_time:29492ms step_avg:40.07ms
step:737/2330 train_time:29515ms step_avg:40.05ms
step:738/2330 train_time:29573ms step_avg:40.07ms
step:739/2330 train_time:29596ms step_avg:40.05ms
step:740/2330 train_time:29654ms step_avg:40.07ms
step:741/2330 train_time:29677ms step_avg:40.05ms
step:742/2330 train_time:29734ms step_avg:40.07ms
step:743/2330 train_time:29758ms step_avg:40.05ms
step:744/2330 train_time:29816ms step_avg:40.08ms
step:745/2330 train_time:29840ms step_avg:40.05ms
step:746/2330 train_time:29897ms step_avg:40.08ms
step:747/2330 train_time:29921ms step_avg:40.05ms
step:748/2330 train_time:29977ms step_avg:40.08ms
step:749/2330 train_time:30000ms step_avg:40.05ms
step:750/2330 train_time:30058ms step_avg:40.08ms
step:750/2330 val_loss:5.4106 train_time:30156ms step_avg:40.21ms
step:751/2330 train_time:30168ms step_avg:40.17ms
step:752/2330 train_time:30180ms step_avg:40.13ms
step:753/2330 train_time:30190ms step_avg:40.09ms
step:754/2330 train_time:30219ms step_avg:40.08ms
step:755/2330 train_time:30241ms step_avg:40.05ms
step:756/2330 train_time:30298ms step_avg:40.08ms
step:757/2330 train_time:30321ms step_avg:40.05ms
step:758/2330 train_time:30378ms step_avg:40.08ms
step:759/2330 train_time:30401ms step_avg:40.05ms
step:760/2330 train_time:30461ms step_avg:40.08ms
step:761/2330 train_time:30487ms step_avg:40.06ms
step:762/2330 train_time:30547ms step_avg:40.09ms
step:763/2330 train_time:30572ms step_avg:40.07ms
step:764/2330 train_time:30630ms step_avg:40.09ms
step:765/2330 train_time:30654ms step_avg:40.07ms
step:766/2330 train_time:30712ms step_avg:40.09ms
step:767/2330 train_time:30733ms step_avg:40.07ms
step:768/2330 train_time:30790ms step_avg:40.09ms
step:769/2330 train_time:30812ms step_avg:40.07ms
step:770/2330 train_time:30869ms step_avg:40.09ms
step:771/2330 train_time:30891ms step_avg:40.07ms
step:772/2330 train_time:30948ms step_avg:40.09ms
step:773/2330 train_time:30970ms step_avg:40.06ms
step:774/2330 train_time:31026ms step_avg:40.09ms
step:775/2330 train_time:31050ms step_avg:40.06ms
step:776/2330 train_time:31110ms step_avg:40.09ms
step:777/2330 train_time:31133ms step_avg:40.07ms
step:778/2330 train_time:31191ms step_avg:40.09ms
step:779/2330 train_time:31213ms step_avg:40.07ms
step:780/2330 train_time:31270ms step_avg:40.09ms
step:781/2330 train_time:31292ms step_avg:40.07ms
step:782/2330 train_time:31349ms step_avg:40.09ms
step:783/2330 train_time:31373ms step_avg:40.07ms
step:784/2330 train_time:31433ms step_avg:40.09ms
step:785/2330 train_time:31456ms step_avg:40.07ms
step:786/2330 train_time:31515ms step_avg:40.10ms
step:787/2330 train_time:31538ms step_avg:40.07ms
step:788/2330 train_time:31597ms step_avg:40.10ms
step:789/2330 train_time:31620ms step_avg:40.08ms
step:790/2330 train_time:31677ms step_avg:40.10ms
step:791/2330 train_time:31701ms step_avg:40.08ms
step:792/2330 train_time:31758ms step_avg:40.10ms
step:793/2330 train_time:31782ms step_avg:40.08ms
step:794/2330 train_time:31838ms step_avg:40.10ms
step:795/2330 train_time:31862ms step_avg:40.08ms
step:796/2330 train_time:31918ms step_avg:40.10ms
step:797/2330 train_time:31941ms step_avg:40.08ms
step:798/2330 train_time:31999ms step_avg:40.10ms
step:799/2330 train_time:32023ms step_avg:40.08ms
step:800/2330 train_time:32081ms step_avg:40.10ms
step:801/2330 train_time:32105ms step_avg:40.08ms
step:802/2330 train_time:32162ms step_avg:40.10ms
step:803/2330 train_time:32185ms step_avg:40.08ms
step:804/2330 train_time:32241ms step_avg:40.10ms
step:805/2330 train_time:32265ms step_avg:40.08ms
step:806/2330 train_time:32322ms step_avg:40.10ms
step:807/2330 train_time:32345ms step_avg:40.08ms
step:808/2330 train_time:32402ms step_avg:40.10ms
step:809/2330 train_time:32426ms step_avg:40.08ms
step:810/2330 train_time:32483ms step_avg:40.10ms
step:811/2330 train_time:32508ms step_avg:40.08ms
step:812/2330 train_time:32565ms step_avg:40.11ms
step:813/2330 train_time:32590ms step_avg:40.09ms
step:814/2330 train_time:32647ms step_avg:40.11ms
step:815/2330 train_time:32670ms step_avg:40.09ms
step:816/2330 train_time:32728ms step_avg:40.11ms
step:817/2330 train_time:32751ms step_avg:40.09ms
step:818/2330 train_time:32809ms step_avg:40.11ms
step:819/2330 train_time:32832ms step_avg:40.09ms
step:820/2330 train_time:32889ms step_avg:40.11ms
step:821/2330 train_time:32912ms step_avg:40.09ms
step:822/2330 train_time:32969ms step_avg:40.11ms
step:823/2330 train_time:32992ms step_avg:40.09ms
step:824/2330 train_time:33049ms step_avg:40.11ms
step:825/2330 train_time:33072ms step_avg:40.09ms
step:826/2330 train_time:33129ms step_avg:40.11ms
step:827/2330 train_time:33151ms step_avg:40.09ms
step:828/2330 train_time:33209ms step_avg:40.11ms
step:829/2330 train_time:33232ms step_avg:40.09ms
step:830/2330 train_time:33290ms step_avg:40.11ms
step:831/2330 train_time:33312ms step_avg:40.09ms
step:832/2330 train_time:33370ms step_avg:40.11ms
step:833/2330 train_time:33393ms step_avg:40.09ms
step:834/2330 train_time:33452ms step_avg:40.11ms
step:835/2330 train_time:33474ms step_avg:40.09ms
step:836/2330 train_time:33532ms step_avg:40.11ms
step:837/2330 train_time:33554ms step_avg:40.09ms
step:838/2330 train_time:33613ms step_avg:40.11ms
step:839/2330 train_time:33637ms step_avg:40.09ms
step:840/2330 train_time:33694ms step_avg:40.11ms
step:841/2330 train_time:33717ms step_avg:40.09ms
step:842/2330 train_time:33775ms step_avg:40.11ms
step:843/2330 train_time:33798ms step_avg:40.09ms
step:844/2330 train_time:33855ms step_avg:40.11ms
step:845/2330 train_time:33878ms step_avg:40.09ms
step:846/2330 train_time:33936ms step_avg:40.11ms
step:847/2330 train_time:33959ms step_avg:40.09ms
step:848/2330 train_time:34017ms step_avg:40.11ms
step:849/2330 train_time:34040ms step_avg:40.09ms
step:850/2330 train_time:34097ms step_avg:40.11ms
step:851/2330 train_time:34121ms step_avg:40.10ms
step:852/2330 train_time:34178ms step_avg:40.12ms
step:853/2330 train_time:34202ms step_avg:40.10ms
step:854/2330 train_time:34259ms step_avg:40.12ms
step:855/2330 train_time:34283ms step_avg:40.10ms
step:856/2330 train_time:34340ms step_avg:40.12ms
step:857/2330 train_time:34364ms step_avg:40.10ms
step:858/2330 train_time:34422ms step_avg:40.12ms
step:859/2330 train_time:34446ms step_avg:40.10ms
step:860/2330 train_time:34504ms step_avg:40.12ms
step:861/2330 train_time:34528ms step_avg:40.10ms
step:862/2330 train_time:34585ms step_avg:40.12ms
step:863/2330 train_time:34609ms step_avg:40.10ms
step:864/2330 train_time:34665ms step_avg:40.12ms
step:865/2330 train_time:34689ms step_avg:40.10ms
step:866/2330 train_time:34745ms step_avg:40.12ms
step:867/2330 train_time:34769ms step_avg:40.10ms
step:868/2330 train_time:34826ms step_avg:40.12ms
step:869/2330 train_time:34849ms step_avg:40.10ms
step:870/2330 train_time:34906ms step_avg:40.12ms
step:871/2330 train_time:34930ms step_avg:40.10ms
step:872/2330 train_time:34987ms step_avg:40.12ms
step:873/2330 train_time:35010ms step_avg:40.10ms
step:874/2330 train_time:35068ms step_avg:40.12ms
step:875/2330 train_time:35092ms step_avg:40.10ms
step:876/2330 train_time:35149ms step_avg:40.12ms
step:877/2330 train_time:35172ms step_avg:40.10ms
step:878/2330 train_time:35229ms step_avg:40.12ms
step:879/2330 train_time:35252ms step_avg:40.10ms
step:880/2330 train_time:35311ms step_avg:40.13ms
step:881/2330 train_time:35334ms step_avg:40.11ms
step:882/2330 train_time:35392ms step_avg:40.13ms
step:883/2330 train_time:35415ms step_avg:40.11ms
step:884/2330 train_time:35474ms step_avg:40.13ms
step:885/2330 train_time:35496ms step_avg:40.11ms
step:886/2330 train_time:35554ms step_avg:40.13ms
step:887/2330 train_time:35577ms step_avg:40.11ms
step:888/2330 train_time:35635ms step_avg:40.13ms
step:889/2330 train_time:35659ms step_avg:40.11ms
step:890/2330 train_time:35718ms step_avg:40.13ms
step:891/2330 train_time:35741ms step_avg:40.11ms
step:892/2330 train_time:35799ms step_avg:40.13ms
step:893/2330 train_time:35822ms step_avg:40.11ms
step:894/2330 train_time:35880ms step_avg:40.13ms
step:895/2330 train_time:35904ms step_avg:40.12ms
step:896/2330 train_time:35962ms step_avg:40.14ms
step:897/2330 train_time:35986ms step_avg:40.12ms
step:898/2330 train_time:36043ms step_avg:40.14ms
step:899/2330 train_time:36067ms step_avg:40.12ms
step:900/2330 train_time:36124ms step_avg:40.14ms
step:901/2330 train_time:36147ms step_avg:40.12ms
step:902/2330 train_time:36204ms step_avg:40.14ms
step:903/2330 train_time:36228ms step_avg:40.12ms
step:904/2330 train_time:36285ms step_avg:40.14ms
step:905/2330 train_time:36308ms step_avg:40.12ms
step:906/2330 train_time:36365ms step_avg:40.14ms
step:907/2330 train_time:36389ms step_avg:40.12ms
step:908/2330 train_time:36446ms step_avg:40.14ms
step:909/2330 train_time:36469ms step_avg:40.12ms
step:910/2330 train_time:36527ms step_avg:40.14ms
step:911/2330 train_time:36550ms step_avg:40.12ms
step:912/2330 train_time:36608ms step_avg:40.14ms
step:913/2330 train_time:36631ms step_avg:40.12ms
step:914/2330 train_time:36689ms step_avg:40.14ms
step:915/2330 train_time:36712ms step_avg:40.12ms
step:916/2330 train_time:36770ms step_avg:40.14ms
step:917/2330 train_time:36792ms step_avg:40.12ms
step:918/2330 train_time:36851ms step_avg:40.14ms
step:919/2330 train_time:36873ms step_avg:40.12ms
step:920/2330 train_time:36931ms step_avg:40.14ms
step:921/2330 train_time:36953ms step_avg:40.12ms
step:922/2330 train_time:37011ms step_avg:40.14ms
step:923/2330 train_time:37034ms step_avg:40.12ms
step:924/2330 train_time:37092ms step_avg:40.14ms
step:925/2330 train_time:37115ms step_avg:40.12ms
step:926/2330 train_time:37172ms step_avg:40.14ms
step:927/2330 train_time:37195ms step_avg:40.12ms
step:928/2330 train_time:37254ms step_avg:40.14ms
step:929/2330 train_time:37277ms step_avg:40.13ms
step:930/2330 train_time:37334ms step_avg:40.14ms
step:931/2330 train_time:37358ms step_avg:40.13ms
step:932/2330 train_time:37416ms step_avg:40.15ms
step:933/2330 train_time:37439ms step_avg:40.13ms
step:934/2330 train_time:37497ms step_avg:40.15ms
step:935/2330 train_time:37520ms step_avg:40.13ms
step:936/2330 train_time:37578ms step_avg:40.15ms
step:937/2330 train_time:37602ms step_avg:40.13ms
step:938/2330 train_time:37659ms step_avg:40.15ms
step:939/2330 train_time:37683ms step_avg:40.13ms
step:940/2330 train_time:37740ms step_avg:40.15ms
step:941/2330 train_time:37764ms step_avg:40.13ms
step:942/2330 train_time:37822ms step_avg:40.15ms
step:943/2330 train_time:37845ms step_avg:40.13ms
step:944/2330 train_time:37902ms step_avg:40.15ms
step:945/2330 train_time:37925ms step_avg:40.13ms
step:946/2330 train_time:37982ms step_avg:40.15ms
step:947/2330 train_time:38006ms step_avg:40.13ms
step:948/2330 train_time:38064ms step_avg:40.15ms
step:949/2330 train_time:38088ms step_avg:40.13ms
step:950/2330 train_time:38144ms step_avg:40.15ms
step:951/2330 train_time:38168ms step_avg:40.13ms
step:952/2330 train_time:38225ms step_avg:40.15ms
step:953/2330 train_time:38249ms step_avg:40.13ms
step:954/2330 train_time:38306ms step_avg:40.15ms
step:955/2330 train_time:38330ms step_avg:40.14ms
step:956/2330 train_time:38388ms step_avg:40.15ms
step:957/2330 train_time:38410ms step_avg:40.14ms
step:958/2330 train_time:38469ms step_avg:40.16ms
step:959/2330 train_time:38491ms step_avg:40.14ms
step:960/2330 train_time:38550ms step_avg:40.16ms
step:961/2330 train_time:38572ms step_avg:40.14ms
step:962/2330 train_time:38630ms step_avg:40.16ms
step:963/2330 train_time:38653ms step_avg:40.14ms
step:964/2330 train_time:38711ms step_avg:40.16ms
step:965/2330 train_time:38734ms step_avg:40.14ms
step:966/2330 train_time:38792ms step_avg:40.16ms
step:967/2330 train_time:38814ms step_avg:40.14ms
step:968/2330 train_time:38872ms step_avg:40.16ms
step:969/2330 train_time:38894ms step_avg:40.14ms
step:970/2330 train_time:38952ms step_avg:40.16ms
step:971/2330 train_time:38976ms step_avg:40.14ms
step:972/2330 train_time:39034ms step_avg:40.16ms
step:973/2330 train_time:39057ms step_avg:40.14ms
step:974/2330 train_time:39116ms step_avg:40.16ms
step:975/2330 train_time:39138ms step_avg:40.14ms
step:976/2330 train_time:39196ms step_avg:40.16ms
step:977/2330 train_time:39219ms step_avg:40.14ms
step:978/2330 train_time:39276ms step_avg:40.16ms
step:979/2330 train_time:39300ms step_avg:40.14ms
step:980/2330 train_time:39358ms step_avg:40.16ms
step:981/2330 train_time:39381ms step_avg:40.14ms
step:982/2330 train_time:39439ms step_avg:40.16ms
step:983/2330 train_time:39462ms step_avg:40.14ms
step:984/2330 train_time:39519ms step_avg:40.16ms
step:985/2330 train_time:39542ms step_avg:40.14ms
step:986/2330 train_time:39600ms step_avg:40.16ms
step:987/2330 train_time:39624ms step_avg:40.15ms
step:988/2330 train_time:39681ms step_avg:40.16ms
step:989/2330 train_time:39705ms step_avg:40.15ms
step:990/2330 train_time:39762ms step_avg:40.16ms
step:991/2330 train_time:39785ms step_avg:40.15ms
step:992/2330 train_time:39843ms step_avg:40.16ms
step:993/2330 train_time:39866ms step_avg:40.15ms
step:994/2330 train_time:39923ms step_avg:40.16ms
step:995/2330 train_time:39947ms step_avg:40.15ms
step:996/2330 train_time:40004ms step_avg:40.17ms
step:997/2330 train_time:40028ms step_avg:40.15ms
step:998/2330 train_time:40085ms step_avg:40.17ms
step:999/2330 train_time:40108ms step_avg:40.15ms
step:1000/2330 train_time:40166ms step_avg:40.17ms
step:1000/2330 val_loss:5.3477 train_time:40264ms step_avg:40.26ms
step:1001/2330 train_time:40278ms step_avg:40.24ms
step:1002/2330 train_time:40289ms step_avg:40.21ms
step:1003/2330 train_time:40300ms step_avg:40.18ms
step:1004/2330 train_time:40327ms step_avg:40.17ms
step:1005/2330 train_time:40349ms step_avg:40.15ms
step:1006/2330 train_time:40405ms step_avg:40.16ms
step:1007/2330 train_time:40427ms step_avg:40.15ms
step:1008/2330 train_time:40484ms step_avg:40.16ms
step:1009/2330 train_time:40506ms step_avg:40.14ms
step:1010/2330 train_time:40566ms step_avg:40.16ms
step:1011/2330 train_time:40594ms step_avg:40.15ms
step:1012/2330 train_time:40654ms step_avg:40.17ms
step:1013/2330 train_time:40679ms step_avg:40.16ms
step:1014/2330 train_time:40736ms step_avg:40.17ms
step:1015/2330 train_time:40759ms step_avg:40.16ms
step:1016/2330 train_time:40816ms step_avg:40.17ms
step:1017/2330 train_time:40839ms step_avg:40.16ms
step:1018/2330 train_time:40896ms step_avg:40.17ms
step:1019/2330 train_time:40918ms step_avg:40.16ms
step:1020/2330 train_time:40975ms step_avg:40.17ms
step:1021/2330 train_time:40998ms step_avg:40.15ms
step:1022/2330 train_time:41055ms step_avg:40.17ms
step:1023/2330 train_time:41078ms step_avg:40.15ms
step:1024/2330 train_time:41135ms step_avg:40.17ms
step:1025/2330 train_time:41157ms step_avg:40.15ms
step:1026/2330 train_time:41218ms step_avg:40.17ms
step:1027/2330 train_time:41241ms step_avg:40.16ms
step:1028/2330 train_time:41300ms step_avg:40.17ms
step:1029/2330 train_time:41323ms step_avg:40.16ms
step:1030/2330 train_time:41379ms step_avg:40.17ms
step:1031/2330 train_time:41402ms step_avg:40.16ms
step:1032/2330 train_time:41460ms step_avg:40.17ms
step:1033/2330 train_time:41484ms step_avg:40.16ms
step:1034/2330 train_time:41543ms step_avg:40.18ms
step:1035/2330 train_time:41569ms step_avg:40.16ms
step:1036/2330 train_time:41627ms step_avg:40.18ms
step:1037/2330 train_time:41652ms step_avg:40.17ms
step:1038/2330 train_time:41709ms step_avg:40.18ms
step:1039/2330 train_time:41733ms step_avg:40.17ms
step:1040/2330 train_time:41789ms step_avg:40.18ms
step:1041/2330 train_time:41812ms step_avg:40.17ms
step:1042/2330 train_time:41869ms step_avg:40.18ms
step:1043/2330 train_time:41892ms step_avg:40.17ms
step:1044/2330 train_time:41949ms step_avg:40.18ms
step:1045/2330 train_time:41973ms step_avg:40.17ms
step:1046/2330 train_time:42030ms step_avg:40.18ms
step:1047/2330 train_time:42053ms step_avg:40.17ms
step:1048/2330 train_time:42110ms step_avg:40.18ms
step:1049/2330 train_time:42133ms step_avg:40.16ms
step:1050/2330 train_time:42190ms step_avg:40.18ms
step:1051/2330 train_time:42213ms step_avg:40.16ms
step:1052/2330 train_time:42271ms step_avg:40.18ms
step:1053/2330 train_time:42295ms step_avg:40.17ms
step:1054/2330 train_time:42353ms step_avg:40.18ms
step:1055/2330 train_time:42376ms step_avg:40.17ms
step:1056/2330 train_time:42434ms step_avg:40.18ms
step:1057/2330 train_time:42457ms step_avg:40.17ms
step:1058/2330 train_time:42517ms step_avg:40.19ms
step:1059/2330 train_time:42541ms step_avg:40.17ms
step:1060/2330 train_time:42600ms step_avg:40.19ms
step:1061/2330 train_time:42623ms step_avg:40.17ms
step:1062/2330 train_time:42681ms step_avg:40.19ms
step:1063/2330 train_time:42704ms step_avg:40.17ms
step:1064/2330 train_time:42762ms step_avg:40.19ms
step:1065/2330 train_time:42786ms step_avg:40.17ms
step:1066/2330 train_time:42843ms step_avg:40.19ms
step:1067/2330 train_time:42866ms step_avg:40.17ms
step:1068/2330 train_time:42924ms step_avg:40.19ms
step:1069/2330 train_time:42947ms step_avg:40.18ms
step:1070/2330 train_time:43004ms step_avg:40.19ms
step:1071/2330 train_time:43027ms step_avg:40.17ms
step:1072/2330 train_time:43084ms step_avg:40.19ms
step:1073/2330 train_time:43108ms step_avg:40.18ms
step:1074/2330 train_time:43165ms step_avg:40.19ms
step:1075/2330 train_time:43188ms step_avg:40.18ms
step:1076/2330 train_time:43245ms step_avg:40.19ms
step:1077/2330 train_time:43269ms step_avg:40.18ms
step:1078/2330 train_time:43326ms step_avg:40.19ms
step:1079/2330 train_time:43350ms step_avg:40.18ms
step:1080/2330 train_time:43407ms step_avg:40.19ms
step:1081/2330 train_time:43432ms step_avg:40.18ms
step:1082/2330 train_time:43489ms step_avg:40.19ms
step:1083/2330 train_time:43514ms step_avg:40.18ms
step:1084/2330 train_time:43572ms step_avg:40.20ms
step:1085/2330 train_time:43595ms step_avg:40.18ms
step:1086/2330 train_time:43652ms step_avg:40.20ms
step:1087/2330 train_time:43675ms step_avg:40.18ms
step:1088/2330 train_time:43733ms step_avg:40.20ms
step:1089/2330 train_time:43756ms step_avg:40.18ms
step:1090/2330 train_time:43814ms step_avg:40.20ms
step:1091/2330 train_time:43836ms step_avg:40.18ms
step:1092/2330 train_time:43893ms step_avg:40.20ms
step:1093/2330 train_time:43917ms step_avg:40.18ms
step:1094/2330 train_time:43974ms step_avg:40.20ms
step:1095/2330 train_time:43997ms step_avg:40.18ms
step:1096/2330 train_time:44054ms step_avg:40.20ms
step:1097/2330 train_time:44077ms step_avg:40.18ms
step:1098/2330 train_time:44134ms step_avg:40.19ms
step:1099/2330 train_time:44157ms step_avg:40.18ms
step:1100/2330 train_time:44215ms step_avg:40.20ms
step:1101/2330 train_time:44238ms step_avg:40.18ms
step:1102/2330 train_time:44296ms step_avg:40.20ms
step:1103/2330 train_time:44319ms step_avg:40.18ms
step:1104/2330 train_time:44377ms step_avg:40.20ms
step:1105/2330 train_time:44400ms step_avg:40.18ms
step:1106/2330 train_time:44458ms step_avg:40.20ms
step:1107/2330 train_time:44482ms step_avg:40.18ms
step:1108/2330 train_time:44540ms step_avg:40.20ms
step:1109/2330 train_time:44562ms step_avg:40.18ms
step:1110/2330 train_time:44620ms step_avg:40.20ms
step:1111/2330 train_time:44645ms step_avg:40.18ms
step:1112/2330 train_time:44702ms step_avg:40.20ms
step:1113/2330 train_time:44726ms step_avg:40.18ms
step:1114/2330 train_time:44784ms step_avg:40.20ms
step:1115/2330 train_time:44808ms step_avg:40.19ms
step:1116/2330 train_time:44865ms step_avg:40.20ms
step:1117/2330 train_time:44889ms step_avg:40.19ms
step:1118/2330 train_time:44946ms step_avg:40.20ms
step:1119/2330 train_time:44970ms step_avg:40.19ms
step:1120/2330 train_time:45027ms step_avg:40.20ms
step:1121/2330 train_time:45051ms step_avg:40.19ms
step:1122/2330 train_time:45108ms step_avg:40.20ms
step:1123/2330 train_time:45132ms step_avg:40.19ms
step:1124/2330 train_time:45188ms step_avg:40.20ms
step:1125/2330 train_time:45212ms step_avg:40.19ms
step:1126/2330 train_time:45269ms step_avg:40.20ms
step:1127/2330 train_time:45292ms step_avg:40.19ms
step:1128/2330 train_time:45349ms step_avg:40.20ms
step:1129/2330 train_time:45373ms step_avg:40.19ms
step:1130/2330 train_time:45431ms step_avg:40.20ms
step:1131/2330 train_time:45456ms step_avg:40.19ms
step:1132/2330 train_time:45513ms step_avg:40.21ms
step:1133/2330 train_time:45536ms step_avg:40.19ms
step:1134/2330 train_time:45594ms step_avg:40.21ms
step:1135/2330 train_time:45617ms step_avg:40.19ms
step:1136/2330 train_time:45675ms step_avg:40.21ms
step:1137/2330 train_time:45698ms step_avg:40.19ms
step:1138/2330 train_time:45757ms step_avg:40.21ms
step:1139/2330 train_time:45779ms step_avg:40.19ms
step:1140/2330 train_time:45837ms step_avg:40.21ms
step:1141/2330 train_time:45860ms step_avg:40.19ms
step:1142/2330 train_time:45918ms step_avg:40.21ms
step:1143/2330 train_time:45942ms step_avg:40.19ms
step:1144/2330 train_time:46001ms step_avg:40.21ms
step:1145/2330 train_time:46023ms step_avg:40.20ms
step:1146/2330 train_time:46081ms step_avg:40.21ms
step:1147/2330 train_time:46105ms step_avg:40.20ms
step:1148/2330 train_time:46163ms step_avg:40.21ms
step:1149/2330 train_time:46186ms step_avg:40.20ms
step:1150/2330 train_time:46243ms step_avg:40.21ms
step:1151/2330 train_time:46267ms step_avg:40.20ms
step:1152/2330 train_time:46324ms step_avg:40.21ms
step:1153/2330 train_time:46348ms step_avg:40.20ms
step:1154/2330 train_time:46405ms step_avg:40.21ms
step:1155/2330 train_time:46429ms step_avg:40.20ms
step:1156/2330 train_time:46487ms step_avg:40.21ms
step:1157/2330 train_time:46511ms step_avg:40.20ms
step:1158/2330 train_time:46568ms step_avg:40.21ms
step:1159/2330 train_time:46592ms step_avg:40.20ms
step:1160/2330 train_time:46650ms step_avg:40.22ms
step:1161/2330 train_time:46673ms step_avg:40.20ms
step:1162/2330 train_time:46730ms step_avg:40.22ms
step:1163/2330 train_time:46754ms step_avg:40.20ms
step:1164/2330 train_time:46812ms step_avg:40.22ms
step:1165/2330 train_time:46835ms step_avg:40.20ms
step:1166/2330 train_time:46892ms step_avg:40.22ms
step:1167/2330 train_time:46915ms step_avg:40.20ms
step:1168/2330 train_time:46973ms step_avg:40.22ms
step:1169/2330 train_time:46996ms step_avg:40.20ms
step:1170/2330 train_time:47055ms step_avg:40.22ms
step:1171/2330 train_time:47077ms step_avg:40.20ms
step:1172/2330 train_time:47135ms step_avg:40.22ms
step:1173/2330 train_time:47158ms step_avg:40.20ms
step:1174/2330 train_time:47217ms step_avg:40.22ms
step:1175/2330 train_time:47239ms step_avg:40.20ms
step:1176/2330 train_time:47297ms step_avg:40.22ms
step:1177/2330 train_time:47320ms step_avg:40.20ms
step:1178/2330 train_time:47378ms step_avg:40.22ms
step:1179/2330 train_time:47401ms step_avg:40.20ms
step:1180/2330 train_time:47460ms step_avg:40.22ms
step:1181/2330 train_time:47482ms step_avg:40.21ms
step:1182/2330 train_time:47540ms step_avg:40.22ms
step:1183/2330 train_time:47563ms step_avg:40.21ms
step:1184/2330 train_time:47622ms step_avg:40.22ms
step:1185/2330 train_time:47646ms step_avg:40.21ms
step:1186/2330 train_time:47703ms step_avg:40.22ms
step:1187/2330 train_time:47727ms step_avg:40.21ms
step:1188/2330 train_time:47784ms step_avg:40.22ms
step:1189/2330 train_time:47808ms step_avg:40.21ms
step:1190/2330 train_time:47865ms step_avg:40.22ms
step:1191/2330 train_time:47889ms step_avg:40.21ms
step:1192/2330 train_time:47946ms step_avg:40.22ms
step:1193/2330 train_time:47969ms step_avg:40.21ms
step:1194/2330 train_time:48026ms step_avg:40.22ms
step:1195/2330 train_time:48050ms step_avg:40.21ms
step:1196/2330 train_time:48107ms step_avg:40.22ms
step:1197/2330 train_time:48130ms step_avg:40.21ms
step:1198/2330 train_time:48187ms step_avg:40.22ms
step:1199/2330 train_time:48211ms step_avg:40.21ms
step:1200/2330 train_time:48267ms step_avg:40.22ms
step:1201/2330 train_time:48291ms step_avg:40.21ms
step:1202/2330 train_time:48347ms step_avg:40.22ms
step:1203/2330 train_time:48371ms step_avg:40.21ms
step:1204/2330 train_time:48429ms step_avg:40.22ms
step:1205/2330 train_time:48453ms step_avg:40.21ms
step:1206/2330 train_time:48510ms step_avg:40.22ms
step:1207/2330 train_time:48534ms step_avg:40.21ms
step:1208/2330 train_time:48593ms step_avg:40.23ms
step:1209/2330 train_time:48616ms step_avg:40.21ms
step:1210/2330 train_time:48673ms step_avg:40.23ms
step:1211/2330 train_time:48696ms step_avg:40.21ms
step:1212/2330 train_time:48754ms step_avg:40.23ms
step:1213/2330 train_time:48777ms step_avg:40.21ms
step:1214/2330 train_time:48835ms step_avg:40.23ms
step:1215/2330 train_time:48858ms step_avg:40.21ms
step:1216/2330 train_time:48915ms step_avg:40.23ms
step:1217/2330 train_time:48938ms step_avg:40.21ms
step:1218/2330 train_time:48996ms step_avg:40.23ms
step:1219/2330 train_time:49018ms step_avg:40.21ms
step:1220/2330 train_time:49076ms step_avg:40.23ms
step:1221/2330 train_time:49099ms step_avg:40.21ms
step:1222/2330 train_time:49158ms step_avg:40.23ms
step:1223/2330 train_time:49181ms step_avg:40.21ms
step:1224/2330 train_time:49239ms step_avg:40.23ms
step:1225/2330 train_time:49261ms step_avg:40.21ms
step:1226/2330 train_time:49320ms step_avg:40.23ms
step:1227/2330 train_time:49343ms step_avg:40.21ms
step:1228/2330 train_time:49401ms step_avg:40.23ms
step:1229/2330 train_time:49424ms step_avg:40.22ms
step:1230/2330 train_time:49482ms step_avg:40.23ms
step:1231/2330 train_time:49506ms step_avg:40.22ms
step:1232/2330 train_time:49564ms step_avg:40.23ms
step:1233/2330 train_time:49588ms step_avg:40.22ms
step:1234/2330 train_time:49644ms step_avg:40.23ms
step:1235/2330 train_time:49668ms step_avg:40.22ms
step:1236/2330 train_time:49726ms step_avg:40.23ms
step:1237/2330 train_time:49749ms step_avg:40.22ms
step:1238/2330 train_time:49806ms step_avg:40.23ms
step:1239/2330 train_time:49831ms step_avg:40.22ms
step:1240/2330 train_time:49887ms step_avg:40.23ms
step:1241/2330 train_time:49911ms step_avg:40.22ms
step:1242/2330 train_time:49967ms step_avg:40.23ms
step:1243/2330 train_time:49990ms step_avg:40.22ms
step:1244/2330 train_time:50047ms step_avg:40.23ms
step:1245/2330 train_time:50071ms step_avg:40.22ms
step:1246/2330 train_time:50128ms step_avg:40.23ms
step:1247/2330 train_time:50153ms step_avg:40.22ms
step:1248/2330 train_time:50210ms step_avg:40.23ms
step:1249/2330 train_time:50234ms step_avg:40.22ms
step:1250/2330 train_time:50293ms step_avg:40.23ms
step:1250/2330 val_loss:5.3086 train_time:50390ms step_avg:40.31ms
step:1251/2330 train_time:50403ms step_avg:40.29ms
step:1252/2330 train_time:50415ms step_avg:40.27ms
step:1253/2330 train_time:50426ms step_avg:40.24ms
step:1254/2330 train_time:50453ms step_avg:40.23ms
step:1255/2330 train_time:50475ms step_avg:40.22ms
step:1256/2330 train_time:50531ms step_avg:40.23ms
step:1257/2330 train_time:50553ms step_avg:40.22ms
step:1258/2330 train_time:50609ms step_avg:40.23ms
step:1259/2330 train_time:50631ms step_avg:40.22ms
step:1260/2330 train_time:50691ms step_avg:40.23ms
step:1261/2330 train_time:50716ms step_avg:40.22ms
step:1262/2330 train_time:50778ms step_avg:40.24ms
step:1263/2330 train_time:50803ms step_avg:40.22ms
step:1264/2330 train_time:50861ms step_avg:40.24ms
step:1265/2330 train_time:50885ms step_avg:40.22ms
step:1266/2330 train_time:50941ms step_avg:40.24ms
step:1267/2330 train_time:50965ms step_avg:40.22ms
step:1268/2330 train_time:51021ms step_avg:40.24ms
step:1269/2330 train_time:51045ms step_avg:40.22ms
step:1270/2330 train_time:51101ms step_avg:40.24ms
step:1271/2330 train_time:51124ms step_avg:40.22ms
step:1272/2330 train_time:51181ms step_avg:40.24ms
step:1273/2330 train_time:51204ms step_avg:40.22ms
step:1274/2330 train_time:51261ms step_avg:40.24ms
step:1275/2330 train_time:51284ms step_avg:40.22ms
step:1276/2330 train_time:51343ms step_avg:40.24ms
step:1277/2330 train_time:51368ms step_avg:40.23ms
step:1278/2330 train_time:51425ms step_avg:40.24ms
step:1279/2330 train_time:51448ms step_avg:40.23ms
step:1280/2330 train_time:51505ms step_avg:40.24ms
step:1281/2330 train_time:51528ms step_avg:40.22ms
step:1282/2330 train_time:51584ms step_avg:40.24ms
step:1283/2330 train_time:51608ms step_avg:40.22ms
step:1284/2330 train_time:51666ms step_avg:40.24ms
step:1285/2330 train_time:51691ms step_avg:40.23ms
step:1286/2330 train_time:51749ms step_avg:40.24ms
step:1287/2330 train_time:51774ms step_avg:40.23ms
step:1288/2330 train_time:51831ms step_avg:40.24ms
step:1289/2330 train_time:51854ms step_avg:40.23ms
step:1290/2330 train_time:51913ms step_avg:40.24ms
step:1291/2330 train_time:51935ms step_avg:40.23ms
step:1292/2330 train_time:51992ms step_avg:40.24ms
step:1293/2330 train_time:52015ms step_avg:40.23ms
step:1294/2330 train_time:52072ms step_avg:40.24ms
step:1295/2330 train_time:52095ms step_avg:40.23ms
step:1296/2330 train_time:52152ms step_avg:40.24ms
step:1297/2330 train_time:52174ms step_avg:40.23ms
step:1298/2330 train_time:52232ms step_avg:40.24ms
step:1299/2330 train_time:52255ms step_avg:40.23ms
step:1300/2330 train_time:52313ms step_avg:40.24ms
step:1301/2330 train_time:52335ms step_avg:40.23ms
step:1302/2330 train_time:52392ms step_avg:40.24ms
step:1303/2330 train_time:52415ms step_avg:40.23ms
step:1304/2330 train_time:52472ms step_avg:40.24ms
step:1305/2330 train_time:52494ms step_avg:40.23ms
step:1306/2330 train_time:52552ms step_avg:40.24ms
step:1307/2330 train_time:52575ms step_avg:40.23ms
step:1308/2330 train_time:52633ms step_avg:40.24ms
step:1309/2330 train_time:52656ms step_avg:40.23ms
step:1310/2330 train_time:52713ms step_avg:40.24ms
step:1311/2330 train_time:52737ms step_avg:40.23ms
step:1312/2330 train_time:52795ms step_avg:40.24ms
step:1313/2330 train_time:52819ms step_avg:40.23ms
step:1314/2330 train_time:52878ms step_avg:40.24ms
step:1315/2330 train_time:52902ms step_avg:40.23ms
step:1316/2330 train_time:52959ms step_avg:40.24ms
step:1317/2330 train_time:52983ms step_avg:40.23ms
step:1318/2330 train_time:53040ms step_avg:40.24ms
step:1319/2330 train_time:53063ms step_avg:40.23ms
step:1320/2330 train_time:53121ms step_avg:40.24ms
step:1321/2330 train_time:53144ms step_avg:40.23ms
step:1322/2330 train_time:53200ms step_avg:40.24ms
step:1323/2330 train_time:53224ms step_avg:40.23ms
step:1324/2330 train_time:53282ms step_avg:40.24ms
step:1325/2330 train_time:53305ms step_avg:40.23ms
step:1326/2330 train_time:53362ms step_avg:40.24ms
step:1327/2330 train_time:53385ms step_avg:40.23ms
step:1328/2330 train_time:53442ms step_avg:40.24ms
step:1329/2330 train_time:53466ms step_avg:40.23ms
step:1330/2330 train_time:53523ms step_avg:40.24ms
step:1331/2330 train_time:53546ms step_avg:40.23ms
step:1332/2330 train_time:53604ms step_avg:40.24ms
step:1333/2330 train_time:53628ms step_avg:40.23ms
step:1334/2330 train_time:53685ms step_avg:40.24ms
step:1335/2330 train_time:53710ms step_avg:40.23ms
step:1336/2330 train_time:53767ms step_avg:40.25ms
step:1337/2330 train_time:53792ms step_avg:40.23ms
step:1338/2330 train_time:53850ms step_avg:40.25ms
step:1339/2330 train_time:53873ms step_avg:40.23ms
step:1340/2330 train_time:53931ms step_avg:40.25ms
step:1341/2330 train_time:53954ms step_avg:40.23ms
step:1342/2330 train_time:54012ms step_avg:40.25ms
step:1343/2330 train_time:54034ms step_avg:40.23ms
step:1344/2330 train_time:54092ms step_avg:40.25ms
step:1345/2330 train_time:54114ms step_avg:40.23ms
step:1346/2330 train_time:54171ms step_avg:40.25ms
step:1347/2330 train_time:54194ms step_avg:40.23ms
step:1348/2330 train_time:54251ms step_avg:40.25ms
step:1349/2330 train_time:54275ms step_avg:40.23ms
step:1350/2330 train_time:54332ms step_avg:40.25ms
step:1351/2330 train_time:54355ms step_avg:40.23ms
step:1352/2330 train_time:54411ms step_avg:40.25ms
step:1353/2330 train_time:54434ms step_avg:40.23ms
step:1354/2330 train_time:54493ms step_avg:40.25ms
step:1355/2330 train_time:54516ms step_avg:40.23ms
step:1356/2330 train_time:54573ms step_avg:40.25ms
step:1357/2330 train_time:54595ms step_avg:40.23ms
step:1358/2330 train_time:54653ms step_avg:40.25ms
step:1359/2330 train_time:54676ms step_avg:40.23ms
step:1360/2330 train_time:54734ms step_avg:40.25ms
step:1361/2330 train_time:54758ms step_avg:40.23ms
step:1362/2330 train_time:54816ms step_avg:40.25ms
step:1363/2330 train_time:54840ms step_avg:40.24ms
step:1364/2330 train_time:54898ms step_avg:40.25ms
step:1365/2330 train_time:54921ms step_avg:40.24ms
step:1366/2330 train_time:54979ms step_avg:40.25ms
step:1367/2330 train_time:55002ms step_avg:40.24ms
step:1368/2330 train_time:55060ms step_avg:40.25ms
step:1369/2330 train_time:55083ms step_avg:40.24ms
step:1370/2330 train_time:55140ms step_avg:40.25ms
step:1371/2330 train_time:55164ms step_avg:40.24ms
step:1372/2330 train_time:55221ms step_avg:40.25ms
step:1373/2330 train_time:55244ms step_avg:40.24ms
step:1374/2330 train_time:55302ms step_avg:40.25ms
step:1375/2330 train_time:55325ms step_avg:40.24ms
step:1376/2330 train_time:55382ms step_avg:40.25ms
step:1377/2330 train_time:55406ms step_avg:40.24ms
step:1378/2330 train_time:55463ms step_avg:40.25ms
step:1379/2330 train_time:55487ms step_avg:40.24ms
step:1380/2330 train_time:55544ms step_avg:40.25ms
step:1381/2330 train_time:55568ms step_avg:40.24ms
step:1382/2330 train_time:55624ms step_avg:40.25ms
step:1383/2330 train_time:55648ms step_avg:40.24ms
step:1384/2330 train_time:55704ms step_avg:40.25ms
step:1385/2330 train_time:55728ms step_avg:40.24ms
step:1386/2330 train_time:55786ms step_avg:40.25ms
step:1387/2330 train_time:55812ms step_avg:40.24ms
step:1388/2330 train_time:55869ms step_avg:40.25ms
step:1389/2330 train_time:55893ms step_avg:40.24ms
step:1390/2330 train_time:55951ms step_avg:40.25ms
step:1391/2330 train_time:55974ms step_avg:40.24ms
step:1392/2330 train_time:56031ms step_avg:40.25ms
step:1393/2330 train_time:56054ms step_avg:40.24ms
step:1394/2330 train_time:56112ms step_avg:40.25ms
step:1395/2330 train_time:56134ms step_avg:40.24ms
step:1396/2330 train_time:56191ms step_avg:40.25ms
step:1397/2330 train_time:56213ms step_avg:40.24ms
step:1398/2330 train_time:56272ms step_avg:40.25ms
step:1399/2330 train_time:56294ms step_avg:40.24ms
step:1400/2330 train_time:56353ms step_avg:40.25ms
step:1401/2330 train_time:56375ms step_avg:40.24ms
step:1402/2330 train_time:56433ms step_avg:40.25ms
step:1403/2330 train_time:56456ms step_avg:40.24ms
step:1404/2330 train_time:56514ms step_avg:40.25ms
step:1405/2330 train_time:56537ms step_avg:40.24ms
step:1406/2330 train_time:56596ms step_avg:40.25ms
step:1407/2330 train_time:56618ms step_avg:40.24ms
step:1408/2330 train_time:56676ms step_avg:40.25ms
step:1409/2330 train_time:56699ms step_avg:40.24ms
step:1410/2330 train_time:56757ms step_avg:40.25ms
step:1411/2330 train_time:56779ms step_avg:40.24ms
step:1412/2330 train_time:56837ms step_avg:40.25ms
step:1413/2330 train_time:56860ms step_avg:40.24ms
step:1414/2330 train_time:56919ms step_avg:40.25ms
step:1415/2330 train_time:56943ms step_avg:40.24ms
step:1416/2330 train_time:57001ms step_avg:40.25ms
step:1417/2330 train_time:57024ms step_avg:40.24ms
step:1418/2330 train_time:57081ms step_avg:40.25ms
step:1419/2330 train_time:57105ms step_avg:40.24ms
step:1420/2330 train_time:57162ms step_avg:40.25ms
step:1421/2330 train_time:57186ms step_avg:40.24ms
step:1422/2330 train_time:57243ms step_avg:40.26ms
step:1423/2330 train_time:57268ms step_avg:40.24ms
step:1424/2330 train_time:57324ms step_avg:40.26ms
step:1425/2330 train_time:57348ms step_avg:40.24ms
step:1426/2330 train_time:57404ms step_avg:40.26ms
step:1427/2330 train_time:57428ms step_avg:40.24ms
step:1428/2330 train_time:57484ms step_avg:40.25ms
step:1429/2330 train_time:57508ms step_avg:40.24ms
step:1430/2330 train_time:57565ms step_avg:40.25ms
step:1431/2330 train_time:57589ms step_avg:40.24ms
step:1432/2330 train_time:57646ms step_avg:40.26ms
step:1433/2330 train_time:57670ms step_avg:40.24ms
step:1434/2330 train_time:57728ms step_avg:40.26ms
step:1435/2330 train_time:57752ms step_avg:40.25ms
step:1436/2330 train_time:57811ms step_avg:40.26ms
step:1437/2330 train_time:57834ms step_avg:40.25ms
step:1438/2330 train_time:57891ms step_avg:40.26ms
step:1439/2330 train_time:57914ms step_avg:40.25ms
step:1440/2330 train_time:57971ms step_avg:40.26ms
step:1441/2330 train_time:57994ms step_avg:40.25ms
step:1442/2330 train_time:58052ms step_avg:40.26ms
step:1443/2330 train_time:58074ms step_avg:40.25ms
step:1444/2330 train_time:58131ms step_avg:40.26ms
step:1445/2330 train_time:58154ms step_avg:40.24ms
step:1446/2330 train_time:58213ms step_avg:40.26ms
step:1447/2330 train_time:58235ms step_avg:40.25ms
step:1448/2330 train_time:58293ms step_avg:40.26ms
step:1449/2330 train_time:58315ms step_avg:40.25ms
step:1450/2330 train_time:58373ms step_avg:40.26ms
step:1451/2330 train_time:58396ms step_avg:40.25ms
step:1452/2330 train_time:58454ms step_avg:40.26ms
step:1453/2330 train_time:58477ms step_avg:40.25ms
step:1454/2330 train_time:58535ms step_avg:40.26ms
step:1455/2330 train_time:58558ms step_avg:40.25ms
step:1456/2330 train_time:58615ms step_avg:40.26ms
step:1457/2330 train_time:58639ms step_avg:40.25ms
step:1458/2330 train_time:58697ms step_avg:40.26ms
step:1459/2330 train_time:58720ms step_avg:40.25ms
step:1460/2330 train_time:58778ms step_avg:40.26ms
step:1461/2330 train_time:58801ms step_avg:40.25ms
step:1462/2330 train_time:58859ms step_avg:40.26ms
step:1463/2330 train_time:58882ms step_avg:40.25ms
step:1464/2330 train_time:58940ms step_avg:40.26ms
step:1465/2330 train_time:58963ms step_avg:40.25ms
step:1466/2330 train_time:59021ms step_avg:40.26ms
step:1467/2330 train_time:59045ms step_avg:40.25ms
step:1468/2330 train_time:59102ms step_avg:40.26ms
step:1469/2330 train_time:59126ms step_avg:40.25ms
step:1470/2330 train_time:59184ms step_avg:40.26ms
step:1471/2330 train_time:59207ms step_avg:40.25ms
step:1472/2330 train_time:59263ms step_avg:40.26ms
step:1473/2330 train_time:59287ms step_avg:40.25ms
step:1474/2330 train_time:59344ms step_avg:40.26ms
step:1475/2330 train_time:59368ms step_avg:40.25ms
step:1476/2330 train_time:59424ms step_avg:40.26ms
step:1477/2330 train_time:59449ms step_avg:40.25ms
step:1478/2330 train_time:59505ms step_avg:40.26ms
step:1479/2330 train_time:59529ms step_avg:40.25ms
step:1480/2330 train_time:59586ms step_avg:40.26ms
step:1481/2330 train_time:59609ms step_avg:40.25ms
step:1482/2330 train_time:59666ms step_avg:40.26ms
step:1483/2330 train_time:59690ms step_avg:40.25ms
step:1484/2330 train_time:59749ms step_avg:40.26ms
step:1485/2330 train_time:59773ms step_avg:40.25ms
step:1486/2330 train_time:59832ms step_avg:40.26ms
step:1487/2330 train_time:59855ms step_avg:40.25ms
step:1488/2330 train_time:59913ms step_avg:40.26ms
step:1489/2330 train_time:59936ms step_avg:40.25ms
step:1490/2330 train_time:59994ms step_avg:40.26ms
step:1491/2330 train_time:60016ms step_avg:40.25ms
step:1492/2330 train_time:60074ms step_avg:40.26ms
step:1493/2330 train_time:60097ms step_avg:40.25ms
step:1494/2330 train_time:60155ms step_avg:40.26ms
step:1495/2330 train_time:60178ms step_avg:40.25ms
step:1496/2330 train_time:60236ms step_avg:40.26ms
step:1497/2330 train_time:60258ms step_avg:40.25ms
step:1498/2330 train_time:60316ms step_avg:40.26ms
step:1499/2330 train_time:60339ms step_avg:40.25ms
step:1500/2330 train_time:60397ms step_avg:40.26ms
step:1500/2330 val_loss:5.2689 train_time:60496ms step_avg:40.33ms
step:1501/2330 train_time:60509ms step_avg:40.31ms
step:1502/2330 train_time:60521ms step_avg:40.29ms
step:1503/2330 train_time:60532ms step_avg:40.27ms
step:1504/2330 train_time:60561ms step_avg:40.27ms
step:1505/2330 train_time:60582ms step_avg:40.25ms
step:1506/2330 train_time:60640ms step_avg:40.27ms
step:1507/2330 train_time:60662ms step_avg:40.25ms
step:1508/2330 train_time:60719ms step_avg:40.26ms
step:1509/2330 train_time:60742ms step_avg:40.25ms
step:1510/2330 train_time:60800ms step_avg:40.26ms
step:1511/2330 train_time:60826ms step_avg:40.26ms
step:1512/2330 train_time:60889ms step_avg:40.27ms
step:1513/2330 train_time:60912ms step_avg:40.26ms
step:1514/2330 train_time:60969ms step_avg:40.27ms
step:1515/2330 train_time:60992ms step_avg:40.26ms
step:1516/2330 train_time:61049ms step_avg:40.27ms
step:1517/2330 train_time:61072ms step_avg:40.26ms
step:1518/2330 train_time:61129ms step_avg:40.27ms
step:1519/2330 train_time:61153ms step_avg:40.26ms
step:1520/2330 train_time:61209ms step_avg:40.27ms
step:1521/2330 train_time:61232ms step_avg:40.26ms
step:1522/2330 train_time:61289ms step_avg:40.27ms
step:1523/2330 train_time:61313ms step_avg:40.26ms
step:1524/2330 train_time:61369ms step_avg:40.27ms
step:1525/2330 train_time:61392ms step_avg:40.26ms
step:1526/2330 train_time:61450ms step_avg:40.27ms
step:1527/2330 train_time:61475ms step_avg:40.26ms
step:1528/2330 train_time:61533ms step_avg:40.27ms
step:1529/2330 train_time:61558ms step_avg:40.26ms
step:1530/2330 train_time:61615ms step_avg:40.27ms
step:1531/2330 train_time:61637ms step_avg:40.26ms
step:1532/2330 train_time:61694ms step_avg:40.27ms
step:1533/2330 train_time:61717ms step_avg:40.26ms
step:1534/2330 train_time:61775ms step_avg:40.27ms
step:1535/2330 train_time:61800ms step_avg:40.26ms
step:1536/2330 train_time:61859ms step_avg:40.27ms
step:1537/2330 train_time:61882ms step_avg:40.26ms
step:1538/2330 train_time:61940ms step_avg:40.27ms
step:1539/2330 train_time:61963ms step_avg:40.26ms
step:1540/2330 train_time:62021ms step_avg:40.27ms
step:1541/2330 train_time:62044ms step_avg:40.26ms
step:1542/2330 train_time:62102ms step_avg:40.27ms
step:1543/2330 train_time:62126ms step_avg:40.26ms
step:1544/2330 train_time:62184ms step_avg:40.27ms
step:1545/2330 train_time:62207ms step_avg:40.26ms
step:1546/2330 train_time:62264ms step_avg:40.27ms
step:1547/2330 train_time:62287ms step_avg:40.26ms
step:1548/2330 train_time:62344ms step_avg:40.27ms
step:1549/2330 train_time:62368ms step_avg:40.26ms
step:1550/2330 train_time:62425ms step_avg:40.27ms
step:1551/2330 train_time:62449ms step_avg:40.26ms
step:1552/2330 train_time:62506ms step_avg:40.27ms
step:1553/2330 train_time:62531ms step_avg:40.26ms
step:1554/2330 train_time:62588ms step_avg:40.28ms
step:1555/2330 train_time:62612ms step_avg:40.26ms
step:1556/2330 train_time:62669ms step_avg:40.28ms
step:1557/2330 train_time:62693ms step_avg:40.27ms
step:1558/2330 train_time:62750ms step_avg:40.28ms
step:1559/2330 train_time:62774ms step_avg:40.27ms
step:1560/2330 train_time:62831ms step_avg:40.28ms
step:1561/2330 train_time:62855ms step_avg:40.27ms
step:1562/2330 train_time:62912ms step_avg:40.28ms
step:1563/2330 train_time:62935ms step_avg:40.27ms
step:1564/2330 train_time:62993ms step_avg:40.28ms
step:1565/2330 train_time:63017ms step_avg:40.27ms
step:1566/2330 train_time:63075ms step_avg:40.28ms
step:1567/2330 train_time:63097ms step_avg:40.27ms
step:1568/2330 train_time:63155ms step_avg:40.28ms
step:1569/2330 train_time:63177ms step_avg:40.27ms
step:1570/2330 train_time:63236ms step_avg:40.28ms
step:1571/2330 train_time:63259ms step_avg:40.27ms
step:1572/2330 train_time:63316ms step_avg:40.28ms
step:1573/2330 train_time:63340ms step_avg:40.27ms
step:1574/2330 train_time:63397ms step_avg:40.28ms
step:1575/2330 train_time:63421ms step_avg:40.27ms
step:1576/2330 train_time:63478ms step_avg:40.28ms
step:1577/2330 train_time:63500ms step_avg:40.27ms
step:1578/2330 train_time:63559ms step_avg:40.28ms
step:1579/2330 train_time:63582ms step_avg:40.27ms
step:1580/2330 train_time:63641ms step_avg:40.28ms
step:1581/2330 train_time:63664ms step_avg:40.27ms
step:1582/2330 train_time:63722ms step_avg:40.28ms
step:1583/2330 train_time:63745ms step_avg:40.27ms
step:1584/2330 train_time:63803ms step_avg:40.28ms
step:1585/2330 train_time:63828ms step_avg:40.27ms
step:1586/2330 train_time:63886ms step_avg:40.28ms
step:1587/2330 train_time:63910ms step_avg:40.27ms
step:1588/2330 train_time:63966ms step_avg:40.28ms
step:1589/2330 train_time:63990ms step_avg:40.27ms
step:1590/2330 train_time:64048ms step_avg:40.28ms
step:1591/2330 train_time:64071ms step_avg:40.27ms
step:1592/2330 train_time:64128ms step_avg:40.28ms
step:1593/2330 train_time:64152ms step_avg:40.27ms
step:1594/2330 train_time:64208ms step_avg:40.28ms
step:1595/2330 train_time:64232ms step_avg:40.27ms
step:1596/2330 train_time:64289ms step_avg:40.28ms
step:1597/2330 train_time:64313ms step_avg:40.27ms
step:1598/2330 train_time:64370ms step_avg:40.28ms
step:1599/2330 train_time:64393ms step_avg:40.27ms
step:1600/2330 train_time:64450ms step_avg:40.28ms
step:1601/2330 train_time:64473ms step_avg:40.27ms
step:1602/2330 train_time:64530ms step_avg:40.28ms
step:1603/2330 train_time:64555ms step_avg:40.27ms
step:1604/2330 train_time:64612ms step_avg:40.28ms
step:1605/2330 train_time:64636ms step_avg:40.27ms
step:1606/2330 train_time:64694ms step_avg:40.28ms
step:1607/2330 train_time:64718ms step_avg:40.27ms
step:1608/2330 train_time:64775ms step_avg:40.28ms
step:1609/2330 train_time:64798ms step_avg:40.27ms
step:1610/2330 train_time:64855ms step_avg:40.28ms
step:1611/2330 train_time:64878ms step_avg:40.27ms
step:1612/2330 train_time:64936ms step_avg:40.28ms
step:1613/2330 train_time:64958ms step_avg:40.27ms
step:1614/2330 train_time:65016ms step_avg:40.28ms
step:1615/2330 train_time:65039ms step_avg:40.27ms
step:1616/2330 train_time:65097ms step_avg:40.28ms
step:1617/2330 train_time:65119ms step_avg:40.27ms
step:1618/2330 train_time:65177ms step_avg:40.28ms
step:1619/2330 train_time:65200ms step_avg:40.27ms
step:1620/2330 train_time:65257ms step_avg:40.28ms
step:1621/2330 train_time:65280ms step_avg:40.27ms
step:1622/2330 train_time:65338ms step_avg:40.28ms
step:1623/2330 train_time:65360ms step_avg:40.27ms
step:1624/2330 train_time:65418ms step_avg:40.28ms
step:1625/2330 train_time:65440ms step_avg:40.27ms
step:1626/2330 train_time:65498ms step_avg:40.28ms
step:1627/2330 train_time:65521ms step_avg:40.27ms
step:1628/2330 train_time:65579ms step_avg:40.28ms
step:1629/2330 train_time:65602ms step_avg:40.27ms
step:1630/2330 train_time:65661ms step_avg:40.28ms
step:1631/2330 train_time:65684ms step_avg:40.27ms
step:1632/2330 train_time:65742ms step_avg:40.28ms
step:1633/2330 train_time:65766ms step_avg:40.27ms
step:1634/2330 train_time:65824ms step_avg:40.28ms
step:1635/2330 train_time:65847ms step_avg:40.27ms
step:1636/2330 train_time:65904ms step_avg:40.28ms
step:1637/2330 train_time:65928ms step_avg:40.27ms
step:1638/2330 train_time:65985ms step_avg:40.28ms
step:1639/2330 train_time:66009ms step_avg:40.27ms
step:1640/2330 train_time:66066ms step_avg:40.28ms
step:1641/2330 train_time:66090ms step_avg:40.27ms
step:1642/2330 train_time:66147ms step_avg:40.28ms
step:1643/2330 train_time:66171ms step_avg:40.27ms
step:1644/2330 train_time:66228ms step_avg:40.28ms
step:1645/2330 train_time:66252ms step_avg:40.28ms
step:1646/2330 train_time:66309ms step_avg:40.28ms
step:1647/2330 train_time:66333ms step_avg:40.27ms
step:1648/2330 train_time:66390ms step_avg:40.29ms
step:1649/2330 train_time:66415ms step_avg:40.28ms
step:1650/2330 train_time:66472ms step_avg:40.29ms
step:1651/2330 train_time:66495ms step_avg:40.28ms
step:1652/2330 train_time:66553ms step_avg:40.29ms
step:1653/2330 train_time:66577ms step_avg:40.28ms
step:1654/2330 train_time:66635ms step_avg:40.29ms
step:1655/2330 train_time:66657ms step_avg:40.28ms
step:1656/2330 train_time:66714ms step_avg:40.29ms
step:1657/2330 train_time:66738ms step_avg:40.28ms
step:1658/2330 train_time:66795ms step_avg:40.29ms
step:1659/2330 train_time:66817ms step_avg:40.28ms
step:1660/2330 train_time:66875ms step_avg:40.29ms
step:1661/2330 train_time:66899ms step_avg:40.28ms
step:1662/2330 train_time:66956ms step_avg:40.29ms
step:1663/2330 train_time:66979ms step_avg:40.28ms
step:1664/2330 train_time:67039ms step_avg:40.29ms
step:1665/2330 train_time:67061ms step_avg:40.28ms
step:1666/2330 train_time:67119ms step_avg:40.29ms
step:1667/2330 train_time:67142ms step_avg:40.28ms
step:1668/2330 train_time:67200ms step_avg:40.29ms
step:1669/2330 train_time:67224ms step_avg:40.28ms
step:1670/2330 train_time:67282ms step_avg:40.29ms
step:1671/2330 train_time:67306ms step_avg:40.28ms
step:1672/2330 train_time:67363ms step_avg:40.29ms
step:1673/2330 train_time:67387ms step_avg:40.28ms
step:1674/2330 train_time:67444ms step_avg:40.29ms
step:1675/2330 train_time:67468ms step_avg:40.28ms
step:1676/2330 train_time:67526ms step_avg:40.29ms
step:1677/2330 train_time:67550ms step_avg:40.28ms
step:1678/2330 train_time:67607ms step_avg:40.29ms
step:1679/2330 train_time:67631ms step_avg:40.28ms
step:1680/2330 train_time:67689ms step_avg:40.29ms
step:1681/2330 train_time:67713ms step_avg:40.28ms
step:1682/2330 train_time:67769ms step_avg:40.29ms
step:1683/2330 train_time:67793ms step_avg:40.28ms
step:1684/2330 train_time:67849ms step_avg:40.29ms
step:1685/2330 train_time:67873ms step_avg:40.28ms
step:1686/2330 train_time:67931ms step_avg:40.29ms
step:1687/2330 train_time:67955ms step_avg:40.28ms
step:1688/2330 train_time:68012ms step_avg:40.29ms
step:1689/2330 train_time:68035ms step_avg:40.28ms
step:1690/2330 train_time:68092ms step_avg:40.29ms
step:1691/2330 train_time:68116ms step_avg:40.28ms
step:1692/2330 train_time:68173ms step_avg:40.29ms
step:1693/2330 train_time:68196ms step_avg:40.28ms
step:1694/2330 train_time:68254ms step_avg:40.29ms
step:1695/2330 train_time:68277ms step_avg:40.28ms
step:1696/2330 train_time:68334ms step_avg:40.29ms
step:1697/2330 train_time:68358ms step_avg:40.28ms
step:1698/2330 train_time:68416ms step_avg:40.29ms
step:1699/2330 train_time:68439ms step_avg:40.28ms
step:1700/2330 train_time:68496ms step_avg:40.29ms
step:1701/2330 train_time:68519ms step_avg:40.28ms
step:1702/2330 train_time:68576ms step_avg:40.29ms
step:1703/2330 train_time:68599ms step_avg:40.28ms
step:1704/2330 train_time:68657ms step_avg:40.29ms
step:1705/2330 train_time:68679ms step_avg:40.28ms
step:1706/2330 train_time:68738ms step_avg:40.29ms
step:1707/2330 train_time:68760ms step_avg:40.28ms
step:1708/2330 train_time:68818ms step_avg:40.29ms
step:1709/2330 train_time:68840ms step_avg:40.28ms
step:1710/2330 train_time:68898ms step_avg:40.29ms
step:1711/2330 train_time:68921ms step_avg:40.28ms
step:1712/2330 train_time:68980ms step_avg:40.29ms
step:1713/2330 train_time:69003ms step_avg:40.28ms
step:1714/2330 train_time:69061ms step_avg:40.29ms
step:1715/2330 train_time:69085ms step_avg:40.28ms
step:1716/2330 train_time:69143ms step_avg:40.29ms
step:1717/2330 train_time:69166ms step_avg:40.28ms
step:1718/2330 train_time:69224ms step_avg:40.29ms
step:1719/2330 train_time:69248ms step_avg:40.28ms
step:1720/2330 train_time:69305ms step_avg:40.29ms
step:1721/2330 train_time:69329ms step_avg:40.28ms
step:1722/2330 train_time:69386ms step_avg:40.29ms
step:1723/2330 train_time:69410ms step_avg:40.28ms
step:1724/2330 train_time:69467ms step_avg:40.29ms
step:1725/2330 train_time:69491ms step_avg:40.28ms
step:1726/2330 train_time:69549ms step_avg:40.29ms
step:1727/2330 train_time:69573ms step_avg:40.29ms
step:1728/2330 train_time:69630ms step_avg:40.30ms
step:1729/2330 train_time:69653ms step_avg:40.29ms
step:1730/2330 train_time:69710ms step_avg:40.29ms
step:1731/2330 train_time:69734ms step_avg:40.29ms
step:1732/2330 train_time:69791ms step_avg:40.29ms
step:1733/2330 train_time:69814ms step_avg:40.29ms
step:1734/2330 train_time:69871ms step_avg:40.29ms
step:1735/2330 train_time:69895ms step_avg:40.29ms
step:1736/2330 train_time:69953ms step_avg:40.30ms
step:1737/2330 train_time:69977ms step_avg:40.29ms
step:1738/2330 train_time:70036ms step_avg:40.30ms
step:1739/2330 train_time:70059ms step_avg:40.29ms
step:1740/2330 train_time:70117ms step_avg:40.30ms
step:1741/2330 train_time:70140ms step_avg:40.29ms
step:1742/2330 train_time:70197ms step_avg:40.30ms
step:1743/2330 train_time:70220ms step_avg:40.29ms
step:1744/2330 train_time:70278ms step_avg:40.30ms
step:1745/2330 train_time:70301ms step_avg:40.29ms
step:1746/2330 train_time:70359ms step_avg:40.30ms
step:1747/2330 train_time:70381ms step_avg:40.29ms
step:1748/2330 train_time:70439ms step_avg:40.30ms
step:1749/2330 train_time:70463ms step_avg:40.29ms
step:1750/2330 train_time:70522ms step_avg:40.30ms
step:1750/2330 val_loss:5.2194 train_time:70621ms step_avg:40.35ms
step:1751/2330 train_time:70634ms step_avg:40.34ms
step:1752/2330 train_time:70646ms step_avg:40.32ms
step:1753/2330 train_time:70656ms step_avg:40.31ms
step:1754/2330 train_time:70683ms step_avg:40.30ms
step:1755/2330 train_time:70705ms step_avg:40.29ms
step:1756/2330 train_time:70761ms step_avg:40.30ms
step:1757/2330 train_time:70784ms step_avg:40.29ms
step:1758/2330 train_time:70839ms step_avg:40.30ms
step:1759/2330 train_time:70862ms step_avg:40.29ms
step:1760/2330 train_time:70920ms step_avg:40.30ms
step:1761/2330 train_time:70947ms step_avg:40.29ms
step:1762/2330 train_time:71009ms step_avg:40.30ms
step:1763/2330 train_time:71034ms step_avg:40.29ms
step:1764/2330 train_time:71092ms step_avg:40.30ms
step:1765/2330 train_time:71115ms step_avg:40.29ms
step:1766/2330 train_time:71172ms step_avg:40.30ms
step:1767/2330 train_time:71194ms step_avg:40.29ms
step:1768/2330 train_time:71250ms step_avg:40.30ms
step:1769/2330 train_time:71273ms step_avg:40.29ms
step:1770/2330 train_time:71330ms step_avg:40.30ms
step:1771/2330 train_time:71352ms step_avg:40.29ms
step:1772/2330 train_time:71408ms step_avg:40.30ms
step:1773/2330 train_time:71430ms step_avg:40.29ms
step:1774/2330 train_time:71487ms step_avg:40.30ms
step:1775/2330 train_time:71510ms step_avg:40.29ms
step:1776/2330 train_time:71571ms step_avg:40.30ms
step:1777/2330 train_time:71596ms step_avg:40.29ms
step:1778/2330 train_time:71653ms step_avg:40.30ms
step:1779/2330 train_time:71676ms step_avg:40.29ms
step:1780/2330 train_time:71733ms step_avg:40.30ms
step:1781/2330 train_time:71756ms step_avg:40.29ms
step:1782/2330 train_time:71813ms step_avg:40.30ms
step:1783/2330 train_time:71836ms step_avg:40.29ms
step:1784/2330 train_time:71895ms step_avg:40.30ms
step:1785/2330 train_time:71919ms step_avg:40.29ms
step:1786/2330 train_time:71979ms step_avg:40.30ms
step:1787/2330 train_time:72003ms step_avg:40.29ms
step:1788/2330 train_time:72061ms step_avg:40.30ms
step:1789/2330 train_time:72084ms step_avg:40.29ms
step:1790/2330 train_time:72141ms step_avg:40.30ms
step:1791/2330 train_time:72165ms step_avg:40.29ms
step:1792/2330 train_time:72221ms step_avg:40.30ms
step:1793/2330 train_time:72245ms step_avg:40.29ms
step:1794/2330 train_time:72302ms step_avg:40.30ms
step:1795/2330 train_time:72325ms step_avg:40.29ms
step:1796/2330 train_time:72381ms step_avg:40.30ms
step:1797/2330 train_time:72404ms step_avg:40.29ms
step:1798/2330 train_time:72461ms step_avg:40.30ms
step:1799/2330 train_time:72485ms step_avg:40.29ms
step:1800/2330 train_time:72542ms step_avg:40.30ms
step:1801/2330 train_time:72566ms step_avg:40.29ms
step:1802/2330 train_time:72624ms step_avg:40.30ms
step:1803/2330 train_time:72647ms step_avg:40.29ms
step:1804/2330 train_time:72704ms step_avg:40.30ms
step:1805/2330 train_time:72728ms step_avg:40.29ms
step:1806/2330 train_time:72784ms step_avg:40.30ms
step:1807/2330 train_time:72807ms step_avg:40.29ms
step:1808/2330 train_time:72865ms step_avg:40.30ms
step:1809/2330 train_time:72890ms step_avg:40.29ms
step:1810/2330 train_time:72948ms step_avg:40.30ms
step:1811/2330 train_time:72972ms step_avg:40.29ms
step:1812/2330 train_time:73029ms step_avg:40.30ms
step:1813/2330 train_time:73053ms step_avg:40.29ms
step:1814/2330 train_time:73112ms step_avg:40.30ms
step:1815/2330 train_time:73135ms step_avg:40.29ms
step:1816/2330 train_time:73193ms step_avg:40.30ms
step:1817/2330 train_time:73216ms step_avg:40.29ms
step:1818/2330 train_time:73273ms step_avg:40.30ms
step:1819/2330 train_time:73295ms step_avg:40.29ms
step:1820/2330 train_time:73352ms step_avg:40.30ms
step:1821/2330 train_time:73375ms step_avg:40.29ms
step:1822/2330 train_time:73433ms step_avg:40.30ms
step:1823/2330 train_time:73456ms step_avg:40.29ms
step:1824/2330 train_time:73515ms step_avg:40.30ms
step:1825/2330 train_time:73537ms step_avg:40.29ms
step:1826/2330 train_time:73596ms step_avg:40.30ms
step:1827/2330 train_time:73619ms step_avg:40.30ms
step:1828/2330 train_time:73677ms step_avg:40.30ms
step:1829/2330 train_time:73700ms step_avg:40.30ms
step:1830/2330 train_time:73757ms step_avg:40.30ms
step:1831/2330 train_time:73780ms step_avg:40.30ms
step:1832/2330 train_time:73839ms step_avg:40.30ms
step:1833/2330 train_time:73862ms step_avg:40.30ms
step:1834/2330 train_time:73920ms step_avg:40.31ms
step:1835/2330 train_time:73944ms step_avg:40.30ms
step:1836/2330 train_time:74002ms step_avg:40.31ms
step:1837/2330 train_time:74026ms step_avg:40.30ms
step:1838/2330 train_time:74084ms step_avg:40.31ms
step:1839/2330 train_time:74108ms step_avg:40.30ms
step:1840/2330 train_time:74165ms step_avg:40.31ms
step:1841/2330 train_time:74189ms step_avg:40.30ms
step:1842/2330 train_time:74246ms step_avg:40.31ms
step:1843/2330 train_time:74269ms step_avg:40.30ms
step:1844/2330 train_time:74326ms step_avg:40.31ms
step:1845/2330 train_time:74350ms step_avg:40.30ms
step:1846/2330 train_time:74407ms step_avg:40.31ms
step:1847/2330 train_time:74430ms step_avg:40.30ms
step:1848/2330 train_time:74488ms step_avg:40.31ms
step:1849/2330 train_time:74511ms step_avg:40.30ms
step:1850/2330 train_time:74568ms step_avg:40.31ms
step:1851/2330 train_time:74591ms step_avg:40.30ms
step:1852/2330 train_time:74648ms step_avg:40.31ms
step:1853/2330 train_time:74671ms step_avg:40.30ms
step:1854/2330 train_time:74729ms step_avg:40.31ms
step:1855/2330 train_time:74751ms step_avg:40.30ms
step:1856/2330 train_time:74810ms step_avg:40.31ms
step:1857/2330 train_time:74832ms step_avg:40.30ms
step:1858/2330 train_time:74891ms step_avg:40.31ms
step:1859/2330 train_time:74914ms step_avg:40.30ms
step:1860/2330 train_time:74972ms step_avg:40.31ms
step:1861/2330 train_time:74994ms step_avg:40.30ms
step:1862/2330 train_time:75053ms step_avg:40.31ms
step:1863/2330 train_time:75075ms step_avg:40.30ms
step:1864/2330 train_time:75134ms step_avg:40.31ms
step:1865/2330 train_time:75156ms step_avg:40.30ms
step:1866/2330 train_time:75214ms step_avg:40.31ms
step:1867/2330 train_time:75237ms step_avg:40.30ms
step:1868/2330 train_time:75294ms step_avg:40.31ms
step:1869/2330 train_time:75316ms step_avg:40.30ms
step:1870/2330 train_time:75374ms step_avg:40.31ms
step:1871/2330 train_time:75397ms step_avg:40.30ms
step:1872/2330 train_time:75454ms step_avg:40.31ms
step:1873/2330 train_time:75477ms step_avg:40.30ms
step:1874/2330 train_time:75535ms step_avg:40.31ms
step:1875/2330 train_time:75558ms step_avg:40.30ms
step:1876/2330 train_time:75616ms step_avg:40.31ms
step:1877/2330 train_time:75639ms step_avg:40.30ms
step:1878/2330 train_time:75697ms step_avg:40.31ms
step:1879/2330 train_time:75720ms step_avg:40.30ms
step:1880/2330 train_time:75778ms step_avg:40.31ms
step:1881/2330 train_time:75801ms step_avg:40.30ms
step:1882/2330 train_time:75859ms step_avg:40.31ms
step:1883/2330 train_time:75883ms step_avg:40.30ms
step:1884/2330 train_time:75941ms step_avg:40.31ms
step:1885/2330 train_time:75966ms step_avg:40.30ms
step:1886/2330 train_time:76023ms step_avg:40.31ms
step:1887/2330 train_time:76046ms step_avg:40.30ms
step:1888/2330 train_time:76103ms step_avg:40.31ms
step:1889/2330 train_time:76126ms step_avg:40.30ms
step:1890/2330 train_time:76184ms step_avg:40.31ms
step:1891/2330 train_time:76208ms step_avg:40.30ms
step:1892/2330 train_time:76265ms step_avg:40.31ms
step:1893/2330 train_time:76288ms step_avg:40.30ms
step:1894/2330 train_time:76345ms step_avg:40.31ms
step:1895/2330 train_time:76369ms step_avg:40.30ms
step:1896/2330 train_time:76426ms step_avg:40.31ms
step:1897/2330 train_time:76450ms step_avg:40.30ms
step:1898/2330 train_time:76508ms step_avg:40.31ms
step:1899/2330 train_time:76531ms step_avg:40.30ms
step:1900/2330 train_time:76588ms step_avg:40.31ms
step:1901/2330 train_time:76611ms step_avg:40.30ms
step:1902/2330 train_time:76670ms step_avg:40.31ms
step:1903/2330 train_time:76692ms step_avg:40.30ms
step:1904/2330 train_time:76751ms step_avg:40.31ms
step:1905/2330 train_time:76773ms step_avg:40.30ms
step:1906/2330 train_time:76831ms step_avg:40.31ms
step:1907/2330 train_time:76854ms step_avg:40.30ms
step:1908/2330 train_time:76912ms step_avg:40.31ms
step:1909/2330 train_time:76935ms step_avg:40.30ms
step:1910/2330 train_time:76994ms step_avg:40.31ms
step:1911/2330 train_time:77017ms step_avg:40.30ms
step:1912/2330 train_time:77076ms step_avg:40.31ms
step:1913/2330 train_time:77099ms step_avg:40.30ms
step:1914/2330 train_time:77157ms step_avg:40.31ms
step:1915/2330 train_time:77180ms step_avg:40.30ms
step:1916/2330 train_time:77238ms step_avg:40.31ms
step:1917/2330 train_time:77262ms step_avg:40.30ms
step:1918/2330 train_time:77320ms step_avg:40.31ms
step:1919/2330 train_time:77343ms step_avg:40.30ms
step:1920/2330 train_time:77400ms step_avg:40.31ms
step:1921/2330 train_time:77425ms step_avg:40.30ms
step:1922/2330 train_time:77482ms step_avg:40.31ms
step:1923/2330 train_time:77505ms step_avg:40.30ms
step:1924/2330 train_time:77563ms step_avg:40.31ms
step:1925/2330 train_time:77587ms step_avg:40.30ms
step:1926/2330 train_time:77644ms step_avg:40.31ms
step:1927/2330 train_time:77667ms step_avg:40.30ms
step:1928/2330 train_time:77724ms step_avg:40.31ms
step:1929/2330 train_time:77748ms step_avg:40.30ms
step:1930/2330 train_time:77804ms step_avg:40.31ms
step:1931/2330 train_time:77828ms step_avg:40.30ms
step:1932/2330 train_time:77886ms step_avg:40.31ms
step:1933/2330 train_time:77909ms step_avg:40.30ms
step:1934/2330 train_time:77967ms step_avg:40.31ms
step:1935/2330 train_time:77990ms step_avg:40.30ms
step:1936/2330 train_time:78048ms step_avg:40.31ms
step:1937/2330 train_time:78071ms step_avg:40.31ms
step:1938/2330 train_time:78128ms step_avg:40.31ms
step:1939/2330 train_time:78151ms step_avg:40.30ms
step:1940/2330 train_time:78209ms step_avg:40.31ms
step:1941/2330 train_time:78231ms step_avg:40.30ms
step:1942/2330 train_time:78289ms step_avg:40.31ms
step:1943/2330 train_time:78312ms step_avg:40.30ms
step:1944/2330 train_time:78369ms step_avg:40.31ms
step:1945/2330 train_time:78392ms step_avg:40.30ms
step:1946/2330 train_time:78450ms step_avg:40.31ms
step:1947/2330 train_time:78472ms step_avg:40.30ms
step:1948/2330 train_time:78530ms step_avg:40.31ms
step:1949/2330 train_time:78553ms step_avg:40.30ms
step:1950/2330 train_time:78610ms step_avg:40.31ms
step:1951/2330 train_time:78633ms step_avg:40.30ms
step:1952/2330 train_time:78692ms step_avg:40.31ms
step:1953/2330 train_time:78715ms step_avg:40.30ms
step:1954/2330 train_time:78773ms step_avg:40.31ms
step:1955/2330 train_time:78795ms step_avg:40.30ms
step:1956/2330 train_time:78852ms step_avg:40.31ms
step:1957/2330 train_time:78875ms step_avg:40.30ms
step:1958/2330 train_time:78932ms step_avg:40.31ms
step:1959/2330 train_time:78955ms step_avg:40.30ms
step:1960/2330 train_time:79013ms step_avg:40.31ms
step:1961/2330 train_time:79035ms step_avg:40.30ms
step:1962/2330 train_time:79093ms step_avg:40.31ms
step:1963/2330 train_time:79115ms step_avg:40.30ms
step:1964/2330 train_time:79173ms step_avg:40.31ms
step:1965/2330 train_time:79196ms step_avg:40.30ms
step:1966/2330 train_time:79254ms step_avg:40.31ms
step:1967/2330 train_time:79278ms step_avg:40.30ms
step:1968/2330 train_time:79336ms step_avg:40.31ms
step:1969/2330 train_time:79359ms step_avg:40.30ms
step:1970/2330 train_time:79416ms step_avg:40.31ms
step:1971/2330 train_time:79440ms step_avg:40.30ms
step:1972/2330 train_time:79497ms step_avg:40.31ms
step:1973/2330 train_time:79521ms step_avg:40.30ms
step:1974/2330 train_time:79579ms step_avg:40.31ms
step:1975/2330 train_time:79602ms step_avg:40.30ms
step:1976/2330 train_time:79660ms step_avg:40.31ms
step:1977/2330 train_time:79683ms step_avg:40.30ms
step:1978/2330 train_time:79741ms step_avg:40.31ms
step:1979/2330 train_time:79764ms step_avg:40.31ms
step:1980/2330 train_time:79821ms step_avg:40.31ms
step:1981/2330 train_time:79845ms step_avg:40.31ms
step:1982/2330 train_time:79902ms step_avg:40.31ms
step:1983/2330 train_time:79926ms step_avg:40.31ms
step:1984/2330 train_time:79983ms step_avg:40.31ms
step:1985/2330 train_time:80007ms step_avg:40.31ms
step:1986/2330 train_time:80063ms step_avg:40.31ms
step:1987/2330 train_time:80087ms step_avg:40.31ms
step:1988/2330 train_time:80143ms step_avg:40.31ms
step:1989/2330 train_time:80167ms step_avg:40.31ms
step:1990/2330 train_time:80226ms step_avg:40.31ms
step:1991/2330 train_time:80251ms step_avg:40.31ms
step:1992/2330 train_time:80308ms step_avg:40.32ms
step:1993/2330 train_time:80332ms step_avg:40.31ms
step:1994/2330 train_time:80390ms step_avg:40.32ms
step:1995/2330 train_time:80413ms step_avg:40.31ms
step:1996/2330 train_time:80471ms step_avg:40.32ms
step:1997/2330 train_time:80494ms step_avg:40.31ms
step:1998/2330 train_time:80551ms step_avg:40.32ms
step:1999/2330 train_time:80575ms step_avg:40.31ms
step:2000/2330 train_time:80633ms step_avg:40.32ms
step:2000/2330 val_loss:5.1838 train_time:80731ms step_avg:40.37ms
step:2001/2330 train_time:80743ms step_avg:40.35ms
step:2002/2330 train_time:80755ms step_avg:40.34ms
step:2003/2330 train_time:80766ms step_avg:40.32ms
step:2004/2330 train_time:80793ms step_avg:40.32ms
step:2005/2330 train_time:80816ms step_avg:40.31ms
step:2006/2330 train_time:80873ms step_avg:40.32ms
step:2007/2330 train_time:80895ms step_avg:40.31ms
step:2008/2330 train_time:80952ms step_avg:40.31ms
step:2009/2330 train_time:80975ms step_avg:40.31ms
step:2010/2330 train_time:81034ms step_avg:40.32ms
step:2011/2330 train_time:81062ms step_avg:40.31ms
step:2012/2330 train_time:81123ms step_avg:40.32ms
step:2013/2330 train_time:81148ms step_avg:40.31ms
step:2014/2330 train_time:81207ms step_avg:40.32ms
step:2015/2330 train_time:81229ms step_avg:40.31ms
step:2016/2330 train_time:81287ms step_avg:40.32ms
step:2017/2330 train_time:81309ms step_avg:40.31ms
step:2018/2330 train_time:81366ms step_avg:40.32ms
step:2019/2330 train_time:81388ms step_avg:40.31ms
step:2020/2330 train_time:81445ms step_avg:40.32ms
step:2021/2330 train_time:81467ms step_avg:40.31ms
step:2022/2330 train_time:81524ms step_avg:40.32ms
step:2023/2330 train_time:81547ms step_avg:40.31ms
step:2024/2330 train_time:81602ms step_avg:40.32ms
step:2025/2330 train_time:81625ms step_avg:40.31ms
step:2026/2330 train_time:81682ms step_avg:40.32ms
step:2027/2330 train_time:81706ms step_avg:40.31ms
step:2028/2330 train_time:81763ms step_avg:40.32ms
step:2029/2330 train_time:81787ms step_avg:40.31ms
step:2030/2330 train_time:81844ms step_avg:40.32ms
step:2031/2330 train_time:81867ms step_avg:40.31ms
step:2032/2330 train_time:81923ms step_avg:40.32ms
step:2033/2330 train_time:81948ms step_avg:40.31ms
step:2034/2330 train_time:82007ms step_avg:40.32ms
step:2035/2330 train_time:82031ms step_avg:40.31ms
step:2036/2330 train_time:82091ms step_avg:40.32ms
step:2037/2330 train_time:82114ms step_avg:40.31ms
step:2038/2330 train_time:82172ms step_avg:40.32ms
step:2039/2330 train_time:82195ms step_avg:40.31ms
step:2040/2330 train_time:82253ms step_avg:40.32ms
step:2041/2330 train_time:82276ms step_avg:40.31ms
step:2042/2330 train_time:82334ms step_avg:40.32ms
step:2043/2330 train_time:82357ms step_avg:40.31ms
step:2044/2330 train_time:82415ms step_avg:40.32ms
step:2045/2330 train_time:82438ms step_avg:40.31ms
step:2046/2330 train_time:82495ms step_avg:40.32ms
step:2047/2330 train_time:82517ms step_avg:40.31ms
step:2048/2330 train_time:82574ms step_avg:40.32ms
step:2049/2330 train_time:82597ms step_avg:40.31ms
step:2050/2330 train_time:82654ms step_avg:40.32ms
step:2051/2330 train_time:82677ms step_avg:40.31ms
step:2052/2330 train_time:82734ms step_avg:40.32ms
step:2053/2330 train_time:82758ms step_avg:40.31ms
step:2054/2330 train_time:82815ms step_avg:40.32ms
step:2055/2330 train_time:82838ms step_avg:40.31ms
step:2056/2330 train_time:82896ms step_avg:40.32ms
step:2057/2330 train_time:82919ms step_avg:40.31ms
step:2058/2330 train_time:82977ms step_avg:40.32ms
step:2059/2330 train_time:83000ms step_avg:40.31ms
step:2060/2330 train_time:83058ms step_avg:40.32ms
step:2061/2330 train_time:83083ms step_avg:40.31ms
step:2062/2330 train_time:83141ms step_avg:40.32ms
step:2063/2330 train_time:83165ms step_avg:40.31ms
step:2064/2330 train_time:83222ms step_avg:40.32ms
step:2065/2330 train_time:83246ms step_avg:40.31ms
step:2066/2330 train_time:83303ms step_avg:40.32ms
step:2067/2330 train_time:83327ms step_avg:40.31ms
step:2068/2330 train_time:83384ms step_avg:40.32ms
step:2069/2330 train_time:83408ms step_avg:40.31ms
step:2070/2330 train_time:83465ms step_avg:40.32ms
step:2071/2330 train_time:83488ms step_avg:40.31ms
step:2072/2330 train_time:83547ms step_avg:40.32ms
step:2073/2330 train_time:83569ms step_avg:40.31ms
step:2074/2330 train_time:83627ms step_avg:40.32ms
step:2075/2330 train_time:83649ms step_avg:40.31ms
step:2076/2330 train_time:83707ms step_avg:40.32ms
step:2077/2330 train_time:83729ms step_avg:40.31ms
step:2078/2330 train_time:83786ms step_avg:40.32ms
step:2079/2330 train_time:83809ms step_avg:40.31ms
step:2080/2330 train_time:83867ms step_avg:40.32ms
step:2081/2330 train_time:83890ms step_avg:40.31ms
step:2082/2330 train_time:83949ms step_avg:40.32ms
step:2083/2330 train_time:83971ms step_avg:40.31ms
step:2084/2330 train_time:84028ms step_avg:40.32ms
step:2085/2330 train_time:84051ms step_avg:40.31ms
step:2086/2330 train_time:84109ms step_avg:40.32ms
step:2087/2330 train_time:84133ms step_avg:40.31ms
step:2088/2330 train_time:84192ms step_avg:40.32ms
step:2089/2330 train_time:84215ms step_avg:40.31ms
step:2090/2330 train_time:84273ms step_avg:40.32ms
step:2091/2330 train_time:84296ms step_avg:40.31ms
step:2092/2330 train_time:84354ms step_avg:40.32ms
step:2093/2330 train_time:84377ms step_avg:40.31ms
step:2094/2330 train_time:84435ms step_avg:40.32ms
step:2095/2330 train_time:84459ms step_avg:40.31ms
step:2096/2330 train_time:84517ms step_avg:40.32ms
step:2097/2330 train_time:84540ms step_avg:40.31ms
step:2098/2330 train_time:84598ms step_avg:40.32ms
step:2099/2330 train_time:84622ms step_avg:40.32ms
step:2100/2330 train_time:84679ms step_avg:40.32ms
step:2101/2330 train_time:84702ms step_avg:40.32ms
step:2102/2330 train_time:84759ms step_avg:40.32ms
step:2103/2330 train_time:84782ms step_avg:40.31ms
step:2104/2330 train_time:84839ms step_avg:40.32ms
step:2105/2330 train_time:84863ms step_avg:40.31ms
step:2106/2330 train_time:84919ms step_avg:40.32ms
step:2107/2330 train_time:84944ms step_avg:40.31ms
step:2108/2330 train_time:85000ms step_avg:40.32ms
step:2109/2330 train_time:85024ms step_avg:40.31ms
step:2110/2330 train_time:85081ms step_avg:40.32ms
step:2111/2330 train_time:85105ms step_avg:40.32ms
step:2112/2330 train_time:85162ms step_avg:40.32ms
step:2113/2330 train_time:85186ms step_avg:40.32ms
step:2114/2330 train_time:85243ms step_avg:40.32ms
step:2115/2330 train_time:85267ms step_avg:40.32ms
step:2116/2330 train_time:85324ms step_avg:40.32ms
step:2117/2330 train_time:85348ms step_avg:40.32ms
step:2118/2330 train_time:85406ms step_avg:40.32ms
step:2119/2330 train_time:85429ms step_avg:40.32ms
step:2120/2330 train_time:85487ms step_avg:40.32ms
step:2121/2330 train_time:85510ms step_avg:40.32ms
step:2122/2330 train_time:85568ms step_avg:40.32ms
step:2123/2330 train_time:85591ms step_avg:40.32ms
step:2124/2330 train_time:85649ms step_avg:40.32ms
step:2125/2330 train_time:85671ms step_avg:40.32ms
step:2126/2330 train_time:85728ms step_avg:40.32ms
step:2127/2330 train_time:85751ms step_avg:40.32ms
step:2128/2330 train_time:85809ms step_avg:40.32ms
step:2129/2330 train_time:85832ms step_avg:40.32ms
step:2130/2330 train_time:85889ms step_avg:40.32ms
step:2131/2330 train_time:85912ms step_avg:40.32ms
step:2132/2330 train_time:85970ms step_avg:40.32ms
step:2133/2330 train_time:85992ms step_avg:40.32ms
step:2134/2330 train_time:86050ms step_avg:40.32ms
step:2135/2330 train_time:86073ms step_avg:40.32ms
step:2136/2330 train_time:86130ms step_avg:40.32ms
step:2137/2330 train_time:86153ms step_avg:40.32ms
step:2138/2330 train_time:86212ms step_avg:40.32ms
step:2139/2330 train_time:86235ms step_avg:40.32ms
step:2140/2330 train_time:86293ms step_avg:40.32ms
step:2141/2330 train_time:86316ms step_avg:40.32ms
step:2142/2330 train_time:86374ms step_avg:40.32ms
step:2143/2330 train_time:86397ms step_avg:40.32ms
step:2144/2330 train_time:86455ms step_avg:40.32ms
step:2145/2330 train_time:86478ms step_avg:40.32ms
step:2146/2330 train_time:86536ms step_avg:40.32ms
step:2147/2330 train_time:86560ms step_avg:40.32ms
step:2148/2330 train_time:86618ms step_avg:40.32ms
step:2149/2330 train_time:86641ms step_avg:40.32ms
step:2150/2330 train_time:86698ms step_avg:40.32ms
step:2151/2330 train_time:86721ms step_avg:40.32ms
step:2152/2330 train_time:86779ms step_avg:40.32ms
step:2153/2330 train_time:86803ms step_avg:40.32ms
step:2154/2330 train_time:86860ms step_avg:40.33ms
step:2155/2330 train_time:86884ms step_avg:40.32ms
step:2156/2330 train_time:86941ms step_avg:40.33ms
step:2157/2330 train_time:86965ms step_avg:40.32ms
step:2158/2330 train_time:87022ms step_avg:40.33ms
step:2159/2330 train_time:87045ms step_avg:40.32ms
step:2160/2330 train_time:87102ms step_avg:40.32ms
step:2161/2330 train_time:87126ms step_avg:40.32ms
step:2162/2330 train_time:87183ms step_avg:40.33ms
step:2163/2330 train_time:87207ms step_avg:40.32ms
step:2164/2330 train_time:87264ms step_avg:40.33ms
step:2165/2330 train_time:87288ms step_avg:40.32ms
step:2166/2330 train_time:87345ms step_avg:40.33ms
step:2167/2330 train_time:87369ms step_avg:40.32ms
step:2168/2330 train_time:87427ms step_avg:40.33ms
step:2169/2330 train_time:87449ms step_avg:40.32ms
step:2170/2330 train_time:87508ms step_avg:40.33ms
step:2171/2330 train_time:87531ms step_avg:40.32ms
step:2172/2330 train_time:87589ms step_avg:40.33ms
step:2173/2330 train_time:87611ms step_avg:40.32ms
step:2174/2330 train_time:87669ms step_avg:40.33ms
step:2175/2330 train_time:87692ms step_avg:40.32ms
step:2176/2330 train_time:87751ms step_avg:40.33ms
step:2177/2330 train_time:87775ms step_avg:40.32ms
step:2178/2330 train_time:87832ms step_avg:40.33ms
step:2179/2330 train_time:87855ms step_avg:40.32ms
step:2180/2330 train_time:87913ms step_avg:40.33ms
step:2181/2330 train_time:87936ms step_avg:40.32ms
step:2182/2330 train_time:87994ms step_avg:40.33ms
step:2183/2330 train_time:88017ms step_avg:40.32ms
step:2184/2330 train_time:88075ms step_avg:40.33ms
step:2185/2330 train_time:88098ms step_avg:40.32ms
step:2186/2330 train_time:88156ms step_avg:40.33ms
step:2187/2330 train_time:88180ms step_avg:40.32ms
step:2188/2330 train_time:88238ms step_avg:40.33ms
step:2189/2330 train_time:88262ms step_avg:40.32ms
step:2190/2330 train_time:88319ms step_avg:40.33ms
step:2191/2330 train_time:88343ms step_avg:40.32ms
step:2192/2330 train_time:88400ms step_avg:40.33ms
step:2193/2330 train_time:88423ms step_avg:40.32ms
step:2194/2330 train_time:88480ms step_avg:40.33ms
step:2195/2330 train_time:88504ms step_avg:40.32ms
step:2196/2330 train_time:88561ms step_avg:40.33ms
step:2197/2330 train_time:88584ms step_avg:40.32ms
step:2198/2330 train_time:88641ms step_avg:40.33ms
step:2199/2330 train_time:88666ms step_avg:40.32ms
step:2200/2330 train_time:88723ms step_avg:40.33ms
step:2201/2330 train_time:88747ms step_avg:40.32ms
step:2202/2330 train_time:88804ms step_avg:40.33ms
step:2203/2330 train_time:88828ms step_avg:40.32ms
step:2204/2330 train_time:88885ms step_avg:40.33ms
step:2205/2330 train_time:88909ms step_avg:40.32ms
step:2206/2330 train_time:88966ms step_avg:40.33ms
step:2207/2330 train_time:88989ms step_avg:40.32ms
step:2208/2330 train_time:89046ms step_avg:40.33ms
step:2209/2330 train_time:89068ms step_avg:40.32ms
step:2210/2330 train_time:89127ms step_avg:40.33ms
step:2211/2330 train_time:89150ms step_avg:40.32ms
step:2212/2330 train_time:89208ms step_avg:40.33ms
step:2213/2330 train_time:89230ms step_avg:40.32ms
step:2214/2330 train_time:89288ms step_avg:40.33ms
step:2215/2330 train_time:89310ms step_avg:40.32ms
step:2216/2330 train_time:89368ms step_avg:40.33ms
step:2217/2330 train_time:89390ms step_avg:40.32ms
step:2218/2330 train_time:89449ms step_avg:40.33ms
step:2219/2330 train_time:89471ms step_avg:40.32ms
step:2220/2330 train_time:89529ms step_avg:40.33ms
step:2221/2330 train_time:89551ms step_avg:40.32ms
step:2222/2330 train_time:89610ms step_avg:40.33ms
step:2223/2330 train_time:89632ms step_avg:40.32ms
step:2224/2330 train_time:89690ms step_avg:40.33ms
step:2225/2330 train_time:89712ms step_avg:40.32ms
step:2226/2330 train_time:89769ms step_avg:40.33ms
step:2227/2330 train_time:89792ms step_avg:40.32ms
step:2228/2330 train_time:89849ms step_avg:40.33ms
step:2229/2330 train_time:89872ms step_avg:40.32ms
step:2230/2330 train_time:89929ms step_avg:40.33ms
step:2231/2330 train_time:89952ms step_avg:40.32ms
step:2232/2330 train_time:90010ms step_avg:40.33ms
step:2233/2330 train_time:90033ms step_avg:40.32ms
step:2234/2330 train_time:90091ms step_avg:40.33ms
step:2235/2330 train_time:90114ms step_avg:40.32ms
step:2236/2330 train_time:90173ms step_avg:40.33ms
step:2237/2330 train_time:90196ms step_avg:40.32ms
step:2238/2330 train_time:90254ms step_avg:40.33ms
step:2239/2330 train_time:90277ms step_avg:40.32ms
step:2240/2330 train_time:90335ms step_avg:40.33ms
step:2241/2330 train_time:90359ms step_avg:40.32ms
step:2242/2330 train_time:90418ms step_avg:40.33ms
step:2243/2330 train_time:90441ms step_avg:40.32ms
step:2244/2330 train_time:90498ms step_avg:40.33ms
step:2245/2330 train_time:90522ms step_avg:40.32ms
step:2246/2330 train_time:90580ms step_avg:40.33ms
step:2247/2330 train_time:90603ms step_avg:40.32ms
step:2248/2330 train_time:90660ms step_avg:40.33ms
step:2249/2330 train_time:90683ms step_avg:40.32ms
step:2250/2330 train_time:90740ms step_avg:40.33ms
step:2250/2330 val_loss:5.1582 train_time:90838ms step_avg:40.37ms
step:2251/2330 train_time:90851ms step_avg:40.36ms
step:2252/2330 train_time:90863ms step_avg:40.35ms
step:2253/2330 train_time:90874ms step_avg:40.33ms
step:2254/2330 train_time:90901ms step_avg:40.33ms
step:2255/2330 train_time:90923ms step_avg:40.32ms
step:2256/2330 train_time:90980ms step_avg:40.33ms
step:2257/2330 train_time:91002ms step_avg:40.32ms
step:2258/2330 train_time:91059ms step_avg:40.33ms
step:2259/2330 train_time:91082ms step_avg:40.32ms
step:2260/2330 train_time:91140ms step_avg:40.33ms
step:2261/2330 train_time:91168ms step_avg:40.32ms
step:2262/2330 train_time:91229ms step_avg:40.33ms
step:2263/2330 train_time:91254ms step_avg:40.32ms
step:2264/2330 train_time:91311ms step_avg:40.33ms
step:2265/2330 train_time:91333ms step_avg:40.32ms
step:2266/2330 train_time:91389ms step_avg:40.33ms
step:2267/2330 train_time:91412ms step_avg:40.32ms
step:2268/2330 train_time:91468ms step_avg:40.33ms
step:2269/2330 train_time:91491ms step_avg:40.32ms
step:2270/2330 train_time:91548ms step_avg:40.33ms
step:2271/2330 train_time:91570ms step_avg:40.32ms
step:2272/2330 train_time:91627ms step_avg:40.33ms
step:2273/2330 train_time:91650ms step_avg:40.32ms
step:2274/2330 train_time:91707ms step_avg:40.33ms
step:2275/2330 train_time:91729ms step_avg:40.32ms
step:2276/2330 train_time:91789ms step_avg:40.33ms
step:2277/2330 train_time:91812ms step_avg:40.32ms
step:2278/2330 train_time:91871ms step_avg:40.33ms
step:2279/2330 train_time:91894ms step_avg:40.32ms
step:2280/2330 train_time:91952ms step_avg:40.33ms
step:2281/2330 train_time:91975ms step_avg:40.32ms
step:2282/2330 train_time:92032ms step_avg:40.33ms
step:2283/2330 train_time:92055ms step_avg:40.32ms
step:2284/2330 train_time:92113ms step_avg:40.33ms
step:2285/2330 train_time:92137ms step_avg:40.32ms
step:2286/2330 train_time:92196ms step_avg:40.33ms
step:2287/2330 train_time:92220ms step_avg:40.32ms
step:2288/2330 train_time:92278ms step_avg:40.33ms
step:2289/2330 train_time:92303ms step_avg:40.32ms
step:2290/2330 train_time:92360ms step_avg:40.33ms
step:2291/2330 train_time:92384ms step_avg:40.32ms
step:2292/2330 train_time:92441ms step_avg:40.33ms
step:2293/2330 train_time:92464ms step_avg:40.32ms
step:2294/2330 train_time:92521ms step_avg:40.33ms
step:2295/2330 train_time:92544ms step_avg:40.32ms
step:2296/2330 train_time:92601ms step_avg:40.33ms
step:2297/2330 train_time:92625ms step_avg:40.32ms
step:2298/2330 train_time:92682ms step_avg:40.33ms
step:2299/2330 train_time:92705ms step_avg:40.32ms
step:2300/2330 train_time:92762ms step_avg:40.33ms
step:2301/2330 train_time:92786ms step_avg:40.32ms
step:2302/2330 train_time:92843ms step_avg:40.33ms
step:2303/2330 train_time:92866ms step_avg:40.32ms
step:2304/2330 train_time:92924ms step_avg:40.33ms
step:2305/2330 train_time:92947ms step_avg:40.32ms
step:2306/2330 train_time:93003ms step_avg:40.33ms
step:2307/2330 train_time:93027ms step_avg:40.32ms
step:2308/2330 train_time:93084ms step_avg:40.33ms
step:2309/2330 train_time:93109ms step_avg:40.32ms
step:2310/2330 train_time:93167ms step_avg:40.33ms
step:2311/2330 train_time:93191ms step_avg:40.32ms
step:2312/2330 train_time:93248ms step_avg:40.33ms
step:2313/2330 train_time:93272ms step_avg:40.33ms
step:2314/2330 train_time:93330ms step_avg:40.33ms
step:2315/2330 train_time:93352ms step_avg:40.32ms
step:2316/2330 train_time:93410ms step_avg:40.33ms
step:2317/2330 train_time:93433ms step_avg:40.32ms
step:2318/2330 train_time:93491ms step_avg:40.33ms
step:2319/2330 train_time:93513ms step_avg:40.32ms
step:2320/2330 train_time:93571ms step_avg:40.33ms
step:2321/2330 train_time:93593ms step_avg:40.32ms
step:2322/2330 train_time:93651ms step_avg:40.33ms
step:2323/2330 train_time:93673ms step_avg:40.32ms
step:2324/2330 train_time:93732ms step_avg:40.33ms
step:2325/2330 train_time:93755ms step_avg:40.32ms
step:2326/2330 train_time:93813ms step_avg:40.33ms
step:2327/2330 train_time:93835ms step_avg:40.32ms
step:2328/2330 train_time:93893ms step_avg:40.33ms
step:2329/2330 train_time:93916ms step_avg:40.32ms
step:2330/2330 train_time:93974ms step_avg:40.33ms
step:2330/2330 val_loss:5.1506 train_time:94072ms step_avg:40.37ms
peak memory allocated: 29712 MiB reserved: 38888 MiB
