import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = divide_by_norm(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = divide_by_norm(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "gd_5e-2"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=5e-2, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 03:25:11 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   33C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   31C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   31C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   32C    P0             115W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:96ms step_avg:96.15ms
step:2/2330 train_time:246ms step_avg:123.00ms
step:3/2330 train_time:267ms step_avg:89.05ms
step:4/2330 train_time:295ms step_avg:73.81ms
step:5/2330 train_time:351ms step_avg:70.28ms
step:6/2330 train_time:411ms step_avg:68.55ms
step:7/2330 train_time:469ms step_avg:67.00ms
step:8/2330 train_time:530ms step_avg:66.25ms
step:9/2330 train_time:588ms step_avg:65.32ms
step:10/2330 train_time:649ms step_avg:64.88ms
step:11/2330 train_time:706ms step_avg:64.22ms
step:12/2330 train_time:767ms step_avg:63.91ms
step:13/2330 train_time:825ms step_avg:63.46ms
step:14/2330 train_time:886ms step_avg:63.26ms
step:15/2330 train_time:944ms step_avg:62.91ms
step:16/2330 train_time:1004ms step_avg:62.77ms
step:17/2330 train_time:1062ms step_avg:62.48ms
step:18/2330 train_time:1124ms step_avg:62.43ms
step:19/2330 train_time:1187ms step_avg:62.49ms
step:20/2330 train_time:1254ms step_avg:62.68ms
step:21/2330 train_time:1315ms step_avg:62.61ms
step:22/2330 train_time:1376ms step_avg:62.53ms
step:23/2330 train_time:1434ms step_avg:62.34ms
step:24/2330 train_time:1495ms step_avg:62.30ms
step:25/2330 train_time:1553ms step_avg:62.13ms
step:26/2330 train_time:1615ms step_avg:62.11ms
step:27/2330 train_time:1673ms step_avg:61.96ms
step:28/2330 train_time:1735ms step_avg:61.96ms
step:29/2330 train_time:1793ms step_avg:61.84ms
step:30/2330 train_time:1855ms step_avg:61.83ms
step:31/2330 train_time:1914ms step_avg:61.73ms
step:32/2330 train_time:1975ms step_avg:61.70ms
step:33/2330 train_time:2033ms step_avg:61.61ms
step:34/2330 train_time:2094ms step_avg:61.60ms
step:35/2330 train_time:2154ms step_avg:61.54ms
step:36/2330 train_time:2217ms step_avg:61.58ms
step:37/2330 train_time:2278ms step_avg:61.56ms
step:38/2330 train_time:2340ms step_avg:61.57ms
step:39/2330 train_time:2399ms step_avg:61.51ms
step:40/2330 train_time:2461ms step_avg:61.53ms
step:41/2330 train_time:2520ms step_avg:61.45ms
step:42/2330 train_time:2582ms step_avg:61.49ms
step:43/2330 train_time:2640ms step_avg:61.40ms
step:44/2330 train_time:2702ms step_avg:61.40ms
step:45/2330 train_time:2760ms step_avg:61.34ms
step:46/2330 train_time:2822ms step_avg:61.36ms
step:47/2330 train_time:2881ms step_avg:61.29ms
step:48/2330 train_time:2943ms step_avg:61.30ms
step:49/2330 train_time:3001ms step_avg:61.24ms
step:50/2330 train_time:3062ms step_avg:61.25ms
step:51/2330 train_time:3121ms step_avg:61.19ms
step:52/2330 train_time:3183ms step_avg:61.21ms
step:53/2330 train_time:3242ms step_avg:61.17ms
step:54/2330 train_time:3305ms step_avg:61.20ms
step:55/2330 train_time:3364ms step_avg:61.17ms
step:56/2330 train_time:3428ms step_avg:61.21ms
step:57/2330 train_time:3486ms step_avg:61.16ms
step:58/2330 train_time:3549ms step_avg:61.18ms
step:59/2330 train_time:3608ms step_avg:61.14ms
step:60/2330 train_time:3670ms step_avg:61.16ms
step:61/2330 train_time:3729ms step_avg:61.13ms
step:62/2330 train_time:3790ms step_avg:61.13ms
step:63/2330 train_time:3850ms step_avg:61.11ms
step:64/2330 train_time:3911ms step_avg:61.12ms
step:65/2330 train_time:3971ms step_avg:61.09ms
step:66/2330 train_time:4032ms step_avg:61.10ms
step:67/2330 train_time:4090ms step_avg:61.05ms
step:68/2330 train_time:4153ms step_avg:61.07ms
step:69/2330 train_time:4212ms step_avg:61.04ms
step:70/2330 train_time:4273ms step_avg:61.04ms
step:71/2330 train_time:4332ms step_avg:61.02ms
step:72/2330 train_time:4393ms step_avg:61.02ms
step:73/2330 train_time:4452ms step_avg:60.99ms
step:74/2330 train_time:4514ms step_avg:61.01ms
step:75/2330 train_time:4573ms step_avg:60.98ms
step:76/2330 train_time:4635ms step_avg:60.98ms
step:77/2330 train_time:4694ms step_avg:60.96ms
step:78/2330 train_time:4756ms step_avg:60.97ms
step:79/2330 train_time:4815ms step_avg:60.95ms
step:80/2330 train_time:4878ms step_avg:60.97ms
step:81/2330 train_time:4936ms step_avg:60.94ms
step:82/2330 train_time:4998ms step_avg:60.95ms
step:83/2330 train_time:5057ms step_avg:60.93ms
step:84/2330 train_time:5119ms step_avg:60.94ms
step:85/2330 train_time:5178ms step_avg:60.91ms
step:86/2330 train_time:5239ms step_avg:60.92ms
step:87/2330 train_time:5298ms step_avg:60.89ms
step:88/2330 train_time:5360ms step_avg:60.91ms
step:89/2330 train_time:5419ms step_avg:60.89ms
step:90/2330 train_time:5481ms step_avg:60.90ms
step:91/2330 train_time:5540ms step_avg:60.88ms
step:92/2330 train_time:5602ms step_avg:60.89ms
step:93/2330 train_time:5660ms step_avg:60.86ms
step:94/2330 train_time:5723ms step_avg:60.88ms
step:95/2330 train_time:5782ms step_avg:60.86ms
step:96/2330 train_time:5843ms step_avg:60.87ms
step:97/2330 train_time:5902ms step_avg:60.85ms
step:98/2330 train_time:5965ms step_avg:60.86ms
step:99/2330 train_time:6023ms step_avg:60.84ms
step:100/2330 train_time:6085ms step_avg:60.85ms
step:101/2330 train_time:6144ms step_avg:60.83ms
step:102/2330 train_time:6205ms step_avg:60.83ms
step:103/2330 train_time:6264ms step_avg:60.82ms
step:104/2330 train_time:6327ms step_avg:60.84ms
step:105/2330 train_time:6386ms step_avg:60.82ms
step:106/2330 train_time:6448ms step_avg:60.83ms
step:107/2330 train_time:6508ms step_avg:60.82ms
step:108/2330 train_time:6569ms step_avg:60.83ms
step:109/2330 train_time:6628ms step_avg:60.81ms
step:110/2330 train_time:6690ms step_avg:60.81ms
step:111/2330 train_time:6747ms step_avg:60.79ms
step:112/2330 train_time:6810ms step_avg:60.80ms
step:113/2330 train_time:6868ms step_avg:60.78ms
step:114/2330 train_time:6931ms step_avg:60.80ms
step:115/2330 train_time:6990ms step_avg:60.78ms
step:116/2330 train_time:7051ms step_avg:60.79ms
step:117/2330 train_time:7110ms step_avg:60.77ms
step:118/2330 train_time:7171ms step_avg:60.77ms
step:119/2330 train_time:7230ms step_avg:60.75ms
step:120/2330 train_time:7292ms step_avg:60.76ms
step:121/2330 train_time:7351ms step_avg:60.75ms
step:122/2330 train_time:7413ms step_avg:60.77ms
step:123/2330 train_time:7472ms step_avg:60.75ms
step:124/2330 train_time:7534ms step_avg:60.76ms
step:125/2330 train_time:7593ms step_avg:60.74ms
step:126/2330 train_time:7655ms step_avg:60.75ms
step:127/2330 train_time:7713ms step_avg:60.73ms
step:128/2330 train_time:7775ms step_avg:60.74ms
step:129/2330 train_time:7834ms step_avg:60.73ms
step:130/2330 train_time:7896ms step_avg:60.74ms
step:131/2330 train_time:7955ms step_avg:60.72ms
step:132/2330 train_time:8017ms step_avg:60.74ms
step:133/2330 train_time:8076ms step_avg:60.72ms
step:134/2330 train_time:8140ms step_avg:60.75ms
step:135/2330 train_time:8199ms step_avg:60.73ms
step:136/2330 train_time:8261ms step_avg:60.74ms
step:137/2330 train_time:8320ms step_avg:60.73ms
step:138/2330 train_time:8382ms step_avg:60.74ms
step:139/2330 train_time:8441ms step_avg:60.72ms
step:140/2330 train_time:8503ms step_avg:60.73ms
step:141/2330 train_time:8561ms step_avg:60.72ms
step:142/2330 train_time:8623ms step_avg:60.73ms
step:143/2330 train_time:8682ms step_avg:60.71ms
step:144/2330 train_time:8744ms step_avg:60.72ms
step:145/2330 train_time:8803ms step_avg:60.71ms
step:146/2330 train_time:8864ms step_avg:60.71ms
step:147/2330 train_time:8923ms step_avg:60.70ms
step:148/2330 train_time:8985ms step_avg:60.71ms
step:149/2330 train_time:9045ms step_avg:60.70ms
step:150/2330 train_time:9107ms step_avg:60.71ms
step:151/2330 train_time:9166ms step_avg:60.70ms
step:152/2330 train_time:9228ms step_avg:60.71ms
step:153/2330 train_time:9287ms step_avg:60.70ms
step:154/2330 train_time:9348ms step_avg:60.70ms
step:155/2330 train_time:9408ms step_avg:60.69ms
step:156/2330 train_time:9469ms step_avg:60.70ms
step:157/2330 train_time:9528ms step_avg:60.69ms
step:158/2330 train_time:9590ms step_avg:60.70ms
step:159/2330 train_time:9649ms step_avg:60.69ms
step:160/2330 train_time:9713ms step_avg:60.70ms
step:161/2330 train_time:9771ms step_avg:60.69ms
step:162/2330 train_time:9832ms step_avg:60.69ms
step:163/2330 train_time:9890ms step_avg:60.68ms
step:164/2330 train_time:9952ms step_avg:60.68ms
step:165/2330 train_time:10011ms step_avg:60.67ms
step:166/2330 train_time:10073ms step_avg:60.68ms
step:167/2330 train_time:10132ms step_avg:60.67ms
step:168/2330 train_time:10194ms step_avg:60.68ms
step:169/2330 train_time:10253ms step_avg:60.67ms
step:170/2330 train_time:10315ms step_avg:60.68ms
step:171/2330 train_time:10374ms step_avg:60.66ms
step:172/2330 train_time:10435ms step_avg:60.67ms
step:173/2330 train_time:10495ms step_avg:60.66ms
step:174/2330 train_time:10557ms step_avg:60.67ms
step:175/2330 train_time:10617ms step_avg:60.67ms
step:176/2330 train_time:10679ms step_avg:60.67ms
step:177/2330 train_time:10737ms step_avg:60.66ms
step:178/2330 train_time:10800ms step_avg:60.67ms
step:179/2330 train_time:10858ms step_avg:60.66ms
step:180/2330 train_time:10920ms step_avg:60.67ms
step:181/2330 train_time:10978ms step_avg:60.65ms
step:182/2330 train_time:11041ms step_avg:60.67ms
step:183/2330 train_time:11099ms step_avg:60.65ms
step:184/2330 train_time:11161ms step_avg:60.66ms
step:185/2330 train_time:11219ms step_avg:60.64ms
step:186/2330 train_time:11281ms step_avg:60.65ms
step:187/2330 train_time:11340ms step_avg:60.64ms
step:188/2330 train_time:11402ms step_avg:60.65ms
step:189/2330 train_time:11461ms step_avg:60.64ms
step:190/2330 train_time:11523ms step_avg:60.65ms
step:191/2330 train_time:11582ms step_avg:60.64ms
step:192/2330 train_time:11644ms step_avg:60.64ms
step:193/2330 train_time:11703ms step_avg:60.64ms
step:194/2330 train_time:11764ms step_avg:60.64ms
step:195/2330 train_time:11823ms step_avg:60.63ms
step:196/2330 train_time:11884ms step_avg:60.63ms
step:197/2330 train_time:11943ms step_avg:60.62ms
step:198/2330 train_time:12005ms step_avg:60.63ms
step:199/2330 train_time:12064ms step_avg:60.62ms
step:200/2330 train_time:12126ms step_avg:60.63ms
step:201/2330 train_time:12185ms step_avg:60.62ms
step:202/2330 train_time:12246ms step_avg:60.63ms
step:203/2330 train_time:12305ms step_avg:60.62ms
step:204/2330 train_time:12367ms step_avg:60.62ms
step:205/2330 train_time:12426ms step_avg:60.62ms
step:206/2330 train_time:12488ms step_avg:60.62ms
step:207/2330 train_time:12547ms step_avg:60.61ms
step:208/2330 train_time:12609ms step_avg:60.62ms
step:209/2330 train_time:12668ms step_avg:60.61ms
step:210/2330 train_time:12729ms step_avg:60.61ms
step:211/2330 train_time:12788ms step_avg:60.61ms
step:212/2330 train_time:12850ms step_avg:60.61ms
step:213/2330 train_time:12909ms step_avg:60.61ms
step:214/2330 train_time:12971ms step_avg:60.61ms
step:215/2330 train_time:13030ms step_avg:60.60ms
step:216/2330 train_time:13092ms step_avg:60.61ms
step:217/2330 train_time:13150ms step_avg:60.60ms
step:218/2330 train_time:13213ms step_avg:60.61ms
step:219/2330 train_time:13271ms step_avg:60.60ms
step:220/2330 train_time:13333ms step_avg:60.61ms
step:221/2330 train_time:13392ms step_avg:60.60ms
step:222/2330 train_time:13454ms step_avg:60.61ms
step:223/2330 train_time:13514ms step_avg:60.60ms
step:224/2330 train_time:13575ms step_avg:60.60ms
step:225/2330 train_time:13634ms step_avg:60.59ms
step:226/2330 train_time:13696ms step_avg:60.60ms
step:227/2330 train_time:13755ms step_avg:60.59ms
step:228/2330 train_time:13816ms step_avg:60.60ms
step:229/2330 train_time:13875ms step_avg:60.59ms
step:230/2330 train_time:13937ms step_avg:60.60ms
step:231/2330 train_time:13997ms step_avg:60.59ms
step:232/2330 train_time:14059ms step_avg:60.60ms
step:233/2330 train_time:14118ms step_avg:60.59ms
step:234/2330 train_time:14180ms step_avg:60.60ms
step:235/2330 train_time:14239ms step_avg:60.59ms
step:236/2330 train_time:14301ms step_avg:60.60ms
step:237/2330 train_time:14359ms step_avg:60.59ms
step:238/2330 train_time:14421ms step_avg:60.59ms
step:239/2330 train_time:14480ms step_avg:60.59ms
step:240/2330 train_time:14542ms step_avg:60.59ms
step:241/2330 train_time:14600ms step_avg:60.58ms
step:242/2330 train_time:14663ms step_avg:60.59ms
step:243/2330 train_time:14721ms step_avg:60.58ms
step:244/2330 train_time:14783ms step_avg:60.59ms
step:245/2330 train_time:14842ms step_avg:60.58ms
step:246/2330 train_time:14904ms step_avg:60.59ms
step:247/2330 train_time:14964ms step_avg:60.58ms
step:248/2330 train_time:15026ms step_avg:60.59ms
step:249/2330 train_time:15085ms step_avg:60.58ms
step:250/2330 train_time:15146ms step_avg:60.59ms
step:250/2330 val_loss:4.7981 train_time:15219ms step_avg:60.87ms
step:251/2330 train_time:15240ms step_avg:60.72ms
step:252/2330 train_time:15269ms step_avg:60.59ms
step:253/2330 train_time:15330ms step_avg:60.59ms
step:254/2330 train_time:15396ms step_avg:60.62ms
step:255/2330 train_time:15460ms step_avg:60.63ms
step:256/2330 train_time:15523ms step_avg:60.64ms
step:257/2330 train_time:15582ms step_avg:60.63ms
step:258/2330 train_time:15644ms step_avg:60.64ms
step:259/2330 train_time:15702ms step_avg:60.62ms
step:260/2330 train_time:15764ms step_avg:60.63ms
step:261/2330 train_time:15821ms step_avg:60.62ms
step:262/2330 train_time:15882ms step_avg:60.62ms
step:263/2330 train_time:15940ms step_avg:60.61ms
step:264/2330 train_time:16001ms step_avg:60.61ms
step:265/2330 train_time:16059ms step_avg:60.60ms
step:266/2330 train_time:16121ms step_avg:60.61ms
step:267/2330 train_time:16180ms step_avg:60.60ms
step:268/2330 train_time:16243ms step_avg:60.61ms
step:269/2330 train_time:16303ms step_avg:60.61ms
step:270/2330 train_time:16368ms step_avg:60.62ms
step:271/2330 train_time:16426ms step_avg:60.61ms
step:272/2330 train_time:16489ms step_avg:60.62ms
step:273/2330 train_time:16548ms step_avg:60.62ms
step:274/2330 train_time:16610ms step_avg:60.62ms
step:275/2330 train_time:16669ms step_avg:60.61ms
step:276/2330 train_time:16730ms step_avg:60.62ms
step:277/2330 train_time:16789ms step_avg:60.61ms
step:278/2330 train_time:16850ms step_avg:60.61ms
step:279/2330 train_time:16908ms step_avg:60.60ms
step:280/2330 train_time:16971ms step_avg:60.61ms
step:281/2330 train_time:17029ms step_avg:60.60ms
step:282/2330 train_time:17091ms step_avg:60.60ms
step:283/2330 train_time:17149ms step_avg:60.60ms
step:284/2330 train_time:17211ms step_avg:60.60ms
step:285/2330 train_time:17270ms step_avg:60.60ms
step:286/2330 train_time:17333ms step_avg:60.60ms
step:287/2330 train_time:17393ms step_avg:60.60ms
step:288/2330 train_time:17455ms step_avg:60.61ms
step:289/2330 train_time:17515ms step_avg:60.60ms
step:290/2330 train_time:17577ms step_avg:60.61ms
step:291/2330 train_time:17636ms step_avg:60.61ms
step:292/2330 train_time:17699ms step_avg:60.61ms
step:293/2330 train_time:17759ms step_avg:60.61ms
step:294/2330 train_time:17821ms step_avg:60.61ms
step:295/2330 train_time:17879ms step_avg:60.61ms
step:296/2330 train_time:17941ms step_avg:60.61ms
step:297/2330 train_time:18000ms step_avg:60.61ms
step:298/2330 train_time:18063ms step_avg:60.61ms
step:299/2330 train_time:18120ms step_avg:60.60ms
step:300/2330 train_time:18183ms step_avg:60.61ms
step:301/2330 train_time:18241ms step_avg:60.60ms
step:302/2330 train_time:18303ms step_avg:60.61ms
step:303/2330 train_time:18362ms step_avg:60.60ms
step:304/2330 train_time:18424ms step_avg:60.61ms
step:305/2330 train_time:18483ms step_avg:60.60ms
step:306/2330 train_time:18546ms step_avg:60.61ms
step:307/2330 train_time:18604ms step_avg:60.60ms
step:308/2330 train_time:18667ms step_avg:60.61ms
step:309/2330 train_time:18726ms step_avg:60.60ms
step:310/2330 train_time:18788ms step_avg:60.61ms
step:311/2330 train_time:18848ms step_avg:60.60ms
step:312/2330 train_time:18910ms step_avg:60.61ms
step:313/2330 train_time:18969ms step_avg:60.60ms
step:314/2330 train_time:19030ms step_avg:60.61ms
step:315/2330 train_time:19089ms step_avg:60.60ms
step:316/2330 train_time:19151ms step_avg:60.60ms
step:317/2330 train_time:19209ms step_avg:60.60ms
step:318/2330 train_time:19272ms step_avg:60.60ms
step:319/2330 train_time:19330ms step_avg:60.60ms
step:320/2330 train_time:19392ms step_avg:60.60ms
step:321/2330 train_time:19450ms step_avg:60.59ms
step:322/2330 train_time:19512ms step_avg:60.60ms
step:323/2330 train_time:19571ms step_avg:60.59ms
step:324/2330 train_time:19633ms step_avg:60.59ms
step:325/2330 train_time:19692ms step_avg:60.59ms
step:326/2330 train_time:19753ms step_avg:60.59ms
step:327/2330 train_time:19812ms step_avg:60.59ms
step:328/2330 train_time:19874ms step_avg:60.59ms
step:329/2330 train_time:19933ms step_avg:60.59ms
step:330/2330 train_time:19995ms step_avg:60.59ms
step:331/2330 train_time:20055ms step_avg:60.59ms
step:332/2330 train_time:20117ms step_avg:60.59ms
step:333/2330 train_time:20176ms step_avg:60.59ms
step:334/2330 train_time:20237ms step_avg:60.59ms
step:335/2330 train_time:20297ms step_avg:60.59ms
step:336/2330 train_time:20359ms step_avg:60.59ms
step:337/2330 train_time:20417ms step_avg:60.58ms
step:338/2330 train_time:20479ms step_avg:60.59ms
step:339/2330 train_time:20538ms step_avg:60.58ms
step:340/2330 train_time:20600ms step_avg:60.59ms
step:341/2330 train_time:20660ms step_avg:60.59ms
step:342/2330 train_time:20721ms step_avg:60.59ms
step:343/2330 train_time:20781ms step_avg:60.59ms
step:344/2330 train_time:20842ms step_avg:60.59ms
step:345/2330 train_time:20901ms step_avg:60.58ms
step:346/2330 train_time:20963ms step_avg:60.59ms
step:347/2330 train_time:21022ms step_avg:60.58ms
step:348/2330 train_time:21084ms step_avg:60.58ms
step:349/2330 train_time:21142ms step_avg:60.58ms
step:350/2330 train_time:21204ms step_avg:60.58ms
step:351/2330 train_time:21263ms step_avg:60.58ms
step:352/2330 train_time:21324ms step_avg:60.58ms
step:353/2330 train_time:21383ms step_avg:60.58ms
step:354/2330 train_time:21446ms step_avg:60.58ms
step:355/2330 train_time:21505ms step_avg:60.58ms
step:356/2330 train_time:21567ms step_avg:60.58ms
step:357/2330 train_time:21625ms step_avg:60.58ms
step:358/2330 train_time:21688ms step_avg:60.58ms
step:359/2330 train_time:21747ms step_avg:60.58ms
step:360/2330 train_time:21808ms step_avg:60.58ms
step:361/2330 train_time:21867ms step_avg:60.57ms
step:362/2330 train_time:21929ms step_avg:60.58ms
step:363/2330 train_time:21987ms step_avg:60.57ms
step:364/2330 train_time:22049ms step_avg:60.58ms
step:365/2330 train_time:22108ms step_avg:60.57ms
step:366/2330 train_time:22170ms step_avg:60.58ms
step:367/2330 train_time:22229ms step_avg:60.57ms
step:368/2330 train_time:22291ms step_avg:60.57ms
step:369/2330 train_time:22350ms step_avg:60.57ms
step:370/2330 train_time:22412ms step_avg:60.57ms
step:371/2330 train_time:22472ms step_avg:60.57ms
step:372/2330 train_time:22533ms step_avg:60.57ms
step:373/2330 train_time:22592ms step_avg:60.57ms
step:374/2330 train_time:22654ms step_avg:60.57ms
step:375/2330 train_time:22713ms step_avg:60.57ms
step:376/2330 train_time:22775ms step_avg:60.57ms
step:377/2330 train_time:22834ms step_avg:60.57ms
step:378/2330 train_time:22896ms step_avg:60.57ms
step:379/2330 train_time:22955ms step_avg:60.57ms
step:380/2330 train_time:23017ms step_avg:60.57ms
step:381/2330 train_time:23076ms step_avg:60.57ms
step:382/2330 train_time:23138ms step_avg:60.57ms
step:383/2330 train_time:23197ms step_avg:60.57ms
step:384/2330 train_time:23260ms step_avg:60.57ms
step:385/2330 train_time:23319ms step_avg:60.57ms
step:386/2330 train_time:23381ms step_avg:60.57ms
step:387/2330 train_time:23440ms step_avg:60.57ms
step:388/2330 train_time:23502ms step_avg:60.57ms
step:389/2330 train_time:23561ms step_avg:60.57ms
step:390/2330 train_time:23622ms step_avg:60.57ms
step:391/2330 train_time:23681ms step_avg:60.57ms
step:392/2330 train_time:23743ms step_avg:60.57ms
step:393/2330 train_time:23801ms step_avg:60.56ms
step:394/2330 train_time:23862ms step_avg:60.56ms
step:395/2330 train_time:23921ms step_avg:60.56ms
step:396/2330 train_time:23982ms step_avg:60.56ms
step:397/2330 train_time:24041ms step_avg:60.56ms
step:398/2330 train_time:24103ms step_avg:60.56ms
step:399/2330 train_time:24163ms step_avg:60.56ms
step:400/2330 train_time:24224ms step_avg:60.56ms
step:401/2330 train_time:24283ms step_avg:60.56ms
step:402/2330 train_time:24346ms step_avg:60.56ms
step:403/2330 train_time:24404ms step_avg:60.56ms
step:404/2330 train_time:24467ms step_avg:60.56ms
step:405/2330 train_time:24525ms step_avg:60.56ms
step:406/2330 train_time:24587ms step_avg:60.56ms
step:407/2330 train_time:24646ms step_avg:60.55ms
step:408/2330 train_time:24708ms step_avg:60.56ms
step:409/2330 train_time:24767ms step_avg:60.55ms
step:410/2330 train_time:24829ms step_avg:60.56ms
step:411/2330 train_time:24887ms step_avg:60.55ms
step:412/2330 train_time:24949ms step_avg:60.56ms
step:413/2330 train_time:25008ms step_avg:60.55ms
step:414/2330 train_time:25070ms step_avg:60.56ms
step:415/2330 train_time:25129ms step_avg:60.55ms
step:416/2330 train_time:25190ms step_avg:60.55ms
step:417/2330 train_time:25249ms step_avg:60.55ms
step:418/2330 train_time:25311ms step_avg:60.55ms
step:419/2330 train_time:25370ms step_avg:60.55ms
step:420/2330 train_time:25433ms step_avg:60.55ms
step:421/2330 train_time:25491ms step_avg:60.55ms
step:422/2330 train_time:25553ms step_avg:60.55ms
step:423/2330 train_time:25611ms step_avg:60.55ms
step:424/2330 train_time:25673ms step_avg:60.55ms
step:425/2330 train_time:25732ms step_avg:60.54ms
step:426/2330 train_time:25793ms step_avg:60.55ms
step:427/2330 train_time:25852ms step_avg:60.54ms
step:428/2330 train_time:25914ms step_avg:60.55ms
step:429/2330 train_time:25973ms step_avg:60.54ms
step:430/2330 train_time:26034ms step_avg:60.54ms
step:431/2330 train_time:26093ms step_avg:60.54ms
step:432/2330 train_time:26155ms step_avg:60.54ms
step:433/2330 train_time:26215ms step_avg:60.54ms
step:434/2330 train_time:26277ms step_avg:60.55ms
step:435/2330 train_time:26336ms step_avg:60.54ms
step:436/2330 train_time:26398ms step_avg:60.55ms
step:437/2330 train_time:26457ms step_avg:60.54ms
step:438/2330 train_time:26519ms step_avg:60.55ms
step:439/2330 train_time:26579ms step_avg:60.54ms
step:440/2330 train_time:26640ms step_avg:60.55ms
step:441/2330 train_time:26700ms step_avg:60.54ms
step:442/2330 train_time:26761ms step_avg:60.55ms
step:443/2330 train_time:26820ms step_avg:60.54ms
step:444/2330 train_time:26882ms step_avg:60.54ms
step:445/2330 train_time:26941ms step_avg:60.54ms
step:446/2330 train_time:27004ms step_avg:60.55ms
step:447/2330 train_time:27063ms step_avg:60.54ms
step:448/2330 train_time:27124ms step_avg:60.54ms
step:449/2330 train_time:27183ms step_avg:60.54ms
step:450/2330 train_time:27245ms step_avg:60.54ms
step:451/2330 train_time:27304ms step_avg:60.54ms
step:452/2330 train_time:27367ms step_avg:60.55ms
step:453/2330 train_time:27425ms step_avg:60.54ms
step:454/2330 train_time:27488ms step_avg:60.55ms
step:455/2330 train_time:27546ms step_avg:60.54ms
step:456/2330 train_time:27608ms step_avg:60.54ms
step:457/2330 train_time:27666ms step_avg:60.54ms
step:458/2330 train_time:27729ms step_avg:60.54ms
step:459/2330 train_time:27788ms step_avg:60.54ms
step:460/2330 train_time:27850ms step_avg:60.54ms
step:461/2330 train_time:27908ms step_avg:60.54ms
step:462/2330 train_time:27970ms step_avg:60.54ms
step:463/2330 train_time:28030ms step_avg:60.54ms
step:464/2330 train_time:28092ms step_avg:60.54ms
step:465/2330 train_time:28151ms step_avg:60.54ms
step:466/2330 train_time:28213ms step_avg:60.54ms
step:467/2330 train_time:28272ms step_avg:60.54ms
step:468/2330 train_time:28334ms step_avg:60.54ms
step:469/2330 train_time:28393ms step_avg:60.54ms
step:470/2330 train_time:28455ms step_avg:60.54ms
step:471/2330 train_time:28514ms step_avg:60.54ms
step:472/2330 train_time:28577ms step_avg:60.54ms
step:473/2330 train_time:28635ms step_avg:60.54ms
step:474/2330 train_time:28698ms step_avg:60.54ms
step:475/2330 train_time:28757ms step_avg:60.54ms
step:476/2330 train_time:28819ms step_avg:60.55ms
step:477/2330 train_time:28878ms step_avg:60.54ms
step:478/2330 train_time:28940ms step_avg:60.54ms
step:479/2330 train_time:28999ms step_avg:60.54ms
step:480/2330 train_time:29061ms step_avg:60.54ms
step:481/2330 train_time:29120ms step_avg:60.54ms
step:482/2330 train_time:29181ms step_avg:60.54ms
step:483/2330 train_time:29241ms step_avg:60.54ms
step:484/2330 train_time:29303ms step_avg:60.54ms
step:485/2330 train_time:29363ms step_avg:60.54ms
step:486/2330 train_time:29423ms step_avg:60.54ms
step:487/2330 train_time:29482ms step_avg:60.54ms
step:488/2330 train_time:29544ms step_avg:60.54ms
step:489/2330 train_time:29602ms step_avg:60.54ms
step:490/2330 train_time:29666ms step_avg:60.54ms
step:491/2330 train_time:29724ms step_avg:60.54ms
step:492/2330 train_time:29786ms step_avg:60.54ms
step:493/2330 train_time:29846ms step_avg:60.54ms
step:494/2330 train_time:29908ms step_avg:60.54ms
step:495/2330 train_time:29967ms step_avg:60.54ms
step:496/2330 train_time:30029ms step_avg:60.54ms
step:497/2330 train_time:30089ms step_avg:60.54ms
step:498/2330 train_time:30151ms step_avg:60.54ms
step:499/2330 train_time:30210ms step_avg:60.54ms
step:500/2330 train_time:30273ms step_avg:60.55ms
step:500/2330 val_loss:4.1806 train_time:30344ms step_avg:60.69ms
step:501/2330 train_time:30365ms step_avg:60.61ms
step:502/2330 train_time:30397ms step_avg:60.55ms
step:503/2330 train_time:30455ms step_avg:60.55ms
step:504/2330 train_time:30520ms step_avg:60.56ms
step:505/2330 train_time:30580ms step_avg:60.55ms
step:506/2330 train_time:30642ms step_avg:60.56ms
step:507/2330 train_time:30700ms step_avg:60.55ms
step:508/2330 train_time:30761ms step_avg:60.55ms
step:509/2330 train_time:30820ms step_avg:60.55ms
step:510/2330 train_time:30881ms step_avg:60.55ms
step:511/2330 train_time:30940ms step_avg:60.55ms
step:512/2330 train_time:31002ms step_avg:60.55ms
step:513/2330 train_time:31060ms step_avg:60.55ms
step:514/2330 train_time:31122ms step_avg:60.55ms
step:515/2330 train_time:31180ms step_avg:60.54ms
step:516/2330 train_time:31244ms step_avg:60.55ms
step:517/2330 train_time:31305ms step_avg:60.55ms
step:518/2330 train_time:31366ms step_avg:60.55ms
step:519/2330 train_time:31427ms step_avg:60.55ms
step:520/2330 train_time:31490ms step_avg:60.56ms
step:521/2330 train_time:31548ms step_avg:60.55ms
step:522/2330 train_time:31610ms step_avg:60.56ms
step:523/2330 train_time:31668ms step_avg:60.55ms
step:524/2330 train_time:31730ms step_avg:60.55ms
step:525/2330 train_time:31789ms step_avg:60.55ms
step:526/2330 train_time:31851ms step_avg:60.55ms
step:527/2330 train_time:31910ms step_avg:60.55ms
step:528/2330 train_time:31972ms step_avg:60.55ms
step:529/2330 train_time:32030ms step_avg:60.55ms
step:530/2330 train_time:32093ms step_avg:60.55ms
step:531/2330 train_time:32152ms step_avg:60.55ms
step:532/2330 train_time:32215ms step_avg:60.55ms
step:533/2330 train_time:32273ms step_avg:60.55ms
step:534/2330 train_time:32335ms step_avg:60.55ms
step:535/2330 train_time:32394ms step_avg:60.55ms
step:536/2330 train_time:32455ms step_avg:60.55ms
step:537/2330 train_time:32514ms step_avg:60.55ms
step:538/2330 train_time:32576ms step_avg:60.55ms
step:539/2330 train_time:32634ms step_avg:60.55ms
step:540/2330 train_time:32696ms step_avg:60.55ms
step:541/2330 train_time:32755ms step_avg:60.54ms
step:542/2330 train_time:32816ms step_avg:60.55ms
step:543/2330 train_time:32875ms step_avg:60.54ms
step:544/2330 train_time:32937ms step_avg:60.55ms
step:545/2330 train_time:32996ms step_avg:60.54ms
step:546/2330 train_time:33057ms step_avg:60.54ms
step:547/2330 train_time:33116ms step_avg:60.54ms
step:548/2330 train_time:33178ms step_avg:60.54ms
step:549/2330 train_time:33237ms step_avg:60.54ms
step:550/2330 train_time:33298ms step_avg:60.54ms
step:551/2330 train_time:33357ms step_avg:60.54ms
step:552/2330 train_time:33419ms step_avg:60.54ms
step:553/2330 train_time:33478ms step_avg:60.54ms
step:554/2330 train_time:33540ms step_avg:60.54ms
step:555/2330 train_time:33599ms step_avg:60.54ms
step:556/2330 train_time:33662ms step_avg:60.54ms
step:557/2330 train_time:33721ms step_avg:60.54ms
step:558/2330 train_time:33782ms step_avg:60.54ms
step:559/2330 train_time:33842ms step_avg:60.54ms
step:560/2330 train_time:33904ms step_avg:60.54ms
step:561/2330 train_time:33963ms step_avg:60.54ms
step:562/2330 train_time:34025ms step_avg:60.54ms
step:563/2330 train_time:34083ms step_avg:60.54ms
step:564/2330 train_time:34145ms step_avg:60.54ms
step:565/2330 train_time:34205ms step_avg:60.54ms
step:566/2330 train_time:34266ms step_avg:60.54ms
step:567/2330 train_time:34325ms step_avg:60.54ms
step:568/2330 train_time:34387ms step_avg:60.54ms
step:569/2330 train_time:34445ms step_avg:60.54ms
step:570/2330 train_time:34508ms step_avg:60.54ms
step:571/2330 train_time:34566ms step_avg:60.54ms
step:572/2330 train_time:34629ms step_avg:60.54ms
step:573/2330 train_time:34687ms step_avg:60.54ms
step:574/2330 train_time:34749ms step_avg:60.54ms
step:575/2330 train_time:34809ms step_avg:60.54ms
step:576/2330 train_time:34871ms step_avg:60.54ms
step:577/2330 train_time:34930ms step_avg:60.54ms
step:578/2330 train_time:34992ms step_avg:60.54ms
step:579/2330 train_time:35051ms step_avg:60.54ms
step:580/2330 train_time:35113ms step_avg:60.54ms
step:581/2330 train_time:35171ms step_avg:60.54ms
step:582/2330 train_time:35233ms step_avg:60.54ms
step:583/2330 train_time:35292ms step_avg:60.54ms
step:584/2330 train_time:35354ms step_avg:60.54ms
step:585/2330 train_time:35413ms step_avg:60.54ms
step:586/2330 train_time:35474ms step_avg:60.54ms
step:587/2330 train_time:35533ms step_avg:60.53ms
step:588/2330 train_time:35595ms step_avg:60.54ms
step:589/2330 train_time:35653ms step_avg:60.53ms
step:590/2330 train_time:35716ms step_avg:60.54ms
step:591/2330 train_time:35775ms step_avg:60.53ms
step:592/2330 train_time:35837ms step_avg:60.53ms
step:593/2330 train_time:35896ms step_avg:60.53ms
step:594/2330 train_time:35957ms step_avg:60.53ms
step:595/2330 train_time:36017ms step_avg:60.53ms
step:596/2330 train_time:36079ms step_avg:60.54ms
step:597/2330 train_time:36138ms step_avg:60.53ms
step:598/2330 train_time:36200ms step_avg:60.54ms
step:599/2330 train_time:36259ms step_avg:60.53ms
step:600/2330 train_time:36322ms step_avg:60.54ms
step:601/2330 train_time:36381ms step_avg:60.53ms
step:602/2330 train_time:36444ms step_avg:60.54ms
step:603/2330 train_time:36503ms step_avg:60.54ms
step:604/2330 train_time:36565ms step_avg:60.54ms
step:605/2330 train_time:36624ms step_avg:60.53ms
step:606/2330 train_time:36686ms step_avg:60.54ms
step:607/2330 train_time:36745ms step_avg:60.53ms
step:608/2330 train_time:36807ms step_avg:60.54ms
step:609/2330 train_time:36866ms step_avg:60.53ms
step:610/2330 train_time:36927ms step_avg:60.54ms
step:611/2330 train_time:36986ms step_avg:60.53ms
step:612/2330 train_time:37049ms step_avg:60.54ms
step:613/2330 train_time:37108ms step_avg:60.53ms
step:614/2330 train_time:37169ms step_avg:60.54ms
step:615/2330 train_time:37228ms step_avg:60.53ms
step:616/2330 train_time:37291ms step_avg:60.54ms
step:617/2330 train_time:37350ms step_avg:60.53ms
step:618/2330 train_time:37412ms step_avg:60.54ms
step:619/2330 train_time:37471ms step_avg:60.54ms
step:620/2330 train_time:37534ms step_avg:60.54ms
step:621/2330 train_time:37593ms step_avg:60.54ms
step:622/2330 train_time:37655ms step_avg:60.54ms
step:623/2330 train_time:37714ms step_avg:60.54ms
step:624/2330 train_time:37776ms step_avg:60.54ms
step:625/2330 train_time:37834ms step_avg:60.53ms
step:626/2330 train_time:37895ms step_avg:60.54ms
step:627/2330 train_time:37953ms step_avg:60.53ms
step:628/2330 train_time:38016ms step_avg:60.54ms
step:629/2330 train_time:38074ms step_avg:60.53ms
step:630/2330 train_time:38136ms step_avg:60.53ms
step:631/2330 train_time:38194ms step_avg:60.53ms
step:632/2330 train_time:38257ms step_avg:60.53ms
step:633/2330 train_time:38317ms step_avg:60.53ms
step:634/2330 train_time:38378ms step_avg:60.53ms
step:635/2330 train_time:38437ms step_avg:60.53ms
step:636/2330 train_time:38499ms step_avg:60.53ms
step:637/2330 train_time:38557ms step_avg:60.53ms
step:638/2330 train_time:38619ms step_avg:60.53ms
step:639/2330 train_time:38678ms step_avg:60.53ms
step:640/2330 train_time:38739ms step_avg:60.53ms
step:641/2330 train_time:38799ms step_avg:60.53ms
step:642/2330 train_time:38861ms step_avg:60.53ms
step:643/2330 train_time:38920ms step_avg:60.53ms
step:644/2330 train_time:38982ms step_avg:60.53ms
step:645/2330 train_time:39040ms step_avg:60.53ms
step:646/2330 train_time:39103ms step_avg:60.53ms
step:647/2330 train_time:39161ms step_avg:60.53ms
step:648/2330 train_time:39223ms step_avg:60.53ms
step:649/2330 train_time:39282ms step_avg:60.53ms
step:650/2330 train_time:39343ms step_avg:60.53ms
step:651/2330 train_time:39403ms step_avg:60.53ms
step:652/2330 train_time:39464ms step_avg:60.53ms
step:653/2330 train_time:39523ms step_avg:60.52ms
step:654/2330 train_time:39584ms step_avg:60.53ms
step:655/2330 train_time:39642ms step_avg:60.52ms
step:656/2330 train_time:39704ms step_avg:60.52ms
step:657/2330 train_time:39764ms step_avg:60.52ms
step:658/2330 train_time:39826ms step_avg:60.53ms
step:659/2330 train_time:39884ms step_avg:60.52ms
step:660/2330 train_time:39946ms step_avg:60.52ms
step:661/2330 train_time:40005ms step_avg:60.52ms
step:662/2330 train_time:40067ms step_avg:60.52ms
step:663/2330 train_time:40126ms step_avg:60.52ms
step:664/2330 train_time:40187ms step_avg:60.52ms
step:665/2330 train_time:40246ms step_avg:60.52ms
step:666/2330 train_time:40308ms step_avg:60.52ms
step:667/2330 train_time:40366ms step_avg:60.52ms
step:668/2330 train_time:40428ms step_avg:60.52ms
step:669/2330 train_time:40487ms step_avg:60.52ms
step:670/2330 train_time:40549ms step_avg:60.52ms
step:671/2330 train_time:40608ms step_avg:60.52ms
step:672/2330 train_time:40670ms step_avg:60.52ms
step:673/2330 train_time:40729ms step_avg:60.52ms
step:674/2330 train_time:40791ms step_avg:60.52ms
step:675/2330 train_time:40850ms step_avg:60.52ms
step:676/2330 train_time:40913ms step_avg:60.52ms
step:677/2330 train_time:40972ms step_avg:60.52ms
step:678/2330 train_time:41035ms step_avg:60.52ms
step:679/2330 train_time:41093ms step_avg:60.52ms
step:680/2330 train_time:41155ms step_avg:60.52ms
step:681/2330 train_time:41214ms step_avg:60.52ms
step:682/2330 train_time:41275ms step_avg:60.52ms
step:683/2330 train_time:41333ms step_avg:60.52ms
step:684/2330 train_time:41396ms step_avg:60.52ms
step:685/2330 train_time:41454ms step_avg:60.52ms
step:686/2330 train_time:41516ms step_avg:60.52ms
step:687/2330 train_time:41575ms step_avg:60.52ms
step:688/2330 train_time:41637ms step_avg:60.52ms
step:689/2330 train_time:41696ms step_avg:60.52ms
step:690/2330 train_time:41758ms step_avg:60.52ms
step:691/2330 train_time:41818ms step_avg:60.52ms
step:692/2330 train_time:41880ms step_avg:60.52ms
step:693/2330 train_time:41938ms step_avg:60.52ms
step:694/2330 train_time:42000ms step_avg:60.52ms
step:695/2330 train_time:42060ms step_avg:60.52ms
step:696/2330 train_time:42122ms step_avg:60.52ms
step:697/2330 train_time:42181ms step_avg:60.52ms
step:698/2330 train_time:42243ms step_avg:60.52ms
step:699/2330 train_time:42302ms step_avg:60.52ms
step:700/2330 train_time:42364ms step_avg:60.52ms
step:701/2330 train_time:42422ms step_avg:60.52ms
step:702/2330 train_time:42483ms step_avg:60.52ms
step:703/2330 train_time:42542ms step_avg:60.51ms
step:704/2330 train_time:42604ms step_avg:60.52ms
step:705/2330 train_time:42663ms step_avg:60.51ms
step:706/2330 train_time:42724ms step_avg:60.52ms
step:707/2330 train_time:42782ms step_avg:60.51ms
step:708/2330 train_time:42843ms step_avg:60.51ms
step:709/2330 train_time:42903ms step_avg:60.51ms
step:710/2330 train_time:42966ms step_avg:60.51ms
step:711/2330 train_time:43025ms step_avg:60.51ms
step:712/2330 train_time:43087ms step_avg:60.52ms
step:713/2330 train_time:43145ms step_avg:60.51ms
step:714/2330 train_time:43207ms step_avg:60.51ms
step:715/2330 train_time:43265ms step_avg:60.51ms
step:716/2330 train_time:43327ms step_avg:60.51ms
step:717/2330 train_time:43385ms step_avg:60.51ms
step:718/2330 train_time:43447ms step_avg:60.51ms
step:719/2330 train_time:43506ms step_avg:60.51ms
step:720/2330 train_time:43567ms step_avg:60.51ms
step:721/2330 train_time:43626ms step_avg:60.51ms
step:722/2330 train_time:43687ms step_avg:60.51ms
step:723/2330 train_time:43746ms step_avg:60.51ms
step:724/2330 train_time:43808ms step_avg:60.51ms
step:725/2330 train_time:43867ms step_avg:60.51ms
step:726/2330 train_time:43928ms step_avg:60.51ms
step:727/2330 train_time:43988ms step_avg:60.51ms
step:728/2330 train_time:44050ms step_avg:60.51ms
step:729/2330 train_time:44108ms step_avg:60.51ms
step:730/2330 train_time:44170ms step_avg:60.51ms
step:731/2330 train_time:44229ms step_avg:60.51ms
step:732/2330 train_time:44291ms step_avg:60.51ms
step:733/2330 train_time:44349ms step_avg:60.50ms
step:734/2330 train_time:44411ms step_avg:60.51ms
step:735/2330 train_time:44470ms step_avg:60.50ms
step:736/2330 train_time:44531ms step_avg:60.50ms
step:737/2330 train_time:44590ms step_avg:60.50ms
step:738/2330 train_time:44652ms step_avg:60.50ms
step:739/2330 train_time:44711ms step_avg:60.50ms
step:740/2330 train_time:44773ms step_avg:60.50ms
step:741/2330 train_time:44832ms step_avg:60.50ms
step:742/2330 train_time:44894ms step_avg:60.50ms
step:743/2330 train_time:44952ms step_avg:60.50ms
step:744/2330 train_time:45014ms step_avg:60.50ms
step:745/2330 train_time:45073ms step_avg:60.50ms
step:746/2330 train_time:45136ms step_avg:60.50ms
step:747/2330 train_time:45195ms step_avg:60.50ms
step:748/2330 train_time:45256ms step_avg:60.50ms
step:749/2330 train_time:45315ms step_avg:60.50ms
step:750/2330 train_time:45377ms step_avg:60.50ms
step:750/2330 val_loss:3.9715 train_time:45448ms step_avg:60.60ms
step:751/2330 train_time:45470ms step_avg:60.55ms
step:752/2330 train_time:45499ms step_avg:60.50ms
step:753/2330 train_time:45560ms step_avg:60.50ms
step:754/2330 train_time:45626ms step_avg:60.51ms
step:755/2330 train_time:45689ms step_avg:60.51ms
step:756/2330 train_time:45750ms step_avg:60.52ms
step:757/2330 train_time:45808ms step_avg:60.51ms
step:758/2330 train_time:45869ms step_avg:60.51ms
step:759/2330 train_time:45928ms step_avg:60.51ms
step:760/2330 train_time:45989ms step_avg:60.51ms
step:761/2330 train_time:46047ms step_avg:60.51ms
step:762/2330 train_time:46109ms step_avg:60.51ms
step:763/2330 train_time:46167ms step_avg:60.51ms
step:764/2330 train_time:46228ms step_avg:60.51ms
step:765/2330 train_time:46287ms step_avg:60.51ms
step:766/2330 train_time:46348ms step_avg:60.51ms
step:767/2330 train_time:46407ms step_avg:60.50ms
step:768/2330 train_time:46471ms step_avg:60.51ms
step:769/2330 train_time:46532ms step_avg:60.51ms
step:770/2330 train_time:46597ms step_avg:60.52ms
step:771/2330 train_time:46658ms step_avg:60.52ms
step:772/2330 train_time:46721ms step_avg:60.52ms
step:773/2330 train_time:46782ms step_avg:60.52ms
step:774/2330 train_time:46844ms step_avg:60.52ms
step:775/2330 train_time:46904ms step_avg:60.52ms
step:776/2330 train_time:46966ms step_avg:60.52ms
step:777/2330 train_time:47025ms step_avg:60.52ms
step:778/2330 train_time:47087ms step_avg:60.52ms
step:779/2330 train_time:47146ms step_avg:60.52ms
step:780/2330 train_time:47207ms step_avg:60.52ms
step:781/2330 train_time:47266ms step_avg:60.52ms
step:782/2330 train_time:47327ms step_avg:60.52ms
step:783/2330 train_time:47388ms step_avg:60.52ms
step:784/2330 train_time:47450ms step_avg:60.52ms
step:785/2330 train_time:47510ms step_avg:60.52ms
step:786/2330 train_time:47573ms step_avg:60.53ms
step:787/2330 train_time:47634ms step_avg:60.53ms
step:788/2330 train_time:47697ms step_avg:60.53ms
step:789/2330 train_time:47757ms step_avg:60.53ms
step:790/2330 train_time:47820ms step_avg:60.53ms
step:791/2330 train_time:47879ms step_avg:60.53ms
step:792/2330 train_time:47942ms step_avg:60.53ms
step:793/2330 train_time:48000ms step_avg:60.53ms
step:794/2330 train_time:48063ms step_avg:60.53ms
step:795/2330 train_time:48123ms step_avg:60.53ms
step:796/2330 train_time:48185ms step_avg:60.53ms
step:797/2330 train_time:48245ms step_avg:60.53ms
step:798/2330 train_time:48306ms step_avg:60.53ms
step:799/2330 train_time:48366ms step_avg:60.53ms
step:800/2330 train_time:48429ms step_avg:60.54ms
step:801/2330 train_time:48488ms step_avg:60.53ms
step:802/2330 train_time:48551ms step_avg:60.54ms
step:803/2330 train_time:48610ms step_avg:60.54ms
step:804/2330 train_time:48674ms step_avg:60.54ms
step:805/2330 train_time:48734ms step_avg:60.54ms
step:806/2330 train_time:48798ms step_avg:60.54ms
step:807/2330 train_time:48857ms step_avg:60.54ms
step:808/2330 train_time:48919ms step_avg:60.54ms
step:809/2330 train_time:48978ms step_avg:60.54ms
step:810/2330 train_time:49041ms step_avg:60.54ms
step:811/2330 train_time:49100ms step_avg:60.54ms
step:812/2330 train_time:49163ms step_avg:60.55ms
step:813/2330 train_time:49223ms step_avg:60.54ms
step:814/2330 train_time:49285ms step_avg:60.55ms
step:815/2330 train_time:49345ms step_avg:60.55ms
step:816/2330 train_time:49407ms step_avg:60.55ms
step:817/2330 train_time:49467ms step_avg:60.55ms
step:818/2330 train_time:49529ms step_avg:60.55ms
step:819/2330 train_time:49589ms step_avg:60.55ms
step:820/2330 train_time:49651ms step_avg:60.55ms
step:821/2330 train_time:49711ms step_avg:60.55ms
step:822/2330 train_time:49775ms step_avg:60.55ms
step:823/2330 train_time:49834ms step_avg:60.55ms
step:824/2330 train_time:49897ms step_avg:60.55ms
step:825/2330 train_time:49956ms step_avg:60.55ms
step:826/2330 train_time:50018ms step_avg:60.55ms
step:827/2330 train_time:50078ms step_avg:60.55ms
step:828/2330 train_time:50141ms step_avg:60.56ms
step:829/2330 train_time:50201ms step_avg:60.56ms
step:830/2330 train_time:50263ms step_avg:60.56ms
step:831/2330 train_time:50323ms step_avg:60.56ms
step:832/2330 train_time:50385ms step_avg:60.56ms
step:833/2330 train_time:50446ms step_avg:60.56ms
step:834/2330 train_time:50508ms step_avg:60.56ms
step:835/2330 train_time:50567ms step_avg:60.56ms
step:836/2330 train_time:50631ms step_avg:60.56ms
step:837/2330 train_time:50690ms step_avg:60.56ms
step:838/2330 train_time:50753ms step_avg:60.56ms
step:839/2330 train_time:50812ms step_avg:60.56ms
step:840/2330 train_time:50875ms step_avg:60.57ms
step:841/2330 train_time:50934ms step_avg:60.56ms
step:842/2330 train_time:50997ms step_avg:60.57ms
step:843/2330 train_time:51056ms step_avg:60.56ms
step:844/2330 train_time:51118ms step_avg:60.57ms
step:845/2330 train_time:51178ms step_avg:60.57ms
step:846/2330 train_time:51241ms step_avg:60.57ms
step:847/2330 train_time:51301ms step_avg:60.57ms
step:848/2330 train_time:51364ms step_avg:60.57ms
step:849/2330 train_time:51424ms step_avg:60.57ms
step:850/2330 train_time:51487ms step_avg:60.57ms
step:851/2330 train_time:51547ms step_avg:60.57ms
step:852/2330 train_time:51609ms step_avg:60.57ms
step:853/2330 train_time:51669ms step_avg:60.57ms
step:854/2330 train_time:51731ms step_avg:60.58ms
step:855/2330 train_time:51791ms step_avg:60.57ms
step:856/2330 train_time:51854ms step_avg:60.58ms
step:857/2330 train_time:51914ms step_avg:60.58ms
step:858/2330 train_time:51977ms step_avg:60.58ms
step:859/2330 train_time:52037ms step_avg:60.58ms
step:860/2330 train_time:52100ms step_avg:60.58ms
step:861/2330 train_time:52159ms step_avg:60.58ms
step:862/2330 train_time:52222ms step_avg:60.58ms
step:863/2330 train_time:52282ms step_avg:60.58ms
step:864/2330 train_time:52345ms step_avg:60.58ms
step:865/2330 train_time:52404ms step_avg:60.58ms
step:866/2330 train_time:52467ms step_avg:60.59ms
step:867/2330 train_time:52527ms step_avg:60.59ms
step:868/2330 train_time:52590ms step_avg:60.59ms
step:869/2330 train_time:52651ms step_avg:60.59ms
step:870/2330 train_time:52712ms step_avg:60.59ms
step:871/2330 train_time:52772ms step_avg:60.59ms
step:872/2330 train_time:52835ms step_avg:60.59ms
step:873/2330 train_time:52894ms step_avg:60.59ms
step:874/2330 train_time:52957ms step_avg:60.59ms
step:875/2330 train_time:53016ms step_avg:60.59ms
step:876/2330 train_time:53078ms step_avg:60.59ms
step:877/2330 train_time:53138ms step_avg:60.59ms
step:878/2330 train_time:53200ms step_avg:60.59ms
step:879/2330 train_time:53260ms step_avg:60.59ms
step:880/2330 train_time:53323ms step_avg:60.59ms
step:881/2330 train_time:53383ms step_avg:60.59ms
step:882/2330 train_time:53446ms step_avg:60.60ms
step:883/2330 train_time:53505ms step_avg:60.59ms
step:884/2330 train_time:53567ms step_avg:60.60ms
step:885/2330 train_time:53628ms step_avg:60.60ms
step:886/2330 train_time:53690ms step_avg:60.60ms
step:887/2330 train_time:53749ms step_avg:60.60ms
step:888/2330 train_time:53812ms step_avg:60.60ms
step:889/2330 train_time:53871ms step_avg:60.60ms
step:890/2330 train_time:53934ms step_avg:60.60ms
step:891/2330 train_time:53993ms step_avg:60.60ms
step:892/2330 train_time:54057ms step_avg:60.60ms
step:893/2330 train_time:54116ms step_avg:60.60ms
step:894/2330 train_time:54179ms step_avg:60.60ms
step:895/2330 train_time:54238ms step_avg:60.60ms
step:896/2330 train_time:54301ms step_avg:60.60ms
step:897/2330 train_time:54360ms step_avg:60.60ms
step:898/2330 train_time:54422ms step_avg:60.60ms
step:899/2330 train_time:54482ms step_avg:60.60ms
step:900/2330 train_time:54545ms step_avg:60.61ms
step:901/2330 train_time:54606ms step_avg:60.61ms
step:902/2330 train_time:54668ms step_avg:60.61ms
step:903/2330 train_time:54727ms step_avg:60.61ms
step:904/2330 train_time:54790ms step_avg:60.61ms
step:905/2330 train_time:54849ms step_avg:60.61ms
step:906/2330 train_time:54912ms step_avg:60.61ms
step:907/2330 train_time:54971ms step_avg:60.61ms
step:908/2330 train_time:55034ms step_avg:60.61ms
step:909/2330 train_time:55093ms step_avg:60.61ms
step:910/2330 train_time:55157ms step_avg:60.61ms
step:911/2330 train_time:55217ms step_avg:60.61ms
step:912/2330 train_time:55280ms step_avg:60.61ms
step:913/2330 train_time:55339ms step_avg:60.61ms
step:914/2330 train_time:55402ms step_avg:60.61ms
step:915/2330 train_time:55462ms step_avg:60.61ms
step:916/2330 train_time:55524ms step_avg:60.62ms
step:917/2330 train_time:55584ms step_avg:60.62ms
step:918/2330 train_time:55647ms step_avg:60.62ms
step:919/2330 train_time:55707ms step_avg:60.62ms
step:920/2330 train_time:55769ms step_avg:60.62ms
step:921/2330 train_time:55829ms step_avg:60.62ms
step:922/2330 train_time:55891ms step_avg:60.62ms
step:923/2330 train_time:55950ms step_avg:60.62ms
step:924/2330 train_time:56012ms step_avg:60.62ms
step:925/2330 train_time:56073ms step_avg:60.62ms
step:926/2330 train_time:56136ms step_avg:60.62ms
step:927/2330 train_time:56196ms step_avg:60.62ms
step:928/2330 train_time:56258ms step_avg:60.62ms
step:929/2330 train_time:56318ms step_avg:60.62ms
step:930/2330 train_time:56380ms step_avg:60.62ms
step:931/2330 train_time:56439ms step_avg:60.62ms
step:932/2330 train_time:56502ms step_avg:60.62ms
step:933/2330 train_time:56561ms step_avg:60.62ms
step:934/2330 train_time:56625ms step_avg:60.63ms
step:935/2330 train_time:56685ms step_avg:60.63ms
step:936/2330 train_time:56748ms step_avg:60.63ms
step:937/2330 train_time:56807ms step_avg:60.63ms
step:938/2330 train_time:56870ms step_avg:60.63ms
step:939/2330 train_time:56930ms step_avg:60.63ms
step:940/2330 train_time:56993ms step_avg:60.63ms
step:941/2330 train_time:57052ms step_avg:60.63ms
step:942/2330 train_time:57114ms step_avg:60.63ms
step:943/2330 train_time:57174ms step_avg:60.63ms
step:944/2330 train_time:57237ms step_avg:60.63ms
step:945/2330 train_time:57297ms step_avg:60.63ms
step:946/2330 train_time:57359ms step_avg:60.63ms
step:947/2330 train_time:57420ms step_avg:60.63ms
step:948/2330 train_time:57482ms step_avg:60.63ms
step:949/2330 train_time:57542ms step_avg:60.63ms
step:950/2330 train_time:57604ms step_avg:60.64ms
step:951/2330 train_time:57664ms step_avg:60.63ms
step:952/2330 train_time:57726ms step_avg:60.64ms
step:953/2330 train_time:57786ms step_avg:60.64ms
step:954/2330 train_time:57849ms step_avg:60.64ms
step:955/2330 train_time:57908ms step_avg:60.64ms
step:956/2330 train_time:57971ms step_avg:60.64ms
step:957/2330 train_time:58031ms step_avg:60.64ms
step:958/2330 train_time:58094ms step_avg:60.64ms
step:959/2330 train_time:58154ms step_avg:60.64ms
step:960/2330 train_time:58217ms step_avg:60.64ms
step:961/2330 train_time:58276ms step_avg:60.64ms
step:962/2330 train_time:58339ms step_avg:60.64ms
step:963/2330 train_time:58399ms step_avg:60.64ms
step:964/2330 train_time:58462ms step_avg:60.65ms
step:965/2330 train_time:58521ms step_avg:60.64ms
step:966/2330 train_time:58583ms step_avg:60.65ms
step:967/2330 train_time:58642ms step_avg:60.64ms
step:968/2330 train_time:58705ms step_avg:60.65ms
step:969/2330 train_time:58765ms step_avg:60.65ms
step:970/2330 train_time:58828ms step_avg:60.65ms
step:971/2330 train_time:58888ms step_avg:60.65ms
step:972/2330 train_time:58951ms step_avg:60.65ms
step:973/2330 train_time:59010ms step_avg:60.65ms
step:974/2330 train_time:59072ms step_avg:60.65ms
step:975/2330 train_time:59132ms step_avg:60.65ms
step:976/2330 train_time:59194ms step_avg:60.65ms
step:977/2330 train_time:59253ms step_avg:60.65ms
step:978/2330 train_time:59317ms step_avg:60.65ms
step:979/2330 train_time:59376ms step_avg:60.65ms
step:980/2330 train_time:59439ms step_avg:60.65ms
step:981/2330 train_time:59498ms step_avg:60.65ms
step:982/2330 train_time:59561ms step_avg:60.65ms
step:983/2330 train_time:59620ms step_avg:60.65ms
step:984/2330 train_time:59683ms step_avg:60.65ms
step:985/2330 train_time:59743ms step_avg:60.65ms
step:986/2330 train_time:59805ms step_avg:60.65ms
step:987/2330 train_time:59866ms step_avg:60.65ms
step:988/2330 train_time:59928ms step_avg:60.66ms
step:989/2330 train_time:59988ms step_avg:60.65ms
step:990/2330 train_time:60051ms step_avg:60.66ms
step:991/2330 train_time:60109ms step_avg:60.65ms
step:992/2330 train_time:60172ms step_avg:60.66ms
step:993/2330 train_time:60232ms step_avg:60.66ms
step:994/2330 train_time:60294ms step_avg:60.66ms
step:995/2330 train_time:60355ms step_avg:60.66ms
step:996/2330 train_time:60417ms step_avg:60.66ms
step:997/2330 train_time:60477ms step_avg:60.66ms
step:998/2330 train_time:60540ms step_avg:60.66ms
step:999/2330 train_time:60599ms step_avg:60.66ms
step:1000/2330 train_time:60662ms step_avg:60.66ms
step:1000/2330 val_loss:3.8241 train_time:60735ms step_avg:60.74ms
step:1001/2330 train_time:60756ms step_avg:60.70ms
step:1002/2330 train_time:60786ms step_avg:60.66ms
step:1003/2330 train_time:60848ms step_avg:60.67ms
step:1004/2330 train_time:60918ms step_avg:60.67ms
step:1005/2330 train_time:60979ms step_avg:60.68ms
step:1006/2330 train_time:61042ms step_avg:60.68ms
step:1007/2330 train_time:61102ms step_avg:60.68ms
step:1008/2330 train_time:61165ms step_avg:60.68ms
step:1009/2330 train_time:61224ms step_avg:60.68ms
step:1010/2330 train_time:61286ms step_avg:60.68ms
step:1011/2330 train_time:61345ms step_avg:60.68ms
step:1012/2330 train_time:61407ms step_avg:60.68ms
step:1013/2330 train_time:61466ms step_avg:60.68ms
step:1014/2330 train_time:61528ms step_avg:60.68ms
step:1015/2330 train_time:61586ms step_avg:60.68ms
step:1016/2330 train_time:61650ms step_avg:60.68ms
step:1017/2330 train_time:61710ms step_avg:60.68ms
step:1018/2330 train_time:61772ms step_avg:60.68ms
step:1019/2330 train_time:61833ms step_avg:60.68ms
step:1020/2330 train_time:61896ms step_avg:60.68ms
step:1021/2330 train_time:61956ms step_avg:60.68ms
step:1022/2330 train_time:62019ms step_avg:60.68ms
step:1023/2330 train_time:62078ms step_avg:60.68ms
step:1024/2330 train_time:62141ms step_avg:60.68ms
step:1025/2330 train_time:62201ms step_avg:60.68ms
step:1026/2330 train_time:62264ms step_avg:60.69ms
step:1027/2330 train_time:62323ms step_avg:60.68ms
step:1028/2330 train_time:62385ms step_avg:60.69ms
step:1029/2330 train_time:62444ms step_avg:60.68ms
step:1030/2330 train_time:62506ms step_avg:60.69ms
step:1031/2330 train_time:62566ms step_avg:60.68ms
step:1032/2330 train_time:62629ms step_avg:60.69ms
step:1033/2330 train_time:62688ms step_avg:60.69ms
step:1034/2330 train_time:62751ms step_avg:60.69ms
step:1035/2330 train_time:62811ms step_avg:60.69ms
step:1036/2330 train_time:62874ms step_avg:60.69ms
step:1037/2330 train_time:62933ms step_avg:60.69ms
step:1038/2330 train_time:62995ms step_avg:60.69ms
step:1039/2330 train_time:63055ms step_avg:60.69ms
step:1040/2330 train_time:63118ms step_avg:60.69ms
step:1041/2330 train_time:63177ms step_avg:60.69ms
step:1042/2330 train_time:63240ms step_avg:60.69ms
step:1043/2330 train_time:63300ms step_avg:60.69ms
step:1044/2330 train_time:63362ms step_avg:60.69ms
step:1045/2330 train_time:63421ms step_avg:60.69ms
step:1046/2330 train_time:63484ms step_avg:60.69ms
step:1047/2330 train_time:63543ms step_avg:60.69ms
step:1048/2330 train_time:63606ms step_avg:60.69ms
step:1049/2330 train_time:63667ms step_avg:60.69ms
step:1050/2330 train_time:63730ms step_avg:60.70ms
step:1051/2330 train_time:63790ms step_avg:60.69ms
step:1052/2330 train_time:63853ms step_avg:60.70ms
step:1053/2330 train_time:63912ms step_avg:60.70ms
step:1054/2330 train_time:63976ms step_avg:60.70ms
step:1055/2330 train_time:64034ms step_avg:60.70ms
step:1056/2330 train_time:64097ms step_avg:60.70ms
step:1057/2330 train_time:64156ms step_avg:60.70ms
step:1058/2330 train_time:64219ms step_avg:60.70ms
step:1059/2330 train_time:64278ms step_avg:60.70ms
step:1060/2330 train_time:64341ms step_avg:60.70ms
step:1061/2330 train_time:64401ms step_avg:60.70ms
step:1062/2330 train_time:64464ms step_avg:60.70ms
step:1063/2330 train_time:64523ms step_avg:60.70ms
step:1064/2330 train_time:64586ms step_avg:60.70ms
step:1065/2330 train_time:64646ms step_avg:60.70ms
step:1066/2330 train_time:64709ms step_avg:60.70ms
step:1067/2330 train_time:64770ms step_avg:60.70ms
step:1068/2330 train_time:64833ms step_avg:60.70ms
step:1069/2330 train_time:64893ms step_avg:60.70ms
step:1070/2330 train_time:64956ms step_avg:60.71ms
step:1071/2330 train_time:65015ms step_avg:60.70ms
step:1072/2330 train_time:65077ms step_avg:60.71ms
step:1073/2330 train_time:65137ms step_avg:60.71ms
step:1074/2330 train_time:65199ms step_avg:60.71ms
step:1075/2330 train_time:65258ms step_avg:60.70ms
step:1076/2330 train_time:65320ms step_avg:60.71ms
step:1077/2330 train_time:65380ms step_avg:60.71ms
step:1078/2330 train_time:65442ms step_avg:60.71ms
step:1079/2330 train_time:65502ms step_avg:60.71ms
step:1080/2330 train_time:65564ms step_avg:60.71ms
step:1081/2330 train_time:65624ms step_avg:60.71ms
step:1082/2330 train_time:65687ms step_avg:60.71ms
step:1083/2330 train_time:65747ms step_avg:60.71ms
step:1084/2330 train_time:65810ms step_avg:60.71ms
step:1085/2330 train_time:65870ms step_avg:60.71ms
step:1086/2330 train_time:65933ms step_avg:60.71ms
step:1087/2330 train_time:65992ms step_avg:60.71ms
step:1088/2330 train_time:66055ms step_avg:60.71ms
step:1089/2330 train_time:66114ms step_avg:60.71ms
step:1090/2330 train_time:66177ms step_avg:60.71ms
step:1091/2330 train_time:66237ms step_avg:60.71ms
step:1092/2330 train_time:66299ms step_avg:60.71ms
step:1093/2330 train_time:66359ms step_avg:60.71ms
step:1094/2330 train_time:66422ms step_avg:60.71ms
step:1095/2330 train_time:66481ms step_avg:60.71ms
step:1096/2330 train_time:66544ms step_avg:60.72ms
step:1097/2330 train_time:66603ms step_avg:60.71ms
step:1098/2330 train_time:66666ms step_avg:60.72ms
step:1099/2330 train_time:66726ms step_avg:60.72ms
step:1100/2330 train_time:66789ms step_avg:60.72ms
step:1101/2330 train_time:66849ms step_avg:60.72ms
step:1102/2330 train_time:66912ms step_avg:60.72ms
step:1103/2330 train_time:66972ms step_avg:60.72ms
step:1104/2330 train_time:67034ms step_avg:60.72ms
step:1105/2330 train_time:67095ms step_avg:60.72ms
step:1106/2330 train_time:67157ms step_avg:60.72ms
step:1107/2330 train_time:67216ms step_avg:60.72ms
step:1108/2330 train_time:67279ms step_avg:60.72ms
step:1109/2330 train_time:67338ms step_avg:60.72ms
step:1110/2330 train_time:67401ms step_avg:60.72ms
step:1111/2330 train_time:67460ms step_avg:60.72ms
step:1112/2330 train_time:67522ms step_avg:60.72ms
step:1113/2330 train_time:67581ms step_avg:60.72ms
step:1114/2330 train_time:67643ms step_avg:60.72ms
step:1115/2330 train_time:67704ms step_avg:60.72ms
step:1116/2330 train_time:67767ms step_avg:60.72ms
step:1117/2330 train_time:67827ms step_avg:60.72ms
step:1118/2330 train_time:67890ms step_avg:60.72ms
step:1119/2330 train_time:67951ms step_avg:60.72ms
step:1120/2330 train_time:68014ms step_avg:60.73ms
step:1121/2330 train_time:68073ms step_avg:60.73ms
step:1122/2330 train_time:68135ms step_avg:60.73ms
step:1123/2330 train_time:68194ms step_avg:60.72ms
step:1124/2330 train_time:68257ms step_avg:60.73ms
step:1125/2330 train_time:68316ms step_avg:60.73ms
step:1126/2330 train_time:68380ms step_avg:60.73ms
step:1127/2330 train_time:68440ms step_avg:60.73ms
step:1128/2330 train_time:68502ms step_avg:60.73ms
step:1129/2330 train_time:68561ms step_avg:60.73ms
step:1130/2330 train_time:68624ms step_avg:60.73ms
step:1131/2330 train_time:68684ms step_avg:60.73ms
step:1132/2330 train_time:68746ms step_avg:60.73ms
step:1133/2330 train_time:68807ms step_avg:60.73ms
step:1134/2330 train_time:68872ms step_avg:60.73ms
step:1135/2330 train_time:68931ms step_avg:60.73ms
step:1136/2330 train_time:68993ms step_avg:60.73ms
step:1137/2330 train_time:69052ms step_avg:60.73ms
step:1138/2330 train_time:69116ms step_avg:60.73ms
step:1139/2330 train_time:69176ms step_avg:60.73ms
step:1140/2330 train_time:69238ms step_avg:60.74ms
step:1141/2330 train_time:69298ms step_avg:60.73ms
step:1142/2330 train_time:69361ms step_avg:60.74ms
step:1143/2330 train_time:69421ms step_avg:60.74ms
step:1144/2330 train_time:69483ms step_avg:60.74ms
step:1145/2330 train_time:69542ms step_avg:60.74ms
step:1146/2330 train_time:69604ms step_avg:60.74ms
step:1147/2330 train_time:69664ms step_avg:60.74ms
step:1148/2330 train_time:69727ms step_avg:60.74ms
step:1149/2330 train_time:69787ms step_avg:60.74ms
step:1150/2330 train_time:69850ms step_avg:60.74ms
step:1151/2330 train_time:69910ms step_avg:60.74ms
step:1152/2330 train_time:69973ms step_avg:60.74ms
step:1153/2330 train_time:70033ms step_avg:60.74ms
step:1154/2330 train_time:70096ms step_avg:60.74ms
step:1155/2330 train_time:70155ms step_avg:60.74ms
step:1156/2330 train_time:70218ms step_avg:60.74ms
step:1157/2330 train_time:70277ms step_avg:60.74ms
step:1158/2330 train_time:70340ms step_avg:60.74ms
step:1159/2330 train_time:70399ms step_avg:60.74ms
step:1160/2330 train_time:70461ms step_avg:60.74ms
step:1161/2330 train_time:70520ms step_avg:60.74ms
step:1162/2330 train_time:70583ms step_avg:60.74ms
step:1163/2330 train_time:70643ms step_avg:60.74ms
step:1164/2330 train_time:70707ms step_avg:60.74ms
step:1165/2330 train_time:70767ms step_avg:60.74ms
step:1166/2330 train_time:70830ms step_avg:60.75ms
step:1167/2330 train_time:70891ms step_avg:60.75ms
step:1168/2330 train_time:70953ms step_avg:60.75ms
step:1169/2330 train_time:71012ms step_avg:60.75ms
step:1170/2330 train_time:71075ms step_avg:60.75ms
step:1171/2330 train_time:71133ms step_avg:60.75ms
step:1172/2330 train_time:71196ms step_avg:60.75ms
step:1173/2330 train_time:71255ms step_avg:60.75ms
step:1174/2330 train_time:71317ms step_avg:60.75ms
step:1175/2330 train_time:71377ms step_avg:60.75ms
step:1176/2330 train_time:71440ms step_avg:60.75ms
step:1177/2330 train_time:71499ms step_avg:60.75ms
step:1178/2330 train_time:71561ms step_avg:60.75ms
step:1179/2330 train_time:71621ms step_avg:60.75ms
step:1180/2330 train_time:71684ms step_avg:60.75ms
step:1181/2330 train_time:71744ms step_avg:60.75ms
step:1182/2330 train_time:71807ms step_avg:60.75ms
step:1183/2330 train_time:71867ms step_avg:60.75ms
step:1184/2330 train_time:71930ms step_avg:60.75ms
step:1185/2330 train_time:71990ms step_avg:60.75ms
step:1186/2330 train_time:72053ms step_avg:60.75ms
step:1187/2330 train_time:72112ms step_avg:60.75ms
step:1188/2330 train_time:72176ms step_avg:60.75ms
step:1189/2330 train_time:72235ms step_avg:60.75ms
step:1190/2330 train_time:72297ms step_avg:60.75ms
step:1191/2330 train_time:72357ms step_avg:60.75ms
step:1192/2330 train_time:72420ms step_avg:60.76ms
step:1193/2330 train_time:72480ms step_avg:60.75ms
step:1194/2330 train_time:72542ms step_avg:60.76ms
step:1195/2330 train_time:72601ms step_avg:60.75ms
step:1196/2330 train_time:72664ms step_avg:60.76ms
step:1197/2330 train_time:72724ms step_avg:60.76ms
step:1198/2330 train_time:72788ms step_avg:60.76ms
step:1199/2330 train_time:72847ms step_avg:60.76ms
step:1200/2330 train_time:72910ms step_avg:60.76ms
step:1201/2330 train_time:72970ms step_avg:60.76ms
step:1202/2330 train_time:73033ms step_avg:60.76ms
step:1203/2330 train_time:73094ms step_avg:60.76ms
step:1204/2330 train_time:73156ms step_avg:60.76ms
step:1205/2330 train_time:73214ms step_avg:60.76ms
step:1206/2330 train_time:73277ms step_avg:60.76ms
step:1207/2330 train_time:73337ms step_avg:60.76ms
step:1208/2330 train_time:73400ms step_avg:60.76ms
step:1209/2330 train_time:73459ms step_avg:60.76ms
step:1210/2330 train_time:73522ms step_avg:60.76ms
step:1211/2330 train_time:73581ms step_avg:60.76ms
step:1212/2330 train_time:73644ms step_avg:60.76ms
step:1213/2330 train_time:73704ms step_avg:60.76ms
step:1214/2330 train_time:73767ms step_avg:60.76ms
step:1215/2330 train_time:73826ms step_avg:60.76ms
step:1216/2330 train_time:73889ms step_avg:60.76ms
step:1217/2330 train_time:73949ms step_avg:60.76ms
step:1218/2330 train_time:74012ms step_avg:60.77ms
step:1219/2330 train_time:74073ms step_avg:60.77ms
step:1220/2330 train_time:74134ms step_avg:60.77ms
step:1221/2330 train_time:74194ms step_avg:60.76ms
step:1222/2330 train_time:74257ms step_avg:60.77ms
step:1223/2330 train_time:74316ms step_avg:60.76ms
step:1224/2330 train_time:74379ms step_avg:60.77ms
step:1225/2330 train_time:74438ms step_avg:60.77ms
step:1226/2330 train_time:74501ms step_avg:60.77ms
step:1227/2330 train_time:74561ms step_avg:60.77ms
step:1228/2330 train_time:74623ms step_avg:60.77ms
step:1229/2330 train_time:74683ms step_avg:60.77ms
step:1230/2330 train_time:74746ms step_avg:60.77ms
step:1231/2330 train_time:74806ms step_avg:60.77ms
step:1232/2330 train_time:74870ms step_avg:60.77ms
step:1233/2330 train_time:74929ms step_avg:60.77ms
step:1234/2330 train_time:74992ms step_avg:60.77ms
step:1235/2330 train_time:75051ms step_avg:60.77ms
step:1236/2330 train_time:75113ms step_avg:60.77ms
step:1237/2330 train_time:75174ms step_avg:60.77ms
step:1238/2330 train_time:75235ms step_avg:60.77ms
step:1239/2330 train_time:75295ms step_avg:60.77ms
step:1240/2330 train_time:75357ms step_avg:60.77ms
step:1241/2330 train_time:75416ms step_avg:60.77ms
step:1242/2330 train_time:75480ms step_avg:60.77ms
step:1243/2330 train_time:75540ms step_avg:60.77ms
step:1244/2330 train_time:75602ms step_avg:60.77ms
step:1245/2330 train_time:75663ms step_avg:60.77ms
step:1246/2330 train_time:75725ms step_avg:60.77ms
step:1247/2330 train_time:75785ms step_avg:60.77ms
step:1248/2330 train_time:75847ms step_avg:60.78ms
step:1249/2330 train_time:75908ms step_avg:60.77ms
step:1250/2330 train_time:75971ms step_avg:60.78ms
step:1250/2330 val_loss:3.7299 train_time:76044ms step_avg:60.84ms
step:1251/2330 train_time:76065ms step_avg:60.80ms
step:1252/2330 train_time:76095ms step_avg:60.78ms
step:1253/2330 train_time:76158ms step_avg:60.78ms
step:1254/2330 train_time:76226ms step_avg:60.79ms
step:1255/2330 train_time:76287ms step_avg:60.79ms
step:1256/2330 train_time:76350ms step_avg:60.79ms
step:1257/2330 train_time:76410ms step_avg:60.79ms
step:1258/2330 train_time:76473ms step_avg:60.79ms
step:1259/2330 train_time:76531ms step_avg:60.79ms
step:1260/2330 train_time:76593ms step_avg:60.79ms
step:1261/2330 train_time:76652ms step_avg:60.79ms
step:1262/2330 train_time:76713ms step_avg:60.79ms
step:1263/2330 train_time:76772ms step_avg:60.79ms
step:1264/2330 train_time:76834ms step_avg:60.79ms
step:1265/2330 train_time:76892ms step_avg:60.78ms
step:1266/2330 train_time:76954ms step_avg:60.79ms
step:1267/2330 train_time:77013ms step_avg:60.78ms
step:1268/2330 train_time:77078ms step_avg:60.79ms
step:1269/2330 train_time:77139ms step_avg:60.79ms
step:1270/2330 train_time:77204ms step_avg:60.79ms
step:1271/2330 train_time:77264ms step_avg:60.79ms
step:1272/2330 train_time:77328ms step_avg:60.79ms
step:1273/2330 train_time:77389ms step_avg:60.79ms
step:1274/2330 train_time:77452ms step_avg:60.79ms
step:1275/2330 train_time:77512ms step_avg:60.79ms
step:1276/2330 train_time:77573ms step_avg:60.79ms
step:1277/2330 train_time:77632ms step_avg:60.79ms
step:1278/2330 train_time:77694ms step_avg:60.79ms
step:1279/2330 train_time:77752ms step_avg:60.79ms
step:1280/2330 train_time:77813ms step_avg:60.79ms
step:1281/2330 train_time:77873ms step_avg:60.79ms
step:1282/2330 train_time:77935ms step_avg:60.79ms
step:1283/2330 train_time:77995ms step_avg:60.79ms
step:1284/2330 train_time:78058ms step_avg:60.79ms
step:1285/2330 train_time:78118ms step_avg:60.79ms
step:1286/2330 train_time:78182ms step_avg:60.79ms
step:1287/2330 train_time:78242ms step_avg:60.79ms
step:1288/2330 train_time:78306ms step_avg:60.80ms
step:1289/2330 train_time:78366ms step_avg:60.80ms
step:1290/2330 train_time:78429ms step_avg:60.80ms
step:1291/2330 train_time:78490ms step_avg:60.80ms
step:1292/2330 train_time:78552ms step_avg:60.80ms
step:1293/2330 train_time:78611ms step_avg:60.80ms
step:1294/2330 train_time:78673ms step_avg:60.80ms
step:1295/2330 train_time:78732ms step_avg:60.80ms
step:1296/2330 train_time:78795ms step_avg:60.80ms
step:1297/2330 train_time:78853ms step_avg:60.80ms
step:1298/2330 train_time:78916ms step_avg:60.80ms
step:1299/2330 train_time:78974ms step_avg:60.80ms
step:1300/2330 train_time:79037ms step_avg:60.80ms
step:1301/2330 train_time:79098ms step_avg:60.80ms
step:1302/2330 train_time:79160ms step_avg:60.80ms
step:1303/2330 train_time:79221ms step_avg:60.80ms
step:1304/2330 train_time:79284ms step_avg:60.80ms
step:1305/2330 train_time:79344ms step_avg:60.80ms
step:1306/2330 train_time:79408ms step_avg:60.80ms
step:1307/2330 train_time:79467ms step_avg:60.80ms
step:1308/2330 train_time:79531ms step_avg:60.80ms
step:1309/2330 train_time:79591ms step_avg:60.80ms
step:1310/2330 train_time:79654ms step_avg:60.80ms
step:1311/2330 train_time:79714ms step_avg:60.80ms
step:1312/2330 train_time:79776ms step_avg:60.80ms
step:1313/2330 train_time:79835ms step_avg:60.80ms
step:1314/2330 train_time:79897ms step_avg:60.80ms
step:1315/2330 train_time:79956ms step_avg:60.80ms
step:1316/2330 train_time:80018ms step_avg:60.80ms
step:1317/2330 train_time:80077ms step_avg:60.80ms
step:1318/2330 train_time:80141ms step_avg:60.80ms
step:1319/2330 train_time:80201ms step_avg:60.80ms
step:1320/2330 train_time:80263ms step_avg:60.81ms
step:1321/2330 train_time:80324ms step_avg:60.81ms
step:1322/2330 train_time:80387ms step_avg:60.81ms
step:1323/2330 train_time:80447ms step_avg:60.81ms
step:1324/2330 train_time:80510ms step_avg:60.81ms
step:1325/2330 train_time:80570ms step_avg:60.81ms
step:1326/2330 train_time:80633ms step_avg:60.81ms
step:1327/2330 train_time:80693ms step_avg:60.81ms
step:1328/2330 train_time:80755ms step_avg:60.81ms
step:1329/2330 train_time:80815ms step_avg:60.81ms
step:1330/2330 train_time:80876ms step_avg:60.81ms
step:1331/2330 train_time:80934ms step_avg:60.81ms
step:1332/2330 train_time:80997ms step_avg:60.81ms
step:1333/2330 train_time:81056ms step_avg:60.81ms
step:1334/2330 train_time:81118ms step_avg:60.81ms
step:1335/2330 train_time:81178ms step_avg:60.81ms
step:1336/2330 train_time:81241ms step_avg:60.81ms
step:1337/2330 train_time:81302ms step_avg:60.81ms
step:1338/2330 train_time:81365ms step_avg:60.81ms
step:1339/2330 train_time:81425ms step_avg:60.81ms
step:1340/2330 train_time:81489ms step_avg:60.81ms
step:1341/2330 train_time:81549ms step_avg:60.81ms
step:1342/2330 train_time:81611ms step_avg:60.81ms
step:1343/2330 train_time:81670ms step_avg:60.81ms
step:1344/2330 train_time:81734ms step_avg:60.81ms
step:1345/2330 train_time:81794ms step_avg:60.81ms
step:1346/2330 train_time:81857ms step_avg:60.81ms
step:1347/2330 train_time:81916ms step_avg:60.81ms
step:1348/2330 train_time:81978ms step_avg:60.81ms
step:1349/2330 train_time:82037ms step_avg:60.81ms
step:1350/2330 train_time:82100ms step_avg:60.82ms
step:1351/2330 train_time:82159ms step_avg:60.81ms
step:1352/2330 train_time:82222ms step_avg:60.82ms
step:1353/2330 train_time:82282ms step_avg:60.81ms
step:1354/2330 train_time:82344ms step_avg:60.82ms
step:1355/2330 train_time:82404ms step_avg:60.81ms
step:1356/2330 train_time:82466ms step_avg:60.82ms
step:1357/2330 train_time:82526ms step_avg:60.82ms
step:1358/2330 train_time:82589ms step_avg:60.82ms
step:1359/2330 train_time:82649ms step_avg:60.82ms
step:1360/2330 train_time:82712ms step_avg:60.82ms
step:1361/2330 train_time:82772ms step_avg:60.82ms
step:1362/2330 train_time:82835ms step_avg:60.82ms
step:1363/2330 train_time:82895ms step_avg:60.82ms
step:1364/2330 train_time:82958ms step_avg:60.82ms
step:1365/2330 train_time:83017ms step_avg:60.82ms
step:1366/2330 train_time:83079ms step_avg:60.82ms
step:1367/2330 train_time:83138ms step_avg:60.82ms
step:1368/2330 train_time:83201ms step_avg:60.82ms
step:1369/2330 train_time:83260ms step_avg:60.82ms
step:1370/2330 train_time:83323ms step_avg:60.82ms
step:1371/2330 train_time:83382ms step_avg:60.82ms
step:1372/2330 train_time:83446ms step_avg:60.82ms
step:1373/2330 train_time:83505ms step_avg:60.82ms
step:1374/2330 train_time:83568ms step_avg:60.82ms
step:1375/2330 train_time:83629ms step_avg:60.82ms
step:1376/2330 train_time:83691ms step_avg:60.82ms
step:1377/2330 train_time:83751ms step_avg:60.82ms
step:1378/2330 train_time:83813ms step_avg:60.82ms
step:1379/2330 train_time:83873ms step_avg:60.82ms
step:1380/2330 train_time:83936ms step_avg:60.82ms
step:1381/2330 train_time:83996ms step_avg:60.82ms
step:1382/2330 train_time:84058ms step_avg:60.82ms
step:1383/2330 train_time:84118ms step_avg:60.82ms
step:1384/2330 train_time:84181ms step_avg:60.82ms
step:1385/2330 train_time:84240ms step_avg:60.82ms
step:1386/2330 train_time:84303ms step_avg:60.82ms
step:1387/2330 train_time:84362ms step_avg:60.82ms
step:1388/2330 train_time:84424ms step_avg:60.82ms
step:1389/2330 train_time:84483ms step_avg:60.82ms
step:1390/2330 train_time:84546ms step_avg:60.82ms
step:1391/2330 train_time:84606ms step_avg:60.82ms
step:1392/2330 train_time:84669ms step_avg:60.83ms
step:1393/2330 train_time:84729ms step_avg:60.82ms
step:1394/2330 train_time:84792ms step_avg:60.83ms
step:1395/2330 train_time:84852ms step_avg:60.83ms
step:1396/2330 train_time:84915ms step_avg:60.83ms
step:1397/2330 train_time:84975ms step_avg:60.83ms
step:1398/2330 train_time:85037ms step_avg:60.83ms
step:1399/2330 train_time:85098ms step_avg:60.83ms
step:1400/2330 train_time:85160ms step_avg:60.83ms
step:1401/2330 train_time:85219ms step_avg:60.83ms
step:1402/2330 train_time:85281ms step_avg:60.83ms
step:1403/2330 train_time:85341ms step_avg:60.83ms
step:1404/2330 train_time:85404ms step_avg:60.83ms
step:1405/2330 train_time:85463ms step_avg:60.83ms
step:1406/2330 train_time:85525ms step_avg:60.83ms
step:1407/2330 train_time:85585ms step_avg:60.83ms
step:1408/2330 train_time:85648ms step_avg:60.83ms
step:1409/2330 train_time:85707ms step_avg:60.83ms
step:1410/2330 train_time:85771ms step_avg:60.83ms
step:1411/2330 train_time:85831ms step_avg:60.83ms
step:1412/2330 train_time:85895ms step_avg:60.83ms
step:1413/2330 train_time:85955ms step_avg:60.83ms
step:1414/2330 train_time:86017ms step_avg:60.83ms
step:1415/2330 train_time:86077ms step_avg:60.83ms
step:1416/2330 train_time:86138ms step_avg:60.83ms
step:1417/2330 train_time:86198ms step_avg:60.83ms
step:1418/2330 train_time:86260ms step_avg:60.83ms
step:1419/2330 train_time:86320ms step_avg:60.83ms
step:1420/2330 train_time:86383ms step_avg:60.83ms
step:1421/2330 train_time:86442ms step_avg:60.83ms
step:1422/2330 train_time:86505ms step_avg:60.83ms
step:1423/2330 train_time:86565ms step_avg:60.83ms
step:1424/2330 train_time:86627ms step_avg:60.83ms
step:1425/2330 train_time:86687ms step_avg:60.83ms
step:1426/2330 train_time:86750ms step_avg:60.83ms
step:1427/2330 train_time:86811ms step_avg:60.83ms
step:1428/2330 train_time:86875ms step_avg:60.84ms
step:1429/2330 train_time:86934ms step_avg:60.84ms
step:1430/2330 train_time:86997ms step_avg:60.84ms
step:1431/2330 train_time:87056ms step_avg:60.84ms
step:1432/2330 train_time:87118ms step_avg:60.84ms
step:1433/2330 train_time:87177ms step_avg:60.84ms
step:1434/2330 train_time:87239ms step_avg:60.84ms
step:1435/2330 train_time:87299ms step_avg:60.84ms
step:1436/2330 train_time:87362ms step_avg:60.84ms
step:1437/2330 train_time:87421ms step_avg:60.84ms
step:1438/2330 train_time:87484ms step_avg:60.84ms
step:1439/2330 train_time:87543ms step_avg:60.84ms
step:1440/2330 train_time:87606ms step_avg:60.84ms
step:1441/2330 train_time:87667ms step_avg:60.84ms
step:1442/2330 train_time:87729ms step_avg:60.84ms
step:1443/2330 train_time:87790ms step_avg:60.84ms
step:1444/2330 train_time:87853ms step_avg:60.84ms
step:1445/2330 train_time:87913ms step_avg:60.84ms
step:1446/2330 train_time:87976ms step_avg:60.84ms
step:1447/2330 train_time:88036ms step_avg:60.84ms
step:1448/2330 train_time:88100ms step_avg:60.84ms
step:1449/2330 train_time:88158ms step_avg:60.84ms
step:1450/2330 train_time:88221ms step_avg:60.84ms
step:1451/2330 train_time:88279ms step_avg:60.84ms
step:1452/2330 train_time:88342ms step_avg:60.84ms
step:1453/2330 train_time:88401ms step_avg:60.84ms
step:1454/2330 train_time:88463ms step_avg:60.84ms
step:1455/2330 train_time:88523ms step_avg:60.84ms
step:1456/2330 train_time:88586ms step_avg:60.84ms
step:1457/2330 train_time:88645ms step_avg:60.84ms
step:1458/2330 train_time:88708ms step_avg:60.84ms
step:1459/2330 train_time:88768ms step_avg:60.84ms
step:1460/2330 train_time:88831ms step_avg:60.84ms
step:1461/2330 train_time:88891ms step_avg:60.84ms
step:1462/2330 train_time:88953ms step_avg:60.84ms
step:1463/2330 train_time:89013ms step_avg:60.84ms
step:1464/2330 train_time:89075ms step_avg:60.84ms
step:1465/2330 train_time:89135ms step_avg:60.84ms
step:1466/2330 train_time:89197ms step_avg:60.84ms
step:1467/2330 train_time:89256ms step_avg:60.84ms
step:1468/2330 train_time:89318ms step_avg:60.84ms
step:1469/2330 train_time:89377ms step_avg:60.84ms
step:1470/2330 train_time:89440ms step_avg:60.84ms
step:1471/2330 train_time:89501ms step_avg:60.84ms
step:1472/2330 train_time:89563ms step_avg:60.84ms
step:1473/2330 train_time:89623ms step_avg:60.84ms
step:1474/2330 train_time:89686ms step_avg:60.85ms
step:1475/2330 train_time:89746ms step_avg:60.84ms
step:1476/2330 train_time:89809ms step_avg:60.85ms
step:1477/2330 train_time:89868ms step_avg:60.84ms
step:1478/2330 train_time:89933ms step_avg:60.85ms
step:1479/2330 train_time:89993ms step_avg:60.85ms
step:1480/2330 train_time:90055ms step_avg:60.85ms
step:1481/2330 train_time:90116ms step_avg:60.85ms
step:1482/2330 train_time:90178ms step_avg:60.85ms
step:1483/2330 train_time:90237ms step_avg:60.85ms
step:1484/2330 train_time:90300ms step_avg:60.85ms
step:1485/2330 train_time:90359ms step_avg:60.85ms
step:1486/2330 train_time:90422ms step_avg:60.85ms
step:1487/2330 train_time:90481ms step_avg:60.85ms
step:1488/2330 train_time:90543ms step_avg:60.85ms
step:1489/2330 train_time:90602ms step_avg:60.85ms
step:1490/2330 train_time:90664ms step_avg:60.85ms
step:1491/2330 train_time:90724ms step_avg:60.85ms
step:1492/2330 train_time:90787ms step_avg:60.85ms
step:1493/2330 train_time:90847ms step_avg:60.85ms
step:1494/2330 train_time:90910ms step_avg:60.85ms
step:1495/2330 train_time:90971ms step_avg:60.85ms
step:1496/2330 train_time:91034ms step_avg:60.85ms
step:1497/2330 train_time:91094ms step_avg:60.85ms
step:1498/2330 train_time:91156ms step_avg:60.85ms
step:1499/2330 train_time:91216ms step_avg:60.85ms
step:1500/2330 train_time:91278ms step_avg:60.85ms
step:1500/2330 val_loss:3.6596 train_time:91351ms step_avg:60.90ms
step:1501/2330 train_time:91373ms step_avg:60.87ms
step:1502/2330 train_time:91402ms step_avg:60.85ms
step:1503/2330 train_time:91464ms step_avg:60.85ms
step:1504/2330 train_time:91532ms step_avg:60.86ms
step:1505/2330 train_time:91594ms step_avg:60.86ms
step:1506/2330 train_time:91657ms step_avg:60.86ms
step:1507/2330 train_time:91716ms step_avg:60.86ms
step:1508/2330 train_time:91778ms step_avg:60.86ms
step:1509/2330 train_time:91836ms step_avg:60.86ms
step:1510/2330 train_time:91898ms step_avg:60.86ms
step:1511/2330 train_time:91956ms step_avg:60.86ms
step:1512/2330 train_time:92019ms step_avg:60.86ms
step:1513/2330 train_time:92077ms step_avg:60.86ms
step:1514/2330 train_time:92140ms step_avg:60.86ms
step:1515/2330 train_time:92200ms step_avg:60.86ms
step:1516/2330 train_time:92262ms step_avg:60.86ms
step:1517/2330 train_time:92321ms step_avg:60.86ms
step:1518/2330 train_time:92385ms step_avg:60.86ms
step:1519/2330 train_time:92446ms step_avg:60.86ms
step:1520/2330 train_time:92510ms step_avg:60.86ms
step:1521/2330 train_time:92570ms step_avg:60.86ms
step:1522/2330 train_time:92634ms step_avg:60.86ms
step:1523/2330 train_time:92694ms step_avg:60.86ms
step:1524/2330 train_time:92757ms step_avg:60.86ms
step:1525/2330 train_time:92817ms step_avg:60.86ms
step:1526/2330 train_time:92878ms step_avg:60.86ms
step:1527/2330 train_time:92936ms step_avg:60.86ms
step:1528/2330 train_time:92998ms step_avg:60.86ms
step:1529/2330 train_time:93058ms step_avg:60.86ms
step:1530/2330 train_time:93120ms step_avg:60.86ms
step:1531/2330 train_time:93179ms step_avg:60.86ms
step:1532/2330 train_time:93242ms step_avg:60.86ms
step:1533/2330 train_time:93303ms step_avg:60.86ms
step:1534/2330 train_time:93366ms step_avg:60.86ms
step:1535/2330 train_time:93426ms step_avg:60.86ms
step:1536/2330 train_time:93490ms step_avg:60.87ms
step:1537/2330 train_time:93551ms step_avg:60.87ms
step:1538/2330 train_time:93615ms step_avg:60.87ms
step:1539/2330 train_time:93675ms step_avg:60.87ms
step:1540/2330 train_time:93738ms step_avg:60.87ms
step:1541/2330 train_time:93798ms step_avg:60.87ms
step:1542/2330 train_time:93861ms step_avg:60.87ms
step:1543/2330 train_time:93920ms step_avg:60.87ms
step:1544/2330 train_time:93982ms step_avg:60.87ms
step:1545/2330 train_time:94041ms step_avg:60.87ms
step:1546/2330 train_time:94104ms step_avg:60.87ms
step:1547/2330 train_time:94163ms step_avg:60.87ms
step:1548/2330 train_time:94227ms step_avg:60.87ms
step:1549/2330 train_time:94286ms step_avg:60.87ms
step:1550/2330 train_time:94350ms step_avg:60.87ms
step:1551/2330 train_time:94411ms step_avg:60.87ms
step:1552/2330 train_time:94474ms step_avg:60.87ms
step:1553/2330 train_time:94534ms step_avg:60.87ms
step:1554/2330 train_time:94598ms step_avg:60.87ms
step:1555/2330 train_time:94658ms step_avg:60.87ms
step:1556/2330 train_time:94721ms step_avg:60.87ms
step:1557/2330 train_time:94780ms step_avg:60.87ms
step:1558/2330 train_time:94844ms step_avg:60.88ms
step:1559/2330 train_time:94903ms step_avg:60.87ms
step:1560/2330 train_time:94965ms step_avg:60.88ms
step:1561/2330 train_time:95026ms step_avg:60.87ms
step:1562/2330 train_time:95088ms step_avg:60.88ms
step:1563/2330 train_time:95148ms step_avg:60.88ms
step:1564/2330 train_time:95211ms step_avg:60.88ms
step:1565/2330 train_time:95271ms step_avg:60.88ms
step:1566/2330 train_time:95334ms step_avg:60.88ms
step:1567/2330 train_time:95395ms step_avg:60.88ms
step:1568/2330 train_time:95457ms step_avg:60.88ms
step:1569/2330 train_time:95517ms step_avg:60.88ms
step:1570/2330 train_time:95581ms step_avg:60.88ms
step:1571/2330 train_time:95641ms step_avg:60.88ms
step:1572/2330 train_time:95704ms step_avg:60.88ms
step:1573/2330 train_time:95764ms step_avg:60.88ms
step:1574/2330 train_time:95826ms step_avg:60.88ms
step:1575/2330 train_time:95887ms step_avg:60.88ms
step:1576/2330 train_time:95950ms step_avg:60.88ms
step:1577/2330 train_time:96009ms step_avg:60.88ms
step:1578/2330 train_time:96073ms step_avg:60.88ms
step:1579/2330 train_time:96132ms step_avg:60.88ms
step:1580/2330 train_time:96195ms step_avg:60.88ms
step:1581/2330 train_time:96256ms step_avg:60.88ms
step:1582/2330 train_time:96319ms step_avg:60.88ms
step:1583/2330 train_time:96379ms step_avg:60.88ms
step:1584/2330 train_time:96442ms step_avg:60.88ms
step:1585/2330 train_time:96501ms step_avg:60.88ms
step:1586/2330 train_time:96564ms step_avg:60.89ms
step:1587/2330 train_time:96625ms step_avg:60.89ms
step:1588/2330 train_time:96688ms step_avg:60.89ms
step:1589/2330 train_time:96747ms step_avg:60.89ms
step:1590/2330 train_time:96811ms step_avg:60.89ms
step:1591/2330 train_time:96871ms step_avg:60.89ms
step:1592/2330 train_time:96934ms step_avg:60.89ms
step:1593/2330 train_time:96994ms step_avg:60.89ms
step:1594/2330 train_time:97057ms step_avg:60.89ms
step:1595/2330 train_time:97117ms step_avg:60.89ms
step:1596/2330 train_time:97179ms step_avg:60.89ms
step:1597/2330 train_time:97238ms step_avg:60.89ms
step:1598/2330 train_time:97302ms step_avg:60.89ms
step:1599/2330 train_time:97362ms step_avg:60.89ms
step:1600/2330 train_time:97425ms step_avg:60.89ms
step:1601/2330 train_time:97485ms step_avg:60.89ms
step:1602/2330 train_time:97549ms step_avg:60.89ms
step:1603/2330 train_time:97609ms step_avg:60.89ms
step:1604/2330 train_time:97671ms step_avg:60.89ms
step:1605/2330 train_time:97731ms step_avg:60.89ms
step:1606/2330 train_time:97795ms step_avg:60.89ms
step:1607/2330 train_time:97855ms step_avg:60.89ms
step:1608/2330 train_time:97919ms step_avg:60.89ms
step:1609/2330 train_time:97978ms step_avg:60.89ms
step:1610/2330 train_time:98040ms step_avg:60.89ms
step:1611/2330 train_time:98100ms step_avg:60.89ms
step:1612/2330 train_time:98163ms step_avg:60.90ms
step:1613/2330 train_time:98223ms step_avg:60.89ms
step:1614/2330 train_time:98285ms step_avg:60.90ms
step:1615/2330 train_time:98345ms step_avg:60.90ms
step:1616/2330 train_time:98409ms step_avg:60.90ms
step:1617/2330 train_time:98468ms step_avg:60.90ms
step:1618/2330 train_time:98532ms step_avg:60.90ms
step:1619/2330 train_time:98592ms step_avg:60.90ms
step:1620/2330 train_time:98656ms step_avg:60.90ms
step:1621/2330 train_time:98716ms step_avg:60.90ms
step:1622/2330 train_time:98779ms step_avg:60.90ms
step:1623/2330 train_time:98838ms step_avg:60.90ms
step:1624/2330 train_time:98901ms step_avg:60.90ms
step:1625/2330 train_time:98961ms step_avg:60.90ms
step:1626/2330 train_time:99024ms step_avg:60.90ms
step:1627/2330 train_time:99084ms step_avg:60.90ms
step:1628/2330 train_time:99146ms step_avg:60.90ms
step:1629/2330 train_time:99207ms step_avg:60.90ms
step:1630/2330 train_time:99270ms step_avg:60.90ms
step:1631/2330 train_time:99330ms step_avg:60.90ms
step:1632/2330 train_time:99394ms step_avg:60.90ms
step:1633/2330 train_time:99454ms step_avg:60.90ms
step:1634/2330 train_time:99518ms step_avg:60.90ms
step:1635/2330 train_time:99577ms step_avg:60.90ms
step:1636/2330 train_time:99641ms step_avg:60.91ms
step:1637/2330 train_time:99700ms step_avg:60.90ms
step:1638/2330 train_time:99764ms step_avg:60.91ms
step:1639/2330 train_time:99823ms step_avg:60.90ms
step:1640/2330 train_time:99886ms step_avg:60.91ms
step:1641/2330 train_time:99947ms step_avg:60.91ms
step:1642/2330 train_time:100010ms step_avg:60.91ms
step:1643/2330 train_time:100070ms step_avg:60.91ms
step:1644/2330 train_time:100133ms step_avg:60.91ms
step:1645/2330 train_time:100193ms step_avg:60.91ms
step:1646/2330 train_time:100256ms step_avg:60.91ms
step:1647/2330 train_time:100317ms step_avg:60.91ms
step:1648/2330 train_time:100380ms step_avg:60.91ms
step:1649/2330 train_time:100439ms step_avg:60.91ms
step:1650/2330 train_time:100502ms step_avg:60.91ms
step:1651/2330 train_time:100562ms step_avg:60.91ms
step:1652/2330 train_time:100626ms step_avg:60.91ms
step:1653/2330 train_time:100686ms step_avg:60.91ms
step:1654/2330 train_time:100749ms step_avg:60.91ms
step:1655/2330 train_time:100809ms step_avg:60.91ms
step:1656/2330 train_time:100872ms step_avg:60.91ms
step:1657/2330 train_time:100933ms step_avg:60.91ms
step:1658/2330 train_time:100995ms step_avg:60.91ms
step:1659/2330 train_time:101055ms step_avg:60.91ms
step:1660/2330 train_time:101119ms step_avg:60.91ms
step:1661/2330 train_time:101178ms step_avg:60.91ms
step:1662/2330 train_time:101241ms step_avg:60.91ms
step:1663/2330 train_time:101300ms step_avg:60.91ms
step:1664/2330 train_time:101363ms step_avg:60.91ms
step:1665/2330 train_time:101424ms step_avg:60.92ms
step:1666/2330 train_time:101486ms step_avg:60.92ms
step:1667/2330 train_time:101546ms step_avg:60.92ms
step:1668/2330 train_time:101609ms step_avg:60.92ms
step:1669/2330 train_time:101668ms step_avg:60.92ms
step:1670/2330 train_time:101732ms step_avg:60.92ms
step:1671/2330 train_time:101792ms step_avg:60.92ms
step:1672/2330 train_time:101856ms step_avg:60.92ms
step:1673/2330 train_time:101916ms step_avg:60.92ms
step:1674/2330 train_time:101978ms step_avg:60.92ms
step:1675/2330 train_time:102038ms step_avg:60.92ms
step:1676/2330 train_time:102101ms step_avg:60.92ms
step:1677/2330 train_time:102161ms step_avg:60.92ms
step:1678/2330 train_time:102224ms step_avg:60.92ms
step:1679/2330 train_time:102284ms step_avg:60.92ms
step:1680/2330 train_time:102346ms step_avg:60.92ms
step:1681/2330 train_time:102407ms step_avg:60.92ms
step:1682/2330 train_time:102470ms step_avg:60.92ms
step:1683/2330 train_time:102529ms step_avg:60.92ms
step:1684/2330 train_time:102592ms step_avg:60.92ms
step:1685/2330 train_time:102652ms step_avg:60.92ms
step:1686/2330 train_time:102715ms step_avg:60.92ms
step:1687/2330 train_time:102775ms step_avg:60.92ms
step:1688/2330 train_time:102839ms step_avg:60.92ms
step:1689/2330 train_time:102898ms step_avg:60.92ms
step:1690/2330 train_time:102962ms step_avg:60.92ms
step:1691/2330 train_time:103022ms step_avg:60.92ms
step:1692/2330 train_time:103085ms step_avg:60.92ms
step:1693/2330 train_time:103145ms step_avg:60.92ms
step:1694/2330 train_time:103208ms step_avg:60.93ms
step:1695/2330 train_time:103267ms step_avg:60.92ms
step:1696/2330 train_time:103331ms step_avg:60.93ms
step:1697/2330 train_time:103391ms step_avg:60.93ms
step:1698/2330 train_time:103454ms step_avg:60.93ms
step:1699/2330 train_time:103514ms step_avg:60.93ms
step:1700/2330 train_time:103576ms step_avg:60.93ms
step:1701/2330 train_time:103636ms step_avg:60.93ms
step:1702/2330 train_time:103699ms step_avg:60.93ms
step:1703/2330 train_time:103759ms step_avg:60.93ms
step:1704/2330 train_time:103822ms step_avg:60.93ms
step:1705/2330 train_time:103882ms step_avg:60.93ms
step:1706/2330 train_time:103944ms step_avg:60.93ms
step:1707/2330 train_time:104004ms step_avg:60.93ms
step:1708/2330 train_time:104068ms step_avg:60.93ms
step:1709/2330 train_time:104128ms step_avg:60.93ms
step:1710/2330 train_time:104190ms step_avg:60.93ms
step:1711/2330 train_time:104252ms step_avg:60.93ms
step:1712/2330 train_time:104315ms step_avg:60.93ms
step:1713/2330 train_time:104375ms step_avg:60.93ms
step:1714/2330 train_time:104438ms step_avg:60.93ms
step:1715/2330 train_time:104499ms step_avg:60.93ms
step:1716/2330 train_time:104560ms step_avg:60.93ms
step:1717/2330 train_time:104620ms step_avg:60.93ms
step:1718/2330 train_time:104684ms step_avg:60.93ms
step:1719/2330 train_time:104743ms step_avg:60.93ms
step:1720/2330 train_time:104807ms step_avg:60.93ms
step:1721/2330 train_time:104867ms step_avg:60.93ms
step:1722/2330 train_time:104930ms step_avg:60.94ms
step:1723/2330 train_time:104990ms step_avg:60.93ms
step:1724/2330 train_time:105053ms step_avg:60.94ms
step:1725/2330 train_time:105113ms step_avg:60.94ms
step:1726/2330 train_time:105177ms step_avg:60.94ms
step:1727/2330 train_time:105237ms step_avg:60.94ms
step:1728/2330 train_time:105299ms step_avg:60.94ms
step:1729/2330 train_time:105359ms step_avg:60.94ms
step:1730/2330 train_time:105422ms step_avg:60.94ms
step:1731/2330 train_time:105483ms step_avg:60.94ms
step:1732/2330 train_time:105546ms step_avg:60.94ms
step:1733/2330 train_time:105606ms step_avg:60.94ms
step:1734/2330 train_time:105669ms step_avg:60.94ms
step:1735/2330 train_time:105730ms step_avg:60.94ms
step:1736/2330 train_time:105793ms step_avg:60.94ms
step:1737/2330 train_time:105853ms step_avg:60.94ms
step:1738/2330 train_time:105917ms step_avg:60.94ms
step:1739/2330 train_time:105977ms step_avg:60.94ms
step:1740/2330 train_time:106039ms step_avg:60.94ms
step:1741/2330 train_time:106100ms step_avg:60.94ms
step:1742/2330 train_time:106162ms step_avg:60.94ms
step:1743/2330 train_time:106223ms step_avg:60.94ms
step:1744/2330 train_time:106286ms step_avg:60.94ms
step:1745/2330 train_time:106345ms step_avg:60.94ms
step:1746/2330 train_time:106409ms step_avg:60.94ms
step:1747/2330 train_time:106469ms step_avg:60.94ms
step:1748/2330 train_time:106533ms step_avg:60.95ms
step:1749/2330 train_time:106593ms step_avg:60.95ms
step:1750/2330 train_time:106657ms step_avg:60.95ms
step:1750/2330 val_loss:3.5757 train_time:106729ms step_avg:60.99ms
step:1751/2330 train_time:106751ms step_avg:60.97ms
step:1752/2330 train_time:106781ms step_avg:60.95ms
step:1753/2330 train_time:106847ms step_avg:60.95ms
step:1754/2330 train_time:106915ms step_avg:60.95ms
step:1755/2330 train_time:106976ms step_avg:60.95ms
step:1756/2330 train_time:107039ms step_avg:60.96ms
step:1757/2330 train_time:107098ms step_avg:60.96ms
step:1758/2330 train_time:107161ms step_avg:60.96ms
step:1759/2330 train_time:107220ms step_avg:60.95ms
step:1760/2330 train_time:107283ms step_avg:60.96ms
step:1761/2330 train_time:107341ms step_avg:60.95ms
step:1762/2330 train_time:107404ms step_avg:60.96ms
step:1763/2330 train_time:107462ms step_avg:60.95ms
step:1764/2330 train_time:107525ms step_avg:60.95ms
step:1765/2330 train_time:107584ms step_avg:60.95ms
step:1766/2330 train_time:107646ms step_avg:60.95ms
step:1767/2330 train_time:107707ms step_avg:60.95ms
step:1768/2330 train_time:107772ms step_avg:60.96ms
step:1769/2330 train_time:107833ms step_avg:60.96ms
step:1770/2330 train_time:107896ms step_avg:60.96ms
step:1771/2330 train_time:107956ms step_avg:60.96ms
step:1772/2330 train_time:108019ms step_avg:60.96ms
step:1773/2330 train_time:108079ms step_avg:60.96ms
step:1774/2330 train_time:108142ms step_avg:60.96ms
step:1775/2330 train_time:108201ms step_avg:60.96ms
step:1776/2330 train_time:108264ms step_avg:60.96ms
step:1777/2330 train_time:108324ms step_avg:60.96ms
step:1778/2330 train_time:108387ms step_avg:60.96ms
step:1779/2330 train_time:108446ms step_avg:60.96ms
step:1780/2330 train_time:108508ms step_avg:60.96ms
step:1781/2330 train_time:108568ms step_avg:60.96ms
step:1782/2330 train_time:108631ms step_avg:60.96ms
step:1783/2330 train_time:108690ms step_avg:60.96ms
step:1784/2330 train_time:108753ms step_avg:60.96ms
step:1785/2330 train_time:108814ms step_avg:60.96ms
step:1786/2330 train_time:108876ms step_avg:60.96ms
step:1787/2330 train_time:108937ms step_avg:60.96ms
step:1788/2330 train_time:109000ms step_avg:60.96ms
step:1789/2330 train_time:109060ms step_avg:60.96ms
step:1790/2330 train_time:109124ms step_avg:60.96ms
step:1791/2330 train_time:109183ms step_avg:60.96ms
step:1792/2330 train_time:109246ms step_avg:60.96ms
step:1793/2330 train_time:109306ms step_avg:60.96ms
step:1794/2330 train_time:109368ms step_avg:60.96ms
step:1795/2330 train_time:109427ms step_avg:60.96ms
step:1796/2330 train_time:109490ms step_avg:60.96ms
step:1797/2330 train_time:109550ms step_avg:60.96ms
step:1798/2330 train_time:109613ms step_avg:60.96ms
step:1799/2330 train_time:109673ms step_avg:60.96ms
step:1800/2330 train_time:109735ms step_avg:60.96ms
step:1801/2330 train_time:109795ms step_avg:60.96ms
step:1802/2330 train_time:109859ms step_avg:60.97ms
step:1803/2330 train_time:109919ms step_avg:60.96ms
step:1804/2330 train_time:109983ms step_avg:60.97ms
step:1805/2330 train_time:110043ms step_avg:60.97ms
step:1806/2330 train_time:110106ms step_avg:60.97ms
step:1807/2330 train_time:110166ms step_avg:60.97ms
step:1808/2330 train_time:110229ms step_avg:60.97ms
step:1809/2330 train_time:110289ms step_avg:60.97ms
step:1810/2330 train_time:110351ms step_avg:60.97ms
step:1811/2330 train_time:110411ms step_avg:60.97ms
step:1812/2330 train_time:110474ms step_avg:60.97ms
step:1813/2330 train_time:110533ms step_avg:60.97ms
step:1814/2330 train_time:110596ms step_avg:60.97ms
step:1815/2330 train_time:110656ms step_avg:60.97ms
step:1816/2330 train_time:110718ms step_avg:60.97ms
step:1817/2330 train_time:110778ms step_avg:60.97ms
step:1818/2330 train_time:110841ms step_avg:60.97ms
step:1819/2330 train_time:110901ms step_avg:60.97ms
step:1820/2330 train_time:110965ms step_avg:60.97ms
step:1821/2330 train_time:111025ms step_avg:60.97ms
step:1822/2330 train_time:111087ms step_avg:60.97ms
step:1823/2330 train_time:111147ms step_avg:60.97ms
step:1824/2330 train_time:111210ms step_avg:60.97ms
step:1825/2330 train_time:111269ms step_avg:60.97ms
step:1826/2330 train_time:111332ms step_avg:60.97ms
step:1827/2330 train_time:111392ms step_avg:60.97ms
step:1828/2330 train_time:111454ms step_avg:60.97ms
step:1829/2330 train_time:111514ms step_avg:60.97ms
step:1830/2330 train_time:111576ms step_avg:60.97ms
step:1831/2330 train_time:111636ms step_avg:60.97ms
step:1832/2330 train_time:111698ms step_avg:60.97ms
step:1833/2330 train_time:111758ms step_avg:60.97ms
step:1834/2330 train_time:111820ms step_avg:60.97ms
step:1835/2330 train_time:111881ms step_avg:60.97ms
step:1836/2330 train_time:111944ms step_avg:60.97ms
step:1837/2330 train_time:112004ms step_avg:60.97ms
step:1838/2330 train_time:112066ms step_avg:60.97ms
step:1839/2330 train_time:112127ms step_avg:60.97ms
step:1840/2330 train_time:112190ms step_avg:60.97ms
step:1841/2330 train_time:112249ms step_avg:60.97ms
step:1842/2330 train_time:112312ms step_avg:60.97ms
step:1843/2330 train_time:112372ms step_avg:60.97ms
step:1844/2330 train_time:112436ms step_avg:60.97ms
step:1845/2330 train_time:112495ms step_avg:60.97ms
step:1846/2330 train_time:112557ms step_avg:60.97ms
step:1847/2330 train_time:112618ms step_avg:60.97ms
step:1848/2330 train_time:112679ms step_avg:60.97ms
step:1849/2330 train_time:112739ms step_avg:60.97ms
step:1850/2330 train_time:112802ms step_avg:60.97ms
step:1851/2330 train_time:112861ms step_avg:60.97ms
step:1852/2330 train_time:112925ms step_avg:60.97ms
step:1853/2330 train_time:112985ms step_avg:60.97ms
step:1854/2330 train_time:113048ms step_avg:60.98ms
step:1855/2330 train_time:113107ms step_avg:60.97ms
step:1856/2330 train_time:113171ms step_avg:60.98ms
step:1857/2330 train_time:113231ms step_avg:60.98ms
step:1858/2330 train_time:113293ms step_avg:60.98ms
step:1859/2330 train_time:113353ms step_avg:60.98ms
step:1860/2330 train_time:113416ms step_avg:60.98ms
step:1861/2330 train_time:113476ms step_avg:60.98ms
step:1862/2330 train_time:113539ms step_avg:60.98ms
step:1863/2330 train_time:113598ms step_avg:60.98ms
step:1864/2330 train_time:113661ms step_avg:60.98ms
step:1865/2330 train_time:113720ms step_avg:60.98ms
step:1866/2330 train_time:113783ms step_avg:60.98ms
step:1867/2330 train_time:113843ms step_avg:60.98ms
step:1868/2330 train_time:113906ms step_avg:60.98ms
step:1869/2330 train_time:113965ms step_avg:60.98ms
step:1870/2330 train_time:114028ms step_avg:60.98ms
step:1871/2330 train_time:114088ms step_avg:60.98ms
step:1872/2330 train_time:114152ms step_avg:60.98ms
step:1873/2330 train_time:114211ms step_avg:60.98ms
step:1874/2330 train_time:114273ms step_avg:60.98ms
step:1875/2330 train_time:114334ms step_avg:60.98ms
step:1876/2330 train_time:114396ms step_avg:60.98ms
step:1877/2330 train_time:114455ms step_avg:60.98ms
step:1878/2330 train_time:114518ms step_avg:60.98ms
step:1879/2330 train_time:114578ms step_avg:60.98ms
step:1880/2330 train_time:114641ms step_avg:60.98ms
step:1881/2330 train_time:114700ms step_avg:60.98ms
step:1882/2330 train_time:114763ms step_avg:60.98ms
step:1883/2330 train_time:114822ms step_avg:60.98ms
step:1884/2330 train_time:114885ms step_avg:60.98ms
step:1885/2330 train_time:114945ms step_avg:60.98ms
step:1886/2330 train_time:115007ms step_avg:60.98ms
step:1887/2330 train_time:115067ms step_avg:60.98ms
step:1888/2330 train_time:115130ms step_avg:60.98ms
step:1889/2330 train_time:115190ms step_avg:60.98ms
step:1890/2330 train_time:115253ms step_avg:60.98ms
step:1891/2330 train_time:115313ms step_avg:60.98ms
step:1892/2330 train_time:115375ms step_avg:60.98ms
step:1893/2330 train_time:115435ms step_avg:60.98ms
step:1894/2330 train_time:115498ms step_avg:60.98ms
step:1895/2330 train_time:115557ms step_avg:60.98ms
step:1896/2330 train_time:115620ms step_avg:60.98ms
step:1897/2330 train_time:115680ms step_avg:60.98ms
step:1898/2330 train_time:115743ms step_avg:60.98ms
step:1899/2330 train_time:115803ms step_avg:60.98ms
step:1900/2330 train_time:115865ms step_avg:60.98ms
step:1901/2330 train_time:115925ms step_avg:60.98ms
step:1902/2330 train_time:115989ms step_avg:60.98ms
step:1903/2330 train_time:116048ms step_avg:60.98ms
step:1904/2330 train_time:116111ms step_avg:60.98ms
step:1905/2330 train_time:116171ms step_avg:60.98ms
step:1906/2330 train_time:116234ms step_avg:60.98ms
step:1907/2330 train_time:116293ms step_avg:60.98ms
step:1908/2330 train_time:116356ms step_avg:60.98ms
step:1909/2330 train_time:116416ms step_avg:60.98ms
step:1910/2330 train_time:116478ms step_avg:60.98ms
step:1911/2330 train_time:116538ms step_avg:60.98ms
step:1912/2330 train_time:116602ms step_avg:60.98ms
step:1913/2330 train_time:116661ms step_avg:60.98ms
step:1914/2330 train_time:116724ms step_avg:60.98ms
step:1915/2330 train_time:116783ms step_avg:60.98ms
step:1916/2330 train_time:116847ms step_avg:60.98ms
step:1917/2330 train_time:116906ms step_avg:60.98ms
step:1918/2330 train_time:116969ms step_avg:60.98ms
step:1919/2330 train_time:117028ms step_avg:60.98ms
step:1920/2330 train_time:117091ms step_avg:60.99ms
step:1921/2330 train_time:117151ms step_avg:60.98ms
step:1922/2330 train_time:117214ms step_avg:60.99ms
step:1923/2330 train_time:117274ms step_avg:60.98ms
step:1924/2330 train_time:117337ms step_avg:60.99ms
step:1925/2330 train_time:117397ms step_avg:60.99ms
step:1926/2330 train_time:117459ms step_avg:60.99ms
step:1927/2330 train_time:117519ms step_avg:60.99ms
step:1928/2330 train_time:117582ms step_avg:60.99ms
step:1929/2330 train_time:117642ms step_avg:60.99ms
step:1930/2330 train_time:117705ms step_avg:60.99ms
step:1931/2330 train_time:117764ms step_avg:60.99ms
step:1932/2330 train_time:117828ms step_avg:60.99ms
step:1933/2330 train_time:117888ms step_avg:60.99ms
step:1934/2330 train_time:117950ms step_avg:60.99ms
step:1935/2330 train_time:118010ms step_avg:60.99ms
step:1936/2330 train_time:118073ms step_avg:60.99ms
step:1937/2330 train_time:118133ms step_avg:60.99ms
step:1938/2330 train_time:118195ms step_avg:60.99ms
step:1939/2330 train_time:118255ms step_avg:60.99ms
step:1940/2330 train_time:118319ms step_avg:60.99ms
step:1941/2330 train_time:118378ms step_avg:60.99ms
step:1942/2330 train_time:118441ms step_avg:60.99ms
step:1943/2330 train_time:118500ms step_avg:60.99ms
step:1944/2330 train_time:118564ms step_avg:60.99ms
step:1945/2330 train_time:118624ms step_avg:60.99ms
step:1946/2330 train_time:118688ms step_avg:60.99ms
step:1947/2330 train_time:118748ms step_avg:60.99ms
step:1948/2330 train_time:118811ms step_avg:60.99ms
step:1949/2330 train_time:118871ms step_avg:60.99ms
step:1950/2330 train_time:118933ms step_avg:60.99ms
step:1951/2330 train_time:118992ms step_avg:60.99ms
step:1952/2330 train_time:119055ms step_avg:60.99ms
step:1953/2330 train_time:119114ms step_avg:60.99ms
step:1954/2330 train_time:119177ms step_avg:60.99ms
step:1955/2330 train_time:119237ms step_avg:60.99ms
step:1956/2330 train_time:119300ms step_avg:60.99ms
step:1957/2330 train_time:119360ms step_avg:60.99ms
step:1958/2330 train_time:119423ms step_avg:60.99ms
step:1959/2330 train_time:119482ms step_avg:60.99ms
step:1960/2330 train_time:119545ms step_avg:60.99ms
step:1961/2330 train_time:119605ms step_avg:60.99ms
step:1962/2330 train_time:119668ms step_avg:60.99ms
step:1963/2330 train_time:119728ms step_avg:60.99ms
step:1964/2330 train_time:119791ms step_avg:60.99ms
step:1965/2330 train_time:119851ms step_avg:60.99ms
step:1966/2330 train_time:119913ms step_avg:60.99ms
step:1967/2330 train_time:119973ms step_avg:60.99ms
step:1968/2330 train_time:120036ms step_avg:60.99ms
step:1969/2330 train_time:120096ms step_avg:60.99ms
step:1970/2330 train_time:120159ms step_avg:60.99ms
step:1971/2330 train_time:120218ms step_avg:60.99ms
step:1972/2330 train_time:120280ms step_avg:60.99ms
step:1973/2330 train_time:120341ms step_avg:60.99ms
step:1974/2330 train_time:120403ms step_avg:60.99ms
step:1975/2330 train_time:120463ms step_avg:60.99ms
step:1976/2330 train_time:120526ms step_avg:60.99ms
step:1977/2330 train_time:120586ms step_avg:60.99ms
step:1978/2330 train_time:120649ms step_avg:61.00ms
step:1979/2330 train_time:120710ms step_avg:61.00ms
step:1980/2330 train_time:120773ms step_avg:61.00ms
step:1981/2330 train_time:120833ms step_avg:61.00ms
step:1982/2330 train_time:120896ms step_avg:61.00ms
step:1983/2330 train_time:120955ms step_avg:61.00ms
step:1984/2330 train_time:121017ms step_avg:61.00ms
step:1985/2330 train_time:121077ms step_avg:61.00ms
step:1986/2330 train_time:121140ms step_avg:61.00ms
step:1987/2330 train_time:121200ms step_avg:61.00ms
step:1988/2330 train_time:121263ms step_avg:61.00ms
step:1989/2330 train_time:121322ms step_avg:61.00ms
step:1990/2330 train_time:121385ms step_avg:61.00ms
step:1991/2330 train_time:121445ms step_avg:61.00ms
step:1992/2330 train_time:121508ms step_avg:61.00ms
step:1993/2330 train_time:121569ms step_avg:61.00ms
step:1994/2330 train_time:121632ms step_avg:61.00ms
step:1995/2330 train_time:121691ms step_avg:61.00ms
step:1996/2330 train_time:121755ms step_avg:61.00ms
step:1997/2330 train_time:121814ms step_avg:61.00ms
step:1998/2330 train_time:121877ms step_avg:61.00ms
step:1999/2330 train_time:121936ms step_avg:61.00ms
step:2000/2330 train_time:121999ms step_avg:61.00ms
step:2000/2330 val_loss:3.5193 train_time:122072ms step_avg:61.04ms
step:2001/2330 train_time:122094ms step_avg:61.02ms
step:2002/2330 train_time:122124ms step_avg:61.00ms
step:2003/2330 train_time:122190ms step_avg:61.00ms
step:2004/2330 train_time:122258ms step_avg:61.01ms
step:2005/2330 train_time:122319ms step_avg:61.01ms
step:2006/2330 train_time:122380ms step_avg:61.01ms
step:2007/2330 train_time:122440ms step_avg:61.01ms
step:2008/2330 train_time:122502ms step_avg:61.01ms
step:2009/2330 train_time:122562ms step_avg:61.01ms
step:2010/2330 train_time:122624ms step_avg:61.01ms
step:2011/2330 train_time:122684ms step_avg:61.01ms
step:2012/2330 train_time:122746ms step_avg:61.01ms
step:2013/2330 train_time:122805ms step_avg:61.01ms
step:2014/2330 train_time:122868ms step_avg:61.01ms
step:2015/2330 train_time:122926ms step_avg:61.01ms
step:2016/2330 train_time:122988ms step_avg:61.01ms
step:2017/2330 train_time:123049ms step_avg:61.01ms
step:2018/2330 train_time:123114ms step_avg:61.01ms
step:2019/2330 train_time:123175ms step_avg:61.01ms
step:2020/2330 train_time:123239ms step_avg:61.01ms
step:2021/2330 train_time:123301ms step_avg:61.01ms
step:2022/2330 train_time:123364ms step_avg:61.01ms
step:2023/2330 train_time:123424ms step_avg:61.01ms
step:2024/2330 train_time:123487ms step_avg:61.01ms
step:2025/2330 train_time:123547ms step_avg:61.01ms
step:2026/2330 train_time:123609ms step_avg:61.01ms
step:2027/2330 train_time:123668ms step_avg:61.01ms
step:2028/2330 train_time:123730ms step_avg:61.01ms
step:2029/2330 train_time:123789ms step_avg:61.01ms
step:2030/2330 train_time:123852ms step_avg:61.01ms
step:2031/2330 train_time:123912ms step_avg:61.01ms
step:2032/2330 train_time:123974ms step_avg:61.01ms
step:2033/2330 train_time:124034ms step_avg:61.01ms
step:2034/2330 train_time:124098ms step_avg:61.01ms
step:2035/2330 train_time:124159ms step_avg:61.01ms
step:2036/2330 train_time:124222ms step_avg:61.01ms
step:2037/2330 train_time:124284ms step_avg:61.01ms
step:2038/2330 train_time:124348ms step_avg:61.01ms
step:2039/2330 train_time:124407ms step_avg:61.01ms
step:2040/2330 train_time:124470ms step_avg:61.01ms
step:2041/2330 train_time:124530ms step_avg:61.01ms
step:2042/2330 train_time:124594ms step_avg:61.02ms
step:2043/2330 train_time:124653ms step_avg:61.01ms
step:2044/2330 train_time:124716ms step_avg:61.02ms
step:2045/2330 train_time:124776ms step_avg:61.02ms
step:2046/2330 train_time:124838ms step_avg:61.02ms
step:2047/2330 train_time:124898ms step_avg:61.01ms
step:2048/2330 train_time:124960ms step_avg:61.02ms
step:2049/2330 train_time:125020ms step_avg:61.02ms
step:2050/2330 train_time:125083ms step_avg:61.02ms
step:2051/2330 train_time:125145ms step_avg:61.02ms
step:2052/2330 train_time:125209ms step_avg:61.02ms
step:2053/2330 train_time:125268ms step_avg:61.02ms
step:2054/2330 train_time:125331ms step_avg:61.02ms
step:2055/2330 train_time:125391ms step_avg:61.02ms
step:2056/2330 train_time:125455ms step_avg:61.02ms
step:2057/2330 train_time:125515ms step_avg:61.02ms
step:2058/2330 train_time:125577ms step_avg:61.02ms
step:2059/2330 train_time:125637ms step_avg:61.02ms
step:2060/2330 train_time:125700ms step_avg:61.02ms
step:2061/2330 train_time:125760ms step_avg:61.02ms
step:2062/2330 train_time:125823ms step_avg:61.02ms
step:2063/2330 train_time:125883ms step_avg:61.02ms
step:2064/2330 train_time:125945ms step_avg:61.02ms
step:2065/2330 train_time:126004ms step_avg:61.02ms
step:2066/2330 train_time:126068ms step_avg:61.02ms
step:2067/2330 train_time:126127ms step_avg:61.02ms
step:2068/2330 train_time:126191ms step_avg:61.02ms
step:2069/2330 train_time:126251ms step_avg:61.02ms
step:2070/2330 train_time:126313ms step_avg:61.02ms
step:2071/2330 train_time:126374ms step_avg:61.02ms
step:2072/2330 train_time:126438ms step_avg:61.02ms
step:2073/2330 train_time:126497ms step_avg:61.02ms
step:2074/2330 train_time:126560ms step_avg:61.02ms
step:2075/2330 train_time:126619ms step_avg:61.02ms
step:2076/2330 train_time:126682ms step_avg:61.02ms
step:2077/2330 train_time:126742ms step_avg:61.02ms
step:2078/2330 train_time:126805ms step_avg:61.02ms
step:2079/2330 train_time:126864ms step_avg:61.02ms
step:2080/2330 train_time:126926ms step_avg:61.02ms
step:2081/2330 train_time:126986ms step_avg:61.02ms
step:2082/2330 train_time:127050ms step_avg:61.02ms
step:2083/2330 train_time:127109ms step_avg:61.02ms
step:2084/2330 train_time:127172ms step_avg:61.02ms
step:2085/2330 train_time:127232ms step_avg:61.02ms
step:2086/2330 train_time:127295ms step_avg:61.02ms
step:2087/2330 train_time:127355ms step_avg:61.02ms
step:2088/2330 train_time:127417ms step_avg:61.02ms
step:2089/2330 train_time:127478ms step_avg:61.02ms
step:2090/2330 train_time:127540ms step_avg:61.02ms
step:2091/2330 train_time:127601ms step_avg:61.02ms
step:2092/2330 train_time:127664ms step_avg:61.02ms
step:2093/2330 train_time:127724ms step_avg:61.02ms
step:2094/2330 train_time:127787ms step_avg:61.03ms
step:2095/2330 train_time:127847ms step_avg:61.02ms
step:2096/2330 train_time:127909ms step_avg:61.03ms
step:2097/2330 train_time:127969ms step_avg:61.02ms
step:2098/2330 train_time:128032ms step_avg:61.03ms
step:2099/2330 train_time:128092ms step_avg:61.03ms
step:2100/2330 train_time:128155ms step_avg:61.03ms
step:2101/2330 train_time:128215ms step_avg:61.03ms
step:2102/2330 train_time:128277ms step_avg:61.03ms
step:2103/2330 train_time:128336ms step_avg:61.03ms
step:2104/2330 train_time:128399ms step_avg:61.03ms
step:2105/2330 train_time:128459ms step_avg:61.03ms
step:2106/2330 train_time:128522ms step_avg:61.03ms
step:2107/2330 train_time:128581ms step_avg:61.03ms
step:2108/2330 train_time:128645ms step_avg:61.03ms
step:2109/2330 train_time:128704ms step_avg:61.03ms
step:2110/2330 train_time:128767ms step_avg:61.03ms
step:2111/2330 train_time:128827ms step_avg:61.03ms
step:2112/2330 train_time:128890ms step_avg:61.03ms
step:2113/2330 train_time:128949ms step_avg:61.03ms
step:2114/2330 train_time:129012ms step_avg:61.03ms
step:2115/2330 train_time:129071ms step_avg:61.03ms
step:2116/2330 train_time:129134ms step_avg:61.03ms
step:2117/2330 train_time:129194ms step_avg:61.03ms
step:2118/2330 train_time:129256ms step_avg:61.03ms
step:2119/2330 train_time:129315ms step_avg:61.03ms
step:2120/2330 train_time:129379ms step_avg:61.03ms
step:2121/2330 train_time:129438ms step_avg:61.03ms
step:2122/2330 train_time:129502ms step_avg:61.03ms
step:2123/2330 train_time:129562ms step_avg:61.03ms
step:2124/2330 train_time:129624ms step_avg:61.03ms
step:2125/2330 train_time:129685ms step_avg:61.03ms
step:2126/2330 train_time:129747ms step_avg:61.03ms
step:2127/2330 train_time:129806ms step_avg:61.03ms
step:2128/2330 train_time:129869ms step_avg:61.03ms
step:2129/2330 train_time:129929ms step_avg:61.03ms
step:2130/2330 train_time:129992ms step_avg:61.03ms
step:2131/2330 train_time:130052ms step_avg:61.03ms
step:2132/2330 train_time:130114ms step_avg:61.03ms
step:2133/2330 train_time:130174ms step_avg:61.03ms
step:2134/2330 train_time:130237ms step_avg:61.03ms
step:2135/2330 train_time:130296ms step_avg:61.03ms
step:2136/2330 train_time:130359ms step_avg:61.03ms
step:2137/2330 train_time:130420ms step_avg:61.03ms
step:2138/2330 train_time:130483ms step_avg:61.03ms
step:2139/2330 train_time:130543ms step_avg:61.03ms
step:2140/2330 train_time:130605ms step_avg:61.03ms
step:2141/2330 train_time:130664ms step_avg:61.03ms
step:2142/2330 train_time:130728ms step_avg:61.03ms
step:2143/2330 train_time:130788ms step_avg:61.03ms
step:2144/2330 train_time:130851ms step_avg:61.03ms
step:2145/2330 train_time:130910ms step_avg:61.03ms
step:2146/2330 train_time:130973ms step_avg:61.03ms
step:2147/2330 train_time:131033ms step_avg:61.03ms
step:2148/2330 train_time:131096ms step_avg:61.03ms
step:2149/2330 train_time:131156ms step_avg:61.03ms
step:2150/2330 train_time:131219ms step_avg:61.03ms
step:2151/2330 train_time:131279ms step_avg:61.03ms
step:2152/2330 train_time:131342ms step_avg:61.03ms
step:2153/2330 train_time:131401ms step_avg:61.03ms
step:2154/2330 train_time:131464ms step_avg:61.03ms
step:2155/2330 train_time:131524ms step_avg:61.03ms
step:2156/2330 train_time:131586ms step_avg:61.03ms
step:2157/2330 train_time:131647ms step_avg:61.03ms
step:2158/2330 train_time:131709ms step_avg:61.03ms
step:2159/2330 train_time:131771ms step_avg:61.03ms
step:2160/2330 train_time:131833ms step_avg:61.03ms
step:2161/2330 train_time:131893ms step_avg:61.03ms
step:2162/2330 train_time:131956ms step_avg:61.03ms
step:2163/2330 train_time:132015ms step_avg:61.03ms
step:2164/2330 train_time:132078ms step_avg:61.03ms
step:2165/2330 train_time:132138ms step_avg:61.03ms
step:2166/2330 train_time:132201ms step_avg:61.03ms
step:2167/2330 train_time:132261ms step_avg:61.03ms
step:2168/2330 train_time:132323ms step_avg:61.03ms
step:2169/2330 train_time:132383ms step_avg:61.03ms
step:2170/2330 train_time:132446ms step_avg:61.03ms
step:2171/2330 train_time:132506ms step_avg:61.03ms
step:2172/2330 train_time:132569ms step_avg:61.04ms
step:2173/2330 train_time:132629ms step_avg:61.03ms
step:2174/2330 train_time:132691ms step_avg:61.04ms
step:2175/2330 train_time:132751ms step_avg:61.03ms
step:2176/2330 train_time:132814ms step_avg:61.04ms
step:2177/2330 train_time:132874ms step_avg:61.04ms
step:2178/2330 train_time:132937ms step_avg:61.04ms
step:2179/2330 train_time:132996ms step_avg:61.04ms
step:2180/2330 train_time:133059ms step_avg:61.04ms
step:2181/2330 train_time:133119ms step_avg:61.04ms
step:2182/2330 train_time:133182ms step_avg:61.04ms
step:2183/2330 train_time:133242ms step_avg:61.04ms
step:2184/2330 train_time:133304ms step_avg:61.04ms
step:2185/2330 train_time:133365ms step_avg:61.04ms
step:2186/2330 train_time:133428ms step_avg:61.04ms
step:2187/2330 train_time:133487ms step_avg:61.04ms
step:2188/2330 train_time:133550ms step_avg:61.04ms
step:2189/2330 train_time:133609ms step_avg:61.04ms
step:2190/2330 train_time:133672ms step_avg:61.04ms
step:2191/2330 train_time:133732ms step_avg:61.04ms
step:2192/2330 train_time:133795ms step_avg:61.04ms
step:2193/2330 train_time:133854ms step_avg:61.04ms
step:2194/2330 train_time:133917ms step_avg:61.04ms
step:2195/2330 train_time:133976ms step_avg:61.04ms
step:2196/2330 train_time:134039ms step_avg:61.04ms
step:2197/2330 train_time:134099ms step_avg:61.04ms
step:2198/2330 train_time:134162ms step_avg:61.04ms
step:2199/2330 train_time:134222ms step_avg:61.04ms
step:2200/2330 train_time:134286ms step_avg:61.04ms
step:2201/2330 train_time:134345ms step_avg:61.04ms
step:2202/2330 train_time:134407ms step_avg:61.04ms
step:2203/2330 train_time:134467ms step_avg:61.04ms
step:2204/2330 train_time:134529ms step_avg:61.04ms
step:2205/2330 train_time:134589ms step_avg:61.04ms
step:2206/2330 train_time:134652ms step_avg:61.04ms
step:2207/2330 train_time:134712ms step_avg:61.04ms
step:2208/2330 train_time:134775ms step_avg:61.04ms
step:2209/2330 train_time:134834ms step_avg:61.04ms
step:2210/2330 train_time:134897ms step_avg:61.04ms
step:2211/2330 train_time:134957ms step_avg:61.04ms
step:2212/2330 train_time:135020ms step_avg:61.04ms
step:2213/2330 train_time:135080ms step_avg:61.04ms
step:2214/2330 train_time:135143ms step_avg:61.04ms
step:2215/2330 train_time:135203ms step_avg:61.04ms
step:2216/2330 train_time:135266ms step_avg:61.04ms
step:2217/2330 train_time:135326ms step_avg:61.04ms
step:2218/2330 train_time:135389ms step_avg:61.04ms
step:2219/2330 train_time:135448ms step_avg:61.04ms
step:2220/2330 train_time:135511ms step_avg:61.04ms
step:2221/2330 train_time:135570ms step_avg:61.04ms
step:2222/2330 train_time:135633ms step_avg:61.04ms
step:2223/2330 train_time:135693ms step_avg:61.04ms
step:2224/2330 train_time:135756ms step_avg:61.04ms
step:2225/2330 train_time:135815ms step_avg:61.04ms
step:2226/2330 train_time:135878ms step_avg:61.04ms
step:2227/2330 train_time:135938ms step_avg:61.04ms
step:2228/2330 train_time:136001ms step_avg:61.04ms
step:2229/2330 train_time:136061ms step_avg:61.04ms
step:2230/2330 train_time:136123ms step_avg:61.04ms
step:2231/2330 train_time:136183ms step_avg:61.04ms
step:2232/2330 train_time:136247ms step_avg:61.04ms
step:2233/2330 train_time:136307ms step_avg:61.04ms
step:2234/2330 train_time:136369ms step_avg:61.04ms
step:2235/2330 train_time:136428ms step_avg:61.04ms
step:2236/2330 train_time:136491ms step_avg:61.04ms
step:2237/2330 train_time:136552ms step_avg:61.04ms
step:2238/2330 train_time:136614ms step_avg:61.04ms
step:2239/2330 train_time:136674ms step_avg:61.04ms
step:2240/2330 train_time:136737ms step_avg:61.04ms
step:2241/2330 train_time:136797ms step_avg:61.04ms
step:2242/2330 train_time:136860ms step_avg:61.04ms
step:2243/2330 train_time:136920ms step_avg:61.04ms
step:2244/2330 train_time:136983ms step_avg:61.04ms
step:2245/2330 train_time:137043ms step_avg:61.04ms
step:2246/2330 train_time:137105ms step_avg:61.04ms
step:2247/2330 train_time:137166ms step_avg:61.04ms
step:2248/2330 train_time:137228ms step_avg:61.04ms
step:2249/2330 train_time:137287ms step_avg:61.04ms
step:2250/2330 train_time:137351ms step_avg:61.04ms
step:2250/2330 val_loss:3.4774 train_time:137423ms step_avg:61.08ms
step:2251/2330 train_time:137446ms step_avg:61.06ms
step:2252/2330 train_time:137476ms step_avg:61.05ms
step:2253/2330 train_time:137538ms step_avg:61.05ms
step:2254/2330 train_time:137608ms step_avg:61.05ms
step:2255/2330 train_time:137668ms step_avg:61.05ms
step:2256/2330 train_time:137731ms step_avg:61.05ms
step:2257/2330 train_time:137790ms step_avg:61.05ms
step:2258/2330 train_time:137853ms step_avg:61.05ms
step:2259/2330 train_time:137912ms step_avg:61.05ms
step:2260/2330 train_time:137975ms step_avg:61.05ms
step:2261/2330 train_time:138033ms step_avg:61.05ms
step:2262/2330 train_time:138095ms step_avg:61.05ms
step:2263/2330 train_time:138155ms step_avg:61.05ms
step:2264/2330 train_time:138217ms step_avg:61.05ms
step:2265/2330 train_time:138276ms step_avg:61.05ms
step:2266/2330 train_time:138339ms step_avg:61.05ms
step:2267/2330 train_time:138400ms step_avg:61.05ms
step:2268/2330 train_time:138465ms step_avg:61.05ms
step:2269/2330 train_time:138525ms step_avg:61.05ms
step:2270/2330 train_time:138589ms step_avg:61.05ms
step:2271/2330 train_time:138650ms step_avg:61.05ms
step:2272/2330 train_time:138713ms step_avg:61.05ms
step:2273/2330 train_time:138773ms step_avg:61.05ms
step:2274/2330 train_time:138835ms step_avg:61.05ms
step:2275/2330 train_time:138894ms step_avg:61.05ms
step:2276/2330 train_time:138956ms step_avg:61.05ms
step:2277/2330 train_time:139016ms step_avg:61.05ms
step:2278/2330 train_time:139078ms step_avg:61.05ms
step:2279/2330 train_time:139137ms step_avg:61.05ms
step:2280/2330 train_time:139200ms step_avg:61.05ms
step:2281/2330 train_time:139259ms step_avg:61.05ms
step:2282/2330 train_time:139322ms step_avg:61.05ms
step:2283/2330 train_time:139382ms step_avg:61.05ms
step:2284/2330 train_time:139446ms step_avg:61.05ms
step:2285/2330 train_time:139505ms step_avg:61.05ms
step:2286/2330 train_time:139570ms step_avg:61.05ms
step:2287/2330 train_time:139629ms step_avg:61.05ms
step:2288/2330 train_time:139693ms step_avg:61.05ms
step:2289/2330 train_time:139753ms step_avg:61.05ms
step:2290/2330 train_time:139816ms step_avg:61.06ms
step:2291/2330 train_time:139876ms step_avg:61.05ms
step:2292/2330 train_time:139938ms step_avg:61.06ms
step:2293/2330 train_time:139998ms step_avg:61.05ms
step:2294/2330 train_time:140061ms step_avg:61.06ms
step:2295/2330 train_time:140119ms step_avg:61.05ms
step:2296/2330 train_time:140182ms step_avg:61.05ms
step:2297/2330 train_time:140241ms step_avg:61.05ms
step:2298/2330 train_time:140304ms step_avg:61.05ms
step:2299/2330 train_time:140365ms step_avg:61.05ms
step:2300/2330 train_time:140427ms step_avg:61.06ms
step:2301/2330 train_time:140487ms step_avg:61.05ms
step:2302/2330 train_time:140550ms step_avg:61.06ms
step:2303/2330 train_time:140609ms step_avg:61.05ms
step:2304/2330 train_time:140675ms step_avg:61.06ms
step:2305/2330 train_time:140734ms step_avg:61.06ms
step:2306/2330 train_time:140798ms step_avg:61.06ms
step:2307/2330 train_time:140857ms step_avg:61.06ms
step:2308/2330 train_time:140920ms step_avg:61.06ms
step:2309/2330 train_time:140980ms step_avg:61.06ms
step:2310/2330 train_time:141042ms step_avg:61.06ms
step:2311/2330 train_time:141101ms step_avg:61.06ms
step:2312/2330 train_time:141164ms step_avg:61.06ms
step:2313/2330 train_time:141223ms step_avg:61.06ms
step:2314/2330 train_time:141286ms step_avg:61.06ms
step:2315/2330 train_time:141345ms step_avg:61.06ms
step:2316/2330 train_time:141409ms step_avg:61.06ms
step:2317/2330 train_time:141469ms step_avg:61.06ms
step:2318/2330 train_time:141532ms step_avg:61.06ms
step:2319/2330 train_time:141592ms step_avg:61.06ms
step:2320/2330 train_time:141655ms step_avg:61.06ms
step:2321/2330 train_time:141716ms step_avg:61.06ms
step:2322/2330 train_time:141779ms step_avg:61.06ms
step:2323/2330 train_time:141839ms step_avg:61.06ms
step:2324/2330 train_time:141902ms step_avg:61.06ms
step:2325/2330 train_time:141962ms step_avg:61.06ms
step:2326/2330 train_time:142024ms step_avg:61.06ms
step:2327/2330 train_time:142084ms step_avg:61.06ms
step:2328/2330 train_time:142146ms step_avg:61.06ms
step:2329/2330 train_time:142205ms step_avg:61.06ms
step:2330/2330 train_time:142268ms step_avg:61.06ms
step:2330/2330 val_loss:3.4629 train_time:142341ms step_avg:61.09ms
peak memory allocated: 29226 MiB reserved: 44076 MiB
