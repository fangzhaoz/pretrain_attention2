import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "adam_lr7e-4"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-1, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.Adam(
    hidden_matrix_params + gate_params,
    lr=7e-4,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 00:15:01 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   33C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   31C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   31C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   32C    P0             115W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:82ms step_avg:81.81ms
step:2/2330 train_time:180ms step_avg:89.81ms
step:3/2330 train_time:198ms step_avg:66.10ms
step:4/2330 train_time:221ms step_avg:55.16ms
step:5/2330 train_time:276ms step_avg:55.17ms
step:6/2330 train_time:335ms step_avg:55.79ms
step:7/2330 train_time:390ms step_avg:55.78ms
step:8/2330 train_time:450ms step_avg:56.29ms
step:9/2330 train_time:506ms step_avg:56.24ms
step:10/2330 train_time:566ms step_avg:56.61ms
step:11/2330 train_time:622ms step_avg:56.51ms
step:12/2330 train_time:682ms step_avg:56.82ms
step:13/2330 train_time:738ms step_avg:56.73ms
step:14/2330 train_time:797ms step_avg:56.94ms
step:15/2330 train_time:853ms step_avg:56.90ms
step:16/2330 train_time:913ms step_avg:57.07ms
step:17/2330 train_time:969ms step_avg:56.99ms
step:18/2330 train_time:1030ms step_avg:57.20ms
step:19/2330 train_time:1086ms step_avg:57.17ms
step:20/2330 train_time:1153ms step_avg:57.67ms
step:21/2330 train_time:1211ms step_avg:57.65ms
step:22/2330 train_time:1274ms step_avg:57.93ms
step:23/2330 train_time:1331ms step_avg:57.86ms
step:24/2330 train_time:1392ms step_avg:58.01ms
step:25/2330 train_time:1448ms step_avg:57.94ms
step:26/2330 train_time:1509ms step_avg:58.04ms
step:27/2330 train_time:1565ms step_avg:57.96ms
step:28/2330 train_time:1626ms step_avg:58.07ms
step:29/2330 train_time:1682ms step_avg:58.01ms
step:30/2330 train_time:1743ms step_avg:58.11ms
step:31/2330 train_time:1799ms step_avg:58.04ms
step:32/2330 train_time:1859ms step_avg:58.10ms
step:33/2330 train_time:1915ms step_avg:58.02ms
step:34/2330 train_time:1975ms step_avg:58.10ms
step:35/2330 train_time:2031ms step_avg:58.04ms
step:36/2330 train_time:2094ms step_avg:58.17ms
step:37/2330 train_time:2152ms step_avg:58.16ms
step:38/2330 train_time:2215ms step_avg:58.28ms
step:39/2330 train_time:2271ms step_avg:58.24ms
step:40/2330 train_time:2334ms step_avg:58.35ms
step:41/2330 train_time:2390ms step_avg:58.30ms
step:42/2330 train_time:2452ms step_avg:58.38ms
step:43/2330 train_time:2508ms step_avg:58.33ms
step:44/2330 train_time:2569ms step_avg:58.38ms
step:45/2330 train_time:2625ms step_avg:58.34ms
step:46/2330 train_time:2687ms step_avg:58.41ms
step:47/2330 train_time:2743ms step_avg:58.36ms
step:48/2330 train_time:2805ms step_avg:58.43ms
step:49/2330 train_time:2861ms step_avg:58.38ms
step:50/2330 train_time:2922ms step_avg:58.45ms
step:51/2330 train_time:2978ms step_avg:58.40ms
step:52/2330 train_time:3039ms step_avg:58.44ms
step:53/2330 train_time:3095ms step_avg:58.39ms
step:54/2330 train_time:3157ms step_avg:58.46ms
step:55/2330 train_time:3213ms step_avg:58.42ms
step:56/2330 train_time:3274ms step_avg:58.47ms
step:57/2330 train_time:3331ms step_avg:58.44ms
step:58/2330 train_time:3392ms step_avg:58.49ms
step:59/2330 train_time:3449ms step_avg:58.46ms
step:60/2330 train_time:3510ms step_avg:58.50ms
step:61/2330 train_time:3567ms step_avg:58.47ms
step:62/2330 train_time:3627ms step_avg:58.50ms
step:63/2330 train_time:3684ms step_avg:58.48ms
step:64/2330 train_time:3745ms step_avg:58.51ms
step:65/2330 train_time:3801ms step_avg:58.48ms
step:66/2330 train_time:3863ms step_avg:58.52ms
step:67/2330 train_time:3919ms step_avg:58.49ms
step:68/2330 train_time:3980ms step_avg:58.53ms
step:69/2330 train_time:4036ms step_avg:58.50ms
step:70/2330 train_time:4097ms step_avg:58.53ms
step:71/2330 train_time:4154ms step_avg:58.51ms
step:72/2330 train_time:4215ms step_avg:58.54ms
step:73/2330 train_time:4271ms step_avg:58.51ms
step:74/2330 train_time:4333ms step_avg:58.55ms
step:75/2330 train_time:4389ms step_avg:58.51ms
step:76/2330 train_time:4450ms step_avg:58.56ms
step:77/2330 train_time:4507ms step_avg:58.53ms
step:78/2330 train_time:4568ms step_avg:58.56ms
step:79/2330 train_time:4624ms step_avg:58.54ms
step:80/2330 train_time:4686ms step_avg:58.57ms
step:81/2330 train_time:4742ms step_avg:58.55ms
step:82/2330 train_time:4804ms step_avg:58.58ms
step:83/2330 train_time:4860ms step_avg:58.56ms
step:84/2330 train_time:4921ms step_avg:58.59ms
step:85/2330 train_time:4977ms step_avg:58.55ms
step:86/2330 train_time:5038ms step_avg:58.58ms
step:87/2330 train_time:5095ms step_avg:58.56ms
step:88/2330 train_time:5156ms step_avg:58.59ms
step:89/2330 train_time:5211ms step_avg:58.55ms
step:90/2330 train_time:5273ms step_avg:58.59ms
step:91/2330 train_time:5330ms step_avg:58.57ms
step:92/2330 train_time:5391ms step_avg:58.59ms
step:93/2330 train_time:5447ms step_avg:58.57ms
step:94/2330 train_time:5510ms step_avg:58.61ms
step:95/2330 train_time:5566ms step_avg:58.59ms
step:96/2330 train_time:5627ms step_avg:58.61ms
step:97/2330 train_time:5683ms step_avg:58.59ms
step:98/2330 train_time:5744ms step_avg:58.62ms
step:99/2330 train_time:5801ms step_avg:58.60ms
step:100/2330 train_time:5863ms step_avg:58.63ms
step:101/2330 train_time:5920ms step_avg:58.61ms
step:102/2330 train_time:5980ms step_avg:58.63ms
step:103/2330 train_time:6037ms step_avg:58.61ms
step:104/2330 train_time:6097ms step_avg:58.62ms
step:105/2330 train_time:6153ms step_avg:58.60ms
step:106/2330 train_time:6214ms step_avg:58.63ms
step:107/2330 train_time:6270ms step_avg:58.60ms
step:108/2330 train_time:6331ms step_avg:58.62ms
step:109/2330 train_time:6387ms step_avg:58.60ms
step:110/2330 train_time:6448ms step_avg:58.62ms
step:111/2330 train_time:6505ms step_avg:58.60ms
step:112/2330 train_time:6565ms step_avg:58.62ms
step:113/2330 train_time:6621ms step_avg:58.59ms
step:114/2330 train_time:6683ms step_avg:58.62ms
step:115/2330 train_time:6738ms step_avg:58.60ms
step:116/2330 train_time:6800ms step_avg:58.62ms
step:117/2330 train_time:6857ms step_avg:58.60ms
step:118/2330 train_time:6918ms step_avg:58.62ms
step:119/2330 train_time:6974ms step_avg:58.60ms
step:120/2330 train_time:7034ms step_avg:58.62ms
step:121/2330 train_time:7091ms step_avg:58.60ms
step:122/2330 train_time:7152ms step_avg:58.63ms
step:123/2330 train_time:7209ms step_avg:58.61ms
step:124/2330 train_time:7269ms step_avg:58.62ms
step:125/2330 train_time:7325ms step_avg:58.60ms
step:126/2330 train_time:7387ms step_avg:58.63ms
step:127/2330 train_time:7444ms step_avg:58.61ms
step:128/2330 train_time:7505ms step_avg:58.63ms
step:129/2330 train_time:7561ms step_avg:58.61ms
step:130/2330 train_time:7621ms step_avg:58.63ms
step:131/2330 train_time:7677ms step_avg:58.60ms
step:132/2330 train_time:7739ms step_avg:58.63ms
step:133/2330 train_time:7795ms step_avg:58.61ms
step:134/2330 train_time:7856ms step_avg:58.63ms
step:135/2330 train_time:7913ms step_avg:58.61ms
step:136/2330 train_time:7973ms step_avg:58.63ms
step:137/2330 train_time:8030ms step_avg:58.61ms
step:138/2330 train_time:8091ms step_avg:58.63ms
step:139/2330 train_time:8148ms step_avg:58.62ms
step:140/2330 train_time:8210ms step_avg:58.64ms
step:141/2330 train_time:8266ms step_avg:58.63ms
step:142/2330 train_time:8327ms step_avg:58.64ms
step:143/2330 train_time:8383ms step_avg:58.62ms
step:144/2330 train_time:8443ms step_avg:58.63ms
step:145/2330 train_time:8500ms step_avg:58.62ms
step:146/2330 train_time:8560ms step_avg:58.63ms
step:147/2330 train_time:8616ms step_avg:58.61ms
step:148/2330 train_time:8677ms step_avg:58.63ms
step:149/2330 train_time:8733ms step_avg:58.61ms
step:150/2330 train_time:8795ms step_avg:58.63ms
step:151/2330 train_time:8851ms step_avg:58.62ms
step:152/2330 train_time:8913ms step_avg:58.64ms
step:153/2330 train_time:8969ms step_avg:58.62ms
step:154/2330 train_time:9030ms step_avg:58.63ms
step:155/2330 train_time:9086ms step_avg:58.62ms
step:156/2330 train_time:9148ms step_avg:58.64ms
step:157/2330 train_time:9204ms step_avg:58.62ms
step:158/2330 train_time:9265ms step_avg:58.64ms
step:159/2330 train_time:9321ms step_avg:58.62ms
step:160/2330 train_time:9382ms step_avg:58.64ms
step:161/2330 train_time:9438ms step_avg:58.62ms
step:162/2330 train_time:9499ms step_avg:58.63ms
step:163/2330 train_time:9555ms step_avg:58.62ms
step:164/2330 train_time:9616ms step_avg:58.63ms
step:165/2330 train_time:9672ms step_avg:58.62ms
step:166/2330 train_time:9733ms step_avg:58.63ms
step:167/2330 train_time:9789ms step_avg:58.62ms
step:168/2330 train_time:9851ms step_avg:58.64ms
step:169/2330 train_time:9907ms step_avg:58.62ms
step:170/2330 train_time:9969ms step_avg:58.64ms
step:171/2330 train_time:10026ms step_avg:58.63ms
step:172/2330 train_time:10088ms step_avg:58.65ms
step:173/2330 train_time:10144ms step_avg:58.64ms
step:174/2330 train_time:10205ms step_avg:58.65ms
step:175/2330 train_time:10261ms step_avg:58.64ms
step:176/2330 train_time:10322ms step_avg:58.65ms
step:177/2330 train_time:10378ms step_avg:58.63ms
step:178/2330 train_time:10439ms step_avg:58.65ms
step:179/2330 train_time:10495ms step_avg:58.63ms
step:180/2330 train_time:10556ms step_avg:58.64ms
step:181/2330 train_time:10612ms step_avg:58.63ms
step:182/2330 train_time:10673ms step_avg:58.64ms
step:183/2330 train_time:10729ms step_avg:58.63ms
step:184/2330 train_time:10791ms step_avg:58.65ms
step:185/2330 train_time:10847ms step_avg:58.63ms
step:186/2330 train_time:10907ms step_avg:58.64ms
step:187/2330 train_time:10963ms step_avg:58.63ms
step:188/2330 train_time:11024ms step_avg:58.64ms
step:189/2330 train_time:11080ms step_avg:58.63ms
step:190/2330 train_time:11142ms step_avg:58.64ms
step:191/2330 train_time:11197ms step_avg:58.63ms
step:192/2330 train_time:11259ms step_avg:58.64ms
step:193/2330 train_time:11315ms step_avg:58.63ms
step:194/2330 train_time:11376ms step_avg:58.64ms
step:195/2330 train_time:11432ms step_avg:58.62ms
step:196/2330 train_time:11493ms step_avg:58.64ms
step:197/2330 train_time:11550ms step_avg:58.63ms
step:198/2330 train_time:11610ms step_avg:58.64ms
step:199/2330 train_time:11667ms step_avg:58.63ms
step:200/2330 train_time:11727ms step_avg:58.64ms
step:201/2330 train_time:11783ms step_avg:58.62ms
step:202/2330 train_time:11845ms step_avg:58.64ms
step:203/2330 train_time:11901ms step_avg:58.63ms
step:204/2330 train_time:11961ms step_avg:58.63ms
step:205/2330 train_time:12017ms step_avg:58.62ms
step:206/2330 train_time:12078ms step_avg:58.63ms
step:207/2330 train_time:12134ms step_avg:58.62ms
step:208/2330 train_time:12195ms step_avg:58.63ms
step:209/2330 train_time:12251ms step_avg:58.62ms
step:210/2330 train_time:12312ms step_avg:58.63ms
step:211/2330 train_time:12369ms step_avg:58.62ms
step:212/2330 train_time:12430ms step_avg:58.63ms
step:213/2330 train_time:12487ms step_avg:58.62ms
step:214/2330 train_time:12548ms step_avg:58.63ms
step:215/2330 train_time:12605ms step_avg:58.63ms
step:216/2330 train_time:12665ms step_avg:58.64ms
step:217/2330 train_time:12721ms step_avg:58.62ms
step:218/2330 train_time:12781ms step_avg:58.63ms
step:219/2330 train_time:12837ms step_avg:58.62ms
step:220/2330 train_time:12898ms step_avg:58.63ms
step:221/2330 train_time:12955ms step_avg:58.62ms
step:222/2330 train_time:13015ms step_avg:58.63ms
step:223/2330 train_time:13071ms step_avg:58.61ms
step:224/2330 train_time:13132ms step_avg:58.63ms
step:225/2330 train_time:13188ms step_avg:58.61ms
step:226/2330 train_time:13249ms step_avg:58.63ms
step:227/2330 train_time:13306ms step_avg:58.62ms
step:228/2330 train_time:13367ms step_avg:58.63ms
step:229/2330 train_time:13424ms step_avg:58.62ms
step:230/2330 train_time:13485ms step_avg:58.63ms
step:231/2330 train_time:13541ms step_avg:58.62ms
step:232/2330 train_time:13602ms step_avg:58.63ms
step:233/2330 train_time:13659ms step_avg:58.62ms
step:234/2330 train_time:13718ms step_avg:58.63ms
step:235/2330 train_time:13774ms step_avg:58.61ms
step:236/2330 train_time:13835ms step_avg:58.62ms
step:237/2330 train_time:13892ms step_avg:58.61ms
step:238/2330 train_time:13953ms step_avg:58.63ms
step:239/2330 train_time:14009ms step_avg:58.61ms
step:240/2330 train_time:14070ms step_avg:58.63ms
step:241/2330 train_time:14127ms step_avg:58.62ms
step:242/2330 train_time:14187ms step_avg:58.62ms
step:243/2330 train_time:14244ms step_avg:58.62ms
step:244/2330 train_time:14305ms step_avg:58.63ms
step:245/2330 train_time:14361ms step_avg:58.62ms
step:246/2330 train_time:14423ms step_avg:58.63ms
step:247/2330 train_time:14479ms step_avg:58.62ms
step:248/2330 train_time:14540ms step_avg:58.63ms
step:249/2330 train_time:14596ms step_avg:58.62ms
step:250/2330 train_time:14657ms step_avg:58.63ms
step:250/2330 val_loss:4.8930 train_time:14734ms step_avg:58.94ms
step:251/2330 train_time:14753ms step_avg:58.78ms
step:252/2330 train_time:14776ms step_avg:58.63ms
step:253/2330 train_time:14833ms step_avg:58.63ms
step:254/2330 train_time:14895ms step_avg:58.64ms
step:255/2330 train_time:14951ms step_avg:58.63ms
step:256/2330 train_time:15016ms step_avg:58.65ms
step:257/2330 train_time:15071ms step_avg:58.64ms
step:258/2330 train_time:15133ms step_avg:58.65ms
step:259/2330 train_time:15189ms step_avg:58.64ms
step:260/2330 train_time:15250ms step_avg:58.65ms
step:261/2330 train_time:15306ms step_avg:58.64ms
step:262/2330 train_time:15367ms step_avg:58.65ms
step:263/2330 train_time:15423ms step_avg:58.64ms
step:264/2330 train_time:15483ms step_avg:58.65ms
step:265/2330 train_time:15539ms step_avg:58.64ms
step:266/2330 train_time:15599ms step_avg:58.64ms
step:267/2330 train_time:15655ms step_avg:58.63ms
step:268/2330 train_time:15715ms step_avg:58.64ms
step:269/2330 train_time:15773ms step_avg:58.64ms
step:270/2330 train_time:15833ms step_avg:58.64ms
step:271/2330 train_time:15890ms step_avg:58.63ms
step:272/2330 train_time:15952ms step_avg:58.65ms
step:273/2330 train_time:16009ms step_avg:58.64ms
step:274/2330 train_time:16071ms step_avg:58.65ms
step:275/2330 train_time:16127ms step_avg:58.65ms
step:276/2330 train_time:16188ms step_avg:58.65ms
step:277/2330 train_time:16244ms step_avg:58.64ms
step:278/2330 train_time:16305ms step_avg:58.65ms
step:279/2330 train_time:16361ms step_avg:58.64ms
step:280/2330 train_time:16422ms step_avg:58.65ms
step:281/2330 train_time:16478ms step_avg:58.64ms
step:282/2330 train_time:16538ms step_avg:58.65ms
step:283/2330 train_time:16594ms step_avg:58.64ms
step:284/2330 train_time:16655ms step_avg:58.64ms
step:285/2330 train_time:16712ms step_avg:58.64ms
step:286/2330 train_time:16772ms step_avg:58.64ms
step:287/2330 train_time:16829ms step_avg:58.64ms
step:288/2330 train_time:16891ms step_avg:58.65ms
step:289/2330 train_time:16947ms step_avg:58.64ms
step:290/2330 train_time:17009ms step_avg:58.65ms
step:291/2330 train_time:17066ms step_avg:58.65ms
step:292/2330 train_time:17129ms step_avg:58.66ms
step:293/2330 train_time:17185ms step_avg:58.65ms
step:294/2330 train_time:17245ms step_avg:58.66ms
step:295/2330 train_time:17302ms step_avg:58.65ms
step:296/2330 train_time:17362ms step_avg:58.66ms
step:297/2330 train_time:17419ms step_avg:58.65ms
step:298/2330 train_time:17479ms step_avg:58.66ms
step:299/2330 train_time:17536ms step_avg:58.65ms
step:300/2330 train_time:17596ms step_avg:58.65ms
step:301/2330 train_time:17653ms step_avg:58.65ms
step:302/2330 train_time:17713ms step_avg:58.65ms
step:303/2330 train_time:17770ms step_avg:58.65ms
step:304/2330 train_time:17831ms step_avg:58.66ms
step:305/2330 train_time:17888ms step_avg:58.65ms
step:306/2330 train_time:17949ms step_avg:58.66ms
step:307/2330 train_time:18005ms step_avg:58.65ms
step:308/2330 train_time:18069ms step_avg:58.67ms
step:309/2330 train_time:18126ms step_avg:58.66ms
step:310/2330 train_time:18186ms step_avg:58.66ms
step:311/2330 train_time:18243ms step_avg:58.66ms
step:312/2330 train_time:18304ms step_avg:58.67ms
step:313/2330 train_time:18360ms step_avg:58.66ms
step:314/2330 train_time:18421ms step_avg:58.66ms
step:315/2330 train_time:18477ms step_avg:58.66ms
step:316/2330 train_time:18537ms step_avg:58.66ms
step:317/2330 train_time:18594ms step_avg:58.65ms
step:318/2330 train_time:18654ms step_avg:58.66ms
step:319/2330 train_time:18710ms step_avg:58.65ms
step:320/2330 train_time:18771ms step_avg:58.66ms
step:321/2330 train_time:18828ms step_avg:58.65ms
step:322/2330 train_time:18889ms step_avg:58.66ms
step:323/2330 train_time:18946ms step_avg:58.66ms
step:324/2330 train_time:19006ms step_avg:58.66ms
step:325/2330 train_time:19063ms step_avg:58.65ms
step:326/2330 train_time:19125ms step_avg:58.66ms
step:327/2330 train_time:19181ms step_avg:58.66ms
step:328/2330 train_time:19243ms step_avg:58.67ms
step:329/2330 train_time:19299ms step_avg:58.66ms
step:330/2330 train_time:19360ms step_avg:58.67ms
step:331/2330 train_time:19416ms step_avg:58.66ms
step:332/2330 train_time:19477ms step_avg:58.66ms
step:333/2330 train_time:19533ms step_avg:58.66ms
step:334/2330 train_time:19593ms step_avg:58.66ms
step:335/2330 train_time:19649ms step_avg:58.65ms
step:336/2330 train_time:19710ms step_avg:58.66ms
step:337/2330 train_time:19767ms step_avg:58.65ms
step:338/2330 train_time:19829ms step_avg:58.67ms
step:339/2330 train_time:19886ms step_avg:58.66ms
step:340/2330 train_time:19947ms step_avg:58.67ms
step:341/2330 train_time:20004ms step_avg:58.66ms
step:342/2330 train_time:20065ms step_avg:58.67ms
step:343/2330 train_time:20122ms step_avg:58.66ms
step:344/2330 train_time:20183ms step_avg:58.67ms
step:345/2330 train_time:20240ms step_avg:58.67ms
step:346/2330 train_time:20301ms step_avg:58.67ms
step:347/2330 train_time:20357ms step_avg:58.67ms
step:348/2330 train_time:20418ms step_avg:58.67ms
step:349/2330 train_time:20475ms step_avg:58.67ms
step:350/2330 train_time:20535ms step_avg:58.67ms
step:351/2330 train_time:20591ms step_avg:58.66ms
step:352/2330 train_time:20652ms step_avg:58.67ms
step:353/2330 train_time:20708ms step_avg:58.66ms
step:354/2330 train_time:20770ms step_avg:58.67ms
step:355/2330 train_time:20826ms step_avg:58.67ms
step:356/2330 train_time:20887ms step_avg:58.67ms
step:357/2330 train_time:20944ms step_avg:58.67ms
step:358/2330 train_time:21005ms step_avg:58.67ms
step:359/2330 train_time:21061ms step_avg:58.67ms
step:360/2330 train_time:21122ms step_avg:58.67ms
step:361/2330 train_time:21179ms step_avg:58.67ms
step:362/2330 train_time:21240ms step_avg:58.67ms
step:363/2330 train_time:21297ms step_avg:58.67ms
step:364/2330 train_time:21358ms step_avg:58.67ms
step:365/2330 train_time:21414ms step_avg:58.67ms
step:366/2330 train_time:21475ms step_avg:58.68ms
step:367/2330 train_time:21532ms step_avg:58.67ms
step:368/2330 train_time:21592ms step_avg:58.67ms
step:369/2330 train_time:21648ms step_avg:58.67ms
step:370/2330 train_time:21709ms step_avg:58.67ms
step:371/2330 train_time:21765ms step_avg:58.67ms
step:372/2330 train_time:21826ms step_avg:58.67ms
step:373/2330 train_time:21883ms step_avg:58.67ms
step:374/2330 train_time:21944ms step_avg:58.67ms
step:375/2330 train_time:22001ms step_avg:58.67ms
step:376/2330 train_time:22062ms step_avg:58.68ms
step:377/2330 train_time:22119ms step_avg:58.67ms
step:378/2330 train_time:22180ms step_avg:58.68ms
step:379/2330 train_time:22237ms step_avg:58.67ms
step:380/2330 train_time:22297ms step_avg:58.68ms
step:381/2330 train_time:22354ms step_avg:58.67ms
step:382/2330 train_time:22414ms step_avg:58.68ms
step:383/2330 train_time:22471ms step_avg:58.67ms
step:384/2330 train_time:22532ms step_avg:58.68ms
step:385/2330 train_time:22588ms step_avg:58.67ms
step:386/2330 train_time:22649ms step_avg:58.68ms
step:387/2330 train_time:22706ms step_avg:58.67ms
step:388/2330 train_time:22766ms step_avg:58.68ms
step:389/2330 train_time:22823ms step_avg:58.67ms
step:390/2330 train_time:22885ms step_avg:58.68ms
step:391/2330 train_time:22941ms step_avg:58.67ms
step:392/2330 train_time:23002ms step_avg:58.68ms
step:393/2330 train_time:23059ms step_avg:58.67ms
step:394/2330 train_time:23120ms step_avg:58.68ms
step:395/2330 train_time:23177ms step_avg:58.68ms
step:396/2330 train_time:23238ms step_avg:58.68ms
step:397/2330 train_time:23295ms step_avg:58.68ms
step:398/2330 train_time:23355ms step_avg:58.68ms
step:399/2330 train_time:23411ms step_avg:58.67ms
step:400/2330 train_time:23472ms step_avg:58.68ms
step:401/2330 train_time:23528ms step_avg:58.67ms
step:402/2330 train_time:23589ms step_avg:58.68ms
step:403/2330 train_time:23646ms step_avg:58.67ms
step:404/2330 train_time:23707ms step_avg:58.68ms
step:405/2330 train_time:23764ms step_avg:58.68ms
step:406/2330 train_time:23825ms step_avg:58.68ms
step:407/2330 train_time:23881ms step_avg:58.67ms
step:408/2330 train_time:23943ms step_avg:58.68ms
step:409/2330 train_time:23999ms step_avg:58.68ms
step:410/2330 train_time:24060ms step_avg:58.68ms
step:411/2330 train_time:24116ms step_avg:58.68ms
step:412/2330 train_time:24178ms step_avg:58.68ms
step:413/2330 train_time:24234ms step_avg:58.68ms
step:414/2330 train_time:24295ms step_avg:58.68ms
step:415/2330 train_time:24351ms step_avg:58.68ms
step:416/2330 train_time:24412ms step_avg:58.68ms
step:417/2330 train_time:24468ms step_avg:58.68ms
step:418/2330 train_time:24530ms step_avg:58.68ms
step:419/2330 train_time:24586ms step_avg:58.68ms
step:420/2330 train_time:24647ms step_avg:58.68ms
step:421/2330 train_time:24704ms step_avg:58.68ms
step:422/2330 train_time:24765ms step_avg:58.69ms
step:423/2330 train_time:24822ms step_avg:58.68ms
step:424/2330 train_time:24883ms step_avg:58.69ms
step:425/2330 train_time:24940ms step_avg:58.68ms
step:426/2330 train_time:25002ms step_avg:58.69ms
step:427/2330 train_time:25058ms step_avg:58.68ms
step:428/2330 train_time:25119ms step_avg:58.69ms
step:429/2330 train_time:25175ms step_avg:58.68ms
step:430/2330 train_time:25237ms step_avg:58.69ms
step:431/2330 train_time:25293ms step_avg:58.68ms
step:432/2330 train_time:25353ms step_avg:58.69ms
step:433/2330 train_time:25409ms step_avg:58.68ms
step:434/2330 train_time:25471ms step_avg:58.69ms
step:435/2330 train_time:25527ms step_avg:58.68ms
step:436/2330 train_time:25588ms step_avg:58.69ms
step:437/2330 train_time:25644ms step_avg:58.68ms
step:438/2330 train_time:25705ms step_avg:58.69ms
step:439/2330 train_time:25762ms step_avg:58.68ms
step:440/2330 train_time:25823ms step_avg:58.69ms
step:441/2330 train_time:25880ms step_avg:58.68ms
step:442/2330 train_time:25941ms step_avg:58.69ms
step:443/2330 train_time:25997ms step_avg:58.68ms
step:444/2330 train_time:26059ms step_avg:58.69ms
step:445/2330 train_time:26115ms step_avg:58.69ms
step:446/2330 train_time:26176ms step_avg:58.69ms
step:447/2330 train_time:26233ms step_avg:58.69ms
step:448/2330 train_time:26294ms step_avg:58.69ms
step:449/2330 train_time:26349ms step_avg:58.68ms
step:450/2330 train_time:26411ms step_avg:58.69ms
step:451/2330 train_time:26467ms step_avg:58.69ms
step:452/2330 train_time:26528ms step_avg:58.69ms
step:453/2330 train_time:26585ms step_avg:58.69ms
step:454/2330 train_time:26647ms step_avg:58.69ms
step:455/2330 train_time:26704ms step_avg:58.69ms
step:456/2330 train_time:26764ms step_avg:58.69ms
step:457/2330 train_time:26820ms step_avg:58.69ms
step:458/2330 train_time:26882ms step_avg:58.69ms
step:459/2330 train_time:26939ms step_avg:58.69ms
step:460/2330 train_time:27000ms step_avg:58.70ms
step:461/2330 train_time:27057ms step_avg:58.69ms
step:462/2330 train_time:27118ms step_avg:58.70ms
step:463/2330 train_time:27175ms step_avg:58.69ms
step:464/2330 train_time:27235ms step_avg:58.70ms
step:465/2330 train_time:27291ms step_avg:58.69ms
step:466/2330 train_time:27352ms step_avg:58.70ms
step:467/2330 train_time:27408ms step_avg:58.69ms
step:468/2330 train_time:27469ms step_avg:58.70ms
step:469/2330 train_time:27526ms step_avg:58.69ms
step:470/2330 train_time:27587ms step_avg:58.69ms
step:471/2330 train_time:27643ms step_avg:58.69ms
step:472/2330 train_time:27704ms step_avg:58.69ms
step:473/2330 train_time:27760ms step_avg:58.69ms
step:474/2330 train_time:27821ms step_avg:58.69ms
step:475/2330 train_time:27878ms step_avg:58.69ms
step:476/2330 train_time:27940ms step_avg:58.70ms
step:477/2330 train_time:27997ms step_avg:58.69ms
step:478/2330 train_time:28057ms step_avg:58.70ms
step:479/2330 train_time:28114ms step_avg:58.69ms
step:480/2330 train_time:28175ms step_avg:58.70ms
step:481/2330 train_time:28231ms step_avg:58.69ms
step:482/2330 train_time:28293ms step_avg:58.70ms
step:483/2330 train_time:28348ms step_avg:58.69ms
step:484/2330 train_time:28409ms step_avg:58.70ms
step:485/2330 train_time:28465ms step_avg:58.69ms
step:486/2330 train_time:28527ms step_avg:58.70ms
step:487/2330 train_time:28583ms step_avg:58.69ms
step:488/2330 train_time:28645ms step_avg:58.70ms
step:489/2330 train_time:28701ms step_avg:58.69ms
step:490/2330 train_time:28763ms step_avg:58.70ms
step:491/2330 train_time:28819ms step_avg:58.69ms
step:492/2330 train_time:28880ms step_avg:58.70ms
step:493/2330 train_time:28937ms step_avg:58.70ms
step:494/2330 train_time:28998ms step_avg:58.70ms
step:495/2330 train_time:29055ms step_avg:58.70ms
step:496/2330 train_time:29116ms step_avg:58.70ms
step:497/2330 train_time:29172ms step_avg:58.70ms
step:498/2330 train_time:29234ms step_avg:58.70ms
step:499/2330 train_time:29290ms step_avg:58.70ms
step:500/2330 train_time:29351ms step_avg:58.70ms
step:500/2330 val_loss:4.4105 train_time:29429ms step_avg:58.86ms
step:501/2330 train_time:29448ms step_avg:58.78ms
step:502/2330 train_time:29471ms step_avg:58.71ms
step:503/2330 train_time:29528ms step_avg:58.70ms
step:504/2330 train_time:29595ms step_avg:58.72ms
step:505/2330 train_time:29652ms step_avg:58.72ms
step:506/2330 train_time:29716ms step_avg:58.73ms
step:507/2330 train_time:29773ms step_avg:58.72ms
step:508/2330 train_time:29834ms step_avg:58.73ms
step:509/2330 train_time:29890ms step_avg:58.72ms
step:510/2330 train_time:29950ms step_avg:58.72ms
step:511/2330 train_time:30006ms step_avg:58.72ms
step:512/2330 train_time:30066ms step_avg:58.72ms
step:513/2330 train_time:30121ms step_avg:58.72ms
step:514/2330 train_time:30182ms step_avg:58.72ms
step:515/2330 train_time:30238ms step_avg:58.71ms
step:516/2330 train_time:30298ms step_avg:58.72ms
step:517/2330 train_time:30355ms step_avg:58.71ms
step:518/2330 train_time:30416ms step_avg:58.72ms
step:519/2330 train_time:30474ms step_avg:58.72ms
step:520/2330 train_time:30535ms step_avg:58.72ms
step:521/2330 train_time:30592ms step_avg:58.72ms
step:522/2330 train_time:30654ms step_avg:58.73ms
step:523/2330 train_time:30712ms step_avg:58.72ms
step:524/2330 train_time:30774ms step_avg:58.73ms
step:525/2330 train_time:30830ms step_avg:58.72ms
step:526/2330 train_time:30891ms step_avg:58.73ms
step:527/2330 train_time:30948ms step_avg:58.72ms
step:528/2330 train_time:31008ms step_avg:58.73ms
step:529/2330 train_time:31064ms step_avg:58.72ms
step:530/2330 train_time:31125ms step_avg:58.73ms
step:531/2330 train_time:31181ms step_avg:58.72ms
step:532/2330 train_time:31242ms step_avg:58.73ms
step:533/2330 train_time:31298ms step_avg:58.72ms
step:534/2330 train_time:31359ms step_avg:58.72ms
step:535/2330 train_time:31415ms step_avg:58.72ms
step:536/2330 train_time:31477ms step_avg:58.73ms
step:537/2330 train_time:31533ms step_avg:58.72ms
step:538/2330 train_time:31596ms step_avg:58.73ms
step:539/2330 train_time:31652ms step_avg:58.72ms
step:540/2330 train_time:31714ms step_avg:58.73ms
step:541/2330 train_time:31771ms step_avg:58.73ms
step:542/2330 train_time:31832ms step_avg:58.73ms
step:543/2330 train_time:31889ms step_avg:58.73ms
step:544/2330 train_time:31949ms step_avg:58.73ms
step:545/2330 train_time:32006ms step_avg:58.73ms
step:546/2330 train_time:32067ms step_avg:58.73ms
step:547/2330 train_time:32124ms step_avg:58.73ms
step:548/2330 train_time:32185ms step_avg:58.73ms
step:549/2330 train_time:32241ms step_avg:58.73ms
step:550/2330 train_time:32302ms step_avg:58.73ms
step:551/2330 train_time:32358ms step_avg:58.73ms
step:552/2330 train_time:32419ms step_avg:58.73ms
step:553/2330 train_time:32475ms step_avg:58.73ms
step:554/2330 train_time:32537ms step_avg:58.73ms
step:555/2330 train_time:32592ms step_avg:58.72ms
step:556/2330 train_time:32655ms step_avg:58.73ms
step:557/2330 train_time:32711ms step_avg:58.73ms
step:558/2330 train_time:32774ms step_avg:58.73ms
step:559/2330 train_time:32830ms step_avg:58.73ms
step:560/2330 train_time:32892ms step_avg:58.73ms
step:561/2330 train_time:32949ms step_avg:58.73ms
step:562/2330 train_time:33010ms step_avg:58.74ms
step:563/2330 train_time:33067ms step_avg:58.73ms
step:564/2330 train_time:33128ms step_avg:58.74ms
step:565/2330 train_time:33185ms step_avg:58.73ms
step:566/2330 train_time:33247ms step_avg:58.74ms
step:567/2330 train_time:33303ms step_avg:58.74ms
step:568/2330 train_time:33364ms step_avg:58.74ms
step:569/2330 train_time:33420ms step_avg:58.73ms
step:570/2330 train_time:33481ms step_avg:58.74ms
step:571/2330 train_time:33538ms step_avg:58.73ms
step:572/2330 train_time:33598ms step_avg:58.74ms
step:573/2330 train_time:33654ms step_avg:58.73ms
step:574/2330 train_time:33716ms step_avg:58.74ms
step:575/2330 train_time:33772ms step_avg:58.73ms
step:576/2330 train_time:33834ms step_avg:58.74ms
step:577/2330 train_time:33891ms step_avg:58.74ms
step:578/2330 train_time:33952ms step_avg:58.74ms
step:579/2330 train_time:34010ms step_avg:58.74ms
step:580/2330 train_time:34071ms step_avg:58.74ms
step:581/2330 train_time:34128ms step_avg:58.74ms
step:582/2330 train_time:34188ms step_avg:58.74ms
step:583/2330 train_time:34245ms step_avg:58.74ms
step:584/2330 train_time:34306ms step_avg:58.74ms
step:585/2330 train_time:34362ms step_avg:58.74ms
step:586/2330 train_time:34423ms step_avg:58.74ms
step:587/2330 train_time:34479ms step_avg:58.74ms
step:588/2330 train_time:34540ms step_avg:58.74ms
step:589/2330 train_time:34596ms step_avg:58.74ms
step:590/2330 train_time:34658ms step_avg:58.74ms
step:591/2330 train_time:34714ms step_avg:58.74ms
step:592/2330 train_time:34775ms step_avg:58.74ms
step:593/2330 train_time:34832ms step_avg:58.74ms
step:594/2330 train_time:34894ms step_avg:58.74ms
step:595/2330 train_time:34950ms step_avg:58.74ms
step:596/2330 train_time:35012ms step_avg:58.75ms
step:597/2330 train_time:35069ms step_avg:58.74ms
step:598/2330 train_time:35131ms step_avg:58.75ms
step:599/2330 train_time:35187ms step_avg:58.74ms
step:600/2330 train_time:35249ms step_avg:58.75ms
step:601/2330 train_time:35306ms step_avg:58.75ms
step:602/2330 train_time:35369ms step_avg:58.75ms
step:603/2330 train_time:35426ms step_avg:58.75ms
step:604/2330 train_time:35487ms step_avg:58.75ms
step:605/2330 train_time:35543ms step_avg:58.75ms
step:606/2330 train_time:35604ms step_avg:58.75ms
step:607/2330 train_time:35661ms step_avg:58.75ms
step:608/2330 train_time:35722ms step_avg:58.75ms
step:609/2330 train_time:35779ms step_avg:58.75ms
step:610/2330 train_time:35840ms step_avg:58.75ms
step:611/2330 train_time:35896ms step_avg:58.75ms
step:612/2330 train_time:35957ms step_avg:58.75ms
step:613/2330 train_time:36013ms step_avg:58.75ms
step:614/2330 train_time:36076ms step_avg:58.76ms
step:615/2330 train_time:36132ms step_avg:58.75ms
step:616/2330 train_time:36195ms step_avg:58.76ms
step:617/2330 train_time:36251ms step_avg:58.75ms
step:618/2330 train_time:36314ms step_avg:58.76ms
step:619/2330 train_time:36371ms step_avg:58.76ms
step:620/2330 train_time:36432ms step_avg:58.76ms
step:621/2330 train_time:36489ms step_avg:58.76ms
step:622/2330 train_time:36550ms step_avg:58.76ms
step:623/2330 train_time:36607ms step_avg:58.76ms
step:624/2330 train_time:36668ms step_avg:58.76ms
step:625/2330 train_time:36725ms step_avg:58.76ms
step:626/2330 train_time:36786ms step_avg:58.76ms
step:627/2330 train_time:36842ms step_avg:58.76ms
step:628/2330 train_time:36903ms step_avg:58.76ms
step:629/2330 train_time:36959ms step_avg:58.76ms
step:630/2330 train_time:37020ms step_avg:58.76ms
step:631/2330 train_time:37076ms step_avg:58.76ms
step:632/2330 train_time:37138ms step_avg:58.76ms
step:633/2330 train_time:37194ms step_avg:58.76ms
step:634/2330 train_time:37256ms step_avg:58.76ms
step:635/2330 train_time:37312ms step_avg:58.76ms
step:636/2330 train_time:37375ms step_avg:58.77ms
step:637/2330 train_time:37432ms step_avg:58.76ms
step:638/2330 train_time:37493ms step_avg:58.77ms
step:639/2330 train_time:37550ms step_avg:58.76ms
step:640/2330 train_time:37612ms step_avg:58.77ms
step:641/2330 train_time:37669ms step_avg:58.77ms
step:642/2330 train_time:37730ms step_avg:58.77ms
step:643/2330 train_time:37788ms step_avg:58.77ms
step:644/2330 train_time:37849ms step_avg:58.77ms
step:645/2330 train_time:37906ms step_avg:58.77ms
step:646/2330 train_time:37967ms step_avg:58.77ms
step:647/2330 train_time:38024ms step_avg:58.77ms
step:648/2330 train_time:38085ms step_avg:58.77ms
step:649/2330 train_time:38141ms step_avg:58.77ms
step:650/2330 train_time:38202ms step_avg:58.77ms
step:651/2330 train_time:38258ms step_avg:58.77ms
step:652/2330 train_time:38320ms step_avg:58.77ms
step:653/2330 train_time:38376ms step_avg:58.77ms
step:654/2330 train_time:38437ms step_avg:58.77ms
step:655/2330 train_time:38492ms step_avg:58.77ms
step:656/2330 train_time:38554ms step_avg:58.77ms
step:657/2330 train_time:38611ms step_avg:58.77ms
step:658/2330 train_time:38673ms step_avg:58.77ms
step:659/2330 train_time:38729ms step_avg:58.77ms
step:660/2330 train_time:38792ms step_avg:58.78ms
step:661/2330 train_time:38849ms step_avg:58.77ms
step:662/2330 train_time:38910ms step_avg:58.78ms
step:663/2330 train_time:38967ms step_avg:58.77ms
step:664/2330 train_time:39029ms step_avg:58.78ms
step:665/2330 train_time:39085ms step_avg:58.78ms
step:666/2330 train_time:39147ms step_avg:58.78ms
step:667/2330 train_time:39203ms step_avg:58.78ms
step:668/2330 train_time:39264ms step_avg:58.78ms
step:669/2330 train_time:39320ms step_avg:58.77ms
step:670/2330 train_time:39381ms step_avg:58.78ms
step:671/2330 train_time:39437ms step_avg:58.77ms
step:672/2330 train_time:39499ms step_avg:58.78ms
step:673/2330 train_time:39555ms step_avg:58.77ms
step:674/2330 train_time:39616ms step_avg:58.78ms
step:675/2330 train_time:39672ms step_avg:58.77ms
step:676/2330 train_time:39735ms step_avg:58.78ms
step:677/2330 train_time:39791ms step_avg:58.78ms
step:678/2330 train_time:39852ms step_avg:58.78ms
step:679/2330 train_time:39909ms step_avg:58.78ms
step:680/2330 train_time:39971ms step_avg:58.78ms
step:681/2330 train_time:40028ms step_avg:58.78ms
step:682/2330 train_time:40089ms step_avg:58.78ms
step:683/2330 train_time:40146ms step_avg:58.78ms
step:684/2330 train_time:40207ms step_avg:58.78ms
step:685/2330 train_time:40264ms step_avg:58.78ms
step:686/2330 train_time:40325ms step_avg:58.78ms
step:687/2330 train_time:40382ms step_avg:58.78ms
step:688/2330 train_time:40442ms step_avg:58.78ms
step:689/2330 train_time:40498ms step_avg:58.78ms
step:690/2330 train_time:40560ms step_avg:58.78ms
step:691/2330 train_time:40616ms step_avg:58.78ms
step:692/2330 train_time:40677ms step_avg:58.78ms
step:693/2330 train_time:40733ms step_avg:58.78ms
step:694/2330 train_time:40795ms step_avg:58.78ms
step:695/2330 train_time:40851ms step_avg:58.78ms
step:696/2330 train_time:40913ms step_avg:58.78ms
step:697/2330 train_time:40969ms step_avg:58.78ms
step:698/2330 train_time:41031ms step_avg:58.78ms
step:699/2330 train_time:41088ms step_avg:58.78ms
step:700/2330 train_time:41150ms step_avg:58.79ms
step:701/2330 train_time:41207ms step_avg:58.78ms
step:702/2330 train_time:41269ms step_avg:58.79ms
step:703/2330 train_time:41326ms step_avg:58.78ms
step:704/2330 train_time:41388ms step_avg:58.79ms
step:705/2330 train_time:41444ms step_avg:58.79ms
step:706/2330 train_time:41507ms step_avg:58.79ms
step:707/2330 train_time:41563ms step_avg:58.79ms
step:708/2330 train_time:41624ms step_avg:58.79ms
step:709/2330 train_time:41680ms step_avg:58.79ms
step:710/2330 train_time:41741ms step_avg:58.79ms
step:711/2330 train_time:41797ms step_avg:58.79ms
step:712/2330 train_time:41858ms step_avg:58.79ms
step:713/2330 train_time:41914ms step_avg:58.79ms
step:714/2330 train_time:41976ms step_avg:58.79ms
step:715/2330 train_time:42032ms step_avg:58.79ms
step:716/2330 train_time:42095ms step_avg:58.79ms
step:717/2330 train_time:42151ms step_avg:58.79ms
step:718/2330 train_time:42214ms step_avg:58.79ms
step:719/2330 train_time:42272ms step_avg:58.79ms
step:720/2330 train_time:42332ms step_avg:58.79ms
step:721/2330 train_time:42389ms step_avg:58.79ms
step:722/2330 train_time:42451ms step_avg:58.80ms
step:723/2330 train_time:42508ms step_avg:58.79ms
step:724/2330 train_time:42571ms step_avg:58.80ms
step:725/2330 train_time:42627ms step_avg:58.80ms
step:726/2330 train_time:42689ms step_avg:58.80ms
step:727/2330 train_time:42745ms step_avg:58.80ms
step:728/2330 train_time:42807ms step_avg:58.80ms
step:729/2330 train_time:42863ms step_avg:58.80ms
step:730/2330 train_time:42924ms step_avg:58.80ms
step:731/2330 train_time:42981ms step_avg:58.80ms
step:732/2330 train_time:43042ms step_avg:58.80ms
step:733/2330 train_time:43098ms step_avg:58.80ms
step:734/2330 train_time:43159ms step_avg:58.80ms
step:735/2330 train_time:43215ms step_avg:58.80ms
step:736/2330 train_time:43277ms step_avg:58.80ms
step:737/2330 train_time:43333ms step_avg:58.80ms
step:738/2330 train_time:43395ms step_avg:58.80ms
step:739/2330 train_time:43452ms step_avg:58.80ms
step:740/2330 train_time:43513ms step_avg:58.80ms
step:741/2330 train_time:43569ms step_avg:58.80ms
step:742/2330 train_time:43631ms step_avg:58.80ms
step:743/2330 train_time:43689ms step_avg:58.80ms
step:744/2330 train_time:43749ms step_avg:58.80ms
step:745/2330 train_time:43806ms step_avg:58.80ms
step:746/2330 train_time:43867ms step_avg:58.80ms
step:747/2330 train_time:43923ms step_avg:58.80ms
step:748/2330 train_time:43986ms step_avg:58.80ms
step:749/2330 train_time:44042ms step_avg:58.80ms
step:750/2330 train_time:44103ms step_avg:58.80ms
step:750/2330 val_loss:4.2214 train_time:44181ms step_avg:58.91ms
step:751/2330 train_time:44199ms step_avg:58.85ms
step:752/2330 train_time:44224ms step_avg:58.81ms
step:753/2330 train_time:44282ms step_avg:58.81ms
step:754/2330 train_time:44345ms step_avg:58.81ms
step:755/2330 train_time:44403ms step_avg:58.81ms
step:756/2330 train_time:44467ms step_avg:58.82ms
step:757/2330 train_time:44523ms step_avg:58.82ms
step:758/2330 train_time:44582ms step_avg:58.82ms
step:759/2330 train_time:44638ms step_avg:58.81ms
step:760/2330 train_time:44699ms step_avg:58.81ms
step:761/2330 train_time:44755ms step_avg:58.81ms
step:762/2330 train_time:44815ms step_avg:58.81ms
step:763/2330 train_time:44872ms step_avg:58.81ms
step:764/2330 train_time:44932ms step_avg:58.81ms
step:765/2330 train_time:44989ms step_avg:58.81ms
step:766/2330 train_time:45049ms step_avg:58.81ms
step:767/2330 train_time:45106ms step_avg:58.81ms
step:768/2330 train_time:45168ms step_avg:58.81ms
step:769/2330 train_time:45226ms step_avg:58.81ms
step:770/2330 train_time:45288ms step_avg:58.82ms
step:771/2330 train_time:45346ms step_avg:58.81ms
step:772/2330 train_time:45410ms step_avg:58.82ms
step:773/2330 train_time:45469ms step_avg:58.82ms
step:774/2330 train_time:45533ms step_avg:58.83ms
step:775/2330 train_time:45591ms step_avg:58.83ms
step:776/2330 train_time:45653ms step_avg:58.83ms
step:777/2330 train_time:45710ms step_avg:58.83ms
step:778/2330 train_time:45771ms step_avg:58.83ms
step:779/2330 train_time:45827ms step_avg:58.83ms
step:780/2330 train_time:45889ms step_avg:58.83ms
step:781/2330 train_time:45945ms step_avg:58.83ms
step:782/2330 train_time:46006ms step_avg:58.83ms
step:783/2330 train_time:46062ms step_avg:58.83ms
step:784/2330 train_time:46124ms step_avg:58.83ms
step:785/2330 train_time:46181ms step_avg:58.83ms
step:786/2330 train_time:46243ms step_avg:58.83ms
step:787/2330 train_time:46300ms step_avg:58.83ms
step:788/2330 train_time:46363ms step_avg:58.84ms
step:789/2330 train_time:46420ms step_avg:58.83ms
step:790/2330 train_time:46483ms step_avg:58.84ms
step:791/2330 train_time:46540ms step_avg:58.84ms
step:792/2330 train_time:46602ms step_avg:58.84ms
step:793/2330 train_time:46659ms step_avg:58.84ms
step:794/2330 train_time:46721ms step_avg:58.84ms
step:795/2330 train_time:46777ms step_avg:58.84ms
step:796/2330 train_time:46839ms step_avg:58.84ms
step:797/2330 train_time:46896ms step_avg:58.84ms
step:798/2330 train_time:46958ms step_avg:58.84ms
step:799/2330 train_time:47014ms step_avg:58.84ms
step:800/2330 train_time:47077ms step_avg:58.85ms
step:801/2330 train_time:47134ms step_avg:58.84ms
step:802/2330 train_time:47196ms step_avg:58.85ms
step:803/2330 train_time:47254ms step_avg:58.85ms
step:804/2330 train_time:47318ms step_avg:58.85ms
step:805/2330 train_time:47376ms step_avg:58.85ms
step:806/2330 train_time:47438ms step_avg:58.86ms
step:807/2330 train_time:47495ms step_avg:58.85ms
step:808/2330 train_time:47559ms step_avg:58.86ms
step:809/2330 train_time:47616ms step_avg:58.86ms
step:810/2330 train_time:47679ms step_avg:58.86ms
step:811/2330 train_time:47736ms step_avg:58.86ms
step:812/2330 train_time:47798ms step_avg:58.86ms
step:813/2330 train_time:47855ms step_avg:58.86ms
step:814/2330 train_time:47916ms step_avg:58.87ms
step:815/2330 train_time:47974ms step_avg:58.86ms
step:816/2330 train_time:48035ms step_avg:58.87ms
step:817/2330 train_time:48092ms step_avg:58.86ms
step:818/2330 train_time:48154ms step_avg:58.87ms
step:819/2330 train_time:48212ms step_avg:58.87ms
step:820/2330 train_time:48276ms step_avg:58.87ms
step:821/2330 train_time:48333ms step_avg:58.87ms
step:822/2330 train_time:48397ms step_avg:58.88ms
step:823/2330 train_time:48454ms step_avg:58.88ms
step:824/2330 train_time:48518ms step_avg:58.88ms
step:825/2330 train_time:48575ms step_avg:58.88ms
step:826/2330 train_time:48638ms step_avg:58.88ms
step:827/2330 train_time:48695ms step_avg:58.88ms
step:828/2330 train_time:48757ms step_avg:58.89ms
step:829/2330 train_time:48815ms step_avg:58.88ms
step:830/2330 train_time:48877ms step_avg:58.89ms
step:831/2330 train_time:48934ms step_avg:58.89ms
step:832/2330 train_time:48994ms step_avg:58.89ms
step:833/2330 train_time:49051ms step_avg:58.88ms
step:834/2330 train_time:49114ms step_avg:58.89ms
step:835/2330 train_time:49171ms step_avg:58.89ms
step:836/2330 train_time:49235ms step_avg:58.89ms
step:837/2330 train_time:49293ms step_avg:58.89ms
step:838/2330 train_time:49354ms step_avg:58.90ms
step:839/2330 train_time:49412ms step_avg:58.89ms
step:840/2330 train_time:49475ms step_avg:58.90ms
step:841/2330 train_time:49532ms step_avg:58.90ms
step:842/2330 train_time:49595ms step_avg:58.90ms
step:843/2330 train_time:49653ms step_avg:58.90ms
step:844/2330 train_time:49716ms step_avg:58.90ms
step:845/2330 train_time:49773ms step_avg:58.90ms
step:846/2330 train_time:49836ms step_avg:58.91ms
step:847/2330 train_time:49893ms step_avg:58.91ms
step:848/2330 train_time:49955ms step_avg:58.91ms
step:849/2330 train_time:50012ms step_avg:58.91ms
step:850/2330 train_time:50074ms step_avg:58.91ms
step:851/2330 train_time:50132ms step_avg:58.91ms
step:852/2330 train_time:50193ms step_avg:58.91ms
step:853/2330 train_time:50251ms step_avg:58.91ms
step:854/2330 train_time:50314ms step_avg:58.92ms
step:855/2330 train_time:50372ms step_avg:58.91ms
step:856/2330 train_time:50433ms step_avg:58.92ms
step:857/2330 train_time:50492ms step_avg:58.92ms
step:858/2330 train_time:50554ms step_avg:58.92ms
step:859/2330 train_time:50612ms step_avg:58.92ms
step:860/2330 train_time:50673ms step_avg:58.92ms
step:861/2330 train_time:50731ms step_avg:58.92ms
step:862/2330 train_time:50794ms step_avg:58.93ms
step:863/2330 train_time:50851ms step_avg:58.92ms
step:864/2330 train_time:50913ms step_avg:58.93ms
step:865/2330 train_time:50970ms step_avg:58.92ms
step:866/2330 train_time:51032ms step_avg:58.93ms
step:867/2330 train_time:51090ms step_avg:58.93ms
step:868/2330 train_time:51153ms step_avg:58.93ms
step:869/2330 train_time:51210ms step_avg:58.93ms
step:870/2330 train_time:51274ms step_avg:58.94ms
step:871/2330 train_time:51331ms step_avg:58.93ms
step:872/2330 train_time:51393ms step_avg:58.94ms
step:873/2330 train_time:51451ms step_avg:58.94ms
step:874/2330 train_time:51513ms step_avg:58.94ms
step:875/2330 train_time:51571ms step_avg:58.94ms
step:876/2330 train_time:51633ms step_avg:58.94ms
step:877/2330 train_time:51691ms step_avg:58.94ms
step:878/2330 train_time:51753ms step_avg:58.94ms
step:879/2330 train_time:51811ms step_avg:58.94ms
step:880/2330 train_time:51873ms step_avg:58.95ms
step:881/2330 train_time:51931ms step_avg:58.95ms
step:882/2330 train_time:51992ms step_avg:58.95ms
step:883/2330 train_time:52049ms step_avg:58.95ms
step:884/2330 train_time:52112ms step_avg:58.95ms
step:885/2330 train_time:52170ms step_avg:58.95ms
step:886/2330 train_time:52233ms step_avg:58.95ms
step:887/2330 train_time:52290ms step_avg:58.95ms
step:888/2330 train_time:52353ms step_avg:58.96ms
step:889/2330 train_time:52411ms step_avg:58.96ms
step:890/2330 train_time:52473ms step_avg:58.96ms
step:891/2330 train_time:52531ms step_avg:58.96ms
step:892/2330 train_time:52594ms step_avg:58.96ms
step:893/2330 train_time:52651ms step_avg:58.96ms
step:894/2330 train_time:52714ms step_avg:58.96ms
step:895/2330 train_time:52772ms step_avg:58.96ms
step:896/2330 train_time:52834ms step_avg:58.97ms
step:897/2330 train_time:52892ms step_avg:58.97ms
step:898/2330 train_time:52954ms step_avg:58.97ms
step:899/2330 train_time:53012ms step_avg:58.97ms
step:900/2330 train_time:53074ms step_avg:58.97ms
step:901/2330 train_time:53132ms step_avg:58.97ms
step:902/2330 train_time:53193ms step_avg:58.97ms
step:903/2330 train_time:53250ms step_avg:58.97ms
step:904/2330 train_time:53312ms step_avg:58.97ms
step:905/2330 train_time:53370ms step_avg:58.97ms
step:906/2330 train_time:53433ms step_avg:58.98ms
step:907/2330 train_time:53491ms step_avg:58.98ms
step:908/2330 train_time:53553ms step_avg:58.98ms
step:909/2330 train_time:53611ms step_avg:58.98ms
step:910/2330 train_time:53673ms step_avg:58.98ms
step:911/2330 train_time:53731ms step_avg:58.98ms
step:912/2330 train_time:53793ms step_avg:58.98ms
step:913/2330 train_time:53850ms step_avg:58.98ms
step:914/2330 train_time:53912ms step_avg:58.98ms
step:915/2330 train_time:53969ms step_avg:58.98ms
step:916/2330 train_time:54032ms step_avg:58.99ms
step:917/2330 train_time:54089ms step_avg:58.99ms
step:918/2330 train_time:54150ms step_avg:58.99ms
step:919/2330 train_time:54208ms step_avg:58.99ms
step:920/2330 train_time:54270ms step_avg:58.99ms
step:921/2330 train_time:54328ms step_avg:58.99ms
step:922/2330 train_time:54389ms step_avg:58.99ms
step:923/2330 train_time:54446ms step_avg:58.99ms
step:924/2330 train_time:54508ms step_avg:58.99ms
step:925/2330 train_time:54565ms step_avg:58.99ms
step:926/2330 train_time:54629ms step_avg:58.99ms
step:927/2330 train_time:54686ms step_avg:58.99ms
step:928/2330 train_time:54748ms step_avg:59.00ms
step:929/2330 train_time:54804ms step_avg:58.99ms
step:930/2330 train_time:54867ms step_avg:59.00ms
step:931/2330 train_time:54923ms step_avg:58.99ms
step:932/2330 train_time:54985ms step_avg:59.00ms
step:933/2330 train_time:55042ms step_avg:58.99ms
step:934/2330 train_time:55104ms step_avg:59.00ms
step:935/2330 train_time:55161ms step_avg:59.00ms
step:936/2330 train_time:55223ms step_avg:59.00ms
step:937/2330 train_time:55280ms step_avg:59.00ms
step:938/2330 train_time:55341ms step_avg:59.00ms
step:939/2330 train_time:55398ms step_avg:59.00ms
step:940/2330 train_time:55460ms step_avg:59.00ms
step:941/2330 train_time:55518ms step_avg:59.00ms
step:942/2330 train_time:55580ms step_avg:59.00ms
step:943/2330 train_time:55637ms step_avg:59.00ms
step:944/2330 train_time:55700ms step_avg:59.00ms
step:945/2330 train_time:55757ms step_avg:59.00ms
step:946/2330 train_time:55820ms step_avg:59.01ms
step:947/2330 train_time:55877ms step_avg:59.00ms
step:948/2330 train_time:55939ms step_avg:59.01ms
step:949/2330 train_time:55996ms step_avg:59.01ms
step:950/2330 train_time:56059ms step_avg:59.01ms
step:951/2330 train_time:56117ms step_avg:59.01ms
step:952/2330 train_time:56178ms step_avg:59.01ms
step:953/2330 train_time:56235ms step_avg:59.01ms
step:954/2330 train_time:56298ms step_avg:59.01ms
step:955/2330 train_time:56355ms step_avg:59.01ms
step:956/2330 train_time:56418ms step_avg:59.01ms
step:957/2330 train_time:56476ms step_avg:59.01ms
step:958/2330 train_time:56538ms step_avg:59.02ms
step:959/2330 train_time:56596ms step_avg:59.02ms
step:960/2330 train_time:56658ms step_avg:59.02ms
step:961/2330 train_time:56715ms step_avg:59.02ms
step:962/2330 train_time:56778ms step_avg:59.02ms
step:963/2330 train_time:56835ms step_avg:59.02ms
step:964/2330 train_time:56897ms step_avg:59.02ms
step:965/2330 train_time:56955ms step_avg:59.02ms
step:966/2330 train_time:57018ms step_avg:59.02ms
step:967/2330 train_time:57076ms step_avg:59.02ms
step:968/2330 train_time:57136ms step_avg:59.03ms
step:969/2330 train_time:57193ms step_avg:59.02ms
step:970/2330 train_time:57256ms step_avg:59.03ms
step:971/2330 train_time:57314ms step_avg:59.03ms
step:972/2330 train_time:57376ms step_avg:59.03ms
step:973/2330 train_time:57434ms step_avg:59.03ms
step:974/2330 train_time:57495ms step_avg:59.03ms
step:975/2330 train_time:57553ms step_avg:59.03ms
step:976/2330 train_time:57616ms step_avg:59.03ms
step:977/2330 train_time:57674ms step_avg:59.03ms
step:978/2330 train_time:57737ms step_avg:59.04ms
step:979/2330 train_time:57794ms step_avg:59.03ms
step:980/2330 train_time:57857ms step_avg:59.04ms
step:981/2330 train_time:57914ms step_avg:59.04ms
step:982/2330 train_time:57977ms step_avg:59.04ms
step:983/2330 train_time:58034ms step_avg:59.04ms
step:984/2330 train_time:58097ms step_avg:59.04ms
step:985/2330 train_time:58154ms step_avg:59.04ms
step:986/2330 train_time:58216ms step_avg:59.04ms
step:987/2330 train_time:58273ms step_avg:59.04ms
step:988/2330 train_time:58336ms step_avg:59.04ms
step:989/2330 train_time:58393ms step_avg:59.04ms
step:990/2330 train_time:58455ms step_avg:59.05ms
step:991/2330 train_time:58513ms step_avg:59.04ms
step:992/2330 train_time:58575ms step_avg:59.05ms
step:993/2330 train_time:58633ms step_avg:59.05ms
step:994/2330 train_time:58695ms step_avg:59.05ms
step:995/2330 train_time:58752ms step_avg:59.05ms
step:996/2330 train_time:58815ms step_avg:59.05ms
step:997/2330 train_time:58873ms step_avg:59.05ms
step:998/2330 train_time:58935ms step_avg:59.05ms
step:999/2330 train_time:58993ms step_avg:59.05ms
step:1000/2330 train_time:59055ms step_avg:59.06ms
step:1000/2330 val_loss:4.0840 train_time:59135ms step_avg:59.14ms
step:1001/2330 train_time:59155ms step_avg:59.10ms
step:1002/2330 train_time:59177ms step_avg:59.06ms
step:1003/2330 train_time:59234ms step_avg:59.06ms
step:1004/2330 train_time:59299ms step_avg:59.06ms
step:1005/2330 train_time:59356ms step_avg:59.06ms
step:1006/2330 train_time:59420ms step_avg:59.07ms
step:1007/2330 train_time:59476ms step_avg:59.06ms
step:1008/2330 train_time:59538ms step_avg:59.07ms
step:1009/2330 train_time:59595ms step_avg:59.06ms
step:1010/2330 train_time:59657ms step_avg:59.07ms
step:1011/2330 train_time:59713ms step_avg:59.06ms
step:1012/2330 train_time:59774ms step_avg:59.07ms
step:1013/2330 train_time:59831ms step_avg:59.06ms
step:1014/2330 train_time:59892ms step_avg:59.07ms
step:1015/2330 train_time:59949ms step_avg:59.06ms
step:1016/2330 train_time:60009ms step_avg:59.06ms
step:1017/2330 train_time:60068ms step_avg:59.06ms
step:1018/2330 train_time:60134ms step_avg:59.07ms
step:1019/2330 train_time:60192ms step_avg:59.07ms
step:1020/2330 train_time:60258ms step_avg:59.08ms
step:1021/2330 train_time:60316ms step_avg:59.08ms
step:1022/2330 train_time:60378ms step_avg:59.08ms
step:1023/2330 train_time:60435ms step_avg:59.08ms
step:1024/2330 train_time:60498ms step_avg:59.08ms
step:1025/2330 train_time:60554ms step_avg:59.08ms
step:1026/2330 train_time:60616ms step_avg:59.08ms
step:1027/2330 train_time:60673ms step_avg:59.08ms
step:1028/2330 train_time:60735ms step_avg:59.08ms
step:1029/2330 train_time:60792ms step_avg:59.08ms
step:1030/2330 train_time:60853ms step_avg:59.08ms
step:1031/2330 train_time:60909ms step_avg:59.08ms
step:1032/2330 train_time:60971ms step_avg:59.08ms
step:1033/2330 train_time:61028ms step_avg:59.08ms
step:1034/2330 train_time:61092ms step_avg:59.08ms
step:1035/2330 train_time:61150ms step_avg:59.08ms
step:1036/2330 train_time:61215ms step_avg:59.09ms
step:1037/2330 train_time:61274ms step_avg:59.09ms
step:1038/2330 train_time:61338ms step_avg:59.09ms
step:1039/2330 train_time:61395ms step_avg:59.09ms
step:1040/2330 train_time:61458ms step_avg:59.09ms
step:1041/2330 train_time:61515ms step_avg:59.09ms
step:1042/2330 train_time:61577ms step_avg:59.10ms
step:1043/2330 train_time:61634ms step_avg:59.09ms
step:1044/2330 train_time:61696ms step_avg:59.10ms
step:1045/2330 train_time:61753ms step_avg:59.09ms
step:1046/2330 train_time:61815ms step_avg:59.10ms
step:1047/2330 train_time:61872ms step_avg:59.09ms
step:1048/2330 train_time:61933ms step_avg:59.10ms
step:1049/2330 train_time:61990ms step_avg:59.09ms
step:1050/2330 train_time:62053ms step_avg:59.10ms
step:1051/2330 train_time:62111ms step_avg:59.10ms
step:1052/2330 train_time:62175ms step_avg:59.10ms
step:1053/2330 train_time:62232ms step_avg:59.10ms
step:1054/2330 train_time:62296ms step_avg:59.10ms
step:1055/2330 train_time:62354ms step_avg:59.10ms
step:1056/2330 train_time:62417ms step_avg:59.11ms
step:1057/2330 train_time:62474ms step_avg:59.11ms
step:1058/2330 train_time:62536ms step_avg:59.11ms
step:1059/2330 train_time:62593ms step_avg:59.11ms
step:1060/2330 train_time:62656ms step_avg:59.11ms
step:1061/2330 train_time:62713ms step_avg:59.11ms
step:1062/2330 train_time:62776ms step_avg:59.11ms
step:1063/2330 train_time:62832ms step_avg:59.11ms
step:1064/2330 train_time:62893ms step_avg:59.11ms
step:1065/2330 train_time:62950ms step_avg:59.11ms
step:1066/2330 train_time:63013ms step_avg:59.11ms
step:1067/2330 train_time:63070ms step_avg:59.11ms
step:1068/2330 train_time:63134ms step_avg:59.11ms
step:1069/2330 train_time:63192ms step_avg:59.11ms
step:1070/2330 train_time:63256ms step_avg:59.12ms
step:1071/2330 train_time:63313ms step_avg:59.12ms
step:1072/2330 train_time:63376ms step_avg:59.12ms
step:1073/2330 train_time:63434ms step_avg:59.12ms
step:1074/2330 train_time:63497ms step_avg:59.12ms
step:1075/2330 train_time:63554ms step_avg:59.12ms
step:1076/2330 train_time:63616ms step_avg:59.12ms
step:1077/2330 train_time:63673ms step_avg:59.12ms
step:1078/2330 train_time:63735ms step_avg:59.12ms
step:1079/2330 train_time:63792ms step_avg:59.12ms
step:1080/2330 train_time:63853ms step_avg:59.12ms
step:1081/2330 train_time:63910ms step_avg:59.12ms
step:1082/2330 train_time:63973ms step_avg:59.12ms
step:1083/2330 train_time:64030ms step_avg:59.12ms
step:1084/2330 train_time:64092ms step_avg:59.13ms
step:1085/2330 train_time:64150ms step_avg:59.12ms
step:1086/2330 train_time:64214ms step_avg:59.13ms
step:1087/2330 train_time:64271ms step_avg:59.13ms
step:1088/2330 train_time:64335ms step_avg:59.13ms
step:1089/2330 train_time:64392ms step_avg:59.13ms
step:1090/2330 train_time:64456ms step_avg:59.13ms
step:1091/2330 train_time:64513ms step_avg:59.13ms
step:1092/2330 train_time:64576ms step_avg:59.14ms
step:1093/2330 train_time:64633ms step_avg:59.13ms
step:1094/2330 train_time:64696ms step_avg:59.14ms
step:1095/2330 train_time:64752ms step_avg:59.13ms
step:1096/2330 train_time:64816ms step_avg:59.14ms
step:1097/2330 train_time:64872ms step_avg:59.14ms
step:1098/2330 train_time:64935ms step_avg:59.14ms
step:1099/2330 train_time:64992ms step_avg:59.14ms
step:1100/2330 train_time:65054ms step_avg:59.14ms
step:1101/2330 train_time:65112ms step_avg:59.14ms
step:1102/2330 train_time:65174ms step_avg:59.14ms
step:1103/2330 train_time:65232ms step_avg:59.14ms
step:1104/2330 train_time:65294ms step_avg:59.14ms
step:1105/2330 train_time:65352ms step_avg:59.14ms
step:1106/2330 train_time:65415ms step_avg:59.15ms
step:1107/2330 train_time:65473ms step_avg:59.14ms
step:1108/2330 train_time:65534ms step_avg:59.15ms
step:1109/2330 train_time:65592ms step_avg:59.15ms
step:1110/2330 train_time:65655ms step_avg:59.15ms
step:1111/2330 train_time:65712ms step_avg:59.15ms
step:1112/2330 train_time:65775ms step_avg:59.15ms
step:1113/2330 train_time:65831ms step_avg:59.15ms
step:1114/2330 train_time:65894ms step_avg:59.15ms
step:1115/2330 train_time:65951ms step_avg:59.15ms
step:1116/2330 train_time:66013ms step_avg:59.15ms
step:1117/2330 train_time:66071ms step_avg:59.15ms
step:1118/2330 train_time:66133ms step_avg:59.15ms
step:1119/2330 train_time:66190ms step_avg:59.15ms
step:1120/2330 train_time:66255ms step_avg:59.16ms
step:1121/2330 train_time:66313ms step_avg:59.16ms
step:1122/2330 train_time:66376ms step_avg:59.16ms
step:1123/2330 train_time:66433ms step_avg:59.16ms
step:1124/2330 train_time:66495ms step_avg:59.16ms
step:1125/2330 train_time:66553ms step_avg:59.16ms
step:1126/2330 train_time:66615ms step_avg:59.16ms
step:1127/2330 train_time:66673ms step_avg:59.16ms
step:1128/2330 train_time:66735ms step_avg:59.16ms
step:1129/2330 train_time:66793ms step_avg:59.16ms
step:1130/2330 train_time:66854ms step_avg:59.16ms
step:1131/2330 train_time:66911ms step_avg:59.16ms
step:1132/2330 train_time:66974ms step_avg:59.16ms
step:1133/2330 train_time:67032ms step_avg:59.16ms
step:1134/2330 train_time:67093ms step_avg:59.17ms
step:1135/2330 train_time:67150ms step_avg:59.16ms
step:1136/2330 train_time:67214ms step_avg:59.17ms
step:1137/2330 train_time:67272ms step_avg:59.17ms
step:1138/2330 train_time:67334ms step_avg:59.17ms
step:1139/2330 train_time:67392ms step_avg:59.17ms
step:1140/2330 train_time:67454ms step_avg:59.17ms
step:1141/2330 train_time:67511ms step_avg:59.17ms
step:1142/2330 train_time:67574ms step_avg:59.17ms
step:1143/2330 train_time:67632ms step_avg:59.17ms
step:1144/2330 train_time:67694ms step_avg:59.17ms
step:1145/2330 train_time:67751ms step_avg:59.17ms
step:1146/2330 train_time:67814ms step_avg:59.17ms
step:1147/2330 train_time:67871ms step_avg:59.17ms
step:1148/2330 train_time:67933ms step_avg:59.18ms
step:1149/2330 train_time:67991ms step_avg:59.17ms
step:1150/2330 train_time:68054ms step_avg:59.18ms
step:1151/2330 train_time:68111ms step_avg:59.18ms
step:1152/2330 train_time:68173ms step_avg:59.18ms
step:1153/2330 train_time:68230ms step_avg:59.18ms
step:1154/2330 train_time:68293ms step_avg:59.18ms
step:1155/2330 train_time:68350ms step_avg:59.18ms
step:1156/2330 train_time:68414ms step_avg:59.18ms
step:1157/2330 train_time:68471ms step_avg:59.18ms
step:1158/2330 train_time:68534ms step_avg:59.18ms
step:1159/2330 train_time:68591ms step_avg:59.18ms
step:1160/2330 train_time:68654ms step_avg:59.18ms
step:1161/2330 train_time:68712ms step_avg:59.18ms
step:1162/2330 train_time:68774ms step_avg:59.19ms
step:1163/2330 train_time:68832ms step_avg:59.18ms
step:1164/2330 train_time:68894ms step_avg:59.19ms
step:1165/2330 train_time:68952ms step_avg:59.19ms
step:1166/2330 train_time:69014ms step_avg:59.19ms
step:1167/2330 train_time:69071ms step_avg:59.19ms
step:1168/2330 train_time:69134ms step_avg:59.19ms
step:1169/2330 train_time:69191ms step_avg:59.19ms
step:1170/2330 train_time:69253ms step_avg:59.19ms
step:1171/2330 train_time:69311ms step_avg:59.19ms
step:1172/2330 train_time:69373ms step_avg:59.19ms
step:1173/2330 train_time:69430ms step_avg:59.19ms
step:1174/2330 train_time:69492ms step_avg:59.19ms
step:1175/2330 train_time:69550ms step_avg:59.19ms
step:1176/2330 train_time:69613ms step_avg:59.19ms
step:1177/2330 train_time:69670ms step_avg:59.19ms
step:1178/2330 train_time:69733ms step_avg:59.20ms
step:1179/2330 train_time:69789ms step_avg:59.19ms
step:1180/2330 train_time:69851ms step_avg:59.20ms
step:1181/2330 train_time:69909ms step_avg:59.19ms
step:1182/2330 train_time:69972ms step_avg:59.20ms
step:1183/2330 train_time:70030ms step_avg:59.20ms
step:1184/2330 train_time:70092ms step_avg:59.20ms
step:1185/2330 train_time:70149ms step_avg:59.20ms
step:1186/2330 train_time:70211ms step_avg:59.20ms
step:1187/2330 train_time:70268ms step_avg:59.20ms
step:1188/2330 train_time:70332ms step_avg:59.20ms
step:1189/2330 train_time:70389ms step_avg:59.20ms
step:1190/2330 train_time:70452ms step_avg:59.20ms
step:1191/2330 train_time:70510ms step_avg:59.20ms
step:1192/2330 train_time:70573ms step_avg:59.21ms
step:1193/2330 train_time:70630ms step_avg:59.20ms
step:1194/2330 train_time:70693ms step_avg:59.21ms
step:1195/2330 train_time:70749ms step_avg:59.20ms
step:1196/2330 train_time:70812ms step_avg:59.21ms
step:1197/2330 train_time:70870ms step_avg:59.21ms
step:1198/2330 train_time:70932ms step_avg:59.21ms
step:1199/2330 train_time:70989ms step_avg:59.21ms
step:1200/2330 train_time:71052ms step_avg:59.21ms
step:1201/2330 train_time:71110ms step_avg:59.21ms
step:1202/2330 train_time:71173ms step_avg:59.21ms
step:1203/2330 train_time:71231ms step_avg:59.21ms
step:1204/2330 train_time:71293ms step_avg:59.21ms
step:1205/2330 train_time:71350ms step_avg:59.21ms
step:1206/2330 train_time:71412ms step_avg:59.21ms
step:1207/2330 train_time:71470ms step_avg:59.21ms
step:1208/2330 train_time:71533ms step_avg:59.22ms
step:1209/2330 train_time:71591ms step_avg:59.21ms
step:1210/2330 train_time:71653ms step_avg:59.22ms
step:1211/2330 train_time:71711ms step_avg:59.22ms
step:1212/2330 train_time:71774ms step_avg:59.22ms
step:1213/2330 train_time:71831ms step_avg:59.22ms
step:1214/2330 train_time:71894ms step_avg:59.22ms
step:1215/2330 train_time:71951ms step_avg:59.22ms
step:1216/2330 train_time:72013ms step_avg:59.22ms
step:1217/2330 train_time:72070ms step_avg:59.22ms
step:1218/2330 train_time:72133ms step_avg:59.22ms
step:1219/2330 train_time:72190ms step_avg:59.22ms
step:1220/2330 train_time:72253ms step_avg:59.22ms
step:1221/2330 train_time:72310ms step_avg:59.22ms
step:1222/2330 train_time:72373ms step_avg:59.22ms
step:1223/2330 train_time:72430ms step_avg:59.22ms
step:1224/2330 train_time:72493ms step_avg:59.23ms
step:1225/2330 train_time:72550ms step_avg:59.22ms
step:1226/2330 train_time:72614ms step_avg:59.23ms
step:1227/2330 train_time:72671ms step_avg:59.23ms
step:1228/2330 train_time:72733ms step_avg:59.23ms
step:1229/2330 train_time:72790ms step_avg:59.23ms
step:1230/2330 train_time:72853ms step_avg:59.23ms
step:1231/2330 train_time:72910ms step_avg:59.23ms
step:1232/2330 train_time:72974ms step_avg:59.23ms
step:1233/2330 train_time:73031ms step_avg:59.23ms
step:1234/2330 train_time:73093ms step_avg:59.23ms
step:1235/2330 train_time:73150ms step_avg:59.23ms
step:1236/2330 train_time:73212ms step_avg:59.23ms
step:1237/2330 train_time:73270ms step_avg:59.23ms
step:1238/2330 train_time:73332ms step_avg:59.23ms
step:1239/2330 train_time:73389ms step_avg:59.23ms
step:1240/2330 train_time:73452ms step_avg:59.24ms
step:1241/2330 train_time:73510ms step_avg:59.23ms
step:1242/2330 train_time:73572ms step_avg:59.24ms
step:1243/2330 train_time:73629ms step_avg:59.24ms
step:1244/2330 train_time:73692ms step_avg:59.24ms
step:1245/2330 train_time:73749ms step_avg:59.24ms
step:1246/2330 train_time:73813ms step_avg:59.24ms
step:1247/2330 train_time:73871ms step_avg:59.24ms
step:1248/2330 train_time:73934ms step_avg:59.24ms
step:1249/2330 train_time:73991ms step_avg:59.24ms
step:1250/2330 train_time:74053ms step_avg:59.24ms
step:1250/2330 val_loss:4.0148 train_time:74133ms step_avg:59.31ms
step:1251/2330 train_time:74153ms step_avg:59.28ms
step:1252/2330 train_time:74176ms step_avg:59.25ms
step:1253/2330 train_time:74236ms step_avg:59.25ms
step:1254/2330 train_time:74302ms step_avg:59.25ms
step:1255/2330 train_time:74360ms step_avg:59.25ms
step:1256/2330 train_time:74424ms step_avg:59.25ms
step:1257/2330 train_time:74481ms step_avg:59.25ms
step:1258/2330 train_time:74542ms step_avg:59.25ms
step:1259/2330 train_time:74599ms step_avg:59.25ms
step:1260/2330 train_time:74661ms step_avg:59.25ms
step:1261/2330 train_time:74718ms step_avg:59.25ms
step:1262/2330 train_time:74779ms step_avg:59.25ms
step:1263/2330 train_time:74835ms step_avg:59.25ms
step:1264/2330 train_time:74898ms step_avg:59.25ms
step:1265/2330 train_time:74954ms step_avg:59.25ms
step:1266/2330 train_time:75015ms step_avg:59.25ms
step:1267/2330 train_time:75073ms step_avg:59.25ms
step:1268/2330 train_time:75135ms step_avg:59.25ms
step:1269/2330 train_time:75195ms step_avg:59.25ms
step:1270/2330 train_time:75260ms step_avg:59.26ms
step:1271/2330 train_time:75318ms step_avg:59.26ms
step:1272/2330 train_time:75382ms step_avg:59.26ms
step:1273/2330 train_time:75440ms step_avg:59.26ms
step:1274/2330 train_time:75503ms step_avg:59.26ms
step:1275/2330 train_time:75560ms step_avg:59.26ms
step:1276/2330 train_time:75622ms step_avg:59.26ms
step:1277/2330 train_time:75678ms step_avg:59.26ms
step:1278/2330 train_time:75741ms step_avg:59.27ms
step:1279/2330 train_time:75798ms step_avg:59.26ms
step:1280/2330 train_time:75859ms step_avg:59.26ms
step:1281/2330 train_time:75916ms step_avg:59.26ms
step:1282/2330 train_time:75978ms step_avg:59.27ms
step:1283/2330 train_time:76036ms step_avg:59.26ms
step:1284/2330 train_time:76099ms step_avg:59.27ms
step:1285/2330 train_time:76157ms step_avg:59.27ms
step:1286/2330 train_time:76221ms step_avg:59.27ms
step:1287/2330 train_time:76279ms step_avg:59.27ms
step:1288/2330 train_time:76344ms step_avg:59.27ms
step:1289/2330 train_time:76401ms step_avg:59.27ms
step:1290/2330 train_time:76464ms step_avg:59.27ms
step:1291/2330 train_time:76521ms step_avg:59.27ms
step:1292/2330 train_time:76584ms step_avg:59.28ms
step:1293/2330 train_time:76641ms step_avg:59.27ms
step:1294/2330 train_time:76702ms step_avg:59.28ms
step:1295/2330 train_time:76759ms step_avg:59.27ms
step:1296/2330 train_time:76821ms step_avg:59.28ms
step:1297/2330 train_time:76878ms step_avg:59.27ms
step:1298/2330 train_time:76940ms step_avg:59.28ms
step:1299/2330 train_time:76997ms step_avg:59.27ms
step:1300/2330 train_time:77060ms step_avg:59.28ms
step:1301/2330 train_time:77118ms step_avg:59.28ms
step:1302/2330 train_time:77181ms step_avg:59.28ms
step:1303/2330 train_time:77239ms step_avg:59.28ms
step:1304/2330 train_time:77303ms step_avg:59.28ms
step:1305/2330 train_time:77361ms step_avg:59.28ms
step:1306/2330 train_time:77424ms step_avg:59.28ms
step:1307/2330 train_time:77481ms step_avg:59.28ms
step:1308/2330 train_time:77544ms step_avg:59.28ms
step:1309/2330 train_time:77601ms step_avg:59.28ms
step:1310/2330 train_time:77663ms step_avg:59.28ms
step:1311/2330 train_time:77720ms step_avg:59.28ms
step:1312/2330 train_time:77783ms step_avg:59.29ms
step:1313/2330 train_time:77839ms step_avg:59.28ms
step:1314/2330 train_time:77901ms step_avg:59.29ms
step:1315/2330 train_time:77958ms step_avg:59.28ms
step:1316/2330 train_time:78021ms step_avg:59.29ms
step:1317/2330 train_time:78078ms step_avg:59.28ms
step:1318/2330 train_time:78141ms step_avg:59.29ms
step:1319/2330 train_time:78199ms step_avg:59.29ms
step:1320/2330 train_time:78263ms step_avg:59.29ms
step:1321/2330 train_time:78320ms step_avg:59.29ms
step:1322/2330 train_time:78385ms step_avg:59.29ms
step:1323/2330 train_time:78442ms step_avg:59.29ms
step:1324/2330 train_time:78505ms step_avg:59.29ms
step:1325/2330 train_time:78563ms step_avg:59.29ms
step:1326/2330 train_time:78625ms step_avg:59.29ms
step:1327/2330 train_time:78682ms step_avg:59.29ms
step:1328/2330 train_time:78743ms step_avg:59.29ms
step:1329/2330 train_time:78800ms step_avg:59.29ms
step:1330/2330 train_time:78863ms step_avg:59.30ms
step:1331/2330 train_time:78920ms step_avg:59.29ms
step:1332/2330 train_time:78981ms step_avg:59.30ms
step:1333/2330 train_time:79038ms step_avg:59.29ms
step:1334/2330 train_time:79101ms step_avg:59.30ms
step:1335/2330 train_time:79158ms step_avg:59.29ms
step:1336/2330 train_time:79221ms step_avg:59.30ms
step:1337/2330 train_time:79279ms step_avg:59.30ms
step:1338/2330 train_time:79344ms step_avg:59.30ms
step:1339/2330 train_time:79401ms step_avg:59.30ms
step:1340/2330 train_time:79464ms step_avg:59.30ms
step:1341/2330 train_time:79521ms step_avg:59.30ms
step:1342/2330 train_time:79584ms step_avg:59.30ms
step:1343/2330 train_time:79641ms step_avg:59.30ms
step:1344/2330 train_time:79703ms step_avg:59.30ms
step:1345/2330 train_time:79760ms step_avg:59.30ms
step:1346/2330 train_time:79822ms step_avg:59.30ms
step:1347/2330 train_time:79879ms step_avg:59.30ms
step:1348/2330 train_time:79941ms step_avg:59.30ms
step:1349/2330 train_time:79999ms step_avg:59.30ms
step:1350/2330 train_time:80060ms step_avg:59.30ms
step:1351/2330 train_time:80118ms step_avg:59.30ms
step:1352/2330 train_time:80180ms step_avg:59.31ms
step:1353/2330 train_time:80238ms step_avg:59.30ms
step:1354/2330 train_time:80301ms step_avg:59.31ms
step:1355/2330 train_time:80359ms step_avg:59.31ms
step:1356/2330 train_time:80422ms step_avg:59.31ms
step:1357/2330 train_time:80479ms step_avg:59.31ms
step:1358/2330 train_time:80542ms step_avg:59.31ms
step:1359/2330 train_time:80600ms step_avg:59.31ms
step:1360/2330 train_time:80663ms step_avg:59.31ms
step:1361/2330 train_time:80720ms step_avg:59.31ms
step:1362/2330 train_time:80782ms step_avg:59.31ms
step:1363/2330 train_time:80839ms step_avg:59.31ms
step:1364/2330 train_time:80901ms step_avg:59.31ms
step:1365/2330 train_time:80959ms step_avg:59.31ms
step:1366/2330 train_time:81020ms step_avg:59.31ms
step:1367/2330 train_time:81078ms step_avg:59.31ms
step:1368/2330 train_time:81140ms step_avg:59.31ms
step:1369/2330 train_time:81197ms step_avg:59.31ms
step:1370/2330 train_time:81261ms step_avg:59.31ms
step:1371/2330 train_time:81318ms step_avg:59.31ms
step:1372/2330 train_time:81382ms step_avg:59.32ms
step:1373/2330 train_time:81439ms step_avg:59.31ms
step:1374/2330 train_time:81502ms step_avg:59.32ms
step:1375/2330 train_time:81560ms step_avg:59.32ms
step:1376/2330 train_time:81622ms step_avg:59.32ms
step:1377/2330 train_time:81680ms step_avg:59.32ms
step:1378/2330 train_time:81742ms step_avg:59.32ms
step:1379/2330 train_time:81800ms step_avg:59.32ms
step:1380/2330 train_time:81861ms step_avg:59.32ms
step:1381/2330 train_time:81919ms step_avg:59.32ms
step:1382/2330 train_time:81981ms step_avg:59.32ms
step:1383/2330 train_time:82039ms step_avg:59.32ms
step:1384/2330 train_time:82101ms step_avg:59.32ms
step:1385/2330 train_time:82157ms step_avg:59.32ms
step:1386/2330 train_time:82220ms step_avg:59.32ms
step:1387/2330 train_time:82278ms step_avg:59.32ms
step:1388/2330 train_time:82341ms step_avg:59.32ms
step:1389/2330 train_time:82399ms step_avg:59.32ms
step:1390/2330 train_time:82463ms step_avg:59.33ms
step:1391/2330 train_time:82520ms step_avg:59.32ms
step:1392/2330 train_time:82583ms step_avg:59.33ms
step:1393/2330 train_time:82640ms step_avg:59.33ms
step:1394/2330 train_time:82704ms step_avg:59.33ms
step:1395/2330 train_time:82761ms step_avg:59.33ms
step:1396/2330 train_time:82824ms step_avg:59.33ms
step:1397/2330 train_time:82880ms step_avg:59.33ms
step:1398/2330 train_time:82943ms step_avg:59.33ms
step:1399/2330 train_time:83000ms step_avg:59.33ms
step:1400/2330 train_time:83063ms step_avg:59.33ms
step:1401/2330 train_time:83120ms step_avg:59.33ms
step:1402/2330 train_time:83182ms step_avg:59.33ms
step:1403/2330 train_time:83240ms step_avg:59.33ms
step:1404/2330 train_time:83303ms step_avg:59.33ms
step:1405/2330 train_time:83360ms step_avg:59.33ms
step:1406/2330 train_time:83424ms step_avg:59.33ms
step:1407/2330 train_time:83481ms step_avg:59.33ms
step:1408/2330 train_time:83543ms step_avg:59.33ms
step:1409/2330 train_time:83601ms step_avg:59.33ms
step:1410/2330 train_time:83665ms step_avg:59.34ms
step:1411/2330 train_time:83722ms step_avg:59.34ms
step:1412/2330 train_time:83784ms step_avg:59.34ms
step:1413/2330 train_time:83841ms step_avg:59.34ms
step:1414/2330 train_time:83904ms step_avg:59.34ms
step:1415/2330 train_time:83961ms step_avg:59.34ms
step:1416/2330 train_time:84024ms step_avg:59.34ms
step:1417/2330 train_time:84080ms step_avg:59.34ms
step:1418/2330 train_time:84144ms step_avg:59.34ms
step:1419/2330 train_time:84201ms step_avg:59.34ms
step:1420/2330 train_time:84263ms step_avg:59.34ms
step:1421/2330 train_time:84320ms step_avg:59.34ms
step:1422/2330 train_time:84384ms step_avg:59.34ms
step:1423/2330 train_time:84441ms step_avg:59.34ms
step:1424/2330 train_time:84503ms step_avg:59.34ms
step:1425/2330 train_time:84560ms step_avg:59.34ms
step:1426/2330 train_time:84624ms step_avg:59.34ms
step:1427/2330 train_time:84681ms step_avg:59.34ms
step:1428/2330 train_time:84745ms step_avg:59.35ms
step:1429/2330 train_time:84802ms step_avg:59.34ms
step:1430/2330 train_time:84864ms step_avg:59.35ms
step:1431/2330 train_time:84921ms step_avg:59.34ms
step:1432/2330 train_time:84983ms step_avg:59.35ms
step:1433/2330 train_time:85040ms step_avg:59.34ms
step:1434/2330 train_time:85103ms step_avg:59.35ms
step:1435/2330 train_time:85160ms step_avg:59.35ms
step:1436/2330 train_time:85223ms step_avg:59.35ms
step:1437/2330 train_time:85280ms step_avg:59.35ms
step:1438/2330 train_time:85343ms step_avg:59.35ms
step:1439/2330 train_time:85401ms step_avg:59.35ms
step:1440/2330 train_time:85463ms step_avg:59.35ms
step:1441/2330 train_time:85521ms step_avg:59.35ms
step:1442/2330 train_time:85583ms step_avg:59.35ms
step:1443/2330 train_time:85641ms step_avg:59.35ms
step:1444/2330 train_time:85704ms step_avg:59.35ms
step:1445/2330 train_time:85761ms step_avg:59.35ms
step:1446/2330 train_time:85824ms step_avg:59.35ms
step:1447/2330 train_time:85881ms step_avg:59.35ms
step:1448/2330 train_time:85944ms step_avg:59.35ms
step:1449/2330 train_time:86001ms step_avg:59.35ms
step:1450/2330 train_time:86063ms step_avg:59.35ms
step:1451/2330 train_time:86120ms step_avg:59.35ms
step:1452/2330 train_time:86183ms step_avg:59.35ms
step:1453/2330 train_time:86240ms step_avg:59.35ms
step:1454/2330 train_time:86303ms step_avg:59.36ms
step:1455/2330 train_time:86360ms step_avg:59.35ms
step:1456/2330 train_time:86423ms step_avg:59.36ms
step:1457/2330 train_time:86480ms step_avg:59.36ms
step:1458/2330 train_time:86543ms step_avg:59.36ms
step:1459/2330 train_time:86601ms step_avg:59.36ms
step:1460/2330 train_time:86664ms step_avg:59.36ms
step:1461/2330 train_time:86721ms step_avg:59.36ms
step:1462/2330 train_time:86783ms step_avg:59.36ms
step:1463/2330 train_time:86841ms step_avg:59.36ms
step:1464/2330 train_time:86904ms step_avg:59.36ms
step:1465/2330 train_time:86960ms step_avg:59.36ms
step:1466/2330 train_time:87024ms step_avg:59.36ms
step:1467/2330 train_time:87081ms step_avg:59.36ms
step:1468/2330 train_time:87143ms step_avg:59.36ms
step:1469/2330 train_time:87200ms step_avg:59.36ms
step:1470/2330 train_time:87263ms step_avg:59.36ms
step:1471/2330 train_time:87320ms step_avg:59.36ms
step:1472/2330 train_time:87382ms step_avg:59.36ms
step:1473/2330 train_time:87440ms step_avg:59.36ms
step:1474/2330 train_time:87503ms step_avg:59.36ms
step:1475/2330 train_time:87560ms step_avg:59.36ms
step:1476/2330 train_time:87623ms step_avg:59.37ms
step:1477/2330 train_time:87680ms step_avg:59.36ms
step:1478/2330 train_time:87744ms step_avg:59.37ms
step:1479/2330 train_time:87802ms step_avg:59.37ms
step:1480/2330 train_time:87864ms step_avg:59.37ms
step:1481/2330 train_time:87921ms step_avg:59.37ms
step:1482/2330 train_time:87983ms step_avg:59.37ms
step:1483/2330 train_time:88040ms step_avg:59.37ms
step:1484/2330 train_time:88104ms step_avg:59.37ms
step:1485/2330 train_time:88160ms step_avg:59.37ms
step:1486/2330 train_time:88223ms step_avg:59.37ms
step:1487/2330 train_time:88280ms step_avg:59.37ms
step:1488/2330 train_time:88343ms step_avg:59.37ms
step:1489/2330 train_time:88400ms step_avg:59.37ms
step:1490/2330 train_time:88463ms step_avg:59.37ms
step:1491/2330 train_time:88521ms step_avg:59.37ms
step:1492/2330 train_time:88584ms step_avg:59.37ms
step:1493/2330 train_time:88641ms step_avg:59.37ms
step:1494/2330 train_time:88704ms step_avg:59.37ms
step:1495/2330 train_time:88761ms step_avg:59.37ms
step:1496/2330 train_time:88824ms step_avg:59.37ms
step:1497/2330 train_time:88882ms step_avg:59.37ms
step:1498/2330 train_time:88944ms step_avg:59.38ms
step:1499/2330 train_time:89002ms step_avg:59.37ms
step:1500/2330 train_time:89065ms step_avg:59.38ms
step:1500/2330 val_loss:3.9176 train_time:89144ms step_avg:59.43ms
step:1501/2330 train_time:89164ms step_avg:59.40ms
step:1502/2330 train_time:89187ms step_avg:59.38ms
step:1503/2330 train_time:89247ms step_avg:59.38ms
step:1504/2330 train_time:89315ms step_avg:59.38ms
step:1505/2330 train_time:89372ms step_avg:59.38ms
step:1506/2330 train_time:89435ms step_avg:59.39ms
step:1507/2330 train_time:89492ms step_avg:59.38ms
step:1508/2330 train_time:89553ms step_avg:59.39ms
step:1509/2330 train_time:89610ms step_avg:59.38ms
step:1510/2330 train_time:89672ms step_avg:59.39ms
step:1511/2330 train_time:89729ms step_avg:59.38ms
step:1512/2330 train_time:89790ms step_avg:59.39ms
step:1513/2330 train_time:89847ms step_avg:59.38ms
step:1514/2330 train_time:89908ms step_avg:59.38ms
step:1515/2330 train_time:89964ms step_avg:59.38ms
step:1516/2330 train_time:90025ms step_avg:59.38ms
step:1517/2330 train_time:90083ms step_avg:59.38ms
step:1518/2330 train_time:90146ms step_avg:59.38ms
step:1519/2330 train_time:90205ms step_avg:59.38ms
step:1520/2330 train_time:90271ms step_avg:59.39ms
step:1521/2330 train_time:90329ms step_avg:59.39ms
step:1522/2330 train_time:90391ms step_avg:59.39ms
step:1523/2330 train_time:90449ms step_avg:59.39ms
step:1524/2330 train_time:90512ms step_avg:59.39ms
step:1525/2330 train_time:90570ms step_avg:59.39ms
step:1526/2330 train_time:90631ms step_avg:59.39ms
step:1527/2330 train_time:90688ms step_avg:59.39ms
step:1528/2330 train_time:90749ms step_avg:59.39ms
step:1529/2330 train_time:90807ms step_avg:59.39ms
step:1530/2330 train_time:90868ms step_avg:59.39ms
step:1531/2330 train_time:90925ms step_avg:59.39ms
step:1532/2330 train_time:90987ms step_avg:59.39ms
step:1533/2330 train_time:91045ms step_avg:59.39ms
step:1534/2330 train_time:91108ms step_avg:59.39ms
step:1535/2330 train_time:91166ms step_avg:59.39ms
step:1536/2330 train_time:91231ms step_avg:59.40ms
step:1537/2330 train_time:91290ms step_avg:59.39ms
step:1538/2330 train_time:91353ms step_avg:59.40ms
step:1539/2330 train_time:91411ms step_avg:59.40ms
step:1540/2330 train_time:91476ms step_avg:59.40ms
step:1541/2330 train_time:91533ms step_avg:59.40ms
step:1542/2330 train_time:91595ms step_avg:59.40ms
step:1543/2330 train_time:91652ms step_avg:59.40ms
step:1544/2330 train_time:91714ms step_avg:59.40ms
step:1545/2330 train_time:91771ms step_avg:59.40ms
step:1546/2330 train_time:91834ms step_avg:59.40ms
step:1547/2330 train_time:91891ms step_avg:59.40ms
step:1548/2330 train_time:91953ms step_avg:59.40ms
step:1549/2330 train_time:92010ms step_avg:59.40ms
step:1550/2330 train_time:92074ms step_avg:59.40ms
step:1551/2330 train_time:92131ms step_avg:59.40ms
step:1552/2330 train_time:92196ms step_avg:59.40ms
step:1553/2330 train_time:92253ms step_avg:59.40ms
step:1554/2330 train_time:92317ms step_avg:59.41ms
step:1555/2330 train_time:92375ms step_avg:59.41ms
step:1556/2330 train_time:92438ms step_avg:59.41ms
step:1557/2330 train_time:92495ms step_avg:59.41ms
step:1558/2330 train_time:92559ms step_avg:59.41ms
step:1559/2330 train_time:92616ms step_avg:59.41ms
step:1560/2330 train_time:92677ms step_avg:59.41ms
step:1561/2330 train_time:92734ms step_avg:59.41ms
step:1562/2330 train_time:92798ms step_avg:59.41ms
step:1563/2330 train_time:92855ms step_avg:59.41ms
step:1564/2330 train_time:92917ms step_avg:59.41ms
step:1565/2330 train_time:92974ms step_avg:59.41ms
step:1566/2330 train_time:93037ms step_avg:59.41ms
step:1567/2330 train_time:93094ms step_avg:59.41ms
step:1568/2330 train_time:93157ms step_avg:59.41ms
step:1569/2330 train_time:93214ms step_avg:59.41ms
step:1570/2330 train_time:93278ms step_avg:59.41ms
step:1571/2330 train_time:93335ms step_avg:59.41ms
step:1572/2330 train_time:93398ms step_avg:59.41ms
step:1573/2330 train_time:93456ms step_avg:59.41ms
step:1574/2330 train_time:93518ms step_avg:59.41ms
step:1575/2330 train_time:93576ms step_avg:59.41ms
step:1576/2330 train_time:93638ms step_avg:59.41ms
step:1577/2330 train_time:93695ms step_avg:59.41ms
step:1578/2330 train_time:93758ms step_avg:59.42ms
step:1579/2330 train_time:93815ms step_avg:59.41ms
step:1580/2330 train_time:93877ms step_avg:59.42ms
step:1581/2330 train_time:93934ms step_avg:59.41ms
step:1582/2330 train_time:93997ms step_avg:59.42ms
step:1583/2330 train_time:94054ms step_avg:59.41ms
step:1584/2330 train_time:94117ms step_avg:59.42ms
step:1585/2330 train_time:94174ms step_avg:59.42ms
step:1586/2330 train_time:94237ms step_avg:59.42ms
step:1587/2330 train_time:94294ms step_avg:59.42ms
step:1588/2330 train_time:94358ms step_avg:59.42ms
step:1589/2330 train_time:94415ms step_avg:59.42ms
step:1590/2330 train_time:94479ms step_avg:59.42ms
step:1591/2330 train_time:94535ms step_avg:59.42ms
step:1592/2330 train_time:94599ms step_avg:59.42ms
step:1593/2330 train_time:94656ms step_avg:59.42ms
step:1594/2330 train_time:94719ms step_avg:59.42ms
step:1595/2330 train_time:94777ms step_avg:59.42ms
step:1596/2330 train_time:94838ms step_avg:59.42ms
step:1597/2330 train_time:94895ms step_avg:59.42ms
step:1598/2330 train_time:94957ms step_avg:59.42ms
step:1599/2330 train_time:95014ms step_avg:59.42ms
step:1600/2330 train_time:95078ms step_avg:59.42ms
step:1601/2330 train_time:95136ms step_avg:59.42ms
step:1602/2330 train_time:95197ms step_avg:59.42ms
step:1603/2330 train_time:95255ms step_avg:59.42ms
step:1604/2330 train_time:95318ms step_avg:59.42ms
step:1605/2330 train_time:95375ms step_avg:59.42ms
step:1606/2330 train_time:95438ms step_avg:59.43ms
step:1607/2330 train_time:95495ms step_avg:59.42ms
step:1608/2330 train_time:95558ms step_avg:59.43ms
step:1609/2330 train_time:95615ms step_avg:59.42ms
step:1610/2330 train_time:95678ms step_avg:59.43ms
step:1611/2330 train_time:95735ms step_avg:59.43ms
step:1612/2330 train_time:95797ms step_avg:59.43ms
step:1613/2330 train_time:95854ms step_avg:59.43ms
step:1614/2330 train_time:95916ms step_avg:59.43ms
step:1615/2330 train_time:95974ms step_avg:59.43ms
step:1616/2330 train_time:96038ms step_avg:59.43ms
step:1617/2330 train_time:96095ms step_avg:59.43ms
step:1618/2330 train_time:96157ms step_avg:59.43ms
step:1619/2330 train_time:96214ms step_avg:59.43ms
step:1620/2330 train_time:96277ms step_avg:59.43ms
step:1621/2330 train_time:96334ms step_avg:59.43ms
step:1622/2330 train_time:96399ms step_avg:59.43ms
step:1623/2330 train_time:96456ms step_avg:59.43ms
step:1624/2330 train_time:96518ms step_avg:59.43ms
step:1625/2330 train_time:96576ms step_avg:59.43ms
step:1626/2330 train_time:96638ms step_avg:59.43ms
step:1627/2330 train_time:96695ms step_avg:59.43ms
step:1628/2330 train_time:96758ms step_avg:59.43ms
step:1629/2330 train_time:96815ms step_avg:59.43ms
step:1630/2330 train_time:96877ms step_avg:59.43ms
step:1631/2330 train_time:96934ms step_avg:59.43ms
step:1632/2330 train_time:96998ms step_avg:59.43ms
step:1633/2330 train_time:97055ms step_avg:59.43ms
step:1634/2330 train_time:97117ms step_avg:59.44ms
step:1635/2330 train_time:97175ms step_avg:59.43ms
step:1636/2330 train_time:97237ms step_avg:59.44ms
step:1637/2330 train_time:97295ms step_avg:59.43ms
step:1638/2330 train_time:97357ms step_avg:59.44ms
step:1639/2330 train_time:97415ms step_avg:59.44ms
step:1640/2330 train_time:97478ms step_avg:59.44ms
step:1641/2330 train_time:97535ms step_avg:59.44ms
step:1642/2330 train_time:97598ms step_avg:59.44ms
step:1643/2330 train_time:97655ms step_avg:59.44ms
step:1644/2330 train_time:97718ms step_avg:59.44ms
step:1645/2330 train_time:97775ms step_avg:59.44ms
step:1646/2330 train_time:97838ms step_avg:59.44ms
step:1647/2330 train_time:97895ms step_avg:59.44ms
step:1648/2330 train_time:97958ms step_avg:59.44ms
step:1649/2330 train_time:98015ms step_avg:59.44ms
step:1650/2330 train_time:98077ms step_avg:59.44ms
step:1651/2330 train_time:98134ms step_avg:59.44ms
step:1652/2330 train_time:98198ms step_avg:59.44ms
step:1653/2330 train_time:98255ms step_avg:59.44ms
step:1654/2330 train_time:98318ms step_avg:59.44ms
step:1655/2330 train_time:98375ms step_avg:59.44ms
step:1656/2330 train_time:98439ms step_avg:59.44ms
step:1657/2330 train_time:98496ms step_avg:59.44ms
step:1658/2330 train_time:98559ms step_avg:59.44ms
step:1659/2330 train_time:98616ms step_avg:59.44ms
step:1660/2330 train_time:98678ms step_avg:59.44ms
step:1661/2330 train_time:98736ms step_avg:59.44ms
step:1662/2330 train_time:98799ms step_avg:59.45ms
step:1663/2330 train_time:98856ms step_avg:59.44ms
step:1664/2330 train_time:98919ms step_avg:59.45ms
step:1665/2330 train_time:98976ms step_avg:59.44ms
step:1666/2330 train_time:99038ms step_avg:59.45ms
step:1667/2330 train_time:99095ms step_avg:59.45ms
step:1668/2330 train_time:99158ms step_avg:59.45ms
step:1669/2330 train_time:99215ms step_avg:59.45ms
step:1670/2330 train_time:99278ms step_avg:59.45ms
step:1671/2330 train_time:99336ms step_avg:59.45ms
step:1672/2330 train_time:99399ms step_avg:59.45ms
step:1673/2330 train_time:99456ms step_avg:59.45ms
step:1674/2330 train_time:99519ms step_avg:59.45ms
step:1675/2330 train_time:99577ms step_avg:59.45ms
step:1676/2330 train_time:99638ms step_avg:59.45ms
step:1677/2330 train_time:99695ms step_avg:59.45ms
step:1678/2330 train_time:99758ms step_avg:59.45ms
step:1679/2330 train_time:99815ms step_avg:59.45ms
step:1680/2330 train_time:99878ms step_avg:59.45ms
step:1681/2330 train_time:99935ms step_avg:59.45ms
step:1682/2330 train_time:99998ms step_avg:59.45ms
step:1683/2330 train_time:100055ms step_avg:59.45ms
step:1684/2330 train_time:100119ms step_avg:59.45ms
step:1685/2330 train_time:100176ms step_avg:59.45ms
step:1686/2330 train_time:100238ms step_avg:59.45ms
step:1687/2330 train_time:100295ms step_avg:59.45ms
step:1688/2330 train_time:100359ms step_avg:59.45ms
step:1689/2330 train_time:100416ms step_avg:59.45ms
step:1690/2330 train_time:100479ms step_avg:59.45ms
step:1691/2330 train_time:100536ms step_avg:59.45ms
step:1692/2330 train_time:100599ms step_avg:59.46ms
step:1693/2330 train_time:100656ms step_avg:59.45ms
step:1694/2330 train_time:100719ms step_avg:59.46ms
step:1695/2330 train_time:100777ms step_avg:59.46ms
step:1696/2330 train_time:100839ms step_avg:59.46ms
step:1697/2330 train_time:100896ms step_avg:59.46ms
step:1698/2330 train_time:100958ms step_avg:59.46ms
step:1699/2330 train_time:101016ms step_avg:59.46ms
step:1700/2330 train_time:101079ms step_avg:59.46ms
step:1701/2330 train_time:101135ms step_avg:59.46ms
step:1702/2330 train_time:101199ms step_avg:59.46ms
step:1703/2330 train_time:101257ms step_avg:59.46ms
step:1704/2330 train_time:101319ms step_avg:59.46ms
step:1705/2330 train_time:101376ms step_avg:59.46ms
step:1706/2330 train_time:101439ms step_avg:59.46ms
step:1707/2330 train_time:101496ms step_avg:59.46ms
step:1708/2330 train_time:101559ms step_avg:59.46ms
step:1709/2330 train_time:101617ms step_avg:59.46ms
step:1710/2330 train_time:101679ms step_avg:59.46ms
step:1711/2330 train_time:101736ms step_avg:59.46ms
step:1712/2330 train_time:101799ms step_avg:59.46ms
step:1713/2330 train_time:101856ms step_avg:59.46ms
step:1714/2330 train_time:101919ms step_avg:59.46ms
step:1715/2330 train_time:101976ms step_avg:59.46ms
step:1716/2330 train_time:102039ms step_avg:59.46ms
step:1717/2330 train_time:102096ms step_avg:59.46ms
step:1718/2330 train_time:102159ms step_avg:59.46ms
step:1719/2330 train_time:102216ms step_avg:59.46ms
step:1720/2330 train_time:102279ms step_avg:59.46ms
step:1721/2330 train_time:102337ms step_avg:59.46ms
step:1722/2330 train_time:102400ms step_avg:59.47ms
step:1723/2330 train_time:102457ms step_avg:59.46ms
step:1724/2330 train_time:102520ms step_avg:59.47ms
step:1725/2330 train_time:102576ms step_avg:59.46ms
step:1726/2330 train_time:102639ms step_avg:59.47ms
step:1727/2330 train_time:102696ms step_avg:59.47ms
step:1728/2330 train_time:102759ms step_avg:59.47ms
step:1729/2330 train_time:102816ms step_avg:59.47ms
step:1730/2330 train_time:102879ms step_avg:59.47ms
step:1731/2330 train_time:102937ms step_avg:59.47ms
step:1732/2330 train_time:103000ms step_avg:59.47ms
step:1733/2330 train_time:103057ms step_avg:59.47ms
step:1734/2330 train_time:103120ms step_avg:59.47ms
step:1735/2330 train_time:103177ms step_avg:59.47ms
step:1736/2330 train_time:103240ms step_avg:59.47ms
step:1737/2330 train_time:103297ms step_avg:59.47ms
step:1738/2330 train_time:103359ms step_avg:59.47ms
step:1739/2330 train_time:103416ms step_avg:59.47ms
step:1740/2330 train_time:103480ms step_avg:59.47ms
step:1741/2330 train_time:103537ms step_avg:59.47ms
step:1742/2330 train_time:103599ms step_avg:59.47ms
step:1743/2330 train_time:103656ms step_avg:59.47ms
step:1744/2330 train_time:103720ms step_avg:59.47ms
step:1745/2330 train_time:103779ms step_avg:59.47ms
step:1746/2330 train_time:103839ms step_avg:59.47ms
step:1747/2330 train_time:103896ms step_avg:59.47ms
step:1748/2330 train_time:103960ms step_avg:59.47ms
step:1749/2330 train_time:104017ms step_avg:59.47ms
step:1750/2330 train_time:104079ms step_avg:59.47ms
step:1750/2330 val_loss:3.8312 train_time:104159ms step_avg:59.52ms
step:1751/2330 train_time:104179ms step_avg:59.50ms
step:1752/2330 train_time:104201ms step_avg:59.48ms
step:1753/2330 train_time:104257ms step_avg:59.47ms
step:1754/2330 train_time:104327ms step_avg:59.48ms
step:1755/2330 train_time:104384ms step_avg:59.48ms
step:1756/2330 train_time:104451ms step_avg:59.48ms
step:1757/2330 train_time:104507ms step_avg:59.48ms
step:1758/2330 train_time:104570ms step_avg:59.48ms
step:1759/2330 train_time:104626ms step_avg:59.48ms
step:1760/2330 train_time:104689ms step_avg:59.48ms
step:1761/2330 train_time:104746ms step_avg:59.48ms
step:1762/2330 train_time:104808ms step_avg:59.48ms
step:1763/2330 train_time:104865ms step_avg:59.48ms
step:1764/2330 train_time:104927ms step_avg:59.48ms
step:1765/2330 train_time:104984ms step_avg:59.48ms
step:1766/2330 train_time:105045ms step_avg:59.48ms
step:1767/2330 train_time:105105ms step_avg:59.48ms
step:1768/2330 train_time:105168ms step_avg:59.48ms
step:1769/2330 train_time:105227ms step_avg:59.48ms
step:1770/2330 train_time:105290ms step_avg:59.49ms
step:1771/2330 train_time:105349ms step_avg:59.49ms
step:1772/2330 train_time:105413ms step_avg:59.49ms
step:1773/2330 train_time:105470ms step_avg:59.49ms
step:1774/2330 train_time:105533ms step_avg:59.49ms
step:1775/2330 train_time:105590ms step_avg:59.49ms
step:1776/2330 train_time:105653ms step_avg:59.49ms
step:1777/2330 train_time:105710ms step_avg:59.49ms
step:1778/2330 train_time:105773ms step_avg:59.49ms
step:1779/2330 train_time:105831ms step_avg:59.49ms
step:1780/2330 train_time:105892ms step_avg:59.49ms
step:1781/2330 train_time:105950ms step_avg:59.49ms
step:1782/2330 train_time:106012ms step_avg:59.49ms
step:1783/2330 train_time:106071ms step_avg:59.49ms
step:1784/2330 train_time:106134ms step_avg:59.49ms
step:1785/2330 train_time:106193ms step_avg:59.49ms
step:1786/2330 train_time:106258ms step_avg:59.49ms
step:1787/2330 train_time:106316ms step_avg:59.49ms
step:1788/2330 train_time:106379ms step_avg:59.50ms
step:1789/2330 train_time:106436ms step_avg:59.49ms
step:1790/2330 train_time:106500ms step_avg:59.50ms
step:1791/2330 train_time:106556ms step_avg:59.50ms
step:1792/2330 train_time:106619ms step_avg:59.50ms
step:1793/2330 train_time:106676ms step_avg:59.50ms
step:1794/2330 train_time:106739ms step_avg:59.50ms
step:1795/2330 train_time:106796ms step_avg:59.50ms
step:1796/2330 train_time:106859ms step_avg:59.50ms
step:1797/2330 train_time:106916ms step_avg:59.50ms
step:1798/2330 train_time:106979ms step_avg:59.50ms
step:1799/2330 train_time:107036ms step_avg:59.50ms
step:1800/2330 train_time:107100ms step_avg:59.50ms
step:1801/2330 train_time:107158ms step_avg:59.50ms
step:1802/2330 train_time:107221ms step_avg:59.50ms
step:1803/2330 train_time:107279ms step_avg:59.50ms
step:1804/2330 train_time:107341ms step_avg:59.50ms
step:1805/2330 train_time:107399ms step_avg:59.50ms
step:1806/2330 train_time:107462ms step_avg:59.50ms
step:1807/2330 train_time:107519ms step_avg:59.50ms
step:1808/2330 train_time:107581ms step_avg:59.50ms
step:1809/2330 train_time:107638ms step_avg:59.50ms
step:1810/2330 train_time:107701ms step_avg:59.50ms
step:1811/2330 train_time:107758ms step_avg:59.50ms
step:1812/2330 train_time:107820ms step_avg:59.50ms
step:1813/2330 train_time:107877ms step_avg:59.50ms
step:1814/2330 train_time:107940ms step_avg:59.50ms
step:1815/2330 train_time:107997ms step_avg:59.50ms
step:1816/2330 train_time:108061ms step_avg:59.50ms
step:1817/2330 train_time:108118ms step_avg:59.50ms
step:1818/2330 train_time:108182ms step_avg:59.51ms
step:1819/2330 train_time:108240ms step_avg:59.51ms
step:1820/2330 train_time:108303ms step_avg:59.51ms
step:1821/2330 train_time:108361ms step_avg:59.51ms
step:1822/2330 train_time:108423ms step_avg:59.51ms
step:1823/2330 train_time:108480ms step_avg:59.51ms
step:1824/2330 train_time:108542ms step_avg:59.51ms
step:1825/2330 train_time:108599ms step_avg:59.51ms
step:1826/2330 train_time:108661ms step_avg:59.51ms
step:1827/2330 train_time:108718ms step_avg:59.51ms
step:1828/2330 train_time:108782ms step_avg:59.51ms
step:1829/2330 train_time:108838ms step_avg:59.51ms
step:1830/2330 train_time:108901ms step_avg:59.51ms
step:1831/2330 train_time:108958ms step_avg:59.51ms
step:1832/2330 train_time:109021ms step_avg:59.51ms
step:1833/2330 train_time:109077ms step_avg:59.51ms
step:1834/2330 train_time:109140ms step_avg:59.51ms
step:1835/2330 train_time:109198ms step_avg:59.51ms
step:1836/2330 train_time:109261ms step_avg:59.51ms
step:1837/2330 train_time:109319ms step_avg:59.51ms
step:1838/2330 train_time:109382ms step_avg:59.51ms
step:1839/2330 train_time:109439ms step_avg:59.51ms
step:1840/2330 train_time:109502ms step_avg:59.51ms
step:1841/2330 train_time:109559ms step_avg:59.51ms
step:1842/2330 train_time:109621ms step_avg:59.51ms
step:1843/2330 train_time:109678ms step_avg:59.51ms
step:1844/2330 train_time:109741ms step_avg:59.51ms
step:1845/2330 train_time:109798ms step_avg:59.51ms
step:1846/2330 train_time:109860ms step_avg:59.51ms
step:1847/2330 train_time:109918ms step_avg:59.51ms
step:1848/2330 train_time:109980ms step_avg:59.51ms
step:1849/2330 train_time:110037ms step_avg:59.51ms
step:1850/2330 train_time:110100ms step_avg:59.51ms
step:1851/2330 train_time:110158ms step_avg:59.51ms
step:1852/2330 train_time:110220ms step_avg:59.51ms
step:1853/2330 train_time:110277ms step_avg:59.51ms
step:1854/2330 train_time:110341ms step_avg:59.51ms
step:1855/2330 train_time:110398ms step_avg:59.51ms
step:1856/2330 train_time:110461ms step_avg:59.52ms
step:1857/2330 train_time:110519ms step_avg:59.51ms
step:1858/2330 train_time:110580ms step_avg:59.52ms
step:1859/2330 train_time:110638ms step_avg:59.51ms
step:1860/2330 train_time:110700ms step_avg:59.52ms
step:1861/2330 train_time:110758ms step_avg:59.52ms
step:1862/2330 train_time:110820ms step_avg:59.52ms
step:1863/2330 train_time:110877ms step_avg:59.52ms
step:1864/2330 train_time:110940ms step_avg:59.52ms
step:1865/2330 train_time:110997ms step_avg:59.52ms
step:1866/2330 train_time:111060ms step_avg:59.52ms
step:1867/2330 train_time:111117ms step_avg:59.52ms
step:1868/2330 train_time:111180ms step_avg:59.52ms
step:1869/2330 train_time:111237ms step_avg:59.52ms
step:1870/2330 train_time:111301ms step_avg:59.52ms
step:1871/2330 train_time:111358ms step_avg:59.52ms
step:1872/2330 train_time:111421ms step_avg:59.52ms
step:1873/2330 train_time:111478ms step_avg:59.52ms
step:1874/2330 train_time:111541ms step_avg:59.52ms
step:1875/2330 train_time:111599ms step_avg:59.52ms
step:1876/2330 train_time:111662ms step_avg:59.52ms
step:1877/2330 train_time:111719ms step_avg:59.52ms
step:1878/2330 train_time:111780ms step_avg:59.52ms
step:1879/2330 train_time:111838ms step_avg:59.52ms
step:1880/2330 train_time:111900ms step_avg:59.52ms
step:1881/2330 train_time:111957ms step_avg:59.52ms
step:1882/2330 train_time:112020ms step_avg:59.52ms
step:1883/2330 train_time:112076ms step_avg:59.52ms
step:1884/2330 train_time:112140ms step_avg:59.52ms
step:1885/2330 train_time:112197ms step_avg:59.52ms
step:1886/2330 train_time:112262ms step_avg:59.52ms
step:1887/2330 train_time:112319ms step_avg:59.52ms
step:1888/2330 train_time:112381ms step_avg:59.52ms
step:1889/2330 train_time:112438ms step_avg:59.52ms
step:1890/2330 train_time:112501ms step_avg:59.52ms
step:1891/2330 train_time:112559ms step_avg:59.52ms
step:1892/2330 train_time:112621ms step_avg:59.52ms
step:1893/2330 train_time:112678ms step_avg:59.52ms
step:1894/2330 train_time:112741ms step_avg:59.53ms
step:1895/2330 train_time:112798ms step_avg:59.52ms
step:1896/2330 train_time:112860ms step_avg:59.53ms
step:1897/2330 train_time:112918ms step_avg:59.52ms
step:1898/2330 train_time:112980ms step_avg:59.53ms
step:1899/2330 train_time:113037ms step_avg:59.52ms
step:1900/2330 train_time:113099ms step_avg:59.53ms
step:1901/2330 train_time:113158ms step_avg:59.53ms
step:1902/2330 train_time:113221ms step_avg:59.53ms
step:1903/2330 train_time:113277ms step_avg:59.53ms
step:1904/2330 train_time:113340ms step_avg:59.53ms
step:1905/2330 train_time:113397ms step_avg:59.53ms
step:1906/2330 train_time:113460ms step_avg:59.53ms
step:1907/2330 train_time:113518ms step_avg:59.53ms
step:1908/2330 train_time:113580ms step_avg:59.53ms
step:1909/2330 train_time:113638ms step_avg:59.53ms
step:1910/2330 train_time:113700ms step_avg:59.53ms
step:1911/2330 train_time:113757ms step_avg:59.53ms
step:1912/2330 train_time:113819ms step_avg:59.53ms
step:1913/2330 train_time:113877ms step_avg:59.53ms
step:1914/2330 train_time:113940ms step_avg:59.53ms
step:1915/2330 train_time:113997ms step_avg:59.53ms
step:1916/2330 train_time:114059ms step_avg:59.53ms
step:1917/2330 train_time:114117ms step_avg:59.53ms
step:1918/2330 train_time:114181ms step_avg:59.53ms
step:1919/2330 train_time:114238ms step_avg:59.53ms
step:1920/2330 train_time:114301ms step_avg:59.53ms
step:1921/2330 train_time:114358ms step_avg:59.53ms
step:1922/2330 train_time:114422ms step_avg:59.53ms
step:1923/2330 train_time:114478ms step_avg:59.53ms
step:1924/2330 train_time:114542ms step_avg:59.53ms
step:1925/2330 train_time:114599ms step_avg:59.53ms
step:1926/2330 train_time:114662ms step_avg:59.53ms
step:1927/2330 train_time:114718ms step_avg:59.53ms
step:1928/2330 train_time:114782ms step_avg:59.53ms
step:1929/2330 train_time:114839ms step_avg:59.53ms
step:1930/2330 train_time:114901ms step_avg:59.53ms
step:1931/2330 train_time:114959ms step_avg:59.53ms
step:1932/2330 train_time:115021ms step_avg:59.53ms
step:1933/2330 train_time:115078ms step_avg:59.53ms
step:1934/2330 train_time:115141ms step_avg:59.54ms
step:1935/2330 train_time:115199ms step_avg:59.53ms
step:1936/2330 train_time:115262ms step_avg:59.54ms
step:1937/2330 train_time:115319ms step_avg:59.53ms
step:1938/2330 train_time:115381ms step_avg:59.54ms
step:1939/2330 train_time:115439ms step_avg:59.54ms
step:1940/2330 train_time:115502ms step_avg:59.54ms
step:1941/2330 train_time:115559ms step_avg:59.54ms
step:1942/2330 train_time:115621ms step_avg:59.54ms
step:1943/2330 train_time:115678ms step_avg:59.54ms
step:1944/2330 train_time:115741ms step_avg:59.54ms
step:1945/2330 train_time:115798ms step_avg:59.54ms
step:1946/2330 train_time:115861ms step_avg:59.54ms
step:1947/2330 train_time:115918ms step_avg:59.54ms
step:1948/2330 train_time:115981ms step_avg:59.54ms
step:1949/2330 train_time:116038ms step_avg:59.54ms
step:1950/2330 train_time:116101ms step_avg:59.54ms
step:1951/2330 train_time:116158ms step_avg:59.54ms
step:1952/2330 train_time:116221ms step_avg:59.54ms
step:1953/2330 train_time:116278ms step_avg:59.54ms
step:1954/2330 train_time:116341ms step_avg:59.54ms
step:1955/2330 train_time:116398ms step_avg:59.54ms
step:1956/2330 train_time:116461ms step_avg:59.54ms
step:1957/2330 train_time:116518ms step_avg:59.54ms
step:1958/2330 train_time:116581ms step_avg:59.54ms
step:1959/2330 train_time:116638ms step_avg:59.54ms
step:1960/2330 train_time:116702ms step_avg:59.54ms
step:1961/2330 train_time:116759ms step_avg:59.54ms
step:1962/2330 train_time:116820ms step_avg:59.54ms
step:1963/2330 train_time:116877ms step_avg:59.54ms
step:1964/2330 train_time:116941ms step_avg:59.54ms
step:1965/2330 train_time:116999ms step_avg:59.54ms
step:1966/2330 train_time:117061ms step_avg:59.54ms
step:1967/2330 train_time:117118ms step_avg:59.54ms
step:1968/2330 train_time:117182ms step_avg:59.54ms
step:1969/2330 train_time:117240ms step_avg:59.54ms
step:1970/2330 train_time:117303ms step_avg:59.54ms
step:1971/2330 train_time:117360ms step_avg:59.54ms
step:1972/2330 train_time:117422ms step_avg:59.54ms
step:1973/2330 train_time:117480ms step_avg:59.54ms
step:1974/2330 train_time:117542ms step_avg:59.55ms
step:1975/2330 train_time:117599ms step_avg:59.54ms
step:1976/2330 train_time:117662ms step_avg:59.55ms
step:1977/2330 train_time:117719ms step_avg:59.54ms
step:1978/2330 train_time:117782ms step_avg:59.55ms
step:1979/2330 train_time:117839ms step_avg:59.54ms
step:1980/2330 train_time:117901ms step_avg:59.55ms
step:1981/2330 train_time:117958ms step_avg:59.54ms
step:1982/2330 train_time:118021ms step_avg:59.55ms
step:1983/2330 train_time:118078ms step_avg:59.54ms
step:1984/2330 train_time:118141ms step_avg:59.55ms
step:1985/2330 train_time:118198ms step_avg:59.55ms
step:1986/2330 train_time:118261ms step_avg:59.55ms
step:1987/2330 train_time:118318ms step_avg:59.55ms
step:1988/2330 train_time:118380ms step_avg:59.55ms
step:1989/2330 train_time:118437ms step_avg:59.55ms
step:1990/2330 train_time:118501ms step_avg:59.55ms
step:1991/2330 train_time:118558ms step_avg:59.55ms
step:1992/2330 train_time:118620ms step_avg:59.55ms
step:1993/2330 train_time:118678ms step_avg:59.55ms
step:1994/2330 train_time:118741ms step_avg:59.55ms
step:1995/2330 train_time:118798ms step_avg:59.55ms
step:1996/2330 train_time:118860ms step_avg:59.55ms
step:1997/2330 train_time:118917ms step_avg:59.55ms
step:1998/2330 train_time:118980ms step_avg:59.55ms
step:1999/2330 train_time:119037ms step_avg:59.55ms
step:2000/2330 train_time:119100ms step_avg:59.55ms
step:2000/2330 val_loss:3.7688 train_time:119180ms step_avg:59.59ms
step:2001/2330 train_time:119201ms step_avg:59.57ms
step:2002/2330 train_time:119223ms step_avg:59.55ms
step:2003/2330 train_time:119282ms step_avg:59.55ms
step:2004/2330 train_time:119350ms step_avg:59.56ms
step:2005/2330 train_time:119407ms step_avg:59.55ms
step:2006/2330 train_time:119470ms step_avg:59.56ms
step:2007/2330 train_time:119527ms step_avg:59.56ms
step:2008/2330 train_time:119590ms step_avg:59.56ms
step:2009/2330 train_time:119647ms step_avg:59.56ms
step:2010/2330 train_time:119708ms step_avg:59.56ms
step:2011/2330 train_time:119765ms step_avg:59.56ms
step:2012/2330 train_time:119827ms step_avg:59.56ms
step:2013/2330 train_time:119883ms step_avg:59.55ms
step:2014/2330 train_time:119945ms step_avg:59.56ms
step:2015/2330 train_time:120002ms step_avg:59.55ms
step:2016/2330 train_time:120063ms step_avg:59.56ms
step:2017/2330 train_time:120120ms step_avg:59.55ms
step:2018/2330 train_time:120186ms step_avg:59.56ms
step:2019/2330 train_time:120244ms step_avg:59.56ms
step:2020/2330 train_time:120309ms step_avg:59.56ms
step:2021/2330 train_time:120367ms step_avg:59.56ms
step:2022/2330 train_time:120431ms step_avg:59.56ms
step:2023/2330 train_time:120487ms step_avg:59.56ms
step:2024/2330 train_time:120551ms step_avg:59.56ms
step:2025/2330 train_time:120608ms step_avg:59.56ms
step:2026/2330 train_time:120670ms step_avg:59.56ms
step:2027/2330 train_time:120728ms step_avg:59.56ms
step:2028/2330 train_time:120789ms step_avg:59.56ms
step:2029/2330 train_time:120847ms step_avg:59.56ms
step:2030/2330 train_time:120908ms step_avg:59.56ms
step:2031/2330 train_time:120965ms step_avg:59.56ms
step:2032/2330 train_time:121025ms step_avg:59.56ms
step:2033/2330 train_time:121083ms step_avg:59.56ms
step:2034/2330 train_time:121146ms step_avg:59.56ms
step:2035/2330 train_time:121204ms step_avg:59.56ms
step:2036/2330 train_time:121267ms step_avg:59.56ms
step:2037/2330 train_time:121325ms step_avg:59.56ms
step:2038/2330 train_time:121388ms step_avg:59.56ms
step:2039/2330 train_time:121445ms step_avg:59.56ms
step:2040/2330 train_time:121508ms step_avg:59.56ms
step:2041/2330 train_time:121565ms step_avg:59.56ms
step:2042/2330 train_time:121628ms step_avg:59.56ms
step:2043/2330 train_time:121685ms step_avg:59.56ms
step:2044/2330 train_time:121748ms step_avg:59.56ms
step:2045/2330 train_time:121805ms step_avg:59.56ms
step:2046/2330 train_time:121868ms step_avg:59.56ms
step:2047/2330 train_time:121925ms step_avg:59.56ms
step:2048/2330 train_time:121987ms step_avg:59.56ms
step:2049/2330 train_time:122044ms step_avg:59.56ms
step:2050/2330 train_time:122107ms step_avg:59.56ms
step:2051/2330 train_time:122164ms step_avg:59.56ms
step:2052/2330 train_time:122227ms step_avg:59.57ms
step:2053/2330 train_time:122285ms step_avg:59.56ms
step:2054/2330 train_time:122348ms step_avg:59.57ms
step:2055/2330 train_time:122405ms step_avg:59.56ms
step:2056/2330 train_time:122469ms step_avg:59.57ms
step:2057/2330 train_time:122526ms step_avg:59.57ms
step:2058/2330 train_time:122589ms step_avg:59.57ms
step:2059/2330 train_time:122646ms step_avg:59.57ms
step:2060/2330 train_time:122709ms step_avg:59.57ms
step:2061/2330 train_time:122766ms step_avg:59.57ms
step:2062/2330 train_time:122829ms step_avg:59.57ms
step:2063/2330 train_time:122886ms step_avg:59.57ms
step:2064/2330 train_time:122948ms step_avg:59.57ms
step:2065/2330 train_time:123005ms step_avg:59.57ms
step:2066/2330 train_time:123068ms step_avg:59.57ms
step:2067/2330 train_time:123125ms step_avg:59.57ms
step:2068/2330 train_time:123189ms step_avg:59.57ms
step:2069/2330 train_time:123247ms step_avg:59.57ms
step:2070/2330 train_time:123309ms step_avg:59.57ms
step:2071/2330 train_time:123367ms step_avg:59.57ms
step:2072/2330 train_time:123430ms step_avg:59.57ms
step:2073/2330 train_time:123488ms step_avg:59.57ms
step:2074/2330 train_time:123550ms step_avg:59.57ms
step:2075/2330 train_time:123607ms step_avg:59.57ms
step:2076/2330 train_time:123669ms step_avg:59.57ms
step:2077/2330 train_time:123726ms step_avg:59.57ms
step:2078/2330 train_time:123789ms step_avg:59.57ms
step:2079/2330 train_time:123846ms step_avg:59.57ms
step:2080/2330 train_time:123908ms step_avg:59.57ms
step:2081/2330 train_time:123965ms step_avg:59.57ms
step:2082/2330 train_time:124027ms step_avg:59.57ms
step:2083/2330 train_time:124085ms step_avg:59.57ms
step:2084/2330 train_time:124147ms step_avg:59.57ms
step:2085/2330 train_time:124204ms step_avg:59.57ms
step:2086/2330 train_time:124267ms step_avg:59.57ms
step:2087/2330 train_time:124324ms step_avg:59.57ms
step:2088/2330 train_time:124387ms step_avg:59.57ms
step:2089/2330 train_time:124444ms step_avg:59.57ms
step:2090/2330 train_time:124508ms step_avg:59.57ms
step:2091/2330 train_time:124565ms step_avg:59.57ms
step:2092/2330 train_time:124627ms step_avg:59.57ms
step:2093/2330 train_time:124685ms step_avg:59.57ms
step:2094/2330 train_time:124747ms step_avg:59.57ms
step:2095/2330 train_time:124803ms step_avg:59.57ms
step:2096/2330 train_time:124867ms step_avg:59.57ms
step:2097/2330 train_time:124924ms step_avg:59.57ms
step:2098/2330 train_time:124987ms step_avg:59.57ms
step:2099/2330 train_time:125044ms step_avg:59.57ms
step:2100/2330 train_time:125106ms step_avg:59.57ms
step:2101/2330 train_time:125163ms step_avg:59.57ms
step:2102/2330 train_time:125226ms step_avg:59.57ms
step:2103/2330 train_time:125284ms step_avg:59.57ms
step:2104/2330 train_time:125346ms step_avg:59.57ms
step:2105/2330 train_time:125403ms step_avg:59.57ms
step:2106/2330 train_time:125467ms step_avg:59.58ms
step:2107/2330 train_time:125524ms step_avg:59.57ms
step:2108/2330 train_time:125587ms step_avg:59.58ms
step:2109/2330 train_time:125645ms step_avg:59.58ms
step:2110/2330 train_time:125707ms step_avg:59.58ms
step:2111/2330 train_time:125765ms step_avg:59.58ms
step:2112/2330 train_time:125826ms step_avg:59.58ms
step:2113/2330 train_time:125883ms step_avg:59.58ms
step:2114/2330 train_time:125946ms step_avg:59.58ms
step:2115/2330 train_time:126003ms step_avg:59.58ms
step:2116/2330 train_time:126065ms step_avg:59.58ms
step:2117/2330 train_time:126122ms step_avg:59.58ms
step:2118/2330 train_time:126184ms step_avg:59.58ms
step:2119/2330 train_time:126242ms step_avg:59.58ms
step:2120/2330 train_time:126305ms step_avg:59.58ms
step:2121/2330 train_time:126362ms step_avg:59.58ms
step:2122/2330 train_time:126424ms step_avg:59.58ms
step:2123/2330 train_time:126482ms step_avg:59.58ms
step:2124/2330 train_time:126545ms step_avg:59.58ms
step:2125/2330 train_time:126602ms step_avg:59.58ms
step:2126/2330 train_time:126665ms step_avg:59.58ms
step:2127/2330 train_time:126723ms step_avg:59.58ms
step:2128/2330 train_time:126786ms step_avg:59.58ms
step:2129/2330 train_time:126843ms step_avg:59.58ms
step:2130/2330 train_time:126906ms step_avg:59.58ms
step:2131/2330 train_time:126963ms step_avg:59.58ms
step:2132/2330 train_time:127026ms step_avg:59.58ms
step:2133/2330 train_time:127083ms step_avg:59.58ms
step:2134/2330 train_time:127146ms step_avg:59.58ms
step:2135/2330 train_time:127203ms step_avg:59.58ms
step:2136/2330 train_time:127266ms step_avg:59.58ms
step:2137/2330 train_time:127322ms step_avg:59.58ms
step:2138/2330 train_time:127385ms step_avg:59.58ms
step:2139/2330 train_time:127442ms step_avg:59.58ms
step:2140/2330 train_time:127505ms step_avg:59.58ms
step:2141/2330 train_time:127563ms step_avg:59.58ms
step:2142/2330 train_time:127625ms step_avg:59.58ms
step:2143/2330 train_time:127683ms step_avg:59.58ms
step:2144/2330 train_time:127745ms step_avg:59.58ms
step:2145/2330 train_time:127802ms step_avg:59.58ms
step:2146/2330 train_time:127865ms step_avg:59.58ms
step:2147/2330 train_time:127922ms step_avg:59.58ms
step:2148/2330 train_time:127985ms step_avg:59.58ms
step:2149/2330 train_time:128042ms step_avg:59.58ms
step:2150/2330 train_time:128105ms step_avg:59.58ms
step:2151/2330 train_time:128162ms step_avg:59.58ms
step:2152/2330 train_time:128225ms step_avg:59.58ms
step:2153/2330 train_time:128282ms step_avg:59.58ms
step:2154/2330 train_time:128343ms step_avg:59.58ms
step:2155/2330 train_time:128400ms step_avg:59.58ms
step:2156/2330 train_time:128464ms step_avg:59.58ms
step:2157/2330 train_time:128521ms step_avg:59.58ms
step:2158/2330 train_time:128585ms step_avg:59.59ms
step:2159/2330 train_time:128643ms step_avg:59.58ms
step:2160/2330 train_time:128705ms step_avg:59.59ms
step:2161/2330 train_time:128763ms step_avg:59.58ms
step:2162/2330 train_time:128825ms step_avg:59.59ms
step:2163/2330 train_time:128882ms step_avg:59.58ms
step:2164/2330 train_time:128945ms step_avg:59.59ms
step:2165/2330 train_time:129002ms step_avg:59.59ms
step:2166/2330 train_time:129064ms step_avg:59.59ms
step:2167/2330 train_time:129122ms step_avg:59.59ms
step:2168/2330 train_time:129185ms step_avg:59.59ms
step:2169/2330 train_time:129241ms step_avg:59.59ms
step:2170/2330 train_time:129304ms step_avg:59.59ms
step:2171/2330 train_time:129361ms step_avg:59.59ms
step:2172/2330 train_time:129424ms step_avg:59.59ms
step:2173/2330 train_time:129482ms step_avg:59.59ms
step:2174/2330 train_time:129544ms step_avg:59.59ms
step:2175/2330 train_time:129602ms step_avg:59.59ms
step:2176/2330 train_time:129665ms step_avg:59.59ms
step:2177/2330 train_time:129722ms step_avg:59.59ms
step:2178/2330 train_time:129785ms step_avg:59.59ms
step:2179/2330 train_time:129842ms step_avg:59.59ms
step:2180/2330 train_time:129905ms step_avg:59.59ms
step:2181/2330 train_time:129962ms step_avg:59.59ms
step:2182/2330 train_time:130025ms step_avg:59.59ms
step:2183/2330 train_time:130082ms step_avg:59.59ms
step:2184/2330 train_time:130145ms step_avg:59.59ms
step:2185/2330 train_time:130202ms step_avg:59.59ms
step:2186/2330 train_time:130264ms step_avg:59.59ms
step:2187/2330 train_time:130322ms step_avg:59.59ms
step:2188/2330 train_time:130384ms step_avg:59.59ms
step:2189/2330 train_time:130442ms step_avg:59.59ms
step:2190/2330 train_time:130504ms step_avg:59.59ms
step:2191/2330 train_time:130562ms step_avg:59.59ms
step:2192/2330 train_time:130625ms step_avg:59.59ms
step:2193/2330 train_time:130682ms step_avg:59.59ms
step:2194/2330 train_time:130745ms step_avg:59.59ms
step:2195/2330 train_time:130802ms step_avg:59.59ms
step:2196/2330 train_time:130864ms step_avg:59.59ms
step:2197/2330 train_time:130921ms step_avg:59.59ms
step:2198/2330 train_time:130984ms step_avg:59.59ms
step:2199/2330 train_time:131042ms step_avg:59.59ms
step:2200/2330 train_time:131105ms step_avg:59.59ms
step:2201/2330 train_time:131162ms step_avg:59.59ms
step:2202/2330 train_time:131225ms step_avg:59.59ms
step:2203/2330 train_time:131282ms step_avg:59.59ms
step:2204/2330 train_time:131345ms step_avg:59.59ms
step:2205/2330 train_time:131402ms step_avg:59.59ms
step:2206/2330 train_time:131465ms step_avg:59.59ms
step:2207/2330 train_time:131522ms step_avg:59.59ms
step:2208/2330 train_time:131585ms step_avg:59.59ms
step:2209/2330 train_time:131642ms step_avg:59.59ms
step:2210/2330 train_time:131706ms step_avg:59.60ms
step:2211/2330 train_time:131762ms step_avg:59.59ms
step:2212/2330 train_time:131826ms step_avg:59.60ms
step:2213/2330 train_time:131883ms step_avg:59.59ms
step:2214/2330 train_time:131946ms step_avg:59.60ms
step:2215/2330 train_time:132003ms step_avg:59.60ms
step:2216/2330 train_time:132066ms step_avg:59.60ms
step:2217/2330 train_time:132123ms step_avg:59.60ms
step:2218/2330 train_time:132186ms step_avg:59.60ms
step:2219/2330 train_time:132243ms step_avg:59.60ms
step:2220/2330 train_time:132306ms step_avg:59.60ms
step:2221/2330 train_time:132363ms step_avg:59.60ms
step:2222/2330 train_time:132426ms step_avg:59.60ms
step:2223/2330 train_time:132482ms step_avg:59.60ms
step:2224/2330 train_time:132546ms step_avg:59.60ms
step:2225/2330 train_time:132603ms step_avg:59.60ms
step:2226/2330 train_time:132666ms step_avg:59.60ms
step:2227/2330 train_time:132723ms step_avg:59.60ms
step:2228/2330 train_time:132786ms step_avg:59.60ms
step:2229/2330 train_time:132843ms step_avg:59.60ms
step:2230/2330 train_time:132906ms step_avg:59.60ms
step:2231/2330 train_time:132963ms step_avg:59.60ms
step:2232/2330 train_time:133026ms step_avg:59.60ms
step:2233/2330 train_time:133083ms step_avg:59.60ms
step:2234/2330 train_time:133146ms step_avg:59.60ms
step:2235/2330 train_time:133203ms step_avg:59.60ms
step:2236/2330 train_time:133266ms step_avg:59.60ms
step:2237/2330 train_time:133323ms step_avg:59.60ms
step:2238/2330 train_time:133386ms step_avg:59.60ms
step:2239/2330 train_time:133443ms step_avg:59.60ms
step:2240/2330 train_time:133506ms step_avg:59.60ms
step:2241/2330 train_time:133563ms step_avg:59.60ms
step:2242/2330 train_time:133625ms step_avg:59.60ms
step:2243/2330 train_time:133682ms step_avg:59.60ms
step:2244/2330 train_time:133744ms step_avg:59.60ms
step:2245/2330 train_time:133802ms step_avg:59.60ms
step:2246/2330 train_time:133864ms step_avg:59.60ms
step:2247/2330 train_time:133921ms step_avg:59.60ms
step:2248/2330 train_time:133985ms step_avg:59.60ms
step:2249/2330 train_time:134042ms step_avg:59.60ms
step:2250/2330 train_time:134104ms step_avg:59.60ms
step:2250/2330 val_loss:3.7193 train_time:134184ms step_avg:59.64ms
step:2251/2330 train_time:134205ms step_avg:59.62ms
step:2252/2330 train_time:134227ms step_avg:59.60ms
step:2253/2330 train_time:134289ms step_avg:59.60ms
step:2254/2330 train_time:134356ms step_avg:59.61ms
step:2255/2330 train_time:134414ms step_avg:59.61ms
step:2256/2330 train_time:134478ms step_avg:59.61ms
step:2257/2330 train_time:134535ms step_avg:59.61ms
step:2258/2330 train_time:134597ms step_avg:59.61ms
step:2259/2330 train_time:134654ms step_avg:59.61ms
step:2260/2330 train_time:134715ms step_avg:59.61ms
step:2261/2330 train_time:134773ms step_avg:59.61ms
step:2262/2330 train_time:134833ms step_avg:59.61ms
step:2263/2330 train_time:134890ms step_avg:59.61ms
step:2264/2330 train_time:134952ms step_avg:59.61ms
step:2265/2330 train_time:135009ms step_avg:59.61ms
step:2266/2330 train_time:135071ms step_avg:59.61ms
step:2267/2330 train_time:135128ms step_avg:59.61ms
step:2268/2330 train_time:135195ms step_avg:59.61ms
step:2269/2330 train_time:135253ms step_avg:59.61ms
step:2270/2330 train_time:135320ms step_avg:59.61ms
step:2271/2330 train_time:135378ms step_avg:59.61ms
step:2272/2330 train_time:135442ms step_avg:59.61ms
step:2273/2330 train_time:135500ms step_avg:59.61ms
step:2274/2330 train_time:135562ms step_avg:59.61ms
step:2275/2330 train_time:135619ms step_avg:59.61ms
step:2276/2330 train_time:135680ms step_avg:59.61ms
step:2277/2330 train_time:135737ms step_avg:59.61ms
step:2278/2330 train_time:135799ms step_avg:59.61ms
step:2279/2330 train_time:135856ms step_avg:59.61ms
step:2280/2330 train_time:135919ms step_avg:59.61ms
step:2281/2330 train_time:135976ms step_avg:59.61ms
step:2282/2330 train_time:136038ms step_avg:59.61ms
step:2283/2330 train_time:136095ms step_avg:59.61ms
step:2284/2330 train_time:136158ms step_avg:59.61ms
step:2285/2330 train_time:136215ms step_avg:59.61ms
step:2286/2330 train_time:136280ms step_avg:59.61ms
step:2287/2330 train_time:136337ms step_avg:59.61ms
step:2288/2330 train_time:136401ms step_avg:59.62ms
step:2289/2330 train_time:136458ms step_avg:59.61ms
step:2290/2330 train_time:136522ms step_avg:59.62ms
step:2291/2330 train_time:136579ms step_avg:59.62ms
step:2292/2330 train_time:136641ms step_avg:59.62ms
step:2293/2330 train_time:136698ms step_avg:59.62ms
step:2294/2330 train_time:136761ms step_avg:59.62ms
step:2295/2330 train_time:136818ms step_avg:59.62ms
step:2296/2330 train_time:136880ms step_avg:59.62ms
step:2297/2330 train_time:136937ms step_avg:59.62ms
step:2298/2330 train_time:137000ms step_avg:59.62ms
step:2299/2330 train_time:137058ms step_avg:59.62ms
step:2300/2330 train_time:137120ms step_avg:59.62ms
step:2301/2330 train_time:137177ms step_avg:59.62ms
step:2302/2330 train_time:137241ms step_avg:59.62ms
step:2303/2330 train_time:137300ms step_avg:59.62ms
step:2304/2330 train_time:137362ms step_avg:59.62ms
step:2305/2330 train_time:137419ms step_avg:59.62ms
step:2306/2330 train_time:137482ms step_avg:59.62ms
step:2307/2330 train_time:137539ms step_avg:59.62ms
step:2308/2330 train_time:137603ms step_avg:59.62ms
step:2309/2330 train_time:137661ms step_avg:59.62ms
step:2310/2330 train_time:137723ms step_avg:59.62ms
step:2311/2330 train_time:137781ms step_avg:59.62ms
step:2312/2330 train_time:137843ms step_avg:59.62ms
step:2313/2330 train_time:137900ms step_avg:59.62ms
step:2314/2330 train_time:137962ms step_avg:59.62ms
step:2315/2330 train_time:138020ms step_avg:59.62ms
step:2316/2330 train_time:138081ms step_avg:59.62ms
step:2317/2330 train_time:138138ms step_avg:59.62ms
step:2318/2330 train_time:138201ms step_avg:59.62ms
step:2319/2330 train_time:138258ms step_avg:59.62ms
step:2320/2330 train_time:138321ms step_avg:59.62ms
step:2321/2330 train_time:138378ms step_avg:59.62ms
step:2322/2330 train_time:138441ms step_avg:59.62ms
step:2323/2330 train_time:138499ms step_avg:59.62ms
step:2324/2330 train_time:138561ms step_avg:59.62ms
step:2325/2330 train_time:138618ms step_avg:59.62ms
step:2326/2330 train_time:138681ms step_avg:59.62ms
step:2327/2330 train_time:138738ms step_avg:59.62ms
step:2328/2330 train_time:138801ms step_avg:59.62ms
step:2329/2330 train_time:138857ms step_avg:59.62ms
step:2330/2330 train_time:138920ms step_avg:59.62ms
step:2330/2330 val_loss:3.7042 train_time:139001ms step_avg:59.66ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
