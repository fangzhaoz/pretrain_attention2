import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_muon_lr5e-4"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=5e-4, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 03:54:34 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   33C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   31C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   31C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   32C    P0             115W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:91ms step_avg:91.41ms
step:2/2330 train_time:154ms step_avg:77.07ms
step:3/2330 train_time:166ms step_avg:55.48ms
step:4/2330 train_time:179ms step_avg:44.74ms
step:5/2330 train_time:191ms step_avg:38.11ms
step:6/2330 train_time:225ms step_avg:37.49ms
step:7/2330 train_time:258ms step_avg:36.92ms
step:8/2330 train_time:303ms step_avg:37.82ms
step:9/2330 train_time:337ms step_avg:37.49ms
step:10/2330 train_time:382ms step_avg:38.15ms
step:11/2330 train_time:416ms step_avg:37.78ms
step:12/2330 train_time:459ms step_avg:38.28ms
step:13/2330 train_time:494ms step_avg:37.99ms
step:14/2330 train_time:538ms step_avg:38.40ms
step:15/2330 train_time:572ms step_avg:38.14ms
step:16/2330 train_time:616ms step_avg:38.48ms
step:17/2330 train_time:650ms step_avg:38.24ms
step:18/2330 train_time:694ms step_avg:38.57ms
step:19/2330 train_time:729ms step_avg:38.36ms
step:20/2330 train_time:773ms step_avg:38.66ms
step:21/2330 train_time:808ms step_avg:38.49ms
step:22/2330 train_time:853ms step_avg:38.76ms
step:23/2330 train_time:888ms step_avg:38.59ms
step:24/2330 train_time:932ms step_avg:38.83ms
step:25/2330 train_time:967ms step_avg:38.67ms
step:26/2330 train_time:1014ms step_avg:38.99ms
step:27/2330 train_time:1052ms step_avg:38.97ms
step:28/2330 train_time:1105ms step_avg:39.45ms
step:29/2330 train_time:1142ms step_avg:39.39ms
step:30/2330 train_time:1188ms step_avg:39.61ms
step:31/2330 train_time:1224ms step_avg:39.49ms
step:32/2330 train_time:1270ms step_avg:39.68ms
step:33/2330 train_time:1305ms step_avg:39.56ms
step:34/2330 train_time:1350ms step_avg:39.72ms
step:35/2330 train_time:1386ms step_avg:39.59ms
step:36/2330 train_time:1430ms step_avg:39.74ms
step:37/2330 train_time:1466ms step_avg:39.61ms
step:38/2330 train_time:1510ms step_avg:39.74ms
step:39/2330 train_time:1545ms step_avg:39.62ms
step:40/2330 train_time:1590ms step_avg:39.75ms
step:41/2330 train_time:1625ms step_avg:39.64ms
step:42/2330 train_time:1669ms step_avg:39.75ms
step:43/2330 train_time:1704ms step_avg:39.64ms
step:44/2330 train_time:1749ms step_avg:39.75ms
step:45/2330 train_time:1784ms step_avg:39.64ms
step:46/2330 train_time:1828ms step_avg:39.75ms
step:47/2330 train_time:1863ms step_avg:39.65ms
step:48/2330 train_time:1909ms step_avg:39.77ms
step:49/2330 train_time:1944ms step_avg:39.68ms
step:50/2330 train_time:1991ms step_avg:39.81ms
step:51/2330 train_time:2027ms step_avg:39.75ms
step:52/2330 train_time:2073ms step_avg:39.87ms
step:53/2330 train_time:2110ms step_avg:39.81ms
step:54/2330 train_time:2156ms step_avg:39.93ms
step:55/2330 train_time:2195ms step_avg:39.90ms
step:56/2330 train_time:2242ms step_avg:40.03ms
step:57/2330 train_time:2277ms step_avg:39.95ms
step:58/2330 train_time:2323ms step_avg:40.06ms
step:59/2330 train_time:2359ms step_avg:39.98ms
step:60/2330 train_time:2403ms step_avg:40.05ms
step:61/2330 train_time:2438ms step_avg:39.97ms
step:62/2330 train_time:2484ms step_avg:40.06ms
step:63/2330 train_time:2519ms step_avg:39.99ms
step:64/2330 train_time:2564ms step_avg:40.07ms
step:65/2330 train_time:2600ms step_avg:39.99ms
step:66/2330 train_time:2644ms step_avg:40.06ms
step:67/2330 train_time:2678ms step_avg:39.98ms
step:68/2330 train_time:2723ms step_avg:40.05ms
step:69/2330 train_time:2759ms step_avg:39.98ms
step:70/2330 train_time:2803ms step_avg:40.05ms
step:71/2330 train_time:2839ms step_avg:39.99ms
step:72/2330 train_time:2884ms step_avg:40.05ms
step:73/2330 train_time:2919ms step_avg:39.99ms
step:74/2330 train_time:2965ms step_avg:40.07ms
step:75/2330 train_time:3002ms step_avg:40.02ms
step:76/2330 train_time:3047ms step_avg:40.09ms
step:77/2330 train_time:3083ms step_avg:40.04ms
step:78/2330 train_time:3130ms step_avg:40.13ms
step:79/2330 train_time:3167ms step_avg:40.09ms
step:80/2330 train_time:3213ms step_avg:40.16ms
step:81/2330 train_time:3248ms step_avg:40.10ms
step:82/2330 train_time:3293ms step_avg:40.16ms
step:83/2330 train_time:3330ms step_avg:40.12ms
step:84/2330 train_time:3375ms step_avg:40.18ms
step:85/2330 train_time:3411ms step_avg:40.13ms
step:86/2330 train_time:3457ms step_avg:40.20ms
step:87/2330 train_time:3493ms step_avg:40.15ms
step:88/2330 train_time:3538ms step_avg:40.21ms
step:89/2330 train_time:3575ms step_avg:40.16ms
step:90/2330 train_time:3619ms step_avg:40.21ms
step:91/2330 train_time:3654ms step_avg:40.15ms
step:92/2330 train_time:3699ms step_avg:40.20ms
step:93/2330 train_time:3735ms step_avg:40.16ms
step:94/2330 train_time:3779ms step_avg:40.20ms
step:95/2330 train_time:3814ms step_avg:40.15ms
step:96/2330 train_time:3860ms step_avg:40.20ms
step:97/2330 train_time:3897ms step_avg:40.17ms
step:98/2330 train_time:3942ms step_avg:40.22ms
step:99/2330 train_time:3978ms step_avg:40.18ms
step:100/2330 train_time:4023ms step_avg:40.23ms
step:101/2330 train_time:4060ms step_avg:40.19ms
step:102/2330 train_time:4105ms step_avg:40.25ms
step:103/2330 train_time:4141ms step_avg:40.20ms
step:104/2330 train_time:4186ms step_avg:40.25ms
step:105/2330 train_time:4221ms step_avg:40.20ms
step:106/2330 train_time:4268ms step_avg:40.27ms
step:107/2330 train_time:4304ms step_avg:40.23ms
step:108/2330 train_time:4349ms step_avg:40.27ms
step:109/2330 train_time:4384ms step_avg:40.22ms
step:110/2330 train_time:4429ms step_avg:40.26ms
step:111/2330 train_time:4464ms step_avg:40.22ms
step:112/2330 train_time:4510ms step_avg:40.27ms
step:113/2330 train_time:4546ms step_avg:40.23ms
step:114/2330 train_time:4590ms step_avg:40.27ms
step:115/2330 train_time:4626ms step_avg:40.22ms
step:116/2330 train_time:4670ms step_avg:40.26ms
step:117/2330 train_time:4706ms step_avg:40.22ms
step:118/2330 train_time:4751ms step_avg:40.26ms
step:119/2330 train_time:4786ms step_avg:40.22ms
step:120/2330 train_time:4831ms step_avg:40.26ms
step:121/2330 train_time:4867ms step_avg:40.22ms
step:122/2330 train_time:4912ms step_avg:40.26ms
step:123/2330 train_time:4948ms step_avg:40.23ms
step:124/2330 train_time:4994ms step_avg:40.27ms
step:125/2330 train_time:5029ms step_avg:40.23ms
step:126/2330 train_time:5075ms step_avg:40.28ms
step:127/2330 train_time:5111ms step_avg:40.24ms
step:128/2330 train_time:5157ms step_avg:40.29ms
step:129/2330 train_time:5195ms step_avg:40.27ms
step:130/2330 train_time:5240ms step_avg:40.31ms
step:131/2330 train_time:5277ms step_avg:40.28ms
step:132/2330 train_time:5323ms step_avg:40.32ms
step:133/2330 train_time:5359ms step_avg:40.29ms
step:134/2330 train_time:5404ms step_avg:40.33ms
step:135/2330 train_time:5440ms step_avg:40.29ms
step:136/2330 train_time:5484ms step_avg:40.33ms
step:137/2330 train_time:5520ms step_avg:40.29ms
step:138/2330 train_time:5565ms step_avg:40.33ms
step:139/2330 train_time:5601ms step_avg:40.29ms
step:140/2330 train_time:5645ms step_avg:40.32ms
step:141/2330 train_time:5681ms step_avg:40.29ms
step:142/2330 train_time:5726ms step_avg:40.32ms
step:143/2330 train_time:5761ms step_avg:40.29ms
step:144/2330 train_time:5806ms step_avg:40.32ms
step:145/2330 train_time:5842ms step_avg:40.29ms
step:146/2330 train_time:5887ms step_avg:40.32ms
step:147/2330 train_time:5923ms step_avg:40.29ms
step:148/2330 train_time:5969ms step_avg:40.33ms
step:149/2330 train_time:6004ms step_avg:40.30ms
step:150/2330 train_time:6049ms step_avg:40.33ms
step:151/2330 train_time:6085ms step_avg:40.30ms
step:152/2330 train_time:6130ms step_avg:40.33ms
step:153/2330 train_time:6166ms step_avg:40.30ms
step:154/2330 train_time:6212ms step_avg:40.34ms
step:155/2330 train_time:6247ms step_avg:40.30ms
step:156/2330 train_time:6292ms step_avg:40.33ms
step:157/2330 train_time:6328ms step_avg:40.31ms
step:158/2330 train_time:6374ms step_avg:40.34ms
step:159/2330 train_time:6411ms step_avg:40.32ms
step:160/2330 train_time:6457ms step_avg:40.36ms
step:161/2330 train_time:6493ms step_avg:40.33ms
step:162/2330 train_time:6538ms step_avg:40.36ms
step:163/2330 train_time:6574ms step_avg:40.33ms
step:164/2330 train_time:6620ms step_avg:40.37ms
step:165/2330 train_time:6655ms step_avg:40.33ms
step:166/2330 train_time:6701ms step_avg:40.37ms
step:167/2330 train_time:6737ms step_avg:40.34ms
step:168/2330 train_time:6781ms step_avg:40.37ms
step:169/2330 train_time:6817ms step_avg:40.34ms
step:170/2330 train_time:6862ms step_avg:40.37ms
step:171/2330 train_time:6899ms step_avg:40.34ms
step:172/2330 train_time:6945ms step_avg:40.38ms
step:173/2330 train_time:6980ms step_avg:40.35ms
step:174/2330 train_time:7025ms step_avg:40.37ms
step:175/2330 train_time:7060ms step_avg:40.34ms
step:176/2330 train_time:7106ms step_avg:40.37ms
step:177/2330 train_time:7142ms step_avg:40.35ms
step:178/2330 train_time:7187ms step_avg:40.38ms
step:179/2330 train_time:7223ms step_avg:40.35ms
step:180/2330 train_time:7268ms step_avg:40.38ms
step:181/2330 train_time:7304ms step_avg:40.35ms
step:182/2330 train_time:7350ms step_avg:40.38ms
step:183/2330 train_time:7386ms step_avg:40.36ms
step:184/2330 train_time:7431ms step_avg:40.39ms
step:185/2330 train_time:7466ms step_avg:40.36ms
step:186/2330 train_time:7511ms step_avg:40.38ms
step:187/2330 train_time:7546ms step_avg:40.35ms
step:188/2330 train_time:7592ms step_avg:40.38ms
step:189/2330 train_time:7627ms step_avg:40.36ms
step:190/2330 train_time:7672ms step_avg:40.38ms
step:191/2330 train_time:7708ms step_avg:40.36ms
step:192/2330 train_time:7753ms step_avg:40.38ms
step:193/2330 train_time:7789ms step_avg:40.36ms
step:194/2330 train_time:7834ms step_avg:40.38ms
step:195/2330 train_time:7869ms step_avg:40.35ms
step:196/2330 train_time:7915ms step_avg:40.38ms
step:197/2330 train_time:7951ms step_avg:40.36ms
step:198/2330 train_time:7997ms step_avg:40.39ms
step:199/2330 train_time:8033ms step_avg:40.37ms
step:200/2330 train_time:8080ms step_avg:40.40ms
step:201/2330 train_time:8115ms step_avg:40.37ms
step:202/2330 train_time:8160ms step_avg:40.40ms
step:203/2330 train_time:8197ms step_avg:40.38ms
step:204/2330 train_time:8243ms step_avg:40.40ms
step:205/2330 train_time:8279ms step_avg:40.38ms
step:206/2330 train_time:8324ms step_avg:40.41ms
step:207/2330 train_time:8360ms step_avg:40.38ms
step:208/2330 train_time:8405ms step_avg:40.41ms
step:209/2330 train_time:8440ms step_avg:40.38ms
step:210/2330 train_time:8485ms step_avg:40.40ms
step:211/2330 train_time:8521ms step_avg:40.38ms
step:212/2330 train_time:8566ms step_avg:40.40ms
step:213/2330 train_time:8602ms step_avg:40.38ms
step:214/2330 train_time:8648ms step_avg:40.41ms
step:215/2330 train_time:8683ms step_avg:40.39ms
step:216/2330 train_time:8728ms step_avg:40.41ms
step:217/2330 train_time:8764ms step_avg:40.39ms
step:218/2330 train_time:8810ms step_avg:40.41ms
step:219/2330 train_time:8845ms step_avg:40.39ms
step:220/2330 train_time:8891ms step_avg:40.41ms
step:221/2330 train_time:8926ms step_avg:40.39ms
step:222/2330 train_time:8971ms step_avg:40.41ms
step:223/2330 train_time:9007ms step_avg:40.39ms
step:224/2330 train_time:9051ms step_avg:40.41ms
step:225/2330 train_time:9087ms step_avg:40.39ms
step:226/2330 train_time:9132ms step_avg:40.41ms
step:227/2330 train_time:9168ms step_avg:40.39ms
step:228/2330 train_time:9213ms step_avg:40.41ms
step:229/2330 train_time:9249ms step_avg:40.39ms
step:230/2330 train_time:9295ms step_avg:40.41ms
step:231/2330 train_time:9331ms step_avg:40.39ms
step:232/2330 train_time:9377ms step_avg:40.42ms
step:233/2330 train_time:9413ms step_avg:40.40ms
step:234/2330 train_time:9460ms step_avg:40.43ms
step:235/2330 train_time:9496ms step_avg:40.41ms
step:236/2330 train_time:9540ms step_avg:40.42ms
step:237/2330 train_time:9577ms step_avg:40.41ms
step:238/2330 train_time:9622ms step_avg:40.43ms
step:239/2330 train_time:9658ms step_avg:40.41ms
step:240/2330 train_time:9702ms step_avg:40.43ms
step:241/2330 train_time:9739ms step_avg:40.41ms
step:242/2330 train_time:9784ms step_avg:40.43ms
step:243/2330 train_time:9820ms step_avg:40.41ms
step:244/2330 train_time:9866ms step_avg:40.43ms
step:245/2330 train_time:9901ms step_avg:40.41ms
step:246/2330 train_time:9946ms step_avg:40.43ms
step:247/2330 train_time:9981ms step_avg:40.41ms
step:248/2330 train_time:10026ms step_avg:40.43ms
step:249/2330 train_time:10062ms step_avg:40.41ms
step:250/2330 train_time:10108ms step_avg:40.43ms
step:250/2330 val_loss:5.6772 train_time:10197ms step_avg:40.79ms
step:251/2330 train_time:10211ms step_avg:40.68ms
step:252/2330 train_time:10223ms step_avg:40.57ms
step:253/2330 train_time:10234ms step_avg:40.45ms
step:254/2330 train_time:10270ms step_avg:40.43ms
step:255/2330 train_time:10305ms step_avg:40.41ms
step:256/2330 train_time:10349ms step_avg:40.42ms
step:257/2330 train_time:10383ms step_avg:40.40ms
step:258/2330 train_time:10428ms step_avg:40.42ms
step:259/2330 train_time:10463ms step_avg:40.40ms
step:260/2330 train_time:10510ms step_avg:40.42ms
step:261/2330 train_time:10549ms step_avg:40.42ms
step:262/2330 train_time:10598ms step_avg:40.45ms
step:263/2330 train_time:10636ms step_avg:40.44ms
step:264/2330 train_time:10682ms step_avg:40.46ms
step:265/2330 train_time:10718ms step_avg:40.45ms
step:266/2330 train_time:10764ms step_avg:40.46ms
step:267/2330 train_time:10799ms step_avg:40.45ms
step:268/2330 train_time:10843ms step_avg:40.46ms
step:269/2330 train_time:10878ms step_avg:40.44ms
step:270/2330 train_time:10923ms step_avg:40.45ms
step:271/2330 train_time:10958ms step_avg:40.44ms
step:272/2330 train_time:11002ms step_avg:40.45ms
step:273/2330 train_time:11037ms step_avg:40.43ms
step:274/2330 train_time:11084ms step_avg:40.45ms
step:275/2330 train_time:11122ms step_avg:40.44ms
step:276/2330 train_time:11170ms step_avg:40.47ms
step:277/2330 train_time:11206ms step_avg:40.45ms
step:278/2330 train_time:11250ms step_avg:40.47ms
step:279/2330 train_time:11285ms step_avg:40.45ms
step:280/2330 train_time:11329ms step_avg:40.46ms
step:281/2330 train_time:11364ms step_avg:40.44ms
step:282/2330 train_time:11409ms step_avg:40.46ms
step:283/2330 train_time:11444ms step_avg:40.44ms
step:284/2330 train_time:11491ms step_avg:40.46ms
step:285/2330 train_time:11528ms step_avg:40.45ms
step:286/2330 train_time:11574ms step_avg:40.47ms
step:287/2330 train_time:11611ms step_avg:40.46ms
step:288/2330 train_time:11657ms step_avg:40.47ms
step:289/2330 train_time:11693ms step_avg:40.46ms
step:290/2330 train_time:11739ms step_avg:40.48ms
step:291/2330 train_time:11775ms step_avg:40.46ms
step:292/2330 train_time:11821ms step_avg:40.48ms
step:293/2330 train_time:11857ms step_avg:40.47ms
step:294/2330 train_time:11901ms step_avg:40.48ms
step:295/2330 train_time:11936ms step_avg:40.46ms
step:296/2330 train_time:11981ms step_avg:40.48ms
step:297/2330 train_time:12017ms step_avg:40.46ms
step:298/2330 train_time:12063ms step_avg:40.48ms
step:299/2330 train_time:12100ms step_avg:40.47ms
step:300/2330 train_time:12146ms step_avg:40.49ms
step:301/2330 train_time:12182ms step_avg:40.47ms
step:302/2330 train_time:12228ms step_avg:40.49ms
step:303/2330 train_time:12263ms step_avg:40.47ms
step:304/2330 train_time:12308ms step_avg:40.49ms
step:305/2330 train_time:12343ms step_avg:40.47ms
step:306/2330 train_time:12388ms step_avg:40.48ms
step:307/2330 train_time:12424ms step_avg:40.47ms
step:308/2330 train_time:12470ms step_avg:40.49ms
step:309/2330 train_time:12506ms step_avg:40.47ms
step:310/2330 train_time:12552ms step_avg:40.49ms
step:311/2330 train_time:12588ms step_avg:40.48ms
step:312/2330 train_time:12633ms step_avg:40.49ms
step:313/2330 train_time:12669ms step_avg:40.48ms
step:314/2330 train_time:12714ms step_avg:40.49ms
step:315/2330 train_time:12749ms step_avg:40.47ms
step:316/2330 train_time:12794ms step_avg:40.49ms
step:317/2330 train_time:12830ms step_avg:40.47ms
step:318/2330 train_time:12875ms step_avg:40.49ms
step:319/2330 train_time:12911ms step_avg:40.47ms
step:320/2330 train_time:12956ms step_avg:40.49ms
step:321/2330 train_time:12991ms step_avg:40.47ms
step:322/2330 train_time:13036ms step_avg:40.49ms
step:323/2330 train_time:13072ms step_avg:40.47ms
step:324/2330 train_time:13117ms step_avg:40.48ms
step:325/2330 train_time:13153ms step_avg:40.47ms
step:326/2330 train_time:13199ms step_avg:40.49ms
step:327/2330 train_time:13236ms step_avg:40.48ms
step:328/2330 train_time:13281ms step_avg:40.49ms
step:329/2330 train_time:13317ms step_avg:40.48ms
step:330/2330 train_time:13364ms step_avg:40.50ms
step:331/2330 train_time:13401ms step_avg:40.49ms
step:332/2330 train_time:13446ms step_avg:40.50ms
step:333/2330 train_time:13482ms step_avg:40.49ms
step:334/2330 train_time:13528ms step_avg:40.50ms
step:335/2330 train_time:13563ms step_avg:40.49ms
step:336/2330 train_time:13609ms step_avg:40.50ms
step:337/2330 train_time:13645ms step_avg:40.49ms
step:338/2330 train_time:13691ms step_avg:40.51ms
step:339/2330 train_time:13726ms step_avg:40.49ms
step:340/2330 train_time:13771ms step_avg:40.50ms
step:341/2330 train_time:13806ms step_avg:40.49ms
step:342/2330 train_time:13852ms step_avg:40.50ms
step:343/2330 train_time:13888ms step_avg:40.49ms
step:344/2330 train_time:13934ms step_avg:40.50ms
step:345/2330 train_time:13969ms step_avg:40.49ms
step:346/2330 train_time:14014ms step_avg:40.50ms
step:347/2330 train_time:14050ms step_avg:40.49ms
step:348/2330 train_time:14094ms step_avg:40.50ms
step:349/2330 train_time:14130ms step_avg:40.49ms
step:350/2330 train_time:14175ms step_avg:40.50ms
step:351/2330 train_time:14212ms step_avg:40.49ms
step:352/2330 train_time:14258ms step_avg:40.50ms
step:353/2330 train_time:14294ms step_avg:40.49ms
step:354/2330 train_time:14340ms step_avg:40.51ms
step:355/2330 train_time:14377ms step_avg:40.50ms
step:356/2330 train_time:14422ms step_avg:40.51ms
step:357/2330 train_time:14458ms step_avg:40.50ms
step:358/2330 train_time:14504ms step_avg:40.51ms
step:359/2330 train_time:14540ms step_avg:40.50ms
step:360/2330 train_time:14585ms step_avg:40.51ms
step:361/2330 train_time:14620ms step_avg:40.50ms
step:362/2330 train_time:14666ms step_avg:40.51ms
step:363/2330 train_time:14701ms step_avg:40.50ms
step:364/2330 train_time:14747ms step_avg:40.51ms
step:365/2330 train_time:14782ms step_avg:40.50ms
step:366/2330 train_time:14828ms step_avg:40.51ms
step:367/2330 train_time:14863ms step_avg:40.50ms
step:368/2330 train_time:14909ms step_avg:40.51ms
step:369/2330 train_time:14944ms step_avg:40.50ms
step:370/2330 train_time:14990ms step_avg:40.51ms
step:371/2330 train_time:15025ms step_avg:40.50ms
step:372/2330 train_time:15070ms step_avg:40.51ms
step:373/2330 train_time:15106ms step_avg:40.50ms
step:374/2330 train_time:15151ms step_avg:40.51ms
step:375/2330 train_time:15186ms step_avg:40.50ms
step:376/2330 train_time:15233ms step_avg:40.51ms
step:377/2330 train_time:15269ms step_avg:40.50ms
step:378/2330 train_time:15314ms step_avg:40.51ms
step:379/2330 train_time:15350ms step_avg:40.50ms
step:380/2330 train_time:15394ms step_avg:40.51ms
step:381/2330 train_time:15430ms step_avg:40.50ms
step:382/2330 train_time:15475ms step_avg:40.51ms
step:383/2330 train_time:15512ms step_avg:40.50ms
step:384/2330 train_time:15559ms step_avg:40.52ms
step:385/2330 train_time:15595ms step_avg:40.51ms
step:386/2330 train_time:15641ms step_avg:40.52ms
step:387/2330 train_time:15676ms step_avg:40.51ms
step:388/2330 train_time:15721ms step_avg:40.52ms
step:389/2330 train_time:15757ms step_avg:40.51ms
step:390/2330 train_time:15804ms step_avg:40.52ms
step:391/2330 train_time:15839ms step_avg:40.51ms
step:392/2330 train_time:15885ms step_avg:40.52ms
step:393/2330 train_time:15920ms step_avg:40.51ms
step:394/2330 train_time:15966ms step_avg:40.52ms
step:395/2330 train_time:16001ms step_avg:40.51ms
step:396/2330 train_time:16046ms step_avg:40.52ms
step:397/2330 train_time:16082ms step_avg:40.51ms
step:398/2330 train_time:16128ms step_avg:40.52ms
step:399/2330 train_time:16164ms step_avg:40.51ms
step:400/2330 train_time:16209ms step_avg:40.52ms
step:401/2330 train_time:16244ms step_avg:40.51ms
step:402/2330 train_time:16290ms step_avg:40.52ms
step:403/2330 train_time:16326ms step_avg:40.51ms
step:404/2330 train_time:16372ms step_avg:40.53ms
step:405/2330 train_time:16408ms step_avg:40.51ms
step:406/2330 train_time:16454ms step_avg:40.53ms
step:407/2330 train_time:16489ms step_avg:40.51ms
step:408/2330 train_time:16534ms step_avg:40.52ms
step:409/2330 train_time:16569ms step_avg:40.51ms
step:410/2330 train_time:16615ms step_avg:40.52ms
step:411/2330 train_time:16651ms step_avg:40.51ms
step:412/2330 train_time:16696ms step_avg:40.53ms
step:413/2330 train_time:16732ms step_avg:40.51ms
step:414/2330 train_time:16777ms step_avg:40.52ms
step:415/2330 train_time:16813ms step_avg:40.51ms
step:416/2330 train_time:16858ms step_avg:40.52ms
step:417/2330 train_time:16896ms step_avg:40.52ms
step:418/2330 train_time:16941ms step_avg:40.53ms
step:419/2330 train_time:16977ms step_avg:40.52ms
step:420/2330 train_time:17023ms step_avg:40.53ms
step:421/2330 train_time:17060ms step_avg:40.52ms
step:422/2330 train_time:17106ms step_avg:40.53ms
step:423/2330 train_time:17141ms step_avg:40.52ms
step:424/2330 train_time:17187ms step_avg:40.53ms
step:425/2330 train_time:17222ms step_avg:40.52ms
step:426/2330 train_time:17268ms step_avg:40.53ms
step:427/2330 train_time:17304ms step_avg:40.52ms
step:428/2330 train_time:17349ms step_avg:40.54ms
step:429/2330 train_time:17385ms step_avg:40.52ms
step:430/2330 train_time:17431ms step_avg:40.54ms
step:431/2330 train_time:17467ms step_avg:40.53ms
step:432/2330 train_time:17513ms step_avg:40.54ms
step:433/2330 train_time:17549ms step_avg:40.53ms
step:434/2330 train_time:17594ms step_avg:40.54ms
step:435/2330 train_time:17629ms step_avg:40.53ms
step:436/2330 train_time:17674ms step_avg:40.54ms
step:437/2330 train_time:17710ms step_avg:40.53ms
step:438/2330 train_time:17755ms step_avg:40.54ms
step:439/2330 train_time:17791ms step_avg:40.53ms
step:440/2330 train_time:17836ms step_avg:40.54ms
step:441/2330 train_time:17872ms step_avg:40.53ms
step:442/2330 train_time:17917ms step_avg:40.54ms
step:443/2330 train_time:17953ms step_avg:40.53ms
step:444/2330 train_time:17999ms step_avg:40.54ms
step:445/2330 train_time:18036ms step_avg:40.53ms
step:446/2330 train_time:18082ms step_avg:40.54ms
step:447/2330 train_time:18117ms step_avg:40.53ms
step:448/2330 train_time:18163ms step_avg:40.54ms
step:449/2330 train_time:18200ms step_avg:40.53ms
step:450/2330 train_time:18245ms step_avg:40.54ms
step:451/2330 train_time:18282ms step_avg:40.54ms
step:452/2330 train_time:18327ms step_avg:40.55ms
step:453/2330 train_time:18362ms step_avg:40.53ms
step:454/2330 train_time:18408ms step_avg:40.55ms
step:455/2330 train_time:18444ms step_avg:40.54ms
step:456/2330 train_time:18489ms step_avg:40.55ms
step:457/2330 train_time:18524ms step_avg:40.53ms
step:458/2330 train_time:18570ms step_avg:40.55ms
step:459/2330 train_time:18605ms step_avg:40.53ms
step:460/2330 train_time:18651ms step_avg:40.55ms
step:461/2330 train_time:18688ms step_avg:40.54ms
step:462/2330 train_time:18733ms step_avg:40.55ms
step:463/2330 train_time:18768ms step_avg:40.54ms
step:464/2330 train_time:18814ms step_avg:40.55ms
step:465/2330 train_time:18849ms step_avg:40.53ms
step:466/2330 train_time:18894ms step_avg:40.54ms
step:467/2330 train_time:18930ms step_avg:40.53ms
step:468/2330 train_time:18976ms step_avg:40.55ms
step:469/2330 train_time:19011ms step_avg:40.54ms
step:470/2330 train_time:19057ms step_avg:40.55ms
step:471/2330 train_time:19093ms step_avg:40.54ms
step:472/2330 train_time:19138ms step_avg:40.55ms
step:473/2330 train_time:19175ms step_avg:40.54ms
step:474/2330 train_time:19221ms step_avg:40.55ms
step:475/2330 train_time:19257ms step_avg:40.54ms
step:476/2330 train_time:19303ms step_avg:40.55ms
step:477/2330 train_time:19339ms step_avg:40.54ms
step:478/2330 train_time:19384ms step_avg:40.55ms
step:479/2330 train_time:19420ms step_avg:40.54ms
step:480/2330 train_time:19465ms step_avg:40.55ms
step:481/2330 train_time:19501ms step_avg:40.54ms
step:482/2330 train_time:19547ms step_avg:40.55ms
step:483/2330 train_time:19582ms step_avg:40.54ms
step:484/2330 train_time:19628ms step_avg:40.55ms
step:485/2330 train_time:19664ms step_avg:40.54ms
step:486/2330 train_time:19709ms step_avg:40.55ms
step:487/2330 train_time:19745ms step_avg:40.55ms
step:488/2330 train_time:19792ms step_avg:40.56ms
step:489/2330 train_time:19828ms step_avg:40.55ms
step:490/2330 train_time:19874ms step_avg:40.56ms
step:491/2330 train_time:19911ms step_avg:40.55ms
step:492/2330 train_time:19955ms step_avg:40.56ms
step:493/2330 train_time:19990ms step_avg:40.55ms
step:494/2330 train_time:20035ms step_avg:40.56ms
step:495/2330 train_time:20070ms step_avg:40.55ms
step:496/2330 train_time:20116ms step_avg:40.56ms
step:497/2330 train_time:20153ms step_avg:40.55ms
step:498/2330 train_time:20198ms step_avg:40.56ms
step:499/2330 train_time:20235ms step_avg:40.55ms
step:500/2330 train_time:20280ms step_avg:40.56ms
step:500/2330 val_loss:5.4710 train_time:20371ms step_avg:40.74ms
step:501/2330 train_time:20384ms step_avg:40.69ms
step:502/2330 train_time:20397ms step_avg:40.63ms
step:503/2330 train_time:20408ms step_avg:40.57ms
step:504/2330 train_time:20445ms step_avg:40.56ms
step:505/2330 train_time:20479ms step_avg:40.55ms
step:506/2330 train_time:20524ms step_avg:40.56ms
step:507/2330 train_time:20558ms step_avg:40.55ms
step:508/2330 train_time:20603ms step_avg:40.56ms
step:509/2330 train_time:20638ms step_avg:40.55ms
step:510/2330 train_time:20686ms step_avg:40.56ms
step:511/2330 train_time:20724ms step_avg:40.56ms
step:512/2330 train_time:20772ms step_avg:40.57ms
step:513/2330 train_time:20808ms step_avg:40.56ms
step:514/2330 train_time:20854ms step_avg:40.57ms
step:515/2330 train_time:20891ms step_avg:40.56ms
step:516/2330 train_time:20936ms step_avg:40.57ms
step:517/2330 train_time:20972ms step_avg:40.56ms
step:518/2330 train_time:21016ms step_avg:40.57ms
step:519/2330 train_time:21052ms step_avg:40.56ms
step:520/2330 train_time:21099ms step_avg:40.57ms
step:521/2330 train_time:21134ms step_avg:40.56ms
step:522/2330 train_time:21180ms step_avg:40.57ms
step:523/2330 train_time:21216ms step_avg:40.57ms
step:524/2330 train_time:21261ms step_avg:40.58ms
step:525/2330 train_time:21298ms step_avg:40.57ms
step:526/2330 train_time:21343ms step_avg:40.58ms
step:527/2330 train_time:21379ms step_avg:40.57ms
step:528/2330 train_time:21424ms step_avg:40.58ms
step:529/2330 train_time:21460ms step_avg:40.57ms
step:530/2330 train_time:21505ms step_avg:40.58ms
step:531/2330 train_time:21540ms step_avg:40.56ms
step:532/2330 train_time:21585ms step_avg:40.57ms
step:533/2330 train_time:21621ms step_avg:40.56ms
step:534/2330 train_time:21667ms step_avg:40.57ms
step:535/2330 train_time:21703ms step_avg:40.57ms
step:536/2330 train_time:21749ms step_avg:40.58ms
step:537/2330 train_time:21785ms step_avg:40.57ms
step:538/2330 train_time:21831ms step_avg:40.58ms
step:539/2330 train_time:21867ms step_avg:40.57ms
step:540/2330 train_time:21913ms step_avg:40.58ms
step:541/2330 train_time:21949ms step_avg:40.57ms
step:542/2330 train_time:21994ms step_avg:40.58ms
step:543/2330 train_time:22029ms step_avg:40.57ms
step:544/2330 train_time:22074ms step_avg:40.58ms
step:545/2330 train_time:22110ms step_avg:40.57ms
step:546/2330 train_time:22155ms step_avg:40.58ms
step:547/2330 train_time:22190ms step_avg:40.57ms
step:548/2330 train_time:22235ms step_avg:40.58ms
step:549/2330 train_time:22271ms step_avg:40.57ms
step:550/2330 train_time:22316ms step_avg:40.58ms
step:551/2330 train_time:22352ms step_avg:40.57ms
step:552/2330 train_time:22396ms step_avg:40.57ms
step:553/2330 train_time:22432ms step_avg:40.56ms
step:554/2330 train_time:22477ms step_avg:40.57ms
step:555/2330 train_time:22512ms step_avg:40.56ms
step:556/2330 train_time:22557ms step_avg:40.57ms
step:557/2330 train_time:22593ms step_avg:40.56ms
step:558/2330 train_time:22641ms step_avg:40.57ms
step:559/2330 train_time:22678ms step_avg:40.57ms
step:560/2330 train_time:22723ms step_avg:40.58ms
step:561/2330 train_time:22761ms step_avg:40.57ms
step:562/2330 train_time:22807ms step_avg:40.58ms
step:563/2330 train_time:22842ms step_avg:40.57ms
step:564/2330 train_time:22888ms step_avg:40.58ms
step:565/2330 train_time:22923ms step_avg:40.57ms
step:566/2330 train_time:22969ms step_avg:40.58ms
step:567/2330 train_time:23004ms step_avg:40.57ms
step:568/2330 train_time:23050ms step_avg:40.58ms
step:569/2330 train_time:23086ms step_avg:40.57ms
step:570/2330 train_time:23131ms step_avg:40.58ms
step:571/2330 train_time:23166ms step_avg:40.57ms
step:572/2330 train_time:23213ms step_avg:40.58ms
step:573/2330 train_time:23249ms step_avg:40.57ms
step:574/2330 train_time:23294ms step_avg:40.58ms
step:575/2330 train_time:23330ms step_avg:40.57ms
step:576/2330 train_time:23375ms step_avg:40.58ms
step:577/2330 train_time:23411ms step_avg:40.57ms
step:578/2330 train_time:23456ms step_avg:40.58ms
step:579/2330 train_time:23492ms step_avg:40.57ms
step:580/2330 train_time:23536ms step_avg:40.58ms
step:581/2330 train_time:23573ms step_avg:40.57ms
step:582/2330 train_time:23618ms step_avg:40.58ms
step:583/2330 train_time:23654ms step_avg:40.57ms
step:584/2330 train_time:23701ms step_avg:40.58ms
step:585/2330 train_time:23738ms step_avg:40.58ms
step:586/2330 train_time:23784ms step_avg:40.59ms
step:587/2330 train_time:23820ms step_avg:40.58ms
step:588/2330 train_time:23867ms step_avg:40.59ms
step:589/2330 train_time:23902ms step_avg:40.58ms
step:590/2330 train_time:23948ms step_avg:40.59ms
step:591/2330 train_time:23984ms step_avg:40.58ms
step:592/2330 train_time:24030ms step_avg:40.59ms
step:593/2330 train_time:24065ms step_avg:40.58ms
step:594/2330 train_time:24110ms step_avg:40.59ms
step:595/2330 train_time:24145ms step_avg:40.58ms
step:596/2330 train_time:24191ms step_avg:40.59ms
step:597/2330 train_time:24227ms step_avg:40.58ms
step:598/2330 train_time:24272ms step_avg:40.59ms
step:599/2330 train_time:24308ms step_avg:40.58ms
step:600/2330 train_time:24354ms step_avg:40.59ms
step:601/2330 train_time:24389ms step_avg:40.58ms
step:602/2330 train_time:24434ms step_avg:40.59ms
step:603/2330 train_time:24470ms step_avg:40.58ms
step:604/2330 train_time:24514ms step_avg:40.59ms
step:605/2330 train_time:24551ms step_avg:40.58ms
step:606/2330 train_time:24596ms step_avg:40.59ms
step:607/2330 train_time:24633ms step_avg:40.58ms
step:608/2330 train_time:24678ms step_avg:40.59ms
step:609/2330 train_time:24715ms step_avg:40.58ms
step:610/2330 train_time:24762ms step_avg:40.59ms
step:611/2330 train_time:24798ms step_avg:40.59ms
step:612/2330 train_time:24843ms step_avg:40.59ms
step:613/2330 train_time:24880ms step_avg:40.59ms
step:614/2330 train_time:24925ms step_avg:40.59ms
step:615/2330 train_time:24961ms step_avg:40.59ms
step:616/2330 train_time:25006ms step_avg:40.59ms
step:617/2330 train_time:25042ms step_avg:40.59ms
step:618/2330 train_time:25088ms step_avg:40.59ms
step:619/2330 train_time:25124ms step_avg:40.59ms
step:620/2330 train_time:25169ms step_avg:40.60ms
step:621/2330 train_time:25205ms step_avg:40.59ms
step:622/2330 train_time:25250ms step_avg:40.60ms
step:623/2330 train_time:25286ms step_avg:40.59ms
step:624/2330 train_time:25332ms step_avg:40.60ms
step:625/2330 train_time:25367ms step_avg:40.59ms
step:626/2330 train_time:25413ms step_avg:40.60ms
step:627/2330 train_time:25449ms step_avg:40.59ms
step:628/2330 train_time:25494ms step_avg:40.60ms
step:629/2330 train_time:25530ms step_avg:40.59ms
step:630/2330 train_time:25575ms step_avg:40.60ms
step:631/2330 train_time:25611ms step_avg:40.59ms
step:632/2330 train_time:25656ms step_avg:40.60ms
step:633/2330 train_time:25693ms step_avg:40.59ms
step:634/2330 train_time:25740ms step_avg:40.60ms
step:635/2330 train_time:25776ms step_avg:40.59ms
step:636/2330 train_time:25822ms step_avg:40.60ms
step:637/2330 train_time:25858ms step_avg:40.59ms
step:638/2330 train_time:25903ms step_avg:40.60ms
step:639/2330 train_time:25938ms step_avg:40.59ms
step:640/2330 train_time:25983ms step_avg:40.60ms
step:641/2330 train_time:26020ms step_avg:40.59ms
step:642/2330 train_time:26066ms step_avg:40.60ms
step:643/2330 train_time:26101ms step_avg:40.59ms
step:644/2330 train_time:26146ms step_avg:40.60ms
step:645/2330 train_time:26182ms step_avg:40.59ms
step:646/2330 train_time:26227ms step_avg:40.60ms
step:647/2330 train_time:26263ms step_avg:40.59ms
step:648/2330 train_time:26309ms step_avg:40.60ms
step:649/2330 train_time:26344ms step_avg:40.59ms
step:650/2330 train_time:26390ms step_avg:40.60ms
step:651/2330 train_time:26425ms step_avg:40.59ms
step:652/2330 train_time:26470ms step_avg:40.60ms
step:653/2330 train_time:26507ms step_avg:40.59ms
step:654/2330 train_time:26553ms step_avg:40.60ms
step:655/2330 train_time:26589ms step_avg:40.59ms
step:656/2330 train_time:26634ms step_avg:40.60ms
step:657/2330 train_time:26670ms step_avg:40.59ms
step:658/2330 train_time:26715ms step_avg:40.60ms
step:659/2330 train_time:26751ms step_avg:40.59ms
step:660/2330 train_time:26796ms step_avg:40.60ms
step:661/2330 train_time:26832ms step_avg:40.59ms
step:662/2330 train_time:26877ms step_avg:40.60ms
step:663/2330 train_time:26913ms step_avg:40.59ms
step:664/2330 train_time:26959ms step_avg:40.60ms
step:665/2330 train_time:26996ms step_avg:40.60ms
step:666/2330 train_time:27042ms step_avg:40.60ms
step:667/2330 train_time:27078ms step_avg:40.60ms
step:668/2330 train_time:27123ms step_avg:40.60ms
step:669/2330 train_time:27160ms step_avg:40.60ms
step:670/2330 train_time:27205ms step_avg:40.61ms
step:671/2330 train_time:27242ms step_avg:40.60ms
step:672/2330 train_time:27288ms step_avg:40.61ms
step:673/2330 train_time:27324ms step_avg:40.60ms
step:674/2330 train_time:27370ms step_avg:40.61ms
step:675/2330 train_time:27405ms step_avg:40.60ms
step:676/2330 train_time:27450ms step_avg:40.61ms
step:677/2330 train_time:27486ms step_avg:40.60ms
step:678/2330 train_time:27532ms step_avg:40.61ms
step:679/2330 train_time:27567ms step_avg:40.60ms
step:680/2330 train_time:27613ms step_avg:40.61ms
step:681/2330 train_time:27649ms step_avg:40.60ms
step:682/2330 train_time:27695ms step_avg:40.61ms
step:683/2330 train_time:27730ms step_avg:40.60ms
step:684/2330 train_time:27776ms step_avg:40.61ms
step:685/2330 train_time:27812ms step_avg:40.60ms
step:686/2330 train_time:27857ms step_avg:40.61ms
step:687/2330 train_time:27893ms step_avg:40.60ms
step:688/2330 train_time:27938ms step_avg:40.61ms
step:689/2330 train_time:27974ms step_avg:40.60ms
step:690/2330 train_time:28022ms step_avg:40.61ms
step:691/2330 train_time:28058ms step_avg:40.60ms
step:692/2330 train_time:28103ms step_avg:40.61ms
step:693/2330 train_time:28138ms step_avg:40.60ms
step:694/2330 train_time:28183ms step_avg:40.61ms
step:695/2330 train_time:28219ms step_avg:40.60ms
step:696/2330 train_time:28265ms step_avg:40.61ms
step:697/2330 train_time:28301ms step_avg:40.60ms
step:698/2330 train_time:28346ms step_avg:40.61ms
step:699/2330 train_time:28384ms step_avg:40.61ms
step:700/2330 train_time:28430ms step_avg:40.61ms
step:701/2330 train_time:28466ms step_avg:40.61ms
step:702/2330 train_time:28511ms step_avg:40.61ms
step:703/2330 train_time:28547ms step_avg:40.61ms
step:704/2330 train_time:28592ms step_avg:40.61ms
step:705/2330 train_time:28628ms step_avg:40.61ms
step:706/2330 train_time:28674ms step_avg:40.61ms
step:707/2330 train_time:28709ms step_avg:40.61ms
step:708/2330 train_time:28754ms step_avg:40.61ms
step:709/2330 train_time:28790ms step_avg:40.61ms
step:710/2330 train_time:28835ms step_avg:40.61ms
step:711/2330 train_time:28871ms step_avg:40.61ms
step:712/2330 train_time:28916ms step_avg:40.61ms
step:713/2330 train_time:28953ms step_avg:40.61ms
step:714/2330 train_time:28998ms step_avg:40.61ms
step:715/2330 train_time:29035ms step_avg:40.61ms
step:716/2330 train_time:29081ms step_avg:40.62ms
step:717/2330 train_time:29118ms step_avg:40.61ms
step:718/2330 train_time:29164ms step_avg:40.62ms
step:719/2330 train_time:29198ms step_avg:40.61ms
step:720/2330 train_time:29243ms step_avg:40.62ms
step:721/2330 train_time:29280ms step_avg:40.61ms
step:722/2330 train_time:29326ms step_avg:40.62ms
step:723/2330 train_time:29362ms step_avg:40.61ms
step:724/2330 train_time:29409ms step_avg:40.62ms
step:725/2330 train_time:29445ms step_avg:40.61ms
step:726/2330 train_time:29491ms step_avg:40.62ms
step:727/2330 train_time:29526ms step_avg:40.61ms
step:728/2330 train_time:29571ms step_avg:40.62ms
step:729/2330 train_time:29607ms step_avg:40.61ms
step:730/2330 train_time:29653ms step_avg:40.62ms
step:731/2330 train_time:29689ms step_avg:40.61ms
step:732/2330 train_time:29734ms step_avg:40.62ms
step:733/2330 train_time:29770ms step_avg:40.61ms
step:734/2330 train_time:29815ms step_avg:40.62ms
step:735/2330 train_time:29851ms step_avg:40.61ms
step:736/2330 train_time:29896ms step_avg:40.62ms
step:737/2330 train_time:29932ms step_avg:40.61ms
step:738/2330 train_time:29977ms step_avg:40.62ms
step:739/2330 train_time:30014ms step_avg:40.61ms
step:740/2330 train_time:30060ms step_avg:40.62ms
step:741/2330 train_time:30097ms step_avg:40.62ms
step:742/2330 train_time:30142ms step_avg:40.62ms
step:743/2330 train_time:30178ms step_avg:40.62ms
step:744/2330 train_time:30223ms step_avg:40.62ms
step:745/2330 train_time:30259ms step_avg:40.62ms
step:746/2330 train_time:30304ms step_avg:40.62ms
step:747/2330 train_time:30341ms step_avg:40.62ms
step:748/2330 train_time:30386ms step_avg:40.62ms
step:749/2330 train_time:30422ms step_avg:40.62ms
step:750/2330 train_time:30468ms step_avg:40.62ms
step:750/2330 val_loss:5.3163 train_time:30557ms step_avg:40.74ms
step:751/2330 train_time:30571ms step_avg:40.71ms
step:752/2330 train_time:30584ms step_avg:40.67ms
step:753/2330 train_time:30595ms step_avg:40.63ms
step:754/2330 train_time:30630ms step_avg:40.62ms
step:755/2330 train_time:30665ms step_avg:40.62ms
step:756/2330 train_time:30710ms step_avg:40.62ms
step:757/2330 train_time:30745ms step_avg:40.61ms
step:758/2330 train_time:30789ms step_avg:40.62ms
step:759/2330 train_time:30825ms step_avg:40.61ms
step:760/2330 train_time:30871ms step_avg:40.62ms
step:761/2330 train_time:30911ms step_avg:40.62ms
step:762/2330 train_time:30962ms step_avg:40.63ms
step:763/2330 train_time:31001ms step_avg:40.63ms
step:764/2330 train_time:31046ms step_avg:40.64ms
step:765/2330 train_time:31083ms step_avg:40.63ms
step:766/2330 train_time:31127ms step_avg:40.64ms
step:767/2330 train_time:31163ms step_avg:40.63ms
step:768/2330 train_time:31208ms step_avg:40.64ms
step:769/2330 train_time:31244ms step_avg:40.63ms
step:770/2330 train_time:31289ms step_avg:40.63ms
step:771/2330 train_time:31325ms step_avg:40.63ms
step:772/2330 train_time:31370ms step_avg:40.63ms
step:773/2330 train_time:31406ms step_avg:40.63ms
step:774/2330 train_time:31451ms step_avg:40.63ms
step:775/2330 train_time:31486ms step_avg:40.63ms
step:776/2330 train_time:31532ms step_avg:40.63ms
step:777/2330 train_time:31569ms step_avg:40.63ms
step:778/2330 train_time:31615ms step_avg:40.64ms
step:779/2330 train_time:31650ms step_avg:40.63ms
step:780/2330 train_time:31695ms step_avg:40.63ms
step:781/2330 train_time:31730ms step_avg:40.63ms
step:782/2330 train_time:31776ms step_avg:40.63ms
step:783/2330 train_time:31812ms step_avg:40.63ms
step:784/2330 train_time:31858ms step_avg:40.63ms
step:785/2330 train_time:31895ms step_avg:40.63ms
step:786/2330 train_time:31942ms step_avg:40.64ms
step:787/2330 train_time:31979ms step_avg:40.63ms
step:788/2330 train_time:32024ms step_avg:40.64ms
step:789/2330 train_time:32061ms step_avg:40.63ms
step:790/2330 train_time:32106ms step_avg:40.64ms
step:791/2330 train_time:32142ms step_avg:40.63ms
step:792/2330 train_time:32188ms step_avg:40.64ms
step:793/2330 train_time:32223ms step_avg:40.63ms
step:794/2330 train_time:32268ms step_avg:40.64ms
step:795/2330 train_time:32305ms step_avg:40.63ms
step:796/2330 train_time:32350ms step_avg:40.64ms
step:797/2330 train_time:32385ms step_avg:40.63ms
step:798/2330 train_time:32431ms step_avg:40.64ms
step:799/2330 train_time:32466ms step_avg:40.63ms
step:800/2330 train_time:32512ms step_avg:40.64ms
step:801/2330 train_time:32547ms step_avg:40.63ms
step:802/2330 train_time:32592ms step_avg:40.64ms
step:803/2330 train_time:32628ms step_avg:40.63ms
step:804/2330 train_time:32673ms step_avg:40.64ms
step:805/2330 train_time:32709ms step_avg:40.63ms
step:806/2330 train_time:32755ms step_avg:40.64ms
step:807/2330 train_time:32792ms step_avg:40.63ms
step:808/2330 train_time:32837ms step_avg:40.64ms
step:809/2330 train_time:32875ms step_avg:40.64ms
step:810/2330 train_time:32922ms step_avg:40.64ms
step:811/2330 train_time:32958ms step_avg:40.64ms
step:812/2330 train_time:33003ms step_avg:40.64ms
step:813/2330 train_time:33039ms step_avg:40.64ms
step:814/2330 train_time:33085ms step_avg:40.65ms
step:815/2330 train_time:33121ms step_avg:40.64ms
step:816/2330 train_time:33167ms step_avg:40.65ms
step:817/2330 train_time:33203ms step_avg:40.64ms
step:818/2330 train_time:33248ms step_avg:40.65ms
step:819/2330 train_time:33284ms step_avg:40.64ms
step:820/2330 train_time:33329ms step_avg:40.65ms
step:821/2330 train_time:33365ms step_avg:40.64ms
step:822/2330 train_time:33409ms step_avg:40.64ms
step:823/2330 train_time:33445ms step_avg:40.64ms
step:824/2330 train_time:33490ms step_avg:40.64ms
step:825/2330 train_time:33526ms step_avg:40.64ms
step:826/2330 train_time:33571ms step_avg:40.64ms
step:827/2330 train_time:33606ms step_avg:40.64ms
step:828/2330 train_time:33651ms step_avg:40.64ms
step:829/2330 train_time:33686ms step_avg:40.63ms
step:830/2330 train_time:33732ms step_avg:40.64ms
step:831/2330 train_time:33768ms step_avg:40.64ms
step:832/2330 train_time:33816ms step_avg:40.64ms
step:833/2330 train_time:33853ms step_avg:40.64ms
step:834/2330 train_time:33899ms step_avg:40.65ms
step:835/2330 train_time:33935ms step_avg:40.64ms
step:836/2330 train_time:33980ms step_avg:40.65ms
step:837/2330 train_time:34018ms step_avg:40.64ms
step:838/2330 train_time:34064ms step_avg:40.65ms
step:839/2330 train_time:34099ms step_avg:40.64ms
step:840/2330 train_time:34145ms step_avg:40.65ms
step:841/2330 train_time:34181ms step_avg:40.64ms
step:842/2330 train_time:34225ms step_avg:40.65ms
step:843/2330 train_time:34261ms step_avg:40.64ms
step:844/2330 train_time:34307ms step_avg:40.65ms
step:845/2330 train_time:34342ms step_avg:40.64ms
step:846/2330 train_time:34388ms step_avg:40.65ms
step:847/2330 train_time:34424ms step_avg:40.64ms
step:848/2330 train_time:34469ms step_avg:40.65ms
step:849/2330 train_time:34504ms step_avg:40.64ms
step:850/2330 train_time:34549ms step_avg:40.65ms
step:851/2330 train_time:34584ms step_avg:40.64ms
step:852/2330 train_time:34629ms step_avg:40.64ms
step:853/2330 train_time:34665ms step_avg:40.64ms
step:854/2330 train_time:34710ms step_avg:40.64ms
step:855/2330 train_time:34746ms step_avg:40.64ms
step:856/2330 train_time:34793ms step_avg:40.65ms
step:857/2330 train_time:34831ms step_avg:40.64ms
step:858/2330 train_time:34877ms step_avg:40.65ms
step:859/2330 train_time:34914ms step_avg:40.65ms
step:860/2330 train_time:34959ms step_avg:40.65ms
step:861/2330 train_time:34996ms step_avg:40.65ms
step:862/2330 train_time:35041ms step_avg:40.65ms
step:863/2330 train_time:35077ms step_avg:40.65ms
step:864/2330 train_time:35122ms step_avg:40.65ms
step:865/2330 train_time:35158ms step_avg:40.64ms
step:866/2330 train_time:35203ms step_avg:40.65ms
step:867/2330 train_time:35239ms step_avg:40.65ms
step:868/2330 train_time:35285ms step_avg:40.65ms
step:869/2330 train_time:35321ms step_avg:40.65ms
step:870/2330 train_time:35366ms step_avg:40.65ms
step:871/2330 train_time:35401ms step_avg:40.64ms
step:872/2330 train_time:35447ms step_avg:40.65ms
step:873/2330 train_time:35483ms step_avg:40.64ms
step:874/2330 train_time:35529ms step_avg:40.65ms
step:875/2330 train_time:35564ms step_avg:40.64ms
step:876/2330 train_time:35609ms step_avg:40.65ms
step:877/2330 train_time:35644ms step_avg:40.64ms
step:878/2330 train_time:35689ms step_avg:40.65ms
step:879/2330 train_time:35725ms step_avg:40.64ms
step:880/2330 train_time:35771ms step_avg:40.65ms
step:881/2330 train_time:35808ms step_avg:40.64ms
step:882/2330 train_time:35855ms step_avg:40.65ms
step:883/2330 train_time:35891ms step_avg:40.65ms
step:884/2330 train_time:35937ms step_avg:40.65ms
step:885/2330 train_time:35973ms step_avg:40.65ms
step:886/2330 train_time:36018ms step_avg:40.65ms
step:887/2330 train_time:36054ms step_avg:40.65ms
step:888/2330 train_time:36100ms step_avg:40.65ms
step:889/2330 train_time:36135ms step_avg:40.65ms
step:890/2330 train_time:36181ms step_avg:40.65ms
step:891/2330 train_time:36217ms step_avg:40.65ms
step:892/2330 train_time:36263ms step_avg:40.65ms
step:893/2330 train_time:36299ms step_avg:40.65ms
step:894/2330 train_time:36345ms step_avg:40.65ms
step:895/2330 train_time:36380ms step_avg:40.65ms
step:896/2330 train_time:36426ms step_avg:40.65ms
step:897/2330 train_time:36462ms step_avg:40.65ms
step:898/2330 train_time:36508ms step_avg:40.65ms
step:899/2330 train_time:36543ms step_avg:40.65ms
step:900/2330 train_time:36589ms step_avg:40.65ms
step:901/2330 train_time:36625ms step_avg:40.65ms
step:902/2330 train_time:36670ms step_avg:40.65ms
step:903/2330 train_time:36705ms step_avg:40.65ms
step:904/2330 train_time:36750ms step_avg:40.65ms
step:905/2330 train_time:36787ms step_avg:40.65ms
step:906/2330 train_time:36833ms step_avg:40.65ms
step:907/2330 train_time:36869ms step_avg:40.65ms
step:908/2330 train_time:36915ms step_avg:40.66ms
step:909/2330 train_time:36953ms step_avg:40.65ms
step:910/2330 train_time:36998ms step_avg:40.66ms
step:911/2330 train_time:37034ms step_avg:40.65ms
step:912/2330 train_time:37079ms step_avg:40.66ms
step:913/2330 train_time:37114ms step_avg:40.65ms
step:914/2330 train_time:37160ms step_avg:40.66ms
step:915/2330 train_time:37197ms step_avg:40.65ms
step:916/2330 train_time:37243ms step_avg:40.66ms
step:917/2330 train_time:37279ms step_avg:40.65ms
step:918/2330 train_time:37324ms step_avg:40.66ms
step:919/2330 train_time:37360ms step_avg:40.65ms
step:920/2330 train_time:37405ms step_avg:40.66ms
step:921/2330 train_time:37441ms step_avg:40.65ms
step:922/2330 train_time:37486ms step_avg:40.66ms
step:923/2330 train_time:37522ms step_avg:40.65ms
step:924/2330 train_time:37568ms step_avg:40.66ms
step:925/2330 train_time:37603ms step_avg:40.65ms
step:926/2330 train_time:37649ms step_avg:40.66ms
step:927/2330 train_time:37684ms step_avg:40.65ms
step:928/2330 train_time:37729ms step_avg:40.66ms
step:929/2330 train_time:37765ms step_avg:40.65ms
step:930/2330 train_time:37811ms step_avg:40.66ms
step:931/2330 train_time:37848ms step_avg:40.65ms
step:932/2330 train_time:37894ms step_avg:40.66ms
step:933/2330 train_time:37931ms step_avg:40.65ms
step:934/2330 train_time:37977ms step_avg:40.66ms
step:935/2330 train_time:38014ms step_avg:40.66ms
step:936/2330 train_time:38059ms step_avg:40.66ms
step:937/2330 train_time:38096ms step_avg:40.66ms
step:938/2330 train_time:38141ms step_avg:40.66ms
step:939/2330 train_time:38177ms step_avg:40.66ms
step:940/2330 train_time:38222ms step_avg:40.66ms
step:941/2330 train_time:38258ms step_avg:40.66ms
step:942/2330 train_time:38305ms step_avg:40.66ms
step:943/2330 train_time:38340ms step_avg:40.66ms
step:944/2330 train_time:38384ms step_avg:40.66ms
step:945/2330 train_time:38420ms step_avg:40.66ms
step:946/2330 train_time:38465ms step_avg:40.66ms
step:947/2330 train_time:38501ms step_avg:40.66ms
step:948/2330 train_time:38546ms step_avg:40.66ms
step:949/2330 train_time:38583ms step_avg:40.66ms
step:950/2330 train_time:38629ms step_avg:40.66ms
step:951/2330 train_time:38664ms step_avg:40.66ms
step:952/2330 train_time:38709ms step_avg:40.66ms
step:953/2330 train_time:38745ms step_avg:40.66ms
step:954/2330 train_time:38791ms step_avg:40.66ms
step:955/2330 train_time:38827ms step_avg:40.66ms
step:956/2330 train_time:38872ms step_avg:40.66ms
step:957/2330 train_time:38910ms step_avg:40.66ms
step:958/2330 train_time:38955ms step_avg:40.66ms
step:959/2330 train_time:38992ms step_avg:40.66ms
step:960/2330 train_time:39038ms step_avg:40.66ms
step:961/2330 train_time:39074ms step_avg:40.66ms
step:962/2330 train_time:39119ms step_avg:40.66ms
step:963/2330 train_time:39155ms step_avg:40.66ms
step:964/2330 train_time:39200ms step_avg:40.66ms
step:965/2330 train_time:39235ms step_avg:40.66ms
step:966/2330 train_time:39281ms step_avg:40.66ms
step:967/2330 train_time:39317ms step_avg:40.66ms
step:968/2330 train_time:39362ms step_avg:40.66ms
step:969/2330 train_time:39399ms step_avg:40.66ms
step:970/2330 train_time:39444ms step_avg:40.66ms
step:971/2330 train_time:39480ms step_avg:40.66ms
step:972/2330 train_time:39526ms step_avg:40.66ms
step:973/2330 train_time:39562ms step_avg:40.66ms
step:974/2330 train_time:39607ms step_avg:40.66ms
step:975/2330 train_time:39642ms step_avg:40.66ms
step:976/2330 train_time:39689ms step_avg:40.66ms
step:977/2330 train_time:39724ms step_avg:40.66ms
step:978/2330 train_time:39769ms step_avg:40.66ms
step:979/2330 train_time:39805ms step_avg:40.66ms
step:980/2330 train_time:39850ms step_avg:40.66ms
step:981/2330 train_time:39887ms step_avg:40.66ms
step:982/2330 train_time:39933ms step_avg:40.66ms
step:983/2330 train_time:39969ms step_avg:40.66ms
step:984/2330 train_time:40015ms step_avg:40.67ms
step:985/2330 train_time:40052ms step_avg:40.66ms
step:986/2330 train_time:40096ms step_avg:40.67ms
step:987/2330 train_time:40132ms step_avg:40.66ms
step:988/2330 train_time:40178ms step_avg:40.67ms
step:989/2330 train_time:40214ms step_avg:40.66ms
step:990/2330 train_time:40260ms step_avg:40.67ms
step:991/2330 train_time:40296ms step_avg:40.66ms
step:992/2330 train_time:40341ms step_avg:40.67ms
step:993/2330 train_time:40377ms step_avg:40.66ms
step:994/2330 train_time:40424ms step_avg:40.67ms
step:995/2330 train_time:40459ms step_avg:40.66ms
step:996/2330 train_time:40504ms step_avg:40.67ms
step:997/2330 train_time:40539ms step_avg:40.66ms
step:998/2330 train_time:40586ms step_avg:40.67ms
step:999/2330 train_time:40622ms step_avg:40.66ms
step:1000/2330 train_time:40668ms step_avg:40.67ms
step:1000/2330 val_loss:5.2847 train_time:40757ms step_avg:40.76ms
step:1001/2330 train_time:40772ms step_avg:40.73ms
step:1002/2330 train_time:40785ms step_avg:40.70ms
step:1003/2330 train_time:40795ms step_avg:40.67ms
step:1004/2330 train_time:40831ms step_avg:40.67ms
step:1005/2330 train_time:40866ms step_avg:40.66ms
step:1006/2330 train_time:40910ms step_avg:40.67ms
step:1007/2330 train_time:40946ms step_avg:40.66ms
step:1008/2330 train_time:40991ms step_avg:40.67ms
step:1009/2330 train_time:41026ms step_avg:40.66ms
step:1010/2330 train_time:41072ms step_avg:40.66ms
step:1011/2330 train_time:41113ms step_avg:40.67ms
step:1012/2330 train_time:41160ms step_avg:40.67ms
step:1013/2330 train_time:41197ms step_avg:40.67ms
step:1014/2330 train_time:41243ms step_avg:40.67ms
step:1015/2330 train_time:41279ms step_avg:40.67ms
step:1016/2330 train_time:41325ms step_avg:40.67ms
step:1017/2330 train_time:41361ms step_avg:40.67ms
step:1018/2330 train_time:41406ms step_avg:40.67ms
step:1019/2330 train_time:41442ms step_avg:40.67ms
step:1020/2330 train_time:41487ms step_avg:40.67ms
step:1021/2330 train_time:41522ms step_avg:40.67ms
step:1022/2330 train_time:41566ms step_avg:40.67ms
step:1023/2330 train_time:41601ms step_avg:40.67ms
step:1024/2330 train_time:41647ms step_avg:40.67ms
step:1025/2330 train_time:41683ms step_avg:40.67ms
step:1026/2330 train_time:41727ms step_avg:40.67ms
step:1027/2330 train_time:41763ms step_avg:40.67ms
step:1028/2330 train_time:41808ms step_avg:40.67ms
step:1029/2330 train_time:41843ms step_avg:40.66ms
step:1030/2330 train_time:41887ms step_avg:40.67ms
step:1031/2330 train_time:41922ms step_avg:40.66ms
step:1032/2330 train_time:41968ms step_avg:40.67ms
step:1033/2330 train_time:42004ms step_avg:40.66ms
step:1034/2330 train_time:42052ms step_avg:40.67ms
step:1035/2330 train_time:42088ms step_avg:40.66ms
step:1036/2330 train_time:42134ms step_avg:40.67ms
step:1037/2330 train_time:42171ms step_avg:40.67ms
step:1038/2330 train_time:42216ms step_avg:40.67ms
step:1039/2330 train_time:42252ms step_avg:40.67ms
step:1040/2330 train_time:42298ms step_avg:40.67ms
step:1041/2330 train_time:42335ms step_avg:40.67ms
step:1042/2330 train_time:42380ms step_avg:40.67ms
step:1043/2330 train_time:42417ms step_avg:40.67ms
step:1044/2330 train_time:42462ms step_avg:40.67ms
step:1045/2330 train_time:42497ms step_avg:40.67ms
step:1046/2330 train_time:42543ms step_avg:40.67ms
step:1047/2330 train_time:42578ms step_avg:40.67ms
step:1048/2330 train_time:42624ms step_avg:40.67ms
step:1049/2330 train_time:42660ms step_avg:40.67ms
step:1050/2330 train_time:42706ms step_avg:40.67ms
step:1051/2330 train_time:42741ms step_avg:40.67ms
step:1052/2330 train_time:42785ms step_avg:40.67ms
step:1053/2330 train_time:42821ms step_avg:40.67ms
step:1054/2330 train_time:42866ms step_avg:40.67ms
step:1055/2330 train_time:42902ms step_avg:40.67ms
step:1056/2330 train_time:42947ms step_avg:40.67ms
step:1057/2330 train_time:42982ms step_avg:40.66ms
step:1058/2330 train_time:43029ms step_avg:40.67ms
step:1059/2330 train_time:43066ms step_avg:40.67ms
step:1060/2330 train_time:43112ms step_avg:40.67ms
step:1061/2330 train_time:43148ms step_avg:40.67ms
step:1062/2330 train_time:43194ms step_avg:40.67ms
step:1063/2330 train_time:43230ms step_avg:40.67ms
step:1064/2330 train_time:43276ms step_avg:40.67ms
step:1065/2330 train_time:43312ms step_avg:40.67ms
step:1066/2330 train_time:43358ms step_avg:40.67ms
step:1067/2330 train_time:43394ms step_avg:40.67ms
step:1068/2330 train_time:43439ms step_avg:40.67ms
step:1069/2330 train_time:43475ms step_avg:40.67ms
step:1070/2330 train_time:43520ms step_avg:40.67ms
step:1071/2330 train_time:43556ms step_avg:40.67ms
step:1072/2330 train_time:43602ms step_avg:40.67ms
step:1073/2330 train_time:43637ms step_avg:40.67ms
step:1074/2330 train_time:43682ms step_avg:40.67ms
step:1075/2330 train_time:43718ms step_avg:40.67ms
step:1076/2330 train_time:43763ms step_avg:40.67ms
step:1077/2330 train_time:43799ms step_avg:40.67ms
step:1078/2330 train_time:43844ms step_avg:40.67ms
step:1079/2330 train_time:43880ms step_avg:40.67ms
step:1080/2330 train_time:43926ms step_avg:40.67ms
step:1081/2330 train_time:43962ms step_avg:40.67ms
step:1082/2330 train_time:44008ms step_avg:40.67ms
step:1083/2330 train_time:44045ms step_avg:40.67ms
step:1084/2330 train_time:44090ms step_avg:40.67ms
step:1085/2330 train_time:44127ms step_avg:40.67ms
step:1086/2330 train_time:44173ms step_avg:40.67ms
step:1087/2330 train_time:44208ms step_avg:40.67ms
step:1088/2330 train_time:44254ms step_avg:40.67ms
step:1089/2330 train_time:44290ms step_avg:40.67ms
step:1090/2330 train_time:44336ms step_avg:40.68ms
step:1091/2330 train_time:44372ms step_avg:40.67ms
step:1092/2330 train_time:44418ms step_avg:40.68ms
step:1093/2330 train_time:44453ms step_avg:40.67ms
step:1094/2330 train_time:44498ms step_avg:40.67ms
step:1095/2330 train_time:44533ms step_avg:40.67ms
step:1096/2330 train_time:44579ms step_avg:40.67ms
step:1097/2330 train_time:44614ms step_avg:40.67ms
step:1098/2330 train_time:44659ms step_avg:40.67ms
step:1099/2330 train_time:44696ms step_avg:40.67ms
step:1100/2330 train_time:44740ms step_avg:40.67ms
step:1101/2330 train_time:44776ms step_avg:40.67ms
step:1102/2330 train_time:44820ms step_avg:40.67ms
step:1103/2330 train_time:44857ms step_avg:40.67ms
step:1104/2330 train_time:44903ms step_avg:40.67ms
step:1105/2330 train_time:44938ms step_avg:40.67ms
step:1106/2330 train_time:44984ms step_avg:40.67ms
step:1107/2330 train_time:45021ms step_avg:40.67ms
step:1108/2330 train_time:45067ms step_avg:40.67ms
step:1109/2330 train_time:45103ms step_avg:40.67ms
step:1110/2330 train_time:45149ms step_avg:40.67ms
step:1111/2330 train_time:45185ms step_avg:40.67ms
step:1112/2330 train_time:45231ms step_avg:40.68ms
step:1113/2330 train_time:45267ms step_avg:40.67ms
step:1114/2330 train_time:45313ms step_avg:40.68ms
step:1115/2330 train_time:45347ms step_avg:40.67ms
step:1116/2330 train_time:45394ms step_avg:40.68ms
step:1117/2330 train_time:45430ms step_avg:40.67ms
step:1118/2330 train_time:45477ms step_avg:40.68ms
step:1119/2330 train_time:45511ms step_avg:40.67ms
step:1120/2330 train_time:45557ms step_avg:40.68ms
step:1121/2330 train_time:45593ms step_avg:40.67ms
step:1122/2330 train_time:45639ms step_avg:40.68ms
step:1123/2330 train_time:45675ms step_avg:40.67ms
step:1124/2330 train_time:45720ms step_avg:40.68ms
step:1125/2330 train_time:45756ms step_avg:40.67ms
step:1126/2330 train_time:45801ms step_avg:40.68ms
step:1127/2330 train_time:45837ms step_avg:40.67ms
step:1128/2330 train_time:45883ms step_avg:40.68ms
step:1129/2330 train_time:45919ms step_avg:40.67ms
step:1130/2330 train_time:45965ms step_avg:40.68ms
step:1131/2330 train_time:46001ms step_avg:40.67ms
step:1132/2330 train_time:46046ms step_avg:40.68ms
step:1133/2330 train_time:46082ms step_avg:40.67ms
step:1134/2330 train_time:46128ms step_avg:40.68ms
step:1135/2330 train_time:46166ms step_avg:40.68ms
step:1136/2330 train_time:46211ms step_avg:40.68ms
step:1137/2330 train_time:46247ms step_avg:40.67ms
step:1138/2330 train_time:46292ms step_avg:40.68ms
step:1139/2330 train_time:46328ms step_avg:40.67ms
step:1140/2330 train_time:46374ms step_avg:40.68ms
step:1141/2330 train_time:46410ms step_avg:40.67ms
step:1142/2330 train_time:46456ms step_avg:40.68ms
step:1143/2330 train_time:46491ms step_avg:40.67ms
step:1144/2330 train_time:46537ms step_avg:40.68ms
step:1145/2330 train_time:46573ms step_avg:40.68ms
step:1146/2330 train_time:46618ms step_avg:40.68ms
step:1147/2330 train_time:46654ms step_avg:40.67ms
step:1148/2330 train_time:46699ms step_avg:40.68ms
step:1149/2330 train_time:46734ms step_avg:40.67ms
step:1150/2330 train_time:46780ms step_avg:40.68ms
step:1151/2330 train_time:46816ms step_avg:40.67ms
step:1152/2330 train_time:46861ms step_avg:40.68ms
step:1153/2330 train_time:46897ms step_avg:40.67ms
step:1154/2330 train_time:46942ms step_avg:40.68ms
step:1155/2330 train_time:46978ms step_avg:40.67ms
step:1156/2330 train_time:47024ms step_avg:40.68ms
step:1157/2330 train_time:47062ms step_avg:40.68ms
step:1158/2330 train_time:47107ms step_avg:40.68ms
step:1159/2330 train_time:47143ms step_avg:40.68ms
step:1160/2330 train_time:47188ms step_avg:40.68ms
step:1161/2330 train_time:47224ms step_avg:40.68ms
step:1162/2330 train_time:47271ms step_avg:40.68ms
step:1163/2330 train_time:47307ms step_avg:40.68ms
step:1164/2330 train_time:47352ms step_avg:40.68ms
step:1165/2330 train_time:47388ms step_avg:40.68ms
step:1166/2330 train_time:47433ms step_avg:40.68ms
step:1167/2330 train_time:47469ms step_avg:40.68ms
step:1168/2330 train_time:47514ms step_avg:40.68ms
step:1169/2330 train_time:47550ms step_avg:40.68ms
step:1170/2330 train_time:47596ms step_avg:40.68ms
step:1171/2330 train_time:47632ms step_avg:40.68ms
step:1172/2330 train_time:47678ms step_avg:40.68ms
step:1173/2330 train_time:47713ms step_avg:40.68ms
step:1174/2330 train_time:47758ms step_avg:40.68ms
step:1175/2330 train_time:47795ms step_avg:40.68ms
step:1176/2330 train_time:47841ms step_avg:40.68ms
step:1177/2330 train_time:47876ms step_avg:40.68ms
step:1178/2330 train_time:47923ms step_avg:40.68ms
step:1179/2330 train_time:47959ms step_avg:40.68ms
step:1180/2330 train_time:48005ms step_avg:40.68ms
step:1181/2330 train_time:48041ms step_avg:40.68ms
step:1182/2330 train_time:48086ms step_avg:40.68ms
step:1183/2330 train_time:48122ms step_avg:40.68ms
step:1184/2330 train_time:48168ms step_avg:40.68ms
step:1185/2330 train_time:48204ms step_avg:40.68ms
step:1186/2330 train_time:48249ms step_avg:40.68ms
step:1187/2330 train_time:48284ms step_avg:40.68ms
step:1188/2330 train_time:48329ms step_avg:40.68ms
step:1189/2330 train_time:48365ms step_avg:40.68ms
step:1190/2330 train_time:48411ms step_avg:40.68ms
step:1191/2330 train_time:48446ms step_avg:40.68ms
step:1192/2330 train_time:48492ms step_avg:40.68ms
step:1193/2330 train_time:48529ms step_avg:40.68ms
step:1194/2330 train_time:48575ms step_avg:40.68ms
step:1195/2330 train_time:48610ms step_avg:40.68ms
step:1196/2330 train_time:48656ms step_avg:40.68ms
step:1197/2330 train_time:48692ms step_avg:40.68ms
step:1198/2330 train_time:48737ms step_avg:40.68ms
step:1199/2330 train_time:48774ms step_avg:40.68ms
step:1200/2330 train_time:48818ms step_avg:40.68ms
step:1201/2330 train_time:48855ms step_avg:40.68ms
step:1202/2330 train_time:48901ms step_avg:40.68ms
step:1203/2330 train_time:48937ms step_avg:40.68ms
step:1204/2330 train_time:48983ms step_avg:40.68ms
step:1205/2330 train_time:49019ms step_avg:40.68ms
step:1206/2330 train_time:49065ms step_avg:40.68ms
step:1207/2330 train_time:49101ms step_avg:40.68ms
step:1208/2330 train_time:49146ms step_avg:40.68ms
step:1209/2330 train_time:49182ms step_avg:40.68ms
step:1210/2330 train_time:49227ms step_avg:40.68ms
step:1211/2330 train_time:49263ms step_avg:40.68ms
step:1212/2330 train_time:49310ms step_avg:40.68ms
step:1213/2330 train_time:49345ms step_avg:40.68ms
step:1214/2330 train_time:49390ms step_avg:40.68ms
step:1215/2330 train_time:49426ms step_avg:40.68ms
step:1216/2330 train_time:49472ms step_avg:40.68ms
step:1217/2330 train_time:49508ms step_avg:40.68ms
step:1218/2330 train_time:49554ms step_avg:40.68ms
step:1219/2330 train_time:49589ms step_avg:40.68ms
step:1220/2330 train_time:49634ms step_avg:40.68ms
step:1221/2330 train_time:49670ms step_avg:40.68ms
step:1222/2330 train_time:49716ms step_avg:40.68ms
step:1223/2330 train_time:49753ms step_avg:40.68ms
step:1224/2330 train_time:49799ms step_avg:40.69ms
step:1225/2330 train_time:49834ms step_avg:40.68ms
step:1226/2330 train_time:49880ms step_avg:40.69ms
step:1227/2330 train_time:49915ms step_avg:40.68ms
step:1228/2330 train_time:49960ms step_avg:40.68ms
step:1229/2330 train_time:49996ms step_avg:40.68ms
step:1230/2330 train_time:50042ms step_avg:40.68ms
step:1231/2330 train_time:50078ms step_avg:40.68ms
step:1232/2330 train_time:50124ms step_avg:40.69ms
step:1233/2330 train_time:50161ms step_avg:40.68ms
step:1234/2330 train_time:50205ms step_avg:40.68ms
step:1235/2330 train_time:50242ms step_avg:40.68ms
step:1236/2330 train_time:50286ms step_avg:40.68ms
step:1237/2330 train_time:50323ms step_avg:40.68ms
step:1238/2330 train_time:50369ms step_avg:40.69ms
step:1239/2330 train_time:50405ms step_avg:40.68ms
step:1240/2330 train_time:50451ms step_avg:40.69ms
step:1241/2330 train_time:50487ms step_avg:40.68ms
step:1242/2330 train_time:50533ms step_avg:40.69ms
step:1243/2330 train_time:50569ms step_avg:40.68ms
step:1244/2330 train_time:50614ms step_avg:40.69ms
step:1245/2330 train_time:50649ms step_avg:40.68ms
step:1246/2330 train_time:50695ms step_avg:40.69ms
step:1247/2330 train_time:50731ms step_avg:40.68ms
step:1248/2330 train_time:50777ms step_avg:40.69ms
step:1249/2330 train_time:50813ms step_avg:40.68ms
step:1250/2330 train_time:50859ms step_avg:40.69ms
step:1250/2330 val_loss:5.2184 train_time:50948ms step_avg:40.76ms
step:1251/2330 train_time:50961ms step_avg:40.74ms
step:1252/2330 train_time:50974ms step_avg:40.71ms
step:1253/2330 train_time:50985ms step_avg:40.69ms
step:1254/2330 train_time:51021ms step_avg:40.69ms
step:1255/2330 train_time:51056ms step_avg:40.68ms
step:1256/2330 train_time:51100ms step_avg:40.69ms
step:1257/2330 train_time:51136ms step_avg:40.68ms
step:1258/2330 train_time:51181ms step_avg:40.68ms
step:1259/2330 train_time:51218ms step_avg:40.68ms
step:1260/2330 train_time:51267ms step_avg:40.69ms
step:1261/2330 train_time:51307ms step_avg:40.69ms
step:1262/2330 train_time:51355ms step_avg:40.69ms
step:1263/2330 train_time:51392ms step_avg:40.69ms
step:1264/2330 train_time:51438ms step_avg:40.69ms
step:1265/2330 train_time:51473ms step_avg:40.69ms
step:1266/2330 train_time:51518ms step_avg:40.69ms
step:1267/2330 train_time:51554ms step_avg:40.69ms
step:1268/2330 train_time:51599ms step_avg:40.69ms
step:1269/2330 train_time:51635ms step_avg:40.69ms
step:1270/2330 train_time:51680ms step_avg:40.69ms
step:1271/2330 train_time:51881ms step_avg:40.82ms
step:1272/2330 train_time:51925ms step_avg:40.82ms
step:1273/2330 train_time:51959ms step_avg:40.82ms
step:1274/2330 train_time:52004ms step_avg:40.82ms
step:1275/2330 train_time:52039ms step_avg:40.81ms
step:1276/2330 train_time:52083ms step_avg:40.82ms
step:1277/2330 train_time:52118ms step_avg:40.81ms
step:1278/2330 train_time:52163ms step_avg:40.82ms
step:1279/2330 train_time:52198ms step_avg:40.81ms
step:1280/2330 train_time:52242ms step_avg:40.81ms
step:1281/2330 train_time:52277ms step_avg:40.81ms
step:1282/2330 train_time:52322ms step_avg:40.81ms
step:1283/2330 train_time:52357ms step_avg:40.81ms
step:1284/2330 train_time:52402ms step_avg:40.81ms
step:1285/2330 train_time:52437ms step_avg:40.81ms
step:1286/2330 train_time:52482ms step_avg:40.81ms
step:1287/2330 train_time:52517ms step_avg:40.81ms
step:1288/2330 train_time:52562ms step_avg:40.81ms
step:1289/2330 train_time:52596ms step_avg:40.80ms
step:1290/2330 train_time:52641ms step_avg:40.81ms
step:1291/2330 train_time:52677ms step_avg:40.80ms
step:1292/2330 train_time:52725ms step_avg:40.81ms
step:1293/2330 train_time:52767ms step_avg:40.81ms
step:1294/2330 train_time:52818ms step_avg:40.82ms
step:1295/2330 train_time:52855ms step_avg:40.81ms
step:1296/2330 train_time:52902ms step_avg:40.82ms
step:1297/2330 train_time:52938ms step_avg:40.82ms
step:1298/2330 train_time:52984ms step_avg:40.82ms
step:1299/2330 train_time:53021ms step_avg:40.82ms
step:1300/2330 train_time:53066ms step_avg:40.82ms
step:1301/2330 train_time:53101ms step_avg:40.82ms
step:1302/2330 train_time:53146ms step_avg:40.82ms
step:1303/2330 train_time:53181ms step_avg:40.81ms
step:1304/2330 train_time:53226ms step_avg:40.82ms
step:1305/2330 train_time:53262ms step_avg:40.81ms
step:1306/2330 train_time:53307ms step_avg:40.82ms
step:1307/2330 train_time:53342ms step_avg:40.81ms
step:1308/2330 train_time:53387ms step_avg:40.82ms
step:1309/2330 train_time:53423ms step_avg:40.81ms
step:1310/2330 train_time:53467ms step_avg:40.81ms
step:1311/2330 train_time:53503ms step_avg:40.81ms
step:1312/2330 train_time:53548ms step_avg:40.81ms
step:1313/2330 train_time:53584ms step_avg:40.81ms
step:1314/2330 train_time:53629ms step_avg:40.81ms
step:1315/2330 train_time:53665ms step_avg:40.81ms
step:1316/2330 train_time:53711ms step_avg:40.81ms
step:1317/2330 train_time:53749ms step_avg:40.81ms
step:1318/2330 train_time:53797ms step_avg:40.82ms
step:1319/2330 train_time:53833ms step_avg:40.81ms
step:1320/2330 train_time:53880ms step_avg:40.82ms
step:1321/2330 train_time:53916ms step_avg:40.81ms
step:1322/2330 train_time:53963ms step_avg:40.82ms
step:1323/2330 train_time:53999ms step_avg:40.82ms
step:1324/2330 train_time:54044ms step_avg:40.82ms
step:1325/2330 train_time:54080ms step_avg:40.81ms
step:1326/2330 train_time:54126ms step_avg:40.82ms
step:1327/2330 train_time:54161ms step_avg:40.81ms
step:1328/2330 train_time:54206ms step_avg:40.82ms
step:1329/2330 train_time:54242ms step_avg:40.81ms
step:1330/2330 train_time:54287ms step_avg:40.82ms
step:1331/2330 train_time:54323ms step_avg:40.81ms
step:1332/2330 train_time:54367ms step_avg:40.82ms
step:1333/2330 train_time:54403ms step_avg:40.81ms
step:1334/2330 train_time:54448ms step_avg:40.82ms
step:1335/2330 train_time:54484ms step_avg:40.81ms
step:1336/2330 train_time:54529ms step_avg:40.82ms
step:1337/2330 train_time:54565ms step_avg:40.81ms
step:1338/2330 train_time:54609ms step_avg:40.81ms
step:1339/2330 train_time:54645ms step_avg:40.81ms
step:1340/2330 train_time:54692ms step_avg:40.81ms
step:1341/2330 train_time:54729ms step_avg:40.81ms
step:1342/2330 train_time:54775ms step_avg:40.82ms
step:1343/2330 train_time:54811ms step_avg:40.81ms
step:1344/2330 train_time:54858ms step_avg:40.82ms
step:1345/2330 train_time:54895ms step_avg:40.81ms
step:1346/2330 train_time:54942ms step_avg:40.82ms
step:1347/2330 train_time:54978ms step_avg:40.81ms
step:1348/2330 train_time:55023ms step_avg:40.82ms
step:1349/2330 train_time:55060ms step_avg:40.82ms
step:1350/2330 train_time:55105ms step_avg:40.82ms
step:1351/2330 train_time:55141ms step_avg:40.81ms
step:1352/2330 train_time:55186ms step_avg:40.82ms
step:1353/2330 train_time:55221ms step_avg:40.81ms
step:1354/2330 train_time:55266ms step_avg:40.82ms
step:1355/2330 train_time:55302ms step_avg:40.81ms
step:1356/2330 train_time:55347ms step_avg:40.82ms
step:1357/2330 train_time:55382ms step_avg:40.81ms
step:1358/2330 train_time:55427ms step_avg:40.82ms
step:1359/2330 train_time:55463ms step_avg:40.81ms
step:1360/2330 train_time:55507ms step_avg:40.81ms
step:1361/2330 train_time:55543ms step_avg:40.81ms
step:1362/2330 train_time:55588ms step_avg:40.81ms
step:1363/2330 train_time:55625ms step_avg:40.81ms
step:1364/2330 train_time:55670ms step_avg:40.81ms
step:1365/2330 train_time:55707ms step_avg:40.81ms
step:1366/2330 train_time:55754ms step_avg:40.82ms
step:1367/2330 train_time:55791ms step_avg:40.81ms
step:1368/2330 train_time:55836ms step_avg:40.82ms
step:1369/2330 train_time:55874ms step_avg:40.81ms
step:1370/2330 train_time:55920ms step_avg:40.82ms
step:1371/2330 train_time:55955ms step_avg:40.81ms
step:1372/2330 train_time:56002ms step_avg:40.82ms
step:1373/2330 train_time:56038ms step_avg:40.81ms
step:1374/2330 train_time:56083ms step_avg:40.82ms
step:1375/2330 train_time:56118ms step_avg:40.81ms
step:1376/2330 train_time:56165ms step_avg:40.82ms
step:1377/2330 train_time:56201ms step_avg:40.81ms
step:1378/2330 train_time:56246ms step_avg:40.82ms
step:1379/2330 train_time:56282ms step_avg:40.81ms
step:1380/2330 train_time:56327ms step_avg:40.82ms
step:1381/2330 train_time:56362ms step_avg:40.81ms
step:1382/2330 train_time:56407ms step_avg:40.82ms
step:1383/2330 train_time:56443ms step_avg:40.81ms
step:1384/2330 train_time:56488ms step_avg:40.82ms
step:1385/2330 train_time:56524ms step_avg:40.81ms
step:1386/2330 train_time:56569ms step_avg:40.81ms
step:1387/2330 train_time:56605ms step_avg:40.81ms
step:1388/2330 train_time:56650ms step_avg:40.81ms
step:1389/2330 train_time:56687ms step_avg:40.81ms
step:1390/2330 train_time:56734ms step_avg:40.82ms
step:1391/2330 train_time:56771ms step_avg:40.81ms
step:1392/2330 train_time:56816ms step_avg:40.82ms
step:1393/2330 train_time:56852ms step_avg:40.81ms
step:1394/2330 train_time:56897ms step_avg:40.82ms
step:1395/2330 train_time:56934ms step_avg:40.81ms
step:1396/2330 train_time:56980ms step_avg:40.82ms
step:1397/2330 train_time:57016ms step_avg:40.81ms
step:1398/2330 train_time:57061ms step_avg:40.82ms
step:1399/2330 train_time:57098ms step_avg:40.81ms
step:1400/2330 train_time:57144ms step_avg:40.82ms
step:1401/2330 train_time:57179ms step_avg:40.81ms
step:1402/2330 train_time:57225ms step_avg:40.82ms
step:1403/2330 train_time:57261ms step_avg:40.81ms
step:1404/2330 train_time:57306ms step_avg:40.82ms
step:1405/2330 train_time:57342ms step_avg:40.81ms
step:1406/2330 train_time:57387ms step_avg:40.82ms
step:1407/2330 train_time:57422ms step_avg:40.81ms
step:1408/2330 train_time:57467ms step_avg:40.81ms
step:1409/2330 train_time:57503ms step_avg:40.81ms
step:1410/2330 train_time:57548ms step_avg:40.81ms
step:1411/2330 train_time:57584ms step_avg:40.81ms
step:1412/2330 train_time:57629ms step_avg:40.81ms
step:1413/2330 train_time:57665ms step_avg:40.81ms
step:1414/2330 train_time:57711ms step_avg:40.81ms
step:1415/2330 train_time:57747ms step_avg:40.81ms
step:1416/2330 train_time:57793ms step_avg:40.81ms
step:1417/2330 train_time:57830ms step_avg:40.81ms
step:1418/2330 train_time:57877ms step_avg:40.82ms
step:1419/2330 train_time:57912ms step_avg:40.81ms
step:1420/2330 train_time:57959ms step_avg:40.82ms
step:1421/2330 train_time:57995ms step_avg:40.81ms
step:1422/2330 train_time:58041ms step_avg:40.82ms
step:1423/2330 train_time:58077ms step_avg:40.81ms
step:1424/2330 train_time:58123ms step_avg:40.82ms
step:1425/2330 train_time:58158ms step_avg:40.81ms
step:1426/2330 train_time:58203ms step_avg:40.82ms
step:1427/2330 train_time:58239ms step_avg:40.81ms
step:1428/2330 train_time:58285ms step_avg:40.82ms
step:1429/2330 train_time:58320ms step_avg:40.81ms
step:1430/2330 train_time:58366ms step_avg:40.82ms
step:1431/2330 train_time:58402ms step_avg:40.81ms
step:1432/2330 train_time:58447ms step_avg:40.82ms
step:1433/2330 train_time:58482ms step_avg:40.81ms
step:1434/2330 train_time:58527ms step_avg:40.81ms
step:1435/2330 train_time:58563ms step_avg:40.81ms
step:1436/2330 train_time:58608ms step_avg:40.81ms
step:1437/2330 train_time:58644ms step_avg:40.81ms
step:1438/2330 train_time:58689ms step_avg:40.81ms
step:1439/2330 train_time:58725ms step_avg:40.81ms
step:1440/2330 train_time:58770ms step_avg:40.81ms
step:1441/2330 train_time:58807ms step_avg:40.81ms
step:1442/2330 train_time:58854ms step_avg:40.81ms
step:1443/2330 train_time:58890ms step_avg:40.81ms
step:1444/2330 train_time:58936ms step_avg:40.81ms
step:1445/2330 train_time:58973ms step_avg:40.81ms
step:1446/2330 train_time:59019ms step_avg:40.82ms
step:1447/2330 train_time:59055ms step_avg:40.81ms
step:1448/2330 train_time:59101ms step_avg:40.82ms
step:1449/2330 train_time:59136ms step_avg:40.81ms
step:1450/2330 train_time:59182ms step_avg:40.82ms
step:1451/2330 train_time:59218ms step_avg:40.81ms
step:1452/2330 train_time:59264ms step_avg:40.82ms
step:1453/2330 train_time:59300ms step_avg:40.81ms
step:1454/2330 train_time:59346ms step_avg:40.82ms
step:1455/2330 train_time:59381ms step_avg:40.81ms
step:1456/2330 train_time:59427ms step_avg:40.81ms
step:1457/2330 train_time:59462ms step_avg:40.81ms
step:1458/2330 train_time:59507ms step_avg:40.81ms
step:1459/2330 train_time:59543ms step_avg:40.81ms
step:1460/2330 train_time:59588ms step_avg:40.81ms
step:1461/2330 train_time:59623ms step_avg:40.81ms
step:1462/2330 train_time:59668ms step_avg:40.81ms
step:1463/2330 train_time:59704ms step_avg:40.81ms
step:1464/2330 train_time:59750ms step_avg:40.81ms
step:1465/2330 train_time:59787ms step_avg:40.81ms
step:1466/2330 train_time:59833ms step_avg:40.81ms
step:1467/2330 train_time:59869ms step_avg:40.81ms
step:1468/2330 train_time:59914ms step_avg:40.81ms
step:1469/2330 train_time:59951ms step_avg:40.81ms
step:1470/2330 train_time:59996ms step_avg:40.81ms
step:1471/2330 train_time:60033ms step_avg:40.81ms
step:1472/2330 train_time:60079ms step_avg:40.81ms
step:1473/2330 train_time:60115ms step_avg:40.81ms
step:1474/2330 train_time:60161ms step_avg:40.81ms
step:1475/2330 train_time:60197ms step_avg:40.81ms
step:1476/2330 train_time:60242ms step_avg:40.81ms
step:1477/2330 train_time:60277ms step_avg:40.81ms
step:1478/2330 train_time:60323ms step_avg:40.81ms
step:1479/2330 train_time:60359ms step_avg:40.81ms
step:1480/2330 train_time:60404ms step_avg:40.81ms
step:1481/2330 train_time:60440ms step_avg:40.81ms
step:1482/2330 train_time:60486ms step_avg:40.81ms
step:1483/2330 train_time:60521ms step_avg:40.81ms
step:1484/2330 train_time:60567ms step_avg:40.81ms
step:1485/2330 train_time:60603ms step_avg:40.81ms
step:1486/2330 train_time:60648ms step_avg:40.81ms
step:1487/2330 train_time:60683ms step_avg:40.81ms
step:1488/2330 train_time:60728ms step_avg:40.81ms
step:1489/2330 train_time:60765ms step_avg:40.81ms
step:1490/2330 train_time:60810ms step_avg:40.81ms
step:1491/2330 train_time:60846ms step_avg:40.81ms
step:1492/2330 train_time:60893ms step_avg:40.81ms
step:1493/2330 train_time:60930ms step_avg:40.81ms
step:1494/2330 train_time:60975ms step_avg:40.81ms
step:1495/2330 train_time:61011ms step_avg:40.81ms
step:1496/2330 train_time:61057ms step_avg:40.81ms
step:1497/2330 train_time:61093ms step_avg:40.81ms
step:1498/2330 train_time:61139ms step_avg:40.81ms
step:1499/2330 train_time:61174ms step_avg:40.81ms
step:1500/2330 train_time:61219ms step_avg:40.81ms
step:1500/2330 val_loss:5.1780 train_time:61307ms step_avg:40.87ms
step:1501/2330 train_time:61321ms step_avg:40.85ms
step:1502/2330 train_time:61334ms step_avg:40.83ms
step:1503/2330 train_time:61345ms step_avg:40.81ms
step:1504/2330 train_time:61381ms step_avg:40.81ms
step:1505/2330 train_time:61416ms step_avg:40.81ms
step:1506/2330 train_time:61460ms step_avg:40.81ms
step:1507/2330 train_time:61496ms step_avg:40.81ms
step:1508/2330 train_time:61540ms step_avg:40.81ms
step:1509/2330 train_time:61574ms step_avg:40.80ms
step:1510/2330 train_time:61623ms step_avg:40.81ms
step:1511/2330 train_time:61663ms step_avg:40.81ms
step:1512/2330 train_time:61710ms step_avg:40.81ms
step:1513/2330 train_time:61745ms step_avg:40.81ms
step:1514/2330 train_time:61793ms step_avg:40.81ms
step:1515/2330 train_time:61829ms step_avg:40.81ms
step:1516/2330 train_time:61874ms step_avg:40.81ms
step:1517/2330 train_time:61910ms step_avg:40.81ms
step:1518/2330 train_time:61955ms step_avg:40.81ms
step:1519/2330 train_time:61991ms step_avg:40.81ms
step:1520/2330 train_time:62038ms step_avg:40.81ms
step:1521/2330 train_time:62075ms step_avg:40.81ms
step:1522/2330 train_time:62120ms step_avg:40.81ms
step:1523/2330 train_time:62155ms step_avg:40.81ms
step:1524/2330 train_time:62201ms step_avg:40.81ms
step:1525/2330 train_time:62237ms step_avg:40.81ms
step:1526/2330 train_time:62283ms step_avg:40.81ms
step:1527/2330 train_time:62319ms step_avg:40.81ms
step:1528/2330 train_time:62540ms step_avg:40.93ms
step:1529/2330 train_time:62575ms step_avg:40.93ms
step:1530/2330 train_time:62619ms step_avg:40.93ms
step:1531/2330 train_time:62653ms step_avg:40.92ms
step:1532/2330 train_time:62794ms step_avg:40.99ms
step:1533/2330 train_time:62828ms step_avg:40.98ms
step:1534/2330 train_time:62872ms step_avg:40.99ms
step:1535/2330 train_time:62907ms step_avg:40.98ms
step:1536/2330 train_time:62952ms step_avg:40.98ms
step:1537/2330 train_time:62986ms step_avg:40.98ms
step:1538/2330 train_time:63031ms step_avg:40.98ms
step:1539/2330 train_time:63066ms step_avg:40.98ms
step:1540/2330 train_time:63111ms step_avg:40.98ms
step:1541/2330 train_time:63146ms step_avg:40.98ms
step:1542/2330 train_time:63190ms step_avg:40.98ms
step:1543/2330 train_time:63226ms step_avg:40.98ms
step:1544/2330 train_time:63270ms step_avg:40.98ms
step:1545/2330 train_time:63306ms step_avg:40.97ms
step:1546/2330 train_time:63350ms step_avg:40.98ms
step:1547/2330 train_time:63385ms step_avg:40.97ms
step:1548/2330 train_time:63430ms step_avg:40.98ms
step:1549/2330 train_time:63465ms step_avg:40.97ms
step:1550/2330 train_time:63509ms step_avg:40.97ms
step:1551/2330 train_time:63543ms step_avg:40.97ms
step:1552/2330 train_time:63588ms step_avg:40.97ms
step:1553/2330 train_time:63623ms step_avg:40.97ms
step:1554/2330 train_time:63672ms step_avg:40.97ms
step:1555/2330 train_time:63714ms step_avg:40.97ms
step:1556/2330 train_time:63764ms step_avg:40.98ms
step:1557/2330 train_time:63800ms step_avg:40.98ms
step:1558/2330 train_time:63847ms step_avg:40.98ms
step:1559/2330 train_time:63884ms step_avg:40.98ms
step:1560/2330 train_time:63929ms step_avg:40.98ms
step:1561/2330 train_time:63965ms step_avg:40.98ms
step:1562/2330 train_time:64010ms step_avg:40.98ms
step:1563/2330 train_time:64045ms step_avg:40.98ms
step:1564/2330 train_time:64090ms step_avg:40.98ms
step:1565/2330 train_time:64125ms step_avg:40.97ms
step:1566/2330 train_time:64170ms step_avg:40.98ms
step:1567/2330 train_time:64204ms step_avg:40.97ms
step:1568/2330 train_time:64249ms step_avg:40.98ms
step:1569/2330 train_time:64284ms step_avg:40.97ms
step:1570/2330 train_time:64329ms step_avg:40.97ms
step:1571/2330 train_time:64364ms step_avg:40.97ms
step:1572/2330 train_time:64408ms step_avg:40.97ms
step:1573/2330 train_time:64444ms step_avg:40.97ms
step:1574/2330 train_time:64488ms step_avg:40.97ms
step:1575/2330 train_time:64523ms step_avg:40.97ms
step:1576/2330 train_time:64569ms step_avg:40.97ms
step:1577/2330 train_time:64607ms step_avg:40.97ms
step:1578/2330 train_time:64654ms step_avg:40.97ms
step:1579/2330 train_time:64692ms step_avg:40.97ms
step:1580/2330 train_time:64740ms step_avg:40.97ms
step:1581/2330 train_time:64778ms step_avg:40.97ms
step:1582/2330 train_time:64823ms step_avg:40.98ms
step:1583/2330 train_time:64860ms step_avg:40.97ms
step:1584/2330 train_time:64907ms step_avg:40.98ms
step:1585/2330 train_time:64942ms step_avg:40.97ms
step:1586/2330 train_time:64988ms step_avg:40.98ms
step:1587/2330 train_time:65024ms step_avg:40.97ms
step:1588/2330 train_time:65069ms step_avg:40.98ms
step:1589/2330 train_time:65105ms step_avg:40.97ms
step:1590/2330 train_time:65149ms step_avg:40.97ms
step:1591/2330 train_time:65184ms step_avg:40.97ms
step:1592/2330 train_time:65229ms step_avg:40.97ms
step:1593/2330 train_time:65264ms step_avg:40.97ms
step:1594/2330 train_time:65309ms step_avg:40.97ms
step:1595/2330 train_time:65344ms step_avg:40.97ms
step:1596/2330 train_time:65388ms step_avg:40.97ms
step:1597/2330 train_time:65423ms step_avg:40.97ms
step:1598/2330 train_time:65468ms step_avg:40.97ms
step:1599/2330 train_time:65504ms step_avg:40.97ms
step:1600/2330 train_time:65550ms step_avg:40.97ms
step:1601/2330 train_time:65586ms step_avg:40.97ms
step:1602/2330 train_time:65632ms step_avg:40.97ms
step:1603/2330 train_time:65669ms step_avg:40.97ms
step:1604/2330 train_time:65715ms step_avg:40.97ms
step:1605/2330 train_time:65753ms step_avg:40.97ms
step:1606/2330 train_time:65800ms step_avg:40.97ms
step:1607/2330 train_time:65837ms step_avg:40.97ms
step:1608/2330 train_time:65883ms step_avg:40.97ms
step:1609/2330 train_time:65918ms step_avg:40.97ms
step:1610/2330 train_time:65965ms step_avg:40.97ms
step:1611/2330 train_time:66001ms step_avg:40.97ms
step:1612/2330 train_time:66046ms step_avg:40.97ms
step:1613/2330 train_time:66081ms step_avg:40.97ms
step:1614/2330 train_time:66126ms step_avg:40.97ms
step:1615/2330 train_time:66162ms step_avg:40.97ms
step:1616/2330 train_time:66207ms step_avg:40.97ms
step:1617/2330 train_time:66242ms step_avg:40.97ms
step:1618/2330 train_time:66287ms step_avg:40.97ms
step:1619/2330 train_time:66322ms step_avg:40.96ms
step:1620/2330 train_time:66366ms step_avg:40.97ms
step:1621/2330 train_time:66401ms step_avg:40.96ms
step:1622/2330 train_time:66447ms step_avg:40.97ms
step:1623/2330 train_time:66483ms step_avg:40.96ms
step:1624/2330 train_time:66528ms step_avg:40.97ms
step:1625/2330 train_time:66564ms step_avg:40.96ms
step:1626/2330 train_time:66609ms step_avg:40.96ms
step:1627/2330 train_time:66645ms step_avg:40.96ms
step:1628/2330 train_time:66692ms step_avg:40.97ms
step:1629/2330 train_time:66729ms step_avg:40.96ms
step:1630/2330 train_time:66775ms step_avg:40.97ms
step:1631/2330 train_time:66812ms step_avg:40.96ms
step:1632/2330 train_time:66860ms step_avg:40.97ms
step:1633/2330 train_time:66896ms step_avg:40.97ms
step:1634/2330 train_time:66943ms step_avg:40.97ms
step:1635/2330 train_time:66978ms step_avg:40.97ms
step:1636/2330 train_time:67023ms step_avg:40.97ms
step:1637/2330 train_time:67059ms step_avg:40.96ms
step:1638/2330 train_time:67105ms step_avg:40.97ms
step:1639/2330 train_time:67140ms step_avg:40.96ms
step:1640/2330 train_time:67185ms step_avg:40.97ms
step:1641/2330 train_time:67221ms step_avg:40.96ms
step:1642/2330 train_time:67266ms step_avg:40.97ms
step:1643/2330 train_time:67301ms step_avg:40.96ms
step:1644/2330 train_time:67346ms step_avg:40.96ms
step:1645/2330 train_time:67382ms step_avg:40.96ms
step:1646/2330 train_time:67427ms step_avg:40.96ms
step:1647/2330 train_time:67463ms step_avg:40.96ms
step:1648/2330 train_time:67509ms step_avg:40.96ms
step:1649/2330 train_time:67544ms step_avg:40.96ms
step:1650/2330 train_time:67590ms step_avg:40.96ms
step:1651/2330 train_time:67627ms step_avg:40.96ms
step:1652/2330 train_time:67672ms step_avg:40.96ms
step:1653/2330 train_time:67708ms step_avg:40.96ms
step:1654/2330 train_time:67753ms step_avg:40.96ms
step:1655/2330 train_time:67789ms step_avg:40.96ms
step:1656/2330 train_time:67835ms step_avg:40.96ms
step:1657/2330 train_time:67871ms step_avg:40.96ms
step:1658/2330 train_time:67917ms step_avg:40.96ms
step:1659/2330 train_time:67953ms step_avg:40.96ms
step:1660/2330 train_time:67999ms step_avg:40.96ms
step:1661/2330 train_time:68034ms step_avg:40.96ms
step:1662/2330 train_time:68080ms step_avg:40.96ms
step:1663/2330 train_time:68117ms step_avg:40.96ms
step:1664/2330 train_time:68163ms step_avg:40.96ms
step:1665/2330 train_time:68199ms step_avg:40.96ms
step:1666/2330 train_time:68243ms step_avg:40.96ms
step:1667/2330 train_time:68279ms step_avg:40.96ms
step:1668/2330 train_time:68324ms step_avg:40.96ms
step:1669/2330 train_time:68360ms step_avg:40.96ms
step:1670/2330 train_time:68405ms step_avg:40.96ms
step:1671/2330 train_time:68440ms step_avg:40.96ms
step:1672/2330 train_time:68485ms step_avg:40.96ms
step:1673/2330 train_time:68521ms step_avg:40.96ms
step:1674/2330 train_time:68567ms step_avg:40.96ms
step:1675/2330 train_time:68603ms step_avg:40.96ms
step:1676/2330 train_time:68649ms step_avg:40.96ms
step:1677/2330 train_time:68684ms step_avg:40.96ms
step:1678/2330 train_time:68729ms step_avg:40.96ms
step:1679/2330 train_time:68766ms step_avg:40.96ms
step:1680/2330 train_time:68812ms step_avg:40.96ms
step:1681/2330 train_time:68848ms step_avg:40.96ms
step:1682/2330 train_time:68893ms step_avg:40.96ms
step:1683/2330 train_time:68929ms step_avg:40.96ms
step:1684/2330 train_time:68974ms step_avg:40.96ms
step:1685/2330 train_time:69011ms step_avg:40.96ms
step:1686/2330 train_time:69057ms step_avg:40.96ms
step:1687/2330 train_time:69094ms step_avg:40.96ms
step:1688/2330 train_time:69140ms step_avg:40.96ms
step:1689/2330 train_time:69175ms step_avg:40.96ms
step:1690/2330 train_time:69220ms step_avg:40.96ms
step:1691/2330 train_time:69257ms step_avg:40.96ms
step:1692/2330 train_time:69302ms step_avg:40.96ms
step:1693/2330 train_time:69337ms step_avg:40.96ms
step:1694/2330 train_time:69383ms step_avg:40.96ms
step:1695/2330 train_time:69419ms step_avg:40.96ms
step:1696/2330 train_time:69464ms step_avg:40.96ms
step:1697/2330 train_time:69499ms step_avg:40.95ms
step:1698/2330 train_time:69546ms step_avg:40.96ms
step:1699/2330 train_time:69581ms step_avg:40.95ms
step:1700/2330 train_time:69626ms step_avg:40.96ms
step:1701/2330 train_time:69662ms step_avg:40.95ms
step:1702/2330 train_time:69708ms step_avg:40.96ms
step:1703/2330 train_time:69745ms step_avg:40.95ms
step:1704/2330 train_time:69791ms step_avg:40.96ms
step:1705/2330 train_time:69827ms step_avg:40.95ms
step:1706/2330 train_time:69872ms step_avg:40.96ms
step:1707/2330 train_time:69908ms step_avg:40.95ms
step:1708/2330 train_time:69953ms step_avg:40.96ms
step:1709/2330 train_time:69989ms step_avg:40.95ms
step:1710/2330 train_time:70034ms step_avg:40.96ms
step:1711/2330 train_time:70070ms step_avg:40.95ms
step:1712/2330 train_time:70117ms step_avg:40.96ms
step:1713/2330 train_time:70154ms step_avg:40.95ms
step:1714/2330 train_time:70199ms step_avg:40.96ms
step:1715/2330 train_time:70235ms step_avg:40.95ms
step:1716/2330 train_time:70280ms step_avg:40.96ms
step:1717/2330 train_time:70317ms step_avg:40.95ms
step:1718/2330 train_time:70362ms step_avg:40.96ms
step:1719/2330 train_time:70398ms step_avg:40.95ms
step:1720/2330 train_time:70443ms step_avg:40.96ms
step:1721/2330 train_time:70479ms step_avg:40.95ms
step:1722/2330 train_time:70525ms step_avg:40.96ms
step:1723/2330 train_time:70560ms step_avg:40.95ms
step:1724/2330 train_time:70606ms step_avg:40.95ms
step:1725/2330 train_time:70642ms step_avg:40.95ms
step:1726/2330 train_time:70688ms step_avg:40.95ms
step:1727/2330 train_time:70724ms step_avg:40.95ms
step:1728/2330 train_time:70770ms step_avg:40.95ms
step:1729/2330 train_time:70806ms step_avg:40.95ms
step:1730/2330 train_time:70852ms step_avg:40.95ms
step:1731/2330 train_time:70888ms step_avg:40.95ms
step:1732/2330 train_time:70933ms step_avg:40.95ms
step:1733/2330 train_time:70969ms step_avg:40.95ms
step:1734/2330 train_time:71015ms step_avg:40.95ms
step:1735/2330 train_time:71051ms step_avg:40.95ms
step:1736/2330 train_time:71097ms step_avg:40.95ms
step:1737/2330 train_time:71134ms step_avg:40.95ms
step:1738/2330 train_time:71180ms step_avg:40.95ms
step:1739/2330 train_time:71215ms step_avg:40.95ms
step:1740/2330 train_time:71260ms step_avg:40.95ms
step:1741/2330 train_time:71296ms step_avg:40.95ms
step:1742/2330 train_time:71342ms step_avg:40.95ms
step:1743/2330 train_time:71377ms step_avg:40.95ms
step:1744/2330 train_time:71422ms step_avg:40.95ms
step:1745/2330 train_time:71458ms step_avg:40.95ms
step:1746/2330 train_time:71503ms step_avg:40.95ms
step:1747/2330 train_time:71539ms step_avg:40.95ms
step:1748/2330 train_time:71585ms step_avg:40.95ms
step:1749/2330 train_time:71621ms step_avg:40.95ms
step:1750/2330 train_time:71667ms step_avg:40.95ms
step:1750/2330 val_loss:5.1414 train_time:71755ms step_avg:41.00ms
step:1751/2330 train_time:71770ms step_avg:40.99ms
step:1752/2330 train_time:71783ms step_avg:40.97ms
step:1753/2330 train_time:71795ms step_avg:40.96ms
step:1754/2330 train_time:71829ms step_avg:40.95ms
step:1755/2330 train_time:71863ms step_avg:40.95ms
step:1756/2330 train_time:71908ms step_avg:40.95ms
step:1757/2330 train_time:71943ms step_avg:40.95ms
step:1758/2330 train_time:71988ms step_avg:40.95ms
step:1759/2330 train_time:72024ms step_avg:40.95ms
step:1760/2330 train_time:72070ms step_avg:40.95ms
step:1761/2330 train_time:72109ms step_avg:40.95ms
step:1762/2330 train_time:72155ms step_avg:40.95ms
step:1763/2330 train_time:72190ms step_avg:40.95ms
step:1764/2330 train_time:72234ms step_avg:40.95ms
step:1765/2330 train_time:72268ms step_avg:40.95ms
step:1766/2330 train_time:72313ms step_avg:40.95ms
step:1767/2330 train_time:72348ms step_avg:40.94ms
step:1768/2330 train_time:72393ms step_avg:40.95ms
step:1769/2330 train_time:72428ms step_avg:40.94ms
step:1770/2330 train_time:72473ms step_avg:40.95ms
step:1771/2330 train_time:72509ms step_avg:40.94ms
step:1772/2330 train_time:72553ms step_avg:40.94ms
step:1773/2330 train_time:72588ms step_avg:40.94ms
step:1774/2330 train_time:72633ms step_avg:40.94ms
step:1775/2330 train_time:72673ms step_avg:40.94ms
step:1776/2330 train_time:72722ms step_avg:40.95ms
step:1777/2330 train_time:72759ms step_avg:40.95ms
step:1778/2330 train_time:72807ms step_avg:40.95ms
step:1779/2330 train_time:72843ms step_avg:40.95ms
step:1780/2330 train_time:72888ms step_avg:40.95ms
step:1781/2330 train_time:72923ms step_avg:40.95ms
step:1782/2330 train_time:72968ms step_avg:40.95ms
step:1783/2330 train_time:73004ms step_avg:40.94ms
step:1784/2330 train_time:73051ms step_avg:40.95ms
step:1785/2330 train_time:73087ms step_avg:40.95ms
step:1786/2330 train_time:73133ms step_avg:40.95ms
step:1787/2330 train_time:73168ms step_avg:40.94ms
step:1788/2330 train_time:73213ms step_avg:40.95ms
step:1789/2330 train_time:73248ms step_avg:40.94ms
step:1790/2330 train_time:73293ms step_avg:40.95ms
step:1791/2330 train_time:73328ms step_avg:40.94ms
step:1792/2330 train_time:73373ms step_avg:40.94ms
step:1793/2330 train_time:73409ms step_avg:40.94ms
step:1794/2330 train_time:73454ms step_avg:40.94ms
step:1795/2330 train_time:73489ms step_avg:40.94ms
step:1796/2330 train_time:73534ms step_avg:40.94ms
step:1797/2330 train_time:73570ms step_avg:40.94ms
step:1798/2330 train_time:73616ms step_avg:40.94ms
step:1799/2330 train_time:73653ms step_avg:40.94ms
step:1800/2330 train_time:73701ms step_avg:40.94ms
step:1801/2330 train_time:73737ms step_avg:40.94ms
step:1802/2330 train_time:73783ms step_avg:40.95ms
step:1803/2330 train_time:73820ms step_avg:40.94ms
step:1804/2330 train_time:73866ms step_avg:40.95ms
step:1805/2330 train_time:73902ms step_avg:40.94ms
step:1806/2330 train_time:73948ms step_avg:40.95ms
step:1807/2330 train_time:73985ms step_avg:40.94ms
step:1808/2330 train_time:74031ms step_avg:40.95ms
step:1809/2330 train_time:74066ms step_avg:40.94ms
step:1810/2330 train_time:74112ms step_avg:40.95ms
step:1811/2330 train_time:74148ms step_avg:40.94ms
step:1812/2330 train_time:74193ms step_avg:40.95ms
step:1813/2330 train_time:74228ms step_avg:40.94ms
step:1814/2330 train_time:74273ms step_avg:40.94ms
step:1815/2330 train_time:74308ms step_avg:40.94ms
step:1816/2330 train_time:74353ms step_avg:40.94ms
step:1817/2330 train_time:74388ms step_avg:40.94ms
step:1818/2330 train_time:74433ms step_avg:40.94ms
step:1819/2330 train_time:74468ms step_avg:40.94ms
step:1820/2330 train_time:74513ms step_avg:40.94ms
step:1821/2330 train_time:74549ms step_avg:40.94ms
step:1822/2330 train_time:74595ms step_avg:40.94ms
step:1823/2330 train_time:74632ms step_avg:40.94ms
step:1824/2330 train_time:74679ms step_avg:40.94ms
step:1825/2330 train_time:74715ms step_avg:40.94ms
step:1826/2330 train_time:74762ms step_avg:40.94ms
step:1827/2330 train_time:74800ms step_avg:40.94ms
step:1828/2330 train_time:74846ms step_avg:40.94ms
step:1829/2330 train_time:74883ms step_avg:40.94ms
step:1830/2330 train_time:74928ms step_avg:40.94ms
step:1831/2330 train_time:74963ms step_avg:40.94ms
step:1832/2330 train_time:75009ms step_avg:40.94ms
step:1833/2330 train_time:75044ms step_avg:40.94ms
step:1834/2330 train_time:75090ms step_avg:40.94ms
step:1835/2330 train_time:75127ms step_avg:40.94ms
step:1836/2330 train_time:75173ms step_avg:40.94ms
step:1837/2330 train_time:75209ms step_avg:40.94ms
step:1838/2330 train_time:75254ms step_avg:40.94ms
step:1839/2330 train_time:75289ms step_avg:40.94ms
step:1840/2330 train_time:75334ms step_avg:40.94ms
step:1841/2330 train_time:75369ms step_avg:40.94ms
step:1842/2330 train_time:75414ms step_avg:40.94ms
step:1843/2330 train_time:75450ms step_avg:40.94ms
step:1844/2330 train_time:75495ms step_avg:40.94ms
step:1845/2330 train_time:75530ms step_avg:40.94ms
step:1846/2330 train_time:75576ms step_avg:40.94ms
step:1847/2330 train_time:75613ms step_avg:40.94ms
step:1848/2330 train_time:75660ms step_avg:40.94ms
step:1849/2330 train_time:75696ms step_avg:40.94ms
step:1850/2330 train_time:75742ms step_avg:40.94ms
step:1851/2330 train_time:75778ms step_avg:40.94ms
step:1852/2330 train_time:75824ms step_avg:40.94ms
step:1853/2330 train_time:75861ms step_avg:40.94ms
step:1854/2330 train_time:75905ms step_avg:40.94ms
step:1855/2330 train_time:75941ms step_avg:40.94ms
step:1856/2330 train_time:75988ms step_avg:40.94ms
step:1857/2330 train_time:76023ms step_avg:40.94ms
step:1858/2330 train_time:76069ms step_avg:40.94ms
step:1859/2330 train_time:76105ms step_avg:40.94ms
step:1860/2330 train_time:76151ms step_avg:40.94ms
step:1861/2330 train_time:76186ms step_avg:40.94ms
step:1862/2330 train_time:76232ms step_avg:40.94ms
step:1863/2330 train_time:76267ms step_avg:40.94ms
step:1864/2330 train_time:76312ms step_avg:40.94ms
step:1865/2330 train_time:76348ms step_avg:40.94ms
step:1866/2330 train_time:76393ms step_avg:40.94ms
step:1867/2330 train_time:76428ms step_avg:40.94ms
step:1868/2330 train_time:76473ms step_avg:40.94ms
step:1869/2330 train_time:76509ms step_avg:40.94ms
step:1870/2330 train_time:76554ms step_avg:40.94ms
step:1871/2330 train_time:76590ms step_avg:40.94ms
step:1872/2330 train_time:76636ms step_avg:40.94ms
step:1873/2330 train_time:76673ms step_avg:40.94ms
step:1874/2330 train_time:76719ms step_avg:40.94ms
step:1875/2330 train_time:76756ms step_avg:40.94ms
step:1876/2330 train_time:76803ms step_avg:40.94ms
step:1877/2330 train_time:76838ms step_avg:40.94ms
step:1878/2330 train_time:76883ms step_avg:40.94ms
step:1879/2330 train_time:76919ms step_avg:40.94ms
step:1880/2330 train_time:76964ms step_avg:40.94ms
step:1881/2330 train_time:77001ms step_avg:40.94ms
step:1882/2330 train_time:77046ms step_avg:40.94ms
step:1883/2330 train_time:77082ms step_avg:40.94ms
step:1884/2330 train_time:77127ms step_avg:40.94ms
step:1885/2330 train_time:77163ms step_avg:40.94ms
step:1886/2330 train_time:77209ms step_avg:40.94ms
step:1887/2330 train_time:77244ms step_avg:40.93ms
step:1888/2330 train_time:77290ms step_avg:40.94ms
step:1889/2330 train_time:77325ms step_avg:40.93ms
step:1890/2330 train_time:77371ms step_avg:40.94ms
step:1891/2330 train_time:77407ms step_avg:40.93ms
step:1892/2330 train_time:77452ms step_avg:40.94ms
step:1893/2330 train_time:77488ms step_avg:40.93ms
step:1894/2330 train_time:77533ms step_avg:40.94ms
step:1895/2330 train_time:77568ms step_avg:40.93ms
step:1896/2330 train_time:77614ms step_avg:40.94ms
step:1897/2330 train_time:77650ms step_avg:40.93ms
step:1898/2330 train_time:77697ms step_avg:40.94ms
step:1899/2330 train_time:77733ms step_avg:40.93ms
step:1900/2330 train_time:77779ms step_avg:40.94ms
step:1901/2330 train_time:77815ms step_avg:40.93ms
step:1902/2330 train_time:77861ms step_avg:40.94ms
step:1903/2330 train_time:77897ms step_avg:40.93ms
step:1904/2330 train_time:77943ms step_avg:40.94ms
step:1905/2330 train_time:77979ms step_avg:40.93ms
step:1906/2330 train_time:78024ms step_avg:40.94ms
step:1907/2330 train_time:78061ms step_avg:40.93ms
step:1908/2330 train_time:78107ms step_avg:40.94ms
step:1909/2330 train_time:78143ms step_avg:40.93ms
step:1910/2330 train_time:78188ms step_avg:40.94ms
step:1911/2330 train_time:78223ms step_avg:40.93ms
step:1912/2330 train_time:78269ms step_avg:40.94ms
step:1913/2330 train_time:78305ms step_avg:40.93ms
step:1914/2330 train_time:78351ms step_avg:40.94ms
step:1915/2330 train_time:78388ms step_avg:40.93ms
step:1916/2330 train_time:78433ms step_avg:40.94ms
step:1917/2330 train_time:78469ms step_avg:40.93ms
step:1918/2330 train_time:78514ms step_avg:40.94ms
step:1919/2330 train_time:78550ms step_avg:40.93ms
step:1920/2330 train_time:78597ms step_avg:40.94ms
step:1921/2330 train_time:78633ms step_avg:40.93ms
step:1922/2330 train_time:78679ms step_avg:40.94ms
step:1923/2330 train_time:78714ms step_avg:40.93ms
step:1924/2330 train_time:78760ms step_avg:40.94ms
step:1925/2330 train_time:78796ms step_avg:40.93ms
step:1926/2330 train_time:78841ms step_avg:40.94ms
step:1927/2330 train_time:78877ms step_avg:40.93ms
step:1928/2330 train_time:78923ms step_avg:40.94ms
step:1929/2330 train_time:78959ms step_avg:40.93ms
step:1930/2330 train_time:79005ms step_avg:40.94ms
step:1931/2330 train_time:79041ms step_avg:40.93ms
step:1932/2330 train_time:79087ms step_avg:40.94ms
step:1933/2330 train_time:79123ms step_avg:40.93ms
step:1934/2330 train_time:79168ms step_avg:40.93ms
step:1935/2330 train_time:79204ms step_avg:40.93ms
step:1936/2330 train_time:79250ms step_avg:40.93ms
step:1937/2330 train_time:79285ms step_avg:40.93ms
step:1938/2330 train_time:79331ms step_avg:40.93ms
step:1939/2330 train_time:79367ms step_avg:40.93ms
step:1940/2330 train_time:79413ms step_avg:40.93ms
step:1941/2330 train_time:79448ms step_avg:40.93ms
step:1942/2330 train_time:79493ms step_avg:40.93ms
step:1943/2330 train_time:79529ms step_avg:40.93ms
step:1944/2330 train_time:79575ms step_avg:40.93ms
step:1945/2330 train_time:79611ms step_avg:40.93ms
step:1946/2330 train_time:79657ms step_avg:40.93ms
step:1947/2330 train_time:79694ms step_avg:40.93ms
step:1948/2330 train_time:79740ms step_avg:40.93ms
step:1949/2330 train_time:79775ms step_avg:40.93ms
step:1950/2330 train_time:79820ms step_avg:40.93ms
step:1951/2330 train_time:79856ms step_avg:40.93ms
step:1952/2330 train_time:79902ms step_avg:40.93ms
step:1953/2330 train_time:79939ms step_avg:40.93ms
step:1954/2330 train_time:79984ms step_avg:40.93ms
step:1955/2330 train_time:80020ms step_avg:40.93ms
step:1956/2330 train_time:80066ms step_avg:40.93ms
step:1957/2330 train_time:80102ms step_avg:40.93ms
step:1958/2330 train_time:80148ms step_avg:40.93ms
step:1959/2330 train_time:80183ms step_avg:40.93ms
step:1960/2330 train_time:80229ms step_avg:40.93ms
step:1961/2330 train_time:80265ms step_avg:40.93ms
step:1962/2330 train_time:80311ms step_avg:40.93ms
step:1963/2330 train_time:80347ms step_avg:40.93ms
step:1964/2330 train_time:80393ms step_avg:40.93ms
step:1965/2330 train_time:80427ms step_avg:40.93ms
step:1966/2330 train_time:80473ms step_avg:40.93ms
step:1967/2330 train_time:80508ms step_avg:40.93ms
step:1968/2330 train_time:80554ms step_avg:40.93ms
step:1969/2330 train_time:80589ms step_avg:40.93ms
step:1970/2330 train_time:80634ms step_avg:40.93ms
step:1971/2330 train_time:80671ms step_avg:40.93ms
step:1972/2330 train_time:80717ms step_avg:40.93ms
step:1973/2330 train_time:80754ms step_avg:40.93ms
step:1974/2330 train_time:80800ms step_avg:40.93ms
step:1975/2330 train_time:80836ms step_avg:40.93ms
step:1976/2330 train_time:80881ms step_avg:40.93ms
step:1977/2330 train_time:80917ms step_avg:40.93ms
step:1978/2330 train_time:80963ms step_avg:40.93ms
step:1979/2330 train_time:81000ms step_avg:40.93ms
step:1980/2330 train_time:81045ms step_avg:40.93ms
step:1981/2330 train_time:81081ms step_avg:40.93ms
step:1982/2330 train_time:81126ms step_avg:40.93ms
step:1983/2330 train_time:81162ms step_avg:40.93ms
step:1984/2330 train_time:81207ms step_avg:40.93ms
step:1985/2330 train_time:81243ms step_avg:40.93ms
step:1986/2330 train_time:81289ms step_avg:40.93ms
step:1987/2330 train_time:81325ms step_avg:40.93ms
step:1988/2330 train_time:81371ms step_avg:40.93ms
step:1989/2330 train_time:81408ms step_avg:40.93ms
step:1990/2330 train_time:81453ms step_avg:40.93ms
step:1991/2330 train_time:81488ms step_avg:40.93ms
step:1992/2330 train_time:81534ms step_avg:40.93ms
step:1993/2330 train_time:81570ms step_avg:40.93ms
step:1994/2330 train_time:81615ms step_avg:40.93ms
step:1995/2330 train_time:81652ms step_avg:40.93ms
step:1996/2330 train_time:81698ms step_avg:40.93ms
step:1997/2330 train_time:81735ms step_avg:40.93ms
step:1998/2330 train_time:81780ms step_avg:40.93ms
step:1999/2330 train_time:81815ms step_avg:40.93ms
step:2000/2330 train_time:81862ms step_avg:40.93ms
step:2000/2330 val_loss:5.1082 train_time:81951ms step_avg:40.98ms
step:2001/2330 train_time:81963ms step_avg:40.96ms
step:2002/2330 train_time:81975ms step_avg:40.95ms
step:2003/2330 train_time:81985ms step_avg:40.93ms
step:2004/2330 train_time:82027ms step_avg:40.93ms
step:2005/2330 train_time:82061ms step_avg:40.93ms
step:2006/2330 train_time:82106ms step_avg:40.93ms
step:2007/2330 train_time:82141ms step_avg:40.93ms
step:2008/2330 train_time:82186ms step_avg:40.93ms
step:2009/2330 train_time:82222ms step_avg:40.93ms
step:2010/2330 train_time:82268ms step_avg:40.93ms
step:2011/2330 train_time:82309ms step_avg:40.93ms
step:2012/2330 train_time:82357ms step_avg:40.93ms
step:2013/2330 train_time:82394ms step_avg:40.93ms
step:2014/2330 train_time:82439ms step_avg:40.93ms
step:2015/2330 train_time:82476ms step_avg:40.93ms
step:2016/2330 train_time:82521ms step_avg:40.93ms
step:2017/2330 train_time:82557ms step_avg:40.93ms
step:2018/2330 train_time:82602ms step_avg:40.93ms
step:2019/2330 train_time:82638ms step_avg:40.93ms
step:2020/2330 train_time:82683ms step_avg:40.93ms
step:2021/2330 train_time:82718ms step_avg:40.93ms
step:2022/2330 train_time:82762ms step_avg:40.93ms
step:2023/2330 train_time:82798ms step_avg:40.93ms
step:2024/2330 train_time:82843ms step_avg:40.93ms
step:2025/2330 train_time:82879ms step_avg:40.93ms
step:2026/2330 train_time:82924ms step_avg:40.93ms
step:2027/2330 train_time:82961ms step_avg:40.93ms
step:2028/2330 train_time:83006ms step_avg:40.93ms
step:2029/2330 train_time:83041ms step_avg:40.93ms
step:2030/2330 train_time:83086ms step_avg:40.93ms
step:2031/2330 train_time:83122ms step_avg:40.93ms
step:2032/2330 train_time:83166ms step_avg:40.93ms
step:2033/2330 train_time:83202ms step_avg:40.93ms
step:2034/2330 train_time:83249ms step_avg:40.93ms
step:2035/2330 train_time:83288ms step_avg:40.93ms
step:2036/2330 train_time:83336ms step_avg:40.93ms
step:2037/2330 train_time:83372ms step_avg:40.93ms
step:2038/2330 train_time:83418ms step_avg:40.93ms
step:2039/2330 train_time:83454ms step_avg:40.93ms
step:2040/2330 train_time:83500ms step_avg:40.93ms
step:2041/2330 train_time:83536ms step_avg:40.93ms
step:2042/2330 train_time:83581ms step_avg:40.93ms
step:2043/2330 train_time:83616ms step_avg:40.93ms
step:2044/2330 train_time:83661ms step_avg:40.93ms
step:2045/2330 train_time:83697ms step_avg:40.93ms
step:2046/2330 train_time:83742ms step_avg:40.93ms
step:2047/2330 train_time:83778ms step_avg:40.93ms
step:2048/2330 train_time:83823ms step_avg:40.93ms
step:2049/2330 train_time:83859ms step_avg:40.93ms
step:2050/2330 train_time:83904ms step_avg:40.93ms
step:2051/2330 train_time:83940ms step_avg:40.93ms
step:2052/2330 train_time:83985ms step_avg:40.93ms
step:2053/2330 train_time:84021ms step_avg:40.93ms
step:2054/2330 train_time:84067ms step_avg:40.93ms
step:2055/2330 train_time:84102ms step_avg:40.93ms
step:2056/2330 train_time:84147ms step_avg:40.93ms
step:2057/2330 train_time:84183ms step_avg:40.93ms
step:2058/2330 train_time:84231ms step_avg:40.93ms
step:2059/2330 train_time:84268ms step_avg:40.93ms
step:2060/2330 train_time:84315ms step_avg:40.93ms
step:2061/2330 train_time:84351ms step_avg:40.93ms
step:2062/2330 train_time:84396ms step_avg:40.93ms
step:2063/2330 train_time:84434ms step_avg:40.93ms
step:2064/2330 train_time:84479ms step_avg:40.93ms
step:2065/2330 train_time:84515ms step_avg:40.93ms
step:2066/2330 train_time:84560ms step_avg:40.93ms
step:2067/2330 train_time:84596ms step_avg:40.93ms
step:2068/2330 train_time:84640ms step_avg:40.93ms
step:2069/2330 train_time:84676ms step_avg:40.93ms
step:2070/2330 train_time:84721ms step_avg:40.93ms
step:2071/2330 train_time:84756ms step_avg:40.93ms
step:2072/2330 train_time:84801ms step_avg:40.93ms
step:2073/2330 train_time:84837ms step_avg:40.92ms
step:2074/2330 train_time:84883ms step_avg:40.93ms
step:2075/2330 train_time:84919ms step_avg:40.92ms
step:2076/2330 train_time:84964ms step_avg:40.93ms
step:2077/2330 train_time:84999ms step_avg:40.92ms
step:2078/2330 train_time:85045ms step_avg:40.93ms
step:2079/2330 train_time:85081ms step_avg:40.92ms
step:2080/2330 train_time:85125ms step_avg:40.93ms
step:2081/2330 train_time:85161ms step_avg:40.92ms
step:2082/2330 train_time:85208ms step_avg:40.93ms
step:2083/2330 train_time:85245ms step_avg:40.92ms
step:2084/2330 train_time:85291ms step_avg:40.93ms
step:2085/2330 train_time:85328ms step_avg:40.92ms
step:2086/2330 train_time:85374ms step_avg:40.93ms
step:2087/2330 train_time:85411ms step_avg:40.93ms
step:2088/2330 train_time:85457ms step_avg:40.93ms
step:2089/2330 train_time:85492ms step_avg:40.93ms
step:2090/2330 train_time:85537ms step_avg:40.93ms
step:2091/2330 train_time:85573ms step_avg:40.92ms
step:2092/2330 train_time:85618ms step_avg:40.93ms
step:2093/2330 train_time:85653ms step_avg:40.92ms
step:2094/2330 train_time:85698ms step_avg:40.93ms
step:2095/2330 train_time:85734ms step_avg:40.92ms
step:2096/2330 train_time:85779ms step_avg:40.93ms
step:2097/2330 train_time:85815ms step_avg:40.92ms
step:2098/2330 train_time:85861ms step_avg:40.92ms
step:2099/2330 train_time:85896ms step_avg:40.92ms
step:2100/2330 train_time:85942ms step_avg:40.92ms
step:2101/2330 train_time:85978ms step_avg:40.92ms
step:2102/2330 train_time:86023ms step_avg:40.92ms
step:2103/2330 train_time:86058ms step_avg:40.92ms
step:2104/2330 train_time:86104ms step_avg:40.92ms
step:2105/2330 train_time:86141ms step_avg:40.92ms
step:2106/2330 train_time:86186ms step_avg:40.92ms
step:2107/2330 train_time:86222ms step_avg:40.92ms
step:2108/2330 train_time:86268ms step_avg:40.92ms
step:2109/2330 train_time:86305ms step_avg:40.92ms
step:2110/2330 train_time:86351ms step_avg:40.92ms
step:2111/2330 train_time:86389ms step_avg:40.92ms
step:2112/2330 train_time:86436ms step_avg:40.93ms
step:2113/2330 train_time:86472ms step_avg:40.92ms
step:2114/2330 train_time:86517ms step_avg:40.93ms
step:2115/2330 train_time:86552ms step_avg:40.92ms
step:2116/2330 train_time:86597ms step_avg:40.92ms
step:2117/2330 train_time:86633ms step_avg:40.92ms
step:2118/2330 train_time:86678ms step_avg:40.92ms
step:2119/2330 train_time:86714ms step_avg:40.92ms
step:2120/2330 train_time:86759ms step_avg:40.92ms
step:2121/2330 train_time:86795ms step_avg:40.92ms
step:2122/2330 train_time:86841ms step_avg:40.92ms
step:2123/2330 train_time:86876ms step_avg:40.92ms
step:2124/2330 train_time:86921ms step_avg:40.92ms
step:2125/2330 train_time:86957ms step_avg:40.92ms
step:2126/2330 train_time:87002ms step_avg:40.92ms
step:2127/2330 train_time:87038ms step_avg:40.92ms
step:2128/2330 train_time:87084ms step_avg:40.92ms
step:2129/2330 train_time:87120ms step_avg:40.92ms
step:2130/2330 train_time:87166ms step_avg:40.92ms
step:2131/2330 train_time:87203ms step_avg:40.92ms
step:2132/2330 train_time:87249ms step_avg:40.92ms
step:2133/2330 train_time:87285ms step_avg:40.92ms
step:2134/2330 train_time:87331ms step_avg:40.92ms
step:2135/2330 train_time:87367ms step_avg:40.92ms
step:2136/2330 train_time:87413ms step_avg:40.92ms
step:2137/2330 train_time:87449ms step_avg:40.92ms
step:2138/2330 train_time:87494ms step_avg:40.92ms
step:2139/2330 train_time:87529ms step_avg:40.92ms
step:2140/2330 train_time:87574ms step_avg:40.92ms
step:2141/2330 train_time:87611ms step_avg:40.92ms
step:2142/2330 train_time:87657ms step_avg:40.92ms
step:2143/2330 train_time:87693ms step_avg:40.92ms
step:2144/2330 train_time:87739ms step_avg:40.92ms
step:2145/2330 train_time:87774ms step_avg:40.92ms
step:2146/2330 train_time:87819ms step_avg:40.92ms
step:2147/2330 train_time:87855ms step_avg:40.92ms
step:2148/2330 train_time:87900ms step_avg:40.92ms
step:2149/2330 train_time:87936ms step_avg:40.92ms
step:2150/2330 train_time:87982ms step_avg:40.92ms
step:2151/2330 train_time:88018ms step_avg:40.92ms
step:2152/2330 train_time:88064ms step_avg:40.92ms
step:2153/2330 train_time:88099ms step_avg:40.92ms
step:2154/2330 train_time:88145ms step_avg:40.92ms
step:2155/2330 train_time:88181ms step_avg:40.92ms
step:2156/2330 train_time:88227ms step_avg:40.92ms
step:2157/2330 train_time:88263ms step_avg:40.92ms
step:2158/2330 train_time:88310ms step_avg:40.92ms
step:2159/2330 train_time:88347ms step_avg:40.92ms
step:2160/2330 train_time:88392ms step_avg:40.92ms
step:2161/2330 train_time:88428ms step_avg:40.92ms
step:2162/2330 train_time:88474ms step_avg:40.92ms
step:2163/2330 train_time:88510ms step_avg:40.92ms
step:2164/2330 train_time:88555ms step_avg:40.92ms
step:2165/2330 train_time:88591ms step_avg:40.92ms
step:2166/2330 train_time:88637ms step_avg:40.92ms
step:2167/2330 train_time:88672ms step_avg:40.92ms
step:2168/2330 train_time:88717ms step_avg:40.92ms
step:2169/2330 train_time:88753ms step_avg:40.92ms
step:2170/2330 train_time:88799ms step_avg:40.92ms
step:2171/2330 train_time:88834ms step_avg:40.92ms
step:2172/2330 train_time:88879ms step_avg:40.92ms
step:2173/2330 train_time:88915ms step_avg:40.92ms
step:2174/2330 train_time:88961ms step_avg:40.92ms
step:2175/2330 train_time:88996ms step_avg:40.92ms
step:2176/2330 train_time:89042ms step_avg:40.92ms
step:2177/2330 train_time:89078ms step_avg:40.92ms
step:2178/2330 train_time:89123ms step_avg:40.92ms
step:2179/2330 train_time:89159ms step_avg:40.92ms
step:2180/2330 train_time:89205ms step_avg:40.92ms
step:2181/2330 train_time:89241ms step_avg:40.92ms
step:2182/2330 train_time:89287ms step_avg:40.92ms
step:2183/2330 train_time:89323ms step_avg:40.92ms
step:2184/2330 train_time:89370ms step_avg:40.92ms
step:2185/2330 train_time:89406ms step_avg:40.92ms
step:2186/2330 train_time:89451ms step_avg:40.92ms
step:2187/2330 train_time:89486ms step_avg:40.92ms
step:2188/2330 train_time:89532ms step_avg:40.92ms
step:2189/2330 train_time:89568ms step_avg:40.92ms
step:2190/2330 train_time:89613ms step_avg:40.92ms
step:2191/2330 train_time:89649ms step_avg:40.92ms
step:2192/2330 train_time:89694ms step_avg:40.92ms
step:2193/2330 train_time:89731ms step_avg:40.92ms
step:2194/2330 train_time:89777ms step_avg:40.92ms
step:2195/2330 train_time:89813ms step_avg:40.92ms
step:2196/2330 train_time:89859ms step_avg:40.92ms
step:2197/2330 train_time:89895ms step_avg:40.92ms
step:2198/2330 train_time:89941ms step_avg:40.92ms
step:2199/2330 train_time:89978ms step_avg:40.92ms
step:2200/2330 train_time:90023ms step_avg:40.92ms
step:2201/2330 train_time:90058ms step_avg:40.92ms
step:2202/2330 train_time:90103ms step_avg:40.92ms
step:2203/2330 train_time:90138ms step_avg:40.92ms
step:2204/2330 train_time:90184ms step_avg:40.92ms
step:2205/2330 train_time:90220ms step_avg:40.92ms
step:2206/2330 train_time:90265ms step_avg:40.92ms
step:2207/2330 train_time:90301ms step_avg:40.92ms
step:2208/2330 train_time:90346ms step_avg:40.92ms
step:2209/2330 train_time:90382ms step_avg:40.92ms
step:2210/2330 train_time:90428ms step_avg:40.92ms
step:2211/2330 train_time:90466ms step_avg:40.92ms
step:2212/2330 train_time:90511ms step_avg:40.92ms
step:2213/2330 train_time:90547ms step_avg:40.92ms
step:2214/2330 train_time:90592ms step_avg:40.92ms
step:2215/2330 train_time:90628ms step_avg:40.92ms
step:2216/2330 train_time:90674ms step_avg:40.92ms
step:2217/2330 train_time:90712ms step_avg:40.92ms
step:2218/2330 train_time:90757ms step_avg:40.92ms
step:2219/2330 train_time:90793ms step_avg:40.92ms
step:2220/2330 train_time:90838ms step_avg:40.92ms
step:2221/2330 train_time:90875ms step_avg:40.92ms
step:2222/2330 train_time:90920ms step_avg:40.92ms
step:2223/2330 train_time:90956ms step_avg:40.92ms
step:2224/2330 train_time:91001ms step_avg:40.92ms
step:2225/2330 train_time:91036ms step_avg:40.92ms
step:2226/2330 train_time:91082ms step_avg:40.92ms
step:2227/2330 train_time:91117ms step_avg:40.91ms
step:2228/2330 train_time:91163ms step_avg:40.92ms
step:2229/2330 train_time:91199ms step_avg:40.91ms
step:2230/2330 train_time:91244ms step_avg:40.92ms
step:2231/2330 train_time:91280ms step_avg:40.91ms
step:2232/2330 train_time:91326ms step_avg:40.92ms
step:2233/2330 train_time:91362ms step_avg:40.91ms
step:2234/2330 train_time:91408ms step_avg:40.92ms
step:2235/2330 train_time:91444ms step_avg:40.91ms
step:2236/2330 train_time:91490ms step_avg:40.92ms
step:2237/2330 train_time:91526ms step_avg:40.91ms
step:2238/2330 train_time:91572ms step_avg:40.92ms
step:2239/2330 train_time:91609ms step_avg:40.91ms
step:2240/2330 train_time:91654ms step_avg:40.92ms
step:2241/2330 train_time:91690ms step_avg:40.91ms
step:2242/2330 train_time:91735ms step_avg:40.92ms
step:2243/2330 train_time:91771ms step_avg:40.91ms
step:2244/2330 train_time:91816ms step_avg:40.92ms
step:2245/2330 train_time:91852ms step_avg:40.91ms
step:2246/2330 train_time:91898ms step_avg:40.92ms
step:2247/2330 train_time:91934ms step_avg:40.91ms
step:2248/2330 train_time:91980ms step_avg:40.92ms
step:2249/2330 train_time:92015ms step_avg:40.91ms
step:2250/2330 train_time:92061ms step_avg:40.92ms
step:2250/2330 val_loss:5.0816 train_time:92150ms step_avg:40.96ms
step:2251/2330 train_time:92163ms step_avg:40.94ms
step:2252/2330 train_time:92175ms step_avg:40.93ms
step:2253/2330 train_time:92186ms step_avg:40.92ms
step:2254/2330 train_time:92224ms step_avg:40.92ms
step:2255/2330 train_time:92258ms step_avg:40.91ms
step:2256/2330 train_time:92303ms step_avg:40.91ms
step:2257/2330 train_time:92338ms step_avg:40.91ms
step:2258/2330 train_time:92383ms step_avg:40.91ms
step:2259/2330 train_time:92418ms step_avg:40.91ms
step:2260/2330 train_time:92467ms step_avg:40.91ms
step:2261/2330 train_time:92505ms step_avg:40.91ms
step:2262/2330 train_time:92552ms step_avg:40.92ms
step:2263/2330 train_time:92589ms step_avg:40.91ms
step:2264/2330 train_time:92638ms step_avg:40.92ms
step:2265/2330 train_time:92674ms step_avg:40.92ms
step:2266/2330 train_time:92718ms step_avg:40.92ms
step:2267/2330 train_time:92752ms step_avg:40.91ms
step:2268/2330 train_time:92798ms step_avg:40.92ms
step:2269/2330 train_time:92834ms step_avg:40.91ms
step:2270/2330 train_time:92879ms step_avg:40.92ms
step:2271/2330 train_time:92914ms step_avg:40.91ms
step:2272/2330 train_time:92958ms step_avg:40.91ms
step:2273/2330 train_time:92994ms step_avg:40.91ms
step:2274/2330 train_time:93039ms step_avg:40.91ms
step:2275/2330 train_time:93075ms step_avg:40.91ms
step:2276/2330 train_time:93121ms step_avg:40.91ms
step:2277/2330 train_time:93157ms step_avg:40.91ms
step:2278/2330 train_time:93203ms step_avg:40.91ms
step:2279/2330 train_time:93238ms step_avg:40.91ms
step:2280/2330 train_time:93282ms step_avg:40.91ms
step:2281/2330 train_time:93318ms step_avg:40.91ms
step:2282/2330 train_time:93362ms step_avg:40.91ms
step:2283/2330 train_time:93398ms step_avg:40.91ms
step:2284/2330 train_time:93445ms step_avg:40.91ms
step:2285/2330 train_time:93481ms step_avg:40.91ms
step:2286/2330 train_time:93529ms step_avg:40.91ms
step:2287/2330 train_time:93566ms step_avg:40.91ms
step:2288/2330 train_time:93612ms step_avg:40.91ms
step:2289/2330 train_time:93648ms step_avg:40.91ms
step:2290/2330 train_time:93694ms step_avg:40.91ms
step:2291/2330 train_time:93730ms step_avg:40.91ms
step:2292/2330 train_time:93775ms step_avg:40.91ms
step:2293/2330 train_time:93811ms step_avg:40.91ms
step:2294/2330 train_time:93856ms step_avg:40.91ms
step:2295/2330 train_time:93892ms step_avg:40.91ms
step:2296/2330 train_time:93937ms step_avg:40.91ms
step:2297/2330 train_time:93973ms step_avg:40.91ms
step:2298/2330 train_time:94018ms step_avg:40.91ms
step:2299/2330 train_time:94054ms step_avg:40.91ms
step:2300/2330 train_time:94099ms step_avg:40.91ms
step:2301/2330 train_time:94134ms step_avg:40.91ms
step:2302/2330 train_time:94180ms step_avg:40.91ms
step:2303/2330 train_time:94214ms step_avg:40.91ms
step:2304/2330 train_time:94260ms step_avg:40.91ms
step:2305/2330 train_time:94296ms step_avg:40.91ms
step:2306/2330 train_time:94341ms step_avg:40.91ms
step:2307/2330 train_time:94378ms step_avg:40.91ms
step:2308/2330 train_time:94424ms step_avg:40.91ms
step:2309/2330 train_time:94459ms step_avg:40.91ms
step:2310/2330 train_time:94505ms step_avg:40.91ms
step:2311/2330 train_time:94541ms step_avg:40.91ms
step:2312/2330 train_time:94588ms step_avg:40.91ms
step:2313/2330 train_time:94624ms step_avg:40.91ms
step:2314/2330 train_time:94670ms step_avg:40.91ms
step:2315/2330 train_time:94706ms step_avg:40.91ms
step:2316/2330 train_time:94750ms step_avg:40.91ms
step:2317/2330 train_time:94786ms step_avg:40.91ms
step:2318/2330 train_time:94831ms step_avg:40.91ms
step:2319/2330 train_time:94867ms step_avg:40.91ms
step:2320/2330 train_time:94913ms step_avg:40.91ms
step:2321/2330 train_time:94950ms step_avg:40.91ms
step:2322/2330 train_time:94996ms step_avg:40.91ms
step:2323/2330 train_time:95032ms step_avg:40.91ms
step:2324/2330 train_time:95078ms step_avg:40.91ms
step:2325/2330 train_time:95113ms step_avg:40.91ms
step:2326/2330 train_time:95158ms step_avg:40.91ms
step:2327/2330 train_time:95193ms step_avg:40.91ms
step:2328/2330 train_time:95238ms step_avg:40.91ms
step:2329/2330 train_time:95275ms step_avg:40.91ms
step:2330/2330 train_time:95320ms step_avg:40.91ms
step:2330/2330 val_loss:5.0750 train_time:95410ms step_avg:40.95ms
peak memory allocated: 29226 MiB reserved: 38888 MiB
