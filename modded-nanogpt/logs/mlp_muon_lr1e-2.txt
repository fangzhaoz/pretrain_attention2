import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_muon_lr1e-2"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-2, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 04:11:12 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   33C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   31C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   31C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   32C    P0             115W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.06ms
step:1/2330 train_time:84ms step_avg:83.90ms
step:2/2330 train_time:158ms step_avg:78.81ms
step:3/2330 train_time:170ms step_avg:56.63ms
step:4/2330 train_time:183ms step_avg:45.68ms
step:5/2330 train_time:193ms step_avg:38.65ms
step:6/2330 train_time:232ms step_avg:38.71ms
step:7/2330 train_time:266ms step_avg:38.00ms
step:8/2330 train_time:309ms step_avg:38.68ms
step:9/2330 train_time:344ms step_avg:38.25ms
step:10/2330 train_time:389ms step_avg:38.85ms
step:11/2330 train_time:423ms step_avg:38.44ms
step:12/2330 train_time:467ms step_avg:38.88ms
step:13/2330 train_time:501ms step_avg:38.55ms
step:14/2330 train_time:546ms step_avg:39.02ms
step:15/2330 train_time:581ms step_avg:38.71ms
step:16/2330 train_time:624ms step_avg:39.03ms
step:17/2330 train_time:659ms step_avg:38.76ms
step:18/2330 train_time:704ms step_avg:39.09ms
step:19/2330 train_time:738ms step_avg:38.86ms
step:20/2330 train_time:782ms step_avg:39.12ms
step:21/2330 train_time:817ms step_avg:38.91ms
step:22/2330 train_time:861ms step_avg:39.16ms
step:23/2330 train_time:896ms step_avg:38.98ms
step:24/2330 train_time:940ms step_avg:39.18ms
step:25/2330 train_time:976ms step_avg:39.04ms
step:26/2330 train_time:1022ms step_avg:39.32ms
step:27/2330 train_time:1060ms step_avg:39.27ms
step:28/2330 train_time:1111ms step_avg:39.69ms
step:29/2330 train_time:1150ms step_avg:39.64ms
step:30/2330 train_time:1196ms step_avg:39.85ms
step:31/2330 train_time:1232ms step_avg:39.74ms
step:32/2330 train_time:1277ms step_avg:39.91ms
step:33/2330 train_time:1312ms step_avg:39.76ms
step:34/2330 train_time:1357ms step_avg:39.90ms
step:35/2330 train_time:1392ms step_avg:39.76ms
step:36/2330 train_time:1437ms step_avg:39.91ms
step:37/2330 train_time:1471ms step_avg:39.77ms
step:38/2330 train_time:1516ms step_avg:39.90ms
step:39/2330 train_time:1551ms step_avg:39.78ms
step:40/2330 train_time:1595ms step_avg:39.89ms
step:41/2330 train_time:1631ms step_avg:39.77ms
step:42/2330 train_time:1675ms step_avg:39.89ms
step:43/2330 train_time:1710ms step_avg:39.77ms
step:44/2330 train_time:1755ms step_avg:39.89ms
step:45/2330 train_time:1790ms step_avg:39.78ms
step:46/2330 train_time:1834ms step_avg:39.88ms
step:47/2330 train_time:1870ms step_avg:39.78ms
step:48/2330 train_time:1914ms step_avg:39.88ms
step:49/2330 train_time:1950ms step_avg:39.80ms
step:50/2330 train_time:1996ms step_avg:39.92ms
step:51/2330 train_time:2031ms step_avg:39.82ms
step:52/2330 train_time:2077ms step_avg:39.94ms
step:53/2330 train_time:2113ms step_avg:39.87ms
step:54/2330 train_time:2160ms step_avg:39.99ms
step:55/2330 train_time:2195ms step_avg:39.91ms
step:56/2330 train_time:2241ms step_avg:40.02ms
step:57/2330 train_time:2278ms step_avg:39.96ms
step:58/2330 train_time:2323ms step_avg:40.06ms
step:59/2330 train_time:2360ms step_avg:40.00ms
step:60/2330 train_time:2405ms step_avg:40.09ms
step:61/2330 train_time:2442ms step_avg:40.03ms
step:62/2330 train_time:2488ms step_avg:40.12ms
step:63/2330 train_time:2523ms step_avg:40.04ms
step:64/2330 train_time:2568ms step_avg:40.12ms
step:65/2330 train_time:2604ms step_avg:40.06ms
step:66/2330 train_time:2649ms step_avg:40.14ms
step:67/2330 train_time:2684ms step_avg:40.06ms
step:68/2330 train_time:2728ms step_avg:40.12ms
step:69/2330 train_time:2764ms step_avg:40.05ms
step:70/2330 train_time:2809ms step_avg:40.13ms
step:71/2330 train_time:2844ms step_avg:40.06ms
step:72/2330 train_time:2889ms step_avg:40.13ms
step:73/2330 train_time:2924ms step_avg:40.06ms
step:74/2330 train_time:2969ms step_avg:40.12ms
step:75/2330 train_time:3005ms step_avg:40.07ms
step:76/2330 train_time:3052ms step_avg:40.16ms
step:77/2330 train_time:3088ms step_avg:40.11ms
step:78/2330 train_time:3135ms step_avg:40.19ms
step:79/2330 train_time:3170ms step_avg:40.13ms
step:80/2330 train_time:3215ms step_avg:40.19ms
step:81/2330 train_time:3251ms step_avg:40.14ms
step:82/2330 train_time:3296ms step_avg:40.19ms
step:83/2330 train_time:3331ms step_avg:40.13ms
step:84/2330 train_time:3376ms step_avg:40.19ms
step:85/2330 train_time:3411ms step_avg:40.13ms
step:86/2330 train_time:3456ms step_avg:40.19ms
step:87/2330 train_time:3491ms step_avg:40.13ms
step:88/2330 train_time:3536ms step_avg:40.18ms
step:89/2330 train_time:3571ms step_avg:40.13ms
step:90/2330 train_time:3616ms step_avg:40.18ms
step:91/2330 train_time:3652ms step_avg:40.13ms
step:92/2330 train_time:3697ms step_avg:40.19ms
step:93/2330 train_time:3733ms step_avg:40.14ms
step:94/2330 train_time:3777ms step_avg:40.19ms
step:95/2330 train_time:3813ms step_avg:40.14ms
step:96/2330 train_time:3857ms step_avg:40.18ms
step:97/2330 train_time:3893ms step_avg:40.13ms
step:98/2330 train_time:3938ms step_avg:40.18ms
step:99/2330 train_time:3974ms step_avg:40.14ms
step:100/2330 train_time:4019ms step_avg:40.19ms
step:101/2330 train_time:4055ms step_avg:40.15ms
step:102/2330 train_time:4100ms step_avg:40.19ms
step:103/2330 train_time:4136ms step_avg:40.15ms
step:104/2330 train_time:4181ms step_avg:40.20ms
step:105/2330 train_time:4216ms step_avg:40.15ms
step:106/2330 train_time:4261ms step_avg:40.20ms
step:107/2330 train_time:4297ms step_avg:40.16ms
step:108/2330 train_time:4343ms step_avg:40.21ms
step:109/2330 train_time:4379ms step_avg:40.17ms
step:110/2330 train_time:4424ms step_avg:40.22ms
step:111/2330 train_time:4459ms step_avg:40.18ms
step:112/2330 train_time:4505ms step_avg:40.22ms
step:113/2330 train_time:4540ms step_avg:40.18ms
step:114/2330 train_time:4587ms step_avg:40.23ms
step:115/2330 train_time:4621ms step_avg:40.19ms
step:116/2330 train_time:4666ms step_avg:40.23ms
step:117/2330 train_time:4703ms step_avg:40.20ms
step:118/2330 train_time:4748ms step_avg:40.24ms
step:119/2330 train_time:4784ms step_avg:40.20ms
step:120/2330 train_time:4829ms step_avg:40.24ms
step:121/2330 train_time:4865ms step_avg:40.21ms
step:122/2330 train_time:4910ms step_avg:40.25ms
step:123/2330 train_time:4947ms step_avg:40.22ms
step:124/2330 train_time:4992ms step_avg:40.26ms
step:125/2330 train_time:5028ms step_avg:40.22ms
step:126/2330 train_time:5073ms step_avg:40.26ms
step:127/2330 train_time:5108ms step_avg:40.22ms
step:128/2330 train_time:5153ms step_avg:40.25ms
step:129/2330 train_time:5189ms step_avg:40.22ms
step:130/2330 train_time:5235ms step_avg:40.27ms
step:131/2330 train_time:5270ms step_avg:40.23ms
step:132/2330 train_time:5314ms step_avg:40.26ms
step:133/2330 train_time:5350ms step_avg:40.23ms
step:134/2330 train_time:5395ms step_avg:40.26ms
step:135/2330 train_time:5430ms step_avg:40.22ms
step:136/2330 train_time:5475ms step_avg:40.26ms
step:137/2330 train_time:5510ms step_avg:40.22ms
step:138/2330 train_time:5555ms step_avg:40.26ms
step:139/2330 train_time:5590ms step_avg:40.22ms
step:140/2330 train_time:5636ms step_avg:40.26ms
step:141/2330 train_time:5671ms step_avg:40.22ms
step:142/2330 train_time:5717ms step_avg:40.26ms
step:143/2330 train_time:5752ms step_avg:40.22ms
step:144/2330 train_time:5798ms step_avg:40.26ms
step:145/2330 train_time:5834ms step_avg:40.23ms
step:146/2330 train_time:5880ms step_avg:40.28ms
step:147/2330 train_time:5916ms step_avg:40.25ms
step:148/2330 train_time:5961ms step_avg:40.28ms
step:149/2330 train_time:5997ms step_avg:40.25ms
step:150/2330 train_time:6042ms step_avg:40.28ms
step:151/2330 train_time:6078ms step_avg:40.25ms
step:152/2330 train_time:6123ms step_avg:40.28ms
step:153/2330 train_time:6158ms step_avg:40.25ms
step:154/2330 train_time:6205ms step_avg:40.29ms
step:155/2330 train_time:6241ms step_avg:40.26ms
step:156/2330 train_time:6287ms step_avg:40.30ms
step:157/2330 train_time:6322ms step_avg:40.27ms
step:158/2330 train_time:6367ms step_avg:40.30ms
step:159/2330 train_time:6403ms step_avg:40.27ms
step:160/2330 train_time:6449ms step_avg:40.30ms
step:161/2330 train_time:6485ms step_avg:40.28ms
step:162/2330 train_time:6529ms step_avg:40.30ms
step:163/2330 train_time:6566ms step_avg:40.28ms
step:164/2330 train_time:6611ms step_avg:40.31ms
step:165/2330 train_time:6646ms step_avg:40.28ms
step:166/2330 train_time:6691ms step_avg:40.31ms
step:167/2330 train_time:6727ms step_avg:40.28ms
step:168/2330 train_time:6773ms step_avg:40.31ms
step:169/2330 train_time:6808ms step_avg:40.28ms
step:170/2330 train_time:6853ms step_avg:40.31ms
step:171/2330 train_time:6888ms step_avg:40.28ms
step:172/2330 train_time:6933ms step_avg:40.31ms
step:173/2330 train_time:6968ms step_avg:40.28ms
step:174/2330 train_time:7013ms step_avg:40.30ms
step:175/2330 train_time:7048ms step_avg:40.27ms
step:176/2330 train_time:7093ms step_avg:40.30ms
step:177/2330 train_time:7128ms step_avg:40.27ms
step:178/2330 train_time:7173ms step_avg:40.30ms
step:179/2330 train_time:7209ms step_avg:40.27ms
step:180/2330 train_time:7255ms step_avg:40.31ms
step:181/2330 train_time:7290ms step_avg:40.28ms
step:182/2330 train_time:7336ms step_avg:40.31ms
step:183/2330 train_time:7371ms step_avg:40.28ms
step:184/2330 train_time:7417ms step_avg:40.31ms
step:185/2330 train_time:7452ms step_avg:40.28ms
step:186/2330 train_time:7496ms step_avg:40.30ms
step:187/2330 train_time:7533ms step_avg:40.28ms
step:188/2330 train_time:7579ms step_avg:40.31ms
step:189/2330 train_time:7614ms step_avg:40.29ms
step:190/2330 train_time:7659ms step_avg:40.31ms
step:191/2330 train_time:7695ms step_avg:40.29ms
step:192/2330 train_time:7741ms step_avg:40.32ms
step:193/2330 train_time:7776ms step_avg:40.29ms
step:194/2330 train_time:7821ms step_avg:40.32ms
step:195/2330 train_time:7857ms step_avg:40.29ms
step:196/2330 train_time:7902ms step_avg:40.32ms
step:197/2330 train_time:7937ms step_avg:40.29ms
step:198/2330 train_time:7982ms step_avg:40.31ms
step:199/2330 train_time:8017ms step_avg:40.29ms
step:200/2330 train_time:8063ms step_avg:40.31ms
step:201/2330 train_time:8099ms step_avg:40.29ms
step:202/2330 train_time:8145ms step_avg:40.32ms
step:203/2330 train_time:8181ms step_avg:40.30ms
step:204/2330 train_time:8226ms step_avg:40.32ms
step:205/2330 train_time:8262ms step_avg:40.30ms
step:206/2330 train_time:8308ms step_avg:40.33ms
step:207/2330 train_time:8344ms step_avg:40.31ms
step:208/2330 train_time:8389ms step_avg:40.33ms
step:209/2330 train_time:8426ms step_avg:40.31ms
step:210/2330 train_time:8471ms step_avg:40.34ms
step:211/2330 train_time:8506ms step_avg:40.31ms
step:212/2330 train_time:8552ms step_avg:40.34ms
step:213/2330 train_time:8587ms step_avg:40.31ms
step:214/2330 train_time:8633ms step_avg:40.34ms
step:215/2330 train_time:8668ms step_avg:40.31ms
step:216/2330 train_time:8713ms step_avg:40.34ms
step:217/2330 train_time:8749ms step_avg:40.32ms
step:218/2330 train_time:8794ms step_avg:40.34ms
step:219/2330 train_time:8829ms step_avg:40.31ms
step:220/2330 train_time:8873ms step_avg:40.33ms
step:221/2330 train_time:8909ms step_avg:40.31ms
step:222/2330 train_time:8954ms step_avg:40.33ms
step:223/2330 train_time:8989ms step_avg:40.31ms
step:224/2330 train_time:9034ms step_avg:40.33ms
step:225/2330 train_time:9069ms step_avg:40.31ms
step:226/2330 train_time:9115ms step_avg:40.33ms
step:227/2330 train_time:9151ms step_avg:40.31ms
step:228/2330 train_time:9196ms step_avg:40.33ms
step:229/2330 train_time:9232ms step_avg:40.31ms
step:230/2330 train_time:9278ms step_avg:40.34ms
step:231/2330 train_time:9313ms step_avg:40.32ms
step:232/2330 train_time:9359ms step_avg:40.34ms
step:233/2330 train_time:9395ms step_avg:40.32ms
step:234/2330 train_time:9440ms step_avg:40.34ms
step:235/2330 train_time:9476ms step_avg:40.32ms
step:236/2330 train_time:9521ms step_avg:40.35ms
step:237/2330 train_time:9557ms step_avg:40.33ms
step:238/2330 train_time:9603ms step_avg:40.35ms
step:239/2330 train_time:9640ms step_avg:40.34ms
step:240/2330 train_time:9686ms step_avg:40.36ms
step:241/2330 train_time:9722ms step_avg:40.34ms
step:242/2330 train_time:9767ms step_avg:40.36ms
step:243/2330 train_time:9803ms step_avg:40.34ms
step:244/2330 train_time:9848ms step_avg:40.36ms
step:245/2330 train_time:9885ms step_avg:40.34ms
step:246/2330 train_time:9929ms step_avg:40.36ms
step:247/2330 train_time:9964ms step_avg:40.34ms
step:248/2330 train_time:10009ms step_avg:40.36ms
step:249/2330 train_time:10045ms step_avg:40.34ms
step:250/2330 train_time:10090ms step_avg:40.36ms
step:250/2330 val_loss:5.4081 train_time:10177ms step_avg:40.71ms
step:251/2330 train_time:10191ms step_avg:40.60ms
step:252/2330 train_time:10202ms step_avg:40.49ms
step:253/2330 train_time:10213ms step_avg:40.37ms
step:254/2330 train_time:10251ms step_avg:40.36ms
step:255/2330 train_time:10285ms step_avg:40.33ms
step:256/2330 train_time:10329ms step_avg:40.35ms
step:257/2330 train_time:10365ms step_avg:40.33ms
step:258/2330 train_time:10409ms step_avg:40.34ms
step:259/2330 train_time:10444ms step_avg:40.32ms
step:260/2330 train_time:10494ms step_avg:40.36ms
step:261/2330 train_time:10533ms step_avg:40.36ms
step:262/2330 train_time:10581ms step_avg:40.38ms
step:263/2330 train_time:10617ms step_avg:40.37ms
step:264/2330 train_time:10662ms step_avg:40.39ms
step:265/2330 train_time:10697ms step_avg:40.37ms
step:266/2330 train_time:10742ms step_avg:40.38ms
step:267/2330 train_time:10778ms step_avg:40.37ms
step:268/2330 train_time:10823ms step_avg:40.38ms
step:269/2330 train_time:10858ms step_avg:40.37ms
step:270/2330 train_time:10903ms step_avg:40.38ms
step:271/2330 train_time:10938ms step_avg:40.36ms
step:272/2330 train_time:10983ms step_avg:40.38ms
step:273/2330 train_time:11019ms step_avg:40.36ms
step:274/2330 train_time:11063ms step_avg:40.38ms
step:275/2330 train_time:11098ms step_avg:40.36ms
step:276/2330 train_time:11143ms step_avg:40.37ms
step:277/2330 train_time:11179ms step_avg:40.36ms
step:278/2330 train_time:11224ms step_avg:40.37ms
step:279/2330 train_time:11259ms step_avg:40.36ms
step:280/2330 train_time:11304ms step_avg:40.37ms
step:281/2330 train_time:11339ms step_avg:40.35ms
step:282/2330 train_time:11384ms step_avg:40.37ms
step:283/2330 train_time:11420ms step_avg:40.35ms
step:284/2330 train_time:11465ms step_avg:40.37ms
step:285/2330 train_time:11501ms step_avg:40.36ms
step:286/2330 train_time:11550ms step_avg:40.39ms
step:287/2330 train_time:11587ms step_avg:40.37ms
step:288/2330 train_time:11634ms step_avg:40.40ms
step:289/2330 train_time:11669ms step_avg:40.38ms
step:290/2330 train_time:11715ms step_avg:40.40ms
step:291/2330 train_time:11751ms step_avg:40.38ms
step:292/2330 train_time:11796ms step_avg:40.40ms
step:293/2330 train_time:11831ms step_avg:40.38ms
step:294/2330 train_time:11876ms step_avg:40.40ms
step:295/2330 train_time:11911ms step_avg:40.38ms
step:296/2330 train_time:11956ms step_avg:40.39ms
step:297/2330 train_time:11992ms step_avg:40.38ms
step:298/2330 train_time:12037ms step_avg:40.39ms
step:299/2330 train_time:12072ms step_avg:40.38ms
step:300/2330 train_time:12117ms step_avg:40.39ms
step:301/2330 train_time:12153ms step_avg:40.37ms
step:302/2330 train_time:12197ms step_avg:40.39ms
step:303/2330 train_time:12232ms step_avg:40.37ms
step:304/2330 train_time:12277ms step_avg:40.39ms
step:305/2330 train_time:12312ms step_avg:40.37ms
step:306/2330 train_time:12356ms step_avg:40.38ms
step:307/2330 train_time:12392ms step_avg:40.37ms
step:308/2330 train_time:12439ms step_avg:40.39ms
step:309/2330 train_time:12475ms step_avg:40.37ms
step:310/2330 train_time:12521ms step_avg:40.39ms
step:311/2330 train_time:12557ms step_avg:40.38ms
step:312/2330 train_time:12603ms step_avg:40.39ms
step:313/2330 train_time:12639ms step_avg:40.38ms
step:314/2330 train_time:12683ms step_avg:40.39ms
step:315/2330 train_time:12719ms step_avg:40.38ms
step:316/2330 train_time:12765ms step_avg:40.39ms
step:317/2330 train_time:12801ms step_avg:40.38ms
step:318/2330 train_time:12846ms step_avg:40.40ms
step:319/2330 train_time:12881ms step_avg:40.38ms
step:320/2330 train_time:12926ms step_avg:40.39ms
step:321/2330 train_time:12961ms step_avg:40.38ms
step:322/2330 train_time:13006ms step_avg:40.39ms
step:323/2330 train_time:13041ms step_avg:40.38ms
step:324/2330 train_time:13086ms step_avg:40.39ms
step:325/2330 train_time:13121ms step_avg:40.37ms
step:326/2330 train_time:13165ms step_avg:40.38ms
step:327/2330 train_time:13202ms step_avg:40.37ms
step:328/2330 train_time:13247ms step_avg:40.39ms
step:329/2330 train_time:13282ms step_avg:40.37ms
step:330/2330 train_time:13326ms step_avg:40.38ms
step:331/2330 train_time:13362ms step_avg:40.37ms
step:332/2330 train_time:13409ms step_avg:40.39ms
step:333/2330 train_time:13445ms step_avg:40.38ms
step:334/2330 train_time:13492ms step_avg:40.39ms
step:335/2330 train_time:13527ms step_avg:40.38ms
step:336/2330 train_time:13572ms step_avg:40.39ms
step:337/2330 train_time:13608ms step_avg:40.38ms
step:338/2330 train_time:13653ms step_avg:40.39ms
step:339/2330 train_time:13688ms step_avg:40.38ms
step:340/2330 train_time:13733ms step_avg:40.39ms
step:341/2330 train_time:13768ms step_avg:40.38ms
step:342/2330 train_time:13813ms step_avg:40.39ms
step:343/2330 train_time:13848ms step_avg:40.37ms
step:344/2330 train_time:13892ms step_avg:40.39ms
step:345/2330 train_time:13928ms step_avg:40.37ms
step:346/2330 train_time:13973ms step_avg:40.38ms
step:347/2330 train_time:14008ms step_avg:40.37ms
step:348/2330 train_time:14054ms step_avg:40.38ms
step:349/2330 train_time:14089ms step_avg:40.37ms
step:350/2330 train_time:14133ms step_avg:40.38ms
step:351/2330 train_time:14168ms step_avg:40.37ms
step:352/2330 train_time:14213ms step_avg:40.38ms
step:353/2330 train_time:14249ms step_avg:40.36ms
step:354/2330 train_time:14293ms step_avg:40.38ms
step:355/2330 train_time:14329ms step_avg:40.36ms
step:356/2330 train_time:14374ms step_avg:40.38ms
step:357/2330 train_time:14409ms step_avg:40.36ms
step:358/2330 train_time:14454ms step_avg:40.38ms
step:359/2330 train_time:14490ms step_avg:40.36ms
step:360/2330 train_time:14535ms step_avg:40.38ms
step:361/2330 train_time:14570ms step_avg:40.36ms
step:362/2330 train_time:14615ms step_avg:40.37ms
step:363/2330 train_time:14650ms step_avg:40.36ms
step:364/2330 train_time:14694ms step_avg:40.37ms
step:365/2330 train_time:14730ms step_avg:40.36ms
step:366/2330 train_time:14775ms step_avg:40.37ms
step:367/2330 train_time:14810ms step_avg:40.36ms
step:368/2330 train_time:14856ms step_avg:40.37ms
step:369/2330 train_time:14891ms step_avg:40.35ms
step:370/2330 train_time:14936ms step_avg:40.37ms
step:371/2330 train_time:14971ms step_avg:40.35ms
step:372/2330 train_time:15015ms step_avg:40.36ms
step:373/2330 train_time:15051ms step_avg:40.35ms
step:374/2330 train_time:15096ms step_avg:40.36ms
step:375/2330 train_time:15131ms step_avg:40.35ms
step:376/2330 train_time:15175ms step_avg:40.36ms
step:377/2330 train_time:15211ms step_avg:40.35ms
step:378/2330 train_time:15256ms step_avg:40.36ms
step:379/2330 train_time:15292ms step_avg:40.35ms
step:380/2330 train_time:15337ms step_avg:40.36ms
step:381/2330 train_time:15373ms step_avg:40.35ms
step:382/2330 train_time:15418ms step_avg:40.36ms
step:383/2330 train_time:15454ms step_avg:40.35ms
step:384/2330 train_time:15499ms step_avg:40.36ms
step:385/2330 train_time:15534ms step_avg:40.35ms
step:386/2330 train_time:15580ms step_avg:40.36ms
step:387/2330 train_time:15615ms step_avg:40.35ms
step:388/2330 train_time:15659ms step_avg:40.36ms
step:389/2330 train_time:15694ms step_avg:40.34ms
step:390/2330 train_time:15738ms step_avg:40.35ms
step:391/2330 train_time:15774ms step_avg:40.34ms
step:392/2330 train_time:15819ms step_avg:40.35ms
step:393/2330 train_time:15854ms step_avg:40.34ms
step:394/2330 train_time:15899ms step_avg:40.35ms
step:395/2330 train_time:15935ms step_avg:40.34ms
step:396/2330 train_time:15980ms step_avg:40.35ms
step:397/2330 train_time:16015ms step_avg:40.34ms
step:398/2330 train_time:16060ms step_avg:40.35ms
step:399/2330 train_time:16095ms step_avg:40.34ms
step:400/2330 train_time:16140ms step_avg:40.35ms
step:401/2330 train_time:16175ms step_avg:40.34ms
step:402/2330 train_time:16220ms step_avg:40.35ms
step:403/2330 train_time:16256ms step_avg:40.34ms
step:404/2330 train_time:16301ms step_avg:40.35ms
step:405/2330 train_time:16338ms step_avg:40.34ms
step:406/2330 train_time:16383ms step_avg:40.35ms
step:407/2330 train_time:16418ms step_avg:40.34ms
step:408/2330 train_time:16463ms step_avg:40.35ms
step:409/2330 train_time:16499ms step_avg:40.34ms
step:410/2330 train_time:16543ms step_avg:40.35ms
step:411/2330 train_time:16579ms step_avg:40.34ms
step:412/2330 train_time:16623ms step_avg:40.35ms
step:413/2330 train_time:16659ms step_avg:40.34ms
step:414/2330 train_time:16705ms step_avg:40.35ms
step:415/2330 train_time:16741ms step_avg:40.34ms
step:416/2330 train_time:16786ms step_avg:40.35ms
step:417/2330 train_time:16821ms step_avg:40.34ms
step:418/2330 train_time:16867ms step_avg:40.35ms
step:419/2330 train_time:16904ms step_avg:40.34ms
step:420/2330 train_time:16950ms step_avg:40.36ms
step:421/2330 train_time:16985ms step_avg:40.34ms
step:422/2330 train_time:17030ms step_avg:40.36ms
step:423/2330 train_time:17066ms step_avg:40.35ms
step:424/2330 train_time:17111ms step_avg:40.36ms
step:425/2330 train_time:17147ms step_avg:40.35ms
step:426/2330 train_time:17193ms step_avg:40.36ms
step:427/2330 train_time:17228ms step_avg:40.35ms
step:428/2330 train_time:17274ms step_avg:40.36ms
step:429/2330 train_time:17309ms step_avg:40.35ms
step:430/2330 train_time:17354ms step_avg:40.36ms
step:431/2330 train_time:17389ms step_avg:40.35ms
step:432/2330 train_time:17434ms step_avg:40.36ms
step:433/2330 train_time:17470ms step_avg:40.35ms
step:434/2330 train_time:17514ms step_avg:40.36ms
step:435/2330 train_time:17550ms step_avg:40.34ms
step:436/2330 train_time:17594ms step_avg:40.35ms
step:437/2330 train_time:17630ms step_avg:40.34ms
step:438/2330 train_time:17675ms step_avg:40.35ms
step:439/2330 train_time:17710ms step_avg:40.34ms
step:440/2330 train_time:17755ms step_avg:40.35ms
step:441/2330 train_time:17790ms step_avg:40.34ms
step:442/2330 train_time:17835ms step_avg:40.35ms
step:443/2330 train_time:17870ms step_avg:40.34ms
step:444/2330 train_time:17915ms step_avg:40.35ms
step:445/2330 train_time:17950ms step_avg:40.34ms
step:446/2330 train_time:17995ms step_avg:40.35ms
step:447/2330 train_time:18030ms step_avg:40.33ms
step:448/2330 train_time:18074ms step_avg:40.34ms
step:449/2330 train_time:18110ms step_avg:40.33ms
step:450/2330 train_time:18154ms step_avg:40.34ms
step:451/2330 train_time:18190ms step_avg:40.33ms
step:452/2330 train_time:18234ms step_avg:40.34ms
step:453/2330 train_time:18269ms step_avg:40.33ms
step:454/2330 train_time:18315ms step_avg:40.34ms
step:455/2330 train_time:18350ms step_avg:40.33ms
step:456/2330 train_time:18395ms step_avg:40.34ms
step:457/2330 train_time:18430ms step_avg:40.33ms
step:458/2330 train_time:18475ms step_avg:40.34ms
step:459/2330 train_time:18511ms step_avg:40.33ms
step:460/2330 train_time:18557ms step_avg:40.34ms
step:461/2330 train_time:18592ms step_avg:40.33ms
step:462/2330 train_time:18636ms step_avg:40.34ms
step:463/2330 train_time:18672ms step_avg:40.33ms
step:464/2330 train_time:18717ms step_avg:40.34ms
step:465/2330 train_time:18752ms step_avg:40.33ms
step:466/2330 train_time:18797ms step_avg:40.34ms
step:467/2330 train_time:18832ms step_avg:40.33ms
step:468/2330 train_time:18877ms step_avg:40.34ms
step:469/2330 train_time:18913ms step_avg:40.33ms
step:470/2330 train_time:18958ms step_avg:40.34ms
step:471/2330 train_time:18993ms step_avg:40.33ms
step:472/2330 train_time:19038ms step_avg:40.34ms
step:473/2330 train_time:19074ms step_avg:40.33ms
step:474/2330 train_time:19119ms step_avg:40.33ms
step:475/2330 train_time:19154ms step_avg:40.32ms
step:476/2330 train_time:19199ms step_avg:40.33ms
step:477/2330 train_time:19235ms step_avg:40.32ms
step:478/2330 train_time:19280ms step_avg:40.34ms
step:479/2330 train_time:19316ms step_avg:40.33ms
step:480/2330 train_time:19361ms step_avg:40.34ms
step:481/2330 train_time:19397ms step_avg:40.33ms
step:482/2330 train_time:19442ms step_avg:40.34ms
step:483/2330 train_time:19478ms step_avg:40.33ms
step:484/2330 train_time:19523ms step_avg:40.34ms
step:485/2330 train_time:19559ms step_avg:40.33ms
step:486/2330 train_time:19605ms step_avg:40.34ms
step:487/2330 train_time:19640ms step_avg:40.33ms
step:488/2330 train_time:19685ms step_avg:40.34ms
step:489/2330 train_time:19721ms step_avg:40.33ms
step:490/2330 train_time:19766ms step_avg:40.34ms
step:491/2330 train_time:19802ms step_avg:40.33ms
step:492/2330 train_time:19847ms step_avg:40.34ms
step:493/2330 train_time:19883ms step_avg:40.33ms
step:494/2330 train_time:19928ms step_avg:40.34ms
step:495/2330 train_time:19963ms step_avg:40.33ms
step:496/2330 train_time:20008ms step_avg:40.34ms
step:497/2330 train_time:20044ms step_avg:40.33ms
step:498/2330 train_time:20089ms step_avg:40.34ms
step:499/2330 train_time:20124ms step_avg:40.33ms
step:500/2330 train_time:20171ms step_avg:40.34ms
step:500/2330 val_loss:5.2866 train_time:20258ms step_avg:40.52ms
step:501/2330 train_time:20271ms step_avg:40.46ms
step:502/2330 train_time:20283ms step_avg:40.40ms
step:503/2330 train_time:20294ms step_avg:40.35ms
step:504/2330 train_time:20332ms step_avg:40.34ms
step:505/2330 train_time:20367ms step_avg:40.33ms
step:506/2330 train_time:20411ms step_avg:40.34ms
step:507/2330 train_time:20445ms step_avg:40.33ms
step:508/2330 train_time:20489ms step_avg:40.33ms
step:509/2330 train_time:20524ms step_avg:40.32ms
step:510/2330 train_time:20571ms step_avg:40.34ms
step:511/2330 train_time:20613ms step_avg:40.34ms
step:512/2330 train_time:20661ms step_avg:40.35ms
step:513/2330 train_time:20698ms step_avg:40.35ms
step:514/2330 train_time:20743ms step_avg:40.36ms
step:515/2330 train_time:20778ms step_avg:40.35ms
step:516/2330 train_time:20823ms step_avg:40.35ms
step:517/2330 train_time:20858ms step_avg:40.34ms
step:518/2330 train_time:20901ms step_avg:40.35ms
step:519/2330 train_time:20936ms step_avg:40.34ms
step:520/2330 train_time:20980ms step_avg:40.35ms
step:521/2330 train_time:21015ms step_avg:40.34ms
step:522/2330 train_time:21059ms step_avg:40.34ms
step:523/2330 train_time:21094ms step_avg:40.33ms
step:524/2330 train_time:21138ms step_avg:40.34ms
step:525/2330 train_time:21173ms step_avg:40.33ms
step:526/2330 train_time:21221ms step_avg:40.34ms
step:527/2330 train_time:21256ms step_avg:40.33ms
step:528/2330 train_time:21302ms step_avg:40.34ms
step:529/2330 train_time:21337ms step_avg:40.33ms
step:530/2330 train_time:21382ms step_avg:40.34ms
step:531/2330 train_time:21417ms step_avg:40.33ms
step:532/2330 train_time:21462ms step_avg:40.34ms
step:533/2330 train_time:21498ms step_avg:40.33ms
step:534/2330 train_time:21543ms step_avg:40.34ms
step:535/2330 train_time:21580ms step_avg:40.34ms
step:536/2330 train_time:21626ms step_avg:40.35ms
step:537/2330 train_time:21661ms step_avg:40.34ms
step:538/2330 train_time:21705ms step_avg:40.34ms
step:539/2330 train_time:21741ms step_avg:40.34ms
step:540/2330 train_time:21786ms step_avg:40.34ms
step:541/2330 train_time:21821ms step_avg:40.33ms
step:542/2330 train_time:21865ms step_avg:40.34ms
step:543/2330 train_time:21901ms step_avg:40.33ms
step:544/2330 train_time:21945ms step_avg:40.34ms
step:545/2330 train_time:21981ms step_avg:40.33ms
step:546/2330 train_time:22026ms step_avg:40.34ms
step:547/2330 train_time:22061ms step_avg:40.33ms
step:548/2330 train_time:22106ms step_avg:40.34ms
step:549/2330 train_time:22142ms step_avg:40.33ms
step:550/2330 train_time:22187ms step_avg:40.34ms
step:551/2330 train_time:22223ms step_avg:40.33ms
step:552/2330 train_time:22267ms step_avg:40.34ms
step:553/2330 train_time:22303ms step_avg:40.33ms
step:554/2330 train_time:22347ms step_avg:40.34ms
step:555/2330 train_time:22383ms step_avg:40.33ms
step:556/2330 train_time:22428ms step_avg:40.34ms
step:557/2330 train_time:22463ms step_avg:40.33ms
step:558/2330 train_time:22508ms step_avg:40.34ms
step:559/2330 train_time:22545ms step_avg:40.33ms
step:560/2330 train_time:22590ms step_avg:40.34ms
step:561/2330 train_time:22626ms step_avg:40.33ms
step:562/2330 train_time:22672ms step_avg:40.34ms
step:563/2330 train_time:22709ms step_avg:40.34ms
step:564/2330 train_time:22755ms step_avg:40.34ms
step:565/2330 train_time:22791ms step_avg:40.34ms
step:566/2330 train_time:22836ms step_avg:40.35ms
step:567/2330 train_time:22871ms step_avg:40.34ms
step:568/2330 train_time:22917ms step_avg:40.35ms
step:569/2330 train_time:22953ms step_avg:40.34ms
step:570/2330 train_time:22997ms step_avg:40.35ms
step:571/2330 train_time:23032ms step_avg:40.34ms
step:572/2330 train_time:23077ms step_avg:40.34ms
step:573/2330 train_time:23113ms step_avg:40.34ms
step:574/2330 train_time:23158ms step_avg:40.34ms
step:575/2330 train_time:23193ms step_avg:40.34ms
step:576/2330 train_time:23238ms step_avg:40.34ms
step:577/2330 train_time:23273ms step_avg:40.33ms
step:578/2330 train_time:23319ms step_avg:40.34ms
step:579/2330 train_time:23354ms step_avg:40.33ms
step:580/2330 train_time:23399ms step_avg:40.34ms
step:581/2330 train_time:23434ms step_avg:40.33ms
step:582/2330 train_time:23479ms step_avg:40.34ms
step:583/2330 train_time:23515ms step_avg:40.33ms
step:584/2330 train_time:23560ms step_avg:40.34ms
step:585/2330 train_time:23595ms step_avg:40.33ms
step:586/2330 train_time:23641ms step_avg:40.34ms
step:587/2330 train_time:23677ms step_avg:40.33ms
step:588/2330 train_time:23722ms step_avg:40.34ms
step:589/2330 train_time:23757ms step_avg:40.33ms
step:590/2330 train_time:23801ms step_avg:40.34ms
step:591/2330 train_time:23836ms step_avg:40.33ms
step:592/2330 train_time:23882ms step_avg:40.34ms
step:593/2330 train_time:23917ms step_avg:40.33ms
step:594/2330 train_time:23961ms step_avg:40.34ms
step:595/2330 train_time:23997ms step_avg:40.33ms
step:596/2330 train_time:24041ms step_avg:40.34ms
step:597/2330 train_time:24076ms step_avg:40.33ms
step:598/2330 train_time:24120ms step_avg:40.34ms
step:599/2330 train_time:24155ms step_avg:40.33ms
step:600/2330 train_time:24200ms step_avg:40.33ms
step:601/2330 train_time:24235ms step_avg:40.32ms
step:602/2330 train_time:24280ms step_avg:40.33ms
step:603/2330 train_time:24316ms step_avg:40.32ms
step:604/2330 train_time:24360ms step_avg:40.33ms
step:605/2330 train_time:24395ms step_avg:40.32ms
step:606/2330 train_time:24440ms step_avg:40.33ms
step:607/2330 train_time:24476ms step_avg:40.32ms
step:608/2330 train_time:24520ms step_avg:40.33ms
step:609/2330 train_time:24557ms step_avg:40.32ms
step:610/2330 train_time:24602ms step_avg:40.33ms
step:611/2330 train_time:24637ms step_avg:40.32ms
step:612/2330 train_time:24682ms step_avg:40.33ms
step:613/2330 train_time:24718ms step_avg:40.32ms
step:614/2330 train_time:24762ms step_avg:40.33ms
step:615/2330 train_time:24798ms step_avg:40.32ms
step:616/2330 train_time:24842ms step_avg:40.33ms
step:617/2330 train_time:24877ms step_avg:40.32ms
step:618/2330 train_time:24922ms step_avg:40.33ms
step:619/2330 train_time:24958ms step_avg:40.32ms
step:620/2330 train_time:25003ms step_avg:40.33ms
step:621/2330 train_time:25039ms step_avg:40.32ms
step:622/2330 train_time:25084ms step_avg:40.33ms
step:623/2330 train_time:25119ms step_avg:40.32ms
step:624/2330 train_time:25164ms step_avg:40.33ms
step:625/2330 train_time:25199ms step_avg:40.32ms
step:626/2330 train_time:25244ms step_avg:40.33ms
step:627/2330 train_time:25279ms step_avg:40.32ms
step:628/2330 train_time:25325ms step_avg:40.33ms
step:629/2330 train_time:25360ms step_avg:40.32ms
step:630/2330 train_time:25404ms step_avg:40.32ms
step:631/2330 train_time:25440ms step_avg:40.32ms
step:632/2330 train_time:25485ms step_avg:40.32ms
step:633/2330 train_time:25521ms step_avg:40.32ms
step:634/2330 train_time:25565ms step_avg:40.32ms
step:635/2330 train_time:25600ms step_avg:40.32ms
step:636/2330 train_time:25645ms step_avg:40.32ms
step:637/2330 train_time:25681ms step_avg:40.31ms
step:638/2330 train_time:25725ms step_avg:40.32ms
step:639/2330 train_time:25760ms step_avg:40.31ms
step:640/2330 train_time:25805ms step_avg:40.32ms
step:641/2330 train_time:25840ms step_avg:40.31ms
step:642/2330 train_time:25885ms step_avg:40.32ms
step:643/2330 train_time:25921ms step_avg:40.31ms
step:644/2330 train_time:25965ms step_avg:40.32ms
step:645/2330 train_time:26001ms step_avg:40.31ms
step:646/2330 train_time:26046ms step_avg:40.32ms
step:647/2330 train_time:26081ms step_avg:40.31ms
step:648/2330 train_time:26126ms step_avg:40.32ms
step:649/2330 train_time:26163ms step_avg:40.31ms
step:650/2330 train_time:26208ms step_avg:40.32ms
step:651/2330 train_time:26243ms step_avg:40.31ms
step:652/2330 train_time:26288ms step_avg:40.32ms
step:653/2330 train_time:26324ms step_avg:40.31ms
step:654/2330 train_time:26369ms step_avg:40.32ms
step:655/2330 train_time:26404ms step_avg:40.31ms
step:656/2330 train_time:26449ms step_avg:40.32ms
step:657/2330 train_time:26484ms step_avg:40.31ms
step:658/2330 train_time:26529ms step_avg:40.32ms
step:659/2330 train_time:26564ms step_avg:40.31ms
step:660/2330 train_time:26609ms step_avg:40.32ms
step:661/2330 train_time:26645ms step_avg:40.31ms
step:662/2330 train_time:26689ms step_avg:40.32ms
step:663/2330 train_time:26725ms step_avg:40.31ms
step:664/2330 train_time:26770ms step_avg:40.32ms
step:665/2330 train_time:26805ms step_avg:40.31ms
step:666/2330 train_time:26850ms step_avg:40.31ms
step:667/2330 train_time:26885ms step_avg:40.31ms
step:668/2330 train_time:26931ms step_avg:40.32ms
step:669/2330 train_time:26967ms step_avg:40.31ms
step:670/2330 train_time:27014ms step_avg:40.32ms
step:671/2330 train_time:27049ms step_avg:40.31ms
step:672/2330 train_time:27095ms step_avg:40.32ms
step:673/2330 train_time:27130ms step_avg:40.31ms
step:674/2330 train_time:27175ms step_avg:40.32ms
step:675/2330 train_time:27210ms step_avg:40.31ms
step:676/2330 train_time:27255ms step_avg:40.32ms
step:677/2330 train_time:27291ms step_avg:40.31ms
step:678/2330 train_time:27336ms step_avg:40.32ms
step:679/2330 train_time:27371ms step_avg:40.31ms
step:680/2330 train_time:27416ms step_avg:40.32ms
step:681/2330 train_time:27451ms step_avg:40.31ms
step:682/2330 train_time:27496ms step_avg:40.32ms
step:683/2330 train_time:27532ms step_avg:40.31ms
step:684/2330 train_time:27577ms step_avg:40.32ms
step:685/2330 train_time:27612ms step_avg:40.31ms
step:686/2330 train_time:27657ms step_avg:40.32ms
step:687/2330 train_time:27693ms step_avg:40.31ms
step:688/2330 train_time:27737ms step_avg:40.32ms
step:689/2330 train_time:27773ms step_avg:40.31ms
step:690/2330 train_time:27818ms step_avg:40.32ms
step:691/2330 train_time:27853ms step_avg:40.31ms
step:692/2330 train_time:27898ms step_avg:40.32ms
step:693/2330 train_time:27933ms step_avg:40.31ms
step:694/2330 train_time:27979ms step_avg:40.31ms
step:695/2330 train_time:28014ms step_avg:40.31ms
step:696/2330 train_time:28059ms step_avg:40.32ms
step:697/2330 train_time:28094ms step_avg:40.31ms
step:698/2330 train_time:28139ms step_avg:40.31ms
step:699/2330 train_time:28174ms step_avg:40.31ms
step:700/2330 train_time:28219ms step_avg:40.31ms
step:701/2330 train_time:28254ms step_avg:40.31ms
step:702/2330 train_time:28299ms step_avg:40.31ms
step:703/2330 train_time:28334ms step_avg:40.30ms
step:704/2330 train_time:28379ms step_avg:40.31ms
step:705/2330 train_time:28414ms step_avg:40.30ms
step:706/2330 train_time:28458ms step_avg:40.31ms
step:707/2330 train_time:28494ms step_avg:40.30ms
step:708/2330 train_time:28538ms step_avg:40.31ms
step:709/2330 train_time:28574ms step_avg:40.30ms
step:710/2330 train_time:28619ms step_avg:40.31ms
step:711/2330 train_time:28654ms step_avg:40.30ms
step:712/2330 train_time:28699ms step_avg:40.31ms
step:713/2330 train_time:28734ms step_avg:40.30ms
step:714/2330 train_time:28779ms step_avg:40.31ms
step:715/2330 train_time:28815ms step_avg:40.30ms
step:716/2330 train_time:28859ms step_avg:40.31ms
step:717/2330 train_time:28895ms step_avg:40.30ms
step:718/2330 train_time:28940ms step_avg:40.31ms
step:719/2330 train_time:28976ms step_avg:40.30ms
step:720/2330 train_time:29021ms step_avg:40.31ms
step:721/2330 train_time:29056ms step_avg:40.30ms
step:722/2330 train_time:29100ms step_avg:40.30ms
step:723/2330 train_time:29135ms step_avg:40.30ms
step:724/2330 train_time:29179ms step_avg:40.30ms
step:725/2330 train_time:29215ms step_avg:40.30ms
step:726/2330 train_time:29260ms step_avg:40.30ms
step:727/2330 train_time:29294ms step_avg:40.29ms
step:728/2330 train_time:29339ms step_avg:40.30ms
step:729/2330 train_time:29375ms step_avg:40.29ms
step:730/2330 train_time:29419ms step_avg:40.30ms
step:731/2330 train_time:29454ms step_avg:40.29ms
step:732/2330 train_time:29499ms step_avg:40.30ms
step:733/2330 train_time:29534ms step_avg:40.29ms
step:734/2330 train_time:29579ms step_avg:40.30ms
step:735/2330 train_time:29614ms step_avg:40.29ms
step:736/2330 train_time:29659ms step_avg:40.30ms
step:737/2330 train_time:29694ms step_avg:40.29ms
step:738/2330 train_time:29739ms step_avg:40.30ms
step:739/2330 train_time:29775ms step_avg:40.29ms
step:740/2330 train_time:29819ms step_avg:40.30ms
step:741/2330 train_time:29855ms step_avg:40.29ms
step:742/2330 train_time:29899ms step_avg:40.29ms
step:743/2330 train_time:29935ms step_avg:40.29ms
step:744/2330 train_time:29980ms step_avg:40.30ms
step:745/2330 train_time:30015ms step_avg:40.29ms
step:746/2330 train_time:30060ms step_avg:40.29ms
step:747/2330 train_time:30095ms step_avg:40.29ms
step:748/2330 train_time:30139ms step_avg:40.29ms
step:749/2330 train_time:30174ms step_avg:40.29ms
step:750/2330 train_time:30219ms step_avg:40.29ms
step:750/2330 val_loss:5.2270 train_time:30306ms step_avg:40.41ms
step:751/2330 train_time:30319ms step_avg:40.37ms
step:752/2330 train_time:30331ms step_avg:40.33ms
step:753/2330 train_time:30341ms step_avg:40.29ms
step:754/2330 train_time:30381ms step_avg:40.29ms
step:755/2330 train_time:30415ms step_avg:40.29ms
step:756/2330 train_time:30459ms step_avg:40.29ms
step:757/2330 train_time:30493ms step_avg:40.28ms
step:758/2330 train_time:30538ms step_avg:40.29ms
step:759/2330 train_time:30572ms step_avg:40.28ms
step:760/2330 train_time:30622ms step_avg:40.29ms
step:761/2330 train_time:30659ms step_avg:40.29ms
step:762/2330 train_time:30706ms step_avg:40.30ms
step:763/2330 train_time:30742ms step_avg:40.29ms
step:764/2330 train_time:30787ms step_avg:40.30ms
step:765/2330 train_time:30823ms step_avg:40.29ms
step:766/2330 train_time:30867ms step_avg:40.30ms
step:767/2330 train_time:30901ms step_avg:40.29ms
step:768/2330 train_time:30945ms step_avg:40.29ms
step:769/2330 train_time:30980ms step_avg:40.29ms
step:770/2330 train_time:31024ms step_avg:40.29ms
step:771/2330 train_time:31059ms step_avg:40.28ms
step:772/2330 train_time:31104ms step_avg:40.29ms
step:773/2330 train_time:31139ms step_avg:40.28ms
step:774/2330 train_time:31184ms step_avg:40.29ms
step:775/2330 train_time:31220ms step_avg:40.28ms
step:776/2330 train_time:31266ms step_avg:40.29ms
step:777/2330 train_time:31302ms step_avg:40.29ms
step:778/2330 train_time:31347ms step_avg:40.29ms
step:779/2330 train_time:31383ms step_avg:40.29ms
step:780/2330 train_time:31428ms step_avg:40.29ms
step:781/2330 train_time:31463ms step_avg:40.29ms
step:782/2330 train_time:31508ms step_avg:40.29ms
step:783/2330 train_time:31543ms step_avg:40.29ms
step:784/2330 train_time:31589ms step_avg:40.29ms
step:785/2330 train_time:31624ms step_avg:40.29ms
step:786/2330 train_time:31670ms step_avg:40.29ms
step:787/2330 train_time:31706ms step_avg:40.29ms
step:788/2330 train_time:31750ms step_avg:40.29ms
step:789/2330 train_time:31786ms step_avg:40.29ms
step:790/2330 train_time:31831ms step_avg:40.29ms
step:791/2330 train_time:31866ms step_avg:40.29ms
step:792/2330 train_time:31912ms step_avg:40.29ms
step:793/2330 train_time:31947ms step_avg:40.29ms
step:794/2330 train_time:31991ms step_avg:40.29ms
step:795/2330 train_time:32026ms step_avg:40.28ms
step:796/2330 train_time:32070ms step_avg:40.29ms
step:797/2330 train_time:32105ms step_avg:40.28ms
step:798/2330 train_time:32150ms step_avg:40.29ms
step:799/2330 train_time:32186ms step_avg:40.28ms
step:800/2330 train_time:32232ms step_avg:40.29ms
step:801/2330 train_time:32268ms step_avg:40.28ms
step:802/2330 train_time:32313ms step_avg:40.29ms
step:803/2330 train_time:32349ms step_avg:40.28ms
step:804/2330 train_time:32394ms step_avg:40.29ms
step:805/2330 train_time:32430ms step_avg:40.29ms
step:806/2330 train_time:32475ms step_avg:40.29ms
step:807/2330 train_time:32511ms step_avg:40.29ms
step:808/2330 train_time:32557ms step_avg:40.29ms
step:809/2330 train_time:32592ms step_avg:40.29ms
step:810/2330 train_time:32637ms step_avg:40.29ms
step:811/2330 train_time:32673ms step_avg:40.29ms
step:812/2330 train_time:32717ms step_avg:40.29ms
step:813/2330 train_time:32752ms step_avg:40.29ms
step:814/2330 train_time:32798ms step_avg:40.29ms
step:815/2330 train_time:32834ms step_avg:40.29ms
step:816/2330 train_time:32879ms step_avg:40.29ms
step:817/2330 train_time:32915ms step_avg:40.29ms
step:818/2330 train_time:32960ms step_avg:40.29ms
step:819/2330 train_time:32996ms step_avg:40.29ms
step:820/2330 train_time:33040ms step_avg:40.29ms
step:821/2330 train_time:33075ms step_avg:40.29ms
step:822/2330 train_time:33120ms step_avg:40.29ms
step:823/2330 train_time:33156ms step_avg:40.29ms
step:824/2330 train_time:33201ms step_avg:40.29ms
step:825/2330 train_time:33236ms step_avg:40.29ms
step:826/2330 train_time:33281ms step_avg:40.29ms
step:827/2330 train_time:33316ms step_avg:40.29ms
step:828/2330 train_time:33361ms step_avg:40.29ms
step:829/2330 train_time:33396ms step_avg:40.28ms
step:830/2330 train_time:33441ms step_avg:40.29ms
step:831/2330 train_time:33476ms step_avg:40.28ms
step:832/2330 train_time:33521ms step_avg:40.29ms
step:833/2330 train_time:33557ms step_avg:40.28ms
step:834/2330 train_time:33603ms step_avg:40.29ms
step:835/2330 train_time:33637ms step_avg:40.28ms
step:836/2330 train_time:33682ms step_avg:40.29ms
step:837/2330 train_time:33718ms step_avg:40.28ms
step:838/2330 train_time:33763ms step_avg:40.29ms
step:839/2330 train_time:33798ms step_avg:40.28ms
step:840/2330 train_time:33844ms step_avg:40.29ms
step:841/2330 train_time:33879ms step_avg:40.28ms
step:842/2330 train_time:33923ms step_avg:40.29ms
step:843/2330 train_time:33958ms step_avg:40.28ms
step:844/2330 train_time:34003ms step_avg:40.29ms
step:845/2330 train_time:34038ms step_avg:40.28ms
step:846/2330 train_time:34082ms step_avg:40.29ms
step:847/2330 train_time:34117ms step_avg:40.28ms
step:848/2330 train_time:34162ms step_avg:40.29ms
step:849/2330 train_time:34197ms step_avg:40.28ms
step:850/2330 train_time:34242ms step_avg:40.29ms
step:851/2330 train_time:34278ms step_avg:40.28ms
step:852/2330 train_time:34322ms step_avg:40.28ms
step:853/2330 train_time:34357ms step_avg:40.28ms
step:854/2330 train_time:34402ms step_avg:40.28ms
step:855/2330 train_time:34438ms step_avg:40.28ms
step:856/2330 train_time:34483ms step_avg:40.28ms
step:857/2330 train_time:34517ms step_avg:40.28ms
step:858/2330 train_time:34563ms step_avg:40.28ms
step:859/2330 train_time:34598ms step_avg:40.28ms
step:860/2330 train_time:34643ms step_avg:40.28ms
step:861/2330 train_time:34678ms step_avg:40.28ms
step:862/2330 train_time:34723ms step_avg:40.28ms
step:863/2330 train_time:34758ms step_avg:40.28ms
step:864/2330 train_time:34803ms step_avg:40.28ms
step:865/2330 train_time:34838ms step_avg:40.27ms
step:866/2330 train_time:34882ms step_avg:40.28ms
step:867/2330 train_time:34917ms step_avg:40.27ms
step:868/2330 train_time:34962ms step_avg:40.28ms
step:869/2330 train_time:34996ms step_avg:40.27ms
step:870/2330 train_time:35042ms step_avg:40.28ms
step:871/2330 train_time:35076ms step_avg:40.27ms
step:872/2330 train_time:35121ms step_avg:40.28ms
step:873/2330 train_time:35156ms step_avg:40.27ms
step:874/2330 train_time:35201ms step_avg:40.28ms
step:875/2330 train_time:35236ms step_avg:40.27ms
step:876/2330 train_time:35281ms step_avg:40.28ms
step:877/2330 train_time:35317ms step_avg:40.27ms
step:878/2330 train_time:35361ms step_avg:40.27ms
step:879/2330 train_time:35396ms step_avg:40.27ms
step:880/2330 train_time:35442ms step_avg:40.28ms
step:881/2330 train_time:35477ms step_avg:40.27ms
step:882/2330 train_time:35522ms step_avg:40.27ms
step:883/2330 train_time:35557ms step_avg:40.27ms
step:884/2330 train_time:35602ms step_avg:40.27ms
step:885/2330 train_time:35637ms step_avg:40.27ms
step:886/2330 train_time:35682ms step_avg:40.27ms
step:887/2330 train_time:35718ms step_avg:40.27ms
step:888/2330 train_time:35762ms step_avg:40.27ms
step:889/2330 train_time:35797ms step_avg:40.27ms
step:890/2330 train_time:35842ms step_avg:40.27ms
step:891/2330 train_time:35878ms step_avg:40.27ms
step:892/2330 train_time:35922ms step_avg:40.27ms
step:893/2330 train_time:35957ms step_avg:40.27ms
step:894/2330 train_time:36002ms step_avg:40.27ms
step:895/2330 train_time:36037ms step_avg:40.27ms
step:896/2330 train_time:36082ms step_avg:40.27ms
step:897/2330 train_time:36117ms step_avg:40.26ms
step:898/2330 train_time:36162ms step_avg:40.27ms
step:899/2330 train_time:36197ms step_avg:40.26ms
step:900/2330 train_time:36242ms step_avg:40.27ms
step:901/2330 train_time:36278ms step_avg:40.26ms
step:902/2330 train_time:36322ms step_avg:40.27ms
step:903/2330 train_time:36357ms step_avg:40.26ms
step:904/2330 train_time:36402ms step_avg:40.27ms
step:905/2330 train_time:36438ms step_avg:40.26ms
step:906/2330 train_time:36482ms step_avg:40.27ms
step:907/2330 train_time:36517ms step_avg:40.26ms
step:908/2330 train_time:36563ms step_avg:40.27ms
step:909/2330 train_time:36598ms step_avg:40.26ms
step:910/2330 train_time:36644ms step_avg:40.27ms
step:911/2330 train_time:36678ms step_avg:40.26ms
step:912/2330 train_time:36723ms step_avg:40.27ms
step:913/2330 train_time:36758ms step_avg:40.26ms
step:914/2330 train_time:36802ms step_avg:40.27ms
step:915/2330 train_time:36837ms step_avg:40.26ms
step:916/2330 train_time:36882ms step_avg:40.26ms
step:917/2330 train_time:36917ms step_avg:40.26ms
step:918/2330 train_time:36962ms step_avg:40.26ms
step:919/2330 train_time:36997ms step_avg:40.26ms
step:920/2330 train_time:37042ms step_avg:40.26ms
step:921/2330 train_time:37077ms step_avg:40.26ms
step:922/2330 train_time:37121ms step_avg:40.26ms
step:923/2330 train_time:37156ms step_avg:40.26ms
step:924/2330 train_time:37201ms step_avg:40.26ms
step:925/2330 train_time:37237ms step_avg:40.26ms
step:926/2330 train_time:37282ms step_avg:40.26ms
step:927/2330 train_time:37317ms step_avg:40.26ms
step:928/2330 train_time:37361ms step_avg:40.26ms
step:929/2330 train_time:37397ms step_avg:40.25ms
step:930/2330 train_time:37441ms step_avg:40.26ms
step:931/2330 train_time:37477ms step_avg:40.25ms
step:932/2330 train_time:37522ms step_avg:40.26ms
step:933/2330 train_time:37557ms step_avg:40.25ms
step:934/2330 train_time:37602ms step_avg:40.26ms
step:935/2330 train_time:37637ms step_avg:40.25ms
step:936/2330 train_time:37682ms step_avg:40.26ms
step:937/2330 train_time:37717ms step_avg:40.25ms
step:938/2330 train_time:37762ms step_avg:40.26ms
step:939/2330 train_time:37797ms step_avg:40.25ms
step:940/2330 train_time:37842ms step_avg:40.26ms
step:941/2330 train_time:37877ms step_avg:40.25ms
step:942/2330 train_time:37922ms step_avg:40.26ms
step:943/2330 train_time:37957ms step_avg:40.25ms
step:944/2330 train_time:38002ms step_avg:40.26ms
step:945/2330 train_time:38037ms step_avg:40.25ms
step:946/2330 train_time:38082ms step_avg:40.26ms
step:947/2330 train_time:38117ms step_avg:40.25ms
step:948/2330 train_time:38162ms step_avg:40.25ms
step:949/2330 train_time:38197ms step_avg:40.25ms
step:950/2330 train_time:38241ms step_avg:40.25ms
step:951/2330 train_time:38277ms step_avg:40.25ms
step:952/2330 train_time:38322ms step_avg:40.25ms
step:953/2330 train_time:38356ms step_avg:40.25ms
step:954/2330 train_time:38402ms step_avg:40.25ms
step:955/2330 train_time:38437ms step_avg:40.25ms
step:956/2330 train_time:38481ms step_avg:40.25ms
step:957/2330 train_time:38517ms step_avg:40.25ms
step:958/2330 train_time:38562ms step_avg:40.25ms
step:959/2330 train_time:38597ms step_avg:40.25ms
step:960/2330 train_time:38642ms step_avg:40.25ms
step:961/2330 train_time:38677ms step_avg:40.25ms
step:962/2330 train_time:38722ms step_avg:40.25ms
step:963/2330 train_time:38757ms step_avg:40.25ms
step:964/2330 train_time:38801ms step_avg:40.25ms
step:965/2330 train_time:38837ms step_avg:40.25ms
step:966/2330 train_time:38882ms step_avg:40.25ms
step:967/2330 train_time:38917ms step_avg:40.25ms
step:968/2330 train_time:38961ms step_avg:40.25ms
step:969/2330 train_time:38996ms step_avg:40.24ms
step:970/2330 train_time:39041ms step_avg:40.25ms
step:971/2330 train_time:39076ms step_avg:40.24ms
step:972/2330 train_time:39121ms step_avg:40.25ms
step:973/2330 train_time:39156ms step_avg:40.24ms
step:974/2330 train_time:39201ms step_avg:40.25ms
step:975/2330 train_time:39236ms step_avg:40.24ms
step:976/2330 train_time:39281ms step_avg:40.25ms
step:977/2330 train_time:39316ms step_avg:40.24ms
step:978/2330 train_time:39361ms step_avg:40.25ms
step:979/2330 train_time:39396ms step_avg:40.24ms
step:980/2330 train_time:39441ms step_avg:40.25ms
step:981/2330 train_time:39476ms step_avg:40.24ms
step:982/2330 train_time:39521ms step_avg:40.25ms
step:983/2330 train_time:39556ms step_avg:40.24ms
step:984/2330 train_time:39601ms step_avg:40.24ms
step:985/2330 train_time:39637ms step_avg:40.24ms
step:986/2330 train_time:39682ms step_avg:40.25ms
step:987/2330 train_time:39717ms step_avg:40.24ms
step:988/2330 train_time:39762ms step_avg:40.24ms
step:989/2330 train_time:39797ms step_avg:40.24ms
step:990/2330 train_time:39841ms step_avg:40.24ms
step:991/2330 train_time:39877ms step_avg:40.24ms
step:992/2330 train_time:39922ms step_avg:40.24ms
step:993/2330 train_time:39957ms step_avg:40.24ms
step:994/2330 train_time:40001ms step_avg:40.24ms
step:995/2330 train_time:40037ms step_avg:40.24ms
step:996/2330 train_time:40081ms step_avg:40.24ms
step:997/2330 train_time:40117ms step_avg:40.24ms
step:998/2330 train_time:40161ms step_avg:40.24ms
step:999/2330 train_time:40197ms step_avg:40.24ms
step:1000/2330 train_time:40241ms step_avg:40.24ms
step:1000/2330 val_loss:5.1893 train_time:40328ms step_avg:40.33ms
step:1001/2330 train_time:40341ms step_avg:40.30ms
step:1002/2330 train_time:40354ms step_avg:40.27ms
step:1003/2330 train_time:40365ms step_avg:40.24ms
step:1004/2330 train_time:40401ms step_avg:40.24ms
step:1005/2330 train_time:40435ms step_avg:40.23ms
step:1006/2330 train_time:40478ms step_avg:40.24ms
step:1007/2330 train_time:40513ms step_avg:40.23ms
step:1008/2330 train_time:40556ms step_avg:40.23ms
step:1009/2330 train_time:40592ms step_avg:40.23ms
step:1010/2330 train_time:40637ms step_avg:40.23ms
step:1011/2330 train_time:40680ms step_avg:40.24ms
step:1012/2330 train_time:40730ms step_avg:40.25ms
step:1013/2330 train_time:40766ms step_avg:40.24ms
step:1014/2330 train_time:40811ms step_avg:40.25ms
step:1015/2330 train_time:40846ms step_avg:40.24ms
step:1016/2330 train_time:40892ms step_avg:40.25ms
step:1017/2330 train_time:40926ms step_avg:40.24ms
step:1018/2330 train_time:40971ms step_avg:40.25ms
step:1019/2330 train_time:41005ms step_avg:40.24ms
step:1020/2330 train_time:41049ms step_avg:40.24ms
step:1021/2330 train_time:41084ms step_avg:40.24ms
step:1022/2330 train_time:41129ms step_avg:40.24ms
step:1023/2330 train_time:41163ms step_avg:40.24ms
step:1024/2330 train_time:41207ms step_avg:40.24ms
step:1025/2330 train_time:41243ms step_avg:40.24ms
step:1026/2330 train_time:41290ms step_avg:40.24ms
step:1027/2330 train_time:41326ms step_avg:40.24ms
step:1028/2330 train_time:41372ms step_avg:40.25ms
step:1029/2330 train_time:41407ms step_avg:40.24ms
step:1030/2330 train_time:41451ms step_avg:40.24ms
step:1031/2330 train_time:41486ms step_avg:40.24ms
step:1032/2330 train_time:41531ms step_avg:40.24ms
step:1033/2330 train_time:41566ms step_avg:40.24ms
step:1034/2330 train_time:41612ms step_avg:40.24ms
step:1035/2330 train_time:41648ms step_avg:40.24ms
step:1036/2330 train_time:41694ms step_avg:40.25ms
step:1037/2330 train_time:41730ms step_avg:40.24ms
step:1038/2330 train_time:41775ms step_avg:40.25ms
step:1039/2330 train_time:41811ms step_avg:40.24ms
step:1040/2330 train_time:41857ms step_avg:40.25ms
step:1041/2330 train_time:41893ms step_avg:40.24ms
step:1042/2330 train_time:41937ms step_avg:40.25ms
step:1043/2330 train_time:41972ms step_avg:40.24ms
step:1044/2330 train_time:42017ms step_avg:40.25ms
step:1045/2330 train_time:42053ms step_avg:40.24ms
step:1046/2330 train_time:42097ms step_avg:40.25ms
step:1047/2330 train_time:42132ms step_avg:40.24ms
step:1048/2330 train_time:42176ms step_avg:40.24ms
step:1049/2330 train_time:42212ms step_avg:40.24ms
step:1050/2330 train_time:42257ms step_avg:40.24ms
step:1051/2330 train_time:42293ms step_avg:40.24ms
step:1052/2330 train_time:42338ms step_avg:40.24ms
step:1053/2330 train_time:42373ms step_avg:40.24ms
step:1054/2330 train_time:42417ms step_avg:40.24ms
step:1055/2330 train_time:42453ms step_avg:40.24ms
step:1056/2330 train_time:42498ms step_avg:40.24ms
step:1057/2330 train_time:42533ms step_avg:40.24ms
step:1058/2330 train_time:42578ms step_avg:40.24ms
step:1059/2330 train_time:42613ms step_avg:40.24ms
step:1060/2330 train_time:42658ms step_avg:40.24ms
step:1061/2330 train_time:42695ms step_avg:40.24ms
step:1062/2330 train_time:42742ms step_avg:40.25ms
step:1063/2330 train_time:42779ms step_avg:40.24ms
step:1064/2330 train_time:42825ms step_avg:40.25ms
step:1065/2330 train_time:42861ms step_avg:40.25ms
step:1066/2330 train_time:42906ms step_avg:40.25ms
step:1067/2330 train_time:42941ms step_avg:40.24ms
step:1068/2330 train_time:42986ms step_avg:40.25ms
step:1069/2330 train_time:43022ms step_avg:40.24ms
step:1070/2330 train_time:43067ms step_avg:40.25ms
step:1071/2330 train_time:43102ms step_avg:40.24ms
step:1072/2330 train_time:43147ms step_avg:40.25ms
step:1073/2330 train_time:43182ms step_avg:40.24ms
step:1074/2330 train_time:43228ms step_avg:40.25ms
step:1075/2330 train_time:43263ms step_avg:40.25ms
step:1076/2330 train_time:43309ms step_avg:40.25ms
step:1077/2330 train_time:43344ms step_avg:40.24ms
step:1078/2330 train_time:43389ms step_avg:40.25ms
step:1079/2330 train_time:43424ms step_avg:40.24ms
step:1080/2330 train_time:43468ms step_avg:40.25ms
step:1081/2330 train_time:43504ms step_avg:40.24ms
step:1082/2330 train_time:43548ms step_avg:40.25ms
step:1083/2330 train_time:43584ms step_avg:40.24ms
step:1084/2330 train_time:43629ms step_avg:40.25ms
step:1085/2330 train_time:43665ms step_avg:40.24ms
step:1086/2330 train_time:43711ms step_avg:40.25ms
step:1087/2330 train_time:43745ms step_avg:40.24ms
step:1088/2330 train_time:43790ms step_avg:40.25ms
step:1089/2330 train_time:43825ms step_avg:40.24ms
step:1090/2330 train_time:43870ms step_avg:40.25ms
step:1091/2330 train_time:43905ms step_avg:40.24ms
step:1092/2330 train_time:43950ms step_avg:40.25ms
step:1093/2330 train_time:43985ms step_avg:40.24ms
step:1094/2330 train_time:44030ms step_avg:40.25ms
step:1095/2330 train_time:44064ms step_avg:40.24ms
step:1096/2330 train_time:44109ms step_avg:40.25ms
step:1097/2330 train_time:44144ms step_avg:40.24ms
step:1098/2330 train_time:44189ms step_avg:40.25ms
step:1099/2330 train_time:44224ms step_avg:40.24ms
step:1100/2330 train_time:44269ms step_avg:40.24ms
step:1101/2330 train_time:44304ms step_avg:40.24ms
step:1102/2330 train_time:44349ms step_avg:40.24ms
step:1103/2330 train_time:44384ms step_avg:40.24ms
step:1104/2330 train_time:44428ms step_avg:40.24ms
step:1105/2330 train_time:44463ms step_avg:40.24ms
step:1106/2330 train_time:44508ms step_avg:40.24ms
step:1107/2330 train_time:44543ms step_avg:40.24ms
step:1108/2330 train_time:44588ms step_avg:40.24ms
step:1109/2330 train_time:44624ms step_avg:40.24ms
step:1110/2330 train_time:44669ms step_avg:40.24ms
step:1111/2330 train_time:44704ms step_avg:40.24ms
step:1112/2330 train_time:44748ms step_avg:40.24ms
step:1113/2330 train_time:44784ms step_avg:40.24ms
step:1114/2330 train_time:44829ms step_avg:40.24ms
step:1115/2330 train_time:44864ms step_avg:40.24ms
step:1116/2330 train_time:44908ms step_avg:40.24ms
step:1117/2330 train_time:44944ms step_avg:40.24ms
step:1118/2330 train_time:44989ms step_avg:40.24ms
step:1119/2330 train_time:45025ms step_avg:40.24ms
step:1120/2330 train_time:45069ms step_avg:40.24ms
step:1121/2330 train_time:45104ms step_avg:40.24ms
step:1122/2330 train_time:45149ms step_avg:40.24ms
step:1123/2330 train_time:45184ms step_avg:40.24ms
step:1124/2330 train_time:45229ms step_avg:40.24ms
step:1125/2330 train_time:45264ms step_avg:40.23ms
step:1126/2330 train_time:45309ms step_avg:40.24ms
step:1127/2330 train_time:45344ms step_avg:40.23ms
step:1128/2330 train_time:45389ms step_avg:40.24ms
step:1129/2330 train_time:45424ms step_avg:40.23ms
step:1130/2330 train_time:45469ms step_avg:40.24ms
step:1131/2330 train_time:45504ms step_avg:40.23ms
step:1132/2330 train_time:45549ms step_avg:40.24ms
step:1133/2330 train_time:45584ms step_avg:40.23ms
step:1134/2330 train_time:45628ms step_avg:40.24ms
step:1135/2330 train_time:45664ms step_avg:40.23ms
step:1136/2330 train_time:45709ms step_avg:40.24ms
step:1137/2330 train_time:45744ms step_avg:40.23ms
step:1138/2330 train_time:45789ms step_avg:40.24ms
step:1139/2330 train_time:45824ms step_avg:40.23ms
step:1140/2330 train_time:45869ms step_avg:40.24ms
step:1141/2330 train_time:45904ms step_avg:40.23ms
step:1142/2330 train_time:45949ms step_avg:40.24ms
step:1143/2330 train_time:45985ms step_avg:40.23ms
step:1144/2330 train_time:46029ms step_avg:40.24ms
step:1145/2330 train_time:46064ms step_avg:40.23ms
step:1146/2330 train_time:46109ms step_avg:40.23ms
step:1147/2330 train_time:46144ms step_avg:40.23ms
step:1148/2330 train_time:46188ms step_avg:40.23ms
step:1149/2330 train_time:46223ms step_avg:40.23ms
step:1150/2330 train_time:46267ms step_avg:40.23ms
step:1151/2330 train_time:46303ms step_avg:40.23ms
step:1152/2330 train_time:46347ms step_avg:40.23ms
step:1153/2330 train_time:46383ms step_avg:40.23ms
step:1154/2330 train_time:46428ms step_avg:40.23ms
step:1155/2330 train_time:46463ms step_avg:40.23ms
step:1156/2330 train_time:46509ms step_avg:40.23ms
step:1157/2330 train_time:46544ms step_avg:40.23ms
step:1158/2330 train_time:46588ms step_avg:40.23ms
step:1159/2330 train_time:46624ms step_avg:40.23ms
step:1160/2330 train_time:46669ms step_avg:40.23ms
step:1161/2330 train_time:46704ms step_avg:40.23ms
step:1162/2330 train_time:46749ms step_avg:40.23ms
step:1163/2330 train_time:46785ms step_avg:40.23ms
step:1164/2330 train_time:46829ms step_avg:40.23ms
step:1165/2330 train_time:46864ms step_avg:40.23ms
step:1166/2330 train_time:46909ms step_avg:40.23ms
step:1167/2330 train_time:46944ms step_avg:40.23ms
step:1168/2330 train_time:46989ms step_avg:40.23ms
step:1169/2330 train_time:47024ms step_avg:40.23ms
step:1170/2330 train_time:47068ms step_avg:40.23ms
step:1171/2330 train_time:47103ms step_avg:40.22ms
step:1172/2330 train_time:47148ms step_avg:40.23ms
step:1173/2330 train_time:47183ms step_avg:40.22ms
step:1174/2330 train_time:47228ms step_avg:40.23ms
step:1175/2330 train_time:47263ms step_avg:40.22ms
step:1176/2330 train_time:47308ms step_avg:40.23ms
step:1177/2330 train_time:47343ms step_avg:40.22ms
step:1178/2330 train_time:47389ms step_avg:40.23ms
step:1179/2330 train_time:47423ms step_avg:40.22ms
step:1180/2330 train_time:47468ms step_avg:40.23ms
step:1181/2330 train_time:47504ms step_avg:40.22ms
step:1182/2330 train_time:47549ms step_avg:40.23ms
step:1183/2330 train_time:47584ms step_avg:40.22ms
step:1184/2330 train_time:47629ms step_avg:40.23ms
step:1185/2330 train_time:47664ms step_avg:40.22ms
step:1186/2330 train_time:47709ms step_avg:40.23ms
step:1187/2330 train_time:47744ms step_avg:40.22ms
step:1188/2330 train_time:47788ms step_avg:40.23ms
step:1189/2330 train_time:47823ms step_avg:40.22ms
step:1190/2330 train_time:47868ms step_avg:40.23ms
step:1191/2330 train_time:47903ms step_avg:40.22ms
step:1192/2330 train_time:47948ms step_avg:40.22ms
step:1193/2330 train_time:47983ms step_avg:40.22ms
step:1194/2330 train_time:48028ms step_avg:40.22ms
step:1195/2330 train_time:48063ms step_avg:40.22ms
step:1196/2330 train_time:48108ms step_avg:40.22ms
step:1197/2330 train_time:48143ms step_avg:40.22ms
step:1198/2330 train_time:48187ms step_avg:40.22ms
step:1199/2330 train_time:48223ms step_avg:40.22ms
step:1200/2330 train_time:48268ms step_avg:40.22ms
step:1201/2330 train_time:48303ms step_avg:40.22ms
step:1202/2330 train_time:48348ms step_avg:40.22ms
step:1203/2330 train_time:48384ms step_avg:40.22ms
step:1204/2330 train_time:48428ms step_avg:40.22ms
step:1205/2330 train_time:48463ms step_avg:40.22ms
step:1206/2330 train_time:48509ms step_avg:40.22ms
step:1207/2330 train_time:48544ms step_avg:40.22ms
step:1208/2330 train_time:48589ms step_avg:40.22ms
step:1209/2330 train_time:48624ms step_avg:40.22ms
step:1210/2330 train_time:48668ms step_avg:40.22ms
step:1211/2330 train_time:48704ms step_avg:40.22ms
step:1212/2330 train_time:48748ms step_avg:40.22ms
step:1213/2330 train_time:48784ms step_avg:40.22ms
step:1214/2330 train_time:48829ms step_avg:40.22ms
step:1215/2330 train_time:48863ms step_avg:40.22ms
step:1216/2330 train_time:48908ms step_avg:40.22ms
step:1217/2330 train_time:48944ms step_avg:40.22ms
step:1218/2330 train_time:48989ms step_avg:40.22ms
step:1219/2330 train_time:49024ms step_avg:40.22ms
step:1220/2330 train_time:49069ms step_avg:40.22ms
step:1221/2330 train_time:49103ms step_avg:40.22ms
step:1222/2330 train_time:49148ms step_avg:40.22ms
step:1223/2330 train_time:49183ms step_avg:40.22ms
step:1224/2330 train_time:49228ms step_avg:40.22ms
step:1225/2330 train_time:49263ms step_avg:40.22ms
step:1226/2330 train_time:49308ms step_avg:40.22ms
step:1227/2330 train_time:49343ms step_avg:40.21ms
step:1228/2330 train_time:49388ms step_avg:40.22ms
step:1229/2330 train_time:49423ms step_avg:40.21ms
step:1230/2330 train_time:49468ms step_avg:40.22ms
step:1231/2330 train_time:49503ms step_avg:40.21ms
step:1232/2330 train_time:49548ms step_avg:40.22ms
step:1233/2330 train_time:49583ms step_avg:40.21ms
step:1234/2330 train_time:49628ms step_avg:40.22ms
step:1235/2330 train_time:49664ms step_avg:40.21ms
step:1236/2330 train_time:49709ms step_avg:40.22ms
step:1237/2330 train_time:49744ms step_avg:40.21ms
step:1238/2330 train_time:49789ms step_avg:40.22ms
step:1239/2330 train_time:49824ms step_avg:40.21ms
step:1240/2330 train_time:49868ms step_avg:40.22ms
step:1241/2330 train_time:49904ms step_avg:40.21ms
step:1242/2330 train_time:49949ms step_avg:40.22ms
step:1243/2330 train_time:49984ms step_avg:40.21ms
step:1244/2330 train_time:50029ms step_avg:40.22ms
step:1245/2330 train_time:50064ms step_avg:40.21ms
step:1246/2330 train_time:50108ms step_avg:40.21ms
step:1247/2330 train_time:50143ms step_avg:40.21ms
step:1248/2330 train_time:50188ms step_avg:40.21ms
step:1249/2330 train_time:50223ms step_avg:40.21ms
step:1250/2330 train_time:50268ms step_avg:40.21ms
step:1250/2330 val_loss:5.1623 train_time:50355ms step_avg:40.28ms
step:1251/2330 train_time:50368ms step_avg:40.26ms
step:1252/2330 train_time:50380ms step_avg:40.24ms
step:1253/2330 train_time:50391ms step_avg:40.22ms
step:1254/2330 train_time:50427ms step_avg:40.21ms
step:1255/2330 train_time:50462ms step_avg:40.21ms
step:1256/2330 train_time:50505ms step_avg:40.21ms
step:1257/2330 train_time:50539ms step_avg:40.21ms
step:1258/2330 train_time:50583ms step_avg:40.21ms
step:1259/2330 train_time:50618ms step_avg:40.20ms
step:1260/2330 train_time:50665ms step_avg:40.21ms
step:1261/2330 train_time:50703ms step_avg:40.21ms
step:1262/2330 train_time:50750ms step_avg:40.21ms
step:1263/2330 train_time:50786ms step_avg:40.21ms
step:1264/2330 train_time:50832ms step_avg:40.21ms
step:1265/2330 train_time:50868ms step_avg:40.21ms
step:1266/2330 train_time:50912ms step_avg:40.22ms
step:1267/2330 train_time:50948ms step_avg:40.21ms
step:1268/2330 train_time:50993ms step_avg:40.22ms
step:1269/2330 train_time:51028ms step_avg:40.21ms
step:1270/2330 train_time:51073ms step_avg:40.21ms
step:1271/2330 train_time:51108ms step_avg:40.21ms
step:1272/2330 train_time:51153ms step_avg:40.21ms
step:1273/2330 train_time:51188ms step_avg:40.21ms
step:1274/2330 train_time:51232ms step_avg:40.21ms
step:1275/2330 train_time:51267ms step_avg:40.21ms
step:1276/2330 train_time:51314ms step_avg:40.21ms
step:1277/2330 train_time:51460ms step_avg:40.30ms
step:1278/2330 train_time:51503ms step_avg:40.30ms
step:1279/2330 train_time:51537ms step_avg:40.29ms
step:1280/2330 train_time:51582ms step_avg:40.30ms
step:1281/2330 train_time:51616ms step_avg:40.29ms
step:1282/2330 train_time:51660ms step_avg:40.30ms
step:1283/2330 train_time:51694ms step_avg:40.29ms
step:1284/2330 train_time:51738ms step_avg:40.29ms
step:1285/2330 train_time:51772ms step_avg:40.29ms
step:1286/2330 train_time:51816ms step_avg:40.29ms
step:1287/2330 train_time:51851ms step_avg:40.29ms
step:1288/2330 train_time:51895ms step_avg:40.29ms
step:1289/2330 train_time:51929ms step_avg:40.29ms
step:1290/2330 train_time:51973ms step_avg:40.29ms
step:1291/2330 train_time:52008ms step_avg:40.29ms
step:1292/2330 train_time:52052ms step_avg:40.29ms
step:1293/2330 train_time:52087ms step_avg:40.28ms
step:1294/2330 train_time:52130ms step_avg:40.29ms
step:1295/2330 train_time:52164ms step_avg:40.28ms
step:1296/2330 train_time:52209ms step_avg:40.28ms
step:1297/2330 train_time:52244ms step_avg:40.28ms
step:1298/2330 train_time:52287ms step_avg:40.28ms
step:1299/2330 train_time:52326ms step_avg:40.28ms
step:1300/2330 train_time:52377ms step_avg:40.29ms
step:1301/2330 train_time:52416ms step_avg:40.29ms
step:1302/2330 train_time:52464ms step_avg:40.29ms
step:1303/2330 train_time:52500ms step_avg:40.29ms
step:1304/2330 train_time:52545ms step_avg:40.30ms
step:1305/2330 train_time:52581ms step_avg:40.29ms
step:1306/2330 train_time:52625ms step_avg:40.30ms
step:1307/2330 train_time:52660ms step_avg:40.29ms
step:1308/2330 train_time:52705ms step_avg:40.29ms
step:1309/2330 train_time:52739ms step_avg:40.29ms
step:1310/2330 train_time:52784ms step_avg:40.29ms
step:1311/2330 train_time:52819ms step_avg:40.29ms
step:1312/2330 train_time:52863ms step_avg:40.29ms
step:1313/2330 train_time:52898ms step_avg:40.29ms
step:1314/2330 train_time:52942ms step_avg:40.29ms
step:1315/2330 train_time:52977ms step_avg:40.29ms
step:1316/2330 train_time:53022ms step_avg:40.29ms
step:1317/2330 train_time:53057ms step_avg:40.29ms
step:1318/2330 train_time:53101ms step_avg:40.29ms
step:1319/2330 train_time:53136ms step_avg:40.29ms
step:1320/2330 train_time:53181ms step_avg:40.29ms
step:1321/2330 train_time:53216ms step_avg:40.28ms
step:1322/2330 train_time:53263ms step_avg:40.29ms
step:1323/2330 train_time:53298ms step_avg:40.29ms
step:1324/2330 train_time:53345ms step_avg:40.29ms
step:1325/2330 train_time:53381ms step_avg:40.29ms
step:1326/2330 train_time:53427ms step_avg:40.29ms
step:1327/2330 train_time:53463ms step_avg:40.29ms
step:1328/2330 train_time:53508ms step_avg:40.29ms
step:1329/2330 train_time:53544ms step_avg:40.29ms
step:1330/2330 train_time:53589ms step_avg:40.29ms
step:1331/2330 train_time:53625ms step_avg:40.29ms
step:1332/2330 train_time:53669ms step_avg:40.29ms
step:1333/2330 train_time:53704ms step_avg:40.29ms
step:1334/2330 train_time:53749ms step_avg:40.29ms
step:1335/2330 train_time:53784ms step_avg:40.29ms
step:1336/2330 train_time:53827ms step_avg:40.29ms
step:1337/2330 train_time:53863ms step_avg:40.29ms
step:1338/2330 train_time:53907ms step_avg:40.29ms
step:1339/2330 train_time:53942ms step_avg:40.29ms
step:1340/2330 train_time:53987ms step_avg:40.29ms
step:1341/2330 train_time:54023ms step_avg:40.29ms
step:1342/2330 train_time:54067ms step_avg:40.29ms
step:1343/2330 train_time:54102ms step_avg:40.28ms
step:1344/2330 train_time:54147ms step_avg:40.29ms
step:1345/2330 train_time:54182ms step_avg:40.28ms
step:1346/2330 train_time:54227ms step_avg:40.29ms
step:1347/2330 train_time:54263ms step_avg:40.28ms
step:1348/2330 train_time:54307ms step_avg:40.29ms
step:1349/2330 train_time:54343ms step_avg:40.28ms
step:1350/2330 train_time:54388ms step_avg:40.29ms
step:1351/2330 train_time:54424ms step_avg:40.28ms
step:1352/2330 train_time:54469ms step_avg:40.29ms
step:1353/2330 train_time:54504ms step_avg:40.28ms
step:1354/2330 train_time:54549ms step_avg:40.29ms
step:1355/2330 train_time:54584ms step_avg:40.28ms
step:1356/2330 train_time:54628ms step_avg:40.29ms
step:1357/2330 train_time:54664ms step_avg:40.28ms
step:1358/2330 train_time:54708ms step_avg:40.29ms
step:1359/2330 train_time:54743ms step_avg:40.28ms
step:1360/2330 train_time:54788ms step_avg:40.29ms
step:1361/2330 train_time:54822ms step_avg:40.28ms
step:1362/2330 train_time:54868ms step_avg:40.28ms
step:1363/2330 train_time:54903ms step_avg:40.28ms
step:1364/2330 train_time:54947ms step_avg:40.28ms
step:1365/2330 train_time:54983ms step_avg:40.28ms
step:1366/2330 train_time:55027ms step_avg:40.28ms
step:1367/2330 train_time:55062ms step_avg:40.28ms
step:1368/2330 train_time:55107ms step_avg:40.28ms
step:1369/2330 train_time:55143ms step_avg:40.28ms
step:1370/2330 train_time:55188ms step_avg:40.28ms
step:1371/2330 train_time:55222ms step_avg:40.28ms
step:1372/2330 train_time:55267ms step_avg:40.28ms
step:1373/2330 train_time:55302ms step_avg:40.28ms
step:1374/2330 train_time:55348ms step_avg:40.28ms
step:1375/2330 train_time:55383ms step_avg:40.28ms
step:1376/2330 train_time:55428ms step_avg:40.28ms
step:1377/2330 train_time:55463ms step_avg:40.28ms
step:1378/2330 train_time:55507ms step_avg:40.28ms
step:1379/2330 train_time:55542ms step_avg:40.28ms
step:1380/2330 train_time:55587ms step_avg:40.28ms
step:1381/2330 train_time:55623ms step_avg:40.28ms
step:1382/2330 train_time:55667ms step_avg:40.28ms
step:1383/2330 train_time:55702ms step_avg:40.28ms
step:1384/2330 train_time:55746ms step_avg:40.28ms
step:1385/2330 train_time:55782ms step_avg:40.28ms
step:1386/2330 train_time:55826ms step_avg:40.28ms
step:1387/2330 train_time:55861ms step_avg:40.27ms
step:1388/2330 train_time:55906ms step_avg:40.28ms
step:1389/2330 train_time:55941ms step_avg:40.27ms
step:1390/2330 train_time:55986ms step_avg:40.28ms
step:1391/2330 train_time:56021ms step_avg:40.27ms
step:1392/2330 train_time:56065ms step_avg:40.28ms
step:1393/2330 train_time:56100ms step_avg:40.27ms
step:1394/2330 train_time:56145ms step_avg:40.28ms
step:1395/2330 train_time:56180ms step_avg:40.27ms
step:1396/2330 train_time:56224ms step_avg:40.28ms
step:1397/2330 train_time:56259ms step_avg:40.27ms
step:1398/2330 train_time:56303ms step_avg:40.27ms
step:1399/2330 train_time:56339ms step_avg:40.27ms
step:1400/2330 train_time:56384ms step_avg:40.27ms
step:1401/2330 train_time:56419ms step_avg:40.27ms
step:1402/2330 train_time:56465ms step_avg:40.27ms
step:1403/2330 train_time:56500ms step_avg:40.27ms
step:1404/2330 train_time:56544ms step_avg:40.27ms
step:1405/2330 train_time:56579ms step_avg:40.27ms
step:1406/2330 train_time:56624ms step_avg:40.27ms
step:1407/2330 train_time:56660ms step_avg:40.27ms
step:1408/2330 train_time:56705ms step_avg:40.27ms
step:1409/2330 train_time:56741ms step_avg:40.27ms
step:1410/2330 train_time:56785ms step_avg:40.27ms
step:1411/2330 train_time:56820ms step_avg:40.27ms
step:1412/2330 train_time:56865ms step_avg:40.27ms
step:1413/2330 train_time:56899ms step_avg:40.27ms
step:1414/2330 train_time:56944ms step_avg:40.27ms
step:1415/2330 train_time:56979ms step_avg:40.27ms
step:1416/2330 train_time:57023ms step_avg:40.27ms
step:1417/2330 train_time:57058ms step_avg:40.27ms
step:1418/2330 train_time:57102ms step_avg:40.27ms
step:1419/2330 train_time:57137ms step_avg:40.27ms
step:1420/2330 train_time:57182ms step_avg:40.27ms
step:1421/2330 train_time:57218ms step_avg:40.27ms
step:1422/2330 train_time:57262ms step_avg:40.27ms
step:1423/2330 train_time:57298ms step_avg:40.27ms
step:1424/2330 train_time:57342ms step_avg:40.27ms
step:1425/2330 train_time:57378ms step_avg:40.27ms
step:1426/2330 train_time:57423ms step_avg:40.27ms
step:1427/2330 train_time:57459ms step_avg:40.27ms
step:1428/2330 train_time:57503ms step_avg:40.27ms
step:1429/2330 train_time:57539ms step_avg:40.27ms
step:1430/2330 train_time:57585ms step_avg:40.27ms
step:1431/2330 train_time:57619ms step_avg:40.27ms
step:1432/2330 train_time:57664ms step_avg:40.27ms
step:1433/2330 train_time:57699ms step_avg:40.26ms
step:1434/2330 train_time:57745ms step_avg:40.27ms
step:1435/2330 train_time:57780ms step_avg:40.26ms
step:1436/2330 train_time:57825ms step_avg:40.27ms
step:1437/2330 train_time:57860ms step_avg:40.26ms
step:1438/2330 train_time:57905ms step_avg:40.27ms
step:1439/2330 train_time:57940ms step_avg:40.26ms
step:1440/2330 train_time:57985ms step_avg:40.27ms
step:1441/2330 train_time:58020ms step_avg:40.26ms
step:1442/2330 train_time:58065ms step_avg:40.27ms
step:1443/2330 train_time:58100ms step_avg:40.26ms
step:1444/2330 train_time:58145ms step_avg:40.27ms
step:1445/2330 train_time:58180ms step_avg:40.26ms
step:1446/2330 train_time:58225ms step_avg:40.27ms
step:1447/2330 train_time:58260ms step_avg:40.26ms
step:1448/2330 train_time:58305ms step_avg:40.27ms
step:1449/2330 train_time:58340ms step_avg:40.26ms
step:1450/2330 train_time:58385ms step_avg:40.27ms
step:1451/2330 train_time:58420ms step_avg:40.26ms
step:1452/2330 train_time:58464ms step_avg:40.26ms
step:1453/2330 train_time:58500ms step_avg:40.26ms
step:1454/2330 train_time:58545ms step_avg:40.26ms
step:1455/2330 train_time:58580ms step_avg:40.26ms
step:1456/2330 train_time:58624ms step_avg:40.26ms
step:1457/2330 train_time:58660ms step_avg:40.26ms
step:1458/2330 train_time:58704ms step_avg:40.26ms
step:1459/2330 train_time:58739ms step_avg:40.26ms
step:1460/2330 train_time:58784ms step_avg:40.26ms
step:1461/2330 train_time:58820ms step_avg:40.26ms
step:1462/2330 train_time:58865ms step_avg:40.26ms
step:1463/2330 train_time:58900ms step_avg:40.26ms
step:1464/2330 train_time:58944ms step_avg:40.26ms
step:1465/2330 train_time:58980ms step_avg:40.26ms
step:1466/2330 train_time:59024ms step_avg:40.26ms
step:1467/2330 train_time:59059ms step_avg:40.26ms
step:1468/2330 train_time:59104ms step_avg:40.26ms
step:1469/2330 train_time:59139ms step_avg:40.26ms
step:1470/2330 train_time:59184ms step_avg:40.26ms
step:1471/2330 train_time:59219ms step_avg:40.26ms
step:1472/2330 train_time:59263ms step_avg:40.26ms
step:1473/2330 train_time:59299ms step_avg:40.26ms
step:1474/2330 train_time:59344ms step_avg:40.26ms
step:1475/2330 train_time:59379ms step_avg:40.26ms
step:1476/2330 train_time:59423ms step_avg:40.26ms
step:1477/2330 train_time:59458ms step_avg:40.26ms
step:1478/2330 train_time:59504ms step_avg:40.26ms
step:1479/2330 train_time:59539ms step_avg:40.26ms
step:1480/2330 train_time:59584ms step_avg:40.26ms
step:1481/2330 train_time:59619ms step_avg:40.26ms
step:1482/2330 train_time:59664ms step_avg:40.26ms
step:1483/2330 train_time:59699ms step_avg:40.26ms
step:1484/2330 train_time:59745ms step_avg:40.26ms
step:1485/2330 train_time:59780ms step_avg:40.26ms
step:1486/2330 train_time:59824ms step_avg:40.26ms
step:1487/2330 train_time:59860ms step_avg:40.26ms
step:1488/2330 train_time:59904ms step_avg:40.26ms
step:1489/2330 train_time:59939ms step_avg:40.25ms
step:1490/2330 train_time:59984ms step_avg:40.26ms
step:1491/2330 train_time:60020ms step_avg:40.25ms
step:1492/2330 train_time:60064ms step_avg:40.26ms
step:1493/2330 train_time:60099ms step_avg:40.25ms
step:1494/2330 train_time:60144ms step_avg:40.26ms
step:1495/2330 train_time:60178ms step_avg:40.25ms
step:1496/2330 train_time:60224ms step_avg:40.26ms
step:1497/2330 train_time:60259ms step_avg:40.25ms
step:1498/2330 train_time:60303ms step_avg:40.26ms
step:1499/2330 train_time:60338ms step_avg:40.25ms
step:1500/2330 train_time:60383ms step_avg:40.26ms
step:1500/2330 val_loss:5.1287 train_time:60470ms step_avg:40.31ms
step:1501/2330 train_time:60484ms step_avg:40.30ms
step:1502/2330 train_time:60497ms step_avg:40.28ms
step:1503/2330 train_time:60509ms step_avg:40.26ms
step:1504/2330 train_time:60544ms step_avg:40.26ms
step:1505/2330 train_time:60577ms step_avg:40.25ms
step:1506/2330 train_time:60621ms step_avg:40.25ms
step:1507/2330 train_time:60656ms step_avg:40.25ms
step:1508/2330 train_time:60700ms step_avg:40.25ms
step:1509/2330 train_time:60735ms step_avg:40.25ms
step:1510/2330 train_time:60780ms step_avg:40.25ms
step:1511/2330 train_time:60819ms step_avg:40.25ms
step:1512/2330 train_time:60867ms step_avg:40.26ms
step:1513/2330 train_time:60905ms step_avg:40.25ms
step:1514/2330 train_time:61088ms step_avg:40.35ms
step:1515/2330 train_time:61122ms step_avg:40.34ms
step:1516/2330 train_time:61165ms step_avg:40.35ms
step:1517/2330 train_time:61380ms step_avg:40.46ms
step:1518/2330 train_time:61393ms step_avg:40.44ms
step:1519/2330 train_time:61407ms step_avg:40.43ms
step:1520/2330 train_time:61450ms step_avg:40.43ms
step:1521/2330 train_time:61484ms step_avg:40.42ms
step:1522/2330 train_time:61640ms step_avg:40.50ms
step:1523/2330 train_time:61674ms step_avg:40.49ms
step:1524/2330 train_time:61717ms step_avg:40.50ms
step:1525/2330 train_time:61751ms step_avg:40.49ms
step:1526/2330 train_time:61795ms step_avg:40.49ms
step:1527/2330 train_time:61829ms step_avg:40.49ms
step:1528/2330 train_time:61873ms step_avg:40.49ms
step:1529/2330 train_time:61908ms step_avg:40.49ms
step:1530/2330 train_time:61952ms step_avg:40.49ms
step:1531/2330 train_time:61986ms step_avg:40.49ms
step:1532/2330 train_time:62030ms step_avg:40.49ms
step:1533/2330 train_time:62064ms step_avg:40.49ms
step:1534/2330 train_time:62107ms step_avg:40.49ms
step:1535/2330 train_time:62142ms step_avg:40.48ms
step:1536/2330 train_time:62186ms step_avg:40.49ms
step:1537/2330 train_time:62221ms step_avg:40.48ms
step:1538/2330 train_time:62265ms step_avg:40.48ms
step:1539/2330 train_time:62299ms step_avg:40.48ms
step:1540/2330 train_time:62343ms step_avg:40.48ms
step:1541/2330 train_time:62377ms step_avg:40.48ms
step:1542/2330 train_time:62421ms step_avg:40.48ms
step:1543/2330 train_time:62456ms step_avg:40.48ms
step:1544/2330 train_time:62504ms step_avg:40.48ms
step:1545/2330 train_time:62546ms step_avg:40.48ms
step:1546/2330 train_time:62596ms step_avg:40.49ms
step:1547/2330 train_time:62633ms step_avg:40.49ms
step:1548/2330 train_time:62679ms step_avg:40.49ms
step:1549/2330 train_time:62714ms step_avg:40.49ms
step:1550/2330 train_time:62759ms step_avg:40.49ms
step:1551/2330 train_time:62794ms step_avg:40.49ms
step:1552/2330 train_time:62839ms step_avg:40.49ms
step:1553/2330 train_time:62874ms step_avg:40.49ms
step:1554/2330 train_time:62918ms step_avg:40.49ms
step:1555/2330 train_time:62953ms step_avg:40.48ms
step:1556/2330 train_time:62998ms step_avg:40.49ms
step:1557/2330 train_time:63033ms step_avg:40.48ms
step:1558/2330 train_time:63077ms step_avg:40.49ms
step:1559/2330 train_time:63113ms step_avg:40.48ms
step:1560/2330 train_time:63157ms step_avg:40.49ms
step:1561/2330 train_time:63192ms step_avg:40.48ms
step:1562/2330 train_time:63235ms step_avg:40.48ms
step:1563/2330 train_time:63270ms step_avg:40.48ms
step:1564/2330 train_time:63314ms step_avg:40.48ms
step:1565/2330 train_time:63349ms step_avg:40.48ms
step:1566/2330 train_time:63393ms step_avg:40.48ms
step:1567/2330 train_time:63430ms step_avg:40.48ms
step:1568/2330 train_time:63476ms step_avg:40.48ms
step:1569/2330 train_time:63512ms step_avg:40.48ms
step:1570/2330 train_time:63557ms step_avg:40.48ms
step:1571/2330 train_time:63593ms step_avg:40.48ms
step:1572/2330 train_time:63640ms step_avg:40.48ms
step:1573/2330 train_time:63676ms step_avg:40.48ms
step:1574/2330 train_time:63721ms step_avg:40.48ms
step:1575/2330 train_time:63757ms step_avg:40.48ms
step:1576/2330 train_time:63802ms step_avg:40.48ms
step:1577/2330 train_time:63837ms step_avg:40.48ms
step:1578/2330 train_time:63882ms step_avg:40.48ms
step:1579/2330 train_time:63917ms step_avg:40.48ms
step:1580/2330 train_time:63962ms step_avg:40.48ms
step:1581/2330 train_time:63997ms step_avg:40.48ms
step:1582/2330 train_time:64042ms step_avg:40.48ms
step:1583/2330 train_time:64077ms step_avg:40.48ms
step:1584/2330 train_time:64122ms step_avg:40.48ms
step:1585/2330 train_time:64157ms step_avg:40.48ms
step:1586/2330 train_time:64201ms step_avg:40.48ms
step:1587/2330 train_time:64237ms step_avg:40.48ms
step:1588/2330 train_time:64281ms step_avg:40.48ms
step:1589/2330 train_time:64316ms step_avg:40.48ms
step:1590/2330 train_time:64361ms step_avg:40.48ms
step:1591/2330 train_time:64397ms step_avg:40.48ms
step:1592/2330 train_time:64442ms step_avg:40.48ms
step:1593/2330 train_time:64478ms step_avg:40.48ms
step:1594/2330 train_time:64523ms step_avg:40.48ms
step:1595/2330 train_time:64559ms step_avg:40.48ms
step:1596/2330 train_time:64605ms step_avg:40.48ms
step:1597/2330 train_time:64642ms step_avg:40.48ms
step:1598/2330 train_time:64690ms step_avg:40.48ms
step:1599/2330 train_time:64725ms step_avg:40.48ms
step:1600/2330 train_time:64770ms step_avg:40.48ms
step:1601/2330 train_time:64806ms step_avg:40.48ms
step:1602/2330 train_time:64851ms step_avg:40.48ms
step:1603/2330 train_time:64885ms step_avg:40.48ms
step:1604/2330 train_time:64930ms step_avg:40.48ms
step:1605/2330 train_time:64966ms step_avg:40.48ms
step:1606/2330 train_time:65011ms step_avg:40.48ms
step:1607/2330 train_time:65046ms step_avg:40.48ms
step:1608/2330 train_time:65091ms step_avg:40.48ms
step:1609/2330 train_time:65126ms step_avg:40.48ms
step:1610/2330 train_time:65170ms step_avg:40.48ms
step:1611/2330 train_time:65205ms step_avg:40.47ms
step:1612/2330 train_time:65250ms step_avg:40.48ms
step:1613/2330 train_time:65285ms step_avg:40.47ms
step:1614/2330 train_time:65331ms step_avg:40.48ms
step:1615/2330 train_time:65366ms step_avg:40.47ms
step:1616/2330 train_time:65411ms step_avg:40.48ms
step:1617/2330 train_time:65447ms step_avg:40.47ms
step:1618/2330 train_time:65492ms step_avg:40.48ms
step:1619/2330 train_time:65527ms step_avg:40.47ms
step:1620/2330 train_time:65572ms step_avg:40.48ms
step:1621/2330 train_time:65608ms step_avg:40.47ms
step:1622/2330 train_time:65654ms step_avg:40.48ms
step:1623/2330 train_time:65688ms step_avg:40.47ms
step:1624/2330 train_time:65733ms step_avg:40.48ms
step:1625/2330 train_time:65768ms step_avg:40.47ms
step:1626/2330 train_time:65812ms step_avg:40.47ms
step:1627/2330 train_time:65847ms step_avg:40.47ms
step:1628/2330 train_time:65892ms step_avg:40.47ms
step:1629/2330 train_time:65928ms step_avg:40.47ms
step:1630/2330 train_time:65972ms step_avg:40.47ms
step:1631/2330 train_time:66008ms step_avg:40.47ms
step:1632/2330 train_time:66053ms step_avg:40.47ms
step:1633/2330 train_time:66088ms step_avg:40.47ms
step:1634/2330 train_time:66132ms step_avg:40.47ms
step:1635/2330 train_time:66167ms step_avg:40.47ms
step:1636/2330 train_time:66211ms step_avg:40.47ms
step:1637/2330 train_time:66246ms step_avg:40.47ms
step:1638/2330 train_time:66291ms step_avg:40.47ms
step:1639/2330 train_time:66326ms step_avg:40.47ms
step:1640/2330 train_time:66371ms step_avg:40.47ms
step:1641/2330 train_time:66407ms step_avg:40.47ms
step:1642/2330 train_time:66452ms step_avg:40.47ms
step:1643/2330 train_time:66487ms step_avg:40.47ms
step:1644/2330 train_time:66532ms step_avg:40.47ms
step:1645/2330 train_time:66567ms step_avg:40.47ms
step:1646/2330 train_time:66612ms step_avg:40.47ms
step:1647/2330 train_time:66647ms step_avg:40.47ms
step:1648/2330 train_time:66692ms step_avg:40.47ms
step:1649/2330 train_time:66727ms step_avg:40.47ms
step:1650/2330 train_time:66772ms step_avg:40.47ms
step:1651/2330 train_time:66807ms step_avg:40.46ms
step:1652/2330 train_time:66851ms step_avg:40.47ms
step:1653/2330 train_time:66887ms step_avg:40.46ms
step:1654/2330 train_time:66931ms step_avg:40.47ms
step:1655/2330 train_time:66968ms step_avg:40.46ms
step:1656/2330 train_time:67013ms step_avg:40.47ms
step:1657/2330 train_time:67048ms step_avg:40.46ms
step:1658/2330 train_time:67093ms step_avg:40.47ms
step:1659/2330 train_time:67127ms step_avg:40.46ms
step:1660/2330 train_time:67172ms step_avg:40.47ms
step:1661/2330 train_time:67207ms step_avg:40.46ms
step:1662/2330 train_time:67251ms step_avg:40.46ms
step:1663/2330 train_time:67286ms step_avg:40.46ms
step:1664/2330 train_time:67331ms step_avg:40.46ms
step:1665/2330 train_time:67366ms step_avg:40.46ms
step:1666/2330 train_time:67411ms step_avg:40.46ms
step:1667/2330 train_time:67446ms step_avg:40.46ms
step:1668/2330 train_time:67490ms step_avg:40.46ms
step:1669/2330 train_time:67526ms step_avg:40.46ms
step:1670/2330 train_time:67571ms step_avg:40.46ms
step:1671/2330 train_time:67606ms step_avg:40.46ms
step:1672/2330 train_time:67652ms step_avg:40.46ms
step:1673/2330 train_time:67687ms step_avg:40.46ms
step:1674/2330 train_time:67731ms step_avg:40.46ms
step:1675/2330 train_time:67766ms step_avg:40.46ms
step:1676/2330 train_time:67811ms step_avg:40.46ms
step:1677/2330 train_time:67846ms step_avg:40.46ms
step:1678/2330 train_time:67891ms step_avg:40.46ms
step:1679/2330 train_time:67927ms step_avg:40.46ms
step:1680/2330 train_time:67972ms step_avg:40.46ms
step:1681/2330 train_time:68007ms step_avg:40.46ms
step:1682/2330 train_time:68052ms step_avg:40.46ms
step:1683/2330 train_time:68088ms step_avg:40.46ms
step:1684/2330 train_time:68132ms step_avg:40.46ms
step:1685/2330 train_time:68167ms step_avg:40.46ms
step:1686/2330 train_time:68211ms step_avg:40.46ms
step:1687/2330 train_time:68247ms step_avg:40.45ms
step:1688/2330 train_time:68291ms step_avg:40.46ms
step:1689/2330 train_time:68327ms step_avg:40.45ms
step:1690/2330 train_time:68371ms step_avg:40.46ms
step:1691/2330 train_time:68406ms step_avg:40.45ms
step:1692/2330 train_time:68452ms step_avg:40.46ms
step:1693/2330 train_time:68486ms step_avg:40.45ms
step:1694/2330 train_time:68531ms step_avg:40.46ms
step:1695/2330 train_time:68567ms step_avg:40.45ms
step:1696/2330 train_time:68611ms step_avg:40.45ms
step:1697/2330 train_time:68646ms step_avg:40.45ms
step:1698/2330 train_time:68691ms step_avg:40.45ms
step:1699/2330 train_time:68726ms step_avg:40.45ms
step:1700/2330 train_time:68771ms step_avg:40.45ms
step:1701/2330 train_time:68807ms step_avg:40.45ms
step:1702/2330 train_time:68852ms step_avg:40.45ms
step:1703/2330 train_time:68887ms step_avg:40.45ms
step:1704/2330 train_time:68932ms step_avg:40.45ms
step:1705/2330 train_time:68967ms step_avg:40.45ms
step:1706/2330 train_time:69012ms step_avg:40.45ms
step:1707/2330 train_time:69047ms step_avg:40.45ms
step:1708/2330 train_time:69092ms step_avg:40.45ms
step:1709/2330 train_time:69127ms step_avg:40.45ms
step:1710/2330 train_time:69171ms step_avg:40.45ms
step:1711/2330 train_time:69207ms step_avg:40.45ms
step:1712/2330 train_time:69252ms step_avg:40.45ms
step:1713/2330 train_time:69288ms step_avg:40.45ms
step:1714/2330 train_time:69332ms step_avg:40.45ms
step:1715/2330 train_time:69367ms step_avg:40.45ms
step:1716/2330 train_time:69412ms step_avg:40.45ms
step:1717/2330 train_time:69447ms step_avg:40.45ms
step:1718/2330 train_time:69491ms step_avg:40.45ms
step:1719/2330 train_time:69526ms step_avg:40.45ms
step:1720/2330 train_time:69571ms step_avg:40.45ms
step:1721/2330 train_time:69607ms step_avg:40.45ms
step:1722/2330 train_time:69652ms step_avg:40.45ms
step:1723/2330 train_time:69687ms step_avg:40.45ms
step:1724/2330 train_time:69731ms step_avg:40.45ms
step:1725/2330 train_time:69766ms step_avg:40.44ms
step:1726/2330 train_time:69811ms step_avg:40.45ms
step:1727/2330 train_time:69847ms step_avg:40.44ms
step:1728/2330 train_time:69892ms step_avg:40.45ms
step:1729/2330 train_time:69927ms step_avg:40.44ms
step:1730/2330 train_time:69972ms step_avg:40.45ms
step:1731/2330 train_time:70007ms step_avg:40.44ms
step:1732/2330 train_time:70052ms step_avg:40.45ms
step:1733/2330 train_time:70088ms step_avg:40.44ms
step:1734/2330 train_time:70132ms step_avg:40.45ms
step:1735/2330 train_time:70168ms step_avg:40.44ms
step:1736/2330 train_time:70212ms step_avg:40.44ms
step:1737/2330 train_time:70247ms step_avg:40.44ms
step:1738/2330 train_time:70293ms step_avg:40.44ms
step:1739/2330 train_time:70328ms step_avg:40.44ms
step:1740/2330 train_time:70372ms step_avg:40.44ms
step:1741/2330 train_time:70408ms step_avg:40.44ms
step:1742/2330 train_time:70452ms step_avg:40.44ms
step:1743/2330 train_time:70487ms step_avg:40.44ms
step:1744/2330 train_time:70532ms step_avg:40.44ms
step:1745/2330 train_time:70567ms step_avg:40.44ms
step:1746/2330 train_time:70612ms step_avg:40.44ms
step:1747/2330 train_time:70648ms step_avg:40.44ms
step:1748/2330 train_time:70693ms step_avg:40.44ms
step:1749/2330 train_time:70728ms step_avg:40.44ms
step:1750/2330 train_time:70772ms step_avg:40.44ms
step:1750/2330 val_loss:5.0941 train_time:70859ms step_avg:40.49ms
step:1751/2330 train_time:70872ms step_avg:40.48ms
step:1752/2330 train_time:70885ms step_avg:40.46ms
step:1753/2330 train_time:70896ms step_avg:40.44ms
step:1754/2330 train_time:70932ms step_avg:40.44ms
step:1755/2330 train_time:70966ms step_avg:40.44ms
step:1756/2330 train_time:71010ms step_avg:40.44ms
step:1757/2330 train_time:71044ms step_avg:40.43ms
step:1758/2330 train_time:71088ms step_avg:40.44ms
step:1759/2330 train_time:71122ms step_avg:40.43ms
step:1760/2330 train_time:71166ms step_avg:40.44ms
step:1761/2330 train_time:71203ms step_avg:40.43ms
step:1762/2330 train_time:71250ms step_avg:40.44ms
step:1763/2330 train_time:71286ms step_avg:40.43ms
step:1764/2330 train_time:71331ms step_avg:40.44ms
step:1765/2330 train_time:71366ms step_avg:40.43ms
step:1766/2330 train_time:71409ms step_avg:40.44ms
step:1767/2330 train_time:71444ms step_avg:40.43ms
step:1768/2330 train_time:71488ms step_avg:40.43ms
step:1769/2330 train_time:71522ms step_avg:40.43ms
step:1770/2330 train_time:71566ms step_avg:40.43ms
step:1771/2330 train_time:71601ms step_avg:40.43ms
step:1772/2330 train_time:71644ms step_avg:40.43ms
step:1773/2330 train_time:71679ms step_avg:40.43ms
step:1774/2330 train_time:71725ms step_avg:40.43ms
step:1775/2330 train_time:71765ms step_avg:40.43ms
step:1776/2330 train_time:71815ms step_avg:40.44ms
step:1777/2330 train_time:71852ms step_avg:40.43ms
step:1778/2330 train_time:71896ms step_avg:40.44ms
step:1779/2330 train_time:71931ms step_avg:40.43ms
step:1780/2330 train_time:71977ms step_avg:40.44ms
step:1781/2330 train_time:72013ms step_avg:40.43ms
step:1782/2330 train_time:72057ms step_avg:40.44ms
step:1783/2330 train_time:72092ms step_avg:40.43ms
step:1784/2330 train_time:72137ms step_avg:40.44ms
step:1785/2330 train_time:72173ms step_avg:40.43ms
step:1786/2330 train_time:72218ms step_avg:40.44ms
step:1787/2330 train_time:72253ms step_avg:40.43ms
step:1788/2330 train_time:72297ms step_avg:40.43ms
step:1789/2330 train_time:72333ms step_avg:40.43ms
step:1790/2330 train_time:72377ms step_avg:40.43ms
step:1791/2330 train_time:72412ms step_avg:40.43ms
step:1792/2330 train_time:72457ms step_avg:40.43ms
step:1793/2330 train_time:72493ms step_avg:40.43ms
step:1794/2330 train_time:72537ms step_avg:40.43ms
step:1795/2330 train_time:72572ms step_avg:40.43ms
step:1796/2330 train_time:72617ms step_avg:40.43ms
step:1797/2330 train_time:72652ms step_avg:40.43ms
step:1798/2330 train_time:72697ms step_avg:40.43ms
step:1799/2330 train_time:72733ms step_avg:40.43ms
step:1800/2330 train_time:72779ms step_avg:40.43ms
step:1801/2330 train_time:72816ms step_avg:40.43ms
step:1802/2330 train_time:72863ms step_avg:40.43ms
step:1803/2330 train_time:72900ms step_avg:40.43ms
step:1804/2330 train_time:72946ms step_avg:40.44ms
step:1805/2330 train_time:72982ms step_avg:40.43ms
step:1806/2330 train_time:73026ms step_avg:40.44ms
step:1807/2330 train_time:73061ms step_avg:40.43ms
step:1808/2330 train_time:73106ms step_avg:40.43ms
step:1809/2330 train_time:73140ms step_avg:40.43ms
step:1810/2330 train_time:73185ms step_avg:40.43ms
step:1811/2330 train_time:73221ms step_avg:40.43ms
step:1812/2330 train_time:73266ms step_avg:40.43ms
step:1813/2330 train_time:73301ms step_avg:40.43ms
step:1814/2330 train_time:73345ms step_avg:40.43ms
step:1815/2330 train_time:73380ms step_avg:40.43ms
step:1816/2330 train_time:73425ms step_avg:40.43ms
step:1817/2330 train_time:73460ms step_avg:40.43ms
step:1818/2330 train_time:73504ms step_avg:40.43ms
step:1819/2330 train_time:73539ms step_avg:40.43ms
step:1820/2330 train_time:73584ms step_avg:40.43ms
step:1821/2330 train_time:73619ms step_avg:40.43ms
step:1822/2330 train_time:73664ms step_avg:40.43ms
step:1823/2330 train_time:73700ms step_avg:40.43ms
step:1824/2330 train_time:73744ms step_avg:40.43ms
step:1825/2330 train_time:73780ms step_avg:40.43ms
step:1826/2330 train_time:73826ms step_avg:40.43ms
step:1827/2330 train_time:73862ms step_avg:40.43ms
step:1828/2330 train_time:73907ms step_avg:40.43ms
step:1829/2330 train_time:73943ms step_avg:40.43ms
step:1830/2330 train_time:73988ms step_avg:40.43ms
step:1831/2330 train_time:74023ms step_avg:40.43ms
step:1832/2330 train_time:74067ms step_avg:40.43ms
step:1833/2330 train_time:74102ms step_avg:40.43ms
step:1834/2330 train_time:74147ms step_avg:40.43ms
step:1835/2330 train_time:74183ms step_avg:40.43ms
step:1836/2330 train_time:74228ms step_avg:40.43ms
step:1837/2330 train_time:74262ms step_avg:40.43ms
step:1838/2330 train_time:74307ms step_avg:40.43ms
step:1839/2330 train_time:74342ms step_avg:40.42ms
step:1840/2330 train_time:74386ms step_avg:40.43ms
step:1841/2330 train_time:74421ms step_avg:40.42ms
step:1842/2330 train_time:74466ms step_avg:40.43ms
step:1843/2330 train_time:74501ms step_avg:40.42ms
step:1844/2330 train_time:74546ms step_avg:40.43ms
step:1845/2330 train_time:74581ms step_avg:40.42ms
step:1846/2330 train_time:74625ms step_avg:40.43ms
step:1847/2330 train_time:74661ms step_avg:40.42ms
step:1848/2330 train_time:74705ms step_avg:40.42ms
step:1849/2330 train_time:74740ms step_avg:40.42ms
step:1850/2330 train_time:74785ms step_avg:40.42ms
step:1851/2330 train_time:74822ms step_avg:40.42ms
step:1852/2330 train_time:74866ms step_avg:40.42ms
step:1853/2330 train_time:74902ms step_avg:40.42ms
step:1854/2330 train_time:74948ms step_avg:40.43ms
step:1855/2330 train_time:74984ms step_avg:40.42ms
step:1856/2330 train_time:75028ms step_avg:40.42ms
step:1857/2330 train_time:75064ms step_avg:40.42ms
step:1858/2330 train_time:75109ms step_avg:40.42ms
step:1859/2330 train_time:75144ms step_avg:40.42ms
step:1860/2330 train_time:75189ms step_avg:40.42ms
step:1861/2330 train_time:75224ms step_avg:40.42ms
step:1862/2330 train_time:75268ms step_avg:40.42ms
step:1863/2330 train_time:75303ms step_avg:40.42ms
step:1864/2330 train_time:75348ms step_avg:40.42ms
step:1865/2330 train_time:75383ms step_avg:40.42ms
step:1866/2330 train_time:75428ms step_avg:40.42ms
step:1867/2330 train_time:75463ms step_avg:40.42ms
step:1868/2330 train_time:75508ms step_avg:40.42ms
step:1869/2330 train_time:75543ms step_avg:40.42ms
step:1870/2330 train_time:75587ms step_avg:40.42ms
step:1871/2330 train_time:75623ms step_avg:40.42ms
step:1872/2330 train_time:75667ms step_avg:40.42ms
step:1873/2330 train_time:75702ms step_avg:40.42ms
step:1874/2330 train_time:75747ms step_avg:40.42ms
step:1875/2330 train_time:75782ms step_avg:40.42ms
step:1876/2330 train_time:75827ms step_avg:40.42ms
step:1877/2330 train_time:75862ms step_avg:40.42ms
step:1878/2330 train_time:75907ms step_avg:40.42ms
step:1879/2330 train_time:75943ms step_avg:40.42ms
step:1880/2330 train_time:75988ms step_avg:40.42ms
step:1881/2330 train_time:76024ms step_avg:40.42ms
step:1882/2330 train_time:76069ms step_avg:40.42ms
step:1883/2330 train_time:76103ms step_avg:40.42ms
step:1884/2330 train_time:76148ms step_avg:40.42ms
step:1885/2330 train_time:76182ms step_avg:40.42ms
step:1886/2330 train_time:76228ms step_avg:40.42ms
step:1887/2330 train_time:76263ms step_avg:40.41ms
step:1888/2330 train_time:76307ms step_avg:40.42ms
step:1889/2330 train_time:76342ms step_avg:40.41ms
step:1890/2330 train_time:76387ms step_avg:40.42ms
step:1891/2330 train_time:76422ms step_avg:40.41ms
step:1892/2330 train_time:76467ms step_avg:40.42ms
step:1893/2330 train_time:76502ms step_avg:40.41ms
step:1894/2330 train_time:76547ms step_avg:40.42ms
step:1895/2330 train_time:76582ms step_avg:40.41ms
step:1896/2330 train_time:76627ms step_avg:40.41ms
step:1897/2330 train_time:76661ms step_avg:40.41ms
step:1898/2330 train_time:76705ms step_avg:40.41ms
step:1899/2330 train_time:76741ms step_avg:40.41ms
step:1900/2330 train_time:76787ms step_avg:40.41ms
step:1901/2330 train_time:76822ms step_avg:40.41ms
step:1902/2330 train_time:76867ms step_avg:40.41ms
step:1903/2330 train_time:76902ms step_avg:40.41ms
step:1904/2330 train_time:76947ms step_avg:40.41ms
step:1905/2330 train_time:76982ms step_avg:40.41ms
step:1906/2330 train_time:77028ms step_avg:40.41ms
step:1907/2330 train_time:77063ms step_avg:40.41ms
step:1908/2330 train_time:77107ms step_avg:40.41ms
step:1909/2330 train_time:77143ms step_avg:40.41ms
step:1910/2330 train_time:77187ms step_avg:40.41ms
step:1911/2330 train_time:77223ms step_avg:40.41ms
step:1912/2330 train_time:77267ms step_avg:40.41ms
step:1913/2330 train_time:77302ms step_avg:40.41ms
step:1914/2330 train_time:77347ms step_avg:40.41ms
step:1915/2330 train_time:77382ms step_avg:40.41ms
step:1916/2330 train_time:77426ms step_avg:40.41ms
step:1917/2330 train_time:77462ms step_avg:40.41ms
step:1918/2330 train_time:77507ms step_avg:40.41ms
step:1919/2330 train_time:77541ms step_avg:40.41ms
step:1920/2330 train_time:77587ms step_avg:40.41ms
step:1921/2330 train_time:77622ms step_avg:40.41ms
step:1922/2330 train_time:77666ms step_avg:40.41ms
step:1923/2330 train_time:77701ms step_avg:40.41ms
step:1924/2330 train_time:77746ms step_avg:40.41ms
step:1925/2330 train_time:77782ms step_avg:40.41ms
step:1926/2330 train_time:77826ms step_avg:40.41ms
step:1927/2330 train_time:77861ms step_avg:40.41ms
step:1928/2330 train_time:77906ms step_avg:40.41ms
step:1929/2330 train_time:77941ms step_avg:40.41ms
step:1930/2330 train_time:77986ms step_avg:40.41ms
step:1931/2330 train_time:78022ms step_avg:40.40ms
step:1932/2330 train_time:78067ms step_avg:40.41ms
step:1933/2330 train_time:78102ms step_avg:40.40ms
step:1934/2330 train_time:78147ms step_avg:40.41ms
step:1935/2330 train_time:78182ms step_avg:40.40ms
step:1936/2330 train_time:78227ms step_avg:40.41ms
step:1937/2330 train_time:78263ms step_avg:40.40ms
step:1938/2330 train_time:78308ms step_avg:40.41ms
step:1939/2330 train_time:78343ms step_avg:40.40ms
step:1940/2330 train_time:78387ms step_avg:40.41ms
step:1941/2330 train_time:78423ms step_avg:40.40ms
step:1942/2330 train_time:78468ms step_avg:40.41ms
step:1943/2330 train_time:78502ms step_avg:40.40ms
step:1944/2330 train_time:78547ms step_avg:40.40ms
step:1945/2330 train_time:78583ms step_avg:40.40ms
step:1946/2330 train_time:78627ms step_avg:40.40ms
step:1947/2330 train_time:78662ms step_avg:40.40ms
step:1948/2330 train_time:78707ms step_avg:40.40ms
step:1949/2330 train_time:78742ms step_avg:40.40ms
step:1950/2330 train_time:78787ms step_avg:40.40ms
step:1951/2330 train_time:78822ms step_avg:40.40ms
step:1952/2330 train_time:78867ms step_avg:40.40ms
step:1953/2330 train_time:78902ms step_avg:40.40ms
step:1954/2330 train_time:78947ms step_avg:40.40ms
step:1955/2330 train_time:78982ms step_avg:40.40ms
step:1956/2330 train_time:79027ms step_avg:40.40ms
step:1957/2330 train_time:79063ms step_avg:40.40ms
step:1958/2330 train_time:79107ms step_avg:40.40ms
step:1959/2330 train_time:79143ms step_avg:40.40ms
step:1960/2330 train_time:79188ms step_avg:40.40ms
step:1961/2330 train_time:79223ms step_avg:40.40ms
step:1962/2330 train_time:79268ms step_avg:40.40ms
step:1963/2330 train_time:79303ms step_avg:40.40ms
step:1964/2330 train_time:79348ms step_avg:40.40ms
step:1965/2330 train_time:79384ms step_avg:40.40ms
step:1966/2330 train_time:79429ms step_avg:40.40ms
step:1967/2330 train_time:79463ms step_avg:40.40ms
step:1968/2330 train_time:79508ms step_avg:40.40ms
step:1969/2330 train_time:79543ms step_avg:40.40ms
step:1970/2330 train_time:79588ms step_avg:40.40ms
step:1971/2330 train_time:79624ms step_avg:40.40ms
step:1972/2330 train_time:79668ms step_avg:40.40ms
step:1973/2330 train_time:79703ms step_avg:40.40ms
step:1974/2330 train_time:79748ms step_avg:40.40ms
step:1975/2330 train_time:79783ms step_avg:40.40ms
step:1976/2330 train_time:79828ms step_avg:40.40ms
step:1977/2330 train_time:79863ms step_avg:40.40ms
step:1978/2330 train_time:79908ms step_avg:40.40ms
step:1979/2330 train_time:79943ms step_avg:40.40ms
step:1980/2330 train_time:79987ms step_avg:40.40ms
step:1981/2330 train_time:80023ms step_avg:40.40ms
step:1982/2330 train_time:80067ms step_avg:40.40ms
step:1983/2330 train_time:80102ms step_avg:40.39ms
step:1984/2330 train_time:80147ms step_avg:40.40ms
step:1985/2330 train_time:80183ms step_avg:40.39ms
step:1986/2330 train_time:80228ms step_avg:40.40ms
step:1987/2330 train_time:80263ms step_avg:40.39ms
step:1988/2330 train_time:80308ms step_avg:40.40ms
step:1989/2330 train_time:80343ms step_avg:40.39ms
step:1990/2330 train_time:80388ms step_avg:40.40ms
step:1991/2330 train_time:80423ms step_avg:40.39ms
step:1992/2330 train_time:80468ms step_avg:40.40ms
step:1993/2330 train_time:80504ms step_avg:40.39ms
step:1994/2330 train_time:80549ms step_avg:40.40ms
step:1995/2330 train_time:80584ms step_avg:40.39ms
step:1996/2330 train_time:80629ms step_avg:40.40ms
step:1997/2330 train_time:80663ms step_avg:40.39ms
step:1998/2330 train_time:80708ms step_avg:40.39ms
step:1999/2330 train_time:80743ms step_avg:40.39ms
step:2000/2330 train_time:80788ms step_avg:40.39ms
step:2000/2330 val_loss:5.0639 train_time:80874ms step_avg:40.44ms
step:2001/2330 train_time:80887ms step_avg:40.42ms
step:2002/2330 train_time:80899ms step_avg:40.41ms
step:2003/2330 train_time:80910ms step_avg:40.39ms
step:2004/2330 train_time:80948ms step_avg:40.39ms
step:2005/2330 train_time:80982ms step_avg:40.39ms
step:2006/2330 train_time:81027ms step_avg:40.39ms
step:2007/2330 train_time:81061ms step_avg:40.39ms
step:2008/2330 train_time:81105ms step_avg:40.39ms
step:2009/2330 train_time:81140ms step_avg:40.39ms
step:2010/2330 train_time:81188ms step_avg:40.39ms
step:2011/2330 train_time:81226ms step_avg:40.39ms
step:2012/2330 train_time:81273ms step_avg:40.39ms
step:2013/2330 train_time:81309ms step_avg:40.39ms
step:2014/2330 train_time:81354ms step_avg:40.39ms
step:2015/2330 train_time:81390ms step_avg:40.39ms
step:2016/2330 train_time:81433ms step_avg:40.39ms
step:2017/2330 train_time:81469ms step_avg:40.39ms
step:2018/2330 train_time:81513ms step_avg:40.39ms
step:2019/2330 train_time:81548ms step_avg:40.39ms
step:2020/2330 train_time:81593ms step_avg:40.39ms
step:2021/2330 train_time:81628ms step_avg:40.39ms
step:2022/2330 train_time:81671ms step_avg:40.39ms
step:2023/2330 train_time:81706ms step_avg:40.39ms
step:2024/2330 train_time:81750ms step_avg:40.39ms
step:2025/2330 train_time:81786ms step_avg:40.39ms
step:2026/2330 train_time:81831ms step_avg:40.39ms
step:2027/2330 train_time:81867ms step_avg:40.39ms
step:2028/2330 train_time:81911ms step_avg:40.39ms
step:2029/2330 train_time:81946ms step_avg:40.39ms
step:2030/2330 train_time:81990ms step_avg:40.39ms
step:2031/2330 train_time:82024ms step_avg:40.39ms
step:2032/2330 train_time:82069ms step_avg:40.39ms
step:2033/2330 train_time:82104ms step_avg:40.39ms
step:2034/2330 train_time:82152ms step_avg:40.39ms
step:2035/2330 train_time:82188ms step_avg:40.39ms
step:2036/2330 train_time:82234ms step_avg:40.39ms
step:2037/2330 train_time:82270ms step_avg:40.39ms
step:2038/2330 train_time:82315ms step_avg:40.39ms
step:2039/2330 train_time:82350ms step_avg:40.39ms
step:2040/2330 train_time:82395ms step_avg:40.39ms
step:2041/2330 train_time:82431ms step_avg:40.39ms
step:2042/2330 train_time:82475ms step_avg:40.39ms
step:2043/2330 train_time:82510ms step_avg:40.39ms
step:2044/2330 train_time:82555ms step_avg:40.39ms
step:2045/2330 train_time:82589ms step_avg:40.39ms
step:2046/2330 train_time:82633ms step_avg:40.39ms
step:2047/2330 train_time:82668ms step_avg:40.39ms
step:2048/2330 train_time:82713ms step_avg:40.39ms
step:2049/2330 train_time:82747ms step_avg:40.38ms
step:2050/2330 train_time:82793ms step_avg:40.39ms
step:2051/2330 train_time:82827ms step_avg:40.38ms
step:2052/2330 train_time:82872ms step_avg:40.39ms
step:2053/2330 train_time:82907ms step_avg:40.38ms
step:2054/2330 train_time:82951ms step_avg:40.39ms
step:2055/2330 train_time:82986ms step_avg:40.38ms
step:2056/2330 train_time:83030ms step_avg:40.38ms
step:2057/2330 train_time:83066ms step_avg:40.38ms
step:2058/2330 train_time:83112ms step_avg:40.38ms
step:2059/2330 train_time:83148ms step_avg:40.38ms
step:2060/2330 train_time:83193ms step_avg:40.39ms
step:2061/2330 train_time:83228ms step_avg:40.38ms
step:2062/2330 train_time:83273ms step_avg:40.38ms
step:2063/2330 train_time:83309ms step_avg:40.38ms
step:2064/2330 train_time:83354ms step_avg:40.38ms
step:2065/2330 train_time:83390ms step_avg:40.38ms
step:2066/2330 train_time:83434ms step_avg:40.38ms
step:2067/2330 train_time:83469ms step_avg:40.38ms
step:2068/2330 train_time:83514ms step_avg:40.38ms
step:2069/2330 train_time:83548ms step_avg:40.38ms
step:2070/2330 train_time:83593ms step_avg:40.38ms
step:2071/2330 train_time:83628ms step_avg:40.38ms
step:2072/2330 train_time:83672ms step_avg:40.38ms
step:2073/2330 train_time:83707ms step_avg:40.38ms
step:2074/2330 train_time:83751ms step_avg:40.38ms
step:2075/2330 train_time:83786ms step_avg:40.38ms
step:2076/2330 train_time:83830ms step_avg:40.38ms
step:2077/2330 train_time:83866ms step_avg:40.38ms
step:2078/2330 train_time:83910ms step_avg:40.38ms
step:2079/2330 train_time:83945ms step_avg:40.38ms
step:2080/2330 train_time:83990ms step_avg:40.38ms
step:2081/2330 train_time:84025ms step_avg:40.38ms
step:2082/2330 train_time:84070ms step_avg:40.38ms
step:2083/2330 train_time:84105ms step_avg:40.38ms
step:2084/2330 train_time:84151ms step_avg:40.38ms
step:2085/2330 train_time:84187ms step_avg:40.38ms
step:2086/2330 train_time:84232ms step_avg:40.38ms
step:2087/2330 train_time:84267ms step_avg:40.38ms
step:2088/2330 train_time:84312ms step_avg:40.38ms
step:2089/2330 train_time:84348ms step_avg:40.38ms
step:2090/2330 train_time:84393ms step_avg:40.38ms
step:2091/2330 train_time:84428ms step_avg:40.38ms
step:2092/2330 train_time:84473ms step_avg:40.38ms
step:2093/2330 train_time:84508ms step_avg:40.38ms
step:2094/2330 train_time:84553ms step_avg:40.38ms
step:2095/2330 train_time:84587ms step_avg:40.38ms
step:2096/2330 train_time:84631ms step_avg:40.38ms
step:2097/2330 train_time:84667ms step_avg:40.38ms
step:2098/2330 train_time:84711ms step_avg:40.38ms
step:2099/2330 train_time:84746ms step_avg:40.37ms
step:2100/2330 train_time:84790ms step_avg:40.38ms
step:2101/2330 train_time:84825ms step_avg:40.37ms
step:2102/2330 train_time:84869ms step_avg:40.38ms
step:2103/2330 train_time:84904ms step_avg:40.37ms
step:2104/2330 train_time:84949ms step_avg:40.37ms
step:2105/2330 train_time:84984ms step_avg:40.37ms
step:2106/2330 train_time:85028ms step_avg:40.37ms
step:2107/2330 train_time:85064ms step_avg:40.37ms
step:2108/2330 train_time:85109ms step_avg:40.37ms
step:2109/2330 train_time:85144ms step_avg:40.37ms
step:2110/2330 train_time:85189ms step_avg:40.37ms
step:2111/2330 train_time:85225ms step_avg:40.37ms
step:2112/2330 train_time:85270ms step_avg:40.37ms
step:2113/2330 train_time:85306ms step_avg:40.37ms
step:2114/2330 train_time:85351ms step_avg:40.37ms
step:2115/2330 train_time:85387ms step_avg:40.37ms
step:2116/2330 train_time:85431ms step_avg:40.37ms
step:2117/2330 train_time:85466ms step_avg:40.37ms
step:2118/2330 train_time:85511ms step_avg:40.37ms
step:2119/2330 train_time:85546ms step_avg:40.37ms
step:2120/2330 train_time:85591ms step_avg:40.37ms
step:2121/2330 train_time:85625ms step_avg:40.37ms
step:2122/2330 train_time:85670ms step_avg:40.37ms
step:2123/2330 train_time:85704ms step_avg:40.37ms
step:2124/2330 train_time:85749ms step_avg:40.37ms
step:2125/2330 train_time:85785ms step_avg:40.37ms
step:2126/2330 train_time:85829ms step_avg:40.37ms
step:2127/2330 train_time:85864ms step_avg:40.37ms
step:2128/2330 train_time:85908ms step_avg:40.37ms
step:2129/2330 train_time:85943ms step_avg:40.37ms
step:2130/2330 train_time:85988ms step_avg:40.37ms
step:2131/2330 train_time:86023ms step_avg:40.37ms
step:2132/2330 train_time:86068ms step_avg:40.37ms
step:2133/2330 train_time:86104ms step_avg:40.37ms
step:2134/2330 train_time:86149ms step_avg:40.37ms
step:2135/2330 train_time:86184ms step_avg:40.37ms
step:2136/2330 train_time:86229ms step_avg:40.37ms
step:2137/2330 train_time:86264ms step_avg:40.37ms
step:2138/2330 train_time:86309ms step_avg:40.37ms
step:2139/2330 train_time:86345ms step_avg:40.37ms
step:2140/2330 train_time:86391ms step_avg:40.37ms
step:2141/2330 train_time:86426ms step_avg:40.37ms
step:2142/2330 train_time:86471ms step_avg:40.37ms
step:2143/2330 train_time:86506ms step_avg:40.37ms
step:2144/2330 train_time:86551ms step_avg:40.37ms
step:2145/2330 train_time:86586ms step_avg:40.37ms
step:2146/2330 train_time:86630ms step_avg:40.37ms
step:2147/2330 train_time:86666ms step_avg:40.37ms
step:2148/2330 train_time:86710ms step_avg:40.37ms
step:2149/2330 train_time:86746ms step_avg:40.37ms
step:2150/2330 train_time:86790ms step_avg:40.37ms
step:2151/2330 train_time:86825ms step_avg:40.36ms
step:2152/2330 train_time:86870ms step_avg:40.37ms
step:2153/2330 train_time:86905ms step_avg:40.36ms
step:2154/2330 train_time:86949ms step_avg:40.37ms
step:2155/2330 train_time:86985ms step_avg:40.36ms
step:2156/2330 train_time:87030ms step_avg:40.37ms
step:2157/2330 train_time:87065ms step_avg:40.36ms
step:2158/2330 train_time:87110ms step_avg:40.37ms
step:2159/2330 train_time:87145ms step_avg:40.36ms
step:2160/2330 train_time:87190ms step_avg:40.37ms
step:2161/2330 train_time:87225ms step_avg:40.36ms
step:2162/2330 train_time:87270ms step_avg:40.37ms
step:2163/2330 train_time:87305ms step_avg:40.36ms
step:2164/2330 train_time:87351ms step_avg:40.37ms
step:2165/2330 train_time:87385ms step_avg:40.36ms
step:2166/2330 train_time:87430ms step_avg:40.36ms
step:2167/2330 train_time:87466ms step_avg:40.36ms
step:2168/2330 train_time:87511ms step_avg:40.36ms
step:2169/2330 train_time:87546ms step_avg:40.36ms
step:2170/2330 train_time:87591ms step_avg:40.36ms
step:2171/2330 train_time:87626ms step_avg:40.36ms
step:2172/2330 train_time:87671ms step_avg:40.36ms
step:2173/2330 train_time:87705ms step_avg:40.36ms
step:2174/2330 train_time:87750ms step_avg:40.36ms
step:2175/2330 train_time:87786ms step_avg:40.36ms
step:2176/2330 train_time:87830ms step_avg:40.36ms
step:2177/2330 train_time:87865ms step_avg:40.36ms
step:2178/2330 train_time:87909ms step_avg:40.36ms
step:2179/2330 train_time:87944ms step_avg:40.36ms
step:2180/2330 train_time:87989ms step_avg:40.36ms
step:2181/2330 train_time:88024ms step_avg:40.36ms
step:2182/2330 train_time:88069ms step_avg:40.36ms
step:2183/2330 train_time:88104ms step_avg:40.36ms
step:2184/2330 train_time:88150ms step_avg:40.36ms
step:2185/2330 train_time:88186ms step_avg:40.36ms
step:2186/2330 train_time:88230ms step_avg:40.36ms
step:2187/2330 train_time:88265ms step_avg:40.36ms
step:2188/2330 train_time:88310ms step_avg:40.36ms
step:2189/2330 train_time:88346ms step_avg:40.36ms
step:2190/2330 train_time:88390ms step_avg:40.36ms
step:2191/2330 train_time:88426ms step_avg:40.36ms
step:2192/2330 train_time:88471ms step_avg:40.36ms
step:2193/2330 train_time:88506ms step_avg:40.36ms
step:2194/2330 train_time:88550ms step_avg:40.36ms
step:2195/2330 train_time:88586ms step_avg:40.36ms
step:2196/2330 train_time:88630ms step_avg:40.36ms
step:2197/2330 train_time:88665ms step_avg:40.36ms
step:2198/2330 train_time:88709ms step_avg:40.36ms
step:2199/2330 train_time:88744ms step_avg:40.36ms
step:2200/2330 train_time:88788ms step_avg:40.36ms
step:2201/2330 train_time:88824ms step_avg:40.36ms
step:2202/2330 train_time:88868ms step_avg:40.36ms
step:2203/2330 train_time:88903ms step_avg:40.36ms
step:2204/2330 train_time:88947ms step_avg:40.36ms
step:2205/2330 train_time:88983ms step_avg:40.36ms
step:2206/2330 train_time:89027ms step_avg:40.36ms
step:2207/2330 train_time:89063ms step_avg:40.35ms
step:2208/2330 train_time:89108ms step_avg:40.36ms
step:2209/2330 train_time:89143ms step_avg:40.35ms
step:2210/2330 train_time:89188ms step_avg:40.36ms
step:2211/2330 train_time:89223ms step_avg:40.35ms
step:2212/2330 train_time:89268ms step_avg:40.36ms
step:2213/2330 train_time:89304ms step_avg:40.35ms
step:2214/2330 train_time:89350ms step_avg:40.36ms
step:2215/2330 train_time:89385ms step_avg:40.35ms
step:2216/2330 train_time:89431ms step_avg:40.36ms
step:2217/2330 train_time:89466ms step_avg:40.35ms
step:2218/2330 train_time:89511ms step_avg:40.36ms
step:2219/2330 train_time:89546ms step_avg:40.35ms
step:2220/2330 train_time:89591ms step_avg:40.36ms
step:2221/2330 train_time:89625ms step_avg:40.35ms
step:2222/2330 train_time:89670ms step_avg:40.36ms
step:2223/2330 train_time:89705ms step_avg:40.35ms
step:2224/2330 train_time:89750ms step_avg:40.36ms
step:2225/2330 train_time:89786ms step_avg:40.35ms
step:2226/2330 train_time:89830ms step_avg:40.35ms
step:2227/2330 train_time:89865ms step_avg:40.35ms
step:2228/2330 train_time:89910ms step_avg:40.35ms
step:2229/2330 train_time:89945ms step_avg:40.35ms
step:2230/2330 train_time:89989ms step_avg:40.35ms
step:2231/2330 train_time:90025ms step_avg:40.35ms
step:2232/2330 train_time:90069ms step_avg:40.35ms
step:2233/2330 train_time:90104ms step_avg:40.35ms
step:2234/2330 train_time:90149ms step_avg:40.35ms
step:2235/2330 train_time:90183ms step_avg:40.35ms
step:2236/2330 train_time:90229ms step_avg:40.35ms
step:2237/2330 train_time:90264ms step_avg:40.35ms
step:2238/2330 train_time:90308ms step_avg:40.35ms
step:2239/2330 train_time:90344ms step_avg:40.35ms
step:2240/2330 train_time:90389ms step_avg:40.35ms
step:2241/2330 train_time:90425ms step_avg:40.35ms
step:2242/2330 train_time:90470ms step_avg:40.35ms
step:2243/2330 train_time:90505ms step_avg:40.35ms
step:2244/2330 train_time:90549ms step_avg:40.35ms
step:2245/2330 train_time:90585ms step_avg:40.35ms
step:2246/2330 train_time:90630ms step_avg:40.35ms
step:2247/2330 train_time:90665ms step_avg:40.35ms
step:2248/2330 train_time:90709ms step_avg:40.35ms
step:2249/2330 train_time:90745ms step_avg:40.35ms
step:2250/2330 train_time:90789ms step_avg:40.35ms
step:2250/2330 val_loss:5.0387 train_time:90876ms step_avg:40.39ms
step:2251/2330 train_time:90889ms step_avg:40.38ms
step:2252/2330 train_time:90901ms step_avg:40.36ms
step:2253/2330 train_time:90912ms step_avg:40.35ms
step:2254/2330 train_time:90950ms step_avg:40.35ms
step:2255/2330 train_time:90984ms step_avg:40.35ms
step:2256/2330 train_time:91028ms step_avg:40.35ms
step:2257/2330 train_time:91063ms step_avg:40.35ms
step:2258/2330 train_time:91106ms step_avg:40.35ms
step:2259/2330 train_time:91142ms step_avg:40.35ms
step:2260/2330 train_time:91190ms step_avg:40.35ms
step:2261/2330 train_time:91228ms step_avg:40.35ms
step:2262/2330 train_time:91275ms step_avg:40.35ms
step:2263/2330 train_time:91311ms step_avg:40.35ms
step:2264/2330 train_time:91357ms step_avg:40.35ms
step:2265/2330 train_time:91392ms step_avg:40.35ms
step:2266/2330 train_time:91436ms step_avg:40.35ms
step:2267/2330 train_time:91472ms step_avg:40.35ms
step:2268/2330 train_time:91516ms step_avg:40.35ms
step:2269/2330 train_time:91551ms step_avg:40.35ms
step:2270/2330 train_time:91595ms step_avg:40.35ms
step:2271/2330 train_time:91629ms step_avg:40.35ms
step:2272/2330 train_time:91673ms step_avg:40.35ms
step:2273/2330 train_time:91708ms step_avg:40.35ms
step:2274/2330 train_time:91752ms step_avg:40.35ms
step:2275/2330 train_time:91788ms step_avg:40.35ms
step:2276/2330 train_time:91834ms step_avg:40.35ms
step:2277/2330 train_time:91869ms step_avg:40.35ms
step:2278/2330 train_time:91913ms step_avg:40.35ms
step:2279/2330 train_time:91949ms step_avg:40.35ms
step:2280/2330 train_time:91994ms step_avg:40.35ms
step:2281/2330 train_time:92029ms step_avg:40.35ms
step:2282/2330 train_time:92073ms step_avg:40.35ms
step:2283/2330 train_time:92108ms step_avg:40.35ms
step:2284/2330 train_time:92155ms step_avg:40.35ms
step:2285/2330 train_time:92190ms step_avg:40.35ms
step:2286/2330 train_time:92236ms step_avg:40.35ms
step:2287/2330 train_time:92271ms step_avg:40.35ms
step:2288/2330 train_time:92316ms step_avg:40.35ms
step:2289/2330 train_time:92352ms step_avg:40.35ms
step:2290/2330 train_time:92397ms step_avg:40.35ms
step:2291/2330 train_time:92431ms step_avg:40.35ms
step:2292/2330 train_time:92476ms step_avg:40.35ms
step:2293/2330 train_time:92510ms step_avg:40.34ms
step:2294/2330 train_time:92555ms step_avg:40.35ms
step:2295/2330 train_time:92590ms step_avg:40.34ms
step:2296/2330 train_time:92633ms step_avg:40.35ms
step:2297/2330 train_time:92668ms step_avg:40.34ms
step:2298/2330 train_time:92713ms step_avg:40.35ms
step:2299/2330 train_time:92748ms step_avg:40.34ms
step:2300/2330 train_time:92793ms step_avg:40.34ms
step:2301/2330 train_time:92829ms step_avg:40.34ms
step:2302/2330 train_time:92873ms step_avg:40.34ms
step:2303/2330 train_time:92908ms step_avg:40.34ms
step:2304/2330 train_time:92953ms step_avg:40.34ms
step:2305/2330 train_time:92987ms step_avg:40.34ms
step:2306/2330 train_time:93032ms step_avg:40.34ms
step:2307/2330 train_time:93067ms step_avg:40.34ms
step:2308/2330 train_time:93112ms step_avg:40.34ms
step:2309/2330 train_time:93147ms step_avg:40.34ms
step:2310/2330 train_time:93192ms step_avg:40.34ms
step:2311/2330 train_time:93228ms step_avg:40.34ms
step:2312/2330 train_time:93273ms step_avg:40.34ms
step:2313/2330 train_time:93309ms step_avg:40.34ms
step:2314/2330 train_time:93354ms step_avg:40.34ms
step:2315/2330 train_time:93389ms step_avg:40.34ms
step:2316/2330 train_time:93433ms step_avg:40.34ms
step:2317/2330 train_time:93469ms step_avg:40.34ms
step:2318/2330 train_time:93514ms step_avg:40.34ms
step:2319/2330 train_time:93549ms step_avg:40.34ms
step:2320/2330 train_time:93593ms step_avg:40.34ms
step:2321/2330 train_time:93628ms step_avg:40.34ms
step:2322/2330 train_time:93672ms step_avg:40.34ms
step:2323/2330 train_time:93707ms step_avg:40.34ms
step:2324/2330 train_time:93752ms step_avg:40.34ms
step:2325/2330 train_time:93788ms step_avg:40.34ms
step:2326/2330 train_time:93832ms step_avg:40.34ms
step:2327/2330 train_time:93867ms step_avg:40.34ms
step:2328/2330 train_time:93912ms step_avg:40.34ms
step:2329/2330 train_time:93946ms step_avg:40.34ms
step:2330/2330 train_time:93991ms step_avg:40.34ms
step:2330/2330 val_loss:5.0320 train_time:94078ms step_avg:40.38ms
peak memory allocated: 29226 MiB reserved: 38888 MiB
