import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_muon_lr7e-3"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=7e-3, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 04:08:26 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   33C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   32C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:81ms step_avg:80.96ms
step:2/2330 train_time:158ms step_avg:78.93ms
step:3/2330 train_time:169ms step_avg:56.46ms
step:4/2330 train_time:181ms step_avg:45.29ms
step:5/2330 train_time:193ms step_avg:38.62ms
step:6/2330 train_time:278ms step_avg:46.36ms
step:7/2330 train_time:312ms step_avg:44.52ms
step:8/2330 train_time:355ms step_avg:44.35ms
step:9/2330 train_time:390ms step_avg:43.32ms
step:10/2330 train_time:434ms step_avg:43.38ms
step:11/2330 train_time:468ms step_avg:42.55ms
step:12/2330 train_time:513ms step_avg:42.75ms
step:13/2330 train_time:548ms step_avg:42.15ms
step:14/2330 train_time:592ms step_avg:42.27ms
step:15/2330 train_time:626ms step_avg:41.74ms
step:16/2330 train_time:671ms step_avg:41.92ms
step:17/2330 train_time:706ms step_avg:41.52ms
step:18/2330 train_time:750ms step_avg:41.66ms
step:19/2330 train_time:784ms step_avg:41.26ms
step:20/2330 train_time:829ms step_avg:41.43ms
step:21/2330 train_time:863ms step_avg:41.11ms
step:22/2330 train_time:907ms step_avg:41.24ms
step:23/2330 train_time:942ms step_avg:40.97ms
step:24/2330 train_time:986ms step_avg:41.10ms
step:25/2330 train_time:1022ms step_avg:40.87ms
step:26/2330 train_time:1066ms step_avg:40.99ms
step:27/2330 train_time:1101ms step_avg:40.78ms
step:28/2330 train_time:1149ms step_avg:41.05ms
step:29/2330 train_time:1190ms step_avg:41.04ms
step:30/2330 train_time:1239ms step_avg:41.31ms
step:31/2330 train_time:1277ms step_avg:41.19ms
step:32/2330 train_time:1323ms step_avg:41.33ms
step:33/2330 train_time:1358ms step_avg:41.16ms
step:34/2330 train_time:1404ms step_avg:41.28ms
step:35/2330 train_time:1439ms step_avg:41.11ms
step:36/2330 train_time:1483ms step_avg:41.20ms
step:37/2330 train_time:1518ms step_avg:41.02ms
step:38/2330 train_time:1563ms step_avg:41.12ms
step:39/2330 train_time:1598ms step_avg:40.98ms
step:40/2330 train_time:1643ms step_avg:41.07ms
step:41/2330 train_time:1677ms step_avg:40.91ms
step:42/2330 train_time:1722ms step_avg:40.99ms
step:43/2330 train_time:1757ms step_avg:40.86ms
step:44/2330 train_time:1802ms step_avg:40.95ms
step:45/2330 train_time:1837ms step_avg:40.82ms
step:46/2330 train_time:1882ms step_avg:40.91ms
step:47/2330 train_time:1916ms step_avg:40.77ms
step:48/2330 train_time:1962ms step_avg:40.87ms
step:49/2330 train_time:1997ms step_avg:40.76ms
step:50/2330 train_time:2042ms step_avg:40.84ms
step:51/2330 train_time:2077ms step_avg:40.73ms
step:52/2330 train_time:2124ms step_avg:40.84ms
step:53/2330 train_time:2160ms step_avg:40.75ms
step:54/2330 train_time:2206ms step_avg:40.86ms
step:55/2330 train_time:2243ms step_avg:40.78ms
step:56/2330 train_time:2289ms step_avg:40.88ms
step:57/2330 train_time:2325ms step_avg:40.79ms
step:58/2330 train_time:2371ms step_avg:40.87ms
step:59/2330 train_time:2407ms step_avg:40.80ms
step:60/2330 train_time:2453ms step_avg:40.89ms
step:61/2330 train_time:2489ms step_avg:40.81ms
step:62/2330 train_time:2534ms step_avg:40.87ms
step:63/2330 train_time:2571ms step_avg:40.80ms
step:64/2330 train_time:2615ms step_avg:40.87ms
step:65/2330 train_time:2650ms step_avg:40.77ms
step:66/2330 train_time:2695ms step_avg:40.84ms
step:67/2330 train_time:2730ms step_avg:40.74ms
step:68/2330 train_time:2775ms step_avg:40.81ms
step:69/2330 train_time:2811ms step_avg:40.74ms
step:70/2330 train_time:2855ms step_avg:40.79ms
step:71/2330 train_time:2891ms step_avg:40.72ms
step:72/2330 train_time:2936ms step_avg:40.77ms
step:73/2330 train_time:2972ms step_avg:40.71ms
step:74/2330 train_time:3018ms step_avg:40.79ms
step:75/2330 train_time:3054ms step_avg:40.72ms
step:76/2330 train_time:3099ms step_avg:40.78ms
step:77/2330 train_time:3135ms step_avg:40.72ms
step:78/2330 train_time:3181ms step_avg:40.78ms
step:79/2330 train_time:3217ms step_avg:40.72ms
step:80/2330 train_time:3262ms step_avg:40.78ms
step:81/2330 train_time:3298ms step_avg:40.71ms
step:82/2330 train_time:3344ms step_avg:40.78ms
step:83/2330 train_time:3379ms step_avg:40.71ms
step:84/2330 train_time:3425ms step_avg:40.77ms
step:85/2330 train_time:3461ms step_avg:40.71ms
step:86/2330 train_time:3505ms step_avg:40.76ms
step:87/2330 train_time:3541ms step_avg:40.70ms
step:88/2330 train_time:3586ms step_avg:40.75ms
step:89/2330 train_time:3622ms step_avg:40.69ms
step:90/2330 train_time:3666ms step_avg:40.73ms
step:91/2330 train_time:3702ms step_avg:40.68ms
step:92/2330 train_time:3746ms step_avg:40.72ms
step:93/2330 train_time:3782ms step_avg:40.66ms
step:94/2330 train_time:3826ms step_avg:40.70ms
step:95/2330 train_time:3862ms step_avg:40.65ms
step:96/2330 train_time:3907ms step_avg:40.70ms
step:97/2330 train_time:3943ms step_avg:40.65ms
step:98/2330 train_time:3989ms step_avg:40.70ms
step:99/2330 train_time:4024ms step_avg:40.65ms
step:100/2330 train_time:4070ms step_avg:40.70ms
step:101/2330 train_time:4106ms step_avg:40.66ms
step:102/2330 train_time:4152ms step_avg:40.71ms
step:103/2330 train_time:4189ms step_avg:40.67ms
step:104/2330 train_time:4234ms step_avg:40.71ms
step:105/2330 train_time:4271ms step_avg:40.67ms
step:106/2330 train_time:4317ms step_avg:40.72ms
step:107/2330 train_time:4353ms step_avg:40.68ms
step:108/2330 train_time:4399ms step_avg:40.73ms
step:109/2330 train_time:4434ms step_avg:40.68ms
step:110/2330 train_time:4479ms step_avg:40.72ms
step:111/2330 train_time:4515ms step_avg:40.68ms
step:112/2330 train_time:4560ms step_avg:40.71ms
step:113/2330 train_time:4595ms step_avg:40.66ms
step:114/2330 train_time:4640ms step_avg:40.70ms
step:115/2330 train_time:4675ms step_avg:40.65ms
step:116/2330 train_time:4720ms step_avg:40.69ms
step:117/2330 train_time:4756ms step_avg:40.65ms
step:118/2330 train_time:4801ms step_avg:40.69ms
step:119/2330 train_time:4836ms step_avg:40.64ms
step:120/2330 train_time:4881ms step_avg:40.68ms
step:121/2330 train_time:4917ms step_avg:40.64ms
step:122/2330 train_time:4962ms step_avg:40.67ms
step:123/2330 train_time:4998ms step_avg:40.63ms
step:124/2330 train_time:5043ms step_avg:40.67ms
step:125/2330 train_time:5078ms step_avg:40.62ms
step:126/2330 train_time:5123ms step_avg:40.66ms
step:127/2330 train_time:5159ms step_avg:40.63ms
step:128/2330 train_time:5205ms step_avg:40.67ms
step:129/2330 train_time:5241ms step_avg:40.63ms
step:130/2330 train_time:5286ms step_avg:40.67ms
step:131/2330 train_time:5322ms step_avg:40.63ms
step:132/2330 train_time:5367ms step_avg:40.66ms
step:133/2330 train_time:5404ms step_avg:40.63ms
step:134/2330 train_time:5448ms step_avg:40.66ms
step:135/2330 train_time:5484ms step_avg:40.62ms
step:136/2330 train_time:5530ms step_avg:40.66ms
step:137/2330 train_time:5568ms step_avg:40.64ms
step:138/2330 train_time:5615ms step_avg:40.69ms
step:139/2330 train_time:5650ms step_avg:40.65ms
step:140/2330 train_time:5696ms step_avg:40.68ms
step:141/2330 train_time:5731ms step_avg:40.64ms
step:142/2330 train_time:5776ms step_avg:40.68ms
step:143/2330 train_time:5812ms step_avg:40.64ms
step:144/2330 train_time:5857ms step_avg:40.68ms
step:145/2330 train_time:5892ms step_avg:40.64ms
step:146/2330 train_time:5937ms step_avg:40.67ms
step:147/2330 train_time:5972ms step_avg:40.63ms
step:148/2330 train_time:6018ms step_avg:40.66ms
step:149/2330 train_time:6054ms step_avg:40.63ms
step:150/2330 train_time:6099ms step_avg:40.66ms
step:151/2330 train_time:6135ms step_avg:40.63ms
step:152/2330 train_time:6180ms step_avg:40.66ms
step:153/2330 train_time:6216ms step_avg:40.63ms
step:154/2330 train_time:6262ms step_avg:40.66ms
step:155/2330 train_time:6297ms step_avg:40.63ms
step:156/2330 train_time:6342ms step_avg:40.65ms
step:157/2330 train_time:6377ms step_avg:40.62ms
step:158/2330 train_time:6423ms step_avg:40.65ms
step:159/2330 train_time:6458ms step_avg:40.62ms
step:160/2330 train_time:6503ms step_avg:40.64ms
step:161/2330 train_time:6539ms step_avg:40.61ms
step:162/2330 train_time:6585ms step_avg:40.65ms
step:163/2330 train_time:6620ms step_avg:40.61ms
step:164/2330 train_time:6665ms step_avg:40.64ms
step:165/2330 train_time:6701ms step_avg:40.61ms
step:166/2330 train_time:6747ms step_avg:40.64ms
step:167/2330 train_time:6782ms step_avg:40.61ms
step:168/2330 train_time:6827ms step_avg:40.64ms
step:169/2330 train_time:6862ms step_avg:40.60ms
step:170/2330 train_time:6906ms step_avg:40.62ms
step:171/2330 train_time:6941ms step_avg:40.59ms
step:172/2330 train_time:6986ms step_avg:40.62ms
step:173/2330 train_time:7022ms step_avg:40.59ms
step:174/2330 train_time:7067ms step_avg:40.61ms
step:175/2330 train_time:7103ms step_avg:40.59ms
step:176/2330 train_time:7148ms step_avg:40.62ms
step:177/2330 train_time:7184ms step_avg:40.59ms
step:178/2330 train_time:7229ms step_avg:40.61ms
step:179/2330 train_time:7265ms step_avg:40.59ms
step:180/2330 train_time:7310ms step_avg:40.61ms
step:181/2330 train_time:7346ms step_avg:40.59ms
step:182/2330 train_time:7392ms step_avg:40.62ms
step:183/2330 train_time:7428ms step_avg:40.59ms
step:184/2330 train_time:7476ms step_avg:40.63ms
step:185/2330 train_time:7512ms step_avg:40.61ms
step:186/2330 train_time:7557ms step_avg:40.63ms
step:187/2330 train_time:7593ms step_avg:40.60ms
step:188/2330 train_time:7638ms step_avg:40.63ms
step:189/2330 train_time:7674ms step_avg:40.60ms
step:190/2330 train_time:7720ms step_avg:40.63ms
step:191/2330 train_time:7755ms step_avg:40.60ms
step:192/2330 train_time:7800ms step_avg:40.63ms
step:193/2330 train_time:7835ms step_avg:40.59ms
step:194/2330 train_time:7880ms step_avg:40.62ms
step:195/2330 train_time:7915ms step_avg:40.59ms
step:196/2330 train_time:7960ms step_avg:40.61ms
step:197/2330 train_time:7996ms step_avg:40.59ms
step:198/2330 train_time:8040ms step_avg:40.61ms
step:199/2330 train_time:8076ms step_avg:40.58ms
step:200/2330 train_time:8122ms step_avg:40.61ms
step:201/2330 train_time:8157ms step_avg:40.58ms
step:202/2330 train_time:8202ms step_avg:40.60ms
step:203/2330 train_time:8237ms step_avg:40.58ms
step:204/2330 train_time:8282ms step_avg:40.60ms
step:205/2330 train_time:8318ms step_avg:40.57ms
step:206/2330 train_time:8363ms step_avg:40.60ms
step:207/2330 train_time:8400ms step_avg:40.58ms
step:208/2330 train_time:8445ms step_avg:40.60ms
step:209/2330 train_time:8482ms step_avg:40.58ms
step:210/2330 train_time:8527ms step_avg:40.61ms
step:211/2330 train_time:8563ms step_avg:40.58ms
step:212/2330 train_time:8608ms step_avg:40.60ms
step:213/2330 train_time:8644ms step_avg:40.58ms
step:214/2330 train_time:8689ms step_avg:40.60ms
step:215/2330 train_time:8725ms step_avg:40.58ms
step:216/2330 train_time:8770ms step_avg:40.60ms
step:217/2330 train_time:8806ms step_avg:40.58ms
step:218/2330 train_time:8852ms step_avg:40.60ms
step:219/2330 train_time:8887ms step_avg:40.58ms
step:220/2330 train_time:8932ms step_avg:40.60ms
step:221/2330 train_time:8968ms step_avg:40.58ms
step:222/2330 train_time:9015ms step_avg:40.61ms
step:223/2330 train_time:9050ms step_avg:40.58ms
step:224/2330 train_time:9095ms step_avg:40.60ms
step:225/2330 train_time:9130ms step_avg:40.58ms
step:226/2330 train_time:9176ms step_avg:40.60ms
step:227/2330 train_time:9211ms step_avg:40.58ms
step:228/2330 train_time:9257ms step_avg:40.60ms
step:229/2330 train_time:9293ms step_avg:40.58ms
step:230/2330 train_time:9339ms step_avg:40.60ms
step:231/2330 train_time:9375ms step_avg:40.59ms
step:232/2330 train_time:9421ms step_avg:40.61ms
step:233/2330 train_time:9457ms step_avg:40.59ms
step:234/2330 train_time:9502ms step_avg:40.61ms
step:235/2330 train_time:9537ms step_avg:40.58ms
step:236/2330 train_time:9582ms step_avg:40.60ms
step:237/2330 train_time:9618ms step_avg:40.58ms
step:238/2330 train_time:9662ms step_avg:40.60ms
step:239/2330 train_time:9698ms step_avg:40.58ms
step:240/2330 train_time:9744ms step_avg:40.60ms
step:241/2330 train_time:9780ms step_avg:40.58ms
step:242/2330 train_time:9825ms step_avg:40.60ms
step:243/2330 train_time:9861ms step_avg:40.58ms
step:244/2330 train_time:9907ms step_avg:40.60ms
step:245/2330 train_time:9942ms step_avg:40.58ms
step:246/2330 train_time:9987ms step_avg:40.60ms
step:247/2330 train_time:10023ms step_avg:40.58ms
step:248/2330 train_time:10068ms step_avg:40.60ms
step:249/2330 train_time:10104ms step_avg:40.58ms
step:250/2330 train_time:10148ms step_avg:40.59ms
step:250/2330 val_loss:5.4109 train_time:10238ms step_avg:40.95ms
step:251/2330 train_time:10252ms step_avg:40.85ms
step:252/2330 train_time:10265ms step_avg:40.73ms
step:253/2330 train_time:10276ms step_avg:40.62ms
step:254/2330 train_time:10313ms step_avg:40.60ms
step:255/2330 train_time:10348ms step_avg:40.58ms
step:256/2330 train_time:10392ms step_avg:40.59ms
step:257/2330 train_time:10426ms step_avg:40.57ms
step:258/2330 train_time:10471ms step_avg:40.58ms
step:259/2330 train_time:10505ms step_avg:40.56ms
step:260/2330 train_time:10554ms step_avg:40.59ms
step:261/2330 train_time:10593ms step_avg:40.58ms
step:262/2330 train_time:10640ms step_avg:40.61ms
step:263/2330 train_time:10675ms step_avg:40.59ms
step:264/2330 train_time:10721ms step_avg:40.61ms
step:265/2330 train_time:10757ms step_avg:40.59ms
step:266/2330 train_time:10803ms step_avg:40.61ms
step:267/2330 train_time:10838ms step_avg:40.59ms
step:268/2330 train_time:10883ms step_avg:40.61ms
step:269/2330 train_time:10918ms step_avg:40.59ms
step:270/2330 train_time:10963ms step_avg:40.61ms
step:271/2330 train_time:10998ms step_avg:40.58ms
step:272/2330 train_time:11043ms step_avg:40.60ms
step:273/2330 train_time:11078ms step_avg:40.58ms
step:274/2330 train_time:11123ms step_avg:40.59ms
step:275/2330 train_time:11159ms step_avg:40.58ms
step:276/2330 train_time:11206ms step_avg:40.60ms
step:277/2330 train_time:11242ms step_avg:40.58ms
step:278/2330 train_time:11288ms step_avg:40.60ms
step:279/2330 train_time:11323ms step_avg:40.58ms
step:280/2330 train_time:11368ms step_avg:40.60ms
step:281/2330 train_time:11403ms step_avg:40.58ms
step:282/2330 train_time:11448ms step_avg:40.60ms
step:283/2330 train_time:11485ms step_avg:40.58ms
step:284/2330 train_time:11531ms step_avg:40.60ms
step:285/2330 train_time:11567ms step_avg:40.59ms
step:286/2330 train_time:11614ms step_avg:40.61ms
step:287/2330 train_time:11650ms step_avg:40.59ms
step:288/2330 train_time:11696ms step_avg:40.61ms
step:289/2330 train_time:11732ms step_avg:40.59ms
step:290/2330 train_time:11777ms step_avg:40.61ms
step:291/2330 train_time:11812ms step_avg:40.59ms
step:292/2330 train_time:11856ms step_avg:40.60ms
step:293/2330 train_time:11892ms step_avg:40.59ms
step:294/2330 train_time:11936ms step_avg:40.60ms
step:295/2330 train_time:11971ms step_avg:40.58ms
step:296/2330 train_time:12016ms step_avg:40.59ms
step:297/2330 train_time:12051ms step_avg:40.58ms
step:298/2330 train_time:12097ms step_avg:40.59ms
step:299/2330 train_time:12132ms step_avg:40.58ms
step:300/2330 train_time:12177ms step_avg:40.59ms
step:301/2330 train_time:12213ms step_avg:40.58ms
step:302/2330 train_time:12258ms step_avg:40.59ms
step:303/2330 train_time:12294ms step_avg:40.57ms
step:304/2330 train_time:12339ms step_avg:40.59ms
step:305/2330 train_time:12374ms step_avg:40.57ms
step:306/2330 train_time:12420ms step_avg:40.59ms
step:307/2330 train_time:12456ms step_avg:40.57ms
step:308/2330 train_time:12502ms step_avg:40.59ms
step:309/2330 train_time:12538ms step_avg:40.58ms
step:310/2330 train_time:12583ms step_avg:40.59ms
step:311/2330 train_time:12619ms step_avg:40.57ms
step:312/2330 train_time:12665ms step_avg:40.59ms
step:313/2330 train_time:12702ms step_avg:40.58ms
step:314/2330 train_time:12748ms step_avg:40.60ms
step:315/2330 train_time:12783ms step_avg:40.58ms
step:316/2330 train_time:12828ms step_avg:40.59ms
step:317/2330 train_time:12864ms step_avg:40.58ms
step:318/2330 train_time:12910ms step_avg:40.60ms
step:319/2330 train_time:12944ms step_avg:40.58ms
step:320/2330 train_time:12990ms step_avg:40.59ms
step:321/2330 train_time:13025ms step_avg:40.58ms
step:322/2330 train_time:13071ms step_avg:40.59ms
step:323/2330 train_time:13106ms step_avg:40.58ms
step:324/2330 train_time:13151ms step_avg:40.59ms
step:325/2330 train_time:13186ms step_avg:40.57ms
step:326/2330 train_time:13232ms step_avg:40.59ms
step:327/2330 train_time:13268ms step_avg:40.57ms
step:328/2330 train_time:13312ms step_avg:40.59ms
step:329/2330 train_time:13348ms step_avg:40.57ms
step:330/2330 train_time:13393ms step_avg:40.59ms
step:331/2330 train_time:13429ms step_avg:40.57ms
step:332/2330 train_time:13474ms step_avg:40.59ms
step:333/2330 train_time:13510ms step_avg:40.57ms
step:334/2330 train_time:13555ms step_avg:40.58ms
step:335/2330 train_time:13590ms step_avg:40.57ms
step:336/2330 train_time:13635ms step_avg:40.58ms
step:337/2330 train_time:13671ms step_avg:40.57ms
step:338/2330 train_time:13716ms step_avg:40.58ms
step:339/2330 train_time:13752ms step_avg:40.57ms
step:340/2330 train_time:13797ms step_avg:40.58ms
step:341/2330 train_time:13831ms step_avg:40.56ms
step:342/2330 train_time:13877ms step_avg:40.58ms
step:343/2330 train_time:13912ms step_avg:40.56ms
step:344/2330 train_time:13956ms step_avg:40.57ms
step:345/2330 train_time:13992ms step_avg:40.56ms
step:346/2330 train_time:14037ms step_avg:40.57ms
step:347/2330 train_time:14073ms step_avg:40.56ms
step:348/2330 train_time:14118ms step_avg:40.57ms
step:349/2330 train_time:14154ms step_avg:40.56ms
step:350/2330 train_time:14200ms step_avg:40.57ms
step:351/2330 train_time:14236ms step_avg:40.56ms
step:352/2330 train_time:14281ms step_avg:40.57ms
step:353/2330 train_time:14317ms step_avg:40.56ms
step:354/2330 train_time:14362ms step_avg:40.57ms
step:355/2330 train_time:14397ms step_avg:40.56ms
step:356/2330 train_time:14442ms step_avg:40.57ms
step:357/2330 train_time:14478ms step_avg:40.55ms
step:358/2330 train_time:14523ms step_avg:40.57ms
step:359/2330 train_time:14559ms step_avg:40.56ms
step:360/2330 train_time:14606ms step_avg:40.57ms
step:361/2330 train_time:14642ms step_avg:40.56ms
step:362/2330 train_time:14688ms step_avg:40.57ms
step:363/2330 train_time:14724ms step_avg:40.56ms
step:364/2330 train_time:14771ms step_avg:40.58ms
step:365/2330 train_time:14806ms step_avg:40.56ms
step:366/2330 train_time:14851ms step_avg:40.58ms
step:367/2330 train_time:14886ms step_avg:40.56ms
step:368/2330 train_time:14931ms step_avg:40.57ms
step:369/2330 train_time:14967ms step_avg:40.56ms
step:370/2330 train_time:15012ms step_avg:40.57ms
step:371/2330 train_time:15047ms step_avg:40.56ms
step:372/2330 train_time:15093ms step_avg:40.57ms
step:373/2330 train_time:15128ms step_avg:40.56ms
step:374/2330 train_time:15173ms step_avg:40.57ms
step:375/2330 train_time:15208ms step_avg:40.55ms
step:376/2330 train_time:15253ms step_avg:40.57ms
step:377/2330 train_time:15288ms step_avg:40.55ms
step:378/2330 train_time:15334ms step_avg:40.56ms
step:379/2330 train_time:15369ms step_avg:40.55ms
step:380/2330 train_time:15414ms step_avg:40.56ms
step:381/2330 train_time:15450ms step_avg:40.55ms
step:382/2330 train_time:15495ms step_avg:40.56ms
step:383/2330 train_time:15531ms step_avg:40.55ms
step:384/2330 train_time:15577ms step_avg:40.57ms
step:385/2330 train_time:15613ms step_avg:40.55ms
step:386/2330 train_time:15658ms step_avg:40.57ms
step:387/2330 train_time:15694ms step_avg:40.55ms
step:388/2330 train_time:15739ms step_avg:40.56ms
step:389/2330 train_time:15776ms step_avg:40.55ms
step:390/2330 train_time:15821ms step_avg:40.57ms
step:391/2330 train_time:15857ms step_avg:40.55ms
step:392/2330 train_time:15902ms step_avg:40.57ms
step:393/2330 train_time:15937ms step_avg:40.55ms
step:394/2330 train_time:15982ms step_avg:40.56ms
step:395/2330 train_time:16018ms step_avg:40.55ms
step:396/2330 train_time:16063ms step_avg:40.56ms
step:397/2330 train_time:16099ms step_avg:40.55ms
step:398/2330 train_time:16145ms step_avg:40.57ms
step:399/2330 train_time:16182ms step_avg:40.56ms
step:400/2330 train_time:16227ms step_avg:40.57ms
step:401/2330 train_time:16262ms step_avg:40.55ms
step:402/2330 train_time:16308ms step_avg:40.57ms
step:403/2330 train_time:16343ms step_avg:40.55ms
step:404/2330 train_time:16388ms step_avg:40.56ms
step:405/2330 train_time:16424ms step_avg:40.55ms
step:406/2330 train_time:16469ms step_avg:40.56ms
step:407/2330 train_time:16505ms step_avg:40.55ms
step:408/2330 train_time:16550ms step_avg:40.56ms
step:409/2330 train_time:16586ms step_avg:40.55ms
step:410/2330 train_time:16631ms step_avg:40.56ms
step:411/2330 train_time:16667ms step_avg:40.55ms
step:412/2330 train_time:16713ms step_avg:40.57ms
step:413/2330 train_time:16749ms step_avg:40.55ms
step:414/2330 train_time:16794ms step_avg:40.56ms
step:415/2330 train_time:16829ms step_avg:40.55ms
step:416/2330 train_time:16874ms step_avg:40.56ms
step:417/2330 train_time:16909ms step_avg:40.55ms
step:418/2330 train_time:16954ms step_avg:40.56ms
step:419/2330 train_time:16989ms step_avg:40.55ms
step:420/2330 train_time:17034ms step_avg:40.56ms
step:421/2330 train_time:17069ms step_avg:40.54ms
step:422/2330 train_time:17115ms step_avg:40.56ms
step:423/2330 train_time:17151ms step_avg:40.55ms
step:424/2330 train_time:17195ms step_avg:40.55ms
step:425/2330 train_time:17231ms step_avg:40.54ms
step:426/2330 train_time:17276ms step_avg:40.55ms
step:427/2330 train_time:17311ms step_avg:40.54ms
step:428/2330 train_time:17356ms step_avg:40.55ms
step:429/2330 train_time:17392ms step_avg:40.54ms
step:430/2330 train_time:17437ms step_avg:40.55ms
step:431/2330 train_time:17473ms step_avg:40.54ms
step:432/2330 train_time:17519ms step_avg:40.55ms
step:433/2330 train_time:17554ms step_avg:40.54ms
step:434/2330 train_time:17600ms step_avg:40.55ms
step:435/2330 train_time:17636ms step_avg:40.54ms
step:436/2330 train_time:17682ms step_avg:40.55ms
step:437/2330 train_time:17717ms step_avg:40.54ms
step:438/2330 train_time:17762ms step_avg:40.55ms
step:439/2330 train_time:17798ms step_avg:40.54ms
step:440/2330 train_time:17842ms step_avg:40.55ms
step:441/2330 train_time:17878ms step_avg:40.54ms
step:442/2330 train_time:17922ms step_avg:40.55ms
step:443/2330 train_time:17958ms step_avg:40.54ms
step:444/2330 train_time:18003ms step_avg:40.55ms
step:445/2330 train_time:18039ms step_avg:40.54ms
step:446/2330 train_time:18084ms step_avg:40.55ms
step:447/2330 train_time:18120ms step_avg:40.54ms
step:448/2330 train_time:18166ms step_avg:40.55ms
step:449/2330 train_time:18202ms step_avg:40.54ms
step:450/2330 train_time:18248ms step_avg:40.55ms
step:451/2330 train_time:18284ms step_avg:40.54ms
step:452/2330 train_time:18328ms step_avg:40.55ms
step:453/2330 train_time:18364ms step_avg:40.54ms
step:454/2330 train_time:18410ms step_avg:40.55ms
step:455/2330 train_time:18446ms step_avg:40.54ms
step:456/2330 train_time:18491ms step_avg:40.55ms
step:457/2330 train_time:18527ms step_avg:40.54ms
step:458/2330 train_time:18571ms step_avg:40.55ms
step:459/2330 train_time:18607ms step_avg:40.54ms
step:460/2330 train_time:18653ms step_avg:40.55ms
step:461/2330 train_time:18688ms step_avg:40.54ms
step:462/2330 train_time:18734ms step_avg:40.55ms
step:463/2330 train_time:18769ms step_avg:40.54ms
step:464/2330 train_time:18814ms step_avg:40.55ms
step:465/2330 train_time:18849ms step_avg:40.54ms
step:466/2330 train_time:18894ms step_avg:40.55ms
step:467/2330 train_time:18929ms step_avg:40.53ms
step:468/2330 train_time:18974ms step_avg:40.54ms
step:469/2330 train_time:19009ms step_avg:40.53ms
step:470/2330 train_time:19054ms step_avg:40.54ms
step:471/2330 train_time:19090ms step_avg:40.53ms
step:472/2330 train_time:19135ms step_avg:40.54ms
step:473/2330 train_time:19171ms step_avg:40.53ms
step:474/2330 train_time:19216ms step_avg:40.54ms
step:475/2330 train_time:19252ms step_avg:40.53ms
step:476/2330 train_time:19297ms step_avg:40.54ms
step:477/2330 train_time:19333ms step_avg:40.53ms
step:478/2330 train_time:19378ms step_avg:40.54ms
step:479/2330 train_time:19414ms step_avg:40.53ms
step:480/2330 train_time:19459ms step_avg:40.54ms
step:481/2330 train_time:19495ms step_avg:40.53ms
step:482/2330 train_time:19539ms step_avg:40.54ms
step:483/2330 train_time:19576ms step_avg:40.53ms
step:484/2330 train_time:19621ms step_avg:40.54ms
step:485/2330 train_time:19657ms step_avg:40.53ms
step:486/2330 train_time:19702ms step_avg:40.54ms
step:487/2330 train_time:19738ms step_avg:40.53ms
step:488/2330 train_time:19783ms step_avg:40.54ms
step:489/2330 train_time:19819ms step_avg:40.53ms
step:490/2330 train_time:19864ms step_avg:40.54ms
step:491/2330 train_time:19899ms step_avg:40.53ms
step:492/2330 train_time:19945ms step_avg:40.54ms
step:493/2330 train_time:19981ms step_avg:40.53ms
step:494/2330 train_time:20026ms step_avg:40.54ms
step:495/2330 train_time:20062ms step_avg:40.53ms
step:496/2330 train_time:20107ms step_avg:40.54ms
step:497/2330 train_time:20142ms step_avg:40.53ms
step:498/2330 train_time:20188ms step_avg:40.54ms
step:499/2330 train_time:20224ms step_avg:40.53ms
step:500/2330 train_time:20269ms step_avg:40.54ms
step:500/2330 val_loss:5.2892 train_time:20358ms step_avg:40.72ms
step:501/2330 train_time:20372ms step_avg:40.66ms
step:502/2330 train_time:20384ms step_avg:40.61ms
step:503/2330 train_time:20394ms step_avg:40.54ms
step:504/2330 train_time:20432ms step_avg:40.54ms
step:505/2330 train_time:20466ms step_avg:40.53ms
step:506/2330 train_time:20510ms step_avg:40.53ms
step:507/2330 train_time:20545ms step_avg:40.52ms
step:508/2330 train_time:20590ms step_avg:40.53ms
step:509/2330 train_time:20625ms step_avg:40.52ms
step:510/2330 train_time:20672ms step_avg:40.53ms
step:511/2330 train_time:20713ms step_avg:40.53ms
step:512/2330 train_time:20760ms step_avg:40.55ms
step:513/2330 train_time:20796ms step_avg:40.54ms
step:514/2330 train_time:20841ms step_avg:40.55ms
step:515/2330 train_time:20877ms step_avg:40.54ms
step:516/2330 train_time:20921ms step_avg:40.55ms
step:517/2330 train_time:20958ms step_avg:40.54ms
step:518/2330 train_time:21003ms step_avg:40.55ms
step:519/2330 train_time:21039ms step_avg:40.54ms
step:520/2330 train_time:21084ms step_avg:40.55ms
step:521/2330 train_time:21119ms step_avg:40.54ms
step:522/2330 train_time:21163ms step_avg:40.54ms
step:523/2330 train_time:21199ms step_avg:40.53ms
step:524/2330 train_time:21243ms step_avg:40.54ms
step:525/2330 train_time:21278ms step_avg:40.53ms
step:526/2330 train_time:21324ms step_avg:40.54ms
step:527/2330 train_time:21361ms step_avg:40.53ms
step:528/2330 train_time:21406ms step_avg:40.54ms
step:529/2330 train_time:21441ms step_avg:40.53ms
step:530/2330 train_time:21486ms step_avg:40.54ms
step:531/2330 train_time:21521ms step_avg:40.53ms
step:532/2330 train_time:21566ms step_avg:40.54ms
step:533/2330 train_time:21602ms step_avg:40.53ms
step:534/2330 train_time:21649ms step_avg:40.54ms
step:535/2330 train_time:21686ms step_avg:40.53ms
step:536/2330 train_time:21732ms step_avg:40.55ms
step:537/2330 train_time:21769ms step_avg:40.54ms
step:538/2330 train_time:21816ms step_avg:40.55ms
step:539/2330 train_time:21852ms step_avg:40.54ms
step:540/2330 train_time:21897ms step_avg:40.55ms
step:541/2330 train_time:21933ms step_avg:40.54ms
step:542/2330 train_time:21978ms step_avg:40.55ms
step:543/2330 train_time:22013ms step_avg:40.54ms
step:544/2330 train_time:22059ms step_avg:40.55ms
step:545/2330 train_time:22094ms step_avg:40.54ms
step:546/2330 train_time:22138ms step_avg:40.55ms
step:547/2330 train_time:22173ms step_avg:40.54ms
step:548/2330 train_time:22219ms step_avg:40.54ms
step:549/2330 train_time:22253ms step_avg:40.53ms
step:550/2330 train_time:22298ms step_avg:40.54ms
step:551/2330 train_time:22333ms step_avg:40.53ms
step:552/2330 train_time:22378ms step_avg:40.54ms
step:553/2330 train_time:22414ms step_avg:40.53ms
step:554/2330 train_time:22458ms step_avg:40.54ms
step:555/2330 train_time:22493ms step_avg:40.53ms
step:556/2330 train_time:22539ms step_avg:40.54ms
step:557/2330 train_time:22574ms step_avg:40.53ms
step:558/2330 train_time:22620ms step_avg:40.54ms
step:559/2330 train_time:22657ms step_avg:40.53ms
step:560/2330 train_time:22702ms step_avg:40.54ms
step:561/2330 train_time:22739ms step_avg:40.53ms
step:562/2330 train_time:22784ms step_avg:40.54ms
step:563/2330 train_time:22820ms step_avg:40.53ms
step:564/2330 train_time:22866ms step_avg:40.54ms
step:565/2330 train_time:22902ms step_avg:40.53ms
step:566/2330 train_time:22947ms step_avg:40.54ms
step:567/2330 train_time:22984ms step_avg:40.54ms
step:568/2330 train_time:23030ms step_avg:40.55ms
step:569/2330 train_time:23066ms step_avg:40.54ms
step:570/2330 train_time:23112ms step_avg:40.55ms
step:571/2330 train_time:23147ms step_avg:40.54ms
step:572/2330 train_time:23192ms step_avg:40.54ms
step:573/2330 train_time:23227ms step_avg:40.54ms
step:574/2330 train_time:23273ms step_avg:40.54ms
step:575/2330 train_time:23308ms step_avg:40.54ms
step:576/2330 train_time:23353ms step_avg:40.54ms
step:577/2330 train_time:23389ms step_avg:40.53ms
step:578/2330 train_time:23433ms step_avg:40.54ms
step:579/2330 train_time:23468ms step_avg:40.53ms
step:580/2330 train_time:23513ms step_avg:40.54ms
step:581/2330 train_time:23549ms step_avg:40.53ms
step:582/2330 train_time:23594ms step_avg:40.54ms
step:583/2330 train_time:23629ms step_avg:40.53ms
step:584/2330 train_time:23674ms step_avg:40.54ms
step:585/2330 train_time:23710ms step_avg:40.53ms
step:586/2330 train_time:23755ms step_avg:40.54ms
step:587/2330 train_time:23791ms step_avg:40.53ms
step:588/2330 train_time:23835ms step_avg:40.54ms
step:589/2330 train_time:23871ms step_avg:40.53ms
step:590/2330 train_time:23916ms step_avg:40.54ms
step:591/2330 train_time:23952ms step_avg:40.53ms
step:592/2330 train_time:23997ms step_avg:40.54ms
step:593/2330 train_time:24032ms step_avg:40.53ms
step:594/2330 train_time:24078ms step_avg:40.53ms
step:595/2330 train_time:24113ms step_avg:40.53ms
step:596/2330 train_time:24158ms step_avg:40.53ms
step:597/2330 train_time:24193ms step_avg:40.52ms
step:598/2330 train_time:24239ms step_avg:40.53ms
step:599/2330 train_time:24274ms step_avg:40.52ms
step:600/2330 train_time:24319ms step_avg:40.53ms
step:601/2330 train_time:24355ms step_avg:40.52ms
step:602/2330 train_time:24400ms step_avg:40.53ms
step:603/2330 train_time:24436ms step_avg:40.52ms
step:604/2330 train_time:24481ms step_avg:40.53ms
step:605/2330 train_time:24517ms step_avg:40.52ms
step:606/2330 train_time:24561ms step_avg:40.53ms
step:607/2330 train_time:24597ms step_avg:40.52ms
step:608/2330 train_time:24643ms step_avg:40.53ms
step:609/2330 train_time:24679ms step_avg:40.52ms
step:610/2330 train_time:24723ms step_avg:40.53ms
step:611/2330 train_time:24758ms step_avg:40.52ms
step:612/2330 train_time:24803ms step_avg:40.53ms
step:613/2330 train_time:24838ms step_avg:40.52ms
step:614/2330 train_time:24883ms step_avg:40.53ms
step:615/2330 train_time:24919ms step_avg:40.52ms
step:616/2330 train_time:24964ms step_avg:40.53ms
step:617/2330 train_time:25000ms step_avg:40.52ms
step:618/2330 train_time:25045ms step_avg:40.53ms
step:619/2330 train_time:25081ms step_avg:40.52ms
step:620/2330 train_time:25126ms step_avg:40.53ms
step:621/2330 train_time:25162ms step_avg:40.52ms
step:622/2330 train_time:25207ms step_avg:40.53ms
step:623/2330 train_time:25243ms step_avg:40.52ms
step:624/2330 train_time:25288ms step_avg:40.53ms
step:625/2330 train_time:25325ms step_avg:40.52ms
step:626/2330 train_time:25370ms step_avg:40.53ms
step:627/2330 train_time:25405ms step_avg:40.52ms
step:628/2330 train_time:25450ms step_avg:40.53ms
step:629/2330 train_time:25487ms step_avg:40.52ms
step:630/2330 train_time:25532ms step_avg:40.53ms
step:631/2330 train_time:25568ms step_avg:40.52ms
step:632/2330 train_time:25613ms step_avg:40.53ms
step:633/2330 train_time:25649ms step_avg:40.52ms
step:634/2330 train_time:25694ms step_avg:40.53ms
step:635/2330 train_time:25729ms step_avg:40.52ms
step:636/2330 train_time:25774ms step_avg:40.53ms
step:637/2330 train_time:25810ms step_avg:40.52ms
step:638/2330 train_time:25855ms step_avg:40.53ms
step:639/2330 train_time:25891ms step_avg:40.52ms
step:640/2330 train_time:25936ms step_avg:40.52ms
step:641/2330 train_time:25971ms step_avg:40.52ms
step:642/2330 train_time:26016ms step_avg:40.52ms
step:643/2330 train_time:26051ms step_avg:40.52ms
step:644/2330 train_time:26096ms step_avg:40.52ms
step:645/2330 train_time:26131ms step_avg:40.51ms
step:646/2330 train_time:26177ms step_avg:40.52ms
step:647/2330 train_time:26213ms step_avg:40.51ms
step:648/2330 train_time:26258ms step_avg:40.52ms
step:649/2330 train_time:26294ms step_avg:40.51ms
step:650/2330 train_time:26340ms step_avg:40.52ms
step:651/2330 train_time:26375ms step_avg:40.51ms
step:652/2330 train_time:26420ms step_avg:40.52ms
step:653/2330 train_time:26456ms step_avg:40.52ms
step:654/2330 train_time:26501ms step_avg:40.52ms
step:655/2330 train_time:26537ms step_avg:40.51ms
step:656/2330 train_time:26582ms step_avg:40.52ms
step:657/2330 train_time:26618ms step_avg:40.51ms
step:658/2330 train_time:26664ms step_avg:40.52ms
step:659/2330 train_time:26700ms step_avg:40.52ms
step:660/2330 train_time:26745ms step_avg:40.52ms
step:661/2330 train_time:26780ms step_avg:40.52ms
step:662/2330 train_time:26826ms step_avg:40.52ms
step:663/2330 train_time:26862ms step_avg:40.52ms
step:664/2330 train_time:26908ms step_avg:40.52ms
step:665/2330 train_time:26944ms step_avg:40.52ms
step:666/2330 train_time:26989ms step_avg:40.52ms
step:667/2330 train_time:27025ms step_avg:40.52ms
step:668/2330 train_time:27071ms step_avg:40.53ms
step:669/2330 train_time:27106ms step_avg:40.52ms
step:670/2330 train_time:27152ms step_avg:40.53ms
step:671/2330 train_time:27187ms step_avg:40.52ms
step:672/2330 train_time:27233ms step_avg:40.52ms
step:673/2330 train_time:27268ms step_avg:40.52ms
step:674/2330 train_time:27313ms step_avg:40.52ms
step:675/2330 train_time:27350ms step_avg:40.52ms
step:676/2330 train_time:27394ms step_avg:40.52ms
step:677/2330 train_time:27430ms step_avg:40.52ms
step:678/2330 train_time:27475ms step_avg:40.52ms
step:679/2330 train_time:27511ms step_avg:40.52ms
step:680/2330 train_time:27556ms step_avg:40.52ms
step:681/2330 train_time:27591ms step_avg:40.52ms
step:682/2330 train_time:27636ms step_avg:40.52ms
step:683/2330 train_time:27671ms step_avg:40.51ms
step:684/2330 train_time:27716ms step_avg:40.52ms
step:685/2330 train_time:27753ms step_avg:40.52ms
step:686/2330 train_time:27797ms step_avg:40.52ms
step:687/2330 train_time:27833ms step_avg:40.51ms
step:688/2330 train_time:27877ms step_avg:40.52ms
step:689/2330 train_time:27913ms step_avg:40.51ms
step:690/2330 train_time:27959ms step_avg:40.52ms
step:691/2330 train_time:27995ms step_avg:40.51ms
step:692/2330 train_time:28039ms step_avg:40.52ms
step:693/2330 train_time:28075ms step_avg:40.51ms
step:694/2330 train_time:28120ms step_avg:40.52ms
step:695/2330 train_time:28156ms step_avg:40.51ms
step:696/2330 train_time:28201ms step_avg:40.52ms
step:697/2330 train_time:28236ms step_avg:40.51ms
step:698/2330 train_time:28281ms step_avg:40.52ms
step:699/2330 train_time:28317ms step_avg:40.51ms
step:700/2330 train_time:28363ms step_avg:40.52ms
step:701/2330 train_time:28399ms step_avg:40.51ms
step:702/2330 train_time:28444ms step_avg:40.52ms
step:703/2330 train_time:28480ms step_avg:40.51ms
step:704/2330 train_time:28525ms step_avg:40.52ms
step:705/2330 train_time:28561ms step_avg:40.51ms
step:706/2330 train_time:28607ms step_avg:40.52ms
step:707/2330 train_time:28644ms step_avg:40.51ms
step:708/2330 train_time:28689ms step_avg:40.52ms
step:709/2330 train_time:28725ms step_avg:40.51ms
step:710/2330 train_time:28770ms step_avg:40.52ms
step:711/2330 train_time:28804ms step_avg:40.51ms
step:712/2330 train_time:28848ms step_avg:40.52ms
step:713/2330 train_time:28884ms step_avg:40.51ms
step:714/2330 train_time:28930ms step_avg:40.52ms
step:715/2330 train_time:28966ms step_avg:40.51ms
step:716/2330 train_time:29012ms step_avg:40.52ms
step:717/2330 train_time:29048ms step_avg:40.51ms
step:718/2330 train_time:29093ms step_avg:40.52ms
step:719/2330 train_time:29130ms step_avg:40.51ms
step:720/2330 train_time:29174ms step_avg:40.52ms
step:721/2330 train_time:29210ms step_avg:40.51ms
step:722/2330 train_time:29255ms step_avg:40.52ms
step:723/2330 train_time:29290ms step_avg:40.51ms
step:724/2330 train_time:29335ms step_avg:40.52ms
step:725/2330 train_time:29372ms step_avg:40.51ms
step:726/2330 train_time:29416ms step_avg:40.52ms
step:727/2330 train_time:29451ms step_avg:40.51ms
step:728/2330 train_time:29496ms step_avg:40.52ms
step:729/2330 train_time:29531ms step_avg:40.51ms
step:730/2330 train_time:29577ms step_avg:40.52ms
step:731/2330 train_time:29613ms step_avg:40.51ms
step:732/2330 train_time:29658ms step_avg:40.52ms
step:733/2330 train_time:29693ms step_avg:40.51ms
step:734/2330 train_time:29738ms step_avg:40.51ms
step:735/2330 train_time:29773ms step_avg:40.51ms
step:736/2330 train_time:29818ms step_avg:40.51ms
step:737/2330 train_time:29855ms step_avg:40.51ms
step:738/2330 train_time:29901ms step_avg:40.52ms
step:739/2330 train_time:29936ms step_avg:40.51ms
step:740/2330 train_time:29981ms step_avg:40.51ms
step:741/2330 train_time:30017ms step_avg:40.51ms
step:742/2330 train_time:30062ms step_avg:40.51ms
step:743/2330 train_time:30098ms step_avg:40.51ms
step:744/2330 train_time:30143ms step_avg:40.51ms
step:745/2330 train_time:30179ms step_avg:40.51ms
step:746/2330 train_time:30224ms step_avg:40.51ms
step:747/2330 train_time:30259ms step_avg:40.51ms
step:748/2330 train_time:30304ms step_avg:40.51ms
step:749/2330 train_time:30340ms step_avg:40.51ms
step:750/2330 train_time:30384ms step_avg:40.51ms
step:750/2330 val_loss:5.2247 train_time:30474ms step_avg:40.63ms
step:751/2330 train_time:30487ms step_avg:40.60ms
step:752/2330 train_time:30500ms step_avg:40.56ms
step:753/2330 train_time:30511ms step_avg:40.52ms
step:754/2330 train_time:30546ms step_avg:40.51ms
step:755/2330 train_time:30581ms step_avg:40.50ms
step:756/2330 train_time:30624ms step_avg:40.51ms
step:757/2330 train_time:30658ms step_avg:40.50ms
step:758/2330 train_time:30702ms step_avg:40.50ms
step:759/2330 train_time:30737ms step_avg:40.50ms
step:760/2330 train_time:30782ms step_avg:40.50ms
step:761/2330 train_time:30821ms step_avg:40.50ms
step:762/2330 train_time:30871ms step_avg:40.51ms
step:763/2330 train_time:30910ms step_avg:40.51ms
step:764/2330 train_time:30956ms step_avg:40.52ms
step:765/2330 train_time:30993ms step_avg:40.51ms
step:766/2330 train_time:31037ms step_avg:40.52ms
step:767/2330 train_time:31072ms step_avg:40.51ms
step:768/2330 train_time:31117ms step_avg:40.52ms
step:769/2330 train_time:31152ms step_avg:40.51ms
step:770/2330 train_time:31196ms step_avg:40.51ms
step:771/2330 train_time:31231ms step_avg:40.51ms
step:772/2330 train_time:31275ms step_avg:40.51ms
step:773/2330 train_time:31310ms step_avg:40.50ms
step:774/2330 train_time:31354ms step_avg:40.51ms
step:775/2330 train_time:31390ms step_avg:40.50ms
step:776/2330 train_time:31437ms step_avg:40.51ms
step:777/2330 train_time:31472ms step_avg:40.51ms
step:778/2330 train_time:31518ms step_avg:40.51ms
step:779/2330 train_time:31554ms step_avg:40.51ms
step:780/2330 train_time:31598ms step_avg:40.51ms
step:781/2330 train_time:31634ms step_avg:40.50ms
step:782/2330 train_time:31679ms step_avg:40.51ms
step:783/2330 train_time:31714ms step_avg:40.50ms
step:784/2330 train_time:31760ms step_avg:40.51ms
step:785/2330 train_time:31796ms step_avg:40.50ms
step:786/2330 train_time:31842ms step_avg:40.51ms
step:787/2330 train_time:31878ms step_avg:40.51ms
step:788/2330 train_time:31924ms step_avg:40.51ms
step:789/2330 train_time:31961ms step_avg:40.51ms
step:790/2330 train_time:32006ms step_avg:40.51ms
step:791/2330 train_time:32042ms step_avg:40.51ms
step:792/2330 train_time:32087ms step_avg:40.51ms
step:793/2330 train_time:32122ms step_avg:40.51ms
step:794/2330 train_time:32166ms step_avg:40.51ms
step:795/2330 train_time:32201ms step_avg:40.50ms
step:796/2330 train_time:32246ms step_avg:40.51ms
step:797/2330 train_time:32282ms step_avg:40.50ms
step:798/2330 train_time:32327ms step_avg:40.51ms
step:799/2330 train_time:32362ms step_avg:40.50ms
step:800/2330 train_time:32407ms step_avg:40.51ms
step:801/2330 train_time:32442ms step_avg:40.50ms
step:802/2330 train_time:32489ms step_avg:40.51ms
step:803/2330 train_time:32525ms step_avg:40.50ms
step:804/2330 train_time:32569ms step_avg:40.51ms
step:805/2330 train_time:32605ms step_avg:40.50ms
step:806/2330 train_time:32650ms step_avg:40.51ms
step:807/2330 train_time:32687ms step_avg:40.50ms
step:808/2330 train_time:32732ms step_avg:40.51ms
step:809/2330 train_time:32768ms step_avg:40.50ms
step:810/2330 train_time:32814ms step_avg:40.51ms
step:811/2330 train_time:32849ms step_avg:40.50ms
step:812/2330 train_time:32894ms step_avg:40.51ms
step:813/2330 train_time:32930ms step_avg:40.50ms
step:814/2330 train_time:32975ms step_avg:40.51ms
step:815/2330 train_time:33009ms step_avg:40.50ms
step:816/2330 train_time:33054ms step_avg:40.51ms
step:817/2330 train_time:33089ms step_avg:40.50ms
step:818/2330 train_time:33134ms step_avg:40.51ms
step:819/2330 train_time:33169ms step_avg:40.50ms
step:820/2330 train_time:33215ms step_avg:40.51ms
step:821/2330 train_time:33250ms step_avg:40.50ms
step:822/2330 train_time:33295ms step_avg:40.50ms
step:823/2330 train_time:33331ms step_avg:40.50ms
step:824/2330 train_time:33376ms step_avg:40.51ms
step:825/2330 train_time:33412ms step_avg:40.50ms
step:826/2330 train_time:33457ms step_avg:40.50ms
step:827/2330 train_time:33492ms step_avg:40.50ms
step:828/2330 train_time:33537ms step_avg:40.50ms
step:829/2330 train_time:33573ms step_avg:40.50ms
step:830/2330 train_time:33619ms step_avg:40.50ms
step:831/2330 train_time:33655ms step_avg:40.50ms
step:832/2330 train_time:33701ms step_avg:40.51ms
step:833/2330 train_time:33738ms step_avg:40.50ms
step:834/2330 train_time:33782ms step_avg:40.51ms
step:835/2330 train_time:33818ms step_avg:40.50ms
step:836/2330 train_time:33863ms step_avg:40.51ms
step:837/2330 train_time:33898ms step_avg:40.50ms
step:838/2330 train_time:33943ms step_avg:40.50ms
step:839/2330 train_time:33979ms step_avg:40.50ms
step:840/2330 train_time:34024ms step_avg:40.50ms
step:841/2330 train_time:34059ms step_avg:40.50ms
step:842/2330 train_time:34104ms step_avg:40.50ms
step:843/2330 train_time:34140ms step_avg:40.50ms
step:844/2330 train_time:34185ms step_avg:40.50ms
step:845/2330 train_time:34220ms step_avg:40.50ms
step:846/2330 train_time:34265ms step_avg:40.50ms
step:847/2330 train_time:34300ms step_avg:40.50ms
step:848/2330 train_time:34345ms step_avg:40.50ms
step:849/2330 train_time:34381ms step_avg:40.50ms
step:850/2330 train_time:34426ms step_avg:40.50ms
step:851/2330 train_time:34462ms step_avg:40.50ms
step:852/2330 train_time:34507ms step_avg:40.50ms
step:853/2330 train_time:34544ms step_avg:40.50ms
step:854/2330 train_time:34590ms step_avg:40.50ms
step:855/2330 train_time:34626ms step_avg:40.50ms
step:856/2330 train_time:34671ms step_avg:40.50ms
step:857/2330 train_time:34707ms step_avg:40.50ms
step:858/2330 train_time:34752ms step_avg:40.50ms
step:859/2330 train_time:34788ms step_avg:40.50ms
step:860/2330 train_time:34833ms step_avg:40.50ms
step:861/2330 train_time:34868ms step_avg:40.50ms
step:862/2330 train_time:34913ms step_avg:40.50ms
step:863/2330 train_time:34948ms step_avg:40.50ms
step:864/2330 train_time:34993ms step_avg:40.50ms
step:865/2330 train_time:35029ms step_avg:40.50ms
step:866/2330 train_time:35073ms step_avg:40.50ms
step:867/2330 train_time:35108ms step_avg:40.49ms
step:868/2330 train_time:35153ms step_avg:40.50ms
step:869/2330 train_time:35189ms step_avg:40.49ms
step:870/2330 train_time:35234ms step_avg:40.50ms
step:871/2330 train_time:35269ms step_avg:40.49ms
step:872/2330 train_time:35314ms step_avg:40.50ms
step:873/2330 train_time:35349ms step_avg:40.49ms
step:874/2330 train_time:35395ms step_avg:40.50ms
step:875/2330 train_time:35431ms step_avg:40.49ms
step:876/2330 train_time:35476ms step_avg:40.50ms
step:877/2330 train_time:35512ms step_avg:40.49ms
step:878/2330 train_time:35557ms step_avg:40.50ms
step:879/2330 train_time:35593ms step_avg:40.49ms
step:880/2330 train_time:35638ms step_avg:40.50ms
step:881/2330 train_time:35674ms step_avg:40.49ms
step:882/2330 train_time:35719ms step_avg:40.50ms
step:883/2330 train_time:35755ms step_avg:40.49ms
step:884/2330 train_time:35800ms step_avg:40.50ms
step:885/2330 train_time:35835ms step_avg:40.49ms
step:886/2330 train_time:35881ms step_avg:40.50ms
step:887/2330 train_time:35917ms step_avg:40.49ms
step:888/2330 train_time:35962ms step_avg:40.50ms
step:889/2330 train_time:35998ms step_avg:40.49ms
step:890/2330 train_time:36043ms step_avg:40.50ms
step:891/2330 train_time:36079ms step_avg:40.49ms
step:892/2330 train_time:36124ms step_avg:40.50ms
step:893/2330 train_time:36160ms step_avg:40.49ms
step:894/2330 train_time:36204ms step_avg:40.50ms
step:895/2330 train_time:36240ms step_avg:40.49ms
step:896/2330 train_time:36285ms step_avg:40.50ms
step:897/2330 train_time:36321ms step_avg:40.49ms
step:898/2330 train_time:36367ms step_avg:40.50ms
step:899/2330 train_time:36402ms step_avg:40.49ms
step:900/2330 train_time:36447ms step_avg:40.50ms
step:901/2330 train_time:36484ms step_avg:40.49ms
step:902/2330 train_time:36530ms step_avg:40.50ms
step:903/2330 train_time:36566ms step_avg:40.49ms
step:904/2330 train_time:36611ms step_avg:40.50ms
step:905/2330 train_time:36647ms step_avg:40.49ms
step:906/2330 train_time:36692ms step_avg:40.50ms
step:907/2330 train_time:36728ms step_avg:40.49ms
step:908/2330 train_time:36773ms step_avg:40.50ms
step:909/2330 train_time:36809ms step_avg:40.49ms
step:910/2330 train_time:36854ms step_avg:40.50ms
step:911/2330 train_time:36889ms step_avg:40.49ms
step:912/2330 train_time:36934ms step_avg:40.50ms
step:913/2330 train_time:36969ms step_avg:40.49ms
step:914/2330 train_time:37015ms step_avg:40.50ms
step:915/2330 train_time:37049ms step_avg:40.49ms
step:916/2330 train_time:37094ms step_avg:40.50ms
step:917/2330 train_time:37129ms step_avg:40.49ms
step:918/2330 train_time:37174ms step_avg:40.50ms
step:919/2330 train_time:37210ms step_avg:40.49ms
step:920/2330 train_time:37254ms step_avg:40.49ms
step:921/2330 train_time:37290ms step_avg:40.49ms
step:922/2330 train_time:37335ms step_avg:40.49ms
step:923/2330 train_time:37371ms step_avg:40.49ms
step:924/2330 train_time:37417ms step_avg:40.49ms
step:925/2330 train_time:37452ms step_avg:40.49ms
step:926/2330 train_time:37498ms step_avg:40.49ms
step:927/2330 train_time:37534ms step_avg:40.49ms
step:928/2330 train_time:37578ms step_avg:40.49ms
step:929/2330 train_time:37614ms step_avg:40.49ms
step:930/2330 train_time:37659ms step_avg:40.49ms
step:931/2330 train_time:37694ms step_avg:40.49ms
step:932/2330 train_time:37740ms step_avg:40.49ms
step:933/2330 train_time:37777ms step_avg:40.49ms
step:934/2330 train_time:37822ms step_avg:40.49ms
step:935/2330 train_time:37858ms step_avg:40.49ms
step:936/2330 train_time:37902ms step_avg:40.49ms
step:937/2330 train_time:37938ms step_avg:40.49ms
step:938/2330 train_time:37983ms step_avg:40.49ms
step:939/2330 train_time:38018ms step_avg:40.49ms
step:940/2330 train_time:38063ms step_avg:40.49ms
step:941/2330 train_time:38099ms step_avg:40.49ms
step:942/2330 train_time:38143ms step_avg:40.49ms
step:943/2330 train_time:38179ms step_avg:40.49ms
step:944/2330 train_time:38223ms step_avg:40.49ms
step:945/2330 train_time:38259ms step_avg:40.49ms
step:946/2330 train_time:38304ms step_avg:40.49ms
step:947/2330 train_time:38339ms step_avg:40.48ms
step:948/2330 train_time:38384ms step_avg:40.49ms
step:949/2330 train_time:38420ms step_avg:40.48ms
step:950/2330 train_time:38465ms step_avg:40.49ms
step:951/2330 train_time:38500ms step_avg:40.48ms
step:952/2330 train_time:38545ms step_avg:40.49ms
step:953/2330 train_time:38581ms step_avg:40.48ms
step:954/2330 train_time:38628ms step_avg:40.49ms
step:955/2330 train_time:38664ms step_avg:40.49ms
step:956/2330 train_time:38709ms step_avg:40.49ms
step:957/2330 train_time:38745ms step_avg:40.49ms
step:958/2330 train_time:38791ms step_avg:40.49ms
step:959/2330 train_time:38825ms step_avg:40.48ms
step:960/2330 train_time:38871ms step_avg:40.49ms
step:961/2330 train_time:38905ms step_avg:40.48ms
step:962/2330 train_time:38951ms step_avg:40.49ms
step:963/2330 train_time:38986ms step_avg:40.48ms
step:964/2330 train_time:39030ms step_avg:40.49ms
step:965/2330 train_time:39066ms step_avg:40.48ms
step:966/2330 train_time:39110ms step_avg:40.49ms
step:967/2330 train_time:39146ms step_avg:40.48ms
step:968/2330 train_time:39191ms step_avg:40.49ms
step:969/2330 train_time:39226ms step_avg:40.48ms
step:970/2330 train_time:39271ms step_avg:40.49ms
step:971/2330 train_time:39306ms step_avg:40.48ms
step:972/2330 train_time:39352ms step_avg:40.49ms
step:973/2330 train_time:39387ms step_avg:40.48ms
step:974/2330 train_time:39432ms step_avg:40.49ms
step:975/2330 train_time:39468ms step_avg:40.48ms
step:976/2330 train_time:39513ms step_avg:40.48ms
step:977/2330 train_time:39548ms step_avg:40.48ms
step:978/2330 train_time:39593ms step_avg:40.48ms
step:979/2330 train_time:39629ms step_avg:40.48ms
step:980/2330 train_time:39674ms step_avg:40.48ms
step:981/2330 train_time:39709ms step_avg:40.48ms
step:982/2330 train_time:39754ms step_avg:40.48ms
step:983/2330 train_time:39790ms step_avg:40.48ms
step:984/2330 train_time:39835ms step_avg:40.48ms
step:985/2330 train_time:39870ms step_avg:40.48ms
step:986/2330 train_time:39914ms step_avg:40.48ms
step:987/2330 train_time:39949ms step_avg:40.48ms
step:988/2330 train_time:39994ms step_avg:40.48ms
step:989/2330 train_time:40029ms step_avg:40.47ms
step:990/2330 train_time:40074ms step_avg:40.48ms
step:991/2330 train_time:40109ms step_avg:40.47ms
step:992/2330 train_time:40154ms step_avg:40.48ms
step:993/2330 train_time:40189ms step_avg:40.47ms
step:994/2330 train_time:40234ms step_avg:40.48ms
step:995/2330 train_time:40269ms step_avg:40.47ms
step:996/2330 train_time:40314ms step_avg:40.48ms
step:997/2330 train_time:40349ms step_avg:40.47ms
step:998/2330 train_time:40394ms step_avg:40.48ms
step:999/2330 train_time:40429ms step_avg:40.47ms
step:1000/2330 train_time:40474ms step_avg:40.47ms
step:1000/2330 val_loss:5.1877 train_time:40562ms step_avg:40.56ms
step:1001/2330 train_time:40576ms step_avg:40.54ms
step:1002/2330 train_time:40588ms step_avg:40.51ms
step:1003/2330 train_time:40600ms step_avg:40.48ms
step:1004/2330 train_time:40636ms step_avg:40.47ms
step:1005/2330 train_time:40670ms step_avg:40.47ms
step:1006/2330 train_time:40714ms step_avg:40.47ms
step:1007/2330 train_time:40749ms step_avg:40.47ms
step:1008/2330 train_time:40792ms step_avg:40.47ms
step:1009/2330 train_time:40827ms step_avg:40.46ms
step:1010/2330 train_time:40874ms step_avg:40.47ms
step:1011/2330 train_time:40916ms step_avg:40.47ms
step:1012/2330 train_time:40966ms step_avg:40.48ms
step:1013/2330 train_time:41003ms step_avg:40.48ms
step:1014/2330 train_time:41048ms step_avg:40.48ms
step:1015/2330 train_time:41084ms step_avg:40.48ms
step:1016/2330 train_time:41129ms step_avg:40.48ms
step:1017/2330 train_time:41164ms step_avg:40.48ms
step:1018/2330 train_time:41209ms step_avg:40.48ms
step:1019/2330 train_time:41243ms step_avg:40.47ms
step:1020/2330 train_time:41288ms step_avg:40.48ms
step:1021/2330 train_time:41323ms step_avg:40.47ms
step:1022/2330 train_time:41367ms step_avg:40.48ms
step:1023/2330 train_time:41402ms step_avg:40.47ms
step:1024/2330 train_time:41447ms step_avg:40.48ms
step:1025/2330 train_time:41483ms step_avg:40.47ms
step:1026/2330 train_time:41529ms step_avg:40.48ms
step:1027/2330 train_time:41565ms step_avg:40.47ms
step:1028/2330 train_time:41610ms step_avg:40.48ms
step:1029/2330 train_time:41645ms step_avg:40.47ms
step:1030/2330 train_time:41690ms step_avg:40.48ms
step:1031/2330 train_time:41725ms step_avg:40.47ms
step:1032/2330 train_time:41770ms step_avg:40.47ms
step:1033/2330 train_time:41805ms step_avg:40.47ms
step:1034/2330 train_time:41852ms step_avg:40.48ms
step:1035/2330 train_time:41889ms step_avg:40.47ms
step:1036/2330 train_time:41934ms step_avg:40.48ms
step:1037/2330 train_time:41970ms step_avg:40.47ms
step:1038/2330 train_time:42015ms step_avg:40.48ms
step:1039/2330 train_time:42051ms step_avg:40.47ms
step:1040/2330 train_time:42096ms step_avg:40.48ms
step:1041/2330 train_time:42132ms step_avg:40.47ms
step:1042/2330 train_time:42178ms step_avg:40.48ms
step:1043/2330 train_time:42214ms step_avg:40.47ms
step:1044/2330 train_time:42259ms step_avg:40.48ms
step:1045/2330 train_time:42294ms step_avg:40.47ms
step:1046/2330 train_time:42338ms step_avg:40.48ms
step:1047/2330 train_time:42373ms step_avg:40.47ms
step:1048/2330 train_time:42418ms step_avg:40.48ms
step:1049/2330 train_time:42454ms step_avg:40.47ms
step:1050/2330 train_time:42500ms step_avg:40.48ms
step:1051/2330 train_time:42535ms step_avg:40.47ms
step:1052/2330 train_time:42580ms step_avg:40.48ms
step:1053/2330 train_time:42616ms step_avg:40.47ms
step:1054/2330 train_time:42661ms step_avg:40.48ms
step:1055/2330 train_time:42697ms step_avg:40.47ms
step:1056/2330 train_time:42742ms step_avg:40.48ms
step:1057/2330 train_time:42777ms step_avg:40.47ms
step:1058/2330 train_time:42822ms step_avg:40.47ms
step:1059/2330 train_time:42859ms step_avg:40.47ms
step:1060/2330 train_time:42904ms step_avg:40.48ms
step:1061/2330 train_time:42940ms step_avg:40.47ms
step:1062/2330 train_time:42984ms step_avg:40.47ms
step:1063/2330 train_time:43020ms step_avg:40.47ms
step:1064/2330 train_time:43065ms step_avg:40.47ms
step:1065/2330 train_time:43100ms step_avg:40.47ms
step:1066/2330 train_time:43146ms step_avg:40.47ms
step:1067/2330 train_time:43181ms step_avg:40.47ms
step:1068/2330 train_time:43227ms step_avg:40.47ms
step:1069/2330 train_time:43262ms step_avg:40.47ms
step:1070/2330 train_time:43307ms step_avg:40.47ms
step:1071/2330 train_time:43342ms step_avg:40.47ms
step:1072/2330 train_time:43387ms step_avg:40.47ms
step:1073/2330 train_time:43422ms step_avg:40.47ms
step:1074/2330 train_time:43468ms step_avg:40.47ms
step:1075/2330 train_time:43504ms step_avg:40.47ms
step:1076/2330 train_time:43548ms step_avg:40.47ms
step:1077/2330 train_time:43584ms step_avg:40.47ms
step:1078/2330 train_time:43629ms step_avg:40.47ms
step:1079/2330 train_time:43665ms step_avg:40.47ms
step:1080/2330 train_time:43710ms step_avg:40.47ms
step:1081/2330 train_time:43745ms step_avg:40.47ms
step:1082/2330 train_time:43790ms step_avg:40.47ms
step:1083/2330 train_time:43827ms step_avg:40.47ms
step:1084/2330 train_time:43873ms step_avg:40.47ms
step:1085/2330 train_time:43909ms step_avg:40.47ms
step:1086/2330 train_time:43954ms step_avg:40.47ms
step:1087/2330 train_time:43990ms step_avg:40.47ms
step:1088/2330 train_time:44035ms step_avg:40.47ms
step:1089/2330 train_time:44071ms step_avg:40.47ms
step:1090/2330 train_time:44117ms step_avg:40.47ms
step:1091/2330 train_time:44155ms step_avg:40.47ms
step:1092/2330 train_time:44199ms step_avg:40.48ms
step:1093/2330 train_time:44234ms step_avg:40.47ms
step:1094/2330 train_time:44280ms step_avg:40.48ms
step:1095/2330 train_time:44316ms step_avg:40.47ms
step:1096/2330 train_time:44361ms step_avg:40.48ms
step:1097/2330 train_time:44396ms step_avg:40.47ms
step:1098/2330 train_time:44441ms step_avg:40.47ms
step:1099/2330 train_time:44477ms step_avg:40.47ms
step:1100/2330 train_time:44522ms step_avg:40.47ms
step:1101/2330 train_time:44558ms step_avg:40.47ms
step:1102/2330 train_time:44603ms step_avg:40.47ms
step:1103/2330 train_time:44638ms step_avg:40.47ms
step:1104/2330 train_time:44683ms step_avg:40.47ms
step:1105/2330 train_time:44718ms step_avg:40.47ms
step:1106/2330 train_time:44764ms step_avg:40.47ms
step:1107/2330 train_time:44799ms step_avg:40.47ms
step:1108/2330 train_time:44845ms step_avg:40.47ms
step:1109/2330 train_time:44880ms step_avg:40.47ms
step:1110/2330 train_time:44925ms step_avg:40.47ms
step:1111/2330 train_time:44960ms step_avg:40.47ms
step:1112/2330 train_time:45005ms step_avg:40.47ms
step:1113/2330 train_time:45042ms step_avg:40.47ms
step:1114/2330 train_time:45086ms step_avg:40.47ms
step:1115/2330 train_time:45122ms step_avg:40.47ms
step:1116/2330 train_time:45167ms step_avg:40.47ms
step:1117/2330 train_time:45202ms step_avg:40.47ms
step:1118/2330 train_time:45248ms step_avg:40.47ms
step:1119/2330 train_time:45283ms step_avg:40.47ms
step:1120/2330 train_time:45327ms step_avg:40.47ms
step:1121/2330 train_time:45363ms step_avg:40.47ms
step:1122/2330 train_time:45407ms step_avg:40.47ms
step:1123/2330 train_time:45443ms step_avg:40.47ms
step:1124/2330 train_time:45488ms step_avg:40.47ms
step:1125/2330 train_time:45523ms step_avg:40.46ms
step:1126/2330 train_time:45567ms step_avg:40.47ms
step:1127/2330 train_time:45604ms step_avg:40.46ms
step:1128/2330 train_time:45649ms step_avg:40.47ms
step:1129/2330 train_time:45685ms step_avg:40.46ms
step:1130/2330 train_time:45730ms step_avg:40.47ms
step:1131/2330 train_time:45766ms step_avg:40.46ms
step:1132/2330 train_time:45811ms step_avg:40.47ms
step:1133/2330 train_time:45847ms step_avg:40.47ms
step:1134/2330 train_time:45892ms step_avg:40.47ms
step:1135/2330 train_time:45927ms step_avg:40.46ms
step:1136/2330 train_time:45972ms step_avg:40.47ms
step:1137/2330 train_time:46007ms step_avg:40.46ms
step:1138/2330 train_time:46052ms step_avg:40.47ms
step:1139/2330 train_time:46087ms step_avg:40.46ms
step:1140/2330 train_time:46132ms step_avg:40.47ms
step:1141/2330 train_time:46167ms step_avg:40.46ms
step:1142/2330 train_time:46212ms step_avg:40.47ms
step:1143/2330 train_time:46248ms step_avg:40.46ms
step:1144/2330 train_time:46293ms step_avg:40.47ms
step:1145/2330 train_time:46328ms step_avg:40.46ms
step:1146/2330 train_time:46373ms step_avg:40.47ms
step:1147/2330 train_time:46409ms step_avg:40.46ms
step:1148/2330 train_time:46454ms step_avg:40.46ms
step:1149/2330 train_time:46489ms step_avg:40.46ms
step:1150/2330 train_time:46535ms step_avg:40.46ms
step:1151/2330 train_time:46570ms step_avg:40.46ms
step:1152/2330 train_time:46616ms step_avg:40.47ms
step:1153/2330 train_time:46653ms step_avg:40.46ms
step:1154/2330 train_time:46699ms step_avg:40.47ms
step:1155/2330 train_time:46735ms step_avg:40.46ms
step:1156/2330 train_time:46780ms step_avg:40.47ms
step:1157/2330 train_time:46816ms step_avg:40.46ms
step:1158/2330 train_time:46862ms step_avg:40.47ms
step:1159/2330 train_time:46897ms step_avg:40.46ms
step:1160/2330 train_time:46942ms step_avg:40.47ms
step:1161/2330 train_time:46977ms step_avg:40.46ms
step:1162/2330 train_time:47023ms step_avg:40.47ms
step:1163/2330 train_time:47058ms step_avg:40.46ms
step:1164/2330 train_time:47103ms step_avg:40.47ms
step:1165/2330 train_time:47138ms step_avg:40.46ms
step:1166/2330 train_time:47183ms step_avg:40.47ms
step:1167/2330 train_time:47218ms step_avg:40.46ms
step:1168/2330 train_time:47263ms step_avg:40.47ms
step:1169/2330 train_time:47298ms step_avg:40.46ms
step:1170/2330 train_time:47344ms step_avg:40.46ms
step:1171/2330 train_time:47379ms step_avg:40.46ms
step:1172/2330 train_time:47424ms step_avg:40.46ms
step:1173/2330 train_time:47459ms step_avg:40.46ms
step:1174/2330 train_time:47504ms step_avg:40.46ms
step:1175/2330 train_time:47539ms step_avg:40.46ms
step:1176/2330 train_time:47584ms step_avg:40.46ms
step:1177/2330 train_time:47619ms step_avg:40.46ms
step:1178/2330 train_time:47664ms step_avg:40.46ms
step:1179/2330 train_time:47700ms step_avg:40.46ms
step:1180/2330 train_time:47744ms step_avg:40.46ms
step:1181/2330 train_time:47780ms step_avg:40.46ms
step:1182/2330 train_time:47825ms step_avg:40.46ms
step:1183/2330 train_time:47860ms step_avg:40.46ms
step:1184/2330 train_time:47905ms step_avg:40.46ms
step:1185/2330 train_time:47940ms step_avg:40.46ms
step:1186/2330 train_time:47984ms step_avg:40.46ms
step:1187/2330 train_time:48020ms step_avg:40.45ms
step:1188/2330 train_time:48064ms step_avg:40.46ms
step:1189/2330 train_time:48100ms step_avg:40.45ms
step:1190/2330 train_time:48145ms step_avg:40.46ms
step:1191/2330 train_time:48181ms step_avg:40.45ms
step:1192/2330 train_time:48225ms step_avg:40.46ms
step:1193/2330 train_time:48260ms step_avg:40.45ms
step:1194/2330 train_time:48306ms step_avg:40.46ms
step:1195/2330 train_time:48340ms step_avg:40.45ms
step:1196/2330 train_time:48385ms step_avg:40.46ms
step:1197/2330 train_time:48420ms step_avg:40.45ms
step:1198/2330 train_time:48465ms step_avg:40.45ms
step:1199/2330 train_time:48500ms step_avg:40.45ms
step:1200/2330 train_time:48545ms step_avg:40.45ms
step:1201/2330 train_time:48581ms step_avg:40.45ms
step:1202/2330 train_time:48626ms step_avg:40.45ms
step:1203/2330 train_time:48661ms step_avg:40.45ms
step:1204/2330 train_time:48706ms step_avg:40.45ms
step:1205/2330 train_time:48742ms step_avg:40.45ms
step:1206/2330 train_time:48787ms step_avg:40.45ms
step:1207/2330 train_time:48822ms step_avg:40.45ms
step:1208/2330 train_time:48868ms step_avg:40.45ms
step:1209/2330 train_time:48903ms step_avg:40.45ms
step:1210/2330 train_time:48947ms step_avg:40.45ms
step:1211/2330 train_time:48983ms step_avg:40.45ms
step:1212/2330 train_time:49027ms step_avg:40.45ms
step:1213/2330 train_time:49063ms step_avg:40.45ms
step:1214/2330 train_time:49108ms step_avg:40.45ms
step:1215/2330 train_time:49143ms step_avg:40.45ms
step:1216/2330 train_time:49188ms step_avg:40.45ms
step:1217/2330 train_time:49223ms step_avg:40.45ms
step:1218/2330 train_time:49268ms step_avg:40.45ms
step:1219/2330 train_time:49303ms step_avg:40.45ms
step:1220/2330 train_time:49349ms step_avg:40.45ms
step:1221/2330 train_time:49384ms step_avg:40.45ms
step:1222/2330 train_time:49428ms step_avg:40.45ms
step:1223/2330 train_time:49464ms step_avg:40.44ms
step:1224/2330 train_time:49509ms step_avg:40.45ms
step:1225/2330 train_time:49544ms step_avg:40.44ms
step:1226/2330 train_time:49589ms step_avg:40.45ms
step:1227/2330 train_time:49625ms step_avg:40.44ms
step:1228/2330 train_time:49671ms step_avg:40.45ms
step:1229/2330 train_time:49706ms step_avg:40.44ms
step:1230/2330 train_time:49751ms step_avg:40.45ms
step:1231/2330 train_time:49787ms step_avg:40.44ms
step:1232/2330 train_time:49832ms step_avg:40.45ms
step:1233/2330 train_time:49868ms step_avg:40.44ms
step:1234/2330 train_time:49912ms step_avg:40.45ms
step:1235/2330 train_time:49948ms step_avg:40.44ms
step:1236/2330 train_time:49993ms step_avg:40.45ms
step:1237/2330 train_time:50028ms step_avg:40.44ms
step:1238/2330 train_time:50073ms step_avg:40.45ms
step:1239/2330 train_time:50108ms step_avg:40.44ms
step:1240/2330 train_time:50153ms step_avg:40.45ms
step:1241/2330 train_time:50189ms step_avg:40.44ms
step:1242/2330 train_time:50233ms step_avg:40.45ms
step:1243/2330 train_time:50270ms step_avg:40.44ms
step:1244/2330 train_time:50315ms step_avg:40.45ms
step:1245/2330 train_time:50351ms step_avg:40.44ms
step:1246/2330 train_time:50396ms step_avg:40.45ms
step:1247/2330 train_time:50432ms step_avg:40.44ms
step:1248/2330 train_time:50477ms step_avg:40.45ms
step:1249/2330 train_time:50513ms step_avg:40.44ms
step:1250/2330 train_time:50559ms step_avg:40.45ms
step:1250/2330 val_loss:5.1630 train_time:50647ms step_avg:40.52ms
step:1251/2330 train_time:50661ms step_avg:40.50ms
step:1252/2330 train_time:50674ms step_avg:40.47ms
step:1253/2330 train_time:50685ms step_avg:40.45ms
step:1254/2330 train_time:50721ms step_avg:40.45ms
step:1255/2330 train_time:50756ms step_avg:40.44ms
step:1256/2330 train_time:50799ms step_avg:40.45ms
step:1257/2330 train_time:50834ms step_avg:40.44ms
step:1258/2330 train_time:50879ms step_avg:40.44ms
step:1259/2330 train_time:50914ms step_avg:40.44ms
step:1260/2330 train_time:50964ms step_avg:40.45ms
step:1261/2330 train_time:51002ms step_avg:40.45ms
step:1262/2330 train_time:51048ms step_avg:40.45ms
step:1263/2330 train_time:51084ms step_avg:40.45ms
step:1264/2330 train_time:51129ms step_avg:40.45ms
step:1265/2330 train_time:51165ms step_avg:40.45ms
step:1266/2330 train_time:51209ms step_avg:40.45ms
step:1267/2330 train_time:51385ms step_avg:40.56ms
step:1268/2330 train_time:51428ms step_avg:40.56ms
step:1269/2330 train_time:51462ms step_avg:40.55ms
step:1270/2330 train_time:51506ms step_avg:40.56ms
step:1271/2330 train_time:51710ms step_avg:40.68ms
step:1272/2330 train_time:51753ms step_avg:40.69ms
step:1273/2330 train_time:51787ms step_avg:40.68ms
step:1274/2330 train_time:51831ms step_avg:40.68ms
step:1275/2330 train_time:51866ms step_avg:40.68ms
step:1276/2330 train_time:51910ms step_avg:40.68ms
step:1277/2330 train_time:51944ms step_avg:40.68ms
step:1278/2330 train_time:51988ms step_avg:40.68ms
step:1279/2330 train_time:52023ms step_avg:40.67ms
step:1280/2330 train_time:52067ms step_avg:40.68ms
step:1281/2330 train_time:52101ms step_avg:40.67ms
step:1282/2330 train_time:52145ms step_avg:40.67ms
step:1283/2330 train_time:52180ms step_avg:40.67ms
step:1284/2330 train_time:52224ms step_avg:40.67ms
step:1285/2330 train_time:52258ms step_avg:40.67ms
step:1286/2330 train_time:52302ms step_avg:40.67ms
step:1287/2330 train_time:52336ms step_avg:40.67ms
step:1288/2330 train_time:52380ms step_avg:40.67ms
step:1289/2330 train_time:52415ms step_avg:40.66ms
step:1290/2330 train_time:52459ms step_avg:40.67ms
step:1291/2330 train_time:52493ms step_avg:40.66ms
step:1292/2330 train_time:52537ms step_avg:40.66ms
step:1293/2330 train_time:52578ms step_avg:40.66ms
step:1294/2330 train_time:52629ms step_avg:40.67ms
step:1295/2330 train_time:52666ms step_avg:40.67ms
step:1296/2330 train_time:52712ms step_avg:40.67ms
step:1297/2330 train_time:52747ms step_avg:40.67ms
step:1298/2330 train_time:52792ms step_avg:40.67ms
step:1299/2330 train_time:52827ms step_avg:40.67ms
step:1300/2330 train_time:52871ms step_avg:40.67ms
step:1301/2330 train_time:52906ms step_avg:40.67ms
step:1302/2330 train_time:52950ms step_avg:40.67ms
step:1303/2330 train_time:52985ms step_avg:40.66ms
step:1304/2330 train_time:53030ms step_avg:40.67ms
step:1305/2330 train_time:53065ms step_avg:40.66ms
step:1306/2330 train_time:53109ms step_avg:40.67ms
step:1307/2330 train_time:53144ms step_avg:40.66ms
step:1308/2330 train_time:53188ms step_avg:40.66ms
step:1309/2330 train_time:53223ms step_avg:40.66ms
step:1310/2330 train_time:53268ms step_avg:40.66ms
step:1311/2330 train_time:53302ms step_avg:40.66ms
step:1312/2330 train_time:53347ms step_avg:40.66ms
step:1313/2330 train_time:53381ms step_avg:40.66ms
step:1314/2330 train_time:53426ms step_avg:40.66ms
step:1315/2330 train_time:53461ms step_avg:40.65ms
step:1316/2330 train_time:53506ms step_avg:40.66ms
step:1317/2330 train_time:53542ms step_avg:40.65ms
step:1318/2330 train_time:53588ms step_avg:40.66ms
step:1319/2330 train_time:53626ms step_avg:40.66ms
step:1320/2330 train_time:53672ms step_avg:40.66ms
step:1321/2330 train_time:53709ms step_avg:40.66ms
step:1322/2330 train_time:53754ms step_avg:40.66ms
step:1323/2330 train_time:53790ms step_avg:40.66ms
step:1324/2330 train_time:53835ms step_avg:40.66ms
step:1325/2330 train_time:53871ms step_avg:40.66ms
step:1326/2330 train_time:53915ms step_avg:40.66ms
step:1327/2330 train_time:53951ms step_avg:40.66ms
step:1328/2330 train_time:53995ms step_avg:40.66ms
step:1329/2330 train_time:54031ms step_avg:40.66ms
step:1330/2330 train_time:54075ms step_avg:40.66ms
step:1331/2330 train_time:54110ms step_avg:40.65ms
step:1332/2330 train_time:54155ms step_avg:40.66ms
step:1333/2330 train_time:54190ms step_avg:40.65ms
step:1334/2330 train_time:54235ms step_avg:40.66ms
step:1335/2330 train_time:54271ms step_avg:40.65ms
step:1336/2330 train_time:54315ms step_avg:40.66ms
step:1337/2330 train_time:54350ms step_avg:40.65ms
step:1338/2330 train_time:54394ms step_avg:40.65ms
step:1339/2330 train_time:54429ms step_avg:40.65ms
step:1340/2330 train_time:54474ms step_avg:40.65ms
step:1341/2330 train_time:54510ms step_avg:40.65ms
step:1342/2330 train_time:54555ms step_avg:40.65ms
step:1343/2330 train_time:54591ms step_avg:40.65ms
step:1344/2330 train_time:54637ms step_avg:40.65ms
step:1345/2330 train_time:54673ms step_avg:40.65ms
step:1346/2330 train_time:54719ms step_avg:40.65ms
step:1347/2330 train_time:54755ms step_avg:40.65ms
step:1348/2330 train_time:54801ms step_avg:40.65ms
step:1349/2330 train_time:54836ms step_avg:40.65ms
step:1350/2330 train_time:54880ms step_avg:40.65ms
step:1351/2330 train_time:54916ms step_avg:40.65ms
step:1352/2330 train_time:54961ms step_avg:40.65ms
step:1353/2330 train_time:54997ms step_avg:40.65ms
step:1354/2330 train_time:55042ms step_avg:40.65ms
step:1355/2330 train_time:55077ms step_avg:40.65ms
step:1356/2330 train_time:55122ms step_avg:40.65ms
step:1357/2330 train_time:55158ms step_avg:40.65ms
step:1358/2330 train_time:55203ms step_avg:40.65ms
step:1359/2330 train_time:55238ms step_avg:40.65ms
step:1360/2330 train_time:55283ms step_avg:40.65ms
step:1361/2330 train_time:55318ms step_avg:40.64ms
step:1362/2330 train_time:55362ms step_avg:40.65ms
step:1363/2330 train_time:55398ms step_avg:40.64ms
step:1364/2330 train_time:55443ms step_avg:40.65ms
step:1365/2330 train_time:55478ms step_avg:40.64ms
step:1366/2330 train_time:55523ms step_avg:40.65ms
step:1367/2330 train_time:55559ms step_avg:40.64ms
step:1368/2330 train_time:55604ms step_avg:40.65ms
step:1369/2330 train_time:55639ms step_avg:40.64ms
step:1370/2330 train_time:55684ms step_avg:40.65ms
step:1371/2330 train_time:55721ms step_avg:40.64ms
step:1372/2330 train_time:55766ms step_avg:40.65ms
step:1373/2330 train_time:55801ms step_avg:40.64ms
step:1374/2330 train_time:55846ms step_avg:40.65ms
step:1375/2330 train_time:55881ms step_avg:40.64ms
step:1376/2330 train_time:55927ms step_avg:40.64ms
step:1377/2330 train_time:55962ms step_avg:40.64ms
step:1378/2330 train_time:56007ms step_avg:40.64ms
step:1379/2330 train_time:56042ms step_avg:40.64ms
step:1380/2330 train_time:56086ms step_avg:40.64ms
step:1381/2330 train_time:56122ms step_avg:40.64ms
step:1382/2330 train_time:56167ms step_avg:40.64ms
step:1383/2330 train_time:56202ms step_avg:40.64ms
step:1384/2330 train_time:56247ms step_avg:40.64ms
step:1385/2330 train_time:56282ms step_avg:40.64ms
step:1386/2330 train_time:56326ms step_avg:40.64ms
step:1387/2330 train_time:56362ms step_avg:40.64ms
step:1388/2330 train_time:56406ms step_avg:40.64ms
step:1389/2330 train_time:56441ms step_avg:40.63ms
step:1390/2330 train_time:56487ms step_avg:40.64ms
step:1391/2330 train_time:56522ms step_avg:40.63ms
step:1392/2330 train_time:56567ms step_avg:40.64ms
step:1393/2330 train_time:56602ms step_avg:40.63ms
step:1394/2330 train_time:56647ms step_avg:40.64ms
step:1395/2330 train_time:56683ms step_avg:40.63ms
step:1396/2330 train_time:56728ms step_avg:40.64ms
step:1397/2330 train_time:56763ms step_avg:40.63ms
step:1398/2330 train_time:56808ms step_avg:40.64ms
step:1399/2330 train_time:56843ms step_avg:40.63ms
step:1400/2330 train_time:56889ms step_avg:40.63ms
step:1401/2330 train_time:56924ms step_avg:40.63ms
step:1402/2330 train_time:56968ms step_avg:40.63ms
step:1403/2330 train_time:57004ms step_avg:40.63ms
step:1404/2330 train_time:57049ms step_avg:40.63ms
step:1405/2330 train_time:57085ms step_avg:40.63ms
step:1406/2330 train_time:57130ms step_avg:40.63ms
step:1407/2330 train_time:57165ms step_avg:40.63ms
step:1408/2330 train_time:57209ms step_avg:40.63ms
step:1409/2330 train_time:57245ms step_avg:40.63ms
step:1410/2330 train_time:57290ms step_avg:40.63ms
step:1411/2330 train_time:57325ms step_avg:40.63ms
step:1412/2330 train_time:57371ms step_avg:40.63ms
step:1413/2330 train_time:57406ms step_avg:40.63ms
step:1414/2330 train_time:57451ms step_avg:40.63ms
step:1415/2330 train_time:57486ms step_avg:40.63ms
step:1416/2330 train_time:57531ms step_avg:40.63ms
step:1417/2330 train_time:57567ms step_avg:40.63ms
step:1418/2330 train_time:57611ms step_avg:40.63ms
step:1419/2330 train_time:57647ms step_avg:40.62ms
step:1420/2330 train_time:57692ms step_avg:40.63ms
step:1421/2330 train_time:57728ms step_avg:40.62ms
step:1422/2330 train_time:57773ms step_avg:40.63ms
step:1423/2330 train_time:57809ms step_avg:40.62ms
step:1424/2330 train_time:57854ms step_avg:40.63ms
step:1425/2330 train_time:57889ms step_avg:40.62ms
step:1426/2330 train_time:57934ms step_avg:40.63ms
step:1427/2330 train_time:57970ms step_avg:40.62ms
step:1428/2330 train_time:58015ms step_avg:40.63ms
step:1429/2330 train_time:58051ms step_avg:40.62ms
step:1430/2330 train_time:58096ms step_avg:40.63ms
step:1431/2330 train_time:58131ms step_avg:40.62ms
step:1432/2330 train_time:58177ms step_avg:40.63ms
step:1433/2330 train_time:58213ms step_avg:40.62ms
step:1434/2330 train_time:58259ms step_avg:40.63ms
step:1435/2330 train_time:58294ms step_avg:40.62ms
step:1436/2330 train_time:58339ms step_avg:40.63ms
step:1437/2330 train_time:58375ms step_avg:40.62ms
step:1438/2330 train_time:58421ms step_avg:40.63ms
step:1439/2330 train_time:58457ms step_avg:40.62ms
step:1440/2330 train_time:58502ms step_avg:40.63ms
step:1441/2330 train_time:58537ms step_avg:40.62ms
step:1442/2330 train_time:58583ms step_avg:40.63ms
step:1443/2330 train_time:58619ms step_avg:40.62ms
step:1444/2330 train_time:58663ms step_avg:40.63ms
step:1445/2330 train_time:58699ms step_avg:40.62ms
step:1446/2330 train_time:58744ms step_avg:40.62ms
step:1447/2330 train_time:58779ms step_avg:40.62ms
step:1448/2330 train_time:58824ms step_avg:40.62ms
step:1449/2330 train_time:58859ms step_avg:40.62ms
step:1450/2330 train_time:58903ms step_avg:40.62ms
step:1451/2330 train_time:58938ms step_avg:40.62ms
step:1452/2330 train_time:58984ms step_avg:40.62ms
step:1453/2330 train_time:59019ms step_avg:40.62ms
step:1454/2330 train_time:59063ms step_avg:40.62ms
step:1455/2330 train_time:59099ms step_avg:40.62ms
step:1456/2330 train_time:59143ms step_avg:40.62ms
step:1457/2330 train_time:59179ms step_avg:40.62ms
step:1458/2330 train_time:59224ms step_avg:40.62ms
step:1459/2330 train_time:59259ms step_avg:40.62ms
step:1460/2330 train_time:59304ms step_avg:40.62ms
step:1461/2330 train_time:59340ms step_avg:40.62ms
step:1462/2330 train_time:59385ms step_avg:40.62ms
step:1463/2330 train_time:59420ms step_avg:40.62ms
step:1464/2330 train_time:59464ms step_avg:40.62ms
step:1465/2330 train_time:59500ms step_avg:40.61ms
step:1466/2330 train_time:59546ms step_avg:40.62ms
step:1467/2330 train_time:59581ms step_avg:40.61ms
step:1468/2330 train_time:59626ms step_avg:40.62ms
step:1469/2330 train_time:59661ms step_avg:40.61ms
step:1470/2330 train_time:59706ms step_avg:40.62ms
step:1471/2330 train_time:59740ms step_avg:40.61ms
step:1472/2330 train_time:59785ms step_avg:40.62ms
step:1473/2330 train_time:59820ms step_avg:40.61ms
step:1474/2330 train_time:59865ms step_avg:40.61ms
step:1475/2330 train_time:59900ms step_avg:40.61ms
step:1476/2330 train_time:59945ms step_avg:40.61ms
step:1477/2330 train_time:59980ms step_avg:40.61ms
step:1478/2330 train_time:60025ms step_avg:40.61ms
step:1479/2330 train_time:60060ms step_avg:40.61ms
step:1480/2330 train_time:60105ms step_avg:40.61ms
step:1481/2330 train_time:60140ms step_avg:40.61ms
step:1482/2330 train_time:60185ms step_avg:40.61ms
step:1483/2330 train_time:60220ms step_avg:40.61ms
step:1484/2330 train_time:60264ms step_avg:40.61ms
step:1485/2330 train_time:60300ms step_avg:40.61ms
step:1486/2330 train_time:60344ms step_avg:40.61ms
step:1487/2330 train_time:60380ms step_avg:40.61ms
step:1488/2330 train_time:60425ms step_avg:40.61ms
step:1489/2330 train_time:60460ms step_avg:40.60ms
step:1490/2330 train_time:60505ms step_avg:40.61ms
step:1491/2330 train_time:60541ms step_avg:40.60ms
step:1492/2330 train_time:60585ms step_avg:40.61ms
step:1493/2330 train_time:60621ms step_avg:40.60ms
step:1494/2330 train_time:60666ms step_avg:40.61ms
step:1495/2330 train_time:60700ms step_avg:40.60ms
step:1496/2330 train_time:60745ms step_avg:40.60ms
step:1497/2330 train_time:60780ms step_avg:40.60ms
step:1498/2330 train_time:60824ms step_avg:40.60ms
step:1499/2330 train_time:60860ms step_avg:40.60ms
step:1500/2330 train_time:60905ms step_avg:40.60ms
step:1500/2330 val_loss:5.1288 train_time:60992ms step_avg:40.66ms
step:1501/2330 train_time:61005ms step_avg:40.64ms
step:1502/2330 train_time:61018ms step_avg:40.62ms
step:1503/2330 train_time:61029ms step_avg:40.61ms
step:1504/2330 train_time:61064ms step_avg:40.60ms
step:1505/2330 train_time:61099ms step_avg:40.60ms
step:1506/2330 train_time:61143ms step_avg:40.60ms
step:1507/2330 train_time:61177ms step_avg:40.60ms
step:1508/2330 train_time:61221ms step_avg:40.60ms
step:1509/2330 train_time:61255ms step_avg:40.59ms
step:1510/2330 train_time:61301ms step_avg:40.60ms
step:1511/2330 train_time:61340ms step_avg:40.60ms
step:1512/2330 train_time:61387ms step_avg:40.60ms
step:1513/2330 train_time:61423ms step_avg:40.60ms
step:1514/2330 train_time:61470ms step_avg:40.60ms
step:1515/2330 train_time:61507ms step_avg:40.60ms
step:1516/2330 train_time:61552ms step_avg:40.60ms
step:1517/2330 train_time:61587ms step_avg:40.60ms
step:1518/2330 train_time:61632ms step_avg:40.60ms
step:1519/2330 train_time:61667ms step_avg:40.60ms
step:1520/2330 train_time:61711ms step_avg:40.60ms
step:1521/2330 train_time:61828ms step_avg:40.65ms
step:1522/2330 train_time:61870ms step_avg:40.65ms
step:1523/2330 train_time:61904ms step_avg:40.65ms
step:1524/2330 train_time:61948ms step_avg:40.65ms
step:1525/2330 train_time:61983ms step_avg:40.64ms
step:1526/2330 train_time:62026ms step_avg:40.65ms
step:1527/2330 train_time:62061ms step_avg:40.64ms
step:1528/2330 train_time:62104ms step_avg:40.64ms
step:1529/2330 train_time:62140ms step_avg:40.64ms
step:1530/2330 train_time:62184ms step_avg:40.64ms
step:1531/2330 train_time:62218ms step_avg:40.64ms
step:1532/2330 train_time:62261ms step_avg:40.64ms
step:1533/2330 train_time:62296ms step_avg:40.64ms
step:1534/2330 train_time:62340ms step_avg:40.64ms
step:1535/2330 train_time:62375ms step_avg:40.63ms
step:1536/2330 train_time:62418ms step_avg:40.64ms
step:1537/2330 train_time:62453ms step_avg:40.63ms
step:1538/2330 train_time:62496ms step_avg:40.63ms
step:1539/2330 train_time:62531ms step_avg:40.63ms
step:1540/2330 train_time:62574ms step_avg:40.63ms
step:1541/2330 train_time:62609ms step_avg:40.63ms
step:1542/2330 train_time:62653ms step_avg:40.63ms
step:1543/2330 train_time:62694ms step_avg:40.63ms
step:1544/2330 train_time:62746ms step_avg:40.64ms
step:1545/2330 train_time:62785ms step_avg:40.64ms
step:1546/2330 train_time:62831ms step_avg:40.64ms
step:1547/2330 train_time:62866ms step_avg:40.64ms
step:1548/2330 train_time:62911ms step_avg:40.64ms
step:1549/2330 train_time:62946ms step_avg:40.64ms
step:1550/2330 train_time:62990ms step_avg:40.64ms
step:1551/2330 train_time:63024ms step_avg:40.63ms
step:1552/2330 train_time:63069ms step_avg:40.64ms
step:1553/2330 train_time:63104ms step_avg:40.63ms
step:1554/2330 train_time:63149ms step_avg:40.64ms
step:1555/2330 train_time:63183ms step_avg:40.63ms
step:1556/2330 train_time:63228ms step_avg:40.63ms
step:1557/2330 train_time:63263ms step_avg:40.63ms
step:1558/2330 train_time:63308ms step_avg:40.63ms
step:1559/2330 train_time:63342ms step_avg:40.63ms
step:1560/2330 train_time:63387ms step_avg:40.63ms
step:1561/2330 train_time:63421ms step_avg:40.63ms
step:1562/2330 train_time:63466ms step_avg:40.63ms
step:1563/2330 train_time:63501ms step_avg:40.63ms
step:1564/2330 train_time:63545ms step_avg:40.63ms
step:1565/2330 train_time:63579ms step_avg:40.63ms
step:1566/2330 train_time:63625ms step_avg:40.63ms
step:1567/2330 train_time:63662ms step_avg:40.63ms
step:1568/2330 train_time:63708ms step_avg:40.63ms
step:1569/2330 train_time:63744ms step_avg:40.63ms
step:1570/2330 train_time:63790ms step_avg:40.63ms
step:1571/2330 train_time:63826ms step_avg:40.63ms
step:1572/2330 train_time:63871ms step_avg:40.63ms
step:1573/2330 train_time:63906ms step_avg:40.63ms
step:1574/2330 train_time:63951ms step_avg:40.63ms
step:1575/2330 train_time:63987ms step_avg:40.63ms
step:1576/2330 train_time:64032ms step_avg:40.63ms
step:1577/2330 train_time:64067ms step_avg:40.63ms
step:1578/2330 train_time:64111ms step_avg:40.63ms
step:1579/2330 train_time:64146ms step_avg:40.62ms
step:1580/2330 train_time:64191ms step_avg:40.63ms
step:1581/2330 train_time:64225ms step_avg:40.62ms
step:1582/2330 train_time:64270ms step_avg:40.63ms
step:1583/2330 train_time:64306ms step_avg:40.62ms
step:1584/2330 train_time:64350ms step_avg:40.63ms
step:1585/2330 train_time:64385ms step_avg:40.62ms
step:1586/2330 train_time:64430ms step_avg:40.62ms
step:1587/2330 train_time:64465ms step_avg:40.62ms
step:1588/2330 train_time:64509ms step_avg:40.62ms
step:1589/2330 train_time:64544ms step_avg:40.62ms
step:1590/2330 train_time:64589ms step_avg:40.62ms
step:1591/2330 train_time:64624ms step_avg:40.62ms
step:1592/2330 train_time:64669ms step_avg:40.62ms
step:1593/2330 train_time:64706ms step_avg:40.62ms
step:1594/2330 train_time:64751ms step_avg:40.62ms
step:1595/2330 train_time:64787ms step_avg:40.62ms
step:1596/2330 train_time:64832ms step_avg:40.62ms
step:1597/2330 train_time:64867ms step_avg:40.62ms
step:1598/2330 train_time:64912ms step_avg:40.62ms
step:1599/2330 train_time:64948ms step_avg:40.62ms
step:1600/2330 train_time:64993ms step_avg:40.62ms
step:1601/2330 train_time:65028ms step_avg:40.62ms
step:1602/2330 train_time:65072ms step_avg:40.62ms
step:1603/2330 train_time:65108ms step_avg:40.62ms
step:1604/2330 train_time:65153ms step_avg:40.62ms
step:1605/2330 train_time:65189ms step_avg:40.62ms
step:1606/2330 train_time:65233ms step_avg:40.62ms
step:1607/2330 train_time:65269ms step_avg:40.62ms
step:1608/2330 train_time:65314ms step_avg:40.62ms
step:1609/2330 train_time:65349ms step_avg:40.61ms
step:1610/2330 train_time:65394ms step_avg:40.62ms
step:1611/2330 train_time:65429ms step_avg:40.61ms
step:1612/2330 train_time:65473ms step_avg:40.62ms
step:1613/2330 train_time:65509ms step_avg:40.61ms
step:1614/2330 train_time:65553ms step_avg:40.62ms
step:1615/2330 train_time:65589ms step_avg:40.61ms
step:1616/2330 train_time:65634ms step_avg:40.61ms
step:1617/2330 train_time:65670ms step_avg:40.61ms
step:1618/2330 train_time:65715ms step_avg:40.61ms
step:1619/2330 train_time:65750ms step_avg:40.61ms
step:1620/2330 train_time:65796ms step_avg:40.61ms
step:1621/2330 train_time:65832ms step_avg:40.61ms
step:1622/2330 train_time:65878ms step_avg:40.62ms
step:1623/2330 train_time:65914ms step_avg:40.61ms
step:1624/2330 train_time:65959ms step_avg:40.62ms
step:1625/2330 train_time:65994ms step_avg:40.61ms
step:1626/2330 train_time:66039ms step_avg:40.61ms
step:1627/2330 train_time:66075ms step_avg:40.61ms
step:1628/2330 train_time:66119ms step_avg:40.61ms
step:1629/2330 train_time:66154ms step_avg:40.61ms
step:1630/2330 train_time:66198ms step_avg:40.61ms
step:1631/2330 train_time:66234ms step_avg:40.61ms
step:1632/2330 train_time:66279ms step_avg:40.61ms
step:1633/2330 train_time:66315ms step_avg:40.61ms
step:1634/2330 train_time:66359ms step_avg:40.61ms
step:1635/2330 train_time:66395ms step_avg:40.61ms
step:1636/2330 train_time:66440ms step_avg:40.61ms
step:1637/2330 train_time:66475ms step_avg:40.61ms
step:1638/2330 train_time:66521ms step_avg:40.61ms
step:1639/2330 train_time:66556ms step_avg:40.61ms
step:1640/2330 train_time:66601ms step_avg:40.61ms
step:1641/2330 train_time:66637ms step_avg:40.61ms
step:1642/2330 train_time:66681ms step_avg:40.61ms
step:1643/2330 train_time:66717ms step_avg:40.61ms
step:1644/2330 train_time:66762ms step_avg:40.61ms
step:1645/2330 train_time:66797ms step_avg:40.61ms
step:1646/2330 train_time:66843ms step_avg:40.61ms
step:1647/2330 train_time:66878ms step_avg:40.61ms
step:1648/2330 train_time:66923ms step_avg:40.61ms
step:1649/2330 train_time:66959ms step_avg:40.61ms
step:1650/2330 train_time:67003ms step_avg:40.61ms
step:1651/2330 train_time:67039ms step_avg:40.61ms
step:1652/2330 train_time:67084ms step_avg:40.61ms
step:1653/2330 train_time:67118ms step_avg:40.60ms
step:1654/2330 train_time:67164ms step_avg:40.61ms
step:1655/2330 train_time:67199ms step_avg:40.60ms
step:1656/2330 train_time:67245ms step_avg:40.61ms
step:1657/2330 train_time:67280ms step_avg:40.60ms
step:1658/2330 train_time:67325ms step_avg:40.61ms
step:1659/2330 train_time:67360ms step_avg:40.60ms
step:1660/2330 train_time:67404ms step_avg:40.60ms
step:1661/2330 train_time:67440ms step_avg:40.60ms
step:1662/2330 train_time:67485ms step_avg:40.60ms
step:1663/2330 train_time:67520ms step_avg:40.60ms
step:1664/2330 train_time:67564ms step_avg:40.60ms
step:1665/2330 train_time:67599ms step_avg:40.60ms
step:1666/2330 train_time:67644ms step_avg:40.60ms
step:1667/2330 train_time:67679ms step_avg:40.60ms
step:1668/2330 train_time:67724ms step_avg:40.60ms
step:1669/2330 train_time:67760ms step_avg:40.60ms
step:1670/2330 train_time:67805ms step_avg:40.60ms
step:1671/2330 train_time:67840ms step_avg:40.60ms
step:1672/2330 train_time:67885ms step_avg:40.60ms
step:1673/2330 train_time:67920ms step_avg:40.60ms
step:1674/2330 train_time:67965ms step_avg:40.60ms
step:1675/2330 train_time:68000ms step_avg:40.60ms
step:1676/2330 train_time:68045ms step_avg:40.60ms
step:1677/2330 train_time:68080ms step_avg:40.60ms
step:1678/2330 train_time:68125ms step_avg:40.60ms
step:1679/2330 train_time:68160ms step_avg:40.60ms
step:1680/2330 train_time:68206ms step_avg:40.60ms
step:1681/2330 train_time:68241ms step_avg:40.60ms
step:1682/2330 train_time:68287ms step_avg:40.60ms
step:1683/2330 train_time:68322ms step_avg:40.60ms
step:1684/2330 train_time:68366ms step_avg:40.60ms
step:1685/2330 train_time:68402ms step_avg:40.59ms
step:1686/2330 train_time:68447ms step_avg:40.60ms
step:1687/2330 train_time:68482ms step_avg:40.59ms
step:1688/2330 train_time:68526ms step_avg:40.60ms
step:1689/2330 train_time:68562ms step_avg:40.59ms
step:1690/2330 train_time:68606ms step_avg:40.60ms
step:1691/2330 train_time:68641ms step_avg:40.59ms
step:1692/2330 train_time:68687ms step_avg:40.59ms
step:1693/2330 train_time:68722ms step_avg:40.59ms
step:1694/2330 train_time:68766ms step_avg:40.59ms
step:1695/2330 train_time:68802ms step_avg:40.59ms
step:1696/2330 train_time:68847ms step_avg:40.59ms
step:1697/2330 train_time:68882ms step_avg:40.59ms
step:1698/2330 train_time:68927ms step_avg:40.59ms
step:1699/2330 train_time:68962ms step_avg:40.59ms
step:1700/2330 train_time:69007ms step_avg:40.59ms
step:1701/2330 train_time:69043ms step_avg:40.59ms
step:1702/2330 train_time:69087ms step_avg:40.59ms
step:1703/2330 train_time:69122ms step_avg:40.59ms
step:1704/2330 train_time:69167ms step_avg:40.59ms
step:1705/2330 train_time:69202ms step_avg:40.59ms
step:1706/2330 train_time:69247ms step_avg:40.59ms
step:1707/2330 train_time:69283ms step_avg:40.59ms
step:1708/2330 train_time:69328ms step_avg:40.59ms
step:1709/2330 train_time:69363ms step_avg:40.59ms
step:1710/2330 train_time:69407ms step_avg:40.59ms
step:1711/2330 train_time:69442ms step_avg:40.59ms
step:1712/2330 train_time:69488ms step_avg:40.59ms
step:1713/2330 train_time:69522ms step_avg:40.58ms
step:1714/2330 train_time:69567ms step_avg:40.59ms
step:1715/2330 train_time:69602ms step_avg:40.58ms
step:1716/2330 train_time:69647ms step_avg:40.59ms
step:1717/2330 train_time:69683ms step_avg:40.58ms
step:1718/2330 train_time:69727ms step_avg:40.59ms
step:1719/2330 train_time:69763ms step_avg:40.58ms
step:1720/2330 train_time:69807ms step_avg:40.59ms
step:1721/2330 train_time:69842ms step_avg:40.58ms
step:1722/2330 train_time:69888ms step_avg:40.59ms
step:1723/2330 train_time:69923ms step_avg:40.58ms
step:1724/2330 train_time:69968ms step_avg:40.58ms
step:1725/2330 train_time:70003ms step_avg:40.58ms
step:1726/2330 train_time:70049ms step_avg:40.58ms
step:1727/2330 train_time:70084ms step_avg:40.58ms
step:1728/2330 train_time:70129ms step_avg:40.58ms
step:1729/2330 train_time:70164ms step_avg:40.58ms
step:1730/2330 train_time:70210ms step_avg:40.58ms
step:1731/2330 train_time:70245ms step_avg:40.58ms
step:1732/2330 train_time:70290ms step_avg:40.58ms
step:1733/2330 train_time:70325ms step_avg:40.58ms
step:1734/2330 train_time:70369ms step_avg:40.58ms
step:1735/2330 train_time:70405ms step_avg:40.58ms
step:1736/2330 train_time:70449ms step_avg:40.58ms
step:1737/2330 train_time:70485ms step_avg:40.58ms
step:1738/2330 train_time:70529ms step_avg:40.58ms
step:1739/2330 train_time:70564ms step_avg:40.58ms
step:1740/2330 train_time:70609ms step_avg:40.58ms
step:1741/2330 train_time:70644ms step_avg:40.58ms
step:1742/2330 train_time:70688ms step_avg:40.58ms
step:1743/2330 train_time:70723ms step_avg:40.58ms
step:1744/2330 train_time:70769ms step_avg:40.58ms
step:1745/2330 train_time:70805ms step_avg:40.58ms
step:1746/2330 train_time:70850ms step_avg:40.58ms
step:1747/2330 train_time:70885ms step_avg:40.58ms
step:1748/2330 train_time:70930ms step_avg:40.58ms
step:1749/2330 train_time:70965ms step_avg:40.57ms
step:1750/2330 train_time:71010ms step_avg:40.58ms
step:1750/2330 val_loss:5.0947 train_time:71098ms step_avg:40.63ms
step:1751/2330 train_time:71113ms step_avg:40.61ms
step:1752/2330 train_time:71125ms step_avg:40.60ms
step:1753/2330 train_time:71137ms step_avg:40.58ms
step:1754/2330 train_time:71174ms step_avg:40.58ms
step:1755/2330 train_time:71208ms step_avg:40.57ms
step:1756/2330 train_time:71252ms step_avg:40.58ms
step:1757/2330 train_time:71287ms step_avg:40.57ms
step:1758/2330 train_time:71330ms step_avg:40.57ms
step:1759/2330 train_time:71365ms step_avg:40.57ms
step:1760/2330 train_time:71409ms step_avg:40.57ms
step:1761/2330 train_time:71444ms step_avg:40.57ms
step:1762/2330 train_time:71490ms step_avg:40.57ms
step:1763/2330 train_time:71528ms step_avg:40.57ms
step:1764/2330 train_time:71575ms step_avg:40.58ms
step:1765/2330 train_time:71611ms step_avg:40.57ms
step:1766/2330 train_time:71655ms step_avg:40.57ms
step:1767/2330 train_time:71690ms step_avg:40.57ms
step:1768/2330 train_time:71734ms step_avg:40.57ms
step:1769/2330 train_time:71769ms step_avg:40.57ms
step:1770/2330 train_time:71813ms step_avg:40.57ms
step:1771/2330 train_time:71848ms step_avg:40.57ms
step:1772/2330 train_time:71892ms step_avg:40.57ms
step:1773/2330 train_time:71927ms step_avg:40.57ms
step:1774/2330 train_time:71971ms step_avg:40.57ms
step:1775/2330 train_time:72010ms step_avg:40.57ms
step:1776/2330 train_time:72059ms step_avg:40.57ms
step:1777/2330 train_time:72097ms step_avg:40.57ms
step:1778/2330 train_time:72145ms step_avg:40.58ms
step:1779/2330 train_time:72181ms step_avg:40.57ms
step:1780/2330 train_time:72226ms step_avg:40.58ms
step:1781/2330 train_time:72261ms step_avg:40.57ms
step:1782/2330 train_time:72305ms step_avg:40.58ms
step:1783/2330 train_time:72340ms step_avg:40.57ms
step:1784/2330 train_time:72385ms step_avg:40.57ms
step:1785/2330 train_time:72421ms step_avg:40.57ms
step:1786/2330 train_time:72466ms step_avg:40.57ms
step:1787/2330 train_time:72502ms step_avg:40.57ms
step:1788/2330 train_time:72547ms step_avg:40.57ms
step:1789/2330 train_time:72582ms step_avg:40.57ms
step:1790/2330 train_time:72626ms step_avg:40.57ms
step:1791/2330 train_time:72662ms step_avg:40.57ms
step:1792/2330 train_time:72705ms step_avg:40.57ms
step:1793/2330 train_time:72740ms step_avg:40.57ms
step:1794/2330 train_time:72785ms step_avg:40.57ms
step:1795/2330 train_time:72821ms step_avg:40.57ms
step:1796/2330 train_time:72865ms step_avg:40.57ms
step:1797/2330 train_time:72901ms step_avg:40.57ms
step:1798/2330 train_time:72947ms step_avg:40.57ms
step:1799/2330 train_time:72983ms step_avg:40.57ms
step:1800/2330 train_time:73030ms step_avg:40.57ms
step:1801/2330 train_time:73065ms step_avg:40.57ms
step:1802/2330 train_time:73111ms step_avg:40.57ms
step:1803/2330 train_time:73146ms step_avg:40.57ms
step:1804/2330 train_time:73191ms step_avg:40.57ms
step:1805/2330 train_time:73226ms step_avg:40.57ms
step:1806/2330 train_time:73270ms step_avg:40.57ms
step:1807/2330 train_time:73306ms step_avg:40.57ms
step:1808/2330 train_time:73351ms step_avg:40.57ms
step:1809/2330 train_time:73386ms step_avg:40.57ms
step:1810/2330 train_time:73431ms step_avg:40.57ms
step:1811/2330 train_time:73466ms step_avg:40.57ms
step:1812/2330 train_time:73510ms step_avg:40.57ms
step:1813/2330 train_time:73545ms step_avg:40.57ms
step:1814/2330 train_time:73589ms step_avg:40.57ms
step:1815/2330 train_time:73624ms step_avg:40.56ms
step:1816/2330 train_time:73669ms step_avg:40.57ms
step:1817/2330 train_time:73704ms step_avg:40.56ms
step:1818/2330 train_time:73748ms step_avg:40.57ms
step:1819/2330 train_time:73783ms step_avg:40.56ms
step:1820/2330 train_time:73828ms step_avg:40.56ms
step:1821/2330 train_time:73864ms step_avg:40.56ms
step:1822/2330 train_time:73908ms step_avg:40.56ms
step:1823/2330 train_time:73944ms step_avg:40.56ms
step:1824/2330 train_time:73990ms step_avg:40.56ms
step:1825/2330 train_time:74026ms step_avg:40.56ms
step:1826/2330 train_time:74071ms step_avg:40.56ms
step:1827/2330 train_time:74106ms step_avg:40.56ms
step:1828/2330 train_time:74151ms step_avg:40.56ms
step:1829/2330 train_time:74186ms step_avg:40.56ms
step:1830/2330 train_time:74231ms step_avg:40.56ms
step:1831/2330 train_time:74266ms step_avg:40.56ms
step:1832/2330 train_time:74310ms step_avg:40.56ms
step:1833/2330 train_time:74347ms step_avg:40.56ms
step:1834/2330 train_time:74391ms step_avg:40.56ms
step:1835/2330 train_time:74426ms step_avg:40.56ms
step:1836/2330 train_time:74471ms step_avg:40.56ms
step:1837/2330 train_time:74506ms step_avg:40.56ms
step:1838/2330 train_time:74550ms step_avg:40.56ms
step:1839/2330 train_time:74585ms step_avg:40.56ms
step:1840/2330 train_time:74629ms step_avg:40.56ms
step:1841/2330 train_time:74664ms step_avg:40.56ms
step:1842/2330 train_time:74708ms step_avg:40.56ms
step:1843/2330 train_time:74744ms step_avg:40.56ms
step:1844/2330 train_time:74789ms step_avg:40.56ms
step:1845/2330 train_time:74823ms step_avg:40.55ms
step:1846/2330 train_time:74869ms step_avg:40.56ms
step:1847/2330 train_time:74904ms step_avg:40.55ms
step:1848/2330 train_time:74949ms step_avg:40.56ms
step:1849/2330 train_time:74985ms step_avg:40.55ms
step:1850/2330 train_time:75030ms step_avg:40.56ms
step:1851/2330 train_time:75065ms step_avg:40.55ms
step:1852/2330 train_time:75110ms step_avg:40.56ms
step:1853/2330 train_time:75146ms step_avg:40.55ms
step:1854/2330 train_time:75191ms step_avg:40.56ms
step:1855/2330 train_time:75226ms step_avg:40.55ms
step:1856/2330 train_time:75271ms step_avg:40.56ms
step:1857/2330 train_time:75306ms step_avg:40.55ms
step:1858/2330 train_time:75350ms step_avg:40.55ms
step:1859/2330 train_time:75386ms step_avg:40.55ms
step:1860/2330 train_time:75431ms step_avg:40.55ms
step:1861/2330 train_time:75467ms step_avg:40.55ms
step:1862/2330 train_time:75511ms step_avg:40.55ms
step:1863/2330 train_time:75546ms step_avg:40.55ms
step:1864/2330 train_time:75590ms step_avg:40.55ms
step:1865/2330 train_time:75625ms step_avg:40.55ms
step:1866/2330 train_time:75669ms step_avg:40.55ms
step:1867/2330 train_time:75704ms step_avg:40.55ms
step:1868/2330 train_time:75749ms step_avg:40.55ms
step:1869/2330 train_time:75784ms step_avg:40.55ms
step:1870/2330 train_time:75829ms step_avg:40.55ms
step:1871/2330 train_time:75864ms step_avg:40.55ms
step:1872/2330 train_time:75909ms step_avg:40.55ms
step:1873/2330 train_time:75945ms step_avg:40.55ms
step:1874/2330 train_time:75989ms step_avg:40.55ms
step:1875/2330 train_time:76025ms step_avg:40.55ms
step:1876/2330 train_time:76070ms step_avg:40.55ms
step:1877/2330 train_time:76106ms step_avg:40.55ms
step:1878/2330 train_time:76151ms step_avg:40.55ms
step:1879/2330 train_time:76186ms step_avg:40.55ms
step:1880/2330 train_time:76231ms step_avg:40.55ms
step:1881/2330 train_time:76266ms step_avg:40.55ms
step:1882/2330 train_time:76311ms step_avg:40.55ms
step:1883/2330 train_time:76345ms step_avg:40.54ms
step:1884/2330 train_time:76390ms step_avg:40.55ms
step:1885/2330 train_time:76426ms step_avg:40.54ms
step:1886/2330 train_time:76470ms step_avg:40.55ms
step:1887/2330 train_time:76505ms step_avg:40.54ms
step:1888/2330 train_time:76550ms step_avg:40.55ms
step:1889/2330 train_time:76584ms step_avg:40.54ms
step:1890/2330 train_time:76629ms step_avg:40.54ms
step:1891/2330 train_time:76665ms step_avg:40.54ms
step:1892/2330 train_time:76709ms step_avg:40.54ms
step:1893/2330 train_time:76744ms step_avg:40.54ms
step:1894/2330 train_time:76788ms step_avg:40.54ms
step:1895/2330 train_time:76824ms step_avg:40.54ms
step:1896/2330 train_time:76869ms step_avg:40.54ms
step:1897/2330 train_time:76904ms step_avg:40.54ms
step:1898/2330 train_time:76949ms step_avg:40.54ms
step:1899/2330 train_time:76985ms step_avg:40.54ms
step:1900/2330 train_time:77030ms step_avg:40.54ms
step:1901/2330 train_time:77066ms step_avg:40.54ms
step:1902/2330 train_time:77110ms step_avg:40.54ms
step:1903/2330 train_time:77145ms step_avg:40.54ms
step:1904/2330 train_time:77190ms step_avg:40.54ms
step:1905/2330 train_time:77225ms step_avg:40.54ms
step:1906/2330 train_time:77271ms step_avg:40.54ms
step:1907/2330 train_time:77306ms step_avg:40.54ms
step:1908/2330 train_time:77350ms step_avg:40.54ms
step:1909/2330 train_time:77385ms step_avg:40.54ms
step:1910/2330 train_time:77430ms step_avg:40.54ms
step:1911/2330 train_time:77465ms step_avg:40.54ms
step:1912/2330 train_time:77510ms step_avg:40.54ms
step:1913/2330 train_time:77545ms step_avg:40.54ms
step:1914/2330 train_time:77589ms step_avg:40.54ms
step:1915/2330 train_time:77625ms step_avg:40.54ms
step:1916/2330 train_time:77669ms step_avg:40.54ms
step:1917/2330 train_time:77704ms step_avg:40.53ms
step:1918/2330 train_time:77749ms step_avg:40.54ms
step:1919/2330 train_time:77785ms step_avg:40.53ms
step:1920/2330 train_time:77830ms step_avg:40.54ms
step:1921/2330 train_time:77864ms step_avg:40.53ms
step:1922/2330 train_time:77909ms step_avg:40.54ms
step:1923/2330 train_time:77945ms step_avg:40.53ms
step:1924/2330 train_time:77990ms step_avg:40.54ms
step:1925/2330 train_time:78024ms step_avg:40.53ms
step:1926/2330 train_time:78069ms step_avg:40.53ms
step:1927/2330 train_time:78104ms step_avg:40.53ms
step:1928/2330 train_time:78149ms step_avg:40.53ms
step:1929/2330 train_time:78185ms step_avg:40.53ms
step:1930/2330 train_time:78230ms step_avg:40.53ms
step:1931/2330 train_time:78265ms step_avg:40.53ms
step:1932/2330 train_time:78310ms step_avg:40.53ms
step:1933/2330 train_time:78345ms step_avg:40.53ms
step:1934/2330 train_time:78390ms step_avg:40.53ms
step:1935/2330 train_time:78425ms step_avg:40.53ms
step:1936/2330 train_time:78470ms step_avg:40.53ms
step:1937/2330 train_time:78505ms step_avg:40.53ms
step:1938/2330 train_time:78550ms step_avg:40.53ms
step:1939/2330 train_time:78585ms step_avg:40.53ms
step:1940/2330 train_time:78629ms step_avg:40.53ms
step:1941/2330 train_time:78664ms step_avg:40.53ms
step:1942/2330 train_time:78709ms step_avg:40.53ms
step:1943/2330 train_time:78745ms step_avg:40.53ms
step:1944/2330 train_time:78790ms step_avg:40.53ms
step:1945/2330 train_time:78825ms step_avg:40.53ms
step:1946/2330 train_time:78870ms step_avg:40.53ms
step:1947/2330 train_time:78904ms step_avg:40.53ms
step:1948/2330 train_time:78949ms step_avg:40.53ms
step:1949/2330 train_time:78985ms step_avg:40.53ms
step:1950/2330 train_time:79030ms step_avg:40.53ms
step:1951/2330 train_time:79065ms step_avg:40.53ms
step:1952/2330 train_time:79109ms step_avg:40.53ms
step:1953/2330 train_time:79144ms step_avg:40.52ms
step:1954/2330 train_time:79189ms step_avg:40.53ms
step:1955/2330 train_time:79224ms step_avg:40.52ms
step:1956/2330 train_time:79269ms step_avg:40.53ms
step:1957/2330 train_time:79305ms step_avg:40.52ms
step:1958/2330 train_time:79350ms step_avg:40.53ms
step:1959/2330 train_time:79385ms step_avg:40.52ms
step:1960/2330 train_time:79429ms step_avg:40.53ms
step:1961/2330 train_time:79465ms step_avg:40.52ms
step:1962/2330 train_time:79509ms step_avg:40.52ms
step:1963/2330 train_time:79544ms step_avg:40.52ms
step:1964/2330 train_time:79589ms step_avg:40.52ms
step:1965/2330 train_time:79624ms step_avg:40.52ms
step:1966/2330 train_time:79669ms step_avg:40.52ms
step:1967/2330 train_time:79704ms step_avg:40.52ms
step:1968/2330 train_time:79749ms step_avg:40.52ms
step:1969/2330 train_time:79784ms step_avg:40.52ms
step:1970/2330 train_time:79829ms step_avg:40.52ms
step:1971/2330 train_time:79864ms step_avg:40.52ms
step:1972/2330 train_time:79909ms step_avg:40.52ms
step:1973/2330 train_time:79944ms step_avg:40.52ms
step:1974/2330 train_time:79989ms step_avg:40.52ms
step:1975/2330 train_time:80025ms step_avg:40.52ms
step:1976/2330 train_time:80069ms step_avg:40.52ms
step:1977/2330 train_time:80105ms step_avg:40.52ms
step:1978/2330 train_time:80150ms step_avg:40.52ms
step:1979/2330 train_time:80185ms step_avg:40.52ms
step:1980/2330 train_time:80230ms step_avg:40.52ms
step:1981/2330 train_time:80265ms step_avg:40.52ms
step:1982/2330 train_time:80309ms step_avg:40.52ms
step:1983/2330 train_time:80344ms step_avg:40.52ms
step:1984/2330 train_time:80389ms step_avg:40.52ms
step:1985/2330 train_time:80425ms step_avg:40.52ms
step:1986/2330 train_time:80470ms step_avg:40.52ms
step:1987/2330 train_time:80504ms step_avg:40.52ms
step:1988/2330 train_time:80549ms step_avg:40.52ms
step:1989/2330 train_time:80584ms step_avg:40.51ms
step:1990/2330 train_time:80629ms step_avg:40.52ms
step:1991/2330 train_time:80664ms step_avg:40.51ms
step:1992/2330 train_time:80708ms step_avg:40.52ms
step:1993/2330 train_time:80744ms step_avg:40.51ms
step:1994/2330 train_time:80788ms step_avg:40.52ms
step:1995/2330 train_time:80824ms step_avg:40.51ms
step:1996/2330 train_time:80868ms step_avg:40.52ms
step:1997/2330 train_time:80904ms step_avg:40.51ms
step:1998/2330 train_time:80949ms step_avg:40.51ms
step:1999/2330 train_time:80984ms step_avg:40.51ms
step:2000/2330 train_time:81029ms step_avg:40.51ms
step:2000/2330 val_loss:5.0644 train_time:81115ms step_avg:40.56ms
step:2001/2330 train_time:81127ms step_avg:40.54ms
step:2002/2330 train_time:81140ms step_avg:40.53ms
step:2003/2330 train_time:81150ms step_avg:40.51ms
step:2004/2330 train_time:81188ms step_avg:40.51ms
step:2005/2330 train_time:81222ms step_avg:40.51ms
step:2006/2330 train_time:81266ms step_avg:40.51ms
step:2007/2330 train_time:81300ms step_avg:40.51ms
step:2008/2330 train_time:81344ms step_avg:40.51ms
step:2009/2330 train_time:81379ms step_avg:40.51ms
step:2010/2330 train_time:81427ms step_avg:40.51ms
step:2011/2330 train_time:81466ms step_avg:40.51ms
step:2012/2330 train_time:81513ms step_avg:40.51ms
step:2013/2330 train_time:81549ms step_avg:40.51ms
step:2014/2330 train_time:81595ms step_avg:40.51ms
step:2015/2330 train_time:81630ms step_avg:40.51ms
step:2016/2330 train_time:81673ms step_avg:40.51ms
step:2017/2330 train_time:81708ms step_avg:40.51ms
step:2018/2330 train_time:81752ms step_avg:40.51ms
step:2019/2330 train_time:81787ms step_avg:40.51ms
step:2020/2330 train_time:81831ms step_avg:40.51ms
step:2021/2330 train_time:81867ms step_avg:40.51ms
step:2022/2330 train_time:81911ms step_avg:40.51ms
step:2023/2330 train_time:81945ms step_avg:40.51ms
step:2024/2330 train_time:81990ms step_avg:40.51ms
step:2025/2330 train_time:82025ms step_avg:40.51ms
step:2026/2330 train_time:82071ms step_avg:40.51ms
step:2027/2330 train_time:82106ms step_avg:40.51ms
step:2028/2330 train_time:82150ms step_avg:40.51ms
step:2029/2330 train_time:82185ms step_avg:40.51ms
step:2030/2330 train_time:82229ms step_avg:40.51ms
step:2031/2330 train_time:82264ms step_avg:40.50ms
step:2032/2330 train_time:82308ms step_avg:40.51ms
step:2033/2330 train_time:82343ms step_avg:40.50ms
step:2034/2330 train_time:82389ms step_avg:40.51ms
step:2035/2330 train_time:82425ms step_avg:40.50ms
step:2036/2330 train_time:82471ms step_avg:40.51ms
step:2037/2330 train_time:82507ms step_avg:40.50ms
step:2038/2330 train_time:82552ms step_avg:40.51ms
step:2039/2330 train_time:82588ms step_avg:40.50ms
step:2040/2330 train_time:82633ms step_avg:40.51ms
step:2041/2330 train_time:82668ms step_avg:40.50ms
step:2042/2330 train_time:82712ms step_avg:40.51ms
step:2043/2330 train_time:82748ms step_avg:40.50ms
step:2044/2330 train_time:82792ms step_avg:40.50ms
step:2045/2330 train_time:82827ms step_avg:40.50ms
step:2046/2330 train_time:82871ms step_avg:40.50ms
step:2047/2330 train_time:82906ms step_avg:40.50ms
step:2048/2330 train_time:82950ms step_avg:40.50ms
step:2049/2330 train_time:82985ms step_avg:40.50ms
step:2050/2330 train_time:83030ms step_avg:40.50ms
step:2051/2330 train_time:83064ms step_avg:40.50ms
step:2052/2330 train_time:83109ms step_avg:40.50ms
step:2053/2330 train_time:83143ms step_avg:40.50ms
step:2054/2330 train_time:83188ms step_avg:40.50ms
step:2055/2330 train_time:83223ms step_avg:40.50ms
step:2056/2330 train_time:83267ms step_avg:40.50ms
step:2057/2330 train_time:83302ms step_avg:40.50ms
step:2058/2330 train_time:83348ms step_avg:40.50ms
step:2059/2330 train_time:83384ms step_avg:40.50ms
step:2060/2330 train_time:83429ms step_avg:40.50ms
step:2061/2330 train_time:83464ms step_avg:40.50ms
step:2062/2330 train_time:83510ms step_avg:40.50ms
step:2063/2330 train_time:83546ms step_avg:40.50ms
step:2064/2330 train_time:83591ms step_avg:40.50ms
step:2065/2330 train_time:83626ms step_avg:40.50ms
step:2066/2330 train_time:83671ms step_avg:40.50ms
step:2067/2330 train_time:83706ms step_avg:40.50ms
step:2068/2330 train_time:83751ms step_avg:40.50ms
step:2069/2330 train_time:83786ms step_avg:40.50ms
step:2070/2330 train_time:83830ms step_avg:40.50ms
step:2071/2330 train_time:83865ms step_avg:40.49ms
step:2072/2330 train_time:83909ms step_avg:40.50ms
step:2073/2330 train_time:83943ms step_avg:40.49ms
step:2074/2330 train_time:83988ms step_avg:40.50ms
step:2075/2330 train_time:84024ms step_avg:40.49ms
step:2076/2330 train_time:84068ms step_avg:40.50ms
step:2077/2330 train_time:84103ms step_avg:40.49ms
step:2078/2330 train_time:84147ms step_avg:40.49ms
step:2079/2330 train_time:84183ms step_avg:40.49ms
step:2080/2330 train_time:84226ms step_avg:40.49ms
step:2081/2330 train_time:84262ms step_avg:40.49ms
step:2082/2330 train_time:84307ms step_avg:40.49ms
step:2083/2330 train_time:84342ms step_avg:40.49ms
step:2084/2330 train_time:84387ms step_avg:40.49ms
step:2085/2330 train_time:84423ms step_avg:40.49ms
step:2086/2330 train_time:84468ms step_avg:40.49ms
step:2087/2330 train_time:84503ms step_avg:40.49ms
step:2088/2330 train_time:84549ms step_avg:40.49ms
step:2089/2330 train_time:84585ms step_avg:40.49ms
step:2090/2330 train_time:84629ms step_avg:40.49ms
step:2091/2330 train_time:84665ms step_avg:40.49ms
step:2092/2330 train_time:84710ms step_avg:40.49ms
step:2093/2330 train_time:84744ms step_avg:40.49ms
step:2094/2330 train_time:84789ms step_avg:40.49ms
step:2095/2330 train_time:84824ms step_avg:40.49ms
step:2096/2330 train_time:84869ms step_avg:40.49ms
step:2097/2330 train_time:84903ms step_avg:40.49ms
step:2098/2330 train_time:84948ms step_avg:40.49ms
step:2099/2330 train_time:84983ms step_avg:40.49ms
step:2100/2330 train_time:85028ms step_avg:40.49ms
step:2101/2330 train_time:85063ms step_avg:40.49ms
step:2102/2330 train_time:85107ms step_avg:40.49ms
step:2103/2330 train_time:85142ms step_avg:40.49ms
step:2104/2330 train_time:85187ms step_avg:40.49ms
step:2105/2330 train_time:85222ms step_avg:40.49ms
step:2106/2330 train_time:85266ms step_avg:40.49ms
step:2107/2330 train_time:85301ms step_avg:40.48ms
step:2108/2330 train_time:85346ms step_avg:40.49ms
step:2109/2330 train_time:85382ms step_avg:40.48ms
step:2110/2330 train_time:85427ms step_avg:40.49ms
step:2111/2330 train_time:85463ms step_avg:40.48ms
step:2112/2330 train_time:85508ms step_avg:40.49ms
step:2113/2330 train_time:85544ms step_avg:40.48ms
step:2114/2330 train_time:85589ms step_avg:40.49ms
step:2115/2330 train_time:85624ms step_avg:40.48ms
step:2116/2330 train_time:85669ms step_avg:40.49ms
step:2117/2330 train_time:85704ms step_avg:40.48ms
step:2118/2330 train_time:85748ms step_avg:40.49ms
step:2119/2330 train_time:85783ms step_avg:40.48ms
step:2120/2330 train_time:85828ms step_avg:40.48ms
step:2121/2330 train_time:85864ms step_avg:40.48ms
step:2122/2330 train_time:85908ms step_avg:40.48ms
step:2123/2330 train_time:85943ms step_avg:40.48ms
step:2124/2330 train_time:85987ms step_avg:40.48ms
step:2125/2330 train_time:86022ms step_avg:40.48ms
step:2126/2330 train_time:86067ms step_avg:40.48ms
step:2127/2330 train_time:86102ms step_avg:40.48ms
step:2128/2330 train_time:86146ms step_avg:40.48ms
step:2129/2330 train_time:86181ms step_avg:40.48ms
step:2130/2330 train_time:86226ms step_avg:40.48ms
step:2131/2330 train_time:86261ms step_avg:40.48ms
step:2132/2330 train_time:86307ms step_avg:40.48ms
step:2133/2330 train_time:86342ms step_avg:40.48ms
step:2134/2330 train_time:86386ms step_avg:40.48ms
step:2135/2330 train_time:86422ms step_avg:40.48ms
step:2136/2330 train_time:86468ms step_avg:40.48ms
step:2137/2330 train_time:86503ms step_avg:40.48ms
step:2138/2330 train_time:86548ms step_avg:40.48ms
step:2139/2330 train_time:86584ms step_avg:40.48ms
step:2140/2330 train_time:86629ms step_avg:40.48ms
step:2141/2330 train_time:86664ms step_avg:40.48ms
step:2142/2330 train_time:86708ms step_avg:40.48ms
step:2143/2330 train_time:86743ms step_avg:40.48ms
step:2144/2330 train_time:86788ms step_avg:40.48ms
step:2145/2330 train_time:86824ms step_avg:40.48ms
step:2146/2330 train_time:86869ms step_avg:40.48ms
step:2147/2330 train_time:86903ms step_avg:40.48ms
step:2148/2330 train_time:86948ms step_avg:40.48ms
step:2149/2330 train_time:86983ms step_avg:40.48ms
step:2150/2330 train_time:87028ms step_avg:40.48ms
step:2151/2330 train_time:87063ms step_avg:40.48ms
step:2152/2330 train_time:87108ms step_avg:40.48ms
step:2153/2330 train_time:87143ms step_avg:40.48ms
step:2154/2330 train_time:87188ms step_avg:40.48ms
step:2155/2330 train_time:87223ms step_avg:40.47ms
step:2156/2330 train_time:87268ms step_avg:40.48ms
step:2157/2330 train_time:87303ms step_avg:40.47ms
step:2158/2330 train_time:87348ms step_avg:40.48ms
step:2159/2330 train_time:87383ms step_avg:40.47ms
step:2160/2330 train_time:87428ms step_avg:40.48ms
step:2161/2330 train_time:87463ms step_avg:40.47ms
step:2162/2330 train_time:87508ms step_avg:40.48ms
step:2163/2330 train_time:87543ms step_avg:40.47ms
step:2164/2330 train_time:87589ms step_avg:40.48ms
step:2165/2330 train_time:87623ms step_avg:40.47ms
step:2166/2330 train_time:87668ms step_avg:40.47ms
step:2167/2330 train_time:87704ms step_avg:40.47ms
step:2168/2330 train_time:87748ms step_avg:40.47ms
step:2169/2330 train_time:87783ms step_avg:40.47ms
step:2170/2330 train_time:87828ms step_avg:40.47ms
step:2171/2330 train_time:87863ms step_avg:40.47ms
step:2172/2330 train_time:87908ms step_avg:40.47ms
step:2173/2330 train_time:87943ms step_avg:40.47ms
step:2174/2330 train_time:87988ms step_avg:40.47ms
step:2175/2330 train_time:88024ms step_avg:40.47ms
step:2176/2330 train_time:88069ms step_avg:40.47ms
step:2177/2330 train_time:88103ms step_avg:40.47ms
step:2178/2330 train_time:88148ms step_avg:40.47ms
step:2179/2330 train_time:88183ms step_avg:40.47ms
step:2180/2330 train_time:88228ms step_avg:40.47ms
step:2181/2330 train_time:88263ms step_avg:40.47ms
step:2182/2330 train_time:88308ms step_avg:40.47ms
step:2183/2330 train_time:88343ms step_avg:40.47ms
step:2184/2330 train_time:88389ms step_avg:40.47ms
step:2185/2330 train_time:88423ms step_avg:40.47ms
step:2186/2330 train_time:88469ms step_avg:40.47ms
step:2187/2330 train_time:88504ms step_avg:40.47ms
step:2188/2330 train_time:88549ms step_avg:40.47ms
step:2189/2330 train_time:88584ms step_avg:40.47ms
step:2190/2330 train_time:88629ms step_avg:40.47ms
step:2191/2330 train_time:88664ms step_avg:40.47ms
step:2192/2330 train_time:88708ms step_avg:40.47ms
step:2193/2330 train_time:88744ms step_avg:40.47ms
step:2194/2330 train_time:88788ms step_avg:40.47ms
step:2195/2330 train_time:88823ms step_avg:40.47ms
step:2196/2330 train_time:88868ms step_avg:40.47ms
step:2197/2330 train_time:88903ms step_avg:40.47ms
step:2198/2330 train_time:88948ms step_avg:40.47ms
step:2199/2330 train_time:88983ms step_avg:40.47ms
step:2200/2330 train_time:89028ms step_avg:40.47ms
step:2201/2330 train_time:89063ms step_avg:40.46ms
step:2202/2330 train_time:89107ms step_avg:40.47ms
step:2203/2330 train_time:89142ms step_avg:40.46ms
step:2204/2330 train_time:89187ms step_avg:40.47ms
step:2205/2330 train_time:89222ms step_avg:40.46ms
step:2206/2330 train_time:89266ms step_avg:40.47ms
step:2207/2330 train_time:89302ms step_avg:40.46ms
step:2208/2330 train_time:89346ms step_avg:40.46ms
step:2209/2330 train_time:89382ms step_avg:40.46ms
step:2210/2330 train_time:89427ms step_avg:40.46ms
step:2211/2330 train_time:89462ms step_avg:40.46ms
step:2212/2330 train_time:89506ms step_avg:40.46ms
step:2213/2330 train_time:89542ms step_avg:40.46ms
step:2214/2330 train_time:89586ms step_avg:40.46ms
step:2215/2330 train_time:89622ms step_avg:40.46ms
step:2216/2330 train_time:89667ms step_avg:40.46ms
step:2217/2330 train_time:89702ms step_avg:40.46ms
step:2218/2330 train_time:89746ms step_avg:40.46ms
step:2219/2330 train_time:89781ms step_avg:40.46ms
step:2220/2330 train_time:89827ms step_avg:40.46ms
step:2221/2330 train_time:89862ms step_avg:40.46ms
step:2222/2330 train_time:89907ms step_avg:40.46ms
step:2223/2330 train_time:89942ms step_avg:40.46ms
step:2224/2330 train_time:89986ms step_avg:40.46ms
step:2225/2330 train_time:90021ms step_avg:40.46ms
step:2226/2330 train_time:90066ms step_avg:40.46ms
step:2227/2330 train_time:90101ms step_avg:40.46ms
step:2228/2330 train_time:90145ms step_avg:40.46ms
step:2229/2330 train_time:90181ms step_avg:40.46ms
step:2230/2330 train_time:90226ms step_avg:40.46ms
step:2231/2330 train_time:90261ms step_avg:40.46ms
step:2232/2330 train_time:90305ms step_avg:40.46ms
step:2233/2330 train_time:90341ms step_avg:40.46ms
step:2234/2330 train_time:90386ms step_avg:40.46ms
step:2235/2330 train_time:90421ms step_avg:40.46ms
step:2236/2330 train_time:90466ms step_avg:40.46ms
step:2237/2330 train_time:90501ms step_avg:40.46ms
step:2238/2330 train_time:90546ms step_avg:40.46ms
step:2239/2330 train_time:90582ms step_avg:40.46ms
step:2240/2330 train_time:90628ms step_avg:40.46ms
step:2241/2330 train_time:90662ms step_avg:40.46ms
step:2242/2330 train_time:90707ms step_avg:40.46ms
step:2243/2330 train_time:90742ms step_avg:40.46ms
step:2244/2330 train_time:90787ms step_avg:40.46ms
step:2245/2330 train_time:90823ms step_avg:40.46ms
step:2246/2330 train_time:90867ms step_avg:40.46ms
step:2247/2330 train_time:90902ms step_avg:40.46ms
step:2248/2330 train_time:90947ms step_avg:40.46ms
step:2249/2330 train_time:90982ms step_avg:40.45ms
step:2250/2330 train_time:91027ms step_avg:40.46ms
step:2250/2330 val_loss:5.0399 train_time:91114ms step_avg:40.49ms
step:2251/2330 train_time:91126ms step_avg:40.48ms
step:2252/2330 train_time:91139ms step_avg:40.47ms
step:2253/2330 train_time:91150ms step_avg:40.46ms
step:2254/2330 train_time:91187ms step_avg:40.46ms
step:2255/2330 train_time:91221ms step_avg:40.45ms
step:2256/2330 train_time:91265ms step_avg:40.45ms
step:2257/2330 train_time:91300ms step_avg:40.45ms
step:2258/2330 train_time:91344ms step_avg:40.45ms
step:2259/2330 train_time:91379ms step_avg:40.45ms
step:2260/2330 train_time:91425ms step_avg:40.45ms
step:2261/2330 train_time:91465ms step_avg:40.45ms
step:2262/2330 train_time:91513ms step_avg:40.46ms
step:2263/2330 train_time:91549ms step_avg:40.45ms
step:2264/2330 train_time:91594ms step_avg:40.46ms
step:2265/2330 train_time:91630ms step_avg:40.45ms
step:2266/2330 train_time:91675ms step_avg:40.46ms
step:2267/2330 train_time:91710ms step_avg:40.45ms
step:2268/2330 train_time:91754ms step_avg:40.46ms
step:2269/2330 train_time:91789ms step_avg:40.45ms
step:2270/2330 train_time:91834ms step_avg:40.46ms
step:2271/2330 train_time:91868ms step_avg:40.45ms
step:2272/2330 train_time:91912ms step_avg:40.45ms
step:2273/2330 train_time:91948ms step_avg:40.45ms
step:2274/2330 train_time:91992ms step_avg:40.45ms
step:2275/2330 train_time:92028ms step_avg:40.45ms
step:2276/2330 train_time:92074ms step_avg:40.45ms
step:2277/2330 train_time:92109ms step_avg:40.45ms
step:2278/2330 train_time:92155ms step_avg:40.45ms
step:2279/2330 train_time:92191ms step_avg:40.45ms
step:2280/2330 train_time:92236ms step_avg:40.45ms
step:2281/2330 train_time:92271ms step_avg:40.45ms
step:2282/2330 train_time:92316ms step_avg:40.45ms
step:2283/2330 train_time:92351ms step_avg:40.45ms
step:2284/2330 train_time:92397ms step_avg:40.45ms
step:2285/2330 train_time:92432ms step_avg:40.45ms
step:2286/2330 train_time:92477ms step_avg:40.45ms
step:2287/2330 train_time:92512ms step_avg:40.45ms
step:2288/2330 train_time:92557ms step_avg:40.45ms
step:2289/2330 train_time:92593ms step_avg:40.45ms
step:2290/2330 train_time:92637ms step_avg:40.45ms
step:2291/2330 train_time:92672ms step_avg:40.45ms
step:2292/2330 train_time:92717ms step_avg:40.45ms
step:2293/2330 train_time:92752ms step_avg:40.45ms
step:2294/2330 train_time:92796ms step_avg:40.45ms
step:2295/2330 train_time:92830ms step_avg:40.45ms
step:2296/2330 train_time:92874ms step_avg:40.45ms
step:2297/2330 train_time:92910ms step_avg:40.45ms
step:2298/2330 train_time:92955ms step_avg:40.45ms
step:2299/2330 train_time:92991ms step_avg:40.45ms
step:2300/2330 train_time:93036ms step_avg:40.45ms
step:2301/2330 train_time:93071ms step_avg:40.45ms
step:2302/2330 train_time:93116ms step_avg:40.45ms
step:2303/2330 train_time:93152ms step_avg:40.45ms
step:2304/2330 train_time:93196ms step_avg:40.45ms
step:2305/2330 train_time:93231ms step_avg:40.45ms
step:2306/2330 train_time:93276ms step_avg:40.45ms
step:2307/2330 train_time:93312ms step_avg:40.45ms
step:2308/2330 train_time:93357ms step_avg:40.45ms
step:2309/2330 train_time:93392ms step_avg:40.45ms
step:2310/2330 train_time:93437ms step_avg:40.45ms
step:2311/2330 train_time:93473ms step_avg:40.45ms
step:2312/2330 train_time:93517ms step_avg:40.45ms
step:2313/2330 train_time:93552ms step_avg:40.45ms
step:2314/2330 train_time:93596ms step_avg:40.45ms
step:2315/2330 train_time:93632ms step_avg:40.45ms
step:2316/2330 train_time:93676ms step_avg:40.45ms
step:2317/2330 train_time:93711ms step_avg:40.45ms
step:2318/2330 train_time:93756ms step_avg:40.45ms
step:2319/2330 train_time:93791ms step_avg:40.44ms
step:2320/2330 train_time:93836ms step_avg:40.45ms
step:2321/2330 train_time:93871ms step_avg:40.44ms
step:2322/2330 train_time:93915ms step_avg:40.45ms
step:2323/2330 train_time:93950ms step_avg:40.44ms
step:2324/2330 train_time:93995ms step_avg:40.45ms
step:2325/2330 train_time:94030ms step_avg:40.44ms
step:2326/2330 train_time:94076ms step_avg:40.45ms
step:2327/2330 train_time:94111ms step_avg:40.44ms
step:2328/2330 train_time:94156ms step_avg:40.45ms
step:2329/2330 train_time:94192ms step_avg:40.44ms
step:2330/2330 train_time:94237ms step_avg:40.44ms
step:2330/2330 val_loss:5.0332 train_time:94325ms step_avg:40.48ms
peak memory allocated: 29226 MiB reserved: 38888 MiB
