import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = divide_by_norm(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = divide_by_norm(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "gd_7e-3"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=7e-3, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 03:12:38 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   30C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   30C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   28C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   31C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:99ms step_avg:99.18ms
step:2/2330 train_time:204ms step_avg:102.19ms
step:3/2330 train_time:226ms step_avg:75.30ms
step:4/2330 train_time:254ms step_avg:63.49ms
step:5/2330 train_time:310ms step_avg:62.04ms
step:6/2330 train_time:370ms step_avg:61.73ms
step:7/2330 train_time:428ms step_avg:61.10ms
step:8/2330 train_time:488ms step_avg:61.04ms
step:9/2330 train_time:546ms step_avg:60.70ms
step:10/2330 train_time:607ms step_avg:60.73ms
step:11/2330 train_time:665ms step_avg:60.43ms
step:12/2330 train_time:726ms step_avg:60.50ms
step:13/2330 train_time:785ms step_avg:60.35ms
step:14/2330 train_time:845ms step_avg:60.37ms
step:15/2330 train_time:903ms step_avg:60.21ms
step:16/2330 train_time:964ms step_avg:60.27ms
step:17/2330 train_time:1022ms step_avg:60.13ms
step:18/2330 train_time:1088ms step_avg:60.45ms
step:19/2330 train_time:1153ms step_avg:60.67ms
step:20/2330 train_time:1215ms step_avg:60.74ms
step:21/2330 train_time:1275ms step_avg:60.69ms
step:22/2330 train_time:1337ms step_avg:60.76ms
step:23/2330 train_time:1395ms step_avg:60.64ms
step:24/2330 train_time:1457ms step_avg:60.69ms
step:25/2330 train_time:1515ms step_avg:60.60ms
step:26/2330 train_time:1576ms step_avg:60.62ms
step:27/2330 train_time:1635ms step_avg:60.55ms
step:28/2330 train_time:1696ms step_avg:60.58ms
step:29/2330 train_time:1755ms step_avg:60.52ms
step:30/2330 train_time:1816ms step_avg:60.55ms
step:31/2330 train_time:1875ms step_avg:60.48ms
step:32/2330 train_time:1937ms step_avg:60.54ms
step:33/2330 train_time:1996ms step_avg:60.48ms
step:34/2330 train_time:2058ms step_avg:60.53ms
step:35/2330 train_time:2117ms step_avg:60.50ms
step:36/2330 train_time:2180ms step_avg:60.56ms
step:37/2330 train_time:2239ms step_avg:60.51ms
step:38/2330 train_time:2302ms step_avg:60.58ms
step:39/2330 train_time:2362ms step_avg:60.57ms
step:40/2330 train_time:2425ms step_avg:60.61ms
step:41/2330 train_time:2483ms step_avg:60.57ms
step:42/2330 train_time:2546ms step_avg:60.63ms
step:43/2330 train_time:2605ms step_avg:60.59ms
step:44/2330 train_time:2667ms step_avg:60.62ms
step:45/2330 train_time:2726ms step_avg:60.57ms
step:46/2330 train_time:2788ms step_avg:60.61ms
step:47/2330 train_time:2847ms step_avg:60.58ms
step:48/2330 train_time:2909ms step_avg:60.60ms
step:49/2330 train_time:2968ms step_avg:60.57ms
step:50/2330 train_time:3029ms step_avg:60.59ms
step:51/2330 train_time:3088ms step_avg:60.55ms
step:52/2330 train_time:3150ms step_avg:60.57ms
step:53/2330 train_time:3209ms step_avg:60.54ms
step:54/2330 train_time:3271ms step_avg:60.58ms
step:55/2330 train_time:3331ms step_avg:60.56ms
step:56/2330 train_time:3392ms step_avg:60.58ms
step:57/2330 train_time:3452ms step_avg:60.56ms
step:58/2330 train_time:3515ms step_avg:60.60ms
step:59/2330 train_time:3574ms step_avg:60.57ms
step:60/2330 train_time:3637ms step_avg:60.61ms
step:61/2330 train_time:3695ms step_avg:60.58ms
step:62/2330 train_time:3757ms step_avg:60.60ms
step:63/2330 train_time:3816ms step_avg:60.57ms
step:64/2330 train_time:3879ms step_avg:60.60ms
step:65/2330 train_time:3937ms step_avg:60.57ms
step:66/2330 train_time:3999ms step_avg:60.59ms
step:67/2330 train_time:4058ms step_avg:60.57ms
step:68/2330 train_time:4121ms step_avg:60.60ms
step:69/2330 train_time:4179ms step_avg:60.57ms
step:70/2330 train_time:4241ms step_avg:60.59ms
step:71/2330 train_time:4301ms step_avg:60.58ms
step:72/2330 train_time:4364ms step_avg:60.61ms
step:73/2330 train_time:4423ms step_avg:60.59ms
step:74/2330 train_time:4486ms step_avg:60.62ms
step:75/2330 train_time:4547ms step_avg:60.63ms
step:76/2330 train_time:4610ms step_avg:60.65ms
step:77/2330 train_time:4669ms step_avg:60.63ms
step:78/2330 train_time:4730ms step_avg:60.64ms
step:79/2330 train_time:4789ms step_avg:60.62ms
step:80/2330 train_time:4851ms step_avg:60.64ms
step:81/2330 train_time:4909ms step_avg:60.61ms
step:82/2330 train_time:4971ms step_avg:60.62ms
step:83/2330 train_time:5029ms step_avg:60.60ms
step:84/2330 train_time:5092ms step_avg:60.62ms
step:85/2330 train_time:5151ms step_avg:60.61ms
step:86/2330 train_time:5214ms step_avg:60.62ms
step:87/2330 train_time:5273ms step_avg:60.61ms
step:88/2330 train_time:5335ms step_avg:60.63ms
step:89/2330 train_time:5395ms step_avg:60.61ms
step:90/2330 train_time:5457ms step_avg:60.63ms
step:91/2330 train_time:5516ms step_avg:60.61ms
step:92/2330 train_time:5579ms step_avg:60.64ms
step:93/2330 train_time:5637ms step_avg:60.62ms
step:94/2330 train_time:5700ms step_avg:60.63ms
step:95/2330 train_time:5759ms step_avg:60.62ms
step:96/2330 train_time:5820ms step_avg:60.63ms
step:97/2330 train_time:5880ms step_avg:60.62ms
step:98/2330 train_time:5942ms step_avg:60.64ms
step:99/2330 train_time:6002ms step_avg:60.63ms
step:100/2330 train_time:6065ms step_avg:60.65ms
step:101/2330 train_time:6124ms step_avg:60.63ms
step:102/2330 train_time:6187ms step_avg:60.66ms
step:103/2330 train_time:6247ms step_avg:60.65ms
step:104/2330 train_time:6308ms step_avg:60.66ms
step:105/2330 train_time:6368ms step_avg:60.64ms
step:106/2330 train_time:6430ms step_avg:60.66ms
step:107/2330 train_time:6489ms step_avg:60.65ms
step:108/2330 train_time:6551ms step_avg:60.66ms
step:109/2330 train_time:6610ms step_avg:60.64ms
step:110/2330 train_time:6671ms step_avg:60.65ms
step:111/2330 train_time:6731ms step_avg:60.64ms
step:112/2330 train_time:6793ms step_avg:60.65ms
step:113/2330 train_time:6852ms step_avg:60.64ms
step:114/2330 train_time:6915ms step_avg:60.65ms
step:115/2330 train_time:6974ms step_avg:60.64ms
step:116/2330 train_time:7038ms step_avg:60.67ms
step:117/2330 train_time:7097ms step_avg:60.66ms
step:118/2330 train_time:7160ms step_avg:60.68ms
step:119/2330 train_time:7218ms step_avg:60.66ms
step:120/2330 train_time:7281ms step_avg:60.67ms
step:121/2330 train_time:7340ms step_avg:60.66ms
step:122/2330 train_time:7402ms step_avg:60.67ms
step:123/2330 train_time:7462ms step_avg:60.67ms
step:124/2330 train_time:7524ms step_avg:60.68ms
step:125/2330 train_time:7584ms step_avg:60.67ms
step:126/2330 train_time:7647ms step_avg:60.69ms
step:127/2330 train_time:7707ms step_avg:60.68ms
step:128/2330 train_time:7769ms step_avg:60.69ms
step:129/2330 train_time:7828ms step_avg:60.68ms
step:130/2330 train_time:7890ms step_avg:60.69ms
step:131/2330 train_time:7950ms step_avg:60.68ms
step:132/2330 train_time:8011ms step_avg:60.69ms
step:133/2330 train_time:8070ms step_avg:60.68ms
step:134/2330 train_time:8132ms step_avg:60.69ms
step:135/2330 train_time:8192ms step_avg:60.68ms
step:136/2330 train_time:8254ms step_avg:60.69ms
step:137/2330 train_time:8314ms step_avg:60.69ms
step:138/2330 train_time:8377ms step_avg:60.70ms
step:139/2330 train_time:8436ms step_avg:60.69ms
step:140/2330 train_time:8498ms step_avg:60.70ms
step:141/2330 train_time:8557ms step_avg:60.69ms
step:142/2330 train_time:8619ms step_avg:60.70ms
step:143/2330 train_time:8677ms step_avg:60.68ms
step:144/2330 train_time:8739ms step_avg:60.69ms
step:145/2330 train_time:8799ms step_avg:60.68ms
step:146/2330 train_time:8861ms step_avg:60.69ms
step:147/2330 train_time:8920ms step_avg:60.68ms
step:148/2330 train_time:8983ms step_avg:60.69ms
step:149/2330 train_time:9044ms step_avg:60.70ms
step:150/2330 train_time:9107ms step_avg:60.71ms
step:151/2330 train_time:9166ms step_avg:60.70ms
step:152/2330 train_time:9229ms step_avg:60.71ms
step:153/2330 train_time:9288ms step_avg:60.71ms
step:154/2330 train_time:9351ms step_avg:60.72ms
step:155/2330 train_time:9409ms step_avg:60.70ms
step:156/2330 train_time:9471ms step_avg:60.71ms
step:157/2330 train_time:9530ms step_avg:60.70ms
step:158/2330 train_time:9592ms step_avg:60.71ms
step:159/2330 train_time:9651ms step_avg:60.70ms
step:160/2330 train_time:9713ms step_avg:60.71ms
step:161/2330 train_time:9772ms step_avg:60.70ms
step:162/2330 train_time:9834ms step_avg:60.71ms
step:163/2330 train_time:9893ms step_avg:60.70ms
step:164/2330 train_time:9958ms step_avg:60.72ms
step:165/2330 train_time:10017ms step_avg:60.71ms
step:166/2330 train_time:10080ms step_avg:60.72ms
step:167/2330 train_time:10139ms step_avg:60.71ms
step:168/2330 train_time:10201ms step_avg:60.72ms
step:169/2330 train_time:10261ms step_avg:60.71ms
step:170/2330 train_time:10323ms step_avg:60.72ms
step:171/2330 train_time:10383ms step_avg:60.72ms
step:172/2330 train_time:10447ms step_avg:60.74ms
step:173/2330 train_time:10506ms step_avg:60.73ms
step:174/2330 train_time:10568ms step_avg:60.73ms
step:175/2330 train_time:10626ms step_avg:60.72ms
step:176/2330 train_time:10689ms step_avg:60.73ms
step:177/2330 train_time:10749ms step_avg:60.73ms
step:178/2330 train_time:10811ms step_avg:60.73ms
step:179/2330 train_time:10870ms step_avg:60.72ms
step:180/2330 train_time:10931ms step_avg:60.73ms
step:181/2330 train_time:10990ms step_avg:60.72ms
step:182/2330 train_time:11053ms step_avg:60.73ms
step:183/2330 train_time:11112ms step_avg:60.72ms
step:184/2330 train_time:11175ms step_avg:60.73ms
step:185/2330 train_time:11235ms step_avg:60.73ms
step:186/2330 train_time:11297ms step_avg:60.74ms
step:187/2330 train_time:11356ms step_avg:60.73ms
step:188/2330 train_time:11419ms step_avg:60.74ms
step:189/2330 train_time:11477ms step_avg:60.73ms
step:190/2330 train_time:11539ms step_avg:60.73ms
step:191/2330 train_time:11598ms step_avg:60.72ms
step:192/2330 train_time:11660ms step_avg:60.73ms
step:193/2330 train_time:11719ms step_avg:60.72ms
step:194/2330 train_time:11782ms step_avg:60.73ms
step:195/2330 train_time:11843ms step_avg:60.73ms
step:196/2330 train_time:11905ms step_avg:60.74ms
step:197/2330 train_time:11965ms step_avg:60.74ms
step:198/2330 train_time:12027ms step_avg:60.74ms
step:199/2330 train_time:12087ms step_avg:60.74ms
step:200/2330 train_time:12151ms step_avg:60.75ms
step:201/2330 train_time:12210ms step_avg:60.74ms
step:202/2330 train_time:12271ms step_avg:60.75ms
step:203/2330 train_time:12329ms step_avg:60.74ms
step:204/2330 train_time:12392ms step_avg:60.74ms
step:205/2330 train_time:12450ms step_avg:60.73ms
step:206/2330 train_time:12512ms step_avg:60.74ms
step:207/2330 train_time:12571ms step_avg:60.73ms
step:208/2330 train_time:12633ms step_avg:60.74ms
step:209/2330 train_time:12692ms step_avg:60.73ms
step:210/2330 train_time:12756ms step_avg:60.74ms
step:211/2330 train_time:12815ms step_avg:60.74ms
step:212/2330 train_time:12878ms step_avg:60.74ms
step:213/2330 train_time:12937ms step_avg:60.74ms
step:214/2330 train_time:13000ms step_avg:60.75ms
step:215/2330 train_time:13058ms step_avg:60.74ms
step:216/2330 train_time:13120ms step_avg:60.74ms
step:217/2330 train_time:13179ms step_avg:60.73ms
step:218/2330 train_time:13242ms step_avg:60.74ms
step:219/2330 train_time:13302ms step_avg:60.74ms
step:220/2330 train_time:13364ms step_avg:60.75ms
step:221/2330 train_time:13424ms step_avg:60.74ms
step:222/2330 train_time:13486ms step_avg:60.75ms
step:223/2330 train_time:13547ms step_avg:60.75ms
step:224/2330 train_time:13609ms step_avg:60.75ms
step:225/2330 train_time:13668ms step_avg:60.75ms
step:226/2330 train_time:13730ms step_avg:60.75ms
step:227/2330 train_time:13789ms step_avg:60.75ms
step:228/2330 train_time:13851ms step_avg:60.75ms
step:229/2330 train_time:13910ms step_avg:60.74ms
step:230/2330 train_time:13972ms step_avg:60.75ms
step:231/2330 train_time:14031ms step_avg:60.74ms
step:232/2330 train_time:14093ms step_avg:60.75ms
step:233/2330 train_time:14153ms step_avg:60.74ms
step:234/2330 train_time:14215ms step_avg:60.75ms
step:235/2330 train_time:14274ms step_avg:60.74ms
step:236/2330 train_time:14336ms step_avg:60.75ms
step:237/2330 train_time:14395ms step_avg:60.74ms
step:238/2330 train_time:14458ms step_avg:60.75ms
step:239/2330 train_time:14517ms step_avg:60.74ms
step:240/2330 train_time:14579ms step_avg:60.75ms
step:241/2330 train_time:14638ms step_avg:60.74ms
step:242/2330 train_time:14700ms step_avg:60.74ms
step:243/2330 train_time:14760ms step_avg:60.74ms
step:244/2330 train_time:14821ms step_avg:60.74ms
step:245/2330 train_time:14881ms step_avg:60.74ms
step:246/2330 train_time:14945ms step_avg:60.75ms
step:247/2330 train_time:15004ms step_avg:60.75ms
step:248/2330 train_time:15067ms step_avg:60.76ms
step:249/2330 train_time:15126ms step_avg:60.75ms
step:250/2330 train_time:15188ms step_avg:60.75ms
step:250/2330 val_loss:5.1607 train_time:15260ms step_avg:61.04ms
step:251/2330 train_time:15282ms step_avg:60.88ms
step:252/2330 train_time:15311ms step_avg:60.76ms
step:253/2330 train_time:15371ms step_avg:60.76ms
step:254/2330 train_time:15440ms step_avg:60.79ms
step:255/2330 train_time:15500ms step_avg:60.78ms
step:256/2330 train_time:15563ms step_avg:60.79ms
step:257/2330 train_time:15622ms step_avg:60.79ms
step:258/2330 train_time:15684ms step_avg:60.79ms
step:259/2330 train_time:15742ms step_avg:60.78ms
step:260/2330 train_time:15804ms step_avg:60.78ms
step:261/2330 train_time:15862ms step_avg:60.77ms
step:262/2330 train_time:15923ms step_avg:60.77ms
step:263/2330 train_time:15982ms step_avg:60.77ms
step:264/2330 train_time:16044ms step_avg:60.77ms
step:265/2330 train_time:16102ms step_avg:60.76ms
step:266/2330 train_time:16164ms step_avg:60.77ms
step:267/2330 train_time:16223ms step_avg:60.76ms
step:268/2330 train_time:16287ms step_avg:60.77ms
step:269/2330 train_time:16348ms step_avg:60.77ms
step:270/2330 train_time:16412ms step_avg:60.79ms
step:271/2330 train_time:16473ms step_avg:60.78ms
step:272/2330 train_time:16535ms step_avg:60.79ms
step:273/2330 train_time:16595ms step_avg:60.79ms
step:274/2330 train_time:16658ms step_avg:60.79ms
step:275/2330 train_time:16718ms step_avg:60.79ms
step:276/2330 train_time:16779ms step_avg:60.79ms
step:277/2330 train_time:16838ms step_avg:60.79ms
step:278/2330 train_time:16899ms step_avg:60.79ms
step:279/2330 train_time:16958ms step_avg:60.78ms
step:280/2330 train_time:17020ms step_avg:60.79ms
step:281/2330 train_time:17079ms step_avg:60.78ms
step:282/2330 train_time:17140ms step_avg:60.78ms
step:283/2330 train_time:17199ms step_avg:60.78ms
step:284/2330 train_time:17262ms step_avg:60.78ms
step:285/2330 train_time:17323ms step_avg:60.78ms
step:286/2330 train_time:17386ms step_avg:60.79ms
step:287/2330 train_time:17446ms step_avg:60.79ms
step:288/2330 train_time:17509ms step_avg:60.80ms
step:289/2330 train_time:17569ms step_avg:60.79ms
step:290/2330 train_time:17631ms step_avg:60.80ms
step:291/2330 train_time:17690ms step_avg:60.79ms
step:292/2330 train_time:17753ms step_avg:60.80ms
step:293/2330 train_time:17813ms step_avg:60.80ms
step:294/2330 train_time:17876ms step_avg:60.80ms
step:295/2330 train_time:17935ms step_avg:60.80ms
step:296/2330 train_time:17998ms step_avg:60.80ms
step:297/2330 train_time:18057ms step_avg:60.80ms
step:298/2330 train_time:18119ms step_avg:60.80ms
step:299/2330 train_time:18178ms step_avg:60.80ms
step:300/2330 train_time:18241ms step_avg:60.80ms
step:301/2330 train_time:18301ms step_avg:60.80ms
step:302/2330 train_time:18363ms step_avg:60.81ms
step:303/2330 train_time:18424ms step_avg:60.81ms
step:304/2330 train_time:18485ms step_avg:60.81ms
step:305/2330 train_time:18544ms step_avg:60.80ms
step:306/2330 train_time:18607ms step_avg:60.81ms
step:307/2330 train_time:18667ms step_avg:60.80ms
step:308/2330 train_time:18730ms step_avg:60.81ms
step:309/2330 train_time:18788ms step_avg:60.80ms
step:310/2330 train_time:18850ms step_avg:60.81ms
step:311/2330 train_time:18909ms step_avg:60.80ms
step:312/2330 train_time:18971ms step_avg:60.80ms
step:313/2330 train_time:19031ms step_avg:60.80ms
step:314/2330 train_time:19093ms step_avg:60.81ms
step:315/2330 train_time:19153ms step_avg:60.80ms
step:316/2330 train_time:19217ms step_avg:60.81ms
step:317/2330 train_time:19276ms step_avg:60.81ms
step:318/2330 train_time:19340ms step_avg:60.82ms
step:319/2330 train_time:19400ms step_avg:60.81ms
step:320/2330 train_time:19461ms step_avg:60.82ms
step:321/2330 train_time:19522ms step_avg:60.81ms
step:322/2330 train_time:19584ms step_avg:60.82ms
step:323/2330 train_time:19642ms step_avg:60.81ms
step:324/2330 train_time:19704ms step_avg:60.81ms
step:325/2330 train_time:19763ms step_avg:60.81ms
step:326/2330 train_time:19826ms step_avg:60.81ms
step:327/2330 train_time:19886ms step_avg:60.81ms
step:328/2330 train_time:19948ms step_avg:60.82ms
step:329/2330 train_time:20007ms step_avg:60.81ms
step:330/2330 train_time:20069ms step_avg:60.81ms
step:331/2330 train_time:20128ms step_avg:60.81ms
step:332/2330 train_time:20191ms step_avg:60.82ms
step:333/2330 train_time:20250ms step_avg:60.81ms
step:334/2330 train_time:20314ms step_avg:60.82ms
step:335/2330 train_time:20374ms step_avg:60.82ms
step:336/2330 train_time:20438ms step_avg:60.83ms
step:337/2330 train_time:20497ms step_avg:60.82ms
step:338/2330 train_time:20560ms step_avg:60.83ms
step:339/2330 train_time:20620ms step_avg:60.82ms
step:340/2330 train_time:20681ms step_avg:60.83ms
step:341/2330 train_time:20740ms step_avg:60.82ms
step:342/2330 train_time:20802ms step_avg:60.82ms
step:343/2330 train_time:20861ms step_avg:60.82ms
step:344/2330 train_time:20923ms step_avg:60.82ms
step:345/2330 train_time:20982ms step_avg:60.82ms
step:346/2330 train_time:21046ms step_avg:60.83ms
step:347/2330 train_time:21105ms step_avg:60.82ms
step:348/2330 train_time:21168ms step_avg:60.83ms
step:349/2330 train_time:21227ms step_avg:60.82ms
step:350/2330 train_time:21290ms step_avg:60.83ms
step:351/2330 train_time:21349ms step_avg:60.82ms
step:352/2330 train_time:21412ms step_avg:60.83ms
step:353/2330 train_time:21472ms step_avg:60.83ms
step:354/2330 train_time:21535ms step_avg:60.83ms
step:355/2330 train_time:21594ms step_avg:60.83ms
step:356/2330 train_time:21657ms step_avg:60.83ms
step:357/2330 train_time:21718ms step_avg:60.83ms
step:358/2330 train_time:21779ms step_avg:60.84ms
step:359/2330 train_time:21838ms step_avg:60.83ms
step:360/2330 train_time:21901ms step_avg:60.83ms
step:361/2330 train_time:21960ms step_avg:60.83ms
step:362/2330 train_time:22021ms step_avg:60.83ms
step:363/2330 train_time:22080ms step_avg:60.83ms
step:364/2330 train_time:22143ms step_avg:60.83ms
step:365/2330 train_time:22203ms step_avg:60.83ms
step:366/2330 train_time:22265ms step_avg:60.83ms
step:367/2330 train_time:22325ms step_avg:60.83ms
step:368/2330 train_time:22388ms step_avg:60.84ms
step:369/2330 train_time:22447ms step_avg:60.83ms
step:370/2330 train_time:22509ms step_avg:60.83ms
step:371/2330 train_time:22568ms step_avg:60.83ms
step:372/2330 train_time:22630ms step_avg:60.83ms
step:373/2330 train_time:22689ms step_avg:60.83ms
step:374/2330 train_time:22752ms step_avg:60.83ms
step:375/2330 train_time:22813ms step_avg:60.83ms
step:376/2330 train_time:22876ms step_avg:60.84ms
step:377/2330 train_time:22936ms step_avg:60.84ms
step:378/2330 train_time:22998ms step_avg:60.84ms
step:379/2330 train_time:23058ms step_avg:60.84ms
step:380/2330 train_time:23121ms step_avg:60.84ms
step:381/2330 train_time:23180ms step_avg:60.84ms
step:382/2330 train_time:23242ms step_avg:60.84ms
step:383/2330 train_time:23301ms step_avg:60.84ms
step:384/2330 train_time:23363ms step_avg:60.84ms
step:385/2330 train_time:23423ms step_avg:60.84ms
step:386/2330 train_time:23485ms step_avg:60.84ms
step:387/2330 train_time:23544ms step_avg:60.84ms
step:388/2330 train_time:23607ms step_avg:60.84ms
step:389/2330 train_time:23666ms step_avg:60.84ms
step:390/2330 train_time:23729ms step_avg:60.84ms
step:391/2330 train_time:23787ms step_avg:60.84ms
step:392/2330 train_time:23849ms step_avg:60.84ms
step:393/2330 train_time:23909ms step_avg:60.84ms
step:394/2330 train_time:23971ms step_avg:60.84ms
step:395/2330 train_time:24031ms step_avg:60.84ms
step:396/2330 train_time:24093ms step_avg:60.84ms
step:397/2330 train_time:24154ms step_avg:60.84ms
step:398/2330 train_time:24217ms step_avg:60.85ms
step:399/2330 train_time:24276ms step_avg:60.84ms
step:400/2330 train_time:24339ms step_avg:60.85ms
step:401/2330 train_time:24398ms step_avg:60.84ms
step:402/2330 train_time:24460ms step_avg:60.85ms
step:403/2330 train_time:24520ms step_avg:60.84ms
step:404/2330 train_time:24581ms step_avg:60.85ms
step:405/2330 train_time:24641ms step_avg:60.84ms
step:406/2330 train_time:24704ms step_avg:60.85ms
step:407/2330 train_time:24762ms step_avg:60.84ms
step:408/2330 train_time:24825ms step_avg:60.85ms
step:409/2330 train_time:24885ms step_avg:60.84ms
step:410/2330 train_time:24948ms step_avg:60.85ms
step:411/2330 train_time:25007ms step_avg:60.84ms
step:412/2330 train_time:25069ms step_avg:60.85ms
step:413/2330 train_time:25128ms step_avg:60.84ms
step:414/2330 train_time:25190ms step_avg:60.85ms
step:415/2330 train_time:25250ms step_avg:60.84ms
step:416/2330 train_time:25313ms step_avg:60.85ms
step:417/2330 train_time:25373ms step_avg:60.85ms
step:418/2330 train_time:25437ms step_avg:60.85ms
step:419/2330 train_time:25497ms step_avg:60.85ms
step:420/2330 train_time:25559ms step_avg:60.85ms
step:421/2330 train_time:25619ms step_avg:60.85ms
step:422/2330 train_time:25681ms step_avg:60.85ms
step:423/2330 train_time:25740ms step_avg:60.85ms
step:424/2330 train_time:25802ms step_avg:60.85ms
step:425/2330 train_time:25861ms step_avg:60.85ms
step:426/2330 train_time:25924ms step_avg:60.85ms
step:427/2330 train_time:25983ms step_avg:60.85ms
step:428/2330 train_time:26046ms step_avg:60.86ms
step:429/2330 train_time:26106ms step_avg:60.85ms
step:430/2330 train_time:26168ms step_avg:60.86ms
step:431/2330 train_time:26227ms step_avg:60.85ms
step:432/2330 train_time:26289ms step_avg:60.86ms
step:433/2330 train_time:26348ms step_avg:60.85ms
step:434/2330 train_time:26411ms step_avg:60.85ms
step:435/2330 train_time:26471ms step_avg:60.85ms
step:436/2330 train_time:26534ms step_avg:60.86ms
step:437/2330 train_time:26593ms step_avg:60.85ms
step:438/2330 train_time:26656ms step_avg:60.86ms
step:439/2330 train_time:26717ms step_avg:60.86ms
step:440/2330 train_time:26779ms step_avg:60.86ms
step:441/2330 train_time:26839ms step_avg:60.86ms
step:442/2330 train_time:26901ms step_avg:60.86ms
step:443/2330 train_time:26960ms step_avg:60.86ms
step:444/2330 train_time:27022ms step_avg:60.86ms
step:445/2330 train_time:27082ms step_avg:60.86ms
step:446/2330 train_time:27143ms step_avg:60.86ms
step:447/2330 train_time:27203ms step_avg:60.86ms
step:448/2330 train_time:27265ms step_avg:60.86ms
step:449/2330 train_time:27325ms step_avg:60.86ms
step:450/2330 train_time:27386ms step_avg:60.86ms
step:451/2330 train_time:27446ms step_avg:60.86ms
step:452/2330 train_time:27509ms step_avg:60.86ms
step:453/2330 train_time:27567ms step_avg:60.86ms
step:454/2330 train_time:27630ms step_avg:60.86ms
step:455/2330 train_time:27689ms step_avg:60.85ms
step:456/2330 train_time:27751ms step_avg:60.86ms
step:457/2330 train_time:27812ms step_avg:60.86ms
step:458/2330 train_time:27875ms step_avg:60.86ms
step:459/2330 train_time:27935ms step_avg:60.86ms
step:460/2330 train_time:27999ms step_avg:60.87ms
step:461/2330 train_time:28059ms step_avg:60.87ms
step:462/2330 train_time:28121ms step_avg:60.87ms
step:463/2330 train_time:28180ms step_avg:60.86ms
step:464/2330 train_time:28241ms step_avg:60.87ms
step:465/2330 train_time:28301ms step_avg:60.86ms
step:466/2330 train_time:28363ms step_avg:60.86ms
step:467/2330 train_time:28422ms step_avg:60.86ms
step:468/2330 train_time:28484ms step_avg:60.86ms
step:469/2330 train_time:28544ms step_avg:60.86ms
step:470/2330 train_time:28608ms step_avg:60.87ms
step:471/2330 train_time:28667ms step_avg:60.86ms
step:472/2330 train_time:28729ms step_avg:60.87ms
step:473/2330 train_time:28788ms step_avg:60.86ms
step:474/2330 train_time:28851ms step_avg:60.87ms
step:475/2330 train_time:28911ms step_avg:60.87ms
step:476/2330 train_time:28973ms step_avg:60.87ms
step:477/2330 train_time:29033ms step_avg:60.87ms
step:478/2330 train_time:29097ms step_avg:60.87ms
step:479/2330 train_time:29156ms step_avg:60.87ms
step:480/2330 train_time:29220ms step_avg:60.87ms
step:481/2330 train_time:29279ms step_avg:60.87ms
step:482/2330 train_time:29341ms step_avg:60.87ms
step:483/2330 train_time:29400ms step_avg:60.87ms
step:484/2330 train_time:29462ms step_avg:60.87ms
step:485/2330 train_time:29521ms step_avg:60.87ms
step:486/2330 train_time:29583ms step_avg:60.87ms
step:487/2330 train_time:29644ms step_avg:60.87ms
step:488/2330 train_time:29707ms step_avg:60.87ms
step:489/2330 train_time:29766ms step_avg:60.87ms
step:490/2330 train_time:29829ms step_avg:60.88ms
step:491/2330 train_time:29889ms step_avg:60.87ms
step:492/2330 train_time:29951ms step_avg:60.88ms
step:493/2330 train_time:30010ms step_avg:60.87ms
step:494/2330 train_time:30073ms step_avg:60.88ms
step:495/2330 train_time:30134ms step_avg:60.88ms
step:496/2330 train_time:30196ms step_avg:60.88ms
step:497/2330 train_time:30256ms step_avg:60.88ms
step:498/2330 train_time:30321ms step_avg:60.88ms
step:499/2330 train_time:30379ms step_avg:60.88ms
step:500/2330 train_time:30441ms step_avg:60.88ms
step:500/2330 val_loss:4.6076 train_time:30512ms step_avg:61.02ms
step:501/2330 train_time:30534ms step_avg:60.95ms
step:502/2330 train_time:30565ms step_avg:60.89ms
step:503/2330 train_time:30627ms step_avg:60.89ms
step:504/2330 train_time:30693ms step_avg:60.90ms
step:505/2330 train_time:30755ms step_avg:60.90ms
step:506/2330 train_time:30818ms step_avg:60.90ms
step:507/2330 train_time:30877ms step_avg:60.90ms
step:508/2330 train_time:30938ms step_avg:60.90ms
step:509/2330 train_time:30998ms step_avg:60.90ms
step:510/2330 train_time:31059ms step_avg:60.90ms
step:511/2330 train_time:31117ms step_avg:60.90ms
step:512/2330 train_time:31179ms step_avg:60.90ms
step:513/2330 train_time:31237ms step_avg:60.89ms
step:514/2330 train_time:31299ms step_avg:60.89ms
step:515/2330 train_time:31358ms step_avg:60.89ms
step:516/2330 train_time:31419ms step_avg:60.89ms
step:517/2330 train_time:31478ms step_avg:60.89ms
step:518/2330 train_time:31540ms step_avg:60.89ms
step:519/2330 train_time:31602ms step_avg:60.89ms
step:520/2330 train_time:31666ms step_avg:60.90ms
step:521/2330 train_time:31726ms step_avg:60.89ms
step:522/2330 train_time:31790ms step_avg:60.90ms
step:523/2330 train_time:31849ms step_avg:60.90ms
step:524/2330 train_time:31912ms step_avg:60.90ms
step:525/2330 train_time:31971ms step_avg:60.90ms
step:526/2330 train_time:32034ms step_avg:60.90ms
step:527/2330 train_time:32094ms step_avg:60.90ms
step:528/2330 train_time:32156ms step_avg:60.90ms
step:529/2330 train_time:32215ms step_avg:60.90ms
step:530/2330 train_time:32277ms step_avg:60.90ms
step:531/2330 train_time:32335ms step_avg:60.90ms
step:532/2330 train_time:32397ms step_avg:60.90ms
step:533/2330 train_time:32457ms step_avg:60.90ms
step:534/2330 train_time:32519ms step_avg:60.90ms
step:535/2330 train_time:32579ms step_avg:60.89ms
step:536/2330 train_time:32641ms step_avg:60.90ms
step:537/2330 train_time:32701ms step_avg:60.90ms
step:538/2330 train_time:32765ms step_avg:60.90ms
step:539/2330 train_time:32824ms step_avg:60.90ms
step:540/2330 train_time:32887ms step_avg:60.90ms
step:541/2330 train_time:32946ms step_avg:60.90ms
step:542/2330 train_time:33008ms step_avg:60.90ms
step:543/2330 train_time:33068ms step_avg:60.90ms
step:544/2330 train_time:33129ms step_avg:60.90ms
step:545/2330 train_time:33188ms step_avg:60.90ms
step:546/2330 train_time:33251ms step_avg:60.90ms
step:547/2330 train_time:33310ms step_avg:60.90ms
step:548/2330 train_time:33372ms step_avg:60.90ms
step:549/2330 train_time:33432ms step_avg:60.90ms
step:550/2330 train_time:33495ms step_avg:60.90ms
step:551/2330 train_time:33556ms step_avg:60.90ms
step:552/2330 train_time:33619ms step_avg:60.90ms
step:553/2330 train_time:33679ms step_avg:60.90ms
step:554/2330 train_time:33742ms step_avg:60.91ms
step:555/2330 train_time:33801ms step_avg:60.90ms
step:556/2330 train_time:33864ms step_avg:60.91ms
step:557/2330 train_time:33923ms step_avg:60.90ms
step:558/2330 train_time:33986ms step_avg:60.91ms
step:559/2330 train_time:34045ms step_avg:60.90ms
step:560/2330 train_time:34107ms step_avg:60.91ms
step:561/2330 train_time:34167ms step_avg:60.90ms
step:562/2330 train_time:34229ms step_avg:60.91ms
step:563/2330 train_time:34288ms step_avg:60.90ms
step:564/2330 train_time:34349ms step_avg:60.90ms
step:565/2330 train_time:34408ms step_avg:60.90ms
step:566/2330 train_time:34471ms step_avg:60.90ms
step:567/2330 train_time:34530ms step_avg:60.90ms
step:568/2330 train_time:34594ms step_avg:60.91ms
step:569/2330 train_time:34655ms step_avg:60.91ms
step:570/2330 train_time:34719ms step_avg:60.91ms
step:571/2330 train_time:34779ms step_avg:60.91ms
step:572/2330 train_time:34841ms step_avg:60.91ms
step:573/2330 train_time:34900ms step_avg:60.91ms
step:574/2330 train_time:34963ms step_avg:60.91ms
step:575/2330 train_time:35022ms step_avg:60.91ms
step:576/2330 train_time:35083ms step_avg:60.91ms
step:577/2330 train_time:35143ms step_avg:60.91ms
step:578/2330 train_time:35206ms step_avg:60.91ms
step:579/2330 train_time:35266ms step_avg:60.91ms
step:580/2330 train_time:35328ms step_avg:60.91ms
step:581/2330 train_time:35387ms step_avg:60.91ms
step:582/2330 train_time:35449ms step_avg:60.91ms
step:583/2330 train_time:35508ms step_avg:60.91ms
step:584/2330 train_time:35570ms step_avg:60.91ms
step:585/2330 train_time:35630ms step_avg:60.91ms
step:586/2330 train_time:35694ms step_avg:60.91ms
step:587/2330 train_time:35756ms step_avg:60.91ms
step:588/2330 train_time:35819ms step_avg:60.92ms
step:589/2330 train_time:35879ms step_avg:60.91ms
step:590/2330 train_time:35941ms step_avg:60.92ms
step:591/2330 train_time:36001ms step_avg:60.92ms
step:592/2330 train_time:36063ms step_avg:60.92ms
step:593/2330 train_time:36122ms step_avg:60.91ms
step:594/2330 train_time:36184ms step_avg:60.92ms
step:595/2330 train_time:36243ms step_avg:60.91ms
step:596/2330 train_time:36306ms step_avg:60.92ms
step:597/2330 train_time:36365ms step_avg:60.91ms
step:598/2330 train_time:36428ms step_avg:60.92ms
step:599/2330 train_time:36486ms step_avg:60.91ms
step:600/2330 train_time:36549ms step_avg:60.91ms
step:601/2330 train_time:36608ms step_avg:60.91ms
step:602/2330 train_time:36671ms step_avg:60.91ms
step:603/2330 train_time:36730ms step_avg:60.91ms
step:604/2330 train_time:36793ms step_avg:60.92ms
step:605/2330 train_time:36854ms step_avg:60.92ms
step:606/2330 train_time:36918ms step_avg:60.92ms
step:607/2330 train_time:36978ms step_avg:60.92ms
step:608/2330 train_time:37039ms step_avg:60.92ms
step:609/2330 train_time:37099ms step_avg:60.92ms
step:610/2330 train_time:37162ms step_avg:60.92ms
step:611/2330 train_time:37221ms step_avg:60.92ms
step:612/2330 train_time:37283ms step_avg:60.92ms
step:613/2330 train_time:37342ms step_avg:60.92ms
step:614/2330 train_time:37404ms step_avg:60.92ms
step:615/2330 train_time:37463ms step_avg:60.92ms
step:616/2330 train_time:37526ms step_avg:60.92ms
step:617/2330 train_time:37585ms step_avg:60.92ms
step:618/2330 train_time:37648ms step_avg:60.92ms
step:619/2330 train_time:37709ms step_avg:60.92ms
step:620/2330 train_time:37771ms step_avg:60.92ms
step:621/2330 train_time:37831ms step_avg:60.92ms
step:622/2330 train_time:37893ms step_avg:60.92ms
step:623/2330 train_time:37954ms step_avg:60.92ms
step:624/2330 train_time:38017ms step_avg:60.92ms
step:625/2330 train_time:38076ms step_avg:60.92ms
step:626/2330 train_time:38139ms step_avg:60.92ms
step:627/2330 train_time:38199ms step_avg:60.92ms
step:628/2330 train_time:38261ms step_avg:60.93ms
step:629/2330 train_time:38320ms step_avg:60.92ms
step:630/2330 train_time:38381ms step_avg:60.92ms
step:631/2330 train_time:38442ms step_avg:60.92ms
step:632/2330 train_time:38503ms step_avg:60.92ms
step:633/2330 train_time:38563ms step_avg:60.92ms
step:634/2330 train_time:38626ms step_avg:60.92ms
step:635/2330 train_time:38686ms step_avg:60.92ms
step:636/2330 train_time:38750ms step_avg:60.93ms
step:637/2330 train_time:38809ms step_avg:60.93ms
step:638/2330 train_time:38871ms step_avg:60.93ms
step:639/2330 train_time:38931ms step_avg:60.92ms
step:640/2330 train_time:38993ms step_avg:60.93ms
step:641/2330 train_time:39053ms step_avg:60.93ms
step:642/2330 train_time:39115ms step_avg:60.93ms
step:643/2330 train_time:39176ms step_avg:60.93ms
step:644/2330 train_time:39239ms step_avg:60.93ms
step:645/2330 train_time:39299ms step_avg:60.93ms
step:646/2330 train_time:39362ms step_avg:60.93ms
step:647/2330 train_time:39420ms step_avg:60.93ms
step:648/2330 train_time:39482ms step_avg:60.93ms
step:649/2330 train_time:39543ms step_avg:60.93ms
step:650/2330 train_time:39604ms step_avg:60.93ms
step:651/2330 train_time:39664ms step_avg:60.93ms
step:652/2330 train_time:39727ms step_avg:60.93ms
step:653/2330 train_time:39786ms step_avg:60.93ms
step:654/2330 train_time:39849ms step_avg:60.93ms
step:655/2330 train_time:39908ms step_avg:60.93ms
step:656/2330 train_time:39971ms step_avg:60.93ms
step:657/2330 train_time:40031ms step_avg:60.93ms
step:658/2330 train_time:40094ms step_avg:60.93ms
step:659/2330 train_time:40155ms step_avg:60.93ms
step:660/2330 train_time:40218ms step_avg:60.94ms
step:661/2330 train_time:40278ms step_avg:60.93ms
step:662/2330 train_time:40341ms step_avg:60.94ms
step:663/2330 train_time:40400ms step_avg:60.93ms
step:664/2330 train_time:40462ms step_avg:60.94ms
step:665/2330 train_time:40520ms step_avg:60.93ms
step:666/2330 train_time:40582ms step_avg:60.93ms
step:667/2330 train_time:40642ms step_avg:60.93ms
step:668/2330 train_time:40704ms step_avg:60.93ms
step:669/2330 train_time:40764ms step_avg:60.93ms
step:670/2330 train_time:40826ms step_avg:60.93ms
step:671/2330 train_time:40886ms step_avg:60.93ms
step:672/2330 train_time:40950ms step_avg:60.94ms
step:673/2330 train_time:41009ms step_avg:60.93ms
step:674/2330 train_time:41071ms step_avg:60.94ms
step:675/2330 train_time:41131ms step_avg:60.93ms
step:676/2330 train_time:41194ms step_avg:60.94ms
step:677/2330 train_time:41254ms step_avg:60.94ms
step:678/2330 train_time:41319ms step_avg:60.94ms
step:679/2330 train_time:41378ms step_avg:60.94ms
step:680/2330 train_time:41441ms step_avg:60.94ms
step:681/2330 train_time:41500ms step_avg:60.94ms
step:682/2330 train_time:41562ms step_avg:60.94ms
step:683/2330 train_time:41622ms step_avg:60.94ms
step:684/2330 train_time:41684ms step_avg:60.94ms
step:685/2330 train_time:41743ms step_avg:60.94ms
step:686/2330 train_time:41805ms step_avg:60.94ms
step:687/2330 train_time:41866ms step_avg:60.94ms
step:688/2330 train_time:41928ms step_avg:60.94ms
step:689/2330 train_time:41987ms step_avg:60.94ms
step:690/2330 train_time:42050ms step_avg:60.94ms
step:691/2330 train_time:42109ms step_avg:60.94ms
step:692/2330 train_time:42171ms step_avg:60.94ms
step:693/2330 train_time:42231ms step_avg:60.94ms
step:694/2330 train_time:42294ms step_avg:60.94ms
step:695/2330 train_time:42355ms step_avg:60.94ms
step:696/2330 train_time:42417ms step_avg:60.94ms
step:697/2330 train_time:42477ms step_avg:60.94ms
step:698/2330 train_time:42540ms step_avg:60.95ms
step:699/2330 train_time:42600ms step_avg:60.94ms
step:700/2330 train_time:42664ms step_avg:60.95ms
step:701/2330 train_time:42723ms step_avg:60.95ms
step:702/2330 train_time:42785ms step_avg:60.95ms
step:703/2330 train_time:42844ms step_avg:60.94ms
step:704/2330 train_time:42907ms step_avg:60.95ms
step:705/2330 train_time:42966ms step_avg:60.95ms
step:706/2330 train_time:43029ms step_avg:60.95ms
step:707/2330 train_time:43088ms step_avg:60.94ms
step:708/2330 train_time:43151ms step_avg:60.95ms
step:709/2330 train_time:43210ms step_avg:60.94ms
step:710/2330 train_time:43272ms step_avg:60.95ms
step:711/2330 train_time:43332ms step_avg:60.95ms
step:712/2330 train_time:43395ms step_avg:60.95ms
step:713/2330 train_time:43456ms step_avg:60.95ms
step:714/2330 train_time:43520ms step_avg:60.95ms
step:715/2330 train_time:43579ms step_avg:60.95ms
step:716/2330 train_time:43641ms step_avg:60.95ms
step:717/2330 train_time:43701ms step_avg:60.95ms
step:718/2330 train_time:43764ms step_avg:60.95ms
step:719/2330 train_time:43823ms step_avg:60.95ms
step:720/2330 train_time:43885ms step_avg:60.95ms
step:721/2330 train_time:43944ms step_avg:60.95ms
step:722/2330 train_time:44006ms step_avg:60.95ms
step:723/2330 train_time:44065ms step_avg:60.95ms
step:724/2330 train_time:44128ms step_avg:60.95ms
step:725/2330 train_time:44187ms step_avg:60.95ms
step:726/2330 train_time:44250ms step_avg:60.95ms
step:727/2330 train_time:44310ms step_avg:60.95ms
step:728/2330 train_time:44373ms step_avg:60.95ms
step:729/2330 train_time:44432ms step_avg:60.95ms
step:730/2330 train_time:44495ms step_avg:60.95ms
step:731/2330 train_time:44556ms step_avg:60.95ms
step:732/2330 train_time:44619ms step_avg:60.95ms
step:733/2330 train_time:44679ms step_avg:60.95ms
step:734/2330 train_time:44741ms step_avg:60.96ms
step:735/2330 train_time:44801ms step_avg:60.95ms
step:736/2330 train_time:44863ms step_avg:60.96ms
step:737/2330 train_time:44921ms step_avg:60.95ms
step:738/2330 train_time:44984ms step_avg:60.95ms
step:739/2330 train_time:45043ms step_avg:60.95ms
step:740/2330 train_time:45106ms step_avg:60.95ms
step:741/2330 train_time:45166ms step_avg:60.95ms
step:742/2330 train_time:45230ms step_avg:60.96ms
step:743/2330 train_time:45289ms step_avg:60.95ms
step:744/2330 train_time:45351ms step_avg:60.96ms
step:745/2330 train_time:45410ms step_avg:60.95ms
step:746/2330 train_time:45472ms step_avg:60.95ms
step:747/2330 train_time:45533ms step_avg:60.95ms
step:748/2330 train_time:45596ms step_avg:60.96ms
step:749/2330 train_time:45657ms step_avg:60.96ms
step:750/2330 train_time:45720ms step_avg:60.96ms
step:750/2330 val_loss:4.2807 train_time:45792ms step_avg:61.06ms
step:751/2330 train_time:45815ms step_avg:61.01ms
step:752/2330 train_time:45844ms step_avg:60.96ms
step:753/2330 train_time:45905ms step_avg:60.96ms
step:754/2330 train_time:45972ms step_avg:60.97ms
step:755/2330 train_time:46033ms step_avg:60.97ms
step:756/2330 train_time:46097ms step_avg:60.98ms
step:757/2330 train_time:46156ms step_avg:60.97ms
step:758/2330 train_time:46218ms step_avg:60.97ms
step:759/2330 train_time:46276ms step_avg:60.97ms
step:760/2330 train_time:46338ms step_avg:60.97ms
step:761/2330 train_time:46397ms step_avg:60.97ms
step:762/2330 train_time:46459ms step_avg:60.97ms
step:763/2330 train_time:46517ms step_avg:60.97ms
step:764/2330 train_time:46579ms step_avg:60.97ms
step:765/2330 train_time:46639ms step_avg:60.97ms
step:766/2330 train_time:46700ms step_avg:60.97ms
step:767/2330 train_time:46761ms step_avg:60.97ms
step:768/2330 train_time:46826ms step_avg:60.97ms
step:769/2330 train_time:46888ms step_avg:60.97ms
step:770/2330 train_time:46953ms step_avg:60.98ms
step:771/2330 train_time:47014ms step_avg:60.98ms
step:772/2330 train_time:47078ms step_avg:60.98ms
step:773/2330 train_time:47138ms step_avg:60.98ms
step:774/2330 train_time:47201ms step_avg:60.98ms
step:775/2330 train_time:47259ms step_avg:60.98ms
step:776/2330 train_time:47322ms step_avg:60.98ms
step:777/2330 train_time:47381ms step_avg:60.98ms
step:778/2330 train_time:47444ms step_avg:60.98ms
step:779/2330 train_time:47503ms step_avg:60.98ms
step:780/2330 train_time:47566ms step_avg:60.98ms
step:781/2330 train_time:47625ms step_avg:60.98ms
step:782/2330 train_time:47689ms step_avg:60.98ms
step:783/2330 train_time:47748ms step_avg:60.98ms
step:784/2330 train_time:47811ms step_avg:60.98ms
step:785/2330 train_time:47873ms step_avg:60.98ms
step:786/2330 train_time:47936ms step_avg:60.99ms
step:787/2330 train_time:47997ms step_avg:60.99ms
step:788/2330 train_time:48059ms step_avg:60.99ms
step:789/2330 train_time:48119ms step_avg:60.99ms
step:790/2330 train_time:48182ms step_avg:60.99ms
step:791/2330 train_time:48243ms step_avg:60.99ms
step:792/2330 train_time:48306ms step_avg:60.99ms
step:793/2330 train_time:48364ms step_avg:60.99ms
step:794/2330 train_time:48427ms step_avg:60.99ms
step:795/2330 train_time:48486ms step_avg:60.99ms
step:796/2330 train_time:48549ms step_avg:60.99ms
step:797/2330 train_time:48609ms step_avg:60.99ms
step:798/2330 train_time:48672ms step_avg:60.99ms
step:799/2330 train_time:48732ms step_avg:60.99ms
step:800/2330 train_time:48796ms step_avg:61.00ms
step:801/2330 train_time:48856ms step_avg:60.99ms
step:802/2330 train_time:48919ms step_avg:61.00ms
step:803/2330 train_time:48979ms step_avg:61.00ms
step:804/2330 train_time:49043ms step_avg:61.00ms
step:805/2330 train_time:49103ms step_avg:61.00ms
step:806/2330 train_time:49166ms step_avg:61.00ms
step:807/2330 train_time:49226ms step_avg:61.00ms
step:808/2330 train_time:49289ms step_avg:61.00ms
step:809/2330 train_time:49348ms step_avg:61.00ms
step:810/2330 train_time:49412ms step_avg:61.00ms
step:811/2330 train_time:49472ms step_avg:61.00ms
step:812/2330 train_time:49536ms step_avg:61.00ms
step:813/2330 train_time:49596ms step_avg:61.00ms
step:814/2330 train_time:49658ms step_avg:61.01ms
step:815/2330 train_time:49718ms step_avg:61.00ms
step:816/2330 train_time:49781ms step_avg:61.01ms
step:817/2330 train_time:49841ms step_avg:61.00ms
step:818/2330 train_time:49905ms step_avg:61.01ms
step:819/2330 train_time:49965ms step_avg:61.01ms
step:820/2330 train_time:50028ms step_avg:61.01ms
step:821/2330 train_time:50088ms step_avg:61.01ms
step:822/2330 train_time:50151ms step_avg:61.01ms
step:823/2330 train_time:50210ms step_avg:61.01ms
step:824/2330 train_time:50274ms step_avg:61.01ms
step:825/2330 train_time:50334ms step_avg:61.01ms
step:826/2330 train_time:50397ms step_avg:61.01ms
step:827/2330 train_time:50457ms step_avg:61.01ms
step:828/2330 train_time:50520ms step_avg:61.01ms
step:829/2330 train_time:50579ms step_avg:61.01ms
step:830/2330 train_time:50642ms step_avg:61.01ms
step:831/2330 train_time:50701ms step_avg:61.01ms
step:832/2330 train_time:50765ms step_avg:61.02ms
step:833/2330 train_time:50825ms step_avg:61.01ms
step:834/2330 train_time:50888ms step_avg:61.02ms
step:835/2330 train_time:50948ms step_avg:61.02ms
step:836/2330 train_time:51012ms step_avg:61.02ms
step:837/2330 train_time:51072ms step_avg:61.02ms
step:838/2330 train_time:51135ms step_avg:61.02ms
step:839/2330 train_time:51196ms step_avg:61.02ms
step:840/2330 train_time:51259ms step_avg:61.02ms
step:841/2330 train_time:51319ms step_avg:61.02ms
step:842/2330 train_time:51382ms step_avg:61.02ms
step:843/2330 train_time:51441ms step_avg:61.02ms
step:844/2330 train_time:51505ms step_avg:61.02ms
step:845/2330 train_time:51564ms step_avg:61.02ms
step:846/2330 train_time:51628ms step_avg:61.03ms
step:847/2330 train_time:51687ms step_avg:61.02ms
step:848/2330 train_time:51751ms step_avg:61.03ms
step:849/2330 train_time:51811ms step_avg:61.03ms
step:850/2330 train_time:51875ms step_avg:61.03ms
step:851/2330 train_time:51934ms step_avg:61.03ms
step:852/2330 train_time:51997ms step_avg:61.03ms
step:853/2330 train_time:52056ms step_avg:61.03ms
step:854/2330 train_time:52120ms step_avg:61.03ms
step:855/2330 train_time:52179ms step_avg:61.03ms
step:856/2330 train_time:52242ms step_avg:61.03ms
step:857/2330 train_time:52303ms step_avg:61.03ms
step:858/2330 train_time:52366ms step_avg:61.03ms
step:859/2330 train_time:52426ms step_avg:61.03ms
step:860/2330 train_time:52489ms step_avg:61.03ms
step:861/2330 train_time:52548ms step_avg:61.03ms
step:862/2330 train_time:52611ms step_avg:61.03ms
step:863/2330 train_time:52672ms step_avg:61.03ms
step:864/2330 train_time:52735ms step_avg:61.04ms
step:865/2330 train_time:52796ms step_avg:61.04ms
step:866/2330 train_time:52858ms step_avg:61.04ms
step:867/2330 train_time:52919ms step_avg:61.04ms
step:868/2330 train_time:52981ms step_avg:61.04ms
step:869/2330 train_time:53041ms step_avg:61.04ms
step:870/2330 train_time:53105ms step_avg:61.04ms
step:871/2330 train_time:53164ms step_avg:61.04ms
step:872/2330 train_time:53227ms step_avg:61.04ms
step:873/2330 train_time:53287ms step_avg:61.04ms
step:874/2330 train_time:53351ms step_avg:61.04ms
step:875/2330 train_time:53411ms step_avg:61.04ms
step:876/2330 train_time:53474ms step_avg:61.04ms
step:877/2330 train_time:53535ms step_avg:61.04ms
step:878/2330 train_time:53598ms step_avg:61.05ms
step:879/2330 train_time:53657ms step_avg:61.04ms
step:880/2330 train_time:53720ms step_avg:61.05ms
step:881/2330 train_time:53780ms step_avg:61.04ms
step:882/2330 train_time:53842ms step_avg:61.05ms
step:883/2330 train_time:53903ms step_avg:61.05ms
step:884/2330 train_time:53965ms step_avg:61.05ms
step:885/2330 train_time:54026ms step_avg:61.05ms
step:886/2330 train_time:54088ms step_avg:61.05ms
step:887/2330 train_time:54147ms step_avg:61.05ms
step:888/2330 train_time:54211ms step_avg:61.05ms
step:889/2330 train_time:54272ms step_avg:61.05ms
step:890/2330 train_time:54335ms step_avg:61.05ms
step:891/2330 train_time:54395ms step_avg:61.05ms
step:892/2330 train_time:54458ms step_avg:61.05ms
step:893/2330 train_time:54517ms step_avg:61.05ms
step:894/2330 train_time:54579ms step_avg:61.05ms
step:895/2330 train_time:54639ms step_avg:61.05ms
step:896/2330 train_time:54702ms step_avg:61.05ms
step:897/2330 train_time:54762ms step_avg:61.05ms
step:898/2330 train_time:54826ms step_avg:61.05ms
step:899/2330 train_time:54886ms step_avg:61.05ms
step:900/2330 train_time:54949ms step_avg:61.05ms
step:901/2330 train_time:55009ms step_avg:61.05ms
step:902/2330 train_time:55071ms step_avg:61.05ms
step:903/2330 train_time:55132ms step_avg:61.05ms
step:904/2330 train_time:55194ms step_avg:61.06ms
step:905/2330 train_time:55254ms step_avg:61.05ms
step:906/2330 train_time:55317ms step_avg:61.06ms
step:907/2330 train_time:55377ms step_avg:61.05ms
step:908/2330 train_time:55439ms step_avg:61.06ms
step:909/2330 train_time:55499ms step_avg:61.06ms
step:910/2330 train_time:55561ms step_avg:61.06ms
step:911/2330 train_time:55621ms step_avg:61.05ms
step:912/2330 train_time:55685ms step_avg:61.06ms
step:913/2330 train_time:55744ms step_avg:61.06ms
step:914/2330 train_time:55808ms step_avg:61.06ms
step:915/2330 train_time:55867ms step_avg:61.06ms
step:916/2330 train_time:55930ms step_avg:61.06ms
step:917/2330 train_time:55990ms step_avg:61.06ms
step:918/2330 train_time:56053ms step_avg:61.06ms
step:919/2330 train_time:56113ms step_avg:61.06ms
step:920/2330 train_time:56176ms step_avg:61.06ms
step:921/2330 train_time:56236ms step_avg:61.06ms
step:922/2330 train_time:56299ms step_avg:61.06ms
step:923/2330 train_time:56359ms step_avg:61.06ms
step:924/2330 train_time:56421ms step_avg:61.06ms
step:925/2330 train_time:56482ms step_avg:61.06ms
step:926/2330 train_time:56544ms step_avg:61.06ms
step:927/2330 train_time:56604ms step_avg:61.06ms
step:928/2330 train_time:56667ms step_avg:61.06ms
step:929/2330 train_time:56727ms step_avg:61.06ms
step:930/2330 train_time:56789ms step_avg:61.06ms
step:931/2330 train_time:56850ms step_avg:61.06ms
step:932/2330 train_time:56913ms step_avg:61.07ms
step:933/2330 train_time:56973ms step_avg:61.06ms
step:934/2330 train_time:57036ms step_avg:61.07ms
step:935/2330 train_time:57097ms step_avg:61.07ms
step:936/2330 train_time:57159ms step_avg:61.07ms
step:937/2330 train_time:57219ms step_avg:61.07ms
step:938/2330 train_time:57282ms step_avg:61.07ms
step:939/2330 train_time:57342ms step_avg:61.07ms
step:940/2330 train_time:57405ms step_avg:61.07ms
step:941/2330 train_time:57464ms step_avg:61.07ms
step:942/2330 train_time:57527ms step_avg:61.07ms
step:943/2330 train_time:57587ms step_avg:61.07ms
step:944/2330 train_time:57650ms step_avg:61.07ms
step:945/2330 train_time:57711ms step_avg:61.07ms
step:946/2330 train_time:57774ms step_avg:61.07ms
step:947/2330 train_time:57833ms step_avg:61.07ms
step:948/2330 train_time:57896ms step_avg:61.07ms
step:949/2330 train_time:57956ms step_avg:61.07ms
step:950/2330 train_time:58019ms step_avg:61.07ms
step:951/2330 train_time:58079ms step_avg:61.07ms
step:952/2330 train_time:58141ms step_avg:61.07ms
step:953/2330 train_time:58202ms step_avg:61.07ms
step:954/2330 train_time:58265ms step_avg:61.07ms
step:955/2330 train_time:58324ms step_avg:61.07ms
step:956/2330 train_time:58387ms step_avg:61.07ms
step:957/2330 train_time:58448ms step_avg:61.07ms
step:958/2330 train_time:58511ms step_avg:61.08ms
step:959/2330 train_time:58572ms step_avg:61.08ms
step:960/2330 train_time:58635ms step_avg:61.08ms
step:961/2330 train_time:58696ms step_avg:61.08ms
step:962/2330 train_time:58757ms step_avg:61.08ms
step:963/2330 train_time:58817ms step_avg:61.08ms
step:964/2330 train_time:58880ms step_avg:61.08ms
step:965/2330 train_time:58940ms step_avg:61.08ms
step:966/2330 train_time:59004ms step_avg:61.08ms
step:967/2330 train_time:59064ms step_avg:61.08ms
step:968/2330 train_time:59127ms step_avg:61.08ms
step:969/2330 train_time:59187ms step_avg:61.08ms
step:970/2330 train_time:59250ms step_avg:61.08ms
step:971/2330 train_time:59310ms step_avg:61.08ms
step:972/2330 train_time:59373ms step_avg:61.08ms
step:973/2330 train_time:59433ms step_avg:61.08ms
step:974/2330 train_time:59495ms step_avg:61.08ms
step:975/2330 train_time:59555ms step_avg:61.08ms
step:976/2330 train_time:59617ms step_avg:61.08ms
step:977/2330 train_time:59677ms step_avg:61.08ms
step:978/2330 train_time:59740ms step_avg:61.08ms
step:979/2330 train_time:59800ms step_avg:61.08ms
step:980/2330 train_time:59862ms step_avg:61.08ms
step:981/2330 train_time:59922ms step_avg:61.08ms
step:982/2330 train_time:59986ms step_avg:61.09ms
step:983/2330 train_time:60046ms step_avg:61.08ms
step:984/2330 train_time:60110ms step_avg:61.09ms
step:985/2330 train_time:60169ms step_avg:61.08ms
step:986/2330 train_time:60232ms step_avg:61.09ms
step:987/2330 train_time:60293ms step_avg:61.09ms
step:988/2330 train_time:60355ms step_avg:61.09ms
step:989/2330 train_time:60415ms step_avg:61.09ms
step:990/2330 train_time:60479ms step_avg:61.09ms
step:991/2330 train_time:60539ms step_avg:61.09ms
step:992/2330 train_time:60601ms step_avg:61.09ms
step:993/2330 train_time:60661ms step_avg:61.09ms
step:994/2330 train_time:60724ms step_avg:61.09ms
step:995/2330 train_time:60784ms step_avg:61.09ms
step:996/2330 train_time:60847ms step_avg:61.09ms
step:997/2330 train_time:60907ms step_avg:61.09ms
step:998/2330 train_time:60971ms step_avg:61.09ms
step:999/2330 train_time:61031ms step_avg:61.09ms
step:1000/2330 train_time:61094ms step_avg:61.09ms
step:1000/2330 val_loss:4.1053 train_time:61166ms step_avg:61.17ms
step:1001/2330 train_time:61188ms step_avg:61.13ms
step:1002/2330 train_time:61218ms step_avg:61.10ms
step:1003/2330 train_time:61283ms step_avg:61.10ms
step:1004/2330 train_time:61350ms step_avg:61.11ms
step:1005/2330 train_time:61409ms step_avg:61.10ms
step:1006/2330 train_time:61473ms step_avg:61.11ms
step:1007/2330 train_time:61532ms step_avg:61.10ms
step:1008/2330 train_time:61594ms step_avg:61.10ms
step:1009/2330 train_time:61653ms step_avg:61.10ms
step:1010/2330 train_time:61715ms step_avg:61.10ms
step:1011/2330 train_time:61774ms step_avg:61.10ms
step:1012/2330 train_time:61836ms step_avg:61.10ms
step:1013/2330 train_time:61895ms step_avg:61.10ms
step:1014/2330 train_time:61957ms step_avg:61.10ms
step:1015/2330 train_time:62016ms step_avg:61.10ms
step:1016/2330 train_time:62078ms step_avg:61.10ms
step:1017/2330 train_time:62139ms step_avg:61.10ms
step:1018/2330 train_time:62204ms step_avg:61.10ms
step:1019/2330 train_time:62266ms step_avg:61.10ms
step:1020/2330 train_time:62331ms step_avg:61.11ms
step:1021/2330 train_time:62391ms step_avg:61.11ms
step:1022/2330 train_time:62455ms step_avg:61.11ms
step:1023/2330 train_time:62515ms step_avg:61.11ms
step:1024/2330 train_time:62577ms step_avg:61.11ms
step:1025/2330 train_time:62638ms step_avg:61.11ms
step:1026/2330 train_time:62701ms step_avg:61.11ms
step:1027/2330 train_time:62761ms step_avg:61.11ms
step:1028/2330 train_time:62823ms step_avg:61.11ms
step:1029/2330 train_time:62882ms step_avg:61.11ms
step:1030/2330 train_time:62945ms step_avg:61.11ms
step:1031/2330 train_time:63004ms step_avg:61.11ms
step:1032/2330 train_time:63066ms step_avg:61.11ms
step:1033/2330 train_time:63126ms step_avg:61.11ms
step:1034/2330 train_time:63189ms step_avg:61.11ms
step:1035/2330 train_time:63251ms step_avg:61.11ms
step:1036/2330 train_time:63315ms step_avg:61.11ms
step:1037/2330 train_time:63375ms step_avg:61.11ms
step:1038/2330 train_time:63439ms step_avg:61.12ms
step:1039/2330 train_time:63501ms step_avg:61.12ms
step:1040/2330 train_time:63565ms step_avg:61.12ms
step:1041/2330 train_time:63625ms step_avg:61.12ms
step:1042/2330 train_time:63687ms step_avg:61.12ms
step:1043/2330 train_time:63746ms step_avg:61.12ms
step:1044/2330 train_time:63808ms step_avg:61.12ms
step:1045/2330 train_time:63868ms step_avg:61.12ms
step:1046/2330 train_time:63931ms step_avg:61.12ms
step:1047/2330 train_time:63992ms step_avg:61.12ms
step:1048/2330 train_time:64055ms step_avg:61.12ms
step:1049/2330 train_time:64114ms step_avg:61.12ms
step:1050/2330 train_time:64178ms step_avg:61.12ms
step:1051/2330 train_time:64239ms step_avg:61.12ms
step:1052/2330 train_time:64303ms step_avg:61.12ms
step:1053/2330 train_time:64364ms step_avg:61.12ms
step:1054/2330 train_time:64427ms step_avg:61.13ms
step:1055/2330 train_time:64487ms step_avg:61.12ms
step:1056/2330 train_time:64551ms step_avg:61.13ms
step:1057/2330 train_time:64611ms step_avg:61.13ms
step:1058/2330 train_time:64674ms step_avg:61.13ms
step:1059/2330 train_time:64734ms step_avg:61.13ms
step:1060/2330 train_time:64796ms step_avg:61.13ms
step:1061/2330 train_time:64856ms step_avg:61.13ms
step:1062/2330 train_time:64920ms step_avg:61.13ms
step:1063/2330 train_time:64980ms step_avg:61.13ms
step:1064/2330 train_time:65043ms step_avg:61.13ms
step:1065/2330 train_time:65103ms step_avg:61.13ms
step:1066/2330 train_time:65165ms step_avg:61.13ms
step:1067/2330 train_time:65225ms step_avg:61.13ms
step:1068/2330 train_time:65288ms step_avg:61.13ms
step:1069/2330 train_time:65348ms step_avg:61.13ms
step:1070/2330 train_time:65412ms step_avg:61.13ms
step:1071/2330 train_time:65472ms step_avg:61.13ms
step:1072/2330 train_time:65535ms step_avg:61.13ms
step:1073/2330 train_time:65596ms step_avg:61.13ms
step:1074/2330 train_time:65659ms step_avg:61.14ms
step:1075/2330 train_time:65720ms step_avg:61.13ms
step:1076/2330 train_time:65783ms step_avg:61.14ms
step:1077/2330 train_time:65843ms step_avg:61.14ms
step:1078/2330 train_time:65906ms step_avg:61.14ms
step:1079/2330 train_time:65965ms step_avg:61.14ms
step:1080/2330 train_time:66027ms step_avg:61.14ms
step:1081/2330 train_time:66086ms step_avg:61.13ms
step:1082/2330 train_time:66149ms step_avg:61.14ms
step:1083/2330 train_time:66209ms step_avg:61.13ms
step:1084/2330 train_time:66272ms step_avg:61.14ms
step:1085/2330 train_time:66333ms step_avg:61.14ms
step:1086/2330 train_time:66397ms step_avg:61.14ms
step:1087/2330 train_time:66457ms step_avg:61.14ms
step:1088/2330 train_time:66520ms step_avg:61.14ms
step:1089/2330 train_time:66580ms step_avg:61.14ms
step:1090/2330 train_time:66643ms step_avg:61.14ms
step:1091/2330 train_time:66703ms step_avg:61.14ms
step:1092/2330 train_time:66766ms step_avg:61.14ms
step:1093/2330 train_time:66826ms step_avg:61.14ms
step:1094/2330 train_time:66889ms step_avg:61.14ms
step:1095/2330 train_time:66949ms step_avg:61.14ms
step:1096/2330 train_time:67013ms step_avg:61.14ms
step:1097/2330 train_time:67071ms step_avg:61.14ms
step:1098/2330 train_time:67134ms step_avg:61.14ms
step:1099/2330 train_time:67194ms step_avg:61.14ms
step:1100/2330 train_time:67257ms step_avg:61.14ms
step:1101/2330 train_time:67319ms step_avg:61.14ms
step:1102/2330 train_time:67382ms step_avg:61.14ms
step:1103/2330 train_time:67442ms step_avg:61.14ms
step:1104/2330 train_time:67505ms step_avg:61.15ms
step:1105/2330 train_time:67565ms step_avg:61.14ms
step:1106/2330 train_time:67628ms step_avg:61.15ms
step:1107/2330 train_time:67687ms step_avg:61.14ms
step:1108/2330 train_time:67750ms step_avg:61.15ms
step:1109/2330 train_time:67810ms step_avg:61.14ms
step:1110/2330 train_time:67873ms step_avg:61.15ms
step:1111/2330 train_time:67934ms step_avg:61.15ms
step:1112/2330 train_time:67996ms step_avg:61.15ms
step:1113/2330 train_time:68056ms step_avg:61.15ms
step:1114/2330 train_time:68119ms step_avg:61.15ms
step:1115/2330 train_time:68180ms step_avg:61.15ms
step:1116/2330 train_time:68243ms step_avg:61.15ms
step:1117/2330 train_time:68303ms step_avg:61.15ms
step:1118/2330 train_time:68366ms step_avg:61.15ms
step:1119/2330 train_time:68426ms step_avg:61.15ms
step:1120/2330 train_time:68489ms step_avg:61.15ms
step:1121/2330 train_time:68549ms step_avg:61.15ms
step:1122/2330 train_time:68613ms step_avg:61.15ms
step:1123/2330 train_time:68673ms step_avg:61.15ms
step:1124/2330 train_time:68737ms step_avg:61.15ms
step:1125/2330 train_time:68796ms step_avg:61.15ms
step:1126/2330 train_time:68859ms step_avg:61.15ms
step:1127/2330 train_time:68921ms step_avg:61.15ms
step:1128/2330 train_time:68984ms step_avg:61.16ms
step:1129/2330 train_time:69043ms step_avg:61.15ms
step:1130/2330 train_time:69106ms step_avg:61.16ms
step:1131/2330 train_time:69165ms step_avg:61.15ms
step:1132/2330 train_time:69229ms step_avg:61.16ms
step:1133/2330 train_time:69288ms step_avg:61.15ms
step:1134/2330 train_time:69351ms step_avg:61.16ms
step:1135/2330 train_time:69410ms step_avg:61.15ms
step:1136/2330 train_time:69474ms step_avg:61.16ms
step:1137/2330 train_time:69534ms step_avg:61.16ms
step:1138/2330 train_time:69597ms step_avg:61.16ms
step:1139/2330 train_time:69657ms step_avg:61.16ms
step:1140/2330 train_time:69720ms step_avg:61.16ms
step:1141/2330 train_time:69780ms step_avg:61.16ms
step:1142/2330 train_time:69843ms step_avg:61.16ms
step:1143/2330 train_time:69904ms step_avg:61.16ms
step:1144/2330 train_time:69966ms step_avg:61.16ms
step:1145/2330 train_time:70026ms step_avg:61.16ms
step:1146/2330 train_time:70088ms step_avg:61.16ms
step:1147/2330 train_time:70148ms step_avg:61.16ms
step:1148/2330 train_time:70211ms step_avg:61.16ms
step:1149/2330 train_time:70272ms step_avg:61.16ms
step:1150/2330 train_time:70335ms step_avg:61.16ms
step:1151/2330 train_time:70394ms step_avg:61.16ms
step:1152/2330 train_time:70457ms step_avg:61.16ms
step:1153/2330 train_time:70518ms step_avg:61.16ms
step:1154/2330 train_time:70582ms step_avg:61.16ms
step:1155/2330 train_time:70642ms step_avg:61.16ms
step:1156/2330 train_time:70705ms step_avg:61.16ms
step:1157/2330 train_time:70764ms step_avg:61.16ms
step:1158/2330 train_time:70827ms step_avg:61.16ms
step:1159/2330 train_time:70886ms step_avg:61.16ms
step:1160/2330 train_time:70949ms step_avg:61.16ms
step:1161/2330 train_time:71009ms step_avg:61.16ms
step:1162/2330 train_time:71072ms step_avg:61.16ms
step:1163/2330 train_time:71132ms step_avg:61.16ms
step:1164/2330 train_time:71196ms step_avg:61.17ms
step:1165/2330 train_time:71256ms step_avg:61.16ms
step:1166/2330 train_time:71318ms step_avg:61.16ms
step:1167/2330 train_time:71378ms step_avg:61.16ms
step:1168/2330 train_time:71441ms step_avg:61.17ms
step:1169/2330 train_time:71501ms step_avg:61.16ms
step:1170/2330 train_time:71565ms step_avg:61.17ms
step:1171/2330 train_time:71625ms step_avg:61.17ms
step:1172/2330 train_time:71687ms step_avg:61.17ms
step:1173/2330 train_time:71747ms step_avg:61.17ms
step:1174/2330 train_time:71810ms step_avg:61.17ms
step:1175/2330 train_time:71871ms step_avg:61.17ms
step:1176/2330 train_time:71934ms step_avg:61.17ms
step:1177/2330 train_time:71994ms step_avg:61.17ms
step:1178/2330 train_time:72056ms step_avg:61.17ms
step:1179/2330 train_time:72117ms step_avg:61.17ms
step:1180/2330 train_time:72180ms step_avg:61.17ms
step:1181/2330 train_time:72240ms step_avg:61.17ms
step:1182/2330 train_time:72303ms step_avg:61.17ms
step:1183/2330 train_time:72363ms step_avg:61.17ms
step:1184/2330 train_time:72426ms step_avg:61.17ms
step:1185/2330 train_time:72486ms step_avg:61.17ms
step:1186/2330 train_time:72548ms step_avg:61.17ms
step:1187/2330 train_time:72609ms step_avg:61.17ms
step:1188/2330 train_time:72671ms step_avg:61.17ms
step:1189/2330 train_time:72731ms step_avg:61.17ms
step:1190/2330 train_time:72793ms step_avg:61.17ms
step:1191/2330 train_time:72853ms step_avg:61.17ms
step:1192/2330 train_time:72917ms step_avg:61.17ms
step:1193/2330 train_time:72977ms step_avg:61.17ms
step:1194/2330 train_time:73040ms step_avg:61.17ms
step:1195/2330 train_time:73100ms step_avg:61.17ms
step:1196/2330 train_time:73163ms step_avg:61.17ms
step:1197/2330 train_time:73224ms step_avg:61.17ms
step:1198/2330 train_time:73286ms step_avg:61.17ms
step:1199/2330 train_time:73346ms step_avg:61.17ms
step:1200/2330 train_time:73408ms step_avg:61.17ms
step:1201/2330 train_time:73467ms step_avg:61.17ms
step:1202/2330 train_time:73530ms step_avg:61.17ms
step:1203/2330 train_time:73590ms step_avg:61.17ms
step:1204/2330 train_time:73653ms step_avg:61.17ms
step:1205/2330 train_time:73713ms step_avg:61.17ms
step:1206/2330 train_time:73776ms step_avg:61.17ms
step:1207/2330 train_time:73837ms step_avg:61.17ms
step:1208/2330 train_time:73899ms step_avg:61.17ms
step:1209/2330 train_time:73959ms step_avg:61.17ms
step:1210/2330 train_time:74023ms step_avg:61.18ms
step:1211/2330 train_time:74082ms step_avg:61.17ms
step:1212/2330 train_time:74145ms step_avg:61.18ms
step:1213/2330 train_time:74205ms step_avg:61.17ms
step:1214/2330 train_time:74268ms step_avg:61.18ms
step:1215/2330 train_time:74328ms step_avg:61.18ms
step:1216/2330 train_time:74391ms step_avg:61.18ms
step:1217/2330 train_time:74451ms step_avg:61.18ms
step:1218/2330 train_time:74514ms step_avg:61.18ms
step:1219/2330 train_time:74574ms step_avg:61.18ms
step:1220/2330 train_time:74638ms step_avg:61.18ms
step:1221/2330 train_time:74698ms step_avg:61.18ms
step:1222/2330 train_time:74762ms step_avg:61.18ms
step:1223/2330 train_time:74823ms step_avg:61.18ms
step:1224/2330 train_time:74885ms step_avg:61.18ms
step:1225/2330 train_time:74945ms step_avg:61.18ms
step:1226/2330 train_time:75008ms step_avg:61.18ms
step:1227/2330 train_time:75067ms step_avg:61.18ms
step:1228/2330 train_time:75131ms step_avg:61.18ms
step:1229/2330 train_time:75191ms step_avg:61.18ms
step:1230/2330 train_time:75254ms step_avg:61.18ms
step:1231/2330 train_time:75313ms step_avg:61.18ms
step:1232/2330 train_time:75376ms step_avg:61.18ms
step:1233/2330 train_time:75437ms step_avg:61.18ms
step:1234/2330 train_time:75500ms step_avg:61.18ms
step:1235/2330 train_time:75560ms step_avg:61.18ms
step:1236/2330 train_time:75623ms step_avg:61.18ms
step:1237/2330 train_time:75684ms step_avg:61.18ms
step:1238/2330 train_time:75746ms step_avg:61.18ms
step:1239/2330 train_time:75806ms step_avg:61.18ms
step:1240/2330 train_time:75870ms step_avg:61.19ms
step:1241/2330 train_time:75930ms step_avg:61.18ms
step:1242/2330 train_time:75993ms step_avg:61.19ms
step:1243/2330 train_time:76054ms step_avg:61.19ms
step:1244/2330 train_time:76116ms step_avg:61.19ms
step:1245/2330 train_time:76177ms step_avg:61.19ms
step:1246/2330 train_time:76241ms step_avg:61.19ms
step:1247/2330 train_time:76301ms step_avg:61.19ms
step:1248/2330 train_time:76363ms step_avg:61.19ms
step:1249/2330 train_time:76424ms step_avg:61.19ms
step:1250/2330 train_time:76487ms step_avg:61.19ms
step:1250/2330 val_loss:3.9976 train_time:76559ms step_avg:61.25ms
step:1251/2330 train_time:76581ms step_avg:61.22ms
step:1252/2330 train_time:76611ms step_avg:61.19ms
step:1253/2330 train_time:76673ms step_avg:61.19ms
step:1254/2330 train_time:76743ms step_avg:61.20ms
step:1255/2330 train_time:76803ms step_avg:61.20ms
step:1256/2330 train_time:76866ms step_avg:61.20ms
step:1257/2330 train_time:76927ms step_avg:61.20ms
step:1258/2330 train_time:76989ms step_avg:61.20ms
step:1259/2330 train_time:77049ms step_avg:61.20ms
step:1260/2330 train_time:77111ms step_avg:61.20ms
step:1261/2330 train_time:77170ms step_avg:61.20ms
step:1262/2330 train_time:77232ms step_avg:61.20ms
step:1263/2330 train_time:77292ms step_avg:61.20ms
step:1264/2330 train_time:77354ms step_avg:61.20ms
step:1265/2330 train_time:77413ms step_avg:61.20ms
step:1266/2330 train_time:77475ms step_avg:61.20ms
step:1267/2330 train_time:77535ms step_avg:61.20ms
step:1268/2330 train_time:77599ms step_avg:61.20ms
step:1269/2330 train_time:77661ms step_avg:61.20ms
step:1270/2330 train_time:77726ms step_avg:61.20ms
step:1271/2330 train_time:77787ms step_avg:61.20ms
step:1272/2330 train_time:77851ms step_avg:61.20ms
step:1273/2330 train_time:77911ms step_avg:61.20ms
step:1274/2330 train_time:77974ms step_avg:61.20ms
step:1275/2330 train_time:78034ms step_avg:61.20ms
step:1276/2330 train_time:78096ms step_avg:61.20ms
step:1277/2330 train_time:78156ms step_avg:61.20ms
step:1278/2330 train_time:78219ms step_avg:61.20ms
step:1279/2330 train_time:78278ms step_avg:61.20ms
step:1280/2330 train_time:78340ms step_avg:61.20ms
step:1281/2330 train_time:78400ms step_avg:61.20ms
step:1282/2330 train_time:78463ms step_avg:61.20ms
step:1283/2330 train_time:78523ms step_avg:61.20ms
step:1284/2330 train_time:78587ms step_avg:61.20ms
step:1285/2330 train_time:78647ms step_avg:61.20ms
step:1286/2330 train_time:78711ms step_avg:61.21ms
step:1287/2330 train_time:78772ms step_avg:61.21ms
step:1288/2330 train_time:78835ms step_avg:61.21ms
step:1289/2330 train_time:78895ms step_avg:61.21ms
step:1290/2330 train_time:78958ms step_avg:61.21ms
step:1291/2330 train_time:79018ms step_avg:61.21ms
step:1292/2330 train_time:79082ms step_avg:61.21ms
step:1293/2330 train_time:79141ms step_avg:61.21ms
step:1294/2330 train_time:79203ms step_avg:61.21ms
step:1295/2330 train_time:79263ms step_avg:61.21ms
step:1296/2330 train_time:79326ms step_avg:61.21ms
step:1297/2330 train_time:79386ms step_avg:61.21ms
step:1298/2330 train_time:79450ms step_avg:61.21ms
step:1299/2330 train_time:79509ms step_avg:61.21ms
step:1300/2330 train_time:79573ms step_avg:61.21ms
step:1301/2330 train_time:79633ms step_avg:61.21ms
step:1302/2330 train_time:79696ms step_avg:61.21ms
step:1303/2330 train_time:79756ms step_avg:61.21ms
step:1304/2330 train_time:79819ms step_avg:61.21ms
step:1305/2330 train_time:79879ms step_avg:61.21ms
step:1306/2330 train_time:79942ms step_avg:61.21ms
step:1307/2330 train_time:80003ms step_avg:61.21ms
step:1308/2330 train_time:80066ms step_avg:61.21ms
step:1309/2330 train_time:80127ms step_avg:61.21ms
step:1310/2330 train_time:80190ms step_avg:61.21ms
step:1311/2330 train_time:80251ms step_avg:61.21ms
step:1312/2330 train_time:80313ms step_avg:61.21ms
step:1313/2330 train_time:80373ms step_avg:61.21ms
step:1314/2330 train_time:80435ms step_avg:61.21ms
step:1315/2330 train_time:80494ms step_avg:61.21ms
step:1316/2330 train_time:80558ms step_avg:61.21ms
step:1317/2330 train_time:80618ms step_avg:61.21ms
step:1318/2330 train_time:80681ms step_avg:61.21ms
step:1319/2330 train_time:80742ms step_avg:61.21ms
step:1320/2330 train_time:80805ms step_avg:61.22ms
step:1321/2330 train_time:80865ms step_avg:61.21ms
step:1322/2330 train_time:80929ms step_avg:61.22ms
step:1323/2330 train_time:80989ms step_avg:61.22ms
step:1324/2330 train_time:81053ms step_avg:61.22ms
step:1325/2330 train_time:81112ms step_avg:61.22ms
step:1326/2330 train_time:81175ms step_avg:61.22ms
step:1327/2330 train_time:81235ms step_avg:61.22ms
step:1328/2330 train_time:81298ms step_avg:61.22ms
step:1329/2330 train_time:81358ms step_avg:61.22ms
step:1330/2330 train_time:81420ms step_avg:61.22ms
step:1331/2330 train_time:81480ms step_avg:61.22ms
step:1332/2330 train_time:81543ms step_avg:61.22ms
step:1333/2330 train_time:81603ms step_avg:61.22ms
step:1334/2330 train_time:81666ms step_avg:61.22ms
step:1335/2330 train_time:81725ms step_avg:61.22ms
step:1336/2330 train_time:81789ms step_avg:61.22ms
step:1337/2330 train_time:81850ms step_avg:61.22ms
step:1338/2330 train_time:81913ms step_avg:61.22ms
step:1339/2330 train_time:81973ms step_avg:61.22ms
step:1340/2330 train_time:82036ms step_avg:61.22ms
step:1341/2330 train_time:82096ms step_avg:61.22ms
step:1342/2330 train_time:82159ms step_avg:61.22ms
step:1343/2330 train_time:82219ms step_avg:61.22ms
step:1344/2330 train_time:82283ms step_avg:61.22ms
step:1345/2330 train_time:82342ms step_avg:61.22ms
step:1346/2330 train_time:82405ms step_avg:61.22ms
step:1347/2330 train_time:82465ms step_avg:61.22ms
step:1348/2330 train_time:82528ms step_avg:61.22ms
step:1349/2330 train_time:82589ms step_avg:61.22ms
step:1350/2330 train_time:82651ms step_avg:61.22ms
step:1351/2330 train_time:82711ms step_avg:61.22ms
step:1352/2330 train_time:82774ms step_avg:61.22ms
step:1353/2330 train_time:82834ms step_avg:61.22ms
step:1354/2330 train_time:82897ms step_avg:61.22ms
step:1355/2330 train_time:82957ms step_avg:61.22ms
step:1356/2330 train_time:83019ms step_avg:61.22ms
step:1357/2330 train_time:83080ms step_avg:61.22ms
step:1358/2330 train_time:83142ms step_avg:61.22ms
step:1359/2330 train_time:83202ms step_avg:61.22ms
step:1360/2330 train_time:83265ms step_avg:61.22ms
step:1361/2330 train_time:83325ms step_avg:61.22ms
step:1362/2330 train_time:83388ms step_avg:61.22ms
step:1363/2330 train_time:83448ms step_avg:61.22ms
step:1364/2330 train_time:83511ms step_avg:61.23ms
step:1365/2330 train_time:83571ms step_avg:61.22ms
step:1366/2330 train_time:83635ms step_avg:61.23ms
step:1367/2330 train_time:83695ms step_avg:61.23ms
step:1368/2330 train_time:83757ms step_avg:61.23ms
step:1369/2330 train_time:83817ms step_avg:61.22ms
step:1370/2330 train_time:83880ms step_avg:61.23ms
step:1371/2330 train_time:83939ms step_avg:61.22ms
step:1372/2330 train_time:84003ms step_avg:61.23ms
step:1373/2330 train_time:84063ms step_avg:61.23ms
step:1374/2330 train_time:84126ms step_avg:61.23ms
step:1375/2330 train_time:84187ms step_avg:61.23ms
step:1376/2330 train_time:84252ms step_avg:61.23ms
step:1377/2330 train_time:84311ms step_avg:61.23ms
step:1378/2330 train_time:84373ms step_avg:61.23ms
step:1379/2330 train_time:84433ms step_avg:61.23ms
step:1380/2330 train_time:84496ms step_avg:61.23ms
step:1381/2330 train_time:84556ms step_avg:61.23ms
step:1382/2330 train_time:84619ms step_avg:61.23ms
step:1383/2330 train_time:84679ms step_avg:61.23ms
step:1384/2330 train_time:84743ms step_avg:61.23ms
step:1385/2330 train_time:84803ms step_avg:61.23ms
step:1386/2330 train_time:84865ms step_avg:61.23ms
step:1387/2330 train_time:84926ms step_avg:61.23ms
step:1388/2330 train_time:84989ms step_avg:61.23ms
step:1389/2330 train_time:85049ms step_avg:61.23ms
step:1390/2330 train_time:85112ms step_avg:61.23ms
step:1391/2330 train_time:85172ms step_avg:61.23ms
step:1392/2330 train_time:85236ms step_avg:61.23ms
step:1393/2330 train_time:85295ms step_avg:61.23ms
step:1394/2330 train_time:85358ms step_avg:61.23ms
step:1395/2330 train_time:85418ms step_avg:61.23ms
step:1396/2330 train_time:85481ms step_avg:61.23ms
step:1397/2330 train_time:85541ms step_avg:61.23ms
step:1398/2330 train_time:85603ms step_avg:61.23ms
step:1399/2330 train_time:85664ms step_avg:61.23ms
step:1400/2330 train_time:85727ms step_avg:61.23ms
step:1401/2330 train_time:85788ms step_avg:61.23ms
step:1402/2330 train_time:85851ms step_avg:61.23ms
step:1403/2330 train_time:85910ms step_avg:61.23ms
step:1404/2330 train_time:85973ms step_avg:61.23ms
step:1405/2330 train_time:86034ms step_avg:61.23ms
step:1406/2330 train_time:86097ms step_avg:61.24ms
step:1407/2330 train_time:86157ms step_avg:61.23ms
step:1408/2330 train_time:86219ms step_avg:61.24ms
step:1409/2330 train_time:86279ms step_avg:61.23ms
step:1410/2330 train_time:86343ms step_avg:61.24ms
step:1411/2330 train_time:86403ms step_avg:61.24ms
step:1412/2330 train_time:86466ms step_avg:61.24ms
step:1413/2330 train_time:86526ms step_avg:61.24ms
step:1414/2330 train_time:86589ms step_avg:61.24ms
step:1415/2330 train_time:86650ms step_avg:61.24ms
step:1416/2330 train_time:86713ms step_avg:61.24ms
step:1417/2330 train_time:86773ms step_avg:61.24ms
step:1418/2330 train_time:86836ms step_avg:61.24ms
step:1419/2330 train_time:86896ms step_avg:61.24ms
step:1420/2330 train_time:86958ms step_avg:61.24ms
step:1421/2330 train_time:87018ms step_avg:61.24ms
step:1422/2330 train_time:87082ms step_avg:61.24ms
step:1423/2330 train_time:87142ms step_avg:61.24ms
step:1424/2330 train_time:87204ms step_avg:61.24ms
step:1425/2330 train_time:87265ms step_avg:61.24ms
step:1426/2330 train_time:87328ms step_avg:61.24ms
step:1427/2330 train_time:87389ms step_avg:61.24ms
step:1428/2330 train_time:87453ms step_avg:61.24ms
step:1429/2330 train_time:87512ms step_avg:61.24ms
step:1430/2330 train_time:87574ms step_avg:61.24ms
step:1431/2330 train_time:87635ms step_avg:61.24ms
step:1432/2330 train_time:87698ms step_avg:61.24ms
step:1433/2330 train_time:87758ms step_avg:61.24ms
step:1434/2330 train_time:87821ms step_avg:61.24ms
step:1435/2330 train_time:87881ms step_avg:61.24ms
step:1436/2330 train_time:87944ms step_avg:61.24ms
step:1437/2330 train_time:88004ms step_avg:61.24ms
step:1438/2330 train_time:88067ms step_avg:61.24ms
step:1439/2330 train_time:88127ms step_avg:61.24ms
step:1440/2330 train_time:88190ms step_avg:61.24ms
step:1441/2330 train_time:88250ms step_avg:61.24ms
step:1442/2330 train_time:88313ms step_avg:61.24ms
step:1443/2330 train_time:88373ms step_avg:61.24ms
step:1444/2330 train_time:88435ms step_avg:61.24ms
step:1445/2330 train_time:88495ms step_avg:61.24ms
step:1446/2330 train_time:88559ms step_avg:61.24ms
step:1447/2330 train_time:88619ms step_avg:61.24ms
step:1448/2330 train_time:88681ms step_avg:61.24ms
step:1449/2330 train_time:88742ms step_avg:61.24ms
step:1450/2330 train_time:88805ms step_avg:61.24ms
step:1451/2330 train_time:88865ms step_avg:61.24ms
step:1452/2330 train_time:88927ms step_avg:61.24ms
step:1453/2330 train_time:88988ms step_avg:61.24ms
step:1454/2330 train_time:89052ms step_avg:61.25ms
step:1455/2330 train_time:89112ms step_avg:61.25ms
step:1456/2330 train_time:89175ms step_avg:61.25ms
step:1457/2330 train_time:89235ms step_avg:61.25ms
step:1458/2330 train_time:89297ms step_avg:61.25ms
step:1459/2330 train_time:89357ms step_avg:61.25ms
step:1460/2330 train_time:89419ms step_avg:61.25ms
step:1461/2330 train_time:89479ms step_avg:61.25ms
step:1462/2330 train_time:89543ms step_avg:61.25ms
step:1463/2330 train_time:89603ms step_avg:61.25ms
step:1464/2330 train_time:89666ms step_avg:61.25ms
step:1465/2330 train_time:89725ms step_avg:61.25ms
step:1466/2330 train_time:89789ms step_avg:61.25ms
step:1467/2330 train_time:89849ms step_avg:61.25ms
step:1468/2330 train_time:89911ms step_avg:61.25ms
step:1469/2330 train_time:89971ms step_avg:61.25ms
step:1470/2330 train_time:90034ms step_avg:61.25ms
step:1471/2330 train_time:90094ms step_avg:61.25ms
step:1472/2330 train_time:90156ms step_avg:61.25ms
step:1473/2330 train_time:90216ms step_avg:61.25ms
step:1474/2330 train_time:90279ms step_avg:61.25ms
step:1475/2330 train_time:90339ms step_avg:61.25ms
step:1476/2330 train_time:90402ms step_avg:61.25ms
step:1477/2330 train_time:90462ms step_avg:61.25ms
step:1478/2330 train_time:90524ms step_avg:61.25ms
step:1479/2330 train_time:90584ms step_avg:61.25ms
step:1480/2330 train_time:90648ms step_avg:61.25ms
step:1481/2330 train_time:90708ms step_avg:61.25ms
step:1482/2330 train_time:90770ms step_avg:61.25ms
step:1483/2330 train_time:90830ms step_avg:61.25ms
step:1484/2330 train_time:90893ms step_avg:61.25ms
step:1485/2330 train_time:90953ms step_avg:61.25ms
step:1486/2330 train_time:91016ms step_avg:61.25ms
step:1487/2330 train_time:91076ms step_avg:61.25ms
step:1488/2330 train_time:91139ms step_avg:61.25ms
step:1489/2330 train_time:91199ms step_avg:61.25ms
step:1490/2330 train_time:91263ms step_avg:61.25ms
step:1491/2330 train_time:91322ms step_avg:61.25ms
step:1492/2330 train_time:91384ms step_avg:61.25ms
step:1493/2330 train_time:91444ms step_avg:61.25ms
step:1494/2330 train_time:91507ms step_avg:61.25ms
step:1495/2330 train_time:91568ms step_avg:61.25ms
step:1496/2330 train_time:91631ms step_avg:61.25ms
step:1497/2330 train_time:91692ms step_avg:61.25ms
step:1498/2330 train_time:91755ms step_avg:61.25ms
step:1499/2330 train_time:91814ms step_avg:61.25ms
step:1500/2330 train_time:91877ms step_avg:61.25ms
step:1500/2330 val_loss:3.9326 train_time:91949ms step_avg:61.30ms
step:1501/2330 train_time:91972ms step_avg:61.27ms
step:1502/2330 train_time:92002ms step_avg:61.25ms
step:1503/2330 train_time:92066ms step_avg:61.26ms
step:1504/2330 train_time:92135ms step_avg:61.26ms
step:1505/2330 train_time:92194ms step_avg:61.26ms
step:1506/2330 train_time:92257ms step_avg:61.26ms
step:1507/2330 train_time:92317ms step_avg:61.26ms
step:1508/2330 train_time:92381ms step_avg:61.26ms
step:1509/2330 train_time:92440ms step_avg:61.26ms
step:1510/2330 train_time:92502ms step_avg:61.26ms
step:1511/2330 train_time:92561ms step_avg:61.26ms
step:1512/2330 train_time:92624ms step_avg:61.26ms
step:1513/2330 train_time:92683ms step_avg:61.26ms
step:1514/2330 train_time:92745ms step_avg:61.26ms
step:1515/2330 train_time:92804ms step_avg:61.26ms
step:1516/2330 train_time:92867ms step_avg:61.26ms
step:1517/2330 train_time:92927ms step_avg:61.26ms
step:1518/2330 train_time:92993ms step_avg:61.26ms
step:1519/2330 train_time:93055ms step_avg:61.26ms
step:1520/2330 train_time:93121ms step_avg:61.26ms
step:1521/2330 train_time:93182ms step_avg:61.26ms
step:1522/2330 train_time:93246ms step_avg:61.27ms
step:1523/2330 train_time:93306ms step_avg:61.26ms
step:1524/2330 train_time:93371ms step_avg:61.27ms
step:1525/2330 train_time:93430ms step_avg:61.27ms
step:1526/2330 train_time:93492ms step_avg:61.27ms
step:1527/2330 train_time:93552ms step_avg:61.27ms
step:1528/2330 train_time:93615ms step_avg:61.27ms
step:1529/2330 train_time:93676ms step_avg:61.27ms
step:1530/2330 train_time:93738ms step_avg:61.27ms
step:1531/2330 train_time:93798ms step_avg:61.27ms
step:1532/2330 train_time:93862ms step_avg:61.27ms
step:1533/2330 train_time:93923ms step_avg:61.27ms
step:1534/2330 train_time:93986ms step_avg:61.27ms
step:1535/2330 train_time:94046ms step_avg:61.27ms
step:1536/2330 train_time:94110ms step_avg:61.27ms
step:1537/2330 train_time:94171ms step_avg:61.27ms
step:1538/2330 train_time:94236ms step_avg:61.27ms
step:1539/2330 train_time:94298ms step_avg:61.27ms
step:1540/2330 train_time:94362ms step_avg:61.27ms
step:1541/2330 train_time:94422ms step_avg:61.27ms
step:1542/2330 train_time:94485ms step_avg:61.27ms
step:1543/2330 train_time:94545ms step_avg:61.27ms
step:1544/2330 train_time:94608ms step_avg:61.27ms
step:1545/2330 train_time:94668ms step_avg:61.27ms
step:1546/2330 train_time:94732ms step_avg:61.28ms
step:1547/2330 train_time:94792ms step_avg:61.27ms
step:1548/2330 train_time:94856ms step_avg:61.28ms
step:1549/2330 train_time:94917ms step_avg:61.28ms
step:1550/2330 train_time:94981ms step_avg:61.28ms
step:1551/2330 train_time:95042ms step_avg:61.28ms
step:1552/2330 train_time:95104ms step_avg:61.28ms
step:1553/2330 train_time:95165ms step_avg:61.28ms
step:1554/2330 train_time:95229ms step_avg:61.28ms
step:1555/2330 train_time:95290ms step_avg:61.28ms
step:1556/2330 train_time:95354ms step_avg:61.28ms
step:1557/2330 train_time:95415ms step_avg:61.28ms
step:1558/2330 train_time:95479ms step_avg:61.28ms
step:1559/2330 train_time:95540ms step_avg:61.28ms
step:1560/2330 train_time:95603ms step_avg:61.28ms
step:1561/2330 train_time:95663ms step_avg:61.28ms
step:1562/2330 train_time:95726ms step_avg:61.28ms
step:1563/2330 train_time:95787ms step_avg:61.28ms
step:1564/2330 train_time:95851ms step_avg:61.29ms
step:1565/2330 train_time:95912ms step_avg:61.29ms
step:1566/2330 train_time:95976ms step_avg:61.29ms
step:1567/2330 train_time:96037ms step_avg:61.29ms
step:1568/2330 train_time:96102ms step_avg:61.29ms
step:1569/2330 train_time:96162ms step_avg:61.29ms
step:1570/2330 train_time:96225ms step_avg:61.29ms
step:1571/2330 train_time:96287ms step_avg:61.29ms
step:1572/2330 train_time:96351ms step_avg:61.29ms
step:1573/2330 train_time:96411ms step_avg:61.29ms
step:1574/2330 train_time:96474ms step_avg:61.29ms
step:1575/2330 train_time:96536ms step_avg:61.29ms
step:1576/2330 train_time:96600ms step_avg:61.29ms
step:1577/2330 train_time:96661ms step_avg:61.29ms
step:1578/2330 train_time:96725ms step_avg:61.30ms
step:1579/2330 train_time:96784ms step_avg:61.29ms
step:1580/2330 train_time:96847ms step_avg:61.30ms
step:1581/2330 train_time:96908ms step_avg:61.30ms
step:1582/2330 train_time:96972ms step_avg:61.30ms
step:1583/2330 train_time:97033ms step_avg:61.30ms
step:1584/2330 train_time:97098ms step_avg:61.30ms
step:1585/2330 train_time:97161ms step_avg:61.30ms
step:1586/2330 train_time:97223ms step_avg:61.30ms
step:1587/2330 train_time:97285ms step_avg:61.30ms
step:1588/2330 train_time:97347ms step_avg:61.30ms
step:1589/2330 train_time:97408ms step_avg:61.30ms
step:1590/2330 train_time:97472ms step_avg:61.30ms
step:1591/2330 train_time:97533ms step_avg:61.30ms
step:1592/2330 train_time:97596ms step_avg:61.30ms
step:1593/2330 train_time:97658ms step_avg:61.30ms
step:1594/2330 train_time:97722ms step_avg:61.31ms
step:1595/2330 train_time:97783ms step_avg:61.31ms
step:1596/2330 train_time:97846ms step_avg:61.31ms
step:1597/2330 train_time:97906ms step_avg:61.31ms
step:1598/2330 train_time:97970ms step_avg:61.31ms
step:1599/2330 train_time:98030ms step_avg:61.31ms
step:1600/2330 train_time:98095ms step_avg:61.31ms
step:1601/2330 train_time:98156ms step_avg:61.31ms
step:1602/2330 train_time:98221ms step_avg:61.31ms
step:1603/2330 train_time:98282ms step_avg:61.31ms
step:1604/2330 train_time:98344ms step_avg:61.31ms
step:1605/2330 train_time:98405ms step_avg:61.31ms
step:1606/2330 train_time:98469ms step_avg:61.31ms
step:1607/2330 train_time:98529ms step_avg:61.31ms
step:1608/2330 train_time:98593ms step_avg:61.31ms
step:1609/2330 train_time:98654ms step_avg:61.31ms
step:1610/2330 train_time:98718ms step_avg:61.32ms
step:1611/2330 train_time:98778ms step_avg:61.31ms
step:1612/2330 train_time:98841ms step_avg:61.32ms
step:1613/2330 train_time:98902ms step_avg:61.32ms
step:1614/2330 train_time:98966ms step_avg:61.32ms
step:1615/2330 train_time:99026ms step_avg:61.32ms
step:1616/2330 train_time:99090ms step_avg:61.32ms
step:1617/2330 train_time:99150ms step_avg:61.32ms
step:1618/2330 train_time:99215ms step_avg:61.32ms
step:1619/2330 train_time:99277ms step_avg:61.32ms
step:1620/2330 train_time:99341ms step_avg:61.32ms
step:1621/2330 train_time:99402ms step_avg:61.32ms
step:1622/2330 train_time:99465ms step_avg:61.32ms
step:1623/2330 train_time:99525ms step_avg:61.32ms
step:1624/2330 train_time:99589ms step_avg:61.32ms
step:1625/2330 train_time:99649ms step_avg:61.32ms
step:1626/2330 train_time:99713ms step_avg:61.32ms
step:1627/2330 train_time:99774ms step_avg:61.32ms
step:1628/2330 train_time:99837ms step_avg:61.32ms
step:1629/2330 train_time:99898ms step_avg:61.32ms
step:1630/2330 train_time:99963ms step_avg:61.33ms
step:1631/2330 train_time:100022ms step_avg:61.33ms
step:1632/2330 train_time:100087ms step_avg:61.33ms
step:1633/2330 train_time:100147ms step_avg:61.33ms
step:1634/2330 train_time:100212ms step_avg:61.33ms
step:1635/2330 train_time:100272ms step_avg:61.33ms
step:1636/2330 train_time:100337ms step_avg:61.33ms
step:1637/2330 train_time:100399ms step_avg:61.33ms
step:1638/2330 train_time:100464ms step_avg:61.33ms
step:1639/2330 train_time:100524ms step_avg:61.33ms
step:1640/2330 train_time:100587ms step_avg:61.33ms
step:1641/2330 train_time:100647ms step_avg:61.33ms
step:1642/2330 train_time:100711ms step_avg:61.33ms
step:1643/2330 train_time:100771ms step_avg:61.33ms
step:1644/2330 train_time:100835ms step_avg:61.34ms
step:1645/2330 train_time:100896ms step_avg:61.34ms
step:1646/2330 train_time:100961ms step_avg:61.34ms
step:1647/2330 train_time:101022ms step_avg:61.34ms
step:1648/2330 train_time:101086ms step_avg:61.34ms
step:1649/2330 train_time:101146ms step_avg:61.34ms
step:1650/2330 train_time:101210ms step_avg:61.34ms
step:1651/2330 train_time:101270ms step_avg:61.34ms
step:1652/2330 train_time:101334ms step_avg:61.34ms
step:1653/2330 train_time:101396ms step_avg:61.34ms
step:1654/2330 train_time:101461ms step_avg:61.34ms
step:1655/2330 train_time:101521ms step_avg:61.34ms
step:1656/2330 train_time:101585ms step_avg:61.34ms
step:1657/2330 train_time:101645ms step_avg:61.34ms
step:1658/2330 train_time:101709ms step_avg:61.34ms
step:1659/2330 train_time:101769ms step_avg:61.34ms
step:1660/2330 train_time:101832ms step_avg:61.34ms
step:1661/2330 train_time:101893ms step_avg:61.34ms
step:1662/2330 train_time:101957ms step_avg:61.35ms
step:1663/2330 train_time:102018ms step_avg:61.35ms
step:1664/2330 train_time:102082ms step_avg:61.35ms
step:1665/2330 train_time:102142ms step_avg:61.35ms
step:1666/2330 train_time:102204ms step_avg:61.35ms
step:1667/2330 train_time:102265ms step_avg:61.35ms
step:1668/2330 train_time:102329ms step_avg:61.35ms
step:1669/2330 train_time:102390ms step_avg:61.35ms
step:1670/2330 train_time:102453ms step_avg:61.35ms
step:1671/2330 train_time:102514ms step_avg:61.35ms
step:1672/2330 train_time:102579ms step_avg:61.35ms
step:1673/2330 train_time:102640ms step_avg:61.35ms
step:1674/2330 train_time:102703ms step_avg:61.35ms
step:1675/2330 train_time:102763ms step_avg:61.35ms
step:1676/2330 train_time:102826ms step_avg:61.35ms
step:1677/2330 train_time:102888ms step_avg:61.35ms
step:1678/2330 train_time:102952ms step_avg:61.35ms
step:1679/2330 train_time:103012ms step_avg:61.35ms
step:1680/2330 train_time:103077ms step_avg:61.36ms
step:1681/2330 train_time:103139ms step_avg:61.36ms
step:1682/2330 train_time:103201ms step_avg:61.36ms
step:1683/2330 train_time:103263ms step_avg:61.36ms
step:1684/2330 train_time:103326ms step_avg:61.36ms
step:1685/2330 train_time:103387ms step_avg:61.36ms
step:1686/2330 train_time:103450ms step_avg:61.36ms
step:1687/2330 train_time:103510ms step_avg:61.36ms
step:1688/2330 train_time:103574ms step_avg:61.36ms
step:1689/2330 train_time:103635ms step_avg:61.36ms
step:1690/2330 train_time:103699ms step_avg:61.36ms
step:1691/2330 train_time:103759ms step_avg:61.36ms
step:1692/2330 train_time:103824ms step_avg:61.36ms
step:1693/2330 train_time:103885ms step_avg:61.36ms
step:1694/2330 train_time:103948ms step_avg:61.36ms
step:1695/2330 train_time:104009ms step_avg:61.36ms
step:1696/2330 train_time:104073ms step_avg:61.36ms
step:1697/2330 train_time:104134ms step_avg:61.36ms
step:1698/2330 train_time:104198ms step_avg:61.36ms
step:1699/2330 train_time:104258ms step_avg:61.36ms
step:1700/2330 train_time:104322ms step_avg:61.37ms
step:1701/2330 train_time:104383ms step_avg:61.37ms
step:1702/2330 train_time:104446ms step_avg:61.37ms
step:1703/2330 train_time:104506ms step_avg:61.37ms
step:1704/2330 train_time:104570ms step_avg:61.37ms
step:1705/2330 train_time:104632ms step_avg:61.37ms
step:1706/2330 train_time:104696ms step_avg:61.37ms
step:1707/2330 train_time:104757ms step_avg:61.37ms
step:1708/2330 train_time:104821ms step_avg:61.37ms
step:1709/2330 train_time:104882ms step_avg:61.37ms
step:1710/2330 train_time:104945ms step_avg:61.37ms
step:1711/2330 train_time:105004ms step_avg:61.37ms
step:1712/2330 train_time:105069ms step_avg:61.37ms
step:1713/2330 train_time:105129ms step_avg:61.37ms
step:1714/2330 train_time:105193ms step_avg:61.37ms
step:1715/2330 train_time:105254ms step_avg:61.37ms
step:1716/2330 train_time:105318ms step_avg:61.37ms
step:1717/2330 train_time:105378ms step_avg:61.37ms
step:1718/2330 train_time:105442ms step_avg:61.37ms
step:1719/2330 train_time:105502ms step_avg:61.37ms
step:1720/2330 train_time:105565ms step_avg:61.38ms
step:1721/2330 train_time:105625ms step_avg:61.37ms
step:1722/2330 train_time:105689ms step_avg:61.38ms
step:1723/2330 train_time:105749ms step_avg:61.37ms
step:1724/2330 train_time:105813ms step_avg:61.38ms
step:1725/2330 train_time:105875ms step_avg:61.38ms
step:1726/2330 train_time:105938ms step_avg:61.38ms
step:1727/2330 train_time:106000ms step_avg:61.38ms
step:1728/2330 train_time:106063ms step_avg:61.38ms
step:1729/2330 train_time:106123ms step_avg:61.38ms
step:1730/2330 train_time:106186ms step_avg:61.38ms
step:1731/2330 train_time:106247ms step_avg:61.38ms
step:1732/2330 train_time:106310ms step_avg:61.38ms
step:1733/2330 train_time:106370ms step_avg:61.38ms
step:1734/2330 train_time:106434ms step_avg:61.38ms
step:1735/2330 train_time:106495ms step_avg:61.38ms
step:1736/2330 train_time:106560ms step_avg:61.38ms
step:1737/2330 train_time:106622ms step_avg:61.38ms
step:1738/2330 train_time:106684ms step_avg:61.38ms
step:1739/2330 train_time:106744ms step_avg:61.38ms
step:1740/2330 train_time:106808ms step_avg:61.38ms
step:1741/2330 train_time:106868ms step_avg:61.38ms
step:1742/2330 train_time:106931ms step_avg:61.38ms
step:1743/2330 train_time:106992ms step_avg:61.38ms
step:1744/2330 train_time:107056ms step_avg:61.39ms
step:1745/2330 train_time:107117ms step_avg:61.39ms
step:1746/2330 train_time:107182ms step_avg:61.39ms
step:1747/2330 train_time:107242ms step_avg:61.39ms
step:1748/2330 train_time:107304ms step_avg:61.39ms
step:1749/2330 train_time:107365ms step_avg:61.39ms
step:1750/2330 train_time:107429ms step_avg:61.39ms
step:1750/2330 val_loss:3.8837 train_time:107505ms step_avg:61.43ms
step:1751/2330 train_time:107527ms step_avg:61.41ms
step:1752/2330 train_time:107559ms step_avg:61.39ms
step:1753/2330 train_time:107627ms step_avg:61.40ms
step:1754/2330 train_time:107691ms step_avg:61.40ms
step:1755/2330 train_time:107751ms step_avg:61.40ms
step:1756/2330 train_time:107816ms step_avg:61.40ms
step:1757/2330 train_time:107877ms step_avg:61.40ms
step:1758/2330 train_time:107940ms step_avg:61.40ms
step:1759/2330 train_time:108001ms step_avg:61.40ms
step:1760/2330 train_time:108063ms step_avg:61.40ms
step:1761/2330 train_time:108123ms step_avg:61.40ms
step:1762/2330 train_time:108185ms step_avg:61.40ms
step:1763/2330 train_time:108244ms step_avg:61.40ms
step:1764/2330 train_time:108306ms step_avg:61.40ms
step:1765/2330 train_time:108367ms step_avg:61.40ms
step:1766/2330 train_time:108429ms step_avg:61.40ms
step:1767/2330 train_time:108491ms step_avg:61.40ms
step:1768/2330 train_time:108556ms step_avg:61.40ms
step:1769/2330 train_time:108620ms step_avg:61.40ms
step:1770/2330 train_time:108684ms step_avg:61.40ms
step:1771/2330 train_time:108745ms step_avg:61.40ms
step:1772/2330 train_time:108809ms step_avg:61.40ms
step:1773/2330 train_time:108869ms step_avg:61.40ms
step:1774/2330 train_time:108932ms step_avg:61.40ms
step:1775/2330 train_time:108993ms step_avg:61.40ms
step:1776/2330 train_time:109057ms step_avg:61.41ms
step:1777/2330 train_time:109118ms step_avg:61.41ms
step:1778/2330 train_time:109181ms step_avg:61.41ms
step:1779/2330 train_time:109241ms step_avg:61.41ms
step:1780/2330 train_time:109304ms step_avg:61.41ms
step:1781/2330 train_time:109364ms step_avg:61.41ms
step:1782/2330 train_time:109426ms step_avg:61.41ms
step:1783/2330 train_time:109487ms step_avg:61.41ms
step:1784/2330 train_time:109550ms step_avg:61.41ms
step:1785/2330 train_time:109612ms step_avg:61.41ms
step:1786/2330 train_time:109676ms step_avg:61.41ms
step:1787/2330 train_time:109739ms step_avg:61.41ms
step:1788/2330 train_time:109803ms step_avg:61.41ms
step:1789/2330 train_time:109864ms step_avg:61.41ms
step:1790/2330 train_time:109925ms step_avg:61.41ms
step:1791/2330 train_time:109987ms step_avg:61.41ms
step:1792/2330 train_time:110050ms step_avg:61.41ms
step:1793/2330 train_time:110111ms step_avg:61.41ms
step:1794/2330 train_time:110175ms step_avg:61.41ms
step:1795/2330 train_time:110235ms step_avg:61.41ms
step:1796/2330 train_time:110298ms step_avg:61.41ms
step:1797/2330 train_time:110359ms step_avg:61.41ms
step:1798/2330 train_time:110423ms step_avg:61.41ms
step:1799/2330 train_time:110482ms step_avg:61.41ms
step:1800/2330 train_time:110546ms step_avg:61.41ms
step:1801/2330 train_time:110606ms step_avg:61.41ms
step:1802/2330 train_time:110671ms step_avg:61.42ms
step:1803/2330 train_time:110731ms step_avg:61.41ms
step:1804/2330 train_time:110796ms step_avg:61.42ms
step:1805/2330 train_time:110856ms step_avg:61.42ms
step:1806/2330 train_time:110921ms step_avg:61.42ms
step:1807/2330 train_time:110982ms step_avg:61.42ms
step:1808/2330 train_time:111045ms step_avg:61.42ms
step:1809/2330 train_time:111105ms step_avg:61.42ms
step:1810/2330 train_time:111169ms step_avg:61.42ms
step:1811/2330 train_time:111229ms step_avg:61.42ms
step:1812/2330 train_time:111294ms step_avg:61.42ms
step:1813/2330 train_time:111354ms step_avg:61.42ms
step:1814/2330 train_time:111417ms step_avg:61.42ms
step:1815/2330 train_time:111477ms step_avg:61.42ms
step:1816/2330 train_time:111542ms step_avg:61.42ms
step:1817/2330 train_time:111602ms step_avg:61.42ms
step:1818/2330 train_time:111665ms step_avg:61.42ms
step:1819/2330 train_time:111726ms step_avg:61.42ms
step:1820/2330 train_time:111789ms step_avg:61.42ms
step:1821/2330 train_time:111850ms step_avg:61.42ms
step:1822/2330 train_time:111914ms step_avg:61.42ms
step:1823/2330 train_time:111975ms step_avg:61.42ms
step:1824/2330 train_time:112039ms step_avg:61.42ms
step:1825/2330 train_time:112099ms step_avg:61.42ms
step:1826/2330 train_time:112163ms step_avg:61.43ms
step:1827/2330 train_time:112223ms step_avg:61.42ms
step:1828/2330 train_time:112285ms step_avg:61.43ms
step:1829/2330 train_time:112346ms step_avg:61.43ms
step:1830/2330 train_time:112410ms step_avg:61.43ms
step:1831/2330 train_time:112471ms step_avg:61.43ms
step:1832/2330 train_time:112535ms step_avg:61.43ms
step:1833/2330 train_time:112597ms step_avg:61.43ms
step:1834/2330 train_time:112661ms step_avg:61.43ms
step:1835/2330 train_time:112722ms step_avg:61.43ms
step:1836/2330 train_time:112784ms step_avg:61.43ms
step:1837/2330 train_time:112845ms step_avg:61.43ms
step:1838/2330 train_time:112908ms step_avg:61.43ms
step:1839/2330 train_time:112969ms step_avg:61.43ms
step:1840/2330 train_time:113033ms step_avg:61.43ms
step:1841/2330 train_time:113094ms step_avg:61.43ms
step:1842/2330 train_time:113159ms step_avg:61.43ms
step:1843/2330 train_time:113218ms step_avg:61.43ms
step:1844/2330 train_time:113282ms step_avg:61.43ms
step:1845/2330 train_time:113343ms step_avg:61.43ms
step:1846/2330 train_time:113406ms step_avg:61.43ms
step:1847/2330 train_time:113466ms step_avg:61.43ms
step:1848/2330 train_time:113529ms step_avg:61.43ms
step:1849/2330 train_time:113590ms step_avg:61.43ms
step:1850/2330 train_time:113655ms step_avg:61.43ms
step:1851/2330 train_time:113715ms step_avg:61.43ms
step:1852/2330 train_time:113779ms step_avg:61.44ms
step:1853/2330 train_time:113840ms step_avg:61.44ms
step:1854/2330 train_time:113905ms step_avg:61.44ms
step:1855/2330 train_time:113965ms step_avg:61.44ms
step:1856/2330 train_time:114028ms step_avg:61.44ms
step:1857/2330 train_time:114088ms step_avg:61.44ms
step:1858/2330 train_time:114152ms step_avg:61.44ms
step:1859/2330 train_time:114212ms step_avg:61.44ms
step:1860/2330 train_time:114277ms step_avg:61.44ms
step:1861/2330 train_time:114338ms step_avg:61.44ms
step:1862/2330 train_time:114402ms step_avg:61.44ms
step:1863/2330 train_time:114463ms step_avg:61.44ms
step:1864/2330 train_time:114526ms step_avg:61.44ms
step:1865/2330 train_time:114586ms step_avg:61.44ms
step:1866/2330 train_time:114650ms step_avg:61.44ms
step:1867/2330 train_time:114710ms step_avg:61.44ms
step:1868/2330 train_time:114774ms step_avg:61.44ms
step:1869/2330 train_time:114835ms step_avg:61.44ms
step:1870/2330 train_time:114899ms step_avg:61.44ms
step:1871/2330 train_time:114960ms step_avg:61.44ms
step:1872/2330 train_time:115023ms step_avg:61.44ms
step:1873/2330 train_time:115083ms step_avg:61.44ms
step:1874/2330 train_time:115145ms step_avg:61.44ms
step:1875/2330 train_time:115206ms step_avg:61.44ms
step:1876/2330 train_time:115271ms step_avg:61.45ms
step:1877/2330 train_time:115332ms step_avg:61.44ms
step:1878/2330 train_time:115396ms step_avg:61.45ms
step:1879/2330 train_time:115458ms step_avg:61.45ms
step:1880/2330 train_time:115521ms step_avg:61.45ms
step:1881/2330 train_time:115582ms step_avg:61.45ms
step:1882/2330 train_time:115645ms step_avg:61.45ms
step:1883/2330 train_time:115704ms step_avg:61.45ms
step:1884/2330 train_time:115768ms step_avg:61.45ms
step:1885/2330 train_time:115829ms step_avg:61.45ms
step:1886/2330 train_time:115894ms step_avg:61.45ms
step:1887/2330 train_time:115955ms step_avg:61.45ms
step:1888/2330 train_time:116018ms step_avg:61.45ms
step:1889/2330 train_time:116079ms step_avg:61.45ms
step:1890/2330 train_time:116144ms step_avg:61.45ms
step:1891/2330 train_time:116203ms step_avg:61.45ms
step:1892/2330 train_time:116267ms step_avg:61.45ms
step:1893/2330 train_time:116327ms step_avg:61.45ms
step:1894/2330 train_time:116391ms step_avg:61.45ms
step:1895/2330 train_time:116451ms step_avg:61.45ms
step:1896/2330 train_time:116516ms step_avg:61.45ms
step:1897/2330 train_time:116577ms step_avg:61.45ms
step:1898/2330 train_time:116641ms step_avg:61.45ms
step:1899/2330 train_time:116702ms step_avg:61.45ms
step:1900/2330 train_time:116766ms step_avg:61.46ms
step:1901/2330 train_time:116826ms step_avg:61.45ms
step:1902/2330 train_time:116889ms step_avg:61.46ms
step:1903/2330 train_time:116948ms step_avg:61.45ms
step:1904/2330 train_time:117013ms step_avg:61.46ms
step:1905/2330 train_time:117074ms step_avg:61.46ms
step:1906/2330 train_time:117137ms step_avg:61.46ms
step:1907/2330 train_time:117198ms step_avg:61.46ms
step:1908/2330 train_time:117263ms step_avg:61.46ms
step:1909/2330 train_time:117322ms step_avg:61.46ms
step:1910/2330 train_time:117386ms step_avg:61.46ms
step:1911/2330 train_time:117446ms step_avg:61.46ms
step:1912/2330 train_time:117509ms step_avg:61.46ms
step:1913/2330 train_time:117571ms step_avg:61.46ms
step:1914/2330 train_time:117634ms step_avg:61.46ms
step:1915/2330 train_time:117695ms step_avg:61.46ms
step:1916/2330 train_time:117760ms step_avg:61.46ms
step:1917/2330 train_time:117820ms step_avg:61.46ms
step:1918/2330 train_time:117885ms step_avg:61.46ms
step:1919/2330 train_time:117945ms step_avg:61.46ms
step:1920/2330 train_time:118008ms step_avg:61.46ms
step:1921/2330 train_time:118069ms step_avg:61.46ms
step:1922/2330 train_time:118133ms step_avg:61.46ms
step:1923/2330 train_time:118193ms step_avg:61.46ms
step:1924/2330 train_time:118258ms step_avg:61.46ms
step:1925/2330 train_time:118318ms step_avg:61.46ms
step:1926/2330 train_time:118381ms step_avg:61.46ms
step:1927/2330 train_time:118442ms step_avg:61.46ms
step:1928/2330 train_time:118506ms step_avg:61.47ms
step:1929/2330 train_time:118567ms step_avg:61.47ms
step:1930/2330 train_time:118629ms step_avg:61.47ms
step:1931/2330 train_time:118689ms step_avg:61.47ms
step:1932/2330 train_time:118754ms step_avg:61.47ms
step:1933/2330 train_time:118815ms step_avg:61.47ms
step:1934/2330 train_time:118878ms step_avg:61.47ms
step:1935/2330 train_time:118938ms step_avg:61.47ms
step:1936/2330 train_time:119002ms step_avg:61.47ms
step:1937/2330 train_time:119063ms step_avg:61.47ms
step:1938/2330 train_time:119125ms step_avg:61.47ms
step:1939/2330 train_time:119185ms step_avg:61.47ms
step:1940/2330 train_time:119251ms step_avg:61.47ms
step:1941/2330 train_time:119310ms step_avg:61.47ms
step:1942/2330 train_time:119374ms step_avg:61.47ms
step:1943/2330 train_time:119435ms step_avg:61.47ms
step:1944/2330 train_time:119498ms step_avg:61.47ms
step:1945/2330 train_time:119559ms step_avg:61.47ms
step:1946/2330 train_time:119623ms step_avg:61.47ms
step:1947/2330 train_time:119682ms step_avg:61.47ms
step:1948/2330 train_time:119746ms step_avg:61.47ms
step:1949/2330 train_time:119805ms step_avg:61.47ms
step:1950/2330 train_time:119870ms step_avg:61.47ms
step:1951/2330 train_time:119930ms step_avg:61.47ms
step:1952/2330 train_time:119994ms step_avg:61.47ms
step:1953/2330 train_time:120054ms step_avg:61.47ms
step:1954/2330 train_time:120118ms step_avg:61.47ms
step:1955/2330 train_time:120179ms step_avg:61.47ms
step:1956/2330 train_time:120243ms step_avg:61.47ms
step:1957/2330 train_time:120303ms step_avg:61.47ms
step:1958/2330 train_time:120366ms step_avg:61.47ms
step:1959/2330 train_time:120427ms step_avg:61.47ms
step:1960/2330 train_time:120491ms step_avg:61.47ms
step:1961/2330 train_time:120551ms step_avg:61.47ms
step:1962/2330 train_time:120614ms step_avg:61.48ms
step:1963/2330 train_time:120676ms step_avg:61.48ms
step:1964/2330 train_time:120740ms step_avg:61.48ms
step:1965/2330 train_time:120801ms step_avg:61.48ms
step:1966/2330 train_time:120865ms step_avg:61.48ms
step:1967/2330 train_time:120925ms step_avg:61.48ms
step:1968/2330 train_time:120988ms step_avg:61.48ms
step:1969/2330 train_time:121048ms step_avg:61.48ms
step:1970/2330 train_time:121112ms step_avg:61.48ms
step:1971/2330 train_time:121172ms step_avg:61.48ms
step:1972/2330 train_time:121236ms step_avg:61.48ms
step:1973/2330 train_time:121297ms step_avg:61.48ms
step:1974/2330 train_time:121362ms step_avg:61.48ms
step:1975/2330 train_time:121422ms step_avg:61.48ms
step:1976/2330 train_time:121485ms step_avg:61.48ms
step:1977/2330 train_time:121545ms step_avg:61.48ms
step:1978/2330 train_time:121608ms step_avg:61.48ms
step:1979/2330 train_time:121669ms step_avg:61.48ms
step:1980/2330 train_time:121734ms step_avg:61.48ms
step:1981/2330 train_time:121795ms step_avg:61.48ms
step:1982/2330 train_time:121860ms step_avg:61.48ms
step:1983/2330 train_time:121920ms step_avg:61.48ms
step:1984/2330 train_time:121983ms step_avg:61.48ms
step:1985/2330 train_time:122044ms step_avg:61.48ms
step:1986/2330 train_time:122107ms step_avg:61.48ms
step:1987/2330 train_time:122167ms step_avg:61.48ms
step:1988/2330 train_time:122230ms step_avg:61.48ms
step:1989/2330 train_time:122290ms step_avg:61.48ms
step:1990/2330 train_time:122354ms step_avg:61.48ms
step:1991/2330 train_time:122415ms step_avg:61.48ms
step:1992/2330 train_time:122479ms step_avg:61.49ms
step:1993/2330 train_time:122540ms step_avg:61.49ms
step:1994/2330 train_time:122604ms step_avg:61.49ms
step:1995/2330 train_time:122664ms step_avg:61.49ms
step:1996/2330 train_time:122728ms step_avg:61.49ms
step:1997/2330 train_time:122789ms step_avg:61.49ms
step:1998/2330 train_time:122853ms step_avg:61.49ms
step:1999/2330 train_time:122913ms step_avg:61.49ms
step:2000/2330 train_time:122978ms step_avg:61.49ms
step:2000/2330 val_loss:3.7368 train_time:123052ms step_avg:61.53ms
step:2001/2330 train_time:123074ms step_avg:61.51ms
step:2002/2330 train_time:123105ms step_avg:61.49ms
step:2003/2330 train_time:123168ms step_avg:61.49ms
step:2004/2330 train_time:123235ms step_avg:61.49ms
step:2005/2330 train_time:123297ms step_avg:61.49ms
step:2006/2330 train_time:123361ms step_avg:61.50ms
step:2007/2330 train_time:123421ms step_avg:61.50ms
step:2008/2330 train_time:123484ms step_avg:61.50ms
step:2009/2330 train_time:123544ms step_avg:61.50ms
step:2010/2330 train_time:123607ms step_avg:61.50ms
step:2011/2330 train_time:123666ms step_avg:61.49ms
step:2012/2330 train_time:123729ms step_avg:61.50ms
step:2013/2330 train_time:123789ms step_avg:61.49ms
step:2014/2330 train_time:123852ms step_avg:61.50ms
step:2015/2330 train_time:123912ms step_avg:61.49ms
step:2016/2330 train_time:123974ms step_avg:61.50ms
step:2017/2330 train_time:124035ms step_avg:61.49ms
step:2018/2330 train_time:124100ms step_avg:61.50ms
step:2019/2330 train_time:124161ms step_avg:61.50ms
step:2020/2330 train_time:124227ms step_avg:61.50ms
step:2021/2330 train_time:124289ms step_avg:61.50ms
step:2022/2330 train_time:124352ms step_avg:61.50ms
step:2023/2330 train_time:124414ms step_avg:61.50ms
step:2024/2330 train_time:124478ms step_avg:61.50ms
step:2025/2330 train_time:124538ms step_avg:61.50ms
step:2026/2330 train_time:124602ms step_avg:61.50ms
step:2027/2330 train_time:124662ms step_avg:61.50ms
step:2028/2330 train_time:124724ms step_avg:61.50ms
step:2029/2330 train_time:124784ms step_avg:61.50ms
step:2030/2330 train_time:124847ms step_avg:61.50ms
step:2031/2330 train_time:124907ms step_avg:61.50ms
step:2032/2330 train_time:124970ms step_avg:61.50ms
step:2033/2330 train_time:125030ms step_avg:61.50ms
step:2034/2330 train_time:125095ms step_avg:61.50ms
step:2035/2330 train_time:125156ms step_avg:61.50ms
step:2036/2330 train_time:125221ms step_avg:61.50ms
step:2037/2330 train_time:125282ms step_avg:61.50ms
step:2038/2330 train_time:125346ms step_avg:61.50ms
step:2039/2330 train_time:125406ms step_avg:61.50ms
step:2040/2330 train_time:125471ms step_avg:61.51ms
step:2041/2330 train_time:125531ms step_avg:61.50ms
step:2042/2330 train_time:125596ms step_avg:61.51ms
step:2043/2330 train_time:125657ms step_avg:61.51ms
step:2044/2330 train_time:125721ms step_avg:61.51ms
step:2045/2330 train_time:125782ms step_avg:61.51ms
step:2046/2330 train_time:125844ms step_avg:61.51ms
step:2047/2330 train_time:125903ms step_avg:61.51ms
step:2048/2330 train_time:125966ms step_avg:61.51ms
step:2049/2330 train_time:126026ms step_avg:61.51ms
step:2050/2330 train_time:126089ms step_avg:61.51ms
step:2051/2330 train_time:126150ms step_avg:61.51ms
step:2052/2330 train_time:126216ms step_avg:61.51ms
step:2053/2330 train_time:126279ms step_avg:61.51ms
step:2054/2330 train_time:126341ms step_avg:61.51ms
step:2055/2330 train_time:126401ms step_avg:61.51ms
step:2056/2330 train_time:126465ms step_avg:61.51ms
step:2057/2330 train_time:126526ms step_avg:61.51ms
step:2058/2330 train_time:126590ms step_avg:61.51ms
step:2059/2330 train_time:126651ms step_avg:61.51ms
step:2060/2330 train_time:126715ms step_avg:61.51ms
step:2061/2330 train_time:126777ms step_avg:61.51ms
step:2062/2330 train_time:126840ms step_avg:61.51ms
step:2063/2330 train_time:126901ms step_avg:61.51ms
step:2064/2330 train_time:126963ms step_avg:61.51ms
step:2065/2330 train_time:127023ms step_avg:61.51ms
step:2066/2330 train_time:127087ms step_avg:61.51ms
step:2067/2330 train_time:127147ms step_avg:61.51ms
step:2068/2330 train_time:127211ms step_avg:61.51ms
step:2069/2330 train_time:127272ms step_avg:61.51ms
step:2070/2330 train_time:127336ms step_avg:61.51ms
step:2071/2330 train_time:127398ms step_avg:61.52ms
step:2072/2330 train_time:127461ms step_avg:61.52ms
step:2073/2330 train_time:127521ms step_avg:61.52ms
step:2074/2330 train_time:127585ms step_avg:61.52ms
step:2075/2330 train_time:127645ms step_avg:61.52ms
step:2076/2330 train_time:127709ms step_avg:61.52ms
step:2077/2330 train_time:127769ms step_avg:61.52ms
step:2078/2330 train_time:127833ms step_avg:61.52ms
step:2079/2330 train_time:127894ms step_avg:61.52ms
step:2080/2330 train_time:127958ms step_avg:61.52ms
step:2081/2330 train_time:128018ms step_avg:61.52ms
step:2082/2330 train_time:128082ms step_avg:61.52ms
step:2083/2330 train_time:128142ms step_avg:61.52ms
step:2084/2330 train_time:128204ms step_avg:61.52ms
step:2085/2330 train_time:128265ms step_avg:61.52ms
step:2086/2330 train_time:128330ms step_avg:61.52ms
step:2087/2330 train_time:128390ms step_avg:61.52ms
step:2088/2330 train_time:128454ms step_avg:61.52ms
step:2089/2330 train_time:128516ms step_avg:61.52ms
step:2090/2330 train_time:128581ms step_avg:61.52ms
step:2091/2330 train_time:128640ms step_avg:61.52ms
step:2092/2330 train_time:128703ms step_avg:61.52ms
step:2093/2330 train_time:128763ms step_avg:61.52ms
step:2094/2330 train_time:128829ms step_avg:61.52ms
step:2095/2330 train_time:128889ms step_avg:61.52ms
step:2096/2330 train_time:128952ms step_avg:61.52ms
step:2097/2330 train_time:129013ms step_avg:61.52ms
step:2098/2330 train_time:129078ms step_avg:61.52ms
step:2099/2330 train_time:129138ms step_avg:61.52ms
step:2100/2330 train_time:129201ms step_avg:61.52ms
step:2101/2330 train_time:129262ms step_avg:61.52ms
step:2102/2330 train_time:129325ms step_avg:61.52ms
step:2103/2330 train_time:129386ms step_avg:61.52ms
step:2104/2330 train_time:129451ms step_avg:61.53ms
step:2105/2330 train_time:129511ms step_avg:61.53ms
step:2106/2330 train_time:129575ms step_avg:61.53ms
step:2107/2330 train_time:129635ms step_avg:61.53ms
step:2108/2330 train_time:129700ms step_avg:61.53ms
step:2109/2330 train_time:129760ms step_avg:61.53ms
step:2110/2330 train_time:129823ms step_avg:61.53ms
step:2111/2330 train_time:129883ms step_avg:61.53ms
step:2112/2330 train_time:129946ms step_avg:61.53ms
step:2113/2330 train_time:130007ms step_avg:61.53ms
step:2114/2330 train_time:130071ms step_avg:61.53ms
step:2115/2330 train_time:130132ms step_avg:61.53ms
step:2116/2330 train_time:130196ms step_avg:61.53ms
step:2117/2330 train_time:130257ms step_avg:61.53ms
step:2118/2330 train_time:130321ms step_avg:61.53ms
step:2119/2330 train_time:130381ms step_avg:61.53ms
step:2120/2330 train_time:130444ms step_avg:61.53ms
step:2121/2330 train_time:130504ms step_avg:61.53ms
step:2122/2330 train_time:130567ms step_avg:61.53ms
step:2123/2330 train_time:130628ms step_avg:61.53ms
step:2124/2330 train_time:130692ms step_avg:61.53ms
step:2125/2330 train_time:130752ms step_avg:61.53ms
step:2126/2330 train_time:130816ms step_avg:61.53ms
step:2127/2330 train_time:130878ms step_avg:61.53ms
step:2128/2330 train_time:130940ms step_avg:61.53ms
step:2129/2330 train_time:131001ms step_avg:61.53ms
step:2130/2330 train_time:131065ms step_avg:61.53ms
step:2131/2330 train_time:131126ms step_avg:61.53ms
step:2132/2330 train_time:131191ms step_avg:61.53ms
step:2133/2330 train_time:131251ms step_avg:61.53ms
step:2134/2330 train_time:131315ms step_avg:61.53ms
step:2135/2330 train_time:131376ms step_avg:61.53ms
step:2136/2330 train_time:131439ms step_avg:61.54ms
step:2137/2330 train_time:131500ms step_avg:61.53ms
step:2138/2330 train_time:131562ms step_avg:61.54ms
step:2139/2330 train_time:131622ms step_avg:61.53ms
step:2140/2330 train_time:131686ms step_avg:61.54ms
step:2141/2330 train_time:131747ms step_avg:61.54ms
step:2142/2330 train_time:131811ms step_avg:61.54ms
step:2143/2330 train_time:131871ms step_avg:61.54ms
step:2144/2330 train_time:131935ms step_avg:61.54ms
step:2145/2330 train_time:131995ms step_avg:61.54ms
step:2146/2330 train_time:132059ms step_avg:61.54ms
step:2147/2330 train_time:132119ms step_avg:61.54ms
step:2148/2330 train_time:132183ms step_avg:61.54ms
step:2149/2330 train_time:132243ms step_avg:61.54ms
step:2150/2330 train_time:132306ms step_avg:61.54ms
step:2151/2330 train_time:132367ms step_avg:61.54ms
step:2152/2330 train_time:132431ms step_avg:61.54ms
step:2153/2330 train_time:132492ms step_avg:61.54ms
step:2154/2330 train_time:132556ms step_avg:61.54ms
step:2155/2330 train_time:132616ms step_avg:61.54ms
step:2156/2330 train_time:132681ms step_avg:61.54ms
step:2157/2330 train_time:132740ms step_avg:61.54ms
step:2158/2330 train_time:132803ms step_avg:61.54ms
step:2159/2330 train_time:132864ms step_avg:61.54ms
step:2160/2330 train_time:132928ms step_avg:61.54ms
step:2161/2330 train_time:132988ms step_avg:61.54ms
step:2162/2330 train_time:133052ms step_avg:61.54ms
step:2163/2330 train_time:133113ms step_avg:61.54ms
step:2164/2330 train_time:133178ms step_avg:61.54ms
step:2165/2330 train_time:133238ms step_avg:61.54ms
step:2166/2330 train_time:133301ms step_avg:61.54ms
step:2167/2330 train_time:133362ms step_avg:61.54ms
step:2168/2330 train_time:133425ms step_avg:61.54ms
step:2169/2330 train_time:133487ms step_avg:61.54ms
step:2170/2330 train_time:133550ms step_avg:61.54ms
step:2171/2330 train_time:133611ms step_avg:61.54ms
step:2172/2330 train_time:133675ms step_avg:61.54ms
step:2173/2330 train_time:133737ms step_avg:61.54ms
step:2174/2330 train_time:133800ms step_avg:61.55ms
step:2175/2330 train_time:133861ms step_avg:61.55ms
step:2176/2330 train_time:133923ms step_avg:61.55ms
step:2177/2330 train_time:133985ms step_avg:61.55ms
step:2178/2330 train_time:134048ms step_avg:61.55ms
step:2179/2330 train_time:134108ms step_avg:61.55ms
step:2180/2330 train_time:134172ms step_avg:61.55ms
step:2181/2330 train_time:134234ms step_avg:61.55ms
step:2182/2330 train_time:134298ms step_avg:61.55ms
step:2183/2330 train_time:134359ms step_avg:61.55ms
step:2184/2330 train_time:134423ms step_avg:61.55ms
step:2185/2330 train_time:134484ms step_avg:61.55ms
step:2186/2330 train_time:134547ms step_avg:61.55ms
step:2187/2330 train_time:134607ms step_avg:61.55ms
step:2188/2330 train_time:134671ms step_avg:61.55ms
step:2189/2330 train_time:134731ms step_avg:61.55ms
step:2190/2330 train_time:134796ms step_avg:61.55ms
step:2191/2330 train_time:134856ms step_avg:61.55ms
step:2192/2330 train_time:134920ms step_avg:61.55ms
step:2193/2330 train_time:134980ms step_avg:61.55ms
step:2194/2330 train_time:135043ms step_avg:61.55ms
step:2195/2330 train_time:135101ms step_avg:61.55ms
step:2196/2330 train_time:135166ms step_avg:61.55ms
step:2197/2330 train_time:135226ms step_avg:61.55ms
step:2198/2330 train_time:135291ms step_avg:61.55ms
step:2199/2330 train_time:135352ms step_avg:61.55ms
step:2200/2330 train_time:135416ms step_avg:61.55ms
step:2201/2330 train_time:135479ms step_avg:61.55ms
step:2202/2330 train_time:135542ms step_avg:61.55ms
step:2203/2330 train_time:135601ms step_avg:61.55ms
step:2204/2330 train_time:135665ms step_avg:61.55ms
step:2205/2330 train_time:135725ms step_avg:61.55ms
step:2206/2330 train_time:135789ms step_avg:61.55ms
step:2207/2330 train_time:135849ms step_avg:61.55ms
step:2208/2330 train_time:135914ms step_avg:61.56ms
step:2209/2330 train_time:135975ms step_avg:61.55ms
step:2210/2330 train_time:136039ms step_avg:61.56ms
step:2211/2330 train_time:136100ms step_avg:61.56ms
step:2212/2330 train_time:136164ms step_avg:61.56ms
step:2213/2330 train_time:136223ms step_avg:61.56ms
step:2214/2330 train_time:136288ms step_avg:61.56ms
step:2215/2330 train_time:136347ms step_avg:61.56ms
step:2216/2330 train_time:136411ms step_avg:61.56ms
step:2217/2330 train_time:136473ms step_avg:61.56ms
step:2218/2330 train_time:136537ms step_avg:61.56ms
step:2219/2330 train_time:136597ms step_avg:61.56ms
step:2220/2330 train_time:136661ms step_avg:61.56ms
step:2221/2330 train_time:136722ms step_avg:61.56ms
step:2222/2330 train_time:136785ms step_avg:61.56ms
step:2223/2330 train_time:136844ms step_avg:61.56ms
step:2224/2330 train_time:136908ms step_avg:61.56ms
step:2225/2330 train_time:136968ms step_avg:61.56ms
step:2226/2330 train_time:137032ms step_avg:61.56ms
step:2227/2330 train_time:137093ms step_avg:61.56ms
step:2228/2330 train_time:137157ms step_avg:61.56ms
step:2229/2330 train_time:137217ms step_avg:61.56ms
step:2230/2330 train_time:137282ms step_avg:61.56ms
step:2231/2330 train_time:137342ms step_avg:61.56ms
step:2232/2330 train_time:137405ms step_avg:61.56ms
step:2233/2330 train_time:137466ms step_avg:61.56ms
step:2234/2330 train_time:137529ms step_avg:61.56ms
step:2235/2330 train_time:137590ms step_avg:61.56ms
step:2236/2330 train_time:137654ms step_avg:61.56ms
step:2237/2330 train_time:137716ms step_avg:61.56ms
step:2238/2330 train_time:137781ms step_avg:61.56ms
step:2239/2330 train_time:137840ms step_avg:61.56ms
step:2240/2330 train_time:137903ms step_avg:61.56ms
step:2241/2330 train_time:137963ms step_avg:61.56ms
step:2242/2330 train_time:138026ms step_avg:61.56ms
step:2243/2330 train_time:138087ms step_avg:61.56ms
step:2244/2330 train_time:138151ms step_avg:61.56ms
step:2245/2330 train_time:138212ms step_avg:61.56ms
step:2246/2330 train_time:138276ms step_avg:61.57ms
step:2247/2330 train_time:138337ms step_avg:61.57ms
step:2248/2330 train_time:138401ms step_avg:61.57ms
step:2249/2330 train_time:138460ms step_avg:61.57ms
step:2250/2330 train_time:138524ms step_avg:61.57ms
step:2250/2330 val_loss:3.6907 train_time:138599ms step_avg:61.60ms
step:2251/2330 train_time:138622ms step_avg:61.58ms
step:2252/2330 train_time:138651ms step_avg:61.57ms
step:2253/2330 train_time:138718ms step_avg:61.57ms
step:2254/2330 train_time:138786ms step_avg:61.57ms
step:2255/2330 train_time:138845ms step_avg:61.57ms
step:2256/2330 train_time:138909ms step_avg:61.57ms
step:2257/2330 train_time:138969ms step_avg:61.57ms
step:2258/2330 train_time:139032ms step_avg:61.57ms
step:2259/2330 train_time:139092ms step_avg:61.57ms
step:2260/2330 train_time:139155ms step_avg:61.57ms
step:2261/2330 train_time:139215ms step_avg:61.57ms
step:2262/2330 train_time:139277ms step_avg:61.57ms
step:2263/2330 train_time:139337ms step_avg:61.57ms
step:2264/2330 train_time:139399ms step_avg:61.57ms
step:2265/2330 train_time:139458ms step_avg:61.57ms
step:2266/2330 train_time:139521ms step_avg:61.57ms
step:2267/2330 train_time:139583ms step_avg:61.57ms
step:2268/2330 train_time:139649ms step_avg:61.57ms
step:2269/2330 train_time:139712ms step_avg:61.57ms
step:2270/2330 train_time:139776ms step_avg:61.58ms
step:2271/2330 train_time:139838ms step_avg:61.58ms
step:2272/2330 train_time:139902ms step_avg:61.58ms
step:2273/2330 train_time:139961ms step_avg:61.58ms
step:2274/2330 train_time:140026ms step_avg:61.58ms
step:2275/2330 train_time:140085ms step_avg:61.58ms
step:2276/2330 train_time:140148ms step_avg:61.58ms
step:2277/2330 train_time:140208ms step_avg:61.58ms
step:2278/2330 train_time:140271ms step_avg:61.58ms
step:2279/2330 train_time:140331ms step_avg:61.58ms
step:2280/2330 train_time:140394ms step_avg:61.58ms
step:2281/2330 train_time:140454ms step_avg:61.58ms
step:2282/2330 train_time:140518ms step_avg:61.58ms
step:2283/2330 train_time:140578ms step_avg:61.58ms
step:2284/2330 train_time:140643ms step_avg:61.58ms
step:2285/2330 train_time:140704ms step_avg:61.58ms
step:2286/2330 train_time:140768ms step_avg:61.58ms
step:2287/2330 train_time:140829ms step_avg:61.58ms
step:2288/2330 train_time:140896ms step_avg:61.58ms
step:2289/2330 train_time:140957ms step_avg:61.58ms
step:2290/2330 train_time:141020ms step_avg:61.58ms
step:2291/2330 train_time:141081ms step_avg:61.58ms
step:2292/2330 train_time:141144ms step_avg:61.58ms
step:2293/2330 train_time:141204ms step_avg:61.58ms
step:2294/2330 train_time:141267ms step_avg:61.58ms
step:2295/2330 train_time:141328ms step_avg:61.58ms
step:2296/2330 train_time:141392ms step_avg:61.58ms
step:2297/2330 train_time:141452ms step_avg:61.58ms
step:2298/2330 train_time:141515ms step_avg:61.58ms
step:2299/2330 train_time:141576ms step_avg:61.58ms
step:2300/2330 train_time:141639ms step_avg:61.58ms
step:2301/2330 train_time:141700ms step_avg:61.58ms
step:2302/2330 train_time:141764ms step_avg:61.58ms
step:2303/2330 train_time:141825ms step_avg:61.58ms
step:2304/2330 train_time:141889ms step_avg:61.58ms
step:2305/2330 train_time:141950ms step_avg:61.58ms
step:2306/2330 train_time:142013ms step_avg:61.58ms
step:2307/2330 train_time:142074ms step_avg:61.58ms
step:2308/2330 train_time:142138ms step_avg:61.59ms
step:2309/2330 train_time:142199ms step_avg:61.58ms
step:2310/2330 train_time:142261ms step_avg:61.58ms
step:2311/2330 train_time:142321ms step_avg:61.58ms
step:2312/2330 train_time:142385ms step_avg:61.59ms
step:2313/2330 train_time:142445ms step_avg:61.58ms
step:2314/2330 train_time:142509ms step_avg:61.59ms
step:2315/2330 train_time:142569ms step_avg:61.58ms
step:2316/2330 train_time:142633ms step_avg:61.59ms
step:2317/2330 train_time:142696ms step_avg:61.59ms
step:2318/2330 train_time:142759ms step_avg:61.59ms
step:2319/2330 train_time:142818ms step_avg:61.59ms
step:2320/2330 train_time:142882ms step_avg:61.59ms
step:2321/2330 train_time:142943ms step_avg:61.59ms
step:2322/2330 train_time:143008ms step_avg:61.59ms
step:2323/2330 train_time:143069ms step_avg:61.59ms
step:2324/2330 train_time:143134ms step_avg:61.59ms
step:2325/2330 train_time:143195ms step_avg:61.59ms
step:2326/2330 train_time:143258ms step_avg:61.59ms
step:2327/2330 train_time:143318ms step_avg:61.59ms
step:2328/2330 train_time:143380ms step_avg:61.59ms
step:2329/2330 train_time:143441ms step_avg:61.59ms
step:2330/2330 train_time:143505ms step_avg:61.59ms
step:2330/2330 val_loss:3.6775 train_time:143580ms step_avg:61.62ms
peak memory allocated: 29226 MiB reserved: 44076 MiB
