import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = divide_by_norm(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = divide_by_norm(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_gd_lr3e-1"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=3e-1, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 05:37:12 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   33C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   31C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   31C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:73ms step_avg:72.86ms
step:2/2330 train_time:141ms step_avg:70.56ms
step:3/2330 train_time:155ms step_avg:51.55ms
step:4/2330 train_time:167ms step_avg:41.69ms
step:5/2330 train_time:178ms step_avg:35.52ms
step:6/2330 train_time:202ms step_avg:33.59ms
step:7/2330 train_time:223ms step_avg:31.81ms
step:8/2330 train_time:277ms step_avg:34.57ms
step:9/2330 train_time:299ms step_avg:33.17ms
step:10/2330 train_time:354ms step_avg:35.38ms
step:11/2330 train_time:376ms step_avg:34.17ms
step:12/2330 train_time:432ms step_avg:35.96ms
step:13/2330 train_time:454ms step_avg:34.91ms
step:14/2330 train_time:509ms step_avg:36.35ms
step:15/2330 train_time:530ms step_avg:35.34ms
step:16/2330 train_time:585ms step_avg:36.54ms
step:17/2330 train_time:606ms step_avg:35.65ms
step:18/2330 train_time:661ms step_avg:36.71ms
step:19/2330 train_time:682ms step_avg:35.91ms
step:20/2330 train_time:737ms step_avg:36.84ms
step:21/2330 train_time:759ms step_avg:36.13ms
step:22/2330 train_time:814ms step_avg:37.00ms
step:23/2330 train_time:836ms step_avg:36.35ms
step:24/2330 train_time:890ms step_avg:37.10ms
step:25/2330 train_time:912ms step_avg:36.47ms
step:26/2330 train_time:967ms step_avg:37.18ms
step:27/2330 train_time:991ms step_avg:36.70ms
step:28/2330 train_time:1050ms step_avg:37.51ms
step:29/2330 train_time:1074ms step_avg:37.03ms
step:30/2330 train_time:1132ms step_avg:37.74ms
step:31/2330 train_time:1154ms step_avg:37.24ms
step:32/2330 train_time:1211ms step_avg:37.84ms
step:33/2330 train_time:1232ms step_avg:37.35ms
step:34/2330 train_time:1288ms step_avg:37.88ms
step:35/2330 train_time:1309ms step_avg:37.41ms
step:36/2330 train_time:1364ms step_avg:37.90ms
step:37/2330 train_time:1386ms step_avg:37.45ms
step:38/2330 train_time:1441ms step_avg:37.91ms
step:39/2330 train_time:1462ms step_avg:37.50ms
step:40/2330 train_time:1517ms step_avg:37.92ms
step:41/2330 train_time:1539ms step_avg:37.53ms
step:42/2330 train_time:1594ms step_avg:37.94ms
step:43/2330 train_time:1615ms step_avg:37.56ms
step:44/2330 train_time:1670ms step_avg:37.95ms
step:45/2330 train_time:1691ms step_avg:37.58ms
step:46/2330 train_time:1745ms step_avg:37.94ms
step:47/2330 train_time:1766ms step_avg:37.58ms
step:48/2330 train_time:1821ms step_avg:37.93ms
step:49/2330 train_time:1842ms step_avg:37.59ms
step:50/2330 train_time:1897ms step_avg:37.93ms
step:51/2330 train_time:1919ms step_avg:37.63ms
step:52/2330 train_time:1976ms step_avg:38.00ms
step:53/2330 train_time:2000ms step_avg:37.73ms
step:54/2330 train_time:2056ms step_avg:38.08ms
step:55/2330 train_time:2080ms step_avg:37.81ms
step:56/2330 train_time:2137ms step_avg:38.15ms
step:57/2330 train_time:2160ms step_avg:37.89ms
step:58/2330 train_time:2216ms step_avg:38.20ms
step:59/2330 train_time:2238ms step_avg:37.94ms
step:60/2330 train_time:2294ms step_avg:38.23ms
step:61/2330 train_time:2316ms step_avg:37.97ms
step:62/2330 train_time:2371ms step_avg:38.25ms
step:63/2330 train_time:2393ms step_avg:37.99ms
step:64/2330 train_time:2448ms step_avg:38.25ms
step:65/2330 train_time:2469ms step_avg:37.98ms
step:66/2330 train_time:2525ms step_avg:38.25ms
step:67/2330 train_time:2546ms step_avg:38.00ms
step:68/2330 train_time:2601ms step_avg:38.25ms
step:69/2330 train_time:2622ms step_avg:38.01ms
step:70/2330 train_time:2677ms step_avg:38.24ms
step:71/2330 train_time:2699ms step_avg:38.02ms
step:72/2330 train_time:2754ms step_avg:38.25ms
step:73/2330 train_time:2776ms step_avg:38.02ms
step:74/2330 train_time:2831ms step_avg:38.26ms
step:75/2330 train_time:2852ms step_avg:38.03ms
step:76/2330 train_time:2908ms step_avg:38.26ms
step:77/2330 train_time:2930ms step_avg:38.05ms
step:78/2330 train_time:2986ms step_avg:38.28ms
step:79/2330 train_time:3008ms step_avg:38.07ms
step:80/2330 train_time:3064ms step_avg:38.30ms
step:81/2330 train_time:3086ms step_avg:38.10ms
step:82/2330 train_time:3143ms step_avg:38.32ms
step:83/2330 train_time:3165ms step_avg:38.13ms
step:84/2330 train_time:3221ms step_avg:38.34ms
step:85/2330 train_time:3243ms step_avg:38.15ms
step:86/2330 train_time:3298ms step_avg:38.35ms
step:87/2330 train_time:3321ms step_avg:38.17ms
step:88/2330 train_time:3377ms step_avg:38.37ms
step:89/2330 train_time:3399ms step_avg:38.19ms
step:90/2330 train_time:3454ms step_avg:38.38ms
step:91/2330 train_time:3477ms step_avg:38.21ms
step:92/2330 train_time:3532ms step_avg:38.39ms
step:93/2330 train_time:3554ms step_avg:38.21ms
step:94/2330 train_time:3608ms step_avg:38.38ms
step:95/2330 train_time:3629ms step_avg:38.20ms
step:96/2330 train_time:3683ms step_avg:38.37ms
step:97/2330 train_time:3705ms step_avg:38.20ms
step:98/2330 train_time:3760ms step_avg:38.37ms
step:99/2330 train_time:3782ms step_avg:38.20ms
step:100/2330 train_time:3837ms step_avg:38.37ms
step:101/2330 train_time:3859ms step_avg:38.21ms
step:102/2330 train_time:3915ms step_avg:38.38ms
step:103/2330 train_time:3937ms step_avg:38.23ms
step:104/2330 train_time:3993ms step_avg:38.39ms
step:105/2330 train_time:4016ms step_avg:38.25ms
step:106/2330 train_time:4072ms step_avg:38.42ms
step:107/2330 train_time:4094ms step_avg:38.27ms
step:108/2330 train_time:4150ms step_avg:38.43ms
step:109/2330 train_time:4172ms step_avg:38.27ms
step:110/2330 train_time:4228ms step_avg:38.43ms
step:111/2330 train_time:4249ms step_avg:38.28ms
step:112/2330 train_time:4306ms step_avg:38.44ms
step:113/2330 train_time:4327ms step_avg:38.30ms
step:114/2330 train_time:4383ms step_avg:38.45ms
step:115/2330 train_time:4405ms step_avg:38.31ms
step:116/2330 train_time:4461ms step_avg:38.45ms
step:117/2330 train_time:4483ms step_avg:38.32ms
step:118/2330 train_time:4538ms step_avg:38.46ms
step:119/2330 train_time:4560ms step_avg:38.32ms
step:120/2330 train_time:4616ms step_avg:38.46ms
step:121/2330 train_time:4638ms step_avg:38.33ms
step:122/2330 train_time:4693ms step_avg:38.47ms
step:123/2330 train_time:4715ms step_avg:38.33ms
step:124/2330 train_time:4770ms step_avg:38.47ms
step:125/2330 train_time:4792ms step_avg:38.33ms
step:126/2330 train_time:4846ms step_avg:38.46ms
step:127/2330 train_time:4868ms step_avg:38.33ms
step:128/2330 train_time:4924ms step_avg:38.47ms
step:129/2330 train_time:4946ms step_avg:38.34ms
step:130/2330 train_time:5002ms step_avg:38.47ms
step:131/2330 train_time:5024ms step_avg:38.35ms
step:132/2330 train_time:5079ms step_avg:38.48ms
step:133/2330 train_time:5102ms step_avg:38.36ms
step:134/2330 train_time:5157ms step_avg:38.49ms
step:135/2330 train_time:5180ms step_avg:38.37ms
step:136/2330 train_time:5236ms step_avg:38.50ms
step:137/2330 train_time:5258ms step_avg:38.38ms
step:138/2330 train_time:5314ms step_avg:38.51ms
step:139/2330 train_time:5336ms step_avg:38.39ms
step:140/2330 train_time:5393ms step_avg:38.52ms
step:141/2330 train_time:5414ms step_avg:38.40ms
step:142/2330 train_time:5472ms step_avg:38.53ms
step:143/2330 train_time:5493ms step_avg:38.41ms
step:144/2330 train_time:5548ms step_avg:38.53ms
step:145/2330 train_time:5569ms step_avg:38.41ms
step:146/2330 train_time:5625ms step_avg:38.52ms
step:147/2330 train_time:5646ms step_avg:38.41ms
step:148/2330 train_time:5701ms step_avg:38.52ms
step:149/2330 train_time:5723ms step_avg:38.41ms
step:150/2330 train_time:5778ms step_avg:38.52ms
step:151/2330 train_time:5800ms step_avg:38.41ms
step:152/2330 train_time:5856ms step_avg:38.52ms
step:153/2330 train_time:5878ms step_avg:38.42ms
step:154/2330 train_time:5933ms step_avg:38.53ms
step:155/2330 train_time:5956ms step_avg:38.42ms
step:156/2330 train_time:6011ms step_avg:38.53ms
step:157/2330 train_time:6034ms step_avg:38.43ms
step:158/2330 train_time:6089ms step_avg:38.54ms
step:159/2330 train_time:6111ms step_avg:38.44ms
step:160/2330 train_time:6167ms step_avg:38.54ms
step:161/2330 train_time:6189ms step_avg:38.44ms
step:162/2330 train_time:6245ms step_avg:38.55ms
step:163/2330 train_time:6267ms step_avg:38.45ms
step:164/2330 train_time:6323ms step_avg:38.55ms
step:165/2330 train_time:6345ms step_avg:38.45ms
step:166/2330 train_time:6401ms step_avg:38.56ms
step:167/2330 train_time:6423ms step_avg:38.46ms
step:168/2330 train_time:6479ms step_avg:38.56ms
step:169/2330 train_time:6501ms step_avg:38.47ms
step:170/2330 train_time:6557ms step_avg:38.57ms
step:171/2330 train_time:6578ms step_avg:38.47ms
step:172/2330 train_time:6634ms step_avg:38.57ms
step:173/2330 train_time:6656ms step_avg:38.47ms
step:174/2330 train_time:6711ms step_avg:38.57ms
step:175/2330 train_time:6733ms step_avg:38.48ms
step:176/2330 train_time:6788ms step_avg:38.57ms
step:177/2330 train_time:6810ms step_avg:38.47ms
step:178/2330 train_time:6866ms step_avg:38.57ms
step:179/2330 train_time:6887ms step_avg:38.48ms
step:180/2330 train_time:6943ms step_avg:38.57ms
step:181/2330 train_time:6965ms step_avg:38.48ms
step:182/2330 train_time:7020ms step_avg:38.57ms
step:183/2330 train_time:7042ms step_avg:38.48ms
step:184/2330 train_time:7098ms step_avg:38.57ms
step:185/2330 train_time:7120ms step_avg:38.49ms
step:186/2330 train_time:7176ms step_avg:38.58ms
step:187/2330 train_time:7199ms step_avg:38.50ms
step:188/2330 train_time:7256ms step_avg:38.59ms
step:189/2330 train_time:7278ms step_avg:38.51ms
step:190/2330 train_time:7334ms step_avg:38.60ms
step:191/2330 train_time:7357ms step_avg:38.52ms
step:192/2330 train_time:7412ms step_avg:38.61ms
step:193/2330 train_time:7434ms step_avg:38.52ms
step:194/2330 train_time:7490ms step_avg:38.61ms
step:195/2330 train_time:7512ms step_avg:38.52ms
step:196/2330 train_time:7568ms step_avg:38.61ms
step:197/2330 train_time:7589ms step_avg:38.52ms
step:198/2330 train_time:7645ms step_avg:38.61ms
step:199/2330 train_time:7667ms step_avg:38.53ms
step:200/2330 train_time:7723ms step_avg:38.61ms
step:201/2330 train_time:7744ms step_avg:38.53ms
step:202/2330 train_time:7799ms step_avg:38.61ms
step:203/2330 train_time:7822ms step_avg:38.53ms
step:204/2330 train_time:7877ms step_avg:38.61ms
step:205/2330 train_time:7900ms step_avg:38.54ms
step:206/2330 train_time:7955ms step_avg:38.62ms
step:207/2330 train_time:7977ms step_avg:38.54ms
step:208/2330 train_time:8034ms step_avg:38.62ms
step:209/2330 train_time:8056ms step_avg:38.55ms
step:210/2330 train_time:8112ms step_avg:38.63ms
step:211/2330 train_time:8133ms step_avg:38.55ms
step:212/2330 train_time:8189ms step_avg:38.63ms
step:213/2330 train_time:8212ms step_avg:38.55ms
step:214/2330 train_time:8268ms step_avg:38.63ms
step:215/2330 train_time:8290ms step_avg:38.56ms
step:216/2330 train_time:8345ms step_avg:38.64ms
step:217/2330 train_time:8368ms step_avg:38.56ms
step:218/2330 train_time:8424ms step_avg:38.64ms
step:219/2330 train_time:8446ms step_avg:38.57ms
step:220/2330 train_time:8502ms step_avg:38.64ms
step:221/2330 train_time:8524ms step_avg:38.57ms
step:222/2330 train_time:8580ms step_avg:38.65ms
step:223/2330 train_time:8602ms step_avg:38.57ms
step:224/2330 train_time:8657ms step_avg:38.65ms
step:225/2330 train_time:8680ms step_avg:38.58ms
step:226/2330 train_time:8736ms step_avg:38.65ms
step:227/2330 train_time:8758ms step_avg:38.58ms
step:228/2330 train_time:8813ms step_avg:38.66ms
step:229/2330 train_time:8835ms step_avg:38.58ms
step:230/2330 train_time:8890ms step_avg:38.65ms
step:231/2330 train_time:8913ms step_avg:38.58ms
step:232/2330 train_time:8968ms step_avg:38.66ms
step:233/2330 train_time:8990ms step_avg:38.58ms
step:234/2330 train_time:9046ms step_avg:38.66ms
step:235/2330 train_time:9067ms step_avg:38.58ms
step:236/2330 train_time:9123ms step_avg:38.66ms
step:237/2330 train_time:9145ms step_avg:38.59ms
step:238/2330 train_time:9201ms step_avg:38.66ms
step:239/2330 train_time:9223ms step_avg:38.59ms
step:240/2330 train_time:9278ms step_avg:38.66ms
step:241/2330 train_time:9301ms step_avg:38.60ms
step:242/2330 train_time:9357ms step_avg:38.67ms
step:243/2330 train_time:9380ms step_avg:38.60ms
step:244/2330 train_time:9435ms step_avg:38.67ms
step:245/2330 train_time:9458ms step_avg:38.61ms
step:246/2330 train_time:9514ms step_avg:38.68ms
step:247/2330 train_time:9536ms step_avg:38.61ms
step:248/2330 train_time:9592ms step_avg:38.68ms
step:249/2330 train_time:9614ms step_avg:38.61ms
step:250/2330 train_time:9671ms step_avg:38.68ms
step:250/2330 val_loss:5.6045 train_time:9764ms step_avg:39.06ms
step:251/2330 train_time:9776ms step_avg:38.95ms
step:252/2330 train_time:9787ms step_avg:38.84ms
step:253/2330 train_time:9798ms step_avg:38.73ms
step:254/2330 train_time:9828ms step_avg:38.69ms
step:255/2330 train_time:9850ms step_avg:38.63ms
step:256/2330 train_time:9904ms step_avg:38.69ms
step:257/2330 train_time:9926ms step_avg:38.62ms
step:258/2330 train_time:9982ms step_avg:38.69ms
step:259/2330 train_time:10004ms step_avg:38.62ms
step:260/2330 train_time:10059ms step_avg:38.69ms
step:261/2330 train_time:10085ms step_avg:38.64ms
step:262/2330 train_time:10145ms step_avg:38.72ms
step:263/2330 train_time:10170ms step_avg:38.67ms
step:264/2330 train_time:10227ms step_avg:38.74ms
step:265/2330 train_time:10250ms step_avg:38.68ms
step:266/2330 train_time:10305ms step_avg:38.74ms
step:267/2330 train_time:10328ms step_avg:38.68ms
step:268/2330 train_time:10384ms step_avg:38.75ms
step:269/2330 train_time:10406ms step_avg:38.69ms
step:270/2330 train_time:10462ms step_avg:38.75ms
step:271/2330 train_time:10485ms step_avg:38.69ms
step:272/2330 train_time:10540ms step_avg:38.75ms
step:273/2330 train_time:10562ms step_avg:38.69ms
step:274/2330 train_time:10617ms step_avg:38.75ms
step:275/2330 train_time:10639ms step_avg:38.69ms
step:276/2330 train_time:10694ms step_avg:38.75ms
step:277/2330 train_time:10716ms step_avg:38.69ms
step:278/2330 train_time:10772ms step_avg:38.75ms
step:279/2330 train_time:10793ms step_avg:38.69ms
step:280/2330 train_time:10849ms step_avg:38.75ms
step:281/2330 train_time:10871ms step_avg:38.69ms
step:282/2330 train_time:10926ms step_avg:38.74ms
step:283/2330 train_time:10948ms step_avg:38.68ms
step:284/2330 train_time:11003ms step_avg:38.74ms
step:285/2330 train_time:11028ms step_avg:38.69ms
step:286/2330 train_time:11085ms step_avg:38.76ms
step:287/2330 train_time:11108ms step_avg:38.71ms
step:288/2330 train_time:11165ms step_avg:38.77ms
step:289/2330 train_time:11188ms step_avg:38.71ms
step:290/2330 train_time:11244ms step_avg:38.77ms
step:291/2330 train_time:11268ms step_avg:38.72ms
step:292/2330 train_time:11323ms step_avg:38.78ms
step:293/2330 train_time:11346ms step_avg:38.72ms
step:294/2330 train_time:11402ms step_avg:38.78ms
step:295/2330 train_time:11425ms step_avg:38.73ms
step:296/2330 train_time:11480ms step_avg:38.79ms
step:297/2330 train_time:11503ms step_avg:38.73ms
step:298/2330 train_time:11559ms step_avg:38.79ms
step:299/2330 train_time:11581ms step_avg:38.73ms
step:300/2330 train_time:11637ms step_avg:38.79ms
step:301/2330 train_time:11659ms step_avg:38.73ms
step:302/2330 train_time:11714ms step_avg:38.79ms
step:303/2330 train_time:11735ms step_avg:38.73ms
step:304/2330 train_time:11790ms step_avg:38.78ms
step:305/2330 train_time:11812ms step_avg:38.73ms
step:306/2330 train_time:11867ms step_avg:38.78ms
step:307/2330 train_time:11889ms step_avg:38.73ms
step:308/2330 train_time:11944ms step_avg:38.78ms
step:309/2330 train_time:11967ms step_avg:38.73ms
step:310/2330 train_time:12023ms step_avg:38.79ms
step:311/2330 train_time:12047ms step_avg:38.74ms
step:312/2330 train_time:12103ms step_avg:38.79ms
step:313/2330 train_time:12127ms step_avg:38.74ms
step:314/2330 train_time:12184ms step_avg:38.80ms
step:315/2330 train_time:12207ms step_avg:38.75ms
step:316/2330 train_time:12263ms step_avg:38.81ms
step:317/2330 train_time:12286ms step_avg:38.76ms
step:318/2330 train_time:12343ms step_avg:38.81ms
step:319/2330 train_time:12366ms step_avg:38.76ms
step:320/2330 train_time:12422ms step_avg:38.82ms
step:321/2330 train_time:12445ms step_avg:38.77ms
step:322/2330 train_time:12500ms step_avg:38.82ms
step:323/2330 train_time:12523ms step_avg:38.77ms
step:324/2330 train_time:12579ms step_avg:38.82ms
step:325/2330 train_time:12602ms step_avg:38.77ms
step:326/2330 train_time:12657ms step_avg:38.82ms
step:327/2330 train_time:12679ms step_avg:38.77ms
step:328/2330 train_time:12734ms step_avg:38.82ms
step:329/2330 train_time:12756ms step_avg:38.77ms
step:330/2330 train_time:12811ms step_avg:38.82ms
step:331/2330 train_time:12833ms step_avg:38.77ms
step:332/2330 train_time:12889ms step_avg:38.82ms
step:333/2330 train_time:12911ms step_avg:38.77ms
step:334/2330 train_time:12967ms step_avg:38.82ms
step:335/2330 train_time:12989ms step_avg:38.77ms
step:336/2330 train_time:13044ms step_avg:38.82ms
step:337/2330 train_time:13068ms step_avg:38.78ms
step:338/2330 train_time:13124ms step_avg:38.83ms
step:339/2330 train_time:13146ms step_avg:38.78ms
step:340/2330 train_time:13203ms step_avg:38.83ms
step:341/2330 train_time:13226ms step_avg:38.79ms
step:342/2330 train_time:13283ms step_avg:38.84ms
step:343/2330 train_time:13306ms step_avg:38.79ms
step:344/2330 train_time:13362ms step_avg:38.84ms
step:345/2330 train_time:13386ms step_avg:38.80ms
step:346/2330 train_time:13441ms step_avg:38.85ms
step:347/2330 train_time:13465ms step_avg:38.80ms
step:348/2330 train_time:13521ms step_avg:38.85ms
step:349/2330 train_time:13544ms step_avg:38.81ms
step:350/2330 train_time:13601ms step_avg:38.86ms
step:351/2330 train_time:13623ms step_avg:38.81ms
step:352/2330 train_time:13679ms step_avg:38.86ms
step:353/2330 train_time:13701ms step_avg:38.81ms
step:354/2330 train_time:13757ms step_avg:38.86ms
step:355/2330 train_time:13779ms step_avg:38.81ms
step:356/2330 train_time:13834ms step_avg:38.86ms
step:357/2330 train_time:13855ms step_avg:38.81ms
step:358/2330 train_time:13911ms step_avg:38.86ms
step:359/2330 train_time:13933ms step_avg:38.81ms
step:360/2330 train_time:13989ms step_avg:38.86ms
step:361/2330 train_time:14011ms step_avg:38.81ms
step:362/2330 train_time:14067ms step_avg:38.86ms
step:363/2330 train_time:14088ms step_avg:38.81ms
step:364/2330 train_time:14145ms step_avg:38.86ms
step:365/2330 train_time:14168ms step_avg:38.82ms
step:366/2330 train_time:14224ms step_avg:38.86ms
step:367/2330 train_time:14247ms step_avg:38.82ms
step:368/2330 train_time:14303ms step_avg:38.87ms
step:369/2330 train_time:14326ms step_avg:38.82ms
step:370/2330 train_time:14382ms step_avg:38.87ms
step:371/2330 train_time:14405ms step_avg:38.83ms
step:372/2330 train_time:14461ms step_avg:38.87ms
step:373/2330 train_time:14484ms step_avg:38.83ms
step:374/2330 train_time:14541ms step_avg:38.88ms
step:375/2330 train_time:14564ms step_avg:38.84ms
step:376/2330 train_time:14621ms step_avg:38.88ms
step:377/2330 train_time:14643ms step_avg:38.84ms
step:378/2330 train_time:14699ms step_avg:38.89ms
step:379/2330 train_time:14722ms step_avg:38.84ms
step:380/2330 train_time:14779ms step_avg:38.89ms
step:381/2330 train_time:14801ms step_avg:38.85ms
step:382/2330 train_time:14856ms step_avg:38.89ms
step:383/2330 train_time:14878ms step_avg:38.85ms
step:384/2330 train_time:14933ms step_avg:38.89ms
step:385/2330 train_time:14954ms step_avg:38.84ms
step:386/2330 train_time:15010ms step_avg:38.89ms
step:387/2330 train_time:15032ms step_avg:38.84ms
step:388/2330 train_time:15088ms step_avg:38.89ms
step:389/2330 train_time:15110ms step_avg:38.84ms
step:390/2330 train_time:15166ms step_avg:38.89ms
step:391/2330 train_time:15188ms step_avg:38.84ms
step:392/2330 train_time:15244ms step_avg:38.89ms
step:393/2330 train_time:15267ms step_avg:38.85ms
step:394/2330 train_time:15323ms step_avg:38.89ms
step:395/2330 train_time:15346ms step_avg:38.85ms
step:396/2330 train_time:15402ms step_avg:38.89ms
step:397/2330 train_time:15426ms step_avg:38.86ms
step:398/2330 train_time:15482ms step_avg:38.90ms
step:399/2330 train_time:15504ms step_avg:38.86ms
step:400/2330 train_time:15560ms step_avg:38.90ms
step:401/2330 train_time:15582ms step_avg:38.86ms
step:402/2330 train_time:15638ms step_avg:38.90ms
step:403/2330 train_time:15662ms step_avg:38.86ms
step:404/2330 train_time:15718ms step_avg:38.91ms
step:405/2330 train_time:15741ms step_avg:38.87ms
step:406/2330 train_time:15797ms step_avg:38.91ms
step:407/2330 train_time:15819ms step_avg:38.87ms
step:408/2330 train_time:15875ms step_avg:38.91ms
step:409/2330 train_time:15897ms step_avg:38.87ms
step:410/2330 train_time:15952ms step_avg:38.91ms
step:411/2330 train_time:15974ms step_avg:38.87ms
step:412/2330 train_time:16030ms step_avg:38.91ms
step:413/2330 train_time:16051ms step_avg:38.87ms
step:414/2330 train_time:16108ms step_avg:38.91ms
step:415/2330 train_time:16130ms step_avg:38.87ms
step:416/2330 train_time:16186ms step_avg:38.91ms
step:417/2330 train_time:16208ms step_avg:38.87ms
step:418/2330 train_time:16264ms step_avg:38.91ms
step:419/2330 train_time:16287ms step_avg:38.87ms
step:420/2330 train_time:16343ms step_avg:38.91ms
step:421/2330 train_time:16366ms step_avg:38.87ms
step:422/2330 train_time:16422ms step_avg:38.92ms
step:423/2330 train_time:16445ms step_avg:38.88ms
step:424/2330 train_time:16501ms step_avg:38.92ms
step:425/2330 train_time:16524ms step_avg:38.88ms
step:426/2330 train_time:16580ms step_avg:38.92ms
step:427/2330 train_time:16604ms step_avg:38.88ms
step:428/2330 train_time:16660ms step_avg:38.92ms
step:429/2330 train_time:16683ms step_avg:38.89ms
step:430/2330 train_time:16739ms step_avg:38.93ms
step:431/2330 train_time:16762ms step_avg:38.89ms
step:432/2330 train_time:16819ms step_avg:38.93ms
step:433/2330 train_time:16841ms step_avg:38.89ms
step:434/2330 train_time:16896ms step_avg:38.93ms
step:435/2330 train_time:16919ms step_avg:38.89ms
step:436/2330 train_time:16975ms step_avg:38.93ms
step:437/2330 train_time:16996ms step_avg:38.89ms
step:438/2330 train_time:17052ms step_avg:38.93ms
step:439/2330 train_time:17073ms step_avg:38.89ms
step:440/2330 train_time:17130ms step_avg:38.93ms
step:441/2330 train_time:17151ms step_avg:38.89ms
step:442/2330 train_time:17208ms step_avg:38.93ms
step:443/2330 train_time:17230ms step_avg:38.89ms
step:444/2330 train_time:17286ms step_avg:38.93ms
step:445/2330 train_time:17309ms step_avg:38.90ms
step:446/2330 train_time:17365ms step_avg:38.94ms
step:447/2330 train_time:17388ms step_avg:38.90ms
step:448/2330 train_time:17444ms step_avg:38.94ms
step:449/2330 train_time:17468ms step_avg:38.90ms
step:450/2330 train_time:17524ms step_avg:38.94ms
step:451/2330 train_time:17547ms step_avg:38.91ms
step:452/2330 train_time:17602ms step_avg:38.94ms
step:453/2330 train_time:17626ms step_avg:38.91ms
step:454/2330 train_time:17682ms step_avg:38.95ms
step:455/2330 train_time:17706ms step_avg:38.91ms
step:456/2330 train_time:17763ms step_avg:38.95ms
step:457/2330 train_time:17787ms step_avg:38.92ms
step:458/2330 train_time:17843ms step_avg:38.96ms
step:459/2330 train_time:17867ms step_avg:38.93ms
step:460/2330 train_time:17923ms step_avg:38.96ms
step:461/2330 train_time:17947ms step_avg:38.93ms
step:462/2330 train_time:18003ms step_avg:38.97ms
step:463/2330 train_time:18026ms step_avg:38.93ms
step:464/2330 train_time:18082ms step_avg:38.97ms
step:465/2330 train_time:18105ms step_avg:38.93ms
step:466/2330 train_time:18160ms step_avg:38.97ms
step:467/2330 train_time:18183ms step_avg:38.94ms
step:468/2330 train_time:18240ms step_avg:38.97ms
step:469/2330 train_time:18263ms step_avg:38.94ms
step:470/2330 train_time:18318ms step_avg:38.98ms
step:471/2330 train_time:18341ms step_avg:38.94ms
step:472/2330 train_time:18396ms step_avg:38.98ms
step:473/2330 train_time:18419ms step_avg:38.94ms
step:474/2330 train_time:18475ms step_avg:38.98ms
step:475/2330 train_time:18496ms step_avg:38.94ms
step:476/2330 train_time:18552ms step_avg:38.97ms
step:477/2330 train_time:18574ms step_avg:38.94ms
step:478/2330 train_time:18630ms step_avg:38.98ms
step:479/2330 train_time:18653ms step_avg:38.94ms
step:480/2330 train_time:18709ms step_avg:38.98ms
step:481/2330 train_time:18732ms step_avg:38.94ms
step:482/2330 train_time:18789ms step_avg:38.98ms
step:483/2330 train_time:18811ms step_avg:38.95ms
step:484/2330 train_time:18867ms step_avg:38.98ms
step:485/2330 train_time:18890ms step_avg:38.95ms
step:486/2330 train_time:18947ms step_avg:38.98ms
step:487/2330 train_time:18969ms step_avg:38.95ms
step:488/2330 train_time:19024ms step_avg:38.98ms
step:489/2330 train_time:19048ms step_avg:38.95ms
step:490/2330 train_time:19103ms step_avg:38.99ms
step:491/2330 train_time:19126ms step_avg:38.95ms
step:492/2330 train_time:19182ms step_avg:38.99ms
step:493/2330 train_time:19205ms step_avg:38.96ms
step:494/2330 train_time:19261ms step_avg:38.99ms
step:495/2330 train_time:19283ms step_avg:38.96ms
step:496/2330 train_time:19339ms step_avg:38.99ms
step:497/2330 train_time:19362ms step_avg:38.96ms
step:498/2330 train_time:19418ms step_avg:38.99ms
step:499/2330 train_time:19440ms step_avg:38.96ms
step:500/2330 train_time:19496ms step_avg:38.99ms
step:500/2330 val_loss:5.4910 train_time:19589ms step_avg:39.18ms
step:501/2330 train_time:19601ms step_avg:39.12ms
step:502/2330 train_time:19612ms step_avg:39.07ms
step:503/2330 train_time:19623ms step_avg:39.01ms
step:504/2330 train_time:19652ms step_avg:38.99ms
step:505/2330 train_time:19674ms step_avg:38.96ms
step:506/2330 train_time:19729ms step_avg:38.99ms
step:507/2330 train_time:19751ms step_avg:38.96ms
step:508/2330 train_time:19806ms step_avg:38.99ms
step:509/2330 train_time:19828ms step_avg:38.95ms
step:510/2330 train_time:19883ms step_avg:38.99ms
step:511/2330 train_time:19906ms step_avg:38.95ms
step:512/2330 train_time:19963ms step_avg:38.99ms
step:513/2330 train_time:19986ms step_avg:38.96ms
step:514/2330 train_time:20043ms step_avg:38.99ms
step:515/2330 train_time:20065ms step_avg:38.96ms
step:516/2330 train_time:20121ms step_avg:38.99ms
step:517/2330 train_time:20143ms step_avg:38.96ms
step:518/2330 train_time:20199ms step_avg:38.99ms
step:519/2330 train_time:20221ms step_avg:38.96ms
step:520/2330 train_time:20277ms step_avg:38.99ms
step:521/2330 train_time:20299ms step_avg:38.96ms
step:522/2330 train_time:20354ms step_avg:38.99ms
step:523/2330 train_time:20377ms step_avg:38.96ms
step:524/2330 train_time:20432ms step_avg:38.99ms
step:525/2330 train_time:20455ms step_avg:38.96ms
step:526/2330 train_time:20512ms step_avg:39.00ms
step:527/2330 train_time:20536ms step_avg:38.97ms
step:528/2330 train_time:20592ms step_avg:39.00ms
step:529/2330 train_time:20615ms step_avg:38.97ms
step:530/2330 train_time:20671ms step_avg:39.00ms
step:531/2330 train_time:20694ms step_avg:38.97ms
step:532/2330 train_time:20749ms step_avg:39.00ms
step:533/2330 train_time:20772ms step_avg:38.97ms
step:534/2330 train_time:20828ms step_avg:39.00ms
step:535/2330 train_time:20851ms step_avg:38.97ms
step:536/2330 train_time:20908ms step_avg:39.01ms
step:537/2330 train_time:20931ms step_avg:38.98ms
step:538/2330 train_time:20988ms step_avg:39.01ms
step:539/2330 train_time:21012ms step_avg:38.98ms
step:540/2330 train_time:21069ms step_avg:39.02ms
step:541/2330 train_time:21092ms step_avg:38.99ms
step:542/2330 train_time:21148ms step_avg:39.02ms
step:543/2330 train_time:21171ms step_avg:38.99ms
step:544/2330 train_time:21227ms step_avg:39.02ms
step:545/2330 train_time:21250ms step_avg:38.99ms
step:546/2330 train_time:21305ms step_avg:39.02ms
step:547/2330 train_time:21327ms step_avg:38.99ms
step:548/2330 train_time:21382ms step_avg:39.02ms
step:549/2330 train_time:21404ms step_avg:38.99ms
step:550/2330 train_time:21461ms step_avg:39.02ms
step:551/2330 train_time:21483ms step_avg:38.99ms
step:552/2330 train_time:21539ms step_avg:39.02ms
step:553/2330 train_time:21561ms step_avg:38.99ms
step:554/2330 train_time:21619ms step_avg:39.02ms
step:555/2330 train_time:21641ms step_avg:38.99ms
step:556/2330 train_time:21697ms step_avg:39.02ms
step:557/2330 train_time:21719ms step_avg:38.99ms
step:558/2330 train_time:21775ms step_avg:39.02ms
step:559/2330 train_time:21798ms step_avg:38.99ms
step:560/2330 train_time:21854ms step_avg:39.02ms
step:561/2330 train_time:21877ms step_avg:39.00ms
step:562/2330 train_time:21934ms step_avg:39.03ms
step:563/2330 train_time:21957ms step_avg:39.00ms
step:564/2330 train_time:22014ms step_avg:39.03ms
step:565/2330 train_time:22037ms step_avg:39.00ms
step:566/2330 train_time:22094ms step_avg:39.03ms
step:567/2330 train_time:22117ms step_avg:39.01ms
step:568/2330 train_time:22173ms step_avg:39.04ms
step:569/2330 train_time:22196ms step_avg:39.01ms
step:570/2330 train_time:22252ms step_avg:39.04ms
step:571/2330 train_time:22275ms step_avg:39.01ms
step:572/2330 train_time:22331ms step_avg:39.04ms
step:573/2330 train_time:22354ms step_avg:39.01ms
step:574/2330 train_time:22409ms step_avg:39.04ms
step:575/2330 train_time:22432ms step_avg:39.01ms
step:576/2330 train_time:22487ms step_avg:39.04ms
step:577/2330 train_time:22510ms step_avg:39.01ms
step:578/2330 train_time:22566ms step_avg:39.04ms
step:579/2330 train_time:22588ms step_avg:39.01ms
step:580/2330 train_time:22643ms step_avg:39.04ms
step:581/2330 train_time:22665ms step_avg:39.01ms
step:582/2330 train_time:22721ms step_avg:39.04ms
step:583/2330 train_time:22743ms step_avg:39.01ms
step:584/2330 train_time:22799ms step_avg:39.04ms
step:585/2330 train_time:22821ms step_avg:39.01ms
step:586/2330 train_time:22878ms step_avg:39.04ms
step:587/2330 train_time:22900ms step_avg:39.01ms
step:588/2330 train_time:22956ms step_avg:39.04ms
step:589/2330 train_time:22978ms step_avg:39.01ms
step:590/2330 train_time:23034ms step_avg:39.04ms
step:591/2330 train_time:23057ms step_avg:39.01ms
step:592/2330 train_time:23113ms step_avg:39.04ms
step:593/2330 train_time:23136ms step_avg:39.02ms
step:594/2330 train_time:23192ms step_avg:39.04ms
step:595/2330 train_time:23215ms step_avg:39.02ms
step:596/2330 train_time:23271ms step_avg:39.04ms
step:597/2330 train_time:23294ms step_avg:39.02ms
step:598/2330 train_time:23350ms step_avg:39.05ms
step:599/2330 train_time:23373ms step_avg:39.02ms
step:600/2330 train_time:23429ms step_avg:39.05ms
step:601/2330 train_time:23452ms step_avg:39.02ms
step:602/2330 train_time:23508ms step_avg:39.05ms
step:603/2330 train_time:23531ms step_avg:39.02ms
step:604/2330 train_time:23587ms step_avg:39.05ms
step:605/2330 train_time:23609ms step_avg:39.02ms
step:606/2330 train_time:23664ms step_avg:39.05ms
step:607/2330 train_time:23686ms step_avg:39.02ms
step:608/2330 train_time:23741ms step_avg:39.05ms
step:609/2330 train_time:23763ms step_avg:39.02ms
step:610/2330 train_time:23820ms step_avg:39.05ms
step:611/2330 train_time:23841ms step_avg:39.02ms
step:612/2330 train_time:23898ms step_avg:39.05ms
step:613/2330 train_time:23920ms step_avg:39.02ms
step:614/2330 train_time:23976ms step_avg:39.05ms
step:615/2330 train_time:23998ms step_avg:39.02ms
step:616/2330 train_time:24054ms step_avg:39.05ms
step:617/2330 train_time:24077ms step_avg:39.02ms
step:618/2330 train_time:24133ms step_avg:39.05ms
step:619/2330 train_time:24156ms step_avg:39.02ms
step:620/2330 train_time:24212ms step_avg:39.05ms
step:621/2330 train_time:24234ms step_avg:39.02ms
step:622/2330 train_time:24290ms step_avg:39.05ms
step:623/2330 train_time:24313ms step_avg:39.03ms
step:624/2330 train_time:24369ms step_avg:39.05ms
step:625/2330 train_time:24392ms step_avg:39.03ms
step:626/2330 train_time:24448ms step_avg:39.05ms
step:627/2330 train_time:24471ms step_avg:39.03ms
step:628/2330 train_time:24527ms step_avg:39.06ms
step:629/2330 train_time:24550ms step_avg:39.03ms
step:630/2330 train_time:24606ms step_avg:39.06ms
step:631/2330 train_time:24628ms step_avg:39.03ms
step:632/2330 train_time:24684ms step_avg:39.06ms
step:633/2330 train_time:24705ms step_avg:39.03ms
step:634/2330 train_time:24761ms step_avg:39.06ms
step:635/2330 train_time:24783ms step_avg:39.03ms
step:636/2330 train_time:24839ms step_avg:39.06ms
step:637/2330 train_time:24862ms step_avg:39.03ms
step:638/2330 train_time:24917ms step_avg:39.06ms
step:639/2330 train_time:24940ms step_avg:39.03ms
step:640/2330 train_time:24996ms step_avg:39.06ms
step:641/2330 train_time:25018ms step_avg:39.03ms
step:642/2330 train_time:25073ms step_avg:39.06ms
step:643/2330 train_time:25096ms step_avg:39.03ms
step:644/2330 train_time:25152ms step_avg:39.06ms
step:645/2330 train_time:25175ms step_avg:39.03ms
step:646/2330 train_time:25231ms step_avg:39.06ms
step:647/2330 train_time:25253ms step_avg:39.03ms
step:648/2330 train_time:25309ms step_avg:39.06ms
step:649/2330 train_time:25332ms step_avg:39.03ms
step:650/2330 train_time:25388ms step_avg:39.06ms
step:651/2330 train_time:25411ms step_avg:39.03ms
step:652/2330 train_time:25467ms step_avg:39.06ms
step:653/2330 train_time:25489ms step_avg:39.03ms
step:654/2330 train_time:25544ms step_avg:39.06ms
step:655/2330 train_time:25566ms step_avg:39.03ms
step:656/2330 train_time:25621ms step_avg:39.06ms
step:657/2330 train_time:25644ms step_avg:39.03ms
step:658/2330 train_time:25700ms step_avg:39.06ms
step:659/2330 train_time:25722ms step_avg:39.03ms
step:660/2330 train_time:25778ms step_avg:39.06ms
step:661/2330 train_time:25800ms step_avg:39.03ms
step:662/2330 train_time:25856ms step_avg:39.06ms
step:663/2330 train_time:25878ms step_avg:39.03ms
step:664/2330 train_time:25934ms step_avg:39.06ms
step:665/2330 train_time:25957ms step_avg:39.03ms
step:666/2330 train_time:26012ms step_avg:39.06ms
step:667/2330 train_time:26035ms step_avg:39.03ms
step:668/2330 train_time:26091ms step_avg:39.06ms
step:669/2330 train_time:26114ms step_avg:39.03ms
step:670/2330 train_time:26171ms step_avg:39.06ms
step:671/2330 train_time:26194ms step_avg:39.04ms
step:672/2330 train_time:26251ms step_avg:39.06ms
step:673/2330 train_time:26273ms step_avg:39.04ms
step:674/2330 train_time:26329ms step_avg:39.06ms
step:675/2330 train_time:26352ms step_avg:39.04ms
step:676/2330 train_time:26408ms step_avg:39.07ms
step:677/2330 train_time:26431ms step_avg:39.04ms
step:678/2330 train_time:26487ms step_avg:39.07ms
step:679/2330 train_time:26509ms step_avg:39.04ms
step:680/2330 train_time:26566ms step_avg:39.07ms
step:681/2330 train_time:26588ms step_avg:39.04ms
step:682/2330 train_time:26644ms step_avg:39.07ms
step:683/2330 train_time:26665ms step_avg:39.04ms
step:684/2330 train_time:26721ms step_avg:39.07ms
step:685/2330 train_time:26743ms step_avg:39.04ms
step:686/2330 train_time:26799ms step_avg:39.07ms
step:687/2330 train_time:26822ms step_avg:39.04ms
step:688/2330 train_time:26878ms step_avg:39.07ms
step:689/2330 train_time:26899ms step_avg:39.04ms
step:690/2330 train_time:26956ms step_avg:39.07ms
step:691/2330 train_time:26979ms step_avg:39.04ms
step:692/2330 train_time:27035ms step_avg:39.07ms
step:693/2330 train_time:27057ms step_avg:39.04ms
step:694/2330 train_time:27114ms step_avg:39.07ms
step:695/2330 train_time:27137ms step_avg:39.05ms
step:696/2330 train_time:27193ms step_avg:39.07ms
step:697/2330 train_time:27215ms step_avg:39.05ms
step:698/2330 train_time:27272ms step_avg:39.07ms
step:699/2330 train_time:27294ms step_avg:39.05ms
step:700/2330 train_time:27351ms step_avg:39.07ms
step:701/2330 train_time:27373ms step_avg:39.05ms
step:702/2330 train_time:27429ms step_avg:39.07ms
step:703/2330 train_time:27452ms step_avg:39.05ms
step:704/2330 train_time:27508ms step_avg:39.07ms
step:705/2330 train_time:27532ms step_avg:39.05ms
step:706/2330 train_time:27588ms step_avg:39.08ms
step:707/2330 train_time:27611ms step_avg:39.05ms
step:708/2330 train_time:27667ms step_avg:39.08ms
step:709/2330 train_time:27689ms step_avg:39.05ms
step:710/2330 train_time:27745ms step_avg:39.08ms
step:711/2330 train_time:27767ms step_avg:39.05ms
step:712/2330 train_time:27822ms step_avg:39.08ms
step:713/2330 train_time:27844ms step_avg:39.05ms
step:714/2330 train_time:27900ms step_avg:39.08ms
step:715/2330 train_time:27921ms step_avg:39.05ms
step:716/2330 train_time:27978ms step_avg:39.08ms
step:717/2330 train_time:28000ms step_avg:39.05ms
step:718/2330 train_time:28056ms step_avg:39.08ms
step:719/2330 train_time:28079ms step_avg:39.05ms
step:720/2330 train_time:28134ms step_avg:39.08ms
step:721/2330 train_time:28157ms step_avg:39.05ms
step:722/2330 train_time:28213ms step_avg:39.08ms
step:723/2330 train_time:28236ms step_avg:39.05ms
step:724/2330 train_time:28292ms step_avg:39.08ms
step:725/2330 train_time:28315ms step_avg:39.05ms
step:726/2330 train_time:28371ms step_avg:39.08ms
step:727/2330 train_time:28394ms step_avg:39.06ms
step:728/2330 train_time:28450ms step_avg:39.08ms
step:729/2330 train_time:28473ms step_avg:39.06ms
step:730/2330 train_time:28529ms step_avg:39.08ms
step:731/2330 train_time:28552ms step_avg:39.06ms
step:732/2330 train_time:28608ms step_avg:39.08ms
step:733/2330 train_time:28631ms step_avg:39.06ms
step:734/2330 train_time:28688ms step_avg:39.08ms
step:735/2330 train_time:28711ms step_avg:39.06ms
step:736/2330 train_time:28768ms step_avg:39.09ms
step:737/2330 train_time:28790ms step_avg:39.06ms
step:738/2330 train_time:28845ms step_avg:39.09ms
step:739/2330 train_time:28868ms step_avg:39.06ms
step:740/2330 train_time:28923ms step_avg:39.08ms
step:741/2330 train_time:28945ms step_avg:39.06ms
step:742/2330 train_time:29001ms step_avg:39.09ms
step:743/2330 train_time:29023ms step_avg:39.06ms
step:744/2330 train_time:29080ms step_avg:39.09ms
step:745/2330 train_time:29102ms step_avg:39.06ms
step:746/2330 train_time:29158ms step_avg:39.09ms
step:747/2330 train_time:29180ms step_avg:39.06ms
step:748/2330 train_time:29236ms step_avg:39.09ms
step:749/2330 train_time:29258ms step_avg:39.06ms
step:750/2330 train_time:29314ms step_avg:39.08ms
step:750/2330 val_loss:5.4302 train_time:29409ms step_avg:39.21ms
step:751/2330 train_time:29422ms step_avg:39.18ms
step:752/2330 train_time:29433ms step_avg:39.14ms
step:753/2330 train_time:29443ms step_avg:39.10ms
step:754/2330 train_time:29471ms step_avg:39.09ms
step:755/2330 train_time:29493ms step_avg:39.06ms
step:756/2330 train_time:29548ms step_avg:39.08ms
step:757/2330 train_time:29570ms step_avg:39.06ms
step:758/2330 train_time:29626ms step_avg:39.08ms
step:759/2330 train_time:29647ms step_avg:39.06ms
step:760/2330 train_time:29703ms step_avg:39.08ms
step:761/2330 train_time:29726ms step_avg:39.06ms
step:762/2330 train_time:29785ms step_avg:39.09ms
step:763/2330 train_time:29807ms step_avg:39.07ms
step:764/2330 train_time:29864ms step_avg:39.09ms
step:765/2330 train_time:29888ms step_avg:39.07ms
step:766/2330 train_time:29943ms step_avg:39.09ms
step:767/2330 train_time:29964ms step_avg:39.07ms
step:768/2330 train_time:30020ms step_avg:39.09ms
step:769/2330 train_time:30041ms step_avg:39.07ms
step:770/2330 train_time:30097ms step_avg:39.09ms
step:771/2330 train_time:30119ms step_avg:39.07ms
step:772/2330 train_time:30174ms step_avg:39.09ms
step:773/2330 train_time:30196ms step_avg:39.06ms
step:774/2330 train_time:30252ms step_avg:39.08ms
step:775/2330 train_time:30274ms step_avg:39.06ms
step:776/2330 train_time:30331ms step_avg:39.09ms
step:777/2330 train_time:30354ms step_avg:39.07ms
step:778/2330 train_time:30410ms step_avg:39.09ms
step:779/2330 train_time:30432ms step_avg:39.07ms
step:780/2330 train_time:30488ms step_avg:39.09ms
step:781/2330 train_time:30510ms step_avg:39.06ms
step:782/2330 train_time:30566ms step_avg:39.09ms
step:783/2330 train_time:30588ms step_avg:39.06ms
step:784/2330 train_time:30644ms step_avg:39.09ms
step:785/2330 train_time:30667ms step_avg:39.07ms
step:786/2330 train_time:30724ms step_avg:39.09ms
step:787/2330 train_time:30746ms step_avg:39.07ms
step:788/2330 train_time:30802ms step_avg:39.09ms
step:789/2330 train_time:30824ms step_avg:39.07ms
step:790/2330 train_time:30881ms step_avg:39.09ms
step:791/2330 train_time:30903ms step_avg:39.07ms
step:792/2330 train_time:30959ms step_avg:39.09ms
step:793/2330 train_time:30981ms step_avg:39.07ms
step:794/2330 train_time:31037ms step_avg:39.09ms
step:795/2330 train_time:31059ms step_avg:39.07ms
step:796/2330 train_time:31114ms step_avg:39.09ms
step:797/2330 train_time:31136ms step_avg:39.07ms
step:798/2330 train_time:31191ms step_avg:39.09ms
step:799/2330 train_time:31214ms step_avg:39.07ms
step:800/2330 train_time:31270ms step_avg:39.09ms
step:801/2330 train_time:31292ms step_avg:39.07ms
step:802/2330 train_time:31348ms step_avg:39.09ms
step:803/2330 train_time:31371ms step_avg:39.07ms
step:804/2330 train_time:31427ms step_avg:39.09ms
step:805/2330 train_time:31449ms step_avg:39.07ms
step:806/2330 train_time:31505ms step_avg:39.09ms
step:807/2330 train_time:31527ms step_avg:39.07ms
step:808/2330 train_time:31583ms step_avg:39.09ms
step:809/2330 train_time:31605ms step_avg:39.07ms
step:810/2330 train_time:31661ms step_avg:39.09ms
step:811/2330 train_time:31683ms step_avg:39.07ms
step:812/2330 train_time:31739ms step_avg:39.09ms
step:813/2330 train_time:31761ms step_avg:39.07ms
step:814/2330 train_time:31817ms step_avg:39.09ms
step:815/2330 train_time:31840ms step_avg:39.07ms
step:816/2330 train_time:31896ms step_avg:39.09ms
step:817/2330 train_time:31918ms step_avg:39.07ms
step:818/2330 train_time:31974ms step_avg:39.09ms
step:819/2330 train_time:31996ms step_avg:39.07ms
step:820/2330 train_time:32052ms step_avg:39.09ms
step:821/2330 train_time:32075ms step_avg:39.07ms
step:822/2330 train_time:32130ms step_avg:39.09ms
step:823/2330 train_time:32153ms step_avg:39.07ms
step:824/2330 train_time:32208ms step_avg:39.09ms
step:825/2330 train_time:32231ms step_avg:39.07ms
step:826/2330 train_time:32287ms step_avg:39.09ms
step:827/2330 train_time:32310ms step_avg:39.07ms
step:828/2330 train_time:32366ms step_avg:39.09ms
step:829/2330 train_time:32387ms step_avg:39.07ms
step:830/2330 train_time:32443ms step_avg:39.09ms
step:831/2330 train_time:32465ms step_avg:39.07ms
step:832/2330 train_time:32520ms step_avg:39.09ms
step:833/2330 train_time:32542ms step_avg:39.07ms
step:834/2330 train_time:32598ms step_avg:39.09ms
step:835/2330 train_time:32620ms step_avg:39.07ms
step:836/2330 train_time:32676ms step_avg:39.09ms
step:837/2330 train_time:32699ms step_avg:39.07ms
step:838/2330 train_time:32755ms step_avg:39.09ms
step:839/2330 train_time:32777ms step_avg:39.07ms
step:840/2330 train_time:32833ms step_avg:39.09ms
step:841/2330 train_time:32857ms step_avg:39.07ms
step:842/2330 train_time:32912ms step_avg:39.09ms
step:843/2330 train_time:32935ms step_avg:39.07ms
step:844/2330 train_time:32991ms step_avg:39.09ms
step:845/2330 train_time:33014ms step_avg:39.07ms
step:846/2330 train_time:33069ms step_avg:39.09ms
step:847/2330 train_time:33092ms step_avg:39.07ms
step:848/2330 train_time:33148ms step_avg:39.09ms
step:849/2330 train_time:33170ms step_avg:39.07ms
step:850/2330 train_time:33226ms step_avg:39.09ms
step:851/2330 train_time:33249ms step_avg:39.07ms
step:852/2330 train_time:33304ms step_avg:39.09ms
step:853/2330 train_time:33326ms step_avg:39.07ms
step:854/2330 train_time:33382ms step_avg:39.09ms
step:855/2330 train_time:33404ms step_avg:39.07ms
step:856/2330 train_time:33460ms step_avg:39.09ms
step:857/2330 train_time:33481ms step_avg:39.07ms
step:858/2330 train_time:33538ms step_avg:39.09ms
step:859/2330 train_time:33560ms step_avg:39.07ms
step:860/2330 train_time:33616ms step_avg:39.09ms
step:861/2330 train_time:33638ms step_avg:39.07ms
step:862/2330 train_time:33694ms step_avg:39.09ms
step:863/2330 train_time:33717ms step_avg:39.07ms
step:864/2330 train_time:33773ms step_avg:39.09ms
step:865/2330 train_time:33796ms step_avg:39.07ms
step:866/2330 train_time:33852ms step_avg:39.09ms
step:867/2330 train_time:33875ms step_avg:39.07ms
step:868/2330 train_time:33931ms step_avg:39.09ms
step:869/2330 train_time:33953ms step_avg:39.07ms
step:870/2330 train_time:34009ms step_avg:39.09ms
step:871/2330 train_time:34032ms step_avg:39.07ms
step:872/2330 train_time:34088ms step_avg:39.09ms
step:873/2330 train_time:34110ms step_avg:39.07ms
step:874/2330 train_time:34166ms step_avg:39.09ms
step:875/2330 train_time:34189ms step_avg:39.07ms
step:876/2330 train_time:34245ms step_avg:39.09ms
step:877/2330 train_time:34269ms step_avg:39.07ms
step:878/2330 train_time:34325ms step_avg:39.09ms
step:879/2330 train_time:34347ms step_avg:39.07ms
step:880/2330 train_time:34402ms step_avg:39.09ms
step:881/2330 train_time:34424ms step_avg:39.07ms
step:882/2330 train_time:34480ms step_avg:39.09ms
step:883/2330 train_time:34501ms step_avg:39.07ms
step:884/2330 train_time:34558ms step_avg:39.09ms
step:885/2330 train_time:34580ms step_avg:39.07ms
step:886/2330 train_time:34636ms step_avg:39.09ms
step:887/2330 train_time:34658ms step_avg:39.07ms
step:888/2330 train_time:34714ms step_avg:39.09ms
step:889/2330 train_time:34736ms step_avg:39.07ms
step:890/2330 train_time:34793ms step_avg:39.09ms
step:891/2330 train_time:34816ms step_avg:39.07ms
step:892/2330 train_time:34872ms step_avg:39.09ms
step:893/2330 train_time:34894ms step_avg:39.08ms
step:894/2330 train_time:34950ms step_avg:39.09ms
step:895/2330 train_time:34973ms step_avg:39.08ms
step:896/2330 train_time:35029ms step_avg:39.09ms
step:897/2330 train_time:35052ms step_avg:39.08ms
step:898/2330 train_time:35108ms step_avg:39.10ms
step:899/2330 train_time:35132ms step_avg:39.08ms
step:900/2330 train_time:35188ms step_avg:39.10ms
step:901/2330 train_time:35210ms step_avg:39.08ms
step:902/2330 train_time:35266ms step_avg:39.10ms
step:903/2330 train_time:35289ms step_avg:39.08ms
step:904/2330 train_time:35345ms step_avg:39.10ms
step:905/2330 train_time:35367ms step_avg:39.08ms
step:906/2330 train_time:35422ms step_avg:39.10ms
step:907/2330 train_time:35444ms step_avg:39.08ms
step:908/2330 train_time:35499ms step_avg:39.10ms
step:909/2330 train_time:35521ms step_avg:39.08ms
step:910/2330 train_time:35577ms step_avg:39.10ms
step:911/2330 train_time:35599ms step_avg:39.08ms
step:912/2330 train_time:35656ms step_avg:39.10ms
step:913/2330 train_time:35677ms step_avg:39.08ms
step:914/2330 train_time:35733ms step_avg:39.10ms
step:915/2330 train_time:35756ms step_avg:39.08ms
step:916/2330 train_time:35811ms step_avg:39.10ms
step:917/2330 train_time:35834ms step_avg:39.08ms
step:918/2330 train_time:35891ms step_avg:39.10ms
step:919/2330 train_time:35913ms step_avg:39.08ms
step:920/2330 train_time:35969ms step_avg:39.10ms
step:921/2330 train_time:35991ms step_avg:39.08ms
step:922/2330 train_time:36047ms step_avg:39.10ms
step:923/2330 train_time:36070ms step_avg:39.08ms
step:924/2330 train_time:36126ms step_avg:39.10ms
step:925/2330 train_time:36149ms step_avg:39.08ms
step:926/2330 train_time:36205ms step_avg:39.10ms
step:927/2330 train_time:36227ms step_avg:39.08ms
step:928/2330 train_time:36282ms step_avg:39.10ms
step:929/2330 train_time:36305ms step_avg:39.08ms
step:930/2330 train_time:36360ms step_avg:39.10ms
step:931/2330 train_time:36383ms step_avg:39.08ms
step:932/2330 train_time:36439ms step_avg:39.10ms
step:933/2330 train_time:36461ms step_avg:39.08ms
step:934/2330 train_time:36517ms step_avg:39.10ms
step:935/2330 train_time:36538ms step_avg:39.08ms
step:936/2330 train_time:36594ms step_avg:39.10ms
step:937/2330 train_time:36616ms step_avg:39.08ms
step:938/2330 train_time:36672ms step_avg:39.10ms
step:939/2330 train_time:36695ms step_avg:39.08ms
step:940/2330 train_time:36751ms step_avg:39.10ms
step:941/2330 train_time:36773ms step_avg:39.08ms
step:942/2330 train_time:36829ms step_avg:39.10ms
step:943/2330 train_time:36852ms step_avg:39.08ms
step:944/2330 train_time:36907ms step_avg:39.10ms
step:945/2330 train_time:36930ms step_avg:39.08ms
step:946/2330 train_time:36986ms step_avg:39.10ms
step:947/2330 train_time:37008ms step_avg:39.08ms
step:948/2330 train_time:37064ms step_avg:39.10ms
step:949/2330 train_time:37086ms step_avg:39.08ms
step:950/2330 train_time:37141ms step_avg:39.10ms
step:951/2330 train_time:37164ms step_avg:39.08ms
step:952/2330 train_time:37220ms step_avg:39.10ms
step:953/2330 train_time:37242ms step_avg:39.08ms
step:954/2330 train_time:37298ms step_avg:39.10ms
step:955/2330 train_time:37320ms step_avg:39.08ms
step:956/2330 train_time:37375ms step_avg:39.10ms
step:957/2330 train_time:37398ms step_avg:39.08ms
step:958/2330 train_time:37453ms step_avg:39.10ms
step:959/2330 train_time:37476ms step_avg:39.08ms
step:960/2330 train_time:37532ms step_avg:39.10ms
step:961/2330 train_time:37555ms step_avg:39.08ms
step:962/2330 train_time:37611ms step_avg:39.10ms
step:963/2330 train_time:37633ms step_avg:39.08ms
step:964/2330 train_time:37690ms step_avg:39.10ms
step:965/2330 train_time:37713ms step_avg:39.08ms
step:966/2330 train_time:37769ms step_avg:39.10ms
step:967/2330 train_time:37791ms step_avg:39.08ms
step:968/2330 train_time:37847ms step_avg:39.10ms
step:969/2330 train_time:37870ms step_avg:39.08ms
step:970/2330 train_time:37925ms step_avg:39.10ms
step:971/2330 train_time:37948ms step_avg:39.08ms
step:972/2330 train_time:38004ms step_avg:39.10ms
step:973/2330 train_time:38026ms step_avg:39.08ms
step:974/2330 train_time:38082ms step_avg:39.10ms
step:975/2330 train_time:38104ms step_avg:39.08ms
step:976/2330 train_time:38160ms step_avg:39.10ms
step:977/2330 train_time:38182ms step_avg:39.08ms
step:978/2330 train_time:38238ms step_avg:39.10ms
step:979/2330 train_time:38260ms step_avg:39.08ms
step:980/2330 train_time:38317ms step_avg:39.10ms
step:981/2330 train_time:38339ms step_avg:39.08ms
step:982/2330 train_time:38395ms step_avg:39.10ms
step:983/2330 train_time:38417ms step_avg:39.08ms
step:984/2330 train_time:38472ms step_avg:39.10ms
step:985/2330 train_time:38495ms step_avg:39.08ms
step:986/2330 train_time:38551ms step_avg:39.10ms
step:987/2330 train_time:38574ms step_avg:39.08ms
step:988/2330 train_time:38630ms step_avg:39.10ms
step:989/2330 train_time:38653ms step_avg:39.08ms
step:990/2330 train_time:38708ms step_avg:39.10ms
step:991/2330 train_time:38731ms step_avg:39.08ms
step:992/2330 train_time:38788ms step_avg:39.10ms
step:993/2330 train_time:38810ms step_avg:39.08ms
step:994/2330 train_time:38866ms step_avg:39.10ms
step:995/2330 train_time:38888ms step_avg:39.08ms
step:996/2330 train_time:38944ms step_avg:39.10ms
step:997/2330 train_time:38966ms step_avg:39.08ms
step:998/2330 train_time:39022ms step_avg:39.10ms
step:999/2330 train_time:39044ms step_avg:39.08ms
step:1000/2330 train_time:39100ms step_avg:39.10ms
step:1000/2330 val_loss:5.3977 train_time:39195ms step_avg:39.19ms
step:1001/2330 train_time:39206ms step_avg:39.17ms
step:1002/2330 train_time:39217ms step_avg:39.14ms
step:1003/2330 train_time:39227ms step_avg:39.11ms
step:1004/2330 train_time:39256ms step_avg:39.10ms
step:1005/2330 train_time:39278ms step_avg:39.08ms
step:1006/2330 train_time:39333ms step_avg:39.10ms
step:1007/2330 train_time:39355ms step_avg:39.08ms
step:1008/2330 train_time:39411ms step_avg:39.10ms
step:1009/2330 train_time:39432ms step_avg:39.08ms
step:1010/2330 train_time:39487ms step_avg:39.10ms
step:1011/2330 train_time:39510ms step_avg:39.08ms
step:1012/2330 train_time:39569ms step_avg:39.10ms
step:1013/2330 train_time:39594ms step_avg:39.09ms
step:1014/2330 train_time:39652ms step_avg:39.10ms
step:1015/2330 train_time:39675ms step_avg:39.09ms
step:1016/2330 train_time:39731ms step_avg:39.11ms
step:1017/2330 train_time:39754ms step_avg:39.09ms
step:1018/2330 train_time:39810ms step_avg:39.11ms
step:1019/2330 train_time:39832ms step_avg:39.09ms
step:1020/2330 train_time:39888ms step_avg:39.11ms
step:1021/2330 train_time:39909ms step_avg:39.09ms
step:1022/2330 train_time:39965ms step_avg:39.10ms
step:1023/2330 train_time:39986ms step_avg:39.09ms
step:1024/2330 train_time:40041ms step_avg:39.10ms
step:1025/2330 train_time:40063ms step_avg:39.09ms
step:1026/2330 train_time:40123ms step_avg:39.11ms
step:1027/2330 train_time:40147ms step_avg:39.09ms
step:1028/2330 train_time:40206ms step_avg:39.11ms
step:1029/2330 train_time:40228ms step_avg:39.09ms
step:1030/2330 train_time:40284ms step_avg:39.11ms
step:1031/2330 train_time:40306ms step_avg:39.09ms
step:1032/2330 train_time:40362ms step_avg:39.11ms
step:1033/2330 train_time:40383ms step_avg:39.09ms
step:1034/2330 train_time:40438ms step_avg:39.11ms
step:1035/2330 train_time:40461ms step_avg:39.09ms
step:1036/2330 train_time:40517ms step_avg:39.11ms
step:1037/2330 train_time:40541ms step_avg:39.09ms
step:1038/2330 train_time:40598ms step_avg:39.11ms
step:1039/2330 train_time:40621ms step_avg:39.10ms
step:1040/2330 train_time:40678ms step_avg:39.11ms
step:1041/2330 train_time:40701ms step_avg:39.10ms
step:1042/2330 train_time:40758ms step_avg:39.11ms
step:1043/2330 train_time:40780ms step_avg:39.10ms
step:1044/2330 train_time:40837ms step_avg:39.12ms
step:1045/2330 train_time:40860ms step_avg:39.10ms
step:1046/2330 train_time:40915ms step_avg:39.12ms
step:1047/2330 train_time:40938ms step_avg:39.10ms
step:1048/2330 train_time:40994ms step_avg:39.12ms
step:1049/2330 train_time:41016ms step_avg:39.10ms
step:1050/2330 train_time:41072ms step_avg:39.12ms
step:1051/2330 train_time:41096ms step_avg:39.10ms
step:1052/2330 train_time:41152ms step_avg:39.12ms
step:1053/2330 train_time:41174ms step_avg:39.10ms
step:1054/2330 train_time:41230ms step_avg:39.12ms
step:1055/2330 train_time:41252ms step_avg:39.10ms
step:1056/2330 train_time:41308ms step_avg:39.12ms
step:1057/2330 train_time:41330ms step_avg:39.10ms
step:1058/2330 train_time:41385ms step_avg:39.12ms
step:1059/2330 train_time:41407ms step_avg:39.10ms
step:1060/2330 train_time:41464ms step_avg:39.12ms
step:1061/2330 train_time:41486ms step_avg:39.10ms
step:1062/2330 train_time:41543ms step_avg:39.12ms
step:1063/2330 train_time:41565ms step_avg:39.10ms
step:1064/2330 train_time:41623ms step_avg:39.12ms
step:1065/2330 train_time:41645ms step_avg:39.10ms
step:1066/2330 train_time:41701ms step_avg:39.12ms
step:1067/2330 train_time:41723ms step_avg:39.10ms
step:1068/2330 train_time:41779ms step_avg:39.12ms
step:1069/2330 train_time:41802ms step_avg:39.10ms
step:1070/2330 train_time:41857ms step_avg:39.12ms
step:1071/2330 train_time:41880ms step_avg:39.10ms
step:1072/2330 train_time:41936ms step_avg:39.12ms
step:1073/2330 train_time:41959ms step_avg:39.10ms
step:1074/2330 train_time:42015ms step_avg:39.12ms
step:1075/2330 train_time:42038ms step_avg:39.11ms
step:1076/2330 train_time:42095ms step_avg:39.12ms
step:1077/2330 train_time:42118ms step_avg:39.11ms
step:1078/2330 train_time:42174ms step_avg:39.12ms
step:1079/2330 train_time:42197ms step_avg:39.11ms
step:1080/2330 train_time:42252ms step_avg:39.12ms
step:1081/2330 train_time:42275ms step_avg:39.11ms
step:1082/2330 train_time:42331ms step_avg:39.12ms
step:1083/2330 train_time:42354ms step_avg:39.11ms
step:1084/2330 train_time:42409ms step_avg:39.12ms
step:1085/2330 train_time:42432ms step_avg:39.11ms
step:1086/2330 train_time:42488ms step_avg:39.12ms
step:1087/2330 train_time:42509ms step_avg:39.11ms
step:1088/2330 train_time:42565ms step_avg:39.12ms
step:1089/2330 train_time:42587ms step_avg:39.11ms
step:1090/2330 train_time:42643ms step_avg:39.12ms
step:1091/2330 train_time:42666ms step_avg:39.11ms
step:1092/2330 train_time:42722ms step_avg:39.12ms
step:1093/2330 train_time:42745ms step_avg:39.11ms
step:1094/2330 train_time:42802ms step_avg:39.12ms
step:1095/2330 train_time:42824ms step_avg:39.11ms
step:1096/2330 train_time:42880ms step_avg:39.12ms
step:1097/2330 train_time:42902ms step_avg:39.11ms
step:1098/2330 train_time:42958ms step_avg:39.12ms
step:1099/2330 train_time:42981ms step_avg:39.11ms
step:1100/2330 train_time:43037ms step_avg:39.12ms
step:1101/2330 train_time:43060ms step_avg:39.11ms
step:1102/2330 train_time:43116ms step_avg:39.12ms
step:1103/2330 train_time:43138ms step_avg:39.11ms
step:1104/2330 train_time:43195ms step_avg:39.13ms
step:1105/2330 train_time:43218ms step_avg:39.11ms
step:1106/2330 train_time:43276ms step_avg:39.13ms
step:1107/2330 train_time:43299ms step_avg:39.11ms
step:1108/2330 train_time:43356ms step_avg:39.13ms
step:1109/2330 train_time:43379ms step_avg:39.12ms
step:1110/2330 train_time:43435ms step_avg:39.13ms
step:1111/2330 train_time:43459ms step_avg:39.12ms
step:1112/2330 train_time:43515ms step_avg:39.13ms
step:1113/2330 train_time:43538ms step_avg:39.12ms
step:1114/2330 train_time:43595ms step_avg:39.13ms
step:1115/2330 train_time:43617ms step_avg:39.12ms
step:1116/2330 train_time:43674ms step_avg:39.13ms
step:1117/2330 train_time:43697ms step_avg:39.12ms
step:1118/2330 train_time:43753ms step_avg:39.13ms
step:1119/2330 train_time:43775ms step_avg:39.12ms
step:1120/2330 train_time:43831ms step_avg:39.14ms
step:1121/2330 train_time:43853ms step_avg:39.12ms
step:1122/2330 train_time:43909ms step_avg:39.13ms
step:1123/2330 train_time:43931ms step_avg:39.12ms
step:1124/2330 train_time:43987ms step_avg:39.13ms
step:1125/2330 train_time:44009ms step_avg:39.12ms
step:1126/2330 train_time:44065ms step_avg:39.13ms
step:1127/2330 train_time:44087ms step_avg:39.12ms
step:1128/2330 train_time:44144ms step_avg:39.13ms
step:1129/2330 train_time:44167ms step_avg:39.12ms
step:1130/2330 train_time:44224ms step_avg:39.14ms
step:1131/2330 train_time:44246ms step_avg:39.12ms
step:1132/2330 train_time:44302ms step_avg:39.14ms
step:1133/2330 train_time:44324ms step_avg:39.12ms
step:1134/2330 train_time:44380ms step_avg:39.14ms
step:1135/2330 train_time:44403ms step_avg:39.12ms
step:1136/2330 train_time:44458ms step_avg:39.14ms
step:1137/2330 train_time:44482ms step_avg:39.12ms
step:1138/2330 train_time:44537ms step_avg:39.14ms
step:1139/2330 train_time:44561ms step_avg:39.12ms
step:1140/2330 train_time:44618ms step_avg:39.14ms
step:1141/2330 train_time:44641ms step_avg:39.12ms
step:1142/2330 train_time:44697ms step_avg:39.14ms
step:1143/2330 train_time:44719ms step_avg:39.12ms
step:1144/2330 train_time:44775ms step_avg:39.14ms
step:1145/2330 train_time:44798ms step_avg:39.13ms
step:1146/2330 train_time:44854ms step_avg:39.14ms
step:1147/2330 train_time:44877ms step_avg:39.13ms
step:1148/2330 train_time:44933ms step_avg:39.14ms
step:1149/2330 train_time:44956ms step_avg:39.13ms
step:1150/2330 train_time:45012ms step_avg:39.14ms
step:1151/2330 train_time:45035ms step_avg:39.13ms
step:1152/2330 train_time:45091ms step_avg:39.14ms
step:1153/2330 train_time:45113ms step_avg:39.13ms
step:1154/2330 train_time:45169ms step_avg:39.14ms
step:1155/2330 train_time:45191ms step_avg:39.13ms
step:1156/2330 train_time:45246ms step_avg:39.14ms
step:1157/2330 train_time:45268ms step_avg:39.13ms
step:1158/2330 train_time:45325ms step_avg:39.14ms
step:1159/2330 train_time:45346ms step_avg:39.13ms
step:1160/2330 train_time:45403ms step_avg:39.14ms
step:1161/2330 train_time:45425ms step_avg:39.13ms
step:1162/2330 train_time:45482ms step_avg:39.14ms
step:1163/2330 train_time:45504ms step_avg:39.13ms
step:1164/2330 train_time:45561ms step_avg:39.14ms
step:1165/2330 train_time:45582ms step_avg:39.13ms
step:1166/2330 train_time:45638ms step_avg:39.14ms
step:1167/2330 train_time:45661ms step_avg:39.13ms
step:1168/2330 train_time:45717ms step_avg:39.14ms
step:1169/2330 train_time:45741ms step_avg:39.13ms
step:1170/2330 train_time:45796ms step_avg:39.14ms
step:1171/2330 train_time:45820ms step_avg:39.13ms
step:1172/2330 train_time:45876ms step_avg:39.14ms
step:1173/2330 train_time:45899ms step_avg:39.13ms
step:1174/2330 train_time:45955ms step_avg:39.14ms
step:1175/2330 train_time:45978ms step_avg:39.13ms
step:1176/2330 train_time:46034ms step_avg:39.14ms
step:1177/2330 train_time:46057ms step_avg:39.13ms
step:1178/2330 train_time:46114ms step_avg:39.15ms
step:1179/2330 train_time:46137ms step_avg:39.13ms
step:1180/2330 train_time:46193ms step_avg:39.15ms
step:1181/2330 train_time:46216ms step_avg:39.13ms
step:1182/2330 train_time:46272ms step_avg:39.15ms
step:1183/2330 train_time:46295ms step_avg:39.13ms
step:1184/2330 train_time:46351ms step_avg:39.15ms
step:1185/2330 train_time:46373ms step_avg:39.13ms
step:1186/2330 train_time:46429ms step_avg:39.15ms
step:1187/2330 train_time:46452ms step_avg:39.13ms
step:1188/2330 train_time:46508ms step_avg:39.15ms
step:1189/2330 train_time:46529ms step_avg:39.13ms
step:1190/2330 train_time:46586ms step_avg:39.15ms
step:1191/2330 train_time:46608ms step_avg:39.13ms
step:1192/2330 train_time:46665ms step_avg:39.15ms
step:1193/2330 train_time:46687ms step_avg:39.13ms
step:1194/2330 train_time:46743ms step_avg:39.15ms
step:1195/2330 train_time:46766ms step_avg:39.13ms
step:1196/2330 train_time:46822ms step_avg:39.15ms
step:1197/2330 train_time:46844ms step_avg:39.13ms
step:1198/2330 train_time:46900ms step_avg:39.15ms
step:1199/2330 train_time:46922ms step_avg:39.13ms
step:1200/2330 train_time:46978ms step_avg:39.15ms
step:1201/2330 train_time:47001ms step_avg:39.14ms
step:1202/2330 train_time:47058ms step_avg:39.15ms
step:1203/2330 train_time:47080ms step_avg:39.14ms
step:1204/2330 train_time:47136ms step_avg:39.15ms
step:1205/2330 train_time:47160ms step_avg:39.14ms
step:1206/2330 train_time:47217ms step_avg:39.15ms
step:1207/2330 train_time:47240ms step_avg:39.14ms
step:1208/2330 train_time:47297ms step_avg:39.15ms
step:1209/2330 train_time:47320ms step_avg:39.14ms
step:1210/2330 train_time:47377ms step_avg:39.15ms
step:1211/2330 train_time:47400ms step_avg:39.14ms
step:1212/2330 train_time:47456ms step_avg:39.16ms
step:1213/2330 train_time:47480ms step_avg:39.14ms
step:1214/2330 train_time:47536ms step_avg:39.16ms
step:1215/2330 train_time:47559ms step_avg:39.14ms
step:1216/2330 train_time:47616ms step_avg:39.16ms
step:1217/2330 train_time:47639ms step_avg:39.14ms
step:1218/2330 train_time:47695ms step_avg:39.16ms
step:1219/2330 train_time:47719ms step_avg:39.15ms
step:1220/2330 train_time:47774ms step_avg:39.16ms
step:1221/2330 train_time:47797ms step_avg:39.15ms
step:1222/2330 train_time:47853ms step_avg:39.16ms
step:1223/2330 train_time:47875ms step_avg:39.15ms
step:1224/2330 train_time:47931ms step_avg:39.16ms
step:1225/2330 train_time:47954ms step_avg:39.15ms
step:1226/2330 train_time:48011ms step_avg:39.16ms
step:1227/2330 train_time:48034ms step_avg:39.15ms
step:1228/2330 train_time:48090ms step_avg:39.16ms
step:1229/2330 train_time:48112ms step_avg:39.15ms
step:1230/2330 train_time:48169ms step_avg:39.16ms
step:1231/2330 train_time:48190ms step_avg:39.15ms
step:1232/2330 train_time:48247ms step_avg:39.16ms
step:1233/2330 train_time:48268ms step_avg:39.15ms
step:1234/2330 train_time:48325ms step_avg:39.16ms
step:1235/2330 train_time:48347ms step_avg:39.15ms
step:1236/2330 train_time:48404ms step_avg:39.16ms
step:1237/2330 train_time:48426ms step_avg:39.15ms
step:1238/2330 train_time:48483ms step_avg:39.16ms
step:1239/2330 train_time:48505ms step_avg:39.15ms
step:1240/2330 train_time:48562ms step_avg:39.16ms
step:1241/2330 train_time:48584ms step_avg:39.15ms
step:1242/2330 train_time:48639ms step_avg:39.16ms
step:1243/2330 train_time:48662ms step_avg:39.15ms
step:1244/2330 train_time:48718ms step_avg:39.16ms
step:1245/2330 train_time:48740ms step_avg:39.15ms
step:1246/2330 train_time:48796ms step_avg:39.16ms
step:1247/2330 train_time:48819ms step_avg:39.15ms
step:1248/2330 train_time:48875ms step_avg:39.16ms
step:1249/2330 train_time:48897ms step_avg:39.15ms
step:1250/2330 train_time:48954ms step_avg:39.16ms
step:1250/2330 val_loss:5.3444 train_time:49050ms step_avg:39.24ms
step:1251/2330 train_time:49063ms step_avg:39.22ms
step:1252/2330 train_time:49075ms step_avg:39.20ms
step:1253/2330 train_time:49084ms step_avg:39.17ms
step:1254/2330 train_time:49112ms step_avg:39.16ms
step:1255/2330 train_time:49134ms step_avg:39.15ms
step:1256/2330 train_time:49188ms step_avg:39.16ms
step:1257/2330 train_time:49210ms step_avg:39.15ms
step:1258/2330 train_time:49265ms step_avg:39.16ms
step:1259/2330 train_time:49286ms step_avg:39.15ms
step:1260/2330 train_time:49345ms step_avg:39.16ms
step:1261/2330 train_time:49369ms step_avg:39.15ms
step:1262/2330 train_time:49430ms step_avg:39.17ms
step:1263/2330 train_time:49452ms step_avg:39.15ms
step:1264/2330 train_time:49510ms step_avg:39.17ms
step:1265/2330 train_time:49532ms step_avg:39.16ms
step:1266/2330 train_time:49588ms step_avg:39.17ms
step:1267/2330 train_time:49610ms step_avg:39.16ms
step:1268/2330 train_time:49666ms step_avg:39.17ms
step:1269/2330 train_time:49688ms step_avg:39.15ms
step:1270/2330 train_time:49744ms step_avg:39.17ms
step:1271/2330 train_time:49766ms step_avg:39.16ms
step:1272/2330 train_time:49822ms step_avg:39.17ms
step:1273/2330 train_time:49844ms step_avg:39.15ms
step:1274/2330 train_time:49899ms step_avg:39.17ms
step:1275/2330 train_time:49921ms step_avg:39.15ms
step:1276/2330 train_time:49977ms step_avg:39.17ms
step:1277/2330 train_time:50000ms step_avg:39.15ms
step:1278/2330 train_time:50057ms step_avg:39.17ms
step:1279/2330 train_time:50079ms step_avg:39.16ms
step:1280/2330 train_time:50135ms step_avg:39.17ms
step:1281/2330 train_time:50158ms step_avg:39.16ms
step:1282/2330 train_time:50213ms step_avg:39.17ms
step:1283/2330 train_time:50235ms step_avg:39.15ms
step:1284/2330 train_time:50291ms step_avg:39.17ms
step:1285/2330 train_time:50315ms step_avg:39.16ms
step:1286/2330 train_time:50373ms step_avg:39.17ms
step:1287/2330 train_time:50395ms step_avg:39.16ms
step:1288/2330 train_time:50452ms step_avg:39.17ms
step:1289/2330 train_time:50476ms step_avg:39.16ms
step:1290/2330 train_time:50533ms step_avg:39.17ms
step:1291/2330 train_time:50555ms step_avg:39.16ms
step:1292/2330 train_time:50612ms step_avg:39.17ms
step:1293/2330 train_time:50634ms step_avg:39.16ms
step:1294/2330 train_time:50689ms step_avg:39.17ms
step:1295/2330 train_time:50711ms step_avg:39.16ms
step:1296/2330 train_time:50767ms step_avg:39.17ms
step:1297/2330 train_time:50789ms step_avg:39.16ms
step:1298/2330 train_time:50846ms step_avg:39.17ms
step:1299/2330 train_time:50867ms step_avg:39.16ms
step:1300/2330 train_time:50924ms step_avg:39.17ms
step:1301/2330 train_time:50946ms step_avg:39.16ms
step:1302/2330 train_time:51002ms step_avg:39.17ms
step:1303/2330 train_time:51025ms step_avg:39.16ms
step:1304/2330 train_time:51081ms step_avg:39.17ms
step:1305/2330 train_time:51103ms step_avg:39.16ms
step:1306/2330 train_time:51159ms step_avg:39.17ms
step:1307/2330 train_time:51182ms step_avg:39.16ms
step:1308/2330 train_time:51237ms step_avg:39.17ms
step:1309/2330 train_time:51261ms step_avg:39.16ms
step:1310/2330 train_time:51317ms step_avg:39.17ms
step:1311/2330 train_time:51341ms step_avg:39.16ms
step:1312/2330 train_time:51399ms step_avg:39.18ms
step:1313/2330 train_time:51422ms step_avg:39.16ms
step:1314/2330 train_time:51479ms step_avg:39.18ms
step:1315/2330 train_time:51503ms step_avg:39.17ms
step:1316/2330 train_time:51560ms step_avg:39.18ms
step:1317/2330 train_time:51584ms step_avg:39.17ms
step:1318/2330 train_time:51639ms step_avg:39.18ms
step:1319/2330 train_time:51663ms step_avg:39.17ms
step:1320/2330 train_time:51719ms step_avg:39.18ms
step:1321/2330 train_time:51742ms step_avg:39.17ms
step:1322/2330 train_time:51797ms step_avg:39.18ms
step:1323/2330 train_time:51820ms step_avg:39.17ms
step:1324/2330 train_time:51876ms step_avg:39.18ms
step:1325/2330 train_time:51899ms step_avg:39.17ms
step:1326/2330 train_time:51954ms step_avg:39.18ms
step:1327/2330 train_time:51977ms step_avg:39.17ms
step:1328/2330 train_time:52032ms step_avg:39.18ms
step:1329/2330 train_time:52054ms step_avg:39.17ms
step:1330/2330 train_time:52110ms step_avg:39.18ms
step:1331/2330 train_time:52131ms step_avg:39.17ms
step:1332/2330 train_time:52187ms step_avg:39.18ms
step:1333/2330 train_time:52209ms step_avg:39.17ms
step:1334/2330 train_time:52266ms step_avg:39.18ms
step:1335/2330 train_time:52288ms step_avg:39.17ms
step:1336/2330 train_time:52345ms step_avg:39.18ms
step:1337/2330 train_time:52367ms step_avg:39.17ms
step:1338/2330 train_time:52424ms step_avg:39.18ms
step:1339/2330 train_time:52447ms step_avg:39.17ms
step:1340/2330 train_time:52504ms step_avg:39.18ms
step:1341/2330 train_time:52526ms step_avg:39.17ms
step:1342/2330 train_time:52582ms step_avg:39.18ms
step:1343/2330 train_time:52604ms step_avg:39.17ms
step:1344/2330 train_time:52660ms step_avg:39.18ms
step:1345/2330 train_time:52683ms step_avg:39.17ms
step:1346/2330 train_time:52739ms step_avg:39.18ms
step:1347/2330 train_time:52761ms step_avg:39.17ms
step:1348/2330 train_time:52817ms step_avg:39.18ms
step:1349/2330 train_time:52840ms step_avg:39.17ms
step:1350/2330 train_time:52895ms step_avg:39.18ms
step:1351/2330 train_time:52918ms step_avg:39.17ms
step:1352/2330 train_time:52974ms step_avg:39.18ms
step:1353/2330 train_time:52996ms step_avg:39.17ms
step:1354/2330 train_time:53052ms step_avg:39.18ms
step:1355/2330 train_time:53074ms step_avg:39.17ms
step:1356/2330 train_time:53131ms step_avg:39.18ms
step:1357/2330 train_time:53153ms step_avg:39.17ms
step:1358/2330 train_time:53208ms step_avg:39.18ms
step:1359/2330 train_time:53230ms step_avg:39.17ms
step:1360/2330 train_time:53285ms step_avg:39.18ms
step:1361/2330 train_time:53307ms step_avg:39.17ms
step:1362/2330 train_time:53363ms step_avg:39.18ms
step:1363/2330 train_time:53385ms step_avg:39.17ms
step:1364/2330 train_time:53442ms step_avg:39.18ms
step:1365/2330 train_time:53464ms step_avg:39.17ms
step:1366/2330 train_time:53520ms step_avg:39.18ms
step:1367/2330 train_time:53543ms step_avg:39.17ms
step:1368/2330 train_time:53599ms step_avg:39.18ms
step:1369/2330 train_time:53622ms step_avg:39.17ms
step:1370/2330 train_time:53679ms step_avg:39.18ms
step:1371/2330 train_time:53701ms step_avg:39.17ms
step:1372/2330 train_time:53758ms step_avg:39.18ms
step:1373/2330 train_time:53781ms step_avg:39.17ms
step:1374/2330 train_time:53837ms step_avg:39.18ms
step:1375/2330 train_time:53860ms step_avg:39.17ms
step:1376/2330 train_time:53916ms step_avg:39.18ms
step:1377/2330 train_time:53939ms step_avg:39.17ms
step:1378/2330 train_time:53995ms step_avg:39.18ms
step:1379/2330 train_time:54017ms step_avg:39.17ms
step:1380/2330 train_time:54073ms step_avg:39.18ms
step:1381/2330 train_time:54096ms step_avg:39.17ms
step:1382/2330 train_time:54153ms step_avg:39.18ms
step:1383/2330 train_time:54175ms step_avg:39.17ms
step:1384/2330 train_time:54230ms step_avg:39.18ms
step:1385/2330 train_time:54253ms step_avg:39.17ms
step:1386/2330 train_time:54307ms step_avg:39.18ms
step:1387/2330 train_time:54329ms step_avg:39.17ms
step:1388/2330 train_time:54385ms step_avg:39.18ms
step:1389/2330 train_time:54408ms step_avg:39.17ms
step:1390/2330 train_time:54464ms step_avg:39.18ms
step:1391/2330 train_time:54486ms step_avg:39.17ms
step:1392/2330 train_time:54542ms step_avg:39.18ms
step:1393/2330 train_time:54564ms step_avg:39.17ms
step:1394/2330 train_time:54621ms step_avg:39.18ms
step:1395/2330 train_time:54643ms step_avg:39.17ms
step:1396/2330 train_time:54699ms step_avg:39.18ms
step:1397/2330 train_time:54722ms step_avg:39.17ms
step:1398/2330 train_time:54778ms step_avg:39.18ms
step:1399/2330 train_time:54801ms step_avg:39.17ms
step:1400/2330 train_time:54857ms step_avg:39.18ms
step:1401/2330 train_time:54880ms step_avg:39.17ms
step:1402/2330 train_time:54937ms step_avg:39.18ms
step:1403/2330 train_time:54960ms step_avg:39.17ms
step:1404/2330 train_time:55016ms step_avg:39.19ms
step:1405/2330 train_time:55039ms step_avg:39.17ms
step:1406/2330 train_time:55096ms step_avg:39.19ms
step:1407/2330 train_time:55119ms step_avg:39.18ms
step:1408/2330 train_time:55175ms step_avg:39.19ms
step:1409/2330 train_time:55198ms step_avg:39.18ms
step:1410/2330 train_time:55255ms step_avg:39.19ms
step:1411/2330 train_time:55278ms step_avg:39.18ms
step:1412/2330 train_time:55333ms step_avg:39.19ms
step:1413/2330 train_time:55356ms step_avg:39.18ms
step:1414/2330 train_time:55412ms step_avg:39.19ms
step:1415/2330 train_time:55434ms step_avg:39.18ms
step:1416/2330 train_time:55490ms step_avg:39.19ms
step:1417/2330 train_time:55512ms step_avg:39.18ms
step:1418/2330 train_time:55567ms step_avg:39.19ms
step:1419/2330 train_time:55589ms step_avg:39.17ms
step:1420/2330 train_time:55645ms step_avg:39.19ms
step:1421/2330 train_time:55667ms step_avg:39.17ms
step:1422/2330 train_time:55723ms step_avg:39.19ms
step:1423/2330 train_time:55746ms step_avg:39.17ms
step:1424/2330 train_time:55802ms step_avg:39.19ms
step:1425/2330 train_time:55824ms step_avg:39.17ms
step:1426/2330 train_time:55881ms step_avg:39.19ms
step:1427/2330 train_time:55903ms step_avg:39.18ms
step:1428/2330 train_time:55959ms step_avg:39.19ms
step:1429/2330 train_time:55981ms step_avg:39.18ms
step:1430/2330 train_time:56038ms step_avg:39.19ms
step:1431/2330 train_time:56060ms step_avg:39.18ms
step:1432/2330 train_time:56116ms step_avg:39.19ms
step:1433/2330 train_time:56140ms step_avg:39.18ms
step:1434/2330 train_time:56196ms step_avg:39.19ms
step:1435/2330 train_time:56220ms step_avg:39.18ms
step:1436/2330 train_time:56276ms step_avg:39.19ms
step:1437/2330 train_time:56299ms step_avg:39.18ms
step:1438/2330 train_time:56356ms step_avg:39.19ms
step:1439/2330 train_time:56380ms step_avg:39.18ms
step:1440/2330 train_time:56436ms step_avg:39.19ms
step:1441/2330 train_time:56460ms step_avg:39.18ms
step:1442/2330 train_time:56516ms step_avg:39.19ms
step:1443/2330 train_time:56539ms step_avg:39.18ms
step:1444/2330 train_time:56596ms step_avg:39.19ms
step:1445/2330 train_time:56618ms step_avg:39.18ms
step:1446/2330 train_time:56675ms step_avg:39.19ms
step:1447/2330 train_time:56697ms step_avg:39.18ms
step:1448/2330 train_time:56752ms step_avg:39.19ms
step:1449/2330 train_time:56775ms step_avg:39.18ms
step:1450/2330 train_time:56831ms step_avg:39.19ms
step:1451/2330 train_time:56853ms step_avg:39.18ms
step:1452/2330 train_time:56909ms step_avg:39.19ms
step:1453/2330 train_time:56931ms step_avg:39.18ms
step:1454/2330 train_time:56987ms step_avg:39.19ms
step:1455/2330 train_time:57009ms step_avg:39.18ms
step:1456/2330 train_time:57066ms step_avg:39.19ms
step:1457/2330 train_time:57088ms step_avg:39.18ms
step:1458/2330 train_time:57145ms step_avg:39.19ms
step:1459/2330 train_time:57166ms step_avg:39.18ms
step:1460/2330 train_time:57223ms step_avg:39.19ms
step:1461/2330 train_time:57246ms step_avg:39.18ms
step:1462/2330 train_time:57302ms step_avg:39.19ms
step:1463/2330 train_time:57325ms step_avg:39.18ms
step:1464/2330 train_time:57381ms step_avg:39.19ms
step:1465/2330 train_time:57404ms step_avg:39.18ms
step:1466/2330 train_time:57459ms step_avg:39.19ms
step:1467/2330 train_time:57482ms step_avg:39.18ms
step:1468/2330 train_time:57538ms step_avg:39.19ms
step:1469/2330 train_time:57561ms step_avg:39.18ms
step:1470/2330 train_time:57618ms step_avg:39.20ms
step:1471/2330 train_time:57642ms step_avg:39.19ms
step:1472/2330 train_time:57698ms step_avg:39.20ms
step:1473/2330 train_time:57721ms step_avg:39.19ms
step:1474/2330 train_time:57777ms step_avg:39.20ms
step:1475/2330 train_time:57800ms step_avg:39.19ms
step:1476/2330 train_time:57856ms step_avg:39.20ms
step:1477/2330 train_time:57879ms step_avg:39.19ms
step:1478/2330 train_time:57936ms step_avg:39.20ms
step:1479/2330 train_time:57959ms step_avg:39.19ms
step:1480/2330 train_time:58015ms step_avg:39.20ms
step:1481/2330 train_time:58038ms step_avg:39.19ms
step:1482/2330 train_time:58094ms step_avg:39.20ms
step:1483/2330 train_time:58116ms step_avg:39.19ms
step:1484/2330 train_time:58174ms step_avg:39.20ms
step:1485/2330 train_time:58197ms step_avg:39.19ms
step:1486/2330 train_time:58253ms step_avg:39.20ms
step:1487/2330 train_time:58275ms step_avg:39.19ms
step:1488/2330 train_time:58331ms step_avg:39.20ms
step:1489/2330 train_time:58353ms step_avg:39.19ms
step:1490/2330 train_time:58409ms step_avg:39.20ms
step:1491/2330 train_time:58431ms step_avg:39.19ms
step:1492/2330 train_time:58487ms step_avg:39.20ms
step:1493/2330 train_time:58509ms step_avg:39.19ms
step:1494/2330 train_time:58566ms step_avg:39.20ms
step:1495/2330 train_time:58588ms step_avg:39.19ms
step:1496/2330 train_time:58645ms step_avg:39.20ms
step:1497/2330 train_time:58668ms step_avg:39.19ms
step:1498/2330 train_time:58724ms step_avg:39.20ms
step:1499/2330 train_time:58747ms step_avg:39.19ms
step:1500/2330 train_time:58804ms step_avg:39.20ms
step:1500/2330 val_loss:5.3079 train_time:58899ms step_avg:39.27ms
step:1501/2330 train_time:58912ms step_avg:39.25ms
step:1502/2330 train_time:58923ms step_avg:39.23ms
step:1503/2330 train_time:58932ms step_avg:39.21ms
step:1504/2330 train_time:58963ms step_avg:39.20ms
step:1505/2330 train_time:58984ms step_avg:39.19ms
step:1506/2330 train_time:59039ms step_avg:39.20ms
step:1507/2330 train_time:59060ms step_avg:39.19ms
step:1508/2330 train_time:59115ms step_avg:39.20ms
step:1509/2330 train_time:59137ms step_avg:39.19ms
step:1510/2330 train_time:59194ms step_avg:39.20ms
step:1511/2330 train_time:59216ms step_avg:39.19ms
step:1512/2330 train_time:59276ms step_avg:39.20ms
step:1513/2330 train_time:59299ms step_avg:39.19ms
step:1514/2330 train_time:59356ms step_avg:39.20ms
step:1515/2330 train_time:59378ms step_avg:39.19ms
step:1516/2330 train_time:59434ms step_avg:39.20ms
step:1517/2330 train_time:59456ms step_avg:39.19ms
step:1518/2330 train_time:59512ms step_avg:39.20ms
step:1519/2330 train_time:59534ms step_avg:39.19ms
step:1520/2330 train_time:59589ms step_avg:39.20ms
step:1521/2330 train_time:59611ms step_avg:39.19ms
step:1522/2330 train_time:59666ms step_avg:39.20ms
step:1523/2330 train_time:59690ms step_avg:39.19ms
step:1524/2330 train_time:59745ms step_avg:39.20ms
step:1525/2330 train_time:59768ms step_avg:39.19ms
step:1526/2330 train_time:59824ms step_avg:39.20ms
step:1527/2330 train_time:59848ms step_avg:39.19ms
step:1528/2330 train_time:59904ms step_avg:39.20ms
step:1529/2330 train_time:59929ms step_avg:39.19ms
step:1530/2330 train_time:59985ms step_avg:39.21ms
step:1531/2330 train_time:60007ms step_avg:39.19ms
step:1532/2330 train_time:60062ms step_avg:39.21ms
step:1533/2330 train_time:60085ms step_avg:39.19ms
step:1534/2330 train_time:60140ms step_avg:39.20ms
step:1535/2330 train_time:60162ms step_avg:39.19ms
step:1536/2330 train_time:60218ms step_avg:39.20ms
step:1537/2330 train_time:60240ms step_avg:39.19ms
step:1538/2330 train_time:60297ms step_avg:39.21ms
step:1539/2330 train_time:60319ms step_avg:39.19ms
step:1540/2330 train_time:60375ms step_avg:39.20ms
step:1541/2330 train_time:60398ms step_avg:39.19ms
step:1542/2330 train_time:60454ms step_avg:39.20ms
step:1543/2330 train_time:60476ms step_avg:39.19ms
step:1544/2330 train_time:60531ms step_avg:39.20ms
step:1545/2330 train_time:60553ms step_avg:39.19ms
step:1546/2330 train_time:60609ms step_avg:39.20ms
step:1547/2330 train_time:60631ms step_avg:39.19ms
step:1548/2330 train_time:60687ms step_avg:39.20ms
step:1549/2330 train_time:60710ms step_avg:39.19ms
step:1550/2330 train_time:60766ms step_avg:39.20ms
step:1551/2330 train_time:60789ms step_avg:39.19ms
step:1552/2330 train_time:60846ms step_avg:39.20ms
step:1553/2330 train_time:60870ms step_avg:39.20ms
step:1554/2330 train_time:60927ms step_avg:39.21ms
step:1555/2330 train_time:60950ms step_avg:39.20ms
step:1556/2330 train_time:61006ms step_avg:39.21ms
step:1557/2330 train_time:61029ms step_avg:39.20ms
step:1558/2330 train_time:61085ms step_avg:39.21ms
step:1559/2330 train_time:61108ms step_avg:39.20ms
step:1560/2330 train_time:61164ms step_avg:39.21ms
step:1561/2330 train_time:61186ms step_avg:39.20ms
step:1562/2330 train_time:61243ms step_avg:39.21ms
step:1563/2330 train_time:61267ms step_avg:39.20ms
step:1564/2330 train_time:61322ms step_avg:39.21ms
step:1565/2330 train_time:61346ms step_avg:39.20ms
step:1566/2330 train_time:61402ms step_avg:39.21ms
step:1567/2330 train_time:61424ms step_avg:39.20ms
step:1568/2330 train_time:61480ms step_avg:39.21ms
step:1569/2330 train_time:61501ms step_avg:39.20ms
step:1570/2330 train_time:61558ms step_avg:39.21ms
step:1571/2330 train_time:61580ms step_avg:39.20ms
step:1572/2330 train_time:61636ms step_avg:39.21ms
step:1573/2330 train_time:61657ms step_avg:39.20ms
step:1574/2330 train_time:61713ms step_avg:39.21ms
step:1575/2330 train_time:61736ms step_avg:39.20ms
step:1576/2330 train_time:61793ms step_avg:39.21ms
step:1577/2330 train_time:61817ms step_avg:39.20ms
step:1578/2330 train_time:61874ms step_avg:39.21ms
step:1579/2330 train_time:61897ms step_avg:39.20ms
step:1580/2330 train_time:61953ms step_avg:39.21ms
step:1581/2330 train_time:61975ms step_avg:39.20ms
step:1582/2330 train_time:62030ms step_avg:39.21ms
step:1583/2330 train_time:62053ms step_avg:39.20ms
step:1584/2330 train_time:62110ms step_avg:39.21ms
step:1585/2330 train_time:62133ms step_avg:39.20ms
step:1586/2330 train_time:62189ms step_avg:39.21ms
step:1587/2330 train_time:62213ms step_avg:39.20ms
step:1588/2330 train_time:62269ms step_avg:39.21ms
step:1589/2330 train_time:62292ms step_avg:39.20ms
step:1590/2330 train_time:62349ms step_avg:39.21ms
step:1591/2330 train_time:62372ms step_avg:39.20ms
step:1592/2330 train_time:62428ms step_avg:39.21ms
step:1593/2330 train_time:62452ms step_avg:39.20ms
step:1594/2330 train_time:62509ms step_avg:39.21ms
step:1595/2330 train_time:62531ms step_avg:39.20ms
step:1596/2330 train_time:62587ms step_avg:39.21ms
step:1597/2330 train_time:62610ms step_avg:39.20ms
step:1598/2330 train_time:62666ms step_avg:39.21ms
step:1599/2330 train_time:62689ms step_avg:39.21ms
step:1600/2330 train_time:62746ms step_avg:39.22ms
step:1601/2330 train_time:62769ms step_avg:39.21ms
step:1602/2330 train_time:62826ms step_avg:39.22ms
step:1603/2330 train_time:62850ms step_avg:39.21ms
step:1604/2330 train_time:62906ms step_avg:39.22ms
step:1605/2330 train_time:62928ms step_avg:39.21ms
step:1606/2330 train_time:62984ms step_avg:39.22ms
step:1607/2330 train_time:63007ms step_avg:39.21ms
step:1608/2330 train_time:63062ms step_avg:39.22ms
step:1609/2330 train_time:63086ms step_avg:39.21ms
step:1610/2330 train_time:63142ms step_avg:39.22ms
step:1611/2330 train_time:63164ms step_avg:39.21ms
step:1612/2330 train_time:63220ms step_avg:39.22ms
step:1613/2330 train_time:63242ms step_avg:39.21ms
step:1614/2330 train_time:63297ms step_avg:39.22ms
step:1615/2330 train_time:63320ms step_avg:39.21ms
step:1616/2330 train_time:63376ms step_avg:39.22ms
step:1617/2330 train_time:63398ms step_avg:39.21ms
step:1618/2330 train_time:63454ms step_avg:39.22ms
step:1619/2330 train_time:63476ms step_avg:39.21ms
step:1620/2330 train_time:63533ms step_avg:39.22ms
step:1621/2330 train_time:63554ms step_avg:39.21ms
step:1622/2330 train_time:63610ms step_avg:39.22ms
step:1623/2330 train_time:63633ms step_avg:39.21ms
step:1624/2330 train_time:63689ms step_avg:39.22ms
step:1625/2330 train_time:63712ms step_avg:39.21ms
step:1626/2330 train_time:63769ms step_avg:39.22ms
step:1627/2330 train_time:63792ms step_avg:39.21ms
step:1628/2330 train_time:63849ms step_avg:39.22ms
step:1629/2330 train_time:63872ms step_avg:39.21ms
step:1630/2330 train_time:63928ms step_avg:39.22ms
step:1631/2330 train_time:63950ms step_avg:39.21ms
step:1632/2330 train_time:64006ms step_avg:39.22ms
step:1633/2330 train_time:64029ms step_avg:39.21ms
step:1634/2330 train_time:64086ms step_avg:39.22ms
step:1635/2330 train_time:64109ms step_avg:39.21ms
step:1636/2330 train_time:64165ms step_avg:39.22ms
step:1637/2330 train_time:64187ms step_avg:39.21ms
step:1638/2330 train_time:64244ms step_avg:39.22ms
step:1639/2330 train_time:64267ms step_avg:39.21ms
step:1640/2330 train_time:64323ms step_avg:39.22ms
step:1641/2330 train_time:64345ms step_avg:39.21ms
step:1642/2330 train_time:64401ms step_avg:39.22ms
step:1643/2330 train_time:64424ms step_avg:39.21ms
step:1644/2330 train_time:64479ms step_avg:39.22ms
step:1645/2330 train_time:64501ms step_avg:39.21ms
step:1646/2330 train_time:64557ms step_avg:39.22ms
step:1647/2330 train_time:64579ms step_avg:39.21ms
step:1648/2330 train_time:64635ms step_avg:39.22ms
step:1649/2330 train_time:64658ms step_avg:39.21ms
step:1650/2330 train_time:64716ms step_avg:39.22ms
step:1651/2330 train_time:64739ms step_avg:39.21ms
step:1652/2330 train_time:64795ms step_avg:39.22ms
step:1653/2330 train_time:64817ms step_avg:39.21ms
step:1654/2330 train_time:64873ms step_avg:39.22ms
step:1655/2330 train_time:64895ms step_avg:39.21ms
step:1656/2330 train_time:64952ms step_avg:39.22ms
step:1657/2330 train_time:64974ms step_avg:39.21ms
step:1658/2330 train_time:65030ms step_avg:39.22ms
step:1659/2330 train_time:65053ms step_avg:39.21ms
step:1660/2330 train_time:65109ms step_avg:39.22ms
step:1661/2330 train_time:65132ms step_avg:39.21ms
step:1662/2330 train_time:65188ms step_avg:39.22ms
step:1663/2330 train_time:65211ms step_avg:39.21ms
step:1664/2330 train_time:65267ms step_avg:39.22ms
step:1665/2330 train_time:65290ms step_avg:39.21ms
step:1666/2330 train_time:65348ms step_avg:39.22ms
step:1667/2330 train_time:65371ms step_avg:39.21ms
step:1668/2330 train_time:65427ms step_avg:39.22ms
step:1669/2330 train_time:65450ms step_avg:39.22ms
step:1670/2330 train_time:65506ms step_avg:39.23ms
step:1671/2330 train_time:65529ms step_avg:39.22ms
step:1672/2330 train_time:65585ms step_avg:39.23ms
step:1673/2330 train_time:65608ms step_avg:39.22ms
step:1674/2330 train_time:65664ms step_avg:39.23ms
step:1675/2330 train_time:65686ms step_avg:39.22ms
step:1676/2330 train_time:65743ms step_avg:39.23ms
step:1677/2330 train_time:65765ms step_avg:39.22ms
step:1678/2330 train_time:65821ms step_avg:39.23ms
step:1679/2330 train_time:65843ms step_avg:39.22ms
step:1680/2330 train_time:65899ms step_avg:39.23ms
step:1681/2330 train_time:65920ms step_avg:39.22ms
step:1682/2330 train_time:65977ms step_avg:39.23ms
step:1683/2330 train_time:65999ms step_avg:39.22ms
step:1684/2330 train_time:66056ms step_avg:39.23ms
step:1685/2330 train_time:66078ms step_avg:39.22ms
step:1686/2330 train_time:66135ms step_avg:39.23ms
step:1687/2330 train_time:66157ms step_avg:39.22ms
step:1688/2330 train_time:66214ms step_avg:39.23ms
step:1689/2330 train_time:66236ms step_avg:39.22ms
step:1690/2330 train_time:66293ms step_avg:39.23ms
step:1691/2330 train_time:66314ms step_avg:39.22ms
step:1692/2330 train_time:66370ms step_avg:39.23ms
step:1693/2330 train_time:66393ms step_avg:39.22ms
step:1694/2330 train_time:66449ms step_avg:39.23ms
step:1695/2330 train_time:66472ms step_avg:39.22ms
step:1696/2330 train_time:66528ms step_avg:39.23ms
step:1697/2330 train_time:66551ms step_avg:39.22ms
step:1698/2330 train_time:66607ms step_avg:39.23ms
step:1699/2330 train_time:66631ms step_avg:39.22ms
step:1700/2330 train_time:66688ms step_avg:39.23ms
step:1701/2330 train_time:66712ms step_avg:39.22ms
step:1702/2330 train_time:66768ms step_avg:39.23ms
step:1703/2330 train_time:66792ms step_avg:39.22ms
step:1704/2330 train_time:66848ms step_avg:39.23ms
step:1705/2330 train_time:66871ms step_avg:39.22ms
step:1706/2330 train_time:66927ms step_avg:39.23ms
step:1707/2330 train_time:66951ms step_avg:39.22ms
step:1708/2330 train_time:67007ms step_avg:39.23ms
step:1709/2330 train_time:67030ms step_avg:39.22ms
step:1710/2330 train_time:67086ms step_avg:39.23ms
step:1711/2330 train_time:67109ms step_avg:39.22ms
step:1712/2330 train_time:67165ms step_avg:39.23ms
step:1713/2330 train_time:67188ms step_avg:39.22ms
step:1714/2330 train_time:67244ms step_avg:39.23ms
step:1715/2330 train_time:67267ms step_avg:39.22ms
step:1716/2330 train_time:67322ms step_avg:39.23ms
step:1717/2330 train_time:67345ms step_avg:39.22ms
step:1718/2330 train_time:67401ms step_avg:39.23ms
step:1719/2330 train_time:67423ms step_avg:39.22ms
step:1720/2330 train_time:67479ms step_avg:39.23ms
step:1721/2330 train_time:67501ms step_avg:39.22ms
step:1722/2330 train_time:67558ms step_avg:39.23ms
step:1723/2330 train_time:67580ms step_avg:39.22ms
step:1724/2330 train_time:67636ms step_avg:39.23ms
step:1725/2330 train_time:67658ms step_avg:39.22ms
step:1726/2330 train_time:67715ms step_avg:39.23ms
step:1727/2330 train_time:67736ms step_avg:39.22ms
step:1728/2330 train_time:67793ms step_avg:39.23ms
step:1729/2330 train_time:67815ms step_avg:39.22ms
step:1730/2330 train_time:67871ms step_avg:39.23ms
step:1731/2330 train_time:67893ms step_avg:39.22ms
step:1732/2330 train_time:67949ms step_avg:39.23ms
step:1733/2330 train_time:67972ms step_avg:39.22ms
step:1734/2330 train_time:68028ms step_avg:39.23ms
step:1735/2330 train_time:68050ms step_avg:39.22ms
step:1736/2330 train_time:68107ms step_avg:39.23ms
step:1737/2330 train_time:68130ms step_avg:39.22ms
step:1738/2330 train_time:68187ms step_avg:39.23ms
step:1739/2330 train_time:68210ms step_avg:39.22ms
step:1740/2330 train_time:68266ms step_avg:39.23ms
step:1741/2330 train_time:68290ms step_avg:39.22ms
step:1742/2330 train_time:68346ms step_avg:39.23ms
step:1743/2330 train_time:68369ms step_avg:39.23ms
step:1744/2330 train_time:68425ms step_avg:39.23ms
step:1745/2330 train_time:68448ms step_avg:39.23ms
step:1746/2330 train_time:68505ms step_avg:39.24ms
step:1747/2330 train_time:68529ms step_avg:39.23ms
step:1748/2330 train_time:68584ms step_avg:39.24ms
step:1749/2330 train_time:68607ms step_avg:39.23ms
step:1750/2330 train_time:68663ms step_avg:39.24ms
step:1750/2330 val_loss:5.2753 train_time:68758ms step_avg:39.29ms
step:1751/2330 train_time:68771ms step_avg:39.27ms
step:1752/2330 train_time:68782ms step_avg:39.26ms
step:1753/2330 train_time:68793ms step_avg:39.24ms
step:1754/2330 train_time:68820ms step_avg:39.24ms
step:1755/2330 train_time:68842ms step_avg:39.23ms
step:1756/2330 train_time:68897ms step_avg:39.24ms
step:1757/2330 train_time:68918ms step_avg:39.22ms
step:1758/2330 train_time:68973ms step_avg:39.23ms
step:1759/2330 train_time:68994ms step_avg:39.22ms
step:1760/2330 train_time:69050ms step_avg:39.23ms
step:1761/2330 train_time:69075ms step_avg:39.23ms
step:1762/2330 train_time:69137ms step_avg:39.24ms
step:1763/2330 train_time:69161ms step_avg:39.23ms
step:1764/2330 train_time:69218ms step_avg:39.24ms
step:1765/2330 train_time:69240ms step_avg:39.23ms
step:1766/2330 train_time:69295ms step_avg:39.24ms
step:1767/2330 train_time:69317ms step_avg:39.23ms
step:1768/2330 train_time:69372ms step_avg:39.24ms
step:1769/2330 train_time:69394ms step_avg:39.23ms
step:1770/2330 train_time:69449ms step_avg:39.24ms
step:1771/2330 train_time:69472ms step_avg:39.23ms
step:1772/2330 train_time:69527ms step_avg:39.24ms
step:1773/2330 train_time:69549ms step_avg:39.23ms
step:1774/2330 train_time:69605ms step_avg:39.24ms
step:1775/2330 train_time:69627ms step_avg:39.23ms
step:1776/2330 train_time:69686ms step_avg:39.24ms
step:1777/2330 train_time:69708ms step_avg:39.23ms
step:1778/2330 train_time:69766ms step_avg:39.24ms
step:1779/2330 train_time:69790ms step_avg:39.23ms
step:1780/2330 train_time:69847ms step_avg:39.24ms
step:1781/2330 train_time:69870ms step_avg:39.23ms
step:1782/2330 train_time:69925ms step_avg:39.24ms
step:1783/2330 train_time:69948ms step_avg:39.23ms
step:1784/2330 train_time:70004ms step_avg:39.24ms
step:1785/2330 train_time:70028ms step_avg:39.23ms
step:1786/2330 train_time:70087ms step_avg:39.24ms
step:1787/2330 train_time:70111ms step_avg:39.23ms
step:1788/2330 train_time:70168ms step_avg:39.24ms
step:1789/2330 train_time:70192ms step_avg:39.24ms
step:1790/2330 train_time:70248ms step_avg:39.24ms
step:1791/2330 train_time:70271ms step_avg:39.24ms
step:1792/2330 train_time:70327ms step_avg:39.25ms
step:1793/2330 train_time:70350ms step_avg:39.24ms
step:1794/2330 train_time:70406ms step_avg:39.25ms
step:1795/2330 train_time:70429ms step_avg:39.24ms
step:1796/2330 train_time:70484ms step_avg:39.25ms
step:1797/2330 train_time:70507ms step_avg:39.24ms
step:1798/2330 train_time:70562ms step_avg:39.24ms
step:1799/2330 train_time:70585ms step_avg:39.24ms
step:1800/2330 train_time:70641ms step_avg:39.25ms
step:1801/2330 train_time:70664ms step_avg:39.24ms
step:1802/2330 train_time:70721ms step_avg:39.25ms
step:1803/2330 train_time:70743ms step_avg:39.24ms
step:1804/2330 train_time:70799ms step_avg:39.25ms
step:1805/2330 train_time:70822ms step_avg:39.24ms
step:1806/2330 train_time:70878ms step_avg:39.25ms
step:1807/2330 train_time:70900ms step_avg:39.24ms
step:1808/2330 train_time:70957ms step_avg:39.25ms
step:1809/2330 train_time:70980ms step_avg:39.24ms
step:1810/2330 train_time:71038ms step_avg:39.25ms
step:1811/2330 train_time:71060ms step_avg:39.24ms
step:1812/2330 train_time:71117ms step_avg:39.25ms
step:1813/2330 train_time:71140ms step_avg:39.24ms
step:1814/2330 train_time:71198ms step_avg:39.25ms
step:1815/2330 train_time:71220ms step_avg:39.24ms
step:1816/2330 train_time:71276ms step_avg:39.25ms
step:1817/2330 train_time:71298ms step_avg:39.24ms
step:1818/2330 train_time:71355ms step_avg:39.25ms
step:1819/2330 train_time:71376ms step_avg:39.24ms
step:1820/2330 train_time:71432ms step_avg:39.25ms
step:1821/2330 train_time:71454ms step_avg:39.24ms
step:1822/2330 train_time:71510ms step_avg:39.25ms
step:1823/2330 train_time:71532ms step_avg:39.24ms
step:1824/2330 train_time:71588ms step_avg:39.25ms
step:1825/2330 train_time:71610ms step_avg:39.24ms
step:1826/2330 train_time:71666ms step_avg:39.25ms
step:1827/2330 train_time:71689ms step_avg:39.24ms
step:1828/2330 train_time:71746ms step_avg:39.25ms
step:1829/2330 train_time:71769ms step_avg:39.24ms
step:1830/2330 train_time:71825ms step_avg:39.25ms
step:1831/2330 train_time:71849ms step_avg:39.24ms
step:1832/2330 train_time:71906ms step_avg:39.25ms
step:1833/2330 train_time:71929ms step_avg:39.24ms
step:1834/2330 train_time:71986ms step_avg:39.25ms
step:1835/2330 train_time:72009ms step_avg:39.24ms
step:1836/2330 train_time:72066ms step_avg:39.25ms
step:1837/2330 train_time:72090ms step_avg:39.24ms
step:1838/2330 train_time:72147ms step_avg:39.25ms
step:1839/2330 train_time:72171ms step_avg:39.24ms
step:1840/2330 train_time:72227ms step_avg:39.25ms
step:1841/2330 train_time:72250ms step_avg:39.24ms
step:1842/2330 train_time:72306ms step_avg:39.25ms
step:1843/2330 train_time:72329ms step_avg:39.25ms
step:1844/2330 train_time:72385ms step_avg:39.25ms
step:1845/2330 train_time:72409ms step_avg:39.25ms
step:1846/2330 train_time:72465ms step_avg:39.26ms
step:1847/2330 train_time:72488ms step_avg:39.25ms
step:1848/2330 train_time:72543ms step_avg:39.26ms
step:1849/2330 train_time:72566ms step_avg:39.25ms
step:1850/2330 train_time:72621ms step_avg:39.25ms
step:1851/2330 train_time:72644ms step_avg:39.25ms
step:1852/2330 train_time:72699ms step_avg:39.25ms
step:1853/2330 train_time:72721ms step_avg:39.25ms
step:1854/2330 train_time:72777ms step_avg:39.25ms
step:1855/2330 train_time:72799ms step_avg:39.24ms
step:1856/2330 train_time:72855ms step_avg:39.25ms
step:1857/2330 train_time:72877ms step_avg:39.24ms
step:1858/2330 train_time:72934ms step_avg:39.25ms
step:1859/2330 train_time:72956ms step_avg:39.24ms
step:1860/2330 train_time:73013ms step_avg:39.25ms
step:1861/2330 train_time:73035ms step_avg:39.24ms
step:1862/2330 train_time:73091ms step_avg:39.25ms
step:1863/2330 train_time:73113ms step_avg:39.24ms
step:1864/2330 train_time:73170ms step_avg:39.25ms
step:1865/2330 train_time:73192ms step_avg:39.24ms
step:1866/2330 train_time:73248ms step_avg:39.25ms
step:1867/2330 train_time:73271ms step_avg:39.25ms
step:1868/2330 train_time:73326ms step_avg:39.25ms
step:1869/2330 train_time:73349ms step_avg:39.25ms
step:1870/2330 train_time:73405ms step_avg:39.25ms
step:1871/2330 train_time:73428ms step_avg:39.25ms
step:1872/2330 train_time:73484ms step_avg:39.25ms
step:1873/2330 train_time:73507ms step_avg:39.25ms
step:1874/2330 train_time:73563ms step_avg:39.25ms
step:1875/2330 train_time:73585ms step_avg:39.25ms
step:1876/2330 train_time:73641ms step_avg:39.25ms
step:1877/2330 train_time:73664ms step_avg:39.25ms
step:1878/2330 train_time:73721ms step_avg:39.26ms
step:1879/2330 train_time:73744ms step_avg:39.25ms
step:1880/2330 train_time:73799ms step_avg:39.25ms
step:1881/2330 train_time:73822ms step_avg:39.25ms
step:1882/2330 train_time:73878ms step_avg:39.25ms
step:1883/2330 train_time:73900ms step_avg:39.25ms
step:1884/2330 train_time:73956ms step_avg:39.25ms
step:1885/2330 train_time:73978ms step_avg:39.25ms
step:1886/2330 train_time:74034ms step_avg:39.25ms
step:1887/2330 train_time:74056ms step_avg:39.25ms
step:1888/2330 train_time:74112ms step_avg:39.25ms
step:1889/2330 train_time:74135ms step_avg:39.25ms
step:1890/2330 train_time:74191ms step_avg:39.25ms
step:1891/2330 train_time:74214ms step_avg:39.25ms
step:1892/2330 train_time:74270ms step_avg:39.25ms
step:1893/2330 train_time:74292ms step_avg:39.25ms
step:1894/2330 train_time:74348ms step_avg:39.25ms
step:1895/2330 train_time:74370ms step_avg:39.25ms
step:1896/2330 train_time:74426ms step_avg:39.25ms
step:1897/2330 train_time:74448ms step_avg:39.25ms
step:1898/2330 train_time:74505ms step_avg:39.25ms
step:1899/2330 train_time:74527ms step_avg:39.25ms
step:1900/2330 train_time:74584ms step_avg:39.25ms
step:1901/2330 train_time:74608ms step_avg:39.25ms
step:1902/2330 train_time:74665ms step_avg:39.26ms
step:1903/2330 train_time:74688ms step_avg:39.25ms
step:1904/2330 train_time:74744ms step_avg:39.26ms
step:1905/2330 train_time:74767ms step_avg:39.25ms
step:1906/2330 train_time:74823ms step_avg:39.26ms
step:1907/2330 train_time:74846ms step_avg:39.25ms
step:1908/2330 train_time:74902ms step_avg:39.26ms
step:1909/2330 train_time:74925ms step_avg:39.25ms
step:1910/2330 train_time:74983ms step_avg:39.26ms
step:1911/2330 train_time:75005ms step_avg:39.25ms
step:1912/2330 train_time:75062ms step_avg:39.26ms
step:1913/2330 train_time:75084ms step_avg:39.25ms
step:1914/2330 train_time:75141ms step_avg:39.26ms
step:1915/2330 train_time:75163ms step_avg:39.25ms
step:1916/2330 train_time:75220ms step_avg:39.26ms
step:1917/2330 train_time:75242ms step_avg:39.25ms
step:1918/2330 train_time:75298ms step_avg:39.26ms
step:1919/2330 train_time:75319ms step_avg:39.25ms
step:1920/2330 train_time:75376ms step_avg:39.26ms
step:1921/2330 train_time:75398ms step_avg:39.25ms
step:1922/2330 train_time:75454ms step_avg:39.26ms
step:1923/2330 train_time:75476ms step_avg:39.25ms
step:1924/2330 train_time:75534ms step_avg:39.26ms
step:1925/2330 train_time:75556ms step_avg:39.25ms
step:1926/2330 train_time:75613ms step_avg:39.26ms
step:1927/2330 train_time:75635ms step_avg:39.25ms
step:1928/2330 train_time:75692ms step_avg:39.26ms
step:1929/2330 train_time:75714ms step_avg:39.25ms
step:1930/2330 train_time:75770ms step_avg:39.26ms
step:1931/2330 train_time:75793ms step_avg:39.25ms
step:1932/2330 train_time:75849ms step_avg:39.26ms
step:1933/2330 train_time:75872ms step_avg:39.25ms
step:1934/2330 train_time:75928ms step_avg:39.26ms
step:1935/2330 train_time:75951ms step_avg:39.25ms
step:1936/2330 train_time:76007ms step_avg:39.26ms
step:1937/2330 train_time:76031ms step_avg:39.25ms
step:1938/2330 train_time:76087ms step_avg:39.26ms
step:1939/2330 train_time:76110ms step_avg:39.25ms
step:1940/2330 train_time:76167ms step_avg:39.26ms
step:1941/2330 train_time:76190ms step_avg:39.25ms
step:1942/2330 train_time:76246ms step_avg:39.26ms
step:1943/2330 train_time:76270ms step_avg:39.25ms
step:1944/2330 train_time:76326ms step_avg:39.26ms
step:1945/2330 train_time:76349ms step_avg:39.25ms
step:1946/2330 train_time:76405ms step_avg:39.26ms
step:1947/2330 train_time:76428ms step_avg:39.25ms
step:1948/2330 train_time:76484ms step_avg:39.26ms
step:1949/2330 train_time:76507ms step_avg:39.25ms
step:1950/2330 train_time:76563ms step_avg:39.26ms
step:1951/2330 train_time:76586ms step_avg:39.25ms
step:1952/2330 train_time:76642ms step_avg:39.26ms
step:1953/2330 train_time:76665ms step_avg:39.25ms
step:1954/2330 train_time:76720ms step_avg:39.26ms
step:1955/2330 train_time:76743ms step_avg:39.25ms
step:1956/2330 train_time:76799ms step_avg:39.26ms
step:1957/2330 train_time:76821ms step_avg:39.25ms
step:1958/2330 train_time:76877ms step_avg:39.26ms
step:1959/2330 train_time:76899ms step_avg:39.25ms
step:1960/2330 train_time:76955ms step_avg:39.26ms
step:1961/2330 train_time:76977ms step_avg:39.25ms
step:1962/2330 train_time:77034ms step_avg:39.26ms
step:1963/2330 train_time:77056ms step_avg:39.25ms
step:1964/2330 train_time:77113ms step_avg:39.26ms
step:1965/2330 train_time:77135ms step_avg:39.25ms
step:1966/2330 train_time:77191ms step_avg:39.26ms
step:1967/2330 train_time:77214ms step_avg:39.25ms
step:1968/2330 train_time:77269ms step_avg:39.26ms
step:1969/2330 train_time:77292ms step_avg:39.25ms
step:1970/2330 train_time:77348ms step_avg:39.26ms
step:1971/2330 train_time:77371ms step_avg:39.25ms
step:1972/2330 train_time:77427ms step_avg:39.26ms
step:1973/2330 train_time:77449ms step_avg:39.25ms
step:1974/2330 train_time:77505ms step_avg:39.26ms
step:1975/2330 train_time:77528ms step_avg:39.25ms
step:1976/2330 train_time:77584ms step_avg:39.26ms
step:1977/2330 train_time:77607ms step_avg:39.25ms
step:1978/2330 train_time:77663ms step_avg:39.26ms
step:1979/2330 train_time:77686ms step_avg:39.26ms
step:1980/2330 train_time:77742ms step_avg:39.26ms
step:1981/2330 train_time:77765ms step_avg:39.26ms
step:1982/2330 train_time:77821ms step_avg:39.26ms
step:1983/2330 train_time:77844ms step_avg:39.26ms
step:1984/2330 train_time:77900ms step_avg:39.26ms
step:1985/2330 train_time:77923ms step_avg:39.26ms
step:1986/2330 train_time:77980ms step_avg:39.26ms
step:1987/2330 train_time:78003ms step_avg:39.26ms
step:1988/2330 train_time:78059ms step_avg:39.26ms
step:1989/2330 train_time:78081ms step_avg:39.26ms
step:1990/2330 train_time:78137ms step_avg:39.26ms
step:1991/2330 train_time:78159ms step_avg:39.26ms
step:1992/2330 train_time:78215ms step_avg:39.26ms
step:1993/2330 train_time:78237ms step_avg:39.26ms
step:1994/2330 train_time:78294ms step_avg:39.26ms
step:1995/2330 train_time:78316ms step_avg:39.26ms
step:1996/2330 train_time:78373ms step_avg:39.26ms
step:1997/2330 train_time:78395ms step_avg:39.26ms
step:1998/2330 train_time:78452ms step_avg:39.27ms
step:1999/2330 train_time:78475ms step_avg:39.26ms
step:2000/2330 train_time:78532ms step_avg:39.27ms
step:2000/2330 val_loss:5.2495 train_time:78630ms step_avg:39.31ms
step:2001/2330 train_time:78642ms step_avg:39.30ms
step:2002/2330 train_time:78654ms step_avg:39.29ms
step:2003/2330 train_time:78663ms step_avg:39.27ms
step:2004/2330 train_time:78692ms step_avg:39.27ms
step:2005/2330 train_time:78714ms step_avg:39.26ms
step:2006/2330 train_time:78769ms step_avg:39.27ms
step:2007/2330 train_time:78791ms step_avg:39.26ms
step:2008/2330 train_time:78846ms step_avg:39.27ms
step:2009/2330 train_time:78868ms step_avg:39.26ms
step:2010/2330 train_time:78924ms step_avg:39.27ms
step:2011/2330 train_time:78951ms step_avg:39.26ms
step:2012/2330 train_time:79012ms step_avg:39.27ms
step:2013/2330 train_time:79037ms step_avg:39.26ms
step:2014/2330 train_time:79094ms step_avg:39.27ms
step:2015/2330 train_time:79117ms step_avg:39.26ms
step:2016/2330 train_time:79173ms step_avg:39.27ms
step:2017/2330 train_time:79195ms step_avg:39.26ms
step:2018/2330 train_time:79250ms step_avg:39.27ms
step:2019/2330 train_time:79272ms step_avg:39.26ms
step:2020/2330 train_time:79328ms step_avg:39.27ms
step:2021/2330 train_time:79350ms step_avg:39.26ms
step:2022/2330 train_time:79405ms step_avg:39.27ms
step:2023/2330 train_time:79428ms step_avg:39.26ms
step:2024/2330 train_time:79484ms step_avg:39.27ms
step:2025/2330 train_time:79506ms step_avg:39.26ms
step:2026/2330 train_time:79564ms step_avg:39.27ms
step:2027/2330 train_time:79588ms step_avg:39.26ms
step:2028/2330 train_time:79645ms step_avg:39.27ms
step:2029/2330 train_time:79668ms step_avg:39.26ms
step:2030/2330 train_time:79724ms step_avg:39.27ms
step:2031/2330 train_time:79745ms step_avg:39.26ms
step:2032/2330 train_time:79801ms step_avg:39.27ms
step:2033/2330 train_time:79824ms step_avg:39.26ms
step:2034/2330 train_time:79880ms step_avg:39.27ms
step:2035/2330 train_time:79903ms step_avg:39.26ms
step:2036/2330 train_time:79959ms step_avg:39.27ms
step:2037/2330 train_time:79983ms step_avg:39.27ms
step:2038/2330 train_time:80040ms step_avg:39.27ms
step:2039/2330 train_time:80063ms step_avg:39.27ms
step:2040/2330 train_time:80119ms step_avg:39.27ms
step:2041/2330 train_time:80141ms step_avg:39.27ms
step:2042/2330 train_time:80197ms step_avg:39.27ms
step:2043/2330 train_time:80219ms step_avg:39.27ms
step:2044/2330 train_time:80275ms step_avg:39.27ms
step:2045/2330 train_time:80296ms step_avg:39.26ms
step:2046/2330 train_time:80352ms step_avg:39.27ms
step:2047/2330 train_time:80375ms step_avg:39.26ms
step:2048/2330 train_time:80431ms step_avg:39.27ms
step:2049/2330 train_time:80453ms step_avg:39.26ms
step:2050/2330 train_time:80509ms step_avg:39.27ms
step:2051/2330 train_time:80531ms step_avg:39.26ms
step:2052/2330 train_time:80588ms step_avg:39.27ms
step:2053/2330 train_time:80611ms step_avg:39.26ms
step:2054/2330 train_time:80667ms step_avg:39.27ms
step:2055/2330 train_time:80690ms step_avg:39.26ms
step:2056/2330 train_time:80746ms step_avg:39.27ms
step:2057/2330 train_time:80768ms step_avg:39.26ms
step:2058/2330 train_time:80824ms step_avg:39.27ms
step:2059/2330 train_time:80848ms step_avg:39.27ms
step:2060/2330 train_time:80904ms step_avg:39.27ms
step:2061/2330 train_time:80928ms step_avg:39.27ms
step:2062/2330 train_time:80985ms step_avg:39.28ms
step:2063/2330 train_time:81009ms step_avg:39.27ms
step:2064/2330 train_time:81066ms step_avg:39.28ms
step:2065/2330 train_time:81090ms step_avg:39.27ms
step:2066/2330 train_time:81146ms step_avg:39.28ms
step:2067/2330 train_time:81170ms step_avg:39.27ms
step:2068/2330 train_time:81227ms step_avg:39.28ms
step:2069/2330 train_time:81251ms step_avg:39.27ms
step:2070/2330 train_time:81306ms step_avg:39.28ms
step:2071/2330 train_time:81329ms step_avg:39.27ms
step:2072/2330 train_time:81384ms step_avg:39.28ms
step:2073/2330 train_time:81407ms step_avg:39.27ms
step:2074/2330 train_time:81463ms step_avg:39.28ms
step:2075/2330 train_time:81486ms step_avg:39.27ms
step:2076/2330 train_time:81542ms step_avg:39.28ms
step:2077/2330 train_time:81564ms step_avg:39.27ms
step:2078/2330 train_time:81620ms step_avg:39.28ms
step:2079/2330 train_time:81642ms step_avg:39.27ms
step:2080/2330 train_time:81697ms step_avg:39.28ms
step:2081/2330 train_time:81720ms step_avg:39.27ms
step:2082/2330 train_time:81776ms step_avg:39.28ms
step:2083/2330 train_time:81799ms step_avg:39.27ms
step:2084/2330 train_time:81856ms step_avg:39.28ms
step:2085/2330 train_time:81879ms step_avg:39.27ms
step:2086/2330 train_time:81935ms step_avg:39.28ms
step:2087/2330 train_time:81957ms step_avg:39.27ms
step:2088/2330 train_time:82014ms step_avg:39.28ms
step:2089/2330 train_time:82036ms step_avg:39.27ms
step:2090/2330 train_time:82094ms step_avg:39.28ms
step:2091/2330 train_time:82116ms step_avg:39.27ms
step:2092/2330 train_time:82175ms step_avg:39.28ms
step:2093/2330 train_time:82197ms step_avg:39.27ms
step:2094/2330 train_time:82253ms step_avg:39.28ms
step:2095/2330 train_time:82275ms step_avg:39.27ms
step:2096/2330 train_time:82332ms step_avg:39.28ms
step:2097/2330 train_time:82354ms step_avg:39.27ms
step:2098/2330 train_time:82410ms step_avg:39.28ms
step:2099/2330 train_time:82432ms step_avg:39.27ms
step:2100/2330 train_time:82487ms step_avg:39.28ms
step:2101/2330 train_time:82510ms step_avg:39.27ms
step:2102/2330 train_time:82566ms step_avg:39.28ms
step:2103/2330 train_time:82589ms step_avg:39.27ms
step:2104/2330 train_time:82646ms step_avg:39.28ms
step:2105/2330 train_time:82668ms step_avg:39.27ms
step:2106/2330 train_time:82725ms step_avg:39.28ms
step:2107/2330 train_time:82748ms step_avg:39.27ms
step:2108/2330 train_time:82804ms step_avg:39.28ms
step:2109/2330 train_time:82827ms step_avg:39.27ms
step:2110/2330 train_time:82883ms step_avg:39.28ms
step:2111/2330 train_time:82906ms step_avg:39.27ms
step:2112/2330 train_time:82962ms step_avg:39.28ms
step:2113/2330 train_time:82985ms step_avg:39.27ms
step:2114/2330 train_time:83042ms step_avg:39.28ms
step:2115/2330 train_time:83066ms step_avg:39.27ms
step:2116/2330 train_time:83121ms step_avg:39.28ms
step:2117/2330 train_time:83144ms step_avg:39.27ms
step:2118/2330 train_time:83201ms step_avg:39.28ms
step:2119/2330 train_time:83223ms step_avg:39.27ms
step:2120/2330 train_time:83279ms step_avg:39.28ms
step:2121/2330 train_time:83301ms step_avg:39.27ms
step:2122/2330 train_time:83357ms step_avg:39.28ms
step:2123/2330 train_time:83379ms step_avg:39.27ms
step:2124/2330 train_time:83435ms step_avg:39.28ms
step:2125/2330 train_time:83458ms step_avg:39.27ms
step:2126/2330 train_time:83514ms step_avg:39.28ms
step:2127/2330 train_time:83536ms step_avg:39.27ms
step:2128/2330 train_time:83593ms step_avg:39.28ms
step:2129/2330 train_time:83615ms step_avg:39.27ms
step:2130/2330 train_time:83671ms step_avg:39.28ms
step:2131/2330 train_time:83694ms step_avg:39.27ms
step:2132/2330 train_time:83751ms step_avg:39.28ms
step:2133/2330 train_time:83774ms step_avg:39.28ms
step:2134/2330 train_time:83830ms step_avg:39.28ms
step:2135/2330 train_time:83852ms step_avg:39.28ms
step:2136/2330 train_time:83909ms step_avg:39.28ms
step:2137/2330 train_time:83932ms step_avg:39.28ms
step:2138/2330 train_time:83988ms step_avg:39.28ms
step:2139/2330 train_time:84011ms step_avg:39.28ms
step:2140/2330 train_time:84068ms step_avg:39.28ms
step:2141/2330 train_time:84091ms step_avg:39.28ms
step:2142/2330 train_time:84147ms step_avg:39.28ms
step:2143/2330 train_time:84170ms step_avg:39.28ms
step:2144/2330 train_time:84226ms step_avg:39.28ms
step:2145/2330 train_time:84249ms step_avg:39.28ms
step:2146/2330 train_time:84306ms step_avg:39.29ms
step:2147/2330 train_time:84329ms step_avg:39.28ms
step:2148/2330 train_time:84385ms step_avg:39.29ms
step:2149/2330 train_time:84408ms step_avg:39.28ms
step:2150/2330 train_time:84464ms step_avg:39.29ms
step:2151/2330 train_time:84487ms step_avg:39.28ms
step:2152/2330 train_time:84543ms step_avg:39.29ms
step:2153/2330 train_time:84566ms step_avg:39.28ms
step:2154/2330 train_time:84622ms step_avg:39.29ms
step:2155/2330 train_time:84644ms step_avg:39.28ms
step:2156/2330 train_time:84700ms step_avg:39.29ms
step:2157/2330 train_time:84722ms step_avg:39.28ms
step:2158/2330 train_time:84778ms step_avg:39.29ms
step:2159/2330 train_time:84799ms step_avg:39.28ms
step:2160/2330 train_time:84856ms step_avg:39.29ms
step:2161/2330 train_time:84878ms step_avg:39.28ms
step:2162/2330 train_time:84934ms step_avg:39.29ms
step:2163/2330 train_time:84957ms step_avg:39.28ms
step:2164/2330 train_time:85014ms step_avg:39.29ms
step:2165/2330 train_time:85036ms step_avg:39.28ms
step:2166/2330 train_time:85093ms step_avg:39.29ms
step:2167/2330 train_time:85116ms step_avg:39.28ms
step:2168/2330 train_time:85172ms step_avg:39.29ms
step:2169/2330 train_time:85194ms step_avg:39.28ms
step:2170/2330 train_time:85251ms step_avg:39.29ms
step:2171/2330 train_time:85273ms step_avg:39.28ms
step:2172/2330 train_time:85330ms step_avg:39.29ms
step:2173/2330 train_time:85352ms step_avg:39.28ms
step:2174/2330 train_time:85408ms step_avg:39.29ms
step:2175/2330 train_time:85430ms step_avg:39.28ms
step:2176/2330 train_time:85486ms step_avg:39.29ms
step:2177/2330 train_time:85509ms step_avg:39.28ms
step:2178/2330 train_time:85565ms step_avg:39.29ms
step:2179/2330 train_time:85588ms step_avg:39.28ms
step:2180/2330 train_time:85644ms step_avg:39.29ms
step:2181/2330 train_time:85667ms step_avg:39.28ms
step:2182/2330 train_time:85722ms step_avg:39.29ms
step:2183/2330 train_time:85746ms step_avg:39.28ms
step:2184/2330 train_time:85802ms step_avg:39.29ms
step:2185/2330 train_time:85825ms step_avg:39.28ms
step:2186/2330 train_time:85881ms step_avg:39.29ms
step:2187/2330 train_time:85904ms step_avg:39.28ms
step:2188/2330 train_time:85960ms step_avg:39.29ms
step:2189/2330 train_time:85984ms step_avg:39.28ms
step:2190/2330 train_time:86041ms step_avg:39.29ms
step:2191/2330 train_time:86062ms step_avg:39.28ms
step:2192/2330 train_time:86119ms step_avg:39.29ms
step:2193/2330 train_time:86141ms step_avg:39.28ms
step:2194/2330 train_time:86197ms step_avg:39.29ms
step:2195/2330 train_time:86219ms step_avg:39.28ms
step:2196/2330 train_time:86276ms step_avg:39.29ms
step:2197/2330 train_time:86297ms step_avg:39.28ms
step:2198/2330 train_time:86354ms step_avg:39.29ms
step:2199/2330 train_time:86376ms step_avg:39.28ms
step:2200/2330 train_time:86434ms step_avg:39.29ms
step:2201/2330 train_time:86456ms step_avg:39.28ms
step:2202/2330 train_time:86512ms step_avg:39.29ms
step:2203/2330 train_time:86534ms step_avg:39.28ms
step:2204/2330 train_time:86591ms step_avg:39.29ms
step:2205/2330 train_time:86613ms step_avg:39.28ms
step:2206/2330 train_time:86669ms step_avg:39.29ms
step:2207/2330 train_time:86692ms step_avg:39.28ms
step:2208/2330 train_time:86748ms step_avg:39.29ms
step:2209/2330 train_time:86770ms step_avg:39.28ms
step:2210/2330 train_time:86826ms step_avg:39.29ms
step:2211/2330 train_time:86850ms step_avg:39.28ms
step:2212/2330 train_time:86906ms step_avg:39.29ms
step:2213/2330 train_time:86930ms step_avg:39.28ms
step:2214/2330 train_time:86986ms step_avg:39.29ms
step:2215/2330 train_time:87010ms step_avg:39.28ms
step:2216/2330 train_time:87066ms step_avg:39.29ms
step:2217/2330 train_time:87090ms step_avg:39.28ms
step:2218/2330 train_time:87147ms step_avg:39.29ms
step:2219/2330 train_time:87170ms step_avg:39.28ms
step:2220/2330 train_time:87226ms step_avg:39.29ms
step:2221/2330 train_time:87249ms step_avg:39.28ms
step:2222/2330 train_time:87306ms step_avg:39.29ms
step:2223/2330 train_time:87329ms step_avg:39.28ms
step:2224/2330 train_time:87385ms step_avg:39.29ms
step:2225/2330 train_time:87407ms step_avg:39.28ms
step:2226/2330 train_time:87463ms step_avg:39.29ms
step:2227/2330 train_time:87486ms step_avg:39.28ms
step:2228/2330 train_time:87543ms step_avg:39.29ms
step:2229/2330 train_time:87567ms step_avg:39.29ms
step:2230/2330 train_time:87623ms step_avg:39.29ms
step:2231/2330 train_time:87645ms step_avg:39.29ms
step:2232/2330 train_time:87701ms step_avg:39.29ms
step:2233/2330 train_time:87724ms step_avg:39.29ms
step:2234/2330 train_time:87779ms step_avg:39.29ms
step:2235/2330 train_time:87802ms step_avg:39.29ms
step:2236/2330 train_time:87858ms step_avg:39.29ms
step:2237/2330 train_time:87880ms step_avg:39.28ms
step:2238/2330 train_time:87936ms step_avg:39.29ms
step:2239/2330 train_time:87958ms step_avg:39.28ms
step:2240/2330 train_time:88014ms step_avg:39.29ms
step:2241/2330 train_time:88036ms step_avg:39.28ms
step:2242/2330 train_time:88093ms step_avg:39.29ms
step:2243/2330 train_time:88115ms step_avg:39.28ms
step:2244/2330 train_time:88172ms step_avg:39.29ms
step:2245/2330 train_time:88194ms step_avg:39.28ms
step:2246/2330 train_time:88250ms step_avg:39.29ms
step:2247/2330 train_time:88272ms step_avg:39.28ms
step:2248/2330 train_time:88327ms step_avg:39.29ms
step:2249/2330 train_time:88350ms step_avg:39.28ms
step:2250/2330 train_time:88406ms step_avg:39.29ms
step:2250/2330 val_loss:5.2279 train_time:88505ms step_avg:39.34ms
step:2251/2330 train_time:88518ms step_avg:39.32ms
step:2252/2330 train_time:88530ms step_avg:39.31ms
step:2253/2330 train_time:88541ms step_avg:39.30ms
step:2254/2330 train_time:88569ms step_avg:39.29ms
step:2255/2330 train_time:88590ms step_avg:39.29ms
step:2256/2330 train_time:88645ms step_avg:39.29ms
step:2257/2330 train_time:88667ms step_avg:39.29ms
step:2258/2330 train_time:88722ms step_avg:39.29ms
step:2259/2330 train_time:88744ms step_avg:39.28ms
step:2260/2330 train_time:88800ms step_avg:39.29ms
step:2261/2330 train_time:88824ms step_avg:39.29ms
step:2262/2330 train_time:88886ms step_avg:39.30ms
step:2263/2330 train_time:88910ms step_avg:39.29ms
step:2264/2330 train_time:88967ms step_avg:39.30ms
step:2265/2330 train_time:88990ms step_avg:39.29ms
step:2266/2330 train_time:89045ms step_avg:39.30ms
step:2267/2330 train_time:89067ms step_avg:39.29ms
step:2268/2330 train_time:89123ms step_avg:39.30ms
step:2269/2330 train_time:89144ms step_avg:39.29ms
step:2270/2330 train_time:89200ms step_avg:39.29ms
step:2271/2330 train_time:89221ms step_avg:39.29ms
step:2272/2330 train_time:89277ms step_avg:39.29ms
step:2273/2330 train_time:89299ms step_avg:39.29ms
step:2274/2330 train_time:89354ms step_avg:39.29ms
step:2275/2330 train_time:89377ms step_avg:39.29ms
step:2276/2330 train_time:89433ms step_avg:39.29ms
step:2277/2330 train_time:89456ms step_avg:39.29ms
step:2278/2330 train_time:89513ms step_avg:39.29ms
step:2279/2330 train_time:89534ms step_avg:39.29ms
step:2280/2330 train_time:89590ms step_avg:39.29ms
step:2281/2330 train_time:89613ms step_avg:39.29ms
step:2282/2330 train_time:89668ms step_avg:39.29ms
step:2283/2330 train_time:89691ms step_avg:39.29ms
step:2284/2330 train_time:89748ms step_avg:39.29ms
step:2285/2330 train_time:89771ms step_avg:39.29ms
step:2286/2330 train_time:89830ms step_avg:39.30ms
step:2287/2330 train_time:89854ms step_avg:39.29ms
step:2288/2330 train_time:89911ms step_avg:39.30ms
step:2289/2330 train_time:89934ms step_avg:39.29ms
step:2290/2330 train_time:89991ms step_avg:39.30ms
step:2291/2330 train_time:90014ms step_avg:39.29ms
step:2292/2330 train_time:90070ms step_avg:39.30ms
step:2293/2330 train_time:90092ms step_avg:39.29ms
step:2294/2330 train_time:90148ms step_avg:39.30ms
step:2295/2330 train_time:90170ms step_avg:39.29ms
step:2296/2330 train_time:90225ms step_avg:39.30ms
step:2297/2330 train_time:90247ms step_avg:39.29ms
step:2298/2330 train_time:90303ms step_avg:39.30ms
step:2299/2330 train_time:90325ms step_avg:39.29ms
step:2300/2330 train_time:90381ms step_avg:39.30ms
step:2301/2330 train_time:90403ms step_avg:39.29ms
step:2302/2330 train_time:90460ms step_avg:39.30ms
step:2303/2330 train_time:90481ms step_avg:39.29ms
step:2304/2330 train_time:90537ms step_avg:39.30ms
step:2305/2330 train_time:90559ms step_avg:39.29ms
step:2306/2330 train_time:90614ms step_avg:39.30ms
step:2307/2330 train_time:90637ms step_avg:39.29ms
step:2308/2330 train_time:90693ms step_avg:39.30ms
step:2309/2330 train_time:90717ms step_avg:39.29ms
step:2310/2330 train_time:90775ms step_avg:39.30ms
step:2311/2330 train_time:90799ms step_avg:39.29ms
step:2312/2330 train_time:90855ms step_avg:39.30ms
step:2313/2330 train_time:90879ms step_avg:39.29ms
step:2314/2330 train_time:90935ms step_avg:39.30ms
step:2315/2330 train_time:90958ms step_avg:39.29ms
step:2316/2330 train_time:91015ms step_avg:39.30ms
step:2317/2330 train_time:91038ms step_avg:39.29ms
step:2318/2330 train_time:91094ms step_avg:39.30ms
step:2319/2330 train_time:91117ms step_avg:39.29ms
step:2320/2330 train_time:91172ms step_avg:39.30ms
step:2321/2330 train_time:91196ms step_avg:39.29ms
step:2322/2330 train_time:91251ms step_avg:39.30ms
step:2323/2330 train_time:91274ms step_avg:39.29ms
step:2324/2330 train_time:91330ms step_avg:39.30ms
step:2325/2330 train_time:91352ms step_avg:39.29ms
step:2326/2330 train_time:91408ms step_avg:39.30ms
step:2327/2330 train_time:91431ms step_avg:39.29ms
step:2328/2330 train_time:91486ms step_avg:39.30ms
step:2329/2330 train_time:91508ms step_avg:39.29ms
step:2330/2330 train_time:91563ms step_avg:39.30ms
step:2330/2330 val_loss:5.2218 train_time:91658ms step_avg:39.34ms
peak memory allocated: 29494 MiB reserved: 38888 MiB
