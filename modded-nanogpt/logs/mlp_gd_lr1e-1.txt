import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = divide_by_norm(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = divide_by_norm(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_gd_lr1e-1"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-1, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 05:34:34 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   34C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   32C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   30C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   30C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   34C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:76ms step_avg:76.48ms
step:2/2330 train_time:149ms step_avg:74.59ms
step:3/2330 train_time:161ms step_avg:53.68ms
step:4/2330 train_time:173ms step_avg:43.36ms
step:5/2330 train_time:184ms step_avg:36.84ms
step:6/2330 train_time:209ms step_avg:34.89ms
step:7/2330 train_time:230ms step_avg:32.93ms
step:8/2330 train_time:284ms step_avg:35.56ms
step:9/2330 train_time:307ms step_avg:34.08ms
step:10/2330 train_time:362ms step_avg:36.19ms
step:11/2330 train_time:384ms step_avg:34.90ms
step:12/2330 train_time:440ms step_avg:36.65ms
step:13/2330 train_time:462ms step_avg:35.52ms
step:14/2330 train_time:518ms step_avg:36.98ms
step:15/2330 train_time:540ms step_avg:35.98ms
step:16/2330 train_time:595ms step_avg:37.20ms
step:17/2330 train_time:617ms step_avg:36.29ms
step:18/2330 train_time:672ms step_avg:37.36ms
step:19/2330 train_time:694ms step_avg:36.52ms
step:20/2330 train_time:750ms step_avg:37.50ms
step:21/2330 train_time:771ms step_avg:36.73ms
step:22/2330 train_time:827ms step_avg:37.58ms
step:23/2330 train_time:849ms step_avg:36.91ms
step:24/2330 train_time:904ms step_avg:37.67ms
step:25/2330 train_time:926ms step_avg:37.06ms
step:26/2330 train_time:984ms step_avg:37.85ms
step:27/2330 train_time:1010ms step_avg:37.41ms
step:28/2330 train_time:1070ms step_avg:38.21ms
step:29/2330 train_time:1097ms step_avg:37.81ms
step:30/2330 train_time:1156ms step_avg:38.52ms
step:31/2330 train_time:1179ms step_avg:38.03ms
step:32/2330 train_time:1236ms step_avg:38.63ms
step:33/2330 train_time:1258ms step_avg:38.13ms
step:34/2330 train_time:1314ms step_avg:38.66ms
step:35/2330 train_time:1337ms step_avg:38.19ms
step:36/2330 train_time:1393ms step_avg:38.69ms
step:37/2330 train_time:1415ms step_avg:38.24ms
step:38/2330 train_time:1471ms step_avg:38.71ms
step:39/2330 train_time:1493ms step_avg:38.27ms
step:40/2330 train_time:1548ms step_avg:38.70ms
step:41/2330 train_time:1570ms step_avg:38.28ms
step:42/2330 train_time:1625ms step_avg:38.68ms
step:43/2330 train_time:1647ms step_avg:38.30ms
step:44/2330 train_time:1702ms step_avg:38.68ms
step:45/2330 train_time:1724ms step_avg:38.31ms
step:46/2330 train_time:1779ms step_avg:38.68ms
step:47/2330 train_time:1801ms step_avg:38.32ms
step:48/2330 train_time:1857ms step_avg:38.69ms
step:49/2330 train_time:1879ms step_avg:38.34ms
step:50/2330 train_time:1936ms step_avg:38.71ms
step:51/2330 train_time:1958ms step_avg:38.39ms
step:52/2330 train_time:2015ms step_avg:38.76ms
step:53/2330 train_time:2038ms step_avg:38.45ms
step:54/2330 train_time:2095ms step_avg:38.80ms
step:55/2330 train_time:2117ms step_avg:38.50ms
step:56/2330 train_time:2175ms step_avg:38.83ms
step:57/2330 train_time:2197ms step_avg:38.54ms
step:58/2330 train_time:2254ms step_avg:38.86ms
step:59/2330 train_time:2276ms step_avg:38.57ms
step:60/2330 train_time:2333ms step_avg:38.88ms
step:61/2330 train_time:2355ms step_avg:38.61ms
step:62/2330 train_time:2412ms step_avg:38.90ms
step:63/2330 train_time:2434ms step_avg:38.63ms
step:64/2330 train_time:2490ms step_avg:38.90ms
step:65/2330 train_time:2512ms step_avg:38.64ms
step:66/2330 train_time:2567ms step_avg:38.90ms
step:67/2330 train_time:2589ms step_avg:38.65ms
step:68/2330 train_time:2645ms step_avg:38.89ms
step:69/2330 train_time:2667ms step_avg:38.66ms
step:70/2330 train_time:2723ms step_avg:38.90ms
step:71/2330 train_time:2745ms step_avg:38.67ms
step:72/2330 train_time:2801ms step_avg:38.90ms
step:73/2330 train_time:2824ms step_avg:38.68ms
step:74/2330 train_time:2880ms step_avg:38.92ms
step:75/2330 train_time:2903ms step_avg:38.71ms
step:76/2330 train_time:2960ms step_avg:38.95ms
step:77/2330 train_time:2984ms step_avg:38.75ms
step:78/2330 train_time:3041ms step_avg:38.98ms
step:79/2330 train_time:3064ms step_avg:38.79ms
step:80/2330 train_time:3122ms step_avg:39.02ms
step:81/2330 train_time:3146ms step_avg:38.84ms
step:82/2330 train_time:3202ms step_avg:39.05ms
step:83/2330 train_time:3225ms step_avg:38.86ms
step:84/2330 train_time:3282ms step_avg:39.08ms
step:85/2330 train_time:3306ms step_avg:38.89ms
step:86/2330 train_time:3362ms step_avg:39.10ms
step:87/2330 train_time:3386ms step_avg:38.92ms
step:88/2330 train_time:3442ms step_avg:39.11ms
step:89/2330 train_time:3464ms step_avg:38.93ms
step:90/2330 train_time:3521ms step_avg:39.12ms
step:91/2330 train_time:3542ms step_avg:38.93ms
step:92/2330 train_time:3599ms step_avg:39.11ms
step:93/2330 train_time:3620ms step_avg:38.93ms
step:94/2330 train_time:3676ms step_avg:39.11ms
step:95/2330 train_time:3698ms step_avg:38.92ms
step:96/2330 train_time:3754ms step_avg:39.11ms
step:97/2330 train_time:3776ms step_avg:38.93ms
step:98/2330 train_time:3832ms step_avg:39.10ms
step:99/2330 train_time:3854ms step_avg:38.93ms
step:100/2330 train_time:3910ms step_avg:39.10ms
step:101/2330 train_time:3932ms step_avg:38.93ms
step:102/2330 train_time:3988ms step_avg:39.10ms
step:103/2330 train_time:4011ms step_avg:38.94ms
step:104/2330 train_time:4067ms step_avg:39.11ms
step:105/2330 train_time:4091ms step_avg:38.96ms
step:106/2330 train_time:4147ms step_avg:39.12ms
step:107/2330 train_time:4171ms step_avg:38.98ms
step:108/2330 train_time:4227ms step_avg:39.14ms
step:109/2330 train_time:4250ms step_avg:38.99ms
step:110/2330 train_time:4306ms step_avg:39.15ms
step:111/2330 train_time:4330ms step_avg:39.01ms
step:112/2330 train_time:4386ms step_avg:39.16ms
step:113/2330 train_time:4409ms step_avg:39.02ms
step:114/2330 train_time:4465ms step_avg:39.17ms
step:115/2330 train_time:4488ms step_avg:39.03ms
step:116/2330 train_time:4545ms step_avg:39.18ms
step:117/2330 train_time:4568ms step_avg:39.04ms
step:118/2330 train_time:4624ms step_avg:39.19ms
step:119/2330 train_time:4647ms step_avg:39.05ms
step:120/2330 train_time:4703ms step_avg:39.19ms
step:121/2330 train_time:4726ms step_avg:39.06ms
step:122/2330 train_time:4781ms step_avg:39.19ms
step:123/2330 train_time:4803ms step_avg:39.05ms
step:124/2330 train_time:4860ms step_avg:39.19ms
step:125/2330 train_time:4882ms step_avg:39.06ms
step:126/2330 train_time:4939ms step_avg:39.20ms
step:127/2330 train_time:4961ms step_avg:39.07ms
step:128/2330 train_time:5018ms step_avg:39.20ms
step:129/2330 train_time:5040ms step_avg:39.07ms
step:130/2330 train_time:5096ms step_avg:39.20ms
step:131/2330 train_time:5119ms step_avg:39.08ms
step:132/2330 train_time:5176ms step_avg:39.21ms
step:133/2330 train_time:5198ms step_avg:39.08ms
step:134/2330 train_time:5255ms step_avg:39.22ms
step:135/2330 train_time:5278ms step_avg:39.10ms
step:136/2330 train_time:5335ms step_avg:39.23ms
step:137/2330 train_time:5358ms step_avg:39.11ms
step:138/2330 train_time:5416ms step_avg:39.24ms
step:139/2330 train_time:5438ms step_avg:39.12ms
step:140/2330 train_time:5494ms step_avg:39.25ms
step:141/2330 train_time:5517ms step_avg:39.13ms
step:142/2330 train_time:5574ms step_avg:39.26ms
step:143/2330 train_time:5597ms step_avg:39.14ms
step:144/2330 train_time:5653ms step_avg:39.26ms
step:145/2330 train_time:5675ms step_avg:39.14ms
step:146/2330 train_time:5732ms step_avg:39.26ms
step:147/2330 train_time:5754ms step_avg:39.14ms
step:148/2330 train_time:5810ms step_avg:39.26ms
step:149/2330 train_time:5832ms step_avg:39.14ms
step:150/2330 train_time:5888ms step_avg:39.25ms
step:151/2330 train_time:5910ms step_avg:39.14ms
step:152/2330 train_time:5966ms step_avg:39.25ms
step:153/2330 train_time:5989ms step_avg:39.14ms
step:154/2330 train_time:6045ms step_avg:39.25ms
step:155/2330 train_time:6068ms step_avg:39.15ms
step:156/2330 train_time:6125ms step_avg:39.26ms
step:157/2330 train_time:6148ms step_avg:39.16ms
step:158/2330 train_time:6204ms step_avg:39.27ms
step:159/2330 train_time:6228ms step_avg:39.17ms
step:160/2330 train_time:6284ms step_avg:39.28ms
step:161/2330 train_time:6308ms step_avg:39.18ms
step:162/2330 train_time:6365ms step_avg:39.29ms
step:163/2330 train_time:6388ms step_avg:39.19ms
step:164/2330 train_time:6445ms step_avg:39.30ms
step:165/2330 train_time:6468ms step_avg:39.20ms
step:166/2330 train_time:6524ms step_avg:39.30ms
step:167/2330 train_time:6548ms step_avg:39.21ms
step:168/2330 train_time:6604ms step_avg:39.31ms
step:169/2330 train_time:6627ms step_avg:39.22ms
step:170/2330 train_time:6683ms step_avg:39.31ms
step:171/2330 train_time:6707ms step_avg:39.22ms
step:172/2330 train_time:6762ms step_avg:39.32ms
step:173/2330 train_time:6785ms step_avg:39.22ms
step:174/2330 train_time:6841ms step_avg:39.31ms
step:175/2330 train_time:6864ms step_avg:39.22ms
step:176/2330 train_time:6920ms step_avg:39.32ms
step:177/2330 train_time:6942ms step_avg:39.22ms
step:178/2330 train_time:6998ms step_avg:39.32ms
step:179/2330 train_time:7020ms step_avg:39.22ms
step:180/2330 train_time:7076ms step_avg:39.31ms
step:181/2330 train_time:7098ms step_avg:39.22ms
step:182/2330 train_time:7155ms step_avg:39.32ms
step:183/2330 train_time:7177ms step_avg:39.22ms
step:184/2330 train_time:7234ms step_avg:39.31ms
step:185/2330 train_time:7256ms step_avg:39.22ms
step:186/2330 train_time:7313ms step_avg:39.32ms
step:187/2330 train_time:7335ms step_avg:39.23ms
step:188/2330 train_time:7393ms step_avg:39.32ms
step:189/2330 train_time:7415ms step_avg:39.23ms
step:190/2330 train_time:7471ms step_avg:39.32ms
step:191/2330 train_time:7493ms step_avg:39.23ms
step:192/2330 train_time:7550ms step_avg:39.32ms
step:193/2330 train_time:7572ms step_avg:39.23ms
step:194/2330 train_time:7628ms step_avg:39.32ms
step:195/2330 train_time:7650ms step_avg:39.23ms
step:196/2330 train_time:7706ms step_avg:39.32ms
step:197/2330 train_time:7729ms step_avg:39.23ms
step:198/2330 train_time:7785ms step_avg:39.32ms
step:199/2330 train_time:7808ms step_avg:39.24ms
step:200/2330 train_time:7864ms step_avg:39.32ms
step:201/2330 train_time:7887ms step_avg:39.24ms
step:202/2330 train_time:7943ms step_avg:39.32ms
step:203/2330 train_time:7967ms step_avg:39.25ms
step:204/2330 train_time:8023ms step_avg:39.33ms
step:205/2330 train_time:8046ms step_avg:39.25ms
step:206/2330 train_time:8103ms step_avg:39.33ms
step:207/2330 train_time:8126ms step_avg:39.26ms
step:208/2330 train_time:8182ms step_avg:39.34ms
step:209/2330 train_time:8205ms step_avg:39.26ms
step:210/2330 train_time:8261ms step_avg:39.34ms
step:211/2330 train_time:8284ms step_avg:39.26ms
step:212/2330 train_time:8341ms step_avg:39.34ms
step:213/2330 train_time:8363ms step_avg:39.27ms
step:214/2330 train_time:8420ms step_avg:39.35ms
step:215/2330 train_time:8442ms step_avg:39.27ms
step:216/2330 train_time:8499ms step_avg:39.35ms
step:217/2330 train_time:8521ms step_avg:39.27ms
step:218/2330 train_time:8578ms step_avg:39.35ms
step:219/2330 train_time:8600ms step_avg:39.27ms
step:220/2330 train_time:8656ms step_avg:39.35ms
step:221/2330 train_time:8678ms step_avg:39.27ms
step:222/2330 train_time:8736ms step_avg:39.35ms
step:223/2330 train_time:8758ms step_avg:39.27ms
step:224/2330 train_time:8814ms step_avg:39.35ms
step:225/2330 train_time:8836ms step_avg:39.27ms
step:226/2330 train_time:8893ms step_avg:39.35ms
step:227/2330 train_time:8915ms step_avg:39.27ms
step:228/2330 train_time:8971ms step_avg:39.35ms
step:229/2330 train_time:8993ms step_avg:39.27ms
step:230/2330 train_time:9049ms step_avg:39.34ms
step:231/2330 train_time:9071ms step_avg:39.27ms
step:232/2330 train_time:9128ms step_avg:39.35ms
step:233/2330 train_time:9151ms step_avg:39.27ms
step:234/2330 train_time:9206ms step_avg:39.34ms
step:235/2330 train_time:9229ms step_avg:39.27ms
step:236/2330 train_time:9285ms step_avg:39.35ms
step:237/2330 train_time:9309ms step_avg:39.28ms
step:238/2330 train_time:9365ms step_avg:39.35ms
step:239/2330 train_time:9388ms step_avg:39.28ms
step:240/2330 train_time:9444ms step_avg:39.35ms
step:241/2330 train_time:9468ms step_avg:39.28ms
step:242/2330 train_time:9524ms step_avg:39.36ms
step:243/2330 train_time:9547ms step_avg:39.29ms
step:244/2330 train_time:9604ms step_avg:39.36ms
step:245/2330 train_time:9628ms step_avg:39.30ms
step:246/2330 train_time:9684ms step_avg:39.37ms
step:247/2330 train_time:9707ms step_avg:39.30ms
step:248/2330 train_time:9764ms step_avg:39.37ms
step:249/2330 train_time:9786ms step_avg:39.30ms
step:250/2330 train_time:9842ms step_avg:39.37ms
step:250/2330 val_loss:5.5069 train_time:9939ms step_avg:39.75ms
step:251/2330 train_time:9951ms step_avg:39.65ms
step:252/2330 train_time:9963ms step_avg:39.54ms
step:253/2330 train_time:9972ms step_avg:39.42ms
step:254/2330 train_time:10001ms step_avg:39.37ms
step:255/2330 train_time:10022ms step_avg:39.30ms
step:256/2330 train_time:10077ms step_avg:39.36ms
step:257/2330 train_time:10099ms step_avg:39.30ms
step:258/2330 train_time:10154ms step_avg:39.36ms
step:259/2330 train_time:10176ms step_avg:39.29ms
step:260/2330 train_time:10231ms step_avg:39.35ms
step:261/2330 train_time:10257ms step_avg:39.30ms
step:262/2330 train_time:10316ms step_avg:39.37ms
step:263/2330 train_time:10341ms step_avg:39.32ms
step:264/2330 train_time:10398ms step_avg:39.39ms
step:265/2330 train_time:10422ms step_avg:39.33ms
step:266/2330 train_time:10478ms step_avg:39.39ms
step:267/2330 train_time:10500ms step_avg:39.33ms
step:268/2330 train_time:10556ms step_avg:39.39ms
step:269/2330 train_time:10578ms step_avg:39.32ms
step:270/2330 train_time:10633ms step_avg:39.38ms
step:271/2330 train_time:10655ms step_avg:39.32ms
step:272/2330 train_time:10710ms step_avg:39.37ms
step:273/2330 train_time:10732ms step_avg:39.31ms
step:274/2330 train_time:10788ms step_avg:39.37ms
step:275/2330 train_time:10809ms step_avg:39.31ms
step:276/2330 train_time:10866ms step_avg:39.37ms
step:277/2330 train_time:10890ms step_avg:39.32ms
step:278/2330 train_time:10947ms step_avg:39.38ms
step:279/2330 train_time:10971ms step_avg:39.32ms
step:280/2330 train_time:11027ms step_avg:39.38ms
step:281/2330 train_time:11051ms step_avg:39.33ms
step:282/2330 train_time:11107ms step_avg:39.39ms
step:283/2330 train_time:11129ms step_avg:39.33ms
step:284/2330 train_time:11186ms step_avg:39.39ms
step:285/2330 train_time:11210ms step_avg:39.33ms
step:286/2330 train_time:11268ms step_avg:39.40ms
step:287/2330 train_time:11292ms step_avg:39.34ms
step:288/2330 train_time:11349ms step_avg:39.41ms
step:289/2330 train_time:11373ms step_avg:39.35ms
step:290/2330 train_time:11429ms step_avg:39.41ms
step:291/2330 train_time:11452ms step_avg:39.35ms
step:292/2330 train_time:11508ms step_avg:39.41ms
step:293/2330 train_time:11531ms step_avg:39.35ms
step:294/2330 train_time:11587ms step_avg:39.41ms
step:295/2330 train_time:11609ms step_avg:39.35ms
step:296/2330 train_time:11664ms step_avg:39.41ms
step:297/2330 train_time:11686ms step_avg:39.35ms
step:298/2330 train_time:11741ms step_avg:39.40ms
step:299/2330 train_time:11763ms step_avg:39.34ms
step:300/2330 train_time:11819ms step_avg:39.40ms
step:301/2330 train_time:11841ms step_avg:39.34ms
step:302/2330 train_time:11898ms step_avg:39.40ms
step:303/2330 train_time:11921ms step_avg:39.34ms
step:304/2330 train_time:11977ms step_avg:39.40ms
step:305/2330 train_time:11999ms step_avg:39.34ms
step:306/2330 train_time:12055ms step_avg:39.40ms
step:307/2330 train_time:12077ms step_avg:39.34ms
step:308/2330 train_time:12134ms step_avg:39.39ms
step:309/2330 train_time:12156ms step_avg:39.34ms
step:310/2330 train_time:12213ms step_avg:39.40ms
step:311/2330 train_time:12236ms step_avg:39.34ms
step:312/2330 train_time:12292ms step_avg:39.40ms
step:313/2330 train_time:12316ms step_avg:39.35ms
step:314/2330 train_time:12371ms step_avg:39.40ms
step:315/2330 train_time:12395ms step_avg:39.35ms
step:316/2330 train_time:12451ms step_avg:39.40ms
step:317/2330 train_time:12474ms step_avg:39.35ms
step:318/2330 train_time:12530ms step_avg:39.40ms
step:319/2330 train_time:12553ms step_avg:39.35ms
step:320/2330 train_time:12608ms step_avg:39.40ms
step:321/2330 train_time:12631ms step_avg:39.35ms
step:322/2330 train_time:12687ms step_avg:39.40ms
step:323/2330 train_time:12710ms step_avg:39.35ms
step:324/2330 train_time:12766ms step_avg:39.40ms
step:325/2330 train_time:12789ms step_avg:39.35ms
step:326/2330 train_time:12845ms step_avg:39.40ms
step:327/2330 train_time:12867ms step_avg:39.35ms
step:328/2330 train_time:12923ms step_avg:39.40ms
step:329/2330 train_time:12946ms step_avg:39.35ms
step:330/2330 train_time:13001ms step_avg:39.40ms
step:331/2330 train_time:13023ms step_avg:39.35ms
step:332/2330 train_time:13080ms step_avg:39.40ms
step:333/2330 train_time:13102ms step_avg:39.34ms
step:334/2330 train_time:13160ms step_avg:39.40ms
step:335/2330 train_time:13182ms step_avg:39.35ms
step:336/2330 train_time:13239ms step_avg:39.40ms
step:337/2330 train_time:13261ms step_avg:39.35ms
step:338/2330 train_time:13317ms step_avg:39.40ms
step:339/2330 train_time:13339ms step_avg:39.35ms
step:340/2330 train_time:13396ms step_avg:39.40ms
step:341/2330 train_time:13419ms step_avg:39.35ms
step:342/2330 train_time:13475ms step_avg:39.40ms
step:343/2330 train_time:13497ms step_avg:39.35ms
step:344/2330 train_time:13554ms step_avg:39.40ms
step:345/2330 train_time:13576ms step_avg:39.35ms
step:346/2330 train_time:13632ms step_avg:39.40ms
step:347/2330 train_time:13655ms step_avg:39.35ms
step:348/2330 train_time:13711ms step_avg:39.40ms
step:349/2330 train_time:13734ms step_avg:39.35ms
step:350/2330 train_time:13789ms step_avg:39.40ms
step:351/2330 train_time:13812ms step_avg:39.35ms
step:352/2330 train_time:13869ms step_avg:39.40ms
step:353/2330 train_time:13892ms step_avg:39.35ms
step:354/2330 train_time:13949ms step_avg:39.40ms
step:355/2330 train_time:13972ms step_avg:39.36ms
step:356/2330 train_time:14028ms step_avg:39.40ms
step:357/2330 train_time:14051ms step_avg:39.36ms
step:358/2330 train_time:14108ms step_avg:39.41ms
step:359/2330 train_time:14132ms step_avg:39.36ms
step:360/2330 train_time:14188ms step_avg:39.41ms
step:361/2330 train_time:14211ms step_avg:39.37ms
step:362/2330 train_time:14267ms step_avg:39.41ms
step:363/2330 train_time:14291ms step_avg:39.37ms
step:364/2330 train_time:14347ms step_avg:39.42ms
step:365/2330 train_time:14370ms step_avg:39.37ms
step:366/2330 train_time:14426ms step_avg:39.42ms
step:367/2330 train_time:14449ms step_avg:39.37ms
step:368/2330 train_time:14505ms step_avg:39.41ms
step:369/2330 train_time:14527ms step_avg:39.37ms
step:370/2330 train_time:14583ms step_avg:39.41ms
step:371/2330 train_time:14605ms step_avg:39.37ms
step:372/2330 train_time:14662ms step_avg:39.41ms
step:373/2330 train_time:14683ms step_avg:39.37ms
step:374/2330 train_time:14740ms step_avg:39.41ms
step:375/2330 train_time:14762ms step_avg:39.36ms
step:376/2330 train_time:14818ms step_avg:39.41ms
step:377/2330 train_time:14839ms step_avg:39.36ms
step:378/2330 train_time:14896ms step_avg:39.41ms
step:379/2330 train_time:14918ms step_avg:39.36ms
step:380/2330 train_time:14975ms step_avg:39.41ms
step:381/2330 train_time:14996ms step_avg:39.36ms
step:382/2330 train_time:15053ms step_avg:39.40ms
step:383/2330 train_time:15075ms step_avg:39.36ms
step:384/2330 train_time:15131ms step_avg:39.40ms
step:385/2330 train_time:15154ms step_avg:39.36ms
step:386/2330 train_time:15210ms step_avg:39.40ms
step:387/2330 train_time:15233ms step_avg:39.36ms
step:388/2330 train_time:15290ms step_avg:39.41ms
step:389/2330 train_time:15313ms step_avg:39.36ms
step:390/2330 train_time:15369ms step_avg:39.41ms
step:391/2330 train_time:15392ms step_avg:39.37ms
step:392/2330 train_time:15449ms step_avg:39.41ms
step:393/2330 train_time:15472ms step_avg:39.37ms
step:394/2330 train_time:15528ms step_avg:39.41ms
step:395/2330 train_time:15550ms step_avg:39.37ms
step:396/2330 train_time:15607ms step_avg:39.41ms
step:397/2330 train_time:15630ms step_avg:39.37ms
step:398/2330 train_time:15686ms step_avg:39.41ms
step:399/2330 train_time:15708ms step_avg:39.37ms
step:400/2330 train_time:15764ms step_avg:39.41ms
step:401/2330 train_time:15787ms step_avg:39.37ms
step:402/2330 train_time:15844ms step_avg:39.41ms
step:403/2330 train_time:15866ms step_avg:39.37ms
step:404/2330 train_time:15923ms step_avg:39.41ms
step:405/2330 train_time:15945ms step_avg:39.37ms
step:406/2330 train_time:16001ms step_avg:39.41ms
step:407/2330 train_time:16023ms step_avg:39.37ms
step:408/2330 train_time:16080ms step_avg:39.41ms
step:409/2330 train_time:16102ms step_avg:39.37ms
step:410/2330 train_time:16159ms step_avg:39.41ms
step:411/2330 train_time:16180ms step_avg:39.37ms
step:412/2330 train_time:16237ms step_avg:39.41ms
step:413/2330 train_time:16259ms step_avg:39.37ms
step:414/2330 train_time:16316ms step_avg:39.41ms
step:415/2330 train_time:16338ms step_avg:39.37ms
step:416/2330 train_time:16395ms step_avg:39.41ms
step:417/2330 train_time:16416ms step_avg:39.37ms
step:418/2330 train_time:16473ms step_avg:39.41ms
step:419/2330 train_time:16495ms step_avg:39.37ms
step:420/2330 train_time:16551ms step_avg:39.41ms
step:421/2330 train_time:16574ms step_avg:39.37ms
step:422/2330 train_time:16629ms step_avg:39.41ms
step:423/2330 train_time:16652ms step_avg:39.37ms
step:424/2330 train_time:16708ms step_avg:39.41ms
step:425/2330 train_time:16731ms step_avg:39.37ms
step:426/2330 train_time:16787ms step_avg:39.41ms
step:427/2330 train_time:16810ms step_avg:39.37ms
step:428/2330 train_time:16866ms step_avg:39.41ms
step:429/2330 train_time:16888ms step_avg:39.37ms
step:430/2330 train_time:16945ms step_avg:39.41ms
step:431/2330 train_time:16968ms step_avg:39.37ms
step:432/2330 train_time:17024ms step_avg:39.41ms
step:433/2330 train_time:17047ms step_avg:39.37ms
step:434/2330 train_time:17104ms step_avg:39.41ms
step:435/2330 train_time:17126ms step_avg:39.37ms
step:436/2330 train_time:17183ms step_avg:39.41ms
step:437/2330 train_time:17204ms step_avg:39.37ms
step:438/2330 train_time:17261ms step_avg:39.41ms
step:439/2330 train_time:17283ms step_avg:39.37ms
step:440/2330 train_time:17339ms step_avg:39.41ms
step:441/2330 train_time:17361ms step_avg:39.37ms
step:442/2330 train_time:17418ms step_avg:39.41ms
step:443/2330 train_time:17440ms step_avg:39.37ms
step:444/2330 train_time:17496ms step_avg:39.41ms
step:445/2330 train_time:17519ms step_avg:39.37ms
step:446/2330 train_time:17575ms step_avg:39.41ms
step:447/2330 train_time:17598ms step_avg:39.37ms
step:448/2330 train_time:17655ms step_avg:39.41ms
step:449/2330 train_time:17676ms step_avg:39.37ms
step:450/2330 train_time:17732ms step_avg:39.40ms
step:451/2330 train_time:17755ms step_avg:39.37ms
step:452/2330 train_time:17811ms step_avg:39.40ms
step:453/2330 train_time:17834ms step_avg:39.37ms
step:454/2330 train_time:17890ms step_avg:39.41ms
step:455/2330 train_time:17913ms step_avg:39.37ms
step:456/2330 train_time:17970ms step_avg:39.41ms
step:457/2330 train_time:17993ms step_avg:39.37ms
step:458/2330 train_time:18049ms step_avg:39.41ms
step:459/2330 train_time:18072ms step_avg:39.37ms
step:460/2330 train_time:18129ms step_avg:39.41ms
step:461/2330 train_time:18152ms step_avg:39.38ms
step:462/2330 train_time:18209ms step_avg:39.41ms
step:463/2330 train_time:18233ms step_avg:39.38ms
step:464/2330 train_time:18289ms step_avg:39.42ms
step:465/2330 train_time:18312ms step_avg:39.38ms
step:466/2330 train_time:18368ms step_avg:39.42ms
step:467/2330 train_time:18391ms step_avg:39.38ms
step:468/2330 train_time:18448ms step_avg:39.42ms
step:469/2330 train_time:18471ms step_avg:39.38ms
step:470/2330 train_time:18527ms step_avg:39.42ms
step:471/2330 train_time:18549ms step_avg:39.38ms
step:472/2330 train_time:18605ms step_avg:39.42ms
step:473/2330 train_time:18628ms step_avg:39.38ms
step:474/2330 train_time:18684ms step_avg:39.42ms
step:475/2330 train_time:18707ms step_avg:39.38ms
step:476/2330 train_time:18762ms step_avg:39.42ms
step:477/2330 train_time:18784ms step_avg:39.38ms
step:478/2330 train_time:18840ms step_avg:39.41ms
step:479/2330 train_time:18863ms step_avg:39.38ms
step:480/2330 train_time:18920ms step_avg:39.42ms
step:481/2330 train_time:18942ms step_avg:39.38ms
step:482/2330 train_time:18998ms step_avg:39.42ms
step:483/2330 train_time:19020ms step_avg:39.38ms
step:484/2330 train_time:19076ms step_avg:39.41ms
step:485/2330 train_time:19099ms step_avg:39.38ms
step:486/2330 train_time:19155ms step_avg:39.41ms
step:487/2330 train_time:19177ms step_avg:39.38ms
step:488/2330 train_time:19234ms step_avg:39.41ms
step:489/2330 train_time:19256ms step_avg:39.38ms
step:490/2330 train_time:19312ms step_avg:39.41ms
step:491/2330 train_time:19334ms step_avg:39.38ms
step:492/2330 train_time:19390ms step_avg:39.41ms
step:493/2330 train_time:19413ms step_avg:39.38ms
step:494/2330 train_time:19469ms step_avg:39.41ms
step:495/2330 train_time:19492ms step_avg:39.38ms
step:496/2330 train_time:19548ms step_avg:39.41ms
step:497/2330 train_time:19571ms step_avg:39.38ms
step:498/2330 train_time:19627ms step_avg:39.41ms
step:499/2330 train_time:19650ms step_avg:39.38ms
step:500/2330 train_time:19706ms step_avg:39.41ms
step:500/2330 val_loss:5.3435 train_time:19804ms step_avg:39.61ms
step:501/2330 train_time:19816ms step_avg:39.55ms
step:502/2330 train_time:19827ms step_avg:39.50ms
step:503/2330 train_time:19836ms step_avg:39.44ms
step:504/2330 train_time:19866ms step_avg:39.42ms
step:505/2330 train_time:19888ms step_avg:39.38ms
step:506/2330 train_time:19943ms step_avg:39.41ms
step:507/2330 train_time:19965ms step_avg:39.38ms
step:508/2330 train_time:20021ms step_avg:39.41ms
step:509/2330 train_time:20044ms step_avg:39.38ms
step:510/2330 train_time:20100ms step_avg:39.41ms
step:511/2330 train_time:20126ms step_avg:39.39ms
step:512/2330 train_time:20186ms step_avg:39.43ms
step:513/2330 train_time:20211ms step_avg:39.40ms
step:514/2330 train_time:20268ms step_avg:39.43ms
step:515/2330 train_time:20292ms step_avg:39.40ms
step:516/2330 train_time:20348ms step_avg:39.43ms
step:517/2330 train_time:20370ms step_avg:39.40ms
step:518/2330 train_time:20425ms step_avg:39.43ms
step:519/2330 train_time:20448ms step_avg:39.40ms
step:520/2330 train_time:20503ms step_avg:39.43ms
step:521/2330 train_time:20526ms step_avg:39.40ms
step:522/2330 train_time:20581ms step_avg:39.43ms
step:523/2330 train_time:20604ms step_avg:39.40ms
step:524/2330 train_time:20660ms step_avg:39.43ms
step:525/2330 train_time:20682ms step_avg:39.39ms
step:526/2330 train_time:20739ms step_avg:39.43ms
step:527/2330 train_time:20762ms step_avg:39.40ms
step:528/2330 train_time:20819ms step_avg:39.43ms
step:529/2330 train_time:20841ms step_avg:39.40ms
step:530/2330 train_time:20896ms step_avg:39.43ms
step:531/2330 train_time:20918ms step_avg:39.39ms
step:532/2330 train_time:20974ms step_avg:39.42ms
step:533/2330 train_time:20996ms step_avg:39.39ms
step:534/2330 train_time:21053ms step_avg:39.43ms
step:535/2330 train_time:21075ms step_avg:39.39ms
step:536/2330 train_time:21133ms step_avg:39.43ms
step:537/2330 train_time:21155ms step_avg:39.39ms
step:538/2330 train_time:21211ms step_avg:39.43ms
step:539/2330 train_time:21234ms step_avg:39.39ms
step:540/2330 train_time:21290ms step_avg:39.43ms
step:541/2330 train_time:21312ms step_avg:39.39ms
step:542/2330 train_time:21368ms step_avg:39.42ms
step:543/2330 train_time:21390ms step_avg:39.39ms
step:544/2330 train_time:21446ms step_avg:39.42ms
step:545/2330 train_time:21468ms step_avg:39.39ms
step:546/2330 train_time:21524ms step_avg:39.42ms
step:547/2330 train_time:21546ms step_avg:39.39ms
step:548/2330 train_time:21602ms step_avg:39.42ms
step:549/2330 train_time:21625ms step_avg:39.39ms
step:550/2330 train_time:21681ms step_avg:39.42ms
step:551/2330 train_time:21704ms step_avg:39.39ms
step:552/2330 train_time:21761ms step_avg:39.42ms
step:553/2330 train_time:21783ms step_avg:39.39ms
step:554/2330 train_time:21839ms step_avg:39.42ms
step:555/2330 train_time:21862ms step_avg:39.39ms
step:556/2330 train_time:21918ms step_avg:39.42ms
step:557/2330 train_time:21940ms step_avg:39.39ms
step:558/2330 train_time:21998ms step_avg:39.42ms
step:559/2330 train_time:22020ms step_avg:39.39ms
step:560/2330 train_time:22077ms step_avg:39.42ms
step:561/2330 train_time:22100ms step_avg:39.39ms
step:562/2330 train_time:22157ms step_avg:39.43ms
step:563/2330 train_time:22180ms step_avg:39.40ms
step:564/2330 train_time:22236ms step_avg:39.43ms
step:565/2330 train_time:22259ms step_avg:39.40ms
step:566/2330 train_time:22316ms step_avg:39.43ms
step:567/2330 train_time:22338ms step_avg:39.40ms
step:568/2330 train_time:22395ms step_avg:39.43ms
step:569/2330 train_time:22417ms step_avg:39.40ms
step:570/2330 train_time:22473ms step_avg:39.43ms
step:571/2330 train_time:22495ms step_avg:39.40ms
step:572/2330 train_time:22551ms step_avg:39.43ms
step:573/2330 train_time:22573ms step_avg:39.39ms
step:574/2330 train_time:22630ms step_avg:39.42ms
step:575/2330 train_time:22652ms step_avg:39.39ms
step:576/2330 train_time:22708ms step_avg:39.42ms
step:577/2330 train_time:22730ms step_avg:39.39ms
step:578/2330 train_time:22787ms step_avg:39.42ms
step:579/2330 train_time:22809ms step_avg:39.39ms
step:580/2330 train_time:22864ms step_avg:39.42ms
step:581/2330 train_time:22887ms step_avg:39.39ms
step:582/2330 train_time:22943ms step_avg:39.42ms
step:583/2330 train_time:22966ms step_avg:39.39ms
step:584/2330 train_time:23022ms step_avg:39.42ms
step:585/2330 train_time:23045ms step_avg:39.39ms
step:586/2330 train_time:23101ms step_avg:39.42ms
step:587/2330 train_time:23125ms step_avg:39.39ms
step:588/2330 train_time:23181ms step_avg:39.42ms
step:589/2330 train_time:23205ms step_avg:39.40ms
step:590/2330 train_time:23261ms step_avg:39.43ms
step:591/2330 train_time:23284ms step_avg:39.40ms
step:592/2330 train_time:23341ms step_avg:39.43ms
step:593/2330 train_time:23364ms step_avg:39.40ms
step:594/2330 train_time:23421ms step_avg:39.43ms
step:595/2330 train_time:23445ms step_avg:39.40ms
step:596/2330 train_time:23501ms step_avg:39.43ms
step:597/2330 train_time:23525ms step_avg:39.40ms
step:598/2330 train_time:23581ms step_avg:39.43ms
step:599/2330 train_time:23604ms step_avg:39.41ms
step:600/2330 train_time:23660ms step_avg:39.43ms
step:601/2330 train_time:23683ms step_avg:39.41ms
step:602/2330 train_time:23738ms step_avg:39.43ms
step:603/2330 train_time:23761ms step_avg:39.40ms
step:604/2330 train_time:23816ms step_avg:39.43ms
step:605/2330 train_time:23839ms step_avg:39.40ms
step:606/2330 train_time:23895ms step_avg:39.43ms
step:607/2330 train_time:23918ms step_avg:39.40ms
step:608/2330 train_time:23974ms step_avg:39.43ms
step:609/2330 train_time:23996ms step_avg:39.40ms
step:610/2330 train_time:24052ms step_avg:39.43ms
step:611/2330 train_time:24074ms step_avg:39.40ms
step:612/2330 train_time:24131ms step_avg:39.43ms
step:613/2330 train_time:24153ms step_avg:39.40ms
step:614/2330 train_time:24209ms step_avg:39.43ms
step:615/2330 train_time:24231ms step_avg:39.40ms
step:616/2330 train_time:24288ms step_avg:39.43ms
step:617/2330 train_time:24311ms step_avg:39.40ms
step:618/2330 train_time:24367ms step_avg:39.43ms
step:619/2330 train_time:24389ms step_avg:39.40ms
step:620/2330 train_time:24446ms step_avg:39.43ms
step:621/2330 train_time:24468ms step_avg:39.40ms
step:622/2330 train_time:24524ms step_avg:39.43ms
step:623/2330 train_time:24547ms step_avg:39.40ms
step:624/2330 train_time:24602ms step_avg:39.43ms
step:625/2330 train_time:24625ms step_avg:39.40ms
step:626/2330 train_time:24681ms step_avg:39.43ms
step:627/2330 train_time:24704ms step_avg:39.40ms
step:628/2330 train_time:24760ms step_avg:39.43ms
step:629/2330 train_time:24782ms step_avg:39.40ms
step:630/2330 train_time:24838ms step_avg:39.43ms
step:631/2330 train_time:24861ms step_avg:39.40ms
step:632/2330 train_time:24918ms step_avg:39.43ms
step:633/2330 train_time:24940ms step_avg:39.40ms
step:634/2330 train_time:24996ms step_avg:39.43ms
step:635/2330 train_time:25019ms step_avg:39.40ms
step:636/2330 train_time:25075ms step_avg:39.43ms
step:637/2330 train_time:25098ms step_avg:39.40ms
step:638/2330 train_time:25154ms step_avg:39.43ms
step:639/2330 train_time:25177ms step_avg:39.40ms
step:640/2330 train_time:25233ms step_avg:39.43ms
step:641/2330 train_time:25255ms step_avg:39.40ms
step:642/2330 train_time:25311ms step_avg:39.43ms
step:643/2330 train_time:25333ms step_avg:39.40ms
step:644/2330 train_time:25390ms step_avg:39.43ms
step:645/2330 train_time:25412ms step_avg:39.40ms
step:646/2330 train_time:25469ms step_avg:39.43ms
step:647/2330 train_time:25491ms step_avg:39.40ms
step:648/2330 train_time:25548ms step_avg:39.43ms
step:649/2330 train_time:25570ms step_avg:39.40ms
step:650/2330 train_time:25627ms step_avg:39.43ms
step:651/2330 train_time:25648ms step_avg:39.40ms
step:652/2330 train_time:25705ms step_avg:39.42ms
step:653/2330 train_time:25727ms step_avg:39.40ms
step:654/2330 train_time:25783ms step_avg:39.42ms
step:655/2330 train_time:25806ms step_avg:39.40ms
step:656/2330 train_time:25862ms step_avg:39.42ms
step:657/2330 train_time:25885ms step_avg:39.40ms
step:658/2330 train_time:25941ms step_avg:39.42ms
step:659/2330 train_time:25964ms step_avg:39.40ms
step:660/2330 train_time:26020ms step_avg:39.42ms
step:661/2330 train_time:26043ms step_avg:39.40ms
step:662/2330 train_time:26099ms step_avg:39.42ms
step:663/2330 train_time:26123ms step_avg:39.40ms
step:664/2330 train_time:26179ms step_avg:39.43ms
step:665/2330 train_time:26202ms step_avg:39.40ms
step:666/2330 train_time:26260ms step_avg:39.43ms
step:667/2330 train_time:26283ms step_avg:39.40ms
step:668/2330 train_time:26339ms step_avg:39.43ms
step:669/2330 train_time:26362ms step_avg:39.40ms
step:670/2330 train_time:26418ms step_avg:39.43ms
step:671/2330 train_time:26441ms step_avg:39.41ms
step:672/2330 train_time:26497ms step_avg:39.43ms
step:673/2330 train_time:26520ms step_avg:39.41ms
step:674/2330 train_time:26576ms step_avg:39.43ms
step:675/2330 train_time:26599ms step_avg:39.41ms
step:676/2330 train_time:26655ms step_avg:39.43ms
step:677/2330 train_time:26676ms step_avg:39.40ms
step:678/2330 train_time:26733ms step_avg:39.43ms
step:679/2330 train_time:26755ms step_avg:39.40ms
step:680/2330 train_time:26812ms step_avg:39.43ms
step:681/2330 train_time:26834ms step_avg:39.40ms
step:682/2330 train_time:26890ms step_avg:39.43ms
step:683/2330 train_time:26911ms step_avg:39.40ms
step:684/2330 train_time:26968ms step_avg:39.43ms
step:685/2330 train_time:26990ms step_avg:39.40ms
step:686/2330 train_time:27046ms step_avg:39.43ms
step:687/2330 train_time:27069ms step_avg:39.40ms
step:688/2330 train_time:27125ms step_avg:39.43ms
step:689/2330 train_time:27148ms step_avg:39.40ms
step:690/2330 train_time:27203ms step_avg:39.42ms
step:691/2330 train_time:27226ms step_avg:39.40ms
step:692/2330 train_time:27282ms step_avg:39.43ms
step:693/2330 train_time:27306ms step_avg:39.40ms
step:694/2330 train_time:27363ms step_avg:39.43ms
step:695/2330 train_time:27386ms step_avg:39.40ms
step:696/2330 train_time:27442ms step_avg:39.43ms
step:697/2330 train_time:27466ms step_avg:39.41ms
step:698/2330 train_time:27522ms step_avg:39.43ms
step:699/2330 train_time:27545ms step_avg:39.41ms
step:700/2330 train_time:27602ms step_avg:39.43ms
step:701/2330 train_time:27625ms step_avg:39.41ms
step:702/2330 train_time:27682ms step_avg:39.43ms
step:703/2330 train_time:27705ms step_avg:39.41ms
step:704/2330 train_time:27761ms step_avg:39.43ms
step:705/2330 train_time:27784ms step_avg:39.41ms
step:706/2330 train_time:27841ms step_avg:39.43ms
step:707/2330 train_time:27863ms step_avg:39.41ms
step:708/2330 train_time:27920ms step_avg:39.43ms
step:709/2330 train_time:27942ms step_avg:39.41ms
step:710/2330 train_time:27999ms step_avg:39.43ms
step:711/2330 train_time:28021ms step_avg:39.41ms
step:712/2330 train_time:28077ms step_avg:39.43ms
step:713/2330 train_time:28099ms step_avg:39.41ms
step:714/2330 train_time:28155ms step_avg:39.43ms
step:715/2330 train_time:28178ms step_avg:39.41ms
step:716/2330 train_time:28234ms step_avg:39.43ms
step:717/2330 train_time:28256ms step_avg:39.41ms
step:718/2330 train_time:28313ms step_avg:39.43ms
step:719/2330 train_time:28334ms step_avg:39.41ms
step:720/2330 train_time:28391ms step_avg:39.43ms
step:721/2330 train_time:28413ms step_avg:39.41ms
step:722/2330 train_time:28470ms step_avg:39.43ms
step:723/2330 train_time:28492ms step_avg:39.41ms
step:724/2330 train_time:28549ms step_avg:39.43ms
step:725/2330 train_time:28571ms step_avg:39.41ms
step:726/2330 train_time:28628ms step_avg:39.43ms
step:727/2330 train_time:28650ms step_avg:39.41ms
step:728/2330 train_time:28707ms step_avg:39.43ms
step:729/2330 train_time:28728ms step_avg:39.41ms
step:730/2330 train_time:28785ms step_avg:39.43ms
step:731/2330 train_time:28808ms step_avg:39.41ms
step:732/2330 train_time:28863ms step_avg:39.43ms
step:733/2330 train_time:28886ms step_avg:39.41ms
step:734/2330 train_time:28942ms step_avg:39.43ms
step:735/2330 train_time:28965ms step_avg:39.41ms
step:736/2330 train_time:29021ms step_avg:39.43ms
step:737/2330 train_time:29044ms step_avg:39.41ms
step:738/2330 train_time:29100ms step_avg:39.43ms
step:739/2330 train_time:29123ms step_avg:39.41ms
step:740/2330 train_time:29180ms step_avg:39.43ms
step:741/2330 train_time:29204ms step_avg:39.41ms
step:742/2330 train_time:29260ms step_avg:39.43ms
step:743/2330 train_time:29283ms step_avg:39.41ms
step:744/2330 train_time:29340ms step_avg:39.44ms
step:745/2330 train_time:29363ms step_avg:39.41ms
step:746/2330 train_time:29420ms step_avg:39.44ms
step:747/2330 train_time:29443ms step_avg:39.42ms
step:748/2330 train_time:29499ms step_avg:39.44ms
step:749/2330 train_time:29522ms step_avg:39.42ms
step:750/2330 train_time:29579ms step_avg:39.44ms
step:750/2330 val_loss:5.2653 train_time:29675ms step_avg:39.57ms
step:751/2330 train_time:29687ms step_avg:39.53ms
step:752/2330 train_time:29698ms step_avg:39.49ms
step:753/2330 train_time:29708ms step_avg:39.45ms
step:754/2330 train_time:29737ms step_avg:39.44ms
step:755/2330 train_time:29759ms step_avg:39.42ms
step:756/2330 train_time:29814ms step_avg:39.44ms
step:757/2330 train_time:29836ms step_avg:39.41ms
step:758/2330 train_time:29892ms step_avg:39.44ms
step:759/2330 train_time:29914ms step_avg:39.41ms
step:760/2330 train_time:29970ms step_avg:39.43ms
step:761/2330 train_time:29997ms step_avg:39.42ms
step:762/2330 train_time:30057ms step_avg:39.45ms
step:763/2330 train_time:30082ms step_avg:39.43ms
step:764/2330 train_time:30140ms step_avg:39.45ms
step:765/2330 train_time:30164ms step_avg:39.43ms
step:766/2330 train_time:30221ms step_avg:39.45ms
step:767/2330 train_time:30242ms step_avg:39.43ms
step:768/2330 train_time:30298ms step_avg:39.45ms
step:769/2330 train_time:30319ms step_avg:39.43ms
step:770/2330 train_time:30375ms step_avg:39.45ms
step:771/2330 train_time:30397ms step_avg:39.42ms
step:772/2330 train_time:30452ms step_avg:39.45ms
step:773/2330 train_time:30475ms step_avg:39.42ms
step:774/2330 train_time:30530ms step_avg:39.45ms
step:775/2330 train_time:30553ms step_avg:39.42ms
step:776/2330 train_time:30610ms step_avg:39.45ms
step:777/2330 train_time:30632ms step_avg:39.42ms
step:778/2330 train_time:30689ms step_avg:39.45ms
step:779/2330 train_time:30711ms step_avg:39.42ms
step:780/2330 train_time:30767ms step_avg:39.44ms
step:781/2330 train_time:30788ms step_avg:39.42ms
step:782/2330 train_time:30844ms step_avg:39.44ms
step:783/2330 train_time:30866ms step_avg:39.42ms
step:784/2330 train_time:30923ms step_avg:39.44ms
step:785/2330 train_time:30946ms step_avg:39.42ms
step:786/2330 train_time:31003ms step_avg:39.44ms
step:787/2330 train_time:31026ms step_avg:39.42ms
step:788/2330 train_time:31082ms step_avg:39.44ms
step:789/2330 train_time:31105ms step_avg:39.42ms
step:790/2330 train_time:31161ms step_avg:39.44ms
step:791/2330 train_time:31183ms step_avg:39.42ms
step:792/2330 train_time:31239ms step_avg:39.44ms
step:793/2330 train_time:31262ms step_avg:39.42ms
step:794/2330 train_time:31318ms step_avg:39.44ms
step:795/2330 train_time:31340ms step_avg:39.42ms
step:796/2330 train_time:31396ms step_avg:39.44ms
step:797/2330 train_time:31418ms step_avg:39.42ms
step:798/2330 train_time:31474ms step_avg:39.44ms
step:799/2330 train_time:31497ms step_avg:39.42ms
step:800/2330 train_time:31553ms step_avg:39.44ms
step:801/2330 train_time:31575ms step_avg:39.42ms
step:802/2330 train_time:31632ms step_avg:39.44ms
step:803/2330 train_time:31654ms step_avg:39.42ms
step:804/2330 train_time:31710ms step_avg:39.44ms
step:805/2330 train_time:31733ms step_avg:39.42ms
step:806/2330 train_time:31788ms step_avg:39.44ms
step:807/2330 train_time:31811ms step_avg:39.42ms
step:808/2330 train_time:31867ms step_avg:39.44ms
step:809/2330 train_time:31890ms step_avg:39.42ms
step:810/2330 train_time:31947ms step_avg:39.44ms
step:811/2330 train_time:31970ms step_avg:39.42ms
step:812/2330 train_time:32027ms step_avg:39.44ms
step:813/2330 train_time:32050ms step_avg:39.42ms
step:814/2330 train_time:32107ms step_avg:39.44ms
step:815/2330 train_time:32130ms step_avg:39.42ms
step:816/2330 train_time:32186ms step_avg:39.44ms
step:817/2330 train_time:32209ms step_avg:39.42ms
step:818/2330 train_time:32266ms step_avg:39.44ms
step:819/2330 train_time:32288ms step_avg:39.42ms
step:820/2330 train_time:32344ms step_avg:39.44ms
step:821/2330 train_time:32366ms step_avg:39.42ms
step:822/2330 train_time:32424ms step_avg:39.44ms
step:823/2330 train_time:32446ms step_avg:39.42ms
step:824/2330 train_time:32503ms step_avg:39.45ms
step:825/2330 train_time:32525ms step_avg:39.42ms
step:826/2330 train_time:32581ms step_avg:39.44ms
step:827/2330 train_time:32604ms step_avg:39.42ms
step:828/2330 train_time:32660ms step_avg:39.44ms
step:829/2330 train_time:32682ms step_avg:39.42ms
step:830/2330 train_time:32739ms step_avg:39.44ms
step:831/2330 train_time:32761ms step_avg:39.42ms
step:832/2330 train_time:32817ms step_avg:39.44ms
step:833/2330 train_time:32839ms step_avg:39.42ms
step:834/2330 train_time:32895ms step_avg:39.44ms
step:835/2330 train_time:32918ms step_avg:39.42ms
step:836/2330 train_time:32974ms step_avg:39.44ms
step:837/2330 train_time:32997ms step_avg:39.42ms
step:838/2330 train_time:33053ms step_avg:39.44ms
step:839/2330 train_time:33077ms step_avg:39.42ms
step:840/2330 train_time:33133ms step_avg:39.44ms
step:841/2330 train_time:33156ms step_avg:39.42ms
step:842/2330 train_time:33212ms step_avg:39.44ms
step:843/2330 train_time:33236ms step_avg:39.43ms
step:844/2330 train_time:33292ms step_avg:39.45ms
step:845/2330 train_time:33316ms step_avg:39.43ms
step:846/2330 train_time:33373ms step_avg:39.45ms
step:847/2330 train_time:33396ms step_avg:39.43ms
step:848/2330 train_time:33452ms step_avg:39.45ms
step:849/2330 train_time:33475ms step_avg:39.43ms
step:850/2330 train_time:33532ms step_avg:39.45ms
step:851/2330 train_time:33555ms step_avg:39.43ms
step:852/2330 train_time:33612ms step_avg:39.45ms
step:853/2330 train_time:33634ms step_avg:39.43ms
step:854/2330 train_time:33690ms step_avg:39.45ms
step:855/2330 train_time:33713ms step_avg:39.43ms
step:856/2330 train_time:33769ms step_avg:39.45ms
step:857/2330 train_time:33792ms step_avg:39.43ms
step:858/2330 train_time:33847ms step_avg:39.45ms
step:859/2330 train_time:33870ms step_avg:39.43ms
step:860/2330 train_time:33926ms step_avg:39.45ms
step:861/2330 train_time:33948ms step_avg:39.43ms
step:862/2330 train_time:34005ms step_avg:39.45ms
step:863/2330 train_time:34026ms step_avg:39.43ms
step:864/2330 train_time:34083ms step_avg:39.45ms
step:865/2330 train_time:34105ms step_avg:39.43ms
step:866/2330 train_time:34161ms step_avg:39.45ms
step:867/2330 train_time:34184ms step_avg:39.43ms
step:868/2330 train_time:34240ms step_avg:39.45ms
step:869/2330 train_time:34262ms step_avg:39.43ms
step:870/2330 train_time:34318ms step_avg:39.45ms
step:871/2330 train_time:34341ms step_avg:39.43ms
step:872/2330 train_time:34397ms step_avg:39.45ms
step:873/2330 train_time:34420ms step_avg:39.43ms
step:874/2330 train_time:34476ms step_avg:39.45ms
step:875/2330 train_time:34498ms step_avg:39.43ms
step:876/2330 train_time:34554ms step_avg:39.45ms
step:877/2330 train_time:34578ms step_avg:39.43ms
step:878/2330 train_time:34633ms step_avg:39.45ms
step:879/2330 train_time:34656ms step_avg:39.43ms
step:880/2330 train_time:34712ms step_avg:39.45ms
step:881/2330 train_time:34735ms step_avg:39.43ms
step:882/2330 train_time:34791ms step_avg:39.45ms
step:883/2330 train_time:34814ms step_avg:39.43ms
step:884/2330 train_time:34870ms step_avg:39.45ms
step:885/2330 train_time:34892ms step_avg:39.43ms
step:886/2330 train_time:34948ms step_avg:39.44ms
step:887/2330 train_time:34971ms step_avg:39.43ms
step:888/2330 train_time:35028ms step_avg:39.45ms
step:889/2330 train_time:35050ms step_avg:39.43ms
step:890/2330 train_time:35107ms step_avg:39.45ms
step:891/2330 train_time:35128ms step_avg:39.43ms
step:892/2330 train_time:35184ms step_avg:39.44ms
step:893/2330 train_time:35206ms step_avg:39.42ms
step:894/2330 train_time:35262ms step_avg:39.44ms
step:895/2330 train_time:35284ms step_avg:39.42ms
step:896/2330 train_time:35341ms step_avg:39.44ms
step:897/2330 train_time:35362ms step_avg:39.42ms
step:898/2330 train_time:35419ms step_avg:39.44ms
step:899/2330 train_time:35441ms step_avg:39.42ms
step:900/2330 train_time:35498ms step_avg:39.44ms
step:901/2330 train_time:35520ms step_avg:39.42ms
step:902/2330 train_time:35576ms step_avg:39.44ms
step:903/2330 train_time:35598ms step_avg:39.42ms
step:904/2330 train_time:35654ms step_avg:39.44ms
step:905/2330 train_time:35677ms step_avg:39.42ms
step:906/2330 train_time:35733ms step_avg:39.44ms
step:907/2330 train_time:35755ms step_avg:39.42ms
step:908/2330 train_time:35811ms step_avg:39.44ms
step:909/2330 train_time:35834ms step_avg:39.42ms
step:910/2330 train_time:35891ms step_avg:39.44ms
step:911/2330 train_time:35915ms step_avg:39.42ms
step:912/2330 train_time:35972ms step_avg:39.44ms
step:913/2330 train_time:35995ms step_avg:39.43ms
step:914/2330 train_time:36051ms step_avg:39.44ms
step:915/2330 train_time:36074ms step_avg:39.43ms
step:916/2330 train_time:36131ms step_avg:39.44ms
step:917/2330 train_time:36153ms step_avg:39.43ms
step:918/2330 train_time:36210ms step_avg:39.44ms
step:919/2330 train_time:36233ms step_avg:39.43ms
step:920/2330 train_time:36289ms step_avg:39.44ms
step:921/2330 train_time:36312ms step_avg:39.43ms
step:922/2330 train_time:36369ms step_avg:39.45ms
step:923/2330 train_time:36392ms step_avg:39.43ms
step:924/2330 train_time:36448ms step_avg:39.45ms
step:925/2330 train_time:36471ms step_avg:39.43ms
step:926/2330 train_time:36527ms step_avg:39.45ms
step:927/2330 train_time:36549ms step_avg:39.43ms
step:928/2330 train_time:36605ms step_avg:39.44ms
step:929/2330 train_time:36626ms step_avg:39.43ms
step:930/2330 train_time:36682ms step_avg:39.44ms
step:931/2330 train_time:36704ms step_avg:39.42ms
step:932/2330 train_time:36761ms step_avg:39.44ms
step:933/2330 train_time:36783ms step_avg:39.42ms
step:934/2330 train_time:36840ms step_avg:39.44ms
step:935/2330 train_time:36862ms step_avg:39.42ms
step:936/2330 train_time:36919ms step_avg:39.44ms
step:937/2330 train_time:36941ms step_avg:39.42ms
step:938/2330 train_time:36998ms step_avg:39.44ms
step:939/2330 train_time:37021ms step_avg:39.43ms
step:940/2330 train_time:37077ms step_avg:39.44ms
step:941/2330 train_time:37099ms step_avg:39.42ms
step:942/2330 train_time:37154ms step_avg:39.44ms
step:943/2330 train_time:37177ms step_avg:39.42ms
step:944/2330 train_time:37233ms step_avg:39.44ms
step:945/2330 train_time:37256ms step_avg:39.42ms
step:946/2330 train_time:37312ms step_avg:39.44ms
step:947/2330 train_time:37336ms step_avg:39.43ms
step:948/2330 train_time:37392ms step_avg:39.44ms
step:949/2330 train_time:37416ms step_avg:39.43ms
step:950/2330 train_time:37472ms step_avg:39.44ms
step:951/2330 train_time:37495ms step_avg:39.43ms
step:952/2330 train_time:37552ms step_avg:39.45ms
step:953/2330 train_time:37575ms step_avg:39.43ms
step:954/2330 train_time:37631ms step_avg:39.45ms
step:955/2330 train_time:37653ms step_avg:39.43ms
step:956/2330 train_time:37709ms step_avg:39.44ms
step:957/2330 train_time:37732ms step_avg:39.43ms
step:958/2330 train_time:37788ms step_avg:39.44ms
step:959/2330 train_time:37811ms step_avg:39.43ms
step:960/2330 train_time:37867ms step_avg:39.45ms
step:961/2330 train_time:37889ms step_avg:39.43ms
step:962/2330 train_time:37945ms step_avg:39.44ms
step:963/2330 train_time:37968ms step_avg:39.43ms
step:964/2330 train_time:38024ms step_avg:39.44ms
step:965/2330 train_time:38046ms step_avg:39.43ms
step:966/2330 train_time:38101ms step_avg:39.44ms
step:967/2330 train_time:38124ms step_avg:39.42ms
step:968/2330 train_time:38180ms step_avg:39.44ms
step:969/2330 train_time:38203ms step_avg:39.42ms
step:970/2330 train_time:38260ms step_avg:39.44ms
step:971/2330 train_time:38282ms step_avg:39.43ms
step:972/2330 train_time:38339ms step_avg:39.44ms
step:973/2330 train_time:38362ms step_avg:39.43ms
step:974/2330 train_time:38418ms step_avg:39.44ms
step:975/2330 train_time:38440ms step_avg:39.43ms
step:976/2330 train_time:38496ms step_avg:39.44ms
step:977/2330 train_time:38519ms step_avg:39.43ms
step:978/2330 train_time:38575ms step_avg:39.44ms
step:979/2330 train_time:38598ms step_avg:39.43ms
step:980/2330 train_time:38654ms step_avg:39.44ms
step:981/2330 train_time:38677ms step_avg:39.43ms
step:982/2330 train_time:38734ms step_avg:39.44ms
step:983/2330 train_time:38757ms step_avg:39.43ms
step:984/2330 train_time:38813ms step_avg:39.44ms
step:985/2330 train_time:38836ms step_avg:39.43ms
step:986/2330 train_time:38893ms step_avg:39.44ms
step:987/2330 train_time:38916ms step_avg:39.43ms
step:988/2330 train_time:38973ms step_avg:39.45ms
step:989/2330 train_time:38996ms step_avg:39.43ms
step:990/2330 train_time:39052ms step_avg:39.45ms
step:991/2330 train_time:39076ms step_avg:39.43ms
step:992/2330 train_time:39132ms step_avg:39.45ms
step:993/2330 train_time:39155ms step_avg:39.43ms
step:994/2330 train_time:39211ms step_avg:39.45ms
step:995/2330 train_time:39234ms step_avg:39.43ms
step:996/2330 train_time:39290ms step_avg:39.45ms
step:997/2330 train_time:39313ms step_avg:39.43ms
step:998/2330 train_time:39369ms step_avg:39.45ms
step:999/2330 train_time:39392ms step_avg:39.43ms
step:1000/2330 train_time:39448ms step_avg:39.45ms
step:1000/2330 val_loss:5.2292 train_time:39545ms step_avg:39.54ms
step:1001/2330 train_time:39557ms step_avg:39.52ms
step:1002/2330 train_time:39570ms step_avg:39.49ms
step:1003/2330 train_time:39580ms step_avg:39.46ms
step:1004/2330 train_time:39607ms step_avg:39.45ms
step:1005/2330 train_time:39628ms step_avg:39.43ms
step:1006/2330 train_time:39684ms step_avg:39.45ms
step:1007/2330 train_time:39705ms step_avg:39.43ms
step:1008/2330 train_time:39760ms step_avg:39.44ms
step:1009/2330 train_time:39781ms step_avg:39.43ms
step:1010/2330 train_time:39837ms step_avg:39.44ms
step:1011/2330 train_time:39863ms step_avg:39.43ms
step:1012/2330 train_time:39924ms step_avg:39.45ms
step:1013/2330 train_time:39948ms step_avg:39.43ms
step:1014/2330 train_time:40005ms step_avg:39.45ms
step:1015/2330 train_time:40027ms step_avg:39.44ms
step:1016/2330 train_time:40083ms step_avg:39.45ms
step:1017/2330 train_time:40106ms step_avg:39.44ms
step:1018/2330 train_time:40161ms step_avg:39.45ms
step:1019/2330 train_time:40183ms step_avg:39.43ms
step:1020/2330 train_time:40239ms step_avg:39.45ms
step:1021/2330 train_time:40261ms step_avg:39.43ms
step:1022/2330 train_time:40316ms step_avg:39.45ms
step:1023/2330 train_time:40339ms step_avg:39.43ms
step:1024/2330 train_time:40395ms step_avg:39.45ms
step:1025/2330 train_time:40416ms step_avg:39.43ms
step:1026/2330 train_time:40473ms step_avg:39.45ms
step:1027/2330 train_time:40496ms step_avg:39.43ms
step:1028/2330 train_time:40552ms step_avg:39.45ms
step:1029/2330 train_time:40575ms step_avg:39.43ms
step:1030/2330 train_time:40630ms step_avg:39.45ms
step:1031/2330 train_time:40653ms step_avg:39.43ms
step:1032/2330 train_time:40709ms step_avg:39.45ms
step:1033/2330 train_time:40731ms step_avg:39.43ms
step:1034/2330 train_time:40788ms step_avg:39.45ms
step:1035/2330 train_time:40811ms step_avg:39.43ms
step:1036/2330 train_time:40868ms step_avg:39.45ms
step:1037/2330 train_time:40892ms step_avg:39.43ms
step:1038/2330 train_time:40949ms step_avg:39.45ms
step:1039/2330 train_time:40973ms step_avg:39.43ms
step:1040/2330 train_time:41030ms step_avg:39.45ms
step:1041/2330 train_time:41053ms step_avg:39.44ms
step:1042/2330 train_time:41110ms step_avg:39.45ms
step:1043/2330 train_time:41133ms step_avg:39.44ms
step:1044/2330 train_time:41189ms step_avg:39.45ms
step:1045/2330 train_time:41213ms step_avg:39.44ms
step:1046/2330 train_time:41269ms step_avg:39.45ms
step:1047/2330 train_time:41291ms step_avg:39.44ms
step:1048/2330 train_time:41347ms step_avg:39.45ms
step:1049/2330 train_time:41370ms step_avg:39.44ms
step:1050/2330 train_time:41426ms step_avg:39.45ms
step:1051/2330 train_time:41448ms step_avg:39.44ms
step:1052/2330 train_time:41504ms step_avg:39.45ms
step:1053/2330 train_time:41526ms step_avg:39.44ms
step:1054/2330 train_time:41582ms step_avg:39.45ms
step:1055/2330 train_time:41603ms step_avg:39.43ms
step:1056/2330 train_time:41659ms step_avg:39.45ms
step:1057/2330 train_time:41681ms step_avg:39.43ms
step:1058/2330 train_time:41738ms step_avg:39.45ms
step:1059/2330 train_time:41760ms step_avg:39.43ms
step:1060/2330 train_time:41816ms step_avg:39.45ms
step:1061/2330 train_time:41839ms step_avg:39.43ms
step:1062/2330 train_time:41896ms step_avg:39.45ms
step:1063/2330 train_time:41918ms step_avg:39.43ms
step:1064/2330 train_time:41975ms step_avg:39.45ms
step:1065/2330 train_time:41997ms step_avg:39.43ms
step:1066/2330 train_time:42054ms step_avg:39.45ms
step:1067/2330 train_time:42077ms step_avg:39.43ms
step:1068/2330 train_time:42134ms step_avg:39.45ms
step:1069/2330 train_time:42156ms step_avg:39.44ms
step:1070/2330 train_time:42212ms step_avg:39.45ms
step:1071/2330 train_time:42235ms step_avg:39.43ms
step:1072/2330 train_time:42290ms step_avg:39.45ms
step:1073/2330 train_time:42313ms step_avg:39.43ms
step:1074/2330 train_time:42369ms step_avg:39.45ms
step:1075/2330 train_time:42392ms step_avg:39.43ms
step:1076/2330 train_time:42448ms step_avg:39.45ms
step:1077/2330 train_time:42471ms step_avg:39.43ms
step:1078/2330 train_time:42528ms step_avg:39.45ms
step:1079/2330 train_time:42551ms step_avg:39.44ms
step:1080/2330 train_time:42607ms step_avg:39.45ms
step:1081/2330 train_time:42630ms step_avg:39.44ms
step:1082/2330 train_time:42686ms step_avg:39.45ms
step:1083/2330 train_time:42708ms step_avg:39.43ms
step:1084/2330 train_time:42764ms step_avg:39.45ms
step:1085/2330 train_time:42786ms step_avg:39.43ms
step:1086/2330 train_time:42843ms step_avg:39.45ms
step:1087/2330 train_time:42865ms step_avg:39.43ms
step:1088/2330 train_time:42922ms step_avg:39.45ms
step:1089/2330 train_time:42944ms step_avg:39.43ms
step:1090/2330 train_time:43000ms step_avg:39.45ms
step:1091/2330 train_time:43022ms step_avg:39.43ms
step:1092/2330 train_time:43078ms step_avg:39.45ms
step:1093/2330 train_time:43100ms step_avg:39.43ms
step:1094/2330 train_time:43157ms step_avg:39.45ms
step:1095/2330 train_time:43179ms step_avg:39.43ms
step:1096/2330 train_time:43235ms step_avg:39.45ms
step:1097/2330 train_time:43257ms step_avg:39.43ms
step:1098/2330 train_time:43313ms step_avg:39.45ms
step:1099/2330 train_time:43336ms step_avg:39.43ms
step:1100/2330 train_time:43392ms step_avg:39.45ms
step:1101/2330 train_time:43415ms step_avg:39.43ms
step:1102/2330 train_time:43471ms step_avg:39.45ms
step:1103/2330 train_time:43494ms step_avg:39.43ms
step:1104/2330 train_time:43550ms step_avg:39.45ms
step:1105/2330 train_time:43573ms step_avg:39.43ms
step:1106/2330 train_time:43629ms step_avg:39.45ms
step:1107/2330 train_time:43652ms step_avg:39.43ms
step:1108/2330 train_time:43708ms step_avg:39.45ms
step:1109/2330 train_time:43731ms step_avg:39.43ms
step:1110/2330 train_time:43787ms step_avg:39.45ms
step:1111/2330 train_time:43811ms step_avg:39.43ms
step:1112/2330 train_time:43869ms step_avg:39.45ms
step:1113/2330 train_time:43892ms step_avg:39.44ms
step:1114/2330 train_time:43949ms step_avg:39.45ms
step:1115/2330 train_time:43972ms step_avg:39.44ms
step:1116/2330 train_time:44029ms step_avg:39.45ms
step:1117/2330 train_time:44052ms step_avg:39.44ms
step:1118/2330 train_time:44109ms step_avg:39.45ms
step:1119/2330 train_time:44132ms step_avg:39.44ms
step:1120/2330 train_time:44189ms step_avg:39.45ms
step:1121/2330 train_time:44212ms step_avg:39.44ms
step:1122/2330 train_time:44268ms step_avg:39.45ms
step:1123/2330 train_time:44291ms step_avg:39.44ms
step:1124/2330 train_time:44347ms step_avg:39.45ms
step:1125/2330 train_time:44369ms step_avg:39.44ms
step:1126/2330 train_time:44425ms step_avg:39.45ms
step:1127/2330 train_time:44448ms step_avg:39.44ms
step:1128/2330 train_time:44503ms step_avg:39.45ms
step:1129/2330 train_time:44526ms step_avg:39.44ms
step:1130/2330 train_time:44582ms step_avg:39.45ms
step:1131/2330 train_time:44604ms step_avg:39.44ms
step:1132/2330 train_time:44660ms step_avg:39.45ms
step:1133/2330 train_time:44682ms step_avg:39.44ms
step:1134/2330 train_time:44738ms step_avg:39.45ms
step:1135/2330 train_time:44760ms step_avg:39.44ms
step:1136/2330 train_time:44817ms step_avg:39.45ms
step:1137/2330 train_time:44839ms step_avg:39.44ms
step:1138/2330 train_time:44896ms step_avg:39.45ms
step:1139/2330 train_time:44918ms step_avg:39.44ms
step:1140/2330 train_time:44975ms step_avg:39.45ms
step:1141/2330 train_time:44997ms step_avg:39.44ms
step:1142/2330 train_time:45054ms step_avg:39.45ms
step:1143/2330 train_time:45077ms step_avg:39.44ms
step:1144/2330 train_time:45132ms step_avg:39.45ms
step:1145/2330 train_time:45155ms step_avg:39.44ms
step:1146/2330 train_time:45210ms step_avg:39.45ms
step:1147/2330 train_time:45233ms step_avg:39.44ms
step:1148/2330 train_time:45289ms step_avg:39.45ms
step:1149/2330 train_time:45312ms step_avg:39.44ms
step:1150/2330 train_time:45368ms step_avg:39.45ms
step:1151/2330 train_time:45391ms step_avg:39.44ms
step:1152/2330 train_time:45447ms step_avg:39.45ms
step:1153/2330 train_time:45470ms step_avg:39.44ms
step:1154/2330 train_time:45527ms step_avg:39.45ms
step:1155/2330 train_time:45550ms step_avg:39.44ms
step:1156/2330 train_time:45606ms step_avg:39.45ms
step:1157/2330 train_time:45629ms step_avg:39.44ms
step:1158/2330 train_time:45685ms step_avg:39.45ms
step:1159/2330 train_time:45708ms step_avg:39.44ms
step:1160/2330 train_time:45764ms step_avg:39.45ms
step:1161/2330 train_time:45786ms step_avg:39.44ms
step:1162/2330 train_time:45843ms step_avg:39.45ms
step:1163/2330 train_time:45866ms step_avg:39.44ms
step:1164/2330 train_time:45922ms step_avg:39.45ms
step:1165/2330 train_time:45944ms step_avg:39.44ms
step:1166/2330 train_time:46001ms step_avg:39.45ms
step:1167/2330 train_time:46023ms step_avg:39.44ms
step:1168/2330 train_time:46080ms step_avg:39.45ms
step:1169/2330 train_time:46102ms step_avg:39.44ms
step:1170/2330 train_time:46159ms step_avg:39.45ms
step:1171/2330 train_time:46181ms step_avg:39.44ms
step:1172/2330 train_time:46238ms step_avg:39.45ms
step:1173/2330 train_time:46260ms step_avg:39.44ms
step:1174/2330 train_time:46317ms step_avg:39.45ms
step:1175/2330 train_time:46340ms step_avg:39.44ms
step:1176/2330 train_time:46396ms step_avg:39.45ms
step:1177/2330 train_time:46419ms step_avg:39.44ms
step:1178/2330 train_time:46476ms step_avg:39.45ms
step:1179/2330 train_time:46498ms step_avg:39.44ms
step:1180/2330 train_time:46554ms step_avg:39.45ms
step:1181/2330 train_time:46577ms step_avg:39.44ms
step:1182/2330 train_time:46633ms step_avg:39.45ms
step:1183/2330 train_time:46655ms step_avg:39.44ms
step:1184/2330 train_time:46711ms step_avg:39.45ms
step:1185/2330 train_time:46734ms step_avg:39.44ms
step:1186/2330 train_time:46790ms step_avg:39.45ms
step:1187/2330 train_time:46813ms step_avg:39.44ms
step:1188/2330 train_time:46869ms step_avg:39.45ms
step:1189/2330 train_time:46892ms step_avg:39.44ms
step:1190/2330 train_time:46949ms step_avg:39.45ms
step:1191/2330 train_time:46973ms step_avg:39.44ms
step:1192/2330 train_time:47030ms step_avg:39.45ms
step:1193/2330 train_time:47053ms step_avg:39.44ms
step:1194/2330 train_time:47109ms step_avg:39.45ms
step:1195/2330 train_time:47132ms step_avg:39.44ms
step:1196/2330 train_time:47188ms step_avg:39.45ms
step:1197/2330 train_time:47210ms step_avg:39.44ms
step:1198/2330 train_time:47267ms step_avg:39.45ms
step:1199/2330 train_time:47289ms step_avg:39.44ms
step:1200/2330 train_time:47346ms step_avg:39.45ms
step:1201/2330 train_time:47369ms step_avg:39.44ms
step:1202/2330 train_time:47425ms step_avg:39.45ms
step:1203/2330 train_time:47447ms step_avg:39.44ms
step:1204/2330 train_time:47503ms step_avg:39.45ms
step:1205/2330 train_time:47525ms step_avg:39.44ms
step:1206/2330 train_time:47580ms step_avg:39.45ms
step:1207/2330 train_time:47602ms step_avg:39.44ms
step:1208/2330 train_time:47658ms step_avg:39.45ms
step:1209/2330 train_time:47680ms step_avg:39.44ms
step:1210/2330 train_time:47737ms step_avg:39.45ms
step:1211/2330 train_time:47758ms step_avg:39.44ms
step:1212/2330 train_time:47814ms step_avg:39.45ms
step:1213/2330 train_time:47837ms step_avg:39.44ms
step:1214/2330 train_time:47892ms step_avg:39.45ms
step:1215/2330 train_time:47915ms step_avg:39.44ms
step:1216/2330 train_time:47971ms step_avg:39.45ms
step:1217/2330 train_time:47994ms step_avg:39.44ms
step:1218/2330 train_time:48050ms step_avg:39.45ms
step:1219/2330 train_time:48073ms step_avg:39.44ms
step:1220/2330 train_time:48129ms step_avg:39.45ms
step:1221/2330 train_time:48152ms step_avg:39.44ms
step:1222/2330 train_time:48208ms step_avg:39.45ms
step:1223/2330 train_time:48231ms step_avg:39.44ms
step:1224/2330 train_time:48288ms step_avg:39.45ms
step:1225/2330 train_time:48311ms step_avg:39.44ms
step:1226/2330 train_time:48367ms step_avg:39.45ms
step:1227/2330 train_time:48390ms step_avg:39.44ms
step:1228/2330 train_time:48446ms step_avg:39.45ms
step:1229/2330 train_time:48469ms step_avg:39.44ms
step:1230/2330 train_time:48525ms step_avg:39.45ms
step:1231/2330 train_time:48547ms step_avg:39.44ms
step:1232/2330 train_time:48604ms step_avg:39.45ms
step:1233/2330 train_time:48626ms step_avg:39.44ms
step:1234/2330 train_time:48683ms step_avg:39.45ms
step:1235/2330 train_time:48705ms step_avg:39.44ms
step:1236/2330 train_time:48761ms step_avg:39.45ms
step:1237/2330 train_time:48783ms step_avg:39.44ms
step:1238/2330 train_time:48840ms step_avg:39.45ms
step:1239/2330 train_time:48862ms step_avg:39.44ms
step:1240/2330 train_time:48918ms step_avg:39.45ms
step:1241/2330 train_time:48940ms step_avg:39.44ms
step:1242/2330 train_time:48997ms step_avg:39.45ms
step:1243/2330 train_time:49019ms step_avg:39.44ms
step:1244/2330 train_time:49075ms step_avg:39.45ms
step:1245/2330 train_time:49098ms step_avg:39.44ms
step:1246/2330 train_time:49155ms step_avg:39.45ms
step:1247/2330 train_time:49176ms step_avg:39.44ms
step:1248/2330 train_time:49233ms step_avg:39.45ms
step:1249/2330 train_time:49255ms step_avg:39.44ms
step:1250/2330 train_time:49310ms step_avg:39.45ms
step:1250/2330 val_loss:5.1935 train_time:49408ms step_avg:39.53ms
step:1251/2330 train_time:49420ms step_avg:39.50ms
step:1252/2330 train_time:49432ms step_avg:39.48ms
step:1253/2330 train_time:49441ms step_avg:39.46ms
step:1254/2330 train_time:49471ms step_avg:39.45ms
step:1255/2330 train_time:49492ms step_avg:39.44ms
step:1256/2330 train_time:49547ms step_avg:39.45ms
step:1257/2330 train_time:49568ms step_avg:39.43ms
step:1258/2330 train_time:49623ms step_avg:39.45ms
step:1259/2330 train_time:49645ms step_avg:39.43ms
step:1260/2330 train_time:49701ms step_avg:39.45ms
step:1261/2330 train_time:49728ms step_avg:39.44ms
step:1262/2330 train_time:49787ms step_avg:39.45ms
step:1263/2330 train_time:49812ms step_avg:39.44ms
step:1264/2330 train_time:49869ms step_avg:39.45ms
step:1265/2330 train_time:49891ms step_avg:39.44ms
step:1266/2330 train_time:49946ms step_avg:39.45ms
step:1267/2330 train_time:49969ms step_avg:39.44ms
step:1268/2330 train_time:50025ms step_avg:39.45ms
step:1269/2330 train_time:50047ms step_avg:39.44ms
step:1270/2330 train_time:50103ms step_avg:39.45ms
step:1271/2330 train_time:50125ms step_avg:39.44ms
step:1272/2330 train_time:50180ms step_avg:39.45ms
step:1273/2330 train_time:50203ms step_avg:39.44ms
step:1274/2330 train_time:50258ms step_avg:39.45ms
step:1275/2330 train_time:50280ms step_avg:39.44ms
step:1276/2330 train_time:50336ms step_avg:39.45ms
step:1277/2330 train_time:50358ms step_avg:39.43ms
step:1278/2330 train_time:50414ms step_avg:39.45ms
step:1279/2330 train_time:50436ms step_avg:39.43ms
step:1280/2330 train_time:50492ms step_avg:39.45ms
step:1281/2330 train_time:50514ms step_avg:39.43ms
step:1282/2330 train_time:50570ms step_avg:39.45ms
step:1283/2330 train_time:50592ms step_avg:39.43ms
step:1284/2330 train_time:50648ms step_avg:39.45ms
step:1285/2330 train_time:50670ms step_avg:39.43ms
step:1286/2330 train_time:50727ms step_avg:39.45ms
step:1287/2330 train_time:50750ms step_avg:39.43ms
step:1288/2330 train_time:50806ms step_avg:39.45ms
step:1289/2330 train_time:50829ms step_avg:39.43ms
step:1290/2330 train_time:50885ms step_avg:39.45ms
step:1291/2330 train_time:50908ms step_avg:39.43ms
step:1292/2330 train_time:50965ms step_avg:39.45ms
step:1293/2330 train_time:50987ms step_avg:39.43ms
step:1294/2330 train_time:51043ms step_avg:39.45ms
step:1295/2330 train_time:51065ms step_avg:39.43ms
step:1296/2330 train_time:51121ms step_avg:39.45ms
step:1297/2330 train_time:51144ms step_avg:39.43ms
step:1298/2330 train_time:51200ms step_avg:39.45ms
step:1299/2330 train_time:51222ms step_avg:39.43ms
step:1300/2330 train_time:51278ms step_avg:39.44ms
step:1301/2330 train_time:51300ms step_avg:39.43ms
step:1302/2330 train_time:51357ms step_avg:39.44ms
step:1303/2330 train_time:51380ms step_avg:39.43ms
step:1304/2330 train_time:51435ms step_avg:39.44ms
step:1305/2330 train_time:51456ms step_avg:39.43ms
step:1306/2330 train_time:51512ms step_avg:39.44ms
step:1307/2330 train_time:51534ms step_avg:39.43ms
step:1308/2330 train_time:51590ms step_avg:39.44ms
step:1309/2330 train_time:51612ms step_avg:39.43ms
step:1310/2330 train_time:51668ms step_avg:39.44ms
step:1311/2330 train_time:51690ms step_avg:39.43ms
step:1312/2330 train_time:51746ms step_avg:39.44ms
step:1313/2330 train_time:51768ms step_avg:39.43ms
step:1314/2330 train_time:51824ms step_avg:39.44ms
step:1315/2330 train_time:51847ms step_avg:39.43ms
step:1316/2330 train_time:51903ms step_avg:39.44ms
step:1317/2330 train_time:51925ms step_avg:39.43ms
step:1318/2330 train_time:51981ms step_avg:39.44ms
step:1319/2330 train_time:52004ms step_avg:39.43ms
step:1320/2330 train_time:52060ms step_avg:39.44ms
step:1321/2330 train_time:52082ms step_avg:39.43ms
step:1322/2330 train_time:52139ms step_avg:39.44ms
step:1323/2330 train_time:52161ms step_avg:39.43ms
step:1324/2330 train_time:52217ms step_avg:39.44ms
step:1325/2330 train_time:52239ms step_avg:39.43ms
step:1326/2330 train_time:52295ms step_avg:39.44ms
step:1327/2330 train_time:52317ms step_avg:39.42ms
step:1328/2330 train_time:52373ms step_avg:39.44ms
step:1329/2330 train_time:52394ms step_avg:39.42ms
step:1330/2330 train_time:52449ms step_avg:39.44ms
step:1331/2330 train_time:52471ms step_avg:39.42ms
step:1332/2330 train_time:52527ms step_avg:39.43ms
step:1333/2330 train_time:52550ms step_avg:39.42ms
step:1334/2330 train_time:52605ms step_avg:39.43ms
step:1335/2330 train_time:52628ms step_avg:39.42ms
step:1336/2330 train_time:52684ms step_avg:39.43ms
step:1337/2330 train_time:52707ms step_avg:39.42ms
step:1338/2330 train_time:52764ms step_avg:39.43ms
step:1339/2330 train_time:52785ms step_avg:39.42ms
step:1340/2330 train_time:52842ms step_avg:39.43ms
step:1341/2330 train_time:52864ms step_avg:39.42ms
step:1342/2330 train_time:52920ms step_avg:39.43ms
step:1343/2330 train_time:52943ms step_avg:39.42ms
step:1344/2330 train_time:52999ms step_avg:39.43ms
step:1345/2330 train_time:53021ms step_avg:39.42ms
step:1346/2330 train_time:53077ms step_avg:39.43ms
step:1347/2330 train_time:53100ms step_avg:39.42ms
step:1348/2330 train_time:53156ms step_avg:39.43ms
step:1349/2330 train_time:53179ms step_avg:39.42ms
step:1350/2330 train_time:53234ms step_avg:39.43ms
step:1351/2330 train_time:53257ms step_avg:39.42ms
step:1352/2330 train_time:53312ms step_avg:39.43ms
step:1353/2330 train_time:53333ms step_avg:39.42ms
step:1354/2330 train_time:53389ms step_avg:39.43ms
step:1355/2330 train_time:53411ms step_avg:39.42ms
step:1356/2330 train_time:53467ms step_avg:39.43ms
step:1357/2330 train_time:53489ms step_avg:39.42ms
step:1358/2330 train_time:53545ms step_avg:39.43ms
step:1359/2330 train_time:53568ms step_avg:39.42ms
step:1360/2330 train_time:53623ms step_avg:39.43ms
step:1361/2330 train_time:53646ms step_avg:39.42ms
step:1362/2330 train_time:53702ms step_avg:39.43ms
step:1363/2330 train_time:53725ms step_avg:39.42ms
step:1364/2330 train_time:53780ms step_avg:39.43ms
step:1365/2330 train_time:53803ms step_avg:39.42ms
step:1366/2330 train_time:53861ms step_avg:39.43ms
step:1367/2330 train_time:53883ms step_avg:39.42ms
step:1368/2330 train_time:53939ms step_avg:39.43ms
step:1369/2330 train_time:53962ms step_avg:39.42ms
step:1370/2330 train_time:54018ms step_avg:39.43ms
step:1371/2330 train_time:54040ms step_avg:39.42ms
step:1372/2330 train_time:54097ms step_avg:39.43ms
step:1373/2330 train_time:54119ms step_avg:39.42ms
step:1374/2330 train_time:54175ms step_avg:39.43ms
step:1375/2330 train_time:54197ms step_avg:39.42ms
step:1376/2330 train_time:54253ms step_avg:39.43ms
step:1377/2330 train_time:54274ms step_avg:39.41ms
step:1378/2330 train_time:54330ms step_avg:39.43ms
step:1379/2330 train_time:54351ms step_avg:39.41ms
step:1380/2330 train_time:54407ms step_avg:39.43ms
step:1381/2330 train_time:54429ms step_avg:39.41ms
step:1382/2330 train_time:54485ms step_avg:39.42ms
step:1383/2330 train_time:54508ms step_avg:39.41ms
step:1384/2330 train_time:54563ms step_avg:39.42ms
step:1385/2330 train_time:54586ms step_avg:39.41ms
step:1386/2330 train_time:54642ms step_avg:39.42ms
step:1387/2330 train_time:54664ms step_avg:39.41ms
step:1388/2330 train_time:54720ms step_avg:39.42ms
step:1389/2330 train_time:54743ms step_avg:39.41ms
step:1390/2330 train_time:54798ms step_avg:39.42ms
step:1391/2330 train_time:54821ms step_avg:39.41ms
step:1392/2330 train_time:54876ms step_avg:39.42ms
step:1393/2330 train_time:54899ms step_avg:39.41ms
step:1394/2330 train_time:54955ms step_avg:39.42ms
step:1395/2330 train_time:54977ms step_avg:39.41ms
step:1396/2330 train_time:55033ms step_avg:39.42ms
step:1397/2330 train_time:55055ms step_avg:39.41ms
step:1398/2330 train_time:55112ms step_avg:39.42ms
step:1399/2330 train_time:55134ms step_avg:39.41ms
step:1400/2330 train_time:55189ms step_avg:39.42ms
step:1401/2330 train_time:55211ms step_avg:39.41ms
step:1402/2330 train_time:55267ms step_avg:39.42ms
step:1403/2330 train_time:55289ms step_avg:39.41ms
step:1404/2330 train_time:55345ms step_avg:39.42ms
step:1405/2330 train_time:55367ms step_avg:39.41ms
step:1406/2330 train_time:55423ms step_avg:39.42ms
step:1407/2330 train_time:55445ms step_avg:39.41ms
step:1408/2330 train_time:55502ms step_avg:39.42ms
step:1409/2330 train_time:55524ms step_avg:39.41ms
step:1410/2330 train_time:55580ms step_avg:39.42ms
step:1411/2330 train_time:55602ms step_avg:39.41ms
step:1412/2330 train_time:55659ms step_avg:39.42ms
step:1413/2330 train_time:55681ms step_avg:39.41ms
step:1414/2330 train_time:55737ms step_avg:39.42ms
step:1415/2330 train_time:55759ms step_avg:39.41ms
step:1416/2330 train_time:55815ms step_avg:39.42ms
step:1417/2330 train_time:55837ms step_avg:39.40ms
step:1418/2330 train_time:55893ms step_avg:39.42ms
step:1419/2330 train_time:55915ms step_avg:39.40ms
step:1420/2330 train_time:55972ms step_avg:39.42ms
step:1421/2330 train_time:55993ms step_avg:39.40ms
step:1422/2330 train_time:56049ms step_avg:39.42ms
step:1423/2330 train_time:56071ms step_avg:39.40ms
step:1424/2330 train_time:56127ms step_avg:39.42ms
step:1425/2330 train_time:56150ms step_avg:39.40ms
step:1426/2330 train_time:56205ms step_avg:39.41ms
step:1427/2330 train_time:56228ms step_avg:39.40ms
step:1428/2330 train_time:56284ms step_avg:39.41ms
step:1429/2330 train_time:56307ms step_avg:39.40ms
step:1430/2330 train_time:56363ms step_avg:39.41ms
step:1431/2330 train_time:56385ms step_avg:39.40ms
step:1432/2330 train_time:56442ms step_avg:39.41ms
step:1433/2330 train_time:56464ms step_avg:39.40ms
step:1434/2330 train_time:56519ms step_avg:39.41ms
step:1435/2330 train_time:56542ms step_avg:39.40ms
step:1436/2330 train_time:56598ms step_avg:39.41ms
step:1437/2330 train_time:56620ms step_avg:39.40ms
step:1438/2330 train_time:56675ms step_avg:39.41ms
step:1439/2330 train_time:56697ms step_avg:39.40ms
step:1440/2330 train_time:56753ms step_avg:39.41ms
step:1441/2330 train_time:56775ms step_avg:39.40ms
step:1442/2330 train_time:56831ms step_avg:39.41ms
step:1443/2330 train_time:56854ms step_avg:39.40ms
step:1444/2330 train_time:56910ms step_avg:39.41ms
step:1445/2330 train_time:56932ms step_avg:39.40ms
step:1446/2330 train_time:56989ms step_avg:39.41ms
step:1447/2330 train_time:57010ms step_avg:39.40ms
step:1448/2330 train_time:57066ms step_avg:39.41ms
step:1449/2330 train_time:57088ms step_avg:39.40ms
step:1450/2330 train_time:57144ms step_avg:39.41ms
step:1451/2330 train_time:57167ms step_avg:39.40ms
step:1452/2330 train_time:57223ms step_avg:39.41ms
step:1453/2330 train_time:57245ms step_avg:39.40ms
step:1454/2330 train_time:57301ms step_avg:39.41ms
step:1455/2330 train_time:57323ms step_avg:39.40ms
step:1456/2330 train_time:57378ms step_avg:39.41ms
step:1457/2330 train_time:57401ms step_avg:39.40ms
step:1458/2330 train_time:57458ms step_avg:39.41ms
step:1459/2330 train_time:57479ms step_avg:39.40ms
step:1460/2330 train_time:57534ms step_avg:39.41ms
step:1461/2330 train_time:57556ms step_avg:39.40ms
step:1462/2330 train_time:57613ms step_avg:39.41ms
step:1463/2330 train_time:57634ms step_avg:39.39ms
step:1464/2330 train_time:57691ms step_avg:39.41ms
step:1465/2330 train_time:57713ms step_avg:39.39ms
step:1466/2330 train_time:57769ms step_avg:39.41ms
step:1467/2330 train_time:57791ms step_avg:39.39ms
step:1468/2330 train_time:57847ms step_avg:39.41ms
step:1469/2330 train_time:57869ms step_avg:39.39ms
step:1470/2330 train_time:57925ms step_avg:39.40ms
step:1471/2330 train_time:57948ms step_avg:39.39ms
step:1472/2330 train_time:58004ms step_avg:39.40ms
step:1473/2330 train_time:58027ms step_avg:39.39ms
step:1474/2330 train_time:58083ms step_avg:39.40ms
step:1475/2330 train_time:58105ms step_avg:39.39ms
step:1476/2330 train_time:58161ms step_avg:39.40ms
step:1477/2330 train_time:58183ms step_avg:39.39ms
step:1478/2330 train_time:58239ms step_avg:39.40ms
step:1479/2330 train_time:58261ms step_avg:39.39ms
step:1480/2330 train_time:58317ms step_avg:39.40ms
step:1481/2330 train_time:58340ms step_avg:39.39ms
step:1482/2330 train_time:58396ms step_avg:39.40ms
step:1483/2330 train_time:58418ms step_avg:39.39ms
step:1484/2330 train_time:58474ms step_avg:39.40ms
step:1485/2330 train_time:58496ms step_avg:39.39ms
step:1486/2330 train_time:58552ms step_avg:39.40ms
step:1487/2330 train_time:58574ms step_avg:39.39ms
step:1488/2330 train_time:58630ms step_avg:39.40ms
step:1489/2330 train_time:58652ms step_avg:39.39ms
step:1490/2330 train_time:58709ms step_avg:39.40ms
step:1491/2330 train_time:58731ms step_avg:39.39ms
step:1492/2330 train_time:58787ms step_avg:39.40ms
step:1493/2330 train_time:58809ms step_avg:39.39ms
step:1494/2330 train_time:58865ms step_avg:39.40ms
step:1495/2330 train_time:58888ms step_avg:39.39ms
step:1496/2330 train_time:58944ms step_avg:39.40ms
step:1497/2330 train_time:58967ms step_avg:39.39ms
step:1498/2330 train_time:59023ms step_avg:39.40ms
step:1499/2330 train_time:59045ms step_avg:39.39ms
step:1500/2330 train_time:59101ms step_avg:39.40ms
step:1500/2330 val_loss:5.1562 train_time:59197ms step_avg:39.46ms
step:1501/2330 train_time:59209ms step_avg:39.45ms
step:1502/2330 train_time:59220ms step_avg:39.43ms
step:1503/2330 train_time:59231ms step_avg:39.41ms
step:1504/2330 train_time:59261ms step_avg:39.40ms
step:1505/2330 train_time:59283ms step_avg:39.39ms
step:1506/2330 train_time:59338ms step_avg:39.40ms
step:1507/2330 train_time:59360ms step_avg:39.39ms
step:1508/2330 train_time:59415ms step_avg:39.40ms
step:1509/2330 train_time:59436ms step_avg:39.39ms
step:1510/2330 train_time:59492ms step_avg:39.40ms
step:1511/2330 train_time:59515ms step_avg:39.39ms
step:1512/2330 train_time:59575ms step_avg:39.40ms
step:1513/2330 train_time:59599ms step_avg:39.39ms
step:1514/2330 train_time:59655ms step_avg:39.40ms
step:1515/2330 train_time:59679ms step_avg:39.39ms
step:1516/2330 train_time:59734ms step_avg:39.40ms
step:1517/2330 train_time:59756ms step_avg:39.39ms
step:1518/2330 train_time:59812ms step_avg:39.40ms
step:1519/2330 train_time:59834ms step_avg:39.39ms
step:1520/2330 train_time:59889ms step_avg:39.40ms
step:1521/2330 train_time:59910ms step_avg:39.39ms
step:1522/2330 train_time:59965ms step_avg:39.40ms
step:1523/2330 train_time:59987ms step_avg:39.39ms
step:1524/2330 train_time:60042ms step_avg:39.40ms
step:1525/2330 train_time:60064ms step_avg:39.39ms
step:1526/2330 train_time:60120ms step_avg:39.40ms
step:1527/2330 train_time:60143ms step_avg:39.39ms
step:1528/2330 train_time:60199ms step_avg:39.40ms
step:1529/2330 train_time:60223ms step_avg:39.39ms
step:1530/2330 train_time:60279ms step_avg:39.40ms
step:1531/2330 train_time:60300ms step_avg:39.39ms
step:1532/2330 train_time:60355ms step_avg:39.40ms
step:1533/2330 train_time:60377ms step_avg:39.39ms
step:1534/2330 train_time:60433ms step_avg:39.40ms
step:1535/2330 train_time:60456ms step_avg:39.39ms
step:1536/2330 train_time:60514ms step_avg:39.40ms
step:1537/2330 train_time:60537ms step_avg:39.39ms
step:1538/2330 train_time:60593ms step_avg:39.40ms
step:1539/2330 train_time:60615ms step_avg:39.39ms
step:1540/2330 train_time:60671ms step_avg:39.40ms
step:1541/2330 train_time:60693ms step_avg:39.39ms
step:1542/2330 train_time:60749ms step_avg:39.40ms
step:1543/2330 train_time:60771ms step_avg:39.38ms
step:1544/2330 train_time:60827ms step_avg:39.40ms
step:1545/2330 train_time:60848ms step_avg:39.38ms
step:1546/2330 train_time:60904ms step_avg:39.39ms
step:1547/2330 train_time:60926ms step_avg:39.38ms
step:1548/2330 train_time:60981ms step_avg:39.39ms
step:1549/2330 train_time:61003ms step_avg:39.38ms
step:1550/2330 train_time:61059ms step_avg:39.39ms
step:1551/2330 train_time:61082ms step_avg:39.38ms
step:1552/2330 train_time:61137ms step_avg:39.39ms
step:1553/2330 train_time:61160ms step_avg:39.38ms
step:1554/2330 train_time:61216ms step_avg:39.39ms
step:1555/2330 train_time:61238ms step_avg:39.38ms
step:1556/2330 train_time:61294ms step_avg:39.39ms
step:1557/2330 train_time:61316ms step_avg:39.38ms
step:1558/2330 train_time:61372ms step_avg:39.39ms
step:1559/2330 train_time:61395ms step_avg:39.38ms
step:1560/2330 train_time:61451ms step_avg:39.39ms
step:1561/2330 train_time:61473ms step_avg:39.38ms
step:1562/2330 train_time:61529ms step_avg:39.39ms
step:1563/2330 train_time:61551ms step_avg:39.38ms
step:1564/2330 train_time:61607ms step_avg:39.39ms
step:1565/2330 train_time:61629ms step_avg:39.38ms
step:1566/2330 train_time:61686ms step_avg:39.39ms
step:1567/2330 train_time:61709ms step_avg:39.38ms
step:1568/2330 train_time:61765ms step_avg:39.39ms
step:1569/2330 train_time:61787ms step_avg:39.38ms
step:1570/2330 train_time:61843ms step_avg:39.39ms
step:1571/2330 train_time:61865ms step_avg:39.38ms
step:1572/2330 train_time:61921ms step_avg:39.39ms
step:1573/2330 train_time:61943ms step_avg:39.38ms
step:1574/2330 train_time:61998ms step_avg:39.39ms
step:1575/2330 train_time:62020ms step_avg:39.38ms
step:1576/2330 train_time:62076ms step_avg:39.39ms
step:1577/2330 train_time:62099ms step_avg:39.38ms
step:1578/2330 train_time:62155ms step_avg:39.39ms
step:1579/2330 train_time:62177ms step_avg:39.38ms
step:1580/2330 train_time:62233ms step_avg:39.39ms
step:1581/2330 train_time:62255ms step_avg:39.38ms
step:1582/2330 train_time:62311ms step_avg:39.39ms
step:1583/2330 train_time:62333ms step_avg:39.38ms
step:1584/2330 train_time:62389ms step_avg:39.39ms
step:1585/2330 train_time:62416ms step_avg:39.38ms
step:1586/2330 train_time:62466ms step_avg:39.39ms
step:1587/2330 train_time:62488ms step_avg:39.37ms
step:1588/2330 train_time:62545ms step_avg:39.39ms
step:1589/2330 train_time:62567ms step_avg:39.38ms
step:1590/2330 train_time:62624ms step_avg:39.39ms
step:1591/2330 train_time:62646ms step_avg:39.38ms
step:1592/2330 train_time:62702ms step_avg:39.39ms
step:1593/2330 train_time:62725ms step_avg:39.38ms
step:1594/2330 train_time:62780ms step_avg:39.39ms
step:1595/2330 train_time:62804ms step_avg:39.38ms
step:1596/2330 train_time:62860ms step_avg:39.39ms
step:1597/2330 train_time:62883ms step_avg:39.38ms
step:1598/2330 train_time:62938ms step_avg:39.39ms
step:1599/2330 train_time:62961ms step_avg:39.38ms
step:1600/2330 train_time:63017ms step_avg:39.39ms
step:1601/2330 train_time:63039ms step_avg:39.38ms
step:1602/2330 train_time:63095ms step_avg:39.38ms
step:1603/2330 train_time:63117ms step_avg:39.37ms
step:1604/2330 train_time:63173ms step_avg:39.38ms
step:1605/2330 train_time:63196ms step_avg:39.37ms
step:1606/2330 train_time:63251ms step_avg:39.38ms
step:1607/2330 train_time:63273ms step_avg:39.37ms
step:1608/2330 train_time:63329ms step_avg:39.38ms
step:1609/2330 train_time:63351ms step_avg:39.37ms
step:1610/2330 train_time:63407ms step_avg:39.38ms
step:1611/2330 train_time:63429ms step_avg:39.37ms
step:1612/2330 train_time:63485ms step_avg:39.38ms
step:1613/2330 train_time:63507ms step_avg:39.37ms
step:1614/2330 train_time:63564ms step_avg:39.38ms
step:1615/2330 train_time:63586ms step_avg:39.37ms
step:1616/2330 train_time:63642ms step_avg:39.38ms
step:1617/2330 train_time:63664ms step_avg:39.37ms
step:1618/2330 train_time:63720ms step_avg:39.38ms
step:1619/2330 train_time:63743ms step_avg:39.37ms
step:1620/2330 train_time:63799ms step_avg:39.38ms
step:1621/2330 train_time:63822ms step_avg:39.37ms
step:1622/2330 train_time:63878ms step_avg:39.38ms
step:1623/2330 train_time:63900ms step_avg:39.37ms
step:1624/2330 train_time:63956ms step_avg:39.38ms
step:1625/2330 train_time:63979ms step_avg:39.37ms
step:1626/2330 train_time:64035ms step_avg:39.38ms
step:1627/2330 train_time:64057ms step_avg:39.37ms
step:1628/2330 train_time:64113ms step_avg:39.38ms
step:1629/2330 train_time:64135ms step_avg:39.37ms
step:1630/2330 train_time:64191ms step_avg:39.38ms
step:1631/2330 train_time:64212ms step_avg:39.37ms
step:1632/2330 train_time:64268ms step_avg:39.38ms
step:1633/2330 train_time:64290ms step_avg:39.37ms
step:1634/2330 train_time:64346ms step_avg:39.38ms
step:1635/2330 train_time:64368ms step_avg:39.37ms
step:1636/2330 train_time:64424ms step_avg:39.38ms
step:1637/2330 train_time:64447ms step_avg:39.37ms
step:1638/2330 train_time:64502ms step_avg:39.38ms
step:1639/2330 train_time:64524ms step_avg:39.37ms
step:1640/2330 train_time:64580ms step_avg:39.38ms
step:1641/2330 train_time:64603ms step_avg:39.37ms
step:1642/2330 train_time:64659ms step_avg:39.38ms
step:1643/2330 train_time:64682ms step_avg:39.37ms
step:1644/2330 train_time:64738ms step_avg:39.38ms
step:1645/2330 train_time:64761ms step_avg:39.37ms
step:1646/2330 train_time:64817ms step_avg:39.38ms
step:1647/2330 train_time:64839ms step_avg:39.37ms
step:1648/2330 train_time:64895ms step_avg:39.38ms
step:1649/2330 train_time:64917ms step_avg:39.37ms
step:1650/2330 train_time:64973ms step_avg:39.38ms
step:1651/2330 train_time:64995ms step_avg:39.37ms
step:1652/2330 train_time:65051ms step_avg:39.38ms
step:1653/2330 train_time:65073ms step_avg:39.37ms
step:1654/2330 train_time:65129ms step_avg:39.38ms
step:1655/2330 train_time:65151ms step_avg:39.37ms
step:1656/2330 train_time:65207ms step_avg:39.38ms
step:1657/2330 train_time:65229ms step_avg:39.37ms
step:1658/2330 train_time:65285ms step_avg:39.38ms
step:1659/2330 train_time:65307ms step_avg:39.37ms
step:1660/2330 train_time:65362ms step_avg:39.37ms
step:1661/2330 train_time:65384ms step_avg:39.36ms
step:1662/2330 train_time:65440ms step_avg:39.37ms
step:1663/2330 train_time:65463ms step_avg:39.36ms
step:1664/2330 train_time:65520ms step_avg:39.37ms
step:1665/2330 train_time:65542ms step_avg:39.36ms
step:1666/2330 train_time:65598ms step_avg:39.37ms
step:1667/2330 train_time:65620ms step_avg:39.36ms
step:1668/2330 train_time:65676ms step_avg:39.37ms
step:1669/2330 train_time:65699ms step_avg:39.36ms
step:1670/2330 train_time:65755ms step_avg:39.37ms
step:1671/2330 train_time:65777ms step_avg:39.36ms
step:1672/2330 train_time:65833ms step_avg:39.37ms
step:1673/2330 train_time:65855ms step_avg:39.36ms
step:1674/2330 train_time:65911ms step_avg:39.37ms
step:1675/2330 train_time:65933ms step_avg:39.36ms
step:1676/2330 train_time:65989ms step_avg:39.37ms
step:1677/2330 train_time:66011ms step_avg:39.36ms
step:1678/2330 train_time:66066ms step_avg:39.37ms
step:1679/2330 train_time:66088ms step_avg:39.36ms
step:1680/2330 train_time:66145ms step_avg:39.37ms
step:1681/2330 train_time:66167ms step_avg:39.36ms
step:1682/2330 train_time:66224ms step_avg:39.37ms
step:1683/2330 train_time:66246ms step_avg:39.36ms
step:1684/2330 train_time:66301ms step_avg:39.37ms
step:1685/2330 train_time:66324ms step_avg:39.36ms
step:1686/2330 train_time:66379ms step_avg:39.37ms
step:1687/2330 train_time:66402ms step_avg:39.36ms
step:1688/2330 train_time:66459ms step_avg:39.37ms
step:1689/2330 train_time:66481ms step_avg:39.36ms
step:1690/2330 train_time:66538ms step_avg:39.37ms
step:1691/2330 train_time:66561ms step_avg:39.36ms
step:1692/2330 train_time:66617ms step_avg:39.37ms
step:1693/2330 train_time:66640ms step_avg:39.36ms
step:1694/2330 train_time:66696ms step_avg:39.37ms
step:1695/2330 train_time:66718ms step_avg:39.36ms
step:1696/2330 train_time:66774ms step_avg:39.37ms
step:1697/2330 train_time:66797ms step_avg:39.36ms
step:1698/2330 train_time:66853ms step_avg:39.37ms
step:1699/2330 train_time:66875ms step_avg:39.36ms
step:1700/2330 train_time:66931ms step_avg:39.37ms
step:1701/2330 train_time:66953ms step_avg:39.36ms
step:1702/2330 train_time:67009ms step_avg:39.37ms
step:1703/2330 train_time:67030ms step_avg:39.36ms
step:1704/2330 train_time:67087ms step_avg:39.37ms
step:1705/2330 train_time:67109ms step_avg:39.36ms
step:1706/2330 train_time:67165ms step_avg:39.37ms
step:1707/2330 train_time:67187ms step_avg:39.36ms
step:1708/2330 train_time:67243ms step_avg:39.37ms
step:1709/2330 train_time:67265ms step_avg:39.36ms
step:1710/2330 train_time:67321ms step_avg:39.37ms
step:1711/2330 train_time:67343ms step_avg:39.36ms
step:1712/2330 train_time:67399ms step_avg:39.37ms
step:1713/2330 train_time:67422ms step_avg:39.36ms
step:1714/2330 train_time:67478ms step_avg:39.37ms
step:1715/2330 train_time:67500ms step_avg:39.36ms
step:1716/2330 train_time:67557ms step_avg:39.37ms
step:1717/2330 train_time:67580ms step_avg:39.36ms
step:1718/2330 train_time:67636ms step_avg:39.37ms
step:1719/2330 train_time:67658ms step_avg:39.36ms
step:1720/2330 train_time:67714ms step_avg:39.37ms
step:1721/2330 train_time:67737ms step_avg:39.36ms
step:1722/2330 train_time:67792ms step_avg:39.37ms
step:1723/2330 train_time:67814ms step_avg:39.36ms
step:1724/2330 train_time:67869ms step_avg:39.37ms
step:1725/2330 train_time:67891ms step_avg:39.36ms
step:1726/2330 train_time:67947ms step_avg:39.37ms
step:1727/2330 train_time:67969ms step_avg:39.36ms
step:1728/2330 train_time:68025ms step_avg:39.37ms
step:1729/2330 train_time:68047ms step_avg:39.36ms
step:1730/2330 train_time:68103ms step_avg:39.37ms
step:1731/2330 train_time:68125ms step_avg:39.36ms
step:1732/2330 train_time:68181ms step_avg:39.37ms
step:1733/2330 train_time:68203ms step_avg:39.36ms
step:1734/2330 train_time:68259ms step_avg:39.36ms
step:1735/2330 train_time:68281ms step_avg:39.36ms
step:1736/2330 train_time:68337ms step_avg:39.36ms
step:1737/2330 train_time:68360ms step_avg:39.35ms
step:1738/2330 train_time:68415ms step_avg:39.36ms
step:1739/2330 train_time:68438ms step_avg:39.35ms
step:1740/2330 train_time:68493ms step_avg:39.36ms
step:1741/2330 train_time:68516ms step_avg:39.35ms
step:1742/2330 train_time:68572ms step_avg:39.36ms
step:1743/2330 train_time:68595ms step_avg:39.35ms
step:1744/2330 train_time:68651ms step_avg:39.36ms
step:1745/2330 train_time:68674ms step_avg:39.35ms
step:1746/2330 train_time:68729ms step_avg:39.36ms
step:1747/2330 train_time:68751ms step_avg:39.35ms
step:1748/2330 train_time:68807ms step_avg:39.36ms
step:1749/2330 train_time:68829ms step_avg:39.35ms
step:1750/2330 train_time:68884ms step_avg:39.36ms
step:1750/2330 val_loss:5.1227 train_time:68980ms step_avg:39.42ms
step:1751/2330 train_time:68992ms step_avg:39.40ms
step:1752/2330 train_time:69004ms step_avg:39.39ms
step:1753/2330 train_time:69013ms step_avg:39.37ms
step:1754/2330 train_time:69042ms step_avg:39.36ms
step:1755/2330 train_time:69064ms step_avg:39.35ms
step:1756/2330 train_time:69119ms step_avg:39.36ms
step:1757/2330 train_time:69140ms step_avg:39.35ms
step:1758/2330 train_time:69196ms step_avg:39.36ms
step:1759/2330 train_time:69217ms step_avg:39.35ms
step:1760/2330 train_time:69273ms step_avg:39.36ms
step:1761/2330 train_time:69299ms step_avg:39.35ms
step:1762/2330 train_time:69358ms step_avg:39.36ms
step:1763/2330 train_time:69381ms step_avg:39.35ms
step:1764/2330 train_time:69438ms step_avg:39.36ms
step:1765/2330 train_time:69460ms step_avg:39.35ms
step:1766/2330 train_time:69516ms step_avg:39.36ms
step:1767/2330 train_time:69539ms step_avg:39.35ms
step:1768/2330 train_time:69595ms step_avg:39.36ms
step:1769/2330 train_time:69616ms step_avg:39.35ms
step:1770/2330 train_time:69671ms step_avg:39.36ms
step:1771/2330 train_time:69693ms step_avg:39.35ms
step:1772/2330 train_time:69749ms step_avg:39.36ms
step:1773/2330 train_time:69770ms step_avg:39.35ms
step:1774/2330 train_time:69825ms step_avg:39.36ms
step:1775/2330 train_time:69847ms step_avg:39.35ms
step:1776/2330 train_time:69905ms step_avg:39.36ms
step:1777/2330 train_time:69929ms step_avg:39.35ms
step:1778/2330 train_time:69985ms step_avg:39.36ms
step:1779/2330 train_time:70008ms step_avg:39.35ms
step:1780/2330 train_time:70064ms step_avg:39.36ms
step:1781/2330 train_time:70087ms step_avg:39.35ms
step:1782/2330 train_time:70143ms step_avg:39.36ms
step:1783/2330 train_time:70167ms step_avg:39.35ms
step:1784/2330 train_time:70223ms step_avg:39.36ms
step:1785/2330 train_time:70246ms step_avg:39.35ms
step:1786/2330 train_time:70305ms step_avg:39.36ms
step:1787/2330 train_time:70329ms step_avg:39.36ms
step:1788/2330 train_time:70386ms step_avg:39.37ms
step:1789/2330 train_time:70410ms step_avg:39.36ms
step:1790/2330 train_time:70466ms step_avg:39.37ms
step:1791/2330 train_time:70490ms step_avg:39.36ms
step:1792/2330 train_time:70546ms step_avg:39.37ms
step:1793/2330 train_time:70569ms step_avg:39.36ms
step:1794/2330 train_time:70625ms step_avg:39.37ms
step:1795/2330 train_time:70647ms step_avg:39.36ms
step:1796/2330 train_time:70703ms step_avg:39.37ms
step:1797/2330 train_time:70725ms step_avg:39.36ms
step:1798/2330 train_time:70780ms step_avg:39.37ms
step:1799/2330 train_time:70803ms step_avg:39.36ms
step:1800/2330 train_time:70858ms step_avg:39.37ms
step:1801/2330 train_time:70880ms step_avg:39.36ms
step:1802/2330 train_time:70936ms step_avg:39.37ms
step:1803/2330 train_time:70958ms step_avg:39.36ms
step:1804/2330 train_time:71014ms step_avg:39.36ms
step:1805/2330 train_time:71036ms step_avg:39.36ms
step:1806/2330 train_time:71093ms step_avg:39.36ms
step:1807/2330 train_time:71115ms step_avg:39.36ms
step:1808/2330 train_time:71172ms step_avg:39.36ms
step:1809/2330 train_time:71194ms step_avg:39.36ms
step:1810/2330 train_time:71251ms step_avg:39.37ms
step:1811/2330 train_time:71273ms step_avg:39.36ms
step:1812/2330 train_time:71330ms step_avg:39.37ms
step:1813/2330 train_time:71352ms step_avg:39.36ms
step:1814/2330 train_time:71409ms step_avg:39.37ms
step:1815/2330 train_time:71432ms step_avg:39.36ms
step:1816/2330 train_time:71488ms step_avg:39.37ms
step:1817/2330 train_time:71511ms step_avg:39.36ms
step:1818/2330 train_time:71567ms step_avg:39.37ms
step:1819/2330 train_time:71590ms step_avg:39.36ms
step:1820/2330 train_time:71645ms step_avg:39.37ms
step:1821/2330 train_time:71668ms step_avg:39.36ms
step:1822/2330 train_time:71724ms step_avg:39.37ms
step:1823/2330 train_time:71747ms step_avg:39.36ms
step:1824/2330 train_time:71804ms step_avg:39.37ms
step:1825/2330 train_time:71827ms step_avg:39.36ms
step:1826/2330 train_time:71883ms step_avg:39.37ms
step:1827/2330 train_time:71906ms step_avg:39.36ms
step:1828/2330 train_time:71961ms step_avg:39.37ms
step:1829/2330 train_time:71984ms step_avg:39.36ms
step:1830/2330 train_time:72041ms step_avg:39.37ms
step:1831/2330 train_time:72064ms step_avg:39.36ms
step:1832/2330 train_time:72120ms step_avg:39.37ms
step:1833/2330 train_time:72142ms step_avg:39.36ms
step:1834/2330 train_time:72198ms step_avg:39.37ms
step:1835/2330 train_time:72221ms step_avg:39.36ms
step:1836/2330 train_time:72278ms step_avg:39.37ms
step:1837/2330 train_time:72300ms step_avg:39.36ms
step:1838/2330 train_time:72356ms step_avg:39.37ms
step:1839/2330 train_time:72378ms step_avg:39.36ms
step:1840/2330 train_time:72434ms step_avg:39.37ms
step:1841/2330 train_time:72456ms step_avg:39.36ms
step:1842/2330 train_time:72513ms step_avg:39.37ms
step:1843/2330 train_time:72535ms step_avg:39.36ms
step:1844/2330 train_time:72591ms step_avg:39.37ms
step:1845/2330 train_time:72613ms step_avg:39.36ms
step:1846/2330 train_time:72669ms step_avg:39.37ms
step:1847/2330 train_time:72691ms step_avg:39.36ms
step:1848/2330 train_time:72747ms step_avg:39.37ms
step:1849/2330 train_time:72769ms step_avg:39.36ms
step:1850/2330 train_time:72826ms step_avg:39.37ms
step:1851/2330 train_time:72848ms step_avg:39.36ms
step:1852/2330 train_time:72905ms step_avg:39.37ms
step:1853/2330 train_time:72928ms step_avg:39.36ms
step:1854/2330 train_time:72984ms step_avg:39.37ms
step:1855/2330 train_time:73008ms step_avg:39.36ms
step:1856/2330 train_time:73064ms step_avg:39.37ms
step:1857/2330 train_time:73087ms step_avg:39.36ms
step:1858/2330 train_time:73143ms step_avg:39.37ms
step:1859/2330 train_time:73167ms step_avg:39.36ms
step:1860/2330 train_time:73223ms step_avg:39.37ms
step:1861/2330 train_time:73246ms step_avg:39.36ms
step:1862/2330 train_time:73302ms step_avg:39.37ms
step:1863/2330 train_time:73324ms step_avg:39.36ms
step:1864/2330 train_time:73380ms step_avg:39.37ms
step:1865/2330 train_time:73403ms step_avg:39.36ms
step:1866/2330 train_time:73458ms step_avg:39.37ms
step:1867/2330 train_time:73481ms step_avg:39.36ms
step:1868/2330 train_time:73536ms step_avg:39.37ms
step:1869/2330 train_time:73557ms step_avg:39.36ms
step:1870/2330 train_time:73614ms step_avg:39.37ms
step:1871/2330 train_time:73635ms step_avg:39.36ms
step:1872/2330 train_time:73692ms step_avg:39.37ms
step:1873/2330 train_time:73714ms step_avg:39.36ms
step:1874/2330 train_time:73771ms step_avg:39.37ms
step:1875/2330 train_time:73793ms step_avg:39.36ms
step:1876/2330 train_time:73850ms step_avg:39.37ms
step:1877/2330 train_time:73872ms step_avg:39.36ms
step:1878/2330 train_time:73928ms step_avg:39.37ms
step:1879/2330 train_time:73951ms step_avg:39.36ms
step:1880/2330 train_time:74007ms step_avg:39.37ms
step:1881/2330 train_time:74030ms step_avg:39.36ms
step:1882/2330 train_time:74086ms step_avg:39.37ms
step:1883/2330 train_time:74109ms step_avg:39.36ms
step:1884/2330 train_time:74165ms step_avg:39.37ms
step:1885/2330 train_time:74188ms step_avg:39.36ms
step:1886/2330 train_time:74245ms step_avg:39.37ms
step:1887/2330 train_time:74267ms step_avg:39.36ms
step:1888/2330 train_time:74325ms step_avg:39.37ms
step:1889/2330 train_time:74348ms step_avg:39.36ms
step:1890/2330 train_time:74405ms step_avg:39.37ms
step:1891/2330 train_time:74428ms step_avg:39.36ms
step:1892/2330 train_time:74485ms step_avg:39.37ms
step:1893/2330 train_time:74508ms step_avg:39.36ms
step:1894/2330 train_time:74564ms step_avg:39.37ms
step:1895/2330 train_time:74587ms step_avg:39.36ms
step:1896/2330 train_time:74643ms step_avg:39.37ms
step:1897/2330 train_time:74666ms step_avg:39.36ms
step:1898/2330 train_time:74722ms step_avg:39.37ms
step:1899/2330 train_time:74745ms step_avg:39.36ms
step:1900/2330 train_time:74801ms step_avg:39.37ms
step:1901/2330 train_time:74823ms step_avg:39.36ms
step:1902/2330 train_time:74879ms step_avg:39.37ms
step:1903/2330 train_time:74902ms step_avg:39.36ms
step:1904/2330 train_time:74958ms step_avg:39.37ms
step:1905/2330 train_time:74979ms step_avg:39.36ms
step:1906/2330 train_time:75035ms step_avg:39.37ms
step:1907/2330 train_time:75056ms step_avg:39.36ms
step:1908/2330 train_time:75112ms step_avg:39.37ms
step:1909/2330 train_time:75134ms step_avg:39.36ms
step:1910/2330 train_time:75191ms step_avg:39.37ms
step:1911/2330 train_time:75213ms step_avg:39.36ms
step:1912/2330 train_time:75269ms step_avg:39.37ms
step:1913/2330 train_time:75291ms step_avg:39.36ms
step:1914/2330 train_time:75347ms step_avg:39.37ms
step:1915/2330 train_time:75370ms step_avg:39.36ms
step:1916/2330 train_time:75427ms step_avg:39.37ms
step:1917/2330 train_time:75449ms step_avg:39.36ms
step:1918/2330 train_time:75505ms step_avg:39.37ms
step:1919/2330 train_time:75529ms step_avg:39.36ms
step:1920/2330 train_time:75585ms step_avg:39.37ms
step:1921/2330 train_time:75608ms step_avg:39.36ms
step:1922/2330 train_time:75663ms step_avg:39.37ms
step:1923/2330 train_time:75687ms step_avg:39.36ms
step:1924/2330 train_time:75744ms step_avg:39.37ms
step:1925/2330 train_time:75767ms step_avg:39.36ms
step:1926/2330 train_time:75823ms step_avg:39.37ms
step:1927/2330 train_time:75846ms step_avg:39.36ms
step:1928/2330 train_time:75903ms step_avg:39.37ms
step:1929/2330 train_time:75925ms step_avg:39.36ms
step:1930/2330 train_time:75981ms step_avg:39.37ms
step:1931/2330 train_time:76004ms step_avg:39.36ms
step:1932/2330 train_time:76059ms step_avg:39.37ms
step:1933/2330 train_time:76083ms step_avg:39.36ms
step:1934/2330 train_time:76139ms step_avg:39.37ms
step:1935/2330 train_time:76161ms step_avg:39.36ms
step:1936/2330 train_time:76217ms step_avg:39.37ms
step:1937/2330 train_time:76239ms step_avg:39.36ms
step:1938/2330 train_time:76295ms step_avg:39.37ms
step:1939/2330 train_time:76317ms step_avg:39.36ms
step:1940/2330 train_time:76374ms step_avg:39.37ms
step:1941/2330 train_time:76396ms step_avg:39.36ms
step:1942/2330 train_time:76452ms step_avg:39.37ms
step:1943/2330 train_time:76475ms step_avg:39.36ms
step:1944/2330 train_time:76531ms step_avg:39.37ms
step:1945/2330 train_time:76554ms step_avg:39.36ms
step:1946/2330 train_time:76612ms step_avg:39.37ms
step:1947/2330 train_time:76636ms step_avg:39.36ms
step:1948/2330 train_time:76692ms step_avg:39.37ms
step:1949/2330 train_time:76715ms step_avg:39.36ms
step:1950/2330 train_time:76771ms step_avg:39.37ms
step:1951/2330 train_time:76793ms step_avg:39.36ms
step:1952/2330 train_time:76850ms step_avg:39.37ms
step:1953/2330 train_time:76873ms step_avg:39.36ms
step:1954/2330 train_time:76929ms step_avg:39.37ms
step:1955/2330 train_time:76951ms step_avg:39.36ms
step:1956/2330 train_time:77007ms step_avg:39.37ms
step:1957/2330 train_time:77030ms step_avg:39.36ms
step:1958/2330 train_time:77086ms step_avg:39.37ms
step:1959/2330 train_time:77110ms step_avg:39.36ms
step:1960/2330 train_time:77166ms step_avg:39.37ms
step:1961/2330 train_time:77189ms step_avg:39.36ms
step:1962/2330 train_time:77245ms step_avg:39.37ms
step:1963/2330 train_time:77268ms step_avg:39.36ms
step:1964/2330 train_time:77324ms step_avg:39.37ms
step:1965/2330 train_time:77347ms step_avg:39.36ms
step:1966/2330 train_time:77402ms step_avg:39.37ms
step:1967/2330 train_time:77426ms step_avg:39.36ms
step:1968/2330 train_time:77482ms step_avg:39.37ms
step:1969/2330 train_time:77505ms step_avg:39.36ms
step:1970/2330 train_time:77561ms step_avg:39.37ms
step:1971/2330 train_time:77584ms step_avg:39.36ms
step:1972/2330 train_time:77639ms step_avg:39.37ms
step:1973/2330 train_time:77662ms step_avg:39.36ms
step:1974/2330 train_time:77717ms step_avg:39.37ms
step:1975/2330 train_time:77739ms step_avg:39.36ms
step:1976/2330 train_time:77795ms step_avg:39.37ms
step:1977/2330 train_time:77816ms step_avg:39.36ms
step:1978/2330 train_time:77873ms step_avg:39.37ms
step:1979/2330 train_time:77895ms step_avg:39.36ms
step:1980/2330 train_time:77951ms step_avg:39.37ms
step:1981/2330 train_time:77973ms step_avg:39.36ms
step:1982/2330 train_time:78030ms step_avg:39.37ms
step:1983/2330 train_time:78052ms step_avg:39.36ms
step:1984/2330 train_time:78108ms step_avg:39.37ms
step:1985/2330 train_time:78131ms step_avg:39.36ms
step:1986/2330 train_time:78186ms step_avg:39.37ms
step:1987/2330 train_time:78209ms step_avg:39.36ms
step:1988/2330 train_time:78265ms step_avg:39.37ms
step:1989/2330 train_time:78287ms step_avg:39.36ms
step:1990/2330 train_time:78343ms step_avg:39.37ms
step:1991/2330 train_time:78366ms step_avg:39.36ms
step:1992/2330 train_time:78423ms step_avg:39.37ms
step:1993/2330 train_time:78446ms step_avg:39.36ms
step:1994/2330 train_time:78503ms step_avg:39.37ms
step:1995/2330 train_time:78526ms step_avg:39.36ms
step:1996/2330 train_time:78582ms step_avg:39.37ms
step:1997/2330 train_time:78605ms step_avg:39.36ms
step:1998/2330 train_time:78662ms step_avg:39.37ms
step:1999/2330 train_time:78684ms step_avg:39.36ms
step:2000/2330 train_time:78740ms step_avg:39.37ms
step:2000/2330 val_loss:5.0927 train_time:78834ms step_avg:39.42ms
step:2001/2330 train_time:78846ms step_avg:39.40ms
step:2002/2330 train_time:78858ms step_avg:39.39ms
step:2003/2330 train_time:78868ms step_avg:39.37ms
step:2004/2330 train_time:78897ms step_avg:39.37ms
step:2005/2330 train_time:78918ms step_avg:39.36ms
step:2006/2330 train_time:78973ms step_avg:39.37ms
step:2007/2330 train_time:78995ms step_avg:39.36ms
step:2008/2330 train_time:79050ms step_avg:39.37ms
step:2009/2330 train_time:79072ms step_avg:39.36ms
step:2010/2330 train_time:79128ms step_avg:39.37ms
step:2011/2330 train_time:79153ms step_avg:39.36ms
step:2012/2330 train_time:79213ms step_avg:39.37ms
step:2013/2330 train_time:79238ms step_avg:39.36ms
step:2014/2330 train_time:79295ms step_avg:39.37ms
step:2015/2330 train_time:79318ms step_avg:39.36ms
step:2016/2330 train_time:79375ms step_avg:39.37ms
step:2017/2330 train_time:79397ms step_avg:39.36ms
step:2018/2330 train_time:79453ms step_avg:39.37ms
step:2019/2330 train_time:79474ms step_avg:39.36ms
step:2020/2330 train_time:79530ms step_avg:39.37ms
step:2021/2330 train_time:79552ms step_avg:39.36ms
step:2022/2330 train_time:79607ms step_avg:39.37ms
step:2023/2330 train_time:79630ms step_avg:39.36ms
step:2024/2330 train_time:79685ms step_avg:39.37ms
step:2025/2330 train_time:79707ms step_avg:39.36ms
step:2026/2330 train_time:79764ms step_avg:39.37ms
step:2027/2330 train_time:79786ms step_avg:39.36ms
step:2028/2330 train_time:79842ms step_avg:39.37ms
step:2029/2330 train_time:79865ms step_avg:39.36ms
step:2030/2330 train_time:79921ms step_avg:39.37ms
step:2031/2330 train_time:79943ms step_avg:39.36ms
step:2032/2330 train_time:79998ms step_avg:39.37ms
step:2033/2330 train_time:80020ms step_avg:39.36ms
step:2034/2330 train_time:80076ms step_avg:39.37ms
step:2035/2330 train_time:80099ms step_avg:39.36ms
step:2036/2330 train_time:80156ms step_avg:39.37ms
step:2037/2330 train_time:80179ms step_avg:39.36ms
step:2038/2330 train_time:80235ms step_avg:39.37ms
step:2039/2330 train_time:80258ms step_avg:39.36ms
step:2040/2330 train_time:80315ms step_avg:39.37ms
step:2041/2330 train_time:80338ms step_avg:39.36ms
step:2042/2330 train_time:80394ms step_avg:39.37ms
step:2043/2330 train_time:80416ms step_avg:39.36ms
step:2044/2330 train_time:80473ms step_avg:39.37ms
step:2045/2330 train_time:80495ms step_avg:39.36ms
step:2046/2330 train_time:80550ms step_avg:39.37ms
step:2047/2330 train_time:80573ms step_avg:39.36ms
step:2048/2330 train_time:80628ms step_avg:39.37ms
step:2049/2330 train_time:80651ms step_avg:39.36ms
step:2050/2330 train_time:80707ms step_avg:39.37ms
step:2051/2330 train_time:80731ms step_avg:39.36ms
step:2052/2330 train_time:80787ms step_avg:39.37ms
step:2053/2330 train_time:80809ms step_avg:39.36ms
step:2054/2330 train_time:80865ms step_avg:39.37ms
step:2055/2330 train_time:80889ms step_avg:39.36ms
step:2056/2330 train_time:80945ms step_avg:39.37ms
step:2057/2330 train_time:80968ms step_avg:39.36ms
step:2058/2330 train_time:81024ms step_avg:39.37ms
step:2059/2330 train_time:81048ms step_avg:39.36ms
step:2060/2330 train_time:81105ms step_avg:39.37ms
step:2061/2330 train_time:81129ms step_avg:39.36ms
step:2062/2330 train_time:81185ms step_avg:39.37ms
step:2063/2330 train_time:81208ms step_avg:39.36ms
step:2064/2330 train_time:81265ms step_avg:39.37ms
step:2065/2330 train_time:81288ms step_avg:39.36ms
step:2066/2330 train_time:81345ms step_avg:39.37ms
step:2067/2330 train_time:81367ms step_avg:39.36ms
step:2068/2330 train_time:81423ms step_avg:39.37ms
step:2069/2330 train_time:81446ms step_avg:39.36ms
step:2070/2330 train_time:81501ms step_avg:39.37ms
step:2071/2330 train_time:81523ms step_avg:39.36ms
step:2072/2330 train_time:81579ms step_avg:39.37ms
step:2073/2330 train_time:81600ms step_avg:39.36ms
step:2074/2330 train_time:81657ms step_avg:39.37ms
step:2075/2330 train_time:81679ms step_avg:39.36ms
step:2076/2330 train_time:81735ms step_avg:39.37ms
step:2077/2330 train_time:81757ms step_avg:39.36ms
step:2078/2330 train_time:81813ms step_avg:39.37ms
step:2079/2330 train_time:81835ms step_avg:39.36ms
step:2080/2330 train_time:81891ms step_avg:39.37ms
step:2081/2330 train_time:81913ms step_avg:39.36ms
step:2082/2330 train_time:81969ms step_avg:39.37ms
step:2083/2330 train_time:81992ms step_avg:39.36ms
step:2084/2330 train_time:82048ms step_avg:39.37ms
step:2085/2330 train_time:82071ms step_avg:39.36ms
step:2086/2330 train_time:82128ms step_avg:39.37ms
step:2087/2330 train_time:82151ms step_avg:39.36ms
step:2088/2330 train_time:82208ms step_avg:39.37ms
step:2089/2330 train_time:82232ms step_avg:39.36ms
step:2090/2330 train_time:82288ms step_avg:39.37ms
step:2091/2330 train_time:82311ms step_avg:39.36ms
step:2092/2330 train_time:82368ms step_avg:39.37ms
step:2093/2330 train_time:82391ms step_avg:39.37ms
step:2094/2330 train_time:82447ms step_avg:39.37ms
step:2095/2330 train_time:82470ms step_avg:39.37ms
step:2096/2330 train_time:82526ms step_avg:39.37ms
step:2097/2330 train_time:82548ms step_avg:39.36ms
step:2098/2330 train_time:82604ms step_avg:39.37ms
step:2099/2330 train_time:82627ms step_avg:39.36ms
step:2100/2330 train_time:82684ms step_avg:39.37ms
step:2101/2330 train_time:82706ms step_avg:39.37ms
step:2102/2330 train_time:82762ms step_avg:39.37ms
step:2103/2330 train_time:82785ms step_avg:39.37ms
step:2104/2330 train_time:82841ms step_avg:39.37ms
step:2105/2330 train_time:82862ms step_avg:39.36ms
step:2106/2330 train_time:82917ms step_avg:39.37ms
step:2107/2330 train_time:82939ms step_avg:39.36ms
step:2108/2330 train_time:82995ms step_avg:39.37ms
step:2109/2330 train_time:83017ms step_avg:39.36ms
step:2110/2330 train_time:83075ms step_avg:39.37ms
step:2111/2330 train_time:83097ms step_avg:39.36ms
step:2112/2330 train_time:83154ms step_avg:39.37ms
step:2113/2330 train_time:83176ms step_avg:39.36ms
step:2114/2330 train_time:83232ms step_avg:39.37ms
step:2115/2330 train_time:83255ms step_avg:39.36ms
step:2116/2330 train_time:83311ms step_avg:39.37ms
step:2117/2330 train_time:83334ms step_avg:39.36ms
step:2118/2330 train_time:83389ms step_avg:39.37ms
step:2119/2330 train_time:83412ms step_avg:39.36ms
step:2120/2330 train_time:83468ms step_avg:39.37ms
step:2121/2330 train_time:83491ms step_avg:39.36ms
step:2122/2330 train_time:83547ms step_avg:39.37ms
step:2123/2330 train_time:83570ms step_avg:39.36ms
step:2124/2330 train_time:83626ms step_avg:39.37ms
step:2125/2330 train_time:83649ms step_avg:39.36ms
step:2126/2330 train_time:83705ms step_avg:39.37ms
step:2127/2330 train_time:83728ms step_avg:39.36ms
step:2128/2330 train_time:83783ms step_avg:39.37ms
step:2129/2330 train_time:83806ms step_avg:39.36ms
step:2130/2330 train_time:83861ms step_avg:39.37ms
step:2131/2330 train_time:83884ms step_avg:39.36ms
step:2132/2330 train_time:83939ms step_avg:39.37ms
step:2133/2330 train_time:83961ms step_avg:39.36ms
step:2134/2330 train_time:84017ms step_avg:39.37ms
step:2135/2330 train_time:84039ms step_avg:39.36ms
step:2136/2330 train_time:84095ms step_avg:39.37ms
step:2137/2330 train_time:84117ms step_avg:39.36ms
step:2138/2330 train_time:84173ms step_avg:39.37ms
step:2139/2330 train_time:84195ms step_avg:39.36ms
step:2140/2330 train_time:84251ms step_avg:39.37ms
step:2141/2330 train_time:84274ms step_avg:39.36ms
step:2142/2330 train_time:84330ms step_avg:39.37ms
step:2143/2330 train_time:84353ms step_avg:39.36ms
step:2144/2330 train_time:84409ms step_avg:39.37ms
step:2145/2330 train_time:84432ms step_avg:39.36ms
step:2146/2330 train_time:84488ms step_avg:39.37ms
step:2147/2330 train_time:84511ms step_avg:39.36ms
step:2148/2330 train_time:84568ms step_avg:39.37ms
step:2149/2330 train_time:84591ms step_avg:39.36ms
step:2150/2330 train_time:84646ms step_avg:39.37ms
step:2151/2330 train_time:84669ms step_avg:39.36ms
step:2152/2330 train_time:84724ms step_avg:39.37ms
step:2153/2330 train_time:84748ms step_avg:39.36ms
step:2154/2330 train_time:84804ms step_avg:39.37ms
step:2155/2330 train_time:84827ms step_avg:39.36ms
step:2156/2330 train_time:84882ms step_avg:39.37ms
step:2157/2330 train_time:84906ms step_avg:39.36ms
step:2158/2330 train_time:84962ms step_avg:39.37ms
step:2159/2330 train_time:84985ms step_avg:39.36ms
step:2160/2330 train_time:85041ms step_avg:39.37ms
step:2161/2330 train_time:85063ms step_avg:39.36ms
step:2162/2330 train_time:85119ms step_avg:39.37ms
step:2163/2330 train_time:85140ms step_avg:39.36ms
step:2164/2330 train_time:85196ms step_avg:39.37ms
step:2165/2330 train_time:85218ms step_avg:39.36ms
step:2166/2330 train_time:85275ms step_avg:39.37ms
step:2167/2330 train_time:85297ms step_avg:39.36ms
step:2168/2330 train_time:85353ms step_avg:39.37ms
step:2169/2330 train_time:85375ms step_avg:39.36ms
step:2170/2330 train_time:85431ms step_avg:39.37ms
step:2171/2330 train_time:85454ms step_avg:39.36ms
step:2172/2330 train_time:85511ms step_avg:39.37ms
step:2173/2330 train_time:85533ms step_avg:39.36ms
step:2174/2330 train_time:85589ms step_avg:39.37ms
step:2175/2330 train_time:85612ms step_avg:39.36ms
step:2176/2330 train_time:85668ms step_avg:39.37ms
step:2177/2330 train_time:85691ms step_avg:39.36ms
step:2178/2330 train_time:85748ms step_avg:39.37ms
step:2179/2330 train_time:85771ms step_avg:39.36ms
step:2180/2330 train_time:85828ms step_avg:39.37ms
step:2181/2330 train_time:85850ms step_avg:39.36ms
step:2182/2330 train_time:85907ms step_avg:39.37ms
step:2183/2330 train_time:85929ms step_avg:39.36ms
step:2184/2330 train_time:85986ms step_avg:39.37ms
step:2185/2330 train_time:86010ms step_avg:39.36ms
step:2186/2330 train_time:86067ms step_avg:39.37ms
step:2187/2330 train_time:86090ms step_avg:39.36ms
step:2188/2330 train_time:86146ms step_avg:39.37ms
step:2189/2330 train_time:86168ms step_avg:39.36ms
step:2190/2330 train_time:86225ms step_avg:39.37ms
step:2191/2330 train_time:86248ms step_avg:39.36ms
step:2192/2330 train_time:86305ms step_avg:39.37ms
step:2193/2330 train_time:86328ms step_avg:39.37ms
step:2194/2330 train_time:86384ms step_avg:39.37ms
step:2195/2330 train_time:86407ms step_avg:39.37ms
step:2196/2330 train_time:86462ms step_avg:39.37ms
step:2197/2330 train_time:86486ms step_avg:39.37ms
step:2198/2330 train_time:86541ms step_avg:39.37ms
step:2199/2330 train_time:86564ms step_avg:39.36ms
step:2200/2330 train_time:86619ms step_avg:39.37ms
step:2201/2330 train_time:86641ms step_avg:39.36ms
step:2202/2330 train_time:86697ms step_avg:39.37ms
step:2203/2330 train_time:86719ms step_avg:39.36ms
step:2204/2330 train_time:86775ms step_avg:39.37ms
step:2205/2330 train_time:86798ms step_avg:39.36ms
step:2206/2330 train_time:86854ms step_avg:39.37ms
step:2207/2330 train_time:86877ms step_avg:39.36ms
step:2208/2330 train_time:86933ms step_avg:39.37ms
step:2209/2330 train_time:86956ms step_avg:39.36ms
step:2210/2330 train_time:87012ms step_avg:39.37ms
step:2211/2330 train_time:87035ms step_avg:39.36ms
step:2212/2330 train_time:87090ms step_avg:39.37ms
step:2213/2330 train_time:87113ms step_avg:39.36ms
step:2214/2330 train_time:87169ms step_avg:39.37ms
step:2215/2330 train_time:87193ms step_avg:39.36ms
step:2216/2330 train_time:87249ms step_avg:39.37ms
step:2217/2330 train_time:87272ms step_avg:39.36ms
step:2218/2330 train_time:87328ms step_avg:39.37ms
step:2219/2330 train_time:87351ms step_avg:39.37ms
step:2220/2330 train_time:87408ms step_avg:39.37ms
step:2221/2330 train_time:87431ms step_avg:39.37ms
step:2222/2330 train_time:87487ms step_avg:39.37ms
step:2223/2330 train_time:87509ms step_avg:39.37ms
step:2224/2330 train_time:87566ms step_avg:39.37ms
step:2225/2330 train_time:87589ms step_avg:39.37ms
step:2226/2330 train_time:87645ms step_avg:39.37ms
step:2227/2330 train_time:87668ms step_avg:39.37ms
step:2228/2330 train_time:87724ms step_avg:39.37ms
step:2229/2330 train_time:87747ms step_avg:39.37ms
step:2230/2330 train_time:87803ms step_avg:39.37ms
step:2231/2330 train_time:87825ms step_avg:39.37ms
step:2232/2330 train_time:87881ms step_avg:39.37ms
step:2233/2330 train_time:87903ms step_avg:39.37ms
step:2234/2330 train_time:87959ms step_avg:39.37ms
step:2235/2330 train_time:87981ms step_avg:39.37ms
step:2236/2330 train_time:88037ms step_avg:39.37ms
step:2237/2330 train_time:88059ms step_avg:39.36ms
step:2238/2330 train_time:88115ms step_avg:39.37ms
step:2239/2330 train_time:88138ms step_avg:39.36ms
step:2240/2330 train_time:88195ms step_avg:39.37ms
step:2241/2330 train_time:88218ms step_avg:39.37ms
step:2242/2330 train_time:88274ms step_avg:39.37ms
step:2243/2330 train_time:88296ms step_avg:39.37ms
step:2244/2330 train_time:88353ms step_avg:39.37ms
step:2245/2330 train_time:88375ms step_avg:39.37ms
step:2246/2330 train_time:88431ms step_avg:39.37ms
step:2247/2330 train_time:88453ms step_avg:39.36ms
step:2248/2330 train_time:88509ms step_avg:39.37ms
step:2249/2330 train_time:88532ms step_avg:39.36ms
step:2250/2330 train_time:88588ms step_avg:39.37ms
step:2250/2330 val_loss:5.0681 train_time:88684ms step_avg:39.41ms
step:2251/2330 train_time:88696ms step_avg:39.40ms
step:2252/2330 train_time:88707ms step_avg:39.39ms
step:2253/2330 train_time:88717ms step_avg:39.38ms
step:2254/2330 train_time:88748ms step_avg:39.37ms
step:2255/2330 train_time:88769ms step_avg:39.37ms
step:2256/2330 train_time:88824ms step_avg:39.37ms
step:2257/2330 train_time:88845ms step_avg:39.36ms
step:2258/2330 train_time:88900ms step_avg:39.37ms
step:2259/2330 train_time:88922ms step_avg:39.36ms
step:2260/2330 train_time:88978ms step_avg:39.37ms
step:2261/2330 train_time:89002ms step_avg:39.36ms
step:2262/2330 train_time:89062ms step_avg:39.37ms
step:2263/2330 train_time:89085ms step_avg:39.37ms
step:2264/2330 train_time:89141ms step_avg:39.37ms
step:2265/2330 train_time:89164ms step_avg:39.37ms
step:2266/2330 train_time:89220ms step_avg:39.37ms
step:2267/2330 train_time:89242ms step_avg:39.37ms
step:2268/2330 train_time:89298ms step_avg:39.37ms
step:2269/2330 train_time:89321ms step_avg:39.37ms
step:2270/2330 train_time:89376ms step_avg:39.37ms
step:2271/2330 train_time:89398ms step_avg:39.36ms
step:2272/2330 train_time:89453ms step_avg:39.37ms
step:2273/2330 train_time:89476ms step_avg:39.36ms
step:2274/2330 train_time:89531ms step_avg:39.37ms
step:2275/2330 train_time:89553ms step_avg:39.36ms
step:2276/2330 train_time:89609ms step_avg:39.37ms
step:2277/2330 train_time:89631ms step_avg:39.36ms
step:2278/2330 train_time:89687ms step_avg:39.37ms
step:2279/2330 train_time:89710ms step_avg:39.36ms
step:2280/2330 train_time:89767ms step_avg:39.37ms
step:2281/2330 train_time:89788ms step_avg:39.36ms
step:2282/2330 train_time:89843ms step_avg:39.37ms
step:2283/2330 train_time:89865ms step_avg:39.36ms
step:2284/2330 train_time:89921ms step_avg:39.37ms
step:2285/2330 train_time:89943ms step_avg:39.36ms
step:2286/2330 train_time:90001ms step_avg:39.37ms
step:2287/2330 train_time:90023ms step_avg:39.36ms
step:2288/2330 train_time:90080ms step_avg:39.37ms
step:2289/2330 train_time:90103ms step_avg:39.36ms
step:2290/2330 train_time:90159ms step_avg:39.37ms
step:2291/2330 train_time:90181ms step_avg:39.36ms
step:2292/2330 train_time:90237ms step_avg:39.37ms
step:2293/2330 train_time:90259ms step_avg:39.36ms
step:2294/2330 train_time:90315ms step_avg:39.37ms
step:2295/2330 train_time:90337ms step_avg:39.36ms
step:2296/2330 train_time:90393ms step_avg:39.37ms
step:2297/2330 train_time:90415ms step_avg:39.36ms
step:2298/2330 train_time:90470ms step_avg:39.37ms
step:2299/2330 train_time:90492ms step_avg:39.36ms
step:2300/2330 train_time:90548ms step_avg:39.37ms
step:2301/2330 train_time:90570ms step_avg:39.36ms
step:2302/2330 train_time:90626ms step_avg:39.37ms
step:2303/2330 train_time:90649ms step_avg:39.36ms
step:2304/2330 train_time:90705ms step_avg:39.37ms
step:2305/2330 train_time:90726ms step_avg:39.36ms
step:2306/2330 train_time:90781ms step_avg:39.37ms
step:2307/2330 train_time:90804ms step_avg:39.36ms
step:2308/2330 train_time:90860ms step_avg:39.37ms
step:2309/2330 train_time:90881ms step_avg:39.36ms
step:2310/2330 train_time:90937ms step_avg:39.37ms
step:2311/2330 train_time:90960ms step_avg:39.36ms
step:2312/2330 train_time:91017ms step_avg:39.37ms
step:2313/2330 train_time:91039ms step_avg:39.36ms
step:2314/2330 train_time:91095ms step_avg:39.37ms
step:2315/2330 train_time:91119ms step_avg:39.36ms
step:2316/2330 train_time:91175ms step_avg:39.37ms
step:2317/2330 train_time:91197ms step_avg:39.36ms
step:2318/2330 train_time:91253ms step_avg:39.37ms
step:2319/2330 train_time:91275ms step_avg:39.36ms
step:2320/2330 train_time:91331ms step_avg:39.37ms
step:2321/2330 train_time:91354ms step_avg:39.36ms
step:2322/2330 train_time:91409ms step_avg:39.37ms
step:2323/2330 train_time:91432ms step_avg:39.36ms
step:2324/2330 train_time:91488ms step_avg:39.37ms
step:2325/2330 train_time:91510ms step_avg:39.36ms
step:2326/2330 train_time:91565ms step_avg:39.37ms
step:2327/2330 train_time:91587ms step_avg:39.36ms
step:2328/2330 train_time:91643ms step_avg:39.37ms
step:2329/2330 train_time:91664ms step_avg:39.36ms
step:2330/2330 train_time:91720ms step_avg:39.36ms
step:2330/2330 val_loss:5.0613 train_time:91815ms step_avg:39.41ms
peak memory allocated: 29226 MiB reserved: 38888 MiB
