import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "adam_lr3e-2"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-1, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.Adam(
    hidden_matrix_params + gate_params,
    lr=3e-2,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 00:39:39 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   33C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   32C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:81ms step_avg:80.74ms
step:2/2330 train_time:180ms step_avg:90.05ms
step:3/2330 train_time:199ms step_avg:66.49ms
step:4/2330 train_time:221ms step_avg:55.29ms
step:5/2330 train_time:276ms step_avg:55.23ms
step:6/2330 train_time:335ms step_avg:55.83ms
step:7/2330 train_time:390ms step_avg:55.68ms
step:8/2330 train_time:449ms step_avg:56.08ms
step:9/2330 train_time:504ms step_avg:55.95ms
step:10/2330 train_time:562ms step_avg:56.23ms
step:11/2330 train_time:617ms step_avg:56.10ms
step:12/2330 train_time:676ms step_avg:56.34ms
step:13/2330 train_time:731ms step_avg:56.23ms
step:14/2330 train_time:790ms step_avg:56.41ms
step:15/2330 train_time:845ms step_avg:56.31ms
step:16/2330 train_time:904ms step_avg:56.49ms
step:17/2330 train_time:959ms step_avg:56.41ms
step:18/2330 train_time:1018ms step_avg:56.56ms
step:19/2330 train_time:1073ms step_avg:56.49ms
step:20/2330 train_time:1132ms step_avg:56.62ms
step:21/2330 train_time:1188ms step_avg:56.56ms
step:22/2330 train_time:1248ms step_avg:56.72ms
step:23/2330 train_time:1303ms step_avg:56.66ms
step:24/2330 train_time:1364ms step_avg:56.84ms
step:25/2330 train_time:1419ms step_avg:56.78ms
step:26/2330 train_time:1478ms step_avg:56.86ms
step:27/2330 train_time:1534ms step_avg:56.82ms
step:28/2330 train_time:1593ms step_avg:56.90ms
step:29/2330 train_time:1649ms step_avg:56.86ms
step:30/2330 train_time:1708ms step_avg:56.94ms
step:31/2330 train_time:1764ms step_avg:56.90ms
step:32/2330 train_time:1823ms step_avg:56.97ms
step:33/2330 train_time:1879ms step_avg:56.95ms
step:34/2330 train_time:1938ms step_avg:57.01ms
step:35/2330 train_time:1994ms step_avg:56.96ms
step:36/2330 train_time:2052ms step_avg:57.01ms
step:37/2330 train_time:2108ms step_avg:56.96ms
step:38/2330 train_time:2167ms step_avg:57.01ms
step:39/2330 train_time:2222ms step_avg:56.98ms
step:40/2330 train_time:2281ms step_avg:57.03ms
step:41/2330 train_time:2337ms step_avg:57.00ms
step:42/2330 train_time:2397ms step_avg:57.08ms
step:43/2330 train_time:2453ms step_avg:57.04ms
step:44/2330 train_time:2513ms step_avg:57.10ms
step:45/2330 train_time:2568ms step_avg:57.06ms
step:46/2330 train_time:2627ms step_avg:57.12ms
step:47/2330 train_time:2683ms step_avg:57.08ms
step:48/2330 train_time:2742ms step_avg:57.12ms
step:49/2330 train_time:2798ms step_avg:57.09ms
step:50/2330 train_time:2857ms step_avg:57.14ms
step:51/2330 train_time:2913ms step_avg:57.11ms
step:52/2330 train_time:2972ms step_avg:57.15ms
step:53/2330 train_time:3028ms step_avg:57.13ms
step:54/2330 train_time:3087ms step_avg:57.17ms
step:55/2330 train_time:3143ms step_avg:57.14ms
step:56/2330 train_time:3202ms step_avg:57.18ms
step:57/2330 train_time:3258ms step_avg:57.16ms
step:58/2330 train_time:3318ms step_avg:57.20ms
step:59/2330 train_time:3373ms step_avg:57.18ms
step:60/2330 train_time:3433ms step_avg:57.21ms
step:61/2330 train_time:3489ms step_avg:57.20ms
step:62/2330 train_time:3548ms step_avg:57.23ms
step:63/2330 train_time:3604ms step_avg:57.21ms
step:64/2330 train_time:3664ms step_avg:57.24ms
step:65/2330 train_time:3719ms step_avg:57.22ms
step:66/2330 train_time:3779ms step_avg:57.25ms
step:67/2330 train_time:3834ms step_avg:57.22ms
step:68/2330 train_time:3895ms step_avg:57.27ms
step:69/2330 train_time:3950ms step_avg:57.25ms
step:70/2330 train_time:4010ms step_avg:57.28ms
step:71/2330 train_time:4066ms step_avg:57.26ms
step:72/2330 train_time:4125ms step_avg:57.29ms
step:73/2330 train_time:4181ms step_avg:57.27ms
step:74/2330 train_time:4240ms step_avg:57.29ms
step:75/2330 train_time:4295ms step_avg:57.27ms
step:76/2330 train_time:4355ms step_avg:57.30ms
step:77/2330 train_time:4411ms step_avg:57.28ms
step:78/2330 train_time:4470ms step_avg:57.31ms
step:79/2330 train_time:4526ms step_avg:57.29ms
step:80/2330 train_time:4585ms step_avg:57.31ms
step:81/2330 train_time:4641ms step_avg:57.29ms
step:82/2330 train_time:4700ms step_avg:57.32ms
step:83/2330 train_time:4755ms step_avg:57.29ms
step:84/2330 train_time:4815ms step_avg:57.32ms
step:85/2330 train_time:4871ms step_avg:57.30ms
step:86/2330 train_time:4930ms step_avg:57.32ms
step:87/2330 train_time:4985ms step_avg:57.30ms
step:88/2330 train_time:5044ms step_avg:57.32ms
step:89/2330 train_time:5100ms step_avg:57.30ms
step:90/2330 train_time:5159ms step_avg:57.33ms
step:91/2330 train_time:5215ms step_avg:57.31ms
step:92/2330 train_time:5275ms step_avg:57.33ms
step:93/2330 train_time:5330ms step_avg:57.31ms
step:94/2330 train_time:5390ms step_avg:57.34ms
step:95/2330 train_time:5446ms step_avg:57.33ms
step:96/2330 train_time:5506ms step_avg:57.35ms
step:97/2330 train_time:5562ms step_avg:57.34ms
step:98/2330 train_time:5621ms step_avg:57.36ms
step:99/2330 train_time:5677ms step_avg:57.34ms
step:100/2330 train_time:5737ms step_avg:57.37ms
step:101/2330 train_time:5793ms step_avg:57.36ms
step:102/2330 train_time:5853ms step_avg:57.38ms
step:103/2330 train_time:5908ms step_avg:57.36ms
step:104/2330 train_time:5968ms step_avg:57.39ms
step:105/2330 train_time:6024ms step_avg:57.37ms
step:106/2330 train_time:6084ms step_avg:57.39ms
step:107/2330 train_time:6139ms step_avg:57.38ms
step:108/2330 train_time:6199ms step_avg:57.40ms
step:109/2330 train_time:6255ms step_avg:57.39ms
step:110/2330 train_time:6315ms step_avg:57.41ms
step:111/2330 train_time:6371ms step_avg:57.40ms
step:112/2330 train_time:6431ms step_avg:57.42ms
step:113/2330 train_time:6487ms step_avg:57.41ms
step:114/2330 train_time:6547ms step_avg:57.43ms
step:115/2330 train_time:6603ms step_avg:57.42ms
step:116/2330 train_time:6663ms step_avg:57.44ms
step:117/2330 train_time:6719ms step_avg:57.43ms
step:118/2330 train_time:6778ms step_avg:57.44ms
step:119/2330 train_time:6834ms step_avg:57.43ms
step:120/2330 train_time:6894ms step_avg:57.45ms
step:121/2330 train_time:6950ms step_avg:57.44ms
step:122/2330 train_time:7010ms step_avg:57.46ms
step:123/2330 train_time:7066ms step_avg:57.45ms
step:124/2330 train_time:7126ms step_avg:57.46ms
step:125/2330 train_time:7181ms step_avg:57.45ms
step:126/2330 train_time:7240ms step_avg:57.46ms
step:127/2330 train_time:7296ms step_avg:57.45ms
step:128/2330 train_time:7356ms step_avg:57.47ms
step:129/2330 train_time:7412ms step_avg:57.46ms
step:130/2330 train_time:7472ms step_avg:57.48ms
step:131/2330 train_time:7528ms step_avg:57.47ms
step:132/2330 train_time:7588ms step_avg:57.49ms
step:133/2330 train_time:7645ms step_avg:57.48ms
step:134/2330 train_time:7704ms step_avg:57.49ms
step:135/2330 train_time:7760ms step_avg:57.48ms
step:136/2330 train_time:7820ms step_avg:57.50ms
step:137/2330 train_time:7876ms step_avg:57.49ms
step:138/2330 train_time:7936ms step_avg:57.51ms
step:139/2330 train_time:7992ms step_avg:57.50ms
step:140/2330 train_time:8052ms step_avg:57.51ms
step:141/2330 train_time:8109ms step_avg:57.51ms
step:142/2330 train_time:8168ms step_avg:57.52ms
step:143/2330 train_time:8224ms step_avg:57.51ms
step:144/2330 train_time:8284ms step_avg:57.53ms
step:145/2330 train_time:8340ms step_avg:57.51ms
step:146/2330 train_time:8399ms step_avg:57.53ms
step:147/2330 train_time:8455ms step_avg:57.52ms
step:148/2330 train_time:8516ms step_avg:57.54ms
step:149/2330 train_time:8572ms step_avg:57.53ms
step:150/2330 train_time:8633ms step_avg:57.55ms
step:151/2330 train_time:8689ms step_avg:57.54ms
step:152/2330 train_time:8748ms step_avg:57.55ms
step:153/2330 train_time:8804ms step_avg:57.54ms
step:154/2330 train_time:8864ms step_avg:57.56ms
step:155/2330 train_time:8920ms step_avg:57.55ms
step:156/2330 train_time:8979ms step_avg:57.56ms
step:157/2330 train_time:9035ms step_avg:57.55ms
step:158/2330 train_time:9096ms step_avg:57.57ms
step:159/2330 train_time:9152ms step_avg:57.56ms
step:160/2330 train_time:9212ms step_avg:57.57ms
step:161/2330 train_time:9268ms step_avg:57.56ms
step:162/2330 train_time:9327ms step_avg:57.57ms
step:163/2330 train_time:9383ms step_avg:57.56ms
step:164/2330 train_time:9443ms step_avg:57.58ms
step:165/2330 train_time:9499ms step_avg:57.57ms
step:166/2330 train_time:9559ms step_avg:57.59ms
step:167/2330 train_time:9615ms step_avg:57.57ms
step:168/2330 train_time:9676ms step_avg:57.60ms
step:169/2330 train_time:9733ms step_avg:57.59ms
step:170/2330 train_time:9793ms step_avg:57.61ms
step:171/2330 train_time:9849ms step_avg:57.60ms
step:172/2330 train_time:9909ms step_avg:57.61ms
step:173/2330 train_time:9964ms step_avg:57.60ms
step:174/2330 train_time:10024ms step_avg:57.61ms
step:175/2330 train_time:10080ms step_avg:57.60ms
step:176/2330 train_time:10140ms step_avg:57.61ms
step:177/2330 train_time:10196ms step_avg:57.61ms
step:178/2330 train_time:10257ms step_avg:57.62ms
step:179/2330 train_time:10313ms step_avg:57.62ms
step:180/2330 train_time:10373ms step_avg:57.63ms
step:181/2330 train_time:10429ms step_avg:57.62ms
step:182/2330 train_time:10488ms step_avg:57.63ms
step:183/2330 train_time:10544ms step_avg:57.62ms
step:184/2330 train_time:10604ms step_avg:57.63ms
step:185/2330 train_time:10659ms step_avg:57.62ms
step:186/2330 train_time:10719ms step_avg:57.63ms
step:187/2330 train_time:10775ms step_avg:57.62ms
step:188/2330 train_time:10836ms step_avg:57.64ms
step:189/2330 train_time:10892ms step_avg:57.63ms
step:190/2330 train_time:10954ms step_avg:57.65ms
step:191/2330 train_time:11009ms step_avg:57.64ms
step:192/2330 train_time:11069ms step_avg:57.65ms
step:193/2330 train_time:11125ms step_avg:57.64ms
step:194/2330 train_time:11185ms step_avg:57.65ms
step:195/2330 train_time:11240ms step_avg:57.64ms
step:196/2330 train_time:11302ms step_avg:57.66ms
step:197/2330 train_time:11358ms step_avg:57.65ms
step:198/2330 train_time:11418ms step_avg:57.67ms
step:199/2330 train_time:11474ms step_avg:57.66ms
step:200/2330 train_time:11534ms step_avg:57.67ms
step:201/2330 train_time:11590ms step_avg:57.66ms
step:202/2330 train_time:11650ms step_avg:57.67ms
step:203/2330 train_time:11707ms step_avg:57.67ms
step:204/2330 train_time:11766ms step_avg:57.68ms
step:205/2330 train_time:11822ms step_avg:57.67ms
step:206/2330 train_time:11882ms step_avg:57.68ms
step:207/2330 train_time:11938ms step_avg:57.67ms
step:208/2330 train_time:11998ms step_avg:57.68ms
step:209/2330 train_time:12054ms step_avg:57.67ms
step:210/2330 train_time:12114ms step_avg:57.69ms
step:211/2330 train_time:12171ms step_avg:57.68ms
step:212/2330 train_time:12230ms step_avg:57.69ms
step:213/2330 train_time:12287ms step_avg:57.68ms
step:214/2330 train_time:12346ms step_avg:57.69ms
step:215/2330 train_time:12402ms step_avg:57.68ms
step:216/2330 train_time:12462ms step_avg:57.69ms
step:217/2330 train_time:12517ms step_avg:57.68ms
step:218/2330 train_time:12578ms step_avg:57.70ms
step:219/2330 train_time:12634ms step_avg:57.69ms
step:220/2330 train_time:12695ms step_avg:57.70ms
step:221/2330 train_time:12751ms step_avg:57.69ms
step:222/2330 train_time:12810ms step_avg:57.70ms
step:223/2330 train_time:12866ms step_avg:57.69ms
step:224/2330 train_time:12926ms step_avg:57.71ms
step:225/2330 train_time:12982ms step_avg:57.70ms
step:226/2330 train_time:13042ms step_avg:57.71ms
step:227/2330 train_time:13097ms step_avg:57.70ms
step:228/2330 train_time:13158ms step_avg:57.71ms
step:229/2330 train_time:13213ms step_avg:57.70ms
step:230/2330 train_time:13275ms step_avg:57.72ms
step:231/2330 train_time:13331ms step_avg:57.71ms
step:232/2330 train_time:13392ms step_avg:57.72ms
step:233/2330 train_time:13448ms step_avg:57.72ms
step:234/2330 train_time:13507ms step_avg:57.72ms
step:235/2330 train_time:13563ms step_avg:57.72ms
step:236/2330 train_time:13623ms step_avg:57.73ms
step:237/2330 train_time:13679ms step_avg:57.72ms
step:238/2330 train_time:13739ms step_avg:57.73ms
step:239/2330 train_time:13795ms step_avg:57.72ms
step:240/2330 train_time:13856ms step_avg:57.73ms
step:241/2330 train_time:13912ms step_avg:57.73ms
step:242/2330 train_time:13973ms step_avg:57.74ms
step:243/2330 train_time:14029ms step_avg:57.73ms
step:244/2330 train_time:14088ms step_avg:57.74ms
step:245/2330 train_time:14144ms step_avg:57.73ms
step:246/2330 train_time:14205ms step_avg:57.74ms
step:247/2330 train_time:14260ms step_avg:57.73ms
step:248/2330 train_time:14321ms step_avg:57.75ms
step:249/2330 train_time:14377ms step_avg:57.74ms
step:250/2330 train_time:14438ms step_avg:57.75ms
step:250/2330 val_loss:6.1840 train_time:14514ms step_avg:58.06ms
step:251/2330 train_time:14534ms step_avg:57.90ms
step:252/2330 train_time:14556ms step_avg:57.76ms
step:253/2330 train_time:14611ms step_avg:57.75ms
step:254/2330 train_time:14675ms step_avg:57.77ms
step:255/2330 train_time:14730ms step_avg:57.76ms
step:256/2330 train_time:14794ms step_avg:57.79ms
step:257/2330 train_time:14850ms step_avg:57.78ms
step:258/2330 train_time:14911ms step_avg:57.79ms
step:259/2330 train_time:14966ms step_avg:57.79ms
step:260/2330 train_time:15027ms step_avg:57.80ms
step:261/2330 train_time:15083ms step_avg:57.79ms
step:262/2330 train_time:15143ms step_avg:57.80ms
step:263/2330 train_time:15199ms step_avg:57.79ms
step:264/2330 train_time:15258ms step_avg:57.80ms
step:265/2330 train_time:15314ms step_avg:57.79ms
step:266/2330 train_time:15374ms step_avg:57.80ms
step:267/2330 train_time:15429ms step_avg:57.79ms
step:268/2330 train_time:15489ms step_avg:57.79ms
step:269/2330 train_time:15544ms step_avg:57.79ms
step:270/2330 train_time:15605ms step_avg:57.80ms
step:271/2330 train_time:15661ms step_avg:57.79ms
step:272/2330 train_time:15723ms step_avg:57.80ms
step:273/2330 train_time:15779ms step_avg:57.80ms
step:274/2330 train_time:15840ms step_avg:57.81ms
step:275/2330 train_time:15897ms step_avg:57.81ms
step:276/2330 train_time:15958ms step_avg:57.82ms
step:277/2330 train_time:16014ms step_avg:57.81ms
step:278/2330 train_time:16074ms step_avg:57.82ms
step:279/2330 train_time:16129ms step_avg:57.81ms
step:280/2330 train_time:16190ms step_avg:57.82ms
step:281/2330 train_time:16245ms step_avg:57.81ms
step:282/2330 train_time:16305ms step_avg:57.82ms
step:283/2330 train_time:16361ms step_avg:57.81ms
step:284/2330 train_time:16421ms step_avg:57.82ms
step:285/2330 train_time:16477ms step_avg:57.81ms
step:286/2330 train_time:16537ms step_avg:57.82ms
step:287/2330 train_time:16592ms step_avg:57.81ms
step:288/2330 train_time:16652ms step_avg:57.82ms
step:289/2330 train_time:16708ms step_avg:57.81ms
step:290/2330 train_time:16769ms step_avg:57.83ms
step:291/2330 train_time:16825ms step_avg:57.82ms
step:292/2330 train_time:16887ms step_avg:57.83ms
step:293/2330 train_time:16942ms step_avg:57.82ms
step:294/2330 train_time:17004ms step_avg:57.84ms
step:295/2330 train_time:17060ms step_avg:57.83ms
step:296/2330 train_time:17119ms step_avg:57.84ms
step:297/2330 train_time:17175ms step_avg:57.83ms
step:298/2330 train_time:17235ms step_avg:57.84ms
step:299/2330 train_time:17291ms step_avg:57.83ms
step:300/2330 train_time:17351ms step_avg:57.84ms
step:301/2330 train_time:17407ms step_avg:57.83ms
step:302/2330 train_time:17467ms step_avg:57.84ms
step:303/2330 train_time:17523ms step_avg:57.83ms
step:304/2330 train_time:17583ms step_avg:57.84ms
step:305/2330 train_time:17638ms step_avg:57.83ms
step:306/2330 train_time:17699ms step_avg:57.84ms
step:307/2330 train_time:17755ms step_avg:57.83ms
step:308/2330 train_time:17816ms step_avg:57.84ms
step:309/2330 train_time:17871ms step_avg:57.83ms
step:310/2330 train_time:17932ms step_avg:57.85ms
step:311/2330 train_time:17988ms step_avg:57.84ms
step:312/2330 train_time:18048ms step_avg:57.85ms
step:313/2330 train_time:18104ms step_avg:57.84ms
step:314/2330 train_time:18165ms step_avg:57.85ms
step:315/2330 train_time:18221ms step_avg:57.84ms
step:316/2330 train_time:18281ms step_avg:57.85ms
step:317/2330 train_time:18337ms step_avg:57.85ms
step:318/2330 train_time:18397ms step_avg:57.85ms
step:319/2330 train_time:18453ms step_avg:57.85ms
step:320/2330 train_time:18513ms step_avg:57.85ms
step:321/2330 train_time:18569ms step_avg:57.85ms
step:322/2330 train_time:18629ms step_avg:57.85ms
step:323/2330 train_time:18685ms step_avg:57.85ms
step:324/2330 train_time:18746ms step_avg:57.86ms
step:325/2330 train_time:18802ms step_avg:57.85ms
step:326/2330 train_time:18863ms step_avg:57.86ms
step:327/2330 train_time:18919ms step_avg:57.86ms
step:328/2330 train_time:18979ms step_avg:57.86ms
step:329/2330 train_time:19034ms step_avg:57.86ms
step:330/2330 train_time:19095ms step_avg:57.86ms
step:331/2330 train_time:19150ms step_avg:57.86ms
step:332/2330 train_time:19211ms step_avg:57.87ms
step:333/2330 train_time:19267ms step_avg:57.86ms
step:334/2330 train_time:19328ms step_avg:57.87ms
step:335/2330 train_time:19384ms step_avg:57.86ms
step:336/2330 train_time:19445ms step_avg:57.87ms
step:337/2330 train_time:19500ms step_avg:57.86ms
step:338/2330 train_time:19560ms step_avg:57.87ms
step:339/2330 train_time:19616ms step_avg:57.87ms
step:340/2330 train_time:19676ms step_avg:57.87ms
step:341/2330 train_time:19731ms step_avg:57.86ms
step:342/2330 train_time:19792ms step_avg:57.87ms
step:343/2330 train_time:19848ms step_avg:57.86ms
step:344/2330 train_time:19909ms step_avg:57.87ms
step:345/2330 train_time:19965ms step_avg:57.87ms
step:346/2330 train_time:20026ms step_avg:57.88ms
step:347/2330 train_time:20082ms step_avg:57.87ms
step:348/2330 train_time:20144ms step_avg:57.88ms
step:349/2330 train_time:20200ms step_avg:57.88ms
step:350/2330 train_time:20260ms step_avg:57.88ms
step:351/2330 train_time:20316ms step_avg:57.88ms
step:352/2330 train_time:20375ms step_avg:57.88ms
step:353/2330 train_time:20431ms step_avg:57.88ms
step:354/2330 train_time:20492ms step_avg:57.89ms
step:355/2330 train_time:20548ms step_avg:57.88ms
step:356/2330 train_time:20608ms step_avg:57.89ms
step:357/2330 train_time:20664ms step_avg:57.88ms
step:358/2330 train_time:20724ms step_avg:57.89ms
step:359/2330 train_time:20780ms step_avg:57.88ms
step:360/2330 train_time:20840ms step_avg:57.89ms
step:361/2330 train_time:20896ms step_avg:57.88ms
step:362/2330 train_time:20957ms step_avg:57.89ms
step:363/2330 train_time:21013ms step_avg:57.89ms
step:364/2330 train_time:21073ms step_avg:57.89ms
step:365/2330 train_time:21129ms step_avg:57.89ms
step:366/2330 train_time:21190ms step_avg:57.90ms
step:367/2330 train_time:21245ms step_avg:57.89ms
step:368/2330 train_time:21307ms step_avg:57.90ms
step:369/2330 train_time:21363ms step_avg:57.89ms
step:370/2330 train_time:21424ms step_avg:57.90ms
step:371/2330 train_time:21480ms step_avg:57.90ms
step:372/2330 train_time:21540ms step_avg:57.90ms
step:373/2330 train_time:21596ms step_avg:57.90ms
step:374/2330 train_time:21656ms step_avg:57.90ms
step:375/2330 train_time:21712ms step_avg:57.90ms
step:376/2330 train_time:21772ms step_avg:57.91ms
step:377/2330 train_time:21828ms step_avg:57.90ms
step:378/2330 train_time:21889ms step_avg:57.91ms
step:379/2330 train_time:21945ms step_avg:57.90ms
step:380/2330 train_time:22006ms step_avg:57.91ms
step:381/2330 train_time:22062ms step_avg:57.91ms
step:382/2330 train_time:22122ms step_avg:57.91ms
step:383/2330 train_time:22179ms step_avg:57.91ms
step:384/2330 train_time:22239ms step_avg:57.91ms
step:385/2330 train_time:22295ms step_avg:57.91ms
step:386/2330 train_time:22355ms step_avg:57.91ms
step:387/2330 train_time:22411ms step_avg:57.91ms
step:388/2330 train_time:22471ms step_avg:57.91ms
step:389/2330 train_time:22526ms step_avg:57.91ms
step:390/2330 train_time:22588ms step_avg:57.92ms
step:391/2330 train_time:22643ms step_avg:57.91ms
step:392/2330 train_time:22704ms step_avg:57.92ms
step:393/2330 train_time:22759ms step_avg:57.91ms
step:394/2330 train_time:22819ms step_avg:57.92ms
step:395/2330 train_time:22875ms step_avg:57.91ms
step:396/2330 train_time:22935ms step_avg:57.92ms
step:397/2330 train_time:22991ms step_avg:57.91ms
step:398/2330 train_time:23051ms step_avg:57.92ms
step:399/2330 train_time:23107ms step_avg:57.91ms
step:400/2330 train_time:23168ms step_avg:57.92ms
step:401/2330 train_time:23224ms step_avg:57.92ms
step:402/2330 train_time:23284ms step_avg:57.92ms
step:403/2330 train_time:23340ms step_avg:57.92ms
step:404/2330 train_time:23400ms step_avg:57.92ms
step:405/2330 train_time:23456ms step_avg:57.92ms
step:406/2330 train_time:23516ms step_avg:57.92ms
step:407/2330 train_time:23572ms step_avg:57.92ms
step:408/2330 train_time:23632ms step_avg:57.92ms
step:409/2330 train_time:23688ms step_avg:57.92ms
step:410/2330 train_time:23750ms step_avg:57.93ms
step:411/2330 train_time:23806ms step_avg:57.92ms
step:412/2330 train_time:23866ms step_avg:57.93ms
step:413/2330 train_time:23922ms step_avg:57.92ms
step:414/2330 train_time:23983ms step_avg:57.93ms
step:415/2330 train_time:24039ms step_avg:57.93ms
step:416/2330 train_time:24100ms step_avg:57.93ms
step:417/2330 train_time:24155ms step_avg:57.93ms
step:418/2330 train_time:24215ms step_avg:57.93ms
step:419/2330 train_time:24272ms step_avg:57.93ms
step:420/2330 train_time:24332ms step_avg:57.93ms
step:421/2330 train_time:24387ms step_avg:57.93ms
step:422/2330 train_time:24448ms step_avg:57.93ms
step:423/2330 train_time:24505ms step_avg:57.93ms
step:424/2330 train_time:24565ms step_avg:57.94ms
step:425/2330 train_time:24621ms step_avg:57.93ms
step:426/2330 train_time:24682ms step_avg:57.94ms
step:427/2330 train_time:24738ms step_avg:57.93ms
step:428/2330 train_time:24798ms step_avg:57.94ms
step:429/2330 train_time:24854ms step_avg:57.93ms
step:430/2330 train_time:24914ms step_avg:57.94ms
step:431/2330 train_time:24970ms step_avg:57.93ms
step:432/2330 train_time:25031ms step_avg:57.94ms
step:433/2330 train_time:25087ms step_avg:57.94ms
step:434/2330 train_time:25149ms step_avg:57.95ms
step:435/2330 train_time:25204ms step_avg:57.94ms
step:436/2330 train_time:25266ms step_avg:57.95ms
step:437/2330 train_time:25322ms step_avg:57.95ms
step:438/2330 train_time:25382ms step_avg:57.95ms
step:439/2330 train_time:25438ms step_avg:57.94ms
step:440/2330 train_time:25498ms step_avg:57.95ms
step:441/2330 train_time:25554ms step_avg:57.95ms
step:442/2330 train_time:25614ms step_avg:57.95ms
step:443/2330 train_time:25670ms step_avg:57.95ms
step:444/2330 train_time:25731ms step_avg:57.95ms
step:445/2330 train_time:25786ms step_avg:57.95ms
step:446/2330 train_time:25847ms step_avg:57.95ms
step:447/2330 train_time:25904ms step_avg:57.95ms
step:448/2330 train_time:25965ms step_avg:57.96ms
step:449/2330 train_time:26021ms step_avg:57.95ms
step:450/2330 train_time:26081ms step_avg:57.96ms
step:451/2330 train_time:26136ms step_avg:57.95ms
step:452/2330 train_time:26197ms step_avg:57.96ms
step:453/2330 train_time:26253ms step_avg:57.95ms
step:454/2330 train_time:26313ms step_avg:57.96ms
step:455/2330 train_time:26369ms step_avg:57.95ms
step:456/2330 train_time:26430ms step_avg:57.96ms
step:457/2330 train_time:26486ms step_avg:57.96ms
step:458/2330 train_time:26547ms step_avg:57.96ms
step:459/2330 train_time:26602ms step_avg:57.96ms
step:460/2330 train_time:26663ms step_avg:57.96ms
step:461/2330 train_time:26720ms step_avg:57.96ms
step:462/2330 train_time:26780ms step_avg:57.97ms
step:463/2330 train_time:26836ms step_avg:57.96ms
step:464/2330 train_time:26896ms step_avg:57.97ms
step:465/2330 train_time:26952ms step_avg:57.96ms
step:466/2330 train_time:27013ms step_avg:57.97ms
step:467/2330 train_time:27069ms step_avg:57.96ms
step:468/2330 train_time:27129ms step_avg:57.97ms
step:469/2330 train_time:27185ms step_avg:57.96ms
step:470/2330 train_time:27247ms step_avg:57.97ms
step:471/2330 train_time:27303ms step_avg:57.97ms
step:472/2330 train_time:27364ms step_avg:57.97ms
step:473/2330 train_time:27419ms step_avg:57.97ms
step:474/2330 train_time:27479ms step_avg:57.97ms
step:475/2330 train_time:27535ms step_avg:57.97ms
step:476/2330 train_time:27596ms step_avg:57.98ms
step:477/2330 train_time:27652ms step_avg:57.97ms
step:478/2330 train_time:27713ms step_avg:57.98ms
step:479/2330 train_time:27768ms step_avg:57.97ms
step:480/2330 train_time:27829ms step_avg:57.98ms
step:481/2330 train_time:27885ms step_avg:57.97ms
step:482/2330 train_time:27946ms step_avg:57.98ms
step:483/2330 train_time:28002ms step_avg:57.97ms
step:484/2330 train_time:28062ms step_avg:57.98ms
step:485/2330 train_time:28118ms step_avg:57.98ms
step:486/2330 train_time:28178ms step_avg:57.98ms
step:487/2330 train_time:28233ms step_avg:57.97ms
step:488/2330 train_time:28295ms step_avg:57.98ms
step:489/2330 train_time:28351ms step_avg:57.98ms
step:490/2330 train_time:28411ms step_avg:57.98ms
step:491/2330 train_time:28467ms step_avg:57.98ms
step:492/2330 train_time:28529ms step_avg:57.99ms
step:493/2330 train_time:28584ms step_avg:57.98ms
step:494/2330 train_time:28646ms step_avg:57.99ms
step:495/2330 train_time:28702ms step_avg:57.98ms
step:496/2330 train_time:28763ms step_avg:57.99ms
step:497/2330 train_time:28819ms step_avg:57.99ms
step:498/2330 train_time:28880ms step_avg:57.99ms
step:499/2330 train_time:28936ms step_avg:57.99ms
step:500/2330 train_time:28996ms step_avg:57.99ms
step:500/2330 val_loss:5.2504 train_time:29074ms step_avg:58.15ms
step:501/2330 train_time:29094ms step_avg:58.07ms
step:502/2330 train_time:29116ms step_avg:58.00ms
step:503/2330 train_time:29172ms step_avg:58.00ms
step:504/2330 train_time:29237ms step_avg:58.01ms
step:505/2330 train_time:29293ms step_avg:58.01ms
step:506/2330 train_time:29356ms step_avg:58.02ms
step:507/2330 train_time:29412ms step_avg:58.01ms
step:508/2330 train_time:29473ms step_avg:58.02ms
step:509/2330 train_time:29528ms step_avg:58.01ms
step:510/2330 train_time:29589ms step_avg:58.02ms
step:511/2330 train_time:29645ms step_avg:58.01ms
step:512/2330 train_time:29704ms step_avg:58.02ms
step:513/2330 train_time:29760ms step_avg:58.01ms
step:514/2330 train_time:29820ms step_avg:58.01ms
step:515/2330 train_time:29875ms step_avg:58.01ms
step:516/2330 train_time:29935ms step_avg:58.01ms
step:517/2330 train_time:29990ms step_avg:58.01ms
step:518/2330 train_time:30050ms step_avg:58.01ms
step:519/2330 train_time:30107ms step_avg:58.01ms
step:520/2330 train_time:30169ms step_avg:58.02ms
step:521/2330 train_time:30225ms step_avg:58.01ms
step:522/2330 train_time:30288ms step_avg:58.02ms
step:523/2330 train_time:30343ms step_avg:58.02ms
step:524/2330 train_time:30405ms step_avg:58.02ms
step:525/2330 train_time:30461ms step_avg:58.02ms
step:526/2330 train_time:30522ms step_avg:58.03ms
step:527/2330 train_time:30577ms step_avg:58.02ms
step:528/2330 train_time:30637ms step_avg:58.02ms
step:529/2330 train_time:30692ms step_avg:58.02ms
step:530/2330 train_time:30753ms step_avg:58.02ms
step:531/2330 train_time:30808ms step_avg:58.02ms
step:532/2330 train_time:30871ms step_avg:58.03ms
step:533/2330 train_time:30926ms step_avg:58.02ms
step:534/2330 train_time:30987ms step_avg:58.03ms
step:535/2330 train_time:31042ms step_avg:58.02ms
step:536/2330 train_time:31102ms step_avg:58.03ms
step:537/2330 train_time:31159ms step_avg:58.02ms
step:538/2330 train_time:31219ms step_avg:58.03ms
step:539/2330 train_time:31274ms step_avg:58.02ms
step:540/2330 train_time:31336ms step_avg:58.03ms
step:541/2330 train_time:31391ms step_avg:58.02ms
step:542/2330 train_time:31453ms step_avg:58.03ms
step:543/2330 train_time:31509ms step_avg:58.03ms
step:544/2330 train_time:31571ms step_avg:58.03ms
step:545/2330 train_time:31626ms step_avg:58.03ms
step:546/2330 train_time:31687ms step_avg:58.03ms
step:547/2330 train_time:31743ms step_avg:58.03ms
step:548/2330 train_time:31803ms step_avg:58.03ms
step:549/2330 train_time:31858ms step_avg:58.03ms
step:550/2330 train_time:31918ms step_avg:58.03ms
step:551/2330 train_time:31974ms step_avg:58.03ms
step:552/2330 train_time:32035ms step_avg:58.03ms
step:553/2330 train_time:32090ms step_avg:58.03ms
step:554/2330 train_time:32151ms step_avg:58.03ms
step:555/2330 train_time:32207ms step_avg:58.03ms
step:556/2330 train_time:32268ms step_avg:58.04ms
step:557/2330 train_time:32324ms step_avg:58.03ms
step:558/2330 train_time:32384ms step_avg:58.04ms
step:559/2330 train_time:32441ms step_avg:58.03ms
step:560/2330 train_time:32501ms step_avg:58.04ms
step:561/2330 train_time:32557ms step_avg:58.03ms
step:562/2330 train_time:32618ms step_avg:58.04ms
step:563/2330 train_time:32673ms step_avg:58.03ms
step:564/2330 train_time:32734ms step_avg:58.04ms
step:565/2330 train_time:32789ms step_avg:58.03ms
step:566/2330 train_time:32852ms step_avg:58.04ms
step:567/2330 train_time:32907ms step_avg:58.04ms
step:568/2330 train_time:32969ms step_avg:58.04ms
step:569/2330 train_time:33025ms step_avg:58.04ms
step:570/2330 train_time:33086ms step_avg:58.04ms
step:571/2330 train_time:33141ms step_avg:58.04ms
step:572/2330 train_time:33202ms step_avg:58.05ms
step:573/2330 train_time:33258ms step_avg:58.04ms
step:574/2330 train_time:33318ms step_avg:58.05ms
step:575/2330 train_time:33374ms step_avg:58.04ms
step:576/2330 train_time:33435ms step_avg:58.05ms
step:577/2330 train_time:33491ms step_avg:58.04ms
step:578/2330 train_time:33553ms step_avg:58.05ms
step:579/2330 train_time:33609ms step_avg:58.05ms
step:580/2330 train_time:33670ms step_avg:58.05ms
step:581/2330 train_time:33726ms step_avg:58.05ms
step:582/2330 train_time:33787ms step_avg:58.05ms
step:583/2330 train_time:33843ms step_avg:58.05ms
step:584/2330 train_time:33904ms step_avg:58.06ms
step:585/2330 train_time:33960ms step_avg:58.05ms
step:586/2330 train_time:34021ms step_avg:58.06ms
step:587/2330 train_time:34077ms step_avg:58.05ms
step:588/2330 train_time:34137ms step_avg:58.06ms
step:589/2330 train_time:34193ms step_avg:58.05ms
step:590/2330 train_time:34254ms step_avg:58.06ms
step:591/2330 train_time:34310ms step_avg:58.05ms
step:592/2330 train_time:34371ms step_avg:58.06ms
step:593/2330 train_time:34426ms step_avg:58.05ms
step:594/2330 train_time:34487ms step_avg:58.06ms
step:595/2330 train_time:34543ms step_avg:58.06ms
step:596/2330 train_time:34604ms step_avg:58.06ms
step:597/2330 train_time:34660ms step_avg:58.06ms
step:598/2330 train_time:34721ms step_avg:58.06ms
step:599/2330 train_time:34776ms step_avg:58.06ms
step:600/2330 train_time:34837ms step_avg:58.06ms
step:601/2330 train_time:34893ms step_avg:58.06ms
step:602/2330 train_time:34954ms step_avg:58.06ms
step:603/2330 train_time:35010ms step_avg:58.06ms
step:604/2330 train_time:35072ms step_avg:58.07ms
step:605/2330 train_time:35128ms step_avg:58.06ms
step:606/2330 train_time:35190ms step_avg:58.07ms
step:607/2330 train_time:35246ms step_avg:58.07ms
step:608/2330 train_time:35307ms step_avg:58.07ms
step:609/2330 train_time:35363ms step_avg:58.07ms
step:610/2330 train_time:35423ms step_avg:58.07ms
step:611/2330 train_time:35478ms step_avg:58.07ms
step:612/2330 train_time:35539ms step_avg:58.07ms
step:613/2330 train_time:35595ms step_avg:58.07ms
step:614/2330 train_time:35655ms step_avg:58.07ms
step:615/2330 train_time:35711ms step_avg:58.07ms
step:616/2330 train_time:35771ms step_avg:58.07ms
step:617/2330 train_time:35827ms step_avg:58.07ms
step:618/2330 train_time:35889ms step_avg:58.07ms
step:619/2330 train_time:35945ms step_avg:58.07ms
step:620/2330 train_time:36005ms step_avg:58.07ms
step:621/2330 train_time:36061ms step_avg:58.07ms
step:622/2330 train_time:36120ms step_avg:58.07ms
step:623/2330 train_time:36176ms step_avg:58.07ms
step:624/2330 train_time:36236ms step_avg:58.07ms
step:625/2330 train_time:36292ms step_avg:58.07ms
step:626/2330 train_time:36353ms step_avg:58.07ms
step:627/2330 train_time:36408ms step_avg:58.07ms
step:628/2330 train_time:36470ms step_avg:58.07ms
step:629/2330 train_time:36525ms step_avg:58.07ms
step:630/2330 train_time:36587ms step_avg:58.07ms
step:631/2330 train_time:36643ms step_avg:58.07ms
step:632/2330 train_time:36703ms step_avg:58.07ms
step:633/2330 train_time:36758ms step_avg:58.07ms
step:634/2330 train_time:36830ms step_avg:58.09ms
step:635/2330 train_time:36874ms step_avg:58.07ms
step:636/2330 train_time:36935ms step_avg:58.07ms
step:637/2330 train_time:36990ms step_avg:58.07ms
step:638/2330 train_time:37052ms step_avg:58.07ms
step:639/2330 train_time:37108ms step_avg:58.07ms
step:640/2330 train_time:37169ms step_avg:58.08ms
step:641/2330 train_time:37225ms step_avg:58.07ms
step:642/2330 train_time:37284ms step_avg:58.08ms
step:643/2330 train_time:37340ms step_avg:58.07ms
step:644/2330 train_time:37401ms step_avg:58.08ms
step:645/2330 train_time:37457ms step_avg:58.07ms
step:646/2330 train_time:37517ms step_avg:58.08ms
step:647/2330 train_time:37573ms step_avg:58.07ms
step:648/2330 train_time:37634ms step_avg:58.08ms
step:649/2330 train_time:37689ms step_avg:58.07ms
step:650/2330 train_time:37751ms step_avg:58.08ms
step:651/2330 train_time:37806ms step_avg:58.07ms
step:652/2330 train_time:37869ms step_avg:58.08ms
step:653/2330 train_time:37925ms step_avg:58.08ms
step:654/2330 train_time:37985ms step_avg:58.08ms
step:655/2330 train_time:38041ms step_avg:58.08ms
step:656/2330 train_time:38102ms step_avg:58.08ms
step:657/2330 train_time:38157ms step_avg:58.08ms
step:658/2330 train_time:38219ms step_avg:58.08ms
step:659/2330 train_time:38275ms step_avg:58.08ms
step:660/2330 train_time:38335ms step_avg:58.08ms
step:661/2330 train_time:38391ms step_avg:58.08ms
step:662/2330 train_time:38452ms step_avg:58.08ms
step:663/2330 train_time:38508ms step_avg:58.08ms
step:664/2330 train_time:38570ms step_avg:58.09ms
step:665/2330 train_time:38625ms step_avg:58.08ms
step:666/2330 train_time:38686ms step_avg:58.09ms
step:667/2330 train_time:38742ms step_avg:58.08ms
step:668/2330 train_time:38802ms step_avg:58.09ms
step:669/2330 train_time:38858ms step_avg:58.08ms
step:670/2330 train_time:38919ms step_avg:58.09ms
step:671/2330 train_time:38975ms step_avg:58.08ms
step:672/2330 train_time:39036ms step_avg:58.09ms
step:673/2330 train_time:39092ms step_avg:58.09ms
step:674/2330 train_time:39153ms step_avg:58.09ms
step:675/2330 train_time:39208ms step_avg:58.09ms
step:676/2330 train_time:39270ms step_avg:58.09ms
step:677/2330 train_time:39326ms step_avg:58.09ms
step:678/2330 train_time:39388ms step_avg:58.09ms
step:679/2330 train_time:39444ms step_avg:58.09ms
step:680/2330 train_time:39505ms step_avg:58.10ms
step:681/2330 train_time:39561ms step_avg:58.09ms
step:682/2330 train_time:39621ms step_avg:58.10ms
step:683/2330 train_time:39677ms step_avg:58.09ms
step:684/2330 train_time:39737ms step_avg:58.10ms
step:685/2330 train_time:39793ms step_avg:58.09ms
step:686/2330 train_time:39854ms step_avg:58.10ms
step:687/2330 train_time:39909ms step_avg:58.09ms
step:688/2330 train_time:39972ms step_avg:58.10ms
step:689/2330 train_time:40027ms step_avg:58.09ms
step:690/2330 train_time:40088ms step_avg:58.10ms
step:691/2330 train_time:40143ms step_avg:58.09ms
step:692/2330 train_time:40205ms step_avg:58.10ms
step:693/2330 train_time:40262ms step_avg:58.10ms
step:694/2330 train_time:40323ms step_avg:58.10ms
step:695/2330 train_time:40378ms step_avg:58.10ms
step:696/2330 train_time:40438ms step_avg:58.10ms
step:697/2330 train_time:40494ms step_avg:58.10ms
step:698/2330 train_time:40555ms step_avg:58.10ms
step:699/2330 train_time:40610ms step_avg:58.10ms
step:700/2330 train_time:40672ms step_avg:58.10ms
step:701/2330 train_time:40728ms step_avg:58.10ms
step:702/2330 train_time:40789ms step_avg:58.10ms
step:703/2330 train_time:40844ms step_avg:58.10ms
step:704/2330 train_time:40905ms step_avg:58.10ms
step:705/2330 train_time:40961ms step_avg:58.10ms
step:706/2330 train_time:41021ms step_avg:58.10ms
step:707/2330 train_time:41077ms step_avg:58.10ms
step:708/2330 train_time:41136ms step_avg:58.10ms
step:709/2330 train_time:41192ms step_avg:58.10ms
step:710/2330 train_time:41253ms step_avg:58.10ms
step:711/2330 train_time:41308ms step_avg:58.10ms
step:712/2330 train_time:41371ms step_avg:58.10ms
step:713/2330 train_time:41427ms step_avg:58.10ms
step:714/2330 train_time:41488ms step_avg:58.11ms
step:715/2330 train_time:41545ms step_avg:58.10ms
step:716/2330 train_time:41605ms step_avg:58.11ms
step:717/2330 train_time:41661ms step_avg:58.10ms
step:718/2330 train_time:41721ms step_avg:58.11ms
step:719/2330 train_time:41777ms step_avg:58.10ms
step:720/2330 train_time:41837ms step_avg:58.11ms
step:721/2330 train_time:41892ms step_avg:58.10ms
step:722/2330 train_time:41954ms step_avg:58.11ms
step:723/2330 train_time:42010ms step_avg:58.10ms
step:724/2330 train_time:42071ms step_avg:58.11ms
step:725/2330 train_time:42127ms step_avg:58.11ms
step:726/2330 train_time:42189ms step_avg:58.11ms
step:727/2330 train_time:42245ms step_avg:58.11ms
step:728/2330 train_time:42306ms step_avg:58.11ms
step:729/2330 train_time:42362ms step_avg:58.11ms
step:730/2330 train_time:42422ms step_avg:58.11ms
step:731/2330 train_time:42478ms step_avg:58.11ms
step:732/2330 train_time:42538ms step_avg:58.11ms
step:733/2330 train_time:42594ms step_avg:58.11ms
step:734/2330 train_time:42655ms step_avg:58.11ms
step:735/2330 train_time:42711ms step_avg:58.11ms
step:736/2330 train_time:42772ms step_avg:58.11ms
step:737/2330 train_time:42828ms step_avg:58.11ms
step:738/2330 train_time:42889ms step_avg:58.12ms
step:739/2330 train_time:42945ms step_avg:58.11ms
step:740/2330 train_time:43006ms step_avg:58.12ms
step:741/2330 train_time:43061ms step_avg:58.11ms
step:742/2330 train_time:43122ms step_avg:58.12ms
step:743/2330 train_time:43177ms step_avg:58.11ms
step:744/2330 train_time:43238ms step_avg:58.12ms
step:745/2330 train_time:43293ms step_avg:58.11ms
step:746/2330 train_time:43354ms step_avg:58.12ms
step:747/2330 train_time:43410ms step_avg:58.11ms
step:748/2330 train_time:43471ms step_avg:58.12ms
step:749/2330 train_time:43527ms step_avg:58.11ms
step:750/2330 train_time:43587ms step_avg:58.12ms
step:750/2330 val_loss:4.8250 train_time:43663ms step_avg:58.22ms
step:751/2330 train_time:43684ms step_avg:58.17ms
step:752/2330 train_time:43706ms step_avg:58.12ms
step:753/2330 train_time:43763ms step_avg:58.12ms
step:754/2330 train_time:43825ms step_avg:58.12ms
step:755/2330 train_time:43882ms step_avg:58.12ms
step:756/2330 train_time:43943ms step_avg:58.13ms
step:757/2330 train_time:43999ms step_avg:58.12ms
step:758/2330 train_time:44060ms step_avg:58.13ms
step:759/2330 train_time:44115ms step_avg:58.12ms
step:760/2330 train_time:44175ms step_avg:58.13ms
step:761/2330 train_time:44231ms step_avg:58.12ms
step:762/2330 train_time:44290ms step_avg:58.12ms
step:763/2330 train_time:44346ms step_avg:58.12ms
step:764/2330 train_time:44406ms step_avg:58.12ms
step:765/2330 train_time:44463ms step_avg:58.12ms
step:766/2330 train_time:44523ms step_avg:58.12ms
step:767/2330 train_time:44579ms step_avg:58.12ms
step:768/2330 train_time:44640ms step_avg:58.13ms
step:769/2330 train_time:44697ms step_avg:58.12ms
step:770/2330 train_time:44758ms step_avg:58.13ms
step:771/2330 train_time:44816ms step_avg:58.13ms
step:772/2330 train_time:44877ms step_avg:58.13ms
step:773/2330 train_time:44934ms step_avg:58.13ms
step:774/2330 train_time:44996ms step_avg:58.13ms
step:775/2330 train_time:45052ms step_avg:58.13ms
step:776/2330 train_time:45115ms step_avg:58.14ms
step:777/2330 train_time:45171ms step_avg:58.14ms
step:778/2330 train_time:45233ms step_avg:58.14ms
step:779/2330 train_time:45289ms step_avg:58.14ms
step:780/2330 train_time:45351ms step_avg:58.14ms
step:781/2330 train_time:45408ms step_avg:58.14ms
step:782/2330 train_time:45469ms step_avg:58.14ms
step:783/2330 train_time:45525ms step_avg:58.14ms
step:784/2330 train_time:45586ms step_avg:58.15ms
step:785/2330 train_time:45642ms step_avg:58.14ms
step:786/2330 train_time:45704ms step_avg:58.15ms
step:787/2330 train_time:45760ms step_avg:58.15ms
step:788/2330 train_time:45822ms step_avg:58.15ms
step:789/2330 train_time:45879ms step_avg:58.15ms
step:790/2330 train_time:45941ms step_avg:58.15ms
step:791/2330 train_time:45998ms step_avg:58.15ms
step:792/2330 train_time:46059ms step_avg:58.16ms
step:793/2330 train_time:46116ms step_avg:58.15ms
step:794/2330 train_time:46177ms step_avg:58.16ms
step:795/2330 train_time:46234ms step_avg:58.16ms
step:796/2330 train_time:46294ms step_avg:58.16ms
step:797/2330 train_time:46351ms step_avg:58.16ms
step:798/2330 train_time:46412ms step_avg:58.16ms
step:799/2330 train_time:46468ms step_avg:58.16ms
step:800/2330 train_time:46530ms step_avg:58.16ms
step:801/2330 train_time:46586ms step_avg:58.16ms
step:802/2330 train_time:46647ms step_avg:58.16ms
step:803/2330 train_time:46704ms step_avg:58.16ms
step:804/2330 train_time:46766ms step_avg:58.17ms
step:805/2330 train_time:46822ms step_avg:58.16ms
step:806/2330 train_time:46886ms step_avg:58.17ms
step:807/2330 train_time:46943ms step_avg:58.17ms
step:808/2330 train_time:47005ms step_avg:58.17ms
step:809/2330 train_time:47062ms step_avg:58.17ms
step:810/2330 train_time:47124ms step_avg:58.18ms
step:811/2330 train_time:47181ms step_avg:58.18ms
step:812/2330 train_time:47243ms step_avg:58.18ms
step:813/2330 train_time:47301ms step_avg:58.18ms
step:814/2330 train_time:47361ms step_avg:58.18ms
step:815/2330 train_time:47418ms step_avg:58.18ms
step:816/2330 train_time:47479ms step_avg:58.18ms
step:817/2330 train_time:47535ms step_avg:58.18ms
step:818/2330 train_time:47597ms step_avg:58.19ms
step:819/2330 train_time:47653ms step_avg:58.18ms
step:820/2330 train_time:47715ms step_avg:58.19ms
step:821/2330 train_time:47771ms step_avg:58.19ms
step:822/2330 train_time:47833ms step_avg:58.19ms
step:823/2330 train_time:47889ms step_avg:58.19ms
step:824/2330 train_time:47951ms step_avg:58.19ms
step:825/2330 train_time:48007ms step_avg:58.19ms
step:826/2330 train_time:48071ms step_avg:58.20ms
step:827/2330 train_time:48127ms step_avg:58.19ms
step:828/2330 train_time:48191ms step_avg:58.20ms
step:829/2330 train_time:48247ms step_avg:58.20ms
step:830/2330 train_time:48309ms step_avg:58.20ms
step:831/2330 train_time:48366ms step_avg:58.20ms
step:832/2330 train_time:48428ms step_avg:58.21ms
step:833/2330 train_time:48485ms step_avg:58.21ms
step:834/2330 train_time:48547ms step_avg:58.21ms
step:835/2330 train_time:48604ms step_avg:58.21ms
step:836/2330 train_time:48664ms step_avg:58.21ms
step:837/2330 train_time:48721ms step_avg:58.21ms
step:838/2330 train_time:48783ms step_avg:58.21ms
step:839/2330 train_time:48839ms step_avg:58.21ms
step:840/2330 train_time:48900ms step_avg:58.21ms
step:841/2330 train_time:48956ms step_avg:58.21ms
step:842/2330 train_time:49018ms step_avg:58.22ms
step:843/2330 train_time:49074ms step_avg:58.21ms
step:844/2330 train_time:49136ms step_avg:58.22ms
step:845/2330 train_time:49192ms step_avg:58.22ms
step:846/2330 train_time:49254ms step_avg:58.22ms
step:847/2330 train_time:49310ms step_avg:58.22ms
step:848/2330 train_time:49373ms step_avg:58.22ms
step:849/2330 train_time:49429ms step_avg:58.22ms
step:850/2330 train_time:49490ms step_avg:58.22ms
step:851/2330 train_time:49546ms step_avg:58.22ms
step:852/2330 train_time:49608ms step_avg:58.23ms
step:853/2330 train_time:49664ms step_avg:58.22ms
step:854/2330 train_time:49727ms step_avg:58.23ms
step:855/2330 train_time:49783ms step_avg:58.23ms
step:856/2330 train_time:49845ms step_avg:58.23ms
step:857/2330 train_time:49902ms step_avg:58.23ms
step:858/2330 train_time:49964ms step_avg:58.23ms
step:859/2330 train_time:50021ms step_avg:58.23ms
step:860/2330 train_time:50083ms step_avg:58.24ms
step:861/2330 train_time:50140ms step_avg:58.23ms
step:862/2330 train_time:50201ms step_avg:58.24ms
step:863/2330 train_time:50259ms step_avg:58.24ms
step:864/2330 train_time:50320ms step_avg:58.24ms
step:865/2330 train_time:50377ms step_avg:58.24ms
step:866/2330 train_time:50437ms step_avg:58.24ms
step:867/2330 train_time:50494ms step_avg:58.24ms
step:868/2330 train_time:50555ms step_avg:58.24ms
step:869/2330 train_time:50611ms step_avg:58.24ms
step:870/2330 train_time:50673ms step_avg:58.24ms
step:871/2330 train_time:50729ms step_avg:58.24ms
step:872/2330 train_time:50791ms step_avg:58.25ms
step:873/2330 train_time:50847ms step_avg:58.24ms
step:874/2330 train_time:50910ms step_avg:58.25ms
step:875/2330 train_time:50966ms step_avg:58.25ms
step:876/2330 train_time:51028ms step_avg:58.25ms
step:877/2330 train_time:51085ms step_avg:58.25ms
step:878/2330 train_time:51147ms step_avg:58.25ms
step:879/2330 train_time:51204ms step_avg:58.25ms
step:880/2330 train_time:51268ms step_avg:58.26ms
step:881/2330 train_time:51325ms step_avg:58.26ms
step:882/2330 train_time:51386ms step_avg:58.26ms
step:883/2330 train_time:51443ms step_avg:58.26ms
step:884/2330 train_time:51504ms step_avg:58.26ms
step:885/2330 train_time:51561ms step_avg:58.26ms
step:886/2330 train_time:51621ms step_avg:58.26ms
step:887/2330 train_time:51679ms step_avg:58.26ms
step:888/2330 train_time:51739ms step_avg:58.26ms
step:889/2330 train_time:51796ms step_avg:58.26ms
step:890/2330 train_time:51856ms step_avg:58.27ms
step:891/2330 train_time:51912ms step_avg:58.26ms
step:892/2330 train_time:51974ms step_avg:58.27ms
step:893/2330 train_time:52030ms step_avg:58.26ms
step:894/2330 train_time:52092ms step_avg:58.27ms
step:895/2330 train_time:52148ms step_avg:58.27ms
step:896/2330 train_time:52211ms step_avg:58.27ms
step:897/2330 train_time:52267ms step_avg:58.27ms
step:898/2330 train_time:52331ms step_avg:58.27ms
step:899/2330 train_time:52387ms step_avg:58.27ms
step:900/2330 train_time:52449ms step_avg:58.28ms
step:901/2330 train_time:52506ms step_avg:58.28ms
step:902/2330 train_time:52568ms step_avg:58.28ms
step:903/2330 train_time:52624ms step_avg:58.28ms
step:904/2330 train_time:52686ms step_avg:58.28ms
step:905/2330 train_time:52742ms step_avg:58.28ms
step:906/2330 train_time:52805ms step_avg:58.28ms
step:907/2330 train_time:52861ms step_avg:58.28ms
step:908/2330 train_time:52923ms step_avg:58.28ms
step:909/2330 train_time:52980ms step_avg:58.28ms
step:910/2330 train_time:53042ms step_avg:58.29ms
step:911/2330 train_time:53099ms step_avg:58.29ms
step:912/2330 train_time:53160ms step_avg:58.29ms
step:913/2330 train_time:53216ms step_avg:58.29ms
step:914/2330 train_time:53277ms step_avg:58.29ms
step:915/2330 train_time:53334ms step_avg:58.29ms
step:916/2330 train_time:53395ms step_avg:58.29ms
step:917/2330 train_time:53451ms step_avg:58.29ms
step:918/2330 train_time:53513ms step_avg:58.29ms
step:919/2330 train_time:53569ms step_avg:58.29ms
step:920/2330 train_time:53631ms step_avg:58.29ms
step:921/2330 train_time:53687ms step_avg:58.29ms
step:922/2330 train_time:53749ms step_avg:58.30ms
step:923/2330 train_time:53806ms step_avg:58.29ms
step:924/2330 train_time:53868ms step_avg:58.30ms
step:925/2330 train_time:53924ms step_avg:58.30ms
step:926/2330 train_time:53988ms step_avg:58.30ms
step:927/2330 train_time:54045ms step_avg:58.30ms
step:928/2330 train_time:54108ms step_avg:58.31ms
step:929/2330 train_time:54165ms step_avg:58.30ms
step:930/2330 train_time:54227ms step_avg:58.31ms
step:931/2330 train_time:54284ms step_avg:58.31ms
step:932/2330 train_time:54347ms step_avg:58.31ms
step:933/2330 train_time:54403ms step_avg:58.31ms
step:934/2330 train_time:54465ms step_avg:58.31ms
step:935/2330 train_time:54522ms step_avg:58.31ms
step:936/2330 train_time:54584ms step_avg:58.32ms
step:937/2330 train_time:54640ms step_avg:58.31ms
step:938/2330 train_time:54701ms step_avg:58.32ms
step:939/2330 train_time:54757ms step_avg:58.31ms
step:940/2330 train_time:54819ms step_avg:58.32ms
step:941/2330 train_time:54875ms step_avg:58.32ms
step:942/2330 train_time:54936ms step_avg:58.32ms
step:943/2330 train_time:54993ms step_avg:58.32ms
step:944/2330 train_time:55054ms step_avg:58.32ms
step:945/2330 train_time:55110ms step_avg:58.32ms
step:946/2330 train_time:55172ms step_avg:58.32ms
step:947/2330 train_time:55229ms step_avg:58.32ms
step:948/2330 train_time:55291ms step_avg:58.32ms
step:949/2330 train_time:55347ms step_avg:58.32ms
step:950/2330 train_time:55409ms step_avg:58.33ms
step:951/2330 train_time:55465ms step_avg:58.32ms
step:952/2330 train_time:55527ms step_avg:58.33ms
step:953/2330 train_time:55585ms step_avg:58.33ms
step:954/2330 train_time:55646ms step_avg:58.33ms
step:955/2330 train_time:55703ms step_avg:58.33ms
step:956/2330 train_time:55765ms step_avg:58.33ms
step:957/2330 train_time:55822ms step_avg:58.33ms
step:958/2330 train_time:55883ms step_avg:58.33ms
step:959/2330 train_time:55940ms step_avg:58.33ms
step:960/2330 train_time:56001ms step_avg:58.33ms
step:961/2330 train_time:56058ms step_avg:58.33ms
step:962/2330 train_time:56119ms step_avg:58.34ms
step:963/2330 train_time:56176ms step_avg:58.33ms
step:964/2330 train_time:56236ms step_avg:58.34ms
step:965/2330 train_time:56292ms step_avg:58.33ms
step:966/2330 train_time:56354ms step_avg:58.34ms
step:967/2330 train_time:56410ms step_avg:58.33ms
step:968/2330 train_time:56472ms step_avg:58.34ms
step:969/2330 train_time:56528ms step_avg:58.34ms
step:970/2330 train_time:56591ms step_avg:58.34ms
step:971/2330 train_time:56647ms step_avg:58.34ms
step:972/2330 train_time:56710ms step_avg:58.34ms
step:973/2330 train_time:56767ms step_avg:58.34ms
step:974/2330 train_time:56829ms step_avg:58.35ms
step:975/2330 train_time:56885ms step_avg:58.34ms
step:976/2330 train_time:56948ms step_avg:58.35ms
step:977/2330 train_time:57004ms step_avg:58.35ms
step:978/2330 train_time:57067ms step_avg:58.35ms
step:979/2330 train_time:57123ms step_avg:58.35ms
step:980/2330 train_time:57186ms step_avg:58.35ms
step:981/2330 train_time:57242ms step_avg:58.35ms
step:982/2330 train_time:57305ms step_avg:58.35ms
step:983/2330 train_time:57361ms step_avg:58.35ms
step:984/2330 train_time:57423ms step_avg:58.36ms
step:985/2330 train_time:57479ms step_avg:58.35ms
step:986/2330 train_time:57540ms step_avg:58.36ms
step:987/2330 train_time:57597ms step_avg:58.36ms
step:988/2330 train_time:57658ms step_avg:58.36ms
step:989/2330 train_time:57715ms step_avg:58.36ms
step:990/2330 train_time:57776ms step_avg:58.36ms
step:991/2330 train_time:57833ms step_avg:58.36ms
step:992/2330 train_time:57894ms step_avg:58.36ms
step:993/2330 train_time:57950ms step_avg:58.36ms
step:994/2330 train_time:58012ms step_avg:58.36ms
step:995/2330 train_time:58068ms step_avg:58.36ms
step:996/2330 train_time:58130ms step_avg:58.36ms
step:997/2330 train_time:58186ms step_avg:58.36ms
step:998/2330 train_time:58250ms step_avg:58.37ms
step:999/2330 train_time:58306ms step_avg:58.36ms
step:1000/2330 train_time:58369ms step_avg:58.37ms
step:1000/2330 val_loss:4.5600 train_time:58448ms step_avg:58.45ms
step:1001/2330 train_time:58467ms step_avg:58.41ms
step:1002/2330 train_time:58489ms step_avg:58.37ms
step:1003/2330 train_time:58545ms step_avg:58.37ms
step:1004/2330 train_time:58614ms step_avg:58.38ms
step:1005/2330 train_time:58670ms step_avg:58.38ms
step:1006/2330 train_time:58735ms step_avg:58.38ms
step:1007/2330 train_time:58792ms step_avg:58.38ms
step:1008/2330 train_time:58852ms step_avg:58.39ms
step:1009/2330 train_time:58908ms step_avg:58.38ms
step:1010/2330 train_time:58970ms step_avg:58.39ms
step:1011/2330 train_time:59026ms step_avg:58.38ms
step:1012/2330 train_time:59086ms step_avg:58.39ms
step:1013/2330 train_time:59143ms step_avg:58.38ms
step:1014/2330 train_time:59203ms step_avg:58.39ms
step:1015/2330 train_time:59259ms step_avg:58.38ms
step:1016/2330 train_time:59320ms step_avg:58.39ms
step:1017/2330 train_time:59377ms step_avg:58.38ms
step:1018/2330 train_time:59441ms step_avg:58.39ms
step:1019/2330 train_time:59498ms step_avg:58.39ms
step:1020/2330 train_time:59561ms step_avg:58.39ms
step:1021/2330 train_time:59618ms step_avg:58.39ms
step:1022/2330 train_time:59681ms step_avg:58.40ms
step:1023/2330 train_time:59737ms step_avg:58.39ms
step:1024/2330 train_time:59799ms step_avg:58.40ms
step:1025/2330 train_time:59855ms step_avg:58.40ms
step:1026/2330 train_time:59916ms step_avg:58.40ms
step:1027/2330 train_time:59973ms step_avg:58.40ms
step:1028/2330 train_time:60034ms step_avg:58.40ms
step:1029/2330 train_time:60090ms step_avg:58.40ms
step:1030/2330 train_time:60152ms step_avg:58.40ms
step:1031/2330 train_time:60209ms step_avg:58.40ms
step:1032/2330 train_time:60270ms step_avg:58.40ms
step:1033/2330 train_time:60327ms step_avg:58.40ms
step:1034/2330 train_time:60390ms step_avg:58.40ms
step:1035/2330 train_time:60447ms step_avg:58.40ms
step:1036/2330 train_time:60511ms step_avg:58.41ms
step:1037/2330 train_time:60569ms step_avg:58.41ms
step:1038/2330 train_time:60631ms step_avg:58.41ms
step:1039/2330 train_time:60688ms step_avg:58.41ms
step:1040/2330 train_time:60750ms step_avg:58.41ms
step:1041/2330 train_time:60808ms step_avg:58.41ms
step:1042/2330 train_time:60868ms step_avg:58.41ms
step:1043/2330 train_time:60925ms step_avg:58.41ms
step:1044/2330 train_time:60985ms step_avg:58.41ms
step:1045/2330 train_time:61043ms step_avg:58.41ms
step:1046/2330 train_time:61103ms step_avg:58.42ms
step:1047/2330 train_time:61160ms step_avg:58.41ms
step:1048/2330 train_time:61220ms step_avg:58.42ms
step:1049/2330 train_time:61276ms step_avg:58.41ms
step:1050/2330 train_time:61337ms step_avg:58.42ms
step:1051/2330 train_time:61394ms step_avg:58.42ms
step:1052/2330 train_time:61457ms step_avg:58.42ms
step:1053/2330 train_time:61513ms step_avg:58.42ms
step:1054/2330 train_time:61576ms step_avg:58.42ms
step:1055/2330 train_time:61632ms step_avg:58.42ms
step:1056/2330 train_time:61695ms step_avg:58.42ms
step:1057/2330 train_time:61752ms step_avg:58.42ms
step:1058/2330 train_time:61814ms step_avg:58.43ms
step:1059/2330 train_time:61870ms step_avg:58.42ms
step:1060/2330 train_time:61932ms step_avg:58.43ms
step:1061/2330 train_time:61989ms step_avg:58.42ms
step:1062/2330 train_time:62051ms step_avg:58.43ms
step:1063/2330 train_time:62107ms step_avg:58.43ms
step:1064/2330 train_time:62168ms step_avg:58.43ms
step:1065/2330 train_time:62225ms step_avg:58.43ms
step:1066/2330 train_time:62286ms step_avg:58.43ms
step:1067/2330 train_time:62343ms step_avg:58.43ms
step:1068/2330 train_time:62404ms step_avg:58.43ms
step:1069/2330 train_time:62461ms step_avg:58.43ms
step:1070/2330 train_time:62522ms step_avg:58.43ms
step:1071/2330 train_time:62579ms step_avg:58.43ms
step:1072/2330 train_time:62639ms step_avg:58.43ms
step:1073/2330 train_time:62696ms step_avg:58.43ms
step:1074/2330 train_time:62758ms step_avg:58.43ms
step:1075/2330 train_time:62814ms step_avg:58.43ms
step:1076/2330 train_time:62876ms step_avg:58.44ms
step:1077/2330 train_time:62933ms step_avg:58.43ms
step:1078/2330 train_time:62995ms step_avg:58.44ms
step:1079/2330 train_time:63051ms step_avg:58.43ms
step:1080/2330 train_time:63112ms step_avg:58.44ms
step:1081/2330 train_time:63169ms step_avg:58.44ms
step:1082/2330 train_time:63231ms step_avg:58.44ms
step:1083/2330 train_time:63288ms step_avg:58.44ms
step:1084/2330 train_time:63350ms step_avg:58.44ms
step:1085/2330 train_time:63407ms step_avg:58.44ms
step:1086/2330 train_time:63468ms step_avg:58.44ms
step:1087/2330 train_time:63526ms step_avg:58.44ms
step:1088/2330 train_time:63588ms step_avg:58.44ms
step:1089/2330 train_time:63645ms step_avg:58.44ms
step:1090/2330 train_time:63707ms step_avg:58.45ms
step:1091/2330 train_time:63764ms step_avg:58.45ms
step:1092/2330 train_time:63825ms step_avg:58.45ms
step:1093/2330 train_time:63882ms step_avg:58.45ms
step:1094/2330 train_time:63943ms step_avg:58.45ms
step:1095/2330 train_time:63999ms step_avg:58.45ms
step:1096/2330 train_time:64060ms step_avg:58.45ms
step:1097/2330 train_time:64116ms step_avg:58.45ms
step:1098/2330 train_time:64177ms step_avg:58.45ms
step:1099/2330 train_time:64234ms step_avg:58.45ms
step:1100/2330 train_time:64296ms step_avg:58.45ms
step:1101/2330 train_time:64353ms step_avg:58.45ms
step:1102/2330 train_time:64415ms step_avg:58.45ms
step:1103/2330 train_time:64472ms step_avg:58.45ms
step:1104/2330 train_time:64533ms step_avg:58.45ms
step:1105/2330 train_time:64589ms step_avg:58.45ms
step:1106/2330 train_time:64652ms step_avg:58.46ms
step:1107/2330 train_time:64709ms step_avg:58.45ms
step:1108/2330 train_time:64771ms step_avg:58.46ms
step:1109/2330 train_time:64828ms step_avg:58.46ms
step:1110/2330 train_time:64890ms step_avg:58.46ms
step:1111/2330 train_time:64947ms step_avg:58.46ms
step:1112/2330 train_time:65008ms step_avg:58.46ms
step:1113/2330 train_time:65065ms step_avg:58.46ms
step:1114/2330 train_time:65125ms step_avg:58.46ms
step:1115/2330 train_time:65183ms step_avg:58.46ms
step:1116/2330 train_time:65244ms step_avg:58.46ms
step:1117/2330 train_time:65301ms step_avg:58.46ms
step:1118/2330 train_time:65362ms step_avg:58.46ms
step:1119/2330 train_time:65418ms step_avg:58.46ms
step:1120/2330 train_time:65479ms step_avg:58.46ms
step:1121/2330 train_time:65535ms step_avg:58.46ms
step:1122/2330 train_time:65597ms step_avg:58.46ms
step:1123/2330 train_time:65653ms step_avg:58.46ms
step:1124/2330 train_time:65715ms step_avg:58.47ms
step:1125/2330 train_time:65772ms step_avg:58.46ms
step:1126/2330 train_time:65834ms step_avg:58.47ms
step:1127/2330 train_time:65891ms step_avg:58.47ms
step:1128/2330 train_time:65954ms step_avg:58.47ms
step:1129/2330 train_time:66010ms step_avg:58.47ms
step:1130/2330 train_time:66073ms step_avg:58.47ms
step:1131/2330 train_time:66129ms step_avg:58.47ms
step:1132/2330 train_time:66191ms step_avg:58.47ms
step:1133/2330 train_time:66248ms step_avg:58.47ms
step:1134/2330 train_time:66310ms step_avg:58.47ms
step:1135/2330 train_time:66367ms step_avg:58.47ms
step:1136/2330 train_time:66429ms step_avg:58.48ms
step:1137/2330 train_time:66487ms step_avg:58.48ms
step:1138/2330 train_time:66547ms step_avg:58.48ms
step:1139/2330 train_time:66605ms step_avg:58.48ms
step:1140/2330 train_time:66665ms step_avg:58.48ms
step:1141/2330 train_time:66723ms step_avg:58.48ms
step:1142/2330 train_time:66783ms step_avg:58.48ms
step:1143/2330 train_time:66841ms step_avg:58.48ms
step:1144/2330 train_time:66901ms step_avg:58.48ms
step:1145/2330 train_time:66958ms step_avg:58.48ms
step:1146/2330 train_time:67018ms step_avg:58.48ms
step:1147/2330 train_time:67074ms step_avg:58.48ms
step:1148/2330 train_time:67137ms step_avg:58.48ms
step:1149/2330 train_time:67193ms step_avg:58.48ms
step:1150/2330 train_time:67255ms step_avg:58.48ms
step:1151/2330 train_time:67311ms step_avg:58.48ms
step:1152/2330 train_time:67374ms step_avg:58.48ms
step:1153/2330 train_time:67430ms step_avg:58.48ms
step:1154/2330 train_time:67494ms step_avg:58.49ms
step:1155/2330 train_time:67550ms step_avg:58.49ms
step:1156/2330 train_time:67613ms step_avg:58.49ms
step:1157/2330 train_time:67669ms step_avg:58.49ms
step:1158/2330 train_time:67732ms step_avg:58.49ms
step:1159/2330 train_time:67789ms step_avg:58.49ms
step:1160/2330 train_time:67852ms step_avg:58.49ms
step:1161/2330 train_time:67909ms step_avg:58.49ms
step:1162/2330 train_time:67971ms step_avg:58.49ms
step:1163/2330 train_time:68029ms step_avg:58.49ms
step:1164/2330 train_time:68089ms step_avg:58.50ms
step:1165/2330 train_time:68146ms step_avg:58.49ms
step:1166/2330 train_time:68207ms step_avg:58.50ms
step:1167/2330 train_time:68264ms step_avg:58.50ms
step:1168/2330 train_time:68325ms step_avg:58.50ms
step:1169/2330 train_time:68382ms step_avg:58.50ms
step:1170/2330 train_time:68442ms step_avg:58.50ms
step:1171/2330 train_time:68499ms step_avg:58.50ms
step:1172/2330 train_time:68560ms step_avg:58.50ms
step:1173/2330 train_time:68617ms step_avg:58.50ms
step:1174/2330 train_time:68677ms step_avg:58.50ms
step:1175/2330 train_time:68734ms step_avg:58.50ms
step:1176/2330 train_time:68797ms step_avg:58.50ms
step:1177/2330 train_time:68853ms step_avg:58.50ms
step:1178/2330 train_time:68915ms step_avg:58.50ms
step:1179/2330 train_time:68971ms step_avg:58.50ms
step:1180/2330 train_time:69033ms step_avg:58.50ms
step:1181/2330 train_time:69089ms step_avg:58.50ms
step:1182/2330 train_time:69152ms step_avg:58.50ms
step:1183/2330 train_time:69208ms step_avg:58.50ms
step:1184/2330 train_time:69271ms step_avg:58.51ms
step:1185/2330 train_time:69328ms step_avg:58.50ms
step:1186/2330 train_time:69389ms step_avg:58.51ms
step:1187/2330 train_time:69446ms step_avg:58.51ms
step:1188/2330 train_time:69509ms step_avg:58.51ms
step:1189/2330 train_time:69565ms step_avg:58.51ms
step:1190/2330 train_time:69627ms step_avg:58.51ms
step:1191/2330 train_time:69684ms step_avg:58.51ms
step:1192/2330 train_time:69746ms step_avg:58.51ms
step:1193/2330 train_time:69804ms step_avg:58.51ms
step:1194/2330 train_time:69864ms step_avg:58.51ms
step:1195/2330 train_time:69921ms step_avg:58.51ms
step:1196/2330 train_time:69982ms step_avg:58.51ms
step:1197/2330 train_time:70039ms step_avg:58.51ms
step:1198/2330 train_time:70100ms step_avg:58.51ms
step:1199/2330 train_time:70156ms step_avg:58.51ms
step:1200/2330 train_time:70218ms step_avg:58.51ms
step:1201/2330 train_time:70274ms step_avg:58.51ms
step:1202/2330 train_time:70336ms step_avg:58.52ms
step:1203/2330 train_time:70392ms step_avg:58.51ms
step:1204/2330 train_time:70454ms step_avg:58.52ms
step:1205/2330 train_time:70511ms step_avg:58.52ms
step:1206/2330 train_time:70573ms step_avg:58.52ms
step:1207/2330 train_time:70629ms step_avg:58.52ms
step:1208/2330 train_time:70692ms step_avg:58.52ms
step:1209/2330 train_time:70748ms step_avg:58.52ms
step:1210/2330 train_time:70812ms step_avg:58.52ms
step:1211/2330 train_time:70869ms step_avg:58.52ms
step:1212/2330 train_time:70932ms step_avg:58.52ms
step:1213/2330 train_time:70989ms step_avg:58.52ms
step:1214/2330 train_time:71050ms step_avg:58.53ms
step:1215/2330 train_time:71107ms step_avg:58.52ms
step:1216/2330 train_time:71169ms step_avg:58.53ms
step:1217/2330 train_time:71226ms step_avg:58.53ms
step:1218/2330 train_time:71287ms step_avg:58.53ms
step:1219/2330 train_time:71344ms step_avg:58.53ms
step:1220/2330 train_time:71405ms step_avg:58.53ms
step:1221/2330 train_time:71461ms step_avg:58.53ms
step:1222/2330 train_time:71523ms step_avg:58.53ms
step:1223/2330 train_time:71579ms step_avg:58.53ms
step:1224/2330 train_time:71640ms step_avg:58.53ms
step:1225/2330 train_time:71696ms step_avg:58.53ms
step:1226/2330 train_time:71758ms step_avg:58.53ms
step:1227/2330 train_time:71815ms step_avg:58.53ms
step:1228/2330 train_time:71877ms step_avg:58.53ms
step:1229/2330 train_time:71933ms step_avg:58.53ms
step:1230/2330 train_time:71996ms step_avg:58.53ms
step:1231/2330 train_time:72052ms step_avg:58.53ms
step:1232/2330 train_time:72116ms step_avg:58.54ms
step:1233/2330 train_time:72172ms step_avg:58.53ms
step:1234/2330 train_time:72234ms step_avg:58.54ms
step:1235/2330 train_time:72290ms step_avg:58.53ms
step:1236/2330 train_time:72353ms step_avg:58.54ms
step:1237/2330 train_time:72409ms step_avg:58.54ms
step:1238/2330 train_time:72471ms step_avg:58.54ms
step:1239/2330 train_time:72528ms step_avg:58.54ms
step:1240/2330 train_time:72590ms step_avg:58.54ms
step:1241/2330 train_time:72647ms step_avg:58.54ms
step:1242/2330 train_time:72709ms step_avg:58.54ms
step:1243/2330 train_time:72766ms step_avg:58.54ms
step:1244/2330 train_time:72827ms step_avg:58.54ms
step:1245/2330 train_time:72885ms step_avg:58.54ms
step:1246/2330 train_time:72946ms step_avg:58.54ms
step:1247/2330 train_time:73003ms step_avg:58.54ms
step:1248/2330 train_time:73064ms step_avg:58.54ms
step:1249/2330 train_time:73121ms step_avg:58.54ms
step:1250/2330 train_time:73182ms step_avg:58.55ms
step:1250/2330 val_loss:4.4091 train_time:73260ms step_avg:58.61ms
step:1251/2330 train_time:73278ms step_avg:58.58ms
step:1252/2330 train_time:73302ms step_avg:58.55ms
step:1253/2330 train_time:73359ms step_avg:58.55ms
step:1254/2330 train_time:73424ms step_avg:58.55ms
step:1255/2330 train_time:73482ms step_avg:58.55ms
step:1256/2330 train_time:73543ms step_avg:58.55ms
step:1257/2330 train_time:73600ms step_avg:58.55ms
step:1258/2330 train_time:73660ms step_avg:58.55ms
step:1259/2330 train_time:73716ms step_avg:58.55ms
step:1260/2330 train_time:73777ms step_avg:58.55ms
step:1261/2330 train_time:73834ms step_avg:58.55ms
step:1262/2330 train_time:73894ms step_avg:58.55ms
step:1263/2330 train_time:73950ms step_avg:58.55ms
step:1264/2330 train_time:74012ms step_avg:58.55ms
step:1265/2330 train_time:74068ms step_avg:58.55ms
step:1266/2330 train_time:74129ms step_avg:58.55ms
step:1267/2330 train_time:74185ms step_avg:58.55ms
step:1268/2330 train_time:74246ms step_avg:58.55ms
step:1269/2330 train_time:74303ms step_avg:58.55ms
step:1270/2330 train_time:74366ms step_avg:58.56ms
step:1271/2330 train_time:74424ms step_avg:58.56ms
step:1272/2330 train_time:74486ms step_avg:58.56ms
step:1273/2330 train_time:74543ms step_avg:58.56ms
step:1274/2330 train_time:74603ms step_avg:58.56ms
step:1275/2330 train_time:74660ms step_avg:58.56ms
step:1276/2330 train_time:74721ms step_avg:58.56ms
step:1277/2330 train_time:74777ms step_avg:58.56ms
step:1278/2330 train_time:74838ms step_avg:58.56ms
step:1279/2330 train_time:74894ms step_avg:58.56ms
step:1280/2330 train_time:74955ms step_avg:58.56ms
step:1281/2330 train_time:75012ms step_avg:58.56ms
step:1282/2330 train_time:75073ms step_avg:58.56ms
step:1283/2330 train_time:75129ms step_avg:58.56ms
step:1284/2330 train_time:75190ms step_avg:58.56ms
step:1285/2330 train_time:75246ms step_avg:58.56ms
step:1286/2330 train_time:75309ms step_avg:58.56ms
step:1287/2330 train_time:75365ms step_avg:58.56ms
step:1288/2330 train_time:75429ms step_avg:58.56ms
step:1289/2330 train_time:75487ms step_avg:58.56ms
step:1290/2330 train_time:75548ms step_avg:58.56ms
step:1291/2330 train_time:75605ms step_avg:58.56ms
step:1292/2330 train_time:75667ms step_avg:58.57ms
step:1293/2330 train_time:75725ms step_avg:58.56ms
step:1294/2330 train_time:75785ms step_avg:58.57ms
step:1295/2330 train_time:75843ms step_avg:58.57ms
step:1296/2330 train_time:75903ms step_avg:58.57ms
step:1297/2330 train_time:75960ms step_avg:58.57ms
step:1298/2330 train_time:76020ms step_avg:58.57ms
step:1299/2330 train_time:76077ms step_avg:58.57ms
step:1300/2330 train_time:76138ms step_avg:58.57ms
step:1301/2330 train_time:76194ms step_avg:58.57ms
step:1302/2330 train_time:76257ms step_avg:58.57ms
step:1303/2330 train_time:76313ms step_avg:58.57ms
step:1304/2330 train_time:76375ms step_avg:58.57ms
step:1305/2330 train_time:76432ms step_avg:58.57ms
step:1306/2330 train_time:76495ms step_avg:58.57ms
step:1307/2330 train_time:76551ms step_avg:58.57ms
step:1308/2330 train_time:76614ms step_avg:58.57ms
step:1309/2330 train_time:76671ms step_avg:58.57ms
step:1310/2330 train_time:76733ms step_avg:58.57ms
step:1311/2330 train_time:76790ms step_avg:58.57ms
step:1312/2330 train_time:76852ms step_avg:58.58ms
step:1313/2330 train_time:76909ms step_avg:58.57ms
step:1314/2330 train_time:76971ms step_avg:58.58ms
step:1315/2330 train_time:77028ms step_avg:58.58ms
step:1316/2330 train_time:77088ms step_avg:58.58ms
step:1317/2330 train_time:77146ms step_avg:58.58ms
step:1318/2330 train_time:77206ms step_avg:58.58ms
step:1319/2330 train_time:77263ms step_avg:58.58ms
step:1320/2330 train_time:77324ms step_avg:58.58ms
step:1321/2330 train_time:77381ms step_avg:58.58ms
step:1322/2330 train_time:77442ms step_avg:58.58ms
step:1323/2330 train_time:77499ms step_avg:58.58ms
step:1324/2330 train_time:77559ms step_avg:58.58ms
step:1325/2330 train_time:77615ms step_avg:58.58ms
step:1326/2330 train_time:77677ms step_avg:58.58ms
step:1327/2330 train_time:77734ms step_avg:58.58ms
step:1328/2330 train_time:77796ms step_avg:58.58ms
step:1329/2330 train_time:77853ms step_avg:58.58ms
step:1330/2330 train_time:77913ms step_avg:58.58ms
step:1331/2330 train_time:77970ms step_avg:58.58ms
step:1332/2330 train_time:78032ms step_avg:58.58ms
step:1333/2330 train_time:78088ms step_avg:58.58ms
step:1334/2330 train_time:78151ms step_avg:58.58ms
step:1335/2330 train_time:78207ms step_avg:58.58ms
step:1336/2330 train_time:78270ms step_avg:58.58ms
step:1337/2330 train_time:78326ms step_avg:58.58ms
step:1338/2330 train_time:78388ms step_avg:58.59ms
step:1339/2330 train_time:78445ms step_avg:58.58ms
step:1340/2330 train_time:78507ms step_avg:58.59ms
step:1341/2330 train_time:78564ms step_avg:58.59ms
step:1342/2330 train_time:78625ms step_avg:58.59ms
step:1343/2330 train_time:78682ms step_avg:58.59ms
step:1344/2330 train_time:78743ms step_avg:58.59ms
step:1345/2330 train_time:78800ms step_avg:58.59ms
step:1346/2330 train_time:78861ms step_avg:58.59ms
step:1347/2330 train_time:78917ms step_avg:58.59ms
step:1348/2330 train_time:78978ms step_avg:58.59ms
step:1349/2330 train_time:79035ms step_avg:58.59ms
step:1350/2330 train_time:79097ms step_avg:58.59ms
step:1351/2330 train_time:79153ms step_avg:58.59ms
step:1352/2330 train_time:79214ms step_avg:58.59ms
step:1353/2330 train_time:79271ms step_avg:58.59ms
step:1354/2330 train_time:79333ms step_avg:58.59ms
step:1355/2330 train_time:79389ms step_avg:58.59ms
step:1356/2330 train_time:79451ms step_avg:58.59ms
step:1357/2330 train_time:79508ms step_avg:58.59ms
step:1358/2330 train_time:79570ms step_avg:58.59ms
step:1359/2330 train_time:79627ms step_avg:58.59ms
step:1360/2330 train_time:79689ms step_avg:58.59ms
step:1361/2330 train_time:79746ms step_avg:58.59ms
step:1362/2330 train_time:79807ms step_avg:58.60ms
step:1363/2330 train_time:79864ms step_avg:58.59ms
step:1364/2330 train_time:79925ms step_avg:58.60ms
step:1365/2330 train_time:79983ms step_avg:58.60ms
step:1366/2330 train_time:80043ms step_avg:58.60ms
step:1367/2330 train_time:80100ms step_avg:58.60ms
step:1368/2330 train_time:80162ms step_avg:58.60ms
step:1369/2330 train_time:80219ms step_avg:58.60ms
step:1370/2330 train_time:80279ms step_avg:58.60ms
step:1371/2330 train_time:80336ms step_avg:58.60ms
step:1372/2330 train_time:80398ms step_avg:58.60ms
step:1373/2330 train_time:80454ms step_avg:58.60ms
step:1374/2330 train_time:80516ms step_avg:58.60ms
step:1375/2330 train_time:80573ms step_avg:58.60ms
step:1376/2330 train_time:80634ms step_avg:58.60ms
step:1377/2330 train_time:80690ms step_avg:58.60ms
step:1378/2330 train_time:80753ms step_avg:58.60ms
step:1379/2330 train_time:80810ms step_avg:58.60ms
step:1380/2330 train_time:80872ms step_avg:58.60ms
step:1381/2330 train_time:80928ms step_avg:58.60ms
step:1382/2330 train_time:80990ms step_avg:58.60ms
step:1383/2330 train_time:81048ms step_avg:58.60ms
step:1384/2330 train_time:81109ms step_avg:58.60ms
step:1385/2330 train_time:81167ms step_avg:58.60ms
step:1386/2330 train_time:81227ms step_avg:58.61ms
step:1387/2330 train_time:81284ms step_avg:58.60ms
step:1388/2330 train_time:81345ms step_avg:58.61ms
step:1389/2330 train_time:81402ms step_avg:58.60ms
step:1390/2330 train_time:81463ms step_avg:58.61ms
step:1391/2330 train_time:81520ms step_avg:58.61ms
step:1392/2330 train_time:81580ms step_avg:58.61ms
step:1393/2330 train_time:81637ms step_avg:58.60ms
step:1394/2330 train_time:81698ms step_avg:58.61ms
step:1395/2330 train_time:81754ms step_avg:58.61ms
step:1396/2330 train_time:81816ms step_avg:58.61ms
step:1397/2330 train_time:81872ms step_avg:58.61ms
step:1398/2330 train_time:81934ms step_avg:58.61ms
step:1399/2330 train_time:81990ms step_avg:58.61ms
step:1400/2330 train_time:82052ms step_avg:58.61ms
step:1401/2330 train_time:82108ms step_avg:58.61ms
step:1402/2330 train_time:82171ms step_avg:58.61ms
step:1403/2330 train_time:82228ms step_avg:58.61ms
step:1404/2330 train_time:82289ms step_avg:58.61ms
step:1405/2330 train_time:82345ms step_avg:58.61ms
step:1406/2330 train_time:82408ms step_avg:58.61ms
step:1407/2330 train_time:82465ms step_avg:58.61ms
step:1408/2330 train_time:82525ms step_avg:58.61ms
step:1409/2330 train_time:82582ms step_avg:58.61ms
step:1410/2330 train_time:82643ms step_avg:58.61ms
step:1411/2330 train_time:82700ms step_avg:58.61ms
step:1412/2330 train_time:82761ms step_avg:58.61ms
step:1413/2330 train_time:82818ms step_avg:58.61ms
step:1414/2330 train_time:82879ms step_avg:58.61ms
step:1415/2330 train_time:82935ms step_avg:58.61ms
step:1416/2330 train_time:82996ms step_avg:58.61ms
step:1417/2330 train_time:83052ms step_avg:58.61ms
step:1418/2330 train_time:83115ms step_avg:58.61ms
step:1419/2330 train_time:83171ms step_avg:58.61ms
step:1420/2330 train_time:83233ms step_avg:58.61ms
step:1421/2330 train_time:83289ms step_avg:58.61ms
step:1422/2330 train_time:83351ms step_avg:58.62ms
step:1423/2330 train_time:83408ms step_avg:58.61ms
step:1424/2330 train_time:83470ms step_avg:58.62ms
step:1425/2330 train_time:83526ms step_avg:58.61ms
step:1426/2330 train_time:83589ms step_avg:58.62ms
step:1427/2330 train_time:83647ms step_avg:58.62ms
step:1428/2330 train_time:83710ms step_avg:58.62ms
step:1429/2330 train_time:83767ms step_avg:58.62ms
step:1430/2330 train_time:83828ms step_avg:58.62ms
step:1431/2330 train_time:83885ms step_avg:58.62ms
step:1432/2330 train_time:83946ms step_avg:58.62ms
step:1433/2330 train_time:84003ms step_avg:58.62ms
step:1434/2330 train_time:84064ms step_avg:58.62ms
step:1435/2330 train_time:84120ms step_avg:58.62ms
step:1436/2330 train_time:84181ms step_avg:58.62ms
step:1437/2330 train_time:84238ms step_avg:58.62ms
step:1438/2330 train_time:84299ms step_avg:58.62ms
step:1439/2330 train_time:84355ms step_avg:58.62ms
step:1440/2330 train_time:84417ms step_avg:58.62ms
step:1441/2330 train_time:84473ms step_avg:58.62ms
step:1442/2330 train_time:84535ms step_avg:58.62ms
step:1443/2330 train_time:84592ms step_avg:58.62ms
step:1444/2330 train_time:84654ms step_avg:58.62ms
step:1445/2330 train_time:84711ms step_avg:58.62ms
step:1446/2330 train_time:84773ms step_avg:58.63ms
step:1447/2330 train_time:84830ms step_avg:58.62ms
step:1448/2330 train_time:84891ms step_avg:58.63ms
step:1449/2330 train_time:84947ms step_avg:58.62ms
step:1450/2330 train_time:85010ms step_avg:58.63ms
step:1451/2330 train_time:85067ms step_avg:58.63ms
step:1452/2330 train_time:85128ms step_avg:58.63ms
step:1453/2330 train_time:85185ms step_avg:58.63ms
step:1454/2330 train_time:85246ms step_avg:58.63ms
step:1455/2330 train_time:85304ms step_avg:58.63ms
step:1456/2330 train_time:85364ms step_avg:58.63ms
step:1457/2330 train_time:85422ms step_avg:58.63ms
step:1458/2330 train_time:85482ms step_avg:58.63ms
step:1459/2330 train_time:85539ms step_avg:58.63ms
step:1460/2330 train_time:85600ms step_avg:58.63ms
step:1461/2330 train_time:85656ms step_avg:58.63ms
step:1462/2330 train_time:85718ms step_avg:58.63ms
step:1463/2330 train_time:85775ms step_avg:58.63ms
step:1464/2330 train_time:85836ms step_avg:58.63ms
step:1465/2330 train_time:85892ms step_avg:58.63ms
step:1466/2330 train_time:85954ms step_avg:58.63ms
step:1467/2330 train_time:86010ms step_avg:58.63ms
step:1468/2330 train_time:86072ms step_avg:58.63ms
step:1469/2330 train_time:86128ms step_avg:58.63ms
step:1470/2330 train_time:86190ms step_avg:58.63ms
step:1471/2330 train_time:86247ms step_avg:58.63ms
step:1472/2330 train_time:86308ms step_avg:58.63ms
step:1473/2330 train_time:86365ms step_avg:58.63ms
step:1474/2330 train_time:86427ms step_avg:58.63ms
step:1475/2330 train_time:86484ms step_avg:58.63ms
step:1476/2330 train_time:86545ms step_avg:58.63ms
step:1477/2330 train_time:86603ms step_avg:58.63ms
step:1478/2330 train_time:86663ms step_avg:58.64ms
step:1479/2330 train_time:86720ms step_avg:58.63ms
step:1480/2330 train_time:86781ms step_avg:58.64ms
step:1481/2330 train_time:86838ms step_avg:58.63ms
step:1482/2330 train_time:86899ms step_avg:58.64ms
step:1483/2330 train_time:86955ms step_avg:58.63ms
step:1484/2330 train_time:87017ms step_avg:58.64ms
step:1485/2330 train_time:87073ms step_avg:58.64ms
step:1486/2330 train_time:87136ms step_avg:58.64ms
step:1487/2330 train_time:87192ms step_avg:58.64ms
step:1488/2330 train_time:87255ms step_avg:58.64ms
step:1489/2330 train_time:87311ms step_avg:58.64ms
step:1490/2330 train_time:87373ms step_avg:58.64ms
step:1491/2330 train_time:87429ms step_avg:58.64ms
step:1492/2330 train_time:87492ms step_avg:58.64ms
step:1493/2330 train_time:87548ms step_avg:58.64ms
step:1494/2330 train_time:87611ms step_avg:58.64ms
step:1495/2330 train_time:87668ms step_avg:58.64ms
step:1496/2330 train_time:87729ms step_avg:58.64ms
step:1497/2330 train_time:87786ms step_avg:58.64ms
step:1498/2330 train_time:87847ms step_avg:58.64ms
step:1499/2330 train_time:87904ms step_avg:58.64ms
step:1500/2330 train_time:87965ms step_avg:58.64ms
step:1500/2330 val_loss:4.2944 train_time:88043ms step_avg:58.70ms
step:1501/2330 train_time:88061ms step_avg:58.67ms
step:1502/2330 train_time:88085ms step_avg:58.65ms
step:1503/2330 train_time:88143ms step_avg:58.64ms
step:1504/2330 train_time:88208ms step_avg:58.65ms
step:1505/2330 train_time:88265ms step_avg:58.65ms
step:1506/2330 train_time:88327ms step_avg:58.65ms
step:1507/2330 train_time:88384ms step_avg:58.65ms
step:1508/2330 train_time:88444ms step_avg:58.65ms
step:1509/2330 train_time:88501ms step_avg:58.65ms
step:1510/2330 train_time:88562ms step_avg:58.65ms
step:1511/2330 train_time:88618ms step_avg:58.65ms
step:1512/2330 train_time:88678ms step_avg:58.65ms
step:1513/2330 train_time:88734ms step_avg:58.65ms
step:1514/2330 train_time:88795ms step_avg:58.65ms
step:1515/2330 train_time:88851ms step_avg:58.65ms
step:1516/2330 train_time:88912ms step_avg:58.65ms
step:1517/2330 train_time:88968ms step_avg:58.65ms
step:1518/2330 train_time:89030ms step_avg:58.65ms
step:1519/2330 train_time:89087ms step_avg:58.65ms
step:1520/2330 train_time:89150ms step_avg:58.65ms
step:1521/2330 train_time:89207ms step_avg:58.65ms
step:1522/2330 train_time:89270ms step_avg:58.65ms
step:1523/2330 train_time:89327ms step_avg:58.65ms
step:1524/2330 train_time:89389ms step_avg:58.65ms
step:1525/2330 train_time:89446ms step_avg:58.65ms
step:1526/2330 train_time:89509ms step_avg:58.66ms
step:1527/2330 train_time:89566ms step_avg:58.65ms
step:1528/2330 train_time:89626ms step_avg:58.66ms
step:1529/2330 train_time:89684ms step_avg:58.66ms
step:1530/2330 train_time:89744ms step_avg:58.66ms
step:1531/2330 train_time:89801ms step_avg:58.66ms
step:1532/2330 train_time:89861ms step_avg:58.66ms
step:1533/2330 train_time:89919ms step_avg:58.66ms
step:1534/2330 train_time:89980ms step_avg:58.66ms
step:1535/2330 train_time:90037ms step_avg:58.66ms
step:1536/2330 train_time:90098ms step_avg:58.66ms
step:1537/2330 train_time:90155ms step_avg:58.66ms
step:1538/2330 train_time:90218ms step_avg:58.66ms
step:1539/2330 train_time:90275ms step_avg:58.66ms
step:1540/2330 train_time:90338ms step_avg:58.66ms
step:1541/2330 train_time:90395ms step_avg:58.66ms
step:1542/2330 train_time:90457ms step_avg:58.66ms
step:1543/2330 train_time:90514ms step_avg:58.66ms
step:1544/2330 train_time:90577ms step_avg:58.66ms
step:1545/2330 train_time:90633ms step_avg:58.66ms
step:1546/2330 train_time:90696ms step_avg:58.66ms
step:1547/2330 train_time:90752ms step_avg:58.66ms
step:1548/2330 train_time:90815ms step_avg:58.67ms
step:1549/2330 train_time:90871ms step_avg:58.66ms
step:1550/2330 train_time:90934ms step_avg:58.67ms
step:1551/2330 train_time:90991ms step_avg:58.67ms
step:1552/2330 train_time:91053ms step_avg:58.67ms
step:1553/2330 train_time:91110ms step_avg:58.67ms
step:1554/2330 train_time:91172ms step_avg:58.67ms
step:1555/2330 train_time:91229ms step_avg:58.67ms
step:1556/2330 train_time:91292ms step_avg:58.67ms
step:1557/2330 train_time:91349ms step_avg:58.67ms
step:1558/2330 train_time:91412ms step_avg:58.67ms
step:1559/2330 train_time:91469ms step_avg:58.67ms
step:1560/2330 train_time:91531ms step_avg:58.67ms
step:1561/2330 train_time:91588ms step_avg:58.67ms
step:1562/2330 train_time:91651ms step_avg:58.68ms
step:1563/2330 train_time:91708ms step_avg:58.67ms
step:1564/2330 train_time:91770ms step_avg:58.68ms
step:1565/2330 train_time:91827ms step_avg:58.68ms
step:1566/2330 train_time:91889ms step_avg:58.68ms
step:1567/2330 train_time:91946ms step_avg:58.68ms
step:1568/2330 train_time:92009ms step_avg:58.68ms
step:1569/2330 train_time:92066ms step_avg:58.68ms
step:1570/2330 train_time:92127ms step_avg:58.68ms
step:1571/2330 train_time:92184ms step_avg:58.68ms
step:1572/2330 train_time:92246ms step_avg:58.68ms
step:1573/2330 train_time:92304ms step_avg:58.68ms
step:1574/2330 train_time:92365ms step_avg:58.68ms
step:1575/2330 train_time:92424ms step_avg:58.68ms
step:1576/2330 train_time:92484ms step_avg:58.68ms
step:1577/2330 train_time:92542ms step_avg:58.68ms
step:1578/2330 train_time:92604ms step_avg:58.68ms
step:1579/2330 train_time:92661ms step_avg:58.68ms
step:1580/2330 train_time:92722ms step_avg:58.68ms
step:1581/2330 train_time:92779ms step_avg:58.68ms
step:1582/2330 train_time:92840ms step_avg:58.69ms
step:1583/2330 train_time:92897ms step_avg:58.68ms
step:1584/2330 train_time:92959ms step_avg:58.69ms
step:1585/2330 train_time:93016ms step_avg:58.69ms
step:1586/2330 train_time:93077ms step_avg:58.69ms
step:1587/2330 train_time:93134ms step_avg:58.69ms
step:1588/2330 train_time:93197ms step_avg:58.69ms
step:1589/2330 train_time:93253ms step_avg:58.69ms
step:1590/2330 train_time:93316ms step_avg:58.69ms
step:1591/2330 train_time:93373ms step_avg:58.69ms
step:1592/2330 train_time:93437ms step_avg:58.69ms
step:1593/2330 train_time:93493ms step_avg:58.69ms
step:1594/2330 train_time:93557ms step_avg:58.69ms
step:1595/2330 train_time:93613ms step_avg:58.69ms
step:1596/2330 train_time:93675ms step_avg:58.69ms
step:1597/2330 train_time:93732ms step_avg:58.69ms
step:1598/2330 train_time:93794ms step_avg:58.69ms
step:1599/2330 train_time:93851ms step_avg:58.69ms
step:1600/2330 train_time:93914ms step_avg:58.70ms
step:1601/2330 train_time:93970ms step_avg:58.69ms
step:1602/2330 train_time:94032ms step_avg:58.70ms
step:1603/2330 train_time:94089ms step_avg:58.70ms
step:1604/2330 train_time:94152ms step_avg:58.70ms
step:1605/2330 train_time:94209ms step_avg:58.70ms
step:1606/2330 train_time:94270ms step_avg:58.70ms
step:1607/2330 train_time:94327ms step_avg:58.70ms
step:1608/2330 train_time:94390ms step_avg:58.70ms
step:1609/2330 train_time:94447ms step_avg:58.70ms
step:1610/2330 train_time:94510ms step_avg:58.70ms
step:1611/2330 train_time:94567ms step_avg:58.70ms
step:1612/2330 train_time:94629ms step_avg:58.70ms
step:1613/2330 train_time:94686ms step_avg:58.70ms
step:1614/2330 train_time:94749ms step_avg:58.70ms
step:1615/2330 train_time:94806ms step_avg:58.70ms
step:1616/2330 train_time:94867ms step_avg:58.71ms
step:1617/2330 train_time:94924ms step_avg:58.70ms
step:1618/2330 train_time:94987ms step_avg:58.71ms
step:1619/2330 train_time:95044ms step_avg:58.71ms
step:1620/2330 train_time:95106ms step_avg:58.71ms
step:1621/2330 train_time:95163ms step_avg:58.71ms
step:1622/2330 train_time:95224ms step_avg:58.71ms
step:1623/2330 train_time:95282ms step_avg:58.71ms
step:1624/2330 train_time:95343ms step_avg:58.71ms
step:1625/2330 train_time:95400ms step_avg:58.71ms
step:1626/2330 train_time:95461ms step_avg:58.71ms
step:1627/2330 train_time:95519ms step_avg:58.71ms
step:1628/2330 train_time:95581ms step_avg:58.71ms
step:1629/2330 train_time:95638ms step_avg:58.71ms
step:1630/2330 train_time:95699ms step_avg:58.71ms
step:1631/2330 train_time:95756ms step_avg:58.71ms
step:1632/2330 train_time:95818ms step_avg:58.71ms
step:1633/2330 train_time:95875ms step_avg:58.71ms
step:1634/2330 train_time:95937ms step_avg:58.71ms
step:1635/2330 train_time:95993ms step_avg:58.71ms
step:1636/2330 train_time:96056ms step_avg:58.71ms
step:1637/2330 train_time:96113ms step_avg:58.71ms
step:1638/2330 train_time:96176ms step_avg:58.72ms
step:1639/2330 train_time:96232ms step_avg:58.71ms
step:1640/2330 train_time:96295ms step_avg:58.72ms
step:1641/2330 train_time:96352ms step_avg:58.72ms
step:1642/2330 train_time:96415ms step_avg:58.72ms
step:1643/2330 train_time:96472ms step_avg:58.72ms
step:1644/2330 train_time:96535ms step_avg:58.72ms
step:1645/2330 train_time:96592ms step_avg:58.72ms
step:1646/2330 train_time:96655ms step_avg:58.72ms
step:1647/2330 train_time:96712ms step_avg:58.72ms
step:1648/2330 train_time:96774ms step_avg:58.72ms
step:1649/2330 train_time:96831ms step_avg:58.72ms
step:1650/2330 train_time:96893ms step_avg:58.72ms
step:1651/2330 train_time:96950ms step_avg:58.72ms
step:1652/2330 train_time:97012ms step_avg:58.72ms
step:1653/2330 train_time:97069ms step_avg:58.72ms
step:1654/2330 train_time:97131ms step_avg:58.73ms
step:1655/2330 train_time:97188ms step_avg:58.72ms
step:1656/2330 train_time:97250ms step_avg:58.73ms
step:1657/2330 train_time:97308ms step_avg:58.73ms
step:1658/2330 train_time:97370ms step_avg:58.73ms
step:1659/2330 train_time:97428ms step_avg:58.73ms
step:1660/2330 train_time:97490ms step_avg:58.73ms
step:1661/2330 train_time:97546ms step_avg:58.73ms
step:1662/2330 train_time:97610ms step_avg:58.73ms
step:1663/2330 train_time:97667ms step_avg:58.73ms
step:1664/2330 train_time:97729ms step_avg:58.73ms
step:1665/2330 train_time:97785ms step_avg:58.73ms
step:1666/2330 train_time:97849ms step_avg:58.73ms
step:1667/2330 train_time:97906ms step_avg:58.73ms
step:1668/2330 train_time:97967ms step_avg:58.73ms
step:1669/2330 train_time:98025ms step_avg:58.73ms
step:1670/2330 train_time:98086ms step_avg:58.73ms
step:1671/2330 train_time:98143ms step_avg:58.73ms
step:1672/2330 train_time:98206ms step_avg:58.74ms
step:1673/2330 train_time:98264ms step_avg:58.74ms
step:1674/2330 train_time:98325ms step_avg:58.74ms
step:1675/2330 train_time:98383ms step_avg:58.74ms
step:1676/2330 train_time:98444ms step_avg:58.74ms
step:1677/2330 train_time:98501ms step_avg:58.74ms
step:1678/2330 train_time:98562ms step_avg:58.74ms
step:1679/2330 train_time:98619ms step_avg:58.74ms
step:1680/2330 train_time:98681ms step_avg:58.74ms
step:1681/2330 train_time:98739ms step_avg:58.74ms
step:1682/2330 train_time:98799ms step_avg:58.74ms
step:1683/2330 train_time:98856ms step_avg:58.74ms
step:1684/2330 train_time:98918ms step_avg:58.74ms
step:1685/2330 train_time:98975ms step_avg:58.74ms
step:1686/2330 train_time:99037ms step_avg:58.74ms
step:1687/2330 train_time:99093ms step_avg:58.74ms
step:1688/2330 train_time:99156ms step_avg:58.74ms
step:1689/2330 train_time:99213ms step_avg:58.74ms
step:1690/2330 train_time:99276ms step_avg:58.74ms
step:1691/2330 train_time:99333ms step_avg:58.74ms
step:1692/2330 train_time:99396ms step_avg:58.74ms
step:1693/2330 train_time:99452ms step_avg:58.74ms
step:1694/2330 train_time:99515ms step_avg:58.75ms
step:1695/2330 train_time:99571ms step_avg:58.74ms
step:1696/2330 train_time:99634ms step_avg:58.75ms
step:1697/2330 train_time:99690ms step_avg:58.75ms
step:1698/2330 train_time:99753ms step_avg:58.75ms
step:1699/2330 train_time:99810ms step_avg:58.75ms
step:1700/2330 train_time:99872ms step_avg:58.75ms
step:1701/2330 train_time:99928ms step_avg:58.75ms
step:1702/2330 train_time:99991ms step_avg:58.75ms
step:1703/2330 train_time:100048ms step_avg:58.75ms
step:1704/2330 train_time:100110ms step_avg:58.75ms
step:1705/2330 train_time:100167ms step_avg:58.75ms
step:1706/2330 train_time:100230ms step_avg:58.75ms
step:1707/2330 train_time:100287ms step_avg:58.75ms
step:1708/2330 train_time:100349ms step_avg:58.75ms
step:1709/2330 train_time:100407ms step_avg:58.75ms
step:1710/2330 train_time:100469ms step_avg:58.75ms
step:1711/2330 train_time:100526ms step_avg:58.75ms
step:1712/2330 train_time:100588ms step_avg:58.75ms
step:1713/2330 train_time:100645ms step_avg:58.75ms
step:1714/2330 train_time:100707ms step_avg:58.76ms
step:1715/2330 train_time:100764ms step_avg:58.75ms
step:1716/2330 train_time:100827ms step_avg:58.76ms
step:1717/2330 train_time:100884ms step_avg:58.76ms
step:1718/2330 train_time:100944ms step_avg:58.76ms
step:1719/2330 train_time:101002ms step_avg:58.76ms
step:1720/2330 train_time:101062ms step_avg:58.76ms
step:1721/2330 train_time:101120ms step_avg:58.76ms
step:1722/2330 train_time:101182ms step_avg:58.76ms
step:1723/2330 train_time:101239ms step_avg:58.76ms
step:1724/2330 train_time:101300ms step_avg:58.76ms
step:1725/2330 train_time:101357ms step_avg:58.76ms
step:1726/2330 train_time:101419ms step_avg:58.76ms
step:1727/2330 train_time:101476ms step_avg:58.76ms
step:1728/2330 train_time:101537ms step_avg:58.76ms
step:1729/2330 train_time:101594ms step_avg:58.76ms
step:1730/2330 train_time:101657ms step_avg:58.76ms
step:1731/2330 train_time:101713ms step_avg:58.76ms
step:1732/2330 train_time:101776ms step_avg:58.76ms
step:1733/2330 train_time:101832ms step_avg:58.76ms
step:1734/2330 train_time:101895ms step_avg:58.76ms
step:1735/2330 train_time:101952ms step_avg:58.76ms
step:1736/2330 train_time:102014ms step_avg:58.76ms
step:1737/2330 train_time:102071ms step_avg:58.76ms
step:1738/2330 train_time:102134ms step_avg:58.77ms
step:1739/2330 train_time:102190ms step_avg:58.76ms
step:1740/2330 train_time:102253ms step_avg:58.77ms
step:1741/2330 train_time:102310ms step_avg:58.77ms
step:1742/2330 train_time:102374ms step_avg:58.77ms
step:1743/2330 train_time:102430ms step_avg:58.77ms
step:1744/2330 train_time:102493ms step_avg:58.77ms
step:1745/2330 train_time:102550ms step_avg:58.77ms
step:1746/2330 train_time:102613ms step_avg:58.77ms
step:1747/2330 train_time:102670ms step_avg:58.77ms
step:1748/2330 train_time:102732ms step_avg:58.77ms
step:1749/2330 train_time:102790ms step_avg:58.77ms
step:1750/2330 train_time:102851ms step_avg:58.77ms
step:1750/2330 val_loss:4.1892 train_time:102930ms step_avg:58.82ms
step:1751/2330 train_time:102949ms step_avg:58.79ms
step:1752/2330 train_time:102972ms step_avg:58.77ms
step:1753/2330 train_time:103028ms step_avg:58.77ms
step:1754/2330 train_time:103094ms step_avg:58.78ms
step:1755/2330 train_time:103151ms step_avg:58.78ms
step:1756/2330 train_time:103219ms step_avg:58.78ms
step:1757/2330 train_time:103276ms step_avg:58.78ms
step:1758/2330 train_time:103338ms step_avg:58.78ms
step:1759/2330 train_time:103394ms step_avg:58.78ms
step:1760/2330 train_time:103456ms step_avg:58.78ms
step:1761/2330 train_time:103512ms step_avg:58.78ms
step:1762/2330 train_time:103573ms step_avg:58.78ms
step:1763/2330 train_time:103630ms step_avg:58.78ms
step:1764/2330 train_time:103691ms step_avg:58.78ms
step:1765/2330 train_time:103748ms step_avg:58.78ms
step:1766/2330 train_time:103808ms step_avg:58.78ms
step:1767/2330 train_time:103867ms step_avg:58.78ms
step:1768/2330 train_time:103929ms step_avg:58.78ms
step:1769/2330 train_time:103988ms step_avg:58.78ms
step:1770/2330 train_time:104048ms step_avg:58.78ms
step:1771/2330 train_time:104106ms step_avg:58.78ms
step:1772/2330 train_time:104167ms step_avg:58.78ms
step:1773/2330 train_time:104223ms step_avg:58.78ms
step:1774/2330 train_time:104286ms step_avg:58.79ms
step:1775/2330 train_time:104343ms step_avg:58.78ms
step:1776/2330 train_time:104404ms step_avg:58.79ms
step:1777/2330 train_time:104461ms step_avg:58.79ms
step:1778/2330 train_time:104524ms step_avg:58.79ms
step:1779/2330 train_time:104580ms step_avg:58.79ms
step:1780/2330 train_time:104642ms step_avg:58.79ms
step:1781/2330 train_time:104698ms step_avg:58.79ms
step:1782/2330 train_time:104759ms step_avg:58.79ms
step:1783/2330 train_time:104816ms step_avg:58.79ms
step:1784/2330 train_time:104878ms step_avg:58.79ms
step:1785/2330 train_time:104935ms step_avg:58.79ms
step:1786/2330 train_time:104999ms step_avg:58.79ms
step:1787/2330 train_time:105056ms step_avg:58.79ms
step:1788/2330 train_time:105119ms step_avg:58.79ms
step:1789/2330 train_time:105176ms step_avg:58.79ms
step:1790/2330 train_time:105238ms step_avg:58.79ms
step:1791/2330 train_time:105294ms step_avg:58.79ms
step:1792/2330 train_time:105357ms step_avg:58.79ms
step:1793/2330 train_time:105414ms step_avg:58.79ms
step:1794/2330 train_time:105476ms step_avg:58.79ms
step:1795/2330 train_time:105533ms step_avg:58.79ms
step:1796/2330 train_time:105595ms step_avg:58.79ms
step:1797/2330 train_time:105651ms step_avg:58.79ms
step:1798/2330 train_time:105713ms step_avg:58.79ms
step:1799/2330 train_time:105770ms step_avg:58.79ms
step:1800/2330 train_time:105832ms step_avg:58.80ms
step:1801/2330 train_time:105890ms step_avg:58.80ms
step:1802/2330 train_time:105952ms step_avg:58.80ms
step:1803/2330 train_time:106009ms step_avg:58.80ms
step:1804/2330 train_time:106070ms step_avg:58.80ms
step:1805/2330 train_time:106127ms step_avg:58.80ms
step:1806/2330 train_time:106189ms step_avg:58.80ms
step:1807/2330 train_time:106247ms step_avg:58.80ms
step:1808/2330 train_time:106308ms step_avg:58.80ms
step:1809/2330 train_time:106365ms step_avg:58.80ms
step:1810/2330 train_time:106426ms step_avg:58.80ms
step:1811/2330 train_time:106484ms step_avg:58.80ms
step:1812/2330 train_time:106545ms step_avg:58.80ms
step:1813/2330 train_time:106602ms step_avg:58.80ms
step:1814/2330 train_time:106664ms step_avg:58.80ms
step:1815/2330 train_time:106720ms step_avg:58.80ms
step:1816/2330 train_time:106782ms step_avg:58.80ms
step:1817/2330 train_time:106838ms step_avg:58.80ms
step:1818/2330 train_time:106900ms step_avg:58.80ms
step:1819/2330 train_time:106957ms step_avg:58.80ms
step:1820/2330 train_time:107019ms step_avg:58.80ms
step:1821/2330 train_time:107076ms step_avg:58.80ms
step:1822/2330 train_time:107139ms step_avg:58.80ms
step:1823/2330 train_time:107195ms step_avg:58.80ms
step:1824/2330 train_time:107258ms step_avg:58.80ms
step:1825/2330 train_time:107315ms step_avg:58.80ms
step:1826/2330 train_time:107378ms step_avg:58.80ms
step:1827/2330 train_time:107434ms step_avg:58.80ms
step:1828/2330 train_time:107497ms step_avg:58.81ms
step:1829/2330 train_time:107554ms step_avg:58.80ms
step:1830/2330 train_time:107616ms step_avg:58.81ms
step:1831/2330 train_time:107672ms step_avg:58.81ms
step:1832/2330 train_time:107734ms step_avg:58.81ms
step:1833/2330 train_time:107792ms step_avg:58.81ms
step:1834/2330 train_time:107853ms step_avg:58.81ms
step:1835/2330 train_time:107911ms step_avg:58.81ms
step:1836/2330 train_time:107971ms step_avg:58.81ms
step:1837/2330 train_time:108029ms step_avg:58.81ms
step:1838/2330 train_time:108091ms step_avg:58.81ms
step:1839/2330 train_time:108148ms step_avg:58.81ms
step:1840/2330 train_time:108209ms step_avg:58.81ms
step:1841/2330 train_time:108267ms step_avg:58.81ms
step:1842/2330 train_time:108328ms step_avg:58.81ms
step:1843/2330 train_time:108386ms step_avg:58.81ms
step:1844/2330 train_time:108447ms step_avg:58.81ms
step:1845/2330 train_time:108504ms step_avg:58.81ms
step:1846/2330 train_time:108566ms step_avg:58.81ms
step:1847/2330 train_time:108623ms step_avg:58.81ms
step:1848/2330 train_time:108684ms step_avg:58.81ms
step:1849/2330 train_time:108741ms step_avg:58.81ms
step:1850/2330 train_time:108803ms step_avg:58.81ms
step:1851/2330 train_time:108860ms step_avg:58.81ms
step:1852/2330 train_time:108921ms step_avg:58.81ms
step:1853/2330 train_time:108978ms step_avg:58.81ms
step:1854/2330 train_time:109040ms step_avg:58.81ms
step:1855/2330 train_time:109096ms step_avg:58.81ms
step:1856/2330 train_time:109160ms step_avg:58.81ms
step:1857/2330 train_time:109217ms step_avg:58.81ms
step:1858/2330 train_time:109279ms step_avg:58.82ms
step:1859/2330 train_time:109336ms step_avg:58.81ms
step:1860/2330 train_time:109399ms step_avg:58.82ms
step:1861/2330 train_time:109455ms step_avg:58.82ms
step:1862/2330 train_time:109519ms step_avg:58.82ms
step:1863/2330 train_time:109575ms step_avg:58.82ms
step:1864/2330 train_time:109637ms step_avg:58.82ms
step:1865/2330 train_time:109694ms step_avg:58.82ms
step:1866/2330 train_time:109756ms step_avg:58.82ms
step:1867/2330 train_time:109813ms step_avg:58.82ms
step:1868/2330 train_time:109876ms step_avg:58.82ms
step:1869/2330 train_time:109933ms step_avg:58.82ms
step:1870/2330 train_time:109994ms step_avg:58.82ms
step:1871/2330 train_time:110052ms step_avg:58.82ms
step:1872/2330 train_time:110114ms step_avg:58.82ms
step:1873/2330 train_time:110172ms step_avg:58.82ms
step:1874/2330 train_time:110233ms step_avg:58.82ms
step:1875/2330 train_time:110291ms step_avg:58.82ms
step:1876/2330 train_time:110352ms step_avg:58.82ms
step:1877/2330 train_time:110410ms step_avg:58.82ms
step:1878/2330 train_time:110471ms step_avg:58.82ms
step:1879/2330 train_time:110528ms step_avg:58.82ms
step:1880/2330 train_time:110590ms step_avg:58.82ms
step:1881/2330 train_time:110647ms step_avg:58.82ms
step:1882/2330 train_time:110708ms step_avg:58.82ms
step:1883/2330 train_time:110766ms step_avg:58.82ms
step:1884/2330 train_time:110827ms step_avg:58.83ms
step:1885/2330 train_time:110884ms step_avg:58.82ms
step:1886/2330 train_time:110945ms step_avg:58.83ms
step:1887/2330 train_time:111002ms step_avg:58.82ms
step:1888/2330 train_time:111064ms step_avg:58.83ms
step:1889/2330 train_time:111120ms step_avg:58.82ms
step:1890/2330 train_time:111182ms step_avg:58.83ms
step:1891/2330 train_time:111240ms step_avg:58.83ms
step:1892/2330 train_time:111302ms step_avg:58.83ms
step:1893/2330 train_time:111359ms step_avg:58.83ms
step:1894/2330 train_time:111420ms step_avg:58.83ms
step:1895/2330 train_time:111476ms step_avg:58.83ms
step:1896/2330 train_time:111540ms step_avg:58.83ms
step:1897/2330 train_time:111596ms step_avg:58.83ms
step:1898/2330 train_time:111660ms step_avg:58.83ms
step:1899/2330 train_time:111717ms step_avg:58.83ms
step:1900/2330 train_time:111779ms step_avg:58.83ms
step:1901/2330 train_time:111836ms step_avg:58.83ms
step:1902/2330 train_time:111899ms step_avg:58.83ms
step:1903/2330 train_time:111955ms step_avg:58.83ms
step:1904/2330 train_time:112017ms step_avg:58.83ms
step:1905/2330 train_time:112074ms step_avg:58.83ms
step:1906/2330 train_time:112136ms step_avg:58.83ms
step:1907/2330 train_time:112193ms step_avg:58.83ms
step:1908/2330 train_time:112255ms step_avg:58.83ms
step:1909/2330 train_time:112311ms step_avg:58.83ms
step:1910/2330 train_time:112373ms step_avg:58.83ms
step:1911/2330 train_time:112431ms step_avg:58.83ms
step:1912/2330 train_time:112494ms step_avg:58.84ms
step:1913/2330 train_time:112551ms step_avg:58.84ms
step:1914/2330 train_time:112614ms step_avg:58.84ms
step:1915/2330 train_time:112671ms step_avg:58.84ms
step:1916/2330 train_time:112734ms step_avg:58.84ms
step:1917/2330 train_time:112791ms step_avg:58.84ms
step:1918/2330 train_time:112854ms step_avg:58.84ms
step:1919/2330 train_time:112912ms step_avg:58.84ms
step:1920/2330 train_time:112973ms step_avg:58.84ms
step:1921/2330 train_time:113030ms step_avg:58.84ms
step:1922/2330 train_time:113092ms step_avg:58.84ms
step:1923/2330 train_time:113149ms step_avg:58.84ms
step:1924/2330 train_time:113212ms step_avg:58.84ms
step:1925/2330 train_time:113269ms step_avg:58.84ms
step:1926/2330 train_time:113330ms step_avg:58.84ms
step:1927/2330 train_time:113387ms step_avg:58.84ms
step:1928/2330 train_time:113450ms step_avg:58.84ms
step:1929/2330 train_time:113507ms step_avg:58.84ms
step:1930/2330 train_time:113568ms step_avg:58.84ms
step:1931/2330 train_time:113626ms step_avg:58.84ms
step:1932/2330 train_time:113686ms step_avg:58.84ms
step:1933/2330 train_time:113743ms step_avg:58.84ms
step:1934/2330 train_time:113805ms step_avg:58.84ms
step:1935/2330 train_time:113862ms step_avg:58.84ms
step:1936/2330 train_time:113925ms step_avg:58.85ms
step:1937/2330 train_time:113981ms step_avg:58.84ms
step:1938/2330 train_time:114042ms step_avg:58.85ms
step:1939/2330 train_time:114099ms step_avg:58.84ms
step:1940/2330 train_time:114161ms step_avg:58.85ms
step:1941/2330 train_time:114218ms step_avg:58.84ms
step:1942/2330 train_time:114280ms step_avg:58.85ms
step:1943/2330 train_time:114336ms step_avg:58.85ms
step:1944/2330 train_time:114399ms step_avg:58.85ms
step:1945/2330 train_time:114455ms step_avg:58.85ms
step:1946/2330 train_time:114519ms step_avg:58.85ms
step:1947/2330 train_time:114576ms step_avg:58.85ms
step:1948/2330 train_time:114638ms step_avg:58.85ms
step:1949/2330 train_time:114695ms step_avg:58.85ms
step:1950/2330 train_time:114756ms step_avg:58.85ms
step:1951/2330 train_time:114813ms step_avg:58.85ms
step:1952/2330 train_time:114876ms step_avg:58.85ms
step:1953/2330 train_time:114933ms step_avg:58.85ms
step:1954/2330 train_time:114996ms step_avg:58.85ms
step:1955/2330 train_time:115052ms step_avg:58.85ms
step:1956/2330 train_time:115115ms step_avg:58.85ms
step:1957/2330 train_time:115172ms step_avg:58.85ms
step:1958/2330 train_time:115234ms step_avg:58.85ms
step:1959/2330 train_time:115291ms step_avg:58.85ms
step:1960/2330 train_time:115352ms step_avg:58.85ms
step:1961/2330 train_time:115409ms step_avg:58.85ms
step:1962/2330 train_time:115472ms step_avg:58.85ms
step:1963/2330 train_time:115529ms step_avg:58.85ms
step:1964/2330 train_time:115590ms step_avg:58.85ms
step:1965/2330 train_time:115648ms step_avg:58.85ms
step:1966/2330 train_time:115709ms step_avg:58.86ms
step:1967/2330 train_time:115766ms step_avg:58.85ms
step:1968/2330 train_time:115828ms step_avg:58.86ms
step:1969/2330 train_time:115886ms step_avg:58.86ms
step:1970/2330 train_time:115947ms step_avg:58.86ms
step:1971/2330 train_time:116005ms step_avg:58.86ms
step:1972/2330 train_time:116066ms step_avg:58.86ms
step:1973/2330 train_time:116123ms step_avg:58.86ms
step:1974/2330 train_time:116185ms step_avg:58.86ms
step:1975/2330 train_time:116242ms step_avg:58.86ms
step:1976/2330 train_time:116303ms step_avg:58.86ms
step:1977/2330 train_time:116360ms step_avg:58.86ms
step:1978/2330 train_time:116423ms step_avg:58.86ms
step:1979/2330 train_time:116479ms step_avg:58.86ms
step:1980/2330 train_time:116543ms step_avg:58.86ms
step:1981/2330 train_time:116599ms step_avg:58.86ms
step:1982/2330 train_time:116661ms step_avg:58.86ms
step:1983/2330 train_time:116718ms step_avg:58.86ms
step:1984/2330 train_time:116780ms step_avg:58.86ms
step:1985/2330 train_time:116837ms step_avg:58.86ms
step:1986/2330 train_time:116899ms step_avg:58.86ms
step:1987/2330 train_time:116956ms step_avg:58.86ms
step:1988/2330 train_time:117019ms step_avg:58.86ms
step:1989/2330 train_time:117076ms step_avg:58.86ms
step:1990/2330 train_time:117138ms step_avg:58.86ms
step:1991/2330 train_time:117194ms step_avg:58.86ms
step:1992/2330 train_time:117257ms step_avg:58.86ms
step:1993/2330 train_time:117313ms step_avg:58.86ms
step:1994/2330 train_time:117376ms step_avg:58.86ms
step:1995/2330 train_time:117433ms step_avg:58.86ms
step:1996/2330 train_time:117496ms step_avg:58.87ms
step:1997/2330 train_time:117552ms step_avg:58.86ms
step:1998/2330 train_time:117615ms step_avg:58.87ms
step:1999/2330 train_time:117672ms step_avg:58.87ms
step:2000/2330 train_time:117735ms step_avg:58.87ms
step:2000/2330 val_loss:4.1182 train_time:117814ms step_avg:58.91ms
step:2001/2330 train_time:117833ms step_avg:58.89ms
step:2002/2330 train_time:117857ms step_avg:58.87ms
step:2003/2330 train_time:117915ms step_avg:58.87ms
step:2004/2330 train_time:117983ms step_avg:58.87ms
step:2005/2330 train_time:118040ms step_avg:58.87ms
step:2006/2330 train_time:118105ms step_avg:58.88ms
step:2007/2330 train_time:118162ms step_avg:58.87ms
step:2008/2330 train_time:118225ms step_avg:58.88ms
step:2009/2330 train_time:118282ms step_avg:58.88ms
step:2010/2330 train_time:118344ms step_avg:58.88ms
step:2011/2330 train_time:118401ms step_avg:58.88ms
step:2012/2330 train_time:118462ms step_avg:58.88ms
step:2013/2330 train_time:118518ms step_avg:58.88ms
step:2014/2330 train_time:118579ms step_avg:58.88ms
step:2015/2330 train_time:118636ms step_avg:58.88ms
step:2016/2330 train_time:118697ms step_avg:58.88ms
step:2017/2330 train_time:118754ms step_avg:58.88ms
step:2018/2330 train_time:118815ms step_avg:58.88ms
step:2019/2330 train_time:118873ms step_avg:58.88ms
step:2020/2330 train_time:118934ms step_avg:58.88ms
step:2021/2330 train_time:118992ms step_avg:58.88ms
step:2022/2330 train_time:119055ms step_avg:58.88ms
step:2023/2330 train_time:119112ms step_avg:58.88ms
step:2024/2330 train_time:119173ms step_avg:58.88ms
step:2025/2330 train_time:119230ms step_avg:58.88ms
step:2026/2330 train_time:119293ms step_avg:58.88ms
step:2027/2330 train_time:119350ms step_avg:58.88ms
step:2028/2330 train_time:119411ms step_avg:58.88ms
step:2029/2330 train_time:119468ms step_avg:58.88ms
step:2030/2330 train_time:119529ms step_avg:58.88ms
step:2031/2330 train_time:119586ms step_avg:58.88ms
step:2032/2330 train_time:119647ms step_avg:58.88ms
step:2033/2330 train_time:119704ms step_avg:58.88ms
step:2034/2330 train_time:119766ms step_avg:58.88ms
step:2035/2330 train_time:119823ms step_avg:58.88ms
step:2036/2330 train_time:119886ms step_avg:58.88ms
step:2037/2330 train_time:119944ms step_avg:58.88ms
step:2038/2330 train_time:120006ms step_avg:58.88ms
step:2039/2330 train_time:120064ms step_avg:58.88ms
step:2040/2330 train_time:120126ms step_avg:58.89ms
step:2041/2330 train_time:120184ms step_avg:58.88ms
step:2042/2330 train_time:120246ms step_avg:58.89ms
step:2043/2330 train_time:120304ms step_avg:58.89ms
step:2044/2330 train_time:120365ms step_avg:58.89ms
step:2045/2330 train_time:120423ms step_avg:58.89ms
step:2046/2330 train_time:120485ms step_avg:58.89ms
step:2047/2330 train_time:120542ms step_avg:58.89ms
step:2048/2330 train_time:120603ms step_avg:58.89ms
step:2049/2330 train_time:120661ms step_avg:58.89ms
step:2050/2330 train_time:120723ms step_avg:58.89ms
step:2051/2330 train_time:120780ms step_avg:58.89ms
step:2052/2330 train_time:120843ms step_avg:58.89ms
step:2053/2330 train_time:120901ms step_avg:58.89ms
step:2054/2330 train_time:120962ms step_avg:58.89ms
step:2055/2330 train_time:121020ms step_avg:58.89ms
step:2056/2330 train_time:121081ms step_avg:58.89ms
step:2057/2330 train_time:121139ms step_avg:58.89ms
step:2058/2330 train_time:121200ms step_avg:58.89ms
step:2059/2330 train_time:121258ms step_avg:58.89ms
step:2060/2330 train_time:121320ms step_avg:58.89ms
step:2061/2330 train_time:121378ms step_avg:58.89ms
step:2062/2330 train_time:121439ms step_avg:58.89ms
step:2063/2330 train_time:121497ms step_avg:58.89ms
step:2064/2330 train_time:121557ms step_avg:58.89ms
step:2065/2330 train_time:121614ms step_avg:58.89ms
step:2066/2330 train_time:121675ms step_avg:58.89ms
step:2067/2330 train_time:121732ms step_avg:58.89ms
step:2068/2330 train_time:121794ms step_avg:58.89ms
step:2069/2330 train_time:121851ms step_avg:58.89ms
step:2070/2330 train_time:121912ms step_avg:58.89ms
step:2071/2330 train_time:121968ms step_avg:58.89ms
step:2072/2330 train_time:122032ms step_avg:58.90ms
step:2073/2330 train_time:122089ms step_avg:58.89ms
step:2074/2330 train_time:122152ms step_avg:58.90ms
step:2075/2330 train_time:122208ms step_avg:58.90ms
step:2076/2330 train_time:122272ms step_avg:58.90ms
step:2077/2330 train_time:122329ms step_avg:58.90ms
step:2078/2330 train_time:122391ms step_avg:58.90ms
step:2079/2330 train_time:122447ms step_avg:58.90ms
step:2080/2330 train_time:122510ms step_avg:58.90ms
step:2081/2330 train_time:122567ms step_avg:58.90ms
step:2082/2330 train_time:122629ms step_avg:58.90ms
step:2083/2330 train_time:122686ms step_avg:58.90ms
step:2084/2330 train_time:122748ms step_avg:58.90ms
step:2085/2330 train_time:122805ms step_avg:58.90ms
step:2086/2330 train_time:122866ms step_avg:58.90ms
step:2087/2330 train_time:122924ms step_avg:58.90ms
step:2088/2330 train_time:122985ms step_avg:58.90ms
step:2089/2330 train_time:123042ms step_avg:58.90ms
step:2090/2330 train_time:123105ms step_avg:58.90ms
step:2091/2330 train_time:123163ms step_avg:58.90ms
step:2092/2330 train_time:123225ms step_avg:58.90ms
step:2093/2330 train_time:123283ms step_avg:58.90ms
step:2094/2330 train_time:123345ms step_avg:58.90ms
step:2095/2330 train_time:123403ms step_avg:58.90ms
step:2096/2330 train_time:123465ms step_avg:58.90ms
step:2097/2330 train_time:123522ms step_avg:58.90ms
step:2098/2330 train_time:123583ms step_avg:58.91ms
step:2099/2330 train_time:123641ms step_avg:58.90ms
step:2100/2330 train_time:123702ms step_avg:58.91ms
step:2101/2330 train_time:123759ms step_avg:58.90ms
step:2102/2330 train_time:123821ms step_avg:58.91ms
step:2103/2330 train_time:123879ms step_avg:58.91ms
step:2104/2330 train_time:123941ms step_avg:58.91ms
step:2105/2330 train_time:123999ms step_avg:58.91ms
step:2106/2330 train_time:124060ms step_avg:58.91ms
step:2107/2330 train_time:124117ms step_avg:58.91ms
step:2108/2330 train_time:124180ms step_avg:58.91ms
step:2109/2330 train_time:124239ms step_avg:58.91ms
step:2110/2330 train_time:124300ms step_avg:58.91ms
step:2111/2330 train_time:124357ms step_avg:58.91ms
step:2112/2330 train_time:124418ms step_avg:58.91ms
step:2113/2330 train_time:124476ms step_avg:58.91ms
step:2114/2330 train_time:124537ms step_avg:58.91ms
step:2115/2330 train_time:124595ms step_avg:58.91ms
step:2116/2330 train_time:124656ms step_avg:58.91ms
step:2117/2330 train_time:124712ms step_avg:58.91ms
step:2118/2330 train_time:124774ms step_avg:58.91ms
step:2119/2330 train_time:124831ms step_avg:58.91ms
step:2120/2330 train_time:124893ms step_avg:58.91ms
step:2121/2330 train_time:124949ms step_avg:58.91ms
step:2122/2330 train_time:125011ms step_avg:58.91ms
step:2123/2330 train_time:125068ms step_avg:58.91ms
step:2124/2330 train_time:125130ms step_avg:58.91ms
step:2125/2330 train_time:125187ms step_avg:58.91ms
step:2126/2330 train_time:125251ms step_avg:58.91ms
step:2127/2330 train_time:125307ms step_avg:58.91ms
step:2128/2330 train_time:125370ms step_avg:58.91ms
step:2129/2330 train_time:125427ms step_avg:58.91ms
step:2130/2330 train_time:125490ms step_avg:58.92ms
step:2131/2330 train_time:125547ms step_avg:58.91ms
step:2132/2330 train_time:125609ms step_avg:58.92ms
step:2133/2330 train_time:125666ms step_avg:58.92ms
step:2134/2330 train_time:125729ms step_avg:58.92ms
step:2135/2330 train_time:125785ms step_avg:58.92ms
step:2136/2330 train_time:125847ms step_avg:58.92ms
step:2137/2330 train_time:125904ms step_avg:58.92ms
step:2138/2330 train_time:125966ms step_avg:58.92ms
step:2139/2330 train_time:126024ms step_avg:58.92ms
step:2140/2330 train_time:126086ms step_avg:58.92ms
step:2141/2330 train_time:126143ms step_avg:58.92ms
step:2142/2330 train_time:126204ms step_avg:58.92ms
step:2143/2330 train_time:126262ms step_avg:58.92ms
step:2144/2330 train_time:126324ms step_avg:58.92ms
step:2145/2330 train_time:126382ms step_avg:58.92ms
step:2146/2330 train_time:126444ms step_avg:58.92ms
step:2147/2330 train_time:126501ms step_avg:58.92ms
step:2148/2330 train_time:126564ms step_avg:58.92ms
step:2149/2330 train_time:126622ms step_avg:58.92ms
step:2150/2330 train_time:126683ms step_avg:58.92ms
step:2151/2330 train_time:126741ms step_avg:58.92ms
step:2152/2330 train_time:126801ms step_avg:58.92ms
step:2153/2330 train_time:126859ms step_avg:58.92ms
step:2154/2330 train_time:126920ms step_avg:58.92ms
step:2155/2330 train_time:126978ms step_avg:58.92ms
step:2156/2330 train_time:127039ms step_avg:58.92ms
step:2157/2330 train_time:127096ms step_avg:58.92ms
step:2158/2330 train_time:127157ms step_avg:58.92ms
step:2159/2330 train_time:127215ms step_avg:58.92ms
step:2160/2330 train_time:127276ms step_avg:58.92ms
step:2161/2330 train_time:127335ms step_avg:58.92ms
step:2162/2330 train_time:127396ms step_avg:58.92ms
step:2163/2330 train_time:127452ms step_avg:58.92ms
step:2164/2330 train_time:127514ms step_avg:58.93ms
step:2165/2330 train_time:127571ms step_avg:58.92ms
step:2166/2330 train_time:127632ms step_avg:58.93ms
step:2167/2330 train_time:127690ms step_avg:58.92ms
step:2168/2330 train_time:127752ms step_avg:58.93ms
step:2169/2330 train_time:127808ms step_avg:58.92ms
step:2170/2330 train_time:127871ms step_avg:58.93ms
step:2171/2330 train_time:127928ms step_avg:58.93ms
step:2172/2330 train_time:127991ms step_avg:58.93ms
step:2173/2330 train_time:128048ms step_avg:58.93ms
step:2174/2330 train_time:128111ms step_avg:58.93ms
step:2175/2330 train_time:128168ms step_avg:58.93ms
step:2176/2330 train_time:128230ms step_avg:58.93ms
step:2177/2330 train_time:128287ms step_avg:58.93ms
step:2178/2330 train_time:128350ms step_avg:58.93ms
step:2179/2330 train_time:128407ms step_avg:58.93ms
step:2180/2330 train_time:128470ms step_avg:58.93ms
step:2181/2330 train_time:128527ms step_avg:58.93ms
step:2182/2330 train_time:128589ms step_avg:58.93ms
step:2183/2330 train_time:128646ms step_avg:58.93ms
step:2184/2330 train_time:128708ms step_avg:58.93ms
step:2185/2330 train_time:128765ms step_avg:58.93ms
step:2186/2330 train_time:128827ms step_avg:58.93ms
step:2187/2330 train_time:128884ms step_avg:58.93ms
step:2188/2330 train_time:128948ms step_avg:58.93ms
step:2189/2330 train_time:129005ms step_avg:58.93ms
step:2190/2330 train_time:129067ms step_avg:58.93ms
step:2191/2330 train_time:129124ms step_avg:58.93ms
step:2192/2330 train_time:129186ms step_avg:58.94ms
step:2193/2330 train_time:129243ms step_avg:58.93ms
step:2194/2330 train_time:129305ms step_avg:58.94ms
step:2195/2330 train_time:129362ms step_avg:58.93ms
step:2196/2330 train_time:129424ms step_avg:58.94ms
step:2197/2330 train_time:129481ms step_avg:58.94ms
step:2198/2330 train_time:129544ms step_avg:58.94ms
step:2199/2330 train_time:129601ms step_avg:58.94ms
step:2200/2330 train_time:129662ms step_avg:58.94ms
step:2201/2330 train_time:129719ms step_avg:58.94ms
step:2202/2330 train_time:129781ms step_avg:58.94ms
step:2203/2330 train_time:129839ms step_avg:58.94ms
step:2204/2330 train_time:129900ms step_avg:58.94ms
step:2205/2330 train_time:129958ms step_avg:58.94ms
step:2206/2330 train_time:130019ms step_avg:58.94ms
step:2207/2330 train_time:130076ms step_avg:58.94ms
step:2208/2330 train_time:130137ms step_avg:58.94ms
step:2209/2330 train_time:130195ms step_avg:58.94ms
step:2210/2330 train_time:130256ms step_avg:58.94ms
step:2211/2330 train_time:130313ms step_avg:58.94ms
step:2212/2330 train_time:130374ms step_avg:58.94ms
step:2213/2330 train_time:130432ms step_avg:58.94ms
step:2214/2330 train_time:130493ms step_avg:58.94ms
step:2215/2330 train_time:130550ms step_avg:58.94ms
step:2216/2330 train_time:130612ms step_avg:58.94ms
step:2217/2330 train_time:130669ms step_avg:58.94ms
step:2218/2330 train_time:130731ms step_avg:58.94ms
step:2219/2330 train_time:130788ms step_avg:58.94ms
step:2220/2330 train_time:130851ms step_avg:58.94ms
step:2221/2330 train_time:130908ms step_avg:58.94ms
step:2222/2330 train_time:130971ms step_avg:58.94ms
step:2223/2330 train_time:131027ms step_avg:58.94ms
step:2224/2330 train_time:131091ms step_avg:58.94ms
step:2225/2330 train_time:131147ms step_avg:58.94ms
step:2226/2330 train_time:131210ms step_avg:58.94ms
step:2227/2330 train_time:131267ms step_avg:58.94ms
step:2228/2330 train_time:131328ms step_avg:58.94ms
step:2229/2330 train_time:131385ms step_avg:58.94ms
step:2230/2330 train_time:131447ms step_avg:58.94ms
step:2231/2330 train_time:131504ms step_avg:58.94ms
step:2232/2330 train_time:131566ms step_avg:58.95ms
step:2233/2330 train_time:131624ms step_avg:58.94ms
step:2234/2330 train_time:131685ms step_avg:58.95ms
step:2235/2330 train_time:131743ms step_avg:58.95ms
step:2236/2330 train_time:131805ms step_avg:58.95ms
step:2237/2330 train_time:131862ms step_avg:58.95ms
step:2238/2330 train_time:131926ms step_avg:58.95ms
step:2239/2330 train_time:131983ms step_avg:58.95ms
step:2240/2330 train_time:132045ms step_avg:58.95ms
step:2241/2330 train_time:132103ms step_avg:58.95ms
step:2242/2330 train_time:132165ms step_avg:58.95ms
step:2243/2330 train_time:132223ms step_avg:58.95ms
step:2244/2330 train_time:132284ms step_avg:58.95ms
step:2245/2330 train_time:132342ms step_avg:58.95ms
step:2246/2330 train_time:132403ms step_avg:58.95ms
step:2247/2330 train_time:132460ms step_avg:58.95ms
step:2248/2330 train_time:132521ms step_avg:58.95ms
step:2249/2330 train_time:132579ms step_avg:58.95ms
step:2250/2330 train_time:132640ms step_avg:58.95ms
step:2250/2330 val_loss:4.0674 train_time:132718ms step_avg:58.99ms
step:2251/2330 train_time:132738ms step_avg:58.97ms
step:2252/2330 train_time:132762ms step_avg:58.95ms
step:2253/2330 train_time:132821ms step_avg:58.95ms
step:2254/2330 train_time:132886ms step_avg:58.96ms
step:2255/2330 train_time:132943ms step_avg:58.95ms
step:2256/2330 train_time:133006ms step_avg:58.96ms
step:2257/2330 train_time:133062ms step_avg:58.96ms
step:2258/2330 train_time:133124ms step_avg:58.96ms
step:2259/2330 train_time:133181ms step_avg:58.96ms
step:2260/2330 train_time:133242ms step_avg:58.96ms
step:2261/2330 train_time:133299ms step_avg:58.96ms
step:2262/2330 train_time:133359ms step_avg:58.96ms
step:2263/2330 train_time:133416ms step_avg:58.96ms
step:2264/2330 train_time:133477ms step_avg:58.96ms
step:2265/2330 train_time:133533ms step_avg:58.96ms
step:2266/2330 train_time:133594ms step_avg:58.96ms
step:2267/2330 train_time:133651ms step_avg:58.95ms
step:2268/2330 train_time:133715ms step_avg:58.96ms
step:2269/2330 train_time:133773ms step_avg:58.96ms
step:2270/2330 train_time:133837ms step_avg:58.96ms
step:2271/2330 train_time:133894ms step_avg:58.96ms
step:2272/2330 train_time:133957ms step_avg:58.96ms
step:2273/2330 train_time:134014ms step_avg:58.96ms
step:2274/2330 train_time:134078ms step_avg:58.96ms
step:2275/2330 train_time:134134ms step_avg:58.96ms
step:2276/2330 train_time:134196ms step_avg:58.96ms
step:2277/2330 train_time:134253ms step_avg:58.96ms
step:2278/2330 train_time:134314ms step_avg:58.96ms
step:2279/2330 train_time:134371ms step_avg:58.96ms
step:2280/2330 train_time:134433ms step_avg:58.96ms
step:2281/2330 train_time:134490ms step_avg:58.96ms
step:2282/2330 train_time:134551ms step_avg:58.96ms
step:2283/2330 train_time:134608ms step_avg:58.96ms
step:2284/2330 train_time:134670ms step_avg:58.96ms
step:2285/2330 train_time:134727ms step_avg:58.96ms
step:2286/2330 train_time:134790ms step_avg:58.96ms
step:2287/2330 train_time:134849ms step_avg:58.96ms
step:2288/2330 train_time:134910ms step_avg:58.96ms
step:2289/2330 train_time:134968ms step_avg:58.96ms
step:2290/2330 train_time:135029ms step_avg:58.96ms
step:2291/2330 train_time:135087ms step_avg:58.96ms
step:2292/2330 train_time:135148ms step_avg:58.97ms
step:2293/2330 train_time:135206ms step_avg:58.96ms
step:2294/2330 train_time:135267ms step_avg:58.97ms
step:2295/2330 train_time:135324ms step_avg:58.96ms
step:2296/2330 train_time:135385ms step_avg:58.97ms
step:2297/2330 train_time:135443ms step_avg:58.97ms
step:2298/2330 train_time:135503ms step_avg:58.97ms
step:2299/2330 train_time:135561ms step_avg:58.97ms
step:2300/2330 train_time:135621ms step_avg:58.97ms
step:2301/2330 train_time:135678ms step_avg:58.96ms
step:2302/2330 train_time:135740ms step_avg:58.97ms
step:2303/2330 train_time:135797ms step_avg:58.97ms
step:2304/2330 train_time:135859ms step_avg:58.97ms
step:2305/2330 train_time:135916ms step_avg:58.97ms
step:2306/2330 train_time:135981ms step_avg:58.97ms
step:2307/2330 train_time:136037ms step_avg:58.97ms
step:2308/2330 train_time:136100ms step_avg:58.97ms
step:2309/2330 train_time:136156ms step_avg:58.97ms
step:2310/2330 train_time:136219ms step_avg:58.97ms
step:2311/2330 train_time:136276ms step_avg:58.97ms
step:2312/2330 train_time:136338ms step_avg:58.97ms
step:2313/2330 train_time:136395ms step_avg:58.97ms
step:2314/2330 train_time:136456ms step_avg:58.97ms
step:2315/2330 train_time:136512ms step_avg:58.97ms
step:2316/2330 train_time:136574ms step_avg:58.97ms
step:2317/2330 train_time:136631ms step_avg:58.97ms
step:2318/2330 train_time:136693ms step_avg:58.97ms
step:2319/2330 train_time:136750ms step_avg:58.97ms
step:2320/2330 train_time:136813ms step_avg:58.97ms
step:2321/2330 train_time:136870ms step_avg:58.97ms
step:2322/2330 train_time:136933ms step_avg:58.97ms
step:2323/2330 train_time:136991ms step_avg:58.97ms
step:2324/2330 train_time:137053ms step_avg:58.97ms
step:2325/2330 train_time:137111ms step_avg:58.97ms
step:2326/2330 train_time:137172ms step_avg:58.97ms
step:2327/2330 train_time:137230ms step_avg:58.97ms
step:2328/2330 train_time:137292ms step_avg:58.97ms
step:2329/2330 train_time:137350ms step_avg:58.97ms
step:2330/2330 train_time:137411ms step_avg:58.97ms
step:2330/2330 val_loss:4.0524 train_time:137490ms step_avg:59.01ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
