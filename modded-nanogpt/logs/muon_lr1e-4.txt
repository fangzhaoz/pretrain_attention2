import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "muon_lr1e-4"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-4, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Oct 28 22:12:32 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   29C    P0             110W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   28C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   27C    P0             108W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   29C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:98ms step_avg:98.46ms
step:2/2330 train_time:226ms step_avg:112.89ms
step:3/2330 train_time:249ms step_avg:82.93ms
step:4/2330 train_time:283ms step_avg:70.73ms
step:5/2330 train_time:340ms step_avg:68.07ms
step:6/2330 train_time:401ms step_avg:66.86ms
step:7/2330 train_time:460ms step_avg:65.73ms
step:8/2330 train_time:521ms step_avg:65.13ms
step:9/2330 train_time:580ms step_avg:64.41ms
step:10/2330 train_time:640ms step_avg:64.02ms
step:11/2330 train_time:699ms step_avg:63.53ms
step:12/2330 train_time:759ms step_avg:63.29ms
step:13/2330 train_time:818ms step_avg:62.94ms
step:14/2330 train_time:879ms step_avg:62.79ms
step:15/2330 train_time:938ms step_avg:62.52ms
step:16/2330 train_time:999ms step_avg:62.41ms
step:17/2330 train_time:1059ms step_avg:62.30ms
step:18/2330 train_time:1125ms step_avg:62.49ms
step:19/2330 train_time:1188ms step_avg:62.51ms
step:20/2330 train_time:1251ms step_avg:62.53ms
step:21/2330 train_time:1310ms step_avg:62.40ms
step:22/2330 train_time:1372ms step_avg:62.36ms
step:23/2330 train_time:1431ms step_avg:62.22ms
step:24/2330 train_time:1492ms step_avg:62.18ms
step:25/2330 train_time:1552ms step_avg:62.07ms
step:26/2330 train_time:1613ms step_avg:62.05ms
step:27/2330 train_time:1672ms step_avg:61.94ms
step:28/2330 train_time:1733ms step_avg:61.91ms
step:29/2330 train_time:1793ms step_avg:61.82ms
step:30/2330 train_time:1855ms step_avg:61.83ms
step:31/2330 train_time:1915ms step_avg:61.78ms
step:32/2330 train_time:1976ms step_avg:61.76ms
step:33/2330 train_time:2036ms step_avg:61.69ms
step:34/2330 train_time:2098ms step_avg:61.69ms
step:35/2330 train_time:2157ms step_avg:61.64ms
step:36/2330 train_time:2220ms step_avg:61.66ms
step:37/2330 train_time:2280ms step_avg:61.63ms
step:38/2330 train_time:2343ms step_avg:61.65ms
step:39/2330 train_time:2403ms step_avg:61.61ms
step:40/2330 train_time:2464ms step_avg:61.61ms
step:41/2330 train_time:2525ms step_avg:61.59ms
step:42/2330 train_time:2587ms step_avg:61.59ms
step:43/2330 train_time:2646ms step_avg:61.55ms
step:44/2330 train_time:2708ms step_avg:61.55ms
step:45/2330 train_time:2768ms step_avg:61.51ms
step:46/2330 train_time:2830ms step_avg:61.52ms
step:47/2330 train_time:2890ms step_avg:61.49ms
step:48/2330 train_time:2952ms step_avg:61.49ms
step:49/2330 train_time:3011ms step_avg:61.45ms
step:50/2330 train_time:3073ms step_avg:61.46ms
step:51/2330 train_time:3133ms step_avg:61.43ms
step:52/2330 train_time:3194ms step_avg:61.43ms
step:53/2330 train_time:3255ms step_avg:61.41ms
step:54/2330 train_time:3317ms step_avg:61.42ms
step:55/2330 train_time:3376ms step_avg:61.39ms
step:56/2330 train_time:3438ms step_avg:61.40ms
step:57/2330 train_time:3498ms step_avg:61.37ms
step:58/2330 train_time:3561ms step_avg:61.39ms
step:59/2330 train_time:3620ms step_avg:61.36ms
step:60/2330 train_time:3682ms step_avg:61.37ms
step:61/2330 train_time:3742ms step_avg:61.34ms
step:62/2330 train_time:3804ms step_avg:61.35ms
step:63/2330 train_time:3864ms step_avg:61.33ms
step:64/2330 train_time:3926ms step_avg:61.34ms
step:65/2330 train_time:3986ms step_avg:61.33ms
step:66/2330 train_time:4048ms step_avg:61.33ms
step:67/2330 train_time:4108ms step_avg:61.31ms
step:68/2330 train_time:4170ms step_avg:61.33ms
step:69/2330 train_time:4231ms step_avg:61.32ms
step:70/2330 train_time:4292ms step_avg:61.32ms
step:71/2330 train_time:4352ms step_avg:61.30ms
step:72/2330 train_time:4414ms step_avg:61.31ms
step:73/2330 train_time:4474ms step_avg:61.28ms
step:74/2330 train_time:4535ms step_avg:61.28ms
step:75/2330 train_time:4595ms step_avg:61.26ms
step:76/2330 train_time:4656ms step_avg:61.27ms
step:77/2330 train_time:4716ms step_avg:61.24ms
step:78/2330 train_time:4777ms step_avg:61.25ms
step:79/2330 train_time:4837ms step_avg:61.23ms
step:80/2330 train_time:4900ms step_avg:61.24ms
step:81/2330 train_time:4960ms step_avg:61.23ms
step:82/2330 train_time:5022ms step_avg:61.25ms
step:83/2330 train_time:5083ms step_avg:61.24ms
step:84/2330 train_time:5145ms step_avg:61.25ms
step:85/2330 train_time:5205ms step_avg:61.24ms
step:86/2330 train_time:5267ms step_avg:61.25ms
step:87/2330 train_time:5327ms step_avg:61.23ms
step:88/2330 train_time:5389ms step_avg:61.24ms
step:89/2330 train_time:5449ms step_avg:61.22ms
step:90/2330 train_time:5511ms step_avg:61.23ms
step:91/2330 train_time:5571ms step_avg:61.22ms
step:92/2330 train_time:5633ms step_avg:61.23ms
step:93/2330 train_time:5694ms step_avg:61.22ms
step:94/2330 train_time:5755ms step_avg:61.23ms
step:95/2330 train_time:5815ms step_avg:61.21ms
step:96/2330 train_time:5877ms step_avg:61.21ms
step:97/2330 train_time:5936ms step_avg:61.19ms
step:98/2330 train_time:5998ms step_avg:61.20ms
step:99/2330 train_time:6058ms step_avg:61.19ms
step:100/2330 train_time:6120ms step_avg:61.20ms
step:101/2330 train_time:6180ms step_avg:61.19ms
step:102/2330 train_time:6243ms step_avg:61.20ms
step:103/2330 train_time:6303ms step_avg:61.20ms
step:104/2330 train_time:6366ms step_avg:61.21ms
step:105/2330 train_time:6426ms step_avg:61.20ms
step:106/2330 train_time:6487ms step_avg:61.20ms
step:107/2330 train_time:6547ms step_avg:61.19ms
step:108/2330 train_time:6610ms step_avg:61.20ms
step:109/2330 train_time:6670ms step_avg:61.19ms
step:110/2330 train_time:6732ms step_avg:61.20ms
step:111/2330 train_time:6791ms step_avg:61.18ms
step:112/2330 train_time:6853ms step_avg:61.19ms
step:113/2330 train_time:6914ms step_avg:61.18ms
step:114/2330 train_time:6975ms step_avg:61.18ms
step:115/2330 train_time:7034ms step_avg:61.17ms
step:116/2330 train_time:7097ms step_avg:61.18ms
step:117/2330 train_time:7157ms step_avg:61.17ms
step:118/2330 train_time:7218ms step_avg:61.17ms
step:119/2330 train_time:7278ms step_avg:61.16ms
step:120/2330 train_time:7341ms step_avg:61.17ms
step:121/2330 train_time:7401ms step_avg:61.16ms
step:122/2330 train_time:7463ms step_avg:61.17ms
step:123/2330 train_time:7523ms step_avg:61.16ms
step:124/2330 train_time:7585ms step_avg:61.17ms
step:125/2330 train_time:7645ms step_avg:61.16ms
step:126/2330 train_time:7707ms step_avg:61.17ms
step:127/2330 train_time:7767ms step_avg:61.16ms
step:128/2330 train_time:7830ms step_avg:61.17ms
step:129/2330 train_time:7889ms step_avg:61.16ms
step:130/2330 train_time:7951ms step_avg:61.16ms
step:131/2330 train_time:8012ms step_avg:61.16ms
step:132/2330 train_time:8074ms step_avg:61.16ms
step:133/2330 train_time:8133ms step_avg:61.15ms
step:134/2330 train_time:8195ms step_avg:61.16ms
step:135/2330 train_time:8256ms step_avg:61.15ms
step:136/2330 train_time:8317ms step_avg:61.16ms
step:137/2330 train_time:8377ms step_avg:61.14ms
step:138/2330 train_time:8438ms step_avg:61.15ms
step:139/2330 train_time:8498ms step_avg:61.13ms
step:140/2330 train_time:8560ms step_avg:61.14ms
step:141/2330 train_time:8620ms step_avg:61.13ms
step:142/2330 train_time:8682ms step_avg:61.14ms
step:143/2330 train_time:8742ms step_avg:61.13ms
step:144/2330 train_time:8805ms step_avg:61.15ms
step:145/2330 train_time:8866ms step_avg:61.14ms
step:146/2330 train_time:8928ms step_avg:61.15ms
step:147/2330 train_time:8988ms step_avg:61.14ms
step:148/2330 train_time:9050ms step_avg:61.15ms
step:149/2330 train_time:9110ms step_avg:61.14ms
step:150/2330 train_time:9172ms step_avg:61.15ms
step:151/2330 train_time:9232ms step_avg:61.14ms
step:152/2330 train_time:9294ms step_avg:61.14ms
step:153/2330 train_time:9353ms step_avg:61.13ms
step:154/2330 train_time:9415ms step_avg:61.14ms
step:155/2330 train_time:9475ms step_avg:61.13ms
step:156/2330 train_time:9537ms step_avg:61.14ms
step:157/2330 train_time:9597ms step_avg:61.13ms
step:158/2330 train_time:9659ms step_avg:61.13ms
step:159/2330 train_time:9719ms step_avg:61.12ms
step:160/2330 train_time:9781ms step_avg:61.13ms
step:161/2330 train_time:9841ms step_avg:61.12ms
step:162/2330 train_time:9903ms step_avg:61.13ms
step:163/2330 train_time:9964ms step_avg:61.13ms
step:164/2330 train_time:10026ms step_avg:61.13ms
step:165/2330 train_time:10086ms step_avg:61.13ms
step:166/2330 train_time:10148ms step_avg:61.13ms
step:167/2330 train_time:10209ms step_avg:61.13ms
step:168/2330 train_time:10271ms step_avg:61.13ms
step:169/2330 train_time:10330ms step_avg:61.13ms
step:170/2330 train_time:10393ms step_avg:61.13ms
step:171/2330 train_time:10453ms step_avg:61.13ms
step:172/2330 train_time:10516ms step_avg:61.14ms
step:173/2330 train_time:10575ms step_avg:61.13ms
step:174/2330 train_time:10637ms step_avg:61.13ms
step:175/2330 train_time:10696ms step_avg:61.12ms
step:176/2330 train_time:10758ms step_avg:61.12ms
step:177/2330 train_time:10818ms step_avg:61.12ms
step:178/2330 train_time:10879ms step_avg:61.12ms
step:179/2330 train_time:10940ms step_avg:61.12ms
step:180/2330 train_time:11003ms step_avg:61.13ms
step:181/2330 train_time:11064ms step_avg:61.13ms
step:182/2330 train_time:11127ms step_avg:61.14ms
step:183/2330 train_time:11187ms step_avg:61.13ms
step:184/2330 train_time:11250ms step_avg:61.14ms
step:185/2330 train_time:11310ms step_avg:61.13ms
step:186/2330 train_time:11371ms step_avg:61.14ms
step:187/2330 train_time:11431ms step_avg:61.13ms
step:188/2330 train_time:11494ms step_avg:61.14ms
step:189/2330 train_time:11554ms step_avg:61.13ms
step:190/2330 train_time:11616ms step_avg:61.14ms
step:191/2330 train_time:11675ms step_avg:61.13ms
step:192/2330 train_time:11737ms step_avg:61.13ms
step:193/2330 train_time:11797ms step_avg:61.12ms
step:194/2330 train_time:11859ms step_avg:61.13ms
step:195/2330 train_time:11919ms step_avg:61.12ms
step:196/2330 train_time:11981ms step_avg:61.13ms
step:197/2330 train_time:12042ms step_avg:61.13ms
step:198/2330 train_time:12105ms step_avg:61.14ms
step:199/2330 train_time:12166ms step_avg:61.14ms
step:200/2330 train_time:12229ms step_avg:61.14ms
step:201/2330 train_time:12289ms step_avg:61.14ms
step:202/2330 train_time:12351ms step_avg:61.14ms
step:203/2330 train_time:12411ms step_avg:61.14ms
step:204/2330 train_time:12473ms step_avg:61.14ms
step:205/2330 train_time:12533ms step_avg:61.14ms
step:206/2330 train_time:12595ms step_avg:61.14ms
step:207/2330 train_time:12655ms step_avg:61.13ms
step:208/2330 train_time:12716ms step_avg:61.14ms
step:209/2330 train_time:12776ms step_avg:61.13ms
step:210/2330 train_time:12838ms step_avg:61.13ms
step:211/2330 train_time:12897ms step_avg:61.12ms
step:212/2330 train_time:12959ms step_avg:61.13ms
step:213/2330 train_time:13019ms step_avg:61.12ms
step:214/2330 train_time:13082ms step_avg:61.13ms
step:215/2330 train_time:13142ms step_avg:61.13ms
step:216/2330 train_time:13205ms step_avg:61.14ms
step:217/2330 train_time:13266ms step_avg:61.13ms
step:218/2330 train_time:13329ms step_avg:61.14ms
step:219/2330 train_time:13389ms step_avg:61.14ms
step:220/2330 train_time:13450ms step_avg:61.14ms
step:221/2330 train_time:13510ms step_avg:61.13ms
step:222/2330 train_time:13572ms step_avg:61.14ms
step:223/2330 train_time:13632ms step_avg:61.13ms
step:224/2330 train_time:13693ms step_avg:61.13ms
step:225/2330 train_time:13753ms step_avg:61.12ms
step:226/2330 train_time:13815ms step_avg:61.13ms
step:227/2330 train_time:13875ms step_avg:61.13ms
step:228/2330 train_time:13937ms step_avg:61.13ms
step:229/2330 train_time:13998ms step_avg:61.12ms
step:230/2330 train_time:14060ms step_avg:61.13ms
step:231/2330 train_time:14120ms step_avg:61.12ms
step:232/2330 train_time:14182ms step_avg:61.13ms
step:233/2330 train_time:14243ms step_avg:61.13ms
step:234/2330 train_time:14306ms step_avg:61.14ms
step:235/2330 train_time:14366ms step_avg:61.13ms
step:236/2330 train_time:14428ms step_avg:61.14ms
step:237/2330 train_time:14488ms step_avg:61.13ms
step:238/2330 train_time:14550ms step_avg:61.13ms
step:239/2330 train_time:14610ms step_avg:61.13ms
step:240/2330 train_time:14672ms step_avg:61.13ms
step:241/2330 train_time:14732ms step_avg:61.13ms
step:242/2330 train_time:14793ms step_avg:61.13ms
step:243/2330 train_time:14853ms step_avg:61.12ms
step:244/2330 train_time:14915ms step_avg:61.13ms
step:245/2330 train_time:14975ms step_avg:61.12ms
step:246/2330 train_time:15038ms step_avg:61.13ms
step:247/2330 train_time:15098ms step_avg:61.12ms
step:248/2330 train_time:15160ms step_avg:61.13ms
step:249/2330 train_time:15220ms step_avg:61.12ms
step:250/2330 train_time:15282ms step_avg:61.13ms
step:250/2330 val_loss:5.2344 train_time:15346ms step_avg:61.38ms
step:251/2330 train_time:15371ms step_avg:61.24ms
step:252/2330 train_time:15407ms step_avg:61.14ms
step:253/2330 train_time:15472ms step_avg:61.16ms
step:254/2330 train_time:15537ms step_avg:61.17ms
step:255/2330 train_time:15598ms step_avg:61.17ms
step:256/2330 train_time:15661ms step_avg:61.18ms
step:257/2330 train_time:15721ms step_avg:61.17ms
step:258/2330 train_time:15782ms step_avg:61.17ms
step:259/2330 train_time:15841ms step_avg:61.16ms
step:260/2330 train_time:15903ms step_avg:61.16ms
step:261/2330 train_time:15962ms step_avg:61.16ms
step:262/2330 train_time:16023ms step_avg:61.16ms
step:263/2330 train_time:16082ms step_avg:61.15ms
step:264/2330 train_time:16143ms step_avg:61.15ms
step:265/2330 train_time:16202ms step_avg:61.14ms
step:266/2330 train_time:16263ms step_avg:61.14ms
step:267/2330 train_time:16324ms step_avg:61.14ms
step:268/2330 train_time:16388ms step_avg:61.15ms
step:269/2330 train_time:16451ms step_avg:61.15ms
step:270/2330 train_time:16514ms step_avg:61.16ms
step:271/2330 train_time:16575ms step_avg:61.16ms
step:272/2330 train_time:16638ms step_avg:61.17ms
step:273/2330 train_time:16698ms step_avg:61.16ms
step:274/2330 train_time:16760ms step_avg:61.17ms
step:275/2330 train_time:16820ms step_avg:61.16ms
step:276/2330 train_time:16882ms step_avg:61.17ms
step:277/2330 train_time:16941ms step_avg:61.16ms
step:278/2330 train_time:17003ms step_avg:61.16ms
step:279/2330 train_time:17062ms step_avg:61.15ms
step:280/2330 train_time:17123ms step_avg:61.15ms
step:281/2330 train_time:17183ms step_avg:61.15ms
step:282/2330 train_time:17244ms step_avg:61.15ms
step:283/2330 train_time:17304ms step_avg:61.15ms
step:284/2330 train_time:17368ms step_avg:61.15ms
step:285/2330 train_time:17429ms step_avg:61.15ms
step:286/2330 train_time:17493ms step_avg:61.16ms
step:287/2330 train_time:17554ms step_avg:61.16ms
step:288/2330 train_time:17616ms step_avg:61.17ms
step:289/2330 train_time:17675ms step_avg:61.16ms
step:290/2330 train_time:17738ms step_avg:61.16ms
step:291/2330 train_time:17798ms step_avg:61.16ms
step:292/2330 train_time:17861ms step_avg:61.17ms
step:293/2330 train_time:17920ms step_avg:61.16ms
step:294/2330 train_time:17982ms step_avg:61.16ms
step:295/2330 train_time:18041ms step_avg:61.16ms
step:296/2330 train_time:18103ms step_avg:61.16ms
step:297/2330 train_time:18163ms step_avg:61.15ms
step:298/2330 train_time:18225ms step_avg:61.16ms
step:299/2330 train_time:18285ms step_avg:61.15ms
step:300/2330 train_time:18348ms step_avg:61.16ms
step:301/2330 train_time:18410ms step_avg:61.16ms
step:302/2330 train_time:18473ms step_avg:61.17ms
step:303/2330 train_time:18533ms step_avg:61.16ms
step:304/2330 train_time:18595ms step_avg:61.17ms
step:305/2330 train_time:18655ms step_avg:61.16ms
step:306/2330 train_time:18717ms step_avg:61.17ms
step:307/2330 train_time:18777ms step_avg:61.16ms
step:308/2330 train_time:18839ms step_avg:61.16ms
step:309/2330 train_time:18899ms step_avg:61.16ms
step:310/2330 train_time:18961ms step_avg:61.16ms
step:311/2330 train_time:19021ms step_avg:61.16ms
step:312/2330 train_time:19083ms step_avg:61.16ms
step:313/2330 train_time:19143ms step_avg:61.16ms
step:314/2330 train_time:19205ms step_avg:61.16ms
step:315/2330 train_time:19265ms step_avg:61.16ms
step:316/2330 train_time:19327ms step_avg:61.16ms
step:317/2330 train_time:19389ms step_avg:61.16ms
step:318/2330 train_time:19451ms step_avg:61.17ms
step:319/2330 train_time:19512ms step_avg:61.16ms
step:320/2330 train_time:19574ms step_avg:61.17ms
step:321/2330 train_time:19634ms step_avg:61.17ms
step:322/2330 train_time:19696ms step_avg:61.17ms
step:323/2330 train_time:19756ms step_avg:61.17ms
step:324/2330 train_time:19818ms step_avg:61.17ms
step:325/2330 train_time:19878ms step_avg:61.16ms
step:326/2330 train_time:19940ms step_avg:61.17ms
step:327/2330 train_time:20000ms step_avg:61.16ms
step:328/2330 train_time:20062ms step_avg:61.16ms
step:329/2330 train_time:20122ms step_avg:61.16ms
step:330/2330 train_time:20183ms step_avg:61.16ms
step:331/2330 train_time:20243ms step_avg:61.16ms
step:332/2330 train_time:20306ms step_avg:61.16ms
step:333/2330 train_time:20366ms step_avg:61.16ms
step:334/2330 train_time:20429ms step_avg:61.16ms
step:335/2330 train_time:20489ms step_avg:61.16ms
step:336/2330 train_time:20552ms step_avg:61.17ms
step:337/2330 train_time:20612ms step_avg:61.16ms
step:338/2330 train_time:20674ms step_avg:61.17ms
step:339/2330 train_time:20734ms step_avg:61.16ms
step:340/2330 train_time:20796ms step_avg:61.17ms
step:341/2330 train_time:20855ms step_avg:61.16ms
step:342/2330 train_time:20917ms step_avg:61.16ms
step:343/2330 train_time:20977ms step_avg:61.16ms
step:344/2330 train_time:21040ms step_avg:61.16ms
step:345/2330 train_time:21101ms step_avg:61.16ms
step:346/2330 train_time:21163ms step_avg:61.17ms
step:347/2330 train_time:21223ms step_avg:61.16ms
step:348/2330 train_time:21284ms step_avg:61.16ms
step:349/2330 train_time:21344ms step_avg:61.16ms
step:350/2330 train_time:21407ms step_avg:61.16ms
step:351/2330 train_time:21467ms step_avg:61.16ms
step:352/2330 train_time:21529ms step_avg:61.16ms
step:353/2330 train_time:21590ms step_avg:61.16ms
step:354/2330 train_time:21651ms step_avg:61.16ms
step:355/2330 train_time:21710ms step_avg:61.15ms
step:356/2330 train_time:21772ms step_avg:61.16ms
step:357/2330 train_time:21833ms step_avg:61.16ms
step:358/2330 train_time:21895ms step_avg:61.16ms
step:359/2330 train_time:21954ms step_avg:61.15ms
step:360/2330 train_time:22016ms step_avg:61.16ms
step:361/2330 train_time:22076ms step_avg:61.15ms
step:362/2330 train_time:22139ms step_avg:61.16ms
step:363/2330 train_time:22201ms step_avg:61.16ms
step:364/2330 train_time:22263ms step_avg:61.16ms
step:365/2330 train_time:22323ms step_avg:61.16ms
step:366/2330 train_time:22385ms step_avg:61.16ms
step:367/2330 train_time:22445ms step_avg:61.16ms
step:368/2330 train_time:22508ms step_avg:61.16ms
step:369/2330 train_time:22569ms step_avg:61.16ms
step:370/2330 train_time:22630ms step_avg:61.16ms
step:371/2330 train_time:22690ms step_avg:61.16ms
step:372/2330 train_time:22752ms step_avg:61.16ms
step:373/2330 train_time:22812ms step_avg:61.16ms
step:374/2330 train_time:22874ms step_avg:61.16ms
step:375/2330 train_time:22934ms step_avg:61.16ms
step:376/2330 train_time:22996ms step_avg:61.16ms
step:377/2330 train_time:23056ms step_avg:61.16ms
step:378/2330 train_time:23118ms step_avg:61.16ms
step:379/2330 train_time:23179ms step_avg:61.16ms
step:380/2330 train_time:23241ms step_avg:61.16ms
step:381/2330 train_time:23301ms step_avg:61.16ms
step:382/2330 train_time:23364ms step_avg:61.16ms
step:383/2330 train_time:23424ms step_avg:61.16ms
step:384/2330 train_time:23486ms step_avg:61.16ms
step:385/2330 train_time:23546ms step_avg:61.16ms
step:386/2330 train_time:23609ms step_avg:61.16ms
step:387/2330 train_time:23669ms step_avg:61.16ms
step:388/2330 train_time:23730ms step_avg:61.16ms
step:389/2330 train_time:23790ms step_avg:61.16ms
step:390/2330 train_time:23852ms step_avg:61.16ms
step:391/2330 train_time:23912ms step_avg:61.16ms
step:392/2330 train_time:23974ms step_avg:61.16ms
step:393/2330 train_time:24034ms step_avg:61.16ms
step:394/2330 train_time:24097ms step_avg:61.16ms
step:395/2330 train_time:24156ms step_avg:61.16ms
step:396/2330 train_time:24219ms step_avg:61.16ms
step:397/2330 train_time:24280ms step_avg:61.16ms
step:398/2330 train_time:24343ms step_avg:61.16ms
step:399/2330 train_time:24404ms step_avg:61.16ms
step:400/2330 train_time:24465ms step_avg:61.16ms
step:401/2330 train_time:24526ms step_avg:61.16ms
step:402/2330 train_time:24588ms step_avg:61.16ms
step:403/2330 train_time:24647ms step_avg:61.16ms
step:404/2330 train_time:24709ms step_avg:61.16ms
step:405/2330 train_time:24769ms step_avg:61.16ms
step:406/2330 train_time:24831ms step_avg:61.16ms
step:407/2330 train_time:24891ms step_avg:61.16ms
step:408/2330 train_time:24953ms step_avg:61.16ms
step:409/2330 train_time:25013ms step_avg:61.16ms
step:410/2330 train_time:25075ms step_avg:61.16ms
step:411/2330 train_time:25135ms step_avg:61.16ms
step:412/2330 train_time:25198ms step_avg:61.16ms
step:413/2330 train_time:25258ms step_avg:61.16ms
step:414/2330 train_time:25321ms step_avg:61.16ms
step:415/2330 train_time:25381ms step_avg:61.16ms
step:416/2330 train_time:25444ms step_avg:61.16ms
step:417/2330 train_time:25504ms step_avg:61.16ms
step:418/2330 train_time:25566ms step_avg:61.16ms
step:419/2330 train_time:25625ms step_avg:61.16ms
step:420/2330 train_time:25688ms step_avg:61.16ms
step:421/2330 train_time:25748ms step_avg:61.16ms
step:422/2330 train_time:25810ms step_avg:61.16ms
step:423/2330 train_time:25870ms step_avg:61.16ms
step:424/2330 train_time:25932ms step_avg:61.16ms
step:425/2330 train_time:25992ms step_avg:61.16ms
step:426/2330 train_time:26054ms step_avg:61.16ms
step:427/2330 train_time:26113ms step_avg:61.16ms
step:428/2330 train_time:26175ms step_avg:61.16ms
step:429/2330 train_time:26235ms step_avg:61.15ms
step:430/2330 train_time:26298ms step_avg:61.16ms
step:431/2330 train_time:26358ms step_avg:61.16ms
step:432/2330 train_time:26422ms step_avg:61.16ms
step:433/2330 train_time:26482ms step_avg:61.16ms
step:434/2330 train_time:26544ms step_avg:61.16ms
step:435/2330 train_time:26604ms step_avg:61.16ms
step:436/2330 train_time:26667ms step_avg:61.16ms
step:437/2330 train_time:26727ms step_avg:61.16ms
step:438/2330 train_time:26789ms step_avg:61.16ms
step:439/2330 train_time:26849ms step_avg:61.16ms
step:440/2330 train_time:26912ms step_avg:61.16ms
step:441/2330 train_time:26971ms step_avg:61.16ms
step:442/2330 train_time:27033ms step_avg:61.16ms
step:443/2330 train_time:27093ms step_avg:61.16ms
step:444/2330 train_time:27155ms step_avg:61.16ms
step:445/2330 train_time:27214ms step_avg:61.16ms
step:446/2330 train_time:27276ms step_avg:61.16ms
step:447/2330 train_time:27337ms step_avg:61.16ms
step:448/2330 train_time:27400ms step_avg:61.16ms
step:449/2330 train_time:27460ms step_avg:61.16ms
step:450/2330 train_time:27522ms step_avg:61.16ms
step:451/2330 train_time:27582ms step_avg:61.16ms
step:452/2330 train_time:27644ms step_avg:61.16ms
step:453/2330 train_time:27705ms step_avg:61.16ms
step:454/2330 train_time:27767ms step_avg:61.16ms
step:455/2330 train_time:27828ms step_avg:61.16ms
step:456/2330 train_time:27891ms step_avg:61.16ms
step:457/2330 train_time:27950ms step_avg:61.16ms
step:458/2330 train_time:28012ms step_avg:61.16ms
step:459/2330 train_time:28071ms step_avg:61.16ms
step:460/2330 train_time:28133ms step_avg:61.16ms
step:461/2330 train_time:28193ms step_avg:61.16ms
step:462/2330 train_time:28255ms step_avg:61.16ms
step:463/2330 train_time:28315ms step_avg:61.15ms
step:464/2330 train_time:28377ms step_avg:61.16ms
step:465/2330 train_time:28438ms step_avg:61.16ms
step:466/2330 train_time:28502ms step_avg:61.16ms
step:467/2330 train_time:28562ms step_avg:61.16ms
step:468/2330 train_time:28624ms step_avg:61.16ms
step:469/2330 train_time:28684ms step_avg:61.16ms
step:470/2330 train_time:28747ms step_avg:61.16ms
step:471/2330 train_time:28807ms step_avg:61.16ms
step:472/2330 train_time:28869ms step_avg:61.16ms
step:473/2330 train_time:28929ms step_avg:61.16ms
step:474/2330 train_time:28991ms step_avg:61.16ms
step:475/2330 train_time:29051ms step_avg:61.16ms
step:476/2330 train_time:29112ms step_avg:61.16ms
step:477/2330 train_time:29172ms step_avg:61.16ms
step:478/2330 train_time:29234ms step_avg:61.16ms
step:479/2330 train_time:29295ms step_avg:61.16ms
step:480/2330 train_time:29357ms step_avg:61.16ms
step:481/2330 train_time:29417ms step_avg:61.16ms
step:482/2330 train_time:29479ms step_avg:61.16ms
step:483/2330 train_time:29539ms step_avg:61.16ms
step:484/2330 train_time:29601ms step_avg:61.16ms
step:485/2330 train_time:29662ms step_avg:61.16ms
step:486/2330 train_time:29724ms step_avg:61.16ms
step:487/2330 train_time:29785ms step_avg:61.16ms
step:488/2330 train_time:29846ms step_avg:61.16ms
step:489/2330 train_time:29907ms step_avg:61.16ms
step:490/2330 train_time:29969ms step_avg:61.16ms
step:491/2330 train_time:30030ms step_avg:61.16ms
step:492/2330 train_time:30092ms step_avg:61.16ms
step:493/2330 train_time:30152ms step_avg:61.16ms
step:494/2330 train_time:30214ms step_avg:61.16ms
step:495/2330 train_time:30274ms step_avg:61.16ms
step:496/2330 train_time:30336ms step_avg:61.16ms
step:497/2330 train_time:30396ms step_avg:61.16ms
step:498/2330 train_time:30458ms step_avg:61.16ms
step:499/2330 train_time:30518ms step_avg:61.16ms
step:500/2330 train_time:30581ms step_avg:61.16ms
step:500/2330 val_loss:4.7518 train_time:30646ms step_avg:61.29ms
step:501/2330 train_time:30669ms step_avg:61.22ms
step:502/2330 train_time:30706ms step_avg:61.17ms
step:503/2330 train_time:30771ms step_avg:61.18ms
step:504/2330 train_time:30836ms step_avg:61.18ms
step:505/2330 train_time:30895ms step_avg:61.18ms
step:506/2330 train_time:30957ms step_avg:61.18ms
step:507/2330 train_time:31017ms step_avg:61.18ms
step:508/2330 train_time:31079ms step_avg:61.18ms
step:509/2330 train_time:31139ms step_avg:61.18ms
step:510/2330 train_time:31200ms step_avg:61.18ms
step:511/2330 train_time:31259ms step_avg:61.17ms
step:512/2330 train_time:31321ms step_avg:61.17ms
step:513/2330 train_time:31380ms step_avg:61.17ms
step:514/2330 train_time:31441ms step_avg:61.17ms
step:515/2330 train_time:31500ms step_avg:61.17ms
step:516/2330 train_time:31562ms step_avg:61.17ms
step:517/2330 train_time:31623ms step_avg:61.17ms
step:518/2330 train_time:31686ms step_avg:61.17ms
step:519/2330 train_time:31747ms step_avg:61.17ms
step:520/2330 train_time:31810ms step_avg:61.17ms
step:521/2330 train_time:31871ms step_avg:61.17ms
step:522/2330 train_time:31934ms step_avg:61.18ms
step:523/2330 train_time:31994ms step_avg:61.17ms
step:524/2330 train_time:32055ms step_avg:61.17ms
step:525/2330 train_time:32116ms step_avg:61.17ms
step:526/2330 train_time:32178ms step_avg:61.17ms
step:527/2330 train_time:32238ms step_avg:61.17ms
step:528/2330 train_time:32299ms step_avg:61.17ms
step:529/2330 train_time:32359ms step_avg:61.17ms
step:530/2330 train_time:32421ms step_avg:61.17ms
step:531/2330 train_time:32480ms step_avg:61.17ms
step:532/2330 train_time:32541ms step_avg:61.17ms
step:533/2330 train_time:32601ms step_avg:61.17ms
step:534/2330 train_time:32664ms step_avg:61.17ms
step:535/2330 train_time:32725ms step_avg:61.17ms
step:536/2330 train_time:32788ms step_avg:61.17ms
step:537/2330 train_time:32849ms step_avg:61.17ms
step:538/2330 train_time:32911ms step_avg:61.17ms
step:539/2330 train_time:32971ms step_avg:61.17ms
step:540/2330 train_time:33034ms step_avg:61.17ms
step:541/2330 train_time:33095ms step_avg:61.17ms
step:542/2330 train_time:33158ms step_avg:61.18ms
step:543/2330 train_time:33218ms step_avg:61.17ms
step:544/2330 train_time:33280ms step_avg:61.18ms
step:545/2330 train_time:33339ms step_avg:61.17ms
step:546/2330 train_time:33401ms step_avg:61.17ms
step:547/2330 train_time:33461ms step_avg:61.17ms
step:548/2330 train_time:33523ms step_avg:61.17ms
step:549/2330 train_time:33583ms step_avg:61.17ms
step:550/2330 train_time:33645ms step_avg:61.17ms
step:551/2330 train_time:33704ms step_avg:61.17ms
step:552/2330 train_time:33768ms step_avg:61.17ms
step:553/2330 train_time:33828ms step_avg:61.17ms
step:554/2330 train_time:33891ms step_avg:61.18ms
step:555/2330 train_time:33952ms step_avg:61.17ms
step:556/2330 train_time:34014ms step_avg:61.18ms
step:557/2330 train_time:34074ms step_avg:61.17ms
step:558/2330 train_time:34137ms step_avg:61.18ms
step:559/2330 train_time:34197ms step_avg:61.17ms
step:560/2330 train_time:34258ms step_avg:61.18ms
step:561/2330 train_time:34318ms step_avg:61.17ms
step:562/2330 train_time:34380ms step_avg:61.18ms
step:563/2330 train_time:34440ms step_avg:61.17ms
step:564/2330 train_time:34502ms step_avg:61.17ms
step:565/2330 train_time:34561ms step_avg:61.17ms
step:566/2330 train_time:34623ms step_avg:61.17ms
step:567/2330 train_time:34683ms step_avg:61.17ms
step:568/2330 train_time:34745ms step_avg:61.17ms
step:569/2330 train_time:34805ms step_avg:61.17ms
step:570/2330 train_time:34868ms step_avg:61.17ms
step:571/2330 train_time:34929ms step_avg:61.17ms
step:572/2330 train_time:34992ms step_avg:61.17ms
step:573/2330 train_time:35052ms step_avg:61.17ms
step:574/2330 train_time:35114ms step_avg:61.18ms
step:575/2330 train_time:35175ms step_avg:61.17ms
step:576/2330 train_time:35237ms step_avg:61.18ms
step:577/2330 train_time:35298ms step_avg:61.17ms
step:578/2330 train_time:35360ms step_avg:61.18ms
step:579/2330 train_time:35420ms step_avg:61.17ms
step:580/2330 train_time:35482ms step_avg:61.18ms
step:581/2330 train_time:35541ms step_avg:61.17ms
step:582/2330 train_time:35603ms step_avg:61.17ms
step:583/2330 train_time:35663ms step_avg:61.17ms
step:584/2330 train_time:35725ms step_avg:61.17ms
step:585/2330 train_time:35786ms step_avg:61.17ms
step:586/2330 train_time:35848ms step_avg:61.17ms
step:587/2330 train_time:35907ms step_avg:61.17ms
step:588/2330 train_time:35970ms step_avg:61.17ms
step:589/2330 train_time:36032ms step_avg:61.17ms
step:590/2330 train_time:36093ms step_avg:61.18ms
step:591/2330 train_time:36153ms step_avg:61.17ms
step:592/2330 train_time:36215ms step_avg:61.17ms
step:593/2330 train_time:36275ms step_avg:61.17ms
step:594/2330 train_time:36337ms step_avg:61.17ms
step:595/2330 train_time:36397ms step_avg:61.17ms
step:596/2330 train_time:36460ms step_avg:61.17ms
step:597/2330 train_time:36520ms step_avg:61.17ms
step:598/2330 train_time:36582ms step_avg:61.17ms
step:599/2330 train_time:36642ms step_avg:61.17ms
step:600/2330 train_time:36703ms step_avg:61.17ms
step:601/2330 train_time:36763ms step_avg:61.17ms
step:602/2330 train_time:36826ms step_avg:61.17ms
step:603/2330 train_time:36886ms step_avg:61.17ms
step:604/2330 train_time:36948ms step_avg:61.17ms
step:605/2330 train_time:37009ms step_avg:61.17ms
step:606/2330 train_time:37072ms step_avg:61.17ms
step:607/2330 train_time:37133ms step_avg:61.17ms
step:608/2330 train_time:37194ms step_avg:61.18ms
step:609/2330 train_time:37254ms step_avg:61.17ms
step:610/2330 train_time:37316ms step_avg:61.17ms
step:611/2330 train_time:37377ms step_avg:61.17ms
step:612/2330 train_time:37439ms step_avg:61.18ms
step:613/2330 train_time:37499ms step_avg:61.17ms
step:614/2330 train_time:37561ms step_avg:61.17ms
step:615/2330 train_time:37621ms step_avg:61.17ms
step:616/2330 train_time:37683ms step_avg:61.17ms
step:617/2330 train_time:37742ms step_avg:61.17ms
step:618/2330 train_time:37804ms step_avg:61.17ms
step:619/2330 train_time:37865ms step_avg:61.17ms
step:620/2330 train_time:37927ms step_avg:61.17ms
step:621/2330 train_time:37987ms step_avg:61.17ms
step:622/2330 train_time:38050ms step_avg:61.17ms
step:623/2330 train_time:38111ms step_avg:61.17ms
step:624/2330 train_time:38173ms step_avg:61.17ms
step:625/2330 train_time:38233ms step_avg:61.17ms
step:626/2330 train_time:38296ms step_avg:61.17ms
step:627/2330 train_time:38356ms step_avg:61.17ms
step:628/2330 train_time:38418ms step_avg:61.17ms
step:629/2330 train_time:38478ms step_avg:61.17ms
step:630/2330 train_time:38541ms step_avg:61.18ms
step:631/2330 train_time:38601ms step_avg:61.17ms
step:632/2330 train_time:38663ms step_avg:61.18ms
step:633/2330 train_time:38724ms step_avg:61.18ms
step:634/2330 train_time:38786ms step_avg:61.18ms
step:635/2330 train_time:38846ms step_avg:61.17ms
step:636/2330 train_time:38907ms step_avg:61.18ms
step:637/2330 train_time:38968ms step_avg:61.17ms
step:638/2330 train_time:39031ms step_avg:61.18ms
step:639/2330 train_time:39091ms step_avg:61.18ms
step:640/2330 train_time:39154ms step_avg:61.18ms
step:641/2330 train_time:39214ms step_avg:61.18ms
step:642/2330 train_time:39276ms step_avg:61.18ms
step:643/2330 train_time:39337ms step_avg:61.18ms
step:644/2330 train_time:39399ms step_avg:61.18ms
step:645/2330 train_time:39459ms step_avg:61.18ms
step:646/2330 train_time:39521ms step_avg:61.18ms
step:647/2330 train_time:39580ms step_avg:61.18ms
step:648/2330 train_time:39643ms step_avg:61.18ms
step:649/2330 train_time:39702ms step_avg:61.17ms
step:650/2330 train_time:39764ms step_avg:61.17ms
step:651/2330 train_time:39823ms step_avg:61.17ms
step:652/2330 train_time:39886ms step_avg:61.17ms
step:653/2330 train_time:39947ms step_avg:61.17ms
step:654/2330 train_time:40009ms step_avg:61.18ms
step:655/2330 train_time:40069ms step_avg:61.17ms
step:656/2330 train_time:40132ms step_avg:61.18ms
step:657/2330 train_time:40193ms step_avg:61.18ms
step:658/2330 train_time:40255ms step_avg:61.18ms
step:659/2330 train_time:40315ms step_avg:61.18ms
step:660/2330 train_time:40377ms step_avg:61.18ms
step:661/2330 train_time:40438ms step_avg:61.18ms
step:662/2330 train_time:40500ms step_avg:61.18ms
step:663/2330 train_time:40560ms step_avg:61.18ms
step:664/2330 train_time:40623ms step_avg:61.18ms
step:665/2330 train_time:40683ms step_avg:61.18ms
step:666/2330 train_time:40745ms step_avg:61.18ms
step:667/2330 train_time:40804ms step_avg:61.18ms
step:668/2330 train_time:40866ms step_avg:61.18ms
step:669/2330 train_time:40927ms step_avg:61.18ms
step:670/2330 train_time:40988ms step_avg:61.18ms
step:671/2330 train_time:41049ms step_avg:61.18ms
step:672/2330 train_time:41112ms step_avg:61.18ms
step:673/2330 train_time:41173ms step_avg:61.18ms
step:674/2330 train_time:41236ms step_avg:61.18ms
step:675/2330 train_time:41296ms step_avg:61.18ms
step:676/2330 train_time:41358ms step_avg:61.18ms
step:677/2330 train_time:41418ms step_avg:61.18ms
step:678/2330 train_time:41480ms step_avg:61.18ms
step:679/2330 train_time:41540ms step_avg:61.18ms
step:680/2330 train_time:41602ms step_avg:61.18ms
step:681/2330 train_time:41662ms step_avg:61.18ms
step:682/2330 train_time:41725ms step_avg:61.18ms
step:683/2330 train_time:41784ms step_avg:61.18ms
step:684/2330 train_time:41847ms step_avg:61.18ms
step:685/2330 train_time:41907ms step_avg:61.18ms
step:686/2330 train_time:41969ms step_avg:61.18ms
step:687/2330 train_time:42029ms step_avg:61.18ms
step:688/2330 train_time:42092ms step_avg:61.18ms
step:689/2330 train_time:42153ms step_avg:61.18ms
step:690/2330 train_time:42215ms step_avg:61.18ms
step:691/2330 train_time:42276ms step_avg:61.18ms
step:692/2330 train_time:42338ms step_avg:61.18ms
step:693/2330 train_time:42397ms step_avg:61.18ms
step:694/2330 train_time:42461ms step_avg:61.18ms
step:695/2330 train_time:42521ms step_avg:61.18ms
step:696/2330 train_time:42582ms step_avg:61.18ms
step:697/2330 train_time:42642ms step_avg:61.18ms
step:698/2330 train_time:42703ms step_avg:61.18ms
step:699/2330 train_time:42764ms step_avg:61.18ms
step:700/2330 train_time:42825ms step_avg:61.18ms
step:701/2330 train_time:42886ms step_avg:61.18ms
step:702/2330 train_time:42948ms step_avg:61.18ms
step:703/2330 train_time:43009ms step_avg:61.18ms
step:704/2330 train_time:43071ms step_avg:61.18ms
step:705/2330 train_time:43131ms step_avg:61.18ms
step:706/2330 train_time:43194ms step_avg:61.18ms
step:707/2330 train_time:43254ms step_avg:61.18ms
step:708/2330 train_time:43316ms step_avg:61.18ms
step:709/2330 train_time:43377ms step_avg:61.18ms
step:710/2330 train_time:43439ms step_avg:61.18ms
step:711/2330 train_time:43500ms step_avg:61.18ms
step:712/2330 train_time:43562ms step_avg:61.18ms
step:713/2330 train_time:43622ms step_avg:61.18ms
step:714/2330 train_time:43684ms step_avg:61.18ms
step:715/2330 train_time:43744ms step_avg:61.18ms
step:716/2330 train_time:43806ms step_avg:61.18ms
step:717/2330 train_time:43866ms step_avg:61.18ms
step:718/2330 train_time:43929ms step_avg:61.18ms
step:719/2330 train_time:43989ms step_avg:61.18ms
step:720/2330 train_time:44051ms step_avg:61.18ms
step:721/2330 train_time:44112ms step_avg:61.18ms
step:722/2330 train_time:44174ms step_avg:61.18ms
step:723/2330 train_time:44234ms step_avg:61.18ms
step:724/2330 train_time:44296ms step_avg:61.18ms
step:725/2330 train_time:44356ms step_avg:61.18ms
step:726/2330 train_time:44420ms step_avg:61.18ms
step:727/2330 train_time:44479ms step_avg:61.18ms
step:728/2330 train_time:44541ms step_avg:61.18ms
step:729/2330 train_time:44602ms step_avg:61.18ms
step:730/2330 train_time:44664ms step_avg:61.18ms
step:731/2330 train_time:44724ms step_avg:61.18ms
step:732/2330 train_time:44786ms step_avg:61.18ms
step:733/2330 train_time:44846ms step_avg:61.18ms
step:734/2330 train_time:44909ms step_avg:61.18ms
step:735/2330 train_time:44969ms step_avg:61.18ms
step:736/2330 train_time:45032ms step_avg:61.18ms
step:737/2330 train_time:45092ms step_avg:61.18ms
step:738/2330 train_time:45154ms step_avg:61.18ms
step:739/2330 train_time:45214ms step_avg:61.18ms
step:740/2330 train_time:45277ms step_avg:61.18ms
step:741/2330 train_time:45337ms step_avg:61.18ms
step:742/2330 train_time:45399ms step_avg:61.18ms
step:743/2330 train_time:45460ms step_avg:61.18ms
step:744/2330 train_time:45522ms step_avg:61.19ms
step:745/2330 train_time:45582ms step_avg:61.18ms
step:746/2330 train_time:45645ms step_avg:61.19ms
step:747/2330 train_time:45704ms step_avg:61.18ms
step:748/2330 train_time:45766ms step_avg:61.18ms
step:749/2330 train_time:45826ms step_avg:61.18ms
step:750/2330 train_time:45888ms step_avg:61.18ms
step:750/2330 val_loss:4.6482 train_time:45953ms step_avg:61.27ms
step:751/2330 train_time:45976ms step_avg:61.22ms
step:752/2330 train_time:46012ms step_avg:61.19ms
step:753/2330 train_time:46077ms step_avg:61.19ms
step:754/2330 train_time:46142ms step_avg:61.20ms
step:755/2330 train_time:46204ms step_avg:61.20ms
step:756/2330 train_time:46267ms step_avg:61.20ms
step:757/2330 train_time:46327ms step_avg:61.20ms
step:758/2330 train_time:46389ms step_avg:61.20ms
step:759/2330 train_time:46448ms step_avg:61.20ms
step:760/2330 train_time:46509ms step_avg:61.20ms
step:761/2330 train_time:46568ms step_avg:61.19ms
step:762/2330 train_time:46629ms step_avg:61.19ms
step:763/2330 train_time:46688ms step_avg:61.19ms
step:764/2330 train_time:46750ms step_avg:61.19ms
step:765/2330 train_time:46810ms step_avg:61.19ms
step:766/2330 train_time:46872ms step_avg:61.19ms
step:767/2330 train_time:46932ms step_avg:61.19ms
step:768/2330 train_time:46996ms step_avg:61.19ms
step:769/2330 train_time:47058ms step_avg:61.19ms
step:770/2330 train_time:47122ms step_avg:61.20ms
step:771/2330 train_time:47184ms step_avg:61.20ms
step:772/2330 train_time:47247ms step_avg:61.20ms
step:773/2330 train_time:47308ms step_avg:61.20ms
step:774/2330 train_time:47371ms step_avg:61.20ms
step:775/2330 train_time:47431ms step_avg:61.20ms
step:776/2330 train_time:47493ms step_avg:61.20ms
step:777/2330 train_time:47554ms step_avg:61.20ms
step:778/2330 train_time:47616ms step_avg:61.20ms
step:779/2330 train_time:47676ms step_avg:61.20ms
step:780/2330 train_time:47739ms step_avg:61.20ms
step:781/2330 train_time:47799ms step_avg:61.20ms
step:782/2330 train_time:47862ms step_avg:61.20ms
step:783/2330 train_time:47923ms step_avg:61.20ms
step:784/2330 train_time:47986ms step_avg:61.21ms
step:785/2330 train_time:48047ms step_avg:61.21ms
step:786/2330 train_time:48110ms step_avg:61.21ms
step:787/2330 train_time:48171ms step_avg:61.21ms
step:788/2330 train_time:48234ms step_avg:61.21ms
step:789/2330 train_time:48295ms step_avg:61.21ms
step:790/2330 train_time:48358ms step_avg:61.21ms
step:791/2330 train_time:48419ms step_avg:61.21ms
step:792/2330 train_time:48481ms step_avg:61.21ms
step:793/2330 train_time:48543ms step_avg:61.21ms
step:794/2330 train_time:48606ms step_avg:61.22ms
step:795/2330 train_time:48666ms step_avg:61.22ms
step:796/2330 train_time:48729ms step_avg:61.22ms
step:797/2330 train_time:48790ms step_avg:61.22ms
step:798/2330 train_time:48852ms step_avg:61.22ms
step:799/2330 train_time:48912ms step_avg:61.22ms
step:800/2330 train_time:48976ms step_avg:61.22ms
step:801/2330 train_time:49037ms step_avg:61.22ms
step:802/2330 train_time:49101ms step_avg:61.22ms
step:803/2330 train_time:49162ms step_avg:61.22ms
step:804/2330 train_time:49225ms step_avg:61.23ms
step:805/2330 train_time:49286ms step_avg:61.22ms
step:806/2330 train_time:49349ms step_avg:61.23ms
step:807/2330 train_time:49409ms step_avg:61.23ms
step:808/2330 train_time:49472ms step_avg:61.23ms
step:809/2330 train_time:49533ms step_avg:61.23ms
step:810/2330 train_time:49597ms step_avg:61.23ms
step:811/2330 train_time:49658ms step_avg:61.23ms
step:812/2330 train_time:49721ms step_avg:61.23ms
step:813/2330 train_time:49782ms step_avg:61.23ms
step:814/2330 train_time:49845ms step_avg:61.23ms
step:815/2330 train_time:49905ms step_avg:61.23ms
step:816/2330 train_time:49968ms step_avg:61.24ms
step:817/2330 train_time:50029ms step_avg:61.24ms
step:818/2330 train_time:50092ms step_avg:61.24ms
step:819/2330 train_time:50152ms step_avg:61.24ms
step:820/2330 train_time:50215ms step_avg:61.24ms
step:821/2330 train_time:50276ms step_avg:61.24ms
step:822/2330 train_time:50339ms step_avg:61.24ms
step:823/2330 train_time:50400ms step_avg:61.24ms
step:824/2330 train_time:50464ms step_avg:61.24ms
step:825/2330 train_time:50526ms step_avg:61.24ms
step:826/2330 train_time:50589ms step_avg:61.25ms
step:827/2330 train_time:50649ms step_avg:61.24ms
step:828/2330 train_time:50711ms step_avg:61.25ms
step:829/2330 train_time:50772ms step_avg:61.24ms
step:830/2330 train_time:50834ms step_avg:61.25ms
step:831/2330 train_time:50895ms step_avg:61.25ms
step:832/2330 train_time:50958ms step_avg:61.25ms
step:833/2330 train_time:51019ms step_avg:61.25ms
step:834/2330 train_time:51082ms step_avg:61.25ms
step:835/2330 train_time:51143ms step_avg:61.25ms
step:836/2330 train_time:51206ms step_avg:61.25ms
step:837/2330 train_time:51267ms step_avg:61.25ms
step:838/2330 train_time:51329ms step_avg:61.25ms
step:839/2330 train_time:51390ms step_avg:61.25ms
step:840/2330 train_time:51453ms step_avg:61.25ms
step:841/2330 train_time:51514ms step_avg:61.25ms
step:842/2330 train_time:51576ms step_avg:61.25ms
step:843/2330 train_time:51638ms step_avg:61.25ms
step:844/2330 train_time:51700ms step_avg:61.26ms
step:845/2330 train_time:51761ms step_avg:61.26ms
step:846/2330 train_time:51825ms step_avg:61.26ms
step:847/2330 train_time:51886ms step_avg:61.26ms
step:848/2330 train_time:51949ms step_avg:61.26ms
step:849/2330 train_time:52009ms step_avg:61.26ms
step:850/2330 train_time:52071ms step_avg:61.26ms
step:851/2330 train_time:52132ms step_avg:61.26ms
step:852/2330 train_time:52195ms step_avg:61.26ms
step:853/2330 train_time:52256ms step_avg:61.26ms
step:854/2330 train_time:52320ms step_avg:61.26ms
step:855/2330 train_time:52381ms step_avg:61.26ms
step:856/2330 train_time:52443ms step_avg:61.27ms
step:857/2330 train_time:52505ms step_avg:61.27ms
step:858/2330 train_time:52568ms step_avg:61.27ms
step:859/2330 train_time:52629ms step_avg:61.27ms
step:860/2330 train_time:52691ms step_avg:61.27ms
step:861/2330 train_time:52752ms step_avg:61.27ms
step:862/2330 train_time:52815ms step_avg:61.27ms
step:863/2330 train_time:52876ms step_avg:61.27ms
step:864/2330 train_time:52939ms step_avg:61.27ms
step:865/2330 train_time:53000ms step_avg:61.27ms
step:866/2330 train_time:53063ms step_avg:61.27ms
step:867/2330 train_time:53124ms step_avg:61.27ms
step:868/2330 train_time:53187ms step_avg:61.28ms
step:869/2330 train_time:53249ms step_avg:61.28ms
step:870/2330 train_time:53311ms step_avg:61.28ms
step:871/2330 train_time:53372ms step_avg:61.28ms
step:872/2330 train_time:53435ms step_avg:61.28ms
step:873/2330 train_time:53496ms step_avg:61.28ms
step:874/2330 train_time:53559ms step_avg:61.28ms
step:875/2330 train_time:53620ms step_avg:61.28ms
step:876/2330 train_time:53683ms step_avg:61.28ms
step:877/2330 train_time:53744ms step_avg:61.28ms
step:878/2330 train_time:53807ms step_avg:61.28ms
step:879/2330 train_time:53867ms step_avg:61.28ms
step:880/2330 train_time:53930ms step_avg:61.28ms
step:881/2330 train_time:53991ms step_avg:61.28ms
step:882/2330 train_time:54053ms step_avg:61.28ms
step:883/2330 train_time:54114ms step_avg:61.28ms
step:884/2330 train_time:54177ms step_avg:61.29ms
step:885/2330 train_time:54238ms step_avg:61.29ms
step:886/2330 train_time:54301ms step_avg:61.29ms
step:887/2330 train_time:54362ms step_avg:61.29ms
step:888/2330 train_time:54425ms step_avg:61.29ms
step:889/2330 train_time:54486ms step_avg:61.29ms
step:890/2330 train_time:54549ms step_avg:61.29ms
step:891/2330 train_time:54609ms step_avg:61.29ms
step:892/2330 train_time:54672ms step_avg:61.29ms
step:893/2330 train_time:54733ms step_avg:61.29ms
step:894/2330 train_time:54796ms step_avg:61.29ms
step:895/2330 train_time:54857ms step_avg:61.29ms
step:896/2330 train_time:54920ms step_avg:61.29ms
step:897/2330 train_time:54981ms step_avg:61.29ms
step:898/2330 train_time:55044ms step_avg:61.30ms
step:899/2330 train_time:55104ms step_avg:61.30ms
step:900/2330 train_time:55167ms step_avg:61.30ms
step:901/2330 train_time:55227ms step_avg:61.30ms
step:902/2330 train_time:55290ms step_avg:61.30ms
step:903/2330 train_time:55351ms step_avg:61.30ms
step:904/2330 train_time:55413ms step_avg:61.30ms
step:905/2330 train_time:55474ms step_avg:61.30ms
step:906/2330 train_time:55537ms step_avg:61.30ms
step:907/2330 train_time:55598ms step_avg:61.30ms
step:908/2330 train_time:55661ms step_avg:61.30ms
step:909/2330 train_time:55721ms step_avg:61.30ms
step:910/2330 train_time:55784ms step_avg:61.30ms
step:911/2330 train_time:55845ms step_avg:61.30ms
step:912/2330 train_time:55908ms step_avg:61.30ms
step:913/2330 train_time:55969ms step_avg:61.30ms
step:914/2330 train_time:56032ms step_avg:61.30ms
step:915/2330 train_time:56092ms step_avg:61.30ms
step:916/2330 train_time:56155ms step_avg:61.30ms
step:917/2330 train_time:56216ms step_avg:61.30ms
step:918/2330 train_time:56279ms step_avg:61.31ms
step:919/2330 train_time:56340ms step_avg:61.31ms
step:920/2330 train_time:56403ms step_avg:61.31ms
step:921/2330 train_time:56465ms step_avg:61.31ms
step:922/2330 train_time:56528ms step_avg:61.31ms
step:923/2330 train_time:56590ms step_avg:61.31ms
step:924/2330 train_time:56652ms step_avg:61.31ms
step:925/2330 train_time:56712ms step_avg:61.31ms
step:926/2330 train_time:56775ms step_avg:61.31ms
step:927/2330 train_time:56835ms step_avg:61.31ms
step:928/2330 train_time:56899ms step_avg:61.31ms
step:929/2330 train_time:56961ms step_avg:61.31ms
step:930/2330 train_time:57023ms step_avg:61.32ms
step:931/2330 train_time:57084ms step_avg:61.31ms
step:932/2330 train_time:57148ms step_avg:61.32ms
step:933/2330 train_time:57208ms step_avg:61.32ms
step:934/2330 train_time:57271ms step_avg:61.32ms
step:935/2330 train_time:57331ms step_avg:61.32ms
step:936/2330 train_time:57394ms step_avg:61.32ms
step:937/2330 train_time:57457ms step_avg:61.32ms
step:938/2330 train_time:57520ms step_avg:61.32ms
step:939/2330 train_time:57581ms step_avg:61.32ms
step:940/2330 train_time:57644ms step_avg:61.32ms
step:941/2330 train_time:57705ms step_avg:61.32ms
step:942/2330 train_time:57768ms step_avg:61.32ms
step:943/2330 train_time:57829ms step_avg:61.32ms
step:944/2330 train_time:57892ms step_avg:61.33ms
step:945/2330 train_time:57952ms step_avg:61.33ms
step:946/2330 train_time:58015ms step_avg:61.33ms
step:947/2330 train_time:58076ms step_avg:61.33ms
step:948/2330 train_time:58139ms step_avg:61.33ms
step:949/2330 train_time:58200ms step_avg:61.33ms
step:950/2330 train_time:58264ms step_avg:61.33ms
step:951/2330 train_time:58325ms step_avg:61.33ms
step:952/2330 train_time:58388ms step_avg:61.33ms
step:953/2330 train_time:58450ms step_avg:61.33ms
step:954/2330 train_time:58512ms step_avg:61.33ms
step:955/2330 train_time:58572ms step_avg:61.33ms
step:956/2330 train_time:58635ms step_avg:61.33ms
step:957/2330 train_time:58696ms step_avg:61.33ms
step:958/2330 train_time:58759ms step_avg:61.33ms
step:959/2330 train_time:58819ms step_avg:61.33ms
step:960/2330 train_time:58883ms step_avg:61.34ms
step:961/2330 train_time:58944ms step_avg:61.34ms
step:962/2330 train_time:59006ms step_avg:61.34ms
step:963/2330 train_time:59068ms step_avg:61.34ms
step:964/2330 train_time:59131ms step_avg:61.34ms
step:965/2330 train_time:59191ms step_avg:61.34ms
step:966/2330 train_time:59254ms step_avg:61.34ms
step:967/2330 train_time:59315ms step_avg:61.34ms
step:968/2330 train_time:59378ms step_avg:61.34ms
step:969/2330 train_time:59438ms step_avg:61.34ms
step:970/2330 train_time:59501ms step_avg:61.34ms
step:971/2330 train_time:59562ms step_avg:61.34ms
step:972/2330 train_time:59625ms step_avg:61.34ms
step:973/2330 train_time:59685ms step_avg:61.34ms
step:974/2330 train_time:59748ms step_avg:61.34ms
step:975/2330 train_time:59808ms step_avg:61.34ms
step:976/2330 train_time:59872ms step_avg:61.34ms
step:977/2330 train_time:59932ms step_avg:61.34ms
step:978/2330 train_time:59995ms step_avg:61.34ms
step:979/2330 train_time:60055ms step_avg:61.34ms
step:980/2330 train_time:60118ms step_avg:61.35ms
step:981/2330 train_time:60179ms step_avg:61.35ms
step:982/2330 train_time:60242ms step_avg:61.35ms
step:983/2330 train_time:60304ms step_avg:61.35ms
step:984/2330 train_time:60367ms step_avg:61.35ms
step:985/2330 train_time:60427ms step_avg:61.35ms
step:986/2330 train_time:60490ms step_avg:61.35ms
step:987/2330 train_time:60551ms step_avg:61.35ms
step:988/2330 train_time:60613ms step_avg:61.35ms
step:989/2330 train_time:60674ms step_avg:61.35ms
step:990/2330 train_time:60737ms step_avg:61.35ms
step:991/2330 train_time:60798ms step_avg:61.35ms
step:992/2330 train_time:60861ms step_avg:61.35ms
step:993/2330 train_time:60922ms step_avg:61.35ms
step:994/2330 train_time:60985ms step_avg:61.35ms
step:995/2330 train_time:61047ms step_avg:61.35ms
step:996/2330 train_time:61111ms step_avg:61.36ms
step:997/2330 train_time:61171ms step_avg:61.36ms
step:998/2330 train_time:61234ms step_avg:61.36ms
step:999/2330 train_time:61295ms step_avg:61.36ms
step:1000/2330 train_time:61358ms step_avg:61.36ms
step:1000/2330 val_loss:4.2964 train_time:61423ms step_avg:61.42ms
step:1001/2330 train_time:61448ms step_avg:61.39ms
step:1002/2330 train_time:61483ms step_avg:61.36ms
step:1003/2330 train_time:61548ms step_avg:61.36ms
step:1004/2330 train_time:61618ms step_avg:61.37ms
step:1005/2330 train_time:61680ms step_avg:61.37ms
step:1006/2330 train_time:61744ms step_avg:61.38ms
step:1007/2330 train_time:61804ms step_avg:61.37ms
step:1008/2330 train_time:61867ms step_avg:61.38ms
step:1009/2330 train_time:61927ms step_avg:61.37ms
step:1010/2330 train_time:61989ms step_avg:61.38ms
step:1011/2330 train_time:62050ms step_avg:61.37ms
step:1012/2330 train_time:62112ms step_avg:61.38ms
step:1013/2330 train_time:62172ms step_avg:61.37ms
step:1014/2330 train_time:62234ms step_avg:61.37ms
step:1015/2330 train_time:62294ms step_avg:61.37ms
step:1016/2330 train_time:62361ms step_avg:61.38ms
step:1017/2330 train_time:62423ms step_avg:61.38ms
step:1018/2330 train_time:62487ms step_avg:61.38ms
step:1019/2330 train_time:62550ms step_avg:61.38ms
step:1020/2330 train_time:62614ms step_avg:61.39ms
step:1021/2330 train_time:62676ms step_avg:61.39ms
step:1022/2330 train_time:62740ms step_avg:61.39ms
step:1023/2330 train_time:62801ms step_avg:61.39ms
step:1024/2330 train_time:62863ms step_avg:61.39ms
step:1025/2330 train_time:62923ms step_avg:61.39ms
step:1026/2330 train_time:62985ms step_avg:61.39ms
step:1027/2330 train_time:63046ms step_avg:61.39ms
step:1028/2330 train_time:63108ms step_avg:61.39ms
step:1029/2330 train_time:63169ms step_avg:61.39ms
step:1030/2330 train_time:63232ms step_avg:61.39ms
step:1031/2330 train_time:63292ms step_avg:61.39ms
step:1032/2330 train_time:63356ms step_avg:61.39ms
step:1033/2330 train_time:63418ms step_avg:61.39ms
step:1034/2330 train_time:63481ms step_avg:61.39ms
step:1035/2330 train_time:63543ms step_avg:61.39ms
step:1036/2330 train_time:63607ms step_avg:61.40ms
step:1037/2330 train_time:63670ms step_avg:61.40ms
step:1038/2330 train_time:63733ms step_avg:61.40ms
step:1039/2330 train_time:63794ms step_avg:61.40ms
step:1040/2330 train_time:63857ms step_avg:61.40ms
step:1041/2330 train_time:63918ms step_avg:61.40ms
step:1042/2330 train_time:63980ms step_avg:61.40ms
step:1043/2330 train_time:64041ms step_avg:61.40ms
step:1044/2330 train_time:64104ms step_avg:61.40ms
step:1045/2330 train_time:64165ms step_avg:61.40ms
step:1046/2330 train_time:64228ms step_avg:61.40ms
step:1047/2330 train_time:64289ms step_avg:61.40ms
step:1048/2330 train_time:64353ms step_avg:61.41ms
step:1049/2330 train_time:64413ms step_avg:61.40ms
step:1050/2330 train_time:64476ms step_avg:61.41ms
step:1051/2330 train_time:64537ms step_avg:61.41ms
step:1052/2330 train_time:64601ms step_avg:61.41ms
step:1053/2330 train_time:64662ms step_avg:61.41ms
step:1054/2330 train_time:64725ms step_avg:61.41ms
step:1055/2330 train_time:64786ms step_avg:61.41ms
step:1056/2330 train_time:64850ms step_avg:61.41ms
step:1057/2330 train_time:64911ms step_avg:61.41ms
step:1058/2330 train_time:64975ms step_avg:61.41ms
step:1059/2330 train_time:65036ms step_avg:61.41ms
step:1060/2330 train_time:65098ms step_avg:61.41ms
step:1061/2330 train_time:65159ms step_avg:61.41ms
step:1062/2330 train_time:65221ms step_avg:61.41ms
step:1063/2330 train_time:65282ms step_avg:61.41ms
step:1064/2330 train_time:65345ms step_avg:61.41ms
step:1065/2330 train_time:65408ms step_avg:61.42ms
step:1066/2330 train_time:65471ms step_avg:61.42ms
step:1067/2330 train_time:65532ms step_avg:61.42ms
step:1068/2330 train_time:65595ms step_avg:61.42ms
step:1069/2330 train_time:65657ms step_avg:61.42ms
step:1070/2330 train_time:65719ms step_avg:61.42ms
step:1071/2330 train_time:65781ms step_avg:61.42ms
step:1072/2330 train_time:65843ms step_avg:61.42ms
step:1073/2330 train_time:65904ms step_avg:61.42ms
step:1074/2330 train_time:65968ms step_avg:61.42ms
step:1075/2330 train_time:66029ms step_avg:61.42ms
step:1076/2330 train_time:66091ms step_avg:61.42ms
step:1077/2330 train_time:66152ms step_avg:61.42ms
step:1078/2330 train_time:66215ms step_avg:61.42ms
step:1079/2330 train_time:66276ms step_avg:61.42ms
step:1080/2330 train_time:66339ms step_avg:61.43ms
step:1081/2330 train_time:66401ms step_avg:61.43ms
step:1082/2330 train_time:66463ms step_avg:61.43ms
step:1083/2330 train_time:66523ms step_avg:61.42ms
step:1084/2330 train_time:66586ms step_avg:61.43ms
step:1085/2330 train_time:66647ms step_avg:61.43ms
step:1086/2330 train_time:66710ms step_avg:61.43ms
step:1087/2330 train_time:66771ms step_avg:61.43ms
step:1088/2330 train_time:66835ms step_avg:61.43ms
step:1089/2330 train_time:66897ms step_avg:61.43ms
step:1090/2330 train_time:66959ms step_avg:61.43ms
step:1091/2330 train_time:67020ms step_avg:61.43ms
step:1092/2330 train_time:67082ms step_avg:61.43ms
step:1093/2330 train_time:67143ms step_avg:61.43ms
step:1094/2330 train_time:67206ms step_avg:61.43ms
step:1095/2330 train_time:67266ms step_avg:61.43ms
step:1096/2330 train_time:67329ms step_avg:61.43ms
step:1097/2330 train_time:67390ms step_avg:61.43ms
step:1098/2330 train_time:67453ms step_avg:61.43ms
step:1099/2330 train_time:67515ms step_avg:61.43ms
step:1100/2330 train_time:67578ms step_avg:61.43ms
step:1101/2330 train_time:67638ms step_avg:61.43ms
step:1102/2330 train_time:67701ms step_avg:61.43ms
step:1103/2330 train_time:67761ms step_avg:61.43ms
step:1104/2330 train_time:67824ms step_avg:61.43ms
step:1105/2330 train_time:67885ms step_avg:61.43ms
step:1106/2330 train_time:67948ms step_avg:61.44ms
step:1107/2330 train_time:68008ms step_avg:61.43ms
step:1108/2330 train_time:68072ms step_avg:61.44ms
step:1109/2330 train_time:68133ms step_avg:61.44ms
step:1110/2330 train_time:68196ms step_avg:61.44ms
step:1111/2330 train_time:68257ms step_avg:61.44ms
step:1112/2330 train_time:68319ms step_avg:61.44ms
step:1113/2330 train_time:68380ms step_avg:61.44ms
step:1114/2330 train_time:68442ms step_avg:61.44ms
step:1115/2330 train_time:68503ms step_avg:61.44ms
step:1116/2330 train_time:68566ms step_avg:61.44ms
step:1117/2330 train_time:68627ms step_avg:61.44ms
step:1118/2330 train_time:68690ms step_avg:61.44ms
step:1119/2330 train_time:68751ms step_avg:61.44ms
step:1120/2330 train_time:68814ms step_avg:61.44ms
step:1121/2330 train_time:68875ms step_avg:61.44ms
step:1122/2330 train_time:68939ms step_avg:61.44ms
step:1123/2330 train_time:69000ms step_avg:61.44ms
step:1124/2330 train_time:69063ms step_avg:61.44ms
step:1125/2330 train_time:69123ms step_avg:61.44ms
step:1126/2330 train_time:69186ms step_avg:61.44ms
step:1127/2330 train_time:69247ms step_avg:61.44ms
step:1128/2330 train_time:69310ms step_avg:61.45ms
step:1129/2330 train_time:69372ms step_avg:61.45ms
step:1130/2330 train_time:69435ms step_avg:61.45ms
step:1131/2330 train_time:69496ms step_avg:61.45ms
step:1132/2330 train_time:69559ms step_avg:61.45ms
step:1133/2330 train_time:69619ms step_avg:61.45ms
step:1134/2330 train_time:69681ms step_avg:61.45ms
step:1135/2330 train_time:69742ms step_avg:61.45ms
step:1136/2330 train_time:69805ms step_avg:61.45ms
step:1137/2330 train_time:69866ms step_avg:61.45ms
step:1138/2330 train_time:69928ms step_avg:61.45ms
step:1139/2330 train_time:69989ms step_avg:61.45ms
step:1140/2330 train_time:70052ms step_avg:61.45ms
step:1141/2330 train_time:70113ms step_avg:61.45ms
step:1142/2330 train_time:70176ms step_avg:61.45ms
step:1143/2330 train_time:70237ms step_avg:61.45ms
step:1144/2330 train_time:70300ms step_avg:61.45ms
step:1145/2330 train_time:70361ms step_avg:61.45ms
step:1146/2330 train_time:70424ms step_avg:61.45ms
step:1147/2330 train_time:70485ms step_avg:61.45ms
step:1148/2330 train_time:70547ms step_avg:61.45ms
step:1149/2330 train_time:70608ms step_avg:61.45ms
step:1150/2330 train_time:70671ms step_avg:61.45ms
step:1151/2330 train_time:70732ms step_avg:61.45ms
step:1152/2330 train_time:70794ms step_avg:61.45ms
step:1153/2330 train_time:70856ms step_avg:61.45ms
step:1154/2330 train_time:70919ms step_avg:61.45ms
step:1155/2330 train_time:70979ms step_avg:61.45ms
step:1156/2330 train_time:71042ms step_avg:61.45ms
step:1157/2330 train_time:71103ms step_avg:61.45ms
step:1158/2330 train_time:71166ms step_avg:61.46ms
step:1159/2330 train_time:71226ms step_avg:61.46ms
step:1160/2330 train_time:71290ms step_avg:61.46ms
step:1161/2330 train_time:71350ms step_avg:61.46ms
step:1162/2330 train_time:71413ms step_avg:61.46ms
step:1163/2330 train_time:71474ms step_avg:61.46ms
step:1164/2330 train_time:71536ms step_avg:61.46ms
step:1165/2330 train_time:71597ms step_avg:61.46ms
step:1166/2330 train_time:71660ms step_avg:61.46ms
step:1167/2330 train_time:71720ms step_avg:61.46ms
step:1168/2330 train_time:71783ms step_avg:61.46ms
step:1169/2330 train_time:71844ms step_avg:61.46ms
step:1170/2330 train_time:71906ms step_avg:61.46ms
step:1171/2330 train_time:71968ms step_avg:61.46ms
step:1172/2330 train_time:72030ms step_avg:61.46ms
step:1173/2330 train_time:72091ms step_avg:61.46ms
step:1174/2330 train_time:72154ms step_avg:61.46ms
step:1175/2330 train_time:72215ms step_avg:61.46ms
step:1176/2330 train_time:72278ms step_avg:61.46ms
step:1177/2330 train_time:72338ms step_avg:61.46ms
step:1178/2330 train_time:72401ms step_avg:61.46ms
step:1179/2330 train_time:72462ms step_avg:61.46ms
step:1180/2330 train_time:72525ms step_avg:61.46ms
step:1181/2330 train_time:72586ms step_avg:61.46ms
step:1182/2330 train_time:72650ms step_avg:61.46ms
step:1183/2330 train_time:72711ms step_avg:61.46ms
step:1184/2330 train_time:72774ms step_avg:61.46ms
step:1185/2330 train_time:72835ms step_avg:61.46ms
step:1186/2330 train_time:72899ms step_avg:61.47ms
step:1187/2330 train_time:72959ms step_avg:61.47ms
step:1188/2330 train_time:73022ms step_avg:61.47ms
step:1189/2330 train_time:73083ms step_avg:61.47ms
step:1190/2330 train_time:73146ms step_avg:61.47ms
step:1191/2330 train_time:73207ms step_avg:61.47ms
step:1192/2330 train_time:73270ms step_avg:61.47ms
step:1193/2330 train_time:73331ms step_avg:61.47ms
step:1194/2330 train_time:73394ms step_avg:61.47ms
step:1195/2330 train_time:73456ms step_avg:61.47ms
step:1196/2330 train_time:73518ms step_avg:61.47ms
step:1197/2330 train_time:73579ms step_avg:61.47ms
step:1198/2330 train_time:73642ms step_avg:61.47ms
step:1199/2330 train_time:73702ms step_avg:61.47ms
step:1200/2330 train_time:73766ms step_avg:61.47ms
step:1201/2330 train_time:73828ms step_avg:61.47ms
step:1202/2330 train_time:73891ms step_avg:61.47ms
step:1203/2330 train_time:73952ms step_avg:61.47ms
step:1204/2330 train_time:74015ms step_avg:61.47ms
step:1205/2330 train_time:74076ms step_avg:61.47ms
step:1206/2330 train_time:74139ms step_avg:61.47ms
step:1207/2330 train_time:74200ms step_avg:61.47ms
step:1208/2330 train_time:74262ms step_avg:61.48ms
step:1209/2330 train_time:74323ms step_avg:61.47ms
step:1210/2330 train_time:74386ms step_avg:61.48ms
step:1211/2330 train_time:74448ms step_avg:61.48ms
step:1212/2330 train_time:74511ms step_avg:61.48ms
step:1213/2330 train_time:74572ms step_avg:61.48ms
step:1214/2330 train_time:74635ms step_avg:61.48ms
step:1215/2330 train_time:74696ms step_avg:61.48ms
step:1216/2330 train_time:74760ms step_avg:61.48ms
step:1217/2330 train_time:74820ms step_avg:61.48ms
step:1218/2330 train_time:74883ms step_avg:61.48ms
step:1219/2330 train_time:74943ms step_avg:61.48ms
step:1220/2330 train_time:75007ms step_avg:61.48ms
step:1221/2330 train_time:75067ms step_avg:61.48ms
step:1222/2330 train_time:75131ms step_avg:61.48ms
step:1223/2330 train_time:75191ms step_avg:61.48ms
step:1224/2330 train_time:75255ms step_avg:61.48ms
step:1225/2330 train_time:75316ms step_avg:61.48ms
step:1226/2330 train_time:75379ms step_avg:61.48ms
step:1227/2330 train_time:75440ms step_avg:61.48ms
step:1228/2330 train_time:75502ms step_avg:61.48ms
step:1229/2330 train_time:75563ms step_avg:61.48ms
step:1230/2330 train_time:75626ms step_avg:61.48ms
step:1231/2330 train_time:75687ms step_avg:61.48ms
step:1232/2330 train_time:75750ms step_avg:61.49ms
step:1233/2330 train_time:75811ms step_avg:61.48ms
step:1234/2330 train_time:75874ms step_avg:61.49ms
step:1235/2330 train_time:75935ms step_avg:61.49ms
step:1236/2330 train_time:75998ms step_avg:61.49ms
step:1237/2330 train_time:76059ms step_avg:61.49ms
step:1238/2330 train_time:76121ms step_avg:61.49ms
step:1239/2330 train_time:76181ms step_avg:61.49ms
step:1240/2330 train_time:76244ms step_avg:61.49ms
step:1241/2330 train_time:76306ms step_avg:61.49ms
step:1242/2330 train_time:76370ms step_avg:61.49ms
step:1243/2330 train_time:76430ms step_avg:61.49ms
step:1244/2330 train_time:76494ms step_avg:61.49ms
step:1245/2330 train_time:76555ms step_avg:61.49ms
step:1246/2330 train_time:76618ms step_avg:61.49ms
step:1247/2330 train_time:76678ms step_avg:61.49ms
step:1248/2330 train_time:76741ms step_avg:61.49ms
step:1249/2330 train_time:76802ms step_avg:61.49ms
step:1250/2330 train_time:76865ms step_avg:61.49ms
step:1250/2330 val_loss:4.1515 train_time:76930ms step_avg:61.54ms
step:1251/2330 train_time:76955ms step_avg:61.51ms
step:1252/2330 train_time:76995ms step_avg:61.50ms
step:1253/2330 train_time:77062ms step_avg:61.50ms
step:1254/2330 train_time:77127ms step_avg:61.50ms
step:1255/2330 train_time:77188ms step_avg:61.50ms
step:1256/2330 train_time:77251ms step_avg:61.51ms
step:1257/2330 train_time:77311ms step_avg:61.50ms
step:1258/2330 train_time:77373ms step_avg:61.51ms
step:1259/2330 train_time:77433ms step_avg:61.50ms
step:1260/2330 train_time:77496ms step_avg:61.50ms
step:1261/2330 train_time:77556ms step_avg:61.50ms
step:1262/2330 train_time:77618ms step_avg:61.50ms
step:1263/2330 train_time:77678ms step_avg:61.50ms
step:1264/2330 train_time:77740ms step_avg:61.50ms
step:1265/2330 train_time:77799ms step_avg:61.50ms
step:1266/2330 train_time:77861ms step_avg:61.50ms
step:1267/2330 train_time:77923ms step_avg:61.50ms
step:1268/2330 train_time:77988ms step_avg:61.50ms
step:1269/2330 train_time:78051ms step_avg:61.51ms
step:1270/2330 train_time:78115ms step_avg:61.51ms
step:1271/2330 train_time:78177ms step_avg:61.51ms
step:1272/2330 train_time:78240ms step_avg:61.51ms
step:1273/2330 train_time:78301ms step_avg:61.51ms
step:1274/2330 train_time:78363ms step_avg:61.51ms
step:1275/2330 train_time:78424ms step_avg:61.51ms
step:1276/2330 train_time:78488ms step_avg:61.51ms
step:1277/2330 train_time:78548ms step_avg:61.51ms
step:1278/2330 train_time:78611ms step_avg:61.51ms
step:1279/2330 train_time:78671ms step_avg:61.51ms
step:1280/2330 train_time:78734ms step_avg:61.51ms
step:1281/2330 train_time:78794ms step_avg:61.51ms
step:1282/2330 train_time:78857ms step_avg:61.51ms
step:1283/2330 train_time:78917ms step_avg:61.51ms
step:1284/2330 train_time:78981ms step_avg:61.51ms
step:1285/2330 train_time:79042ms step_avg:61.51ms
step:1286/2330 train_time:79106ms step_avg:61.51ms
step:1287/2330 train_time:79166ms step_avg:61.51ms
step:1288/2330 train_time:79230ms step_avg:61.51ms
step:1289/2330 train_time:79291ms step_avg:61.51ms
step:1290/2330 train_time:79354ms step_avg:61.51ms
step:1291/2330 train_time:79415ms step_avg:61.51ms
step:1292/2330 train_time:79478ms step_avg:61.52ms
step:1293/2330 train_time:79538ms step_avg:61.51ms
step:1294/2330 train_time:79600ms step_avg:61.51ms
step:1295/2330 train_time:79661ms step_avg:61.51ms
step:1296/2330 train_time:79725ms step_avg:61.52ms
step:1297/2330 train_time:79785ms step_avg:61.51ms
step:1298/2330 train_time:79848ms step_avg:61.52ms
step:1299/2330 train_time:79908ms step_avg:61.52ms
step:1300/2330 train_time:79971ms step_avg:61.52ms
step:1301/2330 train_time:80032ms step_avg:61.52ms
step:1302/2330 train_time:80095ms step_avg:61.52ms
step:1303/2330 train_time:80157ms step_avg:61.52ms
step:1304/2330 train_time:80220ms step_avg:61.52ms
step:1305/2330 train_time:80280ms step_avg:61.52ms
step:1306/2330 train_time:80342ms step_avg:61.52ms
step:1307/2330 train_time:80403ms step_avg:61.52ms
step:1308/2330 train_time:80466ms step_avg:61.52ms
step:1309/2330 train_time:80527ms step_avg:61.52ms
step:1310/2330 train_time:80589ms step_avg:61.52ms
step:1311/2330 train_time:80650ms step_avg:61.52ms
step:1312/2330 train_time:80713ms step_avg:61.52ms
step:1313/2330 train_time:80773ms step_avg:61.52ms
step:1314/2330 train_time:80836ms step_avg:61.52ms
step:1315/2330 train_time:80896ms step_avg:61.52ms
step:1316/2330 train_time:80959ms step_avg:61.52ms
step:1317/2330 train_time:81020ms step_avg:61.52ms
step:1318/2330 train_time:81082ms step_avg:61.52ms
step:1319/2330 train_time:81143ms step_avg:61.52ms
step:1320/2330 train_time:81206ms step_avg:61.52ms
step:1321/2330 train_time:81266ms step_avg:61.52ms
step:1322/2330 train_time:81329ms step_avg:61.52ms
step:1323/2330 train_time:81390ms step_avg:61.52ms
step:1324/2330 train_time:81453ms step_avg:61.52ms
step:1325/2330 train_time:81514ms step_avg:61.52ms
step:1326/2330 train_time:81577ms step_avg:61.52ms
step:1327/2330 train_time:81637ms step_avg:61.52ms
step:1328/2330 train_time:81699ms step_avg:61.52ms
step:1329/2330 train_time:81760ms step_avg:61.52ms
step:1330/2330 train_time:81823ms step_avg:61.52ms
step:1331/2330 train_time:81884ms step_avg:61.52ms
step:1332/2330 train_time:81946ms step_avg:61.52ms
step:1333/2330 train_time:82007ms step_avg:61.52ms
step:1334/2330 train_time:82071ms step_avg:61.52ms
step:1335/2330 train_time:82132ms step_avg:61.52ms
step:1336/2330 train_time:82195ms step_avg:61.52ms
step:1337/2330 train_time:82256ms step_avg:61.52ms
step:1338/2330 train_time:82319ms step_avg:61.52ms
step:1339/2330 train_time:82379ms step_avg:61.52ms
step:1340/2330 train_time:82442ms step_avg:61.52ms
step:1341/2330 train_time:82502ms step_avg:61.52ms
step:1342/2330 train_time:82565ms step_avg:61.52ms
step:1343/2330 train_time:82625ms step_avg:61.52ms
step:1344/2330 train_time:82688ms step_avg:61.52ms
step:1345/2330 train_time:82749ms step_avg:61.52ms
step:1346/2330 train_time:82812ms step_avg:61.52ms
step:1347/2330 train_time:82873ms step_avg:61.52ms
step:1348/2330 train_time:82937ms step_avg:61.53ms
step:1349/2330 train_time:82998ms step_avg:61.53ms
step:1350/2330 train_time:83061ms step_avg:61.53ms
step:1351/2330 train_time:83121ms step_avg:61.53ms
step:1352/2330 train_time:83185ms step_avg:61.53ms
step:1353/2330 train_time:83246ms step_avg:61.53ms
step:1354/2330 train_time:83308ms step_avg:61.53ms
step:1355/2330 train_time:83369ms step_avg:61.53ms
step:1356/2330 train_time:83432ms step_avg:61.53ms
step:1357/2330 train_time:83493ms step_avg:61.53ms
step:1358/2330 train_time:83556ms step_avg:61.53ms
step:1359/2330 train_time:83617ms step_avg:61.53ms
step:1360/2330 train_time:83679ms step_avg:61.53ms
step:1361/2330 train_time:83739ms step_avg:61.53ms
step:1362/2330 train_time:83802ms step_avg:61.53ms
step:1363/2330 train_time:83864ms step_avg:61.53ms
step:1364/2330 train_time:83927ms step_avg:61.53ms
step:1365/2330 train_time:83988ms step_avg:61.53ms
step:1366/2330 train_time:84051ms step_avg:61.53ms
step:1367/2330 train_time:84112ms step_avg:61.53ms
step:1368/2330 train_time:84175ms step_avg:61.53ms
step:1369/2330 train_time:84235ms step_avg:61.53ms
step:1370/2330 train_time:84298ms step_avg:61.53ms
step:1371/2330 train_time:84358ms step_avg:61.53ms
step:1372/2330 train_time:84421ms step_avg:61.53ms
step:1373/2330 train_time:84481ms step_avg:61.53ms
step:1374/2330 train_time:84544ms step_avg:61.53ms
step:1375/2330 train_time:84604ms step_avg:61.53ms
step:1376/2330 train_time:84667ms step_avg:61.53ms
step:1377/2330 train_time:84728ms step_avg:61.53ms
step:1378/2330 train_time:84790ms step_avg:61.53ms
step:1379/2330 train_time:84852ms step_avg:61.53ms
step:1380/2330 train_time:84914ms step_avg:61.53ms
step:1381/2330 train_time:84975ms step_avg:61.53ms
step:1382/2330 train_time:85038ms step_avg:61.53ms
step:1383/2330 train_time:85099ms step_avg:61.53ms
step:1384/2330 train_time:85162ms step_avg:61.53ms
step:1385/2330 train_time:85223ms step_avg:61.53ms
step:1386/2330 train_time:85287ms step_avg:61.53ms
step:1387/2330 train_time:85348ms step_avg:61.53ms
step:1388/2330 train_time:85410ms step_avg:61.53ms
step:1389/2330 train_time:85471ms step_avg:61.53ms
step:1390/2330 train_time:85535ms step_avg:61.54ms
step:1391/2330 train_time:85596ms step_avg:61.54ms
step:1392/2330 train_time:85658ms step_avg:61.54ms
step:1393/2330 train_time:85719ms step_avg:61.54ms
step:1394/2330 train_time:85781ms step_avg:61.54ms
step:1395/2330 train_time:85842ms step_avg:61.54ms
step:1396/2330 train_time:85905ms step_avg:61.54ms
step:1397/2330 train_time:85965ms step_avg:61.54ms
step:1398/2330 train_time:86029ms step_avg:61.54ms
step:1399/2330 train_time:86090ms step_avg:61.54ms
step:1400/2330 train_time:86153ms step_avg:61.54ms
step:1401/2330 train_time:86214ms step_avg:61.54ms
step:1402/2330 train_time:86277ms step_avg:61.54ms
step:1403/2330 train_time:86337ms step_avg:61.54ms
step:1404/2330 train_time:86400ms step_avg:61.54ms
step:1405/2330 train_time:86461ms step_avg:61.54ms
step:1406/2330 train_time:86524ms step_avg:61.54ms
step:1407/2330 train_time:86586ms step_avg:61.54ms
step:1408/2330 train_time:86649ms step_avg:61.54ms
step:1409/2330 train_time:86709ms step_avg:61.54ms
step:1410/2330 train_time:86773ms step_avg:61.54ms
step:1411/2330 train_time:86834ms step_avg:61.54ms
step:1412/2330 train_time:86896ms step_avg:61.54ms
step:1413/2330 train_time:86957ms step_avg:61.54ms
step:1414/2330 train_time:87020ms step_avg:61.54ms
step:1415/2330 train_time:87082ms step_avg:61.54ms
step:1416/2330 train_time:87145ms step_avg:61.54ms
step:1417/2330 train_time:87207ms step_avg:61.54ms
step:1418/2330 train_time:87270ms step_avg:61.54ms
step:1419/2330 train_time:87331ms step_avg:61.54ms
step:1420/2330 train_time:87394ms step_avg:61.54ms
step:1421/2330 train_time:87455ms step_avg:61.54ms
step:1422/2330 train_time:87519ms step_avg:61.55ms
step:1423/2330 train_time:87579ms step_avg:61.55ms
step:1424/2330 train_time:87641ms step_avg:61.55ms
step:1425/2330 train_time:87702ms step_avg:61.54ms
step:1426/2330 train_time:87765ms step_avg:61.55ms
step:1427/2330 train_time:87827ms step_avg:61.55ms
step:1428/2330 train_time:87890ms step_avg:61.55ms
step:1429/2330 train_time:87950ms step_avg:61.55ms
step:1430/2330 train_time:88014ms step_avg:61.55ms
step:1431/2330 train_time:88075ms step_avg:61.55ms
step:1432/2330 train_time:88138ms step_avg:61.55ms
step:1433/2330 train_time:88198ms step_avg:61.55ms
step:1434/2330 train_time:88261ms step_avg:61.55ms
step:1435/2330 train_time:88321ms step_avg:61.55ms
step:1436/2330 train_time:88385ms step_avg:61.55ms
step:1437/2330 train_time:88447ms step_avg:61.55ms
step:1438/2330 train_time:88509ms step_avg:61.55ms
step:1439/2330 train_time:88571ms step_avg:61.55ms
step:1440/2330 train_time:88633ms step_avg:61.55ms
step:1441/2330 train_time:88693ms step_avg:61.55ms
step:1442/2330 train_time:88757ms step_avg:61.55ms
step:1443/2330 train_time:88818ms step_avg:61.55ms
step:1444/2330 train_time:88880ms step_avg:61.55ms
step:1445/2330 train_time:88940ms step_avg:61.55ms
step:1446/2330 train_time:89003ms step_avg:61.55ms
step:1447/2330 train_time:89064ms step_avg:61.55ms
step:1448/2330 train_time:89127ms step_avg:61.55ms
step:1449/2330 train_time:89189ms step_avg:61.55ms
step:1450/2330 train_time:89251ms step_avg:61.55ms
step:1451/2330 train_time:89313ms step_avg:61.55ms
step:1452/2330 train_time:89376ms step_avg:61.55ms
step:1453/2330 train_time:89437ms step_avg:61.55ms
step:1454/2330 train_time:89499ms step_avg:61.55ms
step:1455/2330 train_time:89560ms step_avg:61.55ms
step:1456/2330 train_time:89622ms step_avg:61.55ms
step:1457/2330 train_time:89683ms step_avg:61.55ms
step:1458/2330 train_time:89746ms step_avg:61.55ms
step:1459/2330 train_time:89806ms step_avg:61.55ms
step:1460/2330 train_time:89869ms step_avg:61.55ms
step:1461/2330 train_time:89930ms step_avg:61.55ms
step:1462/2330 train_time:89993ms step_avg:61.55ms
step:1463/2330 train_time:90054ms step_avg:61.55ms
step:1464/2330 train_time:90117ms step_avg:61.56ms
step:1465/2330 train_time:90178ms step_avg:61.56ms
step:1466/2330 train_time:90240ms step_avg:61.56ms
step:1467/2330 train_time:90301ms step_avg:61.55ms
step:1468/2330 train_time:90365ms step_avg:61.56ms
step:1469/2330 train_time:90425ms step_avg:61.56ms
step:1470/2330 train_time:90488ms step_avg:61.56ms
step:1471/2330 train_time:90549ms step_avg:61.56ms
step:1472/2330 train_time:90612ms step_avg:61.56ms
step:1473/2330 train_time:90673ms step_avg:61.56ms
step:1474/2330 train_time:90736ms step_avg:61.56ms
step:1475/2330 train_time:90796ms step_avg:61.56ms
step:1476/2330 train_time:90859ms step_avg:61.56ms
step:1477/2330 train_time:90919ms step_avg:61.56ms
step:1478/2330 train_time:90982ms step_avg:61.56ms
step:1479/2330 train_time:91042ms step_avg:61.56ms
step:1480/2330 train_time:91105ms step_avg:61.56ms
step:1481/2330 train_time:91166ms step_avg:61.56ms
step:1482/2330 train_time:91229ms step_avg:61.56ms
step:1483/2330 train_time:91290ms step_avg:61.56ms
step:1484/2330 train_time:91353ms step_avg:61.56ms
step:1485/2330 train_time:91414ms step_avg:61.56ms
step:1486/2330 train_time:91477ms step_avg:61.56ms
step:1487/2330 train_time:91537ms step_avg:61.56ms
step:1488/2330 train_time:91600ms step_avg:61.56ms
step:1489/2330 train_time:91661ms step_avg:61.56ms
step:1490/2330 train_time:91725ms step_avg:61.56ms
step:1491/2330 train_time:91786ms step_avg:61.56ms
step:1492/2330 train_time:91849ms step_avg:61.56ms
step:1493/2330 train_time:91909ms step_avg:61.56ms
step:1494/2330 train_time:91973ms step_avg:61.56ms
step:1495/2330 train_time:92035ms step_avg:61.56ms
step:1496/2330 train_time:92098ms step_avg:61.56ms
step:1497/2330 train_time:92158ms step_avg:61.56ms
step:1498/2330 train_time:92221ms step_avg:61.56ms
step:1499/2330 train_time:92282ms step_avg:61.56ms
step:1500/2330 train_time:92345ms step_avg:61.56ms
step:1500/2330 val_loss:4.0578 train_time:92409ms step_avg:61.61ms
step:1501/2330 train_time:92433ms step_avg:61.58ms
step:1502/2330 train_time:92471ms step_avg:61.57ms
step:1503/2330 train_time:92535ms step_avg:61.57ms
step:1504/2330 train_time:92601ms step_avg:61.57ms
step:1505/2330 train_time:92662ms step_avg:61.57ms
step:1506/2330 train_time:92726ms step_avg:61.57ms
step:1507/2330 train_time:92786ms step_avg:61.57ms
step:1508/2330 train_time:92847ms step_avg:61.57ms
step:1509/2330 train_time:92907ms step_avg:61.57ms
step:1510/2330 train_time:92969ms step_avg:61.57ms
step:1511/2330 train_time:93030ms step_avg:61.57ms
step:1512/2330 train_time:93092ms step_avg:61.57ms
step:1513/2330 train_time:93153ms step_avg:61.57ms
step:1514/2330 train_time:93216ms step_avg:61.57ms
step:1515/2330 train_time:93276ms step_avg:61.57ms
step:1516/2330 train_time:93339ms step_avg:61.57ms
step:1517/2330 train_time:93400ms step_avg:61.57ms
step:1518/2330 train_time:93463ms step_avg:61.57ms
step:1519/2330 train_time:93525ms step_avg:61.57ms
step:1520/2330 train_time:93589ms step_avg:61.57ms
step:1521/2330 train_time:93652ms step_avg:61.57ms
step:1522/2330 train_time:93715ms step_avg:61.57ms
step:1523/2330 train_time:93777ms step_avg:61.57ms
step:1524/2330 train_time:93840ms step_avg:61.57ms
step:1525/2330 train_time:93901ms step_avg:61.57ms
step:1526/2330 train_time:93964ms step_avg:61.58ms
step:1527/2330 train_time:94024ms step_avg:61.57ms
step:1528/2330 train_time:94087ms step_avg:61.57ms
step:1529/2330 train_time:94147ms step_avg:61.57ms
step:1530/2330 train_time:94210ms step_avg:61.58ms
step:1531/2330 train_time:94272ms step_avg:61.58ms
step:1532/2330 train_time:94336ms step_avg:61.58ms
step:1533/2330 train_time:94398ms step_avg:61.58ms
step:1534/2330 train_time:94462ms step_avg:61.58ms
step:1535/2330 train_time:94524ms step_avg:61.58ms
step:1536/2330 train_time:94588ms step_avg:61.58ms
step:1537/2330 train_time:94650ms step_avg:61.58ms
step:1538/2330 train_time:94713ms step_avg:61.58ms
step:1539/2330 train_time:94775ms step_avg:61.58ms
step:1540/2330 train_time:94839ms step_avg:61.58ms
step:1541/2330 train_time:94901ms step_avg:61.58ms
step:1542/2330 train_time:94964ms step_avg:61.58ms
step:1543/2330 train_time:95024ms step_avg:61.58ms
step:1544/2330 train_time:95087ms step_avg:61.58ms
step:1545/2330 train_time:95148ms step_avg:61.58ms
step:1546/2330 train_time:95212ms step_avg:61.59ms
step:1547/2330 train_time:95274ms step_avg:61.59ms
step:1548/2330 train_time:95337ms step_avg:61.59ms
step:1549/2330 train_time:95399ms step_avg:61.59ms
step:1550/2330 train_time:95463ms step_avg:61.59ms
step:1551/2330 train_time:95524ms step_avg:61.59ms
step:1552/2330 train_time:95587ms step_avg:61.59ms
step:1553/2330 train_time:95650ms step_avg:61.59ms
step:1554/2330 train_time:95714ms step_avg:61.59ms
step:1555/2330 train_time:95775ms step_avg:61.59ms
step:1556/2330 train_time:95839ms step_avg:61.59ms
step:1557/2330 train_time:95900ms step_avg:61.59ms
step:1558/2330 train_time:95964ms step_avg:61.59ms
step:1559/2330 train_time:96025ms step_avg:61.59ms
step:1560/2330 train_time:96089ms step_avg:61.60ms
step:1561/2330 train_time:96150ms step_avg:61.59ms
step:1562/2330 train_time:96214ms step_avg:61.60ms
step:1563/2330 train_time:96275ms step_avg:61.60ms
step:1564/2330 train_time:96338ms step_avg:61.60ms
step:1565/2330 train_time:96400ms step_avg:61.60ms
step:1566/2330 train_time:96463ms step_avg:61.60ms
step:1567/2330 train_time:96525ms step_avg:61.60ms
step:1568/2330 train_time:96589ms step_avg:61.60ms
step:1569/2330 train_time:96649ms step_avg:61.60ms
step:1570/2330 train_time:96714ms step_avg:61.60ms
step:1571/2330 train_time:96776ms step_avg:61.60ms
step:1572/2330 train_time:96840ms step_avg:61.60ms
step:1573/2330 train_time:96903ms step_avg:61.60ms
step:1574/2330 train_time:96965ms step_avg:61.60ms
step:1575/2330 train_time:97027ms step_avg:61.60ms
step:1576/2330 train_time:97089ms step_avg:61.60ms
step:1577/2330 train_time:97150ms step_avg:61.60ms
step:1578/2330 train_time:97215ms step_avg:61.61ms
step:1579/2330 train_time:97276ms step_avg:61.61ms
step:1580/2330 train_time:97339ms step_avg:61.61ms
step:1581/2330 train_time:97400ms step_avg:61.61ms
step:1582/2330 train_time:97464ms step_avg:61.61ms
step:1583/2330 train_time:97526ms step_avg:61.61ms
step:1584/2330 train_time:97589ms step_avg:61.61ms
step:1585/2330 train_time:97650ms step_avg:61.61ms
step:1586/2330 train_time:97715ms step_avg:61.61ms
step:1587/2330 train_time:97777ms step_avg:61.61ms
step:1588/2330 train_time:97840ms step_avg:61.61ms
step:1589/2330 train_time:97902ms step_avg:61.61ms
step:1590/2330 train_time:97965ms step_avg:61.61ms
step:1591/2330 train_time:98026ms step_avg:61.61ms
step:1592/2330 train_time:98089ms step_avg:61.61ms
step:1593/2330 train_time:98150ms step_avg:61.61ms
step:1594/2330 train_time:98214ms step_avg:61.61ms
step:1595/2330 train_time:98276ms step_avg:61.61ms
step:1596/2330 train_time:98339ms step_avg:61.62ms
step:1597/2330 train_time:98400ms step_avg:61.62ms
step:1598/2330 train_time:98465ms step_avg:61.62ms
step:1599/2330 train_time:98525ms step_avg:61.62ms
step:1600/2330 train_time:98588ms step_avg:61.62ms
step:1601/2330 train_time:98649ms step_avg:61.62ms
step:1602/2330 train_time:98714ms step_avg:61.62ms
step:1603/2330 train_time:98775ms step_avg:61.62ms
step:1604/2330 train_time:98839ms step_avg:61.62ms
step:1605/2330 train_time:98901ms step_avg:61.62ms
step:1606/2330 train_time:98964ms step_avg:61.62ms
step:1607/2330 train_time:99026ms step_avg:61.62ms
step:1608/2330 train_time:99089ms step_avg:61.62ms
step:1609/2330 train_time:99151ms step_avg:61.62ms
step:1610/2330 train_time:99214ms step_avg:61.62ms
step:1611/2330 train_time:99276ms step_avg:61.62ms
step:1612/2330 train_time:99339ms step_avg:61.62ms
step:1613/2330 train_time:99401ms step_avg:61.62ms
step:1614/2330 train_time:99464ms step_avg:61.63ms
step:1615/2330 train_time:99526ms step_avg:61.63ms
step:1616/2330 train_time:99589ms step_avg:61.63ms
step:1617/2330 train_time:99650ms step_avg:61.63ms
step:1618/2330 train_time:99715ms step_avg:61.63ms
step:1619/2330 train_time:99775ms step_avg:61.63ms
step:1620/2330 train_time:99840ms step_avg:61.63ms
step:1621/2330 train_time:99902ms step_avg:61.63ms
step:1622/2330 train_time:99965ms step_avg:61.63ms
step:1623/2330 train_time:100025ms step_avg:61.63ms
step:1624/2330 train_time:100089ms step_avg:61.63ms
step:1625/2330 train_time:100150ms step_avg:61.63ms
step:1626/2330 train_time:100215ms step_avg:61.63ms
step:1627/2330 train_time:100276ms step_avg:61.63ms
step:1628/2330 train_time:100340ms step_avg:61.63ms
step:1629/2330 train_time:100401ms step_avg:61.63ms
step:1630/2330 train_time:100464ms step_avg:61.63ms
step:1631/2330 train_time:100526ms step_avg:61.63ms
step:1632/2330 train_time:100589ms step_avg:61.64ms
step:1633/2330 train_time:100650ms step_avg:61.64ms
step:1634/2330 train_time:100714ms step_avg:61.64ms
step:1635/2330 train_time:100775ms step_avg:61.64ms
step:1636/2330 train_time:100838ms step_avg:61.64ms
step:1637/2330 train_time:100900ms step_avg:61.64ms
step:1638/2330 train_time:100964ms step_avg:61.64ms
step:1639/2330 train_time:101025ms step_avg:61.64ms
step:1640/2330 train_time:101088ms step_avg:61.64ms
step:1641/2330 train_time:101150ms step_avg:61.64ms
step:1642/2330 train_time:101214ms step_avg:61.64ms
step:1643/2330 train_time:101276ms step_avg:61.64ms
step:1644/2330 train_time:101339ms step_avg:61.64ms
step:1645/2330 train_time:101401ms step_avg:61.64ms
step:1646/2330 train_time:101465ms step_avg:61.64ms
step:1647/2330 train_time:101526ms step_avg:61.64ms
step:1648/2330 train_time:101589ms step_avg:61.64ms
step:1649/2330 train_time:101651ms step_avg:61.64ms
step:1650/2330 train_time:101714ms step_avg:61.65ms
step:1651/2330 train_time:101776ms step_avg:61.64ms
step:1652/2330 train_time:101839ms step_avg:61.65ms
step:1653/2330 train_time:101901ms step_avg:61.65ms
step:1654/2330 train_time:101966ms step_avg:61.65ms
step:1655/2330 train_time:102028ms step_avg:61.65ms
step:1656/2330 train_time:102091ms step_avg:61.65ms
step:1657/2330 train_time:102152ms step_avg:61.65ms
step:1658/2330 train_time:102215ms step_avg:61.65ms
step:1659/2330 train_time:102276ms step_avg:61.65ms
step:1660/2330 train_time:102339ms step_avg:61.65ms
step:1661/2330 train_time:102401ms step_avg:61.65ms
step:1662/2330 train_time:102464ms step_avg:61.65ms
step:1663/2330 train_time:102525ms step_avg:61.65ms
step:1664/2330 train_time:102588ms step_avg:61.65ms
step:1665/2330 train_time:102650ms step_avg:61.65ms
step:1666/2330 train_time:102715ms step_avg:61.65ms
step:1667/2330 train_time:102776ms step_avg:61.65ms
step:1668/2330 train_time:102839ms step_avg:61.65ms
step:1669/2330 train_time:102902ms step_avg:61.65ms
step:1670/2330 train_time:102964ms step_avg:61.66ms
step:1671/2330 train_time:103025ms step_avg:61.65ms
step:1672/2330 train_time:103088ms step_avg:61.66ms
step:1673/2330 train_time:103150ms step_avg:61.66ms
step:1674/2330 train_time:103215ms step_avg:61.66ms
step:1675/2330 train_time:103276ms step_avg:61.66ms
step:1676/2330 train_time:103339ms step_avg:61.66ms
step:1677/2330 train_time:103400ms step_avg:61.66ms
step:1678/2330 train_time:103463ms step_avg:61.66ms
step:1679/2330 train_time:103525ms step_avg:61.66ms
step:1680/2330 train_time:103589ms step_avg:61.66ms
step:1681/2330 train_time:103650ms step_avg:61.66ms
step:1682/2330 train_time:103714ms step_avg:61.66ms
step:1683/2330 train_time:103775ms step_avg:61.66ms
step:1684/2330 train_time:103840ms step_avg:61.66ms
step:1685/2330 train_time:103901ms step_avg:61.66ms
step:1686/2330 train_time:103965ms step_avg:61.66ms
step:1687/2330 train_time:104027ms step_avg:61.66ms
step:1688/2330 train_time:104090ms step_avg:61.66ms
step:1689/2330 train_time:104152ms step_avg:61.67ms
step:1690/2330 train_time:104216ms step_avg:61.67ms
step:1691/2330 train_time:104278ms step_avg:61.67ms
step:1692/2330 train_time:104341ms step_avg:61.67ms
step:1693/2330 train_time:104402ms step_avg:61.67ms
step:1694/2330 train_time:104465ms step_avg:61.67ms
step:1695/2330 train_time:104526ms step_avg:61.67ms
step:1696/2330 train_time:104589ms step_avg:61.67ms
step:1697/2330 train_time:104651ms step_avg:61.67ms
step:1698/2330 train_time:104715ms step_avg:61.67ms
step:1699/2330 train_time:104776ms step_avg:61.67ms
step:1700/2330 train_time:104840ms step_avg:61.67ms
step:1701/2330 train_time:104902ms step_avg:61.67ms
step:1702/2330 train_time:104965ms step_avg:61.67ms
step:1703/2330 train_time:105026ms step_avg:61.67ms
step:1704/2330 train_time:105089ms step_avg:61.67ms
step:1705/2330 train_time:105151ms step_avg:61.67ms
step:1706/2330 train_time:105216ms step_avg:61.67ms
step:1707/2330 train_time:105277ms step_avg:61.67ms
step:1708/2330 train_time:105340ms step_avg:61.67ms
step:1709/2330 train_time:105402ms step_avg:61.67ms
step:1710/2330 train_time:105465ms step_avg:61.68ms
step:1711/2330 train_time:105526ms step_avg:61.68ms
step:1712/2330 train_time:105589ms step_avg:61.68ms
step:1713/2330 train_time:105651ms step_avg:61.68ms
step:1714/2330 train_time:105715ms step_avg:61.68ms
step:1715/2330 train_time:105776ms step_avg:61.68ms
step:1716/2330 train_time:105839ms step_avg:61.68ms
step:1717/2330 train_time:105901ms step_avg:61.68ms
step:1718/2330 train_time:105964ms step_avg:61.68ms
step:1719/2330 train_time:106026ms step_avg:61.68ms
step:1720/2330 train_time:106089ms step_avg:61.68ms
step:1721/2330 train_time:106150ms step_avg:61.68ms
step:1722/2330 train_time:106214ms step_avg:61.68ms
step:1723/2330 train_time:106276ms step_avg:61.68ms
step:1724/2330 train_time:106339ms step_avg:61.68ms
step:1725/2330 train_time:106402ms step_avg:61.68ms
step:1726/2330 train_time:106465ms step_avg:61.68ms
step:1727/2330 train_time:106526ms step_avg:61.68ms
step:1728/2330 train_time:106589ms step_avg:61.68ms
step:1729/2330 train_time:106651ms step_avg:61.68ms
step:1730/2330 train_time:106715ms step_avg:61.69ms
step:1731/2330 train_time:106776ms step_avg:61.68ms
step:1732/2330 train_time:106840ms step_avg:61.69ms
step:1733/2330 train_time:106903ms step_avg:61.69ms
step:1734/2330 train_time:106965ms step_avg:61.69ms
step:1735/2330 train_time:107026ms step_avg:61.69ms
step:1736/2330 train_time:107090ms step_avg:61.69ms
step:1737/2330 train_time:107151ms step_avg:61.69ms
step:1738/2330 train_time:107215ms step_avg:61.69ms
step:1739/2330 train_time:107276ms step_avg:61.69ms
step:1740/2330 train_time:107339ms step_avg:61.69ms
step:1741/2330 train_time:107402ms step_avg:61.69ms
step:1742/2330 train_time:107465ms step_avg:61.69ms
step:1743/2330 train_time:107526ms step_avg:61.69ms
step:1744/2330 train_time:107589ms step_avg:61.69ms
step:1745/2330 train_time:107651ms step_avg:61.69ms
step:1746/2330 train_time:107715ms step_avg:61.69ms
step:1747/2330 train_time:107776ms step_avg:61.69ms
step:1748/2330 train_time:107839ms step_avg:61.69ms
step:1749/2330 train_time:107901ms step_avg:61.69ms
step:1750/2330 train_time:107964ms step_avg:61.69ms
step:1750/2330 val_loss:3.9732 train_time:108029ms step_avg:61.73ms
step:1751/2330 train_time:108055ms step_avg:61.71ms
step:1752/2330 train_time:108092ms step_avg:61.70ms
step:1753/2330 train_time:108160ms step_avg:61.70ms
step:1754/2330 train_time:108226ms step_avg:61.70ms
step:1755/2330 train_time:108288ms step_avg:61.70ms
step:1756/2330 train_time:108352ms step_avg:61.70ms
step:1757/2330 train_time:108413ms step_avg:61.70ms
step:1758/2330 train_time:108476ms step_avg:61.70ms
step:1759/2330 train_time:108536ms step_avg:61.70ms
step:1760/2330 train_time:108599ms step_avg:61.70ms
step:1761/2330 train_time:108658ms step_avg:61.70ms
step:1762/2330 train_time:108721ms step_avg:61.70ms
step:1763/2330 train_time:108782ms step_avg:61.70ms
step:1764/2330 train_time:108844ms step_avg:61.70ms
step:1765/2330 train_time:108904ms step_avg:61.70ms
step:1766/2330 train_time:108970ms step_avg:61.70ms
step:1767/2330 train_time:109033ms step_avg:61.71ms
step:1768/2330 train_time:109097ms step_avg:61.71ms
step:1769/2330 train_time:109160ms step_avg:61.71ms
step:1770/2330 train_time:109224ms step_avg:61.71ms
step:1771/2330 train_time:109287ms step_avg:61.71ms
step:1772/2330 train_time:109351ms step_avg:61.71ms
step:1773/2330 train_time:109412ms step_avg:61.71ms
step:1774/2330 train_time:109474ms step_avg:61.71ms
step:1775/2330 train_time:109535ms step_avg:61.71ms
step:1776/2330 train_time:109597ms step_avg:61.71ms
step:1777/2330 train_time:109658ms step_avg:61.71ms
step:1778/2330 train_time:109721ms step_avg:61.71ms
step:1779/2330 train_time:109781ms step_avg:61.71ms
step:1780/2330 train_time:109844ms step_avg:61.71ms
step:1781/2330 train_time:109905ms step_avg:61.71ms
step:1782/2330 train_time:109968ms step_avg:61.71ms
step:1783/2330 train_time:110030ms step_avg:61.71ms
step:1784/2330 train_time:110094ms step_avg:61.71ms
step:1785/2330 train_time:110157ms step_avg:61.71ms
step:1786/2330 train_time:110223ms step_avg:61.71ms
step:1787/2330 train_time:110285ms step_avg:61.72ms
step:1788/2330 train_time:110349ms step_avg:61.72ms
step:1789/2330 train_time:110409ms step_avg:61.72ms
step:1790/2330 train_time:110472ms step_avg:61.72ms
step:1791/2330 train_time:110533ms step_avg:61.72ms
step:1792/2330 train_time:110596ms step_avg:61.72ms
step:1793/2330 train_time:110657ms step_avg:61.72ms
step:1794/2330 train_time:110720ms step_avg:61.72ms
step:1795/2330 train_time:110780ms step_avg:61.72ms
step:1796/2330 train_time:110843ms step_avg:61.72ms
step:1797/2330 train_time:110904ms step_avg:61.72ms
step:1798/2330 train_time:110967ms step_avg:61.72ms
step:1799/2330 train_time:111029ms step_avg:61.72ms
step:1800/2330 train_time:111092ms step_avg:61.72ms
step:1801/2330 train_time:111155ms step_avg:61.72ms
step:1802/2330 train_time:111220ms step_avg:61.72ms
step:1803/2330 train_time:111281ms step_avg:61.72ms
step:1804/2330 train_time:111345ms step_avg:61.72ms
step:1805/2330 train_time:111407ms step_avg:61.72ms
step:1806/2330 train_time:111470ms step_avg:61.72ms
step:1807/2330 train_time:111531ms step_avg:61.72ms
step:1808/2330 train_time:111594ms step_avg:61.72ms
step:1809/2330 train_time:111656ms step_avg:61.72ms
step:1810/2330 train_time:111719ms step_avg:61.72ms
step:1811/2330 train_time:111780ms step_avg:61.72ms
step:1812/2330 train_time:111843ms step_avg:61.72ms
step:1813/2330 train_time:111904ms step_avg:61.72ms
step:1814/2330 train_time:111967ms step_avg:61.72ms
step:1815/2330 train_time:112028ms step_avg:61.72ms
step:1816/2330 train_time:112091ms step_avg:61.72ms
step:1817/2330 train_time:112153ms step_avg:61.72ms
step:1818/2330 train_time:112217ms step_avg:61.73ms
step:1819/2330 train_time:112279ms step_avg:61.73ms
step:1820/2330 train_time:112343ms step_avg:61.73ms
step:1821/2330 train_time:112406ms step_avg:61.73ms
step:1822/2330 train_time:112469ms step_avg:61.73ms
step:1823/2330 train_time:112530ms step_avg:61.73ms
step:1824/2330 train_time:112593ms step_avg:61.73ms
step:1825/2330 train_time:112654ms step_avg:61.73ms
step:1826/2330 train_time:112718ms step_avg:61.73ms
step:1827/2330 train_time:112779ms step_avg:61.73ms
step:1828/2330 train_time:112842ms step_avg:61.73ms
step:1829/2330 train_time:112903ms step_avg:61.73ms
step:1830/2330 train_time:112966ms step_avg:61.73ms
step:1831/2330 train_time:113027ms step_avg:61.73ms
step:1832/2330 train_time:113090ms step_avg:61.73ms
step:1833/2330 train_time:113151ms step_avg:61.73ms
step:1834/2330 train_time:113214ms step_avg:61.73ms
step:1835/2330 train_time:113275ms step_avg:61.73ms
step:1836/2330 train_time:113339ms step_avg:61.73ms
step:1837/2330 train_time:113400ms step_avg:61.73ms
step:1838/2330 train_time:113464ms step_avg:61.73ms
step:1839/2330 train_time:113525ms step_avg:61.73ms
step:1840/2330 train_time:113589ms step_avg:61.73ms
step:1841/2330 train_time:113650ms step_avg:61.73ms
step:1842/2330 train_time:113713ms step_avg:61.73ms
step:1843/2330 train_time:113774ms step_avg:61.73ms
step:1844/2330 train_time:113838ms step_avg:61.73ms
step:1845/2330 train_time:113899ms step_avg:61.73ms
step:1846/2330 train_time:113962ms step_avg:61.73ms
step:1847/2330 train_time:114024ms step_avg:61.73ms
step:1848/2330 train_time:114087ms step_avg:61.74ms
step:1849/2330 train_time:114148ms step_avg:61.74ms
step:1850/2330 train_time:114211ms step_avg:61.74ms
step:1851/2330 train_time:114273ms step_avg:61.74ms
step:1852/2330 train_time:114336ms step_avg:61.74ms
step:1853/2330 train_time:114398ms step_avg:61.74ms
step:1854/2330 train_time:114462ms step_avg:61.74ms
step:1855/2330 train_time:114523ms step_avg:61.74ms
step:1856/2330 train_time:114587ms step_avg:61.74ms
step:1857/2330 train_time:114648ms step_avg:61.74ms
step:1858/2330 train_time:114712ms step_avg:61.74ms
step:1859/2330 train_time:114772ms step_avg:61.74ms
step:1860/2330 train_time:114836ms step_avg:61.74ms
step:1861/2330 train_time:114897ms step_avg:61.74ms
step:1862/2330 train_time:114961ms step_avg:61.74ms
step:1863/2330 train_time:115022ms step_avg:61.74ms
step:1864/2330 train_time:115085ms step_avg:61.74ms
step:1865/2330 train_time:115146ms step_avg:61.74ms
step:1866/2330 train_time:115210ms step_avg:61.74ms
step:1867/2330 train_time:115271ms step_avg:61.74ms
step:1868/2330 train_time:115335ms step_avg:61.74ms
step:1869/2330 train_time:115397ms step_avg:61.74ms
step:1870/2330 train_time:115461ms step_avg:61.74ms
step:1871/2330 train_time:115522ms step_avg:61.74ms
step:1872/2330 train_time:115586ms step_avg:61.74ms
step:1873/2330 train_time:115648ms step_avg:61.74ms
step:1874/2330 train_time:115711ms step_avg:61.75ms
step:1875/2330 train_time:115772ms step_avg:61.74ms
step:1876/2330 train_time:115835ms step_avg:61.75ms
step:1877/2330 train_time:115896ms step_avg:61.75ms
step:1878/2330 train_time:115961ms step_avg:61.75ms
step:1879/2330 train_time:116021ms step_avg:61.75ms
step:1880/2330 train_time:116085ms step_avg:61.75ms
step:1881/2330 train_time:116146ms step_avg:61.75ms
step:1882/2330 train_time:116210ms step_avg:61.75ms
step:1883/2330 train_time:116271ms step_avg:61.75ms
step:1884/2330 train_time:116335ms step_avg:61.75ms
step:1885/2330 train_time:116395ms step_avg:61.75ms
step:1886/2330 train_time:116459ms step_avg:61.75ms
step:1887/2330 train_time:116520ms step_avg:61.75ms
step:1888/2330 train_time:116584ms step_avg:61.75ms
step:1889/2330 train_time:116646ms step_avg:61.75ms
step:1890/2330 train_time:116710ms step_avg:61.75ms
step:1891/2330 train_time:116771ms step_avg:61.75ms
step:1892/2330 train_time:116834ms step_avg:61.75ms
step:1893/2330 train_time:116897ms step_avg:61.75ms
step:1894/2330 train_time:116960ms step_avg:61.75ms
step:1895/2330 train_time:117022ms step_avg:61.75ms
step:1896/2330 train_time:117084ms step_avg:61.75ms
step:1897/2330 train_time:117146ms step_avg:61.75ms
step:1898/2330 train_time:117209ms step_avg:61.75ms
step:1899/2330 train_time:117270ms step_avg:61.75ms
step:1900/2330 train_time:117333ms step_avg:61.75ms
step:1901/2330 train_time:117396ms step_avg:61.75ms
step:1902/2330 train_time:117460ms step_avg:61.76ms
step:1903/2330 train_time:117521ms step_avg:61.76ms
step:1904/2330 train_time:117585ms step_avg:61.76ms
step:1905/2330 train_time:117647ms step_avg:61.76ms
step:1906/2330 train_time:117711ms step_avg:61.76ms
step:1907/2330 train_time:117772ms step_avg:61.76ms
step:1908/2330 train_time:117835ms step_avg:61.76ms
step:1909/2330 train_time:117897ms step_avg:61.76ms
step:1910/2330 train_time:117960ms step_avg:61.76ms
step:1911/2330 train_time:118022ms step_avg:61.76ms
step:1912/2330 train_time:118085ms step_avg:61.76ms
step:1913/2330 train_time:118146ms step_avg:61.76ms
step:1914/2330 train_time:118209ms step_avg:61.76ms
step:1915/2330 train_time:118270ms step_avg:61.76ms
step:1916/2330 train_time:118333ms step_avg:61.76ms
step:1917/2330 train_time:118394ms step_avg:61.76ms
step:1918/2330 train_time:118458ms step_avg:61.76ms
step:1919/2330 train_time:118520ms step_avg:61.76ms
step:1920/2330 train_time:118584ms step_avg:61.76ms
step:1921/2330 train_time:118646ms step_avg:61.76ms
step:1922/2330 train_time:118710ms step_avg:61.76ms
step:1923/2330 train_time:118770ms step_avg:61.76ms
step:1924/2330 train_time:118834ms step_avg:61.76ms
step:1925/2330 train_time:118896ms step_avg:61.76ms
step:1926/2330 train_time:118959ms step_avg:61.76ms
step:1927/2330 train_time:119022ms step_avg:61.77ms
step:1928/2330 train_time:119085ms step_avg:61.77ms
step:1929/2330 train_time:119147ms step_avg:61.77ms
step:1930/2330 train_time:119209ms step_avg:61.77ms
step:1931/2330 train_time:119270ms step_avg:61.77ms
step:1932/2330 train_time:119334ms step_avg:61.77ms
step:1933/2330 train_time:119394ms step_avg:61.77ms
step:1934/2330 train_time:119458ms step_avg:61.77ms
step:1935/2330 train_time:119520ms step_avg:61.77ms
step:1936/2330 train_time:119583ms step_avg:61.77ms
step:1937/2330 train_time:119645ms step_avg:61.77ms
step:1938/2330 train_time:119709ms step_avg:61.77ms
step:1939/2330 train_time:119770ms step_avg:61.77ms
step:1940/2330 train_time:119834ms step_avg:61.77ms
step:1941/2330 train_time:119896ms step_avg:61.77ms
step:1942/2330 train_time:119959ms step_avg:61.77ms
step:1943/2330 train_time:120021ms step_avg:61.77ms
step:1944/2330 train_time:120084ms step_avg:61.77ms
step:1945/2330 train_time:120146ms step_avg:61.77ms
step:1946/2330 train_time:120209ms step_avg:61.77ms
step:1947/2330 train_time:120270ms step_avg:61.77ms
step:1948/2330 train_time:120333ms step_avg:61.77ms
step:1949/2330 train_time:120394ms step_avg:61.77ms
step:1950/2330 train_time:120459ms step_avg:61.77ms
step:1951/2330 train_time:120520ms step_avg:61.77ms
step:1952/2330 train_time:120583ms step_avg:61.77ms
step:1953/2330 train_time:120645ms step_avg:61.77ms
step:1954/2330 train_time:120709ms step_avg:61.78ms
step:1955/2330 train_time:120770ms step_avg:61.78ms
step:1956/2330 train_time:120833ms step_avg:61.78ms
step:1957/2330 train_time:120894ms step_avg:61.78ms
step:1958/2330 train_time:120959ms step_avg:61.78ms
step:1959/2330 train_time:121020ms step_avg:61.78ms
step:1960/2330 train_time:121083ms step_avg:61.78ms
step:1961/2330 train_time:121145ms step_avg:61.78ms
step:1962/2330 train_time:121209ms step_avg:61.78ms
step:1963/2330 train_time:121270ms step_avg:61.78ms
step:1964/2330 train_time:121333ms step_avg:61.78ms
step:1965/2330 train_time:121394ms step_avg:61.78ms
step:1966/2330 train_time:121457ms step_avg:61.78ms
step:1967/2330 train_time:121519ms step_avg:61.78ms
step:1968/2330 train_time:121582ms step_avg:61.78ms
step:1969/2330 train_time:121643ms step_avg:61.78ms
step:1970/2330 train_time:121707ms step_avg:61.78ms
step:1971/2330 train_time:121768ms step_avg:61.78ms
step:1972/2330 train_time:121831ms step_avg:61.78ms
step:1973/2330 train_time:121893ms step_avg:61.78ms
step:1974/2330 train_time:121956ms step_avg:61.78ms
step:1975/2330 train_time:122018ms step_avg:61.78ms
step:1976/2330 train_time:122082ms step_avg:61.78ms
step:1977/2330 train_time:122143ms step_avg:61.78ms
step:1978/2330 train_time:122207ms step_avg:61.78ms
step:1979/2330 train_time:122268ms step_avg:61.78ms
step:1980/2330 train_time:122331ms step_avg:61.78ms
step:1981/2330 train_time:122392ms step_avg:61.78ms
step:1982/2330 train_time:122456ms step_avg:61.78ms
step:1983/2330 train_time:122517ms step_avg:61.78ms
step:1984/2330 train_time:122581ms step_avg:61.78ms
step:1985/2330 train_time:122643ms step_avg:61.79ms
step:1986/2330 train_time:122706ms step_avg:61.79ms
step:1987/2330 train_time:122767ms step_avg:61.79ms
step:1988/2330 train_time:122831ms step_avg:61.79ms
step:1989/2330 train_time:122892ms step_avg:61.79ms
step:1990/2330 train_time:122956ms step_avg:61.79ms
step:1991/2330 train_time:123019ms step_avg:61.79ms
step:1992/2330 train_time:123082ms step_avg:61.79ms
step:1993/2330 train_time:123143ms step_avg:61.79ms
step:1994/2330 train_time:123206ms step_avg:61.79ms
step:1995/2330 train_time:123268ms step_avg:61.79ms
step:1996/2330 train_time:123331ms step_avg:61.79ms
step:1997/2330 train_time:123392ms step_avg:61.79ms
step:1998/2330 train_time:123455ms step_avg:61.79ms
step:1999/2330 train_time:123518ms step_avg:61.79ms
step:2000/2330 train_time:123581ms step_avg:61.79ms
step:2000/2330 val_loss:3.8950 train_time:123646ms step_avg:61.82ms
step:2001/2330 train_time:123669ms step_avg:61.80ms
step:2002/2330 train_time:123709ms step_avg:61.79ms
step:2003/2330 train_time:123774ms step_avg:61.79ms
step:2004/2330 train_time:123839ms step_avg:61.80ms
step:2005/2330 train_time:123900ms step_avg:61.80ms
step:2006/2330 train_time:123964ms step_avg:61.80ms
step:2007/2330 train_time:124024ms step_avg:61.80ms
step:2008/2330 train_time:124087ms step_avg:61.80ms
step:2009/2330 train_time:124148ms step_avg:61.80ms
step:2010/2330 train_time:124210ms step_avg:61.80ms
step:2011/2330 train_time:124270ms step_avg:61.80ms
step:2012/2330 train_time:124333ms step_avg:61.80ms
step:2013/2330 train_time:124394ms step_avg:61.80ms
step:2014/2330 train_time:124456ms step_avg:61.80ms
step:2015/2330 train_time:124517ms step_avg:61.79ms
step:2016/2330 train_time:124580ms step_avg:61.80ms
step:2017/2330 train_time:124643ms step_avg:61.80ms
step:2018/2330 train_time:124707ms step_avg:61.80ms
step:2019/2330 train_time:124771ms step_avg:61.80ms
step:2020/2330 train_time:124835ms step_avg:61.80ms
step:2021/2330 train_time:124898ms step_avg:61.80ms
step:2022/2330 train_time:124961ms step_avg:61.80ms
step:2023/2330 train_time:125022ms step_avg:61.80ms
step:2024/2330 train_time:125085ms step_avg:61.80ms
step:2025/2330 train_time:125146ms step_avg:61.80ms
step:2026/2330 train_time:125209ms step_avg:61.80ms
step:2027/2330 train_time:125269ms step_avg:61.80ms
step:2028/2330 train_time:125332ms step_avg:61.80ms
step:2029/2330 train_time:125392ms step_avg:61.80ms
step:2030/2330 train_time:125455ms step_avg:61.80ms
step:2031/2330 train_time:125517ms step_avg:61.80ms
step:2032/2330 train_time:125580ms step_avg:61.80ms
step:2033/2330 train_time:125643ms step_avg:61.80ms
step:2034/2330 train_time:125706ms step_avg:61.80ms
step:2035/2330 train_time:125768ms step_avg:61.80ms
step:2036/2330 train_time:125831ms step_avg:61.80ms
step:2037/2330 train_time:125894ms step_avg:61.80ms
step:2038/2330 train_time:125958ms step_avg:61.80ms
step:2039/2330 train_time:126019ms step_avg:61.80ms
step:2040/2330 train_time:126083ms step_avg:61.81ms
step:2041/2330 train_time:126144ms step_avg:61.81ms
step:2042/2330 train_time:126208ms step_avg:61.81ms
step:2043/2330 train_time:126269ms step_avg:61.81ms
step:2044/2330 train_time:126332ms step_avg:61.81ms
step:2045/2330 train_time:126392ms step_avg:61.81ms
step:2046/2330 train_time:126455ms step_avg:61.81ms
step:2047/2330 train_time:126517ms step_avg:61.81ms
step:2048/2330 train_time:126580ms step_avg:61.81ms
step:2049/2330 train_time:126642ms step_avg:61.81ms
step:2050/2330 train_time:126706ms step_avg:61.81ms
step:2051/2330 train_time:126767ms step_avg:61.81ms
step:2052/2330 train_time:126830ms step_avg:61.81ms
step:2053/2330 train_time:126893ms step_avg:61.81ms
step:2054/2330 train_time:126957ms step_avg:61.81ms
step:2055/2330 train_time:127020ms step_avg:61.81ms
step:2056/2330 train_time:127083ms step_avg:61.81ms
step:2057/2330 train_time:127144ms step_avg:61.81ms
step:2058/2330 train_time:127207ms step_avg:61.81ms
step:2059/2330 train_time:127269ms step_avg:61.81ms
step:2060/2330 train_time:127332ms step_avg:61.81ms
step:2061/2330 train_time:127392ms step_avg:61.81ms
step:2062/2330 train_time:127455ms step_avg:61.81ms
step:2063/2330 train_time:127516ms step_avg:61.81ms
step:2064/2330 train_time:127580ms step_avg:61.81ms
step:2065/2330 train_time:127642ms step_avg:61.81ms
step:2066/2330 train_time:127705ms step_avg:61.81ms
step:2067/2330 train_time:127767ms step_avg:61.81ms
step:2068/2330 train_time:127831ms step_avg:61.81ms
step:2069/2330 train_time:127892ms step_avg:61.81ms
step:2070/2330 train_time:127956ms step_avg:61.81ms
step:2071/2330 train_time:128016ms step_avg:61.81ms
step:2072/2330 train_time:128080ms step_avg:61.81ms
step:2073/2330 train_time:128143ms step_avg:61.82ms
step:2074/2330 train_time:128205ms step_avg:61.82ms
step:2075/2330 train_time:128267ms step_avg:61.82ms
step:2076/2330 train_time:128330ms step_avg:61.82ms
step:2077/2330 train_time:128391ms step_avg:61.82ms
step:2078/2330 train_time:128454ms step_avg:61.82ms
step:2079/2330 train_time:128515ms step_avg:61.82ms
step:2080/2330 train_time:128578ms step_avg:61.82ms
step:2081/2330 train_time:128641ms step_avg:61.82ms
step:2082/2330 train_time:128704ms step_avg:61.82ms
step:2083/2330 train_time:128765ms step_avg:61.82ms
step:2084/2330 train_time:128829ms step_avg:61.82ms
step:2085/2330 train_time:128890ms step_avg:61.82ms
step:2086/2330 train_time:128953ms step_avg:61.82ms
step:2087/2330 train_time:129015ms step_avg:61.82ms
step:2088/2330 train_time:129079ms step_avg:61.82ms
step:2089/2330 train_time:129142ms step_avg:61.82ms
step:2090/2330 train_time:129205ms step_avg:61.82ms
step:2091/2330 train_time:129266ms step_avg:61.82ms
step:2092/2330 train_time:129330ms step_avg:61.82ms
step:2093/2330 train_time:129390ms step_avg:61.82ms
step:2094/2330 train_time:129454ms step_avg:61.82ms
step:2095/2330 train_time:129515ms step_avg:61.82ms
step:2096/2330 train_time:129578ms step_avg:61.82ms
step:2097/2330 train_time:129641ms step_avg:61.82ms
step:2098/2330 train_time:129704ms step_avg:61.82ms
step:2099/2330 train_time:129767ms step_avg:61.82ms
step:2100/2330 train_time:129830ms step_avg:61.82ms
step:2101/2330 train_time:129891ms step_avg:61.82ms
step:2102/2330 train_time:129955ms step_avg:61.82ms
step:2103/2330 train_time:130017ms step_avg:61.82ms
step:2104/2330 train_time:130081ms step_avg:61.83ms
step:2105/2330 train_time:130142ms step_avg:61.83ms
step:2106/2330 train_time:130206ms step_avg:61.83ms
step:2107/2330 train_time:130267ms step_avg:61.83ms
step:2108/2330 train_time:130331ms step_avg:61.83ms
step:2109/2330 train_time:130391ms step_avg:61.83ms
step:2110/2330 train_time:130455ms step_avg:61.83ms
step:2111/2330 train_time:130516ms step_avg:61.83ms
step:2112/2330 train_time:130579ms step_avg:61.83ms
step:2113/2330 train_time:130641ms step_avg:61.83ms
step:2114/2330 train_time:130704ms step_avg:61.83ms
step:2115/2330 train_time:130766ms step_avg:61.83ms
step:2116/2330 train_time:130830ms step_avg:61.83ms
step:2117/2330 train_time:130890ms step_avg:61.83ms
step:2118/2330 train_time:130954ms step_avg:61.83ms
step:2119/2330 train_time:131015ms step_avg:61.83ms
step:2120/2330 train_time:131080ms step_avg:61.83ms
step:2121/2330 train_time:131142ms step_avg:61.83ms
step:2122/2330 train_time:131205ms step_avg:61.83ms
step:2123/2330 train_time:131266ms step_avg:61.83ms
step:2124/2330 train_time:131330ms step_avg:61.83ms
step:2125/2330 train_time:131391ms step_avg:61.83ms
step:2126/2330 train_time:131454ms step_avg:61.83ms
step:2127/2330 train_time:131515ms step_avg:61.83ms
step:2128/2330 train_time:131578ms step_avg:61.83ms
step:2129/2330 train_time:131640ms step_avg:61.83ms
step:2130/2330 train_time:131703ms step_avg:61.83ms
step:2131/2330 train_time:131765ms step_avg:61.83ms
step:2132/2330 train_time:131829ms step_avg:61.83ms
step:2133/2330 train_time:131890ms step_avg:61.83ms
step:2134/2330 train_time:131953ms step_avg:61.83ms
step:2135/2330 train_time:132014ms step_avg:61.83ms
step:2136/2330 train_time:132078ms step_avg:61.83ms
step:2137/2330 train_time:132140ms step_avg:61.83ms
step:2138/2330 train_time:132203ms step_avg:61.83ms
step:2139/2330 train_time:132264ms step_avg:61.83ms
step:2140/2330 train_time:132328ms step_avg:61.84ms
step:2141/2330 train_time:132388ms step_avg:61.83ms
step:2142/2330 train_time:132452ms step_avg:61.84ms
step:2143/2330 train_time:132512ms step_avg:61.83ms
step:2144/2330 train_time:132576ms step_avg:61.84ms
step:2145/2330 train_time:132637ms step_avg:61.84ms
step:2146/2330 train_time:132700ms step_avg:61.84ms
step:2147/2330 train_time:132762ms step_avg:61.84ms
step:2148/2330 train_time:132825ms step_avg:61.84ms
step:2149/2330 train_time:132887ms step_avg:61.84ms
step:2150/2330 train_time:132950ms step_avg:61.84ms
step:2151/2330 train_time:133011ms step_avg:61.84ms
step:2152/2330 train_time:133075ms step_avg:61.84ms
step:2153/2330 train_time:133137ms step_avg:61.84ms
step:2154/2330 train_time:133200ms step_avg:61.84ms
step:2155/2330 train_time:133261ms step_avg:61.84ms
step:2156/2330 train_time:133325ms step_avg:61.84ms
step:2157/2330 train_time:133386ms step_avg:61.84ms
step:2158/2330 train_time:133449ms step_avg:61.84ms
step:2159/2330 train_time:133510ms step_avg:61.84ms
step:2160/2330 train_time:133573ms step_avg:61.84ms
step:2161/2330 train_time:133634ms step_avg:61.84ms
step:2162/2330 train_time:133698ms step_avg:61.84ms
step:2163/2330 train_time:133759ms step_avg:61.84ms
step:2164/2330 train_time:133823ms step_avg:61.84ms
step:2165/2330 train_time:133885ms step_avg:61.84ms
step:2166/2330 train_time:133948ms step_avg:61.84ms
step:2167/2330 train_time:134009ms step_avg:61.84ms
step:2168/2330 train_time:134073ms step_avg:61.84ms
step:2169/2330 train_time:134135ms step_avg:61.84ms
step:2170/2330 train_time:134199ms step_avg:61.84ms
step:2171/2330 train_time:134261ms step_avg:61.84ms
step:2172/2330 train_time:134324ms step_avg:61.84ms
step:2173/2330 train_time:134386ms step_avg:61.84ms
step:2174/2330 train_time:134450ms step_avg:61.84ms
step:2175/2330 train_time:134511ms step_avg:61.84ms
step:2176/2330 train_time:134574ms step_avg:61.84ms
step:2177/2330 train_time:134635ms step_avg:61.84ms
step:2178/2330 train_time:134699ms step_avg:61.85ms
step:2179/2330 train_time:134760ms step_avg:61.84ms
step:2180/2330 train_time:134823ms step_avg:61.85ms
step:2181/2330 train_time:134884ms step_avg:61.85ms
step:2182/2330 train_time:134949ms step_avg:61.85ms
step:2183/2330 train_time:135009ms step_avg:61.85ms
step:2184/2330 train_time:135074ms step_avg:61.85ms
step:2185/2330 train_time:135135ms step_avg:61.85ms
step:2186/2330 train_time:135198ms step_avg:61.85ms
step:2187/2330 train_time:135260ms step_avg:61.85ms
step:2188/2330 train_time:135324ms step_avg:61.85ms
step:2189/2330 train_time:135385ms step_avg:61.85ms
step:2190/2330 train_time:135449ms step_avg:61.85ms
step:2191/2330 train_time:135510ms step_avg:61.85ms
step:2192/2330 train_time:135574ms step_avg:61.85ms
step:2193/2330 train_time:135635ms step_avg:61.85ms
step:2194/2330 train_time:135697ms step_avg:61.85ms
step:2195/2330 train_time:135759ms step_avg:61.85ms
step:2196/2330 train_time:135822ms step_avg:61.85ms
step:2197/2330 train_time:135884ms step_avg:61.85ms
step:2198/2330 train_time:135948ms step_avg:61.85ms
step:2199/2330 train_time:136011ms step_avg:61.85ms
step:2200/2330 train_time:136074ms step_avg:61.85ms
step:2201/2330 train_time:136136ms step_avg:61.85ms
step:2202/2330 train_time:136199ms step_avg:61.85ms
step:2203/2330 train_time:136260ms step_avg:61.85ms
step:2204/2330 train_time:136323ms step_avg:61.85ms
step:2205/2330 train_time:136385ms step_avg:61.85ms
step:2206/2330 train_time:136449ms step_avg:61.85ms
step:2207/2330 train_time:136510ms step_avg:61.85ms
step:2208/2330 train_time:136574ms step_avg:61.85ms
step:2209/2330 train_time:136635ms step_avg:61.85ms
step:2210/2330 train_time:136698ms step_avg:61.85ms
step:2211/2330 train_time:136759ms step_avg:61.85ms
step:2212/2330 train_time:136823ms step_avg:61.85ms
step:2213/2330 train_time:136884ms step_avg:61.85ms
step:2214/2330 train_time:136948ms step_avg:61.86ms
step:2215/2330 train_time:137009ms step_avg:61.86ms
step:2216/2330 train_time:137073ms step_avg:61.86ms
step:2217/2330 train_time:137134ms step_avg:61.86ms
step:2218/2330 train_time:137197ms step_avg:61.86ms
step:2219/2330 train_time:137259ms step_avg:61.86ms
step:2220/2330 train_time:137323ms step_avg:61.86ms
step:2221/2330 train_time:137384ms step_avg:61.86ms
step:2222/2330 train_time:137447ms step_avg:61.86ms
step:2223/2330 train_time:137508ms step_avg:61.86ms
step:2224/2330 train_time:137571ms step_avg:61.86ms
step:2225/2330 train_time:137632ms step_avg:61.86ms
step:2226/2330 train_time:137696ms step_avg:61.86ms
step:2227/2330 train_time:137758ms step_avg:61.86ms
step:2228/2330 train_time:137821ms step_avg:61.86ms
step:2229/2330 train_time:137884ms step_avg:61.86ms
step:2230/2330 train_time:137948ms step_avg:61.86ms
step:2231/2330 train_time:138009ms step_avg:61.86ms
step:2232/2330 train_time:138072ms step_avg:61.86ms
step:2233/2330 train_time:138132ms step_avg:61.86ms
step:2234/2330 train_time:138196ms step_avg:61.86ms
step:2235/2330 train_time:138258ms step_avg:61.86ms
step:2236/2330 train_time:138321ms step_avg:61.86ms
step:2237/2330 train_time:138383ms step_avg:61.86ms
step:2238/2330 train_time:138446ms step_avg:61.86ms
step:2239/2330 train_time:138507ms step_avg:61.86ms
step:2240/2330 train_time:138571ms step_avg:61.86ms
step:2241/2330 train_time:138632ms step_avg:61.86ms
step:2242/2330 train_time:138695ms step_avg:61.86ms
step:2243/2330 train_time:138757ms step_avg:61.86ms
step:2244/2330 train_time:138821ms step_avg:61.86ms
step:2245/2330 train_time:138883ms step_avg:61.86ms
step:2246/2330 train_time:138946ms step_avg:61.86ms
step:2247/2330 train_time:139009ms step_avg:61.86ms
step:2248/2330 train_time:139072ms step_avg:61.86ms
step:2249/2330 train_time:139133ms step_avg:61.86ms
step:2250/2330 train_time:139196ms step_avg:61.86ms
step:2250/2330 val_loss:3.8529 train_time:139262ms step_avg:61.89ms
step:2251/2330 train_time:139284ms step_avg:61.88ms
step:2252/2330 train_time:139324ms step_avg:61.87ms
step:2253/2330 train_time:139390ms step_avg:61.87ms
step:2254/2330 train_time:139454ms step_avg:61.87ms
step:2255/2330 train_time:139515ms step_avg:61.87ms
step:2256/2330 train_time:139578ms step_avg:61.87ms
step:2257/2330 train_time:139639ms step_avg:61.87ms
step:2258/2330 train_time:139702ms step_avg:61.87ms
step:2259/2330 train_time:139763ms step_avg:61.87ms
step:2260/2330 train_time:139825ms step_avg:61.87ms
step:2261/2330 train_time:139886ms step_avg:61.87ms
step:2262/2330 train_time:139949ms step_avg:61.87ms
step:2263/2330 train_time:140009ms step_avg:61.87ms
step:2264/2330 train_time:140073ms step_avg:61.87ms
step:2265/2330 train_time:140133ms step_avg:61.87ms
step:2266/2330 train_time:140197ms step_avg:61.87ms
step:2267/2330 train_time:140261ms step_avg:61.87ms
step:2268/2330 train_time:140325ms step_avg:61.87ms
step:2269/2330 train_time:140387ms step_avg:61.87ms
step:2270/2330 train_time:140451ms step_avg:61.87ms
step:2271/2330 train_time:140513ms step_avg:61.87ms
step:2272/2330 train_time:140577ms step_avg:61.87ms
step:2273/2330 train_time:140638ms step_avg:61.87ms
step:2274/2330 train_time:140701ms step_avg:61.87ms
step:2275/2330 train_time:140763ms step_avg:61.87ms
step:2276/2330 train_time:140825ms step_avg:61.87ms
step:2277/2330 train_time:140886ms step_avg:61.87ms
step:2278/2330 train_time:140949ms step_avg:61.87ms
step:2279/2330 train_time:141009ms step_avg:61.87ms
step:2280/2330 train_time:141072ms step_avg:61.87ms
step:2281/2330 train_time:141132ms step_avg:61.87ms
step:2282/2330 train_time:141196ms step_avg:61.87ms
step:2283/2330 train_time:141258ms step_avg:61.87ms
step:2284/2330 train_time:141322ms step_avg:61.87ms
step:2285/2330 train_time:141384ms step_avg:61.87ms
step:2286/2330 train_time:141448ms step_avg:61.88ms
step:2287/2330 train_time:141510ms step_avg:61.88ms
step:2288/2330 train_time:141573ms step_avg:61.88ms
step:2289/2330 train_time:141635ms step_avg:61.88ms
step:2290/2330 train_time:141698ms step_avg:61.88ms
step:2291/2330 train_time:141759ms step_avg:61.88ms
step:2292/2330 train_time:141822ms step_avg:61.88ms
step:2293/2330 train_time:141884ms step_avg:61.88ms
step:2294/2330 train_time:141947ms step_avg:61.88ms
step:2295/2330 train_time:142008ms step_avg:61.88ms
step:2296/2330 train_time:142071ms step_avg:61.88ms
step:2297/2330 train_time:142132ms step_avg:61.88ms
step:2298/2330 train_time:142195ms step_avg:61.88ms
step:2299/2330 train_time:142256ms step_avg:61.88ms
step:2300/2330 train_time:142321ms step_avg:61.88ms
step:2301/2330 train_time:142383ms step_avg:61.88ms
step:2302/2330 train_time:142447ms step_avg:61.88ms
step:2303/2330 train_time:142508ms step_avg:61.88ms
step:2304/2330 train_time:142571ms step_avg:61.88ms
step:2305/2330 train_time:142633ms step_avg:61.88ms
step:2306/2330 train_time:142696ms step_avg:61.88ms
step:2307/2330 train_time:142758ms step_avg:61.88ms
step:2308/2330 train_time:142821ms step_avg:61.88ms
step:2309/2330 train_time:142882ms step_avg:61.88ms
step:2310/2330 train_time:142946ms step_avg:61.88ms
step:2311/2330 train_time:143007ms step_avg:61.88ms
step:2312/2330 train_time:143071ms step_avg:61.88ms
step:2313/2330 train_time:143131ms step_avg:61.88ms
step:2314/2330 train_time:143194ms step_avg:61.88ms
step:2315/2330 train_time:143256ms step_avg:61.88ms
step:2316/2330 train_time:143320ms step_avg:61.88ms
step:2317/2330 train_time:143382ms step_avg:61.88ms
step:2318/2330 train_time:143446ms step_avg:61.88ms
step:2319/2330 train_time:143507ms step_avg:61.88ms
step:2320/2330 train_time:143571ms step_avg:61.88ms
step:2321/2330 train_time:143632ms step_avg:61.88ms
step:2322/2330 train_time:143696ms step_avg:61.88ms
step:2323/2330 train_time:143758ms step_avg:61.88ms
step:2324/2330 train_time:143821ms step_avg:61.89ms
step:2325/2330 train_time:143882ms step_avg:61.88ms
step:2326/2330 train_time:143946ms step_avg:61.89ms
step:2327/2330 train_time:144007ms step_avg:61.89ms
step:2328/2330 train_time:144070ms step_avg:61.89ms
step:2329/2330 train_time:144132ms step_avg:61.89ms
step:2330/2330 train_time:144195ms step_avg:61.89ms
step:2330/2330 val_loss:3.8424 train_time:144261ms step_avg:61.91ms
peak memory allocated: 29512 MiB reserved: 44076 MiB
