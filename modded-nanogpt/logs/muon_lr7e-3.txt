import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "muon_lr7e-3"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=7e-3, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Oct 28 23:14:01 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   31C    P0             111W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   29C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   28C    P0             108W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   31C    P0             115W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:96ms step_avg:96.27ms
step:2/2330 train_time:187ms step_avg:93.58ms
step:3/2330 train_time:211ms step_avg:70.48ms
step:4/2330 train_time:244ms step_avg:61.00ms
step:5/2330 train_time:302ms step_avg:60.30ms
step:6/2330 train_time:362ms step_avg:60.28ms
step:7/2330 train_time:420ms step_avg:60.02ms
step:8/2330 train_time:481ms step_avg:60.09ms
step:9/2330 train_time:539ms step_avg:59.86ms
step:10/2330 train_time:600ms step_avg:59.98ms
step:11/2330 train_time:659ms step_avg:59.88ms
step:12/2330 train_time:720ms step_avg:59.99ms
step:13/2330 train_time:778ms step_avg:59.86ms
step:14/2330 train_time:839ms step_avg:59.93ms
step:15/2330 train_time:898ms step_avg:59.86ms
step:16/2330 train_time:959ms step_avg:59.93ms
step:17/2330 train_time:1020ms step_avg:60.00ms
step:18/2330 train_time:1085ms step_avg:60.27ms
step:19/2330 train_time:1148ms step_avg:60.40ms
step:20/2330 train_time:1211ms step_avg:60.57ms
step:21/2330 train_time:1271ms step_avg:60.50ms
step:22/2330 train_time:1333ms step_avg:60.60ms
step:23/2330 train_time:1392ms step_avg:60.54ms
step:24/2330 train_time:1454ms step_avg:60.57ms
step:25/2330 train_time:1513ms step_avg:60.53ms
step:26/2330 train_time:1575ms step_avg:60.56ms
step:27/2330 train_time:1634ms step_avg:60.50ms
step:28/2330 train_time:1695ms step_avg:60.54ms
step:29/2330 train_time:1754ms step_avg:60.49ms
step:30/2330 train_time:1815ms step_avg:60.50ms
step:31/2330 train_time:1874ms step_avg:60.47ms
step:32/2330 train_time:1935ms step_avg:60.48ms
step:33/2330 train_time:1995ms step_avg:60.45ms
step:34/2330 train_time:2057ms step_avg:60.51ms
step:35/2330 train_time:2119ms step_avg:60.54ms
step:36/2330 train_time:2182ms step_avg:60.61ms
step:37/2330 train_time:2243ms step_avg:60.62ms
step:38/2330 train_time:2305ms step_avg:60.65ms
step:39/2330 train_time:2365ms step_avg:60.63ms
step:40/2330 train_time:2427ms step_avg:60.68ms
step:41/2330 train_time:2487ms step_avg:60.65ms
step:42/2330 train_time:2548ms step_avg:60.66ms
step:43/2330 train_time:2609ms step_avg:60.66ms
step:44/2330 train_time:2670ms step_avg:60.69ms
step:45/2330 train_time:2731ms step_avg:60.69ms
step:46/2330 train_time:2793ms step_avg:60.71ms
step:47/2330 train_time:2852ms step_avg:60.68ms
step:48/2330 train_time:2914ms step_avg:60.71ms
step:49/2330 train_time:2974ms step_avg:60.69ms
step:50/2330 train_time:3035ms step_avg:60.71ms
step:51/2330 train_time:3095ms step_avg:60.69ms
step:52/2330 train_time:3157ms step_avg:60.71ms
step:53/2330 train_time:3216ms step_avg:60.68ms
step:54/2330 train_time:3278ms step_avg:60.70ms
step:55/2330 train_time:3338ms step_avg:60.69ms
step:56/2330 train_time:3400ms step_avg:60.71ms
step:57/2330 train_time:3459ms step_avg:60.69ms
step:58/2330 train_time:3521ms step_avg:60.71ms
step:59/2330 train_time:3581ms step_avg:60.69ms
step:60/2330 train_time:3642ms step_avg:60.71ms
step:61/2330 train_time:3703ms step_avg:60.70ms
step:62/2330 train_time:3765ms step_avg:60.73ms
step:63/2330 train_time:3825ms step_avg:60.71ms
step:64/2330 train_time:3887ms step_avg:60.73ms
step:65/2330 train_time:3946ms step_avg:60.71ms
step:66/2330 train_time:4008ms step_avg:60.73ms
step:67/2330 train_time:4068ms step_avg:60.72ms
step:68/2330 train_time:4130ms step_avg:60.74ms
step:69/2330 train_time:4191ms step_avg:60.73ms
step:70/2330 train_time:4253ms step_avg:60.76ms
step:71/2330 train_time:4313ms step_avg:60.74ms
step:72/2330 train_time:4374ms step_avg:60.76ms
step:73/2330 train_time:4434ms step_avg:60.74ms
step:74/2330 train_time:4496ms step_avg:60.75ms
step:75/2330 train_time:4555ms step_avg:60.74ms
step:76/2330 train_time:4617ms step_avg:60.75ms
step:77/2330 train_time:4676ms step_avg:60.73ms
step:78/2330 train_time:4737ms step_avg:60.74ms
step:79/2330 train_time:4797ms step_avg:60.72ms
step:80/2330 train_time:4859ms step_avg:60.74ms
step:81/2330 train_time:4919ms step_avg:60.72ms
step:82/2330 train_time:4980ms step_avg:60.74ms
step:83/2330 train_time:5040ms step_avg:60.72ms
step:84/2330 train_time:5102ms step_avg:60.73ms
step:85/2330 train_time:5161ms step_avg:60.72ms
step:86/2330 train_time:5224ms step_avg:60.74ms
step:87/2330 train_time:5284ms step_avg:60.73ms
step:88/2330 train_time:5345ms step_avg:60.74ms
step:89/2330 train_time:5405ms step_avg:60.73ms
step:90/2330 train_time:5467ms step_avg:60.75ms
step:91/2330 train_time:5528ms step_avg:60.74ms
step:92/2330 train_time:5590ms step_avg:60.76ms
step:93/2330 train_time:5650ms step_avg:60.76ms
step:94/2330 train_time:5713ms step_avg:60.77ms
step:95/2330 train_time:5772ms step_avg:60.76ms
step:96/2330 train_time:5834ms step_avg:60.77ms
step:97/2330 train_time:5894ms step_avg:60.76ms
step:98/2330 train_time:5955ms step_avg:60.77ms
step:99/2330 train_time:6014ms step_avg:60.75ms
step:100/2330 train_time:6076ms step_avg:60.76ms
step:101/2330 train_time:6137ms step_avg:60.76ms
step:102/2330 train_time:6199ms step_avg:60.77ms
step:103/2330 train_time:6258ms step_avg:60.76ms
step:104/2330 train_time:6320ms step_avg:60.77ms
step:105/2330 train_time:6380ms step_avg:60.76ms
step:106/2330 train_time:6441ms step_avg:60.77ms
step:107/2330 train_time:6501ms step_avg:60.76ms
step:108/2330 train_time:6564ms step_avg:60.77ms
step:109/2330 train_time:6624ms step_avg:60.77ms
step:110/2330 train_time:6686ms step_avg:60.78ms
step:111/2330 train_time:6745ms step_avg:60.77ms
step:112/2330 train_time:6808ms step_avg:60.79ms
step:113/2330 train_time:6868ms step_avg:60.78ms
step:114/2330 train_time:6930ms step_avg:60.79ms
step:115/2330 train_time:6989ms step_avg:60.78ms
step:116/2330 train_time:7052ms step_avg:60.79ms
step:117/2330 train_time:7112ms step_avg:60.79ms
step:118/2330 train_time:7174ms step_avg:60.80ms
step:119/2330 train_time:7234ms step_avg:60.79ms
step:120/2330 train_time:7295ms step_avg:60.79ms
step:121/2330 train_time:7355ms step_avg:60.78ms
step:122/2330 train_time:7416ms step_avg:60.79ms
step:123/2330 train_time:7475ms step_avg:60.77ms
step:124/2330 train_time:7537ms step_avg:60.78ms
step:125/2330 train_time:7596ms step_avg:60.77ms
step:126/2330 train_time:7658ms step_avg:60.78ms
step:127/2330 train_time:7718ms step_avg:60.77ms
step:128/2330 train_time:7780ms step_avg:60.78ms
step:129/2330 train_time:7840ms step_avg:60.78ms
step:130/2330 train_time:7902ms step_avg:60.78ms
step:131/2330 train_time:7962ms step_avg:60.78ms
step:132/2330 train_time:8024ms step_avg:60.78ms
step:133/2330 train_time:8084ms step_avg:60.78ms
step:134/2330 train_time:8146ms step_avg:60.79ms
step:135/2330 train_time:8206ms step_avg:60.78ms
step:136/2330 train_time:8267ms step_avg:60.79ms
step:137/2330 train_time:8327ms step_avg:60.78ms
step:138/2330 train_time:8390ms step_avg:60.79ms
step:139/2330 train_time:8450ms step_avg:60.79ms
step:140/2330 train_time:8512ms step_avg:60.80ms
step:141/2330 train_time:8571ms step_avg:60.79ms
step:142/2330 train_time:8633ms step_avg:60.80ms
step:143/2330 train_time:8693ms step_avg:60.79ms
step:144/2330 train_time:8754ms step_avg:60.79ms
step:145/2330 train_time:8814ms step_avg:60.79ms
step:146/2330 train_time:8876ms step_avg:60.79ms
step:147/2330 train_time:8935ms step_avg:60.78ms
step:148/2330 train_time:8997ms step_avg:60.79ms
step:149/2330 train_time:9056ms step_avg:60.78ms
step:150/2330 train_time:9118ms step_avg:60.79ms
step:151/2330 train_time:9178ms step_avg:60.78ms
step:152/2330 train_time:9240ms step_avg:60.79ms
step:153/2330 train_time:9301ms step_avg:60.79ms
step:154/2330 train_time:9363ms step_avg:60.80ms
step:155/2330 train_time:9422ms step_avg:60.79ms
step:156/2330 train_time:9484ms step_avg:60.80ms
step:157/2330 train_time:9544ms step_avg:60.79ms
step:158/2330 train_time:9606ms step_avg:60.80ms
step:159/2330 train_time:9666ms step_avg:60.79ms
step:160/2330 train_time:9728ms step_avg:60.80ms
step:161/2330 train_time:9788ms step_avg:60.80ms
step:162/2330 train_time:9850ms step_avg:60.80ms
step:163/2330 train_time:9910ms step_avg:60.80ms
step:164/2330 train_time:9972ms step_avg:60.81ms
step:165/2330 train_time:10032ms step_avg:60.80ms
step:166/2330 train_time:10095ms step_avg:60.81ms
step:167/2330 train_time:10154ms step_avg:60.80ms
step:168/2330 train_time:10216ms step_avg:60.81ms
step:169/2330 train_time:10276ms step_avg:60.80ms
step:170/2330 train_time:10338ms step_avg:60.81ms
step:171/2330 train_time:10398ms step_avg:60.80ms
step:172/2330 train_time:10459ms step_avg:60.81ms
step:173/2330 train_time:10518ms step_avg:60.80ms
step:174/2330 train_time:10580ms step_avg:60.81ms
step:175/2330 train_time:10640ms step_avg:60.80ms
step:176/2330 train_time:10702ms step_avg:60.81ms
step:177/2330 train_time:10762ms step_avg:60.80ms
step:178/2330 train_time:10824ms step_avg:60.81ms
step:179/2330 train_time:10884ms step_avg:60.80ms
step:180/2330 train_time:10945ms step_avg:60.81ms
step:181/2330 train_time:11005ms step_avg:60.80ms
step:182/2330 train_time:11067ms step_avg:60.81ms
step:183/2330 train_time:11127ms step_avg:60.80ms
step:184/2330 train_time:11190ms step_avg:60.81ms
step:185/2330 train_time:11250ms step_avg:60.81ms
step:186/2330 train_time:11312ms step_avg:60.82ms
step:187/2330 train_time:11373ms step_avg:60.82ms
step:188/2330 train_time:11434ms step_avg:60.82ms
step:189/2330 train_time:11494ms step_avg:60.82ms
step:190/2330 train_time:11555ms step_avg:60.82ms
step:191/2330 train_time:11615ms step_avg:60.81ms
step:192/2330 train_time:11676ms step_avg:60.81ms
step:193/2330 train_time:11736ms step_avg:60.81ms
step:194/2330 train_time:11798ms step_avg:60.81ms
step:195/2330 train_time:11858ms step_avg:60.81ms
step:196/2330 train_time:11920ms step_avg:60.82ms
step:197/2330 train_time:11979ms step_avg:60.81ms
step:198/2330 train_time:12041ms step_avg:60.81ms
step:199/2330 train_time:12101ms step_avg:60.81ms
step:200/2330 train_time:12163ms step_avg:60.81ms
step:201/2330 train_time:12223ms step_avg:60.81ms
step:202/2330 train_time:12285ms step_avg:60.82ms
step:203/2330 train_time:12345ms step_avg:60.81ms
step:204/2330 train_time:12407ms step_avg:60.82ms
step:205/2330 train_time:12467ms step_avg:60.82ms
step:206/2330 train_time:12529ms step_avg:60.82ms
step:207/2330 train_time:12589ms step_avg:60.82ms
step:208/2330 train_time:12651ms step_avg:60.82ms
step:209/2330 train_time:12712ms step_avg:60.82ms
step:210/2330 train_time:12773ms step_avg:60.83ms
step:211/2330 train_time:12833ms step_avg:60.82ms
step:212/2330 train_time:12895ms step_avg:60.83ms
step:213/2330 train_time:12955ms step_avg:60.82ms
step:214/2330 train_time:13017ms step_avg:60.83ms
step:215/2330 train_time:13076ms step_avg:60.82ms
step:216/2330 train_time:13138ms step_avg:60.82ms
step:217/2330 train_time:13198ms step_avg:60.82ms
step:218/2330 train_time:13259ms step_avg:60.82ms
step:219/2330 train_time:13320ms step_avg:60.82ms
step:220/2330 train_time:13382ms step_avg:60.83ms
step:221/2330 train_time:13442ms step_avg:60.82ms
step:222/2330 train_time:13504ms step_avg:60.83ms
step:223/2330 train_time:13564ms step_avg:60.82ms
step:224/2330 train_time:13626ms step_avg:60.83ms
step:225/2330 train_time:13685ms step_avg:60.82ms
step:226/2330 train_time:13747ms step_avg:60.83ms
step:227/2330 train_time:13807ms step_avg:60.82ms
step:228/2330 train_time:13869ms step_avg:60.83ms
step:229/2330 train_time:13929ms step_avg:60.82ms
step:230/2330 train_time:13991ms step_avg:60.83ms
step:231/2330 train_time:14052ms step_avg:60.83ms
step:232/2330 train_time:14114ms step_avg:60.83ms
step:233/2330 train_time:14173ms step_avg:60.83ms
step:234/2330 train_time:14234ms step_avg:60.83ms
step:235/2330 train_time:14294ms step_avg:60.83ms
step:236/2330 train_time:14356ms step_avg:60.83ms
step:237/2330 train_time:14415ms step_avg:60.82ms
step:238/2330 train_time:14477ms step_avg:60.83ms
step:239/2330 train_time:14537ms step_avg:60.82ms
step:240/2330 train_time:14599ms step_avg:60.83ms
step:241/2330 train_time:14659ms step_avg:60.83ms
step:242/2330 train_time:14721ms step_avg:60.83ms
step:243/2330 train_time:14781ms step_avg:60.83ms
step:244/2330 train_time:14843ms step_avg:60.83ms
step:245/2330 train_time:14903ms step_avg:60.83ms
step:246/2330 train_time:14965ms step_avg:60.83ms
step:247/2330 train_time:15025ms step_avg:60.83ms
step:248/2330 train_time:15087ms step_avg:60.84ms
step:249/2330 train_time:15148ms step_avg:60.83ms
step:250/2330 train_time:15209ms step_avg:60.84ms
step:250/2330 val_loss:4.1754 train_time:15274ms step_avg:61.09ms
step:251/2330 train_time:15296ms step_avg:60.94ms
step:252/2330 train_time:15333ms step_avg:60.85ms
step:253/2330 train_time:15399ms step_avg:60.86ms
step:254/2330 train_time:15463ms step_avg:60.88ms
step:255/2330 train_time:15522ms step_avg:60.87ms
step:256/2330 train_time:15584ms step_avg:60.88ms
step:257/2330 train_time:15643ms step_avg:60.87ms
step:258/2330 train_time:15704ms step_avg:60.87ms
step:259/2330 train_time:15763ms step_avg:60.86ms
step:260/2330 train_time:15824ms step_avg:60.86ms
step:261/2330 train_time:15882ms step_avg:60.85ms
step:262/2330 train_time:15943ms step_avg:60.85ms
step:263/2330 train_time:16002ms step_avg:60.84ms
step:264/2330 train_time:16063ms step_avg:60.84ms
step:265/2330 train_time:16121ms step_avg:60.84ms
step:266/2330 train_time:16184ms step_avg:60.84ms
step:267/2330 train_time:16247ms step_avg:60.85ms
step:268/2330 train_time:16310ms step_avg:60.86ms
step:269/2330 train_time:16371ms step_avg:60.86ms
step:270/2330 train_time:16433ms step_avg:60.86ms
step:271/2330 train_time:16494ms step_avg:60.86ms
step:272/2330 train_time:16556ms step_avg:60.87ms
step:273/2330 train_time:16615ms step_avg:60.86ms
step:274/2330 train_time:16678ms step_avg:60.87ms
step:275/2330 train_time:16738ms step_avg:60.86ms
step:276/2330 train_time:16799ms step_avg:60.87ms
step:277/2330 train_time:16859ms step_avg:60.86ms
step:278/2330 train_time:16921ms step_avg:60.87ms
step:279/2330 train_time:16980ms step_avg:60.86ms
step:280/2330 train_time:17041ms step_avg:60.86ms
step:281/2330 train_time:17101ms step_avg:60.86ms
step:282/2330 train_time:17163ms step_avg:60.86ms
step:283/2330 train_time:17223ms step_avg:60.86ms
step:284/2330 train_time:17285ms step_avg:60.86ms
step:285/2330 train_time:17346ms step_avg:60.86ms
step:286/2330 train_time:17408ms step_avg:60.87ms
step:287/2330 train_time:17469ms step_avg:60.87ms
step:288/2330 train_time:17530ms step_avg:60.87ms
step:289/2330 train_time:17590ms step_avg:60.87ms
step:290/2330 train_time:17653ms step_avg:60.87ms
step:291/2330 train_time:17713ms step_avg:60.87ms
step:292/2330 train_time:17776ms step_avg:60.88ms
step:293/2330 train_time:17836ms step_avg:60.87ms
step:294/2330 train_time:17897ms step_avg:60.88ms
step:295/2330 train_time:17957ms step_avg:60.87ms
step:296/2330 train_time:18019ms step_avg:60.88ms
step:297/2330 train_time:18078ms step_avg:60.87ms
step:298/2330 train_time:18141ms step_avg:60.88ms
step:299/2330 train_time:18201ms step_avg:60.87ms
step:300/2330 train_time:18263ms step_avg:60.88ms
step:301/2330 train_time:18322ms step_avg:60.87ms
step:302/2330 train_time:18384ms step_avg:60.87ms
step:303/2330 train_time:18444ms step_avg:60.87ms
step:304/2330 train_time:18506ms step_avg:60.87ms
step:305/2330 train_time:18566ms step_avg:60.87ms
step:306/2330 train_time:18627ms step_avg:60.87ms
step:307/2330 train_time:18688ms step_avg:60.87ms
step:308/2330 train_time:18750ms step_avg:60.88ms
step:309/2330 train_time:18811ms step_avg:60.88ms
step:310/2330 train_time:18874ms step_avg:60.88ms
step:311/2330 train_time:18933ms step_avg:60.88ms
step:312/2330 train_time:18995ms step_avg:60.88ms
step:313/2330 train_time:19054ms step_avg:60.88ms
step:314/2330 train_time:19116ms step_avg:60.88ms
step:315/2330 train_time:19176ms step_avg:60.87ms
step:316/2330 train_time:19238ms step_avg:60.88ms
step:317/2330 train_time:19299ms step_avg:60.88ms
step:318/2330 train_time:19361ms step_avg:60.88ms
step:319/2330 train_time:19421ms step_avg:60.88ms
step:320/2330 train_time:19483ms step_avg:60.88ms
step:321/2330 train_time:19544ms step_avg:60.88ms
step:322/2330 train_time:19605ms step_avg:60.89ms
step:323/2330 train_time:19665ms step_avg:60.88ms
step:324/2330 train_time:19727ms step_avg:60.88ms
step:325/2330 train_time:19787ms step_avg:60.88ms
step:326/2330 train_time:19848ms step_avg:60.88ms
step:327/2330 train_time:19908ms step_avg:60.88ms
step:328/2330 train_time:19969ms step_avg:60.88ms
step:329/2330 train_time:20029ms step_avg:60.88ms
step:330/2330 train_time:20091ms step_avg:60.88ms
step:331/2330 train_time:20151ms step_avg:60.88ms
step:332/2330 train_time:20214ms step_avg:60.88ms
step:333/2330 train_time:20275ms step_avg:60.89ms
step:334/2330 train_time:20337ms step_avg:60.89ms
step:335/2330 train_time:20397ms step_avg:60.89ms
step:336/2330 train_time:20459ms step_avg:60.89ms
step:337/2330 train_time:20519ms step_avg:60.89ms
step:338/2330 train_time:20581ms step_avg:60.89ms
step:339/2330 train_time:20641ms step_avg:60.89ms
step:340/2330 train_time:20703ms step_avg:60.89ms
step:341/2330 train_time:20762ms step_avg:60.89ms
step:342/2330 train_time:20823ms step_avg:60.89ms
step:343/2330 train_time:20885ms step_avg:60.89ms
step:344/2330 train_time:20946ms step_avg:60.89ms
step:345/2330 train_time:21006ms step_avg:60.89ms
step:346/2330 train_time:21068ms step_avg:60.89ms
step:347/2330 train_time:21127ms step_avg:60.88ms
step:348/2330 train_time:21189ms step_avg:60.89ms
step:349/2330 train_time:21249ms step_avg:60.89ms
step:350/2330 train_time:21312ms step_avg:60.89ms
step:351/2330 train_time:21372ms step_avg:60.89ms
step:352/2330 train_time:21435ms step_avg:60.90ms
step:353/2330 train_time:21496ms step_avg:60.90ms
step:354/2330 train_time:21559ms step_avg:60.90ms
step:355/2330 train_time:21619ms step_avg:60.90ms
step:356/2330 train_time:21681ms step_avg:60.90ms
step:357/2330 train_time:21740ms step_avg:60.90ms
step:358/2330 train_time:21801ms step_avg:60.90ms
step:359/2330 train_time:21861ms step_avg:60.89ms
step:360/2330 train_time:21922ms step_avg:60.90ms
step:361/2330 train_time:21982ms step_avg:60.89ms
step:362/2330 train_time:22044ms step_avg:60.90ms
step:363/2330 train_time:22103ms step_avg:60.89ms
step:364/2330 train_time:22165ms step_avg:60.89ms
step:365/2330 train_time:22225ms step_avg:60.89ms
step:366/2330 train_time:22288ms step_avg:60.90ms
step:367/2330 train_time:22348ms step_avg:60.89ms
step:368/2330 train_time:22410ms step_avg:60.90ms
step:369/2330 train_time:22470ms step_avg:60.89ms
step:370/2330 train_time:22532ms step_avg:60.90ms
step:371/2330 train_time:22593ms step_avg:60.90ms
step:372/2330 train_time:22655ms step_avg:60.90ms
step:373/2330 train_time:22715ms step_avg:60.90ms
step:374/2330 train_time:22777ms step_avg:60.90ms
step:375/2330 train_time:22837ms step_avg:60.90ms
step:376/2330 train_time:22899ms step_avg:60.90ms
step:377/2330 train_time:22959ms step_avg:60.90ms
step:378/2330 train_time:23021ms step_avg:60.90ms
step:379/2330 train_time:23081ms step_avg:60.90ms
step:380/2330 train_time:23143ms step_avg:60.90ms
step:381/2330 train_time:23203ms step_avg:60.90ms
step:382/2330 train_time:23264ms step_avg:60.90ms
step:383/2330 train_time:23324ms step_avg:60.90ms
step:384/2330 train_time:23387ms step_avg:60.90ms
step:385/2330 train_time:23446ms step_avg:60.90ms
step:386/2330 train_time:23508ms step_avg:60.90ms
step:387/2330 train_time:23568ms step_avg:60.90ms
step:388/2330 train_time:23630ms step_avg:60.90ms
step:389/2330 train_time:23690ms step_avg:60.90ms
step:390/2330 train_time:23751ms step_avg:60.90ms
step:391/2330 train_time:23811ms step_avg:60.90ms
step:392/2330 train_time:23874ms step_avg:60.90ms
step:393/2330 train_time:23934ms step_avg:60.90ms
step:394/2330 train_time:23996ms step_avg:60.90ms
step:395/2330 train_time:24056ms step_avg:60.90ms
step:396/2330 train_time:24119ms step_avg:60.91ms
step:397/2330 train_time:24179ms step_avg:60.90ms
step:398/2330 train_time:24241ms step_avg:60.91ms
step:399/2330 train_time:24301ms step_avg:60.91ms
step:400/2330 train_time:24363ms step_avg:60.91ms
step:401/2330 train_time:24423ms step_avg:60.91ms
step:402/2330 train_time:24487ms step_avg:60.91ms
step:403/2330 train_time:24546ms step_avg:60.91ms
step:404/2330 train_time:24608ms step_avg:60.91ms
step:405/2330 train_time:24667ms step_avg:60.91ms
step:406/2330 train_time:24729ms step_avg:60.91ms
step:407/2330 train_time:24789ms step_avg:60.91ms
step:408/2330 train_time:24852ms step_avg:60.91ms
step:409/2330 train_time:24911ms step_avg:60.91ms
step:410/2330 train_time:24974ms step_avg:60.91ms
step:411/2330 train_time:25034ms step_avg:60.91ms
step:412/2330 train_time:25096ms step_avg:60.91ms
step:413/2330 train_time:25156ms step_avg:60.91ms
step:414/2330 train_time:25218ms step_avg:60.91ms
step:415/2330 train_time:25277ms step_avg:60.91ms
step:416/2330 train_time:25340ms step_avg:60.91ms
step:417/2330 train_time:25400ms step_avg:60.91ms
step:418/2330 train_time:25463ms step_avg:60.92ms
step:419/2330 train_time:25522ms step_avg:60.91ms
step:420/2330 train_time:25583ms step_avg:60.91ms
step:421/2330 train_time:25642ms step_avg:60.91ms
step:422/2330 train_time:25704ms step_avg:60.91ms
step:423/2330 train_time:25764ms step_avg:60.91ms
step:424/2330 train_time:25826ms step_avg:60.91ms
step:425/2330 train_time:25886ms step_avg:60.91ms
step:426/2330 train_time:25948ms step_avg:60.91ms
step:427/2330 train_time:26008ms step_avg:60.91ms
step:428/2330 train_time:26070ms step_avg:60.91ms
step:429/2330 train_time:26130ms step_avg:60.91ms
step:430/2330 train_time:26192ms step_avg:60.91ms
step:431/2330 train_time:26252ms step_avg:60.91ms
step:432/2330 train_time:26314ms step_avg:60.91ms
step:433/2330 train_time:26374ms step_avg:60.91ms
step:434/2330 train_time:26436ms step_avg:60.91ms
step:435/2330 train_time:26497ms step_avg:60.91ms
step:436/2330 train_time:26560ms step_avg:60.92ms
step:437/2330 train_time:26619ms step_avg:60.91ms
step:438/2330 train_time:26681ms step_avg:60.92ms
step:439/2330 train_time:26741ms step_avg:60.91ms
step:440/2330 train_time:26802ms step_avg:60.91ms
step:441/2330 train_time:26862ms step_avg:60.91ms
step:442/2330 train_time:26924ms step_avg:60.91ms
step:443/2330 train_time:26984ms step_avg:60.91ms
step:444/2330 train_time:27045ms step_avg:60.91ms
step:445/2330 train_time:27104ms step_avg:60.91ms
step:446/2330 train_time:27166ms step_avg:60.91ms
step:447/2330 train_time:27226ms step_avg:60.91ms
step:448/2330 train_time:27288ms step_avg:60.91ms
step:449/2330 train_time:27348ms step_avg:60.91ms
step:450/2330 train_time:27410ms step_avg:60.91ms
step:451/2330 train_time:27470ms step_avg:60.91ms
step:452/2330 train_time:27532ms step_avg:60.91ms
step:453/2330 train_time:27592ms step_avg:60.91ms
step:454/2330 train_time:27655ms step_avg:60.91ms
step:455/2330 train_time:27714ms step_avg:60.91ms
step:456/2330 train_time:27777ms step_avg:60.91ms
step:457/2330 train_time:27836ms step_avg:60.91ms
step:458/2330 train_time:27899ms step_avg:60.91ms
step:459/2330 train_time:27959ms step_avg:60.91ms
step:460/2330 train_time:28021ms step_avg:60.92ms
step:461/2330 train_time:28081ms step_avg:60.91ms
step:462/2330 train_time:28142ms step_avg:60.91ms
step:463/2330 train_time:28201ms step_avg:60.91ms
step:464/2330 train_time:28263ms step_avg:60.91ms
step:465/2330 train_time:28322ms step_avg:60.91ms
step:466/2330 train_time:28383ms step_avg:60.91ms
step:467/2330 train_time:28443ms step_avg:60.91ms
step:468/2330 train_time:28505ms step_avg:60.91ms
step:469/2330 train_time:28564ms step_avg:60.90ms
step:470/2330 train_time:28626ms step_avg:60.91ms
step:471/2330 train_time:28686ms step_avg:60.90ms
step:472/2330 train_time:28749ms step_avg:60.91ms
step:473/2330 train_time:28808ms step_avg:60.91ms
step:474/2330 train_time:28871ms step_avg:60.91ms
step:475/2330 train_time:28930ms step_avg:60.91ms
step:476/2330 train_time:28992ms step_avg:60.91ms
step:477/2330 train_time:29052ms step_avg:60.91ms
step:478/2330 train_time:29114ms step_avg:60.91ms
step:479/2330 train_time:29175ms step_avg:60.91ms
step:480/2330 train_time:29236ms step_avg:60.91ms
step:481/2330 train_time:29296ms step_avg:60.91ms
step:482/2330 train_time:29358ms step_avg:60.91ms
step:483/2330 train_time:29418ms step_avg:60.91ms
step:484/2330 train_time:29480ms step_avg:60.91ms
step:485/2330 train_time:29540ms step_avg:60.91ms
step:486/2330 train_time:29602ms step_avg:60.91ms
step:487/2330 train_time:29661ms step_avg:60.91ms
step:488/2330 train_time:29723ms step_avg:60.91ms
step:489/2330 train_time:29783ms step_avg:60.90ms
step:490/2330 train_time:29844ms step_avg:60.91ms
step:491/2330 train_time:29903ms step_avg:60.90ms
step:492/2330 train_time:29965ms step_avg:60.91ms
step:493/2330 train_time:30025ms step_avg:60.90ms
step:494/2330 train_time:30087ms step_avg:60.91ms
step:495/2330 train_time:30147ms step_avg:60.90ms
step:496/2330 train_time:30209ms step_avg:60.90ms
step:497/2330 train_time:30268ms step_avg:60.90ms
step:498/2330 train_time:30330ms step_avg:60.90ms
step:499/2330 train_time:30391ms step_avg:60.90ms
step:500/2330 train_time:30452ms step_avg:60.90ms
step:500/2330 val_loss:3.8754 train_time:30517ms step_avg:61.03ms
step:501/2330 train_time:30539ms step_avg:60.96ms
step:502/2330 train_time:30576ms step_avg:60.91ms
step:503/2330 train_time:30640ms step_avg:60.92ms
step:504/2330 train_time:30707ms step_avg:60.93ms
step:505/2330 train_time:30768ms step_avg:60.93ms
step:506/2330 train_time:30830ms step_avg:60.93ms
step:507/2330 train_time:30889ms step_avg:60.93ms
step:508/2330 train_time:30951ms step_avg:60.93ms
step:509/2330 train_time:31011ms step_avg:60.92ms
step:510/2330 train_time:31072ms step_avg:60.93ms
step:511/2330 train_time:31130ms step_avg:60.92ms
step:512/2330 train_time:31192ms step_avg:60.92ms
step:513/2330 train_time:31251ms step_avg:60.92ms
step:514/2330 train_time:31312ms step_avg:60.92ms
step:515/2330 train_time:31372ms step_avg:60.92ms
step:516/2330 train_time:31433ms step_avg:60.92ms
step:517/2330 train_time:31494ms step_avg:60.92ms
step:518/2330 train_time:31556ms step_avg:60.92ms
step:519/2330 train_time:31617ms step_avg:60.92ms
step:520/2330 train_time:31679ms step_avg:60.92ms
step:521/2330 train_time:31739ms step_avg:60.92ms
step:522/2330 train_time:31801ms step_avg:60.92ms
step:523/2330 train_time:31861ms step_avg:60.92ms
step:524/2330 train_time:31924ms step_avg:60.92ms
step:525/2330 train_time:31983ms step_avg:60.92ms
step:526/2330 train_time:32045ms step_avg:60.92ms
step:527/2330 train_time:32104ms step_avg:60.92ms
step:528/2330 train_time:32167ms step_avg:60.92ms
step:529/2330 train_time:32226ms step_avg:60.92ms
step:530/2330 train_time:32289ms step_avg:60.92ms
step:531/2330 train_time:32348ms step_avg:60.92ms
step:532/2330 train_time:32410ms step_avg:60.92ms
step:533/2330 train_time:32470ms step_avg:60.92ms
step:534/2330 train_time:32532ms step_avg:60.92ms
step:535/2330 train_time:32594ms step_avg:60.92ms
step:536/2330 train_time:32656ms step_avg:60.93ms
step:537/2330 train_time:32716ms step_avg:60.92ms
step:538/2330 train_time:32778ms step_avg:60.93ms
step:539/2330 train_time:32838ms step_avg:60.92ms
step:540/2330 train_time:32900ms step_avg:60.93ms
step:541/2330 train_time:32959ms step_avg:60.92ms
step:542/2330 train_time:33020ms step_avg:60.92ms
step:543/2330 train_time:33080ms step_avg:60.92ms
step:544/2330 train_time:33142ms step_avg:60.92ms
step:545/2330 train_time:33201ms step_avg:60.92ms
step:546/2330 train_time:33264ms step_avg:60.92ms
step:547/2330 train_time:33324ms step_avg:60.92ms
step:548/2330 train_time:33386ms step_avg:60.92ms
step:549/2330 train_time:33446ms step_avg:60.92ms
step:550/2330 train_time:33509ms step_avg:60.93ms
step:551/2330 train_time:33569ms step_avg:60.92ms
step:552/2330 train_time:33631ms step_avg:60.93ms
step:553/2330 train_time:33691ms step_avg:60.92ms
step:554/2330 train_time:33754ms step_avg:60.93ms
step:555/2330 train_time:33814ms step_avg:60.93ms
step:556/2330 train_time:33876ms step_avg:60.93ms
step:557/2330 train_time:33935ms step_avg:60.92ms
step:558/2330 train_time:33996ms step_avg:60.93ms
step:559/2330 train_time:34055ms step_avg:60.92ms
step:560/2330 train_time:34117ms step_avg:60.92ms
step:561/2330 train_time:34177ms step_avg:60.92ms
step:562/2330 train_time:34239ms step_avg:60.92ms
step:563/2330 train_time:34298ms step_avg:60.92ms
step:564/2330 train_time:34361ms step_avg:60.92ms
step:565/2330 train_time:34422ms step_avg:60.92ms
step:566/2330 train_time:34484ms step_avg:60.92ms
step:567/2330 train_time:34544ms step_avg:60.92ms
step:568/2330 train_time:34607ms step_avg:60.93ms
step:569/2330 train_time:34668ms step_avg:60.93ms
step:570/2330 train_time:34730ms step_avg:60.93ms
step:571/2330 train_time:34790ms step_avg:60.93ms
step:572/2330 train_time:34852ms step_avg:60.93ms
step:573/2330 train_time:34912ms step_avg:60.93ms
step:574/2330 train_time:34974ms step_avg:60.93ms
step:575/2330 train_time:35034ms step_avg:60.93ms
step:576/2330 train_time:35095ms step_avg:60.93ms
step:577/2330 train_time:35155ms step_avg:60.93ms
step:578/2330 train_time:35216ms step_avg:60.93ms
step:579/2330 train_time:35276ms step_avg:60.93ms
step:580/2330 train_time:35338ms step_avg:60.93ms
step:581/2330 train_time:35397ms step_avg:60.92ms
step:582/2330 train_time:35459ms step_avg:60.93ms
step:583/2330 train_time:35519ms step_avg:60.92ms
step:584/2330 train_time:35581ms step_avg:60.93ms
step:585/2330 train_time:35641ms step_avg:60.92ms
step:586/2330 train_time:35703ms step_avg:60.93ms
step:587/2330 train_time:35764ms step_avg:60.93ms
step:588/2330 train_time:35826ms step_avg:60.93ms
step:589/2330 train_time:35886ms step_avg:60.93ms
step:590/2330 train_time:35947ms step_avg:60.93ms
step:591/2330 train_time:36008ms step_avg:60.93ms
step:592/2330 train_time:36069ms step_avg:60.93ms
step:593/2330 train_time:36129ms step_avg:60.93ms
step:594/2330 train_time:36191ms step_avg:60.93ms
step:595/2330 train_time:36251ms step_avg:60.93ms
step:596/2330 train_time:36313ms step_avg:60.93ms
step:597/2330 train_time:36373ms step_avg:60.93ms
step:598/2330 train_time:36435ms step_avg:60.93ms
step:599/2330 train_time:36495ms step_avg:60.93ms
step:600/2330 train_time:36556ms step_avg:60.93ms
step:601/2330 train_time:36616ms step_avg:60.92ms
step:602/2330 train_time:36677ms step_avg:60.93ms
step:603/2330 train_time:36736ms step_avg:60.92ms
step:604/2330 train_time:36798ms step_avg:60.92ms
step:605/2330 train_time:36858ms step_avg:60.92ms
step:606/2330 train_time:36921ms step_avg:60.92ms
step:607/2330 train_time:36980ms step_avg:60.92ms
step:608/2330 train_time:37042ms step_avg:60.92ms
step:609/2330 train_time:37102ms step_avg:60.92ms
step:610/2330 train_time:37165ms step_avg:60.93ms
step:611/2330 train_time:37224ms step_avg:60.92ms
step:612/2330 train_time:37286ms step_avg:60.93ms
step:613/2330 train_time:37346ms step_avg:60.92ms
step:614/2330 train_time:37409ms step_avg:60.93ms
step:615/2330 train_time:37468ms step_avg:60.92ms
step:616/2330 train_time:37530ms step_avg:60.93ms
step:617/2330 train_time:37591ms step_avg:60.93ms
step:618/2330 train_time:37653ms step_avg:60.93ms
step:619/2330 train_time:37713ms step_avg:60.92ms
step:620/2330 train_time:37774ms step_avg:60.93ms
step:621/2330 train_time:37834ms step_avg:60.92ms
step:622/2330 train_time:37895ms step_avg:60.92ms
step:623/2330 train_time:37955ms step_avg:60.92ms
step:624/2330 train_time:38016ms step_avg:60.92ms
step:625/2330 train_time:38076ms step_avg:60.92ms
step:626/2330 train_time:38138ms step_avg:60.92ms
step:627/2330 train_time:38198ms step_avg:60.92ms
step:628/2330 train_time:38259ms step_avg:60.92ms
step:629/2330 train_time:38319ms step_avg:60.92ms
step:630/2330 train_time:38380ms step_avg:60.92ms
step:631/2330 train_time:38440ms step_avg:60.92ms
step:632/2330 train_time:38502ms step_avg:60.92ms
step:633/2330 train_time:38562ms step_avg:60.92ms
step:634/2330 train_time:38625ms step_avg:60.92ms
step:635/2330 train_time:38684ms step_avg:60.92ms
step:636/2330 train_time:38746ms step_avg:60.92ms
step:637/2330 train_time:38807ms step_avg:60.92ms
step:638/2330 train_time:38869ms step_avg:60.92ms
step:639/2330 train_time:38929ms step_avg:60.92ms
step:640/2330 train_time:38991ms step_avg:60.92ms
step:641/2330 train_time:39052ms step_avg:60.92ms
step:642/2330 train_time:39114ms step_avg:60.92ms
step:643/2330 train_time:39173ms step_avg:60.92ms
step:644/2330 train_time:39235ms step_avg:60.92ms
step:645/2330 train_time:39294ms step_avg:60.92ms
step:646/2330 train_time:39356ms step_avg:60.92ms
step:647/2330 train_time:39415ms step_avg:60.92ms
step:648/2330 train_time:39477ms step_avg:60.92ms
step:649/2330 train_time:39536ms step_avg:60.92ms
step:650/2330 train_time:39598ms step_avg:60.92ms
step:651/2330 train_time:39657ms step_avg:60.92ms
step:652/2330 train_time:39720ms step_avg:60.92ms
step:653/2330 train_time:39779ms step_avg:60.92ms
step:654/2330 train_time:39841ms step_avg:60.92ms
step:655/2330 train_time:39901ms step_avg:60.92ms
step:656/2330 train_time:39964ms step_avg:60.92ms
step:657/2330 train_time:40024ms step_avg:60.92ms
step:658/2330 train_time:40086ms step_avg:60.92ms
step:659/2330 train_time:40146ms step_avg:60.92ms
step:660/2330 train_time:40209ms step_avg:60.92ms
step:661/2330 train_time:40268ms step_avg:60.92ms
step:662/2330 train_time:40330ms step_avg:60.92ms
step:663/2330 train_time:40391ms step_avg:60.92ms
step:664/2330 train_time:40453ms step_avg:60.92ms
step:665/2330 train_time:40513ms step_avg:60.92ms
step:666/2330 train_time:40575ms step_avg:60.92ms
step:667/2330 train_time:40635ms step_avg:60.92ms
step:668/2330 train_time:40698ms step_avg:60.92ms
step:669/2330 train_time:40756ms step_avg:60.92ms
step:670/2330 train_time:40818ms step_avg:60.92ms
step:671/2330 train_time:40877ms step_avg:60.92ms
step:672/2330 train_time:40939ms step_avg:60.92ms
step:673/2330 train_time:40999ms step_avg:60.92ms
step:674/2330 train_time:41060ms step_avg:60.92ms
step:675/2330 train_time:41120ms step_avg:60.92ms
step:676/2330 train_time:41182ms step_avg:60.92ms
step:677/2330 train_time:41242ms step_avg:60.92ms
step:678/2330 train_time:41305ms step_avg:60.92ms
step:679/2330 train_time:41364ms step_avg:60.92ms
step:680/2330 train_time:41426ms step_avg:60.92ms
step:681/2330 train_time:41486ms step_avg:60.92ms
step:682/2330 train_time:41548ms step_avg:60.92ms
step:683/2330 train_time:41608ms step_avg:60.92ms
step:684/2330 train_time:41670ms step_avg:60.92ms
step:685/2330 train_time:41731ms step_avg:60.92ms
step:686/2330 train_time:41793ms step_avg:60.92ms
step:687/2330 train_time:41852ms step_avg:60.92ms
step:688/2330 train_time:41915ms step_avg:60.92ms
step:689/2330 train_time:41974ms step_avg:60.92ms
step:690/2330 train_time:42035ms step_avg:60.92ms
step:691/2330 train_time:42095ms step_avg:60.92ms
step:692/2330 train_time:42156ms step_avg:60.92ms
step:693/2330 train_time:42216ms step_avg:60.92ms
step:694/2330 train_time:42277ms step_avg:60.92ms
step:695/2330 train_time:42337ms step_avg:60.92ms
step:696/2330 train_time:42398ms step_avg:60.92ms
step:697/2330 train_time:42458ms step_avg:60.92ms
step:698/2330 train_time:42521ms step_avg:60.92ms
step:699/2330 train_time:42581ms step_avg:60.92ms
step:700/2330 train_time:42643ms step_avg:60.92ms
step:701/2330 train_time:42703ms step_avg:60.92ms
step:702/2330 train_time:42765ms step_avg:60.92ms
step:703/2330 train_time:42825ms step_avg:60.92ms
step:704/2330 train_time:42887ms step_avg:60.92ms
step:705/2330 train_time:42947ms step_avg:60.92ms
step:706/2330 train_time:43009ms step_avg:60.92ms
step:707/2330 train_time:43069ms step_avg:60.92ms
step:708/2330 train_time:43131ms step_avg:60.92ms
step:709/2330 train_time:43191ms step_avg:60.92ms
step:710/2330 train_time:43253ms step_avg:60.92ms
step:711/2330 train_time:43313ms step_avg:60.92ms
step:712/2330 train_time:43375ms step_avg:60.92ms
step:713/2330 train_time:43434ms step_avg:60.92ms
step:714/2330 train_time:43496ms step_avg:60.92ms
step:715/2330 train_time:43555ms step_avg:60.92ms
step:716/2330 train_time:43617ms step_avg:60.92ms
step:717/2330 train_time:43676ms step_avg:60.91ms
step:718/2330 train_time:43738ms step_avg:60.92ms
step:719/2330 train_time:43797ms step_avg:60.91ms
step:720/2330 train_time:43859ms step_avg:60.92ms
step:721/2330 train_time:43919ms step_avg:60.91ms
step:722/2330 train_time:43981ms step_avg:60.92ms
step:723/2330 train_time:44041ms step_avg:60.91ms
step:724/2330 train_time:44103ms step_avg:60.92ms
step:725/2330 train_time:44163ms step_avg:60.91ms
step:726/2330 train_time:44225ms step_avg:60.92ms
step:727/2330 train_time:44285ms step_avg:60.91ms
step:728/2330 train_time:44347ms step_avg:60.92ms
step:729/2330 train_time:44408ms step_avg:60.92ms
step:730/2330 train_time:44469ms step_avg:60.92ms
step:731/2330 train_time:44530ms step_avg:60.92ms
step:732/2330 train_time:44593ms step_avg:60.92ms
step:733/2330 train_time:44653ms step_avg:60.92ms
step:734/2330 train_time:44715ms step_avg:60.92ms
step:735/2330 train_time:44775ms step_avg:60.92ms
step:736/2330 train_time:44836ms step_avg:60.92ms
step:737/2330 train_time:44896ms step_avg:60.92ms
step:738/2330 train_time:44957ms step_avg:60.92ms
step:739/2330 train_time:45017ms step_avg:60.92ms
step:740/2330 train_time:45079ms step_avg:60.92ms
step:741/2330 train_time:45138ms step_avg:60.92ms
step:742/2330 train_time:45200ms step_avg:60.92ms
step:743/2330 train_time:45260ms step_avg:60.92ms
step:744/2330 train_time:45323ms step_avg:60.92ms
step:745/2330 train_time:45383ms step_avg:60.92ms
step:746/2330 train_time:45445ms step_avg:60.92ms
step:747/2330 train_time:45506ms step_avg:60.92ms
step:748/2330 train_time:45568ms step_avg:60.92ms
step:749/2330 train_time:45628ms step_avg:60.92ms
step:750/2330 train_time:45691ms step_avg:60.92ms
step:750/2330 val_loss:3.7223 train_time:45754ms step_avg:61.01ms
step:751/2330 train_time:45778ms step_avg:60.96ms
step:752/2330 train_time:45815ms step_avg:60.92ms
step:753/2330 train_time:45880ms step_avg:60.93ms
step:754/2330 train_time:45947ms step_avg:60.94ms
step:755/2330 train_time:46008ms step_avg:60.94ms
step:756/2330 train_time:46069ms step_avg:60.94ms
step:757/2330 train_time:46128ms step_avg:60.94ms
step:758/2330 train_time:46189ms step_avg:60.93ms
step:759/2330 train_time:46248ms step_avg:60.93ms
step:760/2330 train_time:46308ms step_avg:60.93ms
step:761/2330 train_time:46367ms step_avg:60.93ms
step:762/2330 train_time:46428ms step_avg:60.93ms
step:763/2330 train_time:46486ms step_avg:60.93ms
step:764/2330 train_time:46548ms step_avg:60.93ms
step:765/2330 train_time:46606ms step_avg:60.92ms
step:766/2330 train_time:46668ms step_avg:60.92ms
step:767/2330 train_time:46730ms step_avg:60.93ms
step:768/2330 train_time:46794ms step_avg:60.93ms
step:769/2330 train_time:46857ms step_avg:60.93ms
step:770/2330 train_time:46920ms step_avg:60.93ms
step:771/2330 train_time:46982ms step_avg:60.94ms
step:772/2330 train_time:47045ms step_avg:60.94ms
step:773/2330 train_time:47106ms step_avg:60.94ms
step:774/2330 train_time:47167ms step_avg:60.94ms
step:775/2330 train_time:47227ms step_avg:60.94ms
step:776/2330 train_time:47289ms step_avg:60.94ms
step:777/2330 train_time:47349ms step_avg:60.94ms
step:778/2330 train_time:47410ms step_avg:60.94ms
step:779/2330 train_time:47469ms step_avg:60.94ms
step:780/2330 train_time:47531ms step_avg:60.94ms
step:781/2330 train_time:47590ms step_avg:60.94ms
step:782/2330 train_time:47653ms step_avg:60.94ms
step:783/2330 train_time:47713ms step_avg:60.94ms
step:784/2330 train_time:47777ms step_avg:60.94ms
step:785/2330 train_time:47838ms step_avg:60.94ms
step:786/2330 train_time:47901ms step_avg:60.94ms
step:787/2330 train_time:47963ms step_avg:60.94ms
step:788/2330 train_time:48026ms step_avg:60.95ms
step:789/2330 train_time:48087ms step_avg:60.95ms
step:790/2330 train_time:48149ms step_avg:60.95ms
step:791/2330 train_time:48209ms step_avg:60.95ms
step:792/2330 train_time:48270ms step_avg:60.95ms
step:793/2330 train_time:48330ms step_avg:60.95ms
step:794/2330 train_time:48391ms step_avg:60.95ms
step:795/2330 train_time:48451ms step_avg:60.94ms
step:796/2330 train_time:48513ms step_avg:60.95ms
step:797/2330 train_time:48572ms step_avg:60.94ms
step:798/2330 train_time:48635ms step_avg:60.95ms
step:799/2330 train_time:48695ms step_avg:60.94ms
step:800/2330 train_time:48758ms step_avg:60.95ms
step:801/2330 train_time:48819ms step_avg:60.95ms
step:802/2330 train_time:48881ms step_avg:60.95ms
step:803/2330 train_time:48943ms step_avg:60.95ms
step:804/2330 train_time:49005ms step_avg:60.95ms
step:805/2330 train_time:49066ms step_avg:60.95ms
step:806/2330 train_time:49128ms step_avg:60.95ms
step:807/2330 train_time:49189ms step_avg:60.95ms
step:808/2330 train_time:49251ms step_avg:60.95ms
step:809/2330 train_time:49311ms step_avg:60.95ms
step:810/2330 train_time:49373ms step_avg:60.95ms
step:811/2330 train_time:49433ms step_avg:60.95ms
step:812/2330 train_time:49495ms step_avg:60.95ms
step:813/2330 train_time:49555ms step_avg:60.95ms
step:814/2330 train_time:49616ms step_avg:60.95ms
step:815/2330 train_time:49677ms step_avg:60.95ms
step:816/2330 train_time:49739ms step_avg:60.96ms
step:817/2330 train_time:49800ms step_avg:60.96ms
step:818/2330 train_time:49863ms step_avg:60.96ms
step:819/2330 train_time:49925ms step_avg:60.96ms
step:820/2330 train_time:49987ms step_avg:60.96ms
step:821/2330 train_time:50048ms step_avg:60.96ms
step:822/2330 train_time:50110ms step_avg:60.96ms
step:823/2330 train_time:50170ms step_avg:60.96ms
step:824/2330 train_time:50232ms step_avg:60.96ms
step:825/2330 train_time:50293ms step_avg:60.96ms
step:826/2330 train_time:50355ms step_avg:60.96ms
step:827/2330 train_time:50415ms step_avg:60.96ms
step:828/2330 train_time:50477ms step_avg:60.96ms
step:829/2330 train_time:50537ms step_avg:60.96ms
step:830/2330 train_time:50599ms step_avg:60.96ms
step:831/2330 train_time:50659ms step_avg:60.96ms
step:832/2330 train_time:50721ms step_avg:60.96ms
step:833/2330 train_time:50782ms step_avg:60.96ms
step:834/2330 train_time:50845ms step_avg:60.97ms
step:835/2330 train_time:50906ms step_avg:60.96ms
step:836/2330 train_time:50968ms step_avg:60.97ms
step:837/2330 train_time:51029ms step_avg:60.97ms
step:838/2330 train_time:51091ms step_avg:60.97ms
step:839/2330 train_time:51151ms step_avg:60.97ms
step:840/2330 train_time:51213ms step_avg:60.97ms
step:841/2330 train_time:51273ms step_avg:60.97ms
step:842/2330 train_time:51335ms step_avg:60.97ms
step:843/2330 train_time:51395ms step_avg:60.97ms
step:844/2330 train_time:51458ms step_avg:60.97ms
step:845/2330 train_time:51519ms step_avg:60.97ms
step:846/2330 train_time:51581ms step_avg:60.97ms
step:847/2330 train_time:51641ms step_avg:60.97ms
step:848/2330 train_time:51704ms step_avg:60.97ms
step:849/2330 train_time:51764ms step_avg:60.97ms
step:850/2330 train_time:51827ms step_avg:60.97ms
step:851/2330 train_time:51887ms step_avg:60.97ms
step:852/2330 train_time:51950ms step_avg:60.97ms
step:853/2330 train_time:52010ms step_avg:60.97ms
step:854/2330 train_time:52072ms step_avg:60.97ms
step:855/2330 train_time:52133ms step_avg:60.97ms
step:856/2330 train_time:52195ms step_avg:60.98ms
step:857/2330 train_time:52256ms step_avg:60.98ms
step:858/2330 train_time:52318ms step_avg:60.98ms
step:859/2330 train_time:52378ms step_avg:60.98ms
step:860/2330 train_time:52441ms step_avg:60.98ms
step:861/2330 train_time:52502ms step_avg:60.98ms
step:862/2330 train_time:52564ms step_avg:60.98ms
step:863/2330 train_time:52624ms step_avg:60.98ms
step:864/2330 train_time:52686ms step_avg:60.98ms
step:865/2330 train_time:52747ms step_avg:60.98ms
step:866/2330 train_time:52809ms step_avg:60.98ms
step:867/2330 train_time:52869ms step_avg:60.98ms
step:868/2330 train_time:52932ms step_avg:60.98ms
step:869/2330 train_time:52991ms step_avg:60.98ms
step:870/2330 train_time:53054ms step_avg:60.98ms
step:871/2330 train_time:53115ms step_avg:60.98ms
step:872/2330 train_time:53177ms step_avg:60.98ms
step:873/2330 train_time:53238ms step_avg:60.98ms
step:874/2330 train_time:53301ms step_avg:60.99ms
step:875/2330 train_time:53362ms step_avg:60.99ms
step:876/2330 train_time:53424ms step_avg:60.99ms
step:877/2330 train_time:53484ms step_avg:60.99ms
step:878/2330 train_time:53547ms step_avg:60.99ms
step:879/2330 train_time:53606ms step_avg:60.99ms
step:880/2330 train_time:53668ms step_avg:60.99ms
step:881/2330 train_time:53728ms step_avg:60.99ms
step:882/2330 train_time:53790ms step_avg:60.99ms
step:883/2330 train_time:53851ms step_avg:60.99ms
step:884/2330 train_time:53913ms step_avg:60.99ms
step:885/2330 train_time:53974ms step_avg:60.99ms
step:886/2330 train_time:54036ms step_avg:60.99ms
step:887/2330 train_time:54096ms step_avg:60.99ms
step:888/2330 train_time:54159ms step_avg:60.99ms
step:889/2330 train_time:54219ms step_avg:60.99ms
step:890/2330 train_time:54282ms step_avg:60.99ms
step:891/2330 train_time:54343ms step_avg:60.99ms
step:892/2330 train_time:54405ms step_avg:60.99ms
step:893/2330 train_time:54466ms step_avg:60.99ms
step:894/2330 train_time:54528ms step_avg:60.99ms
step:895/2330 train_time:54588ms step_avg:60.99ms
step:896/2330 train_time:54650ms step_avg:60.99ms
step:897/2330 train_time:54710ms step_avg:60.99ms
step:898/2330 train_time:54773ms step_avg:60.99ms
step:899/2330 train_time:54832ms step_avg:60.99ms
step:900/2330 train_time:54895ms step_avg:60.99ms
step:901/2330 train_time:54955ms step_avg:60.99ms
step:902/2330 train_time:55017ms step_avg:60.99ms
step:903/2330 train_time:55078ms step_avg:60.99ms
step:904/2330 train_time:55139ms step_avg:60.99ms
step:905/2330 train_time:55200ms step_avg:60.99ms
step:906/2330 train_time:55263ms step_avg:61.00ms
step:907/2330 train_time:55324ms step_avg:61.00ms
step:908/2330 train_time:55386ms step_avg:61.00ms
step:909/2330 train_time:55447ms step_avg:61.00ms
step:910/2330 train_time:55509ms step_avg:61.00ms
step:911/2330 train_time:55569ms step_avg:61.00ms
step:912/2330 train_time:55631ms step_avg:61.00ms
step:913/2330 train_time:55691ms step_avg:61.00ms
step:914/2330 train_time:55753ms step_avg:61.00ms
step:915/2330 train_time:55814ms step_avg:61.00ms
step:916/2330 train_time:55877ms step_avg:61.00ms
step:917/2330 train_time:55937ms step_avg:61.00ms
step:918/2330 train_time:55999ms step_avg:61.00ms
step:919/2330 train_time:56060ms step_avg:61.00ms
step:920/2330 train_time:56123ms step_avg:61.00ms
step:921/2330 train_time:56183ms step_avg:61.00ms
step:922/2330 train_time:56246ms step_avg:61.00ms
step:923/2330 train_time:56306ms step_avg:61.00ms
step:924/2330 train_time:56368ms step_avg:61.00ms
step:925/2330 train_time:56428ms step_avg:61.00ms
step:926/2330 train_time:56491ms step_avg:61.00ms
step:927/2330 train_time:56551ms step_avg:61.00ms
step:928/2330 train_time:56613ms step_avg:61.01ms
step:929/2330 train_time:56673ms step_avg:61.00ms
step:930/2330 train_time:56736ms step_avg:61.01ms
step:931/2330 train_time:56796ms step_avg:61.01ms
step:932/2330 train_time:56858ms step_avg:61.01ms
step:933/2330 train_time:56919ms step_avg:61.01ms
step:934/2330 train_time:56981ms step_avg:61.01ms
step:935/2330 train_time:57041ms step_avg:61.01ms
step:936/2330 train_time:57103ms step_avg:61.01ms
step:937/2330 train_time:57164ms step_avg:61.01ms
step:938/2330 train_time:57226ms step_avg:61.01ms
step:939/2330 train_time:57287ms step_avg:61.01ms
step:940/2330 train_time:57349ms step_avg:61.01ms
step:941/2330 train_time:57409ms step_avg:61.01ms
step:942/2330 train_time:57472ms step_avg:61.01ms
step:943/2330 train_time:57532ms step_avg:61.01ms
step:944/2330 train_time:57594ms step_avg:61.01ms
step:945/2330 train_time:57655ms step_avg:61.01ms
step:946/2330 train_time:57717ms step_avg:61.01ms
step:947/2330 train_time:57778ms step_avg:61.01ms
step:948/2330 train_time:57841ms step_avg:61.01ms
step:949/2330 train_time:57902ms step_avg:61.01ms
step:950/2330 train_time:57964ms step_avg:61.01ms
step:951/2330 train_time:58024ms step_avg:61.01ms
step:952/2330 train_time:58086ms step_avg:61.02ms
step:953/2330 train_time:58147ms step_avg:61.01ms
step:954/2330 train_time:58209ms step_avg:61.02ms
step:955/2330 train_time:58269ms step_avg:61.01ms
step:956/2330 train_time:58331ms step_avg:61.02ms
step:957/2330 train_time:58391ms step_avg:61.01ms
step:958/2330 train_time:58454ms step_avg:61.02ms
step:959/2330 train_time:58515ms step_avg:61.02ms
step:960/2330 train_time:58578ms step_avg:61.02ms
step:961/2330 train_time:58639ms step_avg:61.02ms
step:962/2330 train_time:58701ms step_avg:61.02ms
step:963/2330 train_time:58761ms step_avg:61.02ms
step:964/2330 train_time:58824ms step_avg:61.02ms
step:965/2330 train_time:58884ms step_avg:61.02ms
step:966/2330 train_time:58946ms step_avg:61.02ms
step:967/2330 train_time:59006ms step_avg:61.02ms
step:968/2330 train_time:59068ms step_avg:61.02ms
step:969/2330 train_time:59128ms step_avg:61.02ms
step:970/2330 train_time:59190ms step_avg:61.02ms
step:971/2330 train_time:59251ms step_avg:61.02ms
step:972/2330 train_time:59313ms step_avg:61.02ms
step:973/2330 train_time:59373ms step_avg:61.02ms
step:974/2330 train_time:59435ms step_avg:61.02ms
step:975/2330 train_time:59496ms step_avg:61.02ms
step:976/2330 train_time:59559ms step_avg:61.02ms
step:977/2330 train_time:59620ms step_avg:61.02ms
step:978/2330 train_time:59683ms step_avg:61.03ms
step:979/2330 train_time:59743ms step_avg:61.02ms
step:980/2330 train_time:59805ms step_avg:61.03ms
step:981/2330 train_time:59866ms step_avg:61.03ms
step:982/2330 train_time:59927ms step_avg:61.03ms
step:983/2330 train_time:59988ms step_avg:61.03ms
step:984/2330 train_time:60050ms step_avg:61.03ms
step:985/2330 train_time:60110ms step_avg:61.03ms
step:986/2330 train_time:60173ms step_avg:61.03ms
step:987/2330 train_time:60233ms step_avg:61.03ms
step:988/2330 train_time:60295ms step_avg:61.03ms
step:989/2330 train_time:60355ms step_avg:61.03ms
step:990/2330 train_time:60418ms step_avg:61.03ms
step:991/2330 train_time:60478ms step_avg:61.03ms
step:992/2330 train_time:60541ms step_avg:61.03ms
step:993/2330 train_time:60602ms step_avg:61.03ms
step:994/2330 train_time:60665ms step_avg:61.03ms
step:995/2330 train_time:60726ms step_avg:61.03ms
step:996/2330 train_time:60788ms step_avg:61.03ms
step:997/2330 train_time:60848ms step_avg:61.03ms
step:998/2330 train_time:60910ms step_avg:61.03ms
step:999/2330 train_time:60971ms step_avg:61.03ms
step:1000/2330 train_time:61032ms step_avg:61.03ms
step:1000/2330 val_loss:3.6030 train_time:61097ms step_avg:61.10ms
step:1001/2330 train_time:61121ms step_avg:61.06ms
step:1002/2330 train_time:61157ms step_avg:61.04ms
step:1003/2330 train_time:61224ms step_avg:61.04ms
step:1004/2330 train_time:61290ms step_avg:61.05ms
step:1005/2330 train_time:61351ms step_avg:61.05ms
step:1006/2330 train_time:61412ms step_avg:61.05ms
step:1007/2330 train_time:61472ms step_avg:61.04ms
step:1008/2330 train_time:61533ms step_avg:61.05ms
step:1009/2330 train_time:61593ms step_avg:61.04ms
step:1010/2330 train_time:61655ms step_avg:61.04ms
step:1011/2330 train_time:61714ms step_avg:61.04ms
step:1012/2330 train_time:61775ms step_avg:61.04ms
step:1013/2330 train_time:61834ms step_avg:61.04ms
step:1014/2330 train_time:61896ms step_avg:61.04ms
step:1015/2330 train_time:61955ms step_avg:61.04ms
step:1016/2330 train_time:62019ms step_avg:61.04ms
step:1017/2330 train_time:62081ms step_avg:61.04ms
step:1018/2330 train_time:62144ms step_avg:61.05ms
step:1019/2330 train_time:62206ms step_avg:61.05ms
step:1020/2330 train_time:62270ms step_avg:61.05ms
step:1021/2330 train_time:62330ms step_avg:61.05ms
step:1022/2330 train_time:62393ms step_avg:61.05ms
step:1023/2330 train_time:62453ms step_avg:61.05ms
step:1024/2330 train_time:62515ms step_avg:61.05ms
step:1025/2330 train_time:62576ms step_avg:61.05ms
step:1026/2330 train_time:62638ms step_avg:61.05ms
step:1027/2330 train_time:62697ms step_avg:61.05ms
step:1028/2330 train_time:62759ms step_avg:61.05ms
step:1029/2330 train_time:62820ms step_avg:61.05ms
step:1030/2330 train_time:62882ms step_avg:61.05ms
step:1031/2330 train_time:62943ms step_avg:61.05ms
step:1032/2330 train_time:63005ms step_avg:61.05ms
step:1033/2330 train_time:63065ms step_avg:61.05ms
step:1034/2330 train_time:63129ms step_avg:61.05ms
step:1035/2330 train_time:63191ms step_avg:61.05ms
step:1036/2330 train_time:63253ms step_avg:61.06ms
step:1037/2330 train_time:63313ms step_avg:61.05ms
step:1038/2330 train_time:63376ms step_avg:61.06ms
step:1039/2330 train_time:63436ms step_avg:61.05ms
step:1040/2330 train_time:63498ms step_avg:61.06ms
step:1041/2330 train_time:63559ms step_avg:61.06ms
step:1042/2330 train_time:63621ms step_avg:61.06ms
step:1043/2330 train_time:63681ms step_avg:61.06ms
step:1044/2330 train_time:63744ms step_avg:61.06ms
step:1045/2330 train_time:63804ms step_avg:61.06ms
step:1046/2330 train_time:63867ms step_avg:61.06ms
step:1047/2330 train_time:63927ms step_avg:61.06ms
step:1048/2330 train_time:63989ms step_avg:61.06ms
step:1049/2330 train_time:64050ms step_avg:61.06ms
step:1050/2330 train_time:64113ms step_avg:61.06ms
step:1051/2330 train_time:64173ms step_avg:61.06ms
step:1052/2330 train_time:64236ms step_avg:61.06ms
step:1053/2330 train_time:64296ms step_avg:61.06ms
step:1054/2330 train_time:64358ms step_avg:61.06ms
step:1055/2330 train_time:64419ms step_avg:61.06ms
step:1056/2330 train_time:64481ms step_avg:61.06ms
step:1057/2330 train_time:64542ms step_avg:61.06ms
step:1058/2330 train_time:64605ms step_avg:61.06ms
step:1059/2330 train_time:64665ms step_avg:61.06ms
step:1060/2330 train_time:64728ms step_avg:61.06ms
step:1061/2330 train_time:64788ms step_avg:61.06ms
step:1062/2330 train_time:64850ms step_avg:61.06ms
step:1063/2330 train_time:64911ms step_avg:61.06ms
step:1064/2330 train_time:64973ms step_avg:61.07ms
step:1065/2330 train_time:65034ms step_avg:61.06ms
step:1066/2330 train_time:65096ms step_avg:61.07ms
step:1067/2330 train_time:65156ms step_avg:61.07ms
step:1068/2330 train_time:65219ms step_avg:61.07ms
step:1069/2330 train_time:65280ms step_avg:61.07ms
step:1070/2330 train_time:65342ms step_avg:61.07ms
step:1071/2330 train_time:65403ms step_avg:61.07ms
step:1072/2330 train_time:65467ms step_avg:61.07ms
step:1073/2330 train_time:65528ms step_avg:61.07ms
step:1074/2330 train_time:65590ms step_avg:61.07ms
step:1075/2330 train_time:65651ms step_avg:61.07ms
step:1076/2330 train_time:65713ms step_avg:61.07ms
step:1077/2330 train_time:65773ms step_avg:61.07ms
step:1078/2330 train_time:65835ms step_avg:61.07ms
step:1079/2330 train_time:65895ms step_avg:61.07ms
step:1080/2330 train_time:65957ms step_avg:61.07ms
step:1081/2330 train_time:66018ms step_avg:61.07ms
step:1082/2330 train_time:66080ms step_avg:61.07ms
step:1083/2330 train_time:66141ms step_avg:61.07ms
step:1084/2330 train_time:66204ms step_avg:61.07ms
step:1085/2330 train_time:66264ms step_avg:61.07ms
step:1086/2330 train_time:66327ms step_avg:61.07ms
step:1087/2330 train_time:66387ms step_avg:61.07ms
step:1088/2330 train_time:66449ms step_avg:61.07ms
step:1089/2330 train_time:66509ms step_avg:61.07ms
step:1090/2330 train_time:66572ms step_avg:61.08ms
step:1091/2330 train_time:66633ms step_avg:61.07ms
step:1092/2330 train_time:66695ms step_avg:61.08ms
step:1093/2330 train_time:66756ms step_avg:61.08ms
step:1094/2330 train_time:66818ms step_avg:61.08ms
step:1095/2330 train_time:66878ms step_avg:61.08ms
step:1096/2330 train_time:66941ms step_avg:61.08ms
step:1097/2330 train_time:67001ms step_avg:61.08ms
step:1098/2330 train_time:67063ms step_avg:61.08ms
step:1099/2330 train_time:67124ms step_avg:61.08ms
step:1100/2330 train_time:67187ms step_avg:61.08ms
step:1101/2330 train_time:67248ms step_avg:61.08ms
step:1102/2330 train_time:67310ms step_avg:61.08ms
step:1103/2330 train_time:67371ms step_avg:61.08ms
step:1104/2330 train_time:67434ms step_avg:61.08ms
step:1105/2330 train_time:67494ms step_avg:61.08ms
step:1106/2330 train_time:67557ms step_avg:61.08ms
step:1107/2330 train_time:67617ms step_avg:61.08ms
step:1108/2330 train_time:67679ms step_avg:61.08ms
step:1109/2330 train_time:67740ms step_avg:61.08ms
step:1110/2330 train_time:67802ms step_avg:61.08ms
step:1111/2330 train_time:67863ms step_avg:61.08ms
step:1112/2330 train_time:67926ms step_avg:61.08ms
step:1113/2330 train_time:67986ms step_avg:61.08ms
step:1114/2330 train_time:68049ms step_avg:61.09ms
step:1115/2330 train_time:68109ms step_avg:61.08ms
step:1116/2330 train_time:68172ms step_avg:61.09ms
step:1117/2330 train_time:68232ms step_avg:61.09ms
step:1118/2330 train_time:68294ms step_avg:61.09ms
step:1119/2330 train_time:68355ms step_avg:61.09ms
step:1120/2330 train_time:68417ms step_avg:61.09ms
step:1121/2330 train_time:68477ms step_avg:61.09ms
step:1122/2330 train_time:68540ms step_avg:61.09ms
step:1123/2330 train_time:68601ms step_avg:61.09ms
step:1124/2330 train_time:68663ms step_avg:61.09ms
step:1125/2330 train_time:68724ms step_avg:61.09ms
step:1126/2330 train_time:68786ms step_avg:61.09ms
step:1127/2330 train_time:68847ms step_avg:61.09ms
step:1128/2330 train_time:68910ms step_avg:61.09ms
step:1129/2330 train_time:68970ms step_avg:61.09ms
step:1130/2330 train_time:69033ms step_avg:61.09ms
step:1131/2330 train_time:69093ms step_avg:61.09ms
step:1132/2330 train_time:69155ms step_avg:61.09ms
step:1133/2330 train_time:69215ms step_avg:61.09ms
step:1134/2330 train_time:69278ms step_avg:61.09ms
step:1135/2330 train_time:69338ms step_avg:61.09ms
step:1136/2330 train_time:69400ms step_avg:61.09ms
step:1137/2330 train_time:69461ms step_avg:61.09ms
step:1138/2330 train_time:69523ms step_avg:61.09ms
step:1139/2330 train_time:69584ms step_avg:61.09ms
step:1140/2330 train_time:69648ms step_avg:61.09ms
step:1141/2330 train_time:69708ms step_avg:61.09ms
step:1142/2330 train_time:69770ms step_avg:61.09ms
step:1143/2330 train_time:69831ms step_avg:61.09ms
step:1144/2330 train_time:69893ms step_avg:61.10ms
step:1145/2330 train_time:69954ms step_avg:61.10ms
step:1146/2330 train_time:70016ms step_avg:61.10ms
step:1147/2330 train_time:70076ms step_avg:61.10ms
step:1148/2330 train_time:70138ms step_avg:61.10ms
step:1149/2330 train_time:70199ms step_avg:61.10ms
step:1150/2330 train_time:70262ms step_avg:61.10ms
step:1151/2330 train_time:70323ms step_avg:61.10ms
step:1152/2330 train_time:70386ms step_avg:61.10ms
step:1153/2330 train_time:70446ms step_avg:61.10ms
step:1154/2330 train_time:70509ms step_avg:61.10ms
step:1155/2330 train_time:70569ms step_avg:61.10ms
step:1156/2330 train_time:70632ms step_avg:61.10ms
step:1157/2330 train_time:70692ms step_avg:61.10ms
step:1158/2330 train_time:70754ms step_avg:61.10ms
step:1159/2330 train_time:70814ms step_avg:61.10ms
step:1160/2330 train_time:70876ms step_avg:61.10ms
step:1161/2330 train_time:70936ms step_avg:61.10ms
step:1162/2330 train_time:70998ms step_avg:61.10ms
step:1163/2330 train_time:71058ms step_avg:61.10ms
step:1164/2330 train_time:71120ms step_avg:61.10ms
step:1165/2330 train_time:71181ms step_avg:61.10ms
step:1166/2330 train_time:71244ms step_avg:61.10ms
step:1167/2330 train_time:71305ms step_avg:61.10ms
step:1168/2330 train_time:71368ms step_avg:61.10ms
step:1169/2330 train_time:71428ms step_avg:61.10ms
step:1170/2330 train_time:71490ms step_avg:61.10ms
step:1171/2330 train_time:71551ms step_avg:61.10ms
step:1172/2330 train_time:71614ms step_avg:61.10ms
step:1173/2330 train_time:71674ms step_avg:61.10ms
step:1174/2330 train_time:71736ms step_avg:61.10ms
step:1175/2330 train_time:71797ms step_avg:61.10ms
step:1176/2330 train_time:71859ms step_avg:61.10ms
step:1177/2330 train_time:71920ms step_avg:61.10ms
step:1178/2330 train_time:71982ms step_avg:61.11ms
step:1179/2330 train_time:72043ms step_avg:61.11ms
step:1180/2330 train_time:72106ms step_avg:61.11ms
step:1181/2330 train_time:72167ms step_avg:61.11ms
step:1182/2330 train_time:72229ms step_avg:61.11ms
step:1183/2330 train_time:72290ms step_avg:61.11ms
step:1184/2330 train_time:72352ms step_avg:61.11ms
step:1185/2330 train_time:72412ms step_avg:61.11ms
step:1186/2330 train_time:72474ms step_avg:61.11ms
step:1187/2330 train_time:72535ms step_avg:61.11ms
step:1188/2330 train_time:72597ms step_avg:61.11ms
step:1189/2330 train_time:72657ms step_avg:61.11ms
step:1190/2330 train_time:72720ms step_avg:61.11ms
step:1191/2330 train_time:72781ms step_avg:61.11ms
step:1192/2330 train_time:72844ms step_avg:61.11ms
step:1193/2330 train_time:72904ms step_avg:61.11ms
step:1194/2330 train_time:72966ms step_avg:61.11ms
step:1195/2330 train_time:73027ms step_avg:61.11ms
step:1196/2330 train_time:73090ms step_avg:61.11ms
step:1197/2330 train_time:73150ms step_avg:61.11ms
step:1198/2330 train_time:73213ms step_avg:61.11ms
step:1199/2330 train_time:73273ms step_avg:61.11ms
step:1200/2330 train_time:73335ms step_avg:61.11ms
step:1201/2330 train_time:73396ms step_avg:61.11ms
step:1202/2330 train_time:73459ms step_avg:61.11ms
step:1203/2330 train_time:73519ms step_avg:61.11ms
step:1204/2330 train_time:73582ms step_avg:61.11ms
step:1205/2330 train_time:73643ms step_avg:61.11ms
step:1206/2330 train_time:73706ms step_avg:61.12ms
step:1207/2330 train_time:73767ms step_avg:61.12ms
step:1208/2330 train_time:73829ms step_avg:61.12ms
step:1209/2330 train_time:73890ms step_avg:61.12ms
step:1210/2330 train_time:73952ms step_avg:61.12ms
step:1211/2330 train_time:74013ms step_avg:61.12ms
step:1212/2330 train_time:74075ms step_avg:61.12ms
step:1213/2330 train_time:74135ms step_avg:61.12ms
step:1214/2330 train_time:74197ms step_avg:61.12ms
step:1215/2330 train_time:74258ms step_avg:61.12ms
step:1216/2330 train_time:74321ms step_avg:61.12ms
step:1217/2330 train_time:74382ms step_avg:61.12ms
step:1218/2330 train_time:74445ms step_avg:61.12ms
step:1219/2330 train_time:74506ms step_avg:61.12ms
step:1220/2330 train_time:74568ms step_avg:61.12ms
step:1221/2330 train_time:74628ms step_avg:61.12ms
step:1222/2330 train_time:74691ms step_avg:61.12ms
step:1223/2330 train_time:74751ms step_avg:61.12ms
step:1224/2330 train_time:74814ms step_avg:61.12ms
step:1225/2330 train_time:74874ms step_avg:61.12ms
step:1226/2330 train_time:74936ms step_avg:61.12ms
step:1227/2330 train_time:74996ms step_avg:61.12ms
step:1228/2330 train_time:75059ms step_avg:61.12ms
step:1229/2330 train_time:75120ms step_avg:61.12ms
step:1230/2330 train_time:75182ms step_avg:61.12ms
step:1231/2330 train_time:75243ms step_avg:61.12ms
step:1232/2330 train_time:75306ms step_avg:61.12ms
step:1233/2330 train_time:75367ms step_avg:61.12ms
step:1234/2330 train_time:75429ms step_avg:61.13ms
step:1235/2330 train_time:75490ms step_avg:61.13ms
step:1236/2330 train_time:75553ms step_avg:61.13ms
step:1237/2330 train_time:75613ms step_avg:61.13ms
step:1238/2330 train_time:75675ms step_avg:61.13ms
step:1239/2330 train_time:75735ms step_avg:61.13ms
step:1240/2330 train_time:75797ms step_avg:61.13ms
step:1241/2330 train_time:75857ms step_avg:61.13ms
step:1242/2330 train_time:75919ms step_avg:61.13ms
step:1243/2330 train_time:75980ms step_avg:61.13ms
step:1244/2330 train_time:76043ms step_avg:61.13ms
step:1245/2330 train_time:76104ms step_avg:61.13ms
step:1246/2330 train_time:76166ms step_avg:61.13ms
step:1247/2330 train_time:76226ms step_avg:61.13ms
step:1248/2330 train_time:76289ms step_avg:61.13ms
step:1249/2330 train_time:76350ms step_avg:61.13ms
step:1250/2330 train_time:76412ms step_avg:61.13ms
step:1250/2330 val_loss:3.5476 train_time:76477ms step_avg:61.18ms
step:1251/2330 train_time:76500ms step_avg:61.15ms
step:1252/2330 train_time:76538ms step_avg:61.13ms
step:1253/2330 train_time:76602ms step_avg:61.14ms
step:1254/2330 train_time:76667ms step_avg:61.14ms
step:1255/2330 train_time:76728ms step_avg:61.14ms
step:1256/2330 train_time:76791ms step_avg:61.14ms
step:1257/2330 train_time:76851ms step_avg:61.14ms
step:1258/2330 train_time:76913ms step_avg:61.14ms
step:1259/2330 train_time:76973ms step_avg:61.14ms
step:1260/2330 train_time:77035ms step_avg:61.14ms
step:1261/2330 train_time:77094ms step_avg:61.14ms
step:1262/2330 train_time:77156ms step_avg:61.14ms
step:1263/2330 train_time:77216ms step_avg:61.14ms
step:1264/2330 train_time:77280ms step_avg:61.14ms
step:1265/2330 train_time:77339ms step_avg:61.14ms
step:1266/2330 train_time:77402ms step_avg:61.14ms
step:1267/2330 train_time:77462ms step_avg:61.14ms
step:1268/2330 train_time:77526ms step_avg:61.14ms
step:1269/2330 train_time:77589ms step_avg:61.14ms
step:1270/2330 train_time:77652ms step_avg:61.14ms
step:1271/2330 train_time:77713ms step_avg:61.14ms
step:1272/2330 train_time:77776ms step_avg:61.14ms
step:1273/2330 train_time:77836ms step_avg:61.14ms
step:1274/2330 train_time:77899ms step_avg:61.14ms
step:1275/2330 train_time:77959ms step_avg:61.14ms
step:1276/2330 train_time:78021ms step_avg:61.15ms
step:1277/2330 train_time:78081ms step_avg:61.14ms
step:1278/2330 train_time:78143ms step_avg:61.14ms
step:1279/2330 train_time:78202ms step_avg:61.14ms
step:1280/2330 train_time:78264ms step_avg:61.14ms
step:1281/2330 train_time:78325ms step_avg:61.14ms
step:1282/2330 train_time:78387ms step_avg:61.14ms
step:1283/2330 train_time:78447ms step_avg:61.14ms
step:1284/2330 train_time:78511ms step_avg:61.15ms
step:1285/2330 train_time:78572ms step_avg:61.15ms
step:1286/2330 train_time:78635ms step_avg:61.15ms
step:1287/2330 train_time:78696ms step_avg:61.15ms
step:1288/2330 train_time:78758ms step_avg:61.15ms
step:1289/2330 train_time:78819ms step_avg:61.15ms
step:1290/2330 train_time:78881ms step_avg:61.15ms
step:1291/2330 train_time:78942ms step_avg:61.15ms
step:1292/2330 train_time:79004ms step_avg:61.15ms
step:1293/2330 train_time:79064ms step_avg:61.15ms
step:1294/2330 train_time:79125ms step_avg:61.15ms
step:1295/2330 train_time:79186ms step_avg:61.15ms
step:1296/2330 train_time:79248ms step_avg:61.15ms
step:1297/2330 train_time:79308ms step_avg:61.15ms
step:1298/2330 train_time:79371ms step_avg:61.15ms
step:1299/2330 train_time:79432ms step_avg:61.15ms
step:1300/2330 train_time:79495ms step_avg:61.15ms
step:1301/2330 train_time:79556ms step_avg:61.15ms
step:1302/2330 train_time:79619ms step_avg:61.15ms
step:1303/2330 train_time:79679ms step_avg:61.15ms
step:1304/2330 train_time:79742ms step_avg:61.15ms
step:1305/2330 train_time:79803ms step_avg:61.15ms
step:1306/2330 train_time:79865ms step_avg:61.15ms
step:1307/2330 train_time:79925ms step_avg:61.15ms
step:1308/2330 train_time:79988ms step_avg:61.15ms
step:1309/2330 train_time:80048ms step_avg:61.15ms
step:1310/2330 train_time:80111ms step_avg:61.15ms
step:1311/2330 train_time:80171ms step_avg:61.15ms
step:1312/2330 train_time:80234ms step_avg:61.15ms
step:1313/2330 train_time:80294ms step_avg:61.15ms
step:1314/2330 train_time:80356ms step_avg:61.15ms
step:1315/2330 train_time:80416ms step_avg:61.15ms
step:1316/2330 train_time:80479ms step_avg:61.15ms
step:1317/2330 train_time:80539ms step_avg:61.15ms
step:1318/2330 train_time:80601ms step_avg:61.15ms
step:1319/2330 train_time:80661ms step_avg:61.15ms
step:1320/2330 train_time:80723ms step_avg:61.15ms
step:1321/2330 train_time:80784ms step_avg:61.15ms
step:1322/2330 train_time:80846ms step_avg:61.15ms
step:1323/2330 train_time:80906ms step_avg:61.15ms
step:1324/2330 train_time:80970ms step_avg:61.16ms
step:1325/2330 train_time:81030ms step_avg:61.16ms
step:1326/2330 train_time:81093ms step_avg:61.16ms
step:1327/2330 train_time:81153ms step_avg:61.16ms
step:1328/2330 train_time:81215ms step_avg:61.16ms
step:1329/2330 train_time:81275ms step_avg:61.16ms
step:1330/2330 train_time:81337ms step_avg:61.16ms
step:1331/2330 train_time:81398ms step_avg:61.16ms
step:1332/2330 train_time:81460ms step_avg:61.16ms
step:1333/2330 train_time:81520ms step_avg:61.16ms
step:1334/2330 train_time:81583ms step_avg:61.16ms
step:1335/2330 train_time:81643ms step_avg:61.16ms
step:1336/2330 train_time:81705ms step_avg:61.16ms
step:1337/2330 train_time:81766ms step_avg:61.16ms
step:1338/2330 train_time:81829ms step_avg:61.16ms
step:1339/2330 train_time:81890ms step_avg:61.16ms
step:1340/2330 train_time:81952ms step_avg:61.16ms
step:1341/2330 train_time:82013ms step_avg:61.16ms
step:1342/2330 train_time:82076ms step_avg:61.16ms
step:1343/2330 train_time:82136ms step_avg:61.16ms
step:1344/2330 train_time:82198ms step_avg:61.16ms
step:1345/2330 train_time:82259ms step_avg:61.16ms
step:1346/2330 train_time:82321ms step_avg:61.16ms
step:1347/2330 train_time:82381ms step_avg:61.16ms
step:1348/2330 train_time:82443ms step_avg:61.16ms
step:1349/2330 train_time:82503ms step_avg:61.16ms
step:1350/2330 train_time:82565ms step_avg:61.16ms
step:1351/2330 train_time:82625ms step_avg:61.16ms
step:1352/2330 train_time:82688ms step_avg:61.16ms
step:1353/2330 train_time:82748ms step_avg:61.16ms
step:1354/2330 train_time:82810ms step_avg:61.16ms
step:1355/2330 train_time:82871ms step_avg:61.16ms
step:1356/2330 train_time:82933ms step_avg:61.16ms
step:1357/2330 train_time:82994ms step_avg:61.16ms
step:1358/2330 train_time:83056ms step_avg:61.16ms
step:1359/2330 train_time:83116ms step_avg:61.16ms
step:1360/2330 train_time:83179ms step_avg:61.16ms
step:1361/2330 train_time:83239ms step_avg:61.16ms
step:1362/2330 train_time:83301ms step_avg:61.16ms
step:1363/2330 train_time:83362ms step_avg:61.16ms
step:1364/2330 train_time:83424ms step_avg:61.16ms
step:1365/2330 train_time:83485ms step_avg:61.16ms
step:1366/2330 train_time:83548ms step_avg:61.16ms
step:1367/2330 train_time:83608ms step_avg:61.16ms
step:1368/2330 train_time:83670ms step_avg:61.16ms
step:1369/2330 train_time:83731ms step_avg:61.16ms
step:1370/2330 train_time:83793ms step_avg:61.16ms
step:1371/2330 train_time:83854ms step_avg:61.16ms
step:1372/2330 train_time:83916ms step_avg:61.16ms
step:1373/2330 train_time:83976ms step_avg:61.16ms
step:1374/2330 train_time:84038ms step_avg:61.16ms
step:1375/2330 train_time:84099ms step_avg:61.16ms
step:1376/2330 train_time:84161ms step_avg:61.16ms
step:1377/2330 train_time:84221ms step_avg:61.16ms
step:1378/2330 train_time:84283ms step_avg:61.16ms
step:1379/2330 train_time:84343ms step_avg:61.16ms
step:1380/2330 train_time:84406ms step_avg:61.16ms
step:1381/2330 train_time:84466ms step_avg:61.16ms
step:1382/2330 train_time:84528ms step_avg:61.16ms
step:1383/2330 train_time:84588ms step_avg:61.16ms
step:1384/2330 train_time:84651ms step_avg:61.16ms
step:1385/2330 train_time:84711ms step_avg:61.16ms
step:1386/2330 train_time:84774ms step_avg:61.16ms
step:1387/2330 train_time:84834ms step_avg:61.16ms
step:1388/2330 train_time:84897ms step_avg:61.16ms
step:1389/2330 train_time:84957ms step_avg:61.16ms
step:1390/2330 train_time:85019ms step_avg:61.17ms
step:1391/2330 train_time:85079ms step_avg:61.16ms
step:1392/2330 train_time:85141ms step_avg:61.16ms
step:1393/2330 train_time:85201ms step_avg:61.16ms
step:1394/2330 train_time:85263ms step_avg:61.16ms
step:1395/2330 train_time:85324ms step_avg:61.16ms
step:1396/2330 train_time:85386ms step_avg:61.16ms
step:1397/2330 train_time:85446ms step_avg:61.16ms
step:1398/2330 train_time:85509ms step_avg:61.17ms
step:1399/2330 train_time:85570ms step_avg:61.17ms
step:1400/2330 train_time:85632ms step_avg:61.17ms
step:1401/2330 train_time:85693ms step_avg:61.17ms
step:1402/2330 train_time:85755ms step_avg:61.17ms
step:1403/2330 train_time:85816ms step_avg:61.17ms
step:1404/2330 train_time:85878ms step_avg:61.17ms
step:1405/2330 train_time:85939ms step_avg:61.17ms
step:1406/2330 train_time:86001ms step_avg:61.17ms
step:1407/2330 train_time:86062ms step_avg:61.17ms
step:1408/2330 train_time:86123ms step_avg:61.17ms
step:1409/2330 train_time:86183ms step_avg:61.17ms
step:1410/2330 train_time:86245ms step_avg:61.17ms
step:1411/2330 train_time:86306ms step_avg:61.17ms
step:1412/2330 train_time:86368ms step_avg:61.17ms
step:1413/2330 train_time:86429ms step_avg:61.17ms
step:1414/2330 train_time:86491ms step_avg:61.17ms
step:1415/2330 train_time:86552ms step_avg:61.17ms
step:1416/2330 train_time:86614ms step_avg:61.17ms
step:1417/2330 train_time:86675ms step_avg:61.17ms
step:1418/2330 train_time:86737ms step_avg:61.17ms
step:1419/2330 train_time:86798ms step_avg:61.17ms
step:1420/2330 train_time:86860ms step_avg:61.17ms
step:1421/2330 train_time:86920ms step_avg:61.17ms
step:1422/2330 train_time:86983ms step_avg:61.17ms
step:1423/2330 train_time:87043ms step_avg:61.17ms
step:1424/2330 train_time:87106ms step_avg:61.17ms
step:1425/2330 train_time:87166ms step_avg:61.17ms
step:1426/2330 train_time:87229ms step_avg:61.17ms
step:1427/2330 train_time:87289ms step_avg:61.17ms
step:1428/2330 train_time:87352ms step_avg:61.17ms
step:1429/2330 train_time:87412ms step_avg:61.17ms
step:1430/2330 train_time:87475ms step_avg:61.17ms
step:1431/2330 train_time:87535ms step_avg:61.17ms
step:1432/2330 train_time:87597ms step_avg:61.17ms
step:1433/2330 train_time:87658ms step_avg:61.17ms
step:1434/2330 train_time:87720ms step_avg:61.17ms
step:1435/2330 train_time:87781ms step_avg:61.17ms
step:1436/2330 train_time:87843ms step_avg:61.17ms
step:1437/2330 train_time:87903ms step_avg:61.17ms
step:1438/2330 train_time:87965ms step_avg:61.17ms
step:1439/2330 train_time:88026ms step_avg:61.17ms
step:1440/2330 train_time:88088ms step_avg:61.17ms
step:1441/2330 train_time:88149ms step_avg:61.17ms
step:1442/2330 train_time:88211ms step_avg:61.17ms
step:1443/2330 train_time:88272ms step_avg:61.17ms
step:1444/2330 train_time:88334ms step_avg:61.17ms
step:1445/2330 train_time:88395ms step_avg:61.17ms
step:1446/2330 train_time:88457ms step_avg:61.17ms
step:1447/2330 train_time:88518ms step_avg:61.17ms
step:1448/2330 train_time:88581ms step_avg:61.17ms
step:1449/2330 train_time:88641ms step_avg:61.17ms
step:1450/2330 train_time:88703ms step_avg:61.17ms
step:1451/2330 train_time:88763ms step_avg:61.17ms
step:1452/2330 train_time:88825ms step_avg:61.17ms
step:1453/2330 train_time:88886ms step_avg:61.17ms
step:1454/2330 train_time:88948ms step_avg:61.17ms
step:1455/2330 train_time:89008ms step_avg:61.17ms
step:1456/2330 train_time:89070ms step_avg:61.17ms
step:1457/2330 train_time:89132ms step_avg:61.17ms
step:1458/2330 train_time:89195ms step_avg:61.18ms
step:1459/2330 train_time:89255ms step_avg:61.18ms
step:1460/2330 train_time:89318ms step_avg:61.18ms
step:1461/2330 train_time:89378ms step_avg:61.18ms
step:1462/2330 train_time:89440ms step_avg:61.18ms
step:1463/2330 train_time:89500ms step_avg:61.18ms
step:1464/2330 train_time:89562ms step_avg:61.18ms
step:1465/2330 train_time:89622ms step_avg:61.18ms
step:1466/2330 train_time:89685ms step_avg:61.18ms
step:1467/2330 train_time:89745ms step_avg:61.18ms
step:1468/2330 train_time:89808ms step_avg:61.18ms
step:1469/2330 train_time:89869ms step_avg:61.18ms
step:1470/2330 train_time:89932ms step_avg:61.18ms
step:1471/2330 train_time:89993ms step_avg:61.18ms
step:1472/2330 train_time:90055ms step_avg:61.18ms
step:1473/2330 train_time:90115ms step_avg:61.18ms
step:1474/2330 train_time:90178ms step_avg:61.18ms
step:1475/2330 train_time:90238ms step_avg:61.18ms
step:1476/2330 train_time:90300ms step_avg:61.18ms
step:1477/2330 train_time:90360ms step_avg:61.18ms
step:1478/2330 train_time:90422ms step_avg:61.18ms
step:1479/2330 train_time:90483ms step_avg:61.18ms
step:1480/2330 train_time:90546ms step_avg:61.18ms
step:1481/2330 train_time:90606ms step_avg:61.18ms
step:1482/2330 train_time:90669ms step_avg:61.18ms
step:1483/2330 train_time:90729ms step_avg:61.18ms
step:1484/2330 train_time:90792ms step_avg:61.18ms
step:1485/2330 train_time:90853ms step_avg:61.18ms
step:1486/2330 train_time:90915ms step_avg:61.18ms
step:1487/2330 train_time:90975ms step_avg:61.18ms
step:1488/2330 train_time:91037ms step_avg:61.18ms
step:1489/2330 train_time:91098ms step_avg:61.18ms
step:1490/2330 train_time:91160ms step_avg:61.18ms
step:1491/2330 train_time:91220ms step_avg:61.18ms
step:1492/2330 train_time:91283ms step_avg:61.18ms
step:1493/2330 train_time:91343ms step_avg:61.18ms
step:1494/2330 train_time:91405ms step_avg:61.18ms
step:1495/2330 train_time:91466ms step_avg:61.18ms
step:1496/2330 train_time:91529ms step_avg:61.18ms
step:1497/2330 train_time:91589ms step_avg:61.18ms
step:1498/2330 train_time:91652ms step_avg:61.18ms
step:1499/2330 train_time:91713ms step_avg:61.18ms
step:1500/2330 train_time:91776ms step_avg:61.18ms
step:1500/2330 val_loss:3.4785 train_time:91840ms step_avg:61.23ms
step:1501/2330 train_time:91863ms step_avg:61.20ms
step:1502/2330 train_time:91902ms step_avg:61.19ms
step:1503/2330 train_time:91968ms step_avg:61.19ms
step:1504/2330 train_time:92032ms step_avg:61.19ms
step:1505/2330 train_time:92092ms step_avg:61.19ms
step:1506/2330 train_time:92153ms step_avg:61.19ms
step:1507/2330 train_time:92213ms step_avg:61.19ms
step:1508/2330 train_time:92274ms step_avg:61.19ms
step:1509/2330 train_time:92334ms step_avg:61.19ms
step:1510/2330 train_time:92395ms step_avg:61.19ms
step:1511/2330 train_time:92455ms step_avg:61.19ms
step:1512/2330 train_time:92516ms step_avg:61.19ms
step:1513/2330 train_time:92577ms step_avg:61.19ms
step:1514/2330 train_time:92639ms step_avg:61.19ms
step:1515/2330 train_time:92699ms step_avg:61.19ms
step:1516/2330 train_time:92762ms step_avg:61.19ms
step:1517/2330 train_time:92824ms step_avg:61.19ms
step:1518/2330 train_time:92888ms step_avg:61.19ms
step:1519/2330 train_time:92950ms step_avg:61.19ms
step:1520/2330 train_time:93013ms step_avg:61.19ms
step:1521/2330 train_time:93073ms step_avg:61.19ms
step:1522/2330 train_time:93136ms step_avg:61.19ms
step:1523/2330 train_time:93196ms step_avg:61.19ms
step:1524/2330 train_time:93258ms step_avg:61.19ms
step:1525/2330 train_time:93318ms step_avg:61.19ms
step:1526/2330 train_time:93380ms step_avg:61.19ms
step:1527/2330 train_time:93439ms step_avg:61.19ms
step:1528/2330 train_time:93502ms step_avg:61.19ms
step:1529/2330 train_time:93561ms step_avg:61.19ms
step:1530/2330 train_time:93624ms step_avg:61.19ms
step:1531/2330 train_time:93685ms step_avg:61.19ms
step:1532/2330 train_time:93748ms step_avg:61.19ms
step:1533/2330 train_time:93810ms step_avg:61.19ms
step:1534/2330 train_time:93873ms step_avg:61.19ms
step:1535/2330 train_time:93934ms step_avg:61.20ms
step:1536/2330 train_time:93998ms step_avg:61.20ms
step:1537/2330 train_time:94060ms step_avg:61.20ms
step:1538/2330 train_time:94123ms step_avg:61.20ms
step:1539/2330 train_time:94184ms step_avg:61.20ms
step:1540/2330 train_time:94246ms step_avg:61.20ms
step:1541/2330 train_time:94307ms step_avg:61.20ms
step:1542/2330 train_time:94369ms step_avg:61.20ms
step:1543/2330 train_time:94429ms step_avg:61.20ms
step:1544/2330 train_time:94492ms step_avg:61.20ms
step:1545/2330 train_time:94553ms step_avg:61.20ms
step:1546/2330 train_time:94616ms step_avg:61.20ms
step:1547/2330 train_time:94675ms step_avg:61.20ms
step:1548/2330 train_time:94739ms step_avg:61.20ms
step:1549/2330 train_time:94801ms step_avg:61.20ms
step:1550/2330 train_time:94864ms step_avg:61.20ms
step:1551/2330 train_time:94926ms step_avg:61.20ms
step:1552/2330 train_time:94989ms step_avg:61.20ms
step:1553/2330 train_time:95049ms step_avg:61.20ms
step:1554/2330 train_time:95113ms step_avg:61.21ms
step:1555/2330 train_time:95173ms step_avg:61.20ms
step:1556/2330 train_time:95237ms step_avg:61.21ms
step:1557/2330 train_time:95298ms step_avg:61.21ms
step:1558/2330 train_time:95361ms step_avg:61.21ms
step:1559/2330 train_time:95422ms step_avg:61.21ms
step:1560/2330 train_time:95485ms step_avg:61.21ms
step:1561/2330 train_time:95546ms step_avg:61.21ms
step:1562/2330 train_time:95608ms step_avg:61.21ms
step:1563/2330 train_time:95668ms step_avg:61.21ms
step:1564/2330 train_time:95730ms step_avg:61.21ms
step:1565/2330 train_time:95792ms step_avg:61.21ms
step:1566/2330 train_time:95855ms step_avg:61.21ms
step:1567/2330 train_time:95916ms step_avg:61.21ms
step:1568/2330 train_time:95979ms step_avg:61.21ms
step:1569/2330 train_time:96041ms step_avg:61.21ms
step:1570/2330 train_time:96104ms step_avg:61.21ms
step:1571/2330 train_time:96165ms step_avg:61.21ms
step:1572/2330 train_time:96227ms step_avg:61.21ms
step:1573/2330 train_time:96288ms step_avg:61.21ms
step:1574/2330 train_time:96351ms step_avg:61.21ms
step:1575/2330 train_time:96413ms step_avg:61.21ms
step:1576/2330 train_time:96475ms step_avg:61.22ms
step:1577/2330 train_time:96535ms step_avg:61.21ms
step:1578/2330 train_time:96599ms step_avg:61.22ms
step:1579/2330 train_time:96659ms step_avg:61.22ms
step:1580/2330 train_time:96722ms step_avg:61.22ms
step:1581/2330 train_time:96784ms step_avg:61.22ms
step:1582/2330 train_time:96847ms step_avg:61.22ms
step:1583/2330 train_time:96907ms step_avg:61.22ms
step:1584/2330 train_time:96970ms step_avg:61.22ms
step:1585/2330 train_time:97031ms step_avg:61.22ms
step:1586/2330 train_time:97095ms step_avg:61.22ms
step:1587/2330 train_time:97156ms step_avg:61.22ms
step:1588/2330 train_time:97219ms step_avg:61.22ms
step:1589/2330 train_time:97280ms step_avg:61.22ms
step:1590/2330 train_time:97343ms step_avg:61.22ms
step:1591/2330 train_time:97404ms step_avg:61.22ms
step:1592/2330 train_time:97467ms step_avg:61.22ms
step:1593/2330 train_time:97528ms step_avg:61.22ms
step:1594/2330 train_time:97590ms step_avg:61.22ms
step:1595/2330 train_time:97651ms step_avg:61.22ms
step:1596/2330 train_time:97714ms step_avg:61.22ms
step:1597/2330 train_time:97775ms step_avg:61.22ms
step:1598/2330 train_time:97839ms step_avg:61.23ms
step:1599/2330 train_time:97900ms step_avg:61.23ms
step:1600/2330 train_time:97963ms step_avg:61.23ms
step:1601/2330 train_time:98024ms step_avg:61.23ms
step:1602/2330 train_time:98087ms step_avg:61.23ms
step:1603/2330 train_time:98147ms step_avg:61.23ms
step:1604/2330 train_time:98211ms step_avg:61.23ms
step:1605/2330 train_time:98271ms step_avg:61.23ms
step:1606/2330 train_time:98334ms step_avg:61.23ms
step:1607/2330 train_time:98396ms step_avg:61.23ms
step:1608/2330 train_time:98459ms step_avg:61.23ms
step:1609/2330 train_time:98520ms step_avg:61.23ms
step:1610/2330 train_time:98583ms step_avg:61.23ms
step:1611/2330 train_time:98644ms step_avg:61.23ms
step:1612/2330 train_time:98707ms step_avg:61.23ms
step:1613/2330 train_time:98768ms step_avg:61.23ms
step:1614/2330 train_time:98831ms step_avg:61.23ms
step:1615/2330 train_time:98892ms step_avg:61.23ms
step:1616/2330 train_time:98955ms step_avg:61.23ms
step:1617/2330 train_time:99016ms step_avg:61.23ms
step:1618/2330 train_time:99079ms step_avg:61.24ms
step:1619/2330 train_time:99140ms step_avg:61.24ms
step:1620/2330 train_time:99203ms step_avg:61.24ms
step:1621/2330 train_time:99263ms step_avg:61.24ms
step:1622/2330 train_time:99326ms step_avg:61.24ms
step:1623/2330 train_time:99387ms step_avg:61.24ms
step:1624/2330 train_time:99449ms step_avg:61.24ms
step:1625/2330 train_time:99511ms step_avg:61.24ms
step:1626/2330 train_time:99573ms step_avg:61.24ms
step:1627/2330 train_time:99634ms step_avg:61.24ms
step:1628/2330 train_time:99697ms step_avg:61.24ms
step:1629/2330 train_time:99758ms step_avg:61.24ms
step:1630/2330 train_time:99821ms step_avg:61.24ms
step:1631/2330 train_time:99881ms step_avg:61.24ms
step:1632/2330 train_time:99945ms step_avg:61.24ms
step:1633/2330 train_time:100006ms step_avg:61.24ms
step:1634/2330 train_time:100068ms step_avg:61.24ms
step:1635/2330 train_time:100129ms step_avg:61.24ms
step:1636/2330 train_time:100192ms step_avg:61.24ms
step:1637/2330 train_time:100252ms step_avg:61.24ms
step:1638/2330 train_time:100315ms step_avg:61.24ms
step:1639/2330 train_time:100377ms step_avg:61.24ms
step:1640/2330 train_time:100440ms step_avg:61.24ms
step:1641/2330 train_time:100501ms step_avg:61.24ms
step:1642/2330 train_time:100564ms step_avg:61.24ms
step:1643/2330 train_time:100625ms step_avg:61.24ms
step:1644/2330 train_time:100688ms step_avg:61.25ms
step:1645/2330 train_time:100749ms step_avg:61.25ms
step:1646/2330 train_time:100812ms step_avg:61.25ms
step:1647/2330 train_time:100872ms step_avg:61.25ms
step:1648/2330 train_time:100935ms step_avg:61.25ms
step:1649/2330 train_time:100996ms step_avg:61.25ms
step:1650/2330 train_time:101059ms step_avg:61.25ms
step:1651/2330 train_time:101119ms step_avg:61.25ms
step:1652/2330 train_time:101183ms step_avg:61.25ms
step:1653/2330 train_time:101244ms step_avg:61.25ms
step:1654/2330 train_time:101307ms step_avg:61.25ms
step:1655/2330 train_time:101368ms step_avg:61.25ms
step:1656/2330 train_time:101430ms step_avg:61.25ms
step:1657/2330 train_time:101491ms step_avg:61.25ms
step:1658/2330 train_time:101554ms step_avg:61.25ms
step:1659/2330 train_time:101615ms step_avg:61.25ms
step:1660/2330 train_time:101678ms step_avg:61.25ms
step:1661/2330 train_time:101739ms step_avg:61.25ms
step:1662/2330 train_time:101802ms step_avg:61.25ms
step:1663/2330 train_time:101864ms step_avg:61.25ms
step:1664/2330 train_time:101926ms step_avg:61.25ms
step:1665/2330 train_time:101987ms step_avg:61.25ms
step:1666/2330 train_time:102049ms step_avg:61.25ms
step:1667/2330 train_time:102110ms step_avg:61.25ms
step:1668/2330 train_time:102173ms step_avg:61.25ms
step:1669/2330 train_time:102233ms step_avg:61.25ms
step:1670/2330 train_time:102295ms step_avg:61.25ms
step:1671/2330 train_time:102357ms step_avg:61.25ms
step:1672/2330 train_time:102420ms step_avg:61.26ms
step:1673/2330 train_time:102480ms step_avg:61.26ms
step:1674/2330 train_time:102543ms step_avg:61.26ms
step:1675/2330 train_time:102604ms step_avg:61.26ms
step:1676/2330 train_time:102667ms step_avg:61.26ms
step:1677/2330 train_time:102728ms step_avg:61.26ms
step:1678/2330 train_time:102790ms step_avg:61.26ms
step:1679/2330 train_time:102851ms step_avg:61.26ms
step:1680/2330 train_time:102914ms step_avg:61.26ms
step:1681/2330 train_time:102975ms step_avg:61.26ms
step:1682/2330 train_time:103038ms step_avg:61.26ms
step:1683/2330 train_time:103099ms step_avg:61.26ms
step:1684/2330 train_time:103161ms step_avg:61.26ms
step:1685/2330 train_time:103223ms step_avg:61.26ms
step:1686/2330 train_time:103287ms step_avg:61.26ms
step:1687/2330 train_time:103348ms step_avg:61.26ms
step:1688/2330 train_time:103410ms step_avg:61.26ms
step:1689/2330 train_time:103471ms step_avg:61.26ms
step:1690/2330 train_time:103533ms step_avg:61.26ms
step:1691/2330 train_time:103594ms step_avg:61.26ms
step:1692/2330 train_time:103658ms step_avg:61.26ms
step:1693/2330 train_time:103719ms step_avg:61.26ms
step:1694/2330 train_time:103782ms step_avg:61.26ms
step:1695/2330 train_time:103843ms step_avg:61.26ms
step:1696/2330 train_time:103907ms step_avg:61.27ms
step:1697/2330 train_time:103967ms step_avg:61.27ms
step:1698/2330 train_time:104029ms step_avg:61.27ms
step:1699/2330 train_time:104090ms step_avg:61.27ms
step:1700/2330 train_time:104153ms step_avg:61.27ms
step:1701/2330 train_time:104214ms step_avg:61.27ms
step:1702/2330 train_time:104277ms step_avg:61.27ms
step:1703/2330 train_time:104338ms step_avg:61.27ms
step:1704/2330 train_time:104401ms step_avg:61.27ms
step:1705/2330 train_time:104462ms step_avg:61.27ms
step:1706/2330 train_time:104524ms step_avg:61.27ms
step:1707/2330 train_time:104586ms step_avg:61.27ms
step:1708/2330 train_time:104648ms step_avg:61.27ms
step:1709/2330 train_time:104709ms step_avg:61.27ms
step:1710/2330 train_time:104772ms step_avg:61.27ms
step:1711/2330 train_time:104832ms step_avg:61.27ms
step:1712/2330 train_time:104895ms step_avg:61.27ms
step:1713/2330 train_time:104956ms step_avg:61.27ms
step:1714/2330 train_time:105018ms step_avg:61.27ms
step:1715/2330 train_time:105079ms step_avg:61.27ms
step:1716/2330 train_time:105142ms step_avg:61.27ms
step:1717/2330 train_time:105204ms step_avg:61.27ms
step:1718/2330 train_time:105266ms step_avg:61.27ms
step:1719/2330 train_time:105327ms step_avg:61.27ms
step:1720/2330 train_time:105390ms step_avg:61.27ms
step:1721/2330 train_time:105450ms step_avg:61.27ms
step:1722/2330 train_time:105514ms step_avg:61.27ms
step:1723/2330 train_time:105574ms step_avg:61.27ms
step:1724/2330 train_time:105637ms step_avg:61.27ms
step:1725/2330 train_time:105698ms step_avg:61.27ms
step:1726/2330 train_time:105761ms step_avg:61.28ms
step:1727/2330 train_time:105822ms step_avg:61.28ms
step:1728/2330 train_time:105886ms step_avg:61.28ms
step:1729/2330 train_time:105947ms step_avg:61.28ms
step:1730/2330 train_time:106009ms step_avg:61.28ms
step:1731/2330 train_time:106070ms step_avg:61.28ms
step:1732/2330 train_time:106132ms step_avg:61.28ms
step:1733/2330 train_time:106193ms step_avg:61.28ms
step:1734/2330 train_time:106256ms step_avg:61.28ms
step:1735/2330 train_time:106317ms step_avg:61.28ms
step:1736/2330 train_time:106380ms step_avg:61.28ms
step:1737/2330 train_time:106441ms step_avg:61.28ms
step:1738/2330 train_time:106504ms step_avg:61.28ms
step:1739/2330 train_time:106564ms step_avg:61.28ms
step:1740/2330 train_time:106627ms step_avg:61.28ms
step:1741/2330 train_time:106688ms step_avg:61.28ms
step:1742/2330 train_time:106751ms step_avg:61.28ms
step:1743/2330 train_time:106811ms step_avg:61.28ms
step:1744/2330 train_time:106874ms step_avg:61.28ms
step:1745/2330 train_time:106935ms step_avg:61.28ms
step:1746/2330 train_time:106999ms step_avg:61.28ms
step:1747/2330 train_time:107059ms step_avg:61.28ms
step:1748/2330 train_time:107122ms step_avg:61.28ms
step:1749/2330 train_time:107183ms step_avg:61.28ms
step:1750/2330 train_time:107246ms step_avg:61.28ms
step:1750/2330 val_loss:3.4094 train_time:107311ms step_avg:61.32ms
step:1751/2330 train_time:107335ms step_avg:61.30ms
step:1752/2330 train_time:107372ms step_avg:61.29ms
step:1753/2330 train_time:107440ms step_avg:61.29ms
step:1754/2330 train_time:107505ms step_avg:61.29ms
step:1755/2330 train_time:107566ms step_avg:61.29ms
step:1756/2330 train_time:107630ms step_avg:61.29ms
step:1757/2330 train_time:107689ms step_avg:61.29ms
step:1758/2330 train_time:107752ms step_avg:61.29ms
step:1759/2330 train_time:107811ms step_avg:61.29ms
step:1760/2330 train_time:107873ms step_avg:61.29ms
step:1761/2330 train_time:107932ms step_avg:61.29ms
step:1762/2330 train_time:107995ms step_avg:61.29ms
step:1763/2330 train_time:108054ms step_avg:61.29ms
step:1764/2330 train_time:108116ms step_avg:61.29ms
step:1765/2330 train_time:108176ms step_avg:61.29ms
step:1766/2330 train_time:108240ms step_avg:61.29ms
step:1767/2330 train_time:108301ms step_avg:61.29ms
step:1768/2330 train_time:108365ms step_avg:61.29ms
step:1769/2330 train_time:108428ms step_avg:61.29ms
step:1770/2330 train_time:108492ms step_avg:61.29ms
step:1771/2330 train_time:108554ms step_avg:61.30ms
step:1772/2330 train_time:108616ms step_avg:61.30ms
step:1773/2330 train_time:108678ms step_avg:61.30ms
step:1774/2330 train_time:108740ms step_avg:61.30ms
step:1775/2330 train_time:108801ms step_avg:61.30ms
step:1776/2330 train_time:108864ms step_avg:61.30ms
step:1777/2330 train_time:108924ms step_avg:61.30ms
step:1778/2330 train_time:108987ms step_avg:61.30ms
step:1779/2330 train_time:109047ms step_avg:61.30ms
step:1780/2330 train_time:109109ms step_avg:61.30ms
step:1781/2330 train_time:109169ms step_avg:61.30ms
step:1782/2330 train_time:109232ms step_avg:61.30ms
step:1783/2330 train_time:109292ms step_avg:61.30ms
step:1784/2330 train_time:109356ms step_avg:61.30ms
step:1785/2330 train_time:109418ms step_avg:61.30ms
step:1786/2330 train_time:109482ms step_avg:61.30ms
step:1787/2330 train_time:109544ms step_avg:61.30ms
step:1788/2330 train_time:109607ms step_avg:61.30ms
step:1789/2330 train_time:109667ms step_avg:61.30ms
step:1790/2330 train_time:109730ms step_avg:61.30ms
step:1791/2330 train_time:109791ms step_avg:61.30ms
step:1792/2330 train_time:109853ms step_avg:61.30ms
step:1793/2330 train_time:109914ms step_avg:61.30ms
step:1794/2330 train_time:109977ms step_avg:61.30ms
step:1795/2330 train_time:110037ms step_avg:61.30ms
step:1796/2330 train_time:110100ms step_avg:61.30ms
step:1797/2330 train_time:110160ms step_avg:61.30ms
step:1798/2330 train_time:110223ms step_avg:61.30ms
step:1799/2330 train_time:110285ms step_avg:61.30ms
step:1800/2330 train_time:110349ms step_avg:61.30ms
step:1801/2330 train_time:110410ms step_avg:61.30ms
step:1802/2330 train_time:110473ms step_avg:61.31ms
step:1803/2330 train_time:110534ms step_avg:61.31ms
step:1804/2330 train_time:110597ms step_avg:61.31ms
step:1805/2330 train_time:110659ms step_avg:61.31ms
step:1806/2330 train_time:110722ms step_avg:61.31ms
step:1807/2330 train_time:110784ms step_avg:61.31ms
step:1808/2330 train_time:110847ms step_avg:61.31ms
step:1809/2330 train_time:110908ms step_avg:61.31ms
step:1810/2330 train_time:110970ms step_avg:61.31ms
step:1811/2330 train_time:111032ms step_avg:61.31ms
step:1812/2330 train_time:111094ms step_avg:61.31ms
step:1813/2330 train_time:111155ms step_avg:61.31ms
step:1814/2330 train_time:111217ms step_avg:61.31ms
step:1815/2330 train_time:111278ms step_avg:61.31ms
step:1816/2330 train_time:111341ms step_avg:61.31ms
step:1817/2330 train_time:111403ms step_avg:61.31ms
step:1818/2330 train_time:111466ms step_avg:61.31ms
step:1819/2330 train_time:111527ms step_avg:61.31ms
step:1820/2330 train_time:111590ms step_avg:61.31ms
step:1821/2330 train_time:111651ms step_avg:61.31ms
step:1822/2330 train_time:111714ms step_avg:61.31ms
step:1823/2330 train_time:111775ms step_avg:61.31ms
step:1824/2330 train_time:111838ms step_avg:61.31ms
step:1825/2330 train_time:111899ms step_avg:61.31ms
step:1826/2330 train_time:111962ms step_avg:61.32ms
step:1827/2330 train_time:112024ms step_avg:61.32ms
step:1828/2330 train_time:112087ms step_avg:61.32ms
step:1829/2330 train_time:112147ms step_avg:61.32ms
step:1830/2330 train_time:112210ms step_avg:61.32ms
step:1831/2330 train_time:112270ms step_avg:61.32ms
step:1832/2330 train_time:112333ms step_avg:61.32ms
step:1833/2330 train_time:112394ms step_avg:61.32ms
step:1834/2330 train_time:112457ms step_avg:61.32ms
step:1835/2330 train_time:112518ms step_avg:61.32ms
step:1836/2330 train_time:112582ms step_avg:61.32ms
step:1837/2330 train_time:112644ms step_avg:61.32ms
step:1838/2330 train_time:112707ms step_avg:61.32ms
step:1839/2330 train_time:112768ms step_avg:61.32ms
step:1840/2330 train_time:112831ms step_avg:61.32ms
step:1841/2330 train_time:112892ms step_avg:61.32ms
step:1842/2330 train_time:112955ms step_avg:61.32ms
step:1843/2330 train_time:113015ms step_avg:61.32ms
step:1844/2330 train_time:113079ms step_avg:61.32ms
step:1845/2330 train_time:113140ms step_avg:61.32ms
step:1846/2330 train_time:113203ms step_avg:61.32ms
step:1847/2330 train_time:113264ms step_avg:61.32ms
step:1848/2330 train_time:113327ms step_avg:61.32ms
step:1849/2330 train_time:113388ms step_avg:61.32ms
step:1850/2330 train_time:113451ms step_avg:61.32ms
step:1851/2330 train_time:113512ms step_avg:61.32ms
step:1852/2330 train_time:113575ms step_avg:61.33ms
step:1853/2330 train_time:113635ms step_avg:61.33ms
step:1854/2330 train_time:113699ms step_avg:61.33ms
step:1855/2330 train_time:113761ms step_avg:61.33ms
step:1856/2330 train_time:113824ms step_avg:61.33ms
step:1857/2330 train_time:113885ms step_avg:61.33ms
step:1858/2330 train_time:113948ms step_avg:61.33ms
step:1859/2330 train_time:114009ms step_avg:61.33ms
step:1860/2330 train_time:114072ms step_avg:61.33ms
step:1861/2330 train_time:114132ms step_avg:61.33ms
step:1862/2330 train_time:114195ms step_avg:61.33ms
step:1863/2330 train_time:114257ms step_avg:61.33ms
step:1864/2330 train_time:114320ms step_avg:61.33ms
step:1865/2330 train_time:114381ms step_avg:61.33ms
step:1866/2330 train_time:114444ms step_avg:61.33ms
step:1867/2330 train_time:114505ms step_avg:61.33ms
step:1868/2330 train_time:114568ms step_avg:61.33ms
step:1869/2330 train_time:114629ms step_avg:61.33ms
step:1870/2330 train_time:114692ms step_avg:61.33ms
step:1871/2330 train_time:114752ms step_avg:61.33ms
step:1872/2330 train_time:114815ms step_avg:61.33ms
step:1873/2330 train_time:114875ms step_avg:61.33ms
step:1874/2330 train_time:114939ms step_avg:61.33ms
step:1875/2330 train_time:115000ms step_avg:61.33ms
step:1876/2330 train_time:115062ms step_avg:61.33ms
step:1877/2330 train_time:115123ms step_avg:61.33ms
step:1878/2330 train_time:115186ms step_avg:61.33ms
step:1879/2330 train_time:115247ms step_avg:61.33ms
step:1880/2330 train_time:115310ms step_avg:61.33ms
step:1881/2330 train_time:115371ms step_avg:61.33ms
step:1882/2330 train_time:115434ms step_avg:61.34ms
step:1883/2330 train_time:115494ms step_avg:61.34ms
step:1884/2330 train_time:115558ms step_avg:61.34ms
step:1885/2330 train_time:115619ms step_avg:61.34ms
step:1886/2330 train_time:115682ms step_avg:61.34ms
step:1887/2330 train_time:115743ms step_avg:61.34ms
step:1888/2330 train_time:115806ms step_avg:61.34ms
step:1889/2330 train_time:115868ms step_avg:61.34ms
step:1890/2330 train_time:115931ms step_avg:61.34ms
step:1891/2330 train_time:115991ms step_avg:61.34ms
step:1892/2330 train_time:116054ms step_avg:61.34ms
step:1893/2330 train_time:116114ms step_avg:61.34ms
step:1894/2330 train_time:116177ms step_avg:61.34ms
step:1895/2330 train_time:116238ms step_avg:61.34ms
step:1896/2330 train_time:116301ms step_avg:61.34ms
step:1897/2330 train_time:116363ms step_avg:61.34ms
step:1898/2330 train_time:116426ms step_avg:61.34ms
step:1899/2330 train_time:116487ms step_avg:61.34ms
step:1900/2330 train_time:116550ms step_avg:61.34ms
step:1901/2330 train_time:116611ms step_avg:61.34ms
step:1902/2330 train_time:116673ms step_avg:61.34ms
step:1903/2330 train_time:116734ms step_avg:61.34ms
step:1904/2330 train_time:116797ms step_avg:61.34ms
step:1905/2330 train_time:116858ms step_avg:61.34ms
step:1906/2330 train_time:116921ms step_avg:61.34ms
step:1907/2330 train_time:116983ms step_avg:61.34ms
step:1908/2330 train_time:117046ms step_avg:61.34ms
step:1909/2330 train_time:117107ms step_avg:61.34ms
step:1910/2330 train_time:117170ms step_avg:61.35ms
step:1911/2330 train_time:117230ms step_avg:61.35ms
step:1912/2330 train_time:117293ms step_avg:61.35ms
step:1913/2330 train_time:117354ms step_avg:61.35ms
step:1914/2330 train_time:117416ms step_avg:61.35ms
step:1915/2330 train_time:117477ms step_avg:61.35ms
step:1916/2330 train_time:117541ms step_avg:61.35ms
step:1917/2330 train_time:117603ms step_avg:61.35ms
step:1918/2330 train_time:117665ms step_avg:61.35ms
step:1919/2330 train_time:117726ms step_avg:61.35ms
step:1920/2330 train_time:117789ms step_avg:61.35ms
step:1921/2330 train_time:117849ms step_avg:61.35ms
step:1922/2330 train_time:117912ms step_avg:61.35ms
step:1923/2330 train_time:117973ms step_avg:61.35ms
step:1924/2330 train_time:118036ms step_avg:61.35ms
step:1925/2330 train_time:118097ms step_avg:61.35ms
step:1926/2330 train_time:118160ms step_avg:61.35ms
step:1927/2330 train_time:118222ms step_avg:61.35ms
step:1928/2330 train_time:118285ms step_avg:61.35ms
step:1929/2330 train_time:118346ms step_avg:61.35ms
step:1930/2330 train_time:118409ms step_avg:61.35ms
step:1931/2330 train_time:118470ms step_avg:61.35ms
step:1932/2330 train_time:118533ms step_avg:61.35ms
step:1933/2330 train_time:118593ms step_avg:61.35ms
step:1934/2330 train_time:118657ms step_avg:61.35ms
step:1935/2330 train_time:118717ms step_avg:61.35ms
step:1936/2330 train_time:118781ms step_avg:61.35ms
step:1937/2330 train_time:118842ms step_avg:61.35ms
step:1938/2330 train_time:118905ms step_avg:61.35ms
step:1939/2330 train_time:118966ms step_avg:61.35ms
step:1940/2330 train_time:119029ms step_avg:61.36ms
step:1941/2330 train_time:119090ms step_avg:61.36ms
step:1942/2330 train_time:119153ms step_avg:61.36ms
step:1943/2330 train_time:119214ms step_avg:61.36ms
step:1944/2330 train_time:119277ms step_avg:61.36ms
step:1945/2330 train_time:119338ms step_avg:61.36ms
step:1946/2330 train_time:119401ms step_avg:61.36ms
step:1947/2330 train_time:119462ms step_avg:61.36ms
step:1948/2330 train_time:119525ms step_avg:61.36ms
step:1949/2330 train_time:119586ms step_avg:61.36ms
step:1950/2330 train_time:119649ms step_avg:61.36ms
step:1951/2330 train_time:119710ms step_avg:61.36ms
step:1952/2330 train_time:119773ms step_avg:61.36ms
step:1953/2330 train_time:119833ms step_avg:61.36ms
step:1954/2330 train_time:119897ms step_avg:61.36ms
step:1955/2330 train_time:119958ms step_avg:61.36ms
step:1956/2330 train_time:120021ms step_avg:61.36ms
step:1957/2330 train_time:120082ms step_avg:61.36ms
step:1958/2330 train_time:120146ms step_avg:61.36ms
step:1959/2330 train_time:120206ms step_avg:61.36ms
step:1960/2330 train_time:120270ms step_avg:61.36ms
step:1961/2330 train_time:120330ms step_avg:61.36ms
step:1962/2330 train_time:120393ms step_avg:61.36ms
step:1963/2330 train_time:120455ms step_avg:61.36ms
step:1964/2330 train_time:120517ms step_avg:61.36ms
step:1965/2330 train_time:120578ms step_avg:61.36ms
step:1966/2330 train_time:120642ms step_avg:61.36ms
step:1967/2330 train_time:120702ms step_avg:61.36ms
step:1968/2330 train_time:120765ms step_avg:61.36ms
step:1969/2330 train_time:120826ms step_avg:61.36ms
step:1970/2330 train_time:120889ms step_avg:61.36ms
step:1971/2330 train_time:120951ms step_avg:61.37ms
step:1972/2330 train_time:121013ms step_avg:61.37ms
step:1973/2330 train_time:121074ms step_avg:61.37ms
step:1974/2330 train_time:121136ms step_avg:61.37ms
step:1975/2330 train_time:121198ms step_avg:61.37ms
step:1976/2330 train_time:121261ms step_avg:61.37ms
step:1977/2330 train_time:121322ms step_avg:61.37ms
step:1978/2330 train_time:121385ms step_avg:61.37ms
step:1979/2330 train_time:121445ms step_avg:61.37ms
step:1980/2330 train_time:121508ms step_avg:61.37ms
step:1981/2330 train_time:121569ms step_avg:61.37ms
step:1982/2330 train_time:121632ms step_avg:61.37ms
step:1983/2330 train_time:121693ms step_avg:61.37ms
step:1984/2330 train_time:121757ms step_avg:61.37ms
step:1985/2330 train_time:121817ms step_avg:61.37ms
step:1986/2330 train_time:121881ms step_avg:61.37ms
step:1987/2330 train_time:121942ms step_avg:61.37ms
step:1988/2330 train_time:122005ms step_avg:61.37ms
step:1989/2330 train_time:122065ms step_avg:61.37ms
step:1990/2330 train_time:122129ms step_avg:61.37ms
step:1991/2330 train_time:122190ms step_avg:61.37ms
step:1992/2330 train_time:122252ms step_avg:61.37ms
step:1993/2330 train_time:122313ms step_avg:61.37ms
step:1994/2330 train_time:122376ms step_avg:61.37ms
step:1995/2330 train_time:122437ms step_avg:61.37ms
step:1996/2330 train_time:122501ms step_avg:61.37ms
step:1997/2330 train_time:122562ms step_avg:61.37ms
step:1998/2330 train_time:122626ms step_avg:61.37ms
step:1999/2330 train_time:122686ms step_avg:61.37ms
step:2000/2330 train_time:122750ms step_avg:61.37ms
step:2000/2330 val_loss:3.3600 train_time:122815ms step_avg:61.41ms
step:2001/2330 train_time:122839ms step_avg:61.39ms
step:2002/2330 train_time:122879ms step_avg:61.38ms
step:2003/2330 train_time:122945ms step_avg:61.38ms
step:2004/2330 train_time:123010ms step_avg:61.38ms
step:2005/2330 train_time:123072ms step_avg:61.38ms
step:2006/2330 train_time:123135ms step_avg:61.38ms
step:2007/2330 train_time:123195ms step_avg:61.38ms
step:2008/2330 train_time:123258ms step_avg:61.38ms
step:2009/2330 train_time:123319ms step_avg:61.38ms
step:2010/2330 train_time:123381ms step_avg:61.38ms
step:2011/2330 train_time:123442ms step_avg:61.38ms
step:2012/2330 train_time:123504ms step_avg:61.38ms
step:2013/2330 train_time:123563ms step_avg:61.38ms
step:2014/2330 train_time:123625ms step_avg:61.38ms
step:2015/2330 train_time:123685ms step_avg:61.38ms
step:2016/2330 train_time:123748ms step_avg:61.38ms
step:2017/2330 train_time:123809ms step_avg:61.38ms
step:2018/2330 train_time:123874ms step_avg:61.38ms
step:2019/2330 train_time:123937ms step_avg:61.39ms
step:2020/2330 train_time:124001ms step_avg:61.39ms
step:2021/2330 train_time:124064ms step_avg:61.39ms
step:2022/2330 train_time:124127ms step_avg:61.39ms
step:2023/2330 train_time:124187ms step_avg:61.39ms
step:2024/2330 train_time:124250ms step_avg:61.39ms
step:2025/2330 train_time:124311ms step_avg:61.39ms
step:2026/2330 train_time:124374ms step_avg:61.39ms
step:2027/2330 train_time:124434ms step_avg:61.39ms
step:2028/2330 train_time:124497ms step_avg:61.39ms
step:2029/2330 train_time:124558ms step_avg:61.39ms
step:2030/2330 train_time:124620ms step_avg:61.39ms
step:2031/2330 train_time:124680ms step_avg:61.39ms
step:2032/2330 train_time:124743ms step_avg:61.39ms
step:2033/2330 train_time:124804ms step_avg:61.39ms
step:2034/2330 train_time:124868ms step_avg:61.39ms
step:2035/2330 train_time:124929ms step_avg:61.39ms
step:2036/2330 train_time:124993ms step_avg:61.39ms
step:2037/2330 train_time:125054ms step_avg:61.39ms
step:2038/2330 train_time:125118ms step_avg:61.39ms
step:2039/2330 train_time:125179ms step_avg:61.39ms
step:2040/2330 train_time:125243ms step_avg:61.39ms
step:2041/2330 train_time:125304ms step_avg:61.39ms
step:2042/2330 train_time:125367ms step_avg:61.39ms
step:2043/2330 train_time:125427ms step_avg:61.39ms
step:2044/2330 train_time:125489ms step_avg:61.39ms
step:2045/2330 train_time:125550ms step_avg:61.39ms
step:2046/2330 train_time:125612ms step_avg:61.39ms
step:2047/2330 train_time:125673ms step_avg:61.39ms
step:2048/2330 train_time:125736ms step_avg:61.39ms
step:2049/2330 train_time:125798ms step_avg:61.39ms
step:2050/2330 train_time:125862ms step_avg:61.40ms
step:2051/2330 train_time:125923ms step_avg:61.40ms
step:2052/2330 train_time:125986ms step_avg:61.40ms
step:2053/2330 train_time:126048ms step_avg:61.40ms
step:2054/2330 train_time:126110ms step_avg:61.40ms
step:2055/2330 train_time:126171ms step_avg:61.40ms
step:2056/2330 train_time:126235ms step_avg:61.40ms
step:2057/2330 train_time:126297ms step_avg:61.40ms
step:2058/2330 train_time:126360ms step_avg:61.40ms
step:2059/2330 train_time:126421ms step_avg:61.40ms
step:2060/2330 train_time:126483ms step_avg:61.40ms
step:2061/2330 train_time:126544ms step_avg:61.40ms
step:2062/2330 train_time:126606ms step_avg:61.40ms
step:2063/2330 train_time:126666ms step_avg:61.40ms
step:2064/2330 train_time:126729ms step_avg:61.40ms
step:2065/2330 train_time:126790ms step_avg:61.40ms
step:2066/2330 train_time:126853ms step_avg:61.40ms
step:2067/2330 train_time:126914ms step_avg:61.40ms
step:2068/2330 train_time:126977ms step_avg:61.40ms
step:2069/2330 train_time:127039ms step_avg:61.40ms
step:2070/2330 train_time:127102ms step_avg:61.40ms
step:2071/2330 train_time:127163ms step_avg:61.40ms
step:2072/2330 train_time:127226ms step_avg:61.40ms
step:2073/2330 train_time:127287ms step_avg:61.40ms
step:2074/2330 train_time:127350ms step_avg:61.40ms
step:2075/2330 train_time:127410ms step_avg:61.40ms
step:2076/2330 train_time:127474ms step_avg:61.40ms
step:2077/2330 train_time:127535ms step_avg:61.40ms
step:2078/2330 train_time:127598ms step_avg:61.40ms
step:2079/2330 train_time:127660ms step_avg:61.40ms
step:2080/2330 train_time:127723ms step_avg:61.41ms
step:2081/2330 train_time:127784ms step_avg:61.41ms
step:2082/2330 train_time:127846ms step_avg:61.41ms
step:2083/2330 train_time:127907ms step_avg:61.41ms
step:2084/2330 train_time:127970ms step_avg:61.41ms
step:2085/2330 train_time:128031ms step_avg:61.41ms
step:2086/2330 train_time:128094ms step_avg:61.41ms
step:2087/2330 train_time:128155ms step_avg:61.41ms
step:2088/2330 train_time:128219ms step_avg:61.41ms
step:2089/2330 train_time:128281ms step_avg:61.41ms
step:2090/2330 train_time:128343ms step_avg:61.41ms
step:2091/2330 train_time:128405ms step_avg:61.41ms
step:2092/2330 train_time:128468ms step_avg:61.41ms
step:2093/2330 train_time:128529ms step_avg:61.41ms
step:2094/2330 train_time:128592ms step_avg:61.41ms
step:2095/2330 train_time:128653ms step_avg:61.41ms
step:2096/2330 train_time:128717ms step_avg:61.41ms
step:2097/2330 train_time:128778ms step_avg:61.41ms
step:2098/2330 train_time:128841ms step_avg:61.41ms
step:2099/2330 train_time:128903ms step_avg:61.41ms
step:2100/2330 train_time:128966ms step_avg:61.41ms
step:2101/2330 train_time:129026ms step_avg:61.41ms
step:2102/2330 train_time:129088ms step_avg:61.41ms
step:2103/2330 train_time:129149ms step_avg:61.41ms
step:2104/2330 train_time:129213ms step_avg:61.41ms
step:2105/2330 train_time:129274ms step_avg:61.41ms
step:2106/2330 train_time:129337ms step_avg:61.41ms
step:2107/2330 train_time:129398ms step_avg:61.41ms
step:2108/2330 train_time:129462ms step_avg:61.41ms
step:2109/2330 train_time:129522ms step_avg:61.41ms
step:2110/2330 train_time:129585ms step_avg:61.41ms
step:2111/2330 train_time:129646ms step_avg:61.41ms
step:2112/2330 train_time:129710ms step_avg:61.42ms
step:2113/2330 train_time:129770ms step_avg:61.42ms
step:2114/2330 train_time:129833ms step_avg:61.42ms
step:2115/2330 train_time:129894ms step_avg:61.42ms
step:2116/2330 train_time:129957ms step_avg:61.42ms
step:2117/2330 train_time:130018ms step_avg:61.42ms
step:2118/2330 train_time:130082ms step_avg:61.42ms
step:2119/2330 train_time:130143ms step_avg:61.42ms
step:2120/2330 train_time:130206ms step_avg:61.42ms
step:2121/2330 train_time:130266ms step_avg:61.42ms
step:2122/2330 train_time:130329ms step_avg:61.42ms
step:2123/2330 train_time:130390ms step_avg:61.42ms
step:2124/2330 train_time:130453ms step_avg:61.42ms
step:2125/2330 train_time:130514ms step_avg:61.42ms
step:2126/2330 train_time:130577ms step_avg:61.42ms
step:2127/2330 train_time:130639ms step_avg:61.42ms
step:2128/2330 train_time:130702ms step_avg:61.42ms
step:2129/2330 train_time:130762ms step_avg:61.42ms
step:2130/2330 train_time:130825ms step_avg:61.42ms
step:2131/2330 train_time:130886ms step_avg:61.42ms
step:2132/2330 train_time:130949ms step_avg:61.42ms
step:2133/2330 train_time:131010ms step_avg:61.42ms
step:2134/2330 train_time:131073ms step_avg:61.42ms
step:2135/2330 train_time:131133ms step_avg:61.42ms
step:2136/2330 train_time:131197ms step_avg:61.42ms
step:2137/2330 train_time:131259ms step_avg:61.42ms
step:2138/2330 train_time:131321ms step_avg:61.42ms
step:2139/2330 train_time:131382ms step_avg:61.42ms
step:2140/2330 train_time:131444ms step_avg:61.42ms
step:2141/2330 train_time:131505ms step_avg:61.42ms
step:2142/2330 train_time:131568ms step_avg:61.42ms
step:2143/2330 train_time:131629ms step_avg:61.42ms
step:2144/2330 train_time:131691ms step_avg:61.42ms
step:2145/2330 train_time:131753ms step_avg:61.42ms
step:2146/2330 train_time:131816ms step_avg:61.42ms
step:2147/2330 train_time:131877ms step_avg:61.42ms
step:2148/2330 train_time:131940ms step_avg:61.42ms
step:2149/2330 train_time:132001ms step_avg:61.42ms
step:2150/2330 train_time:132064ms step_avg:61.43ms
step:2151/2330 train_time:132125ms step_avg:61.43ms
step:2152/2330 train_time:132188ms step_avg:61.43ms
step:2153/2330 train_time:132249ms step_avg:61.43ms
step:2154/2330 train_time:132312ms step_avg:61.43ms
step:2155/2330 train_time:132373ms step_avg:61.43ms
step:2156/2330 train_time:132437ms step_avg:61.43ms
step:2157/2330 train_time:132498ms step_avg:61.43ms
step:2158/2330 train_time:132562ms step_avg:61.43ms
step:2159/2330 train_time:132622ms step_avg:61.43ms
step:2160/2330 train_time:132686ms step_avg:61.43ms
step:2161/2330 train_time:132746ms step_avg:61.43ms
step:2162/2330 train_time:132809ms step_avg:61.43ms
step:2163/2330 train_time:132870ms step_avg:61.43ms
step:2164/2330 train_time:132932ms step_avg:61.43ms
step:2165/2330 train_time:132993ms step_avg:61.43ms
step:2166/2330 train_time:133057ms step_avg:61.43ms
step:2167/2330 train_time:133119ms step_avg:61.43ms
step:2168/2330 train_time:133182ms step_avg:61.43ms
step:2169/2330 train_time:133243ms step_avg:61.43ms
step:2170/2330 train_time:133306ms step_avg:61.43ms
step:2171/2330 train_time:133367ms step_avg:61.43ms
step:2172/2330 train_time:133430ms step_avg:61.43ms
step:2173/2330 train_time:133490ms step_avg:61.43ms
step:2174/2330 train_time:133553ms step_avg:61.43ms
step:2175/2330 train_time:133614ms step_avg:61.43ms
step:2176/2330 train_time:133678ms step_avg:61.43ms
step:2177/2330 train_time:133738ms step_avg:61.43ms
step:2178/2330 train_time:133801ms step_avg:61.43ms
step:2179/2330 train_time:133863ms step_avg:61.43ms
step:2180/2330 train_time:133926ms step_avg:61.43ms
step:2181/2330 train_time:133987ms step_avg:61.43ms
step:2182/2330 train_time:134051ms step_avg:61.43ms
step:2183/2330 train_time:134111ms step_avg:61.43ms
step:2184/2330 train_time:134175ms step_avg:61.44ms
step:2185/2330 train_time:134237ms step_avg:61.44ms
step:2186/2330 train_time:134300ms step_avg:61.44ms
step:2187/2330 train_time:134361ms step_avg:61.44ms
step:2188/2330 train_time:134424ms step_avg:61.44ms
step:2189/2330 train_time:134485ms step_avg:61.44ms
step:2190/2330 train_time:134548ms step_avg:61.44ms
step:2191/2330 train_time:134609ms step_avg:61.44ms
step:2192/2330 train_time:134672ms step_avg:61.44ms
step:2193/2330 train_time:134733ms step_avg:61.44ms
step:2194/2330 train_time:134796ms step_avg:61.44ms
step:2195/2330 train_time:134857ms step_avg:61.44ms
step:2196/2330 train_time:134920ms step_avg:61.44ms
step:2197/2330 train_time:134982ms step_avg:61.44ms
step:2198/2330 train_time:135045ms step_avg:61.44ms
step:2199/2330 train_time:135106ms step_avg:61.44ms
step:2200/2330 train_time:135169ms step_avg:61.44ms
step:2201/2330 train_time:135230ms step_avg:61.44ms
step:2202/2330 train_time:135293ms step_avg:61.44ms
step:2203/2330 train_time:135355ms step_avg:61.44ms
step:2204/2330 train_time:135419ms step_avg:61.44ms
step:2205/2330 train_time:135480ms step_avg:61.44ms
step:2206/2330 train_time:135544ms step_avg:61.44ms
step:2207/2330 train_time:135606ms step_avg:61.44ms
step:2208/2330 train_time:135669ms step_avg:61.44ms
step:2209/2330 train_time:135729ms step_avg:61.44ms
step:2210/2330 train_time:135791ms step_avg:61.44ms
step:2211/2330 train_time:135852ms step_avg:61.44ms
step:2212/2330 train_time:135915ms step_avg:61.44ms
step:2213/2330 train_time:135976ms step_avg:61.44ms
step:2214/2330 train_time:136039ms step_avg:61.44ms
step:2215/2330 train_time:136100ms step_avg:61.44ms
step:2216/2330 train_time:136164ms step_avg:61.45ms
step:2217/2330 train_time:136224ms step_avg:61.45ms
step:2218/2330 train_time:136287ms step_avg:61.45ms
step:2219/2330 train_time:136347ms step_avg:61.45ms
step:2220/2330 train_time:136410ms step_avg:61.45ms
step:2221/2330 train_time:136472ms step_avg:61.45ms
step:2222/2330 train_time:136535ms step_avg:61.45ms
step:2223/2330 train_time:136595ms step_avg:61.45ms
step:2224/2330 train_time:136658ms step_avg:61.45ms
step:2225/2330 train_time:136719ms step_avg:61.45ms
step:2226/2330 train_time:136782ms step_avg:61.45ms
step:2227/2330 train_time:136843ms step_avg:61.45ms
step:2228/2330 train_time:136905ms step_avg:61.45ms
step:2229/2330 train_time:136966ms step_avg:61.45ms
step:2230/2330 train_time:137029ms step_avg:61.45ms
step:2231/2330 train_time:137089ms step_avg:61.45ms
step:2232/2330 train_time:137153ms step_avg:61.45ms
step:2233/2330 train_time:137214ms step_avg:61.45ms
step:2234/2330 train_time:137277ms step_avg:61.45ms
step:2235/2330 train_time:137338ms step_avg:61.45ms
step:2236/2330 train_time:137402ms step_avg:61.45ms
step:2237/2330 train_time:137463ms step_avg:61.45ms
step:2238/2330 train_time:137526ms step_avg:61.45ms
step:2239/2330 train_time:137586ms step_avg:61.45ms
step:2240/2330 train_time:137649ms step_avg:61.45ms
step:2241/2330 train_time:137709ms step_avg:61.45ms
step:2242/2330 train_time:137773ms step_avg:61.45ms
step:2243/2330 train_time:137834ms step_avg:61.45ms
step:2244/2330 train_time:137897ms step_avg:61.45ms
step:2245/2330 train_time:137959ms step_avg:61.45ms
step:2246/2330 train_time:138022ms step_avg:61.45ms
step:2247/2330 train_time:138082ms step_avg:61.45ms
step:2248/2330 train_time:138146ms step_avg:61.45ms
step:2249/2330 train_time:138206ms step_avg:61.45ms
step:2250/2330 train_time:138269ms step_avg:61.45ms
step:2250/2330 val_loss:3.3203 train_time:138334ms step_avg:61.48ms
step:2251/2330 train_time:138358ms step_avg:61.46ms
step:2252/2330 train_time:138399ms step_avg:61.46ms
step:2253/2330 train_time:138465ms step_avg:61.46ms
step:2254/2330 train_time:138529ms step_avg:61.46ms
step:2255/2330 train_time:138590ms step_avg:61.46ms
step:2256/2330 train_time:138653ms step_avg:61.46ms
step:2257/2330 train_time:138712ms step_avg:61.46ms
step:2258/2330 train_time:138775ms step_avg:61.46ms
step:2259/2330 train_time:138835ms step_avg:61.46ms
step:2260/2330 train_time:138897ms step_avg:61.46ms
step:2261/2330 train_time:138958ms step_avg:61.46ms
step:2262/2330 train_time:139020ms step_avg:61.46ms
step:2263/2330 train_time:139079ms step_avg:61.46ms
step:2264/2330 train_time:139141ms step_avg:61.46ms
step:2265/2330 train_time:139202ms step_avg:61.46ms
step:2266/2330 train_time:139265ms step_avg:61.46ms
step:2267/2330 train_time:139327ms step_avg:61.46ms
step:2268/2330 train_time:139392ms step_avg:61.46ms
step:2269/2330 train_time:139456ms step_avg:61.46ms
step:2270/2330 train_time:139519ms step_avg:61.46ms
step:2271/2330 train_time:139580ms step_avg:61.46ms
step:2272/2330 train_time:139642ms step_avg:61.46ms
step:2273/2330 train_time:139703ms step_avg:61.46ms
step:2274/2330 train_time:139766ms step_avg:61.46ms
step:2275/2330 train_time:139826ms step_avg:61.46ms
step:2276/2330 train_time:139888ms step_avg:61.46ms
step:2277/2330 train_time:139949ms step_avg:61.46ms
step:2278/2330 train_time:140011ms step_avg:61.46ms
step:2279/2330 train_time:140071ms step_avg:61.46ms
step:2280/2330 train_time:140133ms step_avg:61.46ms
step:2281/2330 train_time:140194ms step_avg:61.46ms
step:2282/2330 train_time:140258ms step_avg:61.46ms
step:2283/2330 train_time:140320ms step_avg:61.46ms
step:2284/2330 train_time:140384ms step_avg:61.46ms
step:2285/2330 train_time:140446ms step_avg:61.46ms
step:2286/2330 train_time:140509ms step_avg:61.47ms
step:2287/2330 train_time:140571ms step_avg:61.47ms
step:2288/2330 train_time:140634ms step_avg:61.47ms
step:2289/2330 train_time:140695ms step_avg:61.47ms
step:2290/2330 train_time:140758ms step_avg:61.47ms
step:2291/2330 train_time:140818ms step_avg:61.47ms
step:2292/2330 train_time:140881ms step_avg:61.47ms
step:2293/2330 train_time:140942ms step_avg:61.47ms
step:2294/2330 train_time:141004ms step_avg:61.47ms
step:2295/2330 train_time:141066ms step_avg:61.47ms
step:2296/2330 train_time:141128ms step_avg:61.47ms
step:2297/2330 train_time:141189ms step_avg:61.47ms
step:2298/2330 train_time:141252ms step_avg:61.47ms
step:2299/2330 train_time:141313ms step_avg:61.47ms
step:2300/2330 train_time:141377ms step_avg:61.47ms
step:2301/2330 train_time:141438ms step_avg:61.47ms
step:2302/2330 train_time:141501ms step_avg:61.47ms
step:2303/2330 train_time:141563ms step_avg:61.47ms
step:2304/2330 train_time:141626ms step_avg:61.47ms
step:2305/2330 train_time:141687ms step_avg:61.47ms
step:2306/2330 train_time:141749ms step_avg:61.47ms
step:2307/2330 train_time:141810ms step_avg:61.47ms
step:2308/2330 train_time:141873ms step_avg:61.47ms
step:2309/2330 train_time:141933ms step_avg:61.47ms
step:2310/2330 train_time:141996ms step_avg:61.47ms
step:2311/2330 train_time:142056ms step_avg:61.47ms
step:2312/2330 train_time:142119ms step_avg:61.47ms
step:2313/2330 train_time:142180ms step_avg:61.47ms
step:2314/2330 train_time:142243ms step_avg:61.47ms
step:2315/2330 train_time:142304ms step_avg:61.47ms
step:2316/2330 train_time:142367ms step_avg:61.47ms
step:2317/2330 train_time:142428ms step_avg:61.47ms
step:2318/2330 train_time:142491ms step_avg:61.47ms
step:2319/2330 train_time:142553ms step_avg:61.47ms
step:2320/2330 train_time:142617ms step_avg:61.47ms
step:2321/2330 train_time:142678ms step_avg:61.47ms
step:2322/2330 train_time:142741ms step_avg:61.47ms
step:2323/2330 train_time:142802ms step_avg:61.47ms
step:2324/2330 train_time:142864ms step_avg:61.47ms
step:2325/2330 train_time:142925ms step_avg:61.47ms
step:2326/2330 train_time:142988ms step_avg:61.47ms
step:2327/2330 train_time:143048ms step_avg:61.47ms
step:2328/2330 train_time:143111ms step_avg:61.47ms
step:2329/2330 train_time:143171ms step_avg:61.47ms
step:2330/2330 train_time:143235ms step_avg:61.47ms
step:2330/2330 val_loss:3.3061 train_time:143300ms step_avg:61.50ms
peak memory allocated: 29226 MiB reserved: 44076 MiB
