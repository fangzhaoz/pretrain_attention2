import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "muon_lr7e-4"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=7e-4, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Oct 28 22:25:16 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   33C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   31C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   31C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             115W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:82ms step_avg:81.68ms
step:2/2330 train_time:177ms step_avg:88.70ms
step:3/2330 train_time:199ms step_avg:66.25ms
step:4/2330 train_time:234ms step_avg:58.43ms
step:5/2330 train_time:291ms step_avg:58.24ms
step:6/2330 train_time:352ms step_avg:58.65ms
step:7/2330 train_time:410ms step_avg:58.60ms
step:8/2330 train_time:471ms step_avg:58.90ms
step:9/2330 train_time:530ms step_avg:58.91ms
step:10/2330 train_time:591ms step_avg:59.11ms
step:11/2330 train_time:650ms step_avg:59.07ms
step:12/2330 train_time:711ms step_avg:59.22ms
step:13/2330 train_time:770ms step_avg:59.20ms
step:14/2330 train_time:831ms step_avg:59.32ms
step:15/2330 train_time:890ms step_avg:59.32ms
step:16/2330 train_time:951ms step_avg:59.43ms
step:17/2330 train_time:1013ms step_avg:59.57ms
step:18/2330 train_time:1079ms step_avg:59.92ms
step:19/2330 train_time:1141ms step_avg:60.07ms
step:20/2330 train_time:1204ms step_avg:60.20ms
step:21/2330 train_time:1263ms step_avg:60.16ms
step:22/2330 train_time:1325ms step_avg:60.21ms
step:23/2330 train_time:1385ms step_avg:60.21ms
step:24/2330 train_time:1446ms step_avg:60.24ms
step:25/2330 train_time:1507ms step_avg:60.27ms
step:26/2330 train_time:1568ms step_avg:60.32ms
step:27/2330 train_time:1628ms step_avg:60.29ms
step:28/2330 train_time:1689ms step_avg:60.33ms
step:29/2330 train_time:1748ms step_avg:60.28ms
step:30/2330 train_time:1809ms step_avg:60.32ms
step:31/2330 train_time:1868ms step_avg:60.27ms
step:32/2330 train_time:1929ms step_avg:60.29ms
step:33/2330 train_time:1990ms step_avg:60.30ms
step:34/2330 train_time:2054ms step_avg:60.42ms
step:35/2330 train_time:2116ms step_avg:60.47ms
step:36/2330 train_time:2180ms step_avg:60.56ms
step:37/2330 train_time:2240ms step_avg:60.54ms
step:38/2330 train_time:2302ms step_avg:60.59ms
step:39/2330 train_time:2362ms step_avg:60.57ms
step:40/2330 train_time:2424ms step_avg:60.59ms
step:41/2330 train_time:2483ms step_avg:60.56ms
step:42/2330 train_time:2544ms step_avg:60.57ms
step:43/2330 train_time:2604ms step_avg:60.55ms
step:44/2330 train_time:2665ms step_avg:60.58ms
step:45/2330 train_time:2725ms step_avg:60.56ms
step:46/2330 train_time:2787ms step_avg:60.59ms
step:47/2330 train_time:2846ms step_avg:60.55ms
step:48/2330 train_time:2908ms step_avg:60.57ms
step:49/2330 train_time:2968ms step_avg:60.56ms
step:50/2330 train_time:3030ms step_avg:60.60ms
step:51/2330 train_time:3090ms step_avg:60.60ms
step:52/2330 train_time:3153ms step_avg:60.64ms
step:53/2330 train_time:3214ms step_avg:60.65ms
step:54/2330 train_time:3277ms step_avg:60.69ms
step:55/2330 train_time:3337ms step_avg:60.67ms
step:56/2330 train_time:3399ms step_avg:60.69ms
step:57/2330 train_time:3458ms step_avg:60.67ms
step:58/2330 train_time:3520ms step_avg:60.69ms
step:59/2330 train_time:3580ms step_avg:60.67ms
step:60/2330 train_time:3641ms step_avg:60.68ms
step:61/2330 train_time:3700ms step_avg:60.66ms
step:62/2330 train_time:3762ms step_avg:60.67ms
step:63/2330 train_time:3821ms step_avg:60.64ms
step:64/2330 train_time:3882ms step_avg:60.66ms
step:65/2330 train_time:3943ms step_avg:60.66ms
step:66/2330 train_time:4005ms step_avg:60.69ms
step:67/2330 train_time:4066ms step_avg:60.68ms
step:68/2330 train_time:4129ms step_avg:60.72ms
step:69/2330 train_time:4190ms step_avg:60.72ms
step:70/2330 train_time:4252ms step_avg:60.74ms
step:71/2330 train_time:4311ms step_avg:60.73ms
step:72/2330 train_time:4374ms step_avg:60.75ms
step:73/2330 train_time:4434ms step_avg:60.73ms
step:74/2330 train_time:4496ms step_avg:60.75ms
step:75/2330 train_time:4555ms step_avg:60.73ms
step:76/2330 train_time:4617ms step_avg:60.75ms
step:77/2330 train_time:4677ms step_avg:60.74ms
step:78/2330 train_time:4739ms step_avg:60.75ms
step:79/2330 train_time:4798ms step_avg:60.73ms
step:80/2330 train_time:4860ms step_avg:60.75ms
step:81/2330 train_time:4919ms step_avg:60.73ms
step:82/2330 train_time:4980ms step_avg:60.74ms
step:83/2330 train_time:5040ms step_avg:60.72ms
step:84/2330 train_time:5102ms step_avg:60.73ms
step:85/2330 train_time:5162ms step_avg:60.73ms
step:86/2330 train_time:5224ms step_avg:60.75ms
step:87/2330 train_time:5285ms step_avg:60.74ms
step:88/2330 train_time:5347ms step_avg:60.76ms
step:89/2330 train_time:5407ms step_avg:60.75ms
step:90/2330 train_time:5469ms step_avg:60.76ms
step:91/2330 train_time:5529ms step_avg:60.76ms
step:92/2330 train_time:5591ms step_avg:60.77ms
step:93/2330 train_time:5650ms step_avg:60.76ms
step:94/2330 train_time:5712ms step_avg:60.77ms
step:95/2330 train_time:5772ms step_avg:60.76ms
step:96/2330 train_time:5834ms step_avg:60.77ms
step:97/2330 train_time:5895ms step_avg:60.77ms
step:98/2330 train_time:5956ms step_avg:60.78ms
step:99/2330 train_time:6016ms step_avg:60.77ms
step:100/2330 train_time:6078ms step_avg:60.78ms
step:101/2330 train_time:6137ms step_avg:60.77ms
step:102/2330 train_time:6199ms step_avg:60.77ms
step:103/2330 train_time:6258ms step_avg:60.76ms
step:104/2330 train_time:6320ms step_avg:60.77ms
step:105/2330 train_time:6380ms step_avg:60.76ms
step:106/2330 train_time:6441ms step_avg:60.77ms
step:107/2330 train_time:6501ms step_avg:60.76ms
step:108/2330 train_time:6563ms step_avg:60.77ms
step:109/2330 train_time:6623ms step_avg:60.76ms
step:110/2330 train_time:6685ms step_avg:60.78ms
step:111/2330 train_time:6746ms step_avg:60.77ms
step:112/2330 train_time:6808ms step_avg:60.79ms
step:113/2330 train_time:6868ms step_avg:60.78ms
step:114/2330 train_time:6931ms step_avg:60.79ms
step:115/2330 train_time:6991ms step_avg:60.79ms
step:116/2330 train_time:7052ms step_avg:60.80ms
step:117/2330 train_time:7113ms step_avg:60.79ms
step:118/2330 train_time:7176ms step_avg:60.81ms
step:119/2330 train_time:7235ms step_avg:60.80ms
step:120/2330 train_time:7297ms step_avg:60.81ms
step:121/2330 train_time:7357ms step_avg:60.80ms
step:122/2330 train_time:7419ms step_avg:60.81ms
step:123/2330 train_time:7478ms step_avg:60.80ms
step:124/2330 train_time:7540ms step_avg:60.81ms
step:125/2330 train_time:7600ms step_avg:60.80ms
step:126/2330 train_time:7661ms step_avg:60.80ms
step:127/2330 train_time:7721ms step_avg:60.80ms
step:128/2330 train_time:7784ms step_avg:60.81ms
step:129/2330 train_time:7844ms step_avg:60.80ms
step:130/2330 train_time:7906ms step_avg:60.81ms
step:131/2330 train_time:7966ms step_avg:60.81ms
step:132/2330 train_time:8028ms step_avg:60.82ms
step:133/2330 train_time:8089ms step_avg:60.82ms
step:134/2330 train_time:8150ms step_avg:60.82ms
step:135/2330 train_time:8210ms step_avg:60.82ms
step:136/2330 train_time:8272ms step_avg:60.83ms
step:137/2330 train_time:8333ms step_avg:60.82ms
step:138/2330 train_time:8395ms step_avg:60.83ms
step:139/2330 train_time:8455ms step_avg:60.83ms
step:140/2330 train_time:8517ms step_avg:60.83ms
step:141/2330 train_time:8577ms step_avg:60.83ms
step:142/2330 train_time:8638ms step_avg:60.83ms
step:143/2330 train_time:8697ms step_avg:60.82ms
step:144/2330 train_time:8758ms step_avg:60.82ms
step:145/2330 train_time:8818ms step_avg:60.81ms
step:146/2330 train_time:8880ms step_avg:60.82ms
step:147/2330 train_time:8939ms step_avg:60.81ms
step:148/2330 train_time:9001ms step_avg:60.82ms
step:149/2330 train_time:9061ms step_avg:60.81ms
step:150/2330 train_time:9123ms step_avg:60.82ms
step:151/2330 train_time:9183ms step_avg:60.82ms
step:152/2330 train_time:9245ms step_avg:60.82ms
step:153/2330 train_time:9306ms step_avg:60.83ms
step:154/2330 train_time:9369ms step_avg:60.84ms
step:155/2330 train_time:9429ms step_avg:60.83ms
step:156/2330 train_time:9490ms step_avg:60.84ms
step:157/2330 train_time:9550ms step_avg:60.83ms
step:158/2330 train_time:9613ms step_avg:60.84ms
step:159/2330 train_time:9674ms step_avg:60.84ms
step:160/2330 train_time:9735ms step_avg:60.85ms
step:161/2330 train_time:9796ms step_avg:60.84ms
step:162/2330 train_time:9858ms step_avg:60.85ms
step:163/2330 train_time:9918ms step_avg:60.85ms
step:164/2330 train_time:9980ms step_avg:60.85ms
step:165/2330 train_time:10039ms step_avg:60.84ms
step:166/2330 train_time:10101ms step_avg:60.85ms
step:167/2330 train_time:10160ms step_avg:60.84ms
step:168/2330 train_time:10222ms step_avg:60.85ms
step:169/2330 train_time:10282ms step_avg:60.84ms
step:170/2330 train_time:10345ms step_avg:60.85ms
step:171/2330 train_time:10405ms step_avg:60.85ms
step:172/2330 train_time:10467ms step_avg:60.85ms
step:173/2330 train_time:10527ms step_avg:60.85ms
step:174/2330 train_time:10590ms step_avg:60.86ms
step:175/2330 train_time:10651ms step_avg:60.86ms
step:176/2330 train_time:10712ms step_avg:60.87ms
step:177/2330 train_time:10773ms step_avg:60.86ms
step:178/2330 train_time:10835ms step_avg:60.87ms
step:179/2330 train_time:10895ms step_avg:60.86ms
step:180/2330 train_time:10957ms step_avg:60.87ms
step:181/2330 train_time:11017ms step_avg:60.87ms
step:182/2330 train_time:11079ms step_avg:60.87ms
step:183/2330 train_time:11138ms step_avg:60.87ms
step:184/2330 train_time:11201ms step_avg:60.87ms
step:185/2330 train_time:11260ms step_avg:60.86ms
step:186/2330 train_time:11321ms step_avg:60.87ms
step:187/2330 train_time:11381ms step_avg:60.86ms
step:188/2330 train_time:11443ms step_avg:60.87ms
step:189/2330 train_time:11503ms step_avg:60.86ms
step:190/2330 train_time:11565ms step_avg:60.87ms
step:191/2330 train_time:11626ms step_avg:60.87ms
step:192/2330 train_time:11689ms step_avg:60.88ms
step:193/2330 train_time:11750ms step_avg:60.88ms
step:194/2330 train_time:11812ms step_avg:60.89ms
step:195/2330 train_time:11871ms step_avg:60.88ms
step:196/2330 train_time:11934ms step_avg:60.89ms
step:197/2330 train_time:11994ms step_avg:60.89ms
step:198/2330 train_time:12057ms step_avg:60.89ms
step:199/2330 train_time:12117ms step_avg:60.89ms
step:200/2330 train_time:12179ms step_avg:60.90ms
step:201/2330 train_time:12239ms step_avg:60.89ms
step:202/2330 train_time:12301ms step_avg:60.89ms
step:203/2330 train_time:12361ms step_avg:60.89ms
step:204/2330 train_time:12422ms step_avg:60.89ms
step:205/2330 train_time:12481ms step_avg:60.88ms
step:206/2330 train_time:12543ms step_avg:60.89ms
step:207/2330 train_time:12603ms step_avg:60.88ms
step:208/2330 train_time:12665ms step_avg:60.89ms
step:209/2330 train_time:12726ms step_avg:60.89ms
step:210/2330 train_time:12789ms step_avg:60.90ms
step:211/2330 train_time:12849ms step_avg:60.90ms
step:212/2330 train_time:12911ms step_avg:60.90ms
step:213/2330 train_time:12971ms step_avg:60.90ms
step:214/2330 train_time:13033ms step_avg:60.90ms
step:215/2330 train_time:13094ms step_avg:60.90ms
step:216/2330 train_time:13155ms step_avg:60.90ms
step:217/2330 train_time:13216ms step_avg:60.90ms
step:218/2330 train_time:13279ms step_avg:60.91ms
step:219/2330 train_time:13338ms step_avg:60.91ms
step:220/2330 train_time:13400ms step_avg:60.91ms
step:221/2330 train_time:13460ms step_avg:60.90ms
step:222/2330 train_time:13522ms step_avg:60.91ms
step:223/2330 train_time:13582ms step_avg:60.90ms
step:224/2330 train_time:13643ms step_avg:60.91ms
step:225/2330 train_time:13703ms step_avg:60.90ms
step:226/2330 train_time:13766ms step_avg:60.91ms
step:227/2330 train_time:13827ms step_avg:60.91ms
step:228/2330 train_time:13889ms step_avg:60.92ms
step:229/2330 train_time:13950ms step_avg:60.92ms
step:230/2330 train_time:14012ms step_avg:60.92ms
step:231/2330 train_time:14072ms step_avg:60.92ms
step:232/2330 train_time:14134ms step_avg:60.92ms
step:233/2330 train_time:14194ms step_avg:60.92ms
step:234/2330 train_time:14257ms step_avg:60.93ms
step:235/2330 train_time:14317ms step_avg:60.92ms
step:236/2330 train_time:14379ms step_avg:60.93ms
step:237/2330 train_time:14439ms step_avg:60.92ms
step:238/2330 train_time:14501ms step_avg:60.93ms
step:239/2330 train_time:14560ms step_avg:60.92ms
step:240/2330 train_time:14622ms step_avg:60.92ms
step:241/2330 train_time:14682ms step_avg:60.92ms
step:242/2330 train_time:14744ms step_avg:60.92ms
step:243/2330 train_time:14805ms step_avg:60.92ms
step:244/2330 train_time:14867ms step_avg:60.93ms
step:245/2330 train_time:14927ms step_avg:60.93ms
step:246/2330 train_time:14991ms step_avg:60.94ms
step:247/2330 train_time:15051ms step_avg:60.93ms
step:248/2330 train_time:15112ms step_avg:60.94ms
step:249/2330 train_time:15173ms step_avg:60.93ms
step:250/2330 train_time:15235ms step_avg:60.94ms
step:250/2330 val_loss:4.8849 train_time:15300ms step_avg:61.20ms
step:251/2330 train_time:15324ms step_avg:61.05ms
step:252/2330 train_time:15359ms step_avg:60.95ms
step:253/2330 train_time:15426ms step_avg:60.97ms
step:254/2330 train_time:15492ms step_avg:60.99ms
step:255/2330 train_time:15552ms step_avg:60.99ms
step:256/2330 train_time:15614ms step_avg:60.99ms
step:257/2330 train_time:15673ms step_avg:60.99ms
step:258/2330 train_time:15735ms step_avg:60.99ms
step:259/2330 train_time:15794ms step_avg:60.98ms
step:260/2330 train_time:15856ms step_avg:60.98ms
step:261/2330 train_time:15914ms step_avg:60.97ms
step:262/2330 train_time:15976ms step_avg:60.98ms
step:263/2330 train_time:16035ms step_avg:60.97ms
step:264/2330 train_time:16096ms step_avg:60.97ms
step:265/2330 train_time:16155ms step_avg:60.96ms
step:266/2330 train_time:16216ms step_avg:60.96ms
step:267/2330 train_time:16277ms step_avg:60.96ms
step:268/2330 train_time:16339ms step_avg:60.97ms
step:269/2330 train_time:16400ms step_avg:60.97ms
step:270/2330 train_time:16464ms step_avg:60.98ms
step:271/2330 train_time:16525ms step_avg:60.98ms
step:272/2330 train_time:16587ms step_avg:60.98ms
step:273/2330 train_time:16649ms step_avg:60.98ms
step:274/2330 train_time:16711ms step_avg:60.99ms
step:275/2330 train_time:16770ms step_avg:60.98ms
step:276/2330 train_time:16832ms step_avg:60.98ms
step:277/2330 train_time:16892ms step_avg:60.98ms
step:278/2330 train_time:16954ms step_avg:60.99ms
step:279/2330 train_time:17014ms step_avg:60.98ms
step:280/2330 train_time:17076ms step_avg:60.98ms
step:281/2330 train_time:17135ms step_avg:60.98ms
step:282/2330 train_time:17196ms step_avg:60.98ms
step:283/2330 train_time:17257ms step_avg:60.98ms
step:284/2330 train_time:17318ms step_avg:60.98ms
step:285/2330 train_time:17379ms step_avg:60.98ms
step:286/2330 train_time:17442ms step_avg:60.99ms
step:287/2330 train_time:17502ms step_avg:60.98ms
step:288/2330 train_time:17564ms step_avg:60.99ms
step:289/2330 train_time:17625ms step_avg:60.99ms
step:290/2330 train_time:17687ms step_avg:60.99ms
step:291/2330 train_time:17747ms step_avg:60.99ms
step:292/2330 train_time:17809ms step_avg:60.99ms
step:293/2330 train_time:17870ms step_avg:60.99ms
step:294/2330 train_time:17932ms step_avg:60.99ms
step:295/2330 train_time:17992ms step_avg:60.99ms
step:296/2330 train_time:18053ms step_avg:60.99ms
step:297/2330 train_time:18113ms step_avg:60.99ms
step:298/2330 train_time:18175ms step_avg:60.99ms
step:299/2330 train_time:18235ms step_avg:60.99ms
step:300/2330 train_time:18297ms step_avg:60.99ms
step:301/2330 train_time:18358ms step_avg:60.99ms
step:302/2330 train_time:18420ms step_avg:60.99ms
step:303/2330 train_time:18481ms step_avg:60.99ms
step:304/2330 train_time:18542ms step_avg:60.99ms
step:305/2330 train_time:18602ms step_avg:60.99ms
step:306/2330 train_time:18664ms step_avg:60.99ms
step:307/2330 train_time:18724ms step_avg:60.99ms
step:308/2330 train_time:18787ms step_avg:61.00ms
step:309/2330 train_time:18848ms step_avg:61.00ms
step:310/2330 train_time:18910ms step_avg:61.00ms
step:311/2330 train_time:18970ms step_avg:61.00ms
step:312/2330 train_time:19032ms step_avg:61.00ms
step:313/2330 train_time:19092ms step_avg:61.00ms
step:314/2330 train_time:19154ms step_avg:61.00ms
step:315/2330 train_time:19214ms step_avg:61.00ms
step:316/2330 train_time:19276ms step_avg:61.00ms
step:317/2330 train_time:19337ms step_avg:61.00ms
step:318/2330 train_time:19399ms step_avg:61.00ms
step:319/2330 train_time:19459ms step_avg:61.00ms
step:320/2330 train_time:19522ms step_avg:61.01ms
step:321/2330 train_time:19581ms step_avg:61.00ms
step:322/2330 train_time:19643ms step_avg:61.00ms
step:323/2330 train_time:19702ms step_avg:61.00ms
step:324/2330 train_time:19764ms step_avg:61.00ms
step:325/2330 train_time:19825ms step_avg:61.00ms
step:326/2330 train_time:19887ms step_avg:61.00ms
step:327/2330 train_time:19949ms step_avg:61.01ms
step:328/2330 train_time:20010ms step_avg:61.01ms
step:329/2330 train_time:20071ms step_avg:61.00ms
step:330/2330 train_time:20132ms step_avg:61.01ms
step:331/2330 train_time:20192ms step_avg:61.00ms
step:332/2330 train_time:20255ms step_avg:61.01ms
step:333/2330 train_time:20315ms step_avg:61.01ms
step:334/2330 train_time:20376ms step_avg:61.01ms
step:335/2330 train_time:20437ms step_avg:61.01ms
step:336/2330 train_time:20498ms step_avg:61.01ms
step:337/2330 train_time:20558ms step_avg:61.00ms
step:338/2330 train_time:20620ms step_avg:61.00ms
step:339/2330 train_time:20679ms step_avg:61.00ms
step:340/2330 train_time:20741ms step_avg:61.00ms
step:341/2330 train_time:20800ms step_avg:61.00ms
step:342/2330 train_time:20863ms step_avg:61.00ms
step:343/2330 train_time:20923ms step_avg:61.00ms
step:344/2330 train_time:20985ms step_avg:61.00ms
step:345/2330 train_time:21046ms step_avg:61.00ms
step:346/2330 train_time:21109ms step_avg:61.01ms
step:347/2330 train_time:21170ms step_avg:61.01ms
step:348/2330 train_time:21232ms step_avg:61.01ms
step:349/2330 train_time:21292ms step_avg:61.01ms
step:350/2330 train_time:21355ms step_avg:61.01ms
step:351/2330 train_time:21415ms step_avg:61.01ms
step:352/2330 train_time:21477ms step_avg:61.01ms
step:353/2330 train_time:21537ms step_avg:61.01ms
step:354/2330 train_time:21599ms step_avg:61.01ms
step:355/2330 train_time:21658ms step_avg:61.01ms
step:356/2330 train_time:21719ms step_avg:61.01ms
step:357/2330 train_time:21779ms step_avg:61.01ms
step:358/2330 train_time:21841ms step_avg:61.01ms
step:359/2330 train_time:21901ms step_avg:61.00ms
step:360/2330 train_time:21963ms step_avg:61.01ms
step:361/2330 train_time:22023ms step_avg:61.01ms
step:362/2330 train_time:22085ms step_avg:61.01ms
step:363/2330 train_time:22146ms step_avg:61.01ms
step:364/2330 train_time:22209ms step_avg:61.01ms
step:365/2330 train_time:22270ms step_avg:61.01ms
step:366/2330 train_time:22332ms step_avg:61.02ms
step:367/2330 train_time:22392ms step_avg:61.01ms
step:368/2330 train_time:22455ms step_avg:61.02ms
step:369/2330 train_time:22515ms step_avg:61.02ms
step:370/2330 train_time:22577ms step_avg:61.02ms
step:371/2330 train_time:22638ms step_avg:61.02ms
step:372/2330 train_time:22700ms step_avg:61.02ms
step:373/2330 train_time:22760ms step_avg:61.02ms
step:374/2330 train_time:22821ms step_avg:61.02ms
step:375/2330 train_time:22881ms step_avg:61.02ms
step:376/2330 train_time:22942ms step_avg:61.02ms
step:377/2330 train_time:23002ms step_avg:61.01ms
step:378/2330 train_time:23064ms step_avg:61.02ms
step:379/2330 train_time:23125ms step_avg:61.01ms
step:380/2330 train_time:23187ms step_avg:61.02ms
step:381/2330 train_time:23248ms step_avg:61.02ms
step:382/2330 train_time:23311ms step_avg:61.02ms
step:383/2330 train_time:23370ms step_avg:61.02ms
step:384/2330 train_time:23433ms step_avg:61.02ms
step:385/2330 train_time:23493ms step_avg:61.02ms
step:386/2330 train_time:23555ms step_avg:61.02ms
step:387/2330 train_time:23616ms step_avg:61.02ms
step:388/2330 train_time:23678ms step_avg:61.02ms
step:389/2330 train_time:23737ms step_avg:61.02ms
step:390/2330 train_time:23800ms step_avg:61.02ms
step:391/2330 train_time:23859ms step_avg:61.02ms
step:392/2330 train_time:23920ms step_avg:61.02ms
step:393/2330 train_time:23980ms step_avg:61.02ms
step:394/2330 train_time:24042ms step_avg:61.02ms
step:395/2330 train_time:24102ms step_avg:61.02ms
step:396/2330 train_time:24164ms step_avg:61.02ms
step:397/2330 train_time:24224ms step_avg:61.02ms
step:398/2330 train_time:24287ms step_avg:61.02ms
step:399/2330 train_time:24349ms step_avg:61.02ms
step:400/2330 train_time:24411ms step_avg:61.03ms
step:401/2330 train_time:24471ms step_avg:61.03ms
step:402/2330 train_time:24533ms step_avg:61.03ms
step:403/2330 train_time:24593ms step_avg:61.03ms
step:404/2330 train_time:24655ms step_avg:61.03ms
step:405/2330 train_time:24715ms step_avg:61.03ms
step:406/2330 train_time:24777ms step_avg:61.03ms
step:407/2330 train_time:24838ms step_avg:61.03ms
step:408/2330 train_time:24899ms step_avg:61.03ms
step:409/2330 train_time:24959ms step_avg:61.03ms
step:410/2330 train_time:25021ms step_avg:61.03ms
step:411/2330 train_time:25081ms step_avg:61.02ms
step:412/2330 train_time:25143ms step_avg:61.03ms
step:413/2330 train_time:25202ms step_avg:61.02ms
step:414/2330 train_time:25264ms step_avg:61.03ms
step:415/2330 train_time:25325ms step_avg:61.02ms
step:416/2330 train_time:25387ms step_avg:61.03ms
step:417/2330 train_time:25448ms step_avg:61.03ms
step:418/2330 train_time:25511ms step_avg:61.03ms
step:419/2330 train_time:25571ms step_avg:61.03ms
step:420/2330 train_time:25633ms step_avg:61.03ms
step:421/2330 train_time:25693ms step_avg:61.03ms
step:422/2330 train_time:25756ms step_avg:61.03ms
step:423/2330 train_time:25817ms step_avg:61.03ms
step:424/2330 train_time:25879ms step_avg:61.03ms
step:425/2330 train_time:25939ms step_avg:61.03ms
step:426/2330 train_time:26000ms step_avg:61.03ms
step:427/2330 train_time:26060ms step_avg:61.03ms
step:428/2330 train_time:26121ms step_avg:61.03ms
step:429/2330 train_time:26182ms step_avg:61.03ms
step:430/2330 train_time:26244ms step_avg:61.03ms
step:431/2330 train_time:26304ms step_avg:61.03ms
step:432/2330 train_time:26367ms step_avg:61.04ms
step:433/2330 train_time:26428ms step_avg:61.03ms
step:434/2330 train_time:26490ms step_avg:61.04ms
step:435/2330 train_time:26551ms step_avg:61.04ms
step:436/2330 train_time:26612ms step_avg:61.04ms
step:437/2330 train_time:26672ms step_avg:61.03ms
step:438/2330 train_time:26735ms step_avg:61.04ms
step:439/2330 train_time:26795ms step_avg:61.04ms
step:440/2330 train_time:26857ms step_avg:61.04ms
step:441/2330 train_time:26917ms step_avg:61.04ms
step:442/2330 train_time:26979ms step_avg:61.04ms
step:443/2330 train_time:27041ms step_avg:61.04ms
step:444/2330 train_time:27103ms step_avg:61.04ms
step:445/2330 train_time:27162ms step_avg:61.04ms
step:446/2330 train_time:27223ms step_avg:61.04ms
step:447/2330 train_time:27283ms step_avg:61.04ms
step:448/2330 train_time:27346ms step_avg:61.04ms
step:449/2330 train_time:27406ms step_avg:61.04ms
step:450/2330 train_time:27468ms step_avg:61.04ms
step:451/2330 train_time:27528ms step_avg:61.04ms
step:452/2330 train_time:27590ms step_avg:61.04ms
step:453/2330 train_time:27651ms step_avg:61.04ms
step:454/2330 train_time:27713ms step_avg:61.04ms
step:455/2330 train_time:27774ms step_avg:61.04ms
step:456/2330 train_time:27836ms step_avg:61.04ms
step:457/2330 train_time:27896ms step_avg:61.04ms
step:458/2330 train_time:27957ms step_avg:61.04ms
step:459/2330 train_time:28017ms step_avg:61.04ms
step:460/2330 train_time:28080ms step_avg:61.04ms
step:461/2330 train_time:28140ms step_avg:61.04ms
step:462/2330 train_time:28202ms step_avg:61.04ms
step:463/2330 train_time:28262ms step_avg:61.04ms
step:464/2330 train_time:28323ms step_avg:61.04ms
step:465/2330 train_time:28384ms step_avg:61.04ms
step:466/2330 train_time:28446ms step_avg:61.04ms
step:467/2330 train_time:28507ms step_avg:61.04ms
step:468/2330 train_time:28569ms step_avg:61.05ms
step:469/2330 train_time:28629ms step_avg:61.04ms
step:470/2330 train_time:28692ms step_avg:61.05ms
step:471/2330 train_time:28752ms step_avg:61.04ms
step:472/2330 train_time:28814ms step_avg:61.05ms
step:473/2330 train_time:28875ms step_avg:61.05ms
step:474/2330 train_time:28937ms step_avg:61.05ms
step:475/2330 train_time:28997ms step_avg:61.05ms
step:476/2330 train_time:29059ms step_avg:61.05ms
step:477/2330 train_time:29119ms step_avg:61.05ms
step:478/2330 train_time:29181ms step_avg:61.05ms
step:479/2330 train_time:29241ms step_avg:61.05ms
step:480/2330 train_time:29302ms step_avg:61.05ms
step:481/2330 train_time:29362ms step_avg:61.04ms
step:482/2330 train_time:29424ms step_avg:61.05ms
step:483/2330 train_time:29484ms step_avg:61.04ms
step:484/2330 train_time:29547ms step_avg:61.05ms
step:485/2330 train_time:29608ms step_avg:61.05ms
step:486/2330 train_time:29671ms step_avg:61.05ms
step:487/2330 train_time:29731ms step_avg:61.05ms
step:488/2330 train_time:29794ms step_avg:61.05ms
step:489/2330 train_time:29854ms step_avg:61.05ms
step:490/2330 train_time:29916ms step_avg:61.05ms
step:491/2330 train_time:29976ms step_avg:61.05ms
step:492/2330 train_time:30039ms step_avg:61.05ms
step:493/2330 train_time:30098ms step_avg:61.05ms
step:494/2330 train_time:30160ms step_avg:61.05ms
step:495/2330 train_time:30219ms step_avg:61.05ms
step:496/2330 train_time:30281ms step_avg:61.05ms
step:497/2330 train_time:30341ms step_avg:61.05ms
step:498/2330 train_time:30403ms step_avg:61.05ms
step:499/2330 train_time:30463ms step_avg:61.05ms
step:500/2330 train_time:30526ms step_avg:61.05ms
step:500/2330 val_loss:4.3309 train_time:30590ms step_avg:61.18ms
step:501/2330 train_time:30614ms step_avg:61.10ms
step:502/2330 train_time:30651ms step_avg:61.06ms
step:503/2330 train_time:30716ms step_avg:61.07ms
step:504/2330 train_time:30782ms step_avg:61.08ms
step:505/2330 train_time:30842ms step_avg:61.07ms
step:506/2330 train_time:30904ms step_avg:61.08ms
step:507/2330 train_time:30964ms step_avg:61.07ms
step:508/2330 train_time:31026ms step_avg:61.08ms
step:509/2330 train_time:31086ms step_avg:61.07ms
step:510/2330 train_time:31147ms step_avg:61.07ms
step:511/2330 train_time:31206ms step_avg:61.07ms
step:512/2330 train_time:31267ms step_avg:61.07ms
step:513/2330 train_time:31327ms step_avg:61.07ms
step:514/2330 train_time:31388ms step_avg:61.07ms
step:515/2330 train_time:31448ms step_avg:61.06ms
step:516/2330 train_time:31509ms step_avg:61.06ms
step:517/2330 train_time:31569ms step_avg:61.06ms
step:518/2330 train_time:31631ms step_avg:61.06ms
step:519/2330 train_time:31693ms step_avg:61.07ms
step:520/2330 train_time:31756ms step_avg:61.07ms
step:521/2330 train_time:31817ms step_avg:61.07ms
step:522/2330 train_time:31881ms step_avg:61.07ms
step:523/2330 train_time:31941ms step_avg:61.07ms
step:524/2330 train_time:32004ms step_avg:61.08ms
step:525/2330 train_time:32063ms step_avg:61.07ms
step:526/2330 train_time:32124ms step_avg:61.07ms
step:527/2330 train_time:32184ms step_avg:61.07ms
step:528/2330 train_time:32245ms step_avg:61.07ms
step:529/2330 train_time:32305ms step_avg:61.07ms
step:530/2330 train_time:32367ms step_avg:61.07ms
step:531/2330 train_time:32426ms step_avg:61.07ms
step:532/2330 train_time:32488ms step_avg:61.07ms
step:533/2330 train_time:32549ms step_avg:61.07ms
step:534/2330 train_time:32611ms step_avg:61.07ms
step:535/2330 train_time:32672ms step_avg:61.07ms
step:536/2330 train_time:32734ms step_avg:61.07ms
step:537/2330 train_time:32794ms step_avg:61.07ms
step:538/2330 train_time:32857ms step_avg:61.07ms
step:539/2330 train_time:32918ms step_avg:61.07ms
step:540/2330 train_time:32981ms step_avg:61.08ms
step:541/2330 train_time:33041ms step_avg:61.07ms
step:542/2330 train_time:33103ms step_avg:61.08ms
step:543/2330 train_time:33163ms step_avg:61.07ms
step:544/2330 train_time:33225ms step_avg:61.08ms
step:545/2330 train_time:33284ms step_avg:61.07ms
step:546/2330 train_time:33346ms step_avg:61.07ms
step:547/2330 train_time:33406ms step_avg:61.07ms
step:548/2330 train_time:33469ms step_avg:61.07ms
step:549/2330 train_time:33528ms step_avg:61.07ms
step:550/2330 train_time:33590ms step_avg:61.07ms
step:551/2330 train_time:33651ms step_avg:61.07ms
step:552/2330 train_time:33713ms step_avg:61.07ms
step:553/2330 train_time:33773ms step_avg:61.07ms
step:554/2330 train_time:33835ms step_avg:61.07ms
step:555/2330 train_time:33895ms step_avg:61.07ms
step:556/2330 train_time:33958ms step_avg:61.08ms
step:557/2330 train_time:34020ms step_avg:61.08ms
step:558/2330 train_time:34082ms step_avg:61.08ms
step:559/2330 train_time:34142ms step_avg:61.08ms
step:560/2330 train_time:34204ms step_avg:61.08ms
step:561/2330 train_time:34263ms step_avg:61.08ms
step:562/2330 train_time:34325ms step_avg:61.08ms
step:563/2330 train_time:34385ms step_avg:61.07ms
step:564/2330 train_time:34447ms step_avg:61.08ms
step:565/2330 train_time:34507ms step_avg:61.07ms
step:566/2330 train_time:34570ms step_avg:61.08ms
step:567/2330 train_time:34629ms step_avg:61.07ms
step:568/2330 train_time:34692ms step_avg:61.08ms
step:569/2330 train_time:34751ms step_avg:61.07ms
step:570/2330 train_time:34813ms step_avg:61.08ms
step:571/2330 train_time:34873ms step_avg:61.07ms
step:572/2330 train_time:34936ms step_avg:61.08ms
step:573/2330 train_time:34997ms step_avg:61.08ms
step:574/2330 train_time:35059ms step_avg:61.08ms
step:575/2330 train_time:35121ms step_avg:61.08ms
step:576/2330 train_time:35184ms step_avg:61.08ms
step:577/2330 train_time:35244ms step_avg:61.08ms
step:578/2330 train_time:35306ms step_avg:61.08ms
step:579/2330 train_time:35365ms step_avg:61.08ms
step:580/2330 train_time:35428ms step_avg:61.08ms
step:581/2330 train_time:35488ms step_avg:61.08ms
step:582/2330 train_time:35551ms step_avg:61.08ms
step:583/2330 train_time:35611ms step_avg:61.08ms
step:584/2330 train_time:35673ms step_avg:61.08ms
step:585/2330 train_time:35733ms step_avg:61.08ms
step:586/2330 train_time:35794ms step_avg:61.08ms
step:587/2330 train_time:35854ms step_avg:61.08ms
step:588/2330 train_time:35916ms step_avg:61.08ms
step:589/2330 train_time:35977ms step_avg:61.08ms
step:590/2330 train_time:36041ms step_avg:61.09ms
step:591/2330 train_time:36101ms step_avg:61.08ms
step:592/2330 train_time:36164ms step_avg:61.09ms
step:593/2330 train_time:36224ms step_avg:61.09ms
step:594/2330 train_time:36286ms step_avg:61.09ms
step:595/2330 train_time:36346ms step_avg:61.09ms
step:596/2330 train_time:36407ms step_avg:61.09ms
step:597/2330 train_time:36468ms step_avg:61.08ms
step:598/2330 train_time:36529ms step_avg:61.09ms
step:599/2330 train_time:36589ms step_avg:61.08ms
step:600/2330 train_time:36651ms step_avg:61.09ms
step:601/2330 train_time:36711ms step_avg:61.08ms
step:602/2330 train_time:36773ms step_avg:61.09ms
step:603/2330 train_time:36833ms step_avg:61.08ms
step:604/2330 train_time:36895ms step_avg:61.08ms
step:605/2330 train_time:36955ms step_avg:61.08ms
step:606/2330 train_time:37017ms step_avg:61.08ms
step:607/2330 train_time:37078ms step_avg:61.08ms
step:608/2330 train_time:37141ms step_avg:61.09ms
step:609/2330 train_time:37201ms step_avg:61.09ms
step:610/2330 train_time:37263ms step_avg:61.09ms
step:611/2330 train_time:37324ms step_avg:61.09ms
step:612/2330 train_time:37387ms step_avg:61.09ms
step:613/2330 train_time:37447ms step_avg:61.09ms
step:614/2330 train_time:37509ms step_avg:61.09ms
step:615/2330 train_time:37568ms step_avg:61.09ms
step:616/2330 train_time:37630ms step_avg:61.09ms
step:617/2330 train_time:37690ms step_avg:61.09ms
step:618/2330 train_time:37753ms step_avg:61.09ms
step:619/2330 train_time:37813ms step_avg:61.09ms
step:620/2330 train_time:37875ms step_avg:61.09ms
step:621/2330 train_time:37935ms step_avg:61.09ms
step:622/2330 train_time:37997ms step_avg:61.09ms
step:623/2330 train_time:38057ms step_avg:61.09ms
step:624/2330 train_time:38121ms step_avg:61.09ms
step:625/2330 train_time:38182ms step_avg:61.09ms
step:626/2330 train_time:38244ms step_avg:61.09ms
step:627/2330 train_time:38304ms step_avg:61.09ms
step:628/2330 train_time:38367ms step_avg:61.09ms
step:629/2330 train_time:38427ms step_avg:61.09ms
step:630/2330 train_time:38489ms step_avg:61.09ms
step:631/2330 train_time:38549ms step_avg:61.09ms
step:632/2330 train_time:38611ms step_avg:61.09ms
step:633/2330 train_time:38671ms step_avg:61.09ms
step:634/2330 train_time:38732ms step_avg:61.09ms
step:635/2330 train_time:38792ms step_avg:61.09ms
step:636/2330 train_time:38854ms step_avg:61.09ms
step:637/2330 train_time:38914ms step_avg:61.09ms
step:638/2330 train_time:38976ms step_avg:61.09ms
step:639/2330 train_time:39036ms step_avg:61.09ms
step:640/2330 train_time:39099ms step_avg:61.09ms
step:641/2330 train_time:39159ms step_avg:61.09ms
step:642/2330 train_time:39222ms step_avg:61.09ms
step:643/2330 train_time:39282ms step_avg:61.09ms
step:644/2330 train_time:39344ms step_avg:61.09ms
step:645/2330 train_time:39405ms step_avg:61.09ms
step:646/2330 train_time:39467ms step_avg:61.09ms
step:647/2330 train_time:39527ms step_avg:61.09ms
step:648/2330 train_time:39589ms step_avg:61.09ms
step:649/2330 train_time:39649ms step_avg:61.09ms
step:650/2330 train_time:39711ms step_avg:61.09ms
step:651/2330 train_time:39771ms step_avg:61.09ms
step:652/2330 train_time:39832ms step_avg:61.09ms
step:653/2330 train_time:39892ms step_avg:61.09ms
step:654/2330 train_time:39954ms step_avg:61.09ms
step:655/2330 train_time:40014ms step_avg:61.09ms
step:656/2330 train_time:40076ms step_avg:61.09ms
step:657/2330 train_time:40137ms step_avg:61.09ms
step:658/2330 train_time:40200ms step_avg:61.09ms
step:659/2330 train_time:40261ms step_avg:61.09ms
step:660/2330 train_time:40323ms step_avg:61.09ms
step:661/2330 train_time:40382ms step_avg:61.09ms
step:662/2330 train_time:40445ms step_avg:61.09ms
step:663/2330 train_time:40505ms step_avg:61.09ms
step:664/2330 train_time:40568ms step_avg:61.10ms
step:665/2330 train_time:40628ms step_avg:61.09ms
step:666/2330 train_time:40690ms step_avg:61.10ms
step:667/2330 train_time:40750ms step_avg:61.09ms
step:668/2330 train_time:40812ms step_avg:61.10ms
step:669/2330 train_time:40873ms step_avg:61.10ms
step:670/2330 train_time:40935ms step_avg:61.10ms
step:671/2330 train_time:40995ms step_avg:61.09ms
step:672/2330 train_time:41057ms step_avg:61.10ms
step:673/2330 train_time:41117ms step_avg:61.10ms
step:674/2330 train_time:41181ms step_avg:61.10ms
step:675/2330 train_time:41241ms step_avg:61.10ms
step:676/2330 train_time:41303ms step_avg:61.10ms
step:677/2330 train_time:41364ms step_avg:61.10ms
step:678/2330 train_time:41426ms step_avg:61.10ms
step:679/2330 train_time:41486ms step_avg:61.10ms
step:680/2330 train_time:41548ms step_avg:61.10ms
step:681/2330 train_time:41608ms step_avg:61.10ms
step:682/2330 train_time:41670ms step_avg:61.10ms
step:683/2330 train_time:41730ms step_avg:61.10ms
step:684/2330 train_time:41792ms step_avg:61.10ms
step:685/2330 train_time:41853ms step_avg:61.10ms
step:686/2330 train_time:41915ms step_avg:61.10ms
step:687/2330 train_time:41974ms step_avg:61.10ms
step:688/2330 train_time:42037ms step_avg:61.10ms
step:689/2330 train_time:42097ms step_avg:61.10ms
step:690/2330 train_time:42159ms step_avg:61.10ms
step:691/2330 train_time:42219ms step_avg:61.10ms
step:692/2330 train_time:42283ms step_avg:61.10ms
step:693/2330 train_time:42343ms step_avg:61.10ms
step:694/2330 train_time:42406ms step_avg:61.10ms
step:695/2330 train_time:42466ms step_avg:61.10ms
step:696/2330 train_time:42528ms step_avg:61.10ms
step:697/2330 train_time:42588ms step_avg:61.10ms
step:698/2330 train_time:42650ms step_avg:61.10ms
step:699/2330 train_time:42710ms step_avg:61.10ms
step:700/2330 train_time:42772ms step_avg:61.10ms
step:701/2330 train_time:42832ms step_avg:61.10ms
step:702/2330 train_time:42894ms step_avg:61.10ms
step:703/2330 train_time:42954ms step_avg:61.10ms
step:704/2330 train_time:43016ms step_avg:61.10ms
step:705/2330 train_time:43077ms step_avg:61.10ms
step:706/2330 train_time:43139ms step_avg:61.10ms
step:707/2330 train_time:43200ms step_avg:61.10ms
step:708/2330 train_time:43263ms step_avg:61.11ms
step:709/2330 train_time:43323ms step_avg:61.10ms
step:710/2330 train_time:43385ms step_avg:61.11ms
step:711/2330 train_time:43445ms step_avg:61.10ms
step:712/2330 train_time:43507ms step_avg:61.11ms
step:713/2330 train_time:43567ms step_avg:61.10ms
step:714/2330 train_time:43629ms step_avg:61.10ms
step:715/2330 train_time:43689ms step_avg:61.10ms
step:716/2330 train_time:43750ms step_avg:61.10ms
step:717/2330 train_time:43811ms step_avg:61.10ms
step:718/2330 train_time:43874ms step_avg:61.11ms
step:719/2330 train_time:43934ms step_avg:61.10ms
step:720/2330 train_time:43995ms step_avg:61.10ms
step:721/2330 train_time:44055ms step_avg:61.10ms
step:722/2330 train_time:44117ms step_avg:61.10ms
step:723/2330 train_time:44177ms step_avg:61.10ms
step:724/2330 train_time:44241ms step_avg:61.11ms
step:725/2330 train_time:44301ms step_avg:61.11ms
step:726/2330 train_time:44363ms step_avg:61.11ms
step:727/2330 train_time:44424ms step_avg:61.11ms
step:728/2330 train_time:44486ms step_avg:61.11ms
step:729/2330 train_time:44546ms step_avg:61.11ms
step:730/2330 train_time:44608ms step_avg:61.11ms
step:731/2330 train_time:44669ms step_avg:61.11ms
step:732/2330 train_time:44731ms step_avg:61.11ms
step:733/2330 train_time:44792ms step_avg:61.11ms
step:734/2330 train_time:44853ms step_avg:61.11ms
step:735/2330 train_time:44913ms step_avg:61.11ms
step:736/2330 train_time:44976ms step_avg:61.11ms
step:737/2330 train_time:45035ms step_avg:61.11ms
step:738/2330 train_time:45097ms step_avg:61.11ms
step:739/2330 train_time:45157ms step_avg:61.11ms
step:740/2330 train_time:45220ms step_avg:61.11ms
step:741/2330 train_time:45281ms step_avg:61.11ms
step:742/2330 train_time:45343ms step_avg:61.11ms
step:743/2330 train_time:45403ms step_avg:61.11ms
step:744/2330 train_time:45465ms step_avg:61.11ms
step:745/2330 train_time:45525ms step_avg:61.11ms
step:746/2330 train_time:45588ms step_avg:61.11ms
step:747/2330 train_time:45648ms step_avg:61.11ms
step:748/2330 train_time:45710ms step_avg:61.11ms
step:749/2330 train_time:45769ms step_avg:61.11ms
step:750/2330 train_time:45831ms step_avg:61.11ms
step:750/2330 val_loss:4.0701 train_time:45896ms step_avg:61.19ms
step:751/2330 train_time:45920ms step_avg:61.15ms
step:752/2330 train_time:45956ms step_avg:61.11ms
step:753/2330 train_time:46021ms step_avg:61.12ms
step:754/2330 train_time:46087ms step_avg:61.12ms
step:755/2330 train_time:46147ms step_avg:61.12ms
step:756/2330 train_time:46209ms step_avg:61.12ms
step:757/2330 train_time:46268ms step_avg:61.12ms
step:758/2330 train_time:46329ms step_avg:61.12ms
step:759/2330 train_time:46388ms step_avg:61.12ms
step:760/2330 train_time:46450ms step_avg:61.12ms
step:761/2330 train_time:46509ms step_avg:61.12ms
step:762/2330 train_time:46570ms step_avg:61.12ms
step:763/2330 train_time:46630ms step_avg:61.11ms
step:764/2330 train_time:46691ms step_avg:61.11ms
step:765/2330 train_time:46750ms step_avg:61.11ms
step:766/2330 train_time:46813ms step_avg:61.11ms
step:767/2330 train_time:46873ms step_avg:61.11ms
step:768/2330 train_time:46937ms step_avg:61.12ms
step:769/2330 train_time:46999ms step_avg:61.12ms
step:770/2330 train_time:47062ms step_avg:61.12ms
step:771/2330 train_time:47124ms step_avg:61.12ms
step:772/2330 train_time:47187ms step_avg:61.12ms
step:773/2330 train_time:47248ms step_avg:61.12ms
step:774/2330 train_time:47311ms step_avg:61.12ms
step:775/2330 train_time:47370ms step_avg:61.12ms
step:776/2330 train_time:47433ms step_avg:61.12ms
step:777/2330 train_time:47493ms step_avg:61.12ms
step:778/2330 train_time:47555ms step_avg:61.13ms
step:779/2330 train_time:47616ms step_avg:61.12ms
step:780/2330 train_time:47678ms step_avg:61.13ms
step:781/2330 train_time:47738ms step_avg:61.12ms
step:782/2330 train_time:47801ms step_avg:61.13ms
step:783/2330 train_time:47861ms step_avg:61.13ms
step:784/2330 train_time:47925ms step_avg:61.13ms
step:785/2330 train_time:47986ms step_avg:61.13ms
step:786/2330 train_time:48049ms step_avg:61.13ms
step:787/2330 train_time:48110ms step_avg:61.13ms
step:788/2330 train_time:48173ms step_avg:61.13ms
step:789/2330 train_time:48234ms step_avg:61.13ms
step:790/2330 train_time:48298ms step_avg:61.14ms
step:791/2330 train_time:48358ms step_avg:61.14ms
step:792/2330 train_time:48421ms step_avg:61.14ms
step:793/2330 train_time:48481ms step_avg:61.14ms
step:794/2330 train_time:48543ms step_avg:61.14ms
step:795/2330 train_time:48604ms step_avg:61.14ms
step:796/2330 train_time:48667ms step_avg:61.14ms
step:797/2330 train_time:48727ms step_avg:61.14ms
step:798/2330 train_time:48789ms step_avg:61.14ms
step:799/2330 train_time:48849ms step_avg:61.14ms
step:800/2330 train_time:48912ms step_avg:61.14ms
step:801/2330 train_time:48973ms step_avg:61.14ms
step:802/2330 train_time:49036ms step_avg:61.14ms
step:803/2330 train_time:49097ms step_avg:61.14ms
step:804/2330 train_time:49161ms step_avg:61.15ms
step:805/2330 train_time:49223ms step_avg:61.15ms
step:806/2330 train_time:49286ms step_avg:61.15ms
step:807/2330 train_time:49347ms step_avg:61.15ms
step:808/2330 train_time:49410ms step_avg:61.15ms
step:809/2330 train_time:49470ms step_avg:61.15ms
step:810/2330 train_time:49533ms step_avg:61.15ms
step:811/2330 train_time:49593ms step_avg:61.15ms
step:812/2330 train_time:49656ms step_avg:61.15ms
step:813/2330 train_time:49716ms step_avg:61.15ms
step:814/2330 train_time:49778ms step_avg:61.15ms
step:815/2330 train_time:49839ms step_avg:61.15ms
step:816/2330 train_time:49902ms step_avg:61.15ms
step:817/2330 train_time:49963ms step_avg:61.15ms
step:818/2330 train_time:50027ms step_avg:61.16ms
step:819/2330 train_time:50087ms step_avg:61.16ms
step:820/2330 train_time:50150ms step_avg:61.16ms
step:821/2330 train_time:50210ms step_avg:61.16ms
step:822/2330 train_time:50273ms step_avg:61.16ms
step:823/2330 train_time:50334ms step_avg:61.16ms
step:824/2330 train_time:50397ms step_avg:61.16ms
step:825/2330 train_time:50458ms step_avg:61.16ms
step:826/2330 train_time:50521ms step_avg:61.16ms
step:827/2330 train_time:50582ms step_avg:61.16ms
step:828/2330 train_time:50645ms step_avg:61.17ms
step:829/2330 train_time:50706ms step_avg:61.17ms
step:830/2330 train_time:50768ms step_avg:61.17ms
step:831/2330 train_time:50829ms step_avg:61.17ms
step:832/2330 train_time:50891ms step_avg:61.17ms
step:833/2330 train_time:50951ms step_avg:61.17ms
step:834/2330 train_time:51013ms step_avg:61.17ms
step:835/2330 train_time:51074ms step_avg:61.17ms
step:836/2330 train_time:51137ms step_avg:61.17ms
step:837/2330 train_time:51198ms step_avg:61.17ms
step:838/2330 train_time:51261ms step_avg:61.17ms
step:839/2330 train_time:51322ms step_avg:61.17ms
step:840/2330 train_time:51385ms step_avg:61.17ms
step:841/2330 train_time:51446ms step_avg:61.17ms
step:842/2330 train_time:51509ms step_avg:61.17ms
step:843/2330 train_time:51569ms step_avg:61.17ms
step:844/2330 train_time:51631ms step_avg:61.17ms
step:845/2330 train_time:51691ms step_avg:61.17ms
step:846/2330 train_time:51754ms step_avg:61.17ms
step:847/2330 train_time:51815ms step_avg:61.17ms
step:848/2330 train_time:51877ms step_avg:61.18ms
step:849/2330 train_time:51937ms step_avg:61.17ms
step:850/2330 train_time:52000ms step_avg:61.18ms
step:851/2330 train_time:52061ms step_avg:61.18ms
step:852/2330 train_time:52124ms step_avg:61.18ms
step:853/2330 train_time:52186ms step_avg:61.18ms
step:854/2330 train_time:52248ms step_avg:61.18ms
step:855/2330 train_time:52309ms step_avg:61.18ms
step:856/2330 train_time:52371ms step_avg:61.18ms
step:857/2330 train_time:52432ms step_avg:61.18ms
step:858/2330 train_time:52494ms step_avg:61.18ms
step:859/2330 train_time:52555ms step_avg:61.18ms
step:860/2330 train_time:52618ms step_avg:61.18ms
step:861/2330 train_time:52679ms step_avg:61.18ms
step:862/2330 train_time:52741ms step_avg:61.18ms
step:863/2330 train_time:52802ms step_avg:61.18ms
step:864/2330 train_time:52865ms step_avg:61.19ms
step:865/2330 train_time:52926ms step_avg:61.19ms
step:866/2330 train_time:52988ms step_avg:61.19ms
step:867/2330 train_time:53048ms step_avg:61.19ms
step:868/2330 train_time:53111ms step_avg:61.19ms
step:869/2330 train_time:53172ms step_avg:61.19ms
step:870/2330 train_time:53235ms step_avg:61.19ms
step:871/2330 train_time:53296ms step_avg:61.19ms
step:872/2330 train_time:53359ms step_avg:61.19ms
step:873/2330 train_time:53420ms step_avg:61.19ms
step:874/2330 train_time:53484ms step_avg:61.19ms
step:875/2330 train_time:53545ms step_avg:61.19ms
step:876/2330 train_time:53609ms step_avg:61.20ms
step:877/2330 train_time:53669ms step_avg:61.20ms
step:878/2330 train_time:53731ms step_avg:61.20ms
step:879/2330 train_time:53791ms step_avg:61.20ms
step:880/2330 train_time:53854ms step_avg:61.20ms
step:881/2330 train_time:53915ms step_avg:61.20ms
step:882/2330 train_time:53978ms step_avg:61.20ms
step:883/2330 train_time:54039ms step_avg:61.20ms
step:884/2330 train_time:54102ms step_avg:61.20ms
step:885/2330 train_time:54162ms step_avg:61.20ms
step:886/2330 train_time:54226ms step_avg:61.20ms
step:887/2330 train_time:54287ms step_avg:61.20ms
step:888/2330 train_time:54349ms step_avg:61.20ms
step:889/2330 train_time:54410ms step_avg:61.20ms
step:890/2330 train_time:54473ms step_avg:61.21ms
step:891/2330 train_time:54533ms step_avg:61.20ms
step:892/2330 train_time:54595ms step_avg:61.21ms
step:893/2330 train_time:54657ms step_avg:61.21ms
step:894/2330 train_time:54721ms step_avg:61.21ms
step:895/2330 train_time:54781ms step_avg:61.21ms
step:896/2330 train_time:54844ms step_avg:61.21ms
step:897/2330 train_time:54905ms step_avg:61.21ms
step:898/2330 train_time:54968ms step_avg:61.21ms
step:899/2330 train_time:55028ms step_avg:61.21ms
step:900/2330 train_time:55090ms step_avg:61.21ms
step:901/2330 train_time:55151ms step_avg:61.21ms
step:902/2330 train_time:55213ms step_avg:61.21ms
step:903/2330 train_time:55274ms step_avg:61.21ms
step:904/2330 train_time:55337ms step_avg:61.21ms
step:905/2330 train_time:55397ms step_avg:61.21ms
step:906/2330 train_time:55460ms step_avg:61.21ms
step:907/2330 train_time:55521ms step_avg:61.21ms
step:908/2330 train_time:55584ms step_avg:61.22ms
step:909/2330 train_time:55645ms step_avg:61.22ms
step:910/2330 train_time:55708ms step_avg:61.22ms
step:911/2330 train_time:55769ms step_avg:61.22ms
step:912/2330 train_time:55831ms step_avg:61.22ms
step:913/2330 train_time:55891ms step_avg:61.22ms
step:914/2330 train_time:55953ms step_avg:61.22ms
step:915/2330 train_time:56014ms step_avg:61.22ms
step:916/2330 train_time:56077ms step_avg:61.22ms
step:917/2330 train_time:56138ms step_avg:61.22ms
step:918/2330 train_time:56201ms step_avg:61.22ms
step:919/2330 train_time:56261ms step_avg:61.22ms
step:920/2330 train_time:56325ms step_avg:61.22ms
step:921/2330 train_time:56386ms step_avg:61.22ms
step:922/2330 train_time:56449ms step_avg:61.22ms
step:923/2330 train_time:56509ms step_avg:61.22ms
step:924/2330 train_time:56572ms step_avg:61.22ms
step:925/2330 train_time:56632ms step_avg:61.22ms
step:926/2330 train_time:56695ms step_avg:61.23ms
step:927/2330 train_time:56756ms step_avg:61.23ms
step:928/2330 train_time:56819ms step_avg:61.23ms
step:929/2330 train_time:56880ms step_avg:61.23ms
step:930/2330 train_time:56942ms step_avg:61.23ms
step:931/2330 train_time:57003ms step_avg:61.23ms
step:932/2330 train_time:57066ms step_avg:61.23ms
step:933/2330 train_time:57127ms step_avg:61.23ms
step:934/2330 train_time:57189ms step_avg:61.23ms
step:935/2330 train_time:57249ms step_avg:61.23ms
step:936/2330 train_time:57312ms step_avg:61.23ms
step:937/2330 train_time:57373ms step_avg:61.23ms
step:938/2330 train_time:57435ms step_avg:61.23ms
step:939/2330 train_time:57496ms step_avg:61.23ms
step:940/2330 train_time:57559ms step_avg:61.23ms
step:941/2330 train_time:57621ms step_avg:61.23ms
step:942/2330 train_time:57684ms step_avg:61.24ms
step:943/2330 train_time:57744ms step_avg:61.23ms
step:944/2330 train_time:57808ms step_avg:61.24ms
step:945/2330 train_time:57868ms step_avg:61.24ms
step:946/2330 train_time:57931ms step_avg:61.24ms
step:947/2330 train_time:57991ms step_avg:61.24ms
step:948/2330 train_time:58053ms step_avg:61.24ms
step:949/2330 train_time:58114ms step_avg:61.24ms
step:950/2330 train_time:58176ms step_avg:61.24ms
step:951/2330 train_time:58237ms step_avg:61.24ms
step:952/2330 train_time:58300ms step_avg:61.24ms
step:953/2330 train_time:58362ms step_avg:61.24ms
step:954/2330 train_time:58426ms step_avg:61.24ms
step:955/2330 train_time:58487ms step_avg:61.24ms
step:956/2330 train_time:58549ms step_avg:61.24ms
step:957/2330 train_time:58610ms step_avg:61.24ms
step:958/2330 train_time:58672ms step_avg:61.24ms
step:959/2330 train_time:58733ms step_avg:61.24ms
step:960/2330 train_time:58796ms step_avg:61.25ms
step:961/2330 train_time:58856ms step_avg:61.25ms
step:962/2330 train_time:58919ms step_avg:61.25ms
step:963/2330 train_time:58980ms step_avg:61.25ms
step:964/2330 train_time:59042ms step_avg:61.25ms
step:965/2330 train_time:59103ms step_avg:61.25ms
step:966/2330 train_time:59166ms step_avg:61.25ms
step:967/2330 train_time:59227ms step_avg:61.25ms
step:968/2330 train_time:59290ms step_avg:61.25ms
step:969/2330 train_time:59350ms step_avg:61.25ms
step:970/2330 train_time:59412ms step_avg:61.25ms
step:971/2330 train_time:59473ms step_avg:61.25ms
step:972/2330 train_time:59536ms step_avg:61.25ms
step:973/2330 train_time:59596ms step_avg:61.25ms
step:974/2330 train_time:59658ms step_avg:61.25ms
step:975/2330 train_time:59719ms step_avg:61.25ms
step:976/2330 train_time:59782ms step_avg:61.25ms
step:977/2330 train_time:59844ms step_avg:61.25ms
step:978/2330 train_time:59907ms step_avg:61.25ms
step:979/2330 train_time:59967ms step_avg:61.25ms
step:980/2330 train_time:60030ms step_avg:61.26ms
step:981/2330 train_time:60091ms step_avg:61.25ms
step:982/2330 train_time:60153ms step_avg:61.26ms
step:983/2330 train_time:60215ms step_avg:61.26ms
step:984/2330 train_time:60278ms step_avg:61.26ms
step:985/2330 train_time:60339ms step_avg:61.26ms
step:986/2330 train_time:60402ms step_avg:61.26ms
step:987/2330 train_time:60463ms step_avg:61.26ms
step:988/2330 train_time:60526ms step_avg:61.26ms
step:989/2330 train_time:60586ms step_avg:61.26ms
step:990/2330 train_time:60649ms step_avg:61.26ms
step:991/2330 train_time:60709ms step_avg:61.26ms
step:992/2330 train_time:60771ms step_avg:61.26ms
step:993/2330 train_time:60832ms step_avg:61.26ms
step:994/2330 train_time:60895ms step_avg:61.26ms
step:995/2330 train_time:60955ms step_avg:61.26ms
step:996/2330 train_time:61018ms step_avg:61.26ms
step:997/2330 train_time:61079ms step_avg:61.26ms
step:998/2330 train_time:61142ms step_avg:61.26ms
step:999/2330 train_time:61203ms step_avg:61.26ms
step:1000/2330 train_time:61265ms step_avg:61.27ms
step:1000/2330 val_loss:3.8848 train_time:61331ms step_avg:61.33ms
step:1001/2330 train_time:61353ms step_avg:61.29ms
step:1002/2330 train_time:61393ms step_avg:61.27ms
step:1003/2330 train_time:61461ms step_avg:61.28ms
step:1004/2330 train_time:61524ms step_avg:61.28ms
step:1005/2330 train_time:61585ms step_avg:61.28ms
step:1006/2330 train_time:61648ms step_avg:61.28ms
step:1007/2330 train_time:61708ms step_avg:61.28ms
step:1008/2330 train_time:61770ms step_avg:61.28ms
step:1009/2330 train_time:61830ms step_avg:61.28ms
step:1010/2330 train_time:61892ms step_avg:61.28ms
step:1011/2330 train_time:61952ms step_avg:61.28ms
step:1012/2330 train_time:62014ms step_avg:61.28ms
step:1013/2330 train_time:62074ms step_avg:61.28ms
step:1014/2330 train_time:62136ms step_avg:61.28ms
step:1015/2330 train_time:62196ms step_avg:61.28ms
step:1016/2330 train_time:62261ms step_avg:61.28ms
step:1017/2330 train_time:62325ms step_avg:61.28ms
step:1018/2330 train_time:62390ms step_avg:61.29ms
step:1019/2330 train_time:62454ms step_avg:61.29ms
step:1020/2330 train_time:62517ms step_avg:61.29ms
step:1021/2330 train_time:62579ms step_avg:61.29ms
step:1022/2330 train_time:62642ms step_avg:61.29ms
step:1023/2330 train_time:62703ms step_avg:61.29ms
step:1024/2330 train_time:62764ms step_avg:61.29ms
step:1025/2330 train_time:62824ms step_avg:61.29ms
step:1026/2330 train_time:62887ms step_avg:61.29ms
step:1027/2330 train_time:62947ms step_avg:61.29ms
step:1028/2330 train_time:63009ms step_avg:61.29ms
step:1029/2330 train_time:63069ms step_avg:61.29ms
step:1030/2330 train_time:63132ms step_avg:61.29ms
step:1031/2330 train_time:63193ms step_avg:61.29ms
step:1032/2330 train_time:63257ms step_avg:61.30ms
step:1033/2330 train_time:63318ms step_avg:61.29ms
step:1034/2330 train_time:63381ms step_avg:61.30ms
step:1035/2330 train_time:63443ms step_avg:61.30ms
step:1036/2330 train_time:63507ms step_avg:61.30ms
step:1037/2330 train_time:63568ms step_avg:61.30ms
step:1038/2330 train_time:63632ms step_avg:61.30ms
step:1039/2330 train_time:63693ms step_avg:61.30ms
step:1040/2330 train_time:63756ms step_avg:61.30ms
step:1041/2330 train_time:63817ms step_avg:61.30ms
step:1042/2330 train_time:63881ms step_avg:61.31ms
step:1043/2330 train_time:63941ms step_avg:61.30ms
step:1044/2330 train_time:64003ms step_avg:61.31ms
step:1045/2330 train_time:64063ms step_avg:61.30ms
step:1046/2330 train_time:64125ms step_avg:61.30ms
step:1047/2330 train_time:64186ms step_avg:61.30ms
step:1048/2330 train_time:64249ms step_avg:61.31ms
step:1049/2330 train_time:64311ms step_avg:61.31ms
step:1050/2330 train_time:64374ms step_avg:61.31ms
step:1051/2330 train_time:64436ms step_avg:61.31ms
step:1052/2330 train_time:64499ms step_avg:61.31ms
step:1053/2330 train_time:64560ms step_avg:61.31ms
step:1054/2330 train_time:64622ms step_avg:61.31ms
step:1055/2330 train_time:64683ms step_avg:61.31ms
step:1056/2330 train_time:64746ms step_avg:61.31ms
step:1057/2330 train_time:64807ms step_avg:61.31ms
step:1058/2330 train_time:64870ms step_avg:61.31ms
step:1059/2330 train_time:64930ms step_avg:61.31ms
step:1060/2330 train_time:64992ms step_avg:61.31ms
step:1061/2330 train_time:65053ms step_avg:61.31ms
step:1062/2330 train_time:65116ms step_avg:61.31ms
step:1063/2330 train_time:65177ms step_avg:61.31ms
step:1064/2330 train_time:65240ms step_avg:61.32ms
step:1065/2330 train_time:65301ms step_avg:61.32ms
step:1066/2330 train_time:65365ms step_avg:61.32ms
step:1067/2330 train_time:65426ms step_avg:61.32ms
step:1068/2330 train_time:65489ms step_avg:61.32ms
step:1069/2330 train_time:65551ms step_avg:61.32ms
step:1070/2330 train_time:65614ms step_avg:61.32ms
step:1071/2330 train_time:65676ms step_avg:61.32ms
step:1072/2330 train_time:65738ms step_avg:61.32ms
step:1073/2330 train_time:65799ms step_avg:61.32ms
step:1074/2330 train_time:65862ms step_avg:61.32ms
step:1075/2330 train_time:65921ms step_avg:61.32ms
step:1076/2330 train_time:65983ms step_avg:61.32ms
step:1077/2330 train_time:66044ms step_avg:61.32ms
step:1078/2330 train_time:66107ms step_avg:61.32ms
step:1079/2330 train_time:66168ms step_avg:61.32ms
step:1080/2330 train_time:66232ms step_avg:61.33ms
step:1081/2330 train_time:66294ms step_avg:61.33ms
step:1082/2330 train_time:66356ms step_avg:61.33ms
step:1083/2330 train_time:66418ms step_avg:61.33ms
step:1084/2330 train_time:66481ms step_avg:61.33ms
step:1085/2330 train_time:66542ms step_avg:61.33ms
step:1086/2330 train_time:66605ms step_avg:61.33ms
step:1087/2330 train_time:66666ms step_avg:61.33ms
step:1088/2330 train_time:66730ms step_avg:61.33ms
step:1089/2330 train_time:66790ms step_avg:61.33ms
step:1090/2330 train_time:66853ms step_avg:61.33ms
step:1091/2330 train_time:66913ms step_avg:61.33ms
step:1092/2330 train_time:66977ms step_avg:61.33ms
step:1093/2330 train_time:67038ms step_avg:61.33ms
step:1094/2330 train_time:67101ms step_avg:61.34ms
step:1095/2330 train_time:67161ms step_avg:61.33ms
step:1096/2330 train_time:67224ms step_avg:61.34ms
step:1097/2330 train_time:67285ms step_avg:61.34ms
step:1098/2330 train_time:67348ms step_avg:61.34ms
step:1099/2330 train_time:67409ms step_avg:61.34ms
step:1100/2330 train_time:67473ms step_avg:61.34ms
step:1101/2330 train_time:67534ms step_avg:61.34ms
step:1102/2330 train_time:67597ms step_avg:61.34ms
step:1103/2330 train_time:67658ms step_avg:61.34ms
step:1104/2330 train_time:67721ms step_avg:61.34ms
step:1105/2330 train_time:67781ms step_avg:61.34ms
step:1106/2330 train_time:67843ms step_avg:61.34ms
step:1107/2330 train_time:67904ms step_avg:61.34ms
step:1108/2330 train_time:67968ms step_avg:61.34ms
step:1109/2330 train_time:68029ms step_avg:61.34ms
step:1110/2330 train_time:68093ms step_avg:61.35ms
step:1111/2330 train_time:68154ms step_avg:61.34ms
step:1112/2330 train_time:68217ms step_avg:61.35ms
step:1113/2330 train_time:68278ms step_avg:61.35ms
step:1114/2330 train_time:68340ms step_avg:61.35ms
step:1115/2330 train_time:68400ms step_avg:61.35ms
step:1116/2330 train_time:68463ms step_avg:61.35ms
step:1117/2330 train_time:68524ms step_avg:61.35ms
step:1118/2330 train_time:68589ms step_avg:61.35ms
step:1119/2330 train_time:68651ms step_avg:61.35ms
step:1120/2330 train_time:68714ms step_avg:61.35ms
step:1121/2330 train_time:68775ms step_avg:61.35ms
step:1122/2330 train_time:68837ms step_avg:61.35ms
step:1123/2330 train_time:68898ms step_avg:61.35ms
step:1124/2330 train_time:68961ms step_avg:61.35ms
step:1125/2330 train_time:69022ms step_avg:61.35ms
step:1126/2330 train_time:69084ms step_avg:61.35ms
step:1127/2330 train_time:69145ms step_avg:61.35ms
step:1128/2330 train_time:69209ms step_avg:61.36ms
step:1129/2330 train_time:69271ms step_avg:61.36ms
step:1130/2330 train_time:69334ms step_avg:61.36ms
step:1131/2330 train_time:69394ms step_avg:61.36ms
step:1132/2330 train_time:69458ms step_avg:61.36ms
step:1133/2330 train_time:69518ms step_avg:61.36ms
step:1134/2330 train_time:69581ms step_avg:61.36ms
step:1135/2330 train_time:69642ms step_avg:61.36ms
step:1136/2330 train_time:69705ms step_avg:61.36ms
step:1137/2330 train_time:69767ms step_avg:61.36ms
step:1138/2330 train_time:69830ms step_avg:61.36ms
step:1139/2330 train_time:69891ms step_avg:61.36ms
step:1140/2330 train_time:69954ms step_avg:61.36ms
step:1141/2330 train_time:70015ms step_avg:61.36ms
step:1142/2330 train_time:70079ms step_avg:61.37ms
step:1143/2330 train_time:70140ms step_avg:61.36ms
step:1144/2330 train_time:70203ms step_avg:61.37ms
step:1145/2330 train_time:70263ms step_avg:61.37ms
step:1146/2330 train_time:70326ms step_avg:61.37ms
step:1147/2330 train_time:70388ms step_avg:61.37ms
step:1148/2330 train_time:70451ms step_avg:61.37ms
step:1149/2330 train_time:70512ms step_avg:61.37ms
step:1150/2330 train_time:70575ms step_avg:61.37ms
step:1151/2330 train_time:70636ms step_avg:61.37ms
step:1152/2330 train_time:70699ms step_avg:61.37ms
step:1153/2330 train_time:70760ms step_avg:61.37ms
step:1154/2330 train_time:70822ms step_avg:61.37ms
step:1155/2330 train_time:70882ms step_avg:61.37ms
step:1156/2330 train_time:70945ms step_avg:61.37ms
step:1157/2330 train_time:71006ms step_avg:61.37ms
step:1158/2330 train_time:71070ms step_avg:61.37ms
step:1159/2330 train_time:71132ms step_avg:61.37ms
step:1160/2330 train_time:71195ms step_avg:61.38ms
step:1161/2330 train_time:71256ms step_avg:61.37ms
step:1162/2330 train_time:71319ms step_avg:61.38ms
step:1163/2330 train_time:71380ms step_avg:61.38ms
step:1164/2330 train_time:71442ms step_avg:61.38ms
step:1165/2330 train_time:71503ms step_avg:61.38ms
step:1166/2330 train_time:71566ms step_avg:61.38ms
step:1167/2330 train_time:71628ms step_avg:61.38ms
step:1168/2330 train_time:71691ms step_avg:61.38ms
step:1169/2330 train_time:71752ms step_avg:61.38ms
step:1170/2330 train_time:71816ms step_avg:61.38ms
step:1171/2330 train_time:71878ms step_avg:61.38ms
step:1172/2330 train_time:71940ms step_avg:61.38ms
step:1173/2330 train_time:72001ms step_avg:61.38ms
step:1174/2330 train_time:72063ms step_avg:61.38ms
step:1175/2330 train_time:72124ms step_avg:61.38ms
step:1176/2330 train_time:72188ms step_avg:61.38ms
step:1177/2330 train_time:72248ms step_avg:61.38ms
step:1178/2330 train_time:72311ms step_avg:61.38ms
step:1179/2330 train_time:72372ms step_avg:61.38ms
step:1180/2330 train_time:72435ms step_avg:61.39ms
step:1181/2330 train_time:72497ms step_avg:61.39ms
step:1182/2330 train_time:72559ms step_avg:61.39ms
step:1183/2330 train_time:72620ms step_avg:61.39ms
step:1184/2330 train_time:72683ms step_avg:61.39ms
step:1185/2330 train_time:72743ms step_avg:61.39ms
step:1186/2330 train_time:72806ms step_avg:61.39ms
step:1187/2330 train_time:72867ms step_avg:61.39ms
step:1188/2330 train_time:72931ms step_avg:61.39ms
step:1189/2330 train_time:72993ms step_avg:61.39ms
step:1190/2330 train_time:73056ms step_avg:61.39ms
step:1191/2330 train_time:73117ms step_avg:61.39ms
step:1192/2330 train_time:73180ms step_avg:61.39ms
step:1193/2330 train_time:73240ms step_avg:61.39ms
step:1194/2330 train_time:73302ms step_avg:61.39ms
step:1195/2330 train_time:73363ms step_avg:61.39ms
step:1196/2330 train_time:73426ms step_avg:61.39ms
step:1197/2330 train_time:73487ms step_avg:61.39ms
step:1198/2330 train_time:73551ms step_avg:61.39ms
step:1199/2330 train_time:73612ms step_avg:61.39ms
step:1200/2330 train_time:73676ms step_avg:61.40ms
step:1201/2330 train_time:73737ms step_avg:61.40ms
step:1202/2330 train_time:73800ms step_avg:61.40ms
step:1203/2330 train_time:73860ms step_avg:61.40ms
step:1204/2330 train_time:73923ms step_avg:61.40ms
step:1205/2330 train_time:73984ms step_avg:61.40ms
step:1206/2330 train_time:74048ms step_avg:61.40ms
step:1207/2330 train_time:74108ms step_avg:61.40ms
step:1208/2330 train_time:74172ms step_avg:61.40ms
step:1209/2330 train_time:74233ms step_avg:61.40ms
step:1210/2330 train_time:74297ms step_avg:61.40ms
step:1211/2330 train_time:74358ms step_avg:61.40ms
step:1212/2330 train_time:74422ms step_avg:61.40ms
step:1213/2330 train_time:74482ms step_avg:61.40ms
step:1214/2330 train_time:74545ms step_avg:61.40ms
step:1215/2330 train_time:74606ms step_avg:61.40ms
step:1216/2330 train_time:74669ms step_avg:61.41ms
step:1217/2330 train_time:74731ms step_avg:61.41ms
step:1218/2330 train_time:74794ms step_avg:61.41ms
step:1219/2330 train_time:74855ms step_avg:61.41ms
step:1220/2330 train_time:74917ms step_avg:61.41ms
step:1221/2330 train_time:74978ms step_avg:61.41ms
step:1222/2330 train_time:75041ms step_avg:61.41ms
step:1223/2330 train_time:75102ms step_avg:61.41ms
step:1224/2330 train_time:75164ms step_avg:61.41ms
step:1225/2330 train_time:75226ms step_avg:61.41ms
step:1226/2330 train_time:75290ms step_avg:61.41ms
step:1227/2330 train_time:75352ms step_avg:61.41ms
step:1228/2330 train_time:75415ms step_avg:61.41ms
step:1229/2330 train_time:75476ms step_avg:61.41ms
step:1230/2330 train_time:75538ms step_avg:61.41ms
step:1231/2330 train_time:75599ms step_avg:61.41ms
step:1232/2330 train_time:75662ms step_avg:61.41ms
step:1233/2330 train_time:75722ms step_avg:61.41ms
step:1234/2330 train_time:75785ms step_avg:61.41ms
step:1235/2330 train_time:75846ms step_avg:61.41ms
step:1236/2330 train_time:75909ms step_avg:61.41ms
step:1237/2330 train_time:75969ms step_avg:61.41ms
step:1238/2330 train_time:76033ms step_avg:61.42ms
step:1239/2330 train_time:76094ms step_avg:61.42ms
step:1240/2330 train_time:76158ms step_avg:61.42ms
step:1241/2330 train_time:76219ms step_avg:61.42ms
step:1242/2330 train_time:76282ms step_avg:61.42ms
step:1243/2330 train_time:76342ms step_avg:61.42ms
step:1244/2330 train_time:76405ms step_avg:61.42ms
step:1245/2330 train_time:76466ms step_avg:61.42ms
step:1246/2330 train_time:76530ms step_avg:61.42ms
step:1247/2330 train_time:76592ms step_avg:61.42ms
step:1248/2330 train_time:76654ms step_avg:61.42ms
step:1249/2330 train_time:76715ms step_avg:61.42ms
step:1250/2330 train_time:76779ms step_avg:61.42ms
step:1250/2330 val_loss:3.7836 train_time:76843ms step_avg:61.47ms
step:1251/2330 train_time:76865ms step_avg:61.44ms
step:1252/2330 train_time:76904ms step_avg:61.43ms
step:1253/2330 train_time:76969ms step_avg:61.43ms
step:1254/2330 train_time:77033ms step_avg:61.43ms
step:1255/2330 train_time:77095ms step_avg:61.43ms
step:1256/2330 train_time:77158ms step_avg:61.43ms
step:1257/2330 train_time:77219ms step_avg:61.43ms
step:1258/2330 train_time:77281ms step_avg:61.43ms
step:1259/2330 train_time:77341ms step_avg:61.43ms
step:1260/2330 train_time:77403ms step_avg:61.43ms
step:1261/2330 train_time:77463ms step_avg:61.43ms
step:1262/2330 train_time:77525ms step_avg:61.43ms
step:1263/2330 train_time:77585ms step_avg:61.43ms
step:1264/2330 train_time:77647ms step_avg:61.43ms
step:1265/2330 train_time:77707ms step_avg:61.43ms
step:1266/2330 train_time:77769ms step_avg:61.43ms
step:1267/2330 train_time:77831ms step_avg:61.43ms
step:1268/2330 train_time:77896ms step_avg:61.43ms
step:1269/2330 train_time:77959ms step_avg:61.43ms
step:1270/2330 train_time:78022ms step_avg:61.43ms
step:1271/2330 train_time:78084ms step_avg:61.43ms
step:1272/2330 train_time:78146ms step_avg:61.44ms
step:1273/2330 train_time:78206ms step_avg:61.43ms
step:1274/2330 train_time:78270ms step_avg:61.44ms
step:1275/2330 train_time:78330ms step_avg:61.44ms
step:1276/2330 train_time:78393ms step_avg:61.44ms
step:1277/2330 train_time:78455ms step_avg:61.44ms
step:1278/2330 train_time:78518ms step_avg:61.44ms
step:1279/2330 train_time:78578ms step_avg:61.44ms
step:1280/2330 train_time:78640ms step_avg:61.44ms
step:1281/2330 train_time:78701ms step_avg:61.44ms
step:1282/2330 train_time:78765ms step_avg:61.44ms
step:1283/2330 train_time:78826ms step_avg:61.44ms
step:1284/2330 train_time:78889ms step_avg:61.44ms
step:1285/2330 train_time:78950ms step_avg:61.44ms
step:1286/2330 train_time:79014ms step_avg:61.44ms
step:1287/2330 train_time:79077ms step_avg:61.44ms
step:1288/2330 train_time:79140ms step_avg:61.44ms
step:1289/2330 train_time:79201ms step_avg:61.44ms
step:1290/2330 train_time:79264ms step_avg:61.44ms
step:1291/2330 train_time:79325ms step_avg:61.44ms
step:1292/2330 train_time:79387ms step_avg:61.45ms
step:1293/2330 train_time:79447ms step_avg:61.44ms
step:1294/2330 train_time:79510ms step_avg:61.45ms
step:1295/2330 train_time:79571ms step_avg:61.45ms
step:1296/2330 train_time:79634ms step_avg:61.45ms
step:1297/2330 train_time:79696ms step_avg:61.45ms
step:1298/2330 train_time:79759ms step_avg:61.45ms
step:1299/2330 train_time:79820ms step_avg:61.45ms
step:1300/2330 train_time:79883ms step_avg:61.45ms
step:1301/2330 train_time:79944ms step_avg:61.45ms
step:1302/2330 train_time:80007ms step_avg:61.45ms
step:1303/2330 train_time:80068ms step_avg:61.45ms
step:1304/2330 train_time:80131ms step_avg:61.45ms
step:1305/2330 train_time:80193ms step_avg:61.45ms
step:1306/2330 train_time:80257ms step_avg:61.45ms
step:1307/2330 train_time:80319ms step_avg:61.45ms
step:1308/2330 train_time:80382ms step_avg:61.45ms
step:1309/2330 train_time:80442ms step_avg:61.45ms
step:1310/2330 train_time:80505ms step_avg:61.45ms
step:1311/2330 train_time:80566ms step_avg:61.45ms
step:1312/2330 train_time:80628ms step_avg:61.45ms
step:1313/2330 train_time:80689ms step_avg:61.45ms
step:1314/2330 train_time:80752ms step_avg:61.46ms
step:1315/2330 train_time:80813ms step_avg:61.45ms
step:1316/2330 train_time:80877ms step_avg:61.46ms
step:1317/2330 train_time:80938ms step_avg:61.46ms
step:1318/2330 train_time:81001ms step_avg:61.46ms
step:1319/2330 train_time:81062ms step_avg:61.46ms
step:1320/2330 train_time:81126ms step_avg:61.46ms
step:1321/2330 train_time:81186ms step_avg:61.46ms
step:1322/2330 train_time:81249ms step_avg:61.46ms
step:1323/2330 train_time:81309ms step_avg:61.46ms
step:1324/2330 train_time:81373ms step_avg:61.46ms
step:1325/2330 train_time:81434ms step_avg:61.46ms
step:1326/2330 train_time:81497ms step_avg:61.46ms
step:1327/2330 train_time:81558ms step_avg:61.46ms
step:1328/2330 train_time:81620ms step_avg:61.46ms
step:1329/2330 train_time:81682ms step_avg:61.46ms
step:1330/2330 train_time:81745ms step_avg:61.46ms
step:1331/2330 train_time:81806ms step_avg:61.46ms
step:1332/2330 train_time:81869ms step_avg:61.46ms
step:1333/2330 train_time:81930ms step_avg:61.46ms
step:1334/2330 train_time:81993ms step_avg:61.46ms
step:1335/2330 train_time:82054ms step_avg:61.46ms
step:1336/2330 train_time:82117ms step_avg:61.46ms
step:1337/2330 train_time:82178ms step_avg:61.46ms
step:1338/2330 train_time:82241ms step_avg:61.47ms
step:1339/2330 train_time:82302ms step_avg:61.47ms
step:1340/2330 train_time:82365ms step_avg:61.47ms
step:1341/2330 train_time:82426ms step_avg:61.47ms
step:1342/2330 train_time:82488ms step_avg:61.47ms
step:1343/2330 train_time:82548ms step_avg:61.47ms
step:1344/2330 train_time:82611ms step_avg:61.47ms
step:1345/2330 train_time:82672ms step_avg:61.47ms
step:1346/2330 train_time:82736ms step_avg:61.47ms
step:1347/2330 train_time:82797ms step_avg:61.47ms
step:1348/2330 train_time:82860ms step_avg:61.47ms
step:1349/2330 train_time:82921ms step_avg:61.47ms
step:1350/2330 train_time:82984ms step_avg:61.47ms
step:1351/2330 train_time:83045ms step_avg:61.47ms
step:1352/2330 train_time:83107ms step_avg:61.47ms
step:1353/2330 train_time:83168ms step_avg:61.47ms
step:1354/2330 train_time:83230ms step_avg:61.47ms
step:1355/2330 train_time:83291ms step_avg:61.47ms
step:1356/2330 train_time:83355ms step_avg:61.47ms
step:1357/2330 train_time:83416ms step_avg:61.47ms
step:1358/2330 train_time:83479ms step_avg:61.47ms
step:1359/2330 train_time:83539ms step_avg:61.47ms
step:1360/2330 train_time:83603ms step_avg:61.47ms
step:1361/2330 train_time:83664ms step_avg:61.47ms
step:1362/2330 train_time:83726ms step_avg:61.47ms
step:1363/2330 train_time:83786ms step_avg:61.47ms
step:1364/2330 train_time:83849ms step_avg:61.47ms
step:1365/2330 train_time:83911ms step_avg:61.47ms
step:1366/2330 train_time:83974ms step_avg:61.47ms
step:1367/2330 train_time:84035ms step_avg:61.47ms
step:1368/2330 train_time:84099ms step_avg:61.48ms
step:1369/2330 train_time:84159ms step_avg:61.47ms
step:1370/2330 train_time:84223ms step_avg:61.48ms
step:1371/2330 train_time:84283ms step_avg:61.48ms
step:1372/2330 train_time:84346ms step_avg:61.48ms
step:1373/2330 train_time:84407ms step_avg:61.48ms
step:1374/2330 train_time:84469ms step_avg:61.48ms
step:1375/2330 train_time:84531ms step_avg:61.48ms
step:1376/2330 train_time:84594ms step_avg:61.48ms
step:1377/2330 train_time:84655ms step_avg:61.48ms
step:1378/2330 train_time:84718ms step_avg:61.48ms
step:1379/2330 train_time:84779ms step_avg:61.48ms
step:1380/2330 train_time:84842ms step_avg:61.48ms
step:1381/2330 train_time:84903ms step_avg:61.48ms
step:1382/2330 train_time:84966ms step_avg:61.48ms
step:1383/2330 train_time:85027ms step_avg:61.48ms
step:1384/2330 train_time:85089ms step_avg:61.48ms
step:1385/2330 train_time:85150ms step_avg:61.48ms
step:1386/2330 train_time:85213ms step_avg:61.48ms
step:1387/2330 train_time:85274ms step_avg:61.48ms
step:1388/2330 train_time:85337ms step_avg:61.48ms
step:1389/2330 train_time:85398ms step_avg:61.48ms
step:1390/2330 train_time:85462ms step_avg:61.48ms
step:1391/2330 train_time:85522ms step_avg:61.48ms
step:1392/2330 train_time:85585ms step_avg:61.48ms
step:1393/2330 train_time:85646ms step_avg:61.48ms
step:1394/2330 train_time:85708ms step_avg:61.48ms
step:1395/2330 train_time:85770ms step_avg:61.48ms
step:1396/2330 train_time:85833ms step_avg:61.48ms
step:1397/2330 train_time:85894ms step_avg:61.48ms
step:1398/2330 train_time:85957ms step_avg:61.49ms
step:1399/2330 train_time:86018ms step_avg:61.49ms
step:1400/2330 train_time:86081ms step_avg:61.49ms
step:1401/2330 train_time:86143ms step_avg:61.49ms
step:1402/2330 train_time:86207ms step_avg:61.49ms
step:1403/2330 train_time:86266ms step_avg:61.49ms
step:1404/2330 train_time:86330ms step_avg:61.49ms
step:1405/2330 train_time:86390ms step_avg:61.49ms
step:1406/2330 train_time:86454ms step_avg:61.49ms
step:1407/2330 train_time:86516ms step_avg:61.49ms
step:1408/2330 train_time:86579ms step_avg:61.49ms
step:1409/2330 train_time:86640ms step_avg:61.49ms
step:1410/2330 train_time:86703ms step_avg:61.49ms
step:1411/2330 train_time:86764ms step_avg:61.49ms
step:1412/2330 train_time:86827ms step_avg:61.49ms
step:1413/2330 train_time:86887ms step_avg:61.49ms
step:1414/2330 train_time:86950ms step_avg:61.49ms
step:1415/2330 train_time:87011ms step_avg:61.49ms
step:1416/2330 train_time:87074ms step_avg:61.49ms
step:1417/2330 train_time:87136ms step_avg:61.49ms
step:1418/2330 train_time:87199ms step_avg:61.49ms
step:1419/2330 train_time:87260ms step_avg:61.49ms
step:1420/2330 train_time:87323ms step_avg:61.50ms
step:1421/2330 train_time:87385ms step_avg:61.50ms
step:1422/2330 train_time:87448ms step_avg:61.50ms
step:1423/2330 train_time:87508ms step_avg:61.50ms
step:1424/2330 train_time:87571ms step_avg:61.50ms
step:1425/2330 train_time:87632ms step_avg:61.50ms
step:1426/2330 train_time:87696ms step_avg:61.50ms
step:1427/2330 train_time:87758ms step_avg:61.50ms
step:1428/2330 train_time:87820ms step_avg:61.50ms
step:1429/2330 train_time:87881ms step_avg:61.50ms
step:1430/2330 train_time:87944ms step_avg:61.50ms
step:1431/2330 train_time:88005ms step_avg:61.50ms
step:1432/2330 train_time:88067ms step_avg:61.50ms
step:1433/2330 train_time:88128ms step_avg:61.50ms
step:1434/2330 train_time:88192ms step_avg:61.50ms
step:1435/2330 train_time:88253ms step_avg:61.50ms
step:1436/2330 train_time:88316ms step_avg:61.50ms
step:1437/2330 train_time:88377ms step_avg:61.50ms
step:1438/2330 train_time:88440ms step_avg:61.50ms
step:1439/2330 train_time:88502ms step_avg:61.50ms
step:1440/2330 train_time:88565ms step_avg:61.50ms
step:1441/2330 train_time:88625ms step_avg:61.50ms
step:1442/2330 train_time:88688ms step_avg:61.50ms
step:1443/2330 train_time:88749ms step_avg:61.50ms
step:1444/2330 train_time:88814ms step_avg:61.51ms
step:1445/2330 train_time:88874ms step_avg:61.50ms
step:1446/2330 train_time:88937ms step_avg:61.51ms
step:1447/2330 train_time:88998ms step_avg:61.50ms
step:1448/2330 train_time:89061ms step_avg:61.51ms
step:1449/2330 train_time:89122ms step_avg:61.51ms
step:1450/2330 train_time:89185ms step_avg:61.51ms
step:1451/2330 train_time:89245ms step_avg:61.51ms
step:1452/2330 train_time:89308ms step_avg:61.51ms
step:1453/2330 train_time:89369ms step_avg:61.51ms
step:1454/2330 train_time:89432ms step_avg:61.51ms
step:1455/2330 train_time:89493ms step_avg:61.51ms
step:1456/2330 train_time:89556ms step_avg:61.51ms
step:1457/2330 train_time:89617ms step_avg:61.51ms
step:1458/2330 train_time:89680ms step_avg:61.51ms
step:1459/2330 train_time:89742ms step_avg:61.51ms
step:1460/2330 train_time:89805ms step_avg:61.51ms
step:1461/2330 train_time:89865ms step_avg:61.51ms
step:1462/2330 train_time:89928ms step_avg:61.51ms
step:1463/2330 train_time:89989ms step_avg:61.51ms
step:1464/2330 train_time:90052ms step_avg:61.51ms
step:1465/2330 train_time:90114ms step_avg:61.51ms
step:1466/2330 train_time:90177ms step_avg:61.51ms
step:1467/2330 train_time:90238ms step_avg:61.51ms
step:1468/2330 train_time:90301ms step_avg:61.51ms
step:1469/2330 train_time:90362ms step_avg:61.51ms
step:1470/2330 train_time:90425ms step_avg:61.51ms
step:1471/2330 train_time:90486ms step_avg:61.51ms
step:1472/2330 train_time:90548ms step_avg:61.51ms
step:1473/2330 train_time:90609ms step_avg:61.51ms
step:1474/2330 train_time:90674ms step_avg:61.52ms
step:1475/2330 train_time:90735ms step_avg:61.52ms
step:1476/2330 train_time:90799ms step_avg:61.52ms
step:1477/2330 train_time:90860ms step_avg:61.52ms
step:1478/2330 train_time:90922ms step_avg:61.52ms
step:1479/2330 train_time:90984ms step_avg:61.52ms
step:1480/2330 train_time:91047ms step_avg:61.52ms
step:1481/2330 train_time:91108ms step_avg:61.52ms
step:1482/2330 train_time:91171ms step_avg:61.52ms
step:1483/2330 train_time:91232ms step_avg:61.52ms
step:1484/2330 train_time:91295ms step_avg:61.52ms
step:1485/2330 train_time:91357ms step_avg:61.52ms
step:1486/2330 train_time:91420ms step_avg:61.52ms
step:1487/2330 train_time:91481ms step_avg:61.52ms
step:1488/2330 train_time:91544ms step_avg:61.52ms
step:1489/2330 train_time:91605ms step_avg:61.52ms
step:1490/2330 train_time:91667ms step_avg:61.52ms
step:1491/2330 train_time:91728ms step_avg:61.52ms
step:1492/2330 train_time:91792ms step_avg:61.52ms
step:1493/2330 train_time:91852ms step_avg:61.52ms
step:1494/2330 train_time:91916ms step_avg:61.52ms
step:1495/2330 train_time:91977ms step_avg:61.52ms
step:1496/2330 train_time:92040ms step_avg:61.52ms
step:1497/2330 train_time:92101ms step_avg:61.52ms
step:1498/2330 train_time:92164ms step_avg:61.52ms
step:1499/2330 train_time:92224ms step_avg:61.52ms
step:1500/2330 train_time:92287ms step_avg:61.52ms
step:1500/2330 val_loss:3.7192 train_time:92351ms step_avg:61.57ms
step:1501/2330 train_time:92374ms step_avg:61.54ms
step:1502/2330 train_time:92412ms step_avg:61.53ms
step:1503/2330 train_time:92479ms step_avg:61.53ms
step:1504/2330 train_time:92544ms step_avg:61.53ms
step:1505/2330 train_time:92606ms step_avg:61.53ms
step:1506/2330 train_time:92669ms step_avg:61.53ms
step:1507/2330 train_time:92729ms step_avg:61.53ms
step:1508/2330 train_time:92791ms step_avg:61.53ms
step:1509/2330 train_time:92852ms step_avg:61.53ms
step:1510/2330 train_time:92914ms step_avg:61.53ms
step:1511/2330 train_time:92974ms step_avg:61.53ms
step:1512/2330 train_time:93036ms step_avg:61.53ms
step:1513/2330 train_time:93096ms step_avg:61.53ms
step:1514/2330 train_time:93158ms step_avg:61.53ms
step:1515/2330 train_time:93218ms step_avg:61.53ms
step:1516/2330 train_time:93281ms step_avg:61.53ms
step:1517/2330 train_time:93342ms step_avg:61.53ms
step:1518/2330 train_time:93406ms step_avg:61.53ms
step:1519/2330 train_time:93469ms step_avg:61.53ms
step:1520/2330 train_time:93533ms step_avg:61.53ms
step:1521/2330 train_time:93597ms step_avg:61.54ms
step:1522/2330 train_time:93661ms step_avg:61.54ms
step:1523/2330 train_time:93722ms step_avg:61.54ms
step:1524/2330 train_time:93785ms step_avg:61.54ms
step:1525/2330 train_time:93846ms step_avg:61.54ms
step:1526/2330 train_time:93909ms step_avg:61.54ms
step:1527/2330 train_time:93969ms step_avg:61.54ms
step:1528/2330 train_time:94032ms step_avg:61.54ms
step:1529/2330 train_time:94093ms step_avg:61.54ms
step:1530/2330 train_time:94156ms step_avg:61.54ms
step:1531/2330 train_time:94217ms step_avg:61.54ms
step:1532/2330 train_time:94282ms step_avg:61.54ms
step:1533/2330 train_time:94343ms step_avg:61.54ms
step:1534/2330 train_time:94406ms step_avg:61.54ms
step:1535/2330 train_time:94469ms step_avg:61.54ms
step:1536/2330 train_time:94533ms step_avg:61.55ms
step:1537/2330 train_time:94596ms step_avg:61.55ms
step:1538/2330 train_time:94660ms step_avg:61.55ms
step:1539/2330 train_time:94721ms step_avg:61.55ms
step:1540/2330 train_time:94784ms step_avg:61.55ms
step:1541/2330 train_time:94846ms step_avg:61.55ms
step:1542/2330 train_time:94909ms step_avg:61.55ms
step:1543/2330 train_time:94970ms step_avg:61.55ms
step:1544/2330 train_time:95033ms step_avg:61.55ms
step:1545/2330 train_time:95094ms step_avg:61.55ms
step:1546/2330 train_time:95157ms step_avg:61.55ms
step:1547/2330 train_time:95219ms step_avg:61.55ms
step:1548/2330 train_time:95282ms step_avg:61.55ms
step:1549/2330 train_time:95344ms step_avg:61.55ms
step:1550/2330 train_time:95408ms step_avg:61.55ms
step:1551/2330 train_time:95469ms step_avg:61.55ms
step:1552/2330 train_time:95533ms step_avg:61.55ms
step:1553/2330 train_time:95596ms step_avg:61.56ms
step:1554/2330 train_time:95661ms step_avg:61.56ms
step:1555/2330 train_time:95723ms step_avg:61.56ms
step:1556/2330 train_time:95787ms step_avg:61.56ms
step:1557/2330 train_time:95849ms step_avg:61.56ms
step:1558/2330 train_time:95913ms step_avg:61.56ms
step:1559/2330 train_time:95974ms step_avg:61.56ms
step:1560/2330 train_time:96037ms step_avg:61.56ms
step:1561/2330 train_time:96098ms step_avg:61.56ms
step:1562/2330 train_time:96161ms step_avg:61.56ms
step:1563/2330 train_time:96222ms step_avg:61.56ms
step:1564/2330 train_time:96286ms step_avg:61.56ms
step:1565/2330 train_time:96347ms step_avg:61.56ms
step:1566/2330 train_time:96411ms step_avg:61.57ms
step:1567/2330 train_time:96472ms step_avg:61.56ms
step:1568/2330 train_time:96535ms step_avg:61.57ms
step:1569/2330 train_time:96597ms step_avg:61.57ms
step:1570/2330 train_time:96661ms step_avg:61.57ms
step:1571/2330 train_time:96723ms step_avg:61.57ms
step:1572/2330 train_time:96786ms step_avg:61.57ms
step:1573/2330 train_time:96848ms step_avg:61.57ms
step:1574/2330 train_time:96911ms step_avg:61.57ms
step:1575/2330 train_time:96972ms step_avg:61.57ms
step:1576/2330 train_time:97035ms step_avg:61.57ms
step:1577/2330 train_time:97097ms step_avg:61.57ms
step:1578/2330 train_time:97160ms step_avg:61.57ms
step:1579/2330 train_time:97221ms step_avg:61.57ms
step:1580/2330 train_time:97286ms step_avg:61.57ms
step:1581/2330 train_time:97348ms step_avg:61.57ms
step:1582/2330 train_time:97411ms step_avg:61.57ms
step:1583/2330 train_time:97473ms step_avg:61.57ms
step:1584/2330 train_time:97536ms step_avg:61.58ms
step:1585/2330 train_time:97597ms step_avg:61.58ms
step:1586/2330 train_time:97662ms step_avg:61.58ms
step:1587/2330 train_time:97724ms step_avg:61.58ms
step:1588/2330 train_time:97788ms step_avg:61.58ms
step:1589/2330 train_time:97850ms step_avg:61.58ms
step:1590/2330 train_time:97912ms step_avg:61.58ms
step:1591/2330 train_time:97974ms step_avg:61.58ms
step:1592/2330 train_time:98037ms step_avg:61.58ms
step:1593/2330 train_time:98098ms step_avg:61.58ms
step:1594/2330 train_time:98162ms step_avg:61.58ms
step:1595/2330 train_time:98223ms step_avg:61.58ms
step:1596/2330 train_time:98286ms step_avg:61.58ms
step:1597/2330 train_time:98348ms step_avg:61.58ms
step:1598/2330 train_time:98412ms step_avg:61.58ms
step:1599/2330 train_time:98473ms step_avg:61.58ms
step:1600/2330 train_time:98537ms step_avg:61.59ms
step:1601/2330 train_time:98599ms step_avg:61.59ms
step:1602/2330 train_time:98662ms step_avg:61.59ms
step:1603/2330 train_time:98724ms step_avg:61.59ms
step:1604/2330 train_time:98788ms step_avg:61.59ms
step:1605/2330 train_time:98850ms step_avg:61.59ms
step:1606/2330 train_time:98913ms step_avg:61.59ms
step:1607/2330 train_time:98974ms step_avg:61.59ms
step:1608/2330 train_time:99038ms step_avg:61.59ms
step:1609/2330 train_time:99099ms step_avg:61.59ms
step:1610/2330 train_time:99162ms step_avg:61.59ms
step:1611/2330 train_time:99224ms step_avg:61.59ms
step:1612/2330 train_time:99287ms step_avg:61.59ms
step:1613/2330 train_time:99348ms step_avg:61.59ms
step:1614/2330 train_time:99411ms step_avg:61.59ms
step:1615/2330 train_time:99473ms step_avg:61.59ms
step:1616/2330 train_time:99536ms step_avg:61.59ms
step:1617/2330 train_time:99598ms step_avg:61.59ms
step:1618/2330 train_time:99662ms step_avg:61.60ms
step:1619/2330 train_time:99723ms step_avg:61.60ms
step:1620/2330 train_time:99787ms step_avg:61.60ms
step:1621/2330 train_time:99848ms step_avg:61.60ms
step:1622/2330 train_time:99912ms step_avg:61.60ms
step:1623/2330 train_time:99973ms step_avg:61.60ms
step:1624/2330 train_time:100037ms step_avg:61.60ms
step:1625/2330 train_time:100099ms step_avg:61.60ms
step:1626/2330 train_time:100163ms step_avg:61.60ms
step:1627/2330 train_time:100224ms step_avg:61.60ms
step:1628/2330 train_time:100287ms step_avg:61.60ms
step:1629/2330 train_time:100349ms step_avg:61.60ms
step:1630/2330 train_time:100412ms step_avg:61.60ms
step:1631/2330 train_time:100473ms step_avg:61.60ms
step:1632/2330 train_time:100537ms step_avg:61.60ms
step:1633/2330 train_time:100599ms step_avg:61.60ms
step:1634/2330 train_time:100663ms step_avg:61.61ms
step:1635/2330 train_time:100725ms step_avg:61.61ms
step:1636/2330 train_time:100789ms step_avg:61.61ms
step:1637/2330 train_time:100850ms step_avg:61.61ms
step:1638/2330 train_time:100913ms step_avg:61.61ms
step:1639/2330 train_time:100974ms step_avg:61.61ms
step:1640/2330 train_time:101038ms step_avg:61.61ms
step:1641/2330 train_time:101100ms step_avg:61.61ms
step:1642/2330 train_time:101163ms step_avg:61.61ms
step:1643/2330 train_time:101225ms step_avg:61.61ms
step:1644/2330 train_time:101288ms step_avg:61.61ms
step:1645/2330 train_time:101350ms step_avg:61.61ms
step:1646/2330 train_time:101412ms step_avg:61.61ms
step:1647/2330 train_time:101473ms step_avg:61.61ms
step:1648/2330 train_time:101539ms step_avg:61.61ms
step:1649/2330 train_time:101600ms step_avg:61.61ms
step:1650/2330 train_time:101664ms step_avg:61.61ms
step:1651/2330 train_time:101726ms step_avg:61.61ms
step:1652/2330 train_time:101790ms step_avg:61.62ms
step:1653/2330 train_time:101851ms step_avg:61.62ms
step:1654/2330 train_time:101914ms step_avg:61.62ms
step:1655/2330 train_time:101976ms step_avg:61.62ms
step:1656/2330 train_time:102040ms step_avg:61.62ms
step:1657/2330 train_time:102101ms step_avg:61.62ms
step:1658/2330 train_time:102165ms step_avg:61.62ms
step:1659/2330 train_time:102227ms step_avg:61.62ms
step:1660/2330 train_time:102290ms step_avg:61.62ms
step:1661/2330 train_time:102351ms step_avg:61.62ms
step:1662/2330 train_time:102414ms step_avg:61.62ms
step:1663/2330 train_time:102476ms step_avg:61.62ms
step:1664/2330 train_time:102541ms step_avg:61.62ms
step:1665/2330 train_time:102602ms step_avg:61.62ms
step:1666/2330 train_time:102666ms step_avg:61.62ms
step:1667/2330 train_time:102728ms step_avg:61.62ms
step:1668/2330 train_time:102792ms step_avg:61.63ms
step:1669/2330 train_time:102853ms step_avg:61.63ms
step:1670/2330 train_time:102915ms step_avg:61.63ms
step:1671/2330 train_time:102977ms step_avg:61.63ms
step:1672/2330 train_time:103041ms step_avg:61.63ms
step:1673/2330 train_time:103103ms step_avg:61.63ms
step:1674/2330 train_time:103167ms step_avg:61.63ms
step:1675/2330 train_time:103228ms step_avg:61.63ms
step:1676/2330 train_time:103292ms step_avg:61.63ms
step:1677/2330 train_time:103353ms step_avg:61.63ms
step:1678/2330 train_time:103417ms step_avg:61.63ms
step:1679/2330 train_time:103480ms step_avg:61.63ms
step:1680/2330 train_time:103544ms step_avg:61.63ms
step:1681/2330 train_time:103605ms step_avg:61.63ms
step:1682/2330 train_time:103669ms step_avg:61.63ms
step:1683/2330 train_time:103731ms step_avg:61.63ms
step:1684/2330 train_time:103794ms step_avg:61.64ms
step:1685/2330 train_time:103855ms step_avg:61.64ms
step:1686/2330 train_time:103919ms step_avg:61.64ms
step:1687/2330 train_time:103982ms step_avg:61.64ms
step:1688/2330 train_time:104045ms step_avg:61.64ms
step:1689/2330 train_time:104106ms step_avg:61.64ms
step:1690/2330 train_time:104169ms step_avg:61.64ms
step:1691/2330 train_time:104231ms step_avg:61.64ms
step:1692/2330 train_time:104294ms step_avg:61.64ms
step:1693/2330 train_time:104354ms step_avg:61.64ms
step:1694/2330 train_time:104418ms step_avg:61.64ms
step:1695/2330 train_time:104479ms step_avg:61.64ms
step:1696/2330 train_time:104543ms step_avg:61.64ms
step:1697/2330 train_time:104604ms step_avg:61.64ms
step:1698/2330 train_time:104668ms step_avg:61.64ms
step:1699/2330 train_time:104729ms step_avg:61.64ms
step:1700/2330 train_time:104793ms step_avg:61.64ms
step:1701/2330 train_time:104854ms step_avg:61.64ms
step:1702/2330 train_time:104919ms step_avg:61.64ms
step:1703/2330 train_time:104981ms step_avg:61.64ms
step:1704/2330 train_time:105045ms step_avg:61.65ms
step:1705/2330 train_time:105107ms step_avg:61.65ms
step:1706/2330 train_time:105170ms step_avg:61.65ms
step:1707/2330 train_time:105231ms step_avg:61.65ms
step:1708/2330 train_time:105294ms step_avg:61.65ms
step:1709/2330 train_time:105355ms step_avg:61.65ms
step:1710/2330 train_time:105419ms step_avg:61.65ms
step:1711/2330 train_time:105481ms step_avg:61.65ms
step:1712/2330 train_time:105545ms step_avg:61.65ms
step:1713/2330 train_time:105606ms step_avg:61.65ms
step:1714/2330 train_time:105670ms step_avg:61.65ms
step:1715/2330 train_time:105731ms step_avg:61.65ms
step:1716/2330 train_time:105793ms step_avg:61.65ms
step:1717/2330 train_time:105854ms step_avg:61.65ms
step:1718/2330 train_time:105917ms step_avg:61.65ms
step:1719/2330 train_time:105980ms step_avg:61.65ms
step:1720/2330 train_time:106045ms step_avg:61.65ms
step:1721/2330 train_time:106107ms step_avg:61.65ms
step:1722/2330 train_time:106170ms step_avg:61.65ms
step:1723/2330 train_time:106231ms step_avg:61.65ms
step:1724/2330 train_time:106294ms step_avg:61.66ms
step:1725/2330 train_time:106355ms step_avg:61.66ms
step:1726/2330 train_time:106419ms step_avg:61.66ms
step:1727/2330 train_time:106480ms step_avg:61.66ms
step:1728/2330 train_time:106544ms step_avg:61.66ms
step:1729/2330 train_time:106606ms step_avg:61.66ms
step:1730/2330 train_time:106670ms step_avg:61.66ms
step:1731/2330 train_time:106731ms step_avg:61.66ms
step:1732/2330 train_time:106794ms step_avg:61.66ms
step:1733/2330 train_time:106855ms step_avg:61.66ms
step:1734/2330 train_time:106919ms step_avg:61.66ms
step:1735/2330 train_time:106981ms step_avg:61.66ms
step:1736/2330 train_time:107045ms step_avg:61.66ms
step:1737/2330 train_time:107107ms step_avg:61.66ms
step:1738/2330 train_time:107170ms step_avg:61.66ms
step:1739/2330 train_time:107231ms step_avg:61.66ms
step:1740/2330 train_time:107294ms step_avg:61.66ms
step:1741/2330 train_time:107356ms step_avg:61.66ms
step:1742/2330 train_time:107421ms step_avg:61.67ms
step:1743/2330 train_time:107483ms step_avg:61.67ms
step:1744/2330 train_time:107547ms step_avg:61.67ms
step:1745/2330 train_time:107608ms step_avg:61.67ms
step:1746/2330 train_time:107672ms step_avg:61.67ms
step:1747/2330 train_time:107733ms step_avg:61.67ms
step:1748/2330 train_time:107797ms step_avg:61.67ms
step:1749/2330 train_time:107858ms step_avg:61.67ms
step:1750/2330 train_time:107921ms step_avg:61.67ms
step:1750/2330 val_loss:3.6287 train_time:107987ms step_avg:61.71ms
step:1751/2330 train_time:108009ms step_avg:61.68ms
step:1752/2330 train_time:108048ms step_avg:61.67ms
step:1753/2330 train_time:108114ms step_avg:61.67ms
step:1754/2330 train_time:108182ms step_avg:61.68ms
step:1755/2330 train_time:108243ms step_avg:61.68ms
step:1756/2330 train_time:108307ms step_avg:61.68ms
step:1757/2330 train_time:108368ms step_avg:61.68ms
step:1758/2330 train_time:108431ms step_avg:61.68ms
step:1759/2330 train_time:108490ms step_avg:61.68ms
step:1760/2330 train_time:108553ms step_avg:61.68ms
step:1761/2330 train_time:108613ms step_avg:61.68ms
step:1762/2330 train_time:108675ms step_avg:61.68ms
step:1763/2330 train_time:108735ms step_avg:61.68ms
step:1764/2330 train_time:108798ms step_avg:61.68ms
step:1765/2330 train_time:108858ms step_avg:61.68ms
step:1766/2330 train_time:108924ms step_avg:61.68ms
step:1767/2330 train_time:108987ms step_avg:61.68ms
step:1768/2330 train_time:109052ms step_avg:61.68ms
step:1769/2330 train_time:109114ms step_avg:61.68ms
step:1770/2330 train_time:109178ms step_avg:61.68ms
step:1771/2330 train_time:109240ms step_avg:61.68ms
step:1772/2330 train_time:109304ms step_avg:61.68ms
step:1773/2330 train_time:109367ms step_avg:61.68ms
step:1774/2330 train_time:109429ms step_avg:61.69ms
step:1775/2330 train_time:109490ms step_avg:61.68ms
step:1776/2330 train_time:109553ms step_avg:61.69ms
step:1777/2330 train_time:109613ms step_avg:61.68ms
step:1778/2330 train_time:109675ms step_avg:61.68ms
step:1779/2330 train_time:109735ms step_avg:61.68ms
step:1780/2330 train_time:109798ms step_avg:61.68ms
step:1781/2330 train_time:109861ms step_avg:61.68ms
step:1782/2330 train_time:109924ms step_avg:61.69ms
step:1783/2330 train_time:109986ms step_avg:61.69ms
step:1784/2330 train_time:110051ms step_avg:61.69ms
step:1785/2330 train_time:110113ms step_avg:61.69ms
step:1786/2330 train_time:110177ms step_avg:61.69ms
step:1787/2330 train_time:110240ms step_avg:61.69ms
step:1788/2330 train_time:110304ms step_avg:61.69ms
step:1789/2330 train_time:110365ms step_avg:61.69ms
step:1790/2330 train_time:110429ms step_avg:61.69ms
step:1791/2330 train_time:110489ms step_avg:61.69ms
step:1792/2330 train_time:110552ms step_avg:61.69ms
step:1793/2330 train_time:110612ms step_avg:61.69ms
step:1794/2330 train_time:110675ms step_avg:61.69ms
step:1795/2330 train_time:110735ms step_avg:61.69ms
step:1796/2330 train_time:110799ms step_avg:61.69ms
step:1797/2330 train_time:110861ms step_avg:61.69ms
step:1798/2330 train_time:110924ms step_avg:61.69ms
step:1799/2330 train_time:110986ms step_avg:61.69ms
step:1800/2330 train_time:111050ms step_avg:61.69ms
step:1801/2330 train_time:111112ms step_avg:61.69ms
step:1802/2330 train_time:111176ms step_avg:61.70ms
step:1803/2330 train_time:111238ms step_avg:61.70ms
step:1804/2330 train_time:111303ms step_avg:61.70ms
step:1805/2330 train_time:111365ms step_avg:61.70ms
step:1806/2330 train_time:111429ms step_avg:61.70ms
step:1807/2330 train_time:111490ms step_avg:61.70ms
step:1808/2330 train_time:111553ms step_avg:61.70ms
step:1809/2330 train_time:111615ms step_avg:61.70ms
step:1810/2330 train_time:111678ms step_avg:61.70ms
step:1811/2330 train_time:111739ms step_avg:61.70ms
step:1812/2330 train_time:111802ms step_avg:61.70ms
step:1813/2330 train_time:111864ms step_avg:61.70ms
step:1814/2330 train_time:111927ms step_avg:61.70ms
step:1815/2330 train_time:111989ms step_avg:61.70ms
step:1816/2330 train_time:112053ms step_avg:61.70ms
step:1817/2330 train_time:112115ms step_avg:61.70ms
step:1818/2330 train_time:112178ms step_avg:61.70ms
step:1819/2330 train_time:112240ms step_avg:61.70ms
step:1820/2330 train_time:112304ms step_avg:61.71ms
step:1821/2330 train_time:112365ms step_avg:61.71ms
step:1822/2330 train_time:112429ms step_avg:61.71ms
step:1823/2330 train_time:112490ms step_avg:61.71ms
step:1824/2330 train_time:112553ms step_avg:61.71ms
step:1825/2330 train_time:112613ms step_avg:61.71ms
step:1826/2330 train_time:112677ms step_avg:61.71ms
step:1827/2330 train_time:112737ms step_avg:61.71ms
step:1828/2330 train_time:112800ms step_avg:61.71ms
step:1829/2330 train_time:112861ms step_avg:61.71ms
step:1830/2330 train_time:112925ms step_avg:61.71ms
step:1831/2330 train_time:112987ms step_avg:61.71ms
step:1832/2330 train_time:113050ms step_avg:61.71ms
step:1833/2330 train_time:113111ms step_avg:61.71ms
step:1834/2330 train_time:113175ms step_avg:61.71ms
step:1835/2330 train_time:113237ms step_avg:61.71ms
step:1836/2330 train_time:113302ms step_avg:61.71ms
step:1837/2330 train_time:113364ms step_avg:61.71ms
step:1838/2330 train_time:113426ms step_avg:61.71ms
step:1839/2330 train_time:113488ms step_avg:61.71ms
step:1840/2330 train_time:113553ms step_avg:61.71ms
step:1841/2330 train_time:113614ms step_avg:61.71ms
step:1842/2330 train_time:113676ms step_avg:61.71ms
step:1843/2330 train_time:113737ms step_avg:61.71ms
step:1844/2330 train_time:113800ms step_avg:61.71ms
step:1845/2330 train_time:113863ms step_avg:61.71ms
step:1846/2330 train_time:113925ms step_avg:61.71ms
step:1847/2330 train_time:113987ms step_avg:61.71ms
step:1848/2330 train_time:114051ms step_avg:61.72ms
step:1849/2330 train_time:114112ms step_avg:61.72ms
step:1850/2330 train_time:114175ms step_avg:61.72ms
step:1851/2330 train_time:114236ms step_avg:61.72ms
step:1852/2330 train_time:114301ms step_avg:61.72ms
step:1853/2330 train_time:114363ms step_avg:61.72ms
step:1854/2330 train_time:114426ms step_avg:61.72ms
step:1855/2330 train_time:114488ms step_avg:61.72ms
step:1856/2330 train_time:114552ms step_avg:61.72ms
step:1857/2330 train_time:114613ms step_avg:61.72ms
step:1858/2330 train_time:114675ms step_avg:61.72ms
step:1859/2330 train_time:114736ms step_avg:61.72ms
step:1860/2330 train_time:114799ms step_avg:61.72ms
step:1861/2330 train_time:114862ms step_avg:61.72ms
step:1862/2330 train_time:114925ms step_avg:61.72ms
step:1863/2330 train_time:114986ms step_avg:61.72ms
step:1864/2330 train_time:115050ms step_avg:61.72ms
step:1865/2330 train_time:115111ms step_avg:61.72ms
step:1866/2330 train_time:115175ms step_avg:61.72ms
step:1867/2330 train_time:115236ms step_avg:61.72ms
step:1868/2330 train_time:115300ms step_avg:61.72ms
step:1869/2330 train_time:115361ms step_avg:61.72ms
step:1870/2330 train_time:115425ms step_avg:61.72ms
step:1871/2330 train_time:115486ms step_avg:61.72ms
step:1872/2330 train_time:115551ms step_avg:61.73ms
step:1873/2330 train_time:115611ms step_avg:61.73ms
step:1874/2330 train_time:115674ms step_avg:61.73ms
step:1875/2330 train_time:115735ms step_avg:61.73ms
step:1876/2330 train_time:115798ms step_avg:61.73ms
step:1877/2330 train_time:115860ms step_avg:61.73ms
step:1878/2330 train_time:115923ms step_avg:61.73ms
step:1879/2330 train_time:115985ms step_avg:61.73ms
step:1880/2330 train_time:116049ms step_avg:61.73ms
step:1881/2330 train_time:116111ms step_avg:61.73ms
step:1882/2330 train_time:116174ms step_avg:61.73ms
step:1883/2330 train_time:116235ms step_avg:61.73ms
step:1884/2330 train_time:116298ms step_avg:61.73ms
step:1885/2330 train_time:116360ms step_avg:61.73ms
step:1886/2330 train_time:116423ms step_avg:61.73ms
step:1887/2330 train_time:116484ms step_avg:61.73ms
step:1888/2330 train_time:116548ms step_avg:61.73ms
step:1889/2330 train_time:116609ms step_avg:61.73ms
step:1890/2330 train_time:116673ms step_avg:61.73ms
step:1891/2330 train_time:116734ms step_avg:61.73ms
step:1892/2330 train_time:116797ms step_avg:61.73ms
step:1893/2330 train_time:116858ms step_avg:61.73ms
step:1894/2330 train_time:116922ms step_avg:61.73ms
step:1895/2330 train_time:116983ms step_avg:61.73ms
step:1896/2330 train_time:117048ms step_avg:61.73ms
step:1897/2330 train_time:117109ms step_avg:61.73ms
step:1898/2330 train_time:117172ms step_avg:61.73ms
step:1899/2330 train_time:117232ms step_avg:61.73ms
step:1900/2330 train_time:117295ms step_avg:61.73ms
step:1901/2330 train_time:117358ms step_avg:61.73ms
step:1902/2330 train_time:117421ms step_avg:61.74ms
step:1903/2330 train_time:117483ms step_avg:61.74ms
step:1904/2330 train_time:117546ms step_avg:61.74ms
step:1905/2330 train_time:117608ms step_avg:61.74ms
step:1906/2330 train_time:117672ms step_avg:61.74ms
step:1907/2330 train_time:117732ms step_avg:61.74ms
step:1908/2330 train_time:117795ms step_avg:61.74ms
step:1909/2330 train_time:117857ms step_avg:61.74ms
step:1910/2330 train_time:117920ms step_avg:61.74ms
step:1911/2330 train_time:117982ms step_avg:61.74ms
step:1912/2330 train_time:118046ms step_avg:61.74ms
step:1913/2330 train_time:118107ms step_avg:61.74ms
step:1914/2330 train_time:118171ms step_avg:61.74ms
step:1915/2330 train_time:118232ms step_avg:61.74ms
step:1916/2330 train_time:118294ms step_avg:61.74ms
step:1917/2330 train_time:118356ms step_avg:61.74ms
step:1918/2330 train_time:118419ms step_avg:61.74ms
step:1919/2330 train_time:118481ms step_avg:61.74ms
step:1920/2330 train_time:118544ms step_avg:61.74ms
step:1921/2330 train_time:118606ms step_avg:61.74ms
step:1922/2330 train_time:118670ms step_avg:61.74ms
step:1923/2330 train_time:118732ms step_avg:61.74ms
step:1924/2330 train_time:118795ms step_avg:61.74ms
step:1925/2330 train_time:118857ms step_avg:61.74ms
step:1926/2330 train_time:118920ms step_avg:61.74ms
step:1927/2330 train_time:118982ms step_avg:61.74ms
step:1928/2330 train_time:119045ms step_avg:61.75ms
step:1929/2330 train_time:119107ms step_avg:61.75ms
step:1930/2330 train_time:119171ms step_avg:61.75ms
step:1931/2330 train_time:119232ms step_avg:61.75ms
step:1932/2330 train_time:119295ms step_avg:61.75ms
step:1933/2330 train_time:119356ms step_avg:61.75ms
step:1934/2330 train_time:119420ms step_avg:61.75ms
step:1935/2330 train_time:119482ms step_avg:61.75ms
step:1936/2330 train_time:119546ms step_avg:61.75ms
step:1937/2330 train_time:119608ms step_avg:61.75ms
step:1938/2330 train_time:119671ms step_avg:61.75ms
step:1939/2330 train_time:119732ms step_avg:61.75ms
step:1940/2330 train_time:119794ms step_avg:61.75ms
step:1941/2330 train_time:119855ms step_avg:61.75ms
step:1942/2330 train_time:119919ms step_avg:61.75ms
step:1943/2330 train_time:119980ms step_avg:61.75ms
step:1944/2330 train_time:120044ms step_avg:61.75ms
step:1945/2330 train_time:120106ms step_avg:61.75ms
step:1946/2330 train_time:120169ms step_avg:61.75ms
step:1947/2330 train_time:120230ms step_avg:61.75ms
step:1948/2330 train_time:120294ms step_avg:61.75ms
step:1949/2330 train_time:120355ms step_avg:61.75ms
step:1950/2330 train_time:120418ms step_avg:61.75ms
step:1951/2330 train_time:120479ms step_avg:61.75ms
step:1952/2330 train_time:120544ms step_avg:61.75ms
step:1953/2330 train_time:120606ms step_avg:61.75ms
step:1954/2330 train_time:120669ms step_avg:61.76ms
step:1955/2330 train_time:120731ms step_avg:61.75ms
step:1956/2330 train_time:120794ms step_avg:61.76ms
step:1957/2330 train_time:120855ms step_avg:61.76ms
step:1958/2330 train_time:120920ms step_avg:61.76ms
step:1959/2330 train_time:120982ms step_avg:61.76ms
step:1960/2330 train_time:121046ms step_avg:61.76ms
step:1961/2330 train_time:121108ms step_avg:61.76ms
step:1962/2330 train_time:121171ms step_avg:61.76ms
step:1963/2330 train_time:121232ms step_avg:61.76ms
step:1964/2330 train_time:121295ms step_avg:61.76ms
step:1965/2330 train_time:121356ms step_avg:61.76ms
step:1966/2330 train_time:121419ms step_avg:61.76ms
step:1967/2330 train_time:121482ms step_avg:61.76ms
step:1968/2330 train_time:121546ms step_avg:61.76ms
step:1969/2330 train_time:121607ms step_avg:61.76ms
step:1970/2330 train_time:121671ms step_avg:61.76ms
step:1971/2330 train_time:121732ms step_avg:61.76ms
step:1972/2330 train_time:121795ms step_avg:61.76ms
step:1973/2330 train_time:121856ms step_avg:61.76ms
step:1974/2330 train_time:121920ms step_avg:61.76ms
step:1975/2330 train_time:121982ms step_avg:61.76ms
step:1976/2330 train_time:122045ms step_avg:61.76ms
step:1977/2330 train_time:122107ms step_avg:61.76ms
step:1978/2330 train_time:122171ms step_avg:61.76ms
step:1979/2330 train_time:122232ms step_avg:61.76ms
step:1980/2330 train_time:122295ms step_avg:61.77ms
step:1981/2330 train_time:122356ms step_avg:61.76ms
step:1982/2330 train_time:122420ms step_avg:61.77ms
step:1983/2330 train_time:122482ms step_avg:61.77ms
step:1984/2330 train_time:122545ms step_avg:61.77ms
step:1985/2330 train_time:122607ms step_avg:61.77ms
step:1986/2330 train_time:122671ms step_avg:61.77ms
step:1987/2330 train_time:122732ms step_avg:61.77ms
step:1988/2330 train_time:122795ms step_avg:61.77ms
step:1989/2330 train_time:122856ms step_avg:61.77ms
step:1990/2330 train_time:122921ms step_avg:61.77ms
step:1991/2330 train_time:122983ms step_avg:61.77ms
step:1992/2330 train_time:123047ms step_avg:61.77ms
step:1993/2330 train_time:123109ms step_avg:61.77ms
step:1994/2330 train_time:123174ms step_avg:61.77ms
step:1995/2330 train_time:123234ms step_avg:61.77ms
step:1996/2330 train_time:123298ms step_avg:61.77ms
step:1997/2330 train_time:123358ms step_avg:61.77ms
step:1998/2330 train_time:123422ms step_avg:61.77ms
step:1999/2330 train_time:123483ms step_avg:61.77ms
step:2000/2330 train_time:123547ms step_avg:61.77ms
step:2000/2330 val_loss:3.5701 train_time:123612ms step_avg:61.81ms
step:2001/2330 train_time:123638ms step_avg:61.79ms
step:2002/2330 train_time:123675ms step_avg:61.78ms
step:2003/2330 train_time:123743ms step_avg:61.78ms
step:2004/2330 train_time:123811ms step_avg:61.78ms
step:2005/2330 train_time:123873ms step_avg:61.78ms
step:2006/2330 train_time:123937ms step_avg:61.78ms
step:2007/2330 train_time:123998ms step_avg:61.78ms
step:2008/2330 train_time:124061ms step_avg:61.78ms
step:2009/2330 train_time:124121ms step_avg:61.78ms
step:2010/2330 train_time:124184ms step_avg:61.78ms
step:2011/2330 train_time:124244ms step_avg:61.78ms
step:2012/2330 train_time:124307ms step_avg:61.78ms
step:2013/2330 train_time:124368ms step_avg:61.78ms
step:2014/2330 train_time:124430ms step_avg:61.78ms
step:2015/2330 train_time:124490ms step_avg:61.78ms
step:2016/2330 train_time:124553ms step_avg:61.78ms
step:2017/2330 train_time:124616ms step_avg:61.78ms
step:2018/2330 train_time:124682ms step_avg:61.78ms
step:2019/2330 train_time:124744ms step_avg:61.79ms
step:2020/2330 train_time:124809ms step_avg:61.79ms
step:2021/2330 train_time:124871ms step_avg:61.79ms
step:2022/2330 train_time:124934ms step_avg:61.79ms
step:2023/2330 train_time:124995ms step_avg:61.79ms
step:2024/2330 train_time:125059ms step_avg:61.79ms
step:2025/2330 train_time:125120ms step_avg:61.79ms
step:2026/2330 train_time:125184ms step_avg:61.79ms
step:2027/2330 train_time:125244ms step_avg:61.79ms
step:2028/2330 train_time:125307ms step_avg:61.79ms
step:2029/2330 train_time:125368ms step_avg:61.79ms
step:2030/2330 train_time:125430ms step_avg:61.79ms
step:2031/2330 train_time:125491ms step_avg:61.79ms
step:2032/2330 train_time:125554ms step_avg:61.79ms
step:2033/2330 train_time:125616ms step_avg:61.79ms
step:2034/2330 train_time:125682ms step_avg:61.79ms
step:2035/2330 train_time:125744ms step_avg:61.79ms
step:2036/2330 train_time:125808ms step_avg:61.79ms
step:2037/2330 train_time:125871ms step_avg:61.79ms
step:2038/2330 train_time:125934ms step_avg:61.79ms
step:2039/2330 train_time:125995ms step_avg:61.79ms
step:2040/2330 train_time:126059ms step_avg:61.79ms
step:2041/2330 train_time:126120ms step_avg:61.79ms
step:2042/2330 train_time:126183ms step_avg:61.79ms
step:2043/2330 train_time:126244ms step_avg:61.79ms
step:2044/2330 train_time:126307ms step_avg:61.79ms
step:2045/2330 train_time:126369ms step_avg:61.79ms
step:2046/2330 train_time:126431ms step_avg:61.79ms
step:2047/2330 train_time:126491ms step_avg:61.79ms
step:2048/2330 train_time:126555ms step_avg:61.79ms
step:2049/2330 train_time:126616ms step_avg:61.79ms
step:2050/2330 train_time:126681ms step_avg:61.80ms
step:2051/2330 train_time:126743ms step_avg:61.80ms
step:2052/2330 train_time:126808ms step_avg:61.80ms
step:2053/2330 train_time:126870ms step_avg:61.80ms
step:2054/2330 train_time:126933ms step_avg:61.80ms
step:2055/2330 train_time:126994ms step_avg:61.80ms
step:2056/2330 train_time:127058ms step_avg:61.80ms
step:2057/2330 train_time:127120ms step_avg:61.80ms
step:2058/2330 train_time:127183ms step_avg:61.80ms
step:2059/2330 train_time:127244ms step_avg:61.80ms
step:2060/2330 train_time:127308ms step_avg:61.80ms
step:2061/2330 train_time:127368ms step_avg:61.80ms
step:2062/2330 train_time:127431ms step_avg:61.80ms
step:2063/2330 train_time:127493ms step_avg:61.80ms
step:2064/2330 train_time:127556ms step_avg:61.80ms
step:2065/2330 train_time:127618ms step_avg:61.80ms
step:2066/2330 train_time:127683ms step_avg:61.80ms
step:2067/2330 train_time:127744ms step_avg:61.80ms
step:2068/2330 train_time:127809ms step_avg:61.80ms
step:2069/2330 train_time:127870ms step_avg:61.80ms
step:2070/2330 train_time:127934ms step_avg:61.80ms
step:2071/2330 train_time:127995ms step_avg:61.80ms
step:2072/2330 train_time:128060ms step_avg:61.80ms
step:2073/2330 train_time:128122ms step_avg:61.81ms
step:2074/2330 train_time:128186ms step_avg:61.81ms
step:2075/2330 train_time:128247ms step_avg:61.81ms
step:2076/2330 train_time:128310ms step_avg:61.81ms
step:2077/2330 train_time:128371ms step_avg:61.81ms
step:2078/2330 train_time:128433ms step_avg:61.81ms
step:2079/2330 train_time:128495ms step_avg:61.81ms
step:2080/2330 train_time:128558ms step_avg:61.81ms
step:2081/2330 train_time:128620ms step_avg:61.81ms
step:2082/2330 train_time:128684ms step_avg:61.81ms
step:2083/2330 train_time:128746ms step_avg:61.81ms
step:2084/2330 train_time:128810ms step_avg:61.81ms
step:2085/2330 train_time:128871ms step_avg:61.81ms
step:2086/2330 train_time:128934ms step_avg:61.81ms
step:2087/2330 train_time:128995ms step_avg:61.81ms
step:2088/2330 train_time:129059ms step_avg:61.81ms
step:2089/2330 train_time:129120ms step_avg:61.81ms
step:2090/2330 train_time:129185ms step_avg:61.81ms
step:2091/2330 train_time:129246ms step_avg:61.81ms
step:2092/2330 train_time:129309ms step_avg:61.81ms
step:2093/2330 train_time:129371ms step_avg:61.81ms
step:2094/2330 train_time:129434ms step_avg:61.81ms
step:2095/2330 train_time:129495ms step_avg:61.81ms
step:2096/2330 train_time:129559ms step_avg:61.81ms
step:2097/2330 train_time:129621ms step_avg:61.81ms
step:2098/2330 train_time:129685ms step_avg:61.81ms
step:2099/2330 train_time:129748ms step_avg:61.81ms
step:2100/2330 train_time:129811ms step_avg:61.81ms
step:2101/2330 train_time:129872ms step_avg:61.81ms
step:2102/2330 train_time:129935ms step_avg:61.81ms
step:2103/2330 train_time:129996ms step_avg:61.81ms
step:2104/2330 train_time:130059ms step_avg:61.82ms
step:2105/2330 train_time:130122ms step_avg:61.82ms
step:2106/2330 train_time:130186ms step_avg:61.82ms
step:2107/2330 train_time:130247ms step_avg:61.82ms
step:2108/2330 train_time:130310ms step_avg:61.82ms
step:2109/2330 train_time:130371ms step_avg:61.82ms
step:2110/2330 train_time:130434ms step_avg:61.82ms
step:2111/2330 train_time:130495ms step_avg:61.82ms
step:2112/2330 train_time:130558ms step_avg:61.82ms
step:2113/2330 train_time:130620ms step_avg:61.82ms
step:2114/2330 train_time:130683ms step_avg:61.82ms
step:2115/2330 train_time:130745ms step_avg:61.82ms
step:2116/2330 train_time:130809ms step_avg:61.82ms
step:2117/2330 train_time:130870ms step_avg:61.82ms
step:2118/2330 train_time:130933ms step_avg:61.82ms
step:2119/2330 train_time:130995ms step_avg:61.82ms
step:2120/2330 train_time:131059ms step_avg:61.82ms
step:2121/2330 train_time:131121ms step_avg:61.82ms
step:2122/2330 train_time:131186ms step_avg:61.82ms
step:2123/2330 train_time:131247ms step_avg:61.82ms
step:2124/2330 train_time:131311ms step_avg:61.82ms
step:2125/2330 train_time:131372ms step_avg:61.82ms
step:2126/2330 train_time:131434ms step_avg:61.82ms
step:2127/2330 train_time:131495ms step_avg:61.82ms
step:2128/2330 train_time:131559ms step_avg:61.82ms
step:2129/2330 train_time:131621ms step_avg:61.82ms
step:2130/2330 train_time:131684ms step_avg:61.82ms
step:2131/2330 train_time:131746ms step_avg:61.82ms
step:2132/2330 train_time:131809ms step_avg:61.82ms
step:2133/2330 train_time:131870ms step_avg:61.82ms
step:2134/2330 train_time:131933ms step_avg:61.82ms
step:2135/2330 train_time:131994ms step_avg:61.82ms
step:2136/2330 train_time:132058ms step_avg:61.82ms
step:2137/2330 train_time:132121ms step_avg:61.83ms
step:2138/2330 train_time:132186ms step_avg:61.83ms
step:2139/2330 train_time:132247ms step_avg:61.83ms
step:2140/2330 train_time:132310ms step_avg:61.83ms
step:2141/2330 train_time:132371ms step_avg:61.83ms
step:2142/2330 train_time:132435ms step_avg:61.83ms
step:2143/2330 train_time:132496ms step_avg:61.83ms
step:2144/2330 train_time:132561ms step_avg:61.83ms
step:2145/2330 train_time:132623ms step_avg:61.83ms
step:2146/2330 train_time:132687ms step_avg:61.83ms
step:2147/2330 train_time:132748ms step_avg:61.83ms
step:2148/2330 train_time:132812ms step_avg:61.83ms
step:2149/2330 train_time:132873ms step_avg:61.83ms
step:2150/2330 train_time:132936ms step_avg:61.83ms
step:2151/2330 train_time:132998ms step_avg:61.83ms
step:2152/2330 train_time:133062ms step_avg:61.83ms
step:2153/2330 train_time:133125ms step_avg:61.83ms
step:2154/2330 train_time:133188ms step_avg:61.83ms
step:2155/2330 train_time:133249ms step_avg:61.83ms
step:2156/2330 train_time:133313ms step_avg:61.83ms
step:2157/2330 train_time:133374ms step_avg:61.83ms
step:2158/2330 train_time:133438ms step_avg:61.83ms
step:2159/2330 train_time:133499ms step_avg:61.83ms
step:2160/2330 train_time:133564ms step_avg:61.83ms
step:2161/2330 train_time:133625ms step_avg:61.84ms
step:2162/2330 train_time:133690ms step_avg:61.84ms
step:2163/2330 train_time:133751ms step_avg:61.84ms
step:2164/2330 train_time:133814ms step_avg:61.84ms
step:2165/2330 train_time:133875ms step_avg:61.84ms
step:2166/2330 train_time:133939ms step_avg:61.84ms
step:2167/2330 train_time:134000ms step_avg:61.84ms
step:2168/2330 train_time:134064ms step_avg:61.84ms
step:2169/2330 train_time:134126ms step_avg:61.84ms
step:2170/2330 train_time:134189ms step_avg:61.84ms
step:2171/2330 train_time:134250ms step_avg:61.84ms
step:2172/2330 train_time:134315ms step_avg:61.84ms
step:2173/2330 train_time:134376ms step_avg:61.84ms
step:2174/2330 train_time:134439ms step_avg:61.84ms
step:2175/2330 train_time:134500ms step_avg:61.84ms
step:2176/2330 train_time:134564ms step_avg:61.84ms
step:2177/2330 train_time:134625ms step_avg:61.84ms
step:2178/2330 train_time:134689ms step_avg:61.84ms
step:2179/2330 train_time:134751ms step_avg:61.84ms
step:2180/2330 train_time:134814ms step_avg:61.84ms
step:2181/2330 train_time:134875ms step_avg:61.84ms
step:2182/2330 train_time:134939ms step_avg:61.84ms
step:2183/2330 train_time:135002ms step_avg:61.84ms
step:2184/2330 train_time:135067ms step_avg:61.84ms
step:2185/2330 train_time:135128ms step_avg:61.84ms
step:2186/2330 train_time:135192ms step_avg:61.84ms
step:2187/2330 train_time:135253ms step_avg:61.84ms
step:2188/2330 train_time:135317ms step_avg:61.84ms
step:2189/2330 train_time:135378ms step_avg:61.84ms
step:2190/2330 train_time:135442ms step_avg:61.85ms
step:2191/2330 train_time:135504ms step_avg:61.85ms
step:2192/2330 train_time:135568ms step_avg:61.85ms
step:2193/2330 train_time:135630ms step_avg:61.85ms
step:2194/2330 train_time:135693ms step_avg:61.85ms
step:2195/2330 train_time:135754ms step_avg:61.85ms
step:2196/2330 train_time:135817ms step_avg:61.85ms
step:2197/2330 train_time:135879ms step_avg:61.85ms
step:2198/2330 train_time:135943ms step_avg:61.85ms
step:2199/2330 train_time:136005ms step_avg:61.85ms
step:2200/2330 train_time:136068ms step_avg:61.85ms
step:2201/2330 train_time:136130ms step_avg:61.85ms
step:2202/2330 train_time:136193ms step_avg:61.85ms
step:2203/2330 train_time:136255ms step_avg:61.85ms
step:2204/2330 train_time:136318ms step_avg:61.85ms
step:2205/2330 train_time:136379ms step_avg:61.85ms
step:2206/2330 train_time:136443ms step_avg:61.85ms
step:2207/2330 train_time:136504ms step_avg:61.85ms
step:2208/2330 train_time:136569ms step_avg:61.85ms
step:2209/2330 train_time:136630ms step_avg:61.85ms
step:2210/2330 train_time:136693ms step_avg:61.85ms
step:2211/2330 train_time:136755ms step_avg:61.85ms
step:2212/2330 train_time:136819ms step_avg:61.85ms
step:2213/2330 train_time:136880ms step_avg:61.85ms
step:2214/2330 train_time:136943ms step_avg:61.85ms
step:2215/2330 train_time:137005ms step_avg:61.85ms
step:2216/2330 train_time:137069ms step_avg:61.85ms
step:2217/2330 train_time:137130ms step_avg:61.85ms
step:2218/2330 train_time:137193ms step_avg:61.85ms
step:2219/2330 train_time:137254ms step_avg:61.85ms
step:2220/2330 train_time:137318ms step_avg:61.85ms
step:2221/2330 train_time:137380ms step_avg:61.86ms
step:2222/2330 train_time:137444ms step_avg:61.86ms
step:2223/2330 train_time:137505ms step_avg:61.86ms
step:2224/2330 train_time:137569ms step_avg:61.86ms
step:2225/2330 train_time:137630ms step_avg:61.86ms
step:2226/2330 train_time:137693ms step_avg:61.86ms
step:2227/2330 train_time:137754ms step_avg:61.86ms
step:2228/2330 train_time:137818ms step_avg:61.86ms
step:2229/2330 train_time:137880ms step_avg:61.86ms
step:2230/2330 train_time:137943ms step_avg:61.86ms
step:2231/2330 train_time:138004ms step_avg:61.86ms
step:2232/2330 train_time:138068ms step_avg:61.86ms
step:2233/2330 train_time:138129ms step_avg:61.86ms
step:2234/2330 train_time:138193ms step_avg:61.86ms
step:2235/2330 train_time:138253ms step_avg:61.86ms
step:2236/2330 train_time:138318ms step_avg:61.86ms
step:2237/2330 train_time:138380ms step_avg:61.86ms
step:2238/2330 train_time:138444ms step_avg:61.86ms
step:2239/2330 train_time:138505ms step_avg:61.86ms
step:2240/2330 train_time:138569ms step_avg:61.86ms
step:2241/2330 train_time:138629ms step_avg:61.86ms
step:2242/2330 train_time:138693ms step_avg:61.86ms
step:2243/2330 train_time:138754ms step_avg:61.86ms
step:2244/2330 train_time:138818ms step_avg:61.86ms
step:2245/2330 train_time:138880ms step_avg:61.86ms
step:2246/2330 train_time:138944ms step_avg:61.86ms
step:2247/2330 train_time:139005ms step_avg:61.86ms
step:2248/2330 train_time:139068ms step_avg:61.86ms
step:2249/2330 train_time:139129ms step_avg:61.86ms
step:2250/2330 train_time:139192ms step_avg:61.86ms
step:2250/2330 val_loss:3.5311 train_time:139258ms step_avg:61.89ms
step:2251/2330 train_time:139282ms step_avg:61.88ms
step:2252/2330 train_time:139322ms step_avg:61.87ms
step:2253/2330 train_time:139390ms step_avg:61.87ms
step:2254/2330 train_time:139456ms step_avg:61.87ms
step:2255/2330 train_time:139518ms step_avg:61.87ms
step:2256/2330 train_time:139582ms step_avg:61.87ms
step:2257/2330 train_time:139641ms step_avg:61.87ms
step:2258/2330 train_time:139704ms step_avg:61.87ms
step:2259/2330 train_time:139764ms step_avg:61.87ms
step:2260/2330 train_time:139827ms step_avg:61.87ms
step:2261/2330 train_time:139887ms step_avg:61.87ms
step:2262/2330 train_time:139950ms step_avg:61.87ms
step:2263/2330 train_time:140011ms step_avg:61.87ms
step:2264/2330 train_time:140073ms step_avg:61.87ms
step:2265/2330 train_time:140134ms step_avg:61.87ms
step:2266/2330 train_time:140198ms step_avg:61.87ms
step:2267/2330 train_time:140260ms step_avg:61.87ms
step:2268/2330 train_time:140326ms step_avg:61.87ms
step:2269/2330 train_time:140388ms step_avg:61.87ms
step:2270/2330 train_time:140453ms step_avg:61.87ms
step:2271/2330 train_time:140515ms step_avg:61.87ms
step:2272/2330 train_time:140579ms step_avg:61.87ms
step:2273/2330 train_time:140640ms step_avg:61.87ms
step:2274/2330 train_time:140703ms step_avg:61.87ms
step:2275/2330 train_time:140764ms step_avg:61.87ms
step:2276/2330 train_time:140827ms step_avg:61.87ms
step:2277/2330 train_time:140887ms step_avg:61.87ms
step:2278/2330 train_time:140949ms step_avg:61.87ms
step:2279/2330 train_time:141010ms step_avg:61.87ms
step:2280/2330 train_time:141073ms step_avg:61.87ms
step:2281/2330 train_time:141134ms step_avg:61.87ms
step:2282/2330 train_time:141197ms step_avg:61.87ms
step:2283/2330 train_time:141259ms step_avg:61.87ms
step:2284/2330 train_time:141322ms step_avg:61.87ms
step:2285/2330 train_time:141384ms step_avg:61.87ms
step:2286/2330 train_time:141448ms step_avg:61.88ms
step:2287/2330 train_time:141511ms step_avg:61.88ms
step:2288/2330 train_time:141574ms step_avg:61.88ms
step:2289/2330 train_time:141636ms step_avg:61.88ms
step:2290/2330 train_time:141700ms step_avg:61.88ms
step:2291/2330 train_time:141761ms step_avg:61.88ms
step:2292/2330 train_time:141824ms step_avg:61.88ms
step:2293/2330 train_time:141885ms step_avg:61.88ms
step:2294/2330 train_time:141947ms step_avg:61.88ms
step:2295/2330 train_time:142008ms step_avg:61.88ms
step:2296/2330 train_time:142072ms step_avg:61.88ms
step:2297/2330 train_time:142134ms step_avg:61.88ms
step:2298/2330 train_time:142197ms step_avg:61.88ms
step:2299/2330 train_time:142258ms step_avg:61.88ms
step:2300/2330 train_time:142322ms step_avg:61.88ms
step:2301/2330 train_time:142383ms step_avg:61.88ms
step:2302/2330 train_time:142446ms step_avg:61.88ms
step:2303/2330 train_time:142508ms step_avg:61.88ms
step:2304/2330 train_time:142572ms step_avg:61.88ms
step:2305/2330 train_time:142634ms step_avg:61.88ms
step:2306/2330 train_time:142698ms step_avg:61.88ms
step:2307/2330 train_time:142760ms step_avg:61.88ms
step:2308/2330 train_time:142824ms step_avg:61.88ms
step:2309/2330 train_time:142885ms step_avg:61.88ms
step:2310/2330 train_time:142947ms step_avg:61.88ms
step:2311/2330 train_time:143008ms step_avg:61.88ms
step:2312/2330 train_time:143070ms step_avg:61.88ms
step:2313/2330 train_time:143132ms step_avg:61.88ms
step:2314/2330 train_time:143195ms step_avg:61.88ms
step:2315/2330 train_time:143256ms step_avg:61.88ms
step:2316/2330 train_time:143319ms step_avg:61.88ms
step:2317/2330 train_time:143381ms step_avg:61.88ms
step:2318/2330 train_time:143444ms step_avg:61.88ms
step:2319/2330 train_time:143506ms step_avg:61.88ms
step:2320/2330 train_time:143569ms step_avg:61.88ms
step:2321/2330 train_time:143631ms step_avg:61.88ms
step:2322/2330 train_time:143695ms step_avg:61.88ms
step:2323/2330 train_time:143756ms step_avg:61.88ms
step:2324/2330 train_time:143820ms step_avg:61.88ms
step:2325/2330 train_time:143881ms step_avg:61.88ms
step:2326/2330 train_time:143944ms step_avg:61.88ms
step:2327/2330 train_time:144004ms step_avg:61.88ms
step:2328/2330 train_time:144067ms step_avg:61.88ms
step:2329/2330 train_time:144129ms step_avg:61.88ms
step:2330/2330 train_time:144193ms step_avg:61.89ms
step:2330/2330 val_loss:3.5194 train_time:144258ms step_avg:61.91ms
peak memory allocated: 29226 MiB reserved: 44076 MiB
