import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_muon_lr3e-2"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=3e-2, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 04:13:57 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   33C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   31C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   31C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:70ms step_avg:69.87ms
step:2/2330 train_time:144ms step_avg:71.83ms
step:3/2330 train_time:156ms step_avg:52.00ms
step:4/2330 train_time:169ms step_avg:42.28ms
step:5/2330 train_time:181ms step_avg:36.18ms
step:6/2330 train_time:215ms step_avg:35.87ms
step:7/2330 train_time:249ms step_avg:35.62ms
step:8/2330 train_time:293ms step_avg:36.61ms
step:9/2330 train_time:327ms step_avg:36.39ms
step:10/2330 train_time:371ms step_avg:37.12ms
step:11/2330 train_time:406ms step_avg:36.92ms
step:12/2330 train_time:450ms step_avg:37.52ms
step:13/2330 train_time:484ms step_avg:37.27ms
step:14/2330 train_time:529ms step_avg:37.79ms
step:15/2330 train_time:564ms step_avg:37.59ms
step:16/2330 train_time:608ms step_avg:38.02ms
step:17/2330 train_time:643ms step_avg:37.84ms
step:18/2330 train_time:688ms step_avg:38.20ms
step:19/2330 train_time:722ms step_avg:38.00ms
step:20/2330 train_time:766ms step_avg:38.30ms
step:21/2330 train_time:801ms step_avg:38.13ms
step:22/2330 train_time:845ms step_avg:38.41ms
step:23/2330 train_time:880ms step_avg:38.26ms
step:24/2330 train_time:924ms step_avg:38.50ms
step:25/2330 train_time:959ms step_avg:38.37ms
step:26/2330 train_time:1005ms step_avg:38.67ms
step:27/2330 train_time:1044ms step_avg:38.68ms
step:28/2330 train_time:1095ms step_avg:39.10ms
step:29/2330 train_time:1133ms step_avg:39.06ms
step:30/2330 train_time:1179ms step_avg:39.29ms
step:31/2330 train_time:1214ms step_avg:39.16ms
step:32/2330 train_time:1258ms step_avg:39.31ms
step:33/2330 train_time:1293ms step_avg:39.19ms
step:34/2330 train_time:1337ms step_avg:39.33ms
step:35/2330 train_time:1373ms step_avg:39.22ms
step:36/2330 train_time:1418ms step_avg:39.38ms
step:37/2330 train_time:1453ms step_avg:39.28ms
step:38/2330 train_time:1498ms step_avg:39.42ms
step:39/2330 train_time:1533ms step_avg:39.32ms
step:40/2330 train_time:1578ms step_avg:39.45ms
step:41/2330 train_time:1613ms step_avg:39.35ms
step:42/2330 train_time:1657ms step_avg:39.46ms
step:43/2330 train_time:1693ms step_avg:39.36ms
step:44/2330 train_time:1737ms step_avg:39.47ms
step:45/2330 train_time:1772ms step_avg:39.38ms
step:46/2330 train_time:1817ms step_avg:39.49ms
step:47/2330 train_time:1852ms step_avg:39.40ms
step:48/2330 train_time:1896ms step_avg:39.51ms
step:49/2330 train_time:1933ms step_avg:39.44ms
step:50/2330 train_time:1978ms step_avg:39.57ms
step:51/2330 train_time:2015ms step_avg:39.50ms
step:52/2330 train_time:2060ms step_avg:39.62ms
step:53/2330 train_time:2096ms step_avg:39.55ms
step:54/2330 train_time:2142ms step_avg:39.66ms
step:55/2330 train_time:2178ms step_avg:39.60ms
step:56/2330 train_time:2223ms step_avg:39.70ms
step:57/2330 train_time:2259ms step_avg:39.62ms
step:58/2330 train_time:2304ms step_avg:39.72ms
step:59/2330 train_time:2339ms step_avg:39.65ms
step:60/2330 train_time:2384ms step_avg:39.74ms
step:61/2330 train_time:2420ms step_avg:39.67ms
step:62/2330 train_time:2464ms step_avg:39.75ms
step:63/2330 train_time:2500ms step_avg:39.68ms
step:64/2330 train_time:2544ms step_avg:39.74ms
step:65/2330 train_time:2579ms step_avg:39.68ms
step:66/2330 train_time:2624ms step_avg:39.76ms
step:67/2330 train_time:2659ms step_avg:39.69ms
step:68/2330 train_time:2704ms step_avg:39.76ms
step:69/2330 train_time:2739ms step_avg:39.70ms
step:70/2330 train_time:2784ms step_avg:39.77ms
step:71/2330 train_time:2819ms step_avg:39.70ms
step:72/2330 train_time:2863ms step_avg:39.77ms
step:73/2330 train_time:2898ms step_avg:39.71ms
step:74/2330 train_time:2943ms step_avg:39.77ms
step:75/2330 train_time:2979ms step_avg:39.72ms
step:76/2330 train_time:3024ms step_avg:39.79ms
step:77/2330 train_time:3060ms step_avg:39.75ms
step:78/2330 train_time:3106ms step_avg:39.82ms
step:79/2330 train_time:3142ms step_avg:39.77ms
step:80/2330 train_time:3188ms step_avg:39.85ms
step:81/2330 train_time:3225ms step_avg:39.82ms
step:82/2330 train_time:3271ms step_avg:39.89ms
step:83/2330 train_time:3307ms step_avg:39.85ms
step:84/2330 train_time:3353ms step_avg:39.91ms
step:85/2330 train_time:3388ms step_avg:39.85ms
step:86/2330 train_time:3432ms step_avg:39.91ms
step:87/2330 train_time:3468ms step_avg:39.86ms
step:88/2330 train_time:3513ms step_avg:39.92ms
step:89/2330 train_time:3548ms step_avg:39.87ms
step:90/2330 train_time:3594ms step_avg:39.94ms
step:91/2330 train_time:3630ms step_avg:39.88ms
step:92/2330 train_time:3674ms step_avg:39.94ms
step:93/2330 train_time:3709ms step_avg:39.89ms
step:94/2330 train_time:3754ms step_avg:39.94ms
step:95/2330 train_time:3789ms step_avg:39.89ms
step:96/2330 train_time:3834ms step_avg:39.94ms
step:97/2330 train_time:3869ms step_avg:39.88ms
step:98/2330 train_time:3914ms step_avg:39.94ms
step:99/2330 train_time:3950ms step_avg:39.90ms
step:100/2330 train_time:3995ms step_avg:39.95ms
step:101/2330 train_time:4031ms step_avg:39.91ms
step:102/2330 train_time:4075ms step_avg:39.95ms
step:103/2330 train_time:4111ms step_avg:39.91ms
step:104/2330 train_time:4156ms step_avg:39.96ms
step:105/2330 train_time:4191ms step_avg:39.92ms
step:106/2330 train_time:4235ms step_avg:39.96ms
step:107/2330 train_time:4270ms step_avg:39.91ms
step:108/2330 train_time:4315ms step_avg:39.96ms
step:109/2330 train_time:4351ms step_avg:39.92ms
step:110/2330 train_time:4395ms step_avg:39.96ms
step:111/2330 train_time:4431ms step_avg:39.92ms
step:112/2330 train_time:4475ms step_avg:39.96ms
step:113/2330 train_time:4511ms step_avg:39.92ms
step:114/2330 train_time:4555ms step_avg:39.96ms
step:115/2330 train_time:4590ms step_avg:39.91ms
step:116/2330 train_time:4635ms step_avg:39.95ms
step:117/2330 train_time:4670ms step_avg:39.91ms
step:118/2330 train_time:4715ms step_avg:39.96ms
step:119/2330 train_time:4750ms step_avg:39.92ms
step:120/2330 train_time:4795ms step_avg:39.96ms
step:121/2330 train_time:4830ms step_avg:39.92ms
step:122/2330 train_time:4875ms step_avg:39.96ms
step:123/2330 train_time:4910ms step_avg:39.92ms
step:124/2330 train_time:4956ms step_avg:39.97ms
step:125/2330 train_time:4991ms step_avg:39.92ms
step:126/2330 train_time:5035ms step_avg:39.96ms
step:127/2330 train_time:5071ms step_avg:39.93ms
step:128/2330 train_time:5116ms step_avg:39.97ms
step:129/2330 train_time:5151ms step_avg:39.93ms
step:130/2330 train_time:5195ms step_avg:39.96ms
step:131/2330 train_time:5230ms step_avg:39.92ms
step:132/2330 train_time:5275ms step_avg:39.96ms
step:133/2330 train_time:5311ms step_avg:39.93ms
step:134/2330 train_time:5356ms step_avg:39.97ms
step:135/2330 train_time:5391ms step_avg:39.93ms
step:136/2330 train_time:5436ms step_avg:39.97ms
step:137/2330 train_time:5471ms step_avg:39.93ms
step:138/2330 train_time:5515ms step_avg:39.97ms
step:139/2330 train_time:5550ms step_avg:39.93ms
step:140/2330 train_time:5595ms step_avg:39.96ms
step:141/2330 train_time:5629ms step_avg:39.93ms
step:142/2330 train_time:5675ms step_avg:39.96ms
step:143/2330 train_time:5710ms step_avg:39.93ms
step:144/2330 train_time:5755ms step_avg:39.97ms
step:145/2330 train_time:5790ms step_avg:39.93ms
step:146/2330 train_time:5835ms step_avg:39.97ms
step:147/2330 train_time:5870ms step_avg:39.93ms
step:148/2330 train_time:5916ms step_avg:39.97ms
step:149/2330 train_time:5951ms step_avg:39.94ms
step:150/2330 train_time:5996ms step_avg:39.97ms
step:151/2330 train_time:6031ms step_avg:39.94ms
step:152/2330 train_time:6075ms step_avg:39.97ms
step:153/2330 train_time:6111ms step_avg:39.94ms
step:154/2330 train_time:6155ms step_avg:39.97ms
step:155/2330 train_time:6190ms step_avg:39.94ms
step:156/2330 train_time:6235ms step_avg:39.97ms
step:157/2330 train_time:6270ms step_avg:39.94ms
step:158/2330 train_time:6315ms step_avg:39.97ms
step:159/2330 train_time:6350ms step_avg:39.93ms
step:160/2330 train_time:6394ms step_avg:39.96ms
step:161/2330 train_time:6430ms step_avg:39.94ms
step:162/2330 train_time:6474ms step_avg:39.96ms
step:163/2330 train_time:6509ms step_avg:39.94ms
step:164/2330 train_time:6555ms step_avg:39.97ms
step:165/2330 train_time:6590ms step_avg:39.94ms
step:166/2330 train_time:6635ms step_avg:39.97ms
step:167/2330 train_time:6671ms step_avg:39.94ms
step:168/2330 train_time:6715ms step_avg:39.97ms
step:169/2330 train_time:6751ms step_avg:39.94ms
step:170/2330 train_time:6795ms step_avg:39.97ms
step:171/2330 train_time:6830ms step_avg:39.94ms
step:172/2330 train_time:6875ms step_avg:39.97ms
step:173/2330 train_time:6910ms step_avg:39.94ms
step:174/2330 train_time:6955ms step_avg:39.97ms
step:175/2330 train_time:6991ms step_avg:39.95ms
step:176/2330 train_time:7036ms step_avg:39.98ms
step:177/2330 train_time:7070ms step_avg:39.95ms
step:178/2330 train_time:7115ms step_avg:39.97ms
step:179/2330 train_time:7151ms step_avg:39.95ms
step:180/2330 train_time:7195ms step_avg:39.97ms
step:181/2330 train_time:7230ms step_avg:39.95ms
step:182/2330 train_time:7274ms step_avg:39.97ms
step:183/2330 train_time:7310ms step_avg:39.94ms
step:184/2330 train_time:7355ms step_avg:39.97ms
step:185/2330 train_time:7390ms step_avg:39.95ms
step:186/2330 train_time:7434ms step_avg:39.97ms
step:187/2330 train_time:7469ms step_avg:39.94ms
step:188/2330 train_time:7515ms step_avg:39.97ms
step:189/2330 train_time:7551ms step_avg:39.95ms
step:190/2330 train_time:7595ms step_avg:39.97ms
step:191/2330 train_time:7629ms step_avg:39.94ms
step:192/2330 train_time:7675ms step_avg:39.97ms
step:193/2330 train_time:7710ms step_avg:39.95ms
step:194/2330 train_time:7755ms step_avg:39.97ms
step:195/2330 train_time:7790ms step_avg:39.95ms
step:196/2330 train_time:7835ms step_avg:39.97ms
step:197/2330 train_time:7870ms step_avg:39.95ms
step:198/2330 train_time:7915ms step_avg:39.97ms
step:199/2330 train_time:7951ms step_avg:39.95ms
step:200/2330 train_time:7995ms step_avg:39.98ms
step:201/2330 train_time:8030ms step_avg:39.95ms
step:202/2330 train_time:8075ms step_avg:39.97ms
step:203/2330 train_time:8110ms step_avg:39.95ms
step:204/2330 train_time:8155ms step_avg:39.98ms
step:205/2330 train_time:8190ms step_avg:39.95ms
step:206/2330 train_time:8235ms step_avg:39.98ms
step:207/2330 train_time:8270ms step_avg:39.95ms
step:208/2330 train_time:8315ms step_avg:39.97ms
step:209/2330 train_time:8349ms step_avg:39.95ms
step:210/2330 train_time:8394ms step_avg:39.97ms
step:211/2330 train_time:8430ms step_avg:39.95ms
step:212/2330 train_time:8475ms step_avg:39.97ms
step:213/2330 train_time:8510ms step_avg:39.95ms
step:214/2330 train_time:8555ms step_avg:39.97ms
step:215/2330 train_time:8590ms step_avg:39.95ms
step:216/2330 train_time:8635ms step_avg:39.98ms
step:217/2330 train_time:8669ms step_avg:39.95ms
step:218/2330 train_time:8714ms step_avg:39.97ms
step:219/2330 train_time:8749ms step_avg:39.95ms
step:220/2330 train_time:8794ms step_avg:39.97ms
step:221/2330 train_time:8830ms step_avg:39.95ms
step:222/2330 train_time:8875ms step_avg:39.98ms
step:223/2330 train_time:8910ms step_avg:39.96ms
step:224/2330 train_time:8954ms step_avg:39.97ms
step:225/2330 train_time:8990ms step_avg:39.95ms
step:226/2330 train_time:9035ms step_avg:39.98ms
step:227/2330 train_time:9070ms step_avg:39.95ms
step:228/2330 train_time:9115ms step_avg:39.98ms
step:229/2330 train_time:9150ms step_avg:39.96ms
step:230/2330 train_time:9195ms step_avg:39.98ms
step:231/2330 train_time:9230ms step_avg:39.95ms
step:232/2330 train_time:9274ms step_avg:39.98ms
step:233/2330 train_time:9310ms step_avg:39.96ms
step:234/2330 train_time:9354ms step_avg:39.98ms
step:235/2330 train_time:9389ms step_avg:39.95ms
step:236/2330 train_time:9434ms step_avg:39.98ms
step:237/2330 train_time:9469ms step_avg:39.95ms
step:238/2330 train_time:9514ms step_avg:39.97ms
step:239/2330 train_time:9549ms step_avg:39.95ms
step:240/2330 train_time:9594ms step_avg:39.97ms
step:241/2330 train_time:9628ms step_avg:39.95ms
step:242/2330 train_time:9673ms step_avg:39.97ms
step:243/2330 train_time:9708ms step_avg:39.95ms
step:244/2330 train_time:9753ms step_avg:39.97ms
step:245/2330 train_time:9789ms step_avg:39.95ms
step:246/2330 train_time:9833ms step_avg:39.97ms
step:247/2330 train_time:9868ms step_avg:39.95ms
step:248/2330 train_time:9913ms step_avg:39.97ms
step:249/2330 train_time:9948ms step_avg:39.95ms
step:250/2330 train_time:9994ms step_avg:39.98ms
step:250/2330 val_loss:5.4063 train_time:10081ms step_avg:40.32ms
step:251/2330 train_time:10094ms step_avg:40.21ms
step:252/2330 train_time:10105ms step_avg:40.10ms
step:253/2330 train_time:10116ms step_avg:39.98ms
step:254/2330 train_time:10153ms step_avg:39.97ms
step:255/2330 train_time:10187ms step_avg:39.95ms
step:256/2330 train_time:10231ms step_avg:39.97ms
step:257/2330 train_time:10266ms step_avg:39.94ms
step:258/2330 train_time:10309ms step_avg:39.96ms
step:259/2330 train_time:10344ms step_avg:39.94ms
step:260/2330 train_time:10388ms step_avg:39.95ms
step:261/2330 train_time:10429ms step_avg:39.96ms
step:262/2330 train_time:10477ms step_avg:39.99ms
step:263/2330 train_time:10513ms step_avg:39.98ms
step:264/2330 train_time:10559ms step_avg:40.00ms
step:265/2330 train_time:10595ms step_avg:39.98ms
step:266/2330 train_time:10639ms step_avg:40.00ms
step:267/2330 train_time:10674ms step_avg:39.98ms
step:268/2330 train_time:10717ms step_avg:39.99ms
step:269/2330 train_time:10752ms step_avg:39.97ms
step:270/2330 train_time:10796ms step_avg:39.99ms
step:271/2330 train_time:10831ms step_avg:39.97ms
step:272/2330 train_time:10875ms step_avg:39.98ms
step:273/2330 train_time:10910ms step_avg:39.96ms
step:274/2330 train_time:10954ms step_avg:39.98ms
step:275/2330 train_time:10990ms step_avg:39.96ms
step:276/2330 train_time:11034ms step_avg:39.98ms
step:277/2330 train_time:11069ms step_avg:39.96ms
step:278/2330 train_time:11113ms step_avg:39.98ms
step:279/2330 train_time:11148ms step_avg:39.96ms
step:280/2330 train_time:11192ms step_avg:39.97ms
step:281/2330 train_time:11227ms step_avg:39.95ms
step:282/2330 train_time:11271ms step_avg:39.97ms
step:283/2330 train_time:11306ms step_avg:39.95ms
step:284/2330 train_time:11352ms step_avg:39.97ms
step:285/2330 train_time:11388ms step_avg:39.96ms
step:286/2330 train_time:11434ms step_avg:39.98ms
step:287/2330 train_time:11471ms step_avg:39.97ms
step:288/2330 train_time:11517ms step_avg:39.99ms
step:289/2330 train_time:11553ms step_avg:39.98ms
step:290/2330 train_time:11598ms step_avg:39.99ms
step:291/2330 train_time:11633ms step_avg:39.98ms
step:292/2330 train_time:11678ms step_avg:39.99ms
step:293/2330 train_time:11713ms step_avg:39.98ms
step:294/2330 train_time:11758ms step_avg:39.99ms
step:295/2330 train_time:11793ms step_avg:39.98ms
step:296/2330 train_time:11838ms step_avg:39.99ms
step:297/2330 train_time:11872ms step_avg:39.97ms
step:298/2330 train_time:11916ms step_avg:39.99ms
step:299/2330 train_time:11952ms step_avg:39.97ms
step:300/2330 train_time:11996ms step_avg:39.99ms
step:301/2330 train_time:12031ms step_avg:39.97ms
step:302/2330 train_time:12076ms step_avg:39.99ms
step:303/2330 train_time:12111ms step_avg:39.97ms
step:304/2330 train_time:12155ms step_avg:39.98ms
step:305/2330 train_time:12190ms step_avg:39.97ms
step:306/2330 train_time:12234ms step_avg:39.98ms
step:307/2330 train_time:12270ms step_avg:39.97ms
step:308/2330 train_time:12315ms step_avg:39.98ms
step:309/2330 train_time:12351ms step_avg:39.97ms
step:310/2330 train_time:12395ms step_avg:39.99ms
step:311/2330 train_time:12432ms step_avg:39.97ms
step:312/2330 train_time:12477ms step_avg:39.99ms
step:313/2330 train_time:12513ms step_avg:39.98ms
step:314/2330 train_time:12558ms step_avg:39.99ms
step:315/2330 train_time:12593ms step_avg:39.98ms
step:316/2330 train_time:12638ms step_avg:39.99ms
step:317/2330 train_time:12673ms step_avg:39.98ms
step:318/2330 train_time:12718ms step_avg:39.99ms
step:319/2330 train_time:12753ms step_avg:39.98ms
step:320/2330 train_time:12798ms step_avg:39.99ms
step:321/2330 train_time:12832ms step_avg:39.98ms
step:322/2330 train_time:12876ms step_avg:39.99ms
step:323/2330 train_time:12911ms step_avg:39.97ms
step:324/2330 train_time:12955ms step_avg:39.99ms
step:325/2330 train_time:12991ms step_avg:39.97ms
step:326/2330 train_time:13036ms step_avg:39.99ms
step:327/2330 train_time:13071ms step_avg:39.97ms
step:328/2330 train_time:13116ms step_avg:39.99ms
step:329/2330 train_time:13150ms step_avg:39.97ms
step:330/2330 train_time:13195ms step_avg:39.99ms
step:331/2330 train_time:13230ms step_avg:39.97ms
step:332/2330 train_time:13275ms step_avg:39.99ms
step:333/2330 train_time:13310ms step_avg:39.97ms
step:334/2330 train_time:13355ms step_avg:39.99ms
step:335/2330 train_time:13391ms step_avg:39.97ms
step:336/2330 train_time:13435ms step_avg:39.99ms
step:337/2330 train_time:13471ms step_avg:39.97ms
step:338/2330 train_time:13516ms step_avg:39.99ms
step:339/2330 train_time:13552ms step_avg:39.98ms
step:340/2330 train_time:13597ms step_avg:39.99ms
step:341/2330 train_time:13633ms step_avg:39.98ms
step:342/2330 train_time:13677ms step_avg:39.99ms
step:343/2330 train_time:13712ms step_avg:39.98ms
step:344/2330 train_time:13757ms step_avg:39.99ms
step:345/2330 train_time:13792ms step_avg:39.98ms
step:346/2330 train_time:13837ms step_avg:39.99ms
step:347/2330 train_time:13871ms step_avg:39.98ms
step:348/2330 train_time:13916ms step_avg:39.99ms
step:349/2330 train_time:13951ms step_avg:39.97ms
step:350/2330 train_time:13995ms step_avg:39.99ms
step:351/2330 train_time:14031ms step_avg:39.97ms
step:352/2330 train_time:14075ms step_avg:39.99ms
step:353/2330 train_time:14110ms step_avg:39.97ms
step:354/2330 train_time:14154ms step_avg:39.98ms
step:355/2330 train_time:14189ms step_avg:39.97ms
step:356/2330 train_time:14233ms step_avg:39.98ms
step:357/2330 train_time:14269ms step_avg:39.97ms
step:358/2330 train_time:14313ms step_avg:39.98ms
step:359/2330 train_time:14349ms step_avg:39.97ms
step:360/2330 train_time:14394ms step_avg:39.98ms
step:361/2330 train_time:14430ms step_avg:39.97ms
step:362/2330 train_time:14475ms step_avg:39.99ms
step:363/2330 train_time:14511ms step_avg:39.97ms
step:364/2330 train_time:14556ms step_avg:39.99ms
step:365/2330 train_time:14592ms step_avg:39.98ms
step:366/2330 train_time:14637ms step_avg:39.99ms
step:367/2330 train_time:14672ms step_avg:39.98ms
step:368/2330 train_time:14717ms step_avg:39.99ms
step:369/2330 train_time:14752ms step_avg:39.98ms
step:370/2330 train_time:14797ms step_avg:39.99ms
step:371/2330 train_time:14833ms step_avg:39.98ms
step:372/2330 train_time:14878ms step_avg:39.99ms
step:373/2330 train_time:14913ms step_avg:39.98ms
step:374/2330 train_time:14958ms step_avg:39.99ms
step:375/2330 train_time:14993ms step_avg:39.98ms
step:376/2330 train_time:15039ms step_avg:40.00ms
step:377/2330 train_time:15074ms step_avg:39.98ms
step:378/2330 train_time:15119ms step_avg:40.00ms
step:379/2330 train_time:15154ms step_avg:39.98ms
step:380/2330 train_time:15199ms step_avg:40.00ms
step:381/2330 train_time:15234ms step_avg:39.98ms
step:382/2330 train_time:15279ms step_avg:40.00ms
step:383/2330 train_time:15314ms step_avg:39.98ms
step:384/2330 train_time:15358ms step_avg:40.00ms
step:385/2330 train_time:15393ms step_avg:39.98ms
step:386/2330 train_time:15439ms step_avg:40.00ms
step:387/2330 train_time:15475ms step_avg:39.99ms
step:388/2330 train_time:15520ms step_avg:40.00ms
step:389/2330 train_time:15554ms step_avg:39.99ms
step:390/2330 train_time:15599ms step_avg:40.00ms
step:391/2330 train_time:15634ms step_avg:39.98ms
step:392/2330 train_time:15678ms step_avg:39.99ms
step:393/2330 train_time:15713ms step_avg:39.98ms
step:394/2330 train_time:15757ms step_avg:39.99ms
step:395/2330 train_time:15792ms step_avg:39.98ms
step:396/2330 train_time:15837ms step_avg:39.99ms
step:397/2330 train_time:15872ms step_avg:39.98ms
step:398/2330 train_time:15916ms step_avg:39.99ms
step:399/2330 train_time:15951ms step_avg:39.98ms
step:400/2330 train_time:15995ms step_avg:39.99ms
step:401/2330 train_time:16031ms step_avg:39.98ms
step:402/2330 train_time:16076ms step_avg:39.99ms
step:403/2330 train_time:16111ms step_avg:39.98ms
step:404/2330 train_time:16155ms step_avg:39.99ms
step:405/2330 train_time:16190ms step_avg:39.98ms
step:406/2330 train_time:16235ms step_avg:39.99ms
step:407/2330 train_time:16270ms step_avg:39.98ms
step:408/2330 train_time:16314ms step_avg:39.99ms
step:409/2330 train_time:16350ms step_avg:39.98ms
step:410/2330 train_time:16395ms step_avg:39.99ms
step:411/2330 train_time:16430ms step_avg:39.98ms
step:412/2330 train_time:16475ms step_avg:39.99ms
step:413/2330 train_time:16511ms step_avg:39.98ms
step:414/2330 train_time:16555ms step_avg:39.99ms
step:415/2330 train_time:16591ms step_avg:39.98ms
step:416/2330 train_time:16635ms step_avg:39.99ms
step:417/2330 train_time:16671ms step_avg:39.98ms
step:418/2330 train_time:16716ms step_avg:39.99ms
step:419/2330 train_time:16751ms step_avg:39.98ms
step:420/2330 train_time:16796ms step_avg:39.99ms
step:421/2330 train_time:16832ms step_avg:39.98ms
step:422/2330 train_time:16876ms step_avg:39.99ms
step:423/2330 train_time:16911ms step_avg:39.98ms
step:424/2330 train_time:16956ms step_avg:39.99ms
step:425/2330 train_time:16992ms step_avg:39.98ms
step:426/2330 train_time:17036ms step_avg:39.99ms
step:427/2330 train_time:17072ms step_avg:39.98ms
step:428/2330 train_time:17116ms step_avg:39.99ms
step:429/2330 train_time:17151ms step_avg:39.98ms
step:430/2330 train_time:17195ms step_avg:39.99ms
step:431/2330 train_time:17230ms step_avg:39.98ms
step:432/2330 train_time:17274ms step_avg:39.99ms
step:433/2330 train_time:17309ms step_avg:39.98ms
step:434/2330 train_time:17354ms step_avg:39.99ms
step:435/2330 train_time:17390ms step_avg:39.98ms
step:436/2330 train_time:17435ms step_avg:39.99ms
step:437/2330 train_time:17470ms step_avg:39.98ms
step:438/2330 train_time:17515ms step_avg:39.99ms
step:439/2330 train_time:17550ms step_avg:39.98ms
step:440/2330 train_time:17595ms step_avg:39.99ms
step:441/2330 train_time:17631ms step_avg:39.98ms
step:442/2330 train_time:17676ms step_avg:39.99ms
step:443/2330 train_time:17711ms step_avg:39.98ms
step:444/2330 train_time:17756ms step_avg:39.99ms
step:445/2330 train_time:17791ms step_avg:39.98ms
step:446/2330 train_time:17836ms step_avg:39.99ms
step:447/2330 train_time:17871ms step_avg:39.98ms
step:448/2330 train_time:17915ms step_avg:39.99ms
step:449/2330 train_time:17951ms step_avg:39.98ms
step:450/2330 train_time:17995ms step_avg:39.99ms
step:451/2330 train_time:18031ms step_avg:39.98ms
step:452/2330 train_time:18075ms step_avg:39.99ms
step:453/2330 train_time:18110ms step_avg:39.98ms
step:454/2330 train_time:18154ms step_avg:39.99ms
step:455/2330 train_time:18190ms step_avg:39.98ms
step:456/2330 train_time:18234ms step_avg:39.99ms
step:457/2330 train_time:18270ms step_avg:39.98ms
step:458/2330 train_time:18314ms step_avg:39.99ms
step:459/2330 train_time:18349ms step_avg:39.98ms
step:460/2330 train_time:18394ms step_avg:39.99ms
step:461/2330 train_time:18430ms step_avg:39.98ms
step:462/2330 train_time:18474ms step_avg:39.99ms
step:463/2330 train_time:18510ms step_avg:39.98ms
step:464/2330 train_time:18554ms step_avg:39.99ms
step:465/2330 train_time:18590ms step_avg:39.98ms
step:466/2330 train_time:18634ms step_avg:39.99ms
step:467/2330 train_time:18670ms step_avg:39.98ms
step:468/2330 train_time:18715ms step_avg:39.99ms
step:469/2330 train_time:18750ms step_avg:39.98ms
step:470/2330 train_time:18795ms step_avg:39.99ms
step:471/2330 train_time:18831ms step_avg:39.98ms
step:472/2330 train_time:18876ms step_avg:39.99ms
step:473/2330 train_time:18911ms step_avg:39.98ms
step:474/2330 train_time:18956ms step_avg:39.99ms
step:475/2330 train_time:18991ms step_avg:39.98ms
step:476/2330 train_time:19036ms step_avg:39.99ms
step:477/2330 train_time:19071ms step_avg:39.98ms
step:478/2330 train_time:19115ms step_avg:39.99ms
step:479/2330 train_time:19151ms step_avg:39.98ms
step:480/2330 train_time:19196ms step_avg:39.99ms
step:481/2330 train_time:19231ms step_avg:39.98ms
step:482/2330 train_time:19276ms step_avg:39.99ms
step:483/2330 train_time:19311ms step_avg:39.98ms
step:484/2330 train_time:19355ms step_avg:39.99ms
step:485/2330 train_time:19391ms step_avg:39.98ms
step:486/2330 train_time:19435ms step_avg:39.99ms
step:487/2330 train_time:19471ms step_avg:39.98ms
step:488/2330 train_time:19515ms step_avg:39.99ms
step:489/2330 train_time:19550ms step_avg:39.98ms
step:490/2330 train_time:19595ms step_avg:39.99ms
step:491/2330 train_time:19631ms step_avg:39.98ms
step:492/2330 train_time:19675ms step_avg:39.99ms
step:493/2330 train_time:19710ms step_avg:39.98ms
step:494/2330 train_time:19755ms step_avg:39.99ms
step:495/2330 train_time:19791ms step_avg:39.98ms
step:496/2330 train_time:19835ms step_avg:39.99ms
step:497/2330 train_time:19870ms step_avg:39.98ms
step:498/2330 train_time:19915ms step_avg:39.99ms
step:499/2330 train_time:19950ms step_avg:39.98ms
step:500/2330 train_time:19995ms step_avg:39.99ms
step:500/2330 val_loss:5.2880 train_time:20083ms step_avg:40.17ms
step:501/2330 train_time:20096ms step_avg:40.11ms
step:502/2330 train_time:20108ms step_avg:40.06ms
step:503/2330 train_time:20119ms step_avg:40.00ms
step:504/2330 train_time:20156ms step_avg:39.99ms
step:505/2330 train_time:20190ms step_avg:39.98ms
step:506/2330 train_time:20233ms step_avg:39.99ms
step:507/2330 train_time:20268ms step_avg:39.98ms
step:508/2330 train_time:20312ms step_avg:39.98ms
step:509/2330 train_time:20346ms step_avg:39.97ms
step:510/2330 train_time:20394ms step_avg:39.99ms
step:511/2330 train_time:20432ms step_avg:39.98ms
step:512/2330 train_time:20478ms step_avg:40.00ms
step:513/2330 train_time:20514ms step_avg:39.99ms
step:514/2330 train_time:20558ms step_avg:40.00ms
step:515/2330 train_time:20593ms step_avg:39.99ms
step:516/2330 train_time:20638ms step_avg:40.00ms
step:517/2330 train_time:20673ms step_avg:39.99ms
step:518/2330 train_time:20717ms step_avg:39.99ms
step:519/2330 train_time:20751ms step_avg:39.98ms
step:520/2330 train_time:20797ms step_avg:39.99ms
step:521/2330 train_time:20832ms step_avg:39.98ms
step:522/2330 train_time:20875ms step_avg:39.99ms
step:523/2330 train_time:20910ms step_avg:39.98ms
step:524/2330 train_time:20954ms step_avg:39.99ms
step:525/2330 train_time:20988ms step_avg:39.98ms
step:526/2330 train_time:21032ms step_avg:39.99ms
step:527/2330 train_time:21068ms step_avg:39.98ms
step:528/2330 train_time:21112ms step_avg:39.98ms
step:529/2330 train_time:21147ms step_avg:39.97ms
step:530/2330 train_time:21190ms step_avg:39.98ms
step:531/2330 train_time:21225ms step_avg:39.97ms
step:532/2330 train_time:21269ms step_avg:39.98ms
step:533/2330 train_time:21304ms step_avg:39.97ms
step:534/2330 train_time:21349ms step_avg:39.98ms
step:535/2330 train_time:21385ms step_avg:39.97ms
step:536/2330 train_time:21431ms step_avg:39.98ms
step:537/2330 train_time:21467ms step_avg:39.98ms
step:538/2330 train_time:21512ms step_avg:39.99ms
step:539/2330 train_time:21548ms step_avg:39.98ms
step:540/2330 train_time:21592ms step_avg:39.99ms
step:541/2330 train_time:21627ms step_avg:39.98ms
step:542/2330 train_time:21672ms step_avg:39.98ms
step:543/2330 train_time:21707ms step_avg:39.98ms
step:544/2330 train_time:21751ms step_avg:39.98ms
step:545/2330 train_time:21786ms step_avg:39.97ms
step:546/2330 train_time:21830ms step_avg:39.98ms
step:547/2330 train_time:21865ms step_avg:39.97ms
step:548/2330 train_time:21909ms step_avg:39.98ms
step:549/2330 train_time:21944ms step_avg:39.97ms
step:550/2330 train_time:21988ms step_avg:39.98ms
step:551/2330 train_time:22023ms step_avg:39.97ms
step:552/2330 train_time:22067ms step_avg:39.98ms
step:553/2330 train_time:22101ms step_avg:39.97ms
step:554/2330 train_time:22145ms step_avg:39.97ms
step:555/2330 train_time:22180ms step_avg:39.96ms
step:556/2330 train_time:22223ms step_avg:39.97ms
step:557/2330 train_time:22260ms step_avg:39.96ms
step:558/2330 train_time:22304ms step_avg:39.97ms
step:559/2330 train_time:22340ms step_avg:39.96ms
step:560/2330 train_time:22385ms step_avg:39.97ms
step:561/2330 train_time:22421ms step_avg:39.97ms
step:562/2330 train_time:22467ms step_avg:39.98ms
step:563/2330 train_time:22502ms step_avg:39.97ms
step:564/2330 train_time:22547ms step_avg:39.98ms
step:565/2330 train_time:22583ms step_avg:39.97ms
step:566/2330 train_time:22627ms step_avg:39.98ms
step:567/2330 train_time:22663ms step_avg:39.97ms
step:568/2330 train_time:22708ms step_avg:39.98ms
step:569/2330 train_time:22744ms step_avg:39.97ms
step:570/2330 train_time:22788ms step_avg:39.98ms
step:571/2330 train_time:22823ms step_avg:39.97ms
step:572/2330 train_time:22868ms step_avg:39.98ms
step:573/2330 train_time:22902ms step_avg:39.97ms
step:574/2330 train_time:22947ms step_avg:39.98ms
step:575/2330 train_time:22982ms step_avg:39.97ms
step:576/2330 train_time:23026ms step_avg:39.98ms
step:577/2330 train_time:23061ms step_avg:39.97ms
step:578/2330 train_time:23105ms step_avg:39.97ms
step:579/2330 train_time:23140ms step_avg:39.97ms
step:580/2330 train_time:23184ms step_avg:39.97ms
step:581/2330 train_time:23219ms step_avg:39.96ms
step:582/2330 train_time:23265ms step_avg:39.97ms
step:583/2330 train_time:23300ms step_avg:39.97ms
step:584/2330 train_time:23344ms step_avg:39.97ms
step:585/2330 train_time:23379ms step_avg:39.96ms
step:586/2330 train_time:23423ms step_avg:39.97ms
step:587/2330 train_time:23459ms step_avg:39.96ms
step:588/2330 train_time:23505ms step_avg:39.97ms
step:589/2330 train_time:23539ms step_avg:39.96ms
step:590/2330 train_time:23584ms step_avg:39.97ms
step:591/2330 train_time:23619ms step_avg:39.96ms
step:592/2330 train_time:23664ms step_avg:39.97ms
step:593/2330 train_time:23699ms step_avg:39.96ms
step:594/2330 train_time:23743ms step_avg:39.97ms
step:595/2330 train_time:23779ms step_avg:39.96ms
step:596/2330 train_time:23823ms step_avg:39.97ms
step:597/2330 train_time:23858ms step_avg:39.96ms
step:598/2330 train_time:23902ms step_avg:39.97ms
step:599/2330 train_time:23937ms step_avg:39.96ms
step:600/2330 train_time:23981ms step_avg:39.97ms
step:601/2330 train_time:24015ms step_avg:39.96ms
step:602/2330 train_time:24059ms step_avg:39.97ms
step:603/2330 train_time:24095ms step_avg:39.96ms
step:604/2330 train_time:24139ms step_avg:39.97ms
step:605/2330 train_time:24174ms step_avg:39.96ms
step:606/2330 train_time:24219ms step_avg:39.96ms
step:607/2330 train_time:24254ms step_avg:39.96ms
step:608/2330 train_time:24297ms step_avg:39.96ms
step:609/2330 train_time:24332ms step_avg:39.95ms
step:610/2330 train_time:24378ms step_avg:39.96ms
step:611/2330 train_time:24413ms step_avg:39.96ms
step:612/2330 train_time:24458ms step_avg:39.96ms
step:613/2330 train_time:24493ms step_avg:39.96ms
step:614/2330 train_time:24537ms step_avg:39.96ms
step:615/2330 train_time:24573ms step_avg:39.96ms
step:616/2330 train_time:24619ms step_avg:39.97ms
step:617/2330 train_time:24655ms step_avg:39.96ms
step:618/2330 train_time:24700ms step_avg:39.97ms
step:619/2330 train_time:24735ms step_avg:39.96ms
step:620/2330 train_time:24779ms step_avg:39.97ms
step:621/2330 train_time:24815ms step_avg:39.96ms
step:622/2330 train_time:24859ms step_avg:39.97ms
step:623/2330 train_time:24894ms step_avg:39.96ms
step:624/2330 train_time:24939ms step_avg:39.97ms
step:625/2330 train_time:24974ms step_avg:39.96ms
step:626/2330 train_time:25019ms step_avg:39.97ms
step:627/2330 train_time:25054ms step_avg:39.96ms
step:628/2330 train_time:25099ms step_avg:39.97ms
step:629/2330 train_time:25134ms step_avg:39.96ms
step:630/2330 train_time:25178ms step_avg:39.97ms
step:631/2330 train_time:25213ms step_avg:39.96ms
step:632/2330 train_time:25258ms step_avg:39.96ms
step:633/2330 train_time:25293ms step_avg:39.96ms
step:634/2330 train_time:25337ms step_avg:39.96ms
step:635/2330 train_time:25373ms step_avg:39.96ms
step:636/2330 train_time:25417ms step_avg:39.96ms
step:637/2330 train_time:25452ms step_avg:39.96ms
step:638/2330 train_time:25497ms step_avg:39.96ms
step:639/2330 train_time:25532ms step_avg:39.96ms
step:640/2330 train_time:25576ms step_avg:39.96ms
step:641/2330 train_time:25611ms step_avg:39.95ms
step:642/2330 train_time:25656ms step_avg:39.96ms
step:643/2330 train_time:25692ms step_avg:39.96ms
step:644/2330 train_time:25736ms step_avg:39.96ms
step:645/2330 train_time:25771ms step_avg:39.95ms
step:646/2330 train_time:25815ms step_avg:39.96ms
step:647/2330 train_time:25851ms step_avg:39.95ms
step:648/2330 train_time:25895ms step_avg:39.96ms
step:649/2330 train_time:25930ms step_avg:39.95ms
step:650/2330 train_time:25974ms step_avg:39.96ms
step:651/2330 train_time:26009ms step_avg:39.95ms
step:652/2330 train_time:26054ms step_avg:39.96ms
step:653/2330 train_time:26089ms step_avg:39.95ms
step:654/2330 train_time:26134ms step_avg:39.96ms
step:655/2330 train_time:26168ms step_avg:39.95ms
step:656/2330 train_time:26213ms step_avg:39.96ms
step:657/2330 train_time:26247ms step_avg:39.95ms
step:658/2330 train_time:26291ms step_avg:39.96ms
step:659/2330 train_time:26327ms step_avg:39.95ms
step:660/2330 train_time:26371ms step_avg:39.96ms
step:661/2330 train_time:26405ms step_avg:39.95ms
step:662/2330 train_time:26451ms step_avg:39.96ms
step:663/2330 train_time:26485ms step_avg:39.95ms
step:664/2330 train_time:26529ms step_avg:39.95ms
step:665/2330 train_time:26565ms step_avg:39.95ms
step:666/2330 train_time:26611ms step_avg:39.96ms
step:667/2330 train_time:26645ms step_avg:39.95ms
step:668/2330 train_time:26691ms step_avg:39.96ms
step:669/2330 train_time:26726ms step_avg:39.95ms
step:670/2330 train_time:26770ms step_avg:39.96ms
step:671/2330 train_time:26806ms step_avg:39.95ms
step:672/2330 train_time:26850ms step_avg:39.95ms
step:673/2330 train_time:26885ms step_avg:39.95ms
step:674/2330 train_time:26929ms step_avg:39.95ms
step:675/2330 train_time:26964ms step_avg:39.95ms
step:676/2330 train_time:27008ms step_avg:39.95ms
step:677/2330 train_time:27043ms step_avg:39.95ms
step:678/2330 train_time:27087ms step_avg:39.95ms
step:679/2330 train_time:27122ms step_avg:39.94ms
step:680/2330 train_time:27167ms step_avg:39.95ms
step:681/2330 train_time:27202ms step_avg:39.94ms
step:682/2330 train_time:27247ms step_avg:39.95ms
step:683/2330 train_time:27282ms step_avg:39.95ms
step:684/2330 train_time:27327ms step_avg:39.95ms
step:685/2330 train_time:27363ms step_avg:39.95ms
step:686/2330 train_time:27409ms step_avg:39.95ms
step:687/2330 train_time:27443ms step_avg:39.95ms
step:688/2330 train_time:27488ms step_avg:39.95ms
step:689/2330 train_time:27523ms step_avg:39.95ms
step:690/2330 train_time:27568ms step_avg:39.95ms
step:691/2330 train_time:27604ms step_avg:39.95ms
step:692/2330 train_time:27649ms step_avg:39.96ms
step:693/2330 train_time:27685ms step_avg:39.95ms
step:694/2330 train_time:27729ms step_avg:39.96ms
step:695/2330 train_time:27764ms step_avg:39.95ms
step:696/2330 train_time:27809ms step_avg:39.96ms
step:697/2330 train_time:27843ms step_avg:39.95ms
step:698/2330 train_time:27888ms step_avg:39.95ms
step:699/2330 train_time:27923ms step_avg:39.95ms
step:700/2330 train_time:27967ms step_avg:39.95ms
step:701/2330 train_time:28003ms step_avg:39.95ms
step:702/2330 train_time:28048ms step_avg:39.95ms
step:703/2330 train_time:28083ms step_avg:39.95ms
step:704/2330 train_time:28127ms step_avg:39.95ms
step:705/2330 train_time:28163ms step_avg:39.95ms
step:706/2330 train_time:28207ms step_avg:39.95ms
step:707/2330 train_time:28242ms step_avg:39.95ms
step:708/2330 train_time:28287ms step_avg:39.95ms
step:709/2330 train_time:28322ms step_avg:39.95ms
step:710/2330 train_time:28367ms step_avg:39.95ms
step:711/2330 train_time:28403ms step_avg:39.95ms
step:712/2330 train_time:28447ms step_avg:39.95ms
step:713/2330 train_time:28482ms step_avg:39.95ms
step:714/2330 train_time:28527ms step_avg:39.95ms
step:715/2330 train_time:28563ms step_avg:39.95ms
step:716/2330 train_time:28608ms step_avg:39.96ms
step:717/2330 train_time:28643ms step_avg:39.95ms
step:718/2330 train_time:28688ms step_avg:39.96ms
step:719/2330 train_time:28723ms step_avg:39.95ms
step:720/2330 train_time:28769ms step_avg:39.96ms
step:721/2330 train_time:28805ms step_avg:39.95ms
step:722/2330 train_time:28849ms step_avg:39.96ms
step:723/2330 train_time:28884ms step_avg:39.95ms
step:724/2330 train_time:28929ms step_avg:39.96ms
step:725/2330 train_time:28963ms step_avg:39.95ms
step:726/2330 train_time:29008ms step_avg:39.96ms
step:727/2330 train_time:29043ms step_avg:39.95ms
step:728/2330 train_time:29088ms step_avg:39.96ms
step:729/2330 train_time:29123ms step_avg:39.95ms
step:730/2330 train_time:29168ms step_avg:39.96ms
step:731/2330 train_time:29202ms step_avg:39.95ms
step:732/2330 train_time:29247ms step_avg:39.96ms
step:733/2330 train_time:29282ms step_avg:39.95ms
step:734/2330 train_time:29327ms step_avg:39.95ms
step:735/2330 train_time:29362ms step_avg:39.95ms
step:736/2330 train_time:29407ms step_avg:39.95ms
step:737/2330 train_time:29442ms step_avg:39.95ms
step:738/2330 train_time:29487ms step_avg:39.96ms
step:739/2330 train_time:29523ms step_avg:39.95ms
step:740/2330 train_time:29567ms step_avg:39.96ms
step:741/2330 train_time:29603ms step_avg:39.95ms
step:742/2330 train_time:29648ms step_avg:39.96ms
step:743/2330 train_time:29683ms step_avg:39.95ms
step:744/2330 train_time:29728ms step_avg:39.96ms
step:745/2330 train_time:29763ms step_avg:39.95ms
step:746/2330 train_time:29808ms step_avg:39.96ms
step:747/2330 train_time:29843ms step_avg:39.95ms
step:748/2330 train_time:29887ms step_avg:39.96ms
step:749/2330 train_time:29922ms step_avg:39.95ms
step:750/2330 train_time:29967ms step_avg:39.96ms
step:750/2330 val_loss:5.2222 train_time:30054ms step_avg:40.07ms
step:751/2330 train_time:30067ms step_avg:40.04ms
step:752/2330 train_time:30079ms step_avg:40.00ms
step:753/2330 train_time:30090ms step_avg:39.96ms
step:754/2330 train_time:30127ms step_avg:39.96ms
step:755/2330 train_time:30160ms step_avg:39.95ms
step:756/2330 train_time:30204ms step_avg:39.95ms
step:757/2330 train_time:30238ms step_avg:39.95ms
step:758/2330 train_time:30283ms step_avg:39.95ms
step:759/2330 train_time:30317ms step_avg:39.94ms
step:760/2330 train_time:30362ms step_avg:39.95ms
step:761/2330 train_time:30399ms step_avg:39.95ms
step:762/2330 train_time:30446ms step_avg:39.96ms
step:763/2330 train_time:30484ms step_avg:39.95ms
step:764/2330 train_time:30530ms step_avg:39.96ms
step:765/2330 train_time:30568ms step_avg:39.96ms
step:766/2330 train_time:30612ms step_avg:39.96ms
step:767/2330 train_time:30646ms step_avg:39.96ms
step:768/2330 train_time:30690ms step_avg:39.96ms
step:769/2330 train_time:30725ms step_avg:39.96ms
step:770/2330 train_time:30769ms step_avg:39.96ms
step:771/2330 train_time:30804ms step_avg:39.95ms
step:772/2330 train_time:30848ms step_avg:39.96ms
step:773/2330 train_time:30882ms step_avg:39.95ms
step:774/2330 train_time:30925ms step_avg:39.95ms
step:775/2330 train_time:30959ms step_avg:39.95ms
step:776/2330 train_time:31004ms step_avg:39.95ms
step:777/2330 train_time:31039ms step_avg:39.95ms
step:778/2330 train_time:31083ms step_avg:39.95ms
step:779/2330 train_time:31118ms step_avg:39.95ms
step:780/2330 train_time:31161ms step_avg:39.95ms
step:781/2330 train_time:31196ms step_avg:39.94ms
step:782/2330 train_time:31240ms step_avg:39.95ms
step:783/2330 train_time:31275ms step_avg:39.94ms
step:784/2330 train_time:31319ms step_avg:39.95ms
step:785/2330 train_time:31355ms step_avg:39.94ms
step:786/2330 train_time:31401ms step_avg:39.95ms
step:787/2330 train_time:31437ms step_avg:39.95ms
step:788/2330 train_time:31483ms step_avg:39.95ms
step:789/2330 train_time:31519ms step_avg:39.95ms
step:790/2330 train_time:31564ms step_avg:39.95ms
step:791/2330 train_time:31600ms step_avg:39.95ms
step:792/2330 train_time:31645ms step_avg:39.96ms
step:793/2330 train_time:31681ms step_avg:39.95ms
step:794/2330 train_time:31725ms step_avg:39.96ms
step:795/2330 train_time:31760ms step_avg:39.95ms
step:796/2330 train_time:31804ms step_avg:39.96ms
step:797/2330 train_time:31839ms step_avg:39.95ms
step:798/2330 train_time:31883ms step_avg:39.95ms
step:799/2330 train_time:31918ms step_avg:39.95ms
step:800/2330 train_time:31962ms step_avg:39.95ms
step:801/2330 train_time:31997ms step_avg:39.95ms
step:802/2330 train_time:32041ms step_avg:39.95ms
step:803/2330 train_time:32076ms step_avg:39.95ms
step:804/2330 train_time:32120ms step_avg:39.95ms
step:805/2330 train_time:32155ms step_avg:39.94ms
step:806/2330 train_time:32199ms step_avg:39.95ms
step:807/2330 train_time:32234ms step_avg:39.94ms
step:808/2330 train_time:32278ms step_avg:39.95ms
step:809/2330 train_time:32313ms step_avg:39.94ms
step:810/2330 train_time:32358ms step_avg:39.95ms
step:811/2330 train_time:32394ms step_avg:39.94ms
step:812/2330 train_time:32439ms step_avg:39.95ms
step:813/2330 train_time:32474ms step_avg:39.94ms
step:814/2330 train_time:32519ms step_avg:39.95ms
step:815/2330 train_time:32554ms step_avg:39.94ms
step:816/2330 train_time:32598ms step_avg:39.95ms
step:817/2330 train_time:32633ms step_avg:39.94ms
step:818/2330 train_time:32678ms step_avg:39.95ms
step:819/2330 train_time:32713ms step_avg:39.94ms
step:820/2330 train_time:32757ms step_avg:39.95ms
step:821/2330 train_time:32792ms step_avg:39.94ms
step:822/2330 train_time:32837ms step_avg:39.95ms
step:823/2330 train_time:32872ms step_avg:39.94ms
step:824/2330 train_time:32916ms step_avg:39.95ms
step:825/2330 train_time:32951ms step_avg:39.94ms
step:826/2330 train_time:32995ms step_avg:39.95ms
step:827/2330 train_time:33030ms step_avg:39.94ms
step:828/2330 train_time:33075ms step_avg:39.95ms
step:829/2330 train_time:33110ms step_avg:39.94ms
step:830/2330 train_time:33154ms step_avg:39.94ms
step:831/2330 train_time:33188ms step_avg:39.94ms
step:832/2330 train_time:33233ms step_avg:39.94ms
step:833/2330 train_time:33268ms step_avg:39.94ms
step:834/2330 train_time:33312ms step_avg:39.94ms
step:835/2330 train_time:33348ms step_avg:39.94ms
step:836/2330 train_time:33393ms step_avg:39.94ms
step:837/2330 train_time:33429ms step_avg:39.94ms
step:838/2330 train_time:33474ms step_avg:39.94ms
step:839/2330 train_time:33509ms step_avg:39.94ms
step:840/2330 train_time:33553ms step_avg:39.94ms
step:841/2330 train_time:33588ms step_avg:39.94ms
step:842/2330 train_time:33633ms step_avg:39.94ms
step:843/2330 train_time:33668ms step_avg:39.94ms
step:844/2330 train_time:33712ms step_avg:39.94ms
step:845/2330 train_time:33747ms step_avg:39.94ms
step:846/2330 train_time:33791ms step_avg:39.94ms
step:847/2330 train_time:33826ms step_avg:39.94ms
step:848/2330 train_time:33871ms step_avg:39.94ms
step:849/2330 train_time:33906ms step_avg:39.94ms
step:850/2330 train_time:33951ms step_avg:39.94ms
step:851/2330 train_time:33986ms step_avg:39.94ms
step:852/2330 train_time:34030ms step_avg:39.94ms
step:853/2330 train_time:34065ms step_avg:39.94ms
step:854/2330 train_time:34109ms step_avg:39.94ms
step:855/2330 train_time:34143ms step_avg:39.93ms
step:856/2330 train_time:34187ms step_avg:39.94ms
step:857/2330 train_time:34222ms step_avg:39.93ms
step:858/2330 train_time:34266ms step_avg:39.94ms
step:859/2330 train_time:34301ms step_avg:39.93ms
step:860/2330 train_time:34345ms step_avg:39.94ms
step:861/2330 train_time:34380ms step_avg:39.93ms
step:862/2330 train_time:34425ms step_avg:39.94ms
step:863/2330 train_time:34460ms step_avg:39.93ms
step:864/2330 train_time:34505ms step_avg:39.94ms
step:865/2330 train_time:34540ms step_avg:39.93ms
step:866/2330 train_time:34585ms step_avg:39.94ms
step:867/2330 train_time:34620ms step_avg:39.93ms
step:868/2330 train_time:34664ms step_avg:39.94ms
step:869/2330 train_time:34699ms step_avg:39.93ms
step:870/2330 train_time:34745ms step_avg:39.94ms
step:871/2330 train_time:34780ms step_avg:39.93ms
step:872/2330 train_time:34825ms step_avg:39.94ms
step:873/2330 train_time:34860ms step_avg:39.93ms
step:874/2330 train_time:34905ms step_avg:39.94ms
step:875/2330 train_time:34940ms step_avg:39.93ms
step:876/2330 train_time:34985ms step_avg:39.94ms
step:877/2330 train_time:35020ms step_avg:39.93ms
step:878/2330 train_time:35065ms step_avg:39.94ms
step:879/2330 train_time:35100ms step_avg:39.93ms
step:880/2330 train_time:35144ms step_avg:39.94ms
step:881/2330 train_time:35179ms step_avg:39.93ms
step:882/2330 train_time:35223ms step_avg:39.94ms
step:883/2330 train_time:35259ms step_avg:39.93ms
step:884/2330 train_time:35303ms step_avg:39.94ms
step:885/2330 train_time:35339ms step_avg:39.93ms
step:886/2330 train_time:35383ms step_avg:39.94ms
step:887/2330 train_time:35419ms step_avg:39.93ms
step:888/2330 train_time:35463ms step_avg:39.94ms
step:889/2330 train_time:35499ms step_avg:39.93ms
step:890/2330 train_time:35544ms step_avg:39.94ms
step:891/2330 train_time:35579ms step_avg:39.93ms
step:892/2330 train_time:35624ms step_avg:39.94ms
step:893/2330 train_time:35659ms step_avg:39.93ms
step:894/2330 train_time:35704ms step_avg:39.94ms
step:895/2330 train_time:35740ms step_avg:39.93ms
step:896/2330 train_time:35785ms step_avg:39.94ms
step:897/2330 train_time:35820ms step_avg:39.93ms
step:898/2330 train_time:35865ms step_avg:39.94ms
step:899/2330 train_time:35900ms step_avg:39.93ms
step:900/2330 train_time:35945ms step_avg:39.94ms
step:901/2330 train_time:35980ms step_avg:39.93ms
step:902/2330 train_time:36025ms step_avg:39.94ms
step:903/2330 train_time:36060ms step_avg:39.93ms
step:904/2330 train_time:36104ms step_avg:39.94ms
step:905/2330 train_time:36139ms step_avg:39.93ms
step:906/2330 train_time:36184ms step_avg:39.94ms
step:907/2330 train_time:36219ms step_avg:39.93ms
step:908/2330 train_time:36264ms step_avg:39.94ms
step:909/2330 train_time:36299ms step_avg:39.93ms
step:910/2330 train_time:36343ms step_avg:39.94ms
step:911/2330 train_time:36378ms step_avg:39.93ms
step:912/2330 train_time:36423ms step_avg:39.94ms
step:913/2330 train_time:36459ms step_avg:39.93ms
step:914/2330 train_time:36504ms step_avg:39.94ms
step:915/2330 train_time:36539ms step_avg:39.93ms
step:916/2330 train_time:36584ms step_avg:39.94ms
step:917/2330 train_time:36619ms step_avg:39.93ms
step:918/2330 train_time:36664ms step_avg:39.94ms
step:919/2330 train_time:36700ms step_avg:39.93ms
step:920/2330 train_time:36745ms step_avg:39.94ms
step:921/2330 train_time:36780ms step_avg:39.94ms
step:922/2330 train_time:36824ms step_avg:39.94ms
step:923/2330 train_time:36860ms step_avg:39.93ms
step:924/2330 train_time:36904ms step_avg:39.94ms
step:925/2330 train_time:36940ms step_avg:39.94ms
step:926/2330 train_time:36984ms step_avg:39.94ms
step:927/2330 train_time:37020ms step_avg:39.94ms
step:928/2330 train_time:37064ms step_avg:39.94ms
step:929/2330 train_time:37099ms step_avg:39.93ms
step:930/2330 train_time:37144ms step_avg:39.94ms
step:931/2330 train_time:37179ms step_avg:39.93ms
step:932/2330 train_time:37224ms step_avg:39.94ms
step:933/2330 train_time:37259ms step_avg:39.94ms
step:934/2330 train_time:37304ms step_avg:39.94ms
step:935/2330 train_time:37339ms step_avg:39.93ms
step:936/2330 train_time:37384ms step_avg:39.94ms
step:937/2330 train_time:37419ms step_avg:39.93ms
step:938/2330 train_time:37463ms step_avg:39.94ms
step:939/2330 train_time:37499ms step_avg:39.93ms
step:940/2330 train_time:37543ms step_avg:39.94ms
step:941/2330 train_time:37579ms step_avg:39.94ms
step:942/2330 train_time:37623ms step_avg:39.94ms
step:943/2330 train_time:37659ms step_avg:39.94ms
step:944/2330 train_time:37704ms step_avg:39.94ms
step:945/2330 train_time:37740ms step_avg:39.94ms
step:946/2330 train_time:37784ms step_avg:39.94ms
step:947/2330 train_time:37820ms step_avg:39.94ms
step:948/2330 train_time:37864ms step_avg:39.94ms
step:949/2330 train_time:37900ms step_avg:39.94ms
step:950/2330 train_time:37944ms step_avg:39.94ms
step:951/2330 train_time:37980ms step_avg:39.94ms
step:952/2330 train_time:38024ms step_avg:39.94ms
step:953/2330 train_time:38059ms step_avg:39.94ms
step:954/2330 train_time:38104ms step_avg:39.94ms
step:955/2330 train_time:38139ms step_avg:39.94ms
step:956/2330 train_time:38184ms step_avg:39.94ms
step:957/2330 train_time:38219ms step_avg:39.94ms
step:958/2330 train_time:38263ms step_avg:39.94ms
step:959/2330 train_time:38298ms step_avg:39.94ms
step:960/2330 train_time:38342ms step_avg:39.94ms
step:961/2330 train_time:38377ms step_avg:39.93ms
step:962/2330 train_time:38422ms step_avg:39.94ms
step:963/2330 train_time:38457ms step_avg:39.93ms
step:964/2330 train_time:38501ms step_avg:39.94ms
step:965/2330 train_time:38537ms step_avg:39.93ms
step:966/2330 train_time:38581ms step_avg:39.94ms
step:967/2330 train_time:38617ms step_avg:39.93ms
step:968/2330 train_time:38662ms step_avg:39.94ms
step:969/2330 train_time:38697ms step_avg:39.93ms
step:970/2330 train_time:38741ms step_avg:39.94ms
step:971/2330 train_time:38776ms step_avg:39.93ms
step:972/2330 train_time:38821ms step_avg:39.94ms
step:973/2330 train_time:38855ms step_avg:39.93ms
step:974/2330 train_time:38900ms step_avg:39.94ms
step:975/2330 train_time:38935ms step_avg:39.93ms
step:976/2330 train_time:38980ms step_avg:39.94ms
step:977/2330 train_time:39015ms step_avg:39.93ms
step:978/2330 train_time:39060ms step_avg:39.94ms
step:979/2330 train_time:39095ms step_avg:39.93ms
step:980/2330 train_time:39139ms step_avg:39.94ms
step:981/2330 train_time:39174ms step_avg:39.93ms
step:982/2330 train_time:39219ms step_avg:39.94ms
step:983/2330 train_time:39253ms step_avg:39.93ms
step:984/2330 train_time:39297ms step_avg:39.94ms
step:985/2330 train_time:39332ms step_avg:39.93ms
step:986/2330 train_time:39376ms step_avg:39.94ms
step:987/2330 train_time:39411ms step_avg:39.93ms
step:988/2330 train_time:39455ms step_avg:39.93ms
step:989/2330 train_time:39490ms step_avg:39.93ms
step:990/2330 train_time:39535ms step_avg:39.93ms
step:991/2330 train_time:39571ms step_avg:39.93ms
step:992/2330 train_time:39615ms step_avg:39.93ms
step:993/2330 train_time:39650ms step_avg:39.93ms
step:994/2330 train_time:39694ms step_avg:39.93ms
step:995/2330 train_time:39729ms step_avg:39.93ms
step:996/2330 train_time:39774ms step_avg:39.93ms
step:997/2330 train_time:39809ms step_avg:39.93ms
step:998/2330 train_time:39854ms step_avg:39.93ms
step:999/2330 train_time:39888ms step_avg:39.93ms
step:1000/2330 train_time:39933ms step_avg:39.93ms
step:1000/2330 val_loss:5.1877 train_time:40019ms step_avg:40.02ms
step:1001/2330 train_time:40031ms step_avg:39.99ms
step:1002/2330 train_time:40043ms step_avg:39.96ms
step:1003/2330 train_time:40053ms step_avg:39.93ms
step:1004/2330 train_time:40093ms step_avg:39.93ms
step:1005/2330 train_time:40127ms step_avg:39.93ms
step:1006/2330 train_time:40170ms step_avg:39.93ms
step:1007/2330 train_time:40204ms step_avg:39.92ms
step:1008/2330 train_time:40248ms step_avg:39.93ms
step:1009/2330 train_time:40283ms step_avg:39.92ms
step:1010/2330 train_time:40328ms step_avg:39.93ms
step:1011/2330 train_time:40368ms step_avg:39.93ms
step:1012/2330 train_time:40418ms step_avg:39.94ms
step:1013/2330 train_time:40456ms step_avg:39.94ms
step:1014/2330 train_time:40501ms step_avg:39.94ms
step:1015/2330 train_time:40536ms step_avg:39.94ms
step:1016/2330 train_time:40581ms step_avg:39.94ms
step:1017/2330 train_time:40616ms step_avg:39.94ms
step:1018/2330 train_time:40660ms step_avg:39.94ms
step:1019/2330 train_time:40694ms step_avg:39.94ms
step:1020/2330 train_time:40739ms step_avg:39.94ms
step:1021/2330 train_time:40774ms step_avg:39.93ms
step:1022/2330 train_time:40817ms step_avg:39.94ms
step:1023/2330 train_time:40852ms step_avg:39.93ms
step:1024/2330 train_time:40895ms step_avg:39.94ms
step:1025/2330 train_time:40930ms step_avg:39.93ms
step:1026/2330 train_time:40977ms step_avg:39.94ms
step:1027/2330 train_time:41013ms step_avg:39.93ms
step:1028/2330 train_time:41058ms step_avg:39.94ms
step:1029/2330 train_time:41092ms step_avg:39.93ms
step:1030/2330 train_time:41137ms step_avg:39.94ms
step:1031/2330 train_time:41172ms step_avg:39.93ms
step:1032/2330 train_time:41216ms step_avg:39.94ms
step:1033/2330 train_time:41251ms step_avg:39.93ms
step:1034/2330 train_time:41296ms step_avg:39.94ms
step:1035/2330 train_time:41332ms step_avg:39.93ms
step:1036/2330 train_time:41378ms step_avg:39.94ms
step:1037/2330 train_time:41415ms step_avg:39.94ms
step:1038/2330 train_time:41460ms step_avg:39.94ms
step:1039/2330 train_time:41496ms step_avg:39.94ms
step:1040/2330 train_time:41540ms step_avg:39.94ms
step:1041/2330 train_time:41575ms step_avg:39.94ms
step:1042/2330 train_time:41620ms step_avg:39.94ms
step:1043/2330 train_time:41654ms step_avg:39.94ms
step:1044/2330 train_time:41699ms step_avg:39.94ms
step:1045/2330 train_time:41734ms step_avg:39.94ms
step:1046/2330 train_time:41778ms step_avg:39.94ms
step:1047/2330 train_time:41813ms step_avg:39.94ms
step:1048/2330 train_time:41856ms step_avg:39.94ms
step:1049/2330 train_time:41892ms step_avg:39.94ms
step:1050/2330 train_time:41936ms step_avg:39.94ms
step:1051/2330 train_time:41972ms step_avg:39.93ms
step:1052/2330 train_time:42017ms step_avg:39.94ms
step:1053/2330 train_time:42053ms step_avg:39.94ms
step:1054/2330 train_time:42097ms step_avg:39.94ms
step:1055/2330 train_time:42133ms step_avg:39.94ms
step:1056/2330 train_time:42177ms step_avg:39.94ms
step:1057/2330 train_time:42212ms step_avg:39.94ms
step:1058/2330 train_time:42257ms step_avg:39.94ms
step:1059/2330 train_time:42292ms step_avg:39.94ms
step:1060/2330 train_time:42338ms step_avg:39.94ms
step:1061/2330 train_time:42374ms step_avg:39.94ms
step:1062/2330 train_time:42419ms step_avg:39.94ms
step:1063/2330 train_time:42455ms step_avg:39.94ms
step:1064/2330 train_time:42500ms step_avg:39.94ms
step:1065/2330 train_time:42535ms step_avg:39.94ms
step:1066/2330 train_time:42579ms step_avg:39.94ms
step:1067/2330 train_time:42615ms step_avg:39.94ms
step:1068/2330 train_time:42659ms step_avg:39.94ms
step:1069/2330 train_time:42694ms step_avg:39.94ms
step:1070/2330 train_time:42738ms step_avg:39.94ms
step:1071/2330 train_time:42774ms step_avg:39.94ms
step:1072/2330 train_time:42818ms step_avg:39.94ms
step:1073/2330 train_time:42853ms step_avg:39.94ms
step:1074/2330 train_time:42897ms step_avg:39.94ms
step:1075/2330 train_time:42932ms step_avg:39.94ms
step:1076/2330 train_time:42976ms step_avg:39.94ms
step:1077/2330 train_time:43011ms step_avg:39.94ms
step:1078/2330 train_time:43056ms step_avg:39.94ms
step:1079/2330 train_time:43091ms step_avg:39.94ms
step:1080/2330 train_time:43135ms step_avg:39.94ms
step:1081/2330 train_time:43169ms step_avg:39.93ms
step:1082/2330 train_time:43214ms step_avg:39.94ms
step:1083/2330 train_time:43249ms step_avg:39.93ms
step:1084/2330 train_time:43293ms step_avg:39.94ms
step:1085/2330 train_time:43328ms step_avg:39.93ms
step:1086/2330 train_time:43372ms step_avg:39.94ms
step:1087/2330 train_time:43408ms step_avg:39.93ms
step:1088/2330 train_time:43453ms step_avg:39.94ms
step:1089/2330 train_time:43488ms step_avg:39.93ms
step:1090/2330 train_time:43533ms step_avg:39.94ms
step:1091/2330 train_time:43569ms step_avg:39.93ms
step:1092/2330 train_time:43614ms step_avg:39.94ms
step:1093/2330 train_time:43650ms step_avg:39.94ms
step:1094/2330 train_time:43694ms step_avg:39.94ms
step:1095/2330 train_time:43729ms step_avg:39.94ms
step:1096/2330 train_time:43773ms step_avg:39.94ms
step:1097/2330 train_time:43808ms step_avg:39.93ms
step:1098/2330 train_time:43852ms step_avg:39.94ms
step:1099/2330 train_time:43887ms step_avg:39.93ms
step:1100/2330 train_time:43931ms step_avg:39.94ms
step:1101/2330 train_time:43967ms step_avg:39.93ms
step:1102/2330 train_time:44011ms step_avg:39.94ms
step:1103/2330 train_time:44046ms step_avg:39.93ms
step:1104/2330 train_time:44091ms step_avg:39.94ms
step:1105/2330 train_time:44126ms step_avg:39.93ms
step:1106/2330 train_time:44171ms step_avg:39.94ms
step:1107/2330 train_time:44206ms step_avg:39.93ms
step:1108/2330 train_time:44251ms step_avg:39.94ms
step:1109/2330 train_time:44286ms step_avg:39.93ms
step:1110/2330 train_time:44331ms step_avg:39.94ms
step:1111/2330 train_time:44366ms step_avg:39.93ms
step:1112/2330 train_time:44410ms step_avg:39.94ms
step:1113/2330 train_time:44447ms step_avg:39.93ms
step:1114/2330 train_time:44492ms step_avg:39.94ms
step:1115/2330 train_time:44528ms step_avg:39.94ms
step:1116/2330 train_time:44573ms step_avg:39.94ms
step:1117/2330 train_time:44609ms step_avg:39.94ms
step:1118/2330 train_time:44654ms step_avg:39.94ms
step:1119/2330 train_time:44689ms step_avg:39.94ms
step:1120/2330 train_time:44733ms step_avg:39.94ms
step:1121/2330 train_time:44768ms step_avg:39.94ms
step:1122/2330 train_time:44813ms step_avg:39.94ms
step:1123/2330 train_time:44848ms step_avg:39.94ms
step:1124/2330 train_time:44892ms step_avg:39.94ms
step:1125/2330 train_time:44927ms step_avg:39.94ms
step:1126/2330 train_time:44971ms step_avg:39.94ms
step:1127/2330 train_time:45006ms step_avg:39.93ms
step:1128/2330 train_time:45049ms step_avg:39.94ms
step:1129/2330 train_time:45085ms step_avg:39.93ms
step:1130/2330 train_time:45129ms step_avg:39.94ms
step:1131/2330 train_time:45164ms step_avg:39.93ms
step:1132/2330 train_time:45208ms step_avg:39.94ms
step:1133/2330 train_time:45243ms step_avg:39.93ms
step:1134/2330 train_time:45288ms step_avg:39.94ms
step:1135/2330 train_time:45323ms step_avg:39.93ms
step:1136/2330 train_time:45367ms step_avg:39.94ms
step:1137/2330 train_time:45403ms step_avg:39.93ms
step:1138/2330 train_time:45448ms step_avg:39.94ms
step:1139/2330 train_time:45483ms step_avg:39.93ms
step:1140/2330 train_time:45528ms step_avg:39.94ms
step:1141/2330 train_time:45563ms step_avg:39.93ms
step:1142/2330 train_time:45608ms step_avg:39.94ms
step:1143/2330 train_time:45643ms step_avg:39.93ms
step:1144/2330 train_time:45688ms step_avg:39.94ms
step:1145/2330 train_time:45723ms step_avg:39.93ms
step:1146/2330 train_time:45768ms step_avg:39.94ms
step:1147/2330 train_time:45802ms step_avg:39.93ms
step:1148/2330 train_time:45846ms step_avg:39.94ms
step:1149/2330 train_time:45881ms step_avg:39.93ms
step:1150/2330 train_time:45926ms step_avg:39.94ms
step:1151/2330 train_time:45960ms step_avg:39.93ms
step:1152/2330 train_time:46005ms step_avg:39.94ms
step:1153/2330 train_time:46040ms step_avg:39.93ms
step:1154/2330 train_time:46084ms step_avg:39.93ms
step:1155/2330 train_time:46119ms step_avg:39.93ms
step:1156/2330 train_time:46163ms step_avg:39.93ms
step:1157/2330 train_time:46199ms step_avg:39.93ms
step:1158/2330 train_time:46243ms step_avg:39.93ms
step:1159/2330 train_time:46277ms step_avg:39.93ms
step:1160/2330 train_time:46322ms step_avg:39.93ms
step:1161/2330 train_time:46357ms step_avg:39.93ms
step:1162/2330 train_time:46401ms step_avg:39.93ms
step:1163/2330 train_time:46437ms step_avg:39.93ms
step:1164/2330 train_time:46482ms step_avg:39.93ms
step:1165/2330 train_time:46517ms step_avg:39.93ms
step:1166/2330 train_time:46561ms step_avg:39.93ms
step:1167/2330 train_time:46596ms step_avg:39.93ms
step:1168/2330 train_time:46640ms step_avg:39.93ms
step:1169/2330 train_time:46675ms step_avg:39.93ms
step:1170/2330 train_time:46720ms step_avg:39.93ms
step:1171/2330 train_time:46755ms step_avg:39.93ms
step:1172/2330 train_time:46799ms step_avg:39.93ms
step:1173/2330 train_time:46834ms step_avg:39.93ms
step:1174/2330 train_time:46878ms step_avg:39.93ms
step:1175/2330 train_time:46914ms step_avg:39.93ms
step:1176/2330 train_time:46958ms step_avg:39.93ms
step:1177/2330 train_time:46993ms step_avg:39.93ms
step:1178/2330 train_time:47038ms step_avg:39.93ms
step:1179/2330 train_time:47073ms step_avg:39.93ms
step:1180/2330 train_time:47118ms step_avg:39.93ms
step:1181/2330 train_time:47153ms step_avg:39.93ms
step:1182/2330 train_time:47198ms step_avg:39.93ms
step:1183/2330 train_time:47233ms step_avg:39.93ms
step:1184/2330 train_time:47278ms step_avg:39.93ms
step:1185/2330 train_time:47314ms step_avg:39.93ms
step:1186/2330 train_time:47359ms step_avg:39.93ms
step:1187/2330 train_time:47395ms step_avg:39.93ms
step:1188/2330 train_time:47439ms step_avg:39.93ms
step:1189/2330 train_time:47474ms step_avg:39.93ms
step:1190/2330 train_time:47519ms step_avg:39.93ms
step:1191/2330 train_time:47554ms step_avg:39.93ms
step:1192/2330 train_time:47599ms step_avg:39.93ms
step:1193/2330 train_time:47634ms step_avg:39.93ms
step:1194/2330 train_time:47678ms step_avg:39.93ms
step:1195/2330 train_time:47714ms step_avg:39.93ms
step:1196/2330 train_time:47758ms step_avg:39.93ms
step:1197/2330 train_time:47793ms step_avg:39.93ms
step:1198/2330 train_time:47838ms step_avg:39.93ms
step:1199/2330 train_time:47873ms step_avg:39.93ms
step:1200/2330 train_time:47918ms step_avg:39.93ms
step:1201/2330 train_time:47953ms step_avg:39.93ms
step:1202/2330 train_time:47998ms step_avg:39.93ms
step:1203/2330 train_time:48032ms step_avg:39.93ms
step:1204/2330 train_time:48077ms step_avg:39.93ms
step:1205/2330 train_time:48113ms step_avg:39.93ms
step:1206/2330 train_time:48157ms step_avg:39.93ms
step:1207/2330 train_time:48192ms step_avg:39.93ms
step:1208/2330 train_time:48238ms step_avg:39.93ms
step:1209/2330 train_time:48273ms step_avg:39.93ms
step:1210/2330 train_time:48317ms step_avg:39.93ms
step:1211/2330 train_time:48353ms step_avg:39.93ms
step:1212/2330 train_time:48398ms step_avg:39.93ms
step:1213/2330 train_time:48433ms step_avg:39.93ms
step:1214/2330 train_time:48477ms step_avg:39.93ms
step:1215/2330 train_time:48513ms step_avg:39.93ms
step:1216/2330 train_time:48558ms step_avg:39.93ms
step:1217/2330 train_time:48593ms step_avg:39.93ms
step:1218/2330 train_time:48637ms step_avg:39.93ms
step:1219/2330 train_time:48672ms step_avg:39.93ms
step:1220/2330 train_time:48716ms step_avg:39.93ms
step:1221/2330 train_time:48751ms step_avg:39.93ms
step:1222/2330 train_time:48796ms step_avg:39.93ms
step:1223/2330 train_time:48830ms step_avg:39.93ms
step:1224/2330 train_time:48876ms step_avg:39.93ms
step:1225/2330 train_time:48911ms step_avg:39.93ms
step:1226/2330 train_time:48955ms step_avg:39.93ms
step:1227/2330 train_time:48990ms step_avg:39.93ms
step:1228/2330 train_time:49034ms step_avg:39.93ms
step:1229/2330 train_time:49069ms step_avg:39.93ms
step:1230/2330 train_time:49113ms step_avg:39.93ms
step:1231/2330 train_time:49148ms step_avg:39.93ms
step:1232/2330 train_time:49192ms step_avg:39.93ms
step:1233/2330 train_time:49228ms step_avg:39.93ms
step:1234/2330 train_time:49272ms step_avg:39.93ms
step:1235/2330 train_time:49307ms step_avg:39.93ms
step:1236/2330 train_time:49352ms step_avg:39.93ms
step:1237/2330 train_time:49387ms step_avg:39.92ms
step:1238/2330 train_time:49432ms step_avg:39.93ms
step:1239/2330 train_time:49467ms step_avg:39.93ms
step:1240/2330 train_time:49512ms step_avg:39.93ms
step:1241/2330 train_time:49548ms step_avg:39.93ms
step:1242/2330 train_time:49592ms step_avg:39.93ms
step:1243/2330 train_time:49627ms step_avg:39.93ms
step:1244/2330 train_time:49672ms step_avg:39.93ms
step:1245/2330 train_time:49707ms step_avg:39.93ms
step:1246/2330 train_time:49751ms step_avg:39.93ms
step:1247/2330 train_time:49787ms step_avg:39.93ms
step:1248/2330 train_time:49831ms step_avg:39.93ms
step:1249/2330 train_time:49866ms step_avg:39.93ms
step:1250/2330 train_time:49911ms step_avg:39.93ms
step:1250/2330 val_loss:5.1606 train_time:49999ms step_avg:40.00ms
step:1251/2330 train_time:50013ms step_avg:39.98ms
step:1252/2330 train_time:50026ms step_avg:39.96ms
step:1253/2330 train_time:50038ms step_avg:39.93ms
step:1254/2330 train_time:50072ms step_avg:39.93ms
step:1255/2330 train_time:50107ms step_avg:39.93ms
step:1256/2330 train_time:50150ms step_avg:39.93ms
step:1257/2330 train_time:50184ms step_avg:39.92ms
step:1258/2330 train_time:50228ms step_avg:39.93ms
step:1259/2330 train_time:50262ms step_avg:39.92ms
step:1260/2330 train_time:50306ms step_avg:39.93ms
step:1261/2330 train_time:50345ms step_avg:39.92ms
step:1262/2330 train_time:50393ms step_avg:39.93ms
step:1263/2330 train_time:50428ms step_avg:39.93ms
step:1264/2330 train_time:50473ms step_avg:39.93ms
step:1265/2330 train_time:50509ms step_avg:39.93ms
step:1266/2330 train_time:50552ms step_avg:39.93ms
step:1267/2330 train_time:50588ms step_avg:39.93ms
step:1268/2330 train_time:50632ms step_avg:39.93ms
step:1269/2330 train_time:50666ms step_avg:39.93ms
step:1270/2330 train_time:50710ms step_avg:39.93ms
step:1271/2330 train_time:50744ms step_avg:39.92ms
step:1272/2330 train_time:50920ms step_avg:40.03ms
step:1273/2330 train_time:50953ms step_avg:40.03ms
step:1274/2330 train_time:50996ms step_avg:40.03ms
step:1275/2330 train_time:51030ms step_avg:40.02ms
step:1276/2330 train_time:51074ms step_avg:40.03ms
step:1277/2330 train_time:51108ms step_avg:40.02ms
step:1278/2330 train_time:51151ms step_avg:40.02ms
step:1279/2330 train_time:51185ms step_avg:40.02ms
step:1280/2330 train_time:51230ms step_avg:40.02ms
step:1281/2330 train_time:51264ms step_avg:40.02ms
step:1282/2330 train_time:51307ms step_avg:40.02ms
step:1283/2330 train_time:51342ms step_avg:40.02ms
step:1284/2330 train_time:51385ms step_avg:40.02ms
step:1285/2330 train_time:51419ms step_avg:40.01ms
step:1286/2330 train_time:51463ms step_avg:40.02ms
step:1287/2330 train_time:51497ms step_avg:40.01ms
step:1288/2330 train_time:51541ms step_avg:40.02ms
step:1289/2330 train_time:51575ms step_avg:40.01ms
step:1290/2330 train_time:51619ms step_avg:40.02ms
step:1291/2330 train_time:51654ms step_avg:40.01ms
step:1292/2330 train_time:51697ms step_avg:40.01ms
step:1293/2330 train_time:51731ms step_avg:40.01ms
step:1294/2330 train_time:51781ms step_avg:40.02ms
step:1295/2330 train_time:51823ms step_avg:40.02ms
step:1296/2330 train_time:51870ms step_avg:40.02ms
step:1297/2330 train_time:51907ms step_avg:40.02ms
step:1298/2330 train_time:51952ms step_avg:40.02ms
step:1299/2330 train_time:51988ms step_avg:40.02ms
step:1300/2330 train_time:52032ms step_avg:40.02ms
step:1301/2330 train_time:52067ms step_avg:40.02ms
step:1302/2330 train_time:52111ms step_avg:40.02ms
step:1303/2330 train_time:52146ms step_avg:40.02ms
step:1304/2330 train_time:52189ms step_avg:40.02ms
step:1305/2330 train_time:52224ms step_avg:40.02ms
step:1306/2330 train_time:52267ms step_avg:40.02ms
step:1307/2330 train_time:52301ms step_avg:40.02ms
step:1308/2330 train_time:52345ms step_avg:40.02ms
step:1309/2330 train_time:52380ms step_avg:40.02ms
step:1310/2330 train_time:52424ms step_avg:40.02ms
step:1311/2330 train_time:52458ms step_avg:40.01ms
step:1312/2330 train_time:52502ms step_avg:40.02ms
step:1313/2330 train_time:52536ms step_avg:40.01ms
step:1314/2330 train_time:52580ms step_avg:40.01ms
step:1315/2330 train_time:52614ms step_avg:40.01ms
step:1316/2330 train_time:52658ms step_avg:40.01ms
step:1317/2330 train_time:52693ms step_avg:40.01ms
step:1318/2330 train_time:52739ms step_avg:40.01ms
step:1319/2330 train_time:52776ms step_avg:40.01ms
step:1320/2330 train_time:52823ms step_avg:40.02ms
step:1321/2330 train_time:52860ms step_avg:40.02ms
step:1322/2330 train_time:52907ms step_avg:40.02ms
step:1323/2330 train_time:52943ms step_avg:40.02ms
step:1324/2330 train_time:52987ms step_avg:40.02ms
step:1325/2330 train_time:53023ms step_avg:40.02ms
step:1326/2330 train_time:53067ms step_avg:40.02ms
step:1327/2330 train_time:53102ms step_avg:40.02ms
step:1328/2330 train_time:53146ms step_avg:40.02ms
step:1329/2330 train_time:53181ms step_avg:40.02ms
step:1330/2330 train_time:53225ms step_avg:40.02ms
step:1331/2330 train_time:53259ms step_avg:40.01ms
step:1332/2330 train_time:53303ms step_avg:40.02ms
step:1333/2330 train_time:53338ms step_avg:40.01ms
step:1334/2330 train_time:53382ms step_avg:40.02ms
step:1335/2330 train_time:53416ms step_avg:40.01ms
step:1336/2330 train_time:53460ms step_avg:40.01ms
step:1337/2330 train_time:53494ms step_avg:40.01ms
step:1338/2330 train_time:53537ms step_avg:40.01ms
step:1339/2330 train_time:53572ms step_avg:40.01ms
step:1340/2330 train_time:53616ms step_avg:40.01ms
step:1341/2330 train_time:53651ms step_avg:40.01ms
step:1342/2330 train_time:53695ms step_avg:40.01ms
step:1343/2330 train_time:53731ms step_avg:40.01ms
step:1344/2330 train_time:53776ms step_avg:40.01ms
step:1345/2330 train_time:53812ms step_avg:40.01ms
step:1346/2330 train_time:53858ms step_avg:40.01ms
step:1347/2330 train_time:53894ms step_avg:40.01ms
step:1348/2330 train_time:53941ms step_avg:40.02ms
step:1349/2330 train_time:53978ms step_avg:40.01ms
step:1350/2330 train_time:54023ms step_avg:40.02ms
step:1351/2330 train_time:54058ms step_avg:40.01ms
step:1352/2330 train_time:54104ms step_avg:40.02ms
step:1353/2330 train_time:54139ms step_avg:40.01ms
step:1354/2330 train_time:54183ms step_avg:40.02ms
step:1355/2330 train_time:54218ms step_avg:40.01ms
step:1356/2330 train_time:54262ms step_avg:40.02ms
step:1357/2330 train_time:54296ms step_avg:40.01ms
step:1358/2330 train_time:54340ms step_avg:40.01ms
step:1359/2330 train_time:54374ms step_avg:40.01ms
step:1360/2330 train_time:54418ms step_avg:40.01ms
step:1361/2330 train_time:54452ms step_avg:40.01ms
step:1362/2330 train_time:54496ms step_avg:40.01ms
step:1363/2330 train_time:54531ms step_avg:40.01ms
step:1364/2330 train_time:54575ms step_avg:40.01ms
step:1365/2330 train_time:54609ms step_avg:40.01ms
step:1366/2330 train_time:54654ms step_avg:40.01ms
step:1367/2330 train_time:54689ms step_avg:40.01ms
step:1368/2330 train_time:54733ms step_avg:40.01ms
step:1369/2330 train_time:54770ms step_avg:40.01ms
step:1370/2330 train_time:54815ms step_avg:40.01ms
step:1371/2330 train_time:54851ms step_avg:40.01ms
step:1372/2330 train_time:54895ms step_avg:40.01ms
step:1373/2330 train_time:54932ms step_avg:40.01ms
step:1374/2330 train_time:54977ms step_avg:40.01ms
step:1375/2330 train_time:55013ms step_avg:40.01ms
step:1376/2330 train_time:55058ms step_avg:40.01ms
step:1377/2330 train_time:55094ms step_avg:40.01ms
step:1378/2330 train_time:55139ms step_avg:40.01ms
step:1379/2330 train_time:55175ms step_avg:40.01ms
step:1380/2330 train_time:55220ms step_avg:40.01ms
step:1381/2330 train_time:55255ms step_avg:40.01ms
step:1382/2330 train_time:55299ms step_avg:40.01ms
step:1383/2330 train_time:55334ms step_avg:40.01ms
step:1384/2330 train_time:55378ms step_avg:40.01ms
step:1385/2330 train_time:55412ms step_avg:40.01ms
step:1386/2330 train_time:55456ms step_avg:40.01ms
step:1387/2330 train_time:55491ms step_avg:40.01ms
step:1388/2330 train_time:55535ms step_avg:40.01ms
step:1389/2330 train_time:55570ms step_avg:40.01ms
step:1390/2330 train_time:55614ms step_avg:40.01ms
step:1391/2330 train_time:55649ms step_avg:40.01ms
step:1392/2330 train_time:55694ms step_avg:40.01ms
step:1393/2330 train_time:55729ms step_avg:40.01ms
step:1394/2330 train_time:55774ms step_avg:40.01ms
step:1395/2330 train_time:55810ms step_avg:40.01ms
step:1396/2330 train_time:55855ms step_avg:40.01ms
step:1397/2330 train_time:55891ms step_avg:40.01ms
step:1398/2330 train_time:55935ms step_avg:40.01ms
step:1399/2330 train_time:55971ms step_avg:40.01ms
step:1400/2330 train_time:56016ms step_avg:40.01ms
step:1401/2330 train_time:56052ms step_avg:40.01ms
step:1402/2330 train_time:56098ms step_avg:40.01ms
step:1403/2330 train_time:56133ms step_avg:40.01ms
step:1404/2330 train_time:56178ms step_avg:40.01ms
step:1405/2330 train_time:56213ms step_avg:40.01ms
step:1406/2330 train_time:56257ms step_avg:40.01ms
step:1407/2330 train_time:56293ms step_avg:40.01ms
step:1408/2330 train_time:56337ms step_avg:40.01ms
step:1409/2330 train_time:56372ms step_avg:40.01ms
step:1410/2330 train_time:56416ms step_avg:40.01ms
step:1411/2330 train_time:56450ms step_avg:40.01ms
step:1412/2330 train_time:56494ms step_avg:40.01ms
step:1413/2330 train_time:56529ms step_avg:40.01ms
step:1414/2330 train_time:56573ms step_avg:40.01ms
step:1415/2330 train_time:56608ms step_avg:40.01ms
step:1416/2330 train_time:56651ms step_avg:40.01ms
step:1417/2330 train_time:56686ms step_avg:40.00ms
step:1418/2330 train_time:56732ms step_avg:40.01ms
step:1419/2330 train_time:56768ms step_avg:40.01ms
step:1420/2330 train_time:56812ms step_avg:40.01ms
step:1421/2330 train_time:56848ms step_avg:40.01ms
step:1422/2330 train_time:56893ms step_avg:40.01ms
step:1423/2330 train_time:56928ms step_avg:40.01ms
step:1424/2330 train_time:56973ms step_avg:40.01ms
step:1425/2330 train_time:57008ms step_avg:40.01ms
step:1426/2330 train_time:57052ms step_avg:40.01ms
step:1427/2330 train_time:57088ms step_avg:40.01ms
step:1428/2330 train_time:57132ms step_avg:40.01ms
step:1429/2330 train_time:57168ms step_avg:40.01ms
step:1430/2330 train_time:57212ms step_avg:40.01ms
step:1431/2330 train_time:57247ms step_avg:40.00ms
step:1432/2330 train_time:57292ms step_avg:40.01ms
step:1433/2330 train_time:57326ms step_avg:40.00ms
step:1434/2330 train_time:57370ms step_avg:40.01ms
step:1435/2330 train_time:57404ms step_avg:40.00ms
step:1436/2330 train_time:57449ms step_avg:40.01ms
step:1437/2330 train_time:57484ms step_avg:40.00ms
step:1438/2330 train_time:57528ms step_avg:40.01ms
step:1439/2330 train_time:57563ms step_avg:40.00ms
step:1440/2330 train_time:57607ms step_avg:40.00ms
step:1441/2330 train_time:57642ms step_avg:40.00ms
step:1442/2330 train_time:57687ms step_avg:40.00ms
step:1443/2330 train_time:57722ms step_avg:40.00ms
step:1444/2330 train_time:57767ms step_avg:40.00ms
step:1445/2330 train_time:57802ms step_avg:40.00ms
step:1446/2330 train_time:57847ms step_avg:40.00ms
step:1447/2330 train_time:57882ms step_avg:40.00ms
step:1448/2330 train_time:57927ms step_avg:40.00ms
step:1449/2330 train_time:57961ms step_avg:40.00ms
step:1450/2330 train_time:58006ms step_avg:40.00ms
step:1451/2330 train_time:58041ms step_avg:40.00ms
step:1452/2330 train_time:58086ms step_avg:40.00ms
step:1453/2330 train_time:58121ms step_avg:40.00ms
step:1454/2330 train_time:58165ms step_avg:40.00ms
step:1455/2330 train_time:58200ms step_avg:40.00ms
step:1456/2330 train_time:58245ms step_avg:40.00ms
step:1457/2330 train_time:58280ms step_avg:40.00ms
step:1458/2330 train_time:58323ms step_avg:40.00ms
step:1459/2330 train_time:58358ms step_avg:40.00ms
step:1460/2330 train_time:58402ms step_avg:40.00ms
step:1461/2330 train_time:58437ms step_avg:40.00ms
step:1462/2330 train_time:58483ms step_avg:40.00ms
step:1463/2330 train_time:58517ms step_avg:40.00ms
step:1464/2330 train_time:58562ms step_avg:40.00ms
step:1465/2330 train_time:58597ms step_avg:40.00ms
step:1466/2330 train_time:58642ms step_avg:40.00ms
step:1467/2330 train_time:58678ms step_avg:40.00ms
step:1468/2330 train_time:58724ms step_avg:40.00ms
step:1469/2330 train_time:58760ms step_avg:40.00ms
step:1470/2330 train_time:58804ms step_avg:40.00ms
step:1471/2330 train_time:58839ms step_avg:40.00ms
step:1472/2330 train_time:58884ms step_avg:40.00ms
step:1473/2330 train_time:58919ms step_avg:40.00ms
step:1474/2330 train_time:58964ms step_avg:40.00ms
step:1475/2330 train_time:58999ms step_avg:40.00ms
step:1476/2330 train_time:59044ms step_avg:40.00ms
step:1477/2330 train_time:59079ms step_avg:40.00ms
step:1478/2330 train_time:59124ms step_avg:40.00ms
step:1479/2330 train_time:59159ms step_avg:40.00ms
step:1480/2330 train_time:59203ms step_avg:40.00ms
step:1481/2330 train_time:59238ms step_avg:40.00ms
step:1482/2330 train_time:59283ms step_avg:40.00ms
step:1483/2330 train_time:59318ms step_avg:40.00ms
step:1484/2330 train_time:59362ms step_avg:40.00ms
step:1485/2330 train_time:59397ms step_avg:40.00ms
step:1486/2330 train_time:59441ms step_avg:40.00ms
step:1487/2330 train_time:59477ms step_avg:40.00ms
step:1488/2330 train_time:59521ms step_avg:40.00ms
step:1489/2330 train_time:59556ms step_avg:40.00ms
step:1490/2330 train_time:59600ms step_avg:40.00ms
step:1491/2330 train_time:59636ms step_avg:40.00ms
step:1492/2330 train_time:59681ms step_avg:40.00ms
step:1493/2330 train_time:59718ms step_avg:40.00ms
step:1494/2330 train_time:59763ms step_avg:40.00ms
step:1495/2330 train_time:59798ms step_avg:40.00ms
step:1496/2330 train_time:59843ms step_avg:40.00ms
step:1497/2330 train_time:59879ms step_avg:40.00ms
step:1498/2330 train_time:59923ms step_avg:40.00ms
step:1499/2330 train_time:59959ms step_avg:40.00ms
step:1500/2330 train_time:60003ms step_avg:40.00ms
step:1500/2330 val_loss:5.1260 train_time:60091ms step_avg:40.06ms
step:1501/2330 train_time:60103ms step_avg:40.04ms
step:1502/2330 train_time:60115ms step_avg:40.02ms
step:1503/2330 train_time:60126ms step_avg:40.00ms
step:1504/2330 train_time:60164ms step_avg:40.00ms
step:1505/2330 train_time:60198ms step_avg:40.00ms
step:1506/2330 train_time:60242ms step_avg:40.00ms
step:1507/2330 train_time:60276ms step_avg:40.00ms
step:1508/2330 train_time:60320ms step_avg:40.00ms
step:1509/2330 train_time:60354ms step_avg:40.00ms
step:1510/2330 train_time:60398ms step_avg:40.00ms
step:1511/2330 train_time:60440ms step_avg:40.00ms
step:1512/2330 train_time:60489ms step_avg:40.01ms
step:1513/2330 train_time:60526ms step_avg:40.00ms
step:1514/2330 train_time:60571ms step_avg:40.01ms
step:1515/2330 train_time:60606ms step_avg:40.00ms
step:1516/2330 train_time:60650ms step_avg:40.01ms
step:1517/2330 train_time:60684ms step_avg:40.00ms
step:1518/2330 train_time:60824ms step_avg:40.07ms
step:1519/2330 train_time:60857ms step_avg:40.06ms
step:1520/2330 train_time:60901ms step_avg:40.07ms
step:1521/2330 train_time:60970ms step_avg:40.09ms
step:1522/2330 train_time:61012ms step_avg:40.09ms
step:1523/2330 train_time:61046ms step_avg:40.08ms
step:1524/2330 train_time:61089ms step_avg:40.08ms
step:1525/2330 train_time:61124ms step_avg:40.08ms
step:1526/2330 train_time:61167ms step_avg:40.08ms
step:1527/2330 train_time:61202ms step_avg:40.08ms
step:1528/2330 train_time:61245ms step_avg:40.08ms
step:1529/2330 train_time:61281ms step_avg:40.08ms
step:1530/2330 train_time:61324ms step_avg:40.08ms
step:1531/2330 train_time:61358ms step_avg:40.08ms
step:1532/2330 train_time:61402ms step_avg:40.08ms
step:1533/2330 train_time:61436ms step_avg:40.08ms
step:1534/2330 train_time:61479ms step_avg:40.08ms
step:1535/2330 train_time:61513ms step_avg:40.07ms
step:1536/2330 train_time:61556ms step_avg:40.08ms
step:1537/2330 train_time:61591ms step_avg:40.07ms
step:1538/2330 train_time:61635ms step_avg:40.07ms
step:1539/2330 train_time:61671ms step_avg:40.07ms
step:1540/2330 train_time:61718ms step_avg:40.08ms
step:1541/2330 train_time:61754ms step_avg:40.07ms
step:1542/2330 train_time:61800ms step_avg:40.08ms
step:1543/2330 train_time:61837ms step_avg:40.08ms
step:1544/2330 train_time:61885ms step_avg:40.08ms
step:1545/2330 train_time:61924ms step_avg:40.08ms
step:1546/2330 train_time:61970ms step_avg:40.08ms
step:1547/2330 train_time:62005ms step_avg:40.08ms
step:1548/2330 train_time:62050ms step_avg:40.08ms
step:1549/2330 train_time:62085ms step_avg:40.08ms
step:1550/2330 train_time:62128ms step_avg:40.08ms
step:1551/2330 train_time:62164ms step_avg:40.08ms
step:1552/2330 train_time:62208ms step_avg:40.08ms
step:1553/2330 train_time:62242ms step_avg:40.08ms
step:1554/2330 train_time:62286ms step_avg:40.08ms
step:1555/2330 train_time:62321ms step_avg:40.08ms
step:1556/2330 train_time:62364ms step_avg:40.08ms
step:1557/2330 train_time:62398ms step_avg:40.08ms
step:1558/2330 train_time:62443ms step_avg:40.08ms
step:1559/2330 train_time:62477ms step_avg:40.08ms
step:1560/2330 train_time:62521ms step_avg:40.08ms
step:1561/2330 train_time:62557ms step_avg:40.07ms
step:1562/2330 train_time:62601ms step_avg:40.08ms
step:1563/2330 train_time:62636ms step_avg:40.07ms
step:1564/2330 train_time:62681ms step_avg:40.08ms
step:1565/2330 train_time:62716ms step_avg:40.07ms
step:1566/2330 train_time:62761ms step_avg:40.08ms
step:1567/2330 train_time:62796ms step_avg:40.07ms
step:1568/2330 train_time:62842ms step_avg:40.08ms
step:1569/2330 train_time:62879ms step_avg:40.08ms
step:1570/2330 train_time:62925ms step_avg:40.08ms
step:1571/2330 train_time:62962ms step_avg:40.08ms
step:1572/2330 train_time:63007ms step_avg:40.08ms
step:1573/2330 train_time:63042ms step_avg:40.08ms
step:1574/2330 train_time:63087ms step_avg:40.08ms
step:1575/2330 train_time:63121ms step_avg:40.08ms
step:1576/2330 train_time:63165ms step_avg:40.08ms
step:1577/2330 train_time:63200ms step_avg:40.08ms
step:1578/2330 train_time:63245ms step_avg:40.08ms
step:1579/2330 train_time:63279ms step_avg:40.08ms
step:1580/2330 train_time:63323ms step_avg:40.08ms
step:1581/2330 train_time:63357ms step_avg:40.07ms
step:1582/2330 train_time:63401ms step_avg:40.08ms
step:1583/2330 train_time:63436ms step_avg:40.07ms
step:1584/2330 train_time:63479ms step_avg:40.08ms
step:1585/2330 train_time:63514ms step_avg:40.07ms
step:1586/2330 train_time:63558ms step_avg:40.07ms
step:1587/2330 train_time:63592ms step_avg:40.07ms
step:1588/2330 train_time:63636ms step_avg:40.07ms
step:1589/2330 train_time:63671ms step_avg:40.07ms
step:1590/2330 train_time:63716ms step_avg:40.07ms
step:1591/2330 train_time:63752ms step_avg:40.07ms
step:1592/2330 train_time:63797ms step_avg:40.07ms
step:1593/2330 train_time:63833ms step_avg:40.07ms
step:1594/2330 train_time:63878ms step_avg:40.07ms
step:1595/2330 train_time:63914ms step_avg:40.07ms
step:1596/2330 train_time:63961ms step_avg:40.08ms
step:1597/2330 train_time:63998ms step_avg:40.07ms
step:1598/2330 train_time:64043ms step_avg:40.08ms
step:1599/2330 train_time:64078ms step_avg:40.07ms
step:1600/2330 train_time:64123ms step_avg:40.08ms
step:1601/2330 train_time:64158ms step_avg:40.07ms
step:1602/2330 train_time:64203ms step_avg:40.08ms
step:1603/2330 train_time:64238ms step_avg:40.07ms
step:1604/2330 train_time:64282ms step_avg:40.08ms
step:1605/2330 train_time:64316ms step_avg:40.07ms
step:1606/2330 train_time:64360ms step_avg:40.07ms
step:1607/2330 train_time:64395ms step_avg:40.07ms
step:1608/2330 train_time:64439ms step_avg:40.07ms
step:1609/2330 train_time:64473ms step_avg:40.07ms
step:1610/2330 train_time:64517ms step_avg:40.07ms
step:1611/2330 train_time:64552ms step_avg:40.07ms
step:1612/2330 train_time:64596ms step_avg:40.07ms
step:1613/2330 train_time:64632ms step_avg:40.07ms
step:1614/2330 train_time:64676ms step_avg:40.07ms
step:1615/2330 train_time:64711ms step_avg:40.07ms
step:1616/2330 train_time:64756ms step_avg:40.07ms
step:1617/2330 train_time:64792ms step_avg:40.07ms
step:1618/2330 train_time:64837ms step_avg:40.07ms
step:1619/2330 train_time:64874ms step_avg:40.07ms
step:1620/2330 train_time:64919ms step_avg:40.07ms
step:1621/2330 train_time:64955ms step_avg:40.07ms
step:1622/2330 train_time:65001ms step_avg:40.07ms
step:1623/2330 train_time:65038ms step_avg:40.07ms
step:1624/2330 train_time:65083ms step_avg:40.08ms
step:1625/2330 train_time:65118ms step_avg:40.07ms
step:1626/2330 train_time:65163ms step_avg:40.08ms
step:1627/2330 train_time:65198ms step_avg:40.07ms
step:1628/2330 train_time:65243ms step_avg:40.08ms
step:1629/2330 train_time:65277ms step_avg:40.07ms
step:1630/2330 train_time:65322ms step_avg:40.07ms
step:1631/2330 train_time:65357ms step_avg:40.07ms
step:1632/2330 train_time:65401ms step_avg:40.07ms
step:1633/2330 train_time:65436ms step_avg:40.07ms
step:1634/2330 train_time:65480ms step_avg:40.07ms
step:1635/2330 train_time:65515ms step_avg:40.07ms
step:1636/2330 train_time:65559ms step_avg:40.07ms
step:1637/2330 train_time:65594ms step_avg:40.07ms
step:1638/2330 train_time:65639ms step_avg:40.07ms
step:1639/2330 train_time:65674ms step_avg:40.07ms
step:1640/2330 train_time:65718ms step_avg:40.07ms
step:1641/2330 train_time:65753ms step_avg:40.07ms
step:1642/2330 train_time:65798ms step_avg:40.07ms
step:1643/2330 train_time:65834ms step_avg:40.07ms
step:1644/2330 train_time:65879ms step_avg:40.07ms
step:1645/2330 train_time:65915ms step_avg:40.07ms
step:1646/2330 train_time:65960ms step_avg:40.07ms
step:1647/2330 train_time:65996ms step_avg:40.07ms
step:1648/2330 train_time:66041ms step_avg:40.07ms
step:1649/2330 train_time:66076ms step_avg:40.07ms
step:1650/2330 train_time:66121ms step_avg:40.07ms
step:1651/2330 train_time:66157ms step_avg:40.07ms
step:1652/2330 train_time:66202ms step_avg:40.07ms
step:1653/2330 train_time:66236ms step_avg:40.07ms
step:1654/2330 train_time:66281ms step_avg:40.07ms
step:1655/2330 train_time:66316ms step_avg:40.07ms
step:1656/2330 train_time:66359ms step_avg:40.07ms
step:1657/2330 train_time:66394ms step_avg:40.07ms
step:1658/2330 train_time:66438ms step_avg:40.07ms
step:1659/2330 train_time:66473ms step_avg:40.07ms
step:1660/2330 train_time:66517ms step_avg:40.07ms
step:1661/2330 train_time:66553ms step_avg:40.07ms
step:1662/2330 train_time:66597ms step_avg:40.07ms
step:1663/2330 train_time:66632ms step_avg:40.07ms
step:1664/2330 train_time:66676ms step_avg:40.07ms
step:1665/2330 train_time:66712ms step_avg:40.07ms
step:1666/2330 train_time:66756ms step_avg:40.07ms
step:1667/2330 train_time:66792ms step_avg:40.07ms
step:1668/2330 train_time:66837ms step_avg:40.07ms
step:1669/2330 train_time:66873ms step_avg:40.07ms
step:1670/2330 train_time:66917ms step_avg:40.07ms
step:1671/2330 train_time:66953ms step_avg:40.07ms
step:1672/2330 train_time:66997ms step_avg:40.07ms
step:1673/2330 train_time:67033ms step_avg:40.07ms
step:1674/2330 train_time:67078ms step_avg:40.07ms
step:1675/2330 train_time:67114ms step_avg:40.07ms
step:1676/2330 train_time:67158ms step_avg:40.07ms
step:1677/2330 train_time:67194ms step_avg:40.07ms
step:1678/2330 train_time:67239ms step_avg:40.07ms
step:1679/2330 train_time:67274ms step_avg:40.07ms
step:1680/2330 train_time:67318ms step_avg:40.07ms
step:1681/2330 train_time:67352ms step_avg:40.07ms
step:1682/2330 train_time:67396ms step_avg:40.07ms
step:1683/2330 train_time:67431ms step_avg:40.07ms
step:1684/2330 train_time:67475ms step_avg:40.07ms
step:1685/2330 train_time:67509ms step_avg:40.06ms
step:1686/2330 train_time:67553ms step_avg:40.07ms
step:1687/2330 train_time:67588ms step_avg:40.06ms
step:1688/2330 train_time:67633ms step_avg:40.07ms
step:1689/2330 train_time:67667ms step_avg:40.06ms
step:1690/2330 train_time:67713ms step_avg:40.07ms
step:1691/2330 train_time:67748ms step_avg:40.06ms
step:1692/2330 train_time:67792ms step_avg:40.07ms
step:1693/2330 train_time:67827ms step_avg:40.06ms
step:1694/2330 train_time:67871ms step_avg:40.07ms
step:1695/2330 train_time:67906ms step_avg:40.06ms
step:1696/2330 train_time:67951ms step_avg:40.07ms
step:1697/2330 train_time:67986ms step_avg:40.06ms
step:1698/2330 train_time:68031ms step_avg:40.07ms
step:1699/2330 train_time:68067ms step_avg:40.06ms
step:1700/2330 train_time:68111ms step_avg:40.07ms
step:1701/2330 train_time:68147ms step_avg:40.06ms
step:1702/2330 train_time:68192ms step_avg:40.07ms
step:1703/2330 train_time:68227ms step_avg:40.06ms
step:1704/2330 train_time:68271ms step_avg:40.07ms
step:1705/2330 train_time:68306ms step_avg:40.06ms
step:1706/2330 train_time:68351ms step_avg:40.06ms
step:1707/2330 train_time:68385ms step_avg:40.06ms
step:1708/2330 train_time:68430ms step_avg:40.06ms
step:1709/2330 train_time:68464ms step_avg:40.06ms
step:1710/2330 train_time:68509ms step_avg:40.06ms
step:1711/2330 train_time:68544ms step_avg:40.06ms
step:1712/2330 train_time:68589ms step_avg:40.06ms
step:1713/2330 train_time:68623ms step_avg:40.06ms
step:1714/2330 train_time:68668ms step_avg:40.06ms
step:1715/2330 train_time:68703ms step_avg:40.06ms
step:1716/2330 train_time:68747ms step_avg:40.06ms
step:1717/2330 train_time:68783ms step_avg:40.06ms
step:1718/2330 train_time:68827ms step_avg:40.06ms
step:1719/2330 train_time:68862ms step_avg:40.06ms
step:1720/2330 train_time:68906ms step_avg:40.06ms
step:1721/2330 train_time:68941ms step_avg:40.06ms
step:1722/2330 train_time:68986ms step_avg:40.06ms
step:1723/2330 train_time:69022ms step_avg:40.06ms
step:1724/2330 train_time:69066ms step_avg:40.06ms
step:1725/2330 train_time:69101ms step_avg:40.06ms
step:1726/2330 train_time:69146ms step_avg:40.06ms
step:1727/2330 train_time:69181ms step_avg:40.06ms
step:1728/2330 train_time:69225ms step_avg:40.06ms
step:1729/2330 train_time:69260ms step_avg:40.06ms
step:1730/2330 train_time:69305ms step_avg:40.06ms
step:1731/2330 train_time:69340ms step_avg:40.06ms
step:1732/2330 train_time:69384ms step_avg:40.06ms
step:1733/2330 train_time:69419ms step_avg:40.06ms
step:1734/2330 train_time:69464ms step_avg:40.06ms
step:1735/2330 train_time:69500ms step_avg:40.06ms
step:1736/2330 train_time:69545ms step_avg:40.06ms
step:1737/2330 train_time:69580ms step_avg:40.06ms
step:1738/2330 train_time:69625ms step_avg:40.06ms
step:1739/2330 train_time:69660ms step_avg:40.06ms
step:1740/2330 train_time:69705ms step_avg:40.06ms
step:1741/2330 train_time:69740ms step_avg:40.06ms
step:1742/2330 train_time:69785ms step_avg:40.06ms
step:1743/2330 train_time:69821ms step_avg:40.06ms
step:1744/2330 train_time:69865ms step_avg:40.06ms
step:1745/2330 train_time:69899ms step_avg:40.06ms
step:1746/2330 train_time:69945ms step_avg:40.06ms
step:1747/2330 train_time:69979ms step_avg:40.06ms
step:1748/2330 train_time:70024ms step_avg:40.06ms
step:1749/2330 train_time:70059ms step_avg:40.06ms
step:1750/2330 train_time:70103ms step_avg:40.06ms
step:1750/2330 val_loss:5.0920 train_time:70191ms step_avg:40.11ms
step:1751/2330 train_time:70204ms step_avg:40.09ms
step:1752/2330 train_time:70216ms step_avg:40.08ms
step:1753/2330 train_time:70227ms step_avg:40.06ms
step:1754/2330 train_time:70265ms step_avg:40.06ms
step:1755/2330 train_time:70298ms step_avg:40.06ms
step:1756/2330 train_time:70342ms step_avg:40.06ms
step:1757/2330 train_time:70376ms step_avg:40.05ms
step:1758/2330 train_time:70419ms step_avg:40.06ms
step:1759/2330 train_time:70454ms step_avg:40.05ms
step:1760/2330 train_time:70497ms step_avg:40.06ms
step:1761/2330 train_time:70534ms step_avg:40.05ms
step:1762/2330 train_time:70582ms step_avg:40.06ms
step:1763/2330 train_time:70620ms step_avg:40.06ms
step:1764/2330 train_time:70665ms step_avg:40.06ms
step:1765/2330 train_time:70700ms step_avg:40.06ms
step:1766/2330 train_time:70743ms step_avg:40.06ms
step:1767/2330 train_time:70777ms step_avg:40.06ms
step:1768/2330 train_time:70821ms step_avg:40.06ms
step:1769/2330 train_time:70856ms step_avg:40.05ms
step:1770/2330 train_time:70900ms step_avg:40.06ms
step:1771/2330 train_time:70935ms step_avg:40.05ms
step:1772/2330 train_time:70978ms step_avg:40.06ms
step:1773/2330 train_time:71013ms step_avg:40.05ms
step:1774/2330 train_time:71056ms step_avg:40.05ms
step:1775/2330 train_time:71092ms step_avg:40.05ms
step:1776/2330 train_time:71142ms step_avg:40.06ms
step:1777/2330 train_time:71180ms step_avg:40.06ms
step:1778/2330 train_time:71227ms step_avg:40.06ms
step:1779/2330 train_time:71262ms step_avg:40.06ms
step:1780/2330 train_time:71306ms step_avg:40.06ms
step:1781/2330 train_time:71341ms step_avg:40.06ms
step:1782/2330 train_time:71385ms step_avg:40.06ms
step:1783/2330 train_time:71420ms step_avg:40.06ms
step:1784/2330 train_time:71465ms step_avg:40.06ms
step:1785/2330 train_time:71501ms step_avg:40.06ms
step:1786/2330 train_time:71545ms step_avg:40.06ms
step:1787/2330 train_time:71579ms step_avg:40.06ms
step:1788/2330 train_time:71624ms step_avg:40.06ms
step:1789/2330 train_time:71659ms step_avg:40.06ms
step:1790/2330 train_time:71703ms step_avg:40.06ms
step:1791/2330 train_time:71738ms step_avg:40.05ms
step:1792/2330 train_time:71782ms step_avg:40.06ms
step:1793/2330 train_time:71816ms step_avg:40.05ms
step:1794/2330 train_time:71860ms step_avg:40.06ms
step:1795/2330 train_time:71894ms step_avg:40.05ms
step:1796/2330 train_time:71937ms step_avg:40.05ms
step:1797/2330 train_time:71972ms step_avg:40.05ms
step:1798/2330 train_time:72016ms step_avg:40.05ms
step:1799/2330 train_time:72052ms step_avg:40.05ms
step:1800/2330 train_time:72098ms step_avg:40.05ms
step:1801/2330 train_time:72133ms step_avg:40.05ms
step:1802/2330 train_time:72179ms step_avg:40.06ms
step:1803/2330 train_time:72215ms step_avg:40.05ms
step:1804/2330 train_time:72261ms step_avg:40.06ms
step:1805/2330 train_time:72296ms step_avg:40.05ms
step:1806/2330 train_time:72341ms step_avg:40.06ms
step:1807/2330 train_time:72376ms step_avg:40.05ms
step:1808/2330 train_time:72420ms step_avg:40.06ms
step:1809/2330 train_time:72455ms step_avg:40.05ms
step:1810/2330 train_time:72499ms step_avg:40.05ms
step:1811/2330 train_time:72534ms step_avg:40.05ms
step:1812/2330 train_time:72579ms step_avg:40.05ms
step:1813/2330 train_time:72613ms step_avg:40.05ms
step:1814/2330 train_time:72658ms step_avg:40.05ms
step:1815/2330 train_time:72693ms step_avg:40.05ms
step:1816/2330 train_time:72737ms step_avg:40.05ms
step:1817/2330 train_time:72771ms step_avg:40.05ms
step:1818/2330 train_time:72816ms step_avg:40.05ms
step:1819/2330 train_time:72851ms step_avg:40.05ms
step:1820/2330 train_time:72895ms step_avg:40.05ms
step:1821/2330 train_time:72929ms step_avg:40.05ms
step:1822/2330 train_time:72973ms step_avg:40.05ms
step:1823/2330 train_time:73007ms step_avg:40.05ms
step:1824/2330 train_time:73052ms step_avg:40.05ms
step:1825/2330 train_time:73088ms step_avg:40.05ms
step:1826/2330 train_time:73133ms step_avg:40.05ms
step:1827/2330 train_time:73168ms step_avg:40.05ms
step:1828/2330 train_time:73214ms step_avg:40.05ms
step:1829/2330 train_time:73249ms step_avg:40.05ms
step:1830/2330 train_time:73296ms step_avg:40.05ms
step:1831/2330 train_time:73330ms step_avg:40.05ms
step:1832/2330 train_time:73375ms step_avg:40.05ms
step:1833/2330 train_time:73410ms step_avg:40.05ms
step:1834/2330 train_time:73455ms step_avg:40.05ms
step:1835/2330 train_time:73490ms step_avg:40.05ms
step:1836/2330 train_time:73534ms step_avg:40.05ms
step:1837/2330 train_time:73569ms step_avg:40.05ms
step:1838/2330 train_time:73613ms step_avg:40.05ms
step:1839/2330 train_time:73647ms step_avg:40.05ms
step:1840/2330 train_time:73691ms step_avg:40.05ms
step:1841/2330 train_time:73725ms step_avg:40.05ms
step:1842/2330 train_time:73770ms step_avg:40.05ms
step:1843/2330 train_time:73805ms step_avg:40.05ms
step:1844/2330 train_time:73849ms step_avg:40.05ms
step:1845/2330 train_time:73884ms step_avg:40.05ms
step:1846/2330 train_time:73928ms step_avg:40.05ms
step:1847/2330 train_time:73962ms step_avg:40.04ms
step:1848/2330 train_time:74007ms step_avg:40.05ms
step:1849/2330 train_time:74042ms step_avg:40.04ms
step:1850/2330 train_time:74086ms step_avg:40.05ms
step:1851/2330 train_time:74122ms step_avg:40.04ms
step:1852/2330 train_time:74167ms step_avg:40.05ms
step:1853/2330 train_time:74203ms step_avg:40.04ms
step:1854/2330 train_time:74247ms step_avg:40.05ms
step:1855/2330 train_time:74282ms step_avg:40.04ms
step:1856/2330 train_time:74327ms step_avg:40.05ms
step:1857/2330 train_time:74363ms step_avg:40.04ms
step:1858/2330 train_time:74407ms step_avg:40.05ms
step:1859/2330 train_time:74442ms step_avg:40.04ms
step:1860/2330 train_time:74487ms step_avg:40.05ms
step:1861/2330 train_time:74523ms step_avg:40.04ms
step:1862/2330 train_time:74567ms step_avg:40.05ms
step:1863/2330 train_time:74602ms step_avg:40.04ms
step:1864/2330 train_time:74646ms step_avg:40.05ms
step:1865/2330 train_time:74681ms step_avg:40.04ms
step:1866/2330 train_time:74726ms step_avg:40.05ms
step:1867/2330 train_time:74762ms step_avg:40.04ms
step:1868/2330 train_time:74806ms step_avg:40.05ms
step:1869/2330 train_time:74841ms step_avg:40.04ms
step:1870/2330 train_time:74885ms step_avg:40.05ms
step:1871/2330 train_time:74920ms step_avg:40.04ms
step:1872/2330 train_time:74965ms step_avg:40.05ms
step:1873/2330 train_time:74999ms step_avg:40.04ms
step:1874/2330 train_time:75044ms step_avg:40.04ms
step:1875/2330 train_time:75079ms step_avg:40.04ms
step:1876/2330 train_time:75123ms step_avg:40.04ms
step:1877/2330 train_time:75158ms step_avg:40.04ms
step:1878/2330 train_time:75203ms step_avg:40.04ms
step:1879/2330 train_time:75237ms step_avg:40.04ms
step:1880/2330 train_time:75281ms step_avg:40.04ms
step:1881/2330 train_time:75316ms step_avg:40.04ms
step:1882/2330 train_time:75361ms step_avg:40.04ms
step:1883/2330 train_time:75396ms step_avg:40.04ms
step:1884/2330 train_time:75440ms step_avg:40.04ms
step:1885/2330 train_time:75475ms step_avg:40.04ms
step:1886/2330 train_time:75520ms step_avg:40.04ms
step:1887/2330 train_time:75555ms step_avg:40.04ms
step:1888/2330 train_time:75600ms step_avg:40.04ms
step:1889/2330 train_time:75635ms step_avg:40.04ms
step:1890/2330 train_time:75679ms step_avg:40.04ms
step:1891/2330 train_time:75714ms step_avg:40.04ms
step:1892/2330 train_time:75759ms step_avg:40.04ms
step:1893/2330 train_time:75795ms step_avg:40.04ms
step:1894/2330 train_time:75839ms step_avg:40.04ms
step:1895/2330 train_time:75874ms step_avg:40.04ms
step:1896/2330 train_time:75918ms step_avg:40.04ms
step:1897/2330 train_time:75953ms step_avg:40.04ms
step:1898/2330 train_time:75997ms step_avg:40.04ms
step:1899/2330 train_time:76031ms step_avg:40.04ms
step:1900/2330 train_time:76076ms step_avg:40.04ms
step:1901/2330 train_time:76112ms step_avg:40.04ms
step:1902/2330 train_time:76156ms step_avg:40.04ms
step:1903/2330 train_time:76191ms step_avg:40.04ms
step:1904/2330 train_time:76236ms step_avg:40.04ms
step:1905/2330 train_time:76271ms step_avg:40.04ms
step:1906/2330 train_time:76315ms step_avg:40.04ms
step:1907/2330 train_time:76352ms step_avg:40.04ms
step:1908/2330 train_time:76397ms step_avg:40.04ms
step:1909/2330 train_time:76432ms step_avg:40.04ms
step:1910/2330 train_time:76476ms step_avg:40.04ms
step:1911/2330 train_time:76511ms step_avg:40.04ms
step:1912/2330 train_time:76556ms step_avg:40.04ms
step:1913/2330 train_time:76592ms step_avg:40.04ms
step:1914/2330 train_time:76636ms step_avg:40.04ms
step:1915/2330 train_time:76671ms step_avg:40.04ms
step:1916/2330 train_time:76716ms step_avg:40.04ms
step:1917/2330 train_time:76752ms step_avg:40.04ms
step:1918/2330 train_time:76797ms step_avg:40.04ms
step:1919/2330 train_time:76832ms step_avg:40.04ms
step:1920/2330 train_time:76877ms step_avg:40.04ms
step:1921/2330 train_time:76911ms step_avg:40.04ms
step:1922/2330 train_time:76956ms step_avg:40.04ms
step:1923/2330 train_time:76990ms step_avg:40.04ms
step:1924/2330 train_time:77034ms step_avg:40.04ms
step:1925/2330 train_time:77069ms step_avg:40.04ms
step:1926/2330 train_time:77114ms step_avg:40.04ms
step:1927/2330 train_time:77150ms step_avg:40.04ms
step:1928/2330 train_time:77195ms step_avg:40.04ms
step:1929/2330 train_time:77230ms step_avg:40.04ms
step:1930/2330 train_time:77275ms step_avg:40.04ms
step:1931/2330 train_time:77310ms step_avg:40.04ms
step:1932/2330 train_time:77355ms step_avg:40.04ms
step:1933/2330 train_time:77391ms step_avg:40.04ms
step:1934/2330 train_time:77436ms step_avg:40.04ms
step:1935/2330 train_time:77471ms step_avg:40.04ms
step:1936/2330 train_time:77516ms step_avg:40.04ms
step:1937/2330 train_time:77551ms step_avg:40.04ms
step:1938/2330 train_time:77596ms step_avg:40.04ms
step:1939/2330 train_time:77632ms step_avg:40.04ms
step:1940/2330 train_time:77677ms step_avg:40.04ms
step:1941/2330 train_time:77712ms step_avg:40.04ms
step:1942/2330 train_time:77756ms step_avg:40.04ms
step:1943/2330 train_time:77792ms step_avg:40.04ms
step:1944/2330 train_time:77836ms step_avg:40.04ms
step:1945/2330 train_time:77871ms step_avg:40.04ms
step:1946/2330 train_time:77915ms step_avg:40.04ms
step:1947/2330 train_time:77950ms step_avg:40.04ms
step:1948/2330 train_time:77995ms step_avg:40.04ms
step:1949/2330 train_time:78030ms step_avg:40.04ms
step:1950/2330 train_time:78074ms step_avg:40.04ms
step:1951/2330 train_time:78108ms step_avg:40.03ms
step:1952/2330 train_time:78153ms step_avg:40.04ms
step:1953/2330 train_time:78188ms step_avg:40.03ms
step:1954/2330 train_time:78232ms step_avg:40.04ms
step:1955/2330 train_time:78267ms step_avg:40.03ms
step:1956/2330 train_time:78311ms step_avg:40.04ms
step:1957/2330 train_time:78346ms step_avg:40.03ms
step:1958/2330 train_time:78391ms step_avg:40.04ms
step:1959/2330 train_time:78427ms step_avg:40.03ms
step:1960/2330 train_time:78472ms step_avg:40.04ms
step:1961/2330 train_time:78507ms step_avg:40.03ms
step:1962/2330 train_time:78551ms step_avg:40.04ms
step:1963/2330 train_time:78587ms step_avg:40.03ms
step:1964/2330 train_time:78631ms step_avg:40.04ms
step:1965/2330 train_time:78665ms step_avg:40.03ms
step:1966/2330 train_time:78710ms step_avg:40.04ms
step:1967/2330 train_time:78746ms step_avg:40.03ms
step:1968/2330 train_time:78790ms step_avg:40.04ms
step:1969/2330 train_time:78826ms step_avg:40.03ms
step:1970/2330 train_time:78870ms step_avg:40.04ms
step:1971/2330 train_time:78905ms step_avg:40.03ms
step:1972/2330 train_time:78949ms step_avg:40.04ms
step:1973/2330 train_time:78984ms step_avg:40.03ms
step:1974/2330 train_time:79029ms step_avg:40.03ms
step:1975/2330 train_time:79064ms step_avg:40.03ms
step:1976/2330 train_time:79108ms step_avg:40.03ms
step:1977/2330 train_time:79144ms step_avg:40.03ms
step:1978/2330 train_time:79188ms step_avg:40.03ms
step:1979/2330 train_time:79224ms step_avg:40.03ms
step:1980/2330 train_time:79268ms step_avg:40.03ms
step:1981/2330 train_time:79304ms step_avg:40.03ms
step:1982/2330 train_time:79348ms step_avg:40.03ms
step:1983/2330 train_time:79384ms step_avg:40.03ms
step:1984/2330 train_time:79429ms step_avg:40.03ms
step:1985/2330 train_time:79464ms step_avg:40.03ms
step:1986/2330 train_time:79509ms step_avg:40.03ms
step:1987/2330 train_time:79544ms step_avg:40.03ms
step:1988/2330 train_time:79588ms step_avg:40.03ms
step:1989/2330 train_time:79624ms step_avg:40.03ms
step:1990/2330 train_time:79669ms step_avg:40.03ms
step:1991/2330 train_time:79705ms step_avg:40.03ms
step:1992/2330 train_time:79749ms step_avg:40.03ms
step:1993/2330 train_time:79785ms step_avg:40.03ms
step:1994/2330 train_time:79829ms step_avg:40.03ms
step:1995/2330 train_time:79864ms step_avg:40.03ms
step:1996/2330 train_time:79909ms step_avg:40.03ms
step:1997/2330 train_time:79944ms step_avg:40.03ms
step:1998/2330 train_time:79988ms step_avg:40.03ms
step:1999/2330 train_time:80023ms step_avg:40.03ms
step:2000/2330 train_time:80067ms step_avg:40.03ms
step:2000/2330 val_loss:5.0623 train_time:80154ms step_avg:40.08ms
step:2001/2330 train_time:80166ms step_avg:40.06ms
step:2002/2330 train_time:80178ms step_avg:40.05ms
step:2003/2330 train_time:80188ms step_avg:40.03ms
step:2004/2330 train_time:80227ms step_avg:40.03ms
step:2005/2330 train_time:80261ms step_avg:40.03ms
step:2006/2330 train_time:80305ms step_avg:40.03ms
step:2007/2330 train_time:80339ms step_avg:40.03ms
step:2008/2330 train_time:80383ms step_avg:40.03ms
step:2009/2330 train_time:80417ms step_avg:40.03ms
step:2010/2330 train_time:80461ms step_avg:40.03ms
step:2011/2330 train_time:80499ms step_avg:40.03ms
step:2012/2330 train_time:80549ms step_avg:40.03ms
step:2013/2330 train_time:80585ms step_avg:40.03ms
step:2014/2330 train_time:80631ms step_avg:40.04ms
step:2015/2330 train_time:80666ms step_avg:40.03ms
step:2016/2330 train_time:80709ms step_avg:40.03ms
step:2017/2330 train_time:80745ms step_avg:40.03ms
step:2018/2330 train_time:80789ms step_avg:40.03ms
step:2019/2330 train_time:80824ms step_avg:40.03ms
step:2020/2330 train_time:80868ms step_avg:40.03ms
step:2021/2330 train_time:80903ms step_avg:40.03ms
step:2022/2330 train_time:80947ms step_avg:40.03ms
step:2023/2330 train_time:80982ms step_avg:40.03ms
step:2024/2330 train_time:81026ms step_avg:40.03ms
step:2025/2330 train_time:81061ms step_avg:40.03ms
step:2026/2330 train_time:81107ms step_avg:40.03ms
step:2027/2330 train_time:81141ms step_avg:40.03ms
step:2028/2330 train_time:81186ms step_avg:40.03ms
step:2029/2330 train_time:81220ms step_avg:40.03ms
step:2030/2330 train_time:81265ms step_avg:40.03ms
step:2031/2330 train_time:81299ms step_avg:40.03ms
step:2032/2330 train_time:81343ms step_avg:40.03ms
step:2033/2330 train_time:81377ms step_avg:40.03ms
step:2034/2330 train_time:81422ms step_avg:40.03ms
step:2035/2330 train_time:81457ms step_avg:40.03ms
step:2036/2330 train_time:81502ms step_avg:40.03ms
step:2037/2330 train_time:81539ms step_avg:40.03ms
step:2038/2330 train_time:81584ms step_avg:40.03ms
step:2039/2330 train_time:81621ms step_avg:40.03ms
step:2040/2330 train_time:81666ms step_avg:40.03ms
step:2041/2330 train_time:81700ms step_avg:40.03ms
step:2042/2330 train_time:81744ms step_avg:40.03ms
step:2043/2330 train_time:81779ms step_avg:40.03ms
step:2044/2330 train_time:81824ms step_avg:40.03ms
step:2045/2330 train_time:81859ms step_avg:40.03ms
step:2046/2330 train_time:81903ms step_avg:40.03ms
step:2047/2330 train_time:81937ms step_avg:40.03ms
step:2048/2330 train_time:81981ms step_avg:40.03ms
step:2049/2330 train_time:82016ms step_avg:40.03ms
step:2050/2330 train_time:82060ms step_avg:40.03ms
step:2051/2330 train_time:82094ms step_avg:40.03ms
step:2052/2330 train_time:82138ms step_avg:40.03ms
step:2053/2330 train_time:82173ms step_avg:40.03ms
step:2054/2330 train_time:82217ms step_avg:40.03ms
step:2055/2330 train_time:82251ms step_avg:40.02ms
step:2056/2330 train_time:82295ms step_avg:40.03ms
step:2057/2330 train_time:82330ms step_avg:40.02ms
step:2058/2330 train_time:82374ms step_avg:40.03ms
step:2059/2330 train_time:82410ms step_avg:40.02ms
step:2060/2330 train_time:82454ms step_avg:40.03ms
step:2061/2330 train_time:82489ms step_avg:40.02ms
step:2062/2330 train_time:82534ms step_avg:40.03ms
step:2063/2330 train_time:82570ms step_avg:40.02ms
step:2064/2330 train_time:82614ms step_avg:40.03ms
step:2065/2330 train_time:82649ms step_avg:40.02ms
step:2066/2330 train_time:82694ms step_avg:40.03ms
step:2067/2330 train_time:82729ms step_avg:40.02ms
step:2068/2330 train_time:82773ms step_avg:40.03ms
step:2069/2330 train_time:82808ms step_avg:40.02ms
step:2070/2330 train_time:82852ms step_avg:40.03ms
step:2071/2330 train_time:82887ms step_avg:40.02ms
step:2072/2330 train_time:82932ms step_avg:40.03ms
step:2073/2330 train_time:82967ms step_avg:40.02ms
step:2074/2330 train_time:83011ms step_avg:40.02ms
step:2075/2330 train_time:83046ms step_avg:40.02ms
step:2076/2330 train_time:83090ms step_avg:40.02ms
step:2077/2330 train_time:83125ms step_avg:40.02ms
step:2078/2330 train_time:83169ms step_avg:40.02ms
step:2079/2330 train_time:83204ms step_avg:40.02ms
step:2080/2330 train_time:83248ms step_avg:40.02ms
step:2081/2330 train_time:83283ms step_avg:40.02ms
step:2082/2330 train_time:83327ms step_avg:40.02ms
step:2083/2330 train_time:83362ms step_avg:40.02ms
step:2084/2330 train_time:83407ms step_avg:40.02ms
step:2085/2330 train_time:83442ms step_avg:40.02ms
step:2086/2330 train_time:83487ms step_avg:40.02ms
step:2087/2330 train_time:83522ms step_avg:40.02ms
step:2088/2330 train_time:83567ms step_avg:40.02ms
step:2089/2330 train_time:83603ms step_avg:40.02ms
step:2090/2330 train_time:83648ms step_avg:40.02ms
step:2091/2330 train_time:83683ms step_avg:40.02ms
step:2092/2330 train_time:83727ms step_avg:40.02ms
step:2093/2330 train_time:83762ms step_avg:40.02ms
step:2094/2330 train_time:83807ms step_avg:40.02ms
step:2095/2330 train_time:83842ms step_avg:40.02ms
step:2096/2330 train_time:83887ms step_avg:40.02ms
step:2097/2330 train_time:83922ms step_avg:40.02ms
step:2098/2330 train_time:83966ms step_avg:40.02ms
step:2099/2330 train_time:84001ms step_avg:40.02ms
step:2100/2330 train_time:84046ms step_avg:40.02ms
step:2101/2330 train_time:84081ms step_avg:40.02ms
step:2102/2330 train_time:84125ms step_avg:40.02ms
step:2103/2330 train_time:84160ms step_avg:40.02ms
step:2104/2330 train_time:84204ms step_avg:40.02ms
step:2105/2330 train_time:84239ms step_avg:40.02ms
step:2106/2330 train_time:84283ms step_avg:40.02ms
step:2107/2330 train_time:84318ms step_avg:40.02ms
step:2108/2330 train_time:84362ms step_avg:40.02ms
step:2109/2330 train_time:84397ms step_avg:40.02ms
step:2110/2330 train_time:84441ms step_avg:40.02ms
step:2111/2330 train_time:84476ms step_avg:40.02ms
step:2112/2330 train_time:84520ms step_avg:40.02ms
step:2113/2330 train_time:84556ms step_avg:40.02ms
step:2114/2330 train_time:84600ms step_avg:40.02ms
step:2115/2330 train_time:84635ms step_avg:40.02ms
step:2116/2330 train_time:84679ms step_avg:40.02ms
step:2117/2330 train_time:84715ms step_avg:40.02ms
step:2118/2330 train_time:84759ms step_avg:40.02ms
step:2119/2330 train_time:84795ms step_avg:40.02ms
step:2120/2330 train_time:84840ms step_avg:40.02ms
step:2121/2330 train_time:84875ms step_avg:40.02ms
step:2122/2330 train_time:84920ms step_avg:40.02ms
step:2123/2330 train_time:84955ms step_avg:40.02ms
step:2124/2330 train_time:84999ms step_avg:40.02ms
step:2125/2330 train_time:85035ms step_avg:40.02ms
step:2126/2330 train_time:85079ms step_avg:40.02ms
step:2127/2330 train_time:85114ms step_avg:40.02ms
step:2128/2330 train_time:85158ms step_avg:40.02ms
step:2129/2330 train_time:85194ms step_avg:40.02ms
step:2130/2330 train_time:85238ms step_avg:40.02ms
step:2131/2330 train_time:85273ms step_avg:40.02ms
step:2132/2330 train_time:85317ms step_avg:40.02ms
step:2133/2330 train_time:85352ms step_avg:40.02ms
step:2134/2330 train_time:85396ms step_avg:40.02ms
step:2135/2330 train_time:85431ms step_avg:40.01ms
step:2136/2330 train_time:85475ms step_avg:40.02ms
step:2137/2330 train_time:85510ms step_avg:40.01ms
step:2138/2330 train_time:85555ms step_avg:40.02ms
step:2139/2330 train_time:85590ms step_avg:40.01ms
step:2140/2330 train_time:85634ms step_avg:40.02ms
step:2141/2330 train_time:85669ms step_avg:40.01ms
step:2142/2330 train_time:85714ms step_avg:40.02ms
step:2143/2330 train_time:85749ms step_avg:40.01ms
step:2144/2330 train_time:85793ms step_avg:40.02ms
step:2145/2330 train_time:85828ms step_avg:40.01ms
step:2146/2330 train_time:85872ms step_avg:40.01ms
step:2147/2330 train_time:85908ms step_avg:40.01ms
step:2148/2330 train_time:85952ms step_avg:40.01ms
step:2149/2330 train_time:85987ms step_avg:40.01ms
step:2150/2330 train_time:86032ms step_avg:40.01ms
step:2151/2330 train_time:86067ms step_avg:40.01ms
step:2152/2330 train_time:86111ms step_avg:40.01ms
step:2153/2330 train_time:86146ms step_avg:40.01ms
step:2154/2330 train_time:86190ms step_avg:40.01ms
step:2155/2330 train_time:86225ms step_avg:40.01ms
step:2156/2330 train_time:86269ms step_avg:40.01ms
step:2157/2330 train_time:86305ms step_avg:40.01ms
step:2158/2330 train_time:86349ms step_avg:40.01ms
step:2159/2330 train_time:86384ms step_avg:40.01ms
step:2160/2330 train_time:86428ms step_avg:40.01ms
step:2161/2330 train_time:86463ms step_avg:40.01ms
step:2162/2330 train_time:86507ms step_avg:40.01ms
step:2163/2330 train_time:86542ms step_avg:40.01ms
step:2164/2330 train_time:86588ms step_avg:40.01ms
step:2165/2330 train_time:86623ms step_avg:40.01ms
step:2166/2330 train_time:86667ms step_avg:40.01ms
step:2167/2330 train_time:86703ms step_avg:40.01ms
step:2168/2330 train_time:86748ms step_avg:40.01ms
step:2169/2330 train_time:86782ms step_avg:40.01ms
step:2170/2330 train_time:86827ms step_avg:40.01ms
step:2171/2330 train_time:86862ms step_avg:40.01ms
step:2172/2330 train_time:86907ms step_avg:40.01ms
step:2173/2330 train_time:86942ms step_avg:40.01ms
step:2174/2330 train_time:86986ms step_avg:40.01ms
step:2175/2330 train_time:87021ms step_avg:40.01ms
step:2176/2330 train_time:87066ms step_avg:40.01ms
step:2177/2330 train_time:87101ms step_avg:40.01ms
step:2178/2330 train_time:87146ms step_avg:40.01ms
step:2179/2330 train_time:87181ms step_avg:40.01ms
step:2180/2330 train_time:87226ms step_avg:40.01ms
step:2181/2330 train_time:87261ms step_avg:40.01ms
step:2182/2330 train_time:87305ms step_avg:40.01ms
step:2183/2330 train_time:87340ms step_avg:40.01ms
step:2184/2330 train_time:87383ms step_avg:40.01ms
step:2185/2330 train_time:87418ms step_avg:40.01ms
step:2186/2330 train_time:87463ms step_avg:40.01ms
step:2187/2330 train_time:87498ms step_avg:40.01ms
step:2188/2330 train_time:87542ms step_avg:40.01ms
step:2189/2330 train_time:87576ms step_avg:40.01ms
step:2190/2330 train_time:87620ms step_avg:40.01ms
step:2191/2330 train_time:87655ms step_avg:40.01ms
step:2192/2330 train_time:87699ms step_avg:40.01ms
step:2193/2330 train_time:87735ms step_avg:40.01ms
step:2194/2330 train_time:87779ms step_avg:40.01ms
step:2195/2330 train_time:87815ms step_avg:40.01ms
step:2196/2330 train_time:87859ms step_avg:40.01ms
step:2197/2330 train_time:87895ms step_avg:40.01ms
step:2198/2330 train_time:87939ms step_avg:40.01ms
step:2199/2330 train_time:87974ms step_avg:40.01ms
step:2200/2330 train_time:88018ms step_avg:40.01ms
step:2201/2330 train_time:88054ms step_avg:40.01ms
step:2202/2330 train_time:88098ms step_avg:40.01ms
step:2203/2330 train_time:88134ms step_avg:40.01ms
step:2204/2330 train_time:88178ms step_avg:40.01ms
step:2205/2330 train_time:88214ms step_avg:40.01ms
step:2206/2330 train_time:88258ms step_avg:40.01ms
step:2207/2330 train_time:88293ms step_avg:40.01ms
step:2208/2330 train_time:88337ms step_avg:40.01ms
step:2209/2330 train_time:88373ms step_avg:40.01ms
step:2210/2330 train_time:88417ms step_avg:40.01ms
step:2211/2330 train_time:88452ms step_avg:40.01ms
step:2212/2330 train_time:88496ms step_avg:40.01ms
step:2213/2330 train_time:88531ms step_avg:40.01ms
step:2214/2330 train_time:88576ms step_avg:40.01ms
step:2215/2330 train_time:88611ms step_avg:40.00ms
step:2216/2330 train_time:88655ms step_avg:40.01ms
step:2217/2330 train_time:88691ms step_avg:40.00ms
step:2218/2330 train_time:88735ms step_avg:40.01ms
step:2219/2330 train_time:88770ms step_avg:40.00ms
step:2220/2330 train_time:88814ms step_avg:40.01ms
step:2221/2330 train_time:88849ms step_avg:40.00ms
step:2222/2330 train_time:88893ms step_avg:40.01ms
step:2223/2330 train_time:88928ms step_avg:40.00ms
step:2224/2330 train_time:88972ms step_avg:40.01ms
step:2225/2330 train_time:89008ms step_avg:40.00ms
step:2226/2330 train_time:89052ms step_avg:40.01ms
step:2227/2330 train_time:89087ms step_avg:40.00ms
step:2228/2330 train_time:89132ms step_avg:40.01ms
step:2229/2330 train_time:89168ms step_avg:40.00ms
step:2230/2330 train_time:89211ms step_avg:40.01ms
step:2231/2330 train_time:89246ms step_avg:40.00ms
step:2232/2330 train_time:89291ms step_avg:40.00ms
step:2233/2330 train_time:89326ms step_avg:40.00ms
step:2234/2330 train_time:89370ms step_avg:40.00ms
step:2235/2330 train_time:89405ms step_avg:40.00ms
step:2236/2330 train_time:89449ms step_avg:40.00ms
step:2237/2330 train_time:89485ms step_avg:40.00ms
step:2238/2330 train_time:89529ms step_avg:40.00ms
step:2239/2330 train_time:89563ms step_avg:40.00ms
step:2240/2330 train_time:89608ms step_avg:40.00ms
step:2241/2330 train_time:89643ms step_avg:40.00ms
step:2242/2330 train_time:89687ms step_avg:40.00ms
step:2243/2330 train_time:89722ms step_avg:40.00ms
step:2244/2330 train_time:89767ms step_avg:40.00ms
step:2245/2330 train_time:89802ms step_avg:40.00ms
step:2246/2330 train_time:89846ms step_avg:40.00ms
step:2247/2330 train_time:89882ms step_avg:40.00ms
step:2248/2330 train_time:89927ms step_avg:40.00ms
step:2249/2330 train_time:89962ms step_avg:40.00ms
step:2250/2330 train_time:90006ms step_avg:40.00ms
step:2250/2330 val_loss:5.0379 train_time:90094ms step_avg:40.04ms
step:2251/2330 train_time:90107ms step_avg:40.03ms
step:2252/2330 train_time:90118ms step_avg:40.02ms
step:2253/2330 train_time:90128ms step_avg:40.00ms
step:2254/2330 train_time:90167ms step_avg:40.00ms
step:2255/2330 train_time:90201ms step_avg:40.00ms
step:2256/2330 train_time:90244ms step_avg:40.00ms
step:2257/2330 train_time:90278ms step_avg:40.00ms
step:2258/2330 train_time:90322ms step_avg:40.00ms
step:2259/2330 train_time:90356ms step_avg:40.00ms
step:2260/2330 train_time:90403ms step_avg:40.00ms
step:2261/2330 train_time:90442ms step_avg:40.00ms
step:2262/2330 train_time:90490ms step_avg:40.00ms
step:2263/2330 train_time:90527ms step_avg:40.00ms
step:2264/2330 train_time:90571ms step_avg:40.01ms
step:2265/2330 train_time:90606ms step_avg:40.00ms
step:2266/2330 train_time:90650ms step_avg:40.00ms
step:2267/2330 train_time:90686ms step_avg:40.00ms
step:2268/2330 train_time:90729ms step_avg:40.00ms
step:2269/2330 train_time:90763ms step_avg:40.00ms
step:2270/2330 train_time:90808ms step_avg:40.00ms
step:2271/2330 train_time:90842ms step_avg:40.00ms
step:2272/2330 train_time:90886ms step_avg:40.00ms
step:2273/2330 train_time:90921ms step_avg:40.00ms
step:2274/2330 train_time:90964ms step_avg:40.00ms
step:2275/2330 train_time:90999ms step_avg:40.00ms
step:2276/2330 train_time:91044ms step_avg:40.00ms
step:2277/2330 train_time:91079ms step_avg:40.00ms
step:2278/2330 train_time:91123ms step_avg:40.00ms
step:2279/2330 train_time:91158ms step_avg:40.00ms
step:2280/2330 train_time:91201ms step_avg:40.00ms
step:2281/2330 train_time:91236ms step_avg:40.00ms
step:2282/2330 train_time:91280ms step_avg:40.00ms
step:2283/2330 train_time:91316ms step_avg:40.00ms
step:2284/2330 train_time:91360ms step_avg:40.00ms
step:2285/2330 train_time:91398ms step_avg:40.00ms
step:2286/2330 train_time:91444ms step_avg:40.00ms
step:2287/2330 train_time:91480ms step_avg:40.00ms
step:2288/2330 train_time:91527ms step_avg:40.00ms
step:2289/2330 train_time:91562ms step_avg:40.00ms
step:2290/2330 train_time:91607ms step_avg:40.00ms
step:2291/2330 train_time:91641ms step_avg:40.00ms
step:2292/2330 train_time:91686ms step_avg:40.00ms
step:2293/2330 train_time:91721ms step_avg:40.00ms
step:2294/2330 train_time:91765ms step_avg:40.00ms
step:2295/2330 train_time:91799ms step_avg:40.00ms
step:2296/2330 train_time:91843ms step_avg:40.00ms
step:2297/2330 train_time:91878ms step_avg:40.00ms
step:2298/2330 train_time:91922ms step_avg:40.00ms
step:2299/2330 train_time:91957ms step_avg:40.00ms
step:2300/2330 train_time:92001ms step_avg:40.00ms
step:2301/2330 train_time:92036ms step_avg:40.00ms
step:2302/2330 train_time:92079ms step_avg:40.00ms
step:2303/2330 train_time:92114ms step_avg:40.00ms
step:2304/2330 train_time:92158ms step_avg:40.00ms
step:2305/2330 train_time:92193ms step_avg:40.00ms
step:2306/2330 train_time:92236ms step_avg:40.00ms
step:2307/2330 train_time:92272ms step_avg:40.00ms
step:2308/2330 train_time:92317ms step_avg:40.00ms
step:2309/2330 train_time:92352ms step_avg:40.00ms
step:2310/2330 train_time:92397ms step_avg:40.00ms
step:2311/2330 train_time:92432ms step_avg:40.00ms
step:2312/2330 train_time:92478ms step_avg:40.00ms
step:2313/2330 train_time:92513ms step_avg:40.00ms
step:2314/2330 train_time:92558ms step_avg:40.00ms
step:2315/2330 train_time:92593ms step_avg:40.00ms
step:2316/2330 train_time:92638ms step_avg:40.00ms
step:2317/2330 train_time:92673ms step_avg:40.00ms
step:2318/2330 train_time:92717ms step_avg:40.00ms
step:2319/2330 train_time:92752ms step_avg:40.00ms
step:2320/2330 train_time:92796ms step_avg:40.00ms
step:2321/2330 train_time:92831ms step_avg:40.00ms
step:2322/2330 train_time:92874ms step_avg:40.00ms
step:2323/2330 train_time:92909ms step_avg:40.00ms
step:2324/2330 train_time:92953ms step_avg:40.00ms
step:2325/2330 train_time:92988ms step_avg:39.99ms
step:2326/2330 train_time:93032ms step_avg:40.00ms
step:2327/2330 train_time:93067ms step_avg:39.99ms
step:2328/2330 train_time:93111ms step_avg:40.00ms
step:2329/2330 train_time:93146ms step_avg:39.99ms
step:2330/2330 train_time:93190ms step_avg:40.00ms
step:2330/2330 val_loss:5.0314 train_time:93278ms step_avg:40.03ms
peak memory allocated: 29226 MiB reserved: 38888 MiB
