import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "muon_lr3e-2"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=3e-2, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Oct 28 23:22:51 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   30C    P0             111W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   29C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   28C    P0             108W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:84ms step_avg:83.52ms
step:2/2330 train_time:181ms step_avg:90.74ms
step:3/2330 train_time:203ms step_avg:67.75ms
step:4/2330 train_time:238ms step_avg:59.60ms
step:5/2330 train_time:296ms step_avg:59.14ms
step:6/2330 train_time:356ms step_avg:59.32ms
step:7/2330 train_time:414ms step_avg:59.21ms
step:8/2330 train_time:475ms step_avg:59.39ms
step:9/2330 train_time:534ms step_avg:59.30ms
step:10/2330 train_time:595ms step_avg:59.54ms
step:11/2330 train_time:654ms step_avg:59.46ms
step:12/2330 train_time:715ms step_avg:59.58ms
step:13/2330 train_time:773ms step_avg:59.48ms
step:14/2330 train_time:834ms step_avg:59.57ms
step:15/2330 train_time:893ms step_avg:59.51ms
step:16/2330 train_time:954ms step_avg:59.61ms
step:17/2330 train_time:1014ms step_avg:59.67ms
step:18/2330 train_time:1079ms step_avg:59.93ms
step:19/2330 train_time:1142ms step_avg:60.13ms
step:20/2330 train_time:1205ms step_avg:60.26ms
step:21/2330 train_time:1265ms step_avg:60.23ms
step:22/2330 train_time:1327ms step_avg:60.31ms
step:23/2330 train_time:1386ms step_avg:60.27ms
step:24/2330 train_time:1448ms step_avg:60.34ms
step:25/2330 train_time:1507ms step_avg:60.30ms
step:26/2330 train_time:1569ms step_avg:60.33ms
step:27/2330 train_time:1629ms step_avg:60.32ms
step:28/2330 train_time:1691ms step_avg:60.38ms
step:29/2330 train_time:1750ms step_avg:60.36ms
step:30/2330 train_time:1812ms step_avg:60.39ms
step:31/2330 train_time:1870ms step_avg:60.34ms
step:32/2330 train_time:1932ms step_avg:60.36ms
step:33/2330 train_time:1991ms step_avg:60.34ms
step:34/2330 train_time:2054ms step_avg:60.42ms
step:35/2330 train_time:2115ms step_avg:60.43ms
step:36/2330 train_time:2178ms step_avg:60.50ms
step:37/2330 train_time:2238ms step_avg:60.49ms
step:38/2330 train_time:2299ms step_avg:60.51ms
step:39/2330 train_time:2358ms step_avg:60.47ms
step:40/2330 train_time:2420ms step_avg:60.50ms
step:41/2330 train_time:2480ms step_avg:60.48ms
step:42/2330 train_time:2541ms step_avg:60.51ms
step:43/2330 train_time:2601ms step_avg:60.49ms
step:44/2330 train_time:2663ms step_avg:60.51ms
step:45/2330 train_time:2722ms step_avg:60.49ms
step:46/2330 train_time:2784ms step_avg:60.52ms
step:47/2330 train_time:2843ms step_avg:60.49ms
step:48/2330 train_time:2904ms step_avg:60.50ms
step:49/2330 train_time:2964ms step_avg:60.48ms
step:50/2330 train_time:3025ms step_avg:60.51ms
step:51/2330 train_time:3087ms step_avg:60.53ms
step:52/2330 train_time:3149ms step_avg:60.56ms
step:53/2330 train_time:3209ms step_avg:60.55ms
step:54/2330 train_time:3271ms step_avg:60.57ms
step:55/2330 train_time:3331ms step_avg:60.56ms
step:56/2330 train_time:3393ms step_avg:60.58ms
step:57/2330 train_time:3452ms step_avg:60.57ms
step:58/2330 train_time:3514ms step_avg:60.59ms
step:59/2330 train_time:3574ms step_avg:60.57ms
step:60/2330 train_time:3636ms step_avg:60.61ms
step:61/2330 train_time:3696ms step_avg:60.59ms
step:62/2330 train_time:3757ms step_avg:60.59ms
step:63/2330 train_time:3817ms step_avg:60.58ms
step:64/2330 train_time:3879ms step_avg:60.60ms
step:65/2330 train_time:3938ms step_avg:60.58ms
step:66/2330 train_time:3999ms step_avg:60.60ms
step:67/2330 train_time:4059ms step_avg:60.58ms
step:68/2330 train_time:4121ms step_avg:60.60ms
step:69/2330 train_time:4181ms step_avg:60.59ms
step:70/2330 train_time:4242ms step_avg:60.61ms
step:71/2330 train_time:4302ms step_avg:60.59ms
step:72/2330 train_time:4363ms step_avg:60.60ms
step:73/2330 train_time:4422ms step_avg:60.58ms
step:74/2330 train_time:4484ms step_avg:60.60ms
step:75/2330 train_time:4544ms step_avg:60.58ms
step:76/2330 train_time:4605ms step_avg:60.60ms
step:77/2330 train_time:4665ms step_avg:60.58ms
step:78/2330 train_time:4727ms step_avg:60.61ms
step:79/2330 train_time:4787ms step_avg:60.60ms
step:80/2330 train_time:4849ms step_avg:60.61ms
step:81/2330 train_time:4908ms step_avg:60.60ms
step:82/2330 train_time:4971ms step_avg:60.62ms
step:83/2330 train_time:5031ms step_avg:60.62ms
step:84/2330 train_time:5093ms step_avg:60.63ms
step:85/2330 train_time:5152ms step_avg:60.61ms
step:86/2330 train_time:5216ms step_avg:60.65ms
step:87/2330 train_time:5275ms step_avg:60.63ms
step:88/2330 train_time:5336ms step_avg:60.64ms
step:89/2330 train_time:5396ms step_avg:60.63ms
step:90/2330 train_time:5457ms step_avg:60.64ms
step:91/2330 train_time:5517ms step_avg:60.63ms
step:92/2330 train_time:5579ms step_avg:60.64ms
step:93/2330 train_time:5639ms step_avg:60.63ms
step:94/2330 train_time:5700ms step_avg:60.64ms
step:95/2330 train_time:5759ms step_avg:60.62ms
step:96/2330 train_time:5820ms step_avg:60.63ms
step:97/2330 train_time:5880ms step_avg:60.62ms
step:98/2330 train_time:5942ms step_avg:60.63ms
step:99/2330 train_time:6002ms step_avg:60.62ms
step:100/2330 train_time:6063ms step_avg:60.63ms
step:101/2330 train_time:6122ms step_avg:60.61ms
step:102/2330 train_time:6184ms step_avg:60.63ms
step:103/2330 train_time:6243ms step_avg:60.62ms
step:104/2330 train_time:6305ms step_avg:60.63ms
step:105/2330 train_time:6365ms step_avg:60.62ms
step:106/2330 train_time:6427ms step_avg:60.63ms
step:107/2330 train_time:6486ms step_avg:60.62ms
step:108/2330 train_time:6548ms step_avg:60.63ms
step:109/2330 train_time:6607ms step_avg:60.62ms
step:110/2330 train_time:6668ms step_avg:60.62ms
step:111/2330 train_time:6728ms step_avg:60.61ms
step:112/2330 train_time:6790ms step_avg:60.62ms
step:113/2330 train_time:6850ms step_avg:60.62ms
step:114/2330 train_time:6912ms step_avg:60.63ms
step:115/2330 train_time:6972ms step_avg:60.63ms
step:116/2330 train_time:7034ms step_avg:60.64ms
step:117/2330 train_time:7094ms step_avg:60.63ms
step:118/2330 train_time:7156ms step_avg:60.64ms
step:119/2330 train_time:7216ms step_avg:60.64ms
step:120/2330 train_time:7277ms step_avg:60.65ms
step:121/2330 train_time:7337ms step_avg:60.64ms
step:122/2330 train_time:7398ms step_avg:60.64ms
step:123/2330 train_time:7457ms step_avg:60.63ms
step:124/2330 train_time:7519ms step_avg:60.64ms
step:125/2330 train_time:7578ms step_avg:60.63ms
step:126/2330 train_time:7640ms step_avg:60.64ms
step:127/2330 train_time:7699ms step_avg:60.62ms
step:128/2330 train_time:7761ms step_avg:60.63ms
step:129/2330 train_time:7820ms step_avg:60.62ms
step:130/2330 train_time:7882ms step_avg:60.63ms
step:131/2330 train_time:7942ms step_avg:60.62ms
step:132/2330 train_time:8003ms step_avg:60.63ms
step:133/2330 train_time:8062ms step_avg:60.61ms
step:134/2330 train_time:8123ms step_avg:60.62ms
step:135/2330 train_time:8182ms step_avg:60.61ms
step:136/2330 train_time:8244ms step_avg:60.62ms
step:137/2330 train_time:8303ms step_avg:60.61ms
step:138/2330 train_time:8365ms step_avg:60.61ms
step:139/2330 train_time:8425ms step_avg:60.61ms
step:140/2330 train_time:8487ms step_avg:60.62ms
step:141/2330 train_time:8547ms step_avg:60.61ms
step:142/2330 train_time:8608ms step_avg:60.62ms
step:143/2330 train_time:8669ms step_avg:60.62ms
step:144/2330 train_time:8731ms step_avg:60.63ms
step:145/2330 train_time:8790ms step_avg:60.62ms
step:146/2330 train_time:8852ms step_avg:60.63ms
step:147/2330 train_time:8911ms step_avg:60.62ms
step:148/2330 train_time:8974ms step_avg:60.63ms
step:149/2330 train_time:9033ms step_avg:60.63ms
step:150/2330 train_time:9095ms step_avg:60.63ms
step:151/2330 train_time:9156ms step_avg:60.63ms
step:152/2330 train_time:9217ms step_avg:60.64ms
step:153/2330 train_time:9276ms step_avg:60.63ms
step:154/2330 train_time:9338ms step_avg:60.63ms
step:155/2330 train_time:9397ms step_avg:60.63ms
step:156/2330 train_time:9459ms step_avg:60.63ms
step:157/2330 train_time:9518ms step_avg:60.62ms
step:158/2330 train_time:9579ms step_avg:60.63ms
step:159/2330 train_time:9639ms step_avg:60.62ms
step:160/2330 train_time:9700ms step_avg:60.63ms
step:161/2330 train_time:9760ms step_avg:60.62ms
step:162/2330 train_time:9821ms step_avg:60.63ms
step:163/2330 train_time:9881ms step_avg:60.62ms
step:164/2330 train_time:9943ms step_avg:60.63ms
step:165/2330 train_time:10001ms step_avg:60.61ms
step:166/2330 train_time:10062ms step_avg:60.62ms
step:167/2330 train_time:10121ms step_avg:60.61ms
step:168/2330 train_time:10183ms step_avg:60.61ms
step:169/2330 train_time:10242ms step_avg:60.60ms
step:170/2330 train_time:10303ms step_avg:60.61ms
step:171/2330 train_time:10363ms step_avg:60.60ms
step:172/2330 train_time:10425ms step_avg:60.61ms
step:173/2330 train_time:10485ms step_avg:60.61ms
step:174/2330 train_time:10547ms step_avg:60.61ms
step:175/2330 train_time:10606ms step_avg:60.61ms
step:176/2330 train_time:10668ms step_avg:60.61ms
step:177/2330 train_time:10728ms step_avg:60.61ms
step:178/2330 train_time:10790ms step_avg:60.62ms
step:179/2330 train_time:10849ms step_avg:60.61ms
step:180/2330 train_time:10911ms step_avg:60.61ms
step:181/2330 train_time:10970ms step_avg:60.61ms
step:182/2330 train_time:11032ms step_avg:60.61ms
step:183/2330 train_time:11092ms step_avg:60.61ms
step:184/2330 train_time:11154ms step_avg:60.62ms
step:185/2330 train_time:11214ms step_avg:60.62ms
step:186/2330 train_time:11276ms step_avg:60.62ms
step:187/2330 train_time:11335ms step_avg:60.62ms
step:188/2330 train_time:11396ms step_avg:60.62ms
step:189/2330 train_time:11456ms step_avg:60.61ms
step:190/2330 train_time:11517ms step_avg:60.62ms
step:191/2330 train_time:11577ms step_avg:60.61ms
step:192/2330 train_time:11638ms step_avg:60.62ms
step:193/2330 train_time:11697ms step_avg:60.61ms
step:194/2330 train_time:11760ms step_avg:60.62ms
step:195/2330 train_time:11819ms step_avg:60.61ms
step:196/2330 train_time:11880ms step_avg:60.61ms
step:197/2330 train_time:11941ms step_avg:60.61ms
step:198/2330 train_time:12002ms step_avg:60.61ms
step:199/2330 train_time:12061ms step_avg:60.61ms
step:200/2330 train_time:12122ms step_avg:60.61ms
step:201/2330 train_time:12181ms step_avg:60.60ms
step:202/2330 train_time:12242ms step_avg:60.60ms
step:203/2330 train_time:12301ms step_avg:60.60ms
step:204/2330 train_time:12361ms step_avg:60.60ms
step:205/2330 train_time:12420ms step_avg:60.59ms
step:206/2330 train_time:12481ms step_avg:60.59ms
step:207/2330 train_time:12541ms step_avg:60.58ms
step:208/2330 train_time:12602ms step_avg:60.59ms
step:209/2330 train_time:12661ms step_avg:60.58ms
step:210/2330 train_time:12722ms step_avg:60.58ms
step:211/2330 train_time:12782ms step_avg:60.58ms
step:212/2330 train_time:12843ms step_avg:60.58ms
step:213/2330 train_time:12903ms step_avg:60.58ms
step:214/2330 train_time:12964ms step_avg:60.58ms
step:215/2330 train_time:13023ms step_avg:60.57ms
step:216/2330 train_time:13084ms step_avg:60.57ms
step:217/2330 train_time:13143ms step_avg:60.57ms
step:218/2330 train_time:13204ms step_avg:60.57ms
step:219/2330 train_time:13263ms step_avg:60.56ms
step:220/2330 train_time:13324ms step_avg:60.56ms
step:221/2330 train_time:13384ms step_avg:60.56ms
step:222/2330 train_time:13446ms step_avg:60.57ms
step:223/2330 train_time:13506ms step_avg:60.56ms
step:224/2330 train_time:13567ms step_avg:60.57ms
step:225/2330 train_time:13626ms step_avg:60.56ms
step:226/2330 train_time:13688ms step_avg:60.57ms
step:227/2330 train_time:13748ms step_avg:60.56ms
step:228/2330 train_time:13809ms step_avg:60.57ms
step:229/2330 train_time:13869ms step_avg:60.56ms
step:230/2330 train_time:13930ms step_avg:60.57ms
step:231/2330 train_time:13990ms step_avg:60.56ms
step:232/2330 train_time:14052ms step_avg:60.57ms
step:233/2330 train_time:14112ms step_avg:60.57ms
step:234/2330 train_time:14173ms step_avg:60.57ms
step:235/2330 train_time:14233ms step_avg:60.57ms
step:236/2330 train_time:14295ms step_avg:60.57ms
step:237/2330 train_time:14355ms step_avg:60.57ms
step:238/2330 train_time:14416ms step_avg:60.57ms
step:239/2330 train_time:14476ms step_avg:60.57ms
step:240/2330 train_time:14537ms step_avg:60.57ms
step:241/2330 train_time:14596ms step_avg:60.57ms
step:242/2330 train_time:14658ms step_avg:60.57ms
step:243/2330 train_time:14718ms step_avg:60.57ms
step:244/2330 train_time:14779ms step_avg:60.57ms
step:245/2330 train_time:14839ms step_avg:60.57ms
step:246/2330 train_time:14901ms step_avg:60.57ms
step:247/2330 train_time:14961ms step_avg:60.57ms
step:248/2330 train_time:15022ms step_avg:60.57ms
step:249/2330 train_time:15081ms step_avg:60.57ms
step:250/2330 train_time:15143ms step_avg:60.57ms
step:250/2330 val_loss:4.0913 train_time:15206ms step_avg:60.82ms
step:251/2330 train_time:15229ms step_avg:60.67ms
step:252/2330 train_time:15266ms step_avg:60.58ms
step:253/2330 train_time:15330ms step_avg:60.59ms
step:254/2330 train_time:15396ms step_avg:60.61ms
step:255/2330 train_time:15455ms step_avg:60.61ms
step:256/2330 train_time:15517ms step_avg:60.61ms
step:257/2330 train_time:15576ms step_avg:60.61ms
step:258/2330 train_time:15637ms step_avg:60.61ms
step:259/2330 train_time:15696ms step_avg:60.60ms
step:260/2330 train_time:15756ms step_avg:60.60ms
step:261/2330 train_time:15815ms step_avg:60.59ms
step:262/2330 train_time:15876ms step_avg:60.59ms
step:263/2330 train_time:15934ms step_avg:60.59ms
step:264/2330 train_time:15994ms step_avg:60.59ms
step:265/2330 train_time:16052ms step_avg:60.58ms
step:266/2330 train_time:16114ms step_avg:60.58ms
step:267/2330 train_time:16174ms step_avg:60.58ms
step:268/2330 train_time:16237ms step_avg:60.58ms
step:269/2330 train_time:16297ms step_avg:60.58ms
step:270/2330 train_time:16360ms step_avg:60.59ms
step:271/2330 train_time:16420ms step_avg:60.59ms
step:272/2330 train_time:16482ms step_avg:60.60ms
step:273/2330 train_time:16541ms step_avg:60.59ms
step:274/2330 train_time:16603ms step_avg:60.60ms
step:275/2330 train_time:16662ms step_avg:60.59ms
step:276/2330 train_time:16724ms step_avg:60.59ms
step:277/2330 train_time:16784ms step_avg:60.59ms
step:278/2330 train_time:16844ms step_avg:60.59ms
step:279/2330 train_time:16903ms step_avg:60.58ms
step:280/2330 train_time:16964ms step_avg:60.59ms
step:281/2330 train_time:17024ms step_avg:60.58ms
step:282/2330 train_time:17085ms step_avg:60.59ms
step:283/2330 train_time:17144ms step_avg:60.58ms
step:284/2330 train_time:17206ms step_avg:60.58ms
step:285/2330 train_time:17265ms step_avg:60.58ms
step:286/2330 train_time:17328ms step_avg:60.59ms
step:287/2330 train_time:17388ms step_avg:60.59ms
step:288/2330 train_time:17450ms step_avg:60.59ms
step:289/2330 train_time:17510ms step_avg:60.59ms
step:290/2330 train_time:17571ms step_avg:60.59ms
step:291/2330 train_time:17630ms step_avg:60.58ms
step:292/2330 train_time:17691ms step_avg:60.59ms
step:293/2330 train_time:17750ms step_avg:60.58ms
step:294/2330 train_time:17812ms step_avg:60.59ms
step:295/2330 train_time:17871ms step_avg:60.58ms
step:296/2330 train_time:17933ms step_avg:60.58ms
step:297/2330 train_time:17992ms step_avg:60.58ms
step:298/2330 train_time:18053ms step_avg:60.58ms
step:299/2330 train_time:18112ms step_avg:60.58ms
step:300/2330 train_time:18174ms step_avg:60.58ms
step:301/2330 train_time:18233ms step_avg:60.57ms
step:302/2330 train_time:18294ms step_avg:60.58ms
step:303/2330 train_time:18353ms step_avg:60.57ms
step:304/2330 train_time:18415ms step_avg:60.58ms
step:305/2330 train_time:18475ms step_avg:60.57ms
step:306/2330 train_time:18536ms step_avg:60.57ms
step:307/2330 train_time:18595ms step_avg:60.57ms
step:308/2330 train_time:18655ms step_avg:60.57ms
step:309/2330 train_time:18714ms step_avg:60.56ms
step:310/2330 train_time:18775ms step_avg:60.57ms
step:311/2330 train_time:18834ms step_avg:60.56ms
step:312/2330 train_time:18895ms step_avg:60.56ms
step:313/2330 train_time:18954ms step_avg:60.56ms
step:314/2330 train_time:19016ms step_avg:60.56ms
step:315/2330 train_time:19075ms step_avg:60.56ms
step:316/2330 train_time:19136ms step_avg:60.56ms
step:317/2330 train_time:19195ms step_avg:60.55ms
step:318/2330 train_time:19257ms step_avg:60.56ms
step:319/2330 train_time:19316ms step_avg:60.55ms
step:320/2330 train_time:19377ms step_avg:60.55ms
step:321/2330 train_time:19436ms step_avg:60.55ms
step:322/2330 train_time:19497ms step_avg:60.55ms
step:323/2330 train_time:19556ms step_avg:60.55ms
step:324/2330 train_time:19618ms step_avg:60.55ms
step:325/2330 train_time:19677ms step_avg:60.54ms
step:326/2330 train_time:19738ms step_avg:60.55ms
step:327/2330 train_time:19798ms step_avg:60.54ms
step:328/2330 train_time:19859ms step_avg:60.55ms
step:329/2330 train_time:19919ms step_avg:60.54ms
step:330/2330 train_time:19981ms step_avg:60.55ms
step:331/2330 train_time:20040ms step_avg:60.54ms
step:332/2330 train_time:20101ms step_avg:60.55ms
step:333/2330 train_time:20160ms step_avg:60.54ms
step:334/2330 train_time:20222ms step_avg:60.54ms
step:335/2330 train_time:20281ms step_avg:60.54ms
step:336/2330 train_time:20343ms step_avg:60.54ms
step:337/2330 train_time:20402ms step_avg:60.54ms
step:338/2330 train_time:20464ms step_avg:60.54ms
step:339/2330 train_time:20523ms step_avg:60.54ms
step:340/2330 train_time:20585ms step_avg:60.54ms
step:341/2330 train_time:20645ms step_avg:60.54ms
step:342/2330 train_time:20707ms step_avg:60.55ms
step:343/2330 train_time:20766ms step_avg:60.54ms
step:344/2330 train_time:20827ms step_avg:60.54ms
step:345/2330 train_time:20887ms step_avg:60.54ms
step:346/2330 train_time:20949ms step_avg:60.55ms
step:347/2330 train_time:21008ms step_avg:60.54ms
step:348/2330 train_time:21070ms step_avg:60.54ms
step:349/2330 train_time:21129ms step_avg:60.54ms
step:350/2330 train_time:21190ms step_avg:60.54ms
step:351/2330 train_time:21249ms step_avg:60.54ms
step:352/2330 train_time:21311ms step_avg:60.54ms
step:353/2330 train_time:21370ms step_avg:60.54ms
step:354/2330 train_time:21431ms step_avg:60.54ms
step:355/2330 train_time:21491ms step_avg:60.54ms
step:356/2330 train_time:21553ms step_avg:60.54ms
step:357/2330 train_time:21612ms step_avg:60.54ms
step:358/2330 train_time:21674ms step_avg:60.54ms
step:359/2330 train_time:21733ms step_avg:60.54ms
step:360/2330 train_time:21795ms step_avg:60.54ms
step:361/2330 train_time:21854ms step_avg:60.54ms
step:362/2330 train_time:21915ms step_avg:60.54ms
step:363/2330 train_time:21974ms step_avg:60.53ms
step:364/2330 train_time:22035ms step_avg:60.53ms
step:365/2330 train_time:22094ms step_avg:60.53ms
step:366/2330 train_time:22155ms step_avg:60.53ms
step:367/2330 train_time:22214ms step_avg:60.53ms
step:368/2330 train_time:22276ms step_avg:60.53ms
step:369/2330 train_time:22335ms step_avg:60.53ms
step:370/2330 train_time:22395ms step_avg:60.53ms
step:371/2330 train_time:22455ms step_avg:60.53ms
step:372/2330 train_time:22517ms step_avg:60.53ms
step:373/2330 train_time:22577ms step_avg:60.53ms
step:374/2330 train_time:22638ms step_avg:60.53ms
step:375/2330 train_time:22697ms step_avg:60.53ms
step:376/2330 train_time:22758ms step_avg:60.53ms
step:377/2330 train_time:22818ms step_avg:60.52ms
step:378/2330 train_time:22879ms step_avg:60.53ms
step:379/2330 train_time:22939ms step_avg:60.53ms
step:380/2330 train_time:23001ms step_avg:60.53ms
step:381/2330 train_time:23061ms step_avg:60.53ms
step:382/2330 train_time:23122ms step_avg:60.53ms
step:383/2330 train_time:23182ms step_avg:60.53ms
step:384/2330 train_time:23244ms step_avg:60.53ms
step:385/2330 train_time:23303ms step_avg:60.53ms
step:386/2330 train_time:23365ms step_avg:60.53ms
step:387/2330 train_time:23424ms step_avg:60.53ms
step:388/2330 train_time:23486ms step_avg:60.53ms
step:389/2330 train_time:23546ms step_avg:60.53ms
step:390/2330 train_time:23608ms step_avg:60.53ms
step:391/2330 train_time:23668ms step_avg:60.53ms
step:392/2330 train_time:23730ms step_avg:60.54ms
step:393/2330 train_time:23790ms step_avg:60.53ms
step:394/2330 train_time:23851ms step_avg:60.54ms
step:395/2330 train_time:23911ms step_avg:60.53ms
step:396/2330 train_time:23972ms step_avg:60.54ms
step:397/2330 train_time:24031ms step_avg:60.53ms
step:398/2330 train_time:24092ms step_avg:60.53ms
step:399/2330 train_time:24152ms step_avg:60.53ms
step:400/2330 train_time:24214ms step_avg:60.53ms
step:401/2330 train_time:24272ms step_avg:60.53ms
step:402/2330 train_time:24334ms step_avg:60.53ms
step:403/2330 train_time:24393ms step_avg:60.53ms
step:404/2330 train_time:24455ms step_avg:60.53ms
step:405/2330 train_time:24514ms step_avg:60.53ms
step:406/2330 train_time:24575ms step_avg:60.53ms
step:407/2330 train_time:24634ms step_avg:60.53ms
step:408/2330 train_time:24695ms step_avg:60.53ms
step:409/2330 train_time:24754ms step_avg:60.52ms
step:410/2330 train_time:24815ms step_avg:60.53ms
step:411/2330 train_time:24875ms step_avg:60.52ms
step:412/2330 train_time:24936ms step_avg:60.52ms
step:413/2330 train_time:24995ms step_avg:60.52ms
step:414/2330 train_time:25056ms step_avg:60.52ms
step:415/2330 train_time:25115ms step_avg:60.52ms
step:416/2330 train_time:25176ms step_avg:60.52ms
step:417/2330 train_time:25235ms step_avg:60.51ms
step:418/2330 train_time:25296ms step_avg:60.52ms
step:419/2330 train_time:25355ms step_avg:60.51ms
step:420/2330 train_time:25417ms step_avg:60.52ms
step:421/2330 train_time:25476ms step_avg:60.51ms
step:422/2330 train_time:25537ms step_avg:60.52ms
step:423/2330 train_time:25597ms step_avg:60.51ms
step:424/2330 train_time:25659ms step_avg:60.52ms
step:425/2330 train_time:25718ms step_avg:60.51ms
step:426/2330 train_time:25780ms step_avg:60.52ms
step:427/2330 train_time:25839ms step_avg:60.51ms
step:428/2330 train_time:25901ms step_avg:60.52ms
step:429/2330 train_time:25960ms step_avg:60.51ms
step:430/2330 train_time:26021ms step_avg:60.51ms
step:431/2330 train_time:26081ms step_avg:60.51ms
step:432/2330 train_time:26142ms step_avg:60.51ms
step:433/2330 train_time:26202ms step_avg:60.51ms
step:434/2330 train_time:26263ms step_avg:60.51ms
step:435/2330 train_time:26323ms step_avg:60.51ms
step:436/2330 train_time:26385ms step_avg:60.52ms
step:437/2330 train_time:26444ms step_avg:60.51ms
step:438/2330 train_time:26506ms step_avg:60.52ms
step:439/2330 train_time:26565ms step_avg:60.51ms
step:440/2330 train_time:26627ms step_avg:60.52ms
step:441/2330 train_time:26687ms step_avg:60.51ms
step:442/2330 train_time:26749ms step_avg:60.52ms
step:443/2330 train_time:26808ms step_avg:60.52ms
step:444/2330 train_time:26869ms step_avg:60.52ms
step:445/2330 train_time:26928ms step_avg:60.51ms
step:446/2330 train_time:26990ms step_avg:60.52ms
step:447/2330 train_time:27050ms step_avg:60.51ms
step:448/2330 train_time:27111ms step_avg:60.52ms
step:449/2330 train_time:27170ms step_avg:60.51ms
step:450/2330 train_time:27232ms step_avg:60.51ms
step:451/2330 train_time:27291ms step_avg:60.51ms
step:452/2330 train_time:27353ms step_avg:60.51ms
step:453/2330 train_time:27412ms step_avg:60.51ms
step:454/2330 train_time:27474ms step_avg:60.51ms
step:455/2330 train_time:27534ms step_avg:60.51ms
step:456/2330 train_time:27595ms step_avg:60.52ms
step:457/2330 train_time:27654ms step_avg:60.51ms
step:458/2330 train_time:27715ms step_avg:60.51ms
step:459/2330 train_time:27774ms step_avg:60.51ms
step:460/2330 train_time:27836ms step_avg:60.51ms
step:461/2330 train_time:27895ms step_avg:60.51ms
step:462/2330 train_time:27957ms step_avg:60.51ms
step:463/2330 train_time:28017ms step_avg:60.51ms
step:464/2330 train_time:28078ms step_avg:60.51ms
step:465/2330 train_time:28137ms step_avg:60.51ms
step:466/2330 train_time:28199ms step_avg:60.51ms
step:467/2330 train_time:28258ms step_avg:60.51ms
step:468/2330 train_time:28320ms step_avg:60.51ms
step:469/2330 train_time:28380ms step_avg:60.51ms
step:470/2330 train_time:28442ms step_avg:60.51ms
step:471/2330 train_time:28502ms step_avg:60.51ms
step:472/2330 train_time:28563ms step_avg:60.52ms
step:473/2330 train_time:28622ms step_avg:60.51ms
step:474/2330 train_time:28684ms step_avg:60.51ms
step:475/2330 train_time:28744ms step_avg:60.51ms
step:476/2330 train_time:28805ms step_avg:60.52ms
step:477/2330 train_time:28865ms step_avg:60.51ms
step:478/2330 train_time:28927ms step_avg:60.52ms
step:479/2330 train_time:28986ms step_avg:60.51ms
step:480/2330 train_time:29048ms step_avg:60.52ms
step:481/2330 train_time:29108ms step_avg:60.52ms
step:482/2330 train_time:29169ms step_avg:60.52ms
step:483/2330 train_time:29229ms step_avg:60.51ms
step:484/2330 train_time:29290ms step_avg:60.52ms
step:485/2330 train_time:29349ms step_avg:60.51ms
step:486/2330 train_time:29411ms step_avg:60.52ms
step:487/2330 train_time:29470ms step_avg:60.51ms
step:488/2330 train_time:29532ms step_avg:60.52ms
step:489/2330 train_time:29591ms step_avg:60.51ms
step:490/2330 train_time:29653ms step_avg:60.52ms
step:491/2330 train_time:29712ms step_avg:60.51ms
step:492/2330 train_time:29774ms step_avg:60.52ms
step:493/2330 train_time:29834ms step_avg:60.51ms
step:494/2330 train_time:29895ms step_avg:60.52ms
step:495/2330 train_time:29954ms step_avg:60.51ms
step:496/2330 train_time:30016ms step_avg:60.52ms
step:497/2330 train_time:30074ms step_avg:60.51ms
step:498/2330 train_time:30135ms step_avg:60.51ms
step:499/2330 train_time:30194ms step_avg:60.51ms
step:500/2330 train_time:30255ms step_avg:60.51ms
step:500/2330 val_loss:3.8371 train_time:30319ms step_avg:60.64ms
step:501/2330 train_time:30341ms step_avg:60.56ms
step:502/2330 train_time:30379ms step_avg:60.52ms
step:503/2330 train_time:30443ms step_avg:60.52ms
step:504/2330 train_time:30506ms step_avg:60.53ms
step:505/2330 train_time:30565ms step_avg:60.52ms
step:506/2330 train_time:30627ms step_avg:60.53ms
step:507/2330 train_time:30685ms step_avg:60.52ms
step:508/2330 train_time:30747ms step_avg:60.52ms
step:509/2330 train_time:30805ms step_avg:60.52ms
step:510/2330 train_time:30866ms step_avg:60.52ms
step:511/2330 train_time:30925ms step_avg:60.52ms
step:512/2330 train_time:30986ms step_avg:60.52ms
step:513/2330 train_time:31045ms step_avg:60.52ms
step:514/2330 train_time:31105ms step_avg:60.52ms
step:515/2330 train_time:31164ms step_avg:60.51ms
step:516/2330 train_time:31226ms step_avg:60.51ms
step:517/2330 train_time:31286ms step_avg:60.51ms
step:518/2330 train_time:31350ms step_avg:60.52ms
step:519/2330 train_time:31411ms step_avg:60.52ms
step:520/2330 train_time:31473ms step_avg:60.52ms
step:521/2330 train_time:31533ms step_avg:60.52ms
step:522/2330 train_time:31594ms step_avg:60.53ms
step:523/2330 train_time:31654ms step_avg:60.52ms
step:524/2330 train_time:31715ms step_avg:60.52ms
step:525/2330 train_time:31774ms step_avg:60.52ms
step:526/2330 train_time:31834ms step_avg:60.52ms
step:527/2330 train_time:31893ms step_avg:60.52ms
step:528/2330 train_time:31955ms step_avg:60.52ms
step:529/2330 train_time:32014ms step_avg:60.52ms
step:530/2330 train_time:32074ms step_avg:60.52ms
step:531/2330 train_time:32133ms step_avg:60.51ms
step:532/2330 train_time:32194ms step_avg:60.51ms
step:533/2330 train_time:32253ms step_avg:60.51ms
step:534/2330 train_time:32315ms step_avg:60.51ms
step:535/2330 train_time:32375ms step_avg:60.51ms
step:536/2330 train_time:32437ms step_avg:60.52ms
step:537/2330 train_time:32497ms step_avg:60.52ms
step:538/2330 train_time:32559ms step_avg:60.52ms
step:539/2330 train_time:32618ms step_avg:60.52ms
step:540/2330 train_time:32680ms step_avg:60.52ms
step:541/2330 train_time:32740ms step_avg:60.52ms
step:542/2330 train_time:32802ms step_avg:60.52ms
step:543/2330 train_time:32862ms step_avg:60.52ms
step:544/2330 train_time:32923ms step_avg:60.52ms
step:545/2330 train_time:32982ms step_avg:60.52ms
step:546/2330 train_time:33044ms step_avg:60.52ms
step:547/2330 train_time:33103ms step_avg:60.52ms
step:548/2330 train_time:33165ms step_avg:60.52ms
step:549/2330 train_time:33224ms step_avg:60.52ms
step:550/2330 train_time:33286ms step_avg:60.52ms
step:551/2330 train_time:33347ms step_avg:60.52ms
step:552/2330 train_time:33409ms step_avg:60.52ms
step:553/2330 train_time:33469ms step_avg:60.52ms
step:554/2330 train_time:33530ms step_avg:60.52ms
step:555/2330 train_time:33590ms step_avg:60.52ms
step:556/2330 train_time:33651ms step_avg:60.52ms
step:557/2330 train_time:33710ms step_avg:60.52ms
step:558/2330 train_time:33772ms step_avg:60.52ms
step:559/2330 train_time:33831ms step_avg:60.52ms
step:560/2330 train_time:33893ms step_avg:60.52ms
step:561/2330 train_time:33952ms step_avg:60.52ms
step:562/2330 train_time:34013ms step_avg:60.52ms
step:563/2330 train_time:34071ms step_avg:60.52ms
step:564/2330 train_time:34133ms step_avg:60.52ms
step:565/2330 train_time:34193ms step_avg:60.52ms
step:566/2330 train_time:34254ms step_avg:60.52ms
step:567/2330 train_time:34314ms step_avg:60.52ms
step:568/2330 train_time:34375ms step_avg:60.52ms
step:569/2330 train_time:34434ms step_avg:60.52ms
step:570/2330 train_time:34496ms step_avg:60.52ms
step:571/2330 train_time:34557ms step_avg:60.52ms
step:572/2330 train_time:34619ms step_avg:60.52ms
step:573/2330 train_time:34678ms step_avg:60.52ms
step:574/2330 train_time:34741ms step_avg:60.52ms
step:575/2330 train_time:34801ms step_avg:60.52ms
step:576/2330 train_time:34863ms step_avg:60.53ms
step:577/2330 train_time:34922ms step_avg:60.52ms
step:578/2330 train_time:34983ms step_avg:60.52ms
step:579/2330 train_time:35043ms step_avg:60.52ms
step:580/2330 train_time:35104ms step_avg:60.52ms
step:581/2330 train_time:35164ms step_avg:60.52ms
step:582/2330 train_time:35225ms step_avg:60.52ms
step:583/2330 train_time:35285ms step_avg:60.52ms
step:584/2330 train_time:35347ms step_avg:60.53ms
step:585/2330 train_time:35406ms step_avg:60.52ms
step:586/2330 train_time:35469ms step_avg:60.53ms
step:587/2330 train_time:35529ms step_avg:60.53ms
step:588/2330 train_time:35590ms step_avg:60.53ms
step:589/2330 train_time:35651ms step_avg:60.53ms
step:590/2330 train_time:35712ms step_avg:60.53ms
step:591/2330 train_time:35771ms step_avg:60.53ms
step:592/2330 train_time:35833ms step_avg:60.53ms
step:593/2330 train_time:35892ms step_avg:60.53ms
step:594/2330 train_time:35954ms step_avg:60.53ms
step:595/2330 train_time:36013ms step_avg:60.53ms
step:596/2330 train_time:36074ms step_avg:60.53ms
step:597/2330 train_time:36133ms step_avg:60.52ms
step:598/2330 train_time:36195ms step_avg:60.53ms
step:599/2330 train_time:36255ms step_avg:60.53ms
step:600/2330 train_time:36316ms step_avg:60.53ms
step:601/2330 train_time:36375ms step_avg:60.52ms
step:602/2330 train_time:36437ms step_avg:60.53ms
step:603/2330 train_time:36496ms step_avg:60.52ms
step:604/2330 train_time:36559ms step_avg:60.53ms
step:605/2330 train_time:36619ms step_avg:60.53ms
step:606/2330 train_time:36681ms step_avg:60.53ms
step:607/2330 train_time:36741ms step_avg:60.53ms
step:608/2330 train_time:36802ms step_avg:60.53ms
step:609/2330 train_time:36861ms step_avg:60.53ms
step:610/2330 train_time:36923ms step_avg:60.53ms
step:611/2330 train_time:36982ms step_avg:60.53ms
step:612/2330 train_time:37044ms step_avg:60.53ms
step:613/2330 train_time:37104ms step_avg:60.53ms
step:614/2330 train_time:37165ms step_avg:60.53ms
step:615/2330 train_time:37225ms step_avg:60.53ms
step:616/2330 train_time:37287ms step_avg:60.53ms
step:617/2330 train_time:37347ms step_avg:60.53ms
step:618/2330 train_time:37408ms step_avg:60.53ms
step:619/2330 train_time:37468ms step_avg:60.53ms
step:620/2330 train_time:37530ms step_avg:60.53ms
step:621/2330 train_time:37589ms step_avg:60.53ms
step:622/2330 train_time:37650ms step_avg:60.53ms
step:623/2330 train_time:37709ms step_avg:60.53ms
step:624/2330 train_time:37771ms step_avg:60.53ms
step:625/2330 train_time:37830ms step_avg:60.53ms
step:626/2330 train_time:37892ms step_avg:60.53ms
step:627/2330 train_time:37952ms step_avg:60.53ms
step:628/2330 train_time:38013ms step_avg:60.53ms
step:629/2330 train_time:38072ms step_avg:60.53ms
step:630/2330 train_time:38133ms step_avg:60.53ms
step:631/2330 train_time:38192ms step_avg:60.53ms
step:632/2330 train_time:38254ms step_avg:60.53ms
step:633/2330 train_time:38313ms step_avg:60.53ms
step:634/2330 train_time:38374ms step_avg:60.53ms
step:635/2330 train_time:38433ms step_avg:60.53ms
step:636/2330 train_time:38495ms step_avg:60.53ms
step:637/2330 train_time:38554ms step_avg:60.52ms
step:638/2330 train_time:38616ms step_avg:60.53ms
step:639/2330 train_time:38676ms step_avg:60.53ms
step:640/2330 train_time:38737ms step_avg:60.53ms
step:641/2330 train_time:38798ms step_avg:60.53ms
step:642/2330 train_time:38860ms step_avg:60.53ms
step:643/2330 train_time:38920ms step_avg:60.53ms
step:644/2330 train_time:38981ms step_avg:60.53ms
step:645/2330 train_time:39041ms step_avg:60.53ms
step:646/2330 train_time:39103ms step_avg:60.53ms
step:647/2330 train_time:39162ms step_avg:60.53ms
step:648/2330 train_time:39224ms step_avg:60.53ms
step:649/2330 train_time:39284ms step_avg:60.53ms
step:650/2330 train_time:39345ms step_avg:60.53ms
step:651/2330 train_time:39405ms step_avg:60.53ms
step:652/2330 train_time:39467ms step_avg:60.53ms
step:653/2330 train_time:39526ms step_avg:60.53ms
step:654/2330 train_time:39588ms step_avg:60.53ms
step:655/2330 train_time:39648ms step_avg:60.53ms
step:656/2330 train_time:39709ms step_avg:60.53ms
step:657/2330 train_time:39769ms step_avg:60.53ms
step:658/2330 train_time:39830ms step_avg:60.53ms
step:659/2330 train_time:39889ms step_avg:60.53ms
step:660/2330 train_time:39950ms step_avg:60.53ms
step:661/2330 train_time:40010ms step_avg:60.53ms
step:662/2330 train_time:40072ms step_avg:60.53ms
step:663/2330 train_time:40131ms step_avg:60.53ms
step:664/2330 train_time:40193ms step_avg:60.53ms
step:665/2330 train_time:40252ms step_avg:60.53ms
step:666/2330 train_time:40313ms step_avg:60.53ms
step:667/2330 train_time:40372ms step_avg:60.53ms
step:668/2330 train_time:40433ms step_avg:60.53ms
step:669/2330 train_time:40493ms step_avg:60.53ms
step:670/2330 train_time:40554ms step_avg:60.53ms
step:671/2330 train_time:40613ms step_avg:60.53ms
step:672/2330 train_time:40675ms step_avg:60.53ms
step:673/2330 train_time:40734ms step_avg:60.53ms
step:674/2330 train_time:40795ms step_avg:60.53ms
step:675/2330 train_time:40855ms step_avg:60.53ms
step:676/2330 train_time:40916ms step_avg:60.53ms
step:677/2330 train_time:40976ms step_avg:60.53ms
step:678/2330 train_time:41038ms step_avg:60.53ms
step:679/2330 train_time:41098ms step_avg:60.53ms
step:680/2330 train_time:41160ms step_avg:60.53ms
step:681/2330 train_time:41220ms step_avg:60.53ms
step:682/2330 train_time:41281ms step_avg:60.53ms
step:683/2330 train_time:41341ms step_avg:60.53ms
step:684/2330 train_time:41402ms step_avg:60.53ms
step:685/2330 train_time:41462ms step_avg:60.53ms
step:686/2330 train_time:41524ms step_avg:60.53ms
step:687/2330 train_time:41584ms step_avg:60.53ms
step:688/2330 train_time:41646ms step_avg:60.53ms
step:689/2330 train_time:41706ms step_avg:60.53ms
step:690/2330 train_time:41768ms step_avg:60.53ms
step:691/2330 train_time:41827ms step_avg:60.53ms
step:692/2330 train_time:41889ms step_avg:60.53ms
step:693/2330 train_time:41949ms step_avg:60.53ms
step:694/2330 train_time:42010ms step_avg:60.53ms
step:695/2330 train_time:42069ms step_avg:60.53ms
step:696/2330 train_time:42131ms step_avg:60.53ms
step:697/2330 train_time:42191ms step_avg:60.53ms
step:698/2330 train_time:42253ms step_avg:60.53ms
step:699/2330 train_time:42312ms step_avg:60.53ms
step:700/2330 train_time:42374ms step_avg:60.53ms
step:701/2330 train_time:42434ms step_avg:60.53ms
step:702/2330 train_time:42494ms step_avg:60.53ms
step:703/2330 train_time:42553ms step_avg:60.53ms
step:704/2330 train_time:42614ms step_avg:60.53ms
step:705/2330 train_time:42673ms step_avg:60.53ms
step:706/2330 train_time:42735ms step_avg:60.53ms
step:707/2330 train_time:42794ms step_avg:60.53ms
step:708/2330 train_time:42857ms step_avg:60.53ms
step:709/2330 train_time:42917ms step_avg:60.53ms
step:710/2330 train_time:42978ms step_avg:60.53ms
step:711/2330 train_time:43038ms step_avg:60.53ms
step:712/2330 train_time:43101ms step_avg:60.53ms
step:713/2330 train_time:43160ms step_avg:60.53ms
step:714/2330 train_time:43222ms step_avg:60.54ms
step:715/2330 train_time:43282ms step_avg:60.53ms
step:716/2330 train_time:43343ms step_avg:60.54ms
step:717/2330 train_time:43403ms step_avg:60.53ms
step:718/2330 train_time:43465ms step_avg:60.54ms
step:719/2330 train_time:43524ms step_avg:60.53ms
step:720/2330 train_time:43586ms step_avg:60.54ms
step:721/2330 train_time:43646ms step_avg:60.53ms
step:722/2330 train_time:43708ms step_avg:60.54ms
step:723/2330 train_time:43767ms step_avg:60.54ms
step:724/2330 train_time:43829ms step_avg:60.54ms
step:725/2330 train_time:43889ms step_avg:60.54ms
step:726/2330 train_time:43951ms step_avg:60.54ms
step:727/2330 train_time:44010ms step_avg:60.54ms
step:728/2330 train_time:44072ms step_avg:60.54ms
step:729/2330 train_time:44131ms step_avg:60.54ms
step:730/2330 train_time:44193ms step_avg:60.54ms
step:731/2330 train_time:44253ms step_avg:60.54ms
step:732/2330 train_time:44315ms step_avg:60.54ms
step:733/2330 train_time:44374ms step_avg:60.54ms
step:734/2330 train_time:44435ms step_avg:60.54ms
step:735/2330 train_time:44493ms step_avg:60.54ms
step:736/2330 train_time:44555ms step_avg:60.54ms
step:737/2330 train_time:44614ms step_avg:60.54ms
step:738/2330 train_time:44675ms step_avg:60.54ms
step:739/2330 train_time:44735ms step_avg:60.53ms
step:740/2330 train_time:44796ms step_avg:60.54ms
step:741/2330 train_time:44856ms step_avg:60.53ms
step:742/2330 train_time:44918ms step_avg:60.54ms
step:743/2330 train_time:44978ms step_avg:60.54ms
step:744/2330 train_time:45041ms step_avg:60.54ms
step:745/2330 train_time:45101ms step_avg:60.54ms
step:746/2330 train_time:45163ms step_avg:60.54ms
step:747/2330 train_time:45223ms step_avg:60.54ms
step:748/2330 train_time:45284ms step_avg:60.54ms
step:749/2330 train_time:45344ms step_avg:60.54ms
step:750/2330 train_time:45406ms step_avg:60.54ms
step:750/2330 val_loss:3.6882 train_time:45469ms step_avg:60.63ms
step:751/2330 train_time:45492ms step_avg:60.57ms
step:752/2330 train_time:45529ms step_avg:60.54ms
step:753/2330 train_time:45593ms step_avg:60.55ms
step:754/2330 train_time:45656ms step_avg:60.55ms
step:755/2330 train_time:45715ms step_avg:60.55ms
step:756/2330 train_time:45778ms step_avg:60.55ms
step:757/2330 train_time:45836ms step_avg:60.55ms
step:758/2330 train_time:45897ms step_avg:60.55ms
step:759/2330 train_time:45958ms step_avg:60.55ms
step:760/2330 train_time:46019ms step_avg:60.55ms
step:761/2330 train_time:46077ms step_avg:60.55ms
step:762/2330 train_time:46138ms step_avg:60.55ms
step:763/2330 train_time:46197ms step_avg:60.55ms
step:764/2330 train_time:46258ms step_avg:60.55ms
step:765/2330 train_time:46316ms step_avg:60.54ms
step:766/2330 train_time:46378ms step_avg:60.55ms
step:767/2330 train_time:46439ms step_avg:60.55ms
step:768/2330 train_time:46502ms step_avg:60.55ms
step:769/2330 train_time:46565ms step_avg:60.55ms
step:770/2330 train_time:46629ms step_avg:60.56ms
step:771/2330 train_time:46690ms step_avg:60.56ms
step:772/2330 train_time:46751ms step_avg:60.56ms
step:773/2330 train_time:46811ms step_avg:60.56ms
step:774/2330 train_time:46873ms step_avg:60.56ms
step:775/2330 train_time:46933ms step_avg:60.56ms
step:776/2330 train_time:46995ms step_avg:60.56ms
step:777/2330 train_time:47055ms step_avg:60.56ms
step:778/2330 train_time:47117ms step_avg:60.56ms
step:779/2330 train_time:47177ms step_avg:60.56ms
step:780/2330 train_time:47239ms step_avg:60.56ms
step:781/2330 train_time:47298ms step_avg:60.56ms
step:782/2330 train_time:47360ms step_avg:60.56ms
step:783/2330 train_time:47421ms step_avg:60.56ms
step:784/2330 train_time:47484ms step_avg:60.57ms
step:785/2330 train_time:47546ms step_avg:60.57ms
step:786/2330 train_time:47608ms step_avg:60.57ms
step:787/2330 train_time:47669ms step_avg:60.57ms
step:788/2330 train_time:47731ms step_avg:60.57ms
step:789/2330 train_time:47791ms step_avg:60.57ms
step:790/2330 train_time:47854ms step_avg:60.57ms
step:791/2330 train_time:47914ms step_avg:60.57ms
step:792/2330 train_time:47976ms step_avg:60.58ms
step:793/2330 train_time:48036ms step_avg:60.57ms
step:794/2330 train_time:48097ms step_avg:60.58ms
step:795/2330 train_time:48157ms step_avg:60.57ms
step:796/2330 train_time:48219ms step_avg:60.58ms
step:797/2330 train_time:48279ms step_avg:60.58ms
step:798/2330 train_time:48341ms step_avg:60.58ms
step:799/2330 train_time:48402ms step_avg:60.58ms
step:800/2330 train_time:48465ms step_avg:60.58ms
step:801/2330 train_time:48526ms step_avg:60.58ms
step:802/2330 train_time:48588ms step_avg:60.58ms
step:803/2330 train_time:48649ms step_avg:60.58ms
step:804/2330 train_time:48711ms step_avg:60.59ms
step:805/2330 train_time:48771ms step_avg:60.59ms
step:806/2330 train_time:48834ms step_avg:60.59ms
step:807/2330 train_time:48894ms step_avg:60.59ms
step:808/2330 train_time:48956ms step_avg:60.59ms
step:809/2330 train_time:49016ms step_avg:60.59ms
step:810/2330 train_time:49078ms step_avg:60.59ms
step:811/2330 train_time:49138ms step_avg:60.59ms
step:812/2330 train_time:49200ms step_avg:60.59ms
step:813/2330 train_time:49259ms step_avg:60.59ms
step:814/2330 train_time:49321ms step_avg:60.59ms
step:815/2330 train_time:49381ms step_avg:60.59ms
step:816/2330 train_time:49444ms step_avg:60.59ms
step:817/2330 train_time:49504ms step_avg:60.59ms
step:818/2330 train_time:49567ms step_avg:60.60ms
step:819/2330 train_time:49628ms step_avg:60.60ms
step:820/2330 train_time:49690ms step_avg:60.60ms
step:821/2330 train_time:49750ms step_avg:60.60ms
step:822/2330 train_time:49813ms step_avg:60.60ms
step:823/2330 train_time:49873ms step_avg:60.60ms
step:824/2330 train_time:49936ms step_avg:60.60ms
step:825/2330 train_time:49995ms step_avg:60.60ms
step:826/2330 train_time:50057ms step_avg:60.60ms
step:827/2330 train_time:50117ms step_avg:60.60ms
step:828/2330 train_time:50179ms step_avg:60.60ms
step:829/2330 train_time:50239ms step_avg:60.60ms
step:830/2330 train_time:50301ms step_avg:60.60ms
step:831/2330 train_time:50362ms step_avg:60.60ms
step:832/2330 train_time:50424ms step_avg:60.61ms
step:833/2330 train_time:50484ms step_avg:60.61ms
step:834/2330 train_time:50547ms step_avg:60.61ms
step:835/2330 train_time:50607ms step_avg:60.61ms
step:836/2330 train_time:50669ms step_avg:60.61ms
step:837/2330 train_time:50729ms step_avg:60.61ms
step:838/2330 train_time:50792ms step_avg:60.61ms
step:839/2330 train_time:50852ms step_avg:60.61ms
step:840/2330 train_time:50914ms step_avg:60.61ms
step:841/2330 train_time:50974ms step_avg:60.61ms
step:842/2330 train_time:51036ms step_avg:60.61ms
step:843/2330 train_time:51096ms step_avg:60.61ms
step:844/2330 train_time:51159ms step_avg:60.61ms
step:845/2330 train_time:51219ms step_avg:60.61ms
step:846/2330 train_time:51281ms step_avg:60.62ms
step:847/2330 train_time:51342ms step_avg:60.62ms
step:848/2330 train_time:51404ms step_avg:60.62ms
step:849/2330 train_time:51464ms step_avg:60.62ms
step:850/2330 train_time:51526ms step_avg:60.62ms
step:851/2330 train_time:51587ms step_avg:60.62ms
step:852/2330 train_time:51649ms step_avg:60.62ms
step:853/2330 train_time:51709ms step_avg:60.62ms
step:854/2330 train_time:51771ms step_avg:60.62ms
step:855/2330 train_time:51832ms step_avg:60.62ms
step:856/2330 train_time:51895ms step_avg:60.62ms
step:857/2330 train_time:51954ms step_avg:60.62ms
step:858/2330 train_time:52016ms step_avg:60.63ms
step:859/2330 train_time:52076ms step_avg:60.62ms
step:860/2330 train_time:52139ms step_avg:60.63ms
step:861/2330 train_time:52199ms step_avg:60.63ms
step:862/2330 train_time:52262ms step_avg:60.63ms
step:863/2330 train_time:52321ms step_avg:60.63ms
step:864/2330 train_time:52384ms step_avg:60.63ms
step:865/2330 train_time:52444ms step_avg:60.63ms
step:866/2330 train_time:52507ms step_avg:60.63ms
step:867/2330 train_time:52567ms step_avg:60.63ms
step:868/2330 train_time:52629ms step_avg:60.63ms
step:869/2330 train_time:52689ms step_avg:60.63ms
step:870/2330 train_time:52752ms step_avg:60.63ms
step:871/2330 train_time:52812ms step_avg:60.63ms
step:872/2330 train_time:52874ms step_avg:60.63ms
step:873/2330 train_time:52934ms step_avg:60.63ms
step:874/2330 train_time:52995ms step_avg:60.64ms
step:875/2330 train_time:53056ms step_avg:60.64ms
step:876/2330 train_time:53118ms step_avg:60.64ms
step:877/2330 train_time:53179ms step_avg:60.64ms
step:878/2330 train_time:53241ms step_avg:60.64ms
step:879/2330 train_time:53301ms step_avg:60.64ms
step:880/2330 train_time:53364ms step_avg:60.64ms
step:881/2330 train_time:53424ms step_avg:60.64ms
step:882/2330 train_time:53486ms step_avg:60.64ms
step:883/2330 train_time:53546ms step_avg:60.64ms
step:884/2330 train_time:53608ms step_avg:60.64ms
step:885/2330 train_time:53669ms step_avg:60.64ms
step:886/2330 train_time:53730ms step_avg:60.64ms
step:887/2330 train_time:53791ms step_avg:60.64ms
step:888/2330 train_time:53852ms step_avg:60.64ms
step:889/2330 train_time:53912ms step_avg:60.64ms
step:890/2330 train_time:53974ms step_avg:60.64ms
step:891/2330 train_time:54034ms step_avg:60.64ms
step:892/2330 train_time:54097ms step_avg:60.65ms
step:893/2330 train_time:54158ms step_avg:60.65ms
step:894/2330 train_time:54220ms step_avg:60.65ms
step:895/2330 train_time:54282ms step_avg:60.65ms
step:896/2330 train_time:54344ms step_avg:60.65ms
step:897/2330 train_time:54404ms step_avg:60.65ms
step:898/2330 train_time:54466ms step_avg:60.65ms
step:899/2330 train_time:54526ms step_avg:60.65ms
step:900/2330 train_time:54588ms step_avg:60.65ms
step:901/2330 train_time:54648ms step_avg:60.65ms
step:902/2330 train_time:54710ms step_avg:60.65ms
step:903/2330 train_time:54771ms step_avg:60.65ms
step:904/2330 train_time:54833ms step_avg:60.66ms
step:905/2330 train_time:54893ms step_avg:60.66ms
step:906/2330 train_time:54954ms step_avg:60.66ms
step:907/2330 train_time:55014ms step_avg:60.65ms
step:908/2330 train_time:55077ms step_avg:60.66ms
step:909/2330 train_time:55137ms step_avg:60.66ms
step:910/2330 train_time:55200ms step_avg:60.66ms
step:911/2330 train_time:55260ms step_avg:60.66ms
step:912/2330 train_time:55322ms step_avg:60.66ms
step:913/2330 train_time:55383ms step_avg:60.66ms
step:914/2330 train_time:55445ms step_avg:60.66ms
step:915/2330 train_time:55506ms step_avg:60.66ms
step:916/2330 train_time:55567ms step_avg:60.66ms
step:917/2330 train_time:55628ms step_avg:60.66ms
step:918/2330 train_time:55690ms step_avg:60.66ms
step:919/2330 train_time:55750ms step_avg:60.66ms
step:920/2330 train_time:55812ms step_avg:60.67ms
step:921/2330 train_time:55873ms step_avg:60.67ms
step:922/2330 train_time:55934ms step_avg:60.67ms
step:923/2330 train_time:55995ms step_avg:60.67ms
step:924/2330 train_time:56057ms step_avg:60.67ms
step:925/2330 train_time:56117ms step_avg:60.67ms
step:926/2330 train_time:56179ms step_avg:60.67ms
step:927/2330 train_time:56240ms step_avg:60.67ms
step:928/2330 train_time:56303ms step_avg:60.67ms
step:929/2330 train_time:56363ms step_avg:60.67ms
step:930/2330 train_time:56425ms step_avg:60.67ms
step:931/2330 train_time:56486ms step_avg:60.67ms
step:932/2330 train_time:56548ms step_avg:60.67ms
step:933/2330 train_time:56608ms step_avg:60.67ms
step:934/2330 train_time:56670ms step_avg:60.67ms
step:935/2330 train_time:56730ms step_avg:60.67ms
step:936/2330 train_time:56792ms step_avg:60.68ms
step:937/2330 train_time:56852ms step_avg:60.67ms
step:938/2330 train_time:56914ms step_avg:60.68ms
step:939/2330 train_time:56974ms step_avg:60.68ms
step:940/2330 train_time:57036ms step_avg:60.68ms
step:941/2330 train_time:57096ms step_avg:60.68ms
step:942/2330 train_time:57159ms step_avg:60.68ms
step:943/2330 train_time:57219ms step_avg:60.68ms
step:944/2330 train_time:57282ms step_avg:60.68ms
step:945/2330 train_time:57341ms step_avg:60.68ms
step:946/2330 train_time:57404ms step_avg:60.68ms
step:947/2330 train_time:57464ms step_avg:60.68ms
step:948/2330 train_time:57526ms step_avg:60.68ms
step:949/2330 train_time:57587ms step_avg:60.68ms
step:950/2330 train_time:57648ms step_avg:60.68ms
step:951/2330 train_time:57709ms step_avg:60.68ms
step:952/2330 train_time:57771ms step_avg:60.68ms
step:953/2330 train_time:57831ms step_avg:60.68ms
step:954/2330 train_time:57894ms step_avg:60.69ms
step:955/2330 train_time:57954ms step_avg:60.68ms
step:956/2330 train_time:58016ms step_avg:60.69ms
step:957/2330 train_time:58076ms step_avg:60.69ms
step:958/2330 train_time:58138ms step_avg:60.69ms
step:959/2330 train_time:58198ms step_avg:60.69ms
step:960/2330 train_time:58261ms step_avg:60.69ms
step:961/2330 train_time:58322ms step_avg:60.69ms
step:962/2330 train_time:58385ms step_avg:60.69ms
step:963/2330 train_time:58445ms step_avg:60.69ms
step:964/2330 train_time:58508ms step_avg:60.69ms
step:965/2330 train_time:58568ms step_avg:60.69ms
step:966/2330 train_time:58630ms step_avg:60.69ms
step:967/2330 train_time:58690ms step_avg:60.69ms
step:968/2330 train_time:58752ms step_avg:60.69ms
step:969/2330 train_time:58813ms step_avg:60.69ms
step:970/2330 train_time:58875ms step_avg:60.70ms
step:971/2330 train_time:58935ms step_avg:60.70ms
step:972/2330 train_time:58997ms step_avg:60.70ms
step:973/2330 train_time:59056ms step_avg:60.70ms
step:974/2330 train_time:59119ms step_avg:60.70ms
step:975/2330 train_time:59180ms step_avg:60.70ms
step:976/2330 train_time:59242ms step_avg:60.70ms
step:977/2330 train_time:59302ms step_avg:60.70ms
step:978/2330 train_time:59365ms step_avg:60.70ms
step:979/2330 train_time:59425ms step_avg:60.70ms
step:980/2330 train_time:59488ms step_avg:60.70ms
step:981/2330 train_time:59548ms step_avg:60.70ms
step:982/2330 train_time:59609ms step_avg:60.70ms
step:983/2330 train_time:59669ms step_avg:60.70ms
step:984/2330 train_time:59730ms step_avg:60.70ms
step:985/2330 train_time:59790ms step_avg:60.70ms
step:986/2330 train_time:59852ms step_avg:60.70ms
step:987/2330 train_time:59913ms step_avg:60.70ms
step:988/2330 train_time:59974ms step_avg:60.70ms
step:989/2330 train_time:60034ms step_avg:60.70ms
step:990/2330 train_time:60097ms step_avg:60.70ms
step:991/2330 train_time:60158ms step_avg:60.70ms
step:992/2330 train_time:60220ms step_avg:60.71ms
step:993/2330 train_time:60281ms step_avg:60.71ms
step:994/2330 train_time:60343ms step_avg:60.71ms
step:995/2330 train_time:60404ms step_avg:60.71ms
step:996/2330 train_time:60466ms step_avg:60.71ms
step:997/2330 train_time:60526ms step_avg:60.71ms
step:998/2330 train_time:60589ms step_avg:60.71ms
step:999/2330 train_time:60648ms step_avg:60.71ms
step:1000/2330 train_time:60710ms step_avg:60.71ms
step:1000/2330 val_loss:3.5760 train_time:60774ms step_avg:60.77ms
step:1001/2330 train_time:60798ms step_avg:60.74ms
step:1002/2330 train_time:60834ms step_avg:60.71ms
step:1003/2330 train_time:60902ms step_avg:60.72ms
step:1004/2330 train_time:60967ms step_avg:60.72ms
step:1005/2330 train_time:61027ms step_avg:60.72ms
step:1006/2330 train_time:61090ms step_avg:60.73ms
step:1007/2330 train_time:61150ms step_avg:60.73ms
step:1008/2330 train_time:61213ms step_avg:60.73ms
step:1009/2330 train_time:61273ms step_avg:60.73ms
step:1010/2330 train_time:61334ms step_avg:60.73ms
step:1011/2330 train_time:61394ms step_avg:60.73ms
step:1012/2330 train_time:61456ms step_avg:60.73ms
step:1013/2330 train_time:61515ms step_avg:60.73ms
step:1014/2330 train_time:61576ms step_avg:60.73ms
step:1015/2330 train_time:61635ms step_avg:60.72ms
step:1016/2330 train_time:61698ms step_avg:60.73ms
step:1017/2330 train_time:61759ms step_avg:60.73ms
step:1018/2330 train_time:61823ms step_avg:60.73ms
step:1019/2330 train_time:61885ms step_avg:60.73ms
step:1020/2330 train_time:61949ms step_avg:60.73ms
step:1021/2330 train_time:62010ms step_avg:60.74ms
step:1022/2330 train_time:62073ms step_avg:60.74ms
step:1023/2330 train_time:62134ms step_avg:60.74ms
step:1024/2330 train_time:62196ms step_avg:60.74ms
step:1025/2330 train_time:62256ms step_avg:60.74ms
step:1026/2330 train_time:62318ms step_avg:60.74ms
step:1027/2330 train_time:62377ms step_avg:60.74ms
step:1028/2330 train_time:62439ms step_avg:60.74ms
step:1029/2330 train_time:62499ms step_avg:60.74ms
step:1030/2330 train_time:62560ms step_avg:60.74ms
step:1031/2330 train_time:62619ms step_avg:60.74ms
step:1032/2330 train_time:62681ms step_avg:60.74ms
step:1033/2330 train_time:62741ms step_avg:60.74ms
step:1034/2330 train_time:62804ms step_avg:60.74ms
step:1035/2330 train_time:62865ms step_avg:60.74ms
step:1036/2330 train_time:62928ms step_avg:60.74ms
step:1037/2330 train_time:62988ms step_avg:60.74ms
step:1038/2330 train_time:63051ms step_avg:60.74ms
step:1039/2330 train_time:63112ms step_avg:60.74ms
step:1040/2330 train_time:63174ms step_avg:60.74ms
step:1041/2330 train_time:63234ms step_avg:60.74ms
step:1042/2330 train_time:63296ms step_avg:60.75ms
step:1043/2330 train_time:63356ms step_avg:60.74ms
step:1044/2330 train_time:63418ms step_avg:60.75ms
step:1045/2330 train_time:63478ms step_avg:60.74ms
step:1046/2330 train_time:63539ms step_avg:60.75ms
step:1047/2330 train_time:63599ms step_avg:60.74ms
step:1048/2330 train_time:63661ms step_avg:60.74ms
step:1049/2330 train_time:63721ms step_avg:60.74ms
step:1050/2330 train_time:63783ms step_avg:60.75ms
step:1051/2330 train_time:63844ms step_avg:60.75ms
step:1052/2330 train_time:63907ms step_avg:60.75ms
step:1053/2330 train_time:63967ms step_avg:60.75ms
step:1054/2330 train_time:64030ms step_avg:60.75ms
step:1055/2330 train_time:64090ms step_avg:60.75ms
step:1056/2330 train_time:64153ms step_avg:60.75ms
step:1057/2330 train_time:64214ms step_avg:60.75ms
step:1058/2330 train_time:64276ms step_avg:60.75ms
step:1059/2330 train_time:64335ms step_avg:60.75ms
step:1060/2330 train_time:64397ms step_avg:60.75ms
step:1061/2330 train_time:64457ms step_avg:60.75ms
step:1062/2330 train_time:64519ms step_avg:60.75ms
step:1063/2330 train_time:64579ms step_avg:60.75ms
step:1064/2330 train_time:64641ms step_avg:60.75ms
step:1065/2330 train_time:64701ms step_avg:60.75ms
step:1066/2330 train_time:64763ms step_avg:60.75ms
step:1067/2330 train_time:64824ms step_avg:60.75ms
step:1068/2330 train_time:64887ms step_avg:60.76ms
step:1069/2330 train_time:64947ms step_avg:60.75ms
step:1070/2330 train_time:65010ms step_avg:60.76ms
step:1071/2330 train_time:65069ms step_avg:60.76ms
step:1072/2330 train_time:65132ms step_avg:60.76ms
step:1073/2330 train_time:65193ms step_avg:60.76ms
step:1074/2330 train_time:65256ms step_avg:60.76ms
step:1075/2330 train_time:65316ms step_avg:60.76ms
step:1076/2330 train_time:65378ms step_avg:60.76ms
step:1077/2330 train_time:65437ms step_avg:60.76ms
step:1078/2330 train_time:65499ms step_avg:60.76ms
step:1079/2330 train_time:65558ms step_avg:60.76ms
step:1080/2330 train_time:65621ms step_avg:60.76ms
step:1081/2330 train_time:65681ms step_avg:60.76ms
step:1082/2330 train_time:65743ms step_avg:60.76ms
step:1083/2330 train_time:65804ms step_avg:60.76ms
step:1084/2330 train_time:65866ms step_avg:60.76ms
step:1085/2330 train_time:65926ms step_avg:60.76ms
step:1086/2330 train_time:65988ms step_avg:60.76ms
step:1087/2330 train_time:66049ms step_avg:60.76ms
step:1088/2330 train_time:66111ms step_avg:60.76ms
step:1089/2330 train_time:66171ms step_avg:60.76ms
step:1090/2330 train_time:66233ms step_avg:60.76ms
step:1091/2330 train_time:66294ms step_avg:60.76ms
step:1092/2330 train_time:66357ms step_avg:60.77ms
step:1093/2330 train_time:66418ms step_avg:60.77ms
step:1094/2330 train_time:66479ms step_avg:60.77ms
step:1095/2330 train_time:66539ms step_avg:60.77ms
step:1096/2330 train_time:66602ms step_avg:60.77ms
step:1097/2330 train_time:66662ms step_avg:60.77ms
step:1098/2330 train_time:66724ms step_avg:60.77ms
step:1099/2330 train_time:66784ms step_avg:60.77ms
step:1100/2330 train_time:66846ms step_avg:60.77ms
step:1101/2330 train_time:66906ms step_avg:60.77ms
step:1102/2330 train_time:66968ms step_avg:60.77ms
step:1103/2330 train_time:67028ms step_avg:60.77ms
step:1104/2330 train_time:67091ms step_avg:60.77ms
step:1105/2330 train_time:67151ms step_avg:60.77ms
step:1106/2330 train_time:67213ms step_avg:60.77ms
step:1107/2330 train_time:67273ms step_avg:60.77ms
step:1108/2330 train_time:67336ms step_avg:60.77ms
step:1109/2330 train_time:67397ms step_avg:60.77ms
step:1110/2330 train_time:67459ms step_avg:60.77ms
step:1111/2330 train_time:67519ms step_avg:60.77ms
step:1112/2330 train_time:67580ms step_avg:60.77ms
step:1113/2330 train_time:67640ms step_avg:60.77ms
step:1114/2330 train_time:67703ms step_avg:60.77ms
step:1115/2330 train_time:67763ms step_avg:60.77ms
step:1116/2330 train_time:67825ms step_avg:60.78ms
step:1117/2330 train_time:67886ms step_avg:60.78ms
step:1118/2330 train_time:67948ms step_avg:60.78ms
step:1119/2330 train_time:68008ms step_avg:60.78ms
step:1120/2330 train_time:68070ms step_avg:60.78ms
step:1121/2330 train_time:68130ms step_avg:60.78ms
step:1122/2330 train_time:68192ms step_avg:60.78ms
step:1123/2330 train_time:68252ms step_avg:60.78ms
step:1124/2330 train_time:68315ms step_avg:60.78ms
step:1125/2330 train_time:68376ms step_avg:60.78ms
step:1126/2330 train_time:68438ms step_avg:60.78ms
step:1127/2330 train_time:68499ms step_avg:60.78ms
step:1128/2330 train_time:68560ms step_avg:60.78ms
step:1129/2330 train_time:68620ms step_avg:60.78ms
step:1130/2330 train_time:68683ms step_avg:60.78ms
step:1131/2330 train_time:68743ms step_avg:60.78ms
step:1132/2330 train_time:68805ms step_avg:60.78ms
step:1133/2330 train_time:68865ms step_avg:60.78ms
step:1134/2330 train_time:68927ms step_avg:60.78ms
step:1135/2330 train_time:68987ms step_avg:60.78ms
step:1136/2330 train_time:69049ms step_avg:60.78ms
step:1137/2330 train_time:69109ms step_avg:60.78ms
step:1138/2330 train_time:69171ms step_avg:60.78ms
step:1139/2330 train_time:69232ms step_avg:60.78ms
step:1140/2330 train_time:69294ms step_avg:60.78ms
step:1141/2330 train_time:69354ms step_avg:60.78ms
step:1142/2330 train_time:69417ms step_avg:60.79ms
step:1143/2330 train_time:69478ms step_avg:60.79ms
step:1144/2330 train_time:69541ms step_avg:60.79ms
step:1145/2330 train_time:69601ms step_avg:60.79ms
step:1146/2330 train_time:69663ms step_avg:60.79ms
step:1147/2330 train_time:69723ms step_avg:60.79ms
step:1148/2330 train_time:69785ms step_avg:60.79ms
step:1149/2330 train_time:69845ms step_avg:60.79ms
step:1150/2330 train_time:69907ms step_avg:60.79ms
step:1151/2330 train_time:69967ms step_avg:60.79ms
step:1152/2330 train_time:70029ms step_avg:60.79ms
step:1153/2330 train_time:70090ms step_avg:60.79ms
step:1154/2330 train_time:70152ms step_avg:60.79ms
step:1155/2330 train_time:70212ms step_avg:60.79ms
step:1156/2330 train_time:70273ms step_avg:60.79ms
step:1157/2330 train_time:70333ms step_avg:60.79ms
step:1158/2330 train_time:70397ms step_avg:60.79ms
step:1159/2330 train_time:70458ms step_avg:60.79ms
step:1160/2330 train_time:70521ms step_avg:60.79ms
step:1161/2330 train_time:70581ms step_avg:60.79ms
step:1162/2330 train_time:70643ms step_avg:60.79ms
step:1163/2330 train_time:70703ms step_avg:60.79ms
step:1164/2330 train_time:70765ms step_avg:60.79ms
step:1165/2330 train_time:70825ms step_avg:60.79ms
step:1166/2330 train_time:70887ms step_avg:60.80ms
step:1167/2330 train_time:70947ms step_avg:60.79ms
step:1168/2330 train_time:71009ms step_avg:60.80ms
step:1169/2330 train_time:71069ms step_avg:60.79ms
step:1170/2330 train_time:71131ms step_avg:60.80ms
step:1171/2330 train_time:71191ms step_avg:60.79ms
step:1172/2330 train_time:71253ms step_avg:60.80ms
step:1173/2330 train_time:71313ms step_avg:60.80ms
step:1174/2330 train_time:71375ms step_avg:60.80ms
step:1175/2330 train_time:71437ms step_avg:60.80ms
step:1176/2330 train_time:71500ms step_avg:60.80ms
step:1177/2330 train_time:71560ms step_avg:60.80ms
step:1178/2330 train_time:71622ms step_avg:60.80ms
step:1179/2330 train_time:71682ms step_avg:60.80ms
step:1180/2330 train_time:71744ms step_avg:60.80ms
step:1181/2330 train_time:71804ms step_avg:60.80ms
step:1182/2330 train_time:71866ms step_avg:60.80ms
step:1183/2330 train_time:71925ms step_avg:60.80ms
step:1184/2330 train_time:71989ms step_avg:60.80ms
step:1185/2330 train_time:72049ms step_avg:60.80ms
step:1186/2330 train_time:72111ms step_avg:60.80ms
step:1187/2330 train_time:72170ms step_avg:60.80ms
step:1188/2330 train_time:72232ms step_avg:60.80ms
step:1189/2330 train_time:72292ms step_avg:60.80ms
step:1190/2330 train_time:72355ms step_avg:60.80ms
step:1191/2330 train_time:72416ms step_avg:60.80ms
step:1192/2330 train_time:72478ms step_avg:60.80ms
step:1193/2330 train_time:72539ms step_avg:60.80ms
step:1194/2330 train_time:72602ms step_avg:60.81ms
step:1195/2330 train_time:72661ms step_avg:60.80ms
step:1196/2330 train_time:72723ms step_avg:60.81ms
step:1197/2330 train_time:72783ms step_avg:60.80ms
step:1198/2330 train_time:72845ms step_avg:60.81ms
step:1199/2330 train_time:72905ms step_avg:60.81ms
step:1200/2330 train_time:72967ms step_avg:60.81ms
step:1201/2330 train_time:73028ms step_avg:60.81ms
step:1202/2330 train_time:73090ms step_avg:60.81ms
step:1203/2330 train_time:73149ms step_avg:60.81ms
step:1204/2330 train_time:73212ms step_avg:60.81ms
step:1205/2330 train_time:73271ms step_avg:60.81ms
step:1206/2330 train_time:73334ms step_avg:60.81ms
step:1207/2330 train_time:73395ms step_avg:60.81ms
step:1208/2330 train_time:73458ms step_avg:60.81ms
step:1209/2330 train_time:73519ms step_avg:60.81ms
step:1210/2330 train_time:73581ms step_avg:60.81ms
step:1211/2330 train_time:73642ms step_avg:60.81ms
step:1212/2330 train_time:73704ms step_avg:60.81ms
step:1213/2330 train_time:73764ms step_avg:60.81ms
step:1214/2330 train_time:73826ms step_avg:60.81ms
step:1215/2330 train_time:73886ms step_avg:60.81ms
step:1216/2330 train_time:73948ms step_avg:60.81ms
step:1217/2330 train_time:74008ms step_avg:60.81ms
step:1218/2330 train_time:74069ms step_avg:60.81ms
step:1219/2330 train_time:74129ms step_avg:60.81ms
step:1220/2330 train_time:74191ms step_avg:60.81ms
step:1221/2330 train_time:74251ms step_avg:60.81ms
step:1222/2330 train_time:74314ms step_avg:60.81ms
step:1223/2330 train_time:74375ms step_avg:60.81ms
step:1224/2330 train_time:74437ms step_avg:60.81ms
step:1225/2330 train_time:74498ms step_avg:60.82ms
step:1226/2330 train_time:74561ms step_avg:60.82ms
step:1227/2330 train_time:74621ms step_avg:60.82ms
step:1228/2330 train_time:74684ms step_avg:60.82ms
step:1229/2330 train_time:74744ms step_avg:60.82ms
step:1230/2330 train_time:74806ms step_avg:60.82ms
step:1231/2330 train_time:74865ms step_avg:60.82ms
step:1232/2330 train_time:74927ms step_avg:60.82ms
step:1233/2330 train_time:74987ms step_avg:60.82ms
step:1234/2330 train_time:75049ms step_avg:60.82ms
step:1235/2330 train_time:75109ms step_avg:60.82ms
step:1236/2330 train_time:75171ms step_avg:60.82ms
step:1237/2330 train_time:75231ms step_avg:60.82ms
step:1238/2330 train_time:75294ms step_avg:60.82ms
step:1239/2330 train_time:75355ms step_avg:60.82ms
step:1240/2330 train_time:75418ms step_avg:60.82ms
step:1241/2330 train_time:75480ms step_avg:60.82ms
step:1242/2330 train_time:75542ms step_avg:60.82ms
step:1243/2330 train_time:75603ms step_avg:60.82ms
step:1244/2330 train_time:75665ms step_avg:60.82ms
step:1245/2330 train_time:75725ms step_avg:60.82ms
step:1246/2330 train_time:75787ms step_avg:60.82ms
step:1247/2330 train_time:75846ms step_avg:60.82ms
step:1248/2330 train_time:75909ms step_avg:60.82ms
step:1249/2330 train_time:75969ms step_avg:60.82ms
step:1250/2330 train_time:76031ms step_avg:60.82ms
step:1250/2330 val_loss:3.5191 train_time:76095ms step_avg:60.88ms
step:1251/2330 train_time:76117ms step_avg:60.84ms
step:1252/2330 train_time:76156ms step_avg:60.83ms
step:1253/2330 train_time:76220ms step_avg:60.83ms
step:1254/2330 train_time:76284ms step_avg:60.83ms
step:1255/2330 train_time:76345ms step_avg:60.83ms
step:1256/2330 train_time:76407ms step_avg:60.83ms
step:1257/2330 train_time:76466ms step_avg:60.83ms
step:1258/2330 train_time:76528ms step_avg:60.83ms
step:1259/2330 train_time:76587ms step_avg:60.83ms
step:1260/2330 train_time:76649ms step_avg:60.83ms
step:1261/2330 train_time:76708ms step_avg:60.83ms
step:1262/2330 train_time:76770ms step_avg:60.83ms
step:1263/2330 train_time:76830ms step_avg:60.83ms
step:1264/2330 train_time:76892ms step_avg:60.83ms
step:1265/2330 train_time:76951ms step_avg:60.83ms
step:1266/2330 train_time:77014ms step_avg:60.83ms
step:1267/2330 train_time:77075ms step_avg:60.83ms
step:1268/2330 train_time:77139ms step_avg:60.84ms
step:1269/2330 train_time:77200ms step_avg:60.84ms
step:1270/2330 train_time:77263ms step_avg:60.84ms
step:1271/2330 train_time:77324ms step_avg:60.84ms
step:1272/2330 train_time:77386ms step_avg:60.84ms
step:1273/2330 train_time:77446ms step_avg:60.84ms
step:1274/2330 train_time:77508ms step_avg:60.84ms
step:1275/2330 train_time:77568ms step_avg:60.84ms
step:1276/2330 train_time:77630ms step_avg:60.84ms
step:1277/2330 train_time:77689ms step_avg:60.84ms
step:1278/2330 train_time:77750ms step_avg:60.84ms
step:1279/2330 train_time:77810ms step_avg:60.84ms
step:1280/2330 train_time:77871ms step_avg:60.84ms
step:1281/2330 train_time:77931ms step_avg:60.84ms
step:1282/2330 train_time:77993ms step_avg:60.84ms
step:1283/2330 train_time:78054ms step_avg:60.84ms
step:1284/2330 train_time:78117ms step_avg:60.84ms
step:1285/2330 train_time:78177ms step_avg:60.84ms
step:1286/2330 train_time:78240ms step_avg:60.84ms
step:1287/2330 train_time:78299ms step_avg:60.84ms
step:1288/2330 train_time:78362ms step_avg:60.84ms
step:1289/2330 train_time:78422ms step_avg:60.84ms
step:1290/2330 train_time:78485ms step_avg:60.84ms
step:1291/2330 train_time:78545ms step_avg:60.84ms
step:1292/2330 train_time:78607ms step_avg:60.84ms
step:1293/2330 train_time:78667ms step_avg:60.84ms
step:1294/2330 train_time:78729ms step_avg:60.84ms
step:1295/2330 train_time:78789ms step_avg:60.84ms
step:1296/2330 train_time:78850ms step_avg:60.84ms
step:1297/2330 train_time:78910ms step_avg:60.84ms
step:1298/2330 train_time:78972ms step_avg:60.84ms
step:1299/2330 train_time:79032ms step_avg:60.84ms
step:1300/2330 train_time:79094ms step_avg:60.84ms
step:1301/2330 train_time:79155ms step_avg:60.84ms
step:1302/2330 train_time:79217ms step_avg:60.84ms
step:1303/2330 train_time:79278ms step_avg:60.84ms
step:1304/2330 train_time:79340ms step_avg:60.84ms
step:1305/2330 train_time:79400ms step_avg:60.84ms
step:1306/2330 train_time:79462ms step_avg:60.84ms
step:1307/2330 train_time:79523ms step_avg:60.84ms
step:1308/2330 train_time:79586ms step_avg:60.85ms
step:1309/2330 train_time:79645ms step_avg:60.84ms
step:1310/2330 train_time:79707ms step_avg:60.85ms
step:1311/2330 train_time:79767ms step_avg:60.84ms
step:1312/2330 train_time:79829ms step_avg:60.85ms
step:1313/2330 train_time:79889ms step_avg:60.84ms
step:1314/2330 train_time:79951ms step_avg:60.85ms
step:1315/2330 train_time:80011ms step_avg:60.85ms
step:1316/2330 train_time:80073ms step_avg:60.85ms
step:1317/2330 train_time:80134ms step_avg:60.85ms
step:1318/2330 train_time:80196ms step_avg:60.85ms
step:1319/2330 train_time:80256ms step_avg:60.85ms
step:1320/2330 train_time:80318ms step_avg:60.85ms
step:1321/2330 train_time:80379ms step_avg:60.85ms
step:1322/2330 train_time:80441ms step_avg:60.85ms
step:1323/2330 train_time:80501ms step_avg:60.85ms
step:1324/2330 train_time:80563ms step_avg:60.85ms
step:1325/2330 train_time:80623ms step_avg:60.85ms
step:1326/2330 train_time:80685ms step_avg:60.85ms
step:1327/2330 train_time:80746ms step_avg:60.85ms
step:1328/2330 train_time:80808ms step_avg:60.85ms
step:1329/2330 train_time:80868ms step_avg:60.85ms
step:1330/2330 train_time:80930ms step_avg:60.85ms
step:1331/2330 train_time:80990ms step_avg:60.85ms
step:1332/2330 train_time:81052ms step_avg:60.85ms
step:1333/2330 train_time:81112ms step_avg:60.85ms
step:1334/2330 train_time:81174ms step_avg:60.85ms
step:1335/2330 train_time:81234ms step_avg:60.85ms
step:1336/2330 train_time:81297ms step_avg:60.85ms
step:1337/2330 train_time:81357ms step_avg:60.85ms
step:1338/2330 train_time:81418ms step_avg:60.85ms
step:1339/2330 train_time:81478ms step_avg:60.85ms
step:1340/2330 train_time:81540ms step_avg:60.85ms
step:1341/2330 train_time:81600ms step_avg:60.85ms
step:1342/2330 train_time:81663ms step_avg:60.85ms
step:1343/2330 train_time:81724ms step_avg:60.85ms
step:1344/2330 train_time:81786ms step_avg:60.85ms
step:1345/2330 train_time:81846ms step_avg:60.85ms
step:1346/2330 train_time:81909ms step_avg:60.85ms
step:1347/2330 train_time:81970ms step_avg:60.85ms
step:1348/2330 train_time:82032ms step_avg:60.85ms
step:1349/2330 train_time:82092ms step_avg:60.85ms
step:1350/2330 train_time:82154ms step_avg:60.85ms
step:1351/2330 train_time:82214ms step_avg:60.85ms
step:1352/2330 train_time:82276ms step_avg:60.85ms
step:1353/2330 train_time:82336ms step_avg:60.85ms
step:1354/2330 train_time:82397ms step_avg:60.85ms
step:1355/2330 train_time:82457ms step_avg:60.85ms
step:1356/2330 train_time:82519ms step_avg:60.85ms
step:1357/2330 train_time:82579ms step_avg:60.85ms
step:1358/2330 train_time:82642ms step_avg:60.86ms
step:1359/2330 train_time:82702ms step_avg:60.86ms
step:1360/2330 train_time:82765ms step_avg:60.86ms
step:1361/2330 train_time:82825ms step_avg:60.86ms
step:1362/2330 train_time:82887ms step_avg:60.86ms
step:1363/2330 train_time:82948ms step_avg:60.86ms
step:1364/2330 train_time:83011ms step_avg:60.86ms
step:1365/2330 train_time:83071ms step_avg:60.86ms
step:1366/2330 train_time:83133ms step_avg:60.86ms
step:1367/2330 train_time:83193ms step_avg:60.86ms
step:1368/2330 train_time:83255ms step_avg:60.86ms
step:1369/2330 train_time:83315ms step_avg:60.86ms
step:1370/2330 train_time:83377ms step_avg:60.86ms
step:1371/2330 train_time:83437ms step_avg:60.86ms
step:1372/2330 train_time:83499ms step_avg:60.86ms
step:1373/2330 train_time:83558ms step_avg:60.86ms
step:1374/2330 train_time:83621ms step_avg:60.86ms
step:1375/2330 train_time:83681ms step_avg:60.86ms
step:1376/2330 train_time:83744ms step_avg:60.86ms
step:1377/2330 train_time:83804ms step_avg:60.86ms
step:1378/2330 train_time:83866ms step_avg:60.86ms
step:1379/2330 train_time:83927ms step_avg:60.86ms
step:1380/2330 train_time:83989ms step_avg:60.86ms
step:1381/2330 train_time:84049ms step_avg:60.86ms
step:1382/2330 train_time:84111ms step_avg:60.86ms
step:1383/2330 train_time:84171ms step_avg:60.86ms
step:1384/2330 train_time:84233ms step_avg:60.86ms
step:1385/2330 train_time:84293ms step_avg:60.86ms
step:1386/2330 train_time:84355ms step_avg:60.86ms
step:1387/2330 train_time:84415ms step_avg:60.86ms
step:1388/2330 train_time:84477ms step_avg:60.86ms
step:1389/2330 train_time:84537ms step_avg:60.86ms
step:1390/2330 train_time:84599ms step_avg:60.86ms
step:1391/2330 train_time:84660ms step_avg:60.86ms
step:1392/2330 train_time:84722ms step_avg:60.86ms
step:1393/2330 train_time:84783ms step_avg:60.86ms
step:1394/2330 train_time:84845ms step_avg:60.86ms
step:1395/2330 train_time:84906ms step_avg:60.86ms
step:1396/2330 train_time:84968ms step_avg:60.87ms
step:1397/2330 train_time:85029ms step_avg:60.87ms
step:1398/2330 train_time:85090ms step_avg:60.87ms
step:1399/2330 train_time:85150ms step_avg:60.87ms
step:1400/2330 train_time:85213ms step_avg:60.87ms
step:1401/2330 train_time:85272ms step_avg:60.87ms
step:1402/2330 train_time:85334ms step_avg:60.87ms
step:1403/2330 train_time:85394ms step_avg:60.87ms
step:1404/2330 train_time:85456ms step_avg:60.87ms
step:1405/2330 train_time:85516ms step_avg:60.87ms
step:1406/2330 train_time:85578ms step_avg:60.87ms
step:1407/2330 train_time:85638ms step_avg:60.87ms
step:1408/2330 train_time:85700ms step_avg:60.87ms
step:1409/2330 train_time:85760ms step_avg:60.87ms
step:1410/2330 train_time:85823ms step_avg:60.87ms
step:1411/2330 train_time:85885ms step_avg:60.87ms
step:1412/2330 train_time:85947ms step_avg:60.87ms
step:1413/2330 train_time:86007ms step_avg:60.87ms
step:1414/2330 train_time:86068ms step_avg:60.87ms
step:1415/2330 train_time:86129ms step_avg:60.87ms
step:1416/2330 train_time:86191ms step_avg:60.87ms
step:1417/2330 train_time:86251ms step_avg:60.87ms
step:1418/2330 train_time:86313ms step_avg:60.87ms
step:1419/2330 train_time:86372ms step_avg:60.87ms
step:1420/2330 train_time:86435ms step_avg:60.87ms
step:1421/2330 train_time:86495ms step_avg:60.87ms
step:1422/2330 train_time:86556ms step_avg:60.87ms
step:1423/2330 train_time:86616ms step_avg:60.87ms
step:1424/2330 train_time:86678ms step_avg:60.87ms
step:1425/2330 train_time:86738ms step_avg:60.87ms
step:1426/2330 train_time:86799ms step_avg:60.87ms
step:1427/2330 train_time:86861ms step_avg:60.87ms
step:1428/2330 train_time:86924ms step_avg:60.87ms
step:1429/2330 train_time:86985ms step_avg:60.87ms
step:1430/2330 train_time:87047ms step_avg:60.87ms
step:1431/2330 train_time:87107ms step_avg:60.87ms
step:1432/2330 train_time:87169ms step_avg:60.87ms
step:1433/2330 train_time:87229ms step_avg:60.87ms
step:1434/2330 train_time:87291ms step_avg:60.87ms
step:1435/2330 train_time:87352ms step_avg:60.87ms
step:1436/2330 train_time:87414ms step_avg:60.87ms
step:1437/2330 train_time:87473ms step_avg:60.87ms
step:1438/2330 train_time:87536ms step_avg:60.87ms
step:1439/2330 train_time:87596ms step_avg:60.87ms
step:1440/2330 train_time:87658ms step_avg:60.87ms
step:1441/2330 train_time:87718ms step_avg:60.87ms
step:1442/2330 train_time:87780ms step_avg:60.87ms
step:1443/2330 train_time:87840ms step_avg:60.87ms
step:1444/2330 train_time:87903ms step_avg:60.87ms
step:1445/2330 train_time:87963ms step_avg:60.87ms
step:1446/2330 train_time:88026ms step_avg:60.88ms
step:1447/2330 train_time:88086ms step_avg:60.87ms
step:1448/2330 train_time:88148ms step_avg:60.88ms
step:1449/2330 train_time:88209ms step_avg:60.88ms
step:1450/2330 train_time:88271ms step_avg:60.88ms
step:1451/2330 train_time:88332ms step_avg:60.88ms
step:1452/2330 train_time:88393ms step_avg:60.88ms
step:1453/2330 train_time:88453ms step_avg:60.88ms
step:1454/2330 train_time:88516ms step_avg:60.88ms
step:1455/2330 train_time:88576ms step_avg:60.88ms
step:1456/2330 train_time:88638ms step_avg:60.88ms
step:1457/2330 train_time:88698ms step_avg:60.88ms
step:1458/2330 train_time:88759ms step_avg:60.88ms
step:1459/2330 train_time:88819ms step_avg:60.88ms
step:1460/2330 train_time:88882ms step_avg:60.88ms
step:1461/2330 train_time:88942ms step_avg:60.88ms
step:1462/2330 train_time:89004ms step_avg:60.88ms
step:1463/2330 train_time:89064ms step_avg:60.88ms
step:1464/2330 train_time:89126ms step_avg:60.88ms
step:1465/2330 train_time:89187ms step_avg:60.88ms
step:1466/2330 train_time:89249ms step_avg:60.88ms
step:1467/2330 train_time:89310ms step_avg:60.88ms
step:1468/2330 train_time:89372ms step_avg:60.88ms
step:1469/2330 train_time:89432ms step_avg:60.88ms
step:1470/2330 train_time:89494ms step_avg:60.88ms
step:1471/2330 train_time:89555ms step_avg:60.88ms
step:1472/2330 train_time:89617ms step_avg:60.88ms
step:1473/2330 train_time:89677ms step_avg:60.88ms
step:1474/2330 train_time:89739ms step_avg:60.88ms
step:1475/2330 train_time:89798ms step_avg:60.88ms
step:1476/2330 train_time:89860ms step_avg:60.88ms
step:1477/2330 train_time:89920ms step_avg:60.88ms
step:1478/2330 train_time:89983ms step_avg:60.88ms
step:1479/2330 train_time:90043ms step_avg:60.88ms
step:1480/2330 train_time:90106ms step_avg:60.88ms
step:1481/2330 train_time:90166ms step_avg:60.88ms
step:1482/2330 train_time:90228ms step_avg:60.88ms
step:1483/2330 train_time:90288ms step_avg:60.88ms
step:1484/2330 train_time:90351ms step_avg:60.88ms
step:1485/2330 train_time:90411ms step_avg:60.88ms
step:1486/2330 train_time:90473ms step_avg:60.88ms
step:1487/2330 train_time:90533ms step_avg:60.88ms
step:1488/2330 train_time:90596ms step_avg:60.88ms
step:1489/2330 train_time:90656ms step_avg:60.88ms
step:1490/2330 train_time:90718ms step_avg:60.88ms
step:1491/2330 train_time:90778ms step_avg:60.88ms
step:1492/2330 train_time:90840ms step_avg:60.88ms
step:1493/2330 train_time:90900ms step_avg:60.88ms
step:1494/2330 train_time:90962ms step_avg:60.88ms
step:1495/2330 train_time:91023ms step_avg:60.88ms
step:1496/2330 train_time:91085ms step_avg:60.89ms
step:1497/2330 train_time:91145ms step_avg:60.89ms
step:1498/2330 train_time:91208ms step_avg:60.89ms
step:1499/2330 train_time:91268ms step_avg:60.89ms
step:1500/2330 train_time:91330ms step_avg:60.89ms
step:1500/2330 val_loss:3.4549 train_time:91394ms step_avg:60.93ms
step:1501/2330 train_time:91417ms step_avg:60.90ms
step:1502/2330 train_time:91457ms step_avg:60.89ms
step:1503/2330 train_time:91522ms step_avg:60.89ms
step:1504/2330 train_time:91585ms step_avg:60.89ms
step:1505/2330 train_time:91645ms step_avg:60.89ms
step:1506/2330 train_time:91707ms step_avg:60.89ms
step:1507/2330 train_time:91767ms step_avg:60.89ms
step:1508/2330 train_time:91828ms step_avg:60.89ms
step:1509/2330 train_time:91888ms step_avg:60.89ms
step:1510/2330 train_time:91949ms step_avg:60.89ms
step:1511/2330 train_time:92008ms step_avg:60.89ms
step:1512/2330 train_time:92070ms step_avg:60.89ms
step:1513/2330 train_time:92129ms step_avg:60.89ms
step:1514/2330 train_time:92192ms step_avg:60.89ms
step:1515/2330 train_time:92251ms step_avg:60.89ms
step:1516/2330 train_time:92314ms step_avg:60.89ms
step:1517/2330 train_time:92375ms step_avg:60.89ms
step:1518/2330 train_time:92440ms step_avg:60.90ms
step:1519/2330 train_time:92502ms step_avg:60.90ms
step:1520/2330 train_time:92565ms step_avg:60.90ms
step:1521/2330 train_time:92626ms step_avg:60.90ms
step:1522/2330 train_time:92688ms step_avg:60.90ms
step:1523/2330 train_time:92748ms step_avg:60.90ms
step:1524/2330 train_time:92810ms step_avg:60.90ms
step:1525/2330 train_time:92869ms step_avg:60.90ms
step:1526/2330 train_time:92931ms step_avg:60.90ms
step:1527/2330 train_time:92991ms step_avg:60.90ms
step:1528/2330 train_time:93053ms step_avg:60.90ms
step:1529/2330 train_time:93113ms step_avg:60.90ms
step:1530/2330 train_time:93175ms step_avg:60.90ms
step:1531/2330 train_time:93235ms step_avg:60.90ms
step:1532/2330 train_time:93298ms step_avg:60.90ms
step:1533/2330 train_time:93360ms step_avg:60.90ms
step:1534/2330 train_time:93423ms step_avg:60.90ms
step:1535/2330 train_time:93484ms step_avg:60.90ms
step:1536/2330 train_time:93547ms step_avg:60.90ms
step:1537/2330 train_time:93609ms step_avg:60.90ms
step:1538/2330 train_time:93671ms step_avg:60.90ms
step:1539/2330 train_time:93732ms step_avg:60.90ms
step:1540/2330 train_time:93795ms step_avg:60.91ms
step:1541/2330 train_time:93855ms step_avg:60.91ms
step:1542/2330 train_time:93918ms step_avg:60.91ms
step:1543/2330 train_time:93978ms step_avg:60.91ms
step:1544/2330 train_time:94041ms step_avg:60.91ms
step:1545/2330 train_time:94101ms step_avg:60.91ms
step:1546/2330 train_time:94164ms step_avg:60.91ms
step:1547/2330 train_time:94224ms step_avg:60.91ms
step:1548/2330 train_time:94287ms step_avg:60.91ms
step:1549/2330 train_time:94347ms step_avg:60.91ms
step:1550/2330 train_time:94411ms step_avg:60.91ms
step:1551/2330 train_time:94473ms step_avg:60.91ms
step:1552/2330 train_time:94537ms step_avg:60.91ms
step:1553/2330 train_time:94598ms step_avg:60.91ms
step:1554/2330 train_time:94660ms step_avg:60.91ms
step:1555/2330 train_time:94721ms step_avg:60.91ms
step:1556/2330 train_time:94783ms step_avg:60.91ms
step:1557/2330 train_time:94843ms step_avg:60.91ms
step:1558/2330 train_time:94906ms step_avg:60.92ms
step:1559/2330 train_time:94966ms step_avg:60.91ms
step:1560/2330 train_time:95028ms step_avg:60.92ms
step:1561/2330 train_time:95088ms step_avg:60.91ms
step:1562/2330 train_time:95151ms step_avg:60.92ms
step:1563/2330 train_time:95212ms step_avg:60.92ms
step:1564/2330 train_time:95274ms step_avg:60.92ms
step:1565/2330 train_time:95335ms step_avg:60.92ms
step:1566/2330 train_time:95398ms step_avg:60.92ms
step:1567/2330 train_time:95459ms step_avg:60.92ms
step:1568/2330 train_time:95522ms step_avg:60.92ms
step:1569/2330 train_time:95582ms step_avg:60.92ms
step:1570/2330 train_time:95645ms step_avg:60.92ms
step:1571/2330 train_time:95705ms step_avg:60.92ms
step:1572/2330 train_time:95767ms step_avg:60.92ms
step:1573/2330 train_time:95828ms step_avg:60.92ms
step:1574/2330 train_time:95891ms step_avg:60.92ms
step:1575/2330 train_time:95951ms step_avg:60.92ms
step:1576/2330 train_time:96014ms step_avg:60.92ms
step:1577/2330 train_time:96074ms step_avg:60.92ms
step:1578/2330 train_time:96137ms step_avg:60.92ms
step:1579/2330 train_time:96197ms step_avg:60.92ms
step:1580/2330 train_time:96260ms step_avg:60.92ms
step:1581/2330 train_time:96320ms step_avg:60.92ms
step:1582/2330 train_time:96383ms step_avg:60.92ms
step:1583/2330 train_time:96444ms step_avg:60.92ms
step:1584/2330 train_time:96506ms step_avg:60.93ms
step:1585/2330 train_time:96566ms step_avg:60.93ms
step:1586/2330 train_time:96629ms step_avg:60.93ms
step:1587/2330 train_time:96689ms step_avg:60.93ms
step:1588/2330 train_time:96752ms step_avg:60.93ms
step:1589/2330 train_time:96813ms step_avg:60.93ms
step:1590/2330 train_time:96876ms step_avg:60.93ms
step:1591/2330 train_time:96937ms step_avg:60.93ms
step:1592/2330 train_time:97000ms step_avg:60.93ms
step:1593/2330 train_time:97060ms step_avg:60.93ms
step:1594/2330 train_time:97122ms step_avg:60.93ms
step:1595/2330 train_time:97182ms step_avg:60.93ms
step:1596/2330 train_time:97244ms step_avg:60.93ms
step:1597/2330 train_time:97304ms step_avg:60.93ms
step:1598/2330 train_time:97367ms step_avg:60.93ms
step:1599/2330 train_time:97428ms step_avg:60.93ms
step:1600/2330 train_time:97490ms step_avg:60.93ms
step:1601/2330 train_time:97551ms step_avg:60.93ms
step:1602/2330 train_time:97614ms step_avg:60.93ms
step:1603/2330 train_time:97674ms step_avg:60.93ms
step:1604/2330 train_time:97737ms step_avg:60.93ms
step:1605/2330 train_time:97798ms step_avg:60.93ms
step:1606/2330 train_time:97860ms step_avg:60.93ms
step:1607/2330 train_time:97920ms step_avg:60.93ms
step:1608/2330 train_time:97982ms step_avg:60.93ms
step:1609/2330 train_time:98043ms step_avg:60.93ms
step:1610/2330 train_time:98104ms step_avg:60.93ms
step:1611/2330 train_time:98165ms step_avg:60.93ms
step:1612/2330 train_time:98228ms step_avg:60.94ms
step:1613/2330 train_time:98288ms step_avg:60.93ms
step:1614/2330 train_time:98350ms step_avg:60.94ms
step:1615/2330 train_time:98410ms step_avg:60.94ms
step:1616/2330 train_time:98473ms step_avg:60.94ms
step:1617/2330 train_time:98534ms step_avg:60.94ms
step:1618/2330 train_time:98596ms step_avg:60.94ms
step:1619/2330 train_time:98657ms step_avg:60.94ms
step:1620/2330 train_time:98720ms step_avg:60.94ms
step:1621/2330 train_time:98780ms step_avg:60.94ms
step:1622/2330 train_time:98842ms step_avg:60.94ms
step:1623/2330 train_time:98902ms step_avg:60.94ms
step:1624/2330 train_time:98964ms step_avg:60.94ms
step:1625/2330 train_time:99024ms step_avg:60.94ms
step:1626/2330 train_time:99087ms step_avg:60.94ms
step:1627/2330 train_time:99148ms step_avg:60.94ms
step:1628/2330 train_time:99210ms step_avg:60.94ms
step:1629/2330 train_time:99271ms step_avg:60.94ms
step:1630/2330 train_time:99334ms step_avg:60.94ms
step:1631/2330 train_time:99395ms step_avg:60.94ms
step:1632/2330 train_time:99457ms step_avg:60.94ms
step:1633/2330 train_time:99517ms step_avg:60.94ms
step:1634/2330 train_time:99581ms step_avg:60.94ms
step:1635/2330 train_time:99641ms step_avg:60.94ms
step:1636/2330 train_time:99703ms step_avg:60.94ms
step:1637/2330 train_time:99764ms step_avg:60.94ms
step:1638/2330 train_time:99826ms step_avg:60.94ms
step:1639/2330 train_time:99887ms step_avg:60.94ms
step:1640/2330 train_time:99949ms step_avg:60.94ms
step:1641/2330 train_time:100010ms step_avg:60.94ms
step:1642/2330 train_time:100073ms step_avg:60.95ms
step:1643/2330 train_time:100135ms step_avg:60.95ms
step:1644/2330 train_time:100198ms step_avg:60.95ms
step:1645/2330 train_time:100258ms step_avg:60.95ms
step:1646/2330 train_time:100321ms step_avg:60.95ms
step:1647/2330 train_time:100381ms step_avg:60.95ms
step:1648/2330 train_time:100443ms step_avg:60.95ms
step:1649/2330 train_time:100504ms step_avg:60.95ms
step:1650/2330 train_time:100567ms step_avg:60.95ms
step:1651/2330 train_time:100627ms step_avg:60.95ms
step:1652/2330 train_time:100689ms step_avg:60.95ms
step:1653/2330 train_time:100750ms step_avg:60.95ms
step:1654/2330 train_time:100813ms step_avg:60.95ms
step:1655/2330 train_time:100873ms step_avg:60.95ms
step:1656/2330 train_time:100936ms step_avg:60.95ms
step:1657/2330 train_time:100996ms step_avg:60.95ms
step:1658/2330 train_time:101059ms step_avg:60.95ms
step:1659/2330 train_time:101120ms step_avg:60.95ms
step:1660/2330 train_time:101183ms step_avg:60.95ms
step:1661/2330 train_time:101243ms step_avg:60.95ms
step:1662/2330 train_time:101305ms step_avg:60.95ms
step:1663/2330 train_time:101365ms step_avg:60.95ms
step:1664/2330 train_time:101427ms step_avg:60.95ms
step:1665/2330 train_time:101488ms step_avg:60.95ms
step:1666/2330 train_time:101551ms step_avg:60.95ms
step:1667/2330 train_time:101612ms step_avg:60.95ms
step:1668/2330 train_time:101675ms step_avg:60.96ms
step:1669/2330 train_time:101736ms step_avg:60.96ms
step:1670/2330 train_time:101798ms step_avg:60.96ms
step:1671/2330 train_time:101858ms step_avg:60.96ms
step:1672/2330 train_time:101920ms step_avg:60.96ms
step:1673/2330 train_time:101981ms step_avg:60.96ms
step:1674/2330 train_time:102043ms step_avg:60.96ms
step:1675/2330 train_time:102104ms step_avg:60.96ms
step:1676/2330 train_time:102166ms step_avg:60.96ms
step:1677/2330 train_time:102227ms step_avg:60.96ms
step:1678/2330 train_time:102290ms step_avg:60.96ms
step:1679/2330 train_time:102351ms step_avg:60.96ms
step:1680/2330 train_time:102414ms step_avg:60.96ms
step:1681/2330 train_time:102475ms step_avg:60.96ms
step:1682/2330 train_time:102538ms step_avg:60.96ms
step:1683/2330 train_time:102600ms step_avg:60.96ms
step:1684/2330 train_time:102663ms step_avg:60.96ms
step:1685/2330 train_time:102723ms step_avg:60.96ms
step:1686/2330 train_time:102785ms step_avg:60.96ms
step:1687/2330 train_time:102845ms step_avg:60.96ms
step:1688/2330 train_time:102907ms step_avg:60.96ms
step:1689/2330 train_time:102968ms step_avg:60.96ms
step:1690/2330 train_time:103031ms step_avg:60.97ms
step:1691/2330 train_time:103092ms step_avg:60.97ms
step:1692/2330 train_time:103156ms step_avg:60.97ms
step:1693/2330 train_time:103216ms step_avg:60.97ms
step:1694/2330 train_time:103279ms step_avg:60.97ms
step:1695/2330 train_time:103340ms step_avg:60.97ms
step:1696/2330 train_time:103402ms step_avg:60.97ms
step:1697/2330 train_time:103462ms step_avg:60.97ms
step:1698/2330 train_time:103524ms step_avg:60.97ms
step:1699/2330 train_time:103585ms step_avg:60.97ms
step:1700/2330 train_time:103648ms step_avg:60.97ms
step:1701/2330 train_time:103708ms step_avg:60.97ms
step:1702/2330 train_time:103771ms step_avg:60.97ms
step:1703/2330 train_time:103831ms step_avg:60.97ms
step:1704/2330 train_time:103895ms step_avg:60.97ms
step:1705/2330 train_time:103956ms step_avg:60.97ms
step:1706/2330 train_time:104019ms step_avg:60.97ms
step:1707/2330 train_time:104080ms step_avg:60.97ms
step:1708/2330 train_time:104142ms step_avg:60.97ms
step:1709/2330 train_time:104202ms step_avg:60.97ms
step:1710/2330 train_time:104265ms step_avg:60.97ms
step:1711/2330 train_time:104325ms step_avg:60.97ms
step:1712/2330 train_time:104388ms step_avg:60.97ms
step:1713/2330 train_time:104449ms step_avg:60.97ms
step:1714/2330 train_time:104511ms step_avg:60.98ms
step:1715/2330 train_time:104572ms step_avg:60.98ms
step:1716/2330 train_time:104635ms step_avg:60.98ms
step:1717/2330 train_time:104696ms step_avg:60.98ms
step:1718/2330 train_time:104758ms step_avg:60.98ms
step:1719/2330 train_time:104819ms step_avg:60.98ms
step:1720/2330 train_time:104882ms step_avg:60.98ms
step:1721/2330 train_time:104942ms step_avg:60.98ms
step:1722/2330 train_time:105004ms step_avg:60.98ms
step:1723/2330 train_time:105065ms step_avg:60.98ms
step:1724/2330 train_time:105127ms step_avg:60.98ms
step:1725/2330 train_time:105188ms step_avg:60.98ms
step:1726/2330 train_time:105251ms step_avg:60.98ms
step:1727/2330 train_time:105312ms step_avg:60.98ms
step:1728/2330 train_time:105375ms step_avg:60.98ms
step:1729/2330 train_time:105436ms step_avg:60.98ms
step:1730/2330 train_time:105498ms step_avg:60.98ms
step:1731/2330 train_time:105559ms step_avg:60.98ms
step:1732/2330 train_time:105621ms step_avg:60.98ms
step:1733/2330 train_time:105682ms step_avg:60.98ms
step:1734/2330 train_time:105744ms step_avg:60.98ms
step:1735/2330 train_time:105804ms step_avg:60.98ms
step:1736/2330 train_time:105867ms step_avg:60.98ms
step:1737/2330 train_time:105928ms step_avg:60.98ms
step:1738/2330 train_time:105990ms step_avg:60.98ms
step:1739/2330 train_time:106051ms step_avg:60.98ms
step:1740/2330 train_time:106114ms step_avg:60.98ms
step:1741/2330 train_time:106175ms step_avg:60.98ms
step:1742/2330 train_time:106237ms step_avg:60.99ms
step:1743/2330 train_time:106298ms step_avg:60.99ms
step:1744/2330 train_time:106361ms step_avg:60.99ms
step:1745/2330 train_time:106421ms step_avg:60.99ms
step:1746/2330 train_time:106483ms step_avg:60.99ms
step:1747/2330 train_time:106544ms step_avg:60.99ms
step:1748/2330 train_time:106606ms step_avg:60.99ms
step:1749/2330 train_time:106667ms step_avg:60.99ms
step:1750/2330 train_time:106730ms step_avg:60.99ms
step:1750/2330 val_loss:3.3854 train_time:106794ms step_avg:61.03ms
step:1751/2330 train_time:106818ms step_avg:61.00ms
step:1752/2330 train_time:106856ms step_avg:60.99ms
step:1753/2330 train_time:106924ms step_avg:61.00ms
step:1754/2330 train_time:106989ms step_avg:61.00ms
step:1755/2330 train_time:107050ms step_avg:61.00ms
step:1756/2330 train_time:107113ms step_avg:61.00ms
step:1757/2330 train_time:107172ms step_avg:61.00ms
step:1758/2330 train_time:107236ms step_avg:61.00ms
step:1759/2330 train_time:107296ms step_avg:61.00ms
step:1760/2330 train_time:107358ms step_avg:61.00ms
step:1761/2330 train_time:107417ms step_avg:61.00ms
step:1762/2330 train_time:107479ms step_avg:61.00ms
step:1763/2330 train_time:107538ms step_avg:61.00ms
step:1764/2330 train_time:107600ms step_avg:61.00ms
step:1765/2330 train_time:107660ms step_avg:61.00ms
step:1766/2330 train_time:107728ms step_avg:61.00ms
step:1767/2330 train_time:107790ms step_avg:61.00ms
step:1768/2330 train_time:107854ms step_avg:61.00ms
step:1769/2330 train_time:107916ms step_avg:61.00ms
step:1770/2330 train_time:107979ms step_avg:61.01ms
step:1771/2330 train_time:108040ms step_avg:61.01ms
step:1772/2330 train_time:108103ms step_avg:61.01ms
step:1773/2330 train_time:108164ms step_avg:61.01ms
step:1774/2330 train_time:108226ms step_avg:61.01ms
step:1775/2330 train_time:108286ms step_avg:61.01ms
step:1776/2330 train_time:108348ms step_avg:61.01ms
step:1777/2330 train_time:108408ms step_avg:61.01ms
step:1778/2330 train_time:108470ms step_avg:61.01ms
step:1779/2330 train_time:108529ms step_avg:61.01ms
step:1780/2330 train_time:108592ms step_avg:61.01ms
step:1781/2330 train_time:108653ms step_avg:61.01ms
step:1782/2330 train_time:108716ms step_avg:61.01ms
step:1783/2330 train_time:108777ms step_avg:61.01ms
step:1784/2330 train_time:108841ms step_avg:61.01ms
step:1785/2330 train_time:108902ms step_avg:61.01ms
step:1786/2330 train_time:108966ms step_avg:61.01ms
step:1787/2330 train_time:109026ms step_avg:61.01ms
step:1788/2330 train_time:109089ms step_avg:61.01ms
step:1789/2330 train_time:109151ms step_avg:61.01ms
step:1790/2330 train_time:109213ms step_avg:61.01ms
step:1791/2330 train_time:109274ms step_avg:61.01ms
step:1792/2330 train_time:109337ms step_avg:61.01ms
step:1793/2330 train_time:109397ms step_avg:61.01ms
step:1794/2330 train_time:109459ms step_avg:61.01ms
step:1795/2330 train_time:109519ms step_avg:61.01ms
step:1796/2330 train_time:109582ms step_avg:61.01ms
step:1797/2330 train_time:109642ms step_avg:61.01ms
step:1798/2330 train_time:109704ms step_avg:61.01ms
step:1799/2330 train_time:109765ms step_avg:61.01ms
step:1800/2330 train_time:109828ms step_avg:61.02ms
step:1801/2330 train_time:109889ms step_avg:61.02ms
step:1802/2330 train_time:109952ms step_avg:61.02ms
step:1803/2330 train_time:110013ms step_avg:61.02ms
step:1804/2330 train_time:110076ms step_avg:61.02ms
step:1805/2330 train_time:110137ms step_avg:61.02ms
step:1806/2330 train_time:110200ms step_avg:61.02ms
step:1807/2330 train_time:110260ms step_avg:61.02ms
step:1808/2330 train_time:110323ms step_avg:61.02ms
step:1809/2330 train_time:110383ms step_avg:61.02ms
step:1810/2330 train_time:110445ms step_avg:61.02ms
step:1811/2330 train_time:110505ms step_avg:61.02ms
step:1812/2330 train_time:110568ms step_avg:61.02ms
step:1813/2330 train_time:110628ms step_avg:61.02ms
step:1814/2330 train_time:110690ms step_avg:61.02ms
step:1815/2330 train_time:110751ms step_avg:61.02ms
step:1816/2330 train_time:110814ms step_avg:61.02ms
step:1817/2330 train_time:110875ms step_avg:61.02ms
step:1818/2330 train_time:110938ms step_avg:61.02ms
step:1819/2330 train_time:110999ms step_avg:61.02ms
step:1820/2330 train_time:111062ms step_avg:61.02ms
step:1821/2330 train_time:111123ms step_avg:61.02ms
step:1822/2330 train_time:111185ms step_avg:61.02ms
step:1823/2330 train_time:111245ms step_avg:61.02ms
step:1824/2330 train_time:111307ms step_avg:61.02ms
step:1825/2330 train_time:111368ms step_avg:61.02ms
step:1826/2330 train_time:111430ms step_avg:61.02ms
step:1827/2330 train_time:111491ms step_avg:61.02ms
step:1828/2330 train_time:111554ms step_avg:61.03ms
step:1829/2330 train_time:111615ms step_avg:61.02ms
step:1830/2330 train_time:111677ms step_avg:61.03ms
step:1831/2330 train_time:111738ms step_avg:61.03ms
step:1832/2330 train_time:111800ms step_avg:61.03ms
step:1833/2330 train_time:111861ms step_avg:61.03ms
step:1834/2330 train_time:111923ms step_avg:61.03ms
step:1835/2330 train_time:111984ms step_avg:61.03ms
step:1836/2330 train_time:112047ms step_avg:61.03ms
step:1837/2330 train_time:112107ms step_avg:61.03ms
step:1838/2330 train_time:112169ms step_avg:61.03ms
step:1839/2330 train_time:112229ms step_avg:61.03ms
step:1840/2330 train_time:112292ms step_avg:61.03ms
step:1841/2330 train_time:112353ms step_avg:61.03ms
step:1842/2330 train_time:112416ms step_avg:61.03ms
step:1843/2330 train_time:112477ms step_avg:61.03ms
step:1844/2330 train_time:112540ms step_avg:61.03ms
step:1845/2330 train_time:112600ms step_avg:61.03ms
step:1846/2330 train_time:112663ms step_avg:61.03ms
step:1847/2330 train_time:112723ms step_avg:61.03ms
step:1848/2330 train_time:112786ms step_avg:61.03ms
step:1849/2330 train_time:112846ms step_avg:61.03ms
step:1850/2330 train_time:112909ms step_avg:61.03ms
step:1851/2330 train_time:112970ms step_avg:61.03ms
step:1852/2330 train_time:113032ms step_avg:61.03ms
step:1853/2330 train_time:113093ms step_avg:61.03ms
step:1854/2330 train_time:113156ms step_avg:61.03ms
step:1855/2330 train_time:113217ms step_avg:61.03ms
step:1856/2330 train_time:113280ms step_avg:61.03ms
step:1857/2330 train_time:113341ms step_avg:61.03ms
step:1858/2330 train_time:113403ms step_avg:61.04ms
step:1859/2330 train_time:113464ms step_avg:61.03ms
step:1860/2330 train_time:113526ms step_avg:61.04ms
step:1861/2330 train_time:113586ms step_avg:61.03ms
step:1862/2330 train_time:113648ms step_avg:61.04ms
step:1863/2330 train_time:113709ms step_avg:61.04ms
step:1864/2330 train_time:113772ms step_avg:61.04ms
step:1865/2330 train_time:113832ms step_avg:61.04ms
step:1866/2330 train_time:113895ms step_avg:61.04ms
step:1867/2330 train_time:113957ms step_avg:61.04ms
step:1868/2330 train_time:114019ms step_avg:61.04ms
step:1869/2330 train_time:114080ms step_avg:61.04ms
step:1870/2330 train_time:114143ms step_avg:61.04ms
step:1871/2330 train_time:114203ms step_avg:61.04ms
step:1872/2330 train_time:114266ms step_avg:61.04ms
step:1873/2330 train_time:114325ms step_avg:61.04ms
step:1874/2330 train_time:114388ms step_avg:61.04ms
step:1875/2330 train_time:114449ms step_avg:61.04ms
step:1876/2330 train_time:114511ms step_avg:61.04ms
step:1877/2330 train_time:114572ms step_avg:61.04ms
step:1878/2330 train_time:114636ms step_avg:61.04ms
step:1879/2330 train_time:114697ms step_avg:61.04ms
step:1880/2330 train_time:114760ms step_avg:61.04ms
step:1881/2330 train_time:114821ms step_avg:61.04ms
step:1882/2330 train_time:114884ms step_avg:61.04ms
step:1883/2330 train_time:114944ms step_avg:61.04ms
step:1884/2330 train_time:115007ms step_avg:61.04ms
step:1885/2330 train_time:115068ms step_avg:61.04ms
step:1886/2330 train_time:115130ms step_avg:61.04ms
step:1887/2330 train_time:115190ms step_avg:61.04ms
step:1888/2330 train_time:115253ms step_avg:61.05ms
step:1889/2330 train_time:115314ms step_avg:61.04ms
step:1890/2330 train_time:115377ms step_avg:61.05ms
step:1891/2330 train_time:115437ms step_avg:61.05ms
step:1892/2330 train_time:115500ms step_avg:61.05ms
step:1893/2330 train_time:115560ms step_avg:61.05ms
step:1894/2330 train_time:115623ms step_avg:61.05ms
step:1895/2330 train_time:115683ms step_avg:61.05ms
step:1896/2330 train_time:115745ms step_avg:61.05ms
step:1897/2330 train_time:115806ms step_avg:61.05ms
step:1898/2330 train_time:115868ms step_avg:61.05ms
step:1899/2330 train_time:115930ms step_avg:61.05ms
step:1900/2330 train_time:115993ms step_avg:61.05ms
step:1901/2330 train_time:116055ms step_avg:61.05ms
step:1902/2330 train_time:116118ms step_avg:61.05ms
step:1903/2330 train_time:116178ms step_avg:61.05ms
step:1904/2330 train_time:116241ms step_avg:61.05ms
step:1905/2330 train_time:116301ms step_avg:61.05ms
step:1906/2330 train_time:116363ms step_avg:61.05ms
step:1907/2330 train_time:116423ms step_avg:61.05ms
step:1908/2330 train_time:116486ms step_avg:61.05ms
step:1909/2330 train_time:116546ms step_avg:61.05ms
step:1910/2330 train_time:116608ms step_avg:61.05ms
step:1911/2330 train_time:116669ms step_avg:61.05ms
step:1912/2330 train_time:116732ms step_avg:61.05ms
step:1913/2330 train_time:116793ms step_avg:61.05ms
step:1914/2330 train_time:116857ms step_avg:61.05ms
step:1915/2330 train_time:116918ms step_avg:61.05ms
step:1916/2330 train_time:116980ms step_avg:61.05ms
step:1917/2330 train_time:117041ms step_avg:61.05ms
step:1918/2330 train_time:117103ms step_avg:61.05ms
step:1919/2330 train_time:117164ms step_avg:61.05ms
step:1920/2330 train_time:117226ms step_avg:61.06ms
step:1921/2330 train_time:117287ms step_avg:61.05ms
step:1922/2330 train_time:117349ms step_avg:61.06ms
step:1923/2330 train_time:117411ms step_avg:61.06ms
step:1924/2330 train_time:117474ms step_avg:61.06ms
step:1925/2330 train_time:117535ms step_avg:61.06ms
step:1926/2330 train_time:117598ms step_avg:61.06ms
step:1927/2330 train_time:117659ms step_avg:61.06ms
step:1928/2330 train_time:117722ms step_avg:61.06ms
step:1929/2330 train_time:117783ms step_avg:61.06ms
step:1930/2330 train_time:117845ms step_avg:61.06ms
step:1931/2330 train_time:117905ms step_avg:61.06ms
step:1932/2330 train_time:117968ms step_avg:61.06ms
step:1933/2330 train_time:118028ms step_avg:61.06ms
step:1934/2330 train_time:118091ms step_avg:61.06ms
step:1935/2330 train_time:118152ms step_avg:61.06ms
step:1936/2330 train_time:118215ms step_avg:61.06ms
step:1937/2330 train_time:118274ms step_avg:61.06ms
step:1938/2330 train_time:118337ms step_avg:61.06ms
step:1939/2330 train_time:118398ms step_avg:61.06ms
step:1940/2330 train_time:118461ms step_avg:61.06ms
step:1941/2330 train_time:118521ms step_avg:61.06ms
step:1942/2330 train_time:118584ms step_avg:61.06ms
step:1943/2330 train_time:118644ms step_avg:61.06ms
step:1944/2330 train_time:118706ms step_avg:61.06ms
step:1945/2330 train_time:118767ms step_avg:61.06ms
step:1946/2330 train_time:118829ms step_avg:61.06ms
step:1947/2330 train_time:118891ms step_avg:61.06ms
step:1948/2330 train_time:118954ms step_avg:61.06ms
step:1949/2330 train_time:119014ms step_avg:61.06ms
step:1950/2330 train_time:119077ms step_avg:61.07ms
step:1951/2330 train_time:119138ms step_avg:61.07ms
step:1952/2330 train_time:119200ms step_avg:61.07ms
step:1953/2330 train_time:119260ms step_avg:61.07ms
step:1954/2330 train_time:119323ms step_avg:61.07ms
step:1955/2330 train_time:119383ms step_avg:61.07ms
step:1956/2330 train_time:119445ms step_avg:61.07ms
step:1957/2330 train_time:119506ms step_avg:61.07ms
step:1958/2330 train_time:119568ms step_avg:61.07ms
step:1959/2330 train_time:119629ms step_avg:61.07ms
step:1960/2330 train_time:119692ms step_avg:61.07ms
step:1961/2330 train_time:119753ms step_avg:61.07ms
step:1962/2330 train_time:119815ms step_avg:61.07ms
step:1963/2330 train_time:119876ms step_avg:61.07ms
step:1964/2330 train_time:119939ms step_avg:61.07ms
step:1965/2330 train_time:120000ms step_avg:61.07ms
step:1966/2330 train_time:120063ms step_avg:61.07ms
step:1967/2330 train_time:120123ms step_avg:61.07ms
step:1968/2330 train_time:120185ms step_avg:61.07ms
step:1969/2330 train_time:120245ms step_avg:61.07ms
step:1970/2330 train_time:120308ms step_avg:61.07ms
step:1971/2330 train_time:120369ms step_avg:61.07ms
step:1972/2330 train_time:120431ms step_avg:61.07ms
step:1973/2330 train_time:120492ms step_avg:61.07ms
step:1974/2330 train_time:120555ms step_avg:61.07ms
step:1975/2330 train_time:120615ms step_avg:61.07ms
step:1976/2330 train_time:120678ms step_avg:61.07ms
step:1977/2330 train_time:120739ms step_avg:61.07ms
step:1978/2330 train_time:120802ms step_avg:61.07ms
step:1979/2330 train_time:120863ms step_avg:61.07ms
step:1980/2330 train_time:120925ms step_avg:61.07ms
step:1981/2330 train_time:120986ms step_avg:61.07ms
step:1982/2330 train_time:121048ms step_avg:61.07ms
step:1983/2330 train_time:121108ms step_avg:61.07ms
step:1984/2330 train_time:121170ms step_avg:61.07ms
step:1985/2330 train_time:121231ms step_avg:61.07ms
step:1986/2330 train_time:121294ms step_avg:61.07ms
step:1987/2330 train_time:121355ms step_avg:61.07ms
step:1988/2330 train_time:121417ms step_avg:61.08ms
step:1989/2330 train_time:121478ms step_avg:61.07ms
step:1990/2330 train_time:121541ms step_avg:61.08ms
step:1991/2330 train_time:121602ms step_avg:61.08ms
step:1992/2330 train_time:121664ms step_avg:61.08ms
step:1993/2330 train_time:121724ms step_avg:61.08ms
step:1994/2330 train_time:121787ms step_avg:61.08ms
step:1995/2330 train_time:121848ms step_avg:61.08ms
step:1996/2330 train_time:121911ms step_avg:61.08ms
step:1997/2330 train_time:121972ms step_avg:61.08ms
step:1998/2330 train_time:122034ms step_avg:61.08ms
step:1999/2330 train_time:122095ms step_avg:61.08ms
step:2000/2330 train_time:122158ms step_avg:61.08ms
step:2000/2330 val_loss:3.3359 train_time:122223ms step_avg:61.11ms
step:2001/2330 train_time:122246ms step_avg:61.09ms
step:2002/2330 train_time:122286ms step_avg:61.08ms
step:2003/2330 train_time:122349ms step_avg:61.08ms
step:2004/2330 train_time:122412ms step_avg:61.08ms
step:2005/2330 train_time:122473ms step_avg:61.08ms
step:2006/2330 train_time:122536ms step_avg:61.08ms
step:2007/2330 train_time:122596ms step_avg:61.08ms
step:2008/2330 train_time:122659ms step_avg:61.09ms
step:2009/2330 train_time:122719ms step_avg:61.08ms
step:2010/2330 train_time:122781ms step_avg:61.09ms
step:2011/2330 train_time:122841ms step_avg:61.08ms
step:2012/2330 train_time:122903ms step_avg:61.09ms
step:2013/2330 train_time:122963ms step_avg:61.08ms
step:2014/2330 train_time:123025ms step_avg:61.08ms
step:2015/2330 train_time:123085ms step_avg:61.08ms
step:2016/2330 train_time:123148ms step_avg:61.09ms
step:2017/2330 train_time:123209ms step_avg:61.09ms
step:2018/2330 train_time:123273ms step_avg:61.09ms
step:2019/2330 train_time:123335ms step_avg:61.09ms
step:2020/2330 train_time:123399ms step_avg:61.09ms
step:2021/2330 train_time:123461ms step_avg:61.09ms
step:2022/2330 train_time:123523ms step_avg:61.09ms
step:2023/2330 train_time:123584ms step_avg:61.09ms
step:2024/2330 train_time:123647ms step_avg:61.09ms
step:2025/2330 train_time:123706ms step_avg:61.09ms
step:2026/2330 train_time:123768ms step_avg:61.09ms
step:2027/2330 train_time:123828ms step_avg:61.09ms
step:2028/2330 train_time:123890ms step_avg:61.09ms
step:2029/2330 train_time:123950ms step_avg:61.09ms
step:2030/2330 train_time:124012ms step_avg:61.09ms
step:2031/2330 train_time:124073ms step_avg:61.09ms
step:2032/2330 train_time:124136ms step_avg:61.09ms
step:2033/2330 train_time:124197ms step_avg:61.09ms
step:2034/2330 train_time:124261ms step_avg:61.09ms
step:2035/2330 train_time:124322ms step_avg:61.09ms
step:2036/2330 train_time:124386ms step_avg:61.09ms
step:2037/2330 train_time:124447ms step_avg:61.09ms
step:2038/2330 train_time:124510ms step_avg:61.09ms
step:2039/2330 train_time:124571ms step_avg:61.09ms
step:2040/2330 train_time:124634ms step_avg:61.10ms
step:2041/2330 train_time:124695ms step_avg:61.09ms
step:2042/2330 train_time:124758ms step_avg:61.10ms
step:2043/2330 train_time:124819ms step_avg:61.10ms
step:2044/2330 train_time:124882ms step_avg:61.10ms
step:2045/2330 train_time:124942ms step_avg:61.10ms
step:2046/2330 train_time:125004ms step_avg:61.10ms
step:2047/2330 train_time:125065ms step_avg:61.10ms
step:2048/2330 train_time:125127ms step_avg:61.10ms
step:2049/2330 train_time:125187ms step_avg:61.10ms
step:2050/2330 train_time:125251ms step_avg:61.10ms
step:2051/2330 train_time:125313ms step_avg:61.10ms
step:2052/2330 train_time:125376ms step_avg:61.10ms
step:2053/2330 train_time:125438ms step_avg:61.10ms
step:2054/2330 train_time:125501ms step_avg:61.10ms
step:2055/2330 train_time:125563ms step_avg:61.10ms
step:2056/2330 train_time:125626ms step_avg:61.10ms
step:2057/2330 train_time:125686ms step_avg:61.10ms
step:2058/2330 train_time:125749ms step_avg:61.10ms
step:2059/2330 train_time:125809ms step_avg:61.10ms
step:2060/2330 train_time:125871ms step_avg:61.10ms
step:2061/2330 train_time:125931ms step_avg:61.10ms
step:2062/2330 train_time:125994ms step_avg:61.10ms
step:2063/2330 train_time:126054ms step_avg:61.10ms
step:2064/2330 train_time:126118ms step_avg:61.10ms
step:2065/2330 train_time:126179ms step_avg:61.10ms
step:2066/2330 train_time:126243ms step_avg:61.11ms
step:2067/2330 train_time:126304ms step_avg:61.11ms
step:2068/2330 train_time:126368ms step_avg:61.11ms
step:2069/2330 train_time:126428ms step_avg:61.11ms
step:2070/2330 train_time:126491ms step_avg:61.11ms
step:2071/2330 train_time:126552ms step_avg:61.11ms
step:2072/2330 train_time:126615ms step_avg:61.11ms
step:2073/2330 train_time:126676ms step_avg:61.11ms
step:2074/2330 train_time:126740ms step_avg:61.11ms
step:2075/2330 train_time:126800ms step_avg:61.11ms
step:2076/2330 train_time:126863ms step_avg:61.11ms
step:2077/2330 train_time:126924ms step_avg:61.11ms
step:2078/2330 train_time:126986ms step_avg:61.11ms
step:2079/2330 train_time:127047ms step_avg:61.11ms
step:2080/2330 train_time:127109ms step_avg:61.11ms
step:2081/2330 train_time:127170ms step_avg:61.11ms
step:2082/2330 train_time:127232ms step_avg:61.11ms
step:2083/2330 train_time:127293ms step_avg:61.11ms
step:2084/2330 train_time:127356ms step_avg:61.11ms
step:2085/2330 train_time:127417ms step_avg:61.11ms
step:2086/2330 train_time:127480ms step_avg:61.11ms
step:2087/2330 train_time:127541ms step_avg:61.11ms
step:2088/2330 train_time:127604ms step_avg:61.11ms
step:2089/2330 train_time:127665ms step_avg:61.11ms
step:2090/2330 train_time:127728ms step_avg:61.11ms
step:2091/2330 train_time:127788ms step_avg:61.11ms
step:2092/2330 train_time:127850ms step_avg:61.11ms
step:2093/2330 train_time:127911ms step_avg:61.11ms
step:2094/2330 train_time:127974ms step_avg:61.11ms
step:2095/2330 train_time:128035ms step_avg:61.11ms
step:2096/2330 train_time:128099ms step_avg:61.12ms
step:2097/2330 train_time:128159ms step_avg:61.12ms
step:2098/2330 train_time:128222ms step_avg:61.12ms
step:2099/2330 train_time:128283ms step_avg:61.12ms
step:2100/2330 train_time:128346ms step_avg:61.12ms
step:2101/2330 train_time:128407ms step_avg:61.12ms
step:2102/2330 train_time:128470ms step_avg:61.12ms
step:2103/2330 train_time:128531ms step_avg:61.12ms
step:2104/2330 train_time:128594ms step_avg:61.12ms
step:2105/2330 train_time:128655ms step_avg:61.12ms
step:2106/2330 train_time:128719ms step_avg:61.12ms
step:2107/2330 train_time:128779ms step_avg:61.12ms
step:2108/2330 train_time:128842ms step_avg:61.12ms
step:2109/2330 train_time:128902ms step_avg:61.12ms
step:2110/2330 train_time:128966ms step_avg:61.12ms
step:2111/2330 train_time:129026ms step_avg:61.12ms
step:2112/2330 train_time:129089ms step_avg:61.12ms
step:2113/2330 train_time:129149ms step_avg:61.12ms
step:2114/2330 train_time:129212ms step_avg:61.12ms
step:2115/2330 train_time:129272ms step_avg:61.12ms
step:2116/2330 train_time:129336ms step_avg:61.12ms
step:2117/2330 train_time:129397ms step_avg:61.12ms
step:2118/2330 train_time:129461ms step_avg:61.12ms
step:2119/2330 train_time:129521ms step_avg:61.12ms
step:2120/2330 train_time:129585ms step_avg:61.12ms
step:2121/2330 train_time:129645ms step_avg:61.12ms
step:2122/2330 train_time:129708ms step_avg:61.13ms
step:2123/2330 train_time:129769ms step_avg:61.13ms
step:2124/2330 train_time:129831ms step_avg:61.13ms
step:2125/2330 train_time:129892ms step_avg:61.13ms
step:2126/2330 train_time:129956ms step_avg:61.13ms
step:2127/2330 train_time:130016ms step_avg:61.13ms
step:2128/2330 train_time:130080ms step_avg:61.13ms
step:2129/2330 train_time:130140ms step_avg:61.13ms
step:2130/2330 train_time:130203ms step_avg:61.13ms
step:2131/2330 train_time:130264ms step_avg:61.13ms
step:2132/2330 train_time:130326ms step_avg:61.13ms
step:2133/2330 train_time:130387ms step_avg:61.13ms
step:2134/2330 train_time:130450ms step_avg:61.13ms
step:2135/2330 train_time:130510ms step_avg:61.13ms
step:2136/2330 train_time:130572ms step_avg:61.13ms
step:2137/2330 train_time:130633ms step_avg:61.13ms
step:2138/2330 train_time:130697ms step_avg:61.13ms
step:2139/2330 train_time:130758ms step_avg:61.13ms
step:2140/2330 train_time:130821ms step_avg:61.13ms
step:2141/2330 train_time:130882ms step_avg:61.13ms
step:2142/2330 train_time:130944ms step_avg:61.13ms
step:2143/2330 train_time:131004ms step_avg:61.13ms
step:2144/2330 train_time:131067ms step_avg:61.13ms
step:2145/2330 train_time:131127ms step_avg:61.13ms
step:2146/2330 train_time:131189ms step_avg:61.13ms
step:2147/2330 train_time:131250ms step_avg:61.13ms
step:2148/2330 train_time:131313ms step_avg:61.13ms
step:2149/2330 train_time:131375ms step_avg:61.13ms
step:2150/2330 train_time:131438ms step_avg:61.13ms
step:2151/2330 train_time:131499ms step_avg:61.13ms
step:2152/2330 train_time:131562ms step_avg:61.13ms
step:2153/2330 train_time:131622ms step_avg:61.13ms
step:2154/2330 train_time:131686ms step_avg:61.14ms
step:2155/2330 train_time:131747ms step_avg:61.14ms
step:2156/2330 train_time:131809ms step_avg:61.14ms
step:2157/2330 train_time:131869ms step_avg:61.14ms
step:2158/2330 train_time:131932ms step_avg:61.14ms
step:2159/2330 train_time:131993ms step_avg:61.14ms
step:2160/2330 train_time:132057ms step_avg:61.14ms
step:2161/2330 train_time:132118ms step_avg:61.14ms
step:2162/2330 train_time:132181ms step_avg:61.14ms
step:2163/2330 train_time:132242ms step_avg:61.14ms
step:2164/2330 train_time:132305ms step_avg:61.14ms
step:2165/2330 train_time:132365ms step_avg:61.14ms
step:2166/2330 train_time:132428ms step_avg:61.14ms
step:2167/2330 train_time:132488ms step_avg:61.14ms
step:2168/2330 train_time:132551ms step_avg:61.14ms
step:2169/2330 train_time:132613ms step_avg:61.14ms
step:2170/2330 train_time:132677ms step_avg:61.14ms
step:2171/2330 train_time:132738ms step_avg:61.14ms
step:2172/2330 train_time:132801ms step_avg:61.14ms
step:2173/2330 train_time:132863ms step_avg:61.14ms
step:2174/2330 train_time:132926ms step_avg:61.14ms
step:2175/2330 train_time:132987ms step_avg:61.14ms
step:2176/2330 train_time:133049ms step_avg:61.14ms
step:2177/2330 train_time:133109ms step_avg:61.14ms
step:2178/2330 train_time:133172ms step_avg:61.14ms
step:2179/2330 train_time:133233ms step_avg:61.14ms
step:2180/2330 train_time:133296ms step_avg:61.14ms
step:2181/2330 train_time:133357ms step_avg:61.14ms
step:2182/2330 train_time:133421ms step_avg:61.15ms
step:2183/2330 train_time:133481ms step_avg:61.15ms
step:2184/2330 train_time:133544ms step_avg:61.15ms
step:2185/2330 train_time:133604ms step_avg:61.15ms
step:2186/2330 train_time:133667ms step_avg:61.15ms
step:2187/2330 train_time:133728ms step_avg:61.15ms
step:2188/2330 train_time:133791ms step_avg:61.15ms
step:2189/2330 train_time:133852ms step_avg:61.15ms
step:2190/2330 train_time:133915ms step_avg:61.15ms
step:2191/2330 train_time:133976ms step_avg:61.15ms
step:2192/2330 train_time:134038ms step_avg:61.15ms
step:2193/2330 train_time:134099ms step_avg:61.15ms
step:2194/2330 train_time:134162ms step_avg:61.15ms
step:2195/2330 train_time:134223ms step_avg:61.15ms
step:2196/2330 train_time:134285ms step_avg:61.15ms
step:2197/2330 train_time:134346ms step_avg:61.15ms
step:2198/2330 train_time:134409ms step_avg:61.15ms
step:2199/2330 train_time:134469ms step_avg:61.15ms
step:2200/2330 train_time:134531ms step_avg:61.15ms
step:2201/2330 train_time:134592ms step_avg:61.15ms
step:2202/2330 train_time:134656ms step_avg:61.15ms
step:2203/2330 train_time:134717ms step_avg:61.15ms
step:2204/2330 train_time:134781ms step_avg:61.15ms
step:2205/2330 train_time:134842ms step_avg:61.15ms
step:2206/2330 train_time:134905ms step_avg:61.15ms
step:2207/2330 train_time:134966ms step_avg:61.15ms
step:2208/2330 train_time:135029ms step_avg:61.15ms
step:2209/2330 train_time:135090ms step_avg:61.15ms
step:2210/2330 train_time:135152ms step_avg:61.15ms
step:2211/2330 train_time:135212ms step_avg:61.15ms
step:2212/2330 train_time:135276ms step_avg:61.16ms
step:2213/2330 train_time:135337ms step_avg:61.16ms
step:2214/2330 train_time:135400ms step_avg:61.16ms
step:2215/2330 train_time:135461ms step_avg:61.16ms
step:2216/2330 train_time:135524ms step_avg:61.16ms
step:2217/2330 train_time:135585ms step_avg:61.16ms
step:2218/2330 train_time:135647ms step_avg:61.16ms
step:2219/2330 train_time:135708ms step_avg:61.16ms
step:2220/2330 train_time:135771ms step_avg:61.16ms
step:2221/2330 train_time:135832ms step_avg:61.16ms
step:2222/2330 train_time:135895ms step_avg:61.16ms
step:2223/2330 train_time:135956ms step_avg:61.16ms
step:2224/2330 train_time:136020ms step_avg:61.16ms
step:2225/2330 train_time:136081ms step_avg:61.16ms
step:2226/2330 train_time:136143ms step_avg:61.16ms
step:2227/2330 train_time:136204ms step_avg:61.16ms
step:2228/2330 train_time:136266ms step_avg:61.16ms
step:2229/2330 train_time:136326ms step_avg:61.16ms
step:2230/2330 train_time:136389ms step_avg:61.16ms
step:2231/2330 train_time:136449ms step_avg:61.16ms
step:2232/2330 train_time:136512ms step_avg:61.16ms
step:2233/2330 train_time:136573ms step_avg:61.16ms
step:2234/2330 train_time:136637ms step_avg:61.16ms
step:2235/2330 train_time:136698ms step_avg:61.16ms
step:2236/2330 train_time:136760ms step_avg:61.16ms
step:2237/2330 train_time:136821ms step_avg:61.16ms
step:2238/2330 train_time:136884ms step_avg:61.16ms
step:2239/2330 train_time:136944ms step_avg:61.16ms
step:2240/2330 train_time:137007ms step_avg:61.16ms
step:2241/2330 train_time:137068ms step_avg:61.16ms
step:2242/2330 train_time:137131ms step_avg:61.16ms
step:2243/2330 train_time:137192ms step_avg:61.16ms
step:2244/2330 train_time:137255ms step_avg:61.17ms
step:2245/2330 train_time:137316ms step_avg:61.17ms
step:2246/2330 train_time:137379ms step_avg:61.17ms
step:2247/2330 train_time:137440ms step_avg:61.17ms
step:2248/2330 train_time:137503ms step_avg:61.17ms
step:2249/2330 train_time:137564ms step_avg:61.17ms
step:2250/2330 train_time:137627ms step_avg:61.17ms
step:2250/2330 val_loss:3.2958 train_time:137691ms step_avg:61.20ms
step:2251/2330 train_time:137716ms step_avg:61.18ms
step:2252/2330 train_time:137754ms step_avg:61.17ms
step:2253/2330 train_time:137820ms step_avg:61.17ms
step:2254/2330 train_time:137884ms step_avg:61.17ms
step:2255/2330 train_time:137945ms step_avg:61.17ms
step:2256/2330 train_time:138007ms step_avg:61.17ms
step:2257/2330 train_time:138067ms step_avg:61.17ms
step:2258/2330 train_time:138129ms step_avg:61.17ms
step:2259/2330 train_time:138189ms step_avg:61.17ms
step:2260/2330 train_time:138251ms step_avg:61.17ms
step:2261/2330 train_time:138311ms step_avg:61.17ms
step:2262/2330 train_time:138373ms step_avg:61.17ms
step:2263/2330 train_time:138432ms step_avg:61.17ms
step:2264/2330 train_time:138494ms step_avg:61.17ms
step:2265/2330 train_time:138554ms step_avg:61.17ms
step:2266/2330 train_time:138616ms step_avg:61.17ms
step:2267/2330 train_time:138677ms step_avg:61.17ms
step:2268/2330 train_time:138743ms step_avg:61.17ms
step:2269/2330 train_time:138806ms step_avg:61.17ms
step:2270/2330 train_time:138869ms step_avg:61.18ms
step:2271/2330 train_time:138930ms step_avg:61.18ms
step:2272/2330 train_time:138993ms step_avg:61.18ms
step:2273/2330 train_time:139053ms step_avg:61.18ms
step:2274/2330 train_time:139115ms step_avg:61.18ms
step:2275/2330 train_time:139175ms step_avg:61.18ms
step:2276/2330 train_time:139237ms step_avg:61.18ms
step:2277/2330 train_time:139297ms step_avg:61.18ms
step:2278/2330 train_time:139359ms step_avg:61.18ms
step:2279/2330 train_time:139419ms step_avg:61.18ms
step:2280/2330 train_time:139481ms step_avg:61.18ms
step:2281/2330 train_time:139541ms step_avg:61.18ms
step:2282/2330 train_time:139604ms step_avg:61.18ms
step:2283/2330 train_time:139664ms step_avg:61.18ms
step:2284/2330 train_time:139727ms step_avg:61.18ms
step:2285/2330 train_time:139789ms step_avg:61.18ms
step:2286/2330 train_time:139852ms step_avg:61.18ms
step:2287/2330 train_time:139913ms step_avg:61.18ms
step:2288/2330 train_time:139975ms step_avg:61.18ms
step:2289/2330 train_time:140036ms step_avg:61.18ms
step:2290/2330 train_time:140099ms step_avg:61.18ms
step:2291/2330 train_time:140159ms step_avg:61.18ms
step:2292/2330 train_time:140221ms step_avg:61.18ms
step:2293/2330 train_time:140281ms step_avg:61.18ms
step:2294/2330 train_time:140344ms step_avg:61.18ms
step:2295/2330 train_time:140403ms step_avg:61.18ms
step:2296/2330 train_time:140465ms step_avg:61.18ms
step:2297/2330 train_time:140526ms step_avg:61.18ms
step:2298/2330 train_time:140589ms step_avg:61.18ms
step:2299/2330 train_time:140650ms step_avg:61.18ms
step:2300/2330 train_time:140713ms step_avg:61.18ms
step:2301/2330 train_time:140774ms step_avg:61.18ms
step:2302/2330 train_time:140837ms step_avg:61.18ms
step:2303/2330 train_time:140898ms step_avg:61.18ms
step:2304/2330 train_time:140961ms step_avg:61.18ms
step:2305/2330 train_time:141022ms step_avg:61.18ms
step:2306/2330 train_time:141085ms step_avg:61.18ms
step:2307/2330 train_time:141145ms step_avg:61.18ms
step:2308/2330 train_time:141207ms step_avg:61.18ms
step:2309/2330 train_time:141268ms step_avg:61.18ms
step:2310/2330 train_time:141330ms step_avg:61.18ms
step:2311/2330 train_time:141390ms step_avg:61.18ms
step:2312/2330 train_time:141452ms step_avg:61.18ms
step:2313/2330 train_time:141513ms step_avg:61.18ms
step:2314/2330 train_time:141575ms step_avg:61.18ms
step:2315/2330 train_time:141635ms step_avg:61.18ms
step:2316/2330 train_time:141699ms step_avg:61.18ms
step:2317/2330 train_time:141759ms step_avg:61.18ms
step:2318/2330 train_time:141822ms step_avg:61.18ms
step:2319/2330 train_time:141883ms step_avg:61.18ms
step:2320/2330 train_time:141946ms step_avg:61.18ms
step:2321/2330 train_time:142007ms step_avg:61.18ms
step:2322/2330 train_time:142070ms step_avg:61.18ms
step:2323/2330 train_time:142130ms step_avg:61.18ms
step:2324/2330 train_time:142192ms step_avg:61.18ms
step:2325/2330 train_time:142253ms step_avg:61.18ms
step:2326/2330 train_time:142316ms step_avg:61.18ms
step:2327/2330 train_time:142375ms step_avg:61.18ms
step:2328/2330 train_time:142437ms step_avg:61.18ms
step:2329/2330 train_time:142498ms step_avg:61.18ms
step:2330/2330 train_time:142560ms step_avg:61.18ms
step:2330/2330 val_loss:3.2820 train_time:142625ms step_avg:61.21ms
peak memory allocated: 29226 MiB reserved: 44076 MiB
